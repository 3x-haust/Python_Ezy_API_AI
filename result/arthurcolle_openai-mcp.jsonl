{"repo_info": {"repo_name": "openai-mcp", "repo_owner": "arthurcolle", "repo_url": "https://github.com/arthurcolle/openai-mcp"}}
{"type": "source_file", "path": "claude.py", "content": "#!/usr/bin/env python3\n\"\"\"Main entry point for Claude Code.\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport logging\nfrom typing import Optional, List, Dict, Any\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\n\ndef main() -> int:\n    \"\"\"Main entry point for Claude Code.\n    \n    Returns:\n        Exit code\n    \"\"\"\n    # Create the main parser\n    parser = argparse.ArgumentParser(\n        description=\"Claude Code - A powerful LLM-powered CLI for software development\"\n    )\n    \n    # Add version information\n    from claude_code import __version__\n    parser.add_argument(\n        \"--version\", \n        action=\"version\", \n        version=f\"Claude Code v{__version__}\"\n    )\n    \n    # Create subparsers for commands\n    subparsers = parser.add_subparsers(\n        title=\"commands\",\n        dest=\"command\",\n        help=\"Command to execute\"\n    )\n    \n    # Add the chat command (default)\n    chat_parser = subparsers.add_parser(\n        \"chat\", \n        help=\"Start an interactive chat session with Claude Code\"\n    )\n    # Add chat-specific arguments here\n    \n    # Add the serve command for MCP server\n    serve_parser = subparsers.add_parser(\n        \"serve\", \n        help=\"Start the Claude Code MCP server\"\n    )\n    \n    # Add serve-specific arguments\n    from claude_code.commands.serve import add_arguments\n    add_arguments(serve_parser)\n    \n    # Parse arguments\n    args = parser.parse_args()\n    \n    # If no command specified, default to chat\n    if not args.command:\n        args.command = \"chat\"\n        \n    # Execute the appropriate command\n    if args.command == \"chat\":\n        # Import and run the chat command\n        from claude_code.claude import main as chat_main\n        return chat_main()\n    elif args.command == \"serve\":\n        # Import and run the serve command\n        from claude_code.commands.serve import execute\n        return execute(args)\n    else:\n        parser.print_help()\n        return 1\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())"}
{"type": "source_file", "path": "claude_code/claude.py", "content": "#!/usr/bin/env python3\n# claude.py\n\"\"\"Claude Code Python Edition - CLI entry point.\"\"\"\n\nimport os\nimport sys\nimport logging\nimport argparse\nfrom typing import Dict, List, Optional, Any\nimport json\nimport signal\nfrom datetime import datetime\n\nimport typer\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.markdown import Markdown\nfrom rich.prompt import Prompt\nfrom rich.syntax import Syntax\nfrom rich.logging import RichHandler\nfrom dotenv import load_dotenv\n\nfrom claude_code.lib.providers import get_provider, list_available_providers\nfrom claude_code.lib.tools.base import ToolRegistry\nfrom claude_code.lib.tools.manager import ToolExecutionManager\nfrom claude_code.lib.tools.file_tools import register_file_tools\nfrom claude_code.lib.ui.tool_visualizer import ToolCallVisualizer, MultiPanelLayout\nfrom claude_code.lib.monitoring.cost_tracker import CostTracker\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(message)s\",\n    datefmt=\"[%X]\",\n    handlers=[RichHandler(rich_tracebacks=True)]\n)\nlogger = logging.getLogger(\"claude_code\")\n\n# Load environment variables\nload_dotenv()\n\n# Get version from package\nVERSION = \"0.1.0\"\n\n# Create typer app\napp = typer.Typer(help=\"Claude Code Python Edition\")\nconsole = Console()\n\n# Global state\nconversation: List[Dict[str, Any]] = []\ntool_registry = ToolRegistry()\ntool_manager: Optional[ToolExecutionManager] = None\ncost_tracker: Optional[CostTracker] = None\nvisualizer: Optional[ToolCallVisualizer] = None\nprovider_name: str = \"\"\nmodel_name: str = \"\"\nuser_config: Dict[str, Any] = {}\n\n\ndef initialize_tools() -> None:\n    \"\"\"Initialize all available tools.\"\"\"\n    global tool_registry, tool_manager\n    \n    # Create the registry and manager\n    tool_registry = ToolRegistry()\n    tool_manager = ToolExecutionManager(tool_registry)\n    \n    # Register file tools\n    register_file_tools(tool_registry)\n    \n    # TODO: Register more tools\n    # register_search_tools(tool_registry)\n    # register_bash_tools(tool_registry)\n    # register_agent_tools(tool_registry)\n    \n    logger.info(f\"Initialized {len(tool_registry.get_all_tools())} tools\")\n\n\ndef setup_visualizer() -> None:\n    \"\"\"Set up the tool visualizer with callbacks.\"\"\"\n    global tool_manager, visualizer\n    \n    if not tool_manager:\n        return\n    \n    # Create visualizer\n    visualizer = ToolCallVisualizer(console)\n    \n    # Set up callbacks\n    def progress_callback(tool_call_id: str, progress: float) -> None:\n        if visualizer:\n            visualizer.update_progress(tool_call_id, progress)\n    \n    def result_callback(tool_call_id: str, result: Any) -> None:\n        if visualizer:\n            visualizer.complete_tool_call(tool_call_id, result)\n    \n    tool_manager.set_progress_callback(progress_callback)\n    tool_manager.set_result_callback(result_callback)\n\n\ndef load_configuration() -> Dict[str, Any]:\n    \"\"\"Load user configuration from file.\"\"\"\n    config_path = os.path.expanduser(\"~/.config/claude_code/config.json\")\n    \n    # Default configuration\n    default_config = {\n        \"provider\": \"openai\",\n        \"model\": None,  # Use provider default\n        \"budget_limit\": None,\n        \"history_file\": os.path.expanduser(\"~/.config/claude_code/usage_history.json\"),\n        \"ui\": {\n            \"theme\": \"dark\",\n            \"show_tool_calls\": True,\n            \"show_cost\": True\n        }\n    }\n    \n    # If configuration file doesn't exist, create it with defaults\n    if not os.path.exists(config_path):\n        try:\n            os.makedirs(os.path.dirname(config_path), exist_ok=True)\n            with open(config_path, 'w', encoding='utf-8') as f:\n                json.dump(default_config, f, indent=2)\n        except Exception as e:\n            logger.warning(f\"Failed to create default configuration: {e}\")\n            return default_config\n    \n    # Load configuration\n    try:\n        with open(config_path, 'r', encoding='utf-8') as f:\n            config = json.load(f)\n            # Merge with defaults for any missing keys\n            for key, value in default_config.items():\n                if key not in config:\n                    config[key] = value\n            return config\n    except Exception as e:\n        logger.warning(f\"Failed to load configuration: {e}\")\n        return default_config\n\n\ndef handle_compact_command() -> str:\n    \"\"\"Handle the /compact command to compress conversation history.\"\"\"\n    global conversation, provider_name, model_name\n    \n    if not conversation:\n        return \"No conversation to compact.\"\n    \n    # Add a system message requesting summarization\n    compact_prompt = (\n        \"Summarize the conversation so far, focusing on the key points, decisions, and context. \"\n        \"Keep important details about the code and tasks. Retain critical file paths, commands, \"\n        \"and code snippets. The summary should be concise but complete enough to continue the \"\n        \"conversation effectively.\"\n    )\n    \n    conversation.append({\"role\": \"user\", \"content\": compact_prompt})\n    \n    # Get the provider\n    provider = get_provider(provider_name, model=model_name)\n    \n    # Make non-streaming API call for compaction\n    response = provider.generate_completion(conversation, stream=False)\n    \n    # Extract summary\n    summary = response[\"content\"] or \"\"\n    \n    # Reset conversation with summary\n    system_message = next((m for m in conversation if m[\"role\"] == \"system\"), None)\n    \n    if system_message:\n        conversation = [system_message]\n    else:\n        conversation = []\n    \n    # Add compacted context\n    conversation.append({\n        \"role\": \"system\", \n        \"content\": f\"This is a compacted conversation. Previous context: {summary}\"\n    })\n    \n    return \"Conversation compacted successfully.\"\n\n\ndef handle_help_command() -> str:\n    \"\"\"Handle the /help command.\"\"\"\n    help_text = \"\"\"\n# Claude Code Python Edition Help\n\n## Commands\n- **/help**: Show this help message\n- **/compact**: Compact the conversation to reduce token usage\n- **/version**: Show version information\n- **/providers**: List available LLM providers\n- **/cost**: Show cost and usage information\n- **/budget [amount]**: Set a budget limit (e.g., /budget 5.00)\n- **/quit, /exit**: Exit the application\n\n## Routine Commands\n- **/routine list**: List all available routines\n- **/routine create <name> <description>**: Create a routine from recent tool executions\n- **/routine run <name>**: Run a routine\n- **/routine delete <name>**: Delete a routine\n\n## Tools\nClaude Code has access to these tools:\n- **View**: Read files\n- **Edit**: Edit files (replace text)\n- **Replace**: Overwrite or create files\n- **GlobTool**: Find files by pattern\n- **GrepTool**: Search file contents\n- **LS**: List directory contents\n- **Bash**: Execute shell commands\n\n## CLI Commands\n- **claude**: Start the Claude Code assistant (main interface)\n- **claude mcp-client**: Start the MCP client to connect to MCP servers\n  - Usage: `claude mcp-client path/to/server.py [--model MODEL]`\n- **claude mcp-multi-agent**: Start the multi-agent MCP client with synchronized agents\n  - Usage: `claude mcp-multi-agent path/to/server.py [--config CONFIG_FILE]`\n\n## Multi-Agent Commands\nWhen using the multi-agent client:\n- **/agents**: List all active agents\n- **/talk <agent> <message>**: Send a direct message to a specific agent\n- **/history**: Show message history\n- **/help**: Show multi-agent help\n\n## Tips\n- Be specific about file paths when requesting file operations\n- For complex tasks, break them down into smaller steps\n- Use /compact periodically for long sessions to save tokens\n- Create routines for repetitive sequences of tool operations\n- In multi-agent mode, use agent specialization for complex problems\n    \"\"\"\n    return help_text\n\n\ndef handle_version_command() -> str:\n    \"\"\"Handle the /version command.\"\"\"\n    import platform\n    python_version = platform.python_version()\n    \n    version_info = f\"\"\"\n# Claude Code Python Edition v{VERSION}\n\n- Python: {python_version}\n- Provider: {provider_name}\n- Model: {model_name}\n- Tools: {len(tool_registry.get_all_tools()) if tool_registry else 0} available\n    \"\"\"\n    return version_info\n\n\ndef handle_providers_command() -> str:\n    \"\"\"Handle the /providers command.\"\"\"\n    providers = list_available_providers()\n    \n    providers_text = \"# Available LLM Providers\\n\\n\"\n    \n    for name, info in providers.items():\n        providers_text += f\"## {info['name']}\\n\"\n        \n        if info['available']:\n            providers_text += f\"- Status: Available\\n\"\n            providers_text += f\"- Current model: {info['current_model']}\\n\"\n            providers_text += f\"- Available models: {', '.join(info['models'])}\\n\"\n        else:\n            providers_text += f\"- Status: Not available ({info['error']})\\n\"\n        \n        providers_text += \"\\n\"\n    \n    return providers_text\n\n\ndef handle_cost_command() -> str:\n    \"\"\"Handle the /cost command.\"\"\"\n    global cost_tracker\n    \n    if not cost_tracker:\n        return \"Cost tracking is not available.\"\n    \n    # Generate a usage report\n    return cost_tracker.generate_usage_report(format=\"markdown\")\n\n\ndef handle_budget_command(args: List[str]) -> str:\n    \"\"\"Handle the /budget command.\"\"\"\n    global cost_tracker\n    \n    if not cost_tracker:\n        return \"Cost tracking is not available.\"\n    \n    if not args:\n        # Show current budget\n        budget = cost_tracker.check_budget()\n        if not budget[\"has_budget\"]:\n            return \"No budget limit is currently set.\"\n        \n        return f\"Current budget: ${budget['limit']:.2f} (${budget['used']:.2f} used, ${budget['remaining']:.2f} remaining)\"\n    \n    # Set new budget\n    try:\n        budget_amount = float(args[0])\n        if budget_amount <= 0:\n            return \"Budget must be a positive number.\"\n        \n        cost_tracker.budget_limit = budget_amount\n        \n        # Update configuration\n        user_config[\"budget_limit\"] = budget_amount\n        \n        # Save configuration\n        config_path = os.path.expanduser(\"~/.config/claude_code/config.json\")\n        try:\n            with open(config_path, 'w', encoding='utf-8') as f:\n                json.dump(user_config, f, indent=2)\n        except Exception as e:\n            logger.warning(f\"Failed to save configuration: {e}\")\n        \n        return f\"Budget set to ${budget_amount:.2f}\"\n    \n    except ValueError:\n        return f\"Invalid budget amount: {args[0]}\"\n\n\ndef handle_routine_list_command() -> str:\n    \"\"\"Handle the /routine list command.\"\"\"\n    global tool_manager\n    \n    if not tool_manager:\n        return \"Tool manager is not initialized.\"\n    \n    routines = tool_manager.registry.get_all_routines()\n    if not routines:\n        return \"No routines available.\"\n    \n    routines_text = \"# Available Routines\\n\\n\"\n    \n    for routine in routines:\n        usage = f\" (Used {routine.usage_count} times)\" if routine.usage_count > 0 else \"\"\n        last_used = \"\"\n        if routine.last_used_at:\n            last_used_time = datetime.fromtimestamp(routine.last_used_at)\n            last_used = f\" (Last used: {last_used_time.strftime('%Y-%m-%d %H:%M')})\"\n        \n        routines_text += f\"## {routine.name}{usage}{last_used}\\n\"\n        routines_text += f\"{routine.description}\\n\\n\"\n        routines_text += f\"**Steps:** {len(routine.steps)}\\n\\n\"\n    \n    return routines_text\n\n\ndef handle_routine_create_command(args: List[str]) -> str:\n    \"\"\"Handle the /routine create command.\"\"\"\n    global tool_manager, visualizer\n    \n    if not tool_manager:\n        return \"Tool manager is not initialized.\"\n    \n    if len(args) < 2:\n        return \"Usage: /routine create <name> <description>\"\n    \n    name = args[0]\n    description = \" \".join(args[1:])\n    \n    # Get recent tool results from visualizer\n    if not visualizer or not hasattr(visualizer, \"recent_tool_results\"):\n        return \"No recent tool executions to create a routine from.\"\n    \n    recent_tool_results = visualizer.recent_tool_results\n    if not recent_tool_results:\n        return \"No recent tool executions to create a routine from.\"\n    \n    try:\n        routine_id = tool_manager.create_routine_from_tool_history(\n            name, description, recent_tool_results\n        )\n        return f\"Created routine '{name}' with {len(recent_tool_results)} steps.\"\n    except Exception as e:\n        logger.exception(f\"Error creating routine: {e}\")\n        return f\"Error creating routine: {str(e)}\"\n\n\ndef handle_routine_run_command(args: List[str]) -> str:\n    \"\"\"Handle the /routine run command.\"\"\"\n    global tool_manager, visualizer\n    \n    if not tool_manager:\n        return \"Tool manager is not initialized.\"\n    \n    if not args:\n        return \"Usage: /routine run <name>\"\n    \n    name = args[0]\n    \n    # Check if routine exists\n    routine = tool_manager.registry.get_routine(name)\n    if not routine:\n        return f\"Routine '{name}' not found.\"\n    \n    try:\n        # Execute the routine\n        execution_id = tool_manager.execute_routine(name)\n        \n        # Wait for completion\n        while True:\n            routine_status = tool_manager.routine_manager.get_active_routines().get(execution_id, {})\n            if routine_status.get(\"status\") != \"running\":\n                break\n            time.sleep(0.1)\n        \n        # Get results\n        results = tool_manager.get_routine_results(execution_id)\n        if not results:\n            return f\"Routine '{name}' completed but returned no results.\"\n        \n        # Format results\n        result_text = f\"# Routine '{name}' Results\\n\\n\"\n        result_text += f\"Executed {len(results)} steps:\\n\\n\"\n        \n        for i, result in enumerate(results):\n            status = \"✅\" if result.status == \"success\" else \"❌\"\n            result_text += f\"## Step {i+1}: {result.name} {status}\\n\"\n            result_text += f\"```\\n{result.result}\\n```\\n\\n\"\n        \n        return result_text\n    \n    except Exception as e:\n        logger.exception(f\"Error executing routine: {e}\")\n        return f\"Error executing routine: {str(e)}\"\n\n\ndef handle_routine_delete_command(args: List[str]) -> str:\n    \"\"\"Handle the /routine delete command.\"\"\"\n    global tool_manager\n    \n    if not tool_manager:\n        return \"Tool manager is not initialized.\"\n    \n    if not args:\n        return \"Usage: /routine delete <name>\"\n    \n    name = args[0]\n    \n    # Check if routine exists\n    routine = tool_manager.registry.get_routine(name)\n    if not routine:\n        return f\"Routine '{name}' not found.\"\n    \n    try:\n        # Remove from registry and save\n        tool_manager.registry.routines.pop(name, None)\n        tool_manager.registry._save_routines()\n        return f\"Deleted routine '{name}'.\"\n    except Exception as e:\n        logger.exception(f\"Error deleting routine: {e}\")\n        return f\"Error deleting routine: {str(e)}\"\n\n\ndef process_special_command(user_input: str) -> Optional[str]:\n    \"\"\"Process special commands starting with /.\"\"\"\n    # Split into command and arguments\n    parts = user_input.strip().split()\n    command = parts[0].lower()\n    args = parts[1:]\n    \n    # Handle commands\n    if command == \"/help\":\n        return handle_help_command()\n    elif command == \"/compact\":\n        return handle_compact_command()\n    elif command == \"/version\":\n        return handle_version_command()\n    elif command == \"/providers\":\n        return handle_providers_command()\n    elif command == \"/cost\":\n        return handle_cost_command()\n    elif command == \"/budget\":\n        return handle_budget_command(args)\n    elif command in [\"/quit\", \"/exit\"]:\n        console.print(\"[bold yellow]Goodbye![/bold yellow]\")\n        sys.exit(0)\n    \n    # Handle routine commands\n    elif command == \"/routine\":\n        if not args:\n            return \"Usage: /routine [list|create|run|delete]\"\n        \n        subcmd = args[0].lower()\n        if subcmd == \"list\":\n            return handle_routine_list_command()\n        elif subcmd == \"create\":\n            return handle_routine_create_command(args[1:])\n        elif subcmd == \"run\":\n            return handle_routine_run_command(args[1:])\n        elif subcmd == \"delete\":\n            return handle_routine_delete_command(args[1:])\n        else:\n            return f\"Unknown routine command: {subcmd}\\nUsage: /routine [list|create|run|delete]\"\n    \n    # Not a recognized command\n    return None\n\n\ndef process_tool_calls(tool_calls: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"Process tool calls and return results.\n    \n    Args:\n        tool_calls: List of tool call dictionaries\n        \n    Returns:\n        List of tool responses\n    \"\"\"\n    global tool_manager, visualizer\n    \n    if not tool_manager:\n        logger.error(\"Tool manager not initialized\")\n        return []\n    \n    # Add tool calls to visualizer\n    if visualizer:\n        for tool_call in tool_calls:\n            function_name = tool_call.get(\"function\", {}).get(\"name\", \"\")\n            tool_call_id = tool_call.get(\"id\", \"unknown\")\n            arguments_str = tool_call.get(\"function\", {}).get(\"arguments\", \"{}\")\n            \n            try:\n                parameters = json.loads(arguments_str)\n                visualizer.add_tool_call(tool_call_id, function_name, parameters)\n            except json.JSONDecodeError:\n                visualizer.add_tool_call(tool_call_id, function_name, {})\n    \n    # Execute tools in parallel\n    tool_results = tool_manager.execute_tools_parallel(tool_calls)\n    \n    # Format results for the conversation\n    tool_responses = []\n    for result in tool_results:\n        tool_responses.append({\n            \"tool_call_id\": result.tool_call_id,\n            \"role\": \"tool\",\n            \"name\": result.name,\n            \"content\": result.result\n        })\n    \n    return tool_responses\n\n\n@app.command(name=\"mcp-client\")\ndef mcp_client(\n    server_script: str = typer.Argument(..., help=\"Path to the server script (.py or .js)\"),\n    model: str = typer.Option(\"claude-3-5-sonnet-20241022\", \"--model\", \"-m\", help=\"Claude model to use\")\n):\n    \"\"\"Run the MCP client to interact with an MCP server.\"\"\"\n    from claude_code.commands.client import execute as client_execute\n    import argparse\n    \n    # Create a namespace with the arguments\n    args = argparse.Namespace()\n    args.server_script = server_script\n    args.model = model\n    \n    # Execute the client\n    return client_execute(args)\n\n\n@app.command(name=\"mcp-multi-agent\")\ndef mcp_multi_agent(\n    server_script: str = typer.Argument(..., help=\"Path to the server script (.py or .js)\"),\n    config: str = typer.Option(None, \"--config\", \"-c\", help=\"Path to agent configuration JSON file\")\n):\n    \"\"\"Run the multi-agent MCP client with agent synchronization.\"\"\"\n    from claude_code.commands.multi_agent_client import execute as multi_agent_execute\n    import argparse\n    \n    # Create a namespace with the arguments\n    args = argparse.Namespace()\n    args.server_script = server_script\n    args.config = config\n    \n    # Execute the multi-agent client\n    return multi_agent_execute(args)\n\n\n@app.command()\ndef main(\n    provider: str = typer.Option(None, \"--provider\", \"-p\", help=\"LLM provider to use\"),\n    model: str = typer.Option(None, \"--model\", \"-m\", help=\"Model to use\"),\n    budget: Optional[float] = typer.Option(None, \"--budget\", \"-b\", help=\"Budget limit in dollars\"),\n    system_prompt: Optional[str] = typer.Option(None, \"--system\", \"-s\", help=\"System prompt file\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\", help=\"Enable verbose output\")\n):\n    \"\"\"Claude Code Python Edition - A LLM-powered coding assistant.\"\"\"\n    global conversation, tool_registry, tool_manager, cost_tracker, visualizer\n    global provider_name, model_name, user_config\n    \n    # Set logging level\n    if verbose:\n        logging.getLogger(\"claude_code\").setLevel(logging.DEBUG)\n    \n    # Show welcome message\n    console.print(Panel.fit(\n        f\"[bold green]Claude Code Python Edition v{VERSION}[/bold green]\\n\"\n        \"Type your questions or commands. Use /help for available commands.\",\n        title=\"Welcome\",\n        border_style=\"green\"\n    ))\n    \n    # Load configuration\n    user_config = load_configuration()\n    \n    # Override with command line arguments\n    if provider:\n        user_config[\"provider\"] = provider\n    if model:\n        user_config[\"model\"] = model\n    if budget is not None:\n        user_config[\"budget_limit\"] = budget\n    \n    # Set provider and model\n    provider_name = user_config[\"provider\"]\n    model_name = user_config[\"model\"]\n    \n    try:\n        # Initialize tools\n        initialize_tools()\n        \n        # Set up cost tracking\n        cost_tracker = CostTracker(\n            budget_limit=user_config[\"budget_limit\"],\n            history_file=user_config[\"history_file\"]\n        )\n        \n        # Get provider\n        provider = get_provider(provider_name, model=model_name)\n        provider_name = provider.name\n        model_name = provider.current_model\n        \n        logger.info(f\"Using {provider_name} with model {model_name}\")\n        \n        # Set up tool visualizer\n        setup_visualizer()\n        if visualizer:\n            visualizer.start()\n        \n        # Load system prompt\n        system_message = \"\"\n        if system_prompt:\n            try:\n                with open(system_prompt, 'r', encoding='utf-8') as f:\n                    system_message = f.read()\n            except Exception as e:\n                logger.error(f\"Failed to load system prompt: {e}\")\n                system_message = get_default_system_prompt()\n        else:\n            system_message = get_default_system_prompt()\n        \n        # Initialize conversation\n        conversation = [{\"role\": \"system\", \"content\": system_message}]\n        \n        # Main interaction loop\n        while True:\n            try:\n                # Get user input\n                user_input = Prompt.ask(\"\\n[bold blue]>>[/bold blue]\")\n                \n                # Handle special commands\n                if user_input.startswith(\"/\"):\n                    result = process_special_command(user_input)\n                    if result:\n                        console.print(Markdown(result))\n                        continue\n                \n                # Add user message to conversation\n                conversation.append({\"role\": \"user\", \"content\": user_input})\n                \n                # Get schemas for all tools\n                tool_schemas = tool_registry.get_tool_schemas() if tool_registry else None\n                \n                # Call the LLM\n                with console.status(\"[bold blue]Thinking...[/bold blue]\", spinner=\"dots\"):\n                    # Stream the response\n                    response_stream = provider.generate_completion(\n                        messages=conversation,\n                        tools=tool_schemas,\n                        stream=True\n                    )\n                    \n                    # Track tool calls from streaming response\n                    current_content = \"\"\n                    current_tool_calls = []\n                    \n                    # Process streaming response\n                    for chunk in response_stream:\n                        # If there's content, print it\n                        if chunk.get(\"content\"):\n                            content_piece = chunk[\"content\"]\n                            current_content += content_piece\n                            console.print(content_piece, end=\"\")\n                        \n                        # Process tool calls\n                        if chunk.get(\"tool_calls\") and not chunk.get(\"delta\", True):\n                            # This is a complete tool call\n                            current_tool_calls = chunk[\"tool_calls\"]\n                            break\n                    \n                    console.print()  # Add newline after content\n                    \n                    # Add assistant response to conversation\n                    conversation.append({\n                        \"role\": \"assistant\", \n                        \"content\": current_content,\n                        \"tool_calls\": current_tool_calls\n                    })\n                    \n                    # Process tool calls if any\n                    if current_tool_calls:\n                        console.print(\"[bold green]Executing tools...[/bold green]\")\n                        \n                        # Process tool calls\n                        tool_responses = process_tool_calls(current_tool_calls)\n                        \n                        # Add tool responses to conversation\n                        conversation.extend(tool_responses)\n                        \n                        # Continue the conversation with tool responses\n                        console.print(\"[bold blue]Continuing with tool results...[/bold blue]\")\n                        \n                        follow_up = provider.generate_completion(\n                            messages=conversation,\n                            tools=tool_schemas,\n                            stream=False\n                        )\n                        \n                        follow_up_text = follow_up.get(\"content\", \"\")\n                        if follow_up_text:\n                            console.print(Markdown(follow_up_text))\n                            \n                            # Add to conversation\n                            conversation.append({\n                                \"role\": \"assistant\", \n                                \"content\": follow_up_text\n                            })\n                    \n                    # Track token usage and cost\n                    if cost_tracker:\n                        # Get token counts - this is an approximation\n                        token_counts = provider.count_message_tokens(conversation[-3:])\n                        cost_info = provider.cost_per_1k_tokens\n                        \n                        # Add request to tracker\n                        cost_tracker.add_request(\n                            provider=provider_name,\n                            model=model_name,\n                            tokens_input=token_counts[\"input\"],\n                            tokens_output=token_counts.get(\"output\", 0) or 150,  # Estimate if not available\n                            input_cost_per_1k=cost_info[\"input\"],\n                            output_cost_per_1k=cost_info[\"output\"]\n                        )\n                        \n                        # Check budget\n                        budget_status = cost_tracker.check_budget()\n                        if budget_status[\"has_budget\"] and budget_status[\"status\"] in [\"critical\", \"exceeded\"]:\n                            console.print(f\"[bold red]{budget_status['message']}[/bold red]\")\n                \n            except KeyboardInterrupt:\n                console.print(\"\\n[bold yellow]Operation cancelled by user.[/bold yellow]\")\n                continue\n            except Exception as e:\n                logger.exception(f\"Error: {str(e)}\")\n                console.print(f\"[bold red]Error:[/bold red] {str(e)}\")\n    \n    finally:\n        # Clean up\n        if visualizer:\n            visualizer.stop()\n        \n        # Save cost history\n        if cost_tracker and hasattr(cost_tracker, '_save_history'):\n            cost_tracker._save_history()\n\n\ndef get_default_system_prompt() -> str:\n    \"\"\"Get the default system prompt.\"\"\"\n    return \"\"\"You are Claude Code Python Edition, a CLI tool that helps users with software engineering tasks.\nUse the available tools to assist the user with their requests.\n\n# Tone and style\nYou should be concise, direct, and to the point. When you run a non-trivial bash command, \nyou should explain what the command does and why you are running it.\nOutput text to communicate with the user; all text you output outside of tool use is displayed to the user.\nRemember that your output will be displayed on a command line interface.\n\n# Tool usage policy\n- When doing file search, remember to search effectively with the available tools.\n- Always use the appropriate tool for the task.\n- Use parallel tool calls when appropriate to improve performance.\n- NEVER commit changes unless the user explicitly asks you to.\n\n# Routines\nYou have access to Routines, which are sequences of tool calls that can be created and reused.\nTo create a routine from recent tool executions, use `/routine create <name> <description>`.\nTo run a routine, use `/routine run <name>`.\nRoutines are ideal for repetitive task sequences like:\n- Deep research across multiple sources\n- Multi-step code updates across files\n- Complex search and replace operations\n- Data processing pipelines\n\n# Tasks\nThe user will primarily request you perform software engineering tasks:\n1. Solving bugs\n2. Adding new functionality \n3. Refactoring code\n4. Explaining code\n5. Writing tests\n\nFor these tasks:\n1. Use search tools to understand the codebase\n2. Implement solutions using the available tools\n3. Verify solutions with tests if possible\n4. Run lint and typecheck commands when appropriate\n5. Consider creating routines for repetitive operations\n\n# Code style\n- Follow the existing code style of the project\n- Maintain consistent naming conventions\n- Use appropriate libraries that are already in the project\n- Add comments when code is complex or non-obvious\n\nIMPORTANT: You should minimize output tokens as much as possible while maintaining helpfulness, \nquality, and accuracy. Answer concisely with short lines of text unless the user asks for detail.\n\"\"\"\n\n\nif __name__ == \"__main__\":\n    # Handle Ctrl+C gracefully\n    signal.signal(signal.SIGINT, lambda sig, frame: sys.exit(0))\n    \n    # Run app\n    app()"}
{"type": "source_file", "path": "claude_code/lib/monitoring/cost_tracker.py", "content": "#!/usr/bin/env python3\n# claude_code/lib/monitoring/cost_tracker.py\n\"\"\"Cost tracking and management.\"\"\"\n\nimport logging\nimport json\nimport os\nimport time\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Tuple\n\nfrom rich.panel import Panel\nfrom rich.table import Table\nfrom rich.text import Text\nfrom rich.box import ROUNDED\n\nlogger = logging.getLogger(__name__)\n\n\nclass CostTracker:\n    \"\"\"Tracks token usage and calculates costs for LLM interactions.\"\"\"\n    \n    def __init__(self, budget_limit: Optional[float] = None, history_file: Optional[str] = None):\n        \"\"\"Initialize the cost tracker.\n        \n        Args:\n            budget_limit: Optional budget limit in dollars\n            history_file: Optional path to a file to store history\n        \"\"\"\n        self.budget_limit = budget_limit\n        self.history_file = history_file\n        \n        # Initialize session counters\n        self.session_start = datetime.now()\n        self.session_tokens_input = 0\n        self.session_tokens_output = 0\n        self.session_cost = 0.0\n        \n        # Request history\n        self.requests: List[Dict[str, Any]] = []\n        \n        # Load history from file if provided\n        self._load_history()\n    \n    def add_request(self, \n                   provider: str, \n                   model: str, \n                   tokens_input: int, \n                   tokens_output: int,\n                   input_cost_per_1k: float,\n                   output_cost_per_1k: float,\n                   request_id: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Add a request to the tracker.\n        \n        Args:\n            provider: Provider name (e.g., \"openai\", \"anthropic\")\n            model: Model name (e.g., \"gpt-4o\", \"claude-3-opus\")\n            tokens_input: Number of input tokens\n            tokens_output: Number of output tokens\n            input_cost_per_1k: Cost per 1,000 input tokens\n            output_cost_per_1k: Cost per 1,000 output tokens\n            request_id: Optional request ID\n            \n        Returns:\n            Dictionary with request information including costs\n        \"\"\"\n        # Calculate costs\n        input_cost = (tokens_input / 1000) * input_cost_per_1k\n        output_cost = (tokens_output / 1000) * output_cost_per_1k\n        total_cost = input_cost + output_cost\n        \n        # Update session counters\n        self.session_tokens_input += tokens_input\n        self.session_tokens_output += tokens_output\n        self.session_cost += total_cost\n        \n        # Create request record\n        request = {\n            \"id\": request_id or f\"{int(time.time())}-{len(self.requests)}\",\n            \"timestamp\": datetime.now().isoformat(),\n            \"provider\": provider,\n            \"model\": model,\n            \"tokens_input\": tokens_input,\n            \"tokens_output\": tokens_output,\n            \"input_cost\": input_cost,\n            \"output_cost\": output_cost,\n            \"total_cost\": total_cost\n        }\n        \n        # Add to history\n        self.requests.append(request)\n        \n        # Save history\n        self._save_history()\n        \n        # Log the request\n        logger.info(\n            f\"Request: {provider}/{model}, \" +\n            f\"Tokens: {tokens_input} in / {tokens_output} out, \" +\n            f\"Cost: ${total_cost:.4f}\"\n        )\n        \n        return request\n    \n    def get_session_stats(self) -> Dict[str, Any]:\n        \"\"\"Get statistics for the current session.\n        \n        Returns:\n            Dictionary with session statistics\n        \"\"\"\n        return {\n            \"start_time\": self.session_start.isoformat(),\n            \"duration_seconds\": (datetime.now() - self.session_start).total_seconds(),\n            \"tokens_input\": self.session_tokens_input,\n            \"tokens_output\": self.session_tokens_output,\n            \"total_tokens\": self.session_tokens_input + self.session_tokens_output,\n            \"total_cost\": self.session_cost,\n            \"request_count\": len(self.requests),\n            \"budget_limit\": self.budget_limit,\n            \"budget_remaining\": None if self.budget_limit is None else self.budget_limit - self.session_cost\n        }\n    \n    def check_budget(self) -> Dict[str, Any]:\n        \"\"\"Check if budget limit is approached or exceeded.\n        \n        Returns:\n            Dictionary with budget status information\n        \"\"\"\n        if self.budget_limit is None:\n            return {\n                \"has_budget\": False,\n                \"status\": \"no_limit\",\n                \"message\": \"No budget limit set\"\n            }\n        \n        remaining = self.budget_limit - self.session_cost\n        percentage_used = (self.session_cost / self.budget_limit) * 100\n        \n        if remaining <= 0:\n            status = \"exceeded\"\n            message = f\"Budget exceeded by ${abs(remaining):.2f}\"\n        elif percentage_used > 90:\n            status = \"critical\"\n            message = f\"Budget critical: ${remaining:.2f} remaining ({percentage_used:.1f}% used)\"\n        elif percentage_used > 75:\n            status = \"warning\"\n            message = f\"Budget warning: ${remaining:.2f} remaining ({percentage_used:.1f}% used)\"\n        else:\n            status = \"ok\"\n            message = f\"Budget OK: ${remaining:.2f} remaining ({percentage_used:.1f}% used)\"\n        \n        return {\n            \"has_budget\": True,\n            \"status\": status,\n            \"message\": message,\n            \"limit\": self.budget_limit,\n            \"used\": self.session_cost,\n            \"remaining\": remaining,\n            \"percentage_used\": percentage_used\n        }\n    \n    def get_usage_by_model(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get usage statistics grouped by model.\n        \n        Returns:\n            Dictionary mapping \"provider/model\" to usage statistics\n        \"\"\"\n        usage: Dict[str, Dict[str, Any]] = {}\n        \n        for request in self.requests:\n            key = f\"{request['provider']}/{request['model']}\"\n            \n            if key not in usage:\n                usage[key] = {\n                    \"provider\": request[\"provider\"],\n                    \"model\": request[\"model\"],\n                    \"request_count\": 0,\n                    \"tokens_input\": 0,\n                    \"tokens_output\": 0,\n                    \"total_cost\": 0.0\n                }\n            \n            usage[key][\"request_count\"] += 1\n            usage[key][\"tokens_input\"] += request[\"tokens_input\"]\n            usage[key][\"tokens_output\"] += request[\"tokens_output\"]\n            usage[key][\"total_cost\"] += request[\"total_cost\"]\n        \n        return usage\n    \n    def get_cost_summary_panel(self) -> Panel:\n        \"\"\"Create a Rich panel with cost summary information.\n        \n        Returns:\n            Rich Panel object\n        \"\"\"\n        # Get stats and budget info\n        stats = self.get_session_stats()\n        budget = self.check_budget()\n        \n        # Create a table for the summary\n        table = Table(show_header=False, box=ROUNDED, expand=True)\n        table.add_column(\"Item\", style=\"bold\")\n        table.add_column(\"Value\")\n        \n        # Add rows with token usage\n        table.add_row(\n            \"Tokens (Input)\",\n            f\"{stats['tokens_input']:,}\"\n        )\n        table.add_row(\n            \"Tokens (Output)\",\n            f\"{stats['tokens_output']:,}\"\n        )\n        table.add_row(\n            \"Total Cost\",\n            f\"${stats['total_cost']:.4f}\"\n        )\n        \n        # Add budget information if available\n        if budget[\"has_budget\"]:\n            # Create styled text for budget status\n            status_text = Text(budget[\"message\"])\n            if budget[\"status\"] == \"exceeded\":\n                status_text.stylize(\"bold red\")\n            elif budget[\"status\"] == \"critical\":\n                status_text.stylize(\"bold yellow\")\n            elif budget[\"status\"] == \"warning\":\n                status_text.stylize(\"yellow\")\n            else:\n                status_text.stylize(\"green\")\n            \n            table.add_row(\"Budget\", status_text)\n        \n        # Create the panel\n        title = \"[bold]Cost & Usage Summary[/bold]\"\n        return Panel(table, title=title, border_style=\"yellow\")\n    \n    def reset_session(self) -> None:\n        \"\"\"Reset the session counters but keep request history.\"\"\"\n        self.session_start = datetime.now()\n        self.session_tokens_input = 0\n        self.session_tokens_output = 0\n        self.session_cost = 0.0\n        \n        logger.info(\"Cost tracking session reset\")\n    \n    def _save_history(self) -> None:\n        \"\"\"Save request history to file if configured.\"\"\"\n        if not self.history_file:\n            return\n        \n        try:\n            # Ensure directory exists\n            directory = os.path.dirname(self.history_file)\n            if directory and not os.path.exists(directory):\n                os.makedirs(directory, exist_ok=True)\n            \n            # Save history\n            with open(self.history_file, 'w', encoding='utf-8') as f:\n                json.dump({\n                    \"session_start\": self.session_start.isoformat(),\n                    \"budget_limit\": self.budget_limit,\n                    \"requests\": self.requests,\n                    \"updated_at\": datetime.now().isoformat()\n                }, f, indent=2)\n        except Exception as e:\n            logger.error(f\"Failed to save cost history: {e}\")\n    \n    def _load_history(self) -> None:\n        \"\"\"Load request history from file if available.\"\"\"\n        if not self.history_file or not os.path.exists(self.history_file):\n            return\n        \n        try:\n            with open(self.history_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                \n                # Load session data\n                self.session_start = datetime.fromisoformat(data.get('session_start', self.session_start.isoformat()))\n                self.budget_limit = data.get('budget_limit', self.budget_limit)\n                \n                # Load requests\n                self.requests = data.get('requests', [])\n                \n                # Recalculate session totals\n                self.session_tokens_input = sum(r.get('tokens_input', 0) for r in self.requests)\n                self.session_tokens_output = sum(r.get('tokens_output', 0) for r in self.requests)\n                self.session_cost = sum(r.get('total_cost', 0) for r in self.requests)\n                \n                logger.info(f\"Loaded cost history with {len(self.requests)} requests\")\n        except Exception as e:\n            logger.error(f\"Failed to load cost history: {e}\")\n    \n    def generate_usage_report(self, format: str = \"text\") -> str:\n        \"\"\"Generate a usage report.\n        \n        Args:\n            format: Output format (\"text\", \"json\", \"markdown\")\n            \n        Returns:\n            Formatted usage report\n        \"\"\"\n        stats = self.get_session_stats()\n        model_usage = self.get_usage_by_model()\n        \n        if format == \"json\":\n            return json.dumps({\n                \"session\": stats,\n                \"models\": model_usage\n            }, indent=2)\n        \n        # Text or markdown format\n        lines = []\n        lines.append(\"# Usage Report\" if format == \"markdown\" else \"USAGE REPORT\")\n        lines.append(\"\")\n        \n        # Session summary\n        lines.append(\"## Session Summary\" if format == \"markdown\" else \"SESSION SUMMARY\")\n        lines.append(f\"- Start time: {stats['start_time']}\")\n        lines.append(f\"- Duration: {stats['duration_seconds'] / 60:.1f} minutes\")\n        lines.append(f\"- Requests: {stats['request_count']}\")\n        lines.append(f\"- Total tokens: {stats['total_tokens']:,} ({stats['tokens_input']:,} in / {stats['tokens_output']:,} out)\")\n        lines.append(f\"- Total cost: ${stats['total_cost']:.4f}\")\n        if stats['budget_limit'] is not None:\n            lines.append(f\"- Budget: ${stats['budget_limit']:.2f} (${stats['budget_remaining']:.2f} remaining)\")\n        lines.append(\"\")\n        \n        # Usage by model\n        lines.append(\"## Usage by Model\" if format == \"markdown\" else \"USAGE BY MODEL\")\n        for key, usage in sorted(model_usage.items(), key=lambda x: x[1]['total_cost'], reverse=True):\n            lines.append(f\"### {key}\" if format == \"markdown\" else key.upper())\n            lines.append(f\"- Requests: {usage['request_count']}\")\n            lines.append(f\"- Tokens: {usage['tokens_input'] + usage['tokens_output']:,} ({usage['tokens_input']:,} in / {usage['tokens_output']:,} out)\")\n            lines.append(f\"- Cost: ${usage['total_cost']:.4f}\")\n            lines.append(\"\")\n        \n        return \"\\n\".join(lines)"}
{"type": "source_file", "path": "claude_code/commands/multi_agent_client.py", "content": "#!/usr/bin/env python3\n# claude_code/commands/multi_agent_client.py\n\"\"\"Multi-agent MCP client implementation with synchronization capabilities.\"\"\"\n\nimport asyncio\nimport sys\nimport os\nimport json\nimport logging\nimport uuid\nimport argparse\nimport time\nfrom typing import Optional, Dict, Any, List, Set, Tuple\nfrom contextlib import AsyncExitStack\nfrom dataclasses import dataclass, field, asdict\n\nfrom rich.console import Console\nfrom rich.prompt import Prompt\nfrom rich.panel import Panel\nfrom rich.markdown import Markdown\nfrom rich.table import Table\nfrom rich.live import Live\nfrom rich import print as rprint\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nfrom anthropic import Anthropic\nfrom dotenv import load_dotenv\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Load environment variables\nload_dotenv()\n\n# Console for rich output\nconsole = Console()\n\n@dataclass\nclass Agent:\n    \"\"\"Agent representation for multi-agent scenarios.\"\"\"\n    id: str\n    name: str\n    role: str\n    model: str\n    system_prompt: str\n    conversation: List[Dict[str, Any]] = field(default_factory=list)\n    connected_agents: Set[str] = field(default_factory=set)\n    message_queue: asyncio.Queue = field(default_factory=asyncio.Queue)\n    \n    def __post_init__(self):\n        \"\"\"Initialize the conversation with system prompt.\"\"\"\n        self.conversation = [{\n            \"role\": \"system\",\n            \"content\": self.system_prompt\n        }]\n\n@dataclass\nclass Message:\n    \"\"\"Message for agent communication.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    sender_id: str = \"\"\n    sender_name: str = \"\"\n    recipient_id: Optional[str] = None  # None means broadcast to all\n    recipient_name: Optional[str] = None\n    content: str = \"\"\n    timestamp: float = field(default_factory=time.time)\n    \n    @classmethod\n    def create(cls, sender_id: str, sender_name: str, content: str, \n               recipient_id: Optional[str] = None, recipient_name: Optional[str] = None) -> 'Message':\n        \"\"\"Create a new message.\"\"\"\n        return cls(\n            sender_id=sender_id,\n            sender_name=sender_name,\n            recipient_id=recipient_id,\n            recipient_name=recipient_name,\n            content=content\n        )\n\nclass AgentCoordinator:\n    \"\"\"Coordinates communication between multiple agents.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the agent coordinator.\"\"\"\n        self.agents: Dict[str, Agent] = {}\n        self.message_history: List[Message] = []\n        self.broadcast_queue: asyncio.Queue = asyncio.Queue()\n    \n    def add_agent(self, agent: Agent) -> None:\n        \"\"\"Add a new agent to the coordinator.\n        \n        Args:\n            agent: The agent to add\n        \"\"\"\n        self.agents[agent.id] = agent\n    \n    def remove_agent(self, agent_id: str) -> None:\n        \"\"\"Remove an agent from the coordinator.\n        \n        Args:\n            agent_id: ID of the agent to remove\n        \"\"\"\n        if agent_id in self.agents:\n            del self.agents[agent_id]\n    \n    async def broadcast_message(self, message: Message) -> None:\n        \"\"\"Broadcast a message to all agents.\n        \n        Args:\n            message: The message to broadcast\n        \"\"\"\n        self.message_history.append(message)\n        \n        for agent_id, agent in self.agents.items():\n            # Don't send message back to sender\n            if agent_id != message.sender_id:\n                await agent.message_queue.put(message)\n                logger.debug(f\"Queued message from {message.sender_name} to {agent.name}\")\n    \n    async def send_direct_message(self, message: Message) -> None:\n        \"\"\"Send a message to a specific agent.\n        \n        Args:\n            message: The message to send\n        \"\"\"\n        self.message_history.append(message)\n        \n        if message.recipient_id in self.agents:\n            recipient = self.agents[message.recipient_id]\n            await recipient.message_queue.put(message)\n            logger.debug(f\"Queued direct message from {message.sender_name} to {recipient.name}\")\n    \n    async def process_message(self, message: Message) -> None:\n        \"\"\"Process an incoming message and route appropriately.\n        \n        Args:\n            message: The message to process\n        \"\"\"\n        if message.recipient_id is None:\n            # Broadcast message\n            await self.broadcast_message(message)\n        else:\n            # Direct message\n            await self.send_direct_message(message)\n    \n    def get_message_history_for_agent(self, agent_id: str) -> List[Dict[str, Any]]:\n        \"\"\"Get conversation messages formatted for a specific agent.\n        \n        Args:\n            agent_id: ID of the agent\n            \n        Returns:\n            List of messages in the format expected by Claude\n        \"\"\"\n        agent = self.agents.get(agent_id)\n        if not agent:\n            return []\n        \n        messages = []\n        \n        # Start with the agent's conversation history\n        messages.extend(agent.conversation)\n        \n        # Add relevant messages from the message history\n        for msg in self.message_history:\n            # Include messages sent by this agent or addressed to this agent\n            # or broadcast messages from other agents\n            if (msg.sender_id == agent_id or \n                msg.recipient_id == agent_id or \n                (msg.recipient_id is None and msg.sender_id != agent_id)):\n                \n                if msg.sender_id == agent_id:\n                    # This agent's own messages\n                    messages.append({\n                        \"role\": \"assistant\",\n                        \"content\": msg.content\n                    })\n                else:\n                    # Messages from other agents\n                    sender = self.agents.get(msg.sender_id)\n                    sender_name = sender.name if sender else msg.sender_name\n                    \n                    if msg.recipient_id is None:\n                        # Broadcast message\n                        messages.append({\n                            \"role\": \"user\",\n                            \"content\": f\"{sender_name}: {msg.content}\"\n                        })\n                    else:\n                        # Direct message\n                        messages.append({\n                            \"role\": \"user\",\n                            \"content\": f\"{sender_name} (direct): {msg.content}\"\n                        })\n        \n        return messages\n\nclass MultiAgentMCPClient:\n    \"\"\"Multi-agent Model Context Protocol client with synchronization capabilities.\"\"\"\n    \n    def __init__(self, config_path: str = None):\n        \"\"\"Initialize the multi-agent MCP client.\n        \n        Args:\n            config_path: Path to the agent configuration file\n        \"\"\"\n        self.session: Optional[ClientSession] = None\n        self.exit_stack = AsyncExitStack()\n        self.anthropic = Anthropic()\n        self.coordinator = AgentCoordinator()\n        self.available_tools = []\n        \n        # Configuration\n        self.config_path = config_path\n        self.agents_config = self._load_agents_config()\n    \n    def _load_agents_config(self) -> List[Dict[str, Any]]:\n        \"\"\"Load agent configurations from file.\n        \n        Returns:\n            List of agent configurations\n        \"\"\"\n        default_config = [{\n            \"name\": \"Assistant\",\n            \"role\": \"general assistant\",\n            \"model\": \"claude-3-5-sonnet-20241022\",\n            \"system_prompt\": \"You are a helpful AI assistant participating in a multi-agent conversation. You can communicate with other agents and humans to solve complex problems.\"\n        }]\n        \n        if not self.config_path:\n            return default_config\n        \n        try:\n            with open(self.config_path, 'r', encoding='utf-8') as f:\n                return json.load(f)\n        except Exception as e:\n            logger.warning(f\"Failed to load agent configuration: {e}\")\n            return default_config\n    \n    def setup_agents(self) -> None:\n        \"\"\"Set up agents based on configuration.\"\"\"\n        for idx, agent_config in enumerate(self.agents_config):\n            agent_id = str(uuid.uuid4())\n            agent = Agent(\n                id=agent_id,\n                name=agent_config.get(\"name\", f\"Agent-{idx+1}\"),\n                role=agent_config.get(\"role\", \"assistant\"),\n                model=agent_config.get(\"model\", \"claude-3-5-sonnet-20241022\"),\n                system_prompt=agent_config.get(\"system_prompt\", \"You are a helpful AI assistant.\")\n            )\n            self.coordinator.add_agent(agent)\n            logger.info(f\"Created agent: {agent.name} ({agent.role})\")\n    \n    async def connect_to_server(self, server_script_path: str):\n        \"\"\"Connect to an MCP server.\n\n        Args:\n            server_script_path: Path to the server script (.py or .js)\n        \"\"\"\n        is_python = server_script_path.endswith('.py')\n        is_js = server_script_path.endswith('.js')\n        if not (is_python or is_js):\n            raise ValueError(\"Server script must be a .py or .js file\")\n\n        command = \"python\" if is_python else \"node\"\n        server_params = StdioServerParameters(\n            command=command,\n            args=[server_script_path],\n            env=None\n        )\n\n        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n        self.stdio, self.write = stdio_transport\n        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n\n        await self.session.initialize()\n\n        # List available tools\n        response = await self.session.list_tools()\n        tools = response.tools\n        self.available_tools = [{\n            \"name\": tool.name,\n            \"description\": tool.description,\n            \"input_schema\": tool.inputSchema\n        } for tool in response.tools]\n        \n        tool_names = [tool.name for tool in tools]\n        logger.info(f\"Connected to server with tools: {tool_names}\")\n        console.print(Panel.fit(\n            f\"[bold green]Connected to MCP server[/bold green]\\n\"\n            f\"Available tools: {', '.join(tool_names)}\",\n            title=\"Connection Status\",\n            border_style=\"green\"\n        ))\n    \n    async def process_agent_query(self, agent_id: str, query: str, is_direct_message: bool = False) -> str:\n        \"\"\"Process a query using Claude and available tools for a specific agent.\n        \n        Args:\n            agent_id: The ID of the agent processing the query\n            query: The query to process\n            is_direct_message: Whether this is a direct message from user\n            \n        Returns:\n            The response text\n        \"\"\"\n        agent = self.coordinator.agents.get(agent_id)\n        if not agent:\n            return \"Error: Agent not found\"\n        \n        # Get the conversation history for this agent\n        messages = self.coordinator.get_message_history_for_agent(agent_id)\n        \n        # Add the current query if it's a direct message\n        if is_direct_message:\n            messages.append({\n                \"role\": \"user\",\n                \"content\": query\n            })\n        \n        # Initial Claude API call\n        response = self.anthropic.messages.create(\n            model=agent.model,\n            max_tokens=1000,\n            messages=messages,\n            tools=self.available_tools\n        )\n\n        # Process response and handle tool calls\n        tool_results = []\n        final_text = \"\"\n        assistant_message_content = []\n        \n        for content in response.content:\n            if content.type == 'text':\n                final_text = content.text\n                assistant_message_content.append(content)\n            elif content.type == 'tool_use':\n                tool_name = content.name\n                tool_args = content.input\n\n                # Execute tool call\n                result = await self.session.call_tool(tool_name, tool_args)\n                tool_results.append({\"call\": tool_name, \"result\": result})\n                console.print(f\"[bold cyan]Agent {agent.name} calling tool {tool_name}[/bold cyan]\")\n\n                assistant_message_content.append(content)\n                messages.append({\n                    \"role\": \"assistant\",\n                    \"content\": assistant_message_content\n                })\n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"tool_result\",\n                            \"tool_use_id\": content.id,\n                            \"content\": result.content\n                        }\n                    ]\n                })\n\n                # Get next response from Claude\n                response = self.anthropic.messages.create(\n                    model=agent.model,\n                    max_tokens=1000,\n                    messages=messages,\n                    tools=self.available_tools\n                )\n\n                final_text = response.content[0].text\n        \n        # Create a message from the agent's response\n        message = Message.create(\n            sender_id=agent_id,\n            sender_name=agent.name,\n            content=final_text,\n            recipient_id=None  # Broadcast to all\n        )\n        \n        # Process the message\n        await self.coordinator.process_message(message)\n        \n        return final_text\n    \n    async def process_user_query(self, query: str, target_agent_id: Optional[str] = None) -> None:\n        \"\"\"Process a query from the user and route it to agents.\n        \n        Args:\n            query: The user query\n            target_agent_id: Optional ID of a specific agent to target\n        \"\"\"\n        # Handle special commands\n        if query.startswith(\"/\"):\n            await self._handle_special_command(query)\n            return\n        \n        if target_agent_id:\n            # Direct message to a specific agent\n            agent = self.coordinator.agents.get(target_agent_id)\n            if not agent:\n                console.print(\"[bold red]Error: Agent not found[/bold red]\")\n                return\n            \n            console.print(f\"[bold blue]User → {agent.name}:[/bold blue] {query}\")\n            \n            response = await self.process_agent_query(target_agent_id, query, is_direct_message=True)\n            console.print(f\"[bold green]{agent.name}:[/bold green] {response}\")\n        else:\n            # Broadcast to all agents\n            console.print(f\"[bold blue]User (broadcast):[/bold blue] {query}\")\n            \n            # Create a message from the user\n            message = Message.create(\n                sender_id=\"user\",\n                sender_name=\"User\",\n                content=query,\n                recipient_id=None  # Broadcast\n            )\n            \n            # Process the message\n            await self.coordinator.process_message(message)\n            \n            # Process in parallel for all agents\n            tasks = []\n            for agent_id in self.coordinator.agents:\n                tasks.append(asyncio.create_task(self.process_agent_query(agent_id, query)))\n            \n            # Wait for all agents to respond\n            await asyncio.gather(*tasks)\n    \n    async def run_agent_thought_loops(self) -> None:\n        \"\"\"Run continuous thought loops for each agent in the background.\"\"\"\n        while True:\n            for agent_id, agent in self.coordinator.agents.items():\n                try:\n                    # Check if there are new messages for this agent\n                    if not agent.message_queue.empty():\n                        message = await agent.message_queue.get()\n                        \n                        # Log the message\n                        if message.recipient_id is None:\n                            console.print(f\"[bold cyan]{message.sender_name} (broadcast):[/bold cyan] {message.content}\")\n                        else:\n                            console.print(f\"[bold cyan]{message.sender_name} → {agent.name}:[/bold cyan] {message.content}\")\n                        \n                        # Give the agent a chance to respond\n                        await self.process_agent_query(agent_id, message.content)\n                        \n                        # Mark the message as processed\n                        agent.message_queue.task_done()\n                \n                except Exception as e:\n                    logger.exception(f\"Error in agent thought loop for {agent.name}: {e}\")\n            \n            # Small delay to prevent CPU hogging\n            await asyncio.sleep(0.1)\n    \n    async def _handle_special_command(self, command: str) -> None:\n        \"\"\"Handle special commands.\n        \n        Args:\n            command: The command string starting with /\n        \"\"\"\n        parts = command.strip().split()\n        cmd = parts[0].lower()\n        args = parts[1:]\n        \n        if cmd == \"/help\":\n            self._show_help()\n        elif cmd == \"/agents\":\n            self._show_agents()\n        elif cmd == \"/talk\":\n            if len(args) < 2:\n                console.print(\"[bold red]Error: /talk requires agent name and message[/bold red]\")\n                return\n            \n            agent_name = args[0]\n            message = \" \".join(args[1:])\n            \n            # Find agent by name\n            target_agent = None\n            for agent_id, agent in self.coordinator.agents.items():\n                if agent.name.lower() == agent_name.lower():\n                    target_agent = agent\n                    break\n            \n            if target_agent:\n                await self.process_user_query(message, target_agent.id)\n            else:\n                console.print(f\"[bold red]Error: Agent '{agent_name}' not found[/bold red]\")\n        elif cmd == \"/history\":\n            self._show_message_history()\n        elif cmd == \"/quit\" or cmd == \"/exit\":\n            console.print(\"[bold yellow]Exiting multi-agent session...[/bold yellow]\")\n            sys.exit(0)\n        else:\n            console.print(f\"[bold red]Unknown command: {cmd}[/bold red]\")\n            self._show_help()\n    \n    def _show_help(self) -> None:\n        \"\"\"Show help information.\"\"\"\n        help_text = \"\"\"\n# Multi-Agent MCP Client Commands\n\n- **/help**: Show this help message\n- **/agents**: List all active agents\n- **/talk <agent> <message>**: Send a direct message to a specific agent\n- **/history**: Show message history\n- **/quit**, **/exit**: Exit the application\n\nTo broadcast a message to all agents, simply type your message without any command.\n        \"\"\"\n        console.print(Markdown(help_text))\n    \n    def _show_agents(self) -> None:\n        \"\"\"Show information about all active agents.\"\"\"\n        table = Table(title=\"Active Agents\")\n        table.add_column(\"Name\", style=\"cyan\")\n        table.add_column(\"Role\", style=\"green\")\n        table.add_column(\"Model\", style=\"blue\")\n        \n        for agent_id, agent in self.coordinator.agents.items():\n            table.add_row(agent.name, agent.role, agent.model)\n        \n        console.print(table)\n    \n    def _show_message_history(self) -> None:\n        \"\"\"Show the message history.\"\"\"\n        if not self.coordinator.message_history:\n            console.print(\"[yellow]No messages in history yet.[/yellow]\")\n            return\n        \n        table = Table(title=\"Message History\")\n        table.add_column(\"Time\", style=\"cyan\")\n        table.add_column(\"From\", style=\"green\")\n        table.add_column(\"To\", style=\"blue\")\n        table.add_column(\"Message\", style=\"white\")\n        \n        for msg in self.coordinator.message_history:\n            timestamp = time.strftime(\"%H:%M:%S\", time.localtime(msg.timestamp))\n            recipient = msg.recipient_name if msg.recipient_name else \"All\"\n            table.add_row(timestamp, msg.sender_name, recipient, msg.content[:50] + (\"...\" if len(msg.content) > 50 else \"\"))\n        \n        console.print(table)\n    \n    async def chat_loop(self) -> None:\n        \"\"\"Run the interactive chat loop.\"\"\"\n        console.print(Panel.fit(\n            \"[bold green]Multi-Agent MCP Client Started![/bold green]\\n\"\n            \"Type your messages to broadcast to all agents or use /help for commands.\",\n            title=\"Welcome\",\n            border_style=\"green\"\n        ))\n        \n        # Start the agent thought loop in the background\n        thought_loop_task = asyncio.create_task(self.run_agent_thought_loops())\n        \n        try:\n            # First, show active agents\n            self._show_agents()\n            \n            # Main chat loop\n            while True:\n                try:\n                    query = Prompt.ask(\"\\n[bold blue]>[/bold blue]\").strip()\n                    \n                    if not query:\n                        continue\n                    \n                    if query.lower() == \"quit\" or query.lower() == \"exit\":\n                        break\n                    \n                    await self.process_user_query(query)\n                \n                except KeyboardInterrupt:\n                    console.print(\"\\n[bold yellow]Operation cancelled.[/bold yellow]\")\n                    continue\n                except Exception as e:\n                    console.print(f\"\\n[bold red]Error: {str(e)}[/bold red]\")\n                    logger.exception(\"Error processing query\")\n        \n        finally:\n            # Cancel the thought loop task\n            thought_loop_task.cancel()\n            try:\n                await thought_loop_task\n            except asyncio.CancelledError:\n                pass\n    \n    async def cleanup(self) -> None:\n        \"\"\"Clean up resources.\"\"\"\n        await self.exit_stack.aclose()\n\n\ndef add_arguments(parser: argparse.ArgumentParser) -> None:\n    \"\"\"Add command-specific arguments to the parser.\n    \n    Args:\n        parser: Argument parser\n    \"\"\"\n    parser.add_argument(\n        \"server_script\",\n        type=str,\n        help=\"Path to the server script (.py or .js)\"\n    )\n    \n    parser.add_argument(\n        \"--config\",\n        type=str,\n        help=\"Path to agent configuration JSON file\"\n    )\n\n\ndef execute(args: argparse.Namespace) -> int:\n    \"\"\"Execute the multi-agent client command.\n    \n    Args:\n        args: Command arguments\n        \n    Returns:\n        Exit code\n    \"\"\"\n    try:\n        client = MultiAgentMCPClient(config_path=args.config)\n        client.setup_agents()\n        \n        async def run_client():\n            try:\n                await client.connect_to_server(args.server_script)\n                await client.chat_loop()\n            finally:\n                await client.cleanup()\n        \n        asyncio.run(run_client())\n        return 0\n    \n    except Exception as e:\n        logger.exception(f\"Error running multi-agent MCP client: {e}\")\n        console.print(f\"[bold red]Error: {str(e)}[/bold red]\")\n        return 1\n\n\ndef main() -> int:\n    \"\"\"Run the multi-agent client command as a standalone script.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Run the Claude Code Multi-Agent MCP client\")\n    add_arguments(parser)\n    args = parser.parse_args()\n    return execute(args)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())"}
{"type": "source_file", "path": "claude_code/lib/monitoring/__init__.py", "content": ""}
{"type": "source_file", "path": "claude_code/commands/serve.py", "content": "#!/usr/bin/env python3\n# claude_code/commands/serve.py\n\"\"\"Command to start the MCP server.\"\"\"\n\nimport os\nimport sys\nimport logging\nimport argparse\nfrom typing import Dict, Any, Optional, List\n\nfrom claude_code.mcp_server import initialize_server\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef add_arguments(parser: argparse.ArgumentParser) -> None:\n    \"\"\"Add command-specific arguments to the parser.\n    \n    Args:\n        parser: Argument parser\n    \"\"\"\n    parser.add_argument(\n        \"--dev\", \n        action=\"store_true\", \n        help=\"Run in development mode with the MCP Inspector\"\n    )\n    \n    parser.add_argument(\n        \"--host\", \n        type=str, \n        default=\"localhost\", \n        help=\"Host to bind the server to\"\n    )\n    \n    parser.add_argument(\n        \"--port\", \n        type=int, \n        default=8000, \n        help=\"Port to bind the server to\"\n    )\n    \n    parser.add_argument(\n        \"--dependencies\", \n        type=str, \n        nargs=\"*\", \n        help=\"Additional dependencies to install\"\n    )\n    \n    parser.add_argument(\n        \"--env-file\", \n        type=str, \n        help=\"Path to environment file (.env)\"\n    )\n\n\ndef execute(args: argparse.Namespace) -> int:\n    \"\"\"Execute the serve command.\n    \n    Args:\n        args: Command arguments\n        \n    Returns:\n        Exit code\n    \"\"\"\n    try:\n        # Initialize the MCP server\n        mcp_server = initialize_server()\n        \n        # Add any additional dependencies\n        if args.dependencies:\n            for dep in args.dependencies:\n                mcp_server.dependencies.append(dep)\n        \n        # Load environment variables from file\n        if args.env_file:\n            if not os.path.exists(args.env_file):\n                logger.error(f\"Environment file not found: {args.env_file}\")\n                return 1\n                \n            import dotenv\n            dotenv.load_dotenv(args.env_file)\n        \n        # Run the server\n        if args.dev:\n            logger.info(f\"Starting MCP server in development mode on {args.host}:{args.port}\")\n            # Use the fastmcp dev mode\n            import subprocess\n            cmd = [\n                \"fastmcp\", \"dev\", \n                \"--module\", \"claude_code.mcp_server:mcp\",\n                \"--host\", args.host,\n                \"--port\", str(args.port)\n            ]\n            return subprocess.call(cmd)\n        else:\n            # Run directly\n            logger.info(f\"Starting MCP server on {args.host}:{args.port}\")\n            logger.info(f\"Visit http://{args.host}:{args.port} for Claude Desktop configuration instructions\")\n            \n            # FastMCP.run() method signature changed to accept host/port\n            try:\n                mcp_server.run(host=args.host, port=args.port)\n            except TypeError:\n                # Fallback for older versions of FastMCP\n                logger.info(\"Using older FastMCP version without host/port parameters\")\n                mcp_server.run()\n                \n            return 0\n            \n    except Exception as e:\n        logger.exception(f\"Error running MCP server: {e}\")\n        return 1\n\n\ndef main() -> int:\n    \"\"\"Run the serve command as a standalone script.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Run the Claude Code MCP server\")\n    add_arguments(parser)\n    args = parser.parse_args()\n    return execute(args)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())"}
{"type": "source_file", "path": "claude_code/lib/providers/__init__.py", "content": "#!/usr/bin/env python3\n# claude_code/lib/providers/__init__.py\n\"\"\"LLM provider module.\"\"\"\n\nimport logging\nimport os\nfrom typing import Dict, Type, Optional\n\nfrom .base import BaseProvider\nfrom .openai import OpenAIProvider\n\nlogger = logging.getLogger(__name__)\n\n# Registry of provider classes\nPROVIDER_REGISTRY: Dict[str, Type[BaseProvider]] = {\n    \"openai\": OpenAIProvider,\n}\n\n# Try to import other providers if available\ntry:\n    from .anthropic import AnthropicProvider\n    PROVIDER_REGISTRY[\"anthropic\"] = AnthropicProvider\nexcept ImportError:\n    logger.debug(\"Anthropic provider not available\")\n\ntry:\n    from .local import LocalProvider\n    PROVIDER_REGISTRY[\"local\"] = LocalProvider\nexcept ImportError:\n    logger.debug(\"Local provider not available\")\n\n\ndef get_provider(name: Optional[str] = None, **kwargs) -> BaseProvider:\n    \"\"\"Get a provider instance by name.\n    \n    Args:\n        name: Provider name, or None to use default provider\n        **kwargs: Additional arguments to pass to the provider constructor\n        \n    Returns:\n        Provider instance\n        \n    Raises:\n        ValueError: If provider is not found\n    \"\"\"\n    # If name is not specified, try to infer from environment\n    if name is None:\n        if os.environ.get(\"OPENAI_API_KEY\"):\n            name = \"openai\"\n        elif os.environ.get(\"ANTHROPIC_API_KEY\"):\n            name = \"anthropic\"\n        else:\n            # Default to OpenAI if nothing else is available\n            name = \"openai\"\n    \n    if name.lower() not in PROVIDER_REGISTRY:\n        raise ValueError(f\"Provider {name} not found. Available providers: {', '.join(PROVIDER_REGISTRY.keys())}\")\n    \n    provider_class = PROVIDER_REGISTRY[name.lower()]\n    return provider_class(**kwargs)\n\n\ndef list_available_providers() -> Dict[str, Dict]:\n    \"\"\"List all available providers and their models.\n    \n    Returns:\n        Dictionary mapping provider names to information about them\n    \"\"\"\n    result = {}\n    \n    for name, provider_class in PROVIDER_REGISTRY.items():\n        try:\n            # Create a temporary instance to get model information\n            # This might fail if API keys are not available\n            instance = provider_class()\n            result[name] = {\n                \"name\": instance.name,\n                \"available\": True,\n                \"models\": instance.available_models,\n                \"current_model\": instance.current_model\n            }\n        except Exception as e:\n            # Provider is available but not configured correctly\n            result[name] = {\n                \"name\": name.capitalize(),\n                \"available\": False,\n                \"error\": str(e),\n                \"models\": [],\n                \"current_model\": None\n            }\n    \n    return result"}
{"type": "source_file", "path": "claude_code/lib/__init__.py", "content": ""}
{"type": "source_file", "path": "claude_code/lib/context/__init__.py", "content": ""}
{"type": "source_file", "path": "claude_code/config/__init__.py", "content": ""}
{"type": "source_file", "path": "claude_code/__init__.py", "content": "\"\"\"\nClaude Code Python Edition - A powerful LLM-powered CLI for software development.\n\nThis package provides a Python reimplementation of Claude Code with enhanced \nreal-time tool visualization and cost management features.\n\"\"\"\n\n__version__ = \"0.1.0\"\n__author__ = \"Claude Code Team\""}
{"type": "source_file", "path": "claude_code/commands/client.py", "content": "#!/usr/bin/env python3\n# claude_code/commands/client.py\n\"\"\"MCP client implementation for testing MCP servers.\"\"\"\n\nimport asyncio\nimport sys\nimport os\nimport logging\nimport argparse\nfrom typing import Optional, Dict, Any\nfrom contextlib import AsyncExitStack\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nfrom anthropic import Anthropic\nfrom dotenv import load_dotenv\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Load environment variables\nload_dotenv()\n\n\nclass MCPClient:\n    \"\"\"Model Context Protocol client for testing MCP servers.\"\"\"\n    \n    def __init__(self, model: str = \"claude-3-5-sonnet-20241022\"):\n        \"\"\"Initialize the MCP client.\n        \n        Args:\n            model: The Claude model to use\n        \"\"\"\n        # Initialize session and client objects\n        self.session: Optional[ClientSession] = None\n        self.exit_stack = AsyncExitStack()\n        self.anthropic = Anthropic()\n        self.model = model\n\n    async def connect_to_server(self, server_script_path: str):\n        \"\"\"Connect to an MCP server.\n\n        Args:\n            server_script_path: Path to the server script (.py or .js)\n        \"\"\"\n        is_python = server_script_path.endswith('.py')\n        is_js = server_script_path.endswith('.js')\n        if not (is_python or is_js):\n            raise ValueError(\"Server script must be a .py or .js file\")\n\n        command = \"python\" if is_python else \"node\"\n        server_params = StdioServerParameters(\n            command=command,\n            args=[server_script_path],\n            env=None\n        )\n\n        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n        self.stdio, self.write = stdio_transport\n        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n\n        await self.session.initialize()\n\n        # List available tools\n        response = await self.session.list_tools()\n        tools = response.tools\n        logger.info(f\"Connected to server with tools: {[tool.name for tool in tools]}\")\n        print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n\n    async def process_query(self, query: str) -> str:\n        \"\"\"Process a query using Claude and available tools.\n        \n        Args:\n            query: The user query\n            \n        Returns:\n            The response text\n        \"\"\"\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": query\n            }\n        ]\n\n        response = await self.session.list_tools()\n        available_tools = [{\n            \"name\": tool.name,\n            \"description\": tool.description,\n            \"input_schema\": tool.inputSchema\n        } for tool in response.tools]\n\n        # Initial Claude API call\n        response = self.anthropic.messages.create(\n            model=self.model,\n            max_tokens=1000,\n            messages=messages,\n            tools=available_tools\n        )\n\n        # Process response and handle tool calls\n        tool_results = []\n        final_text = []\n\n        assistant_message_content = []\n        for content in response.content:\n            if content.type == 'text':\n                final_text.append(content.text)\n                assistant_message_content.append(content)\n            elif content.type == 'tool_use':\n                tool_name = content.name\n                tool_args = content.input\n\n                # Execute tool call\n                result = await self.session.call_tool(tool_name, tool_args)\n                tool_results.append({\"call\": tool_name, \"result\": result})\n                final_text.append(f\"[Calling tool {tool_name} with args {tool_args}]\")\n\n                assistant_message_content.append(content)\n                messages.append({\n                    \"role\": \"assistant\",\n                    \"content\": assistant_message_content\n                })\n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"tool_result\",\n                            \"tool_use_id\": content.id,\n                            \"content\": result.content\n                        }\n                    ]\n                })\n\n                # Get next response from Claude\n                response = self.anthropic.messages.create(\n                    model=self.model,\n                    max_tokens=1000,\n                    messages=messages,\n                    tools=available_tools\n                )\n\n                final_text.append(response.content[0].text)\n\n        return \"\\n\".join(final_text)\n\n    async def chat_loop(self):\n        \"\"\"Run an interactive chat loop.\"\"\"\n        print(\"\\nMCP Client Started!\")\n        print(\"Type your queries or 'quit' to exit.\")\n\n        while True:\n            try:\n                query = input(\"\\nQuery: \").strip()\n\n                if query.lower() == 'quit':\n                    break\n\n                response = await self.process_query(query)\n                print(\"\\n\" + response)\n\n            except Exception as e:\n                print(f\"\\nError: {str(e)}\")\n                logger.exception(\"Error processing query\")\n\n    async def cleanup(self):\n        \"\"\"Clean up resources.\"\"\"\n        await self.exit_stack.aclose()\n\n\ndef add_arguments(parser: argparse.ArgumentParser) -> None:\n    \"\"\"Add command-specific arguments to the parser.\n    \n    Args:\n        parser: Argument parser\n    \"\"\"\n    parser.add_argument(\n        \"server_script\",\n        type=str,\n        help=\"Path to the server script (.py or .js)\"\n    )\n    \n    parser.add_argument(\n        \"--model\",\n        type=str,\n        default=\"claude-3-5-sonnet-20241022\",\n        help=\"Claude model to use\"\n    )\n\n\ndef execute(args: argparse.Namespace) -> int:\n    \"\"\"Execute the client command.\n    \n    Args:\n        args: Command arguments\n        \n    Returns:\n        Exit code\n    \"\"\"\n    try:\n        client = MCPClient(model=args.model)\n        \n        async def run_client():\n            try:\n                await client.connect_to_server(args.server_script)\n                await client.chat_loop()\n            finally:\n                await client.cleanup()\n                \n        asyncio.run(run_client())\n        return 0\n        \n    except Exception as e:\n        logger.exception(f\"Error running MCP client: {e}\")\n        print(f\"\\nError: {str(e)}\")\n        return 1\n\n\ndef main() -> int:\n    \"\"\"Run the client command as a standalone script.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Run the Claude Code MCP client\")\n    add_arguments(parser)\n    args = parser.parse_args()\n    return execute(args)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())"}
{"type": "source_file", "path": "claude_code/lib/monitoring/server_metrics.py", "content": "#!/usr/bin/env python3\n\"\"\"Module for tracking MCP server metrics.\"\"\"\n\nimport os\nimport time\nimport json\nimport logging\nimport threading\nfrom typing import Dict, List, Any, Optional, Callable\nfrom datetime import datetime, timedelta\nfrom collections import deque, Counter\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass ServerMetrics:\n    \"\"\"Tracks MCP server metrics for visualization.\"\"\"\n    \n    def __init__(self, history_size: int = 100, save_interval: int = 60):\n        \"\"\"Initialize the server metrics tracker.\n        \n        Args:\n            history_size: Number of data points to keep in history\n            save_interval: How often to save metrics to disk (in seconds)\n        \"\"\"\n        self._start_time = time.time()\n        self._lock = threading.RLock()\n        self._history_size = history_size\n        self._save_interval = save_interval\n        self._save_path = os.path.expanduser(\"~/.config/claude_code/metrics.json\")\n        \n        # Ensure directory exists\n        os.makedirs(os.path.dirname(self._save_path), exist_ok=True)\n        \n        # Metrics\n        self._request_history = deque(maxlen=history_size)\n        self._tool_calls = Counter()\n        self._resource_calls = Counter()\n        self._connections = 0\n        self._active_connections = set()\n        self._errors = Counter()\n        \n        # Time series data for charts\n        self._time_series = {\n            \"tool_calls\": deque([(time.time(), 0)] * 10, maxlen=10),\n            \"resource_calls\": deque([(time.time(), 0)] * 10, maxlen=10)\n        }\n        \n        # Start auto-save thread\n        self._running = True\n        self._save_thread = threading.Thread(target=self._auto_save, daemon=True)\n        self._save_thread.start()\n        \n        # Load previous metrics if available\n        self._load_metrics()\n    \n    def _auto_save(self):\n        \"\"\"Periodically save metrics to disk.\"\"\"\n        while self._running:\n            time.sleep(self._save_interval)\n            try:\n                self.save_metrics()\n            except Exception as e:\n                logger.error(f\"Error saving metrics: {e}\")\n    \n    def _load_metrics(self):\n        \"\"\"Load metrics from disk if available.\"\"\"\n        try:\n            if os.path.exists(self._save_path):\n                with open(self._save_path, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                \n                with self._lock:\n                    # Load previous tool and resource calls\n                    self._tool_calls = Counter(data.get(\"tool_calls\", {}))\n                    self._resource_calls = Counter(data.get(\"resource_calls\", {}))\n                    \n                    # Don't load time-sensitive data like connections and history\n                    \n                    logger.info(f\"Loaded metrics from {self._save_path}\")\n        except Exception as e:\n            logger.error(f\"Error loading metrics: {e}\")\n    \n    def save_metrics(self):\n        \"\"\"Save metrics to disk.\"\"\"\n        try:\n            with self._lock:\n                data = {\n                    \"tool_calls\": dict(self._tool_calls),\n                    \"resource_calls\": dict(self._resource_calls),\n                    \"total_connections\": self._connections,\n                    \"last_saved\": time.time()\n                }\n            \n            with open(self._save_path, 'w', encoding='utf-8') as f:\n                json.dump(data, f, indent=2)\n            \n            logger.debug(f\"Metrics saved to {self._save_path}\")\n        except Exception as e:\n            logger.error(f\"Error saving metrics: {e}\")\n    \n    def log_tool_call(self, tool_name: str, success: bool = True):\n        \"\"\"Log a tool call.\n        \n        Args:\n            tool_name: The name of the tool that was called\n            success: Whether the call was successful\n        \"\"\"\n        with self._lock:\n            self._tool_calls[tool_name] += 1\n            \n            # Add to request history\n            timestamp = time.time()\n            self._request_history.append({\n                \"type\": \"tool\",\n                \"name\": tool_name,\n                \"success\": success,\n                \"timestamp\": timestamp\n            })\n            \n            # Update time series\n            current_time = time.time()\n            last_time, count = self._time_series[\"tool_calls\"][-1]\n            if current_time - last_time < 60:  # Less than a minute\n                self._time_series[\"tool_calls\"][-1] = (last_time, count + 1)\n            else:\n                self._time_series[\"tool_calls\"].append((current_time, 1))\n    \n    def log_resource_request(self, resource_uri: str, success: bool = True):\n        \"\"\"Log a resource request.\n        \n        Args:\n            resource_uri: The URI of the requested resource\n            success: Whether the request was successful\n        \"\"\"\n        with self._lock:\n            self._resource_calls[resource_uri] += 1\n            \n            # Add to request history\n            timestamp = time.time()\n            self._request_history.append({\n                \"type\": \"resource\",\n                \"uri\": resource_uri,\n                \"success\": success,\n                \"timestamp\": timestamp\n            })\n            \n            # Update time series\n            current_time = time.time()\n            last_time, count = self._time_series[\"resource_calls\"][-1]\n            if current_time - last_time < 60:  # Less than a minute\n                self._time_series[\"resource_calls\"][-1] = (last_time, count + 1)\n            else:\n                self._time_series[\"resource_calls\"].append((current_time, 1))\n    \n    def log_connection(self, client_id: str, connected: bool = True):\n        \"\"\"Log a client connection or disconnection.\n        \n        Args:\n            client_id: Client identifier\n            connected: True for connection, False for disconnection\n        \"\"\"\n        with self._lock:\n            if connected:\n                self._connections += 1\n                self._active_connections.add(client_id)\n            else:\n                self._active_connections.discard(client_id)\n            \n            # Add to request history\n            timestamp = time.time()\n            self._request_history.append({\n                \"type\": \"connection\",\n                \"client_id\": client_id,\n                \"action\": \"connect\" if connected else \"disconnect\",\n                \"timestamp\": timestamp\n            })\n    \n    def log_error(self, error_type: str, message: str):\n        \"\"\"Log an error.\n        \n        Args:\n            error_type: Type of error\n            message: Error message\n        \"\"\"\n        with self._lock:\n            self._errors[error_type] += 1\n            \n            # Add to request history\n            timestamp = time.time()\n            self._request_history.append({\n                \"type\": \"error\",\n                \"error_type\": error_type,\n                \"message\": message,\n                \"timestamp\": timestamp\n            })\n    \n    def get_uptime(self) -> str:\n        \"\"\"Get the server uptime as a human-readable string.\n        \n        Returns:\n            Uptime string (e.g., \"2 hours 15 minutes\")\n        \"\"\"\n        uptime_seconds = time.time() - self._start_time\n        uptime = timedelta(seconds=int(uptime_seconds))\n        \n        days = uptime.days\n        hours, remainder = divmod(uptime.seconds, 3600)\n        minutes, seconds = divmod(remainder, 60)\n        \n        parts = []\n        if days > 0:\n            parts.append(f\"{days} {'day' if days == 1 else 'days'}\")\n        if hours > 0 or days > 0:\n            parts.append(f\"{hours} {'hour' if hours == 1 else 'hours'}\")\n        if minutes > 0 or hours > 0 or days > 0:\n            parts.append(f\"{minutes} {'minute' if minutes == 1 else 'minutes'}\")\n        \n        if not parts:\n            return f\"{seconds} seconds\"\n        \n        return \" \".join(parts)\n    \n    def get_active_connections_count(self) -> int:\n        \"\"\"Get the number of active connections.\n        \n        Returns:\n            Number of active connections\n        \"\"\"\n        with self._lock:\n            return len(self._active_connections)\n    \n    def get_total_connections(self) -> int:\n        \"\"\"Get the total number of connections since startup.\n        \n        Returns:\n            Total connection count\n        \"\"\"\n        with self._lock:\n            return self._connections\n    \n    def get_recent_activity(self, count: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Get recent activity.\n        \n        Args:\n            count: Number of recent events to return\n            \n        Returns:\n            List of recent activity events\n        \"\"\"\n        with self._lock:\n            recent = list(self._request_history)[-count:]\n            \n            # Format timestamps\n            for event in recent:\n                ts = event[\"timestamp\"]\n                event[\"formatted_time\"] = datetime.fromtimestamp(ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n            \n            return recent\n    \n    def get_tool_usage_stats(self) -> Dict[str, int]:\n        \"\"\"Get statistics on tool usage.\n        \n        Returns:\n            Dictionary mapping tool names to call counts\n        \"\"\"\n        with self._lock:\n            return dict(self._tool_calls)\n    \n    def get_resource_usage_stats(self) -> Dict[str, int]:\n        \"\"\"Get statistics on resource usage.\n        \n        Returns:\n            Dictionary mapping resource URIs to request counts\n        \"\"\"\n        with self._lock:\n            return dict(self._resource_calls)\n    \n    def get_error_stats(self) -> Dict[str, int]:\n        \"\"\"Get statistics on errors.\n        \n        Returns:\n            Dictionary mapping error types to counts\n        \"\"\"\n        with self._lock:\n            return dict(self._errors)\n    \n    def get_time_series_data(self) -> Dict[str, List[Dict[str, Any]]]:\n        \"\"\"Get time series data for charts.\n        \n        Returns:\n            Dictionary with time series data\n        \"\"\"\n        with self._lock:\n            result = {}\n            \n            # Convert deques to lists of dictionaries\n            for series_name, series_data in self._time_series.items():\n                result[series_name] = [\n                    {\"timestamp\": ts, \"value\": val, \"formatted_time\": datetime.fromtimestamp(ts).strftime(\"%H:%M:%S\")}\n                    for ts, val in series_data\n                ]\n            \n            return result\n    \n    def get_all_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get all metrics data.\n        \n        Returns:\n            Dictionary with all metrics\n        \"\"\"\n        return {\n            \"uptime\": self.get_uptime(),\n            \"active_connections\": self.get_active_connections_count(),\n            \"total_connections\": self.get_total_connections(),\n            \"recent_activity\": self.get_recent_activity(20),\n            \"tool_usage\": self.get_tool_usage_stats(),\n            \"resource_usage\": self.get_resource_usage_stats(),\n            \"errors\": self.get_error_stats(),\n            \"time_series\": self.get_time_series_data()\n        }\n    \n    def reset_stats(self):\n        \"\"\"Reset all statistics but keep the start time.\"\"\"\n        with self._lock:\n            self._request_history.clear()\n            self._tool_calls.clear()\n            self._resource_calls.clear()\n            self._connections = 0\n            self._active_connections.clear()\n            self._errors.clear()\n            \n            # Reset time series\n            current_time = time.time()\n            self._time_series = {\n                \"tool_calls\": deque([(current_time - (600 - i * 60), 0) for i in range(10)], maxlen=10),\n                \"resource_calls\": deque([(current_time - (600 - i * 60), 0) for i in range(10)], maxlen=10)\n            }\n    \n    def shutdown(self):\n        \"\"\"Shutdown the metrics tracker and save data.\"\"\"\n        self._running = False\n        self.save_metrics()\n\n\n# Singleton instance\n_metrics_instance = None\n\ndef get_metrics() -> ServerMetrics:\n    \"\"\"Get or create the singleton metrics instance.\n    \n    Returns:\n        ServerMetrics instance\n    \"\"\"\n    global _metrics_instance\n    if _metrics_instance is None:\n        _metrics_instance = ServerMetrics()\n    return _metrics_instance"}
{"type": "source_file", "path": "claude_code/commands/__init__.py", "content": "\"\"\"Commands package for Claude Code.\"\"\"\n\nfrom claude_code.commands import serve\nfrom claude_code.commands import client\n"}
{"type": "source_file", "path": "claude_code/examples/echo_server.py", "content": "#!/usr/bin/env python3\n\"\"\"\nExample Echo MCP Server for testing the Claude Code MCP client.\nThis server provides a simple 'echo' tool that returns whatever is sent to it.\n\"\"\"\n\nfrom fastmcp import FastMCP\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Create the MCP server\necho_server = FastMCP(\n    \"Echo Server\",\n    description=\"A simple echo server for testing MCP clients\",\n    dependencies=[]\n)\n\n@echo_server.tool(name=\"echo\", description=\"Echoes back the input message\")\nasync def echo(message: str) -> str:\n    \"\"\"Echo back the input message.\n    \n    Args:\n        message: The message to echo back\n        \n    Returns:\n        The same message\n    \"\"\"\n    logger.info(f\"Received message: {message}\")\n    return f\"Echo: {message}\"\n\n@echo_server.tool(name=\"reverse\", description=\"Reverses the input message\")\nasync def reverse(message: str) -> str:\n    \"\"\"Reverse the input message.\n    \n    Args:\n        message: The message to reverse\n        \n    Returns:\n        The reversed message\n    \"\"\"\n    logger.info(f\"Reversing message: {message}\")\n    return f\"Reversed: {message[::-1]}\"\n\n@echo_server.resource(\"echo://{message}\")\ndef echo_resource(message: str) -> str:\n    \"\"\"Echo resource.\n    \n    Args:\n        message: The message to echo\n        \n    Returns:\n        The echoed message\n    \"\"\"\n    return f\"Resource Echo: {message}\"\n\nif __name__ == \"__main__\":\n    # Run the server\n    echo_server.run()"}
{"type": "source_file", "path": "claude_code/lib/ui/__init__.py", "content": ""}
{"type": "source_file", "path": "claude_code/lib/tools/base.py", "content": "#!/usr/bin/env python3\n# claude_code/lib/tools/base.py\n\"\"\"Base classes for tools.\"\"\"\n\nimport abc\nimport inspect\nimport time\nimport logging\nimport os\nimport json\nfrom typing import Dict, List, Any, Callable, Optional, Type, Union, Sequence\nfrom dataclasses import dataclass\n\nfrom pydantic import BaseModel, Field, validator\n\nlogger = logging.getLogger(__name__)\n\n\nclass ToolParameter(BaseModel):\n    \"\"\"Definition of a tool parameter.\"\"\"\n    \n    name: str\n    description: str\n    type: str\n    required: bool = False\n    \n    class Config:\n        \"\"\"Pydantic config.\"\"\"\n        extra = \"forbid\"\n\n\nclass ToolResult(BaseModel):\n    \"\"\"Result of a tool execution.\"\"\"\n    \n    tool_call_id: str\n    name: str\n    result: str\n    execution_time: float\n    token_usage: int = 0\n    status: str = \"success\"\n    error: Optional[str] = None\n    \n    class Config:\n        \"\"\"Pydantic config.\"\"\"\n        extra = \"forbid\"\n\n\nclass Routine(BaseModel):\n    \"\"\"Definition of a tool routine.\"\"\"\n    \n    name: str\n    description: str\n    steps: List[Dict[str, Any]]\n    usage_count: int = 0\n    created_at: float = Field(default_factory=time.time)\n    last_used_at: Optional[float] = None\n    \n    class Config:\n        \"\"\"Pydantic config.\"\"\"\n        extra = \"allow\"\n\n\nclass Tool(BaseModel):\n    \"\"\"Base class for all tools.\"\"\"\n    \n    name: str\n    description: str\n    parameters: Dict[str, Any]\n    function: Callable\n    needs_permission: bool = False\n    category: str = \"general\"\n    \n    class Config:\n        \"\"\"Pydantic config.\"\"\"\n        arbitrary_types_allowed = True\n        extra = \"forbid\"\n    \n    def execute(self, tool_call: Dict[str, Any]) -> ToolResult:\n        \"\"\"Execute the tool with the given parameters.\n        \n        Args:\n            tool_call: Dictionary containing tool call information\n            \n        Returns:\n            ToolResult with execution result\n        \"\"\"\n        # Extract parameters\n        function_name = tool_call.get(\"function\", {}).get(\"name\", \"\")\n        arguments_str = tool_call.get(\"function\", {}).get(\"arguments\", \"{}\")\n        tool_call_id = tool_call.get(\"id\", \"unknown\")\n        \n        # Parse arguments\n        try:\n            arguments = json.loads(arguments_str)\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to parse arguments: {e}\")\n            return ToolResult(\n                tool_call_id=tool_call_id,\n                name=self.name,\n                result=f\"Error: Failed to parse arguments: {e}\",\n                execution_time=0,\n                status=\"error\",\n                error=str(e)\n            )\n        \n        # Execute function\n        start_time = time.time()\n        try:\n            result = self.function(**arguments)\n            execution_time = time.time() - start_time\n            \n            # Convert result to string if it's not already\n            if not isinstance(result, str):\n                result = str(result)\n            \n            return ToolResult(\n                tool_call_id=tool_call_id,\n                name=self.name,\n                result=result,\n                execution_time=execution_time,\n                status=\"success\"\n            )\n        except Exception as e:\n            execution_time = time.time() - start_time\n            logger.exception(f\"Error executing tool {self.name}: {e}\")\n            return ToolResult(\n                tool_call_id=tool_call_id,\n                name=self.name,\n                result=f\"Error: {str(e)}\",\n                execution_time=execution_time,\n                status=\"error\",\n                error=str(e)\n            )\n\n\nclass ToolRegistry:\n    \"\"\"Registry for tools.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the tool registry.\"\"\"\n        self.tools: Dict[str, Tool] = {}\n        self.routines: Dict[str, Routine] = {}\n        self._routine_file = os.path.join(os.path.expanduser(\"~\"), \".claude_code\", \"routines.json\")\n    \n    def register_tool(self, tool: Tool) -> None:\n        \"\"\"Register a tool.\n        \n        Args:\n            tool: Tool instance to register\n            \n        Raises:\n            ValueError: If a tool with the same name is already registered\n        \"\"\"\n        if tool.name in self.tools:\n            raise ValueError(f\"Tool {tool.name} is already registered\")\n        \n        self.tools[tool.name] = tool\n        logger.debug(f\"Registered tool: {tool.name}\")\n    \n    def register_routine(self, routine: Routine) -> None:\n        \"\"\"Register a routine.\n        \n        Args:\n            routine: Routine to register\n            \n        Raises:\n            ValueError: If a routine with the same name is already registered\n        \"\"\"\n        if routine.name in self.routines:\n            raise ValueError(f\"Routine {routine.name} is already registered\")\n        \n        self.routines[routine.name] = routine\n        logger.debug(f\"Registered routine: {routine.name}\")\n        self._save_routines()\n    \n    def register_routine_from_dict(self, routine_dict: Dict[str, Any]) -> None:\n        \"\"\"Register a routine from a dictionary.\n        \n        Args:\n            routine_dict: Dictionary with routine data\n            \n        Raises:\n            ValueError: If a routine with the same name is already registered\n        \"\"\"\n        routine = Routine(**routine_dict)\n        self.register_routine(routine)\n    \n    def get_tool(self, name: str) -> Optional[Tool]:\n        \"\"\"Get a tool by name.\n        \n        Args:\n            name: Name of the tool\n            \n        Returns:\n            Tool instance or None if not found\n        \"\"\"\n        return self.tools.get(name)\n    \n    def get_routine(self, name: str) -> Optional[Routine]:\n        \"\"\"Get a routine by name.\n        \n        Args:\n            name: Name of the routine\n            \n        Returns:\n            Routine or None if not found\n        \"\"\"\n        return self.routines.get(name)\n    \n    def get_all_tools(self) -> List[Tool]:\n        \"\"\"Get all registered tools.\n        \n        Returns:\n            List of all registered tools\n        \"\"\"\n        return list(self.tools.values())\n    \n    def get_all_routines(self) -> List[Routine]:\n        \"\"\"Get all registered routines.\n        \n        Returns:\n            List of all registered routines\n        \"\"\"\n        return list(self.routines.values())\n    \n    def get_tool_schemas(self) -> List[Dict[str, Any]]:\n        \"\"\"Get OpenAI-compatible schemas for all tools.\n        \n        Returns:\n            List of tool schemas for OpenAI function calling\n        \"\"\"\n        schemas = []\n        for tool in self.tools.values():\n            schemas.append({\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.parameters\n                }\n            })\n        return schemas\n    \n    def record_routine_usage(self, name: str) -> None:\n        \"\"\"Record usage of a routine.\n        \n        Args:\n            name: Name of the routine\n        \"\"\"\n        if name in self.routines:\n            routine = self.routines[name]\n            routine.usage_count += 1\n            routine.last_used_at = time.time()\n            self._save_routines()\n    \n    def _save_routines(self) -> None:\n        \"\"\"Save routines to file.\"\"\"\n        try:\n            # Create directory if it doesn't exist\n            os.makedirs(os.path.dirname(self._routine_file), exist_ok=True)\n            \n            # Convert routines to dict for serialization\n            routines_dict = {name: routine.dict() for name, routine in self.routines.items()}\n            \n            # Save to file\n            with open(self._routine_file, 'w') as f:\n                json.dump(routines_dict, f, indent=2)\n            \n            logger.debug(f\"Saved {len(self.routines)} routines to {self._routine_file}\")\n        except Exception as e:\n            logger.error(f\"Error saving routines: {e}\")\n    \n    def load_routines(self) -> None:\n        \"\"\"Load routines from file.\"\"\"\n        if not os.path.exists(self._routine_file):\n            logger.debug(f\"Routines file not found: {self._routine_file}\")\n            return\n        \n        try:\n            with open(self._routine_file, 'r') as f:\n                routines_dict = json.load(f)\n            \n            # Clear existing routines\n            self.routines.clear()\n            \n            # Register each routine\n            for name, routine_data in routines_dict.items():\n                self.routines[name] = Routine(**routine_data)\n            \n            logger.debug(f\"Loaded {len(self.routines)} routines from {self._routine_file}\")\n        except Exception as e:\n            logger.error(f\"Error loading routines: {e}\")\n\n\n@dataclass\nclass RoutineStep:\n    \"\"\"A step in a routine.\"\"\"\n    tool_name: str\n    args: Dict[str, Any]\n    condition: Optional[Dict[str, Any]] = None\n    store_result: bool = False\n    result_var: Optional[str] = None\n\n\n@dataclass\nclass RoutineDefinition:\n    \"\"\"Definition of a routine.\"\"\"\n    name: str\n    description: str\n    steps: List[RoutineStep]\n\n\ndef tool(name: str, description: str, parameters: Dict[str, Any], \n         needs_permission: bool = False, category: str = \"general\"):\n    \"\"\"Decorator to register a function as a tool.\n    \n    Args:\n        name: Name of the tool\n        description: Description of the tool\n        parameters: Parameter schema for the tool\n        needs_permission: Whether the tool needs user permission\n        category: Category of the tool\n        \n    Returns:\n        Decorator function\n    \"\"\"\n    def decorator(func: Callable) -> Callable:\n        # Set tool metadata on the function\n        func._tool_info = {\n            \"name\": name,\n            \"description\": description,\n            \"parameters\": parameters,\n            \"needs_permission\": needs_permission,\n            \"category\": category\n        }\n        return func\n    return decorator\n\n\ndef create_tools_from_functions(registry: ToolRegistry, functions: List[Callable]) -> None:\n    \"\"\"Create and register tools from functions with _tool_info.\n    \n    Args:\n        registry: Tool registry to register tools with\n        functions: List of functions to create tools from\n    \"\"\"\n    for func in functions:\n        if hasattr(func, \"_tool_info\"):\n            info = func._tool_info\n            tool = Tool(\n                name=info[\"name\"],\n                description=info[\"description\"],\n                parameters=info[\"parameters\"],\n                function=func,\n                needs_permission=info[\"needs_permission\"],\n                category=info[\"category\"]\n            )\n            registry.register_tool(tool)"}
{"type": "source_file", "path": "claude_code/lib/providers/base.py", "content": "#!/usr/bin/env python3\n# claude_code/lib/providers/base.py\n\"\"\"Base provider interface for LLM integration.\"\"\"\n\nimport abc\nfrom typing import Dict, List, Generator, Optional, Any, Union\n\n\nclass BaseProvider(abc.ABC):\n    \"\"\"Abstract base class for LLM providers.\n    \n    This class defines the interface that all LLM providers must implement.\n    Providers are responsible for:\n    - Generating completions from LLMs\n    - Counting tokens\n    - Managing rate limits\n    - Tracking costs\n    \"\"\"\n    \n    @property\n    @abc.abstractmethod\n    def name(self) -> str:\n        \"\"\"Get the name of the provider.\"\"\"\n        pass\n    \n    @property\n    @abc.abstractmethod\n    def available_models(self) -> List[str]:\n        \"\"\"Get a list of available models from this provider.\"\"\"\n        pass\n    \n    @property\n    @abc.abstractmethod\n    def current_model(self) -> str:\n        \"\"\"Get the currently selected model.\"\"\"\n        pass\n    \n    @abc.abstractmethod\n    def set_model(self, model_name: str) -> None:\n        \"\"\"Set the current model.\n        \n        Args:\n            model_name: The name of the model to use\n            \n        Raises:\n            ValueError: If the model is not available\n        \"\"\"\n        pass\n    \n    @abc.abstractmethod\n    def generate_completion(self, \n                           messages: List[Dict[str, Any]], \n                           tools: Optional[List[Dict[str, Any]]] = None,\n                           temperature: float = 0.0,\n                           stream: bool = True) -> Union[Dict[str, Any], Generator[Dict[str, Any], None, None]]:\n        \"\"\"Generate a completion from the provider.\n        \n        Args:\n            messages: List of message dictionaries\n            tools: Optional list of tool dictionaries\n            temperature: Model temperature (0-1)\n            stream: Whether to stream the response\n            \n        Returns:\n            If stream=True, returns a generator of response chunks\n            If stream=False, returns the complete response\n        \"\"\"\n        pass\n    \n    @abc.abstractmethod\n    def count_tokens(self, text: str) -> int:\n        \"\"\"Count tokens in text.\n        \n        Args:\n            text: The text to count tokens for\n            \n        Returns:\n            The number of tokens in the text\n        \"\"\"\n        pass\n    \n    @abc.abstractmethod\n    def count_message_tokens(self, messages: List[Dict[str, Any]]) -> Dict[str, int]:\n        \"\"\"Count tokens in a message list.\n        \n        Args:\n            messages: List of message dictionaries\n            \n        Returns:\n            Dictionary with 'input' and 'output' token counts\n        \"\"\"\n        pass\n    \n    @abc.abstractmethod\n    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about the current model.\n        \n        Returns:\n            Dictionary with model information including:\n            - context_window: Maximum context window size\n            - input_cost_per_1k: Cost per 1K input tokens\n            - output_cost_per_1k: Cost per 1K output tokens\n            - capabilities: List of model capabilities\n        \"\"\"\n        pass\n    \n    @property\n    @abc.abstractmethod\n    def cost_per_1k_tokens(self) -> Dict[str, float]:\n        \"\"\"Get cost per 1K tokens for input and output.\n        \n        Returns:\n            Dictionary with 'input' and 'output' costs\n        \"\"\"\n        pass\n    \n    @abc.abstractmethod\n    def validate_api_key(self) -> bool:\n        \"\"\"Validate the API key.\n        \n        Returns:\n            True if the API key is valid, False otherwise\n        \"\"\"\n        pass\n    \n    @abc.abstractmethod\n    def get_rate_limit_info(self) -> Dict[str, Any]:\n        \"\"\"Get rate limit information.\n        \n        Returns:\n            Dictionary with rate limit information\n        \"\"\"\n        pass"}
{"type": "source_file", "path": "claude_code/util/__init__.py", "content": ""}
{"type": "source_file", "path": "claude_code/mcp_server.py", "content": "#!/usr/bin/env python3\n# claude_code/mcp_server.py\n\"\"\"Model Context Protocol server implementation using FastMCP.\"\"\"\n\nimport os\nimport logging\nimport platform\nimport sys\nimport uuid\nimport time\nfrom typing import Dict, List, Any, Optional, Callable, Union\nimport pathlib\nimport json\nfrom fastmcp import FastMCP, Context, Image\n\nfrom claude_code.lib.tools.base import Tool, ToolRegistry\nfrom claude_code.lib.tools.manager import ToolExecutionManager\nfrom claude_code.lib.tools.file_tools import register_file_tools\nfrom claude_code.lib.monitoring.server_metrics import get_metrics\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Get server metrics\nmetrics = get_metrics()\n\n# Create the FastMCP server\nmcp = FastMCP(\n    \"Claude Code MCP Server\",\n    description=\"A Model Context Protocol server for Claude Code tools\",\n    dependencies=[\"fastmcp>=0.4.1\", \"openai\", \"pydantic\"],\n    homepage_html_file=str(pathlib.Path(__file__).parent / \"examples\" / \"claude_mcp_config.html\")\n)\n\n# Initialize tool registry and manager\ntool_registry = ToolRegistry()\ntool_manager = ToolExecutionManager(tool_registry)\n\n# Register file tools\nregister_file_tools(tool_registry)\n\n\ndef setup_tools():\n    \"\"\"Register all tools from the tool registry with FastMCP.\"\"\"\n    \n    # Get all registered tools\n    registered_tools = tool_registry.get_all_tools()\n    \n    for tool_obj in registered_tools:\n        # Convert the tool execution function to an MCP tool\n        @mcp.tool(name=tool_obj.name, description=tool_obj.description)\n        async def tool_executor(params: Dict[str, Any], ctx: Context) -> str:\n            # Create a tool call in the format expected by ToolExecutionManager\n            tool_call = {\n                \"id\": ctx.request_id,\n                \"function\": {\n                    \"name\": tool_obj.name,\n                    \"arguments\": str(params)\n                }\n            }\n            \n            try:\n                # Log the tool call in metrics\n                metrics.log_tool_call(tool_obj.name)\n                \n                # Execute the tool and get the result\n                result = tool_obj.execute(tool_call)\n                \n                # Report progress when complete\n                await ctx.report_progress(1, 1)\n                \n                return result.result\n            except Exception as e:\n                # Log error in metrics\n                metrics.log_error(f\"tool_{tool_obj.name}\", str(e))\n                raise\n\n\n# Function to register all View resources\ndef register_view_resources():\n    \"\"\"Register file viewing as resources.\"\"\"\n    \n    @mcp.resource(\"file://{file_path}\")\n    def get_file_content(file_path: str) -> str:\n        \"\"\"Get the content of a file\"\"\"\n        try:\n            # Log resource request\n            metrics.log_resource_request(f\"file://{file_path}\")\n            \n            # Get the View tool\n            view_tool = tool_registry.get_tool(\"View\")\n            if not view_tool:\n                metrics.log_error(\"resource_error\", \"View tool not found\")\n                return \"Error: View tool not found\"\n            \n            # Execute the tool to get file content\n            tool_call = {\n                \"id\": \"resource_call\",\n                \"function\": {\n                    \"name\": \"View\",\n                    \"arguments\": json.dumps({\"file_path\": file_path})\n                }\n            }\n            \n            result = view_tool.execute(tool_call)\n            return result.result\n        except Exception as e:\n            metrics.log_error(\"resource_error\", f\"Error viewing file: {str(e)}\")\n            return f\"Error: {str(e)}\"\n\n\n# Register file system resources\n@mcp.resource(\"filesystem://{path}\")\ndef list_directory(path: str) -> str:\n    \"\"\"List files and directories at the given path.\"\"\"\n    try:\n        # Log resource request\n        metrics.log_resource_request(f\"filesystem://{path}\")\n        \n        import os\n        \n        if not os.path.isabs(path):\n            metrics.log_error(\"resource_error\", f\"Path must be absolute: {path}\")\n            return f\"Error: Path must be absolute: {path}\"\n        \n        if not os.path.exists(path):\n            metrics.log_error(\"resource_error\", f\"Path does not exist: {path}\")\n            return f\"Error: Path does not exist: {path}\"\n        \n        if not os.path.isdir(path):\n            metrics.log_error(\"resource_error\", f\"Path is not a directory: {path}\")\n            return f\"Error: Path is not a directory: {path}\"\n        \n        items = os.listdir(path)\n        result = []\n        \n        for item in items:\n            item_path = os.path.join(path, item)\n            if os.path.isdir(item_path):\n                result.append(f\"{item}/\")\n            else:\n                result.append(item)\n        \n        return \"\\n\".join(result)\n    except Exception as e:\n        metrics.log_error(\"resource_error\", f\"Error listing directory: {str(e)}\")\n        return f\"Error: {str(e)}\"\n\n\n# Add system information resource\n@mcp.resource(\"system://info\")\ndef get_system_info() -> str:\n    \"\"\"Get system information\"\"\"\n    try:\n        # Log resource request\n        metrics.log_resource_request(\"system://info\")\n        \n        info = {\n            \"os\": platform.system(),\n            \"os_version\": platform.version(),\n            \"python_version\": sys.version,\n            \"hostname\": platform.node(),\n            \"platform\": platform.platform(),\n            \"architecture\": platform.architecture(),\n            \"processor\": platform.processor(),\n            \"uptime\": metrics.get_uptime()\n        }\n        \n        return \"\\n\".join([f\"{k}: {v}\" for k, v in info.items()])\n    except Exception as e:\n        metrics.log_error(\"resource_error\", f\"Error getting system info: {str(e)}\")\n        return f\"Error: {str(e)}\"\n\n\n# Add configuration resource\n@mcp.resource(\"config://json\")\ndef get_config_json() -> str:\n    \"\"\"Get Claude Desktop MCP configuration in JSON format\"\"\"\n    try:\n        # Log resource request\n        metrics.log_resource_request(\"config://json\")\n        \n        config_path = pathlib.Path(__file__).parent / \"examples\" / \"claude_mcp_config.json\"\n        \n        try:\n            with open(config_path, 'r', encoding='utf-8') as f:\n                config = json.load(f)\n                \n                # Update working directory to actual path\n                current_dir = str(pathlib.Path(__file__).parent.parent.absolute())\n                config[\"workingDirectory\"] = current_dir\n                \n                return json.dumps(config, indent=2)\n        except Exception as e:\n            logger.error(f\"Error reading config file: {e}\")\n            metrics.log_error(\"resource_error\", f\"Error reading config file: {str(e)}\")\n            \n            return json.dumps({\n                \"name\": \"Claude Code Tools\",\n                \"type\": \"local_process\",\n                \"command\": \"python\",\n                \"args\": [\"claude.py\", \"serve\"],\n                \"workingDirectory\": str(pathlib.Path(__file__).parent.parent.absolute()),\n                \"environment\": {},\n                \"description\": \"A Model Context Protocol server for Claude Code tools\"\n            }, indent=2)\n    except Exception as e:\n        metrics.log_error(\"resource_error\", f\"Error in config resource: {str(e)}\")\n        return f\"Error: {str(e)}\"\n\n\n# Add metrics resource\n@mcp.resource(\"metrics://json\")\ndef get_metrics_json() -> str:\n    \"\"\"Get server metrics in JSON format\"\"\"\n    try:\n        # Log resource request\n        metrics.log_resource_request(\"metrics://json\")\n        \n        # Get all metrics\n        all_metrics = metrics.get_all_metrics()\n        \n        return json.dumps(all_metrics, indent=2)\n    except Exception as e:\n        metrics.log_error(\"resource_error\", f\"Error getting metrics: {str(e)}\")\n        return f\"Error: {str(e)}\"\n\n\n# Add metrics tool\n@mcp.tool(name=\"GetServerMetrics\", description=\"Get server metrics and statistics\")\nasync def get_server_metrics(metric_type: str = \"all\") -> str:\n    \"\"\"Get server metrics and statistics.\n    \n    Args:\n        metric_type: Type of metrics to return (all, uptime, tools, resources, errors)\n        \n    Returns:\n        The requested metrics information\n    \"\"\"\n    try:\n        # Log tool call\n        metrics.log_tool_call(\"GetServerMetrics\")\n        \n        if metric_type.lower() == \"all\":\n            all_metrics = metrics.get_all_metrics()\n            return json.dumps(all_metrics, indent=2)\n        \n        elif metric_type.lower() == \"uptime\":\n            return f\"Server uptime: {metrics.get_uptime()}\"\n        \n        elif metric_type.lower() == \"tools\":\n            tool_stats = metrics.get_tool_usage_stats()\n            result = \"Tool Usage Statistics:\\n\\n\"\n            for tool, count in sorted(tool_stats.items(), key=lambda x: x[1], reverse=True):\n                result += f\"- {tool}: {count} calls\\n\"\n            return result\n        \n        elif metric_type.lower() == \"resources\":\n            resource_stats = metrics.get_resource_usage_stats()\n            result = \"Resource Usage Statistics:\\n\\n\"\n            for resource, count in sorted(resource_stats.items(), key=lambda x: x[1], reverse=True):\n                result += f\"- {resource}: {count} requests\\n\"\n            return result\n        \n        elif metric_type.lower() == \"errors\":\n            error_stats = metrics.get_error_stats()\n            if not error_stats:\n                return \"No errors recorded.\"\n                \n            result = \"Error Statistics:\\n\\n\"\n            for error_type, count in sorted(error_stats.items(), key=lambda x: x[1], reverse=True):\n                result += f\"- {error_type}: {count} occurrences\\n\"\n            return result\n        \n        elif metric_type.lower() == \"activity\":\n            recent = metrics.get_recent_activity(15)\n            result = \"Recent Activity:\\n\\n\"\n            for event in recent:\n                time_str = event.get(\"formatted_time\", \"unknown\")\n                if event[\"type\"] == \"tool\":\n                    result += f\"[{time_str}] Tool call: {event['name']}\\n\"\n                elif event[\"type\"] == \"resource\":\n                    result += f\"[{time_str}] Resource request: {event['uri']}\\n\"\n                elif event[\"type\"] == \"connection\":\n                    action = \"connected\" if event[\"action\"] == \"connect\" else \"disconnected\"\n                    result += f\"[{time_str}] Client {event['client_id']} {action}\\n\"\n                elif event[\"type\"] == \"error\":\n                    result += f\"[{time_str}] Error ({event['error_type']}): {event['message']}\\n\"\n            return result\n            \n        else:\n            return f\"Unknown metric type: {metric_type}. Available types: all, uptime, tools, resources, errors, activity\"\n    \n    except Exception as e:\n        metrics.log_error(\"tool_error\", f\"Error in GetServerMetrics: {str(e)}\")\n        return f\"Error retrieving metrics: {str(e)}\"\n\n\n# Add connection tracking\n@mcp.on_connect\nasync def handle_connect(ctx: Context):\n    \"\"\"Track client connections.\"\"\"\n    client_id = str(uuid.uuid4())\n    ctx.client_data[\"id\"] = client_id\n    metrics.log_connection(client_id, connected=True)\n    logger.info(f\"Client connected: {client_id}\")\n\n\n@mcp.on_disconnect\nasync def handle_disconnect(ctx: Context):\n    \"\"\"Track client disconnections.\"\"\"\n    client_id = ctx.client_data.get(\"id\", \"unknown\")\n    metrics.log_connection(client_id, connected=False)\n    logger.info(f\"Client disconnected: {client_id}\")\n\n\n@mcp.tool(name=\"GetConfiguration\", description=\"Get Claude Desktop configuration for this MCP server\")\nasync def get_configuration(format: str = \"json\") -> str:\n    \"\"\"Get configuration for connecting Claude Desktop to this MCP server.\n    \n    Args:\n        format: The format to return (json or text)\n        \n    Returns:\n        The configuration in the requested format\n    \"\"\"\n    if format.lower() == \"json\":\n        return get_config_json()\n    else:\n        # Return text instructions\n        config = json.loads(get_config_json())\n        \n        return f\"\"\"\nTo connect Claude Desktop to this MCP server:\n\n1. Open Claude Desktop and go to Settings\n2. Navigate to \"Model Context Protocol\" section\n3. Click \"Add New Server\"\n4. Use the following settings:\n   - Name: {config['name']}\n   - Type: Local Process\n   - Command: {config['command']}\n   - Arguments: {\" \".join(config['args'])}\n   - Working Directory: {config['workingDirectory']}\n5. Click Save and connect to the server\n\nYou can also visit http://localhost:8000 for more detailed instructions and to download the configuration file.\n\"\"\"\n\n\n# Initialize MCP server\ndef initialize_server():\n    \"\"\"Initialize the MCP server with all tools and resources.\"\"\"\n    # Register all tools\n    setup_tools()\n    \n    # Register resources\n    register_view_resources()\n    \n    # Add metrics tool for server monitoring\n    @mcp.tool(name=\"ResetServerMetrics\", description=\"Reset server metrics tracking\")\n    async def reset_metrics(confirm: bool = False) -> str:\n        \"\"\"Reset server metrics tracking.\n        \n        Args:\n            confirm: Confirmation flag to prevent accidental resets\n            \n        Returns:\n            Confirmation message\n        \"\"\"\n        if not confirm:\n            return \"Please set confirm=true to reset server metrics.\"\n        \n        # Log the call\n        metrics.log_tool_call(\"ResetServerMetrics\")\n        \n        # Reset metrics\n        metrics.reset_stats()\n        \n        return \"Server metrics have been reset successfully.\"\n    \n    logger.info(\"MCP server initialized with all tools and resources\")\n    \n    return mcp\n\n\n# Main function to run the server\ndef main():\n    \"\"\"Run the MCP server\"\"\"\n    # Initialize the server\n    server = initialize_server()\n    \n    # Run the server\n    server.run()\n\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "claude_code/lib/tools/search_tools.py", "content": "#!/usr/bin/env python3\n# claude_code/lib/tools/search_tools.py\n\"\"\"Web search and information retrieval tools.\"\"\"\n\nimport os\nimport logging\nimport json\nimport urllib.parse\nimport requests\nfrom typing import Dict, List, Optional, Any\n\nfrom .base import tool, ToolRegistry\n\nlogger = logging.getLogger(__name__)\n\n\n@tool(\n    name=\"WebSearch\",\n    description=\"Search the web for information using various search engines\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\n                \"type\": \"string\",\n                \"description\": \"The search query\"\n            },\n            \"engine\": {\n                \"type\": \"string\",\n                \"description\": \"Search engine to use (google, bing, duckduckgo)\",\n                \"enum\": [\"google\", \"bing\", \"duckduckgo\"]\n            },\n            \"num_results\": {\n                \"type\": \"integer\",\n                \"description\": \"Number of results to return (max 10)\"\n            }\n        },\n        \"required\": [\"query\"]\n    },\n    category=\"search\"\n)\ndef web_search(query: str, engine: str = \"google\", num_results: int = 5) -> str:\n    \"\"\"Search the web for information.\n    \n    Args:\n        query: Search query\n        engine: Search engine to use\n        num_results: Number of results to return\n        \n    Returns:\n        Search results as formatted text\n    \"\"\"\n    logger.info(f\"Searching web for: {query} using {engine}\")\n    \n    # Validate inputs\n    if num_results > 10:\n        num_results = 10  # Cap at 10 results\n    \n    # Get API key based on engine\n    api_key = None\n    if engine == \"google\":\n        api_key = os.getenv(\"GOOGLE_SEARCH_API_KEY\")\n        cx = os.getenv(\"GOOGLE_SEARCH_CX\")\n        if not api_key or not cx:\n            return \"Error: Google Search API key or CX not configured. Please set GOOGLE_SEARCH_API_KEY and GOOGLE_SEARCH_CX environment variables.\"\n    elif engine == \"bing\":\n        api_key = os.getenv(\"BING_SEARCH_API_KEY\")\n        if not api_key:\n            return \"Error: Bing Search API key not configured. Please set BING_SEARCH_API_KEY environment variable.\"\n    \n    # Perform search based on engine\n    try:\n        if engine == \"google\":\n            return _google_search(query, api_key, cx, num_results)\n        elif engine == \"bing\":\n            return _bing_search(query, api_key, num_results)\n        elif engine == \"duckduckgo\":\n            return _duckduckgo_search(query, num_results)\n        else:\n            return f\"Error: Unsupported search engine: {engine}\"\n    except Exception as e:\n        logger.exception(f\"Error during web search: {str(e)}\")\n        return f\"Error performing search: {str(e)}\"\n\n\ndef _google_search(query: str, api_key: str, cx: str, num_results: int) -> str:\n    \"\"\"Perform Google search using Custom Search API.\"\"\"\n    url = \"https://www.googleapis.com/customsearch/v1\"\n    params = {\n        \"key\": api_key,\n        \"cx\": cx,\n        \"q\": query,\n        \"num\": min(num_results, 10)\n    }\n    \n    response = requests.get(url, params=params)\n    if response.status_code != 200:\n        return f\"Error: Google search failed with status code {response.status_code}: {response.text}\"\n    \n    data = response.json()\n    if \"items\" not in data:\n        return f\"No results found for '{query}'\"\n    \n    results = []\n    for i, item in enumerate(data[\"items\"], 1):\n        title = item.get(\"title\", \"No title\")\n        link = item.get(\"link\", \"No link\")\n        snippet = item.get(\"snippet\", \"No description\").replace(\"\\n\", \" \")\n        results.append(f\"{i}. {title}\\n   URL: {link}\\n   {snippet}\\n\")\n    \n    return f\"Google Search Results for '{query}':\\n\\n\" + \"\\n\".join(results)\n\n\ndef _bing_search(query: str, api_key: str, num_results: int) -> str:\n    \"\"\"Perform Bing search using Bing Web Search API.\"\"\"\n    url = \"https://api.bing.microsoft.com/v7.0/search\"\n    headers = {\"Ocp-Apim-Subscription-Key\": api_key}\n    params = {\n        \"q\": query,\n        \"count\": min(num_results, 10),\n        \"responseFilter\": \"Webpages\"\n    }\n    \n    response = requests.get(url, headers=headers, params=params)\n    if response.status_code != 200:\n        return f\"Error: Bing search failed with status code {response.status_code}: {response.text}\"\n    \n    data = response.json()\n    if \"webPages\" not in data or \"value\" not in data[\"webPages\"]:\n        return f\"No results found for '{query}'\"\n    \n    results = []\n    for i, item in enumerate(data[\"webPages\"][\"value\"], 1):\n        title = item.get(\"name\", \"No title\")\n        link = item.get(\"url\", \"No link\")\n        snippet = item.get(\"snippet\", \"No description\").replace(\"\\n\", \" \")\n        results.append(f\"{i}. {title}\\n   URL: {link}\\n   {snippet}\\n\")\n    \n    return f\"Bing Search Results for '{query}':\\n\\n\" + \"\\n\".join(results)\n\n\ndef _duckduckgo_search(query: str, num_results: int) -> str:\n    \"\"\"Perform DuckDuckGo search using their API.\"\"\"\n    # DuckDuckGo doesn't have an official API, but we can use their instant answer API\n    url = \"https://api.duckduckgo.com/\"\n    params = {\n        \"q\": query,\n        \"format\": \"json\",\n        \"no_html\": 1,\n        \"skip_disambig\": 1\n    }\n    \n    response = requests.get(url, params=params)\n    if response.status_code != 200:\n        return f\"Error: DuckDuckGo search failed with status code {response.status_code}: {response.text}\"\n    \n    data = response.json()\n    \n    results = []\n    \n    # Add the abstract if available\n    if data.get(\"Abstract\"):\n        results.append(f\"Summary: {data['Abstract']}\\n\")\n    \n    # Add related topics\n    if data.get(\"RelatedTopics\"):\n        topics = data[\"RelatedTopics\"][:num_results]\n        for i, topic in enumerate(topics, 1):\n            if \"Text\" in topic:\n                text = topic.get(\"Text\", \"No description\")\n                url = topic.get(\"FirstURL\", \"No URL\")\n                results.append(f\"{i}. {text}\\n   URL: {url}\\n\")\n    \n    if not results:\n        return f\"No results found for '{query}'\"\n    \n    return f\"DuckDuckGo Search Results for '{query}':\\n\\n\" + \"\\n\".join(results)\n\n\n@tool(\n    name=\"WikipediaSearch\",\n    description=\"Search Wikipedia for information on a topic\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"query\": {\n                \"type\": \"string\",\n                \"description\": \"The topic to search for\"\n            },\n            \"language\": {\n                \"type\": \"string\",\n                \"description\": \"Language code (e.g., 'en', 'es', 'fr')\",\n                \"default\": \"en\"\n            }\n        },\n        \"required\": [\"query\"]\n    },\n    category=\"search\"\n)\ndef wikipedia_search(query: str, language: str = \"en\") -> str:\n    \"\"\"Search Wikipedia for information on a topic.\n    \n    Args:\n        query: Topic to search for\n        language: Language code\n        \n    Returns:\n        Wikipedia article summary\n    \"\"\"\n    logger.info(f\"Searching Wikipedia for: {query} in {language}\")\n    \n    try:\n        # Wikipedia API endpoint\n        url = f\"https://{language}.wikipedia.org/api/rest_v1/page/summary/{urllib.parse.quote(query)}\"\n        \n        response = requests.get(url)\n        if response.status_code != 200:\n            # Try search API if direct lookup fails\n            search_url = f\"https://{language}.wikipedia.org/w/api.php\"\n            search_params = {\n                \"action\": \"query\",\n                \"list\": \"search\",\n                \"srsearch\": query,\n                \"format\": \"json\"\n            }\n            \n            search_response = requests.get(search_url, params=search_params)\n            if search_response.status_code != 200:\n                return f\"Error: Wikipedia search failed with status code {search_response.status_code}\"\n            \n            search_data = search_response.json()\n            if \"query\" not in search_data or \"search\" not in search_data[\"query\"] or not search_data[\"query\"][\"search\"]:\n                return f\"No Wikipedia articles found for '{query}'\"\n            \n            # Get the first search result\n            first_result = search_data[\"query\"][\"search\"][0]\n            title = first_result[\"title\"]\n            \n            # Get the summary for the first result\n            url = f\"https://{language}.wikipedia.org/api/rest_v1/page/summary/{urllib.parse.quote(title)}\"\n            response = requests.get(url)\n            if response.status_code != 200:\n                return f\"Error: Wikipedia article lookup failed with status code {response.status_code}\"\n        \n        data = response.json()\n        \n        # Format the response\n        title = data.get(\"title\", \"Unknown\")\n        extract = data.get(\"extract\", \"No information available\")\n        url = data.get(\"content_urls\", {}).get(\"desktop\", {}).get(\"page\", \"\")\n        \n        result = f\"Wikipedia: {title}\\n\\n{extract}\\n\"\n        if url:\n            result += f\"\\nSource: {url}\"\n        \n        return result\n    \n    except Exception as e:\n        logger.exception(f\"Error during Wikipedia search: {str(e)}\")\n        return f\"Error searching Wikipedia: {str(e)}\"\n\n\ndef register_search_tools(registry: ToolRegistry) -> None:\n    \"\"\"Register all search tools with the registry.\n    \n    Args:\n        registry: Tool registry to register with\n    \"\"\"\n    from .base import create_tools_from_functions\n    \n    search_tools = [\n        web_search,\n        wikipedia_search\n    ]\n    \n    create_tools_from_functions(registry, search_tools)\n"}
{"type": "source_file", "path": "deploy_modal_mcp.py", "content": "#!/usr/bin/env python3\n\"\"\"\nDeployment script for Modal MCP Server\n\"\"\"\nimport os\nimport sys\nimport argparse\nimport subprocess\nimport webbrowser\nimport time\nfrom pathlib import Path\n\ndef check_dependencies():\n    \"\"\"Check if required dependencies are installed\"\"\"\n    try:\n        import modal\n        import httpx\n        import fastapi\n        import uvicorn\n        print(\"✅ All required dependencies are installed\")\n        return True\n    except ImportError as e:\n        print(f\"❌ Missing dependency: {e}\")\n        print(\"Please install required dependencies:\")\n        print(\"pip install modal httpx fastapi uvicorn\")\n        return False\n\ndef deploy_modal_server(args):\n    \"\"\"Deploy the Modal OpenAI-compatible server\"\"\"\n    print(\"Deploying Modal OpenAI-compatible server...\")\n    \n    # Run the Modal deployment command\n    cmd = [\"modal\", \"deploy\", \"modal_mcp_server.py\"]\n    \n    try:\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        \n        if result.returncode != 0:\n            print(f\"❌ Error deploying Modal server: {result.stderr}\")\n            return None\n        \n        # Extract the deployment URL from the output\n        for line in result.stdout.splitlines():\n            if \"https://\" in line and \"modal.run\" in line:\n                url = line.strip()\n                print(f\"✅ Modal server deployed at: {url}\")\n                return url\n        \n        print(\"❌ Could not find deployment URL in output\")\n        print(result.stdout)\n        return None\n        \n    except Exception as e:\n        print(f\"❌ Error deploying Modal server: {e}\")\n        return None\n\ndef deploy_mcp_adapter(modal_url, args):\n    \"\"\"Deploy the MCP adapter server\"\"\"\n    print(\"Deploying MCP adapter server...\")\n    \n    # Set environment variables for the adapter\n    os.environ[\"MODAL_API_URL\"] = modal_url\n    os.environ[\"MODAL_API_KEY\"] = args.api_key\n    os.environ[\"DEFAULT_MODEL\"] = args.model\n    \n    # Start the adapter server\n    try:\n        import uvicorn\n        from mcp_modal_adapter import app\n        \n        # Start in a separate process if not in foreground mode\n        if not args.foreground:\n            print(f\"Starting MCP adapter server on port {args.port}...\")\n            cmd = [\n                sys.executable, \"-m\", \"uvicorn\", \"mcp_modal_adapter:app\", \n                \"--host\", \"0.0.0.0\", \"--port\", str(args.port)\n            ]\n            \n            # Use subprocess.Popen to run in background\n            process = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE if not args.verbose else None,\n                stderr=subprocess.PIPE if not args.verbose else None\n            )\n            \n            # Wait a bit to make sure it starts\n            time.sleep(2)\n            \n            # Check if process is still running\n            if process.poll() is None:\n                print(f\"✅ MCP adapter server running on http://localhost:{args.port}\")\n                return f\"http://localhost:{args.port}\"\n            else:\n                stdout, stderr = process.communicate()\n                print(f\"❌ Error starting MCP adapter server: {stderr.decode() if stderr else 'Unknown error'}\")\n                return None\n        else:\n            # Run in foreground\n            print(f\"Starting MCP adapter server on port {args.port} in foreground mode...\")\n            uvicorn.run(app, host=\"0.0.0.0\", port=args.port)\n            return None  # Will never reach here in foreground mode\n            \n    except Exception as e:\n        print(f\"❌ Error starting MCP adapter server: {e}\")\n        return None\n\ndef main():\n    \"\"\"Main entry point\"\"\"\n    parser = argparse.ArgumentParser(description=\"Deploy Modal MCP Server\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port for MCP adapter server\")\n    parser.add_argument(\"--api-key\", type=str, default=\"sk-modal-llm-api-key\", help=\"API key for Modal server\")\n    parser.add_argument(\"--model\", type=str, default=\"phi-4\", help=\"Default model to use\")\n    parser.add_argument(\"--foreground\", action=\"store_true\", help=\"Run MCP adapter in foreground\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Show verbose output\")\n    parser.add_argument(\"--skip-modal-deploy\", action=\"store_true\", help=\"Skip Modal server deployment\")\n    parser.add_argument(\"--modal-url\", type=str, help=\"Use existing Modal server URL\")\n    \n    args = parser.parse_args()\n    \n    # Check dependencies\n    if not check_dependencies():\n        return 1\n    \n    # Deploy Modal server if not skipped\n    modal_url = args.modal_url\n    if not args.skip_modal_deploy and not modal_url:\n        modal_url = deploy_modal_server(args)\n        if not modal_url:\n            return 1\n    \n    # Deploy MCP adapter\n    mcp_url = deploy_mcp_adapter(modal_url, args)\n    if not mcp_url and not args.foreground:\n        return 1\n    \n    # Open browser if not in foreground mode\n    if mcp_url and not args.foreground:\n        print(f\"Opening browser to MCP server health check...\")\n        webbrowser.open(f\"{mcp_url}/health\")\n        \n        print(\"\\nMCP Server is now running!\")\n        print(f\"- Health check: {mcp_url}/health\")\n        print(f\"- List prompts: {mcp_url}/prompts\")\n        print(f\"- Modal API: {modal_url}\")\n        \n        print(\"\\nPress Ctrl+C to stop the server\")\n        try:\n            while True:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            print(\"\\nStopping server...\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"}
{"type": "source_file", "path": "examples/echo_server.py", "content": "#!/usr/bin/env python3\n\"\"\"\nSimple Echo MCP Server Example\n\nThis is a basic implementation of a Model Context Protocol (MCP) server\nthat simply echoes back the parameters it receives.\n\"\"\"\n\nimport os\nimport json\nimport time\nimport uuid\nfrom typing import Dict, List, Any, Optional\nfrom fastapi import FastAPI, HTTPException, Request\nfrom fastapi.responses import JSONResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field\nimport uvicorn\n\n# MCP Protocol Models\nclass MCPHealthResponse(BaseModel):\n    status: str = \"healthy\"\n    version: str = \"1.0.0\"\n    protocol_version: str = \"0.1.0\"\n    provider: str = \"Echo MCP Server\"\n    models: List[str] = [\"echo-model\"]\n\nclass MCPContextRequest(BaseModel):\n    prompt_id: str\n    parameters: Dict[str, Any] = Field(default_factory=dict)\n    model: Optional[str] = None\n    stream: bool = False\n    user: Optional[str] = None\n    conversation_id: Optional[str] = None\n    message_id: Optional[str] = None\n\nclass MCPContextResponse(BaseModel):\n    context: str\n    context_id: str\n    model: str\n    usage: Dict[str, int] = Field(default_factory=dict)\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n\nclass MCPPromptTemplate(BaseModel):\n    id: str\n    template: str\n    description: Optional[str] = None\n    parameters: Dict[str, Dict[str, Any]] = Field(default_factory=dict)\n    default_model: Optional[str] = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n\nclass MCPPromptLibraryResponse(BaseModel):\n    prompts: List[MCPPromptTemplate]\n    count: int\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"Echo MCP Server\",\n    description=\"A simple MCP server that echoes back parameters\",\n    version=\"1.0.0\",\n)\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Define prompt templates\nprompt_templates = {\n    \"echo\": {\n        \"template\": \"You said: {message}\",\n        \"description\": \"Echoes back the message\",\n        \"parameters\": {\n            \"message\": {\n                \"type\": \"string\",\n                \"description\": \"The message to echo\"\n            }\n        },\n        \"default_model\": \"echo-model\",\n        \"metadata\": {\n            \"category\": \"utility\"\n        }\n    },\n    \"reverse\": {\n        \"template\": \"Reversed: {message}\",\n        \"description\": \"Reverses the message\",\n        \"parameters\": {\n            \"message\": {\n                \"type\": \"string\",\n                \"description\": \"The message to reverse\"\n            }\n        },\n        \"default_model\": \"echo-model\",\n        \"metadata\": {\n            \"category\": \"utility\"\n        }\n    }\n}\n\n# MCP Protocol Routes\n@app.get(\"/\", response_model=MCPHealthResponse)\nasync def health_check():\n    \"\"\"Health check endpoint required by MCP protocol\"\"\"\n    return MCPHealthResponse()\n\n@app.post(\"/context\", response_model=MCPContextResponse)\nasync def get_context(request: MCPContextRequest):\n    \"\"\"Get context for a prompt template with parameters\"\"\"\n    try:\n        # Check if prompt template exists\n        if request.prompt_id not in prompt_templates:\n            raise HTTPException(\n                status_code=404,\n                detail=f\"Prompt template '{request.prompt_id}' not found\"\n            )\n        \n        # Get prompt template\n        template = prompt_templates[request.prompt_id]\n        \n        # Use default model if not specified\n        model = request.model or template.get(\"default_model\", \"echo-model\")\n        \n        # Generate context ID\n        context_id = str(uuid.uuid4())\n        \n        # Process template with parameters\n        try:\n            if request.prompt_id == \"echo\":\n                context = f\"Echo: {request.parameters.get('message', '')}\"\n            elif request.prompt_id == \"reverse\":\n                message = request.parameters.get('message', '')\n                context = f\"Reversed: {message[::-1]}\"\n            else:\n                context = template[\"template\"].format(**request.parameters)\n        except KeyError as e:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Missing required parameter: {e}\"\n            )\n        \n        # Calculate token usage (simplified)\n        token_estimate = len(context.split())\n        usage = {\n            \"prompt_tokens\": token_estimate,\n            \"completion_tokens\": 0,\n            \"total_tokens\": token_estimate\n        }\n        \n        return MCPContextResponse(\n            context=context,\n            context_id=context_id,\n            model=model,\n            usage=usage,\n            metadata={\n                \"prompt_id\": request.prompt_id,\n                \"timestamp\": time.time()\n            }\n        )\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Error processing context: {str(e)}\"\n        )\n\n@app.get(\"/prompts\", response_model=MCPPromptLibraryResponse)\nasync def get_prompts():\n    \"\"\"Get available prompt templates\"\"\"\n    prompts = [\n        MCPPromptTemplate(\n            id=prompt_id,\n            template=template[\"template\"],\n            description=template.get(\"description\", \"\"),\n            parameters=template.get(\"parameters\", {}),\n            default_model=template.get(\"default_model\", \"echo-model\"),\n            metadata=template.get(\"metadata\", {})\n        )\n        for prompt_id, template in prompt_templates.items()\n    ]\n    \n    return MCPPromptLibraryResponse(\n        prompts=prompts,\n        count=len(prompts)\n    )\n\n@app.get(\"/prompts/{prompt_id}\", response_model=MCPPromptTemplate)\nasync def get_prompt(prompt_id: str):\n    \"\"\"Get a specific prompt template\"\"\"\n    if prompt_id not in prompt_templates:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Prompt template '{prompt_id}' not found\"\n        )\n    \n    template = prompt_templates[prompt_id]\n    return MCPPromptTemplate(\n        id=prompt_id,\n        template=template[\"template\"],\n        description=template.get(\"description\", \"\"),\n        parameters=template.get(\"parameters\", {}),\n        default_model=template.get(\"default_model\", \"echo-model\"),\n        metadata=template.get(\"metadata\", {})\n    )\n\n# Error handlers\n@app.exception_handler(HTTPException)\nasync def http_exception_handler(request: Request, exc: HTTPException):\n    \"\"\"Handle HTTP exceptions in MCP format\"\"\"\n    return JSONResponse(\n        status_code=exc.status_code,\n        content={\n            \"error\": exc.detail,\n            \"error_type\": \"http_error\",\n            \"status_code\": exc.status_code,\n            \"details\": exc.detail if isinstance(exc.detail, dict) else None\n        }\n    )\n\n@app.exception_handler(Exception)\nasync def general_exception_handler(request: Request, exc: Exception):\n    \"\"\"Handle general exceptions in MCP format\"\"\"\n    return JSONResponse(\n        status_code=500,\n        content={\n            \"error\": str(exc),\n            \"error_type\": \"server_error\",\n            \"status_code\": 500,\n            \"details\": None\n        }\n    )\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n"}
{"type": "source_file", "path": "claude_code/lib/providers/openai.py", "content": "#!/usr/bin/env python3\n# claude_code/lib/providers/openai.py\n\"\"\"OpenAI provider implementation.\"\"\"\n\nimport os\nfrom typing import Dict, List, Generator, Optional, Any, Union\nimport time\nimport logging\nimport json\n\nimport tiktoken\nfrom openai import OpenAI, RateLimitError, APIError\n\nfrom .base import BaseProvider\n\nlogger = logging.getLogger(__name__)\n\n# Model information including context window and pricing\nMODEL_INFO = {\n    \"gpt-3.5-turbo\": {\n        \"context_window\": 16385,\n        \"input_cost_per_1k\": 0.0015,\n        \"output_cost_per_1k\": 0.002,\n        \"capabilities\": [\"function_calling\", \"json_mode\"],\n    },\n    \"gpt-4o\": {\n        \"context_window\": 128000,\n        \"input_cost_per_1k\": 0.005,\n        \"output_cost_per_1k\": 0.015,\n        \"capabilities\": [\"function_calling\", \"json_mode\", \"vision\"],\n    },\n    \"gpt-4-turbo\": {\n        \"context_window\": 128000, \n        \"input_cost_per_1k\": 0.01,\n        \"output_cost_per_1k\": 0.03,\n        \"capabilities\": [\"function_calling\", \"json_mode\", \"vision\"],\n    },\n    \"gpt-4\": {\n        \"context_window\": 8192,\n        \"input_cost_per_1k\": 0.03,\n        \"output_cost_per_1k\": 0.06,\n        \"capabilities\": [\"function_calling\", \"json_mode\"],\n    },\n}\n\nDEFAULT_MODEL = \"gpt-4o\"\n\n\nclass OpenAIProvider(BaseProvider):\n    \"\"\"OpenAI API provider implementation.\"\"\"\n    \n    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):\n        \"\"\"Initialize the OpenAI provider.\n        \n        Args:\n            api_key: OpenAI API key. If None, will use OPENAI_API_KEY environment variable\n            model: Model to use. If None, will use DEFAULT_MODEL\n        \"\"\"\n        self._api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n        if not self._api_key:\n            raise ValueError(\"OpenAI API key is required. Set OPENAI_API_KEY environment variable or pass api_key.\")\n        \n        self._client = OpenAI(api_key=self._api_key)\n        self._model = model or os.environ.get(\"OPENAI_MODEL\", DEFAULT_MODEL)\n        \n        if self._model not in MODEL_INFO:\n            logger.warning(f\"Unknown model: {self._model}. Using {DEFAULT_MODEL} instead.\")\n            self._model = DEFAULT_MODEL\n            \n        # Cache for tokenizers\n        self._tokenizers = {}\n    \n    @property\n    def name(self) -> str:\n        return \"OpenAI\"\n    \n    @property\n    def available_models(self) -> List[str]:\n        return list(MODEL_INFO.keys())\n    \n    @property\n    def current_model(self) -> str:\n        return self._model\n    \n    def set_model(self, model_name: str) -> None:\n        if model_name not in MODEL_INFO:\n            raise ValueError(f\"Unknown model: {model_name}. Available models: {', '.join(self.available_models)}\")\n        self._model = model_name\n    \n    def generate_completion(self, \n                           messages: List[Dict[str, Any]], \n                           tools: Optional[List[Dict[str, Any]]] = None,\n                           temperature: float = 0.0,\n                           stream: bool = True) -> Union[Dict[str, Any], Generator[Dict[str, Any], None, None]]:\n        \"\"\"Generate a completion from OpenAI.\n        \n        Args:\n            messages: List of message dictionaries with 'role' and 'content' keys\n            tools: Optional list of tool dictionaries\n            temperature: Model temperature (0-1)\n            stream: Whether to stream the response\n            \n        Returns:\n            If stream=True, returns a generator of response chunks\n            If stream=False, returns the complete response\n        \"\"\"\n        try:\n            # Convert tools to OpenAI format if provided\n            api_tools = None\n            if tools:\n                api_tools = []\n                for tool in tools:\n                    api_tools.append({\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"name\": tool[\"name\"],\n                            \"description\": tool[\"description\"],\n                            \"parameters\": tool[\"parameters\"]\n                        }\n                    })\n            \n            # Make the API call\n            response = self._client.chat.completions.create(\n                model=self._model,\n                messages=messages,\n                tools=api_tools,\n                temperature=temperature,\n                stream=stream\n            )\n            \n            # Handle streaming and non-streaming responses\n            if stream:\n                return self._process_streaming_response(response)\n            else:\n                return {\n                    \"content\": response.choices[0].message.content,\n                    \"tool_calls\": response.choices[0].message.tool_calls,\n                    \"finish_reason\": response.choices[0].finish_reason,\n                    \"usage\": {\n                        \"prompt_tokens\": response.usage.prompt_tokens,\n                        \"completion_tokens\": response.usage.completion_tokens,\n                        \"total_tokens\": response.usage.total_tokens\n                    }\n                }\n                \n        except RateLimitError as e:\n            logger.error(f\"Rate limit exceeded: {str(e)}\")\n            raise\n        except APIError as e:\n            logger.error(f\"API error: {str(e)}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Error generating completion: {str(e)}\")\n            raise\n    \n    def _process_streaming_response(self, response):\n        \"\"\"Process a streaming response from OpenAI.\"\"\"\n        current_tool_calls = []\n        tool_call_chunks = {}\n        \n        for chunk in response:\n            # Create a result chunk to yield\n            result_chunk = {\n                \"content\": None,\n                \"tool_calls\": None,\n                \"delta\": True\n            }\n            \n            # Process content\n            delta = chunk.choices[0].delta\n            if delta.content:\n                result_chunk[\"content\"] = delta.content\n            \n            # Process tool calls\n            if delta.tool_calls:\n                result_chunk[\"tool_calls\"] = []\n                \n                for tool_call_delta in delta.tool_calls:\n                    # Initialize tool call in chunks dictionary if new\n                    idx = tool_call_delta.index\n                    if idx not in tool_call_chunks:\n                        tool_call_chunks[idx] = {\n                            \"id\": \"\",\n                            \"function\": {\"name\": \"\", \"arguments\": \"\"}\n                        }\n                    \n                    # Update tool call data\n                    if tool_call_delta.id:\n                        tool_call_chunks[idx][\"id\"] = tool_call_delta.id\n                    \n                    if tool_call_delta.function:\n                        if tool_call_delta.function.name:\n                            tool_call_chunks[idx][\"function\"][\"name\"] = tool_call_delta.function.name\n                        \n                        if tool_call_delta.function.arguments:\n                            tool_call_chunks[idx][\"function\"][\"arguments\"] += tool_call_delta.function.arguments\n                    \n                    # Add current state to result\n                    result_chunk[\"tool_calls\"].append(tool_call_chunks[idx])\n            \n            # Yield the chunk\n            yield result_chunk\n        \n        # Final yield with complete tool calls\n        if tool_call_chunks:\n            complete_calls = list(tool_call_chunks.values())\n            yield {\n                \"content\": None,\n                \"tool_calls\": complete_calls,\n                \"delta\": False,\n                \"finish_reason\": \"tool_calls\"\n            }\n    \n    def _get_tokenizer(self, model: str = None) -> Any:\n        \"\"\"Get a tokenizer for the specified model.\"\"\"\n        model = model or self._model\n        \n        if model not in self._tokenizers:\n            try:\n                encoder_name = \"cl100k_base\" if model.startswith(\"gpt-4\") or model.startswith(\"gpt-3.5\") else \"p50k_base\"\n                self._tokenizers[model] = tiktoken.get_encoding(encoder_name)\n            except Exception as e:\n                logger.error(f\"Error loading tokenizer for {model}: {str(e)}\")\n                raise\n        \n        return self._tokenizers[model]\n    \n    def count_tokens(self, text: str) -> int:\n        \"\"\"Count tokens in text.\"\"\"\n        tokenizer = self._get_tokenizer()\n        return len(tokenizer.encode(text))\n    \n    def count_message_tokens(self, messages: List[Dict[str, Any]]) -> Dict[str, int]:\n        \"\"\"Count tokens in a message list.\"\"\"\n        # Simple approximation - in production, would need to match OpenAI's tokenization exactly\n        prompt_tokens = 0\n        \n        for message in messages:\n            # Add tokens for message role\n            prompt_tokens += 4  # ~4 tokens for role\n            \n            # Count content tokens\n            if \"content\" in message and message[\"content\"]:\n                prompt_tokens += self.count_tokens(message[\"content\"])\n            \n            # Count tokens from any tool calls or tool results\n            if \"tool_calls\" in message and message[\"tool_calls\"]:\n                for tool_call in message[\"tool_calls\"]:\n                    prompt_tokens += 4  # ~4 tokens for tool call overhead\n                    prompt_tokens += self.count_tokens(tool_call.get(\"function\", {}).get(\"name\", \"\"))\n                    prompt_tokens += self.count_tokens(tool_call.get(\"function\", {}).get(\"arguments\", \"\"))\n            \n            if \"name\" in message and message[\"name\"]:\n                prompt_tokens += self.count_tokens(message[\"name\"])\n                \n            if \"tool_call_id\" in message and message[\"tool_call_id\"]:\n                prompt_tokens += 10  # ~10 tokens for tool_call_id and overhead\n        \n        # Add ~3 tokens for message formatting\n        prompt_tokens += 3\n        \n        return {\n            \"input\": prompt_tokens,\n            \"output\": 0  # We don't know output tokens yet\n        }\n    \n    def get_model_info(self) -> Dict[str, Any]:\n        \"\"\"Get information about the current model.\"\"\"\n        return MODEL_INFO[self._model]\n    \n    @property\n    def cost_per_1k_tokens(self) -> Dict[str, float]:\n        \"\"\"Get cost per 1K tokens for input and output.\"\"\"\n        info = self.get_model_info()\n        return {\n            \"input\": info[\"input_cost_per_1k\"],\n            \"output\": info[\"output_cost_per_1k\"]\n        }\n    \n    def validate_api_key(self) -> bool:\n        \"\"\"Validate the API key.\"\"\"\n        try:\n            # Make a minimal API call to test the key\n            self._client.models.list(limit=1)\n            return True\n        except Exception as e:\n            logger.error(f\"API key validation failed: {str(e)}\")\n            return False\n    \n    def get_rate_limit_info(self) -> Dict[str, Any]:\n        \"\"\"Get rate limit information.\"\"\"\n        # OpenAI doesn't provide direct rate limit info via API\n        # This is a placeholder implementation\n        return {\n            \"requests_per_minute\": 3500,\n            \"tokens_per_minute\": 90000,\n            \"reset_time\": None\n        }"}
{"type": "source_file", "path": "claude_code/lib/tools/code_tools.py", "content": "#!/usr/bin/env python3\n# claude_code/lib/tools/code_tools.py\n\"\"\"Code analysis and manipulation tools.\"\"\"\n\nimport os\nimport logging\nimport subprocess\nimport tempfile\nimport json\nfrom typing import Dict, List, Optional, Any, Union\nimport ast\nimport re\n\nfrom .base import tool, ToolRegistry\n\nlogger = logging.getLogger(__name__)\n\n\n@tool(\n    name=\"CodeAnalyze\",\n    description=\"Analyze code to extract structure, dependencies, and complexity metrics\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"file_path\": {\n                \"type\": \"string\",\n                \"description\": \"The absolute path to the file to analyze\"\n            },\n            \"analysis_type\": {\n                \"type\": \"string\",\n                \"description\": \"Type of analysis to perform\",\n                \"enum\": [\"structure\", \"complexity\", \"dependencies\", \"all\"],\n                \"default\": \"all\"\n            }\n        },\n        \"required\": [\"file_path\"]\n    },\n    category=\"code\"\n)\ndef analyze_code(file_path: str, analysis_type: str = \"all\") -> str:\n    \"\"\"Analyze code to extract structure and metrics.\n    \n    Args:\n        file_path: Path to the file to analyze\n        analysis_type: Type of analysis to perform\n        \n    Returns:\n        Analysis results as formatted text\n    \"\"\"\n    logger.info(f\"Analyzing code in {file_path} (type: {analysis_type})\")\n    \n    if not os.path.isabs(file_path):\n        return f\"Error: File path must be absolute: {file_path}\"\n    \n    if not os.path.exists(file_path):\n        return f\"Error: File not found: {file_path}\"\n    \n    try:\n        # Read the file\n        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n            code = f.read()\n        \n        # Get file extension\n        _, ext = os.path.splitext(file_path)\n        ext = ext.lower()\n        \n        # Determine language\n        if ext in ['.py']:\n            return _analyze_python(code, analysis_type)\n        elif ext in ['.js', '.jsx', '.ts', '.tsx']:\n            return _analyze_javascript(code, analysis_type)\n        elif ext in ['.java']:\n            return _analyze_java(code, analysis_type)\n        elif ext in ['.c', '.cpp', '.cc', '.h', '.hpp']:\n            return _analyze_cpp(code, analysis_type)\n        else:\n            return _analyze_generic(code, analysis_type)\n    \n    except Exception as e:\n        logger.exception(f\"Error analyzing code: {str(e)}\")\n        return f\"Error analyzing code: {str(e)}\"\n\n\ndef _analyze_python(code: str, analysis_type: str) -> str:\n    \"\"\"Analyze Python code.\"\"\"\n    result = []\n    \n    # Structure analysis\n    if analysis_type in [\"structure\", \"all\"]:\n        try:\n            tree = ast.parse(code)\n            \n            # Extract classes\n            classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]\n            if classes:\n                result.append(\"Classes:\")\n                for cls in classes:\n                    methods = [node.name for node in ast.walk(cls) if isinstance(node, ast.FunctionDef)]\n                    result.append(f\"  - {cls.name}\")\n                    if methods:\n                        result.append(\"    Methods:\")\n                        for method in methods:\n                            result.append(f\"      - {method}\")\n            \n            # Extract functions\n            functions = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef) and \n                         not any(isinstance(parent, ast.ClassDef) for parent in ast.iter_child_nodes(tree))]\n            if functions:\n                result.append(\"\\nFunctions:\")\n                for func in functions:\n                    result.append(f\"  - {func.name}\")\n            \n            # Extract imports\n            imports = []\n            for node in ast.walk(tree):\n                if isinstance(node, ast.Import):\n                    for name in node.names:\n                        imports.append(name.name)\n                elif isinstance(node, ast.ImportFrom):\n                    module = node.module or \"\"\n                    for name in node.names:\n                        imports.append(f\"{module}.{name.name}\")\n            \n            if imports:\n                result.append(\"\\nImports:\")\n                for imp in imports:\n                    result.append(f\"  - {imp}\")\n        \n        except SyntaxError as e:\n            result.append(f\"Error parsing Python code: {str(e)}\")\n    \n    # Complexity analysis\n    if analysis_type in [\"complexity\", \"all\"]:\n        try:\n            # Count lines of code\n            lines = code.count('\\n') + 1\n            non_empty_lines = sum(1 for line in code.split('\\n') if line.strip())\n            comment_lines = sum(1 for line in code.split('\\n') if line.strip().startswith('#'))\n            \n            result.append(\"\\nComplexity Metrics:\")\n            result.append(f\"  - Total lines: {lines}\")\n            result.append(f\"  - Non-empty lines: {non_empty_lines}\")\n            result.append(f\"  - Comment lines: {comment_lines}\")\n            result.append(f\"  - Code lines: {non_empty_lines - comment_lines}\")\n            \n            # Cyclomatic complexity (simplified)\n            tree = ast.parse(code)\n            complexity = 1  # Base complexity\n            for node in ast.walk(tree):\n                if isinstance(node, (ast.If, ast.While, ast.For, ast.comprehension)):\n                    complexity += 1\n                elif isinstance(node, ast.BoolOp) and isinstance(node.op, ast.And):\n                    complexity += len(node.values) - 1\n            \n            result.append(f\"  - Cyclomatic complexity (estimated): {complexity}\")\n        \n        except Exception as e:\n            result.append(f\"Error calculating complexity: {str(e)}\")\n    \n    # Dependencies analysis\n    if analysis_type in [\"dependencies\", \"all\"]:\n        try:\n            # Extract imports\n            tree = ast.parse(code)\n            std_lib_imports = []\n            third_party_imports = []\n            local_imports = []\n            \n            std_lib_modules = [\n                \"abc\", \"argparse\", \"ast\", \"asyncio\", \"base64\", \"collections\", \"concurrent\", \"contextlib\",\n                \"copy\", \"csv\", \"datetime\", \"decimal\", \"enum\", \"functools\", \"glob\", \"gzip\", \"hashlib\",\n                \"http\", \"io\", \"itertools\", \"json\", \"logging\", \"math\", \"multiprocessing\", \"os\", \"pathlib\",\n                \"pickle\", \"random\", \"re\", \"shutil\", \"socket\", \"sqlite3\", \"string\", \"subprocess\", \"sys\",\n                \"tempfile\", \"threading\", \"time\", \"typing\", \"unittest\", \"urllib\", \"uuid\", \"xml\", \"zipfile\"\n            ]\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.Import):\n                    for name in node.names:\n                        module = name.name.split('.')[0]\n                        if module in std_lib_modules:\n                            std_lib_imports.append(name.name)\n                        else:\n                            third_party_imports.append(name.name)\n                \n                elif isinstance(node, ast.ImportFrom):\n                    if node.module:\n                        module = node.module.split('.')[0]\n                        if module in std_lib_modules:\n                            for name in node.names:\n                                std_lib_imports.append(f\"{node.module}.{name.name}\")\n                        elif node.level > 0:  # Relative import\n                            for name in node.names:\n                                local_imports.append(f\"{'.' * node.level}{node.module or ''}.{name.name}\")\n                        else:\n                            for name in node.names:\n                                third_party_imports.append(f\"{node.module}.{name.name}\")\n            \n            result.append(\"\\nDependencies:\")\n            if std_lib_imports:\n                result.append(\"  Standard Library:\")\n                for imp in sorted(set(std_lib_imports)):\n                    result.append(f\"    - {imp}\")\n            \n            if third_party_imports:\n                result.append(\"  Third-Party:\")\n                for imp in sorted(set(third_party_imports)):\n                    result.append(f\"    - {imp}\")\n            \n            if local_imports:\n                result.append(\"  Local/Project:\")\n                for imp in sorted(set(local_imports)):\n                    result.append(f\"    - {imp}\")\n        \n        except Exception as e:\n            result.append(f\"Error analyzing dependencies: {str(e)}\")\n    \n    return \"\\n\".join(result)\n\n\ndef _analyze_javascript(code: str, analysis_type: str) -> str:\n    \"\"\"Analyze JavaScript/TypeScript code.\"\"\"\n    result = []\n    \n    # Structure analysis\n    if analysis_type in [\"structure\", \"all\"]:\n        try:\n            # Extract functions using regex (simplified)\n            function_pattern = r'(function\\s+(\\w+)|const\\s+(\\w+)\\s*=\\s*function|const\\s+(\\w+)\\s*=\\s*\\(.*?\\)\\s*=>)'\n            functions = re.findall(function_pattern, code)\n            \n            if functions:\n                result.append(\"Functions:\")\n                for func in functions:\n                    # Get the first non-empty group which is the function name\n                    func_name = next((name for name in func[1:] if name), \"anonymous\")\n                    result.append(f\"  - {func_name}\")\n            \n            # Extract classes\n            class_pattern = r'class\\s+(\\w+)'\n            classes = re.findall(class_pattern, code)\n            \n            if classes:\n                result.append(\"\\nClasses:\")\n                for cls in classes:\n                    result.append(f\"  - {cls}\")\n            \n            # Extract imports\n            import_pattern = r'import\\s+.*?from\\s+[\\'\"](.+?)[\\'\"]'\n            imports = re.findall(import_pattern, code)\n            \n            if imports:\n                result.append(\"\\nImports:\")\n                for imp in imports:\n                    result.append(f\"  - {imp}\")\n        \n        except Exception as e:\n            result.append(f\"Error parsing JavaScript code: {str(e)}\")\n    \n    # Complexity analysis\n    if analysis_type in [\"complexity\", \"all\"]:\n        try:\n            # Count lines of code\n            lines = code.count('\\n') + 1\n            non_empty_lines = sum(1 for line in code.split('\\n') if line.strip())\n            comment_lines = sum(1 for line in code.split('\\n') \n                               if line.strip().startswith('//') or line.strip().startswith('/*'))\n            \n            result.append(\"\\nComplexity Metrics:\")\n            result.append(f\"  - Total lines: {lines}\")\n            result.append(f\"  - Non-empty lines: {non_empty_lines}\")\n            result.append(f\"  - Comment lines: {comment_lines}\")\n            result.append(f\"  - Code lines: {non_empty_lines - comment_lines}\")\n            \n            # Simplified cyclomatic complexity\n            control_structures = len(re.findall(r'\\b(if|for|while|switch|catch)\\b', code))\n            logical_operators = len(re.findall(r'(&&|\\|\\|)', code))\n            \n            complexity = 1 + control_structures + logical_operators\n            result.append(f\"  - Cyclomatic complexity (estimated): {complexity}\")\n        \n        except Exception as e:\n            result.append(f\"Error calculating complexity: {str(e)}\")\n    \n    # Dependencies analysis\n    if analysis_type in [\"dependencies\", \"all\"]:\n        try:\n            # Extract imports\n            import_pattern = r'import\\s+.*?from\\s+[\\'\"](.+?)[\\'\"]'\n            imports = re.findall(import_pattern, code)\n            \n            node_std_libs = [\n                \"fs\", \"path\", \"http\", \"https\", \"url\", \"querystring\", \"crypto\", \"os\", \n                \"util\", \"stream\", \"events\", \"buffer\", \"assert\", \"zlib\", \"child_process\"\n            ]\n            \n            std_lib_imports = []\n            third_party_imports = []\n            local_imports = []\n            \n            for imp in imports:\n                if imp in node_std_libs:\n                    std_lib_imports.append(imp)\n                elif imp.startswith('.'):\n                    local_imports.append(imp)\n                else:\n                    third_party_imports.append(imp)\n            \n            result.append(\"\\nDependencies:\")\n            if std_lib_imports:\n                result.append(\"  Standard Library:\")\n                for imp in sorted(set(std_lib_imports)):\n                    result.append(f\"    - {imp}\")\n            \n            if third_party_imports:\n                result.append(\"  Third-Party:\")\n                for imp in sorted(set(third_party_imports)):\n                    result.append(f\"    - {imp}\")\n            \n            if local_imports:\n                result.append(\"  Local/Project:\")\n                for imp in sorted(set(local_imports)):\n                    result.append(f\"    - {imp}\")\n        \n        except Exception as e:\n            result.append(f\"Error analyzing dependencies: {str(e)}\")\n    \n    return \"\\n\".join(result)\n\n\ndef _analyze_java(code: str, analysis_type: str) -> str:\n    \"\"\"Analyze Java code.\"\"\"\n    # Simplified Java analysis\n    result = []\n    \n    # Structure analysis\n    if analysis_type in [\"structure\", \"all\"]:\n        try:\n            # Extract class names\n            class_pattern = r'(public|private|protected)?\\s+class\\s+(\\w+)'\n            classes = re.findall(class_pattern, code)\n            \n            if classes:\n                result.append(\"Classes:\")\n                for cls in classes:\n                    result.append(f\"  - {cls[1]}\")\n            \n            # Extract methods\n            method_pattern = r'(public|private|protected)?\\s+\\w+\\s+(\\w+)\\s*\\([^)]*\\)\\s*\\{'\n            methods = re.findall(method_pattern, code)\n            \n            if methods:\n                result.append(\"\\nMethods:\")\n                for method in methods:\n                    result.append(f\"  - {method[1]}\")\n            \n            # Extract imports\n            import_pattern = r'import\\s+(.+?);'\n            imports = re.findall(import_pattern, code)\n            \n            if imports:\n                result.append(\"\\nImports:\")\n                for imp in imports:\n                    result.append(f\"  - {imp}\")\n        \n        except Exception as e:\n            result.append(f\"Error parsing Java code: {str(e)}\")\n    \n    # Complexity analysis\n    if analysis_type in [\"complexity\", \"all\"]:\n        try:\n            # Count lines of code\n            lines = code.count('\\n') + 1\n            non_empty_lines = sum(1 for line in code.split('\\n') if line.strip())\n            comment_lines = sum(1 for line in code.split('\\n') \n                               if line.strip().startswith('//') or line.strip().startswith('/*'))\n            \n            result.append(\"\\nComplexity Metrics:\")\n            result.append(f\"  - Total lines: {lines}\")\n            result.append(f\"  - Non-empty lines: {non_empty_lines}\")\n            result.append(f\"  - Comment lines: {comment_lines}\")\n            result.append(f\"  - Code lines: {non_empty_lines - comment_lines}\")\n            \n            # Simplified cyclomatic complexity\n            control_structures = len(re.findall(r'\\b(if|for|while|switch|catch)\\b', code))\n            logical_operators = len(re.findall(r'(&&|\\|\\|)', code))\n            \n            complexity = 1 + control_structures + logical_operators\n            result.append(f\"  - Cyclomatic complexity (estimated): {complexity}\")\n        \n        except Exception as e:\n            result.append(f\"Error calculating complexity: {str(e)}\")\n    \n    return \"\\n\".join(result)\n\n\ndef _analyze_cpp(code: str, analysis_type: str) -> str:\n    \"\"\"Analyze C/C++ code.\"\"\"\n    # Simplified C/C++ analysis\n    result = []\n    \n    # Structure analysis\n    if analysis_type in [\"structure\", \"all\"]:\n        try:\n            # Extract class names\n            class_pattern = r'class\\s+(\\w+)'\n            classes = re.findall(class_pattern, code)\n            \n            if classes:\n                result.append(\"Classes:\")\n                for cls in classes:\n                    result.append(f\"  - {cls}\")\n            \n            # Extract functions\n            function_pattern = r'(\\w+)\\s+(\\w+)\\s*\\([^)]*\\)\\s*\\{'\n            functions = re.findall(function_pattern, code)\n            \n            if functions:\n                result.append(\"\\nFunctions:\")\n                for func in functions:\n                    # Filter out keywords that might be matched\n                    if func[1] not in ['if', 'for', 'while', 'switch']:\n                        result.append(f\"  - {func[1]} (return type: {func[0]})\")\n            \n            # Extract includes\n            include_pattern = r'#include\\s+[<\"](.+?)[>\"]'\n            includes = re.findall(include_pattern, code)\n            \n            if includes:\n                result.append(\"\\nIncludes:\")\n                for inc in includes:\n                    result.append(f\"  - {inc}\")\n        \n        except Exception as e:\n            result.append(f\"Error parsing C/C++ code: {str(e)}\")\n    \n    # Complexity analysis\n    if analysis_type in [\"complexity\", \"all\"]:\n        try:\n            # Count lines of code\n            lines = code.count('\\n') + 1\n            non_empty_lines = sum(1 for line in code.split('\\n') if line.strip())\n            comment_lines = sum(1 for line in code.split('\\n') \n                               if line.strip().startswith('//') or line.strip().startswith('/*'))\n            \n            result.append(\"\\nComplexity Metrics:\")\n            result.append(f\"  - Total lines: {lines}\")\n            result.append(f\"  - Non-empty lines: {non_empty_lines}\")\n            result.append(f\"  - Comment lines: {comment_lines}\")\n            result.append(f\"  - Code lines: {non_empty_lines - comment_lines}\")\n            \n            # Simplified cyclomatic complexity\n            control_structures = len(re.findall(r'\\b(if|for|while|switch|catch)\\b', code))\n            logical_operators = len(re.findall(r'(&&|\\|\\|)', code))\n            \n            complexity = 1 + control_structures + logical_operators\n            result.append(f\"  - Cyclomatic complexity (estimated): {complexity}\")\n        \n        except Exception as e:\n            result.append(f\"Error calculating complexity: {str(e)}\")\n    \n    return \"\\n\".join(result)\n\n\ndef _analyze_generic(code: str, analysis_type: str) -> str:\n    \"\"\"Generic code analysis for unsupported languages.\"\"\"\n    result = []\n    \n    # Basic analysis for any language\n    try:\n        # Count lines of code\n        lines = code.count('\\n') + 1\n        non_empty_lines = sum(1 for line in code.split('\\n') if line.strip())\n        \n        result.append(\"Basic Code Metrics:\")\n        result.append(f\"  - Total lines: {lines}\")\n        result.append(f\"  - Non-empty lines: {non_empty_lines}\")\n        \n        # Try to identify language\n        language = \"unknown\"\n        if \"def \" in code and \"import \" in code:\n            language = \"Python\"\n        elif \"function \" in code or \"const \" in code or \"let \" in code:\n            language = \"JavaScript\"\n        elif \"public class \" in code or \"private class \" in code:\n            language = \"Java\"\n        elif \"#include\" in code and \"{\" in code:\n            language = \"C/C++\"\n        \n        result.append(f\"  - Detected language: {language}\")\n        \n        # Find potential functions/methods using a generic pattern\n        function_pattern = r'\\b(\\w+)\\s*\\([^)]*\\)\\s*\\{'\n        functions = re.findall(function_pattern, code)\n        \n        if functions:\n            result.append(\"\\nPotential Functions/Methods:\")\n            for func in functions:\n                # Filter out common keywords\n                if func not in ['if', 'for', 'while', 'switch', 'catch']:\n                    result.append(f\"  - {func}\")\n    \n    except Exception as e:\n        result.append(f\"Error analyzing code: {str(e)}\")\n    \n    return \"\\n\".join(result)\n\n\n@tool(\n    name=\"LintCode\",\n    description=\"Lint code to find potential issues and style violations\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"file_path\": {\n                \"type\": \"string\",\n                \"description\": \"The absolute path to the file to lint\"\n            },\n            \"linter\": {\n                \"type\": \"string\",\n                \"description\": \"Linter to use (auto, pylint, eslint, etc.)\",\n                \"default\": \"auto\"\n            }\n        },\n        \"required\": [\"file_path\"]\n    },\n    category=\"code\"\n)\ndef lint_code(file_path: str, linter: str = \"auto\") -> str:\n    \"\"\"Lint code to find potential issues.\n    \n    Args:\n        file_path: Path to the file to lint\n        linter: Linter to use\n        \n    Returns:\n        Linting results as formatted text\n    \"\"\"\n    logger.info(f\"Linting code in {file_path} using {linter}\")\n    \n    if not os.path.isabs(file_path):\n        return f\"Error: File path must be absolute: {file_path}\"\n    \n    if not os.path.exists(file_path):\n        return f\"Error: File not found: {file_path}\"\n    \n    try:\n        # Get file extension\n        _, ext = os.path.splitext(file_path)\n        ext = ext.lower()\n        \n        # Auto-detect linter if not specified\n        if linter == \"auto\":\n            if ext in ['.py']:\n                linter = \"pylint\"\n            elif ext in ['.js', '.jsx']:\n                linter = \"eslint\"\n            elif ext in ['.ts', '.tsx']:\n                linter = \"tslint\"\n            elif ext in ['.java']:\n                linter = \"checkstyle\"\n            elif ext in ['.c', '.cpp', '.cc', '.h', '.hpp']:\n                linter = \"cppcheck\"\n            else:\n                return f\"Error: Could not auto-detect linter for file type {ext}\"\n        \n        # Run appropriate linter\n        if linter == \"pylint\":\n            return _run_pylint(file_path)\n        elif linter == \"eslint\":\n            return _run_eslint(file_path)\n        elif linter == \"tslint\":\n            return _run_tslint(file_path)\n        elif linter == \"checkstyle\":\n            return _run_checkstyle(file_path)\n        elif linter == \"cppcheck\":\n            return _run_cppcheck(file_path)\n        else:\n            return f\"Error: Unsupported linter: {linter}\"\n    \n    except Exception as e:\n        logger.exception(f\"Error linting code: {str(e)}\")\n        return f\"Error linting code: {str(e)}\"\n\n\ndef _run_pylint(file_path: str) -> str:\n    \"\"\"Run pylint on a Python file.\"\"\"\n    try:\n        # Check if pylint is installed\n        try:\n            subprocess.run([\"pylint\", \"--version\"], capture_output=True, check=True)\n        except (subprocess.SubprocessError, FileNotFoundError):\n            return \"Error: pylint is not installed. Please install it with 'pip install pylint'.\"\n        \n        # Run pylint\n        result = subprocess.run(\n            [\"pylint\", \"--output-format=text\", file_path],\n            capture_output=True,\n            text=True\n        )\n        \n        if result.returncode == 0:\n            return \"No issues found.\"\n        \n        # Format output\n        output = result.stdout or result.stderr\n        \n        # Summarize output\n        lines = output.split('\\n')\n        summary_lines = [line for line in lines if \"rated at\" in line]\n        issue_lines = [line for line in lines if re.match(r'^.*?:\\d+:\\d+:', line)]\n        \n        formatted_output = []\n        \n        if issue_lines:\n            formatted_output.append(\"Issues found:\")\n            for line in issue_lines:\n                formatted_output.append(f\"  {line}\")\n        \n        if summary_lines:\n            formatted_output.append(\"\\nSummary:\")\n            for line in summary_lines:\n                formatted_output.append(f\"  {line}\")\n        \n        return \"\\n\".join(formatted_output)\n    \n    except Exception as e:\n        return f\"Error running pylint: {str(e)}\"\n\n\ndef _run_eslint(file_path: str) -> str:\n    \"\"\"Run eslint on a JavaScript file.\"\"\"\n    try:\n        # Check if eslint is installed\n        try:\n            subprocess.run([\"eslint\", \"--version\"], capture_output=True, check=True)\n        except (subprocess.SubprocessError, FileNotFoundError):\n            return \"Error: eslint is not installed. Please install it with 'npm install -g eslint'.\"\n        \n        # Run eslint\n        result = subprocess.run(\n            [\"eslint\", \"--format=stylish\", file_path],\n            capture_output=True,\n            text=True\n        )\n        \n        if result.returncode == 0:\n            return \"No issues found.\"\n        \n        # Format output\n        output = result.stdout or result.stderr\n        \n        # Clean up output\n        lines = output.split('\\n')\n        filtered_lines = [line for line in lines if line.strip() and not line.startswith(\"eslint:\")]\n        \n        return \"\\n\".join(filtered_lines)\n    \n    except Exception as e:\n        return f\"Error running eslint: {str(e)}\"\n\n\ndef _run_tslint(file_path: str) -> str:\n    \"\"\"Run tslint on a TypeScript file.\"\"\"\n    try:\n        # Check if tslint is installed\n        try:\n            subprocess.run([\"tslint\", \"--version\"], capture_output=True, check=True)\n        except (subprocess.SubprocessError, FileNotFoundError):\n            return \"Error: tslint is not installed. Please install it with 'npm install -g tslint'.\"\n        \n        # Run tslint\n        result = subprocess.run(\n            [\"tslint\", \"-t\", \"verbose\", file_path],\n            capture_output=True,\n            text=True\n        )\n        \n        if result.returncode == 0:\n            return \"No issues found.\"\n        \n        # Format output\n        output = result.stdout or result.stderr\n        \n        return output\n    \n    except Exception as e:\n        return f\"Error running tslint: {str(e)}\"\n\n\ndef _run_checkstyle(file_path: str) -> str:\n    \"\"\"Run checkstyle on a Java file.\"\"\"\n    return \"Checkstyle support not implemented. Please install and run checkstyle manually.\"\n\n\ndef _run_cppcheck(file_path: str) -> str:\n    \"\"\"Run cppcheck on a C/C++ file.\"\"\"\n    try:\n        # Check if cppcheck is installed\n        try:\n            subprocess.run([\"cppcheck\", \"--version\"], capture_output=True, check=True)\n        except (subprocess.SubprocessError, FileNotFoundError):\n            return \"Error: cppcheck is not installed. Please install it using your system package manager.\"\n        \n        # Run cppcheck\n        result = subprocess.run(\n            [\"cppcheck\", \"--enable=all\", \"--template='{file}:{line}: {severity}: {message}'\", file_path],\n            capture_output=True,\n            text=True\n        )\n        \n        # Format output\n        output = result.stderr  # cppcheck outputs to stderr\n        \n        if not output or \"no errors found\" in output.lower():\n            return \"No issues found.\"\n        \n        # Clean up output\n        lines = output.split('\\n')\n        filtered_lines = [line for line in lines if line.strip() and \"Checking\" not in line]\n        \n        return \"\\n\".join(filtered_lines)\n    \n    except Exception as e:\n        return f\"Error running cppcheck: {str(e)}\"\n\n\ndef register_code_tools(registry: ToolRegistry) -> None:\n    \"\"\"Register all code analysis tools with the registry.\n    \n    Args:\n        registry: Tool registry to register with\n    \"\"\"\n    from .base import create_tools_from_functions\n    \n    code_tools = [\n        analyze_code,\n        lint_code\n    ]\n    \n    create_tools_from_functions(registry, code_tools)\n"}
{"type": "source_file", "path": "claude_code/lib/rl/grpo.py", "content": "\"\"\"\nGroup Relative Policy Optimization (GRPO) for multi-agent learning in Claude Code.\nThis module provides a multi-agent GRPO implementation that learns from interactions.\n\"\"\"\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\nfrom typing import List, Dict, Tuple, Optional, Any, Union, Callable\nfrom dataclasses import dataclass\nfrom collections import deque\nimport random\nimport time\n\n\n@dataclass\nclass Experience:\n    \"\"\"A single step of experience for reinforcement learning.\"\"\"\n    state: Any\n    action: Any\n    reward: float\n    next_state: Any\n    done: bool\n    info: Optional[Dict[str, Any]] = None\n\n\nclass ExperienceBuffer:\n    \"\"\"Buffer to store and sample experiences for training.\"\"\"\n    \n    def __init__(self, capacity: int = 100000):\n        \"\"\"\n        Initialize the experience buffer.\n        \n        Args:\n            capacity: Maximum number of experiences to store\n        \"\"\"\n        self.buffer = deque(maxlen=capacity)\n    \n    def add(self, experience: Experience) -> None:\n        \"\"\"Add an experience to the buffer.\"\"\"\n        self.buffer.append(experience)\n    \n    def sample(self, batch_size: int) -> List[Experience]:\n        \"\"\"Sample a batch of experiences from the buffer.\"\"\"\n        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n    \n    def __len__(self) -> int:\n        \"\"\"Get the current size of the buffer.\"\"\"\n        return len(self.buffer)\n\n\nclass PolicyNetwork(nn.Module):\n    \"\"\"Neural network to represent a policy.\"\"\"\n    \n    def __init__(self, input_dim: int, hidden_dims: List[int], output_dim: int):\n        \"\"\"\n        Initialize the policy network.\n        \n        Args:\n            input_dim: Dimension of the input state\n            hidden_dims: List of hidden layer dimensions\n            output_dim: Dimension of the action space\n        \"\"\"\n        super(PolicyNetwork, self).__init__()\n        \n        # Create the input layer\n        layers = [nn.Linear(input_dim, hidden_dims[0]), nn.ReLU()]\n        \n        # Create hidden layers\n        for i in range(len(hidden_dims) - 1):\n            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n            layers.append(nn.ReLU())\n        \n        # Create output layer\n        layers.append(nn.Linear(hidden_dims[-1], output_dim))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward pass through the network.\"\"\"\n        return self.network(x)\n\n\nclass ValueNetwork(nn.Module):\n    \"\"\"Neural network to represent a value function.\"\"\"\n    \n    def __init__(self, input_dim: int, hidden_dims: List[int]):\n        \"\"\"\n        Initialize the value network.\n        \n        Args:\n            input_dim: Dimension of the input state\n            hidden_dims: List of hidden layer dimensions\n        \"\"\"\n        super(ValueNetwork, self).__init__()\n        \n        # Create the input layer\n        layers = [nn.Linear(input_dim, hidden_dims[0]), nn.ReLU()]\n        \n        # Create hidden layers\n        for i in range(len(hidden_dims) - 1):\n            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n            layers.append(nn.ReLU())\n        \n        # Create output layer (scalar value)\n        layers.append(nn.Linear(hidden_dims[-1], 1))\n        \n        self.network = nn.Sequential(*layers)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward pass through the network.\"\"\"\n        return self.network(x)\n\n\nclass GRPO:\n    \"\"\"\n    Group Relative Policy Optimization implementation for multi-agent learning.\n    GRPO extends PPO by considering relative performance within a group of agents.\n    \"\"\"\n    \n    def __init__(\n        self,\n        state_dim: int,\n        action_dim: int,\n        hidden_dims: List[int] = [64, 64],\n        lr_policy: float = 3e-4,\n        lr_value: float = 1e-3,\n        gamma: float = 0.99,\n        gae_lambda: float = 0.95,\n        clip_ratio: float = 0.2,\n        target_kl: float = 0.01,\n        value_coef: float = 0.5,\n        entropy_coef: float = 0.01,\n        max_grad_norm: float = 0.5,\n        use_gae: bool = True,\n        normalize_advantages: bool = True,\n        relative_advantage_weight: float = 0.5,\n        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    ):\n        \"\"\"\n        Initialize the GRPO agent.\n        \n        Args:\n            state_dim: Dimension of the state space\n            action_dim: Dimension of the action space\n            hidden_dims: Dimensions of hidden layers in networks\n            lr_policy: Learning rate for policy network\n            lr_value: Learning rate for value network\n            gamma: Discount factor\n            gae_lambda: Lambda for GAE\n            clip_ratio: PPO clipping parameter\n            target_kl: Target KL divergence for early stopping\n            value_coef: Value loss coefficient\n            entropy_coef: Entropy bonus coefficient\n            max_grad_norm: Maximum gradient norm for clipping\n            use_gae: Whether to use GAE\n            normalize_advantages: Whether to normalize advantages\n            relative_advantage_weight: Weight for relative advantage component\n            device: Device to run the model on\n        \"\"\"\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.gamma = gamma\n        self.gae_lambda = gae_lambda\n        self.clip_ratio = clip_ratio\n        self.target_kl = target_kl\n        self.value_coef = value_coef\n        self.entropy_coef = entropy_coef\n        self.max_grad_norm = max_grad_norm\n        self.use_gae = use_gae\n        self.normalize_advantages = normalize_advantages\n        self.relative_advantage_weight = relative_advantage_weight\n        self.device = device\n        \n        # Initialize networks\n        self.policy = PolicyNetwork(state_dim, hidden_dims, action_dim).to(device)\n        self.value = ValueNetwork(state_dim, hidden_dims).to(device)\n        \n        # Initialize optimizers\n        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr_policy)\n        self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr_value)\n        \n        # Initialize experience buffer\n        self.buffer = ExperienceBuffer()\n        \n        # Group-level buffers for relative advantage computation\n        self.group_rewards = []\n        self.agent_id = None  # Will be set when joining a group\n    \n    def set_agent_id(self, agent_id: str) -> None:\n        \"\"\"Set the agent's ID within the group.\"\"\"\n        self.agent_id = agent_id\n    \n    def get_action(self, state: np.ndarray, deterministic: bool = False) -> Tuple[int, float]:\n        \"\"\"\n        Get an action from the policy for the given state.\n        \n        Args:\n            state: The current state\n            deterministic: Whether to return the most likely action\n            \n        Returns:\n            Tuple of (action, log probability)\n        \"\"\"\n        # Convert state to tensor\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        \n        # Get action distributions\n        with torch.no_grad():\n            logits = self.policy(state_tensor)\n            distribution = Categorical(logits=logits)\n            \n            if deterministic:\n                action = torch.argmax(logits, dim=1).item()\n            else:\n                action = distribution.sample().item()\n                \n            log_prob = distribution.log_prob(torch.tensor(action)).item()\n        \n        return action, log_prob\n    \n    def get_value(self, state: np.ndarray) -> float:\n        \"\"\"\n        Get the estimated value of a state.\n        \n        Args:\n            state: The state to evaluate\n            \n        Returns:\n            The estimated value\n        \"\"\"\n        # Convert state to tensor\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        \n        # Get value estimate\n        with torch.no_grad():\n            value = self.value(state_tensor).item()\n        \n        return value\n    \n    def learn(\n        self,\n        batch_size: int = 64,\n        epochs: int = 10,\n        group_rewards: Optional[Dict[str, List[float]]] = None\n    ) -> Dict[str, float]:\n        \"\"\"\n        Update policy and value networks based on collected experience.\n        \n        Args:\n            batch_size: Size of batches to use for updates\n            epochs: Number of epochs to train for\n            group_rewards: Rewards collected by all agents in the group\n            \n        Returns:\n            Dictionary of training metrics\n        \"\"\"\n        if len(self.buffer) < batch_size:\n            return {\"policy_loss\": 0, \"value_loss\": 0, \"kl\": 0}\n        \n        # Prepare data for training\n        states, actions, old_log_probs, returns, advantages = self._prepare_training_data(\n            group_rewards)\n        \n        # Training metrics\n        metrics = {\n            \"policy_loss\": 0,\n            \"value_loss\": 0,\n            \"entropy\": 0,\n            \"kl\": 0,\n        }\n        \n        # Run training for multiple epochs\n        for epoch in range(epochs):\n            # Generate random indices for batching\n            indices = np.random.permutation(len(states))\n            \n            # Process in batches\n            for start_idx in range(0, len(states), batch_size):\n                # Get batch indices\n                batch_indices = indices[start_idx:start_idx + batch_size]\n                \n                # Extract batch data\n                batch_states = states[batch_indices]\n                batch_actions = actions[batch_indices]\n                batch_old_log_probs = old_log_probs[batch_indices]\n                batch_returns = returns[batch_indices]\n                batch_advantages = advantages[batch_indices]\n                \n                # Update policy\n                policy_loss, entropy, kl = self._update_policy(\n                    batch_states, batch_actions, batch_old_log_probs, batch_advantages)\n                \n                # Early stopping based on KL divergence\n                if kl > 1.5 * self.target_kl:\n                    break\n                \n                # Update value function\n                value_loss = self._update_value(batch_states, batch_returns)\n                \n                # Update metrics\n                metrics[\"policy_loss\"] += policy_loss\n                metrics[\"value_loss\"] += value_loss\n                metrics[\"entropy\"] += entropy\n                metrics[\"kl\"] += kl\n            \n            # Check for early stopping after each epoch\n            if metrics[\"kl\"] / (epoch + 1) > self.target_kl:\n                break\n        \n        # Normalize metrics by number of updates\n        num_updates = epochs * ((len(states) + batch_size - 1) // batch_size)\n        for key in metrics:\n            metrics[key] /= num_updates\n        \n        # Clear buffer after training\n        self.buffer = ExperienceBuffer()\n        \n        return metrics\n    \n    def _prepare_training_data(\n        self, group_rewards: Optional[Dict[str, List[float]]] = None\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Prepare data for training from the experience buffer.\n        \n        Args:\n            group_rewards: Rewards collected by all agents in the group\n            \n        Returns:\n            Tuple of (states, actions, old_log_probs, returns, advantages)\n        \"\"\"\n        # Collect experiences from buffer\n        experiences = list(self.buffer.buffer)\n        \n        # Extract components\n        states = torch.FloatTensor([exp.state for exp in experiences]).to(self.device)\n        actions = torch.LongTensor([exp.action for exp in experiences]).to(self.device)\n        rewards = torch.FloatTensor([exp.reward for exp in experiences]).to(self.device)\n        next_states = torch.FloatTensor([exp.next_state for exp in experiences]).to(self.device)\n        dones = torch.FloatTensor([float(exp.done) for exp in experiences]).to(self.device)\n        \n        # Compute values for all states and next states\n        with torch.no_grad():\n            values = self.value(states).squeeze()\n            next_values = self.value(next_states).squeeze()\n        \n        # Compute advantages and returns\n        if self.use_gae:\n            # Generalized Advantage Estimation\n            advantages = self._compute_gae(rewards, values, next_values, dones)\n        else:\n            # Regular advantages\n            advantages = rewards + self.gamma * next_values * (1 - dones) - values\n        \n        # Compute returns (for value function)\n        returns = advantages + values\n        \n        # If group rewards are provided, compute relative advantages\n        if group_rewards is not None and self.agent_id in group_rewards:\n            relative_advantages = self._compute_relative_advantages(\n                advantages, group_rewards)\n            \n            # Combine regular and relative advantages\n            advantages = (1 - self.relative_advantage_weight) * advantages + \\\n                         self.relative_advantage_weight * relative_advantages\n        \n        # Normalize advantages if enabled\n        if self.normalize_advantages:\n            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n        \n        # Get old log probabilities\n        old_log_probs = torch.FloatTensor(\n            [self._compute_log_prob(exp.state, exp.action) for exp in experiences]\n        ).to(self.device)\n        \n        return states, actions, old_log_probs, returns, advantages\n    \n    def _compute_gae(\n        self, rewards: torch.Tensor, values: torch.Tensor, \n        next_values: torch.Tensor, dones: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Compute advantages using Generalized Advantage Estimation.\n        \n        Args:\n            rewards: Batch of rewards\n            values: Batch of state values\n            next_values: Batch of next state values\n            dones: Batch of done flags\n            \n        Returns:\n            Batch of advantage estimates\n        \"\"\"\n        # Initialize advantages\n        advantages = torch.zeros_like(rewards)\n        \n        # Initialize gae\n        gae = 0\n        \n        # Compute advantages in reverse order\n        for t in reversed(range(len(rewards))):\n            # Compute TD error\n            delta = rewards[t] + self.gamma * next_values[t] * (1 - dones[t]) - values[t]\n            \n            # Update gae\n            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n            \n            # Store advantage\n            advantages[t] = gae\n        \n        return advantages\n    \n    def _compute_relative_advantages(\n        self, advantages: torch.Tensor, group_rewards: Dict[str, List[float]]\n    ) -> torch.Tensor:\n        \"\"\"\n        Compute relative advantages compared to other agents in the group.\n        \n        Args:\n            advantages: This agent's advantages\n            group_rewards: Rewards collected by all agents in the group\n            \n        Returns:\n            Relative advantages\n        \"\"\"\n        # Compute mean reward for each agent\n        agent_mean_rewards = {\n            agent_id: sum(rewards) / max(1, len(rewards))\n            for agent_id, rewards in group_rewards.items()\n        }\n        \n        # Compute mean reward across all agents\n        group_mean_reward = sum(agent_mean_rewards.values()) / len(agent_mean_rewards)\n        \n        # Compute relative performance factor\n        # Higher if this agent is doing better than the group average\n        if self.agent_id in agent_mean_rewards:\n            relative_factor = agent_mean_rewards[self.agent_id] / (group_mean_reward + 1e-8)\n        else:\n            relative_factor = 1.0\n        \n        # Apply the relative factor to the advantages\n        relative_advantages = advantages * relative_factor\n        \n        return relative_advantages\n    \n    def _compute_log_prob(self, state: np.ndarray, action: int) -> float:\n        \"\"\"\n        Compute the log probability of an action given a state.\n        \n        Args:\n            state: The state\n            action: The action\n            \n        Returns:\n            The log probability\n        \"\"\"\n        # Convert state to tensor\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        \n        # Get action distribution\n        with torch.no_grad():\n            logits = self.policy(state_tensor)\n            distribution = Categorical(logits=logits)\n            log_prob = distribution.log_prob(torch.tensor(action, device=self.device)).item()\n        \n        return log_prob\n    \n    def _update_policy(\n        self, \n        states: torch.Tensor, \n        actions: torch.Tensor, \n        old_log_probs: torch.Tensor, \n        advantages: torch.Tensor\n    ) -> Tuple[float, float, float]:\n        \"\"\"\n        Update the policy network using PPO.\n        \n        Args:\n            states: Batch of states\n            actions: Batch of actions\n            old_log_probs: Batch of old log probabilities\n            advantages: Batch of advantages\n            \n        Returns:\n            Tuple of (policy_loss, entropy, kl_divergence)\n        \"\"\"\n        # Get action distributions\n        logits = self.policy(states)\n        distribution = Categorical(logits=logits)\n        \n        # Get new log probabilities\n        new_log_probs = distribution.log_prob(actions)\n        \n        # Compute probability ratio\n        ratio = torch.exp(new_log_probs - old_log_probs)\n        \n        # Compute surrogate objectives\n        surrogate1 = ratio * advantages\n        surrogate2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n        \n        # Compute policy loss (negative because we're maximizing)\n        policy_loss = -torch.min(surrogate1, surrogate2).mean()\n        \n        # Compute entropy bonus\n        entropy = distribution.entropy().mean()\n        \n        # Add entropy bonus to loss\n        loss = policy_loss - self.entropy_coef * entropy\n        \n        # Compute approximate KL divergence for monitoring\n        with torch.no_grad():\n            kl = (old_log_probs - new_log_probs).mean().item()\n        \n        # Update policy network\n        self.policy_optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n        self.policy_optimizer.step()\n        \n        return policy_loss.item(), entropy.item(), kl\n    \n    def _update_value(self, states: torch.Tensor, returns: torch.Tensor) -> float:\n        \"\"\"\n        Update the value network.\n        \n        Args:\n            states: Batch of states\n            returns: Batch of returns\n            \n        Returns:\n            Value loss\n        \"\"\"\n        # Get value predictions\n        values = self.value(states).squeeze()\n        \n        # Compute value loss\n        value_loss = F.mse_loss(values, returns)\n        \n        # Update value network\n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        nn.utils.clip_grad_norm_(self.value.parameters(), self.max_grad_norm)\n        self.value_optimizer.step()\n        \n        return value_loss.item()\n\n\nclass MultiAgentGroupRL:\n    \"\"\"\n    Multi-agent reinforcement learning system using GRPO for Claude Code.\n    This class manages multiple GRPO agents that learn in a coordinated way.\n    \"\"\"\n    \n    def __init__(\n        self,\n        agent_configs: List[Dict[str, Any]],\n        feature_extractor: Callable[[Dict[str, Any]], np.ndarray],\n        reward_function: Callable[[Dict[str, Any], str, Any], float],\n        update_interval: int = 1000,\n        training_epochs: int = 10,\n        batch_size: int = 64,\n        save_dir: str = \"./models\",\n        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    ):\n        \"\"\"\n        Initialize the multi-agent RL system.\n        \n        Args:\n            agent_configs: List of configurations for each agent\n            feature_extractor: Function to extract state features\n            reward_function: Function to compute rewards\n            update_interval: How often to update agents (in steps)\n            training_epochs: Number of epochs to train for each update\n            batch_size: Batch size for training\n            save_dir: Directory to save models\n            device: Device to run on\n        \"\"\"\n        self.feature_extractor = feature_extractor\n        self.reward_function = reward_function\n        self.update_interval = update_interval\n        self.training_epochs = training_epochs\n        self.batch_size = batch_size\n        self.save_dir = save_dir\n        self.device = device\n        \n        # Initialize agents\n        self.agents = {}\n        for config in agent_configs:\n            agent_id = config[\"id\"]\n            state_dim = config[\"state_dim\"]\n            action_dim = config[\"action_dim\"]\n            \n            # Create GRPO agent\n            agent = GRPO(\n                state_dim=state_dim,\n                action_dim=action_dim,\n                hidden_dims=config.get(\"hidden_dims\", [64, 64]),\n                device=device,\n                **{k: v for k, v in config.items() if k not in [\"id\", \"state_dim\", \"action_dim\", \"hidden_dims\"]}\n            )\n            \n            # Set agent ID\n            agent.set_agent_id(agent_id)\n            \n            self.agents[agent_id] = agent\n        \n        # Track steps for periodic updates\n        self.total_steps = 0\n        \n        # Store rewards for relative advantage computation\n        self.agent_rewards = {agent_id: [] for agent_id in self.agents}\n    \n    def select_action(\n        self, agent_id: str, observation: Dict[str, Any], deterministic: bool = False\n    ) -> Tuple[Any, float]:\n        \"\"\"\n        Select an action for the specified agent.\n        \n        Args:\n            agent_id: ID of the agent\n            observation: Current observation\n            deterministic: Whether to select deterministically\n            \n        Returns:\n            Tuple of (action, log probability)\n        \"\"\"\n        if agent_id not in self.agents:\n            raise ValueError(f\"Unknown agent ID: {agent_id}\")\n        \n        # Extract features\n        state = self.feature_extractor(observation)\n        \n        # Get action from agent\n        action, log_prob = self.agents[agent_id].get_action(state, deterministic)\n        \n        return action, log_prob\n    \n    def observe(\n        self, \n        agent_id: str, \n        observation: Dict[str, Any],\n        action: Any,\n        reward: float,\n        next_observation: Dict[str, Any],\n        done: bool,\n        info: Optional[Dict[str, Any]] = None\n    ) -> None:\n        \"\"\"\n        Record an observation for the specified agent.\n        \n        Args:\n            agent_id: ID of the agent\n            observation: Current observation\n            action: Action taken\n            reward: Reward received\n            next_observation: Next observation\n            done: Whether the episode is done\n            info: Additional information\n        \"\"\"\n        if agent_id not in self.agents:\n            raise ValueError(f\"Unknown agent ID: {agent_id}\")\n        \n        # Extract features\n        state = self.feature_extractor(observation)\n        next_state = self.feature_extractor(next_observation)\n        \n        # Create experience\n        exp = Experience(\n            state=state,\n            action=action,\n            reward=reward,\n            next_state=next_state,\n            done=done,\n            info=info\n        )\n        \n        # Add experience to agent's buffer\n        self.agents[agent_id].buffer.add(exp)\n        \n        # Store reward for relative advantage computation\n        self.agent_rewards[agent_id].append(reward)\n        \n        # Increment step counter\n        self.total_steps += 1\n        \n        # Perform updates if needed\n        if self.total_steps % self.update_interval == 0:\n            self.update_all_agents()\n    \n    def update_all_agents(self) -> Dict[str, Dict[str, float]]:\n        \"\"\"\n        Update all agents' policies.\n        \n        Returns:\n            Dictionary of training metrics for each agent\n        \"\"\"\n        # Store metrics for each agent\n        metrics = {}\n        \n        # Update each agent\n        for agent_id, agent in self.agents.items():\n            # Train the agent with group rewards\n            agent_metrics = agent.learn(\n                batch_size=self.batch_size,\n                epochs=self.training_epochs,\n                group_rewards=self.agent_rewards\n            )\n            \n            metrics[agent_id] = agent_metrics\n        \n        # Reset reward tracking\n        self.agent_rewards = {agent_id: [] for agent_id in self.agents}\n        \n        return metrics\n    \n    def save_agents(self, suffix: str = \"\") -> None:\n        \"\"\"\n        Save all agents' models.\n        \n        Args:\n            suffix: Optional suffix for saved files\n        \"\"\"\n        import os\n        \n        # Create save directory if it doesn't exist\n        os.makedirs(self.save_dir, exist_ok=True)\n        \n        # Save each agent\n        for agent_id, agent in self.agents.items():\n            # Create file path\n            file_path = os.path.join(self.save_dir, f\"{agent_id}{suffix}.pt\")\n            \n            # Save model\n            torch.save({\n                \"policy_state_dict\": agent.policy.state_dict(),\n                \"value_state_dict\": agent.value.state_dict(),\n                \"policy_optimizer_state_dict\": agent.policy_optimizer.state_dict(),\n                \"value_optimizer_state_dict\": agent.value_optimizer.state_dict(),\n            }, file_path)\n    \n    def load_agents(self, suffix: str = \"\") -> None:\n        \"\"\"\n        Load all agents' models.\n        \n        Args:\n            suffix: Optional suffix for loaded files\n        \"\"\"\n        import os\n        \n        # Load each agent\n        for agent_id, agent in self.agents.items():\n            # Create file path\n            file_path = os.path.join(self.save_dir, f\"{agent_id}{suffix}.pt\")\n            \n            # Check if file exists\n            if not os.path.exists(file_path):\n                print(f\"Warning: Model file not found for agent {agent_id}\")\n                continue\n            \n            # Load model\n            checkpoint = torch.load(file_path, map_location=self.device)\n            \n            # Load state dicts\n            agent.policy.load_state_dict(checkpoint[\"policy_state_dict\"])\n            agent.value.load_state_dict(checkpoint[\"value_state_dict\"])\n            agent.policy_optimizer.load_state_dict(checkpoint[\"policy_optimizer_state_dict\"])\n            agent.value_optimizer.load_state_dict(checkpoint[\"value_optimizer_state_dict\"])\n\n\nclass ToolSelectionGRPO:\n    \"\"\"\n    Specialized GRPO implementation for tool selection in Claude Code.\n    This class adapts the MultiAgentGroupRL for the specific context of tool selection.\n    \"\"\"\n    \n    def __init__(\n        self,\n        tool_registry: Any,  # Should be a reference to the tool registry\n        context_evaluator: Callable,  # Function to evaluate quality of response given context\n        state_dim: int = 768,  # Embedding dimension for query\n        num_agents: int = 3,  # Number of agents in the group\n        update_interval: int = 100,\n        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    ):\n        \"\"\"\n        Initialize the GRPO tool selector.\n        \n        Args:\n            tool_registry: Registry containing available tools\n            context_evaluator: Function to evaluate response quality\n            state_dim: Dimension of state features\n            num_agents: Number of agents in the group\n            update_interval: How often to update agents\n            device: Device to run on\n        \"\"\"\n        self.tool_registry = tool_registry\n        self.context_evaluator = context_evaluator\n        \n        # Get all available tools\n        self.tool_names = tool_registry.get_all_tool_names()\n        self.action_dim = len(self.tool_names)\n        \n        # Define agent configurations\n        agent_configs = [\n            {\n                \"id\": f\"tool_agent_{i}\",\n                \"state_dim\": state_dim,\n                \"action_dim\": self.action_dim,\n                \"hidden_dims\": [256, 128],\n                \"relative_advantage_weight\": 0.7 if i > 0 else 0.3,  # Different weights\n                \"entropy_coef\": 0.02 if i == 0 else 0.01,  # Different exploration rates\n            }\n            for i in range(num_agents)\n        ]\n        \n        # Initialize multi-agent RL system\n        self.rl_system = MultiAgentGroupRL(\n            agent_configs=agent_configs,\n            feature_extractor=self._extract_features,\n            reward_function=self._compute_reward,\n            update_interval=update_interval,\n            device=device,\n        )\n        \n        # Track current episode\n        self.current_episode = {agent_id: {} for agent_id in self.rl_system.agents}\n    \n    def select_tool(self, user_query: str, context: Dict[str, Any], visualizer=None) -> str:\n        \"\"\"\n        Select the best tool to use for a given user query and context.\n        \n        Args:\n            user_query: The user's query\n            context: The current conversation context\n            visualizer: Optional visualizer to display the selection process\n            \n        Returns:\n            The name of the best tool to use\n        \"\"\"\n        # Create observation\n        observation = {\n            \"query\": user_query,\n            \"context\": context,\n        }\n        \n        # If visualizer is provided, start it\n        if visualizer:\n            visualizer.start()\n            visualizer.add_execution(\n                execution_id=\"tool_selection\",\n                tool_name=\"GRPO Tool Selection\",\n                parameters={\"query\": user_query[:100] + \"...\" if len(user_query) > 100 else user_query}\n            )\n        \n        # Select agent to use (round-robin for now)\n        agent_id = f\"tool_agent_{self.rl_system.total_steps % len(self.rl_system.agents)}\"\n        \n        # Update visualizer if provided\n        if visualizer:\n            visualizer.update_progress(\"tool_selection\", 0.3)\n        \n        # Get action from agent\n        action_idx, _ = self.rl_system.select_action(\n            agent_id=agent_id,\n            observation=observation,\n            deterministic=False  # Use exploratory actions during learning\n        )\n        \n        # Update visualizer if provided\n        if visualizer:\n            visualizer.update_progress(\"tool_selection\", 0.6)\n        \n        # Store initial information for the episode\n        self.current_episode[agent_id] = {\n            \"observation\": observation,\n            \"action_idx\": action_idx,\n            \"initial_quality\": self.context_evaluator(context),\n        }\n        \n        # Map action index to tool name\n        tool_name = self.tool_names[action_idx]\n        \n        # Complete visualization if provided\n        if visualizer:\n            # Create detailed metrics for visualization\n            agent_data = {}\n            for aid, agent in self.rl_system.agents.items():\n                # Get all tool probabilities for this agent\n                with torch.no_grad():\n                    state = self.rl_system._extract_features(observation)\n                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n                    logits = agent.policy(state_tensor)\n                    probs = F.softmax(logits, dim=1).squeeze().cpu().numpy()\n                \n                # Add to metrics\n                agent_data[aid] = {\n                    \"selected\": aid == agent_id,\n                    \"tool_probabilities\": {\n                        self.tool_names[i]: float(prob) \n                        for i, prob in enumerate(probs)\n                    }\n                }\n            \n            # Complete the visualization\n            visualizer.complete_execution(\n                execution_id=\"tool_selection\",\n                result={\n                    \"selected_tool\": tool_name,\n                    \"selected_agent\": agent_id,\n                    \"agent_data\": agent_data\n                },\n                status=\"success\"\n            )\n            visualizer.stop()\n        \n        return tool_name\n    \n    def observe_result(\n        self, agent_id: str, result: Any, context: Dict[str, Any], done: bool = True\n    ) -> None:\n        \"\"\"\n        Observe the result of using a tool.\n        \n        Args:\n            agent_id: The ID of the agent that selected the tool\n            result: The result of using the tool\n            context: The updated context after using the tool\n            done: Whether the interaction is complete\n        \"\"\"\n        if agent_id not in self.current_episode:\n            return\n        \n        # Get episode information\n        episode = self.current_episode[agent_id]\n        observation = episode[\"observation\"]\n        action_idx = episode[\"action_idx\"]\n        initial_quality = episode[\"initial_quality\"]\n        \n        # Create next observation\n        next_observation = {\n            \"query\": observation[\"query\"],\n            \"context\": context,\n            \"result\": result,\n        }\n        \n        # Compute reward\n        reward = self._compute_reward(observation, action_idx, result, context, initial_quality)\n        \n        # Record observation\n        self.rl_system.observe(\n            agent_id=agent_id,\n            observation=observation,\n            action=action_idx,\n            reward=reward,\n            next_observation=next_observation,\n            done=done,\n        )\n        \n        # Clear episode if done\n        if done:\n            self.current_episode[agent_id] = {}\n    \n    def _extract_features(self, observation: Dict[str, Any]) -> np.ndarray:\n        \"\"\"Extract features from an observation.\"\"\"\n        # This would ideally use an embedding model\n        # For now, return a random vector as a placeholder\n        return np.random.randn(768)\n    \n    def _compute_reward(\n        self, \n        observation: Dict[str, Any], \n        action_idx: int, \n        result: Any,\n        context: Dict[str, Any], \n        initial_quality: float\n    ) -> float:\n        \"\"\"Compute the reward for an action.\"\"\"\n        # Compute the quality improvement\n        final_quality = self.context_evaluator(context)\n        quality_improvement = final_quality - initial_quality\n        \n        # Base reward on quality improvement\n        reward = max(0, quality_improvement * 10)  # Scale for better learning\n        \n        return reward\n    \n    def update(self) -> Dict[str, Dict[str, float]]:\n        \"\"\"\n        Trigger an update of all agents.\n        \n        Returns:\n            Dictionary of training metrics\n        \"\"\"\n        return self.rl_system.update_all_agents()\n    \n    def save(self, suffix: str = \"\") -> None:\n        \"\"\"Save all agents.\"\"\"\n        self.rl_system.save_agents(suffix)\n    \n    def load(self, suffix: str = \"\") -> None:\n        \"\"\"Load all agents.\"\"\"\n        self.rl_system.load_agents(suffix)"}
{"type": "source_file", "path": "claude_code/lib/tools/__init__.py", "content": "\"\"\"Tools module for Claude Code Python Edition.\"\"\"\n\nfrom .base import Tool, ToolParameter, ToolResult, ToolRegistry, tool\nfrom .manager import ToolExecutionManager\nfrom .file_tools import register_file_tools\nfrom .search_tools import register_search_tools\nfrom .code_tools import register_code_tools\nfrom .ai_tools import register_ai_tools\n\n__all__ = [\n    \"Tool\", \n    \"ToolParameter\", \n    \"ToolResult\", \n    \"ToolRegistry\", \n    \"ToolExecutionManager\", \n    \"tool\",\n    \"register_file_tools\",\n    \"register_search_tools\",\n    \"register_code_tools\",\n    \"register_ai_tools\"\n]\n\ndef register_all_tools(registry: ToolRegistry = None) -> ToolRegistry:\n    \"\"\"Register all available tools with the registry.\n    \n    Args:\n        registry: Existing registry or None to create a new one\n        \n    Returns:\n        Tool registry with all tools registered\n    \"\"\"\n    if registry is None:\n        registry = ToolRegistry()\n    \n    # Register tool categories\n    register_file_tools(registry)\n    register_search_tools(registry)\n    register_code_tools(registry)\n    register_ai_tools(registry)\n    \n    # Load saved routines\n    registry.load_routines()\n    \n    return registry\n"}
{"type": "source_file", "path": "claude_code/lib/rl/tool_optimizer.py", "content": "\"\"\"\nAdvanced tool selection optimization for Claude Code Python.\n\nThis module implements a specialized reinforcement learning system for optimizing\ntool selection based on user queries and context. It uses advanced RL techniques\ncombined with neural models to learn which tools work best for different types of\nqueries over time, featuring transfer learning, meta-learning, and causal reasoning.\n\"\"\"\n\nimport numpy as np\nimport os\nimport json\nimport time\nimport math\nimport random\nfrom typing import Dict, List, Any, Optional, Tuple, Callable, Union, Set\nfrom dataclasses import dataclass, field\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import deque, defaultdict\n\ntry:\n    from sentence_transformers import SentenceTransformer\n    HAVE_SENTENCE_TRANSFORMERS = True\nexcept ImportError:\n    HAVE_SENTENCE_TRANSFORMERS = False\n\ntry:\n    import networkx as nx\n    HAVE_NETWORKX = True\nexcept ImportError:\n    HAVE_NETWORKX = False\n\ntry:\n    import faiss\n    HAVE_FAISS = True\nexcept ImportError:\n    HAVE_FAISS = False\n\nfrom .grpo import ToolSelectionGRPO\n\n# Advanced streaming and reflection capabilities\nclass StreamingReflectionEngine:\n    \"\"\"Engine for real-time streaming of thoughts, self-correction, and reflection.\"\"\"\n    \n    def __init__(self, embedding_dim: int = 768, reflection_buffer_size: int = 1000):\n        \"\"\"Initialize the streaming reflection engine.\n        \n        Args:\n            embedding_dim: Dimension of embeddings\n            reflection_buffer_size: Size of reflection buffer\n        \"\"\"\n        self.embedding_dim = embedding_dim\n        self.reflection_buffer_size = reflection_buffer_size\n        \n        # Reflection memory buffer\n        self.reflection_buffer = deque(maxlen=reflection_buffer_size)\n        \n        # Working memory for current thought stream\n        self.working_memory = []\n        \n        # Long-term memory for learned reflections\n        self.reflection_patterns = {}\n        \n        # Reflection critic neural network\n        self.reflection_critic = nn.Sequential(\n            nn.Linear(embedding_dim, embedding_dim),\n            nn.LayerNorm(embedding_dim),\n            nn.ReLU(),\n            nn.Linear(embedding_dim, embedding_dim // 2),\n            nn.LayerNorm(embedding_dim // 2),\n            nn.ReLU(),\n            nn.Linear(embedding_dim // 2, 3)  # 3 outputs: continue, revise, complete\n        )\n        \n        # Thought revision network\n        self.thought_reviser = nn.Transformer(\n            d_model=embedding_dim,\n            nhead=8,\n            num_encoder_layers=3,\n            num_decoder_layers=3,\n            dim_feedforward=embedding_dim * 4,\n            dropout=0.1\n        )\n        \n        # Self-correction performance metrics\n        self.correction_metrics = {\n            \"total_corrections\": 0,\n            \"helpful_corrections\": 0,\n            \"correction_depth\": [],\n            \"avg_correction_time\": 0.0,\n            \"total_correction_time\": 0.0\n        }\n        \n        # Learning rate for reflection updates\n        self.reflection_lr = 0.001\n        \n        # Optimizer for reflection models\n        self.optimizer = torch.optim.Adam(\n            list(self.reflection_critic.parameters()) + \n            list(self.thought_reviser.parameters()),\n            lr=self.reflection_lr\n        )\n    \n    def start_reflection_stream(self, query_embedding: np.ndarray, context: Dict[str, Any]) -> str:\n        \"\"\"Start a new reflection stream for a query.\n        \n        Args:\n            query_embedding: Embedding of the query\n            context: Additional context\n            \n        Returns:\n            Stream ID for this reflection session\n        \"\"\"\n        stream_id = f\"reflection_{int(time.time())}_{random.randint(0, 10000)}\"\n        \n        # Initialize working memory for this stream\n        self.working_memory = [\n            {\n                \"type\": \"query\",\n                \"embedding\": torch.FloatTensor(query_embedding),\n                \"timestamp\": time.time(),\n                \"context\": context,\n                \"stream_id\": stream_id\n            }\n        ]\n        \n        return stream_id\n    \n    def add_thought(self, \n                   stream_id: str, \n                   thought_embedding: np.ndarray, \n                   thought_text: str,\n                   thought_type: str = \"reasoning\") -> Dict[str, Any]:\n        \"\"\"Add a thought to the reflection stream and get feedback.\n        \n        Args:\n            stream_id: ID of the reflection stream\n            thought_embedding: Embedding of the thought\n            thought_text: Text of the thought\n            thought_type: Type of thought (reasoning, plan, action, etc.)\n            \n        Returns:\n            Feedback on the thought\n        \"\"\"\n        # Convert to tensor\n        thought_tensor = torch.FloatTensor(thought_embedding)\n        \n        # Create thought record\n        thought = {\n            \"type\": thought_type,\n            \"embedding\": thought_tensor,\n            \"text\": thought_text,\n            \"timestamp\": time.time(),\n            \"stream_id\": stream_id,\n            \"depth\": len(self.working_memory)\n        }\n        \n        # Add to working memory\n        self.working_memory.append(thought)\n        \n        # Get reflection feedback\n        feedback = self._reflect_on_thought(thought)\n        \n        # Store in reflection buffer\n        self.reflection_buffer.append({\n            \"thought\": thought,\n            \"feedback\": feedback\n        })\n        \n        return feedback\n    \n    def _reflect_on_thought(self, thought: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Generate reflection on a thought.\n        \n        Args:\n            thought: The thought to reflect on\n            \n        Returns:\n            Reflection feedback\n        \"\"\"\n        # Get thought embedding\n        thought_embedding = thought[\"embedding\"]\n        \n        # Get critic prediction\n        with torch.no_grad():\n            critic_output = self.reflection_critic(thought_embedding.unsqueeze(0))\n            action_probs = F.softmax(critic_output, dim=1).squeeze(0)\n            \n            # Actions: [continue, revise, complete]\n            action_idx = torch.argmax(action_probs).item()\n            action_confidence = action_probs[action_idx].item()\n            \n            actions = [\"continue\", \"revise\", \"complete\"]\n            action = actions[action_idx]\n        \n        # Check if similar to patterns we've seen before\n        pattern_matches = []\n        if len(self.working_memory) >= 3:\n            # Get sequence of last 3 thoughts\n            sequence = [t[\"embedding\"] for t in self.working_memory[-3:]]\n            sequence_tensor = torch.stack(sequence)\n            \n            # Compare to known patterns\n            for pattern_name, pattern_data in self.reflection_patterns.items():\n                if len(pattern_data[\"sequence\"]) == 3:\n                    # Compute similarity\n                    pattern_tensor = torch.stack(pattern_data[\"sequence\"])\n                    similarity = F.cosine_similarity(\n                        sequence_tensor.mean(dim=0).unsqueeze(0),\n                        pattern_tensor.mean(dim=0).unsqueeze(0)\n                    ).item()\n                    \n                    if similarity > 0.7:  # High similarity threshold\n                        pattern_matches.append({\n                            \"pattern\": pattern_name,\n                            \"similarity\": similarity,\n                            \"outcome\": pattern_data[\"outcome\"]\n                        })\n        \n        # Check for circular reasoning\n        is_circular = False\n        if len(self.working_memory) >= 5:\n            recent_thoughts = [t[\"embedding\"] for t in self.working_memory[-5:]]\n            \n            # Check if latest thought is very similar to any of the previous 4\n            latest = recent_thoughts[-1]\n            for prev in recent_thoughts[:-1]:\n                similarity = F.cosine_similarity(latest.unsqueeze(0), prev.unsqueeze(0)).item()\n                if similarity > 0.85:  # Very high similarity threshold\n                    is_circular = True\n                    break\n        \n        # Generate revision suggestion if needed\n        revision_suggestion = None\n        if action == \"revise\" or is_circular:\n            revision_suggestion = self._generate_revision(thought)\n        \n        # Create feedback\n        feedback = {\n            \"action\": action,\n            \"confidence\": action_confidence,\n            \"is_circular\": is_circular,\n            \"pattern_matches\": pattern_matches,\n            \"revision_suggestion\": revision_suggestion,\n            \"timestamp\": time.time()\n        }\n        \n        return feedback\n    \n    def _generate_revision(self, thought: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Generate a revision for a thought.\n        \n        Args:\n            thought: The thought to revise\n            \n        Returns:\n            Revision suggestion\n        \"\"\"\n        # If we have fewer than 2 thoughts, can't generate meaningful revision\n        if len(self.working_memory) < 2:\n            return {\n                \"type\": \"general\",\n                \"embedding\": thought[\"embedding\"].detach().numpy(),\n                \"message\": \"Consider providing more specific reasoning\"\n            }\n        \n        # Get context from previous thoughts\n        context_embeddings = torch.stack([t[\"embedding\"] for t in self.working_memory[:-1]])\n        \n        # Create source and target sequences for transformer\n        src = context_embeddings.unsqueeze(1)  # [seq_len, batch_size, embedding_dim]\n        tgt = thought[\"embedding\"].unsqueeze(0).unsqueeze(1)  # [1, batch_size, embedding_dim]\n        \n        # Generate revision using transformer\n        with torch.no_grad():\n            # Create attention mask \n            src_mask = torch.zeros(src.shape[0], src.shape[0]).bool()\n            \n            # Revised thought\n            revised_embedding = self.thought_reviser(\n                src, \n                tgt,\n                src_mask=src_mask,\n                tgt_mask=torch.zeros(1, 1).bool()\n            )\n            \n            # Extract the output embedding\n            revised_embedding = revised_embedding[0, 0]\n        \n        # Look for insights from reflection buffer\n        insights = []\n        \n        # Find similar thoughts from reflection buffer\n        for entry in self.reflection_buffer:\n            past_thought = entry[\"thought\"]\n            \n            # Skip if from current stream\n            if past_thought.get(\"stream_id\") == thought.get(\"stream_id\"):\n                continue\n                \n            # Compute similarity\n            similarity = F.cosine_similarity(\n                thought[\"embedding\"].unsqueeze(0),\n                past_thought[\"embedding\"].unsqueeze(0)\n            ).item()\n            \n            if similarity > 0.6:  # Significant similarity\n                insights.append({\n                    \"type\": \"similar_thought\",\n                    \"similarity\": similarity,\n                    \"feedback\": entry[\"feedback\"]\n                })\n        \n        # Create revision suggestion\n        revision = {\n            \"type\": \"specific\",\n            \"embedding\": revised_embedding.detach().numpy(),\n            \"insights\": insights[:3],  # Top 3 insights\n            \"message\": \"Consider revising this thought for more clarity and precision\"\n        }\n        \n        return revision\n    \n    def complete_reflection(self, stream_id: str, \n                          outcome: Dict[str, Any],\n                          success: bool) -> Dict[str, Any]:\n        \"\"\"Complete a reflection stream and learn from it.\n        \n        Args:\n            stream_id: ID of the reflection stream\n            outcome: Outcome of the actions taken based on reflections\n            success: Whether the outcome was successful\n            \n        Returns:\n            Reflection summary and metrics\n        \"\"\"\n        # Filter working memory for this stream\n        stream_thoughts = [t for t in self.working_memory if t.get(\"stream_id\") == stream_id]\n        \n        if not stream_thoughts:\n            return {\"status\": \"error\", \"message\": \"Stream not found\"}\n        \n        # Count corrections\n        corrections = sum(1 for t in stream_thoughts if t.get(\"type\") == \"correction\")\n        \n        # Update metrics\n        self.correction_metrics[\"total_corrections\"] += corrections\n        if success:\n            self.correction_metrics[\"helpful_corrections\"] += corrections\n            \n        if corrections > 0:\n            self.correction_metrics[\"correction_depth\"].append(len(stream_thoughts))\n        \n        # Learn from this reflection session\n        self._learn_from_reflection(stream_thoughts, outcome, success)\n        \n        # Extract and store useful thought patterns\n        if success and len(stream_thoughts) >= 3:\n            self._extract_thought_patterns(stream_thoughts, outcome)\n        \n        # Compute summary stats\n        duration = time.time() - stream_thoughts[0][\"timestamp\"]\n        avg_thought_time = duration / len(stream_thoughts)\n        \n        # Generate summary\n        summary = {\n            \"stream_id\": stream_id,\n            \"num_thoughts\": len(stream_thoughts),\n            \"num_corrections\": corrections,\n            \"duration\": duration,\n            \"avg_thought_time\": avg_thought_time,\n            \"success\": success,\n            \"outcome_summary\": outcome.get(\"summary\", \"No summary provided\")\n        }\n        \n        return summary\n    \n    def _learn_from_reflection(self, thoughts: List[Dict[str, Any]], \n                             outcome: Dict[str, Any],\n                             success: bool) -> None:\n        \"\"\"Learn from a completed reflection stream.\n        \n        Args:\n            thoughts: List of thoughts in the stream\n            outcome: Outcome of the actions\n            success: Whether the outcome was successful\n        \"\"\"\n        if not thoughts:\n            return\n            \n        # Skip if too few thoughts to learn from\n        if len(thoughts) < 3:\n            return\n            \n        # Create training examples for reflection critic\n        examples = []\n        \n        for i in range(1, len(thoughts) - 1):\n            # Current thought\n            thought_embedding = thoughts[i][\"embedding\"]\n            \n            # Determine correct action label based on what happened\n            # 0: continue, 1: revise, 2: complete\n            if i == len(thoughts) - 2:\n                # Second-to-last thought should have led to completion\n                label = 2\n            elif thoughts[i+1].get(\"type\") == \"correction\":\n                # This thought was followed by a correction, should have been revised\n                label = 1\n            else:\n                # This thought was good to continue from\n                label = 0\n                \n            # Create example\n            examples.append((thought_embedding, label))\n            \n        # Skip training if too few examples\n        if not examples:\n            return\n            \n        # Update reflection critic with these examples\n        self.optimizer.zero_grad()\n        \n        critic_loss = 0.0\n        for embedding, label in examples:\n            # Forward pass\n            logits = self.reflection_critic(embedding.unsqueeze(0))\n            \n            # Compute loss\n            target = torch.tensor([label], device=embedding.device)\n            loss = F.cross_entropy(logits, target)\n            \n            critic_loss += loss\n            \n        # Scale loss by number of examples\n        critic_loss /= len(examples)\n        \n        # Backpropagation\n        critic_loss.backward()\n        \n        # Update parameters\n        self.optimizer.step()\n    \n    def _extract_thought_patterns(self, thoughts: List[Dict[str, Any]], \n                                outcome: Dict[str, Any]) -> None:\n        \"\"\"Extract useful thought patterns from successful reflection streams.\n        \n        Args:\n            thoughts: List of thoughts in the stream\n            outcome: Outcome information\n        \"\"\"\n        # Need at least 3 thoughts to form a meaningful pattern\n        if len(thoughts) < 3:\n            return\n            \n        # Generate a name for this pattern\n        pattern_id = f\"pattern_{len(self.reflection_patterns) + 1}\"\n        \n        # Extract sequences of 3 consecutive thoughts\n        for i in range(len(thoughts) - 2):\n            sequence = thoughts[i:i+3]\n            \n            # Skip if any thought is a correction - we want clean sequences\n            if any(t.get(\"type\") == \"correction\" for t in sequence):\n                continue\n                \n            # Get embeddings for the sequence\n            sequence_embeddings = [t[\"embedding\"] for t in sequence]\n            \n            # Store the pattern\n            self.reflection_patterns[f\"{pattern_id}_{i}\"] = {\n                \"sequence\": sequence_embeddings,\n                \"outcome\": {\n                    \"success\": outcome.get(\"success\", True),\n                    \"context\": outcome.get(\"context\", {}),\n                    \"summary\": outcome.get(\"summary\", \"\")\n                },\n                \"timestamp\": time.time()\n            }\n            \n            # Limit number of patterns to prevent memory issues\n            if len(self.reflection_patterns) > 100:\n                # Remove oldest pattern\n                oldest_key = min(self.reflection_patterns.keys(), \n                               key=lambda k: self.reflection_patterns[k][\"timestamp\"])\n                del self.reflection_patterns[oldest_key]\n\n\n# Active Learning and Self-Improvement\nclass ActiveLearningSystem:\n    \"\"\"Active learning system that identifies knowledge gaps and seeks targeted improvement.\"\"\"\n    \n    def __init__(self, embedding_dim: int = 768, exploration_rate: float = 0.2):\n        \"\"\"Initialize the active learning system.\n        \n        Args:\n            embedding_dim: Dimension of embeddings\n            exploration_rate: Rate of exploration vs. exploitation\n        \"\"\"\n        self.embedding_dim = embedding_dim\n        self.exploration_rate = exploration_rate\n        \n        # Knowledge graph for tracking what's known/unknown\n        self.knowledge_graph = nx.DiGraph() if HAVE_NETWORKX else None\n        \n        # Uncertainty estimation model\n        self.uncertainty_estimator = nn.Sequential(\n            nn.Linear(embedding_dim, embedding_dim),\n            nn.LayerNorm(embedding_dim),\n            nn.ReLU(),\n            nn.Linear(embedding_dim, embedding_dim // 2),\n            nn.LayerNorm(embedding_dim // 2),\n            nn.ReLU(),\n            nn.Linear(embedding_dim // 2, 2)  # [confidence, uncertainty]\n        )\n        \n        # Knowledge boundaries\n        self.knowledge_centroids = []\n        self.knowledge_radius = {}\n        \n        # Learning curriculum\n        self.learning_targets = []\n        self.learning_progress = {}\n        \n        # Exploration history\n        self.exploration_history = []\n        \n        # Coreset for diversity \n        self.coreset = []\n        self.coreset_embeddings = []\n        \n        # Faiss index for fast nearest neighbor search\n        self.index = None\n        if HAVE_FAISS:\n            self.index = faiss.IndexFlatL2(embedding_dim)\n        \n        # Optimizer for uncertainty estimator\n        self.optimizer = torch.optim.Adam(self.uncertainty_estimator.parameters(), lr=0.001)\n    \n    def estimate_uncertainty(self, query_embedding: np.ndarray) -> Dict[str, float]:\n        \"\"\"Estimate uncertainty for a query or state.\n        \n        Args:\n            query_embedding: Embedding to evaluate\n            \n        Returns:\n            Dictionary with confidence and uncertainty scores\n        \"\"\"\n        # Convert to tensor\n        query_tensor = torch.FloatTensor(query_embedding)\n        \n        # Get uncertainty estimate\n        with torch.no_grad():\n            estimate = self.uncertainty_estimator(query_tensor.unsqueeze(0))\n            confidence, uncertainty = F.softmax(estimate, dim=1).squeeze(0).tolist()\n        \n        # Compute distance-based uncertainty if we have knowledge centroids\n        distance_uncertainty = 0.0\n        if self.knowledge_centroids:\n            # Convert to numpy for distance calculation\n            centroid_array = np.vstack(self.knowledge_centroids)\n            query_array = query_embedding.reshape(1, -1)\n            \n            # Compute distances to all centroids\n            distances = np.linalg.norm(centroid_array - query_array, axis=1)\n            \n            # Get distance to nearest centroid\n            min_dist = np.min(distances)\n            min_idx = np.argmin(distances)\n            nearest_centroid = self.knowledge_centroids[min_idx]\n            \n            # Radius of knowledge around this centroid\n            radius = self.knowledge_radius.get(tuple(nearest_centroid), 1.0)\n            \n            # Normalize distance by radius to get uncertainty\n            distance_uncertainty = min(1.0, min_dist / radius)\n        \n        # Combine model and distance uncertainty\n        combined_uncertainty = 0.7 * uncertainty + 0.3 * distance_uncertainty\n        combined_confidence = 1.0 - combined_uncertainty\n        \n        return {\n            \"confidence\": combined_confidence,\n            \"uncertainty\": combined_uncertainty,\n            \"model_confidence\": confidence,\n            \"model_uncertainty\": uncertainty,\n            \"distance_uncertainty\": distance_uncertainty\n        }\n    \n    def should_explore(self, query_embedding: np.ndarray, context: Dict[str, Any]) -> bool:\n        \"\"\"Determine if we should explore to gather new knowledge for this query.\n        \n        Args:\n            query_embedding: Query embedding\n            context: Additional context\n            \n        Returns:\n            Whether to explore\n        \"\"\"\n        # Estimate uncertainty\n        uncertainty_info = self.estimate_uncertainty(query_embedding)\n        \n        # Always explore if uncertainty is very high\n        if uncertainty_info[\"uncertainty\"] > 0.8:\n            return True\n            \n        # Use epsilon-greedy strategy with adaptive exploration\n        # Higher uncertainty means more likely to explore\n        adaptive_rate = self.exploration_rate * (0.5 + uncertainty_info[\"uncertainty\"])\n        \n        # Apply epsilon-greedy\n        return random.random() < adaptive_rate\n    \n    def add_knowledge(self, query_embedding: np.ndarray, \n                    related_info: Dict[str, Any],\n                    confidence: float) -> None:\n        \"\"\"Add knowledge to the system.\n        \n        Args:\n            query_embedding: Query embedding\n            related_info: Related information (e.g., tool used, outcome)\n            confidence: Confidence in this knowledge\n        \"\"\"\n        # Add to knowledge graph\n        if self.knowledge_graph is not None:\n            # Create node for this query\n            query_key = f\"query_{len(self.knowledge_graph.nodes)}\"\n            self.knowledge_graph.add_node(query_key, \n                                        embedding=query_embedding,\n                                        confidence=confidence,\n                                        timestamp=time.time())\n            \n            # Add related information as connected nodes\n            for key, value in related_info.items():\n                info_key = f\"{key}_{len(self.knowledge_graph.nodes)}\"\n                self.knowledge_graph.add_node(info_key, value=value)\n                self.knowledge_graph.add_edge(query_key, info_key, relation=key)\n                \n        # Update knowledge centroids\n        self._update_knowledge_boundaries(query_embedding, confidence)\n        \n        # Update coreset for diversity\n        self._update_coreset(query_embedding, related_info)\n    \n    def _update_knowledge_boundaries(self, embedding: np.ndarray, confidence: float) -> None:\n        \"\"\"Update knowledge boundaries with new information.\n        \n        Args:\n            embedding: Embedding of new knowledge\n            confidence: Confidence in this knowledge\n        \"\"\"\n        # If no centroids yet, add this as the first one\n        if not self.knowledge_centroids:\n            self.knowledge_centroids.append(embedding)\n            self.knowledge_radius[tuple(embedding)] = 1.0\n            return\n            \n        # Find closest centroid\n        centroid_array = np.vstack(self.knowledge_centroids)\n        query_array = embedding.reshape(1, -1)\n        \n        distances = np.linalg.norm(centroid_array - query_array, axis=1)\n        min_dist = np.min(distances)\n        min_idx = np.argmin(distances)\n        nearest_centroid = self.knowledge_centroids[min_idx]\n        nearest_centroid_tuple = tuple(nearest_centroid)\n        \n        # Get current radius\n        current_radius = self.knowledge_radius.get(nearest_centroid_tuple, 1.0)\n        \n        # If within current radius, update radius based on confidence\n        if min_dist < current_radius:\n            # Higher confidence shrinks radius (more precise knowledge)\n            # Lower confidence expands radius (more uncertainty)\n            new_radius = current_radius * (1.0 - 0.1 * confidence)\n            self.knowledge_radius[nearest_centroid_tuple] = new_radius\n        else:\n            # Outside known areas, add as new centroid\n            if len(self.knowledge_centroids) < 100:  # Limit number of centroids\n                self.knowledge_centroids.append(embedding)\n                self.knowledge_radius[tuple(embedding)] = 1.0\n            \n            # Otherwise, merge with nearest\n            else:\n                # Update nearest centroid with weighted average\n                updated_centroid = 0.8 * nearest_centroid + 0.2 * embedding\n                \n                # Update centroid list\n                self.knowledge_centroids[min_idx] = updated_centroid\n                \n                # Update radius dict\n                self.knowledge_radius[tuple(updated_centroid)] = current_radius\n                del self.knowledge_radius[nearest_centroid_tuple]\n    \n    def _update_coreset(self, embedding: np.ndarray, info: Dict[str, Any]) -> None:\n        \"\"\"Update coreset of diverse examples.\n        \n        Args:\n            embedding: New example embedding\n            info: Related information\n        \"\"\"\n        # Skip if no Faiss\n        if self.index is None:\n            return\n            \n        # If coreset is empty, add first example\n        if not self.coreset_embeddings:\n            self.coreset.append(info)\n            self.coreset_embeddings.append(embedding)\n            self.index.add(np.vstack([embedding]))\n            return\n            \n        # Check if this example is sufficiently different from existing examples\n        # Convert to correct shape for Faiss\n        query = embedding.reshape(1, -1).astype('float32')\n        \n        # Search for nearest neighbors\n        distances, indices = self.index.search(query, 1)\n        \n        # If sufficiently different, add to coreset\n        if distances[0][0] > 0.5:  # Distance threshold\n            if len(self.coreset) < 100:  # Limit coreset size\n                self.coreset.append(info)\n                self.coreset_embeddings.append(embedding)\n                self.index.add(query)\n            else:\n                # Replace most similar item\n                _, indices = self.index.search(query, len(self.coreset))\n                most_similar_idx = indices[0][-1]\n                \n                # Remove from index (need to rebuild index)\n                self.coreset[most_similar_idx] = info\n                self.coreset_embeddings[most_similar_idx] = embedding\n                \n                # Rebuild index\n                self.index = faiss.IndexFlatL2(self.embedding_dim)\n                self.index.add(np.vstack(self.coreset_embeddings).astype('float32'))\n    \n    def identify_knowledge_gaps(self) -> List[Dict[str, Any]]:\n        \"\"\"Identify knowledge gaps for active learning.\n        \n        Returns:\n            List of knowledge gap areas to explore\n        \"\"\"\n        gaps = []\n        \n        # Skip if no knowledge graph\n        if self.knowledge_graph is None:\n            return gaps\n            \n        # Find areas with low confidence\n        low_confidence_nodes = [\n            (node, data) for node, data in self.knowledge_graph.nodes(data=True)\n            if \"confidence\" in data and data[\"confidence\"] < 0.5\n        ]\n        \n        # Group by embedding similarity\n        clusters = {}\n        for node, data in low_confidence_nodes:\n            if \"embedding\" not in data:\n                continue\n                \n            # Find or create cluster\n            assigned = False\n            for cluster_id, cluster_data in clusters.items():\n                centroid = cluster_data[\"centroid\"]\n                \n                # Compute similarity\n                similarity = np.dot(data[\"embedding\"], centroid) / (\n                    np.linalg.norm(data[\"embedding\"]) * np.linalg.norm(centroid)\n                )\n                \n                if similarity > 0.7:  # High similarity threshold\n                    # Add to cluster\n                    cluster_data[\"nodes\"].append((node, data))\n                    \n                    # Update centroid\n                    new_centroid = (centroid * len(cluster_data[\"nodes\"]) + data[\"embedding\"]) / (\n                        len(cluster_data[\"nodes\"]) + 1\n                    )\n                    cluster_data[\"centroid\"] = new_centroid\n                    \n                    assigned = True\n                    break\n            \n            if not assigned:\n                # Create new cluster\n                cluster_id = f\"cluster_{len(clusters)}\"\n                clusters[cluster_id] = {\n                    \"centroid\": data[\"embedding\"],\n                    \"nodes\": [(node, data)]\n                }\n        \n        # Convert clusters to knowledge gaps\n        for cluster_id, cluster_data in clusters.items():\n            if len(cluster_data[\"nodes\"]) >= 2:  # Only consider significant clusters\n                related_info = {}\n                \n                # Collect information about this cluster\n                for node, data in cluster_data[\"nodes\"]:\n                    # Get connected nodes\n                    if self.knowledge_graph.has_node(node):\n                        for _, neighbor, edge_data in self.knowledge_graph.out_edges(node, data=True):\n                            neighbor_data = self.knowledge_graph.nodes[neighbor]\n                            if \"value\" in neighbor_data:\n                                relation = edge_data.get(\"relation\", \"related\")\n                                related_info[relation] = neighbor_data[\"value\"]\n                \n                # Create gap description\n                gap = {\n                    \"id\": cluster_id,\n                    \"centroid\": cluster_data[\"centroid\"],\n                    \"num_instances\": len(cluster_data[\"nodes\"]),\n                    \"related_info\": related_info,\n                    \"confidence\": np.mean([d[\"confidence\"] for _, d in cluster_data[\"nodes\"] if \"confidence\" in d])\n                }\n                \n                gaps.append(gap)\n        \n        # Sort gaps by confidence (ascending) and size (descending)\n        gaps.sort(key=lambda x: (x[\"confidence\"], -x[\"num_instances\"]))\n        \n        return gaps\n    \n    def generate_exploration_query(self, gap: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Generate an exploration query for a knowledge gap.\n        \n        Args:\n            gap: Knowledge gap information\n            \n        Returns:\n            Exploration query\n        \"\"\"\n        # Create query from gap centroid\n        centroid = gap[\"centroid\"]\n        \n        # Find nearest examples in coreset for additional context\n        similar_examples = []\n        if self.coreset_embeddings and len(self.coreset) > 0:\n            # Convert centroid to correct shape\n            query = centroid.reshape(1, -1).astype('float32')\n            \n            # Find nearest neighbors\n            if self.index is not None:\n                distances, indices = self.index.search(query, min(3, len(self.coreset)))\n                \n                # Add nearest examples\n                for i, idx in enumerate(indices[0]):\n                    if idx < len(self.coreset):\n                        similar_examples.append({\n                            \"example\": self.coreset[idx],\n                            \"distance\": distances[0][i]\n                        })\n        \n        # Generate exploration query\n        exploration = {\n            \"embedding\": centroid,\n            \"gap_id\": gap[\"id\"],\n            \"related_info\": gap[\"related_info\"],\n            \"confidence\": gap[\"confidence\"],\n            \"similar_examples\": similar_examples,\n            \"timestamp\": time.time()\n        }\n        \n        return exploration\n    \n    def update_from_exploration(self, \n                              gap_id: str, \n                              query_embedding: np.ndarray,\n                              result: Dict[str, Any],\n                              success: bool) -> None:\n        \"\"\"Update knowledge from exploration results.\n        \n        Args:\n            gap_id: ID of the knowledge gap\n            query_embedding: Embedding of the exploration query\n            result: Result of the exploration\n            success: Whether the exploration was successful\n        \"\"\"\n        # Add to exploration history\n        self.exploration_history.append({\n            \"gap_id\": gap_id,\n            \"embedding\": query_embedding,\n            \"result\": result,\n            \"success\": success,\n            \"timestamp\": time.time()\n        })\n        \n        # Update knowledge with exploration results\n        self.add_knowledge(\n            query_embedding=query_embedding,\n            related_info=result,\n            confidence=0.8 if success else 0.3\n        )\n        \n        # Update uncertainty model from this exploration\n        self._update_uncertainty_model(query_embedding, result, success)\n    \n    def _update_uncertainty_model(self, \n                                query_embedding: np.ndarray,\n                                result: Dict[str, Any],\n                                success: bool) -> None:\n        \"\"\"Update uncertainty estimation model.\n        \n        Args:\n            query_embedding: Query embedding\n            result: Exploration result\n            success: Whether exploration was successful\n        \"\"\"\n        # Convert to tensor\n        query_tensor = torch.FloatTensor(query_embedding)\n        \n        # Target values for training\n        # If success, low uncertainty (high confidence)\n        # If failure, high uncertainty (low confidence)\n        if success:\n            target = torch.tensor([[0.9, 0.1]])  # [confidence, uncertainty]\n        else:\n            target = torch.tensor([[0.2, 0.8]])  # [confidence, uncertainty]\n        \n        # Update model\n        self.optimizer.zero_grad()\n        \n        # Forward pass\n        prediction = self.uncertainty_estimator(query_tensor.unsqueeze(0))\n        prediction = F.softmax(prediction, dim=1)\n        \n        # Compute loss\n        loss = F.mse_loss(prediction, target)\n        \n        # Backpropagation\n        loss.backward()\n        \n        # Update parameters\n        self.optimizer.step()\n\n\n# Multi-task learning system\nclass MultiTaskLearningSystem:\n    \"\"\"System for learning across multiple task types with shared knowledge and specialized adapters.\"\"\"\n    \n    def __init__(self, embedding_dim: int = 768, num_tasks: int = 5):\n        \"\"\"Initialize the multi-task learning system.\n        \n        Args:\n            embedding_dim: Dimension of embeddings\n            num_tasks: Number of task types to support\n        \"\"\"\n        self.embedding_dim = embedding_dim\n        self.num_tasks = num_tasks\n        \n        # Task type registry\n        self.task_types = {}\n        \n        # Shared embedding model (backbone)\n        self.shared_model = nn.Sequential(\n            nn.Linear(embedding_dim, embedding_dim),\n            nn.LayerNorm(embedding_dim),\n            nn.ReLU(),\n            nn.Linear(embedding_dim, embedding_dim),\n            nn.LayerNorm(embedding_dim)\n        )\n        \n        # Task-specific adapter modules\n        self.task_adapters = nn.ModuleDict()\n        \n        # Task projectors (for returning to original space)\n        self.task_projectors = nn.ModuleDict()\n        \n        # Task-specific optimizers\n        self.task_optimizers = {}\n        \n        # Multi-task performance metrics\n        self.task_metrics = {}\n        \n        # Shared optimizer\n        self.shared_optimizer = torch.optim.Adam(self.shared_model.parameters(), lr=0.001)\n    \n    def register_task_type(self, task_name: str, \n                         initial_examples: List[Tuple[np.ndarray, np.ndarray]] = None) -> None:\n        \"\"\"Register a new task type.\n        \n        Args:\n            task_name: Name of the task\n            initial_examples: Optional initial examples (input, output embeddings)\n        \"\"\"\n        if task_name in self.task_types:\n            return\n            \n        # Register task\n        self.task_types[task_name] = {\n            \"examples\": [],\n            \"difficulty\": 0.5,  # Initial difficulty estimate\n            \"performance\": 0.0,  # Initial performance estimate\n            \"timestamp\": time.time()\n        }\n        \n        # Create task adapter\n        self.task_adapters[task_name] = nn.Sequential(\n            nn.Linear(self.embedding_dim, self.embedding_dim // 2),\n            nn.LayerNorm(self.embedding_dim // 2),\n            nn.ReLU(),\n            nn.Linear(self.embedding_dim // 2, self.embedding_dim)\n        )\n        \n        # Create projector back to original space\n        self.task_projectors[task_name] = nn.Linear(self.embedding_dim, self.embedding_dim)\n        \n        # Create optimizer\n        self.task_optimizers[task_name] = torch.optim.Adam(\n            list(self.task_adapters[task_name].parameters()) +\n            list(self.task_projectors[task_name].parameters()),\n            lr=0.001\n        )\n        \n        # Initialize metrics\n        self.task_metrics[task_name] = {\n            \"examples_seen\": 0,\n            \"loss_history\": [],\n            \"accuracy_history\": [],\n            \"last_improvement\": time.time()\n        }\n        \n        # Add initial examples if provided\n        if initial_examples:\n            for input_emb, output_emb in initial_examples:\n                self.add_task_example(task_name, input_emb, output_emb)\n    \n    def add_task_example(self, task_name: str, \n                       input_embedding: np.ndarray,\n                       output_embedding: np.ndarray) -> None:\n        \"\"\"Add an example for a specific task.\n        \n        Args:\n            task_name: Name of the task\n            input_embedding: Input embedding\n            output_embedding: Target output embedding\n        \"\"\"\n        if task_name not in self.task_types:\n            self.register_task_type(task_name)\n            \n        # Convert to tensors\n        input_tensor = torch.FloatTensor(input_embedding)\n        output_tensor = torch.FloatTensor(output_embedding)\n        \n        # Add to examples\n        self.task_types[task_name][\"examples\"].append((input_tensor, output_tensor))\n        \n        # Update metrics\n        self.task_metrics[task_name][\"examples_seen\"] += 1\n        \n        # Limit number of examples stored\n        if len(self.task_types[task_name][\"examples\"]) > 100:\n            self.task_types[task_name][\"examples\"].pop(0)\n            \n        # Update model with this example\n        self._update_model_with_example(task_name, input_tensor, output_tensor)\n    \n    def _update_model_with_example(self, task_name: str,\n                                 input_tensor: torch.Tensor,\n                                 output_tensor: torch.Tensor) -> None:\n        \"\"\"Update models with a new example.\n        \n        Args:\n            task_name: Name of the task\n            input_tensor: Input embedding tensor\n            output_tensor: Target output embedding tensor\n        \"\"\"\n        # Zero gradients\n        self.shared_optimizer.zero_grad()\n        self.task_optimizers[task_name].zero_grad()\n        \n        # Forward pass through shared model\n        shared_features = self.shared_model(input_tensor.unsqueeze(0))\n        \n        # Forward pass through task-specific adapter\n        task_features = self.task_adapters[task_name](shared_features)\n        \n        # Project back to original space\n        predicted_output = self.task_projectors[task_name](task_features)\n        \n        # Compute loss\n        loss = F.mse_loss(predicted_output.squeeze(0), output_tensor)\n        \n        # Backpropagation\n        loss.backward()\n        \n        # Update parameters\n        self.shared_optimizer.step()\n        self.task_optimizers[task_name].step()\n        \n        # Update metrics\n        self.task_metrics[task_name][\"loss_history\"].append(loss.item())\n        \n        # Calculate cosine similarity as a proxy for accuracy\n        with torch.no_grad():\n            cos_sim = F.cosine_similarity(predicted_output.squeeze(0), output_tensor.unsqueeze(0)).item()\n            self.task_metrics[task_name][\"accuracy_history\"].append(cos_sim)\n            \n            # Check if this is an improvement\n            if len(self.task_metrics[task_name][\"accuracy_history\"]) > 1:\n                prev_best = max(self.task_metrics[task_name][\"accuracy_history\"][:-1])\n                if cos_sim > prev_best:\n                    self.task_metrics[task_name][\"last_improvement\"] = time.time()\n            \n            # Update overall performance metric\n            recent_accuracy = self.task_metrics[task_name][\"accuracy_history\"][-10:]\n            self.task_types[task_name][\"performance\"] = sum(recent_accuracy) / len(recent_accuracy)\n    \n    def process_task(self, task_name: str, input_embedding: np.ndarray) -> np.ndarray:\n        \"\"\"Process an input through a specific task pipeline.\n        \n        Args:\n            task_name: Name of the task\n            input_embedding: Input embedding\n            \n        Returns:\n            Predicted output embedding\n        \"\"\"\n        if task_name not in self.task_types:\n            # Unknown task type, create new adapter\n            self.register_task_type(task_name)\n        \n        # Convert to tensor\n        input_tensor = torch.FloatTensor(input_embedding)\n        \n        # Process through model\n        with torch.no_grad():\n            # Shared features\n            shared_features = self.shared_model(input_tensor.unsqueeze(0))\n            \n            # Task-specific processing\n            task_features = self.task_adapters[task_name](shared_features)\n            \n            # Project to output space\n            output_embedding = self.task_projectors[task_name](task_features)\n            \n            # Convert back to numpy\n            output = output_embedding.squeeze(0).numpy()\n        \n        return output\n    \n    def get_task_similarity(self, task_name1: str, task_name2: str) -> float:\n        \"\"\"Calculate similarity between two tasks based on adapter weights.\n        \n        Args:\n            task_name1: First task name\n            task_name2: Second task name\n            \n        Returns:\n            Similarity score (0-1)\n        \"\"\"\n        if task_name1 not in self.task_adapters or task_name2 not in self.task_adapters:\n            return 0.0\n            \n        # Get adapter parameters as vectors\n        params1 = []\n        params2 = []\n        \n        # Extract parameters\n        for p1, p2 in zip(self.task_adapters[task_name1].parameters(),\n                         self.task_adapters[task_name2].parameters()):\n            params1.append(p1.view(-1))\n            params2.append(p2.view(-1))\n            \n        # Concatenate all parameters\n        params1 = torch.cat(params1)\n        params2 = torch.cat(params2)\n        \n        # Compute cosine similarity\n        similarity = F.cosine_similarity(params1.unsqueeze(0), params2.unsqueeze(0)).item()\n        \n        return similarity\n    \n    def find_most_similar_task(self, input_embedding: np.ndarray) -> str:\n        \"\"\"Find the most similar task for a new input.\n        \n        Args:\n            input_embedding: Input embedding\n            \n        Returns:\n            Most similar task name\n        \"\"\"\n        if not self.task_types:\n            return None\n            \n        # Convert to tensor\n        input_tensor = torch.FloatTensor(input_embedding)\n        \n        # Get shared features\n        with torch.no_grad():\n            shared_features = self.shared_model(input_tensor.unsqueeze(0))\n        \n        # Try each task adapter and measure error on this input\n        task_errors = {}\n        for task_name in self.task_types:\n            # Get examples for this task\n            examples = self.task_types[task_name][\"examples\"]\n            if not examples:\n                continue\n                \n            # Compute error for each example\n            errors = []\n            for ex_input, ex_output in examples:\n                # Process input with shared model\n                ex_shared = self.shared_model(ex_input.unsqueeze(0))\n                \n                # Compute feature similarity\n                similarity = F.cosine_similarity(shared_features, ex_shared).item()\n                errors.append(1.0 - similarity)  # Convert to error\n                \n            # Average error for this task\n            if errors:\n                task_errors[task_name] = sum(errors) / len(errors)\n        \n        if not task_errors:\n            return list(self.task_types.keys())[0]  # Return first task if no errors computed\n            \n        # Return task with lowest error\n        return min(task_errors.items(), key=lambda x: x[1])[0]\n    \n    def transfer_knowledge(self, source_task: str, target_task: str, strength: float = 0.3) -> None:\n        \"\"\"Transfer knowledge from source task to target task.\n        \n        Args:\n            source_task: Source task name\n            target_task: Target task name\n            strength: Strength of knowledge transfer (0-1)\n        \"\"\"\n        if source_task not in self.task_adapters or target_task not in self.task_adapters:\n            return\n            \n        # Skip if tasks are identical\n        if source_task == target_task:\n            return\n            \n        # Get source and target adapters\n        source_adapter = self.task_adapters[source_task]\n        target_adapter = self.task_adapters[target_task]\n        \n        # Transfer knowledge through parameter interpolation\n        with torch.no_grad():\n            for source_param, target_param in zip(source_adapter.parameters(), \n                                                target_adapter.parameters()):\n                # Interpolate parameters\n                new_param = (1 - strength) * target_param + strength * source_param\n                target_param.copy_(new_param)\n        \n        # Do the same for projectors\n        source_projector = self.task_projectors[source_task]\n        target_projector = self.task_projectors[target_task]\n        \n        with torch.no_grad():\n            for source_param, target_param in zip(source_projector.parameters(),\n                                                target_projector.parameters()):\n                # Interpolate parameters\n                new_param = (1 - strength) * target_param + strength * source_param\n                target_param.copy_(new_param)\n    \n    def get_task_metrics(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get performance metrics for all tasks.\n        \n        Returns:\n            Dictionary of task metrics\n        \"\"\"\n        metrics = {}\n        \n        for task_name, task_data in self.task_types.items():\n            task_metrics = self.task_metrics[task_name]\n            \n            # Calculate recent performance\n            recent_acc = task_metrics[\"accuracy_history\"][-10:] if task_metrics[\"accuracy_history\"] else []\n            recent_perf = sum(recent_acc) / len(recent_acc) if recent_acc else 0.0\n            \n            # Determine if task is improving\n            improving = False\n            if len(task_metrics[\"accuracy_history\"]) >= 10:\n                first_half = task_metrics[\"accuracy_history\"][-10:-5]\n                second_half = task_metrics[\"accuracy_history\"][-5:]\n                \n                if sum(second_half) / 5 > sum(first_half) / 5:\n                    improving = True\n            \n            # Collect metrics\n            metrics[task_name] = {\n                \"examples_seen\": task_metrics[\"examples_seen\"],\n                \"current_performance\": recent_perf,\n                \"registered_time\": task_data[\"timestamp\"],\n                \"last_improvement\": task_metrics[\"last_improvement\"],\n                \"improving\": improving,\n                \"difficulty\": task_data[\"difficulty\"]\n            }\n            \n            # Compute task similarities\n            similarities = {}\n            for other_task in self.task_types:\n                if other_task != task_name:\n                    similarity = self.get_task_similarity(task_name, other_task)\n                    similarities[other_task] = similarity\n                    \n            metrics[task_name][\"task_similarities\"] = similarities\n        \n        return metrics\n\n# Causal inference system for tool selection\nclass CausalToolSelectionModel:\n    \"\"\"Causal inference system for understanding tool cause-effect relationships.\"\"\"\n    \n    def __init__(self, embedding_dim: int = 768):\n        \"\"\"Initialize the causal inference system.\n        \n        Args:\n            embedding_dim: Dimension of embeddings\n        \"\"\"\n        self.embedding_dim = embedding_dim\n        \n        # Causal graph\n        self.graph = nx.DiGraph() if HAVE_NETWORKX else None\n        \n        # Tool variables (nodes in the graph)\n        self.tool_nodes = set()\n        \n        # Context variables\n        self.context_nodes = set()\n        \n        # Structural equation models\n        self.models = {}\n        \n        # Intervention effects\n        self.interventions = {}\n        \n        # Counterfactual cache\n        self.counterfactuals = {}\n        \n        # Neural estimator for complex relationships\n        self.neural_estimator = nn.Sequential(\n            nn.Linear(embedding_dim * 2, embedding_dim),\n            nn.LayerNorm(embedding_dim),\n            nn.ReLU(),\n            nn.Linear(embedding_dim, embedding_dim // 2),\n            nn.LayerNorm(embedding_dim // 2),\n            nn.ReLU(),\n            nn.Linear(embedding_dim // 2, 1),\n            nn.Sigmoid()\n        )\n        \n        # Optimizer\n        self.optimizer = torch.optim.Adam(self.neural_estimator.parameters(), lr=0.001)\n    \n    def add_tool_node(self, tool_name: str):\n        \"\"\"Add a tool as a node in the causal graph.\n        \n        Args:\n            tool_name: Name of the tool\n        \"\"\"\n        self.tool_nodes.add(tool_name)\n        if self.graph is not None:\n            self.graph.add_node(tool_name, type=\"tool\")\n    \n    def add_context_node(self, context_name: str):\n        \"\"\"Add a context variable as a node.\n        \n        Args:\n            context_name: Name of the context variable\n        \"\"\"\n        self.context_nodes.add(context_name)\n        if self.graph is not None:\n            self.graph.add_node(context_name, type=\"context\")\n    \n    def add_causal_link(self, cause: str, effect: str, strength: float = 0.5):\n        \"\"\"Add a causal link between nodes.\n        \n        Args:\n            cause: Name of the cause node\n            effect: Name of the effect node\n            strength: Strength of the causal relationship (0-1)\n        \"\"\"\n        if self.graph is not None:\n            self.graph.add_edge(cause, effect, weight=strength)\n    \n    def observe(self, query_embedding: np.ndarray, context: Dict[str, Any],\n             tool_sequence: List[str], outcomes: List[Dict[str, Any]]):\n        \"\"\"Record an observation of tool usage and outcomes.\n        \n        Args:\n            query_embedding: Embedding of the query\n            context: Context variables\n            tool_sequence: Sequence of tools used\n            outcomes: Outcomes of each tool (success, result, etc.)\n        \"\"\"\n        # Convert embeddings to tensors\n        query_tensor = torch.FloatTensor(query_embedding)\n        \n        # Process each tool in the sequence\n        for i, (tool, outcome) in enumerate(zip(tool_sequence, outcomes)):\n            # Add tool if not already in graph\n            if tool not in self.tool_nodes:\n                self.add_tool_node(tool)\n            \n            # Add context variables\n            for ctx_name, ctx_value in context.items():\n                ctx_key = f\"{ctx_name}:{ctx_value}\" if isinstance(ctx_value, (str, int, bool)) else ctx_name\n                if ctx_key not in self.context_nodes:\n                    self.add_context_node(ctx_key)\n                \n                # Add causal link from context to tool\n                self.add_causal_link(ctx_key, tool, 0.3)  # Initial strength estimate\n            \n            # Add causal links between tools in sequence\n            if i > 0:\n                prev_tool = tool_sequence[i-1]\n                prev_outcome = outcomes[i-1]\n                \n                # Link strength based on previous success\n                strength = 0.7 if prev_outcome.get(\"success\", False) else 0.2\n                self.add_causal_link(prev_tool, tool, strength)\n                \n                # Update neural estimator\n                if i > 0 and hasattr(prev_outcome, \"embedding\") and hasattr(outcome, \"embedding\"):\n                    # Training example for neural estimator\n                    prev_embed = torch.FloatTensor(prev_outcome[\"embedding\"])\n                    curr_embed = torch.FloatTensor(outcome[\"embedding\"])\n                    \n                    combined = torch.cat([prev_embed, curr_embed])\n                    target = torch.FloatTensor([strength])\n                    \n                    # Update neural estimator\n                    self.optimizer.zero_grad()\n                    pred = self.neural_estimator(combined.unsqueeze(0))\n                    loss = F.mse_loss(pred, target)\n                    loss.backward()\n                    self.optimizer.step()\n    \n    def infer_effects(self, intervention_tool: str) -> Dict[str, float]:\n        \"\"\"Infer the effects of using a specific tool.\n        \n        Args:\n            intervention_tool: The tool to intervene with\n            \n        Returns:\n            Dictionary of effects on other tools/outcomes\n        \"\"\"\n        if self.graph is None:\n            return {}\n            \n        # Use do-calculus to determine causal effects\n        effects = {}\n        \n        # Create a modified graph for the intervention\n        intervention_graph = self.graph.copy()\n        \n        # Remove incoming edges to the intervention tool (do-operator)\n        for pred in list(self.graph.predecessors(intervention_tool)):\n            intervention_graph.remove_edge(pred, intervention_tool)\n        \n        # Compute effect on each tool\n        for tool in self.tool_nodes:\n            if tool == intervention_tool:\n                continue\n                \n            # Check if there's a path from intervention to this tool\n            if nx.has_path(intervention_graph, intervention_tool, tool):\n                # Compute causal effect strength using path weights\n                paths = list(nx.all_simple_paths(intervention_graph, intervention_tool, tool))\n                \n                effect = 0.0\n                for path in paths:\n                    # Calculate path strength as product of edge weights\n                    path_strength = 1.0\n                    for i in range(len(path) - 1):\n                        path_strength *= intervention_graph[path[i]][path[i+1]][\"weight\"]\n                    \n                    effect += path_strength\n                \n                # Normalize for multiple paths\n                if len(paths) > 0:\n                    effect /= len(paths)\n                    \n                effects[tool] = effect\n        \n        # Cache result\n        self.interventions[intervention_tool] = effects\n        \n        return effects\n    \n    def estimate_counterfactual(self, observed_tools: List[str], \n                              alternative_tool: str) -> float:\n        \"\"\"Estimate the outcome difference if an alternative tool had been used.\n        \n        Args:\n            observed_tools: The tools that were actually used\n            alternative_tool: The alternative tool to consider\n            \n        Returns:\n            Estimated improvement (positive) or decline (negative) in outcome\n        \"\"\"\n        # Use nested counterfactual estimation\n        key = (tuple(observed_tools), alternative_tool)\n        \n        if key in self.counterfactuals:\n            return self.counterfactuals[key]\n        \n        if self.graph is None or not observed_tools:\n            return 0.0\n            \n        # Find the position to replace\n        best_pos = 0\n        best_effect = -float('inf')\n        \n        for i in range(len(observed_tools)):\n            # Consider replacing the tool at position i\n            tools_copy = observed_tools.copy()\n            original_tool = tools_copy[i]\n            tools_copy[i] = alternative_tool\n            \n            # Estimate effect of this change\n            effect = 0.0\n            \n            # Effect from replacing the original tool\n            if original_tool in self.interventions:\n                effect -= sum(self.interventions[original_tool].values())\n            \n            # Effect from using the alternative tool\n            if alternative_tool in self.interventions:\n                effect += sum(self.interventions[alternative_tool].values())\n            \n            # Check if this is the best position\n            if effect > best_effect:\n                best_effect = effect\n                best_pos = i\n        \n        # Estimate the counterfactual difference\n        counterfactual = best_effect / len(observed_tools)\n        \n        # Cache the result\n        self.counterfactuals[key] = counterfactual\n        \n        return counterfactual\n\n# Advanced Graph Neural Network for modeling tool relationships\nclass ToolRelationshipGNN(nn.Module):\n    \"\"\"Graph Neural Network for modeling relationships between tools.\"\"\"\n    \n    def __init__(self, embedding_dim: int, hidden_dim: int, num_tools: int):\n        \"\"\"Initialize the GNN with appropriate dimensions.\n        \n        Args:\n            embedding_dim: Dimension of input embeddings\n            hidden_dim: Dimension of hidden layers\n            num_tools: Number of tools in the system\n        \"\"\"\n        super(ToolRelationshipGNN, self).__init__()\n        \n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.num_tools = num_tools\n        \n        # Node embedding layers\n        self.node_encoder = nn.Sequential(\n            nn.Linear(embedding_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU()\n        )\n        \n        # Edge embedding layers\n        self.edge_encoder = nn.Sequential(\n            nn.Linear(embedding_dim * 2, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU()\n        )\n        \n        # Message passing layers\n        self.message_mlp = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n        \n        # Node update layers\n        self.node_update = nn.GRUCell(hidden_dim, hidden_dim)\n        \n        # Output projection\n        self.output_projection = nn.Linear(hidden_dim, embedding_dim)\n        \n        # Attention mechanism for node aggregation\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, 1)\n        )\n    \n    def forward(self, node_embeddings: torch.Tensor, adjacency_matrix: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward pass through the GNN.\n        \n        Args:\n            node_embeddings: Tool embeddings tensor [num_tools, embedding_dim]\n            adjacency_matrix: Tool relationship adjacency matrix [num_tools, num_tools]\n            \n        Returns:\n            Updated node embeddings\n        \"\"\"\n        batch_size = node_embeddings.shape[0]\n        \n        # Initial node encoding\n        node_hidden = self.node_encoder(node_embeddings)  # [batch, num_tools, hidden_dim]\n        \n        # Message passing (3 rounds)\n        for _ in range(3):\n            # Compute messages for each edge\n            messages = []\n            attention_weights = []\n            \n            for i in range(self.num_tools):\n                for j in range(self.num_tools):\n                    # Only consider edges that exist in the adjacency matrix\n                    if adjacency_matrix[i, j] > 0:\n                        # Combine source and destination node features\n                        edge_features = torch.cat([node_hidden[:, i], node_hidden[:, j]], dim=1)\n                        message = self.message_mlp(edge_features)\n                        messages.append((j, message))  # Message to node j\n                        \n                        # Compute attention weight\n                        attn_input = torch.cat([node_hidden[:, j], message], dim=1)\n                        weight = self.attention(attn_input)\n                        attention_weights.append((j, weight))\n            \n            # Aggregate messages for each node using attention\n            aggregated_messages = torch.zeros(batch_size, self.num_tools, self.hidden_dim, \n                                            device=node_embeddings.device)\n            \n            # Group messages by destination node\n            node_messages = defaultdict(list)\n            node_weights = defaultdict(list)\n            \n            for j, message in messages:\n                node_messages[j].append(message)\n                \n            for j, weight in attention_weights:\n                node_weights[j].append(weight)\n                \n            # Apply attention for each node\n            for j in range(self.num_tools):\n                if j in node_messages:\n                    stacked_messages = torch.stack(node_messages[j], dim=1)  # [batch, num_msgs, hidden]\n                    stacked_weights = torch.stack(node_weights[j], dim=1)  # [batch, num_msgs, 1]\n                    \n                    # Apply softmax to get attention distribution\n                    normalized_weights = F.softmax(stacked_weights, dim=1)\n                    \n                    # Weighted sum of messages\n                    node_message = torch.sum(stacked_messages * normalized_weights, dim=1)\n                    aggregated_messages[:, j] = node_message\n            \n            # Update node states using GRU\n            node_hidden_reshaped = node_hidden.view(batch_size * self.num_tools, self.hidden_dim)\n            aggregated_messages_reshaped = aggregated_messages.view(batch_size * self.num_tools, self.hidden_dim)\n            \n            updated_hidden = self.node_update(aggregated_messages_reshaped, node_hidden_reshaped)\n            node_hidden = updated_hidden.view(batch_size, self.num_tools, self.hidden_dim)\n            \n        # Project back to embedding space\n        output_embeddings = self.output_projection(node_hidden)\n        \n        return output_embeddings\n        \n# Enhanced Meta-Learning System\nclass MetaLearningOptimizer:\n    \"\"\"Meta-learning system that learns to generalize across different types of tasks.\"\"\"\n    \n    def __init__(self, embedding_dim: int, num_tools: int, learning_rate: float = 0.001):\n        \"\"\"Initialize the meta-learning optimizer.\n        \n        Args:\n            embedding_dim: Dimension of embeddings\n            num_tools: Number of tools in the system\n            learning_rate: Learning rate for meta-updates\n        \"\"\"\n        self.embedding_dim = embedding_dim\n        self.num_tools = num_tools\n        self.learning_rate = learning_rate\n        \n        # Task type embeddings\n        self.task_embeddings = {}\n        \n        # Tool parameter embeddings\n        self.tool_parameters = nn.ParameterDict()\n        \n        # Meta-network for adaptation\n        self.meta_network = nn.Sequential(\n            nn.Linear(embedding_dim * 2, embedding_dim),\n            nn.LayerNorm(embedding_dim),\n            nn.ReLU(),\n            nn.Linear(embedding_dim, embedding_dim)\n        )\n        \n        # Optimizer for meta-parameters\n        self.optimizer = torch.optim.Adam(self.meta_network.parameters(), lr=learning_rate)\n        \n        # Task buffers for meta-learning\n        self.task_buffers = defaultdict(list)\n        self.max_buffer_size = 100\n        \n    def register_tool(self, tool_name: str, initial_embedding: np.ndarray):\n        \"\"\"Register a tool with the meta-learning system.\n        \n        Args:\n            tool_name: Name of the tool\n            initial_embedding: Initial embedding for the tool\n        \"\"\"\n        self.tool_parameters[tool_name] = nn.Parameter(\n            torch.FloatTensor(initial_embedding), \n            requires_grad=True\n        )\n        \n    def add_task_example(self, task_type: str, query_embedding: np.ndarray, \n                       selected_tool: str, success: bool, reward: float):\n        \"\"\"Add an example to a task buffer for meta-learning.\n        \n        Args:\n            task_type: Type of task (e.g., \"search\", \"explanation\")\n            query_embedding: Embedding of the query\n            selected_tool: Name of the selected tool\n            success: Whether the tool was successful\n            reward: The reward received\n        \"\"\"\n        # Convert to tensor\n        query_tensor = torch.FloatTensor(query_embedding)\n        \n        # Add to task buffer\n        self.task_buffers[task_type].append({\n            \"query\": query_tensor,\n            \"tool\": selected_tool,\n            \"success\": success,\n            \"reward\": reward\n        })\n        \n        # Limit buffer size\n        if len(self.task_buffers[task_type]) > self.max_buffer_size:\n            self.task_buffers[task_type].pop(0)\n        \n    def meta_update(self):\n        \"\"\"Perform a meta-update step to improve adaptation capability.\"\"\"\n        if not self.task_buffers:\n            return\n            \n        # Sample a batch of tasks\n        sampled_tasks = random.sample(list(self.task_buffers.keys()), \n                                    min(5, len(self.task_buffers)))\n        \n        meta_loss = 0.0\n        \n        for task_type in sampled_tasks:\n            # Skip tasks with too few examples\n            if len(self.task_buffers[task_type]) < 5:\n                continue\n                \n            # Sample examples from this task\n            examples = random.sample(self.task_buffers[task_type], \n                                   min(10, len(self.task_buffers[task_type])))\n            \n            # Compute task embedding if not already computed\n            if task_type not in self.task_embeddings:\n                # Average query embeddings as task embedding\n                query_tensors = [ex[\"query\"] for ex in examples]\n                task_embedding = torch.stack(query_tensors).mean(dim=0)\n                self.task_embeddings[task_type] = task_embedding\n            \n            # Create adapted tool parameters for this task\n            adapted_params = {}\n            for tool_name, param in self.tool_parameters.items():\n                # Concatenate task embedding with tool parameter\n                adaptation_input = torch.cat([self.task_embeddings[task_type], param])\n                \n                # Generate adaptation\n                adaptation = self.meta_network(adaptation_input.unsqueeze(0)).squeeze(0)\n                \n                # Apply adaptation\n                adapted_params[tool_name] = param + adaptation\n            \n            # Compute loss for this task\n            task_loss = 0.0\n            for example in examples:\n                query = example[\"query\"]\n                selected_tool = example[\"tool\"]\n                reward = example[\"reward\"]\n                \n                # Compute scores for all tools\n                scores = {}\n                for tool_name, param in adapted_params.items():\n                    score = torch.dot(query, param) / (query.norm() * param.norm())\n                    scores[tool_name] = score\n                \n                # Convert to probability distribution\n                logits = torch.stack(list(scores.values()))\n                probs = F.softmax(logits, dim=0)\n                \n                # Get index of selected tool\n                tool_idx = list(scores.keys()).index(selected_tool)\n                \n                # Negative log likelihood weighted by reward\n                nll = -torch.log(probs[tool_idx])\n                task_loss += nll * (1.0 - reward)  # Lower loss for high rewards\n            \n            # Add to meta loss\n            meta_loss += task_loss / len(examples)\n        \n        # Normalize by number of tasks\n        meta_loss /= len(sampled_tasks)\n        \n        # Update meta-parameters\n        self.optimizer.zero_grad()\n        meta_loss.backward()\n        self.optimizer.step()\n        \n        return meta_loss.item()\n    \n    def get_adapted_embeddings(self, task_type: str) -> Dict[str, np.ndarray]:\n        \"\"\"Get task-adapted embeddings for tools.\n        \n        Args:\n            task_type: Type of task\n            \n        Returns:\n            Dictionary of adapted tool embeddings\n        \"\"\"\n        # Return original embeddings if task type is unknown\n        if task_type not in self.task_embeddings:\n            return {name: param.detach().numpy() for name, param in self.tool_parameters.items()}\n        \n        # Create adapted embeddings\n        adapted_embeddings = {}\n        for tool_name, param in self.tool_parameters.items():\n            # Concatenate task embedding with tool parameter\n            adaptation_input = torch.cat([self.task_embeddings[task_type], param])\n            \n            # Generate adaptation\n            adaptation = self.meta_network(adaptation_input.unsqueeze(0)).squeeze(0)\n            \n            # Apply adaptation\n            adapted_embeddings[tool_name] = (param + adaptation).detach().numpy()\n        \n        return adapted_embeddings\n\n\n@dataclass\nclass ToolUsageRecord:\n    \"\"\"Record of a tool usage for optimization.\"\"\"\n    query: str\n    tool_name: str\n    execution_time: float\n    token_usage: Dict[str, int]\n    success: bool\n    timestamp: float\n\n\nclass ToolUsageTracker:\n    \"\"\"Tracks tool usage for optimization.\"\"\"\n    \n    def __init__(self, max_records: int = 10000):\n        \"\"\"\n        Initialize the tool usage tracker.\n        \n        Args:\n            max_records: Maximum number of records to store\n        \"\"\"\n        self.records = deque(maxlen=max_records)\n    \n    def add_record(self, record: ToolUsageRecord) -> None:\n        \"\"\"Add a record to the tracker.\"\"\"\n        self.records.append(record)\n    \n    def get_tool_stats(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Get statistics about tool usage.\n        \n        Returns:\n            Dictionary of tool statistics\n        \"\"\"\n        stats = {}\n        \n        # Group by tool\n        for record in self.records:\n            if record.tool_name not in stats:\n                stats[record.tool_name] = {\n                    \"count\": 0,\n                    \"success_count\": 0,\n                    \"total_time\": 0,\n                    \"token_usage\": {\"prompt\": 0, \"completion\": 0, \"total\": 0},\n                }\n            \n            stats[record.tool_name][\"count\"] += 1\n            if record.success:\n                stats[record.tool_name][\"success_count\"] += 1\n            stats[record.tool_name][\"total_time\"] += record.execution_time\n            \n            # Update token usage\n            for key, value in record.token_usage.items():\n                stats[record.tool_name][\"token_usage\"][key] += value\n        \n        # Compute derived metrics\n        for tool_name, tool_stats in stats.items():\n            tool_stats[\"success_rate\"] = tool_stats[\"success_count\"] / tool_stats[\"count\"] if tool_stats[\"count\"] > 0 else 0\n            tool_stats[\"avg_time\"] = tool_stats[\"total_time\"] / tool_stats[\"count\"] if tool_stats[\"count\"] > 0 else 0\n            \n            for key in tool_stats[\"token_usage\"]:\n                tool_stats[f\"avg_{key}_tokens\"] = tool_stats[\"token_usage\"][key] / tool_stats[\"count\"] if tool_stats[\"count\"] > 0 else 0\n        \n        return stats\n\n\nclass ToolSelectionOptimizer:\n    \"\"\"\n    Optimizes tool selection based on user queries and context.\n    Uses reinforcement learning to improve tool selection over time.\n    \"\"\"\n    \n    def __init__(\n        self,\n        tool_registry: Any,\n        data_dir: str = \"./data/rl\",\n        enable_rl: bool = True,\n        model_update_interval: int = 100,\n        embedding_model_name: str = \"all-MiniLM-L6-v2\",\n        embedding_cache_size: int = 1000,\n    ):\n        \"\"\"\n        Initialize the tool selection optimizer.\n        \n        Args:\n            tool_registry: Registry containing available tools\n            data_dir: Directory to store data and models\n            enable_rl: Whether to enable reinforcement learning\n            model_update_interval: How often to update models (in observations)\n            embedding_model_name: Name of the sentence embedding model\n            embedding_cache_size: Size of the embedding cache\n        \"\"\"\n        self.tool_registry = tool_registry\n        self.data_dir = data_dir\n        self.enable_rl = enable_rl\n        self.model_update_interval = model_update_interval\n        \n        # Create data directory\n        os.makedirs(data_dir, exist_ok=True)\n        \n        # Initialize tool usage tracker\n        self.tracker = ToolUsageTracker()\n        \n        # Initialize embedding model if available\n        self.embedding_model = None\n        self.embedding_cache = {}\n        self.embedding_cache_keys = deque(maxlen=embedding_cache_size)\n        \n        if HAVE_SENTENCE_TRANSFORMERS and enable_rl:\n            try:\n                self.embedding_model = SentenceTransformer(embedding_model_name)\n            except Exception as e:\n                print(f\"Warning: Failed to load embedding model: {e}\")\n        \n        # Initialize RL system if enabled\n        self.rl_system = None\n        if enable_rl:\n            # Define a simple context evaluator\n            def context_evaluator(context):\n                # This is a placeholder - in a real system, we'd evaluate the quality\n                # based on metrics like response coherence, success rate, etc.\n                return 0.5\n            \n            # Create RL system\n            self.rl_system = ToolSelectionGRPO(\n                tool_registry=tool_registry,\n                context_evaluator=context_evaluator,\n                update_interval=model_update_interval,\n            )\n        \n        # Load existing models and data if available\n        self._load_data()\n    \n    def select_tool(self, query: str, context: Dict[str, Any], visualizer=None) -> str:\n        \"\"\"\n        Select the best tool to use for a given query.\n        \n        Args:\n            query: User query\n            context: Conversation context\n            visualizer: Optional visualizer to display the selection process\n            \n        Returns:\n            Name of the selected tool\n        \"\"\"\n        # If RL is not enabled, use default selection logic\n        if not self.enable_rl or self.rl_system is None:\n            return self._default_tool_selection(query, context)\n        \n        # Use RL system to select tool\n        try:\n            return self.rl_system.select_tool(query, context, visualizer=visualizer)\n        except Exception as e:\n            print(f\"Error in RL tool selection: {e}\")\n            return self._default_tool_selection(query, context)\n    \n    def record_tool_usage(\n        self,\n        query: str,\n        tool_name: str,\n        execution_time: float,\n        token_usage: Dict[str, int],\n        success: bool,\n        context: Optional[Dict[str, Any]] = None,\n        result: Optional[Any] = None,\n    ) -> None:\n        \"\"\"\n        Record tool usage for optimization.\n        \n        Args:\n            query: User query\n            tool_name: Name of the tool used\n            execution_time: Time taken to execute the tool\n            token_usage: Token usage information\n            success: Whether the tool usage was successful\n            context: Conversation context (for RL)\n            result: Result of the tool usage (for RL)\n        \"\"\"\n        # Create and add record\n        record = ToolUsageRecord(\n            query=query,\n            tool_name=tool_name,\n            execution_time=execution_time,\n            token_usage=token_usage,\n            success=success,\n            timestamp=time.time(),\n        )\n        self.tracker.add_record(record)\n        \n        # Update RL system if enabled\n        if self.enable_rl and self.rl_system is not None and context is not None:\n            try:\n                # Find the agent that made this selection\n                for agent_id in self.rl_system.current_episode:\n                    if agent_id in self.rl_system.current_episode and self.rl_system.current_episode[agent_id]:\n                        # Observe the result\n                        self.rl_system.observe_result(\n                            agent_id=agent_id,\n                            result=result,\n                            context=context,\n                            done=True,\n                        )\n            except Exception as e:\n                print(f\"Error updating RL system: {e}\")\n        \n        # Save data periodically\n        if len(self.tracker.records) % 50 == 0:\n            self._save_data()\n    \n    def get_tool_recommendations(self, query: str) -> List[Tuple[str, float]]:\n        \"\"\"\n        Get tool recommendations for a query with confidence scores.\n        \n        Args:\n            query: User query\n            \n        Returns:\n            List of (tool_name, confidence) tuples\n        \"\"\"\n        # Get query embedding\n        if self.embedding_model is not None:\n            try:\n                query_embedding = self._get_embedding(query)\n                \n                # Get all tools and their embeddings\n                tools = self.tool_registry.get_all_tools()\n                tool_scores = []\n                \n                for tool in tools:\n                    # Get tool description embedding\n                    tool_desc = tool.description\n                    tool_embedding = self._get_embedding(tool_desc)\n                    \n                    # Compute similarity score\n                    similarity = self._cosine_similarity(query_embedding, tool_embedding)\n                    tool_scores.append((tool.name, similarity))\n                \n                # Sort by score\n                tool_scores.sort(key=lambda x: x[1], reverse=True)\n                return tool_scores\n            \n            except Exception as e:\n                print(f\"Error computing tool recommendations: {e}\")\n        \n        # Fallback to default ordering\n        return [(tool, 0.5) for tool in self.tool_registry.get_all_tool_names()]\n    \n    def update_model(self) -> Dict[str, Any]:\n        \"\"\"\n        Manually trigger a model update.\n        \n        Returns:\n            Dictionary of update metrics\n        \"\"\"\n        if not self.enable_rl or self.rl_system is None:\n            return {\"status\": \"RL not enabled\"}\n        \n        try:\n            metrics = self.rl_system.update()\n            \n            # Save updated model\n            self.rl_system.save()\n            \n            return {\"status\": \"success\", \"metrics\": metrics}\n        \n        except Exception as e:\n            return {\"status\": \"error\", \"message\": str(e)}\n    \n    def _default_tool_selection(self, query: str, context: Dict[str, Any]) -> str:\n        \"\"\"\n        Default tool selection logic when RL is not available.\n        \n        Args:\n            query: User query\n            context: Conversation context\n            \n        Returns:\n            Name of the selected tool\n        \"\"\"\n        # Use a simple rule-based approach as fallback\n        tools = self.tool_registry.get_all_tool_names()\n        \n        # Look for keywords in the query\n        query_lower = query.lower()\n        \n        if \"file\" in query_lower and \"read\" in query_lower:\n            for tool in tools:\n                if tool.lower() == \"view\":\n                    return tool\n        \n        if \"search\" in query_lower or \"find\" in query_lower:\n            for tool in tools:\n                if \"grep\" in tool.lower():\n                    return tool\n        \n        if \"execute\" in query_lower or \"run\" in query_lower:\n            for tool in tools:\n                if tool.lower() == \"bash\":\n                    return tool\n        \n        if \"edit\" in query_lower or \"change\" in query_lower:\n            for tool in tools:\n                if tool.lower() == \"edit\":\n                    return tool\n        \n        # Default to the first tool\n        return tools[0] if tools else \"View\"\n    \n    def _get_embedding(self, text: str) -> np.ndarray:\n        \"\"\"\n        Get embedding for text with caching.\n        \n        Args:\n            text: Text to embed\n            \n        Returns:\n            Embedding vector\n        \"\"\"\n        if text in self.embedding_cache:\n            return self.embedding_cache[text]\n        \n        if self.embedding_model is None:\n            raise ValueError(\"Embedding model not available\")\n        \n        # Generate embedding\n        embedding = self.embedding_model.encode(text, show_progress_bar=False)\n        \n        # Cache embedding\n        if len(self.embedding_cache_keys) >= self.embedding_cache_keys.maxlen:\n            # Remove oldest key if cache is full\n            oldest_key = self.embedding_cache_keys.popleft()\n            self.embedding_cache.pop(oldest_key, None)\n        \n        self.embedding_cache[text] = embedding\n        self.embedding_cache_keys.append(text)\n        \n        return embedding\n    \n    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:\n        \"\"\"\n        Compute cosine similarity between two vectors.\n        \n        Args:\n            a: First vector\n            b: Second vector\n            \n        Returns:\n            Cosine similarity score\n        \"\"\"\n        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n    \n    def _save_data(self) -> None:\n        \"\"\"Save data and models.\"\"\"\n        try:\n            # Create data file\n            data_path = os.path.join(self.data_dir, \"tool_usage_data.json\")\n            \n            # Convert records to serializable format\n            records_data = []\n            for record in self.tracker.records:\n                records_data.append({\n                    \"query\": record.query,\n                    \"tool_name\": record.tool_name,\n                    \"execution_time\": record.execution_time,\n                    \"token_usage\": record.token_usage,\n                    \"success\": record.success,\n                    \"timestamp\": record.timestamp,\n                })\n            \n            # Write data\n            with open(data_path, \"w\") as f:\n                json.dump(records_data, f)\n            \n            # Save RL model if available\n            if self.enable_rl and self.rl_system is not None:\n                self.rl_system.save()\n        \n        except Exception as e:\n            print(f\"Error saving optimizer data: {e}\")\n    \n    def _load_data(self) -> None:\n        \"\"\"Load data and models.\"\"\"\n        try:\n            # Load data file\n            data_path = os.path.join(self.data_dir, \"tool_usage_data.json\")\n            \n            if os.path.exists(data_path):\n                with open(data_path, \"r\") as f:\n                    records_data = json.load(f)\n                \n                # Convert to records\n                for record_data in records_data:\n                    record = ToolUsageRecord(\n                        query=record_data[\"query\"],\n                        tool_name=record_data[\"tool_name\"],\n                        execution_time=record_data[\"execution_time\"],\n                        token_usage=record_data[\"token_usage\"],\n                        success=record_data[\"success\"],\n                        timestamp=record_data[\"timestamp\"],\n                    )\n                    self.tracker.add_record(record)\n            \n            # Load RL model if available\n            if self.enable_rl and self.rl_system is not None:\n                try:\n                    self.rl_system.load()\n                except Exception as e:\n                    print(f\"Error loading RL model: {e}\")\n        \n        except Exception as e:\n            print(f\"Error loading optimizer data: {e}\")\n\n\nclass ToolSelectionManager:\n    \"\"\"\n    Manages tool selection for Claude Code Python.\n    Provides an interface for selecting tools and recording usage.\n    \"\"\"\n    \n    def __init__(\n        self,\n        tool_registry: Any,\n        enable_optimization: bool = True,\n        data_dir: str = \"./data/rl\",\n    ):\n        \"\"\"\n        Initialize the tool selection manager.\n        \n        Args:\n            tool_registry: Registry containing available tools\n            enable_optimization: Whether to enable optimization\n            data_dir: Directory to store data and models\n        \"\"\"\n        self.tool_registry = tool_registry\n        self.enable_optimization = enable_optimization\n        \n        # Initialize optimizer if enabled\n        self.optimizer = None\n        if enable_optimization:\n            self.optimizer = ToolSelectionOptimizer(\n                tool_registry=tool_registry,\n                data_dir=data_dir,\n                enable_rl=True,\n            )\n    \n    def select_tool(self, query: str, context: Dict[str, Any]) -> str:\n        \"\"\"\n        Select the best tool to use for a given query.\n        \n        Args:\n            query: User query\n            context: Conversation context\n            \n        Returns:\n            Name of the selected tool\n        \"\"\"\n        if self.optimizer is not None:\n            return self.optimizer.select_tool(query, context)\n        \n        # Use default selection if optimizer is not available\n        return self._default_selection(query)\n    \n    def record_tool_usage(\n        self,\n        query: str,\n        tool_name: str,\n        execution_time: float,\n        token_usage: Dict[str, int],\n        success: bool,\n        context: Optional[Dict[str, Any]] = None,\n        result: Optional[Any] = None,\n    ) -> None:\n        \"\"\"\n        Record tool usage for optimization.\n        \n        Args:\n            query: User query\n            tool_name: Name of the tool used\n            execution_time: Time taken to execute the tool\n            token_usage: Token usage information\n            success: Whether the tool usage was successful\n            context: Conversation context (for RL)\n            result: Result of the tool usage (for RL)\n        \"\"\"\n        if self.optimizer is not None:\n            self.optimizer.record_tool_usage(\n                query=query,\n                tool_name=tool_name,\n                execution_time=execution_time,\n                token_usage=token_usage,\n                success=success,\n                context=context,\n                result=result,\n            )\n    \n    def get_tool_recommendations(self, query: str) -> List[Tuple[str, float]]:\n        \"\"\"\n        Get tool recommendations for a query with confidence scores.\n        \n        Args:\n            query: User query\n            \n        Returns:\n            List of (tool_name, confidence) tuples\n        \"\"\"\n        if self.optimizer is not None:\n            return self.optimizer.get_tool_recommendations(query)\n        \n        # Return default recommendations if optimizer is not available\n        return [(tool, 0.5) for tool in self.tool_registry.get_all_tool_names()]\n    \n    def _default_selection(self, query: str) -> str:\n        \"\"\"\n        Default tool selection logic when optimization is not available.\n        \n        Args:\n            query: User query\n            \n        Returns:\n            Name of the selected tool\n        \"\"\"\n        # Use a simple rule-based approach as fallback\n        tools = self.tool_registry.get_all_tool_names()\n        \n        # Default to the first tool\n        return tools[0] if tools else \"View\""}
{"type": "source_file", "path": "claude_code/lib/ui/tool_visualizer.py", "content": "#!/usr/bin/env python3\n# claude_code/lib/ui/tool_visualizer.py\n\"\"\"Real-time tool execution visualization.\"\"\"\n\nimport logging\nimport time\nimport json\nfrom typing import Dict, List, Any, Optional\n\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn\nfrom rich.table import Table\nfrom rich.box import ROUNDED\nfrom rich.text import Text\nfrom rich.live import Live\nfrom rich.layout import Layout\nfrom rich.syntax import Syntax\n\nfrom ..tools.base import ToolResult\n\nlogger = logging.getLogger(__name__)\n\n\nclass ToolCallVisualizer:\n    \"\"\"Visualizes tool calls in real-time.\"\"\"\n    \n    def __init__(self, console: Console):\n        \"\"\"Initialize the tool call visualizer.\n        \n        Args:\n            console: Rich console instance\n        \"\"\"\n        self.console = console\n        self.active_calls: Dict[str, Dict[str, Any]] = {}\n        self.completed_calls: List[Dict[str, Any]] = []\n        self.layout = self._create_layout()\n        self.live = Live(self.layout, console=console, refresh_per_second=4, auto_refresh=False)\n        self.max_completed_calls = 5\n        \n        # Keep track of recent tool results for routines\n        self.recent_tool_results: List[ToolResult] = []\n        self.max_recent_results = 20  # Maximum number of recent results to track\n        \n    def _create_layout(self) -> Layout:\n        \"\"\"Create the layout for the tool call visualization.\n        \n        Returns:\n            Layout object\n        \"\"\"\n        layout = Layout()\n        layout.split(\n            Layout(name=\"active\", size=3),\n            Layout(name=\"completed\", size=3)\n        )\n        return layout\n    \n    def _create_active_calls_panel(self) -> Panel:\n        \"\"\"Create a panel with active tool calls.\n        \n        Returns:\n            Panel with active call information\n        \"\"\"\n        if not self.active_calls:\n            return Panel(\n                \"No active tool calls\",\n                title=\"[bold blue]Active Tool Calls[/bold blue]\",\n                border_style=\"blue\",\n                box=ROUNDED\n            )\n        \n        # Create progress bars for each active call\n        progress = Progress(\n            TextColumn(\"[bold blue]{task.description}\"),\n            BarColumn(),\n            TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n            TimeElapsedColumn(),\n            expand=True,\n            console=self.console\n        )\n        \n        # Add tasks for each active call\n        for call_id, call_info in self.active_calls.items():\n            if \"task_id\" not in call_info:\n                # Create a new task for this call\n                description = f\"{call_info['tool_name']} ({call_id[:6]}...)\"\n                task_id = progress.add_task(description, total=100, completed=int(call_info[\"progress\"] * 100))\n                call_info[\"task_id\"] = task_id\n            else:\n                # Update existing task\n                progress.update(call_info[\"task_id\"], completed=int(call_info[\"progress\"] * 100))\n        \n        # Create a table with parameter information\n        table = Table(show_header=True, header_style=\"bold cyan\", box=ROUNDED, expand=True)\n        table.add_column(\"Tool\")\n        table.add_column(\"Parameters\")\n        \n        for call_id, call_info in self.active_calls.items():\n            # Format parameters nicely\n            params = call_info.get(\"parameters\", {})\n            if params:\n                formatted_params = \"\\n\".join([f\"{k}: {self._format_value(v)}\" for k, v in params.items()])\n            else:\n                formatted_params = \"None\"\n            \n            table.add_row(call_info[\"tool_name\"], formatted_params)\n        \n        return Panel(\n            progress,\n            title=\"[bold blue]Active Tool Calls[/bold blue]\",\n            border_style=\"blue\",\n            box=ROUNDED\n        )\n    \n    def _create_completed_calls_panel(self) -> Panel:\n        \"\"\"Create a panel with completed tool calls.\n        \n        Returns:\n            Panel with completed call information\n        \"\"\"\n        if not self.completed_calls:\n            return Panel(\n                \"No completed tool calls\",\n                title=\"[bold green]Recent Tool Results[/bold green]\",\n                border_style=\"green\",\n                box=ROUNDED\n            )\n        \n        # Create a table for results\n        table = Table(show_header=True, header_style=\"bold green\", box=ROUNDED, expand=True)\n        table.add_column(\"Tool\")\n        table.add_column(\"Status\")\n        table.add_column(\"Time\")\n        table.add_column(\"Result Preview\")\n        \n        # Show only the most recent completed calls\n        for call_info in self.completed_calls[-self.max_completed_calls:]:\n            tool_name = call_info[\"tool_name\"]\n            status = call_info[\"status\"]\n            execution_time = f\"{call_info['execution_time']:.2f}s\"\n            \n            # Format result preview\n            result = call_info.get(\"result\", \"\")\n            if result:\n                # Truncate and format result\n                preview = self._format_result_preview(result, tool_name)\n            else:\n                preview = \"No result\"\n            \n            # Status with color\n            status_text = Text(status)\n            if status == \"success\":\n                status_text.stylize(\"bold green\")\n            else:\n                status_text.stylize(\"bold red\")\n            \n            table.add_row(tool_name, status_text, execution_time, preview)\n        \n        return Panel(\n            table,\n            title=\"[bold green]Recent Tool Results[/bold green]\",\n            border_style=\"green\",\n            box=ROUNDED\n        )\n    \n    def _format_value(self, value: Any) -> str:\n        \"\"\"Format a parameter value for display.\n        \n        Args:\n            value: Parameter value\n            \n        Returns:\n            Formatted string\n        \"\"\"\n        if isinstance(value, (dict, list)):\n            # Convert complex structures to JSON with indentation\n            return json.dumps(value, indent=2)\n        return str(value)\n    \n    def _format_result_preview(self, result: str, tool_name: str) -> str:\n        \"\"\"Format a result preview.\n        \n        Args:\n            result: Result string\n            tool_name: Name of the tool\n            \n        Returns:\n            Formatted preview string\n        \"\"\"\n        # Truncate result for preview\n        if len(result) > 200:\n            preview = result[:200] + \"...\"\n        else:\n            preview = result\n        \n        # Clean up newlines for display\n        preview = preview.replace(\"\\n\", \"\\\\n\")\n        \n        return preview\n    \n    def start(self) -> None:\n        \"\"\"Start the visualization.\"\"\"\n        self.live.start()\n        self.refresh()\n    \n    def stop(self) -> None:\n        \"\"\"Stop the visualization.\"\"\"\n        self.live.stop()\n    \n    def refresh(self) -> None:\n        \"\"\"Refresh the visualization.\"\"\"\n        # Update the layout with current information\n        self.layout[\"active\"].update(self._create_active_calls_panel())\n        self.layout[\"completed\"].update(self._create_completed_calls_panel())\n        \n        # Refresh the live display\n        self.live.refresh()\n    \n    def add_tool_call(self, tool_call_id: str, tool_name: str, parameters: Dict[str, Any]) -> None:\n        \"\"\"Add a new tool call to visualize.\n        \n        Args:\n            tool_call_id: ID of the tool call\n            tool_name: Name of the tool\n            parameters: Tool parameters\n        \"\"\"\n        self.active_calls[tool_call_id] = {\n            \"tool_name\": tool_name,\n            \"parameters\": parameters,\n            \"start_time\": time.time(),\n            \"progress\": 0.0\n        }\n        self.refresh()\n    \n    def update_progress(self, tool_call_id: str, progress: float) -> None:\n        \"\"\"Update the progress of a tool call.\n        \n        Args:\n            tool_call_id: ID of the tool call\n            progress: Progress value (0-1)\n        \"\"\"\n        if tool_call_id in self.active_calls:\n            self.active_calls[tool_call_id][\"progress\"] = progress\n            self.refresh()\n    \n    def complete_tool_call(self, tool_call_id: str, result: ToolResult) -> None:\n        \"\"\"Mark a tool call as complete.\n        \n        Args:\n            tool_call_id: ID of the tool call\n            result: Tool execution result\n        \"\"\"\n        if tool_call_id in self.active_calls:\n            call_info = self.active_calls[tool_call_id].copy()\n            \n            # Add result information\n            call_info[\"result\"] = result.result\n            call_info[\"status\"] = result.status\n            call_info[\"execution_time\"] = result.execution_time\n            call_info[\"end_time\"] = time.time()\n            \n            # Add to completed calls\n            self.completed_calls.append(call_info)\n            \n            # Trim completed calls if needed\n            if len(self.completed_calls) > self.max_completed_calls * 2:\n                self.completed_calls = self.completed_calls[-self.max_completed_calls:]\n            \n            # Remove from active calls\n            del self.active_calls[tool_call_id]\n            \n            # Store in recent tool results for routines\n            if result.status == \"success\":\n                self.recent_tool_results.append(result)\n                # Keep only the most recent results\n                if len(self.recent_tool_results) > self.max_recent_results:\n                    self.recent_tool_results.pop(0)\n            \n            self.refresh()\n    \n    def show_result_detail(self, result: ToolResult) -> None:\n        \"\"\"Display detailed result information.\n        \n        Args:\n            result: Tool execution result\n        \"\"\"\n        # Detect if result might be code\n        content = result.result\n        if content.startswith((\"def \", \"class \", \"import \", \"from \")) or \"```\" in content:\n            # Try to extract code blocks\n            if \"```\" in content:\n                blocks = content.split(\"```\")\n                # Find a code block with a language specifier\n                for i in range(1, len(blocks), 2):\n                    if i < len(blocks):\n                        lang = blocks[i].split(\"\\n\")[0].strip()\n                        code = \"\\n\".join(blocks[i].split(\"\\n\")[1:])\n                        if lang and code:\n                            # Attempt to display as syntax-highlighted code\n                            try:\n                                syntax = Syntax(code, lang, theme=\"monokai\", line_numbers=True)\n                                self.console.print(Panel(syntax, title=f\"[bold]Result: {result.name}[/bold]\"))\n                                return\n                            except Exception:\n                                pass\n            \n            # If we can't extract a code block, try to detect language\n            for lang in [\"python\", \"javascript\", \"bash\", \"json\"]:\n                try:\n                    syntax = Syntax(content, lang, theme=\"monokai\", line_numbers=True)\n                    self.console.print(Panel(syntax, title=f\"[bold]Result: {result.name}[/bold]\"))\n                    return\n                except Exception:\n                    pass\n        \n        # Just print as regular text if not code or if highlighting failed\n        self.console.print(Panel(content, title=f\"[bold]Result: {result.name}[/bold]\"))\n\n\nclass MCTSVisualizer:\n    \"\"\"Visualizes the Monte Carlo Tree Search process in real-time with enhanced intelligence.\"\"\"\n    \n    def __init__(self, console: Console):\n        \"\"\"Initialize the MCTS visualizer.\n        \n        Args:\n            console: Rich console instance\n        \"\"\"\n        self.console = console\n        self.root_node = None\n        self.current_iteration = 0\n        self.max_iterations = 0\n        self.best_action = None\n        self.active_simulation = None\n        self.simulation_path = []\n        self.layout = self._create_layout()\n        self.live = Live(self.layout, console=console, refresh_per_second=10, auto_refresh=False)\n        \n        # Intelligence enhancement - track history\n        self.action_history = {}  # Track action performance over time\n        self.visit_distribution = {}  # Track how visits are distributed\n        self.exploration_patterns = []  # Track exploration patterns\n        self.quality_metrics = {\"search_efficiency\": 0.0, \"exploration_balance\": 0.0}\n        self.auto_improvement_enabled = True\n        \n    def _create_layout(self) -> Layout:\n        \"\"\"Create the layout for MCTS visualization.\n        \n        Returns:\n            Layout object\n        \"\"\"\n        layout = Layout()\n        \n        # Create the main sections with more detailed visualization\n        layout.split(\n            Layout(name=\"header\", size=3),\n            Layout(name=\"main\"),\n            Layout(name=\"intelligence\", size=7),  # New section for intelligence metrics\n            Layout(name=\"stats\", size=5)\n        )\n        \n        # Split the main section into tree, simulation and action insights\n        layout[\"main\"].split_row(\n            Layout(name=\"tree\", ratio=2),\n            Layout(name=\"simulation\", ratio=1),\n            Layout(name=\"insights\", ratio=1)  # New section for action insights\n        )\n        \n        return layout\n        \n    def set_search_parameters(self, root_node: Any, max_iterations: int, additional_params: Dict[str, Any] = None) -> None:\n        \"\"\"Set the search parameters with enhanced intelligence options.\n        \n        Args:\n            root_node: The root node of the search tree\n            max_iterations: Maximum number of iterations\n            additional_params: Additional parameters for intelligent search\n        \"\"\"\n        self.root_node = root_node\n        self.max_iterations = max_iterations\n        self.current_iteration = 0\n        \n        # Initialize intelligence tracking\n        self.action_history = {}\n        self.visit_distribution = {}\n        self.exploration_patterns = []\n        \n        # Set additional intelligence parameters\n        if additional_params:\n            self.auto_improvement_enabled = additional_params.get('auto_improvement', True)\n            \n            # Apply any initial intelligence strategies\n            if additional_params.get('initial_action_bias'):\n                self.action_history = additional_params['initial_action_bias']\n                \n        self.refresh()\n        \n    def update_iteration(self, iteration: int, selected_node: Any = None, \n                        expanded_node: Any = None, simulation_path: List[Any] = None,\n                        simulation_result: float = None, best_action: Any = None,\n                        node_values: Dict[str, float] = None) -> None:\n        \"\"\"Update the current iteration status with enhanced tracking.\n        \n        Args:\n            iteration: Current iteration number\n            selected_node: Node selected in this iteration\n            expanded_node: Node expanded in this iteration\n            simulation_path: Path of the simulation\n            simulation_result: Result of the simulation\n            best_action: Current best action\n            node_values: Values of important nodes in the search (for visualization)\n        \"\"\"\n        self.current_iteration = iteration\n        self.selected_node = selected_node\n        self.expanded_node = expanded_node\n        self.simulation_path = simulation_path or []\n        self.simulation_result = simulation_result\n        \n        if best_action is not None:\n            self.best_action = best_action\n            \n        # Intelligence tracking - update action history\n        if self.simulation_path and simulation_result is not None:\n            for _, action in self.simulation_path:\n                if action is not None:\n                    action_str = str(action)\n                    if action_str not in self.action_history:\n                        self.action_history[action_str] = {\n                            \"visits\": 0,\n                            \"total_value\": 0.0,\n                            \"iterations\": []\n                        }\n                    \n                    self.action_history[action_str][\"visits\"] += 1\n                    self.action_history[action_str][\"total_value\"] += simulation_result\n                    self.action_history[action_str][\"iterations\"].append(iteration)\n        \n        # Update exploration pattern\n        if selected_node:\n            # Record exploration choice\n            self.exploration_patterns.append({\n                \"iteration\": iteration,\n                \"node_depth\": self._get_node_depth(selected_node),\n                \"node_breadth\": len(getattr(selected_node, \"children\", {})),\n                \"value_estimate\": getattr(selected_node, \"value\", 0) / max(1, getattr(selected_node, \"visits\", 1))\n            })\n            \n        # Update visit distribution\n        if self.root_node and hasattr(self.root_node, \"children\"):\n            self._update_visit_distribution()\n            \n        # Update quality metrics\n        self._update_quality_metrics()\n            \n        self.refresh()\n        \n    def start(self) -> None:\n        \"\"\"Start the visualization.\"\"\"\n        self.live.start()\n        self.refresh()\n        \n    def stop(self) -> None:\n        \"\"\"Stop the visualization.\"\"\"\n        self.live.stop()\n        \n    def refresh(self) -> None:\n        \"\"\"Refresh the visualization.\"\"\"\n        # Update header\n        header_content = f\"[bold blue]Enhanced Monte Carlo Tree Search - Iteration {self.current_iteration}/{self.max_iterations}[/bold blue]\"\n        if self.best_action:\n            header_content += f\" | Best Action: {self.best_action}\"\n            \n        intelligence_status = \"[green]Enabled[/green]\" if self.auto_improvement_enabled else \"[yellow]Disabled[/yellow]\"\n        header_content += f\" | Intelligent Search: {intelligence_status}\"\n            \n        self.layout[\"header\"].update(Panel(header_content, border_style=\"blue\"))\n        \n        # Update tree visualization\n        self.layout[\"tree\"].update(self._create_tree_panel())\n        \n        # Update simulation visualization\n        self.layout[\"simulation\"].update(self._create_simulation_panel())\n        \n        # Update action insights panel\n        self.layout[\"insights\"].update(self._create_insights_panel())\n        \n        # Update intelligence metrics\n        self.layout[\"intelligence\"].update(self._create_intelligence_panel())\n        \n        # Update stats\n        self.layout[\"stats\"].update(self._create_stats_panel())\n        \n        # Refresh the live display\n        self.live.refresh()\n        \n    def _create_tree_panel(self) -> Panel:\n        \"\"\"Create a panel showing the current state of the search tree.\n        \n        Returns:\n            Panel with tree visualization\n        \"\"\"\n        if not self.root_node:\n            return Panel(\"No search tree initialized\", title=\"[bold]Search Tree[/bold]\")\n            \n        # Create a table to show the tree structure\n        from rich.tree import Tree\n        from rich import box\n        \n        tree = Tree(\"🔍 Root Node\", guide_style=\"bold blue\")\n        \n        # Limit the depth and breadth for display\n        max_depth = 3\n        max_children = 5\n        \n        def add_node(node, tree_node, depth=0, path=None):\n            if depth >= max_depth or not node or not hasattr(node, \"children\"):\n                return\n                \n            if path is None:\n                path = []\n                \n            # Add children nodes\n            children = list(node.children.items())\n            if not children:\n                return\n                \n            # Sort children by a combination of visits and value\n            def node_score(node_pair):\n                child_node = node_pair[1]\n                visits = getattr(child_node, \"visits\", 0)\n                value = getattr(child_node, \"value\", 0)\n                \n                # Combine visits and value for scoring\n                if visits > 0:\n                    # Use UCB-style formula for ranking\n                    exploitation = value / visits\n                    exploration = (2 * 0.5 * (math.log(node.visits) / visits)) if node.visits > 0 and visits > 0 else 0\n                    return exploitation + exploration\n                return 0\n            \n            # Sort by this smarter formula\n            children.sort(key=node_score, reverse=True)\n            children = children[:max_children]\n            \n            for action, child in children:\n                # Format node information\n                visits = getattr(child, \"visits\", 0)\n                value = getattr(child, \"value\", 0)\n                \n                # Highlight the node with more sophisticated coloring\n                style = \"\"\n                if child == self.selected_node:\n                    style = \"bold yellow\"\n                elif child == self.expanded_node:\n                    style = \"bold green\"\n                else:\n                    # Color based on value\n                    if visits > 0:\n                        avg_value = value / visits\n                        if avg_value > 0.7:\n                            style = \"green\"\n                        elif avg_value > 0.4:\n                            style = \"blue\"\n                        elif avg_value > 0.2:\n                            style = \"yellow\"\n                        else:\n                            style = \"red\"\n                \n                # Create the node label with enhanced information\n                current_path = path + [action]\n                \n                if visits > 0:\n                    avg_value = value / visits\n                    confidence = min(1.0, math.sqrt(visits) / 5) * 100  # Simple confidence estimate\n                    label = f\"[{style}]{action}: (Visits: {visits}, Value: {avg_value:.3f}, Conf: {confidence:.0f}%)[/{style}]\"\n                else:\n                    label = f\"[{style}]{action}: (New)[/{style}]\"\n                \n                # Add the child node to the tree\n                child_tree = tree_node.add(label)\n                \n                # Recursively add its children\n                add_node(child, child_tree, depth + 1, current_path)\n        \n        # Start building the tree from the root\n        if hasattr(self.root_node, \"children\"):\n            # Add math import for node scoring\n            import math\n            add_node(self.root_node, tree)\n            \n        return Panel(tree, title=\"[bold]Search Tree[/bold]\", border_style=\"blue\")\n    \n    def _create_simulation_panel(self) -> Panel:\n        \"\"\"Create a panel showing the current simulation with enhanced analytics.\n        \n        Returns:\n            Panel with simulation visualization\n        \"\"\"\n        if not self.simulation_path:\n            return Panel(\"No active simulation\", title=\"[bold]Current Simulation[/bold]\")\n            \n        # Create a list of simulation steps\n        from rich.table import Table\n        \n        table = Table(box=None, expand=True)\n        table.add_column(\"Step\")\n        table.add_column(\"Action\")\n        table.add_column(\"Expected Value\")  # New column\n        \n        for i, (state, action) in enumerate(self.simulation_path):\n            # Get expected value for this action\n            action_str = str(action) if action is not None else \"None\"\n            expected_value = \"N/A\"\n            \n            if action_str in self.action_history:\n                history = self.action_history[action_str]\n                if history[\"visits\"] > 0:\n                    expected_value = f\"{history['total_value'] / history['visits']:.3f}\"\n            \n            table.add_row(f\"Step {i+1}\", f\"{action}\", expected_value)\n            \n        if self.simulation_result is not None:\n            # Add path quality metric\n            path_quality = \"Low\"\n            if self.simulation_result > 0.7:\n                path_quality = \"[bold green]High[/bold green]\"\n            elif self.simulation_result > 0.4:\n                path_quality = \"[yellow]Medium[/yellow]\"\n            else:\n                path_quality = \"[red]Low[/red]\"\n                \n            table.add_row(\"Result\", \n                        f\"[bold green]{self.simulation_result:.3f}[/bold green]\", \n                        f\"Path Quality: {path_quality}\")\n            \n        return Panel(table, title=\"[bold]Current Simulation[/bold]\", border_style=\"green\")\n    \n    def _create_insights_panel(self) -> Panel:\n        \"\"\"Create a panel showing action insights from learned patterns.\n        \n        Returns:\n            Panel with action insights\n        \"\"\"\n        from rich.table import Table\n        \n        if not self.action_history:\n            return Panel(\"No action insights available yet\", title=\"[bold]Action Insights[/bold]\")\n            \n        # Get top performing actions\n        top_actions = []\n        for action, data in self.action_history.items():\n            if data[\"visits\"] >= 3:  # Only consider actions with enough samples\n                avg_value = data[\"total_value\"] / data[\"visits\"]\n                top_actions.append((action, avg_value, data[\"visits\"]))\n                \n        # Sort by value and take top 5\n        top_actions.sort(key=lambda x: x[1], reverse=True)\n        top_actions = top_actions[:5]\n        \n        # Create insights table\n        table = Table(box=None, expand=True)\n        table.add_column(\"Action\")\n        table.add_column(\"Avg Value\")\n        table.add_column(\"Visits\")\n        table.add_column(\"Trend\")\n        \n        for action, avg_value, visits in top_actions:\n            # Generate trend indicator based on recent performance\n            trend = \"→\"\n            history = self.action_history[action][\"iterations\"]\n            if len(history) >= 5:\n                recent = set(history[-3:])  # Last 3 iterations\n                if self.current_iteration - max(recent) <= 5:\n                    trend = \"↑\"  # Recently used\n                elif self.current_iteration - max(recent) >= 10:\n                    trend = \"↓\"  # Not used recently\n            \n            # Color code based on value\n            if avg_value > 0.7:\n                value_str = f\"[green]{avg_value:.3f}[/green]\"\n            elif avg_value > 0.4:\n                value_str = f\"[blue]{avg_value:.3f}[/blue]\"\n            else:\n                value_str = f\"[yellow]{avg_value:.3f}[/yellow]\"\n                \n            table.add_row(str(action), value_str, str(visits), trend)\n            \n        return Panel(table, title=\"[bold]Action Insights[/bold]\", border_style=\"cyan\")\n    \n    def _create_intelligence_panel(self) -> Panel:\n        \"\"\"Create a panel showing intelligence metrics and learning patterns.\n        \n        Returns:\n            Panel with intelligence visualization\n        \"\"\"\n        from rich.table import Table\n        from rich.columns import Columns\n        \n        # Create metrics table\n        metrics_table = Table(box=None, expand=True)\n        metrics_table.add_column(\"Metric\")\n        metrics_table.add_column(\"Value\")\n        \n        # Add search quality metrics\n        for metric, value in self.quality_metrics.items():\n            formatted_name = metric.replace(\"_\", \" \").title()\n            # Color based on value\n            if value > 0.7:\n                value_str = f\"[green]{value:.2f}[/green]\"\n            elif value > 0.4:\n                value_str = f\"[blue]{value:.2f}[/blue]\"\n            else:\n                value_str = f\"[yellow]{value:.2f}[/yellow]\"\n                \n            metrics_table.add_row(formatted_name, value_str)\n            \n        # Create exploration table\n        exploration_table = Table(box=None, expand=True)\n        exploration_table.add_column(\"Pattern\")\n        exploration_table.add_column(\"Value\")\n        \n        # Add exploration patterns\n        if self.exploration_patterns:\n            # Average depth of exploration\n            avg_depth = sum(p[\"node_depth\"] for p in self.exploration_patterns) / len(self.exploration_patterns)\n            exploration_table.add_row(\"Avg Exploration Depth\", f\"{avg_depth:.2f}\")\n            \n            # Depth trend (increasing or decreasing)\n            if len(self.exploration_patterns) >= 5:\n                recent_avg = sum(p[\"node_depth\"] for p in self.exploration_patterns[-5:]) / 5\n                earlier_avg = sum(p[\"node_depth\"] for p in self.exploration_patterns[:-5]) / max(1, len(self.exploration_patterns) - 5)\n                \n                if recent_avg > earlier_avg * 1.2:\n                    trend = \"[green]Deepening[/green]\"\n                elif recent_avg < earlier_avg * 0.8:\n                    trend = \"[yellow]Shallowing[/yellow]\"\n                else:\n                    trend = \"[blue]Stable[/blue]\"\n                    \n                exploration_table.add_row(\"Depth Trend\", trend)\n                \n            # Exploration-exploitation balance\n            if len(self.exploration_patterns) >= 3:\n                # Higher values = more exploitation of known good paths\n                exploitation_ratio = sum(1 for p in self.exploration_patterns[-10:] \n                                     if p[\"value_estimate\"] > 0.5) / min(10, len(self.exploration_patterns))\n                \n                if exploitation_ratio > 0.7:\n                    balance = \"[yellow]Heavy Exploitation[/yellow]\"\n                elif exploitation_ratio < 0.3:\n                    balance = \"[yellow]Heavy Exploration[/yellow]\"\n                else:\n                    balance = \"[green]Balanced[/green]\"\n                    \n                exploration_table.add_row(\"Search Balance\", balance)\n                \n        # Combine tables into columns\n        columns = Columns([metrics_table, exploration_table])\n        \n        return Panel(columns, title=\"[bold]Intelligence Metrics[/bold]\", border_style=\"magenta\")\n    \n    def _create_stats_panel(self) -> Panel:\n        \"\"\"Create a panel showing search statistics with enhanced metrics.\n        \n        Returns:\n            Panel with statistics\n        \"\"\"\n        if not self.root_node:\n            return Panel(\"No statistics available\", title=\"[bold]Search Statistics[/bold]\")\n            \n        # Collect statistics\n        total_nodes = 0\n        max_depth = 0\n        total_visits = getattr(self.root_node, \"visits\", 0)\n        avg_branching = 0\n        \n        def count_nodes(node, depth=0):\n            nonlocal total_nodes, max_depth, avg_branching\n            if not node or not hasattr(node, \"children\"):\n                return\n                \n            total_nodes += 1\n            max_depth = max(max_depth, depth)\n            \n            # Count children for branching factor\n            num_children = len(node.children)\n            if num_children > 0:\n                avg_branching += num_children\n                \n            for child in node.children.values():\n                count_nodes(child, depth + 1)\n                \n        count_nodes(self.root_node)\n        \n        # Calculate average branching factor\n        if total_nodes > 1:  # Root node doesn't count for avg branching\n            avg_branching /= (total_nodes - 1) \n        \n        # Create a table of statistics\n        from rich.table import Table\n        \n        table = Table(box=None, expand=True)\n        table.add_column(\"Metric\")\n        table.add_column(\"Value\")\n        \n        table.add_row(\"Total Nodes\", str(total_nodes))\n        table.add_row(\"Max Depth\", str(max_depth))\n        table.add_row(\"Total Visits\", str(total_visits))\n        table.add_row(\"Avg Branching\", f\"{avg_branching:.2f}\")\n        table.add_row(\"Progress\", f\"{self.current_iteration / self.max_iterations:.1%}\")\n        \n        # Efficiency estimate (higher is better)\n        if total_visits > 0:\n            visit_efficiency = total_nodes / total_visits\n            efficiency_str = f\"{visit_efficiency:.2f}\"\n            table.add_row(\"Search Efficiency\", efficiency_str)\n        \n        return Panel(table, title=\"[bold]Search Statistics[/bold]\", border_style=\"magenta\")\n    \n    def _get_node_depth(self, node):\n        \"\"\"Calculate the depth of a node in the tree.\"\"\"\n        depth = 0\n        current = node\n        while getattr(current, \"parent\", None) is not None:\n            depth += 1\n            current = current.parent\n        return depth\n    \n    def _update_visit_distribution(self):\n        \"\"\"Update the distribution of visits across the tree.\"\"\"\n        levels = {}\n        \n        def count_visits_by_level(node, depth=0):\n            if not node or not hasattr(node, \"children\"):\n                return\n                \n            # Initialize level if not present\n            if depth not in levels:\n                levels[depth] = {\"visits\": 0, \"nodes\": 0}\n                \n            # Update level stats\n            levels[depth][\"visits\"] += getattr(node, \"visits\", 0)\n            levels[depth][\"nodes\"] += 1\n            \n            # Process children\n            for child in node.children.values():\n                count_visits_by_level(child, depth + 1)\n                \n        # Start counting from root\n        count_visits_by_level(self.root_node)\n        \n        # Update visit distribution\n        self.visit_distribution = levels\n    \n    def _update_quality_metrics(self):\n        \"\"\"Update quality metrics for the search process.\"\"\"\n        # Search efficiency - ratio of valuable nodes to total nodes\n        # Higher values indicate more efficient search\n        if self.visit_distribution:\n            useful_visits = sum(level[\"visits\"] for depth, level in self.visit_distribution.items() \n                               if depth > 0)  # Exclude root\n            total_visits = sum(level[\"visits\"] for level in self.visit_distribution.values())\n            \n            if total_visits > 0:\n                self.quality_metrics[\"search_efficiency\"] = useful_visits / total_visits\n            \n        # Exploration balance - how well the algorithm balances exploration vs exploitation\n        if self.exploration_patterns:\n            # Calculate variance in exploration depth\n            depths = [p[\"node_depth\"] for p in self.exploration_patterns[-20:]]  # Last 20 iterations\n            if depths:\n                import statistics\n                try:\n                    depth_variance = statistics.variance(depths) if len(depths) > 1 else 0\n                    # Normalize to 0-1 range (higher variance = more balanced exploration)\n                    normalized_variance = min(1.0, depth_variance / 5.0)  # Assume variance > 5 is high\n                    self.quality_metrics[\"exploration_balance\"] = normalized_variance\n                except statistics.StatisticsError:\n                    pass\n\n\nclass ParallelExecutionVisualizer:\n    \"\"\"Visualizes parallel execution of tool calls in real-time.\"\"\"\n    \n    def __init__(self, console: Console):\n        \"\"\"Initialize the parallel execution visualizer.\n        \n        Args:\n            console: Rich console instance\n        \"\"\"\n        self.console = console\n        self.active_executions = {}\n        self.completed_executions = []\n        self.layout = self._create_layout()\n        self.live = Live(self.layout, console=console, refresh_per_second=10, auto_refresh=False)\n        \n    def _create_layout(self) -> Layout:\n        \"\"\"Create the layout for parallel execution visualization.\n        \n        Returns:\n            Layout object\n        \"\"\"\n        layout = Layout()\n        \n        # Create the main sections\n        layout.split(\n            Layout(name=\"header\", size=3),\n            Layout(name=\"executions\"),\n            Layout(name=\"metrics\", size=5)\n        )\n        \n        return layout\n        \n    def add_execution(self, execution_id: str, tool_name: str, parameters: Dict[str, Any]) -> None:\n        \"\"\"Add a new execution to visualize.\n        \n        Args:\n            execution_id: Unique ID for the execution\n            tool_name: Name of the tool being executed\n            parameters: Parameters for the execution\n        \"\"\"\n        self.active_executions[execution_id] = {\n            \"tool_name\": tool_name,\n            \"parameters\": parameters,\n            \"start_time\": time.time(),\n            \"progress\": 0.0,\n            \"status\": \"running\"\n        }\n        self.refresh()\n        \n    def update_progress(self, execution_id: str, progress: float) -> None:\n        \"\"\"Update the progress of an execution.\n        \n        Args:\n            execution_id: ID of the execution\n            progress: Progress value (0-1)\n        \"\"\"\n        if execution_id in self.active_executions:\n            self.active_executions[execution_id][\"progress\"] = progress\n            self.refresh()\n            \n    def complete_execution(self, execution_id: str, result: Any, status: str = \"success\") -> None:\n        \"\"\"Mark an execution as complete.\n        \n        Args:\n            execution_id: ID of the execution\n            result: Result of the execution\n            status: Status of completion\n        \"\"\"\n        if execution_id in self.active_executions:\n            execution = self.active_executions[execution_id].copy()\n            execution[\"end_time\"] = time.time()\n            execution[\"duration\"] = execution[\"end_time\"] - execution[\"start_time\"]\n            execution[\"result\"] = result\n            execution[\"status\"] = status\n            \n            # Move to completed executions\n            self.completed_executions.append(execution)\n            del self.active_executions[execution_id]\n            \n            # Limit completed executions list\n            if len(self.completed_executions) > 20:\n                self.completed_executions = self.completed_executions[-20:]\n                \n            self.refresh()\n            \n    def start(self) -> None:\n        \"\"\"Start the visualization.\"\"\"\n        self.live.start()\n        self.refresh()\n        \n    def stop(self) -> None:\n        \"\"\"Stop the visualization.\"\"\"\n        self.live.stop()\n        \n    def refresh(self) -> None:\n        \"\"\"Refresh the visualization.\"\"\"\n        # Update header\n        header_content = f\"[bold blue]Parallel Execution Monitor[/bold blue] | Active: {len(self.active_executions)} | Completed: {len(self.completed_executions)}\"\n        self.layout[\"header\"].update(Panel(header_content, border_style=\"blue\"))\n        \n        # Update executions visualization\n        self.layout[\"executions\"].update(self._create_executions_panel())\n        \n        # Update metrics\n        self.layout[\"metrics\"].update(self._create_metrics_panel())\n        \n        # Refresh the live display\n        self.live.refresh()\n        \n    def _create_executions_panel(self) -> Panel:\n        \"\"\"Create a panel showing active and recent executions.\n        \n        Returns:\n            Panel with executions visualization\n        \"\"\"\n        from rich.table import Table\n        from rich.progress import BarColumn, Progress, TextColumn\n        \n        # Create progress bars for active executions\n        progress_group = Table.grid(expand=True)\n        \n        if self.active_executions:\n            # Create a progress group\n            progress = Progress(\n                TextColumn(\"[bold blue]{task.description}\"),\n                BarColumn(bar_width=None),\n                TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n                TextColumn(\"| Elapsed: {task.elapsed:.2f}s\"),\n                expand=True\n            )\n            \n            # Add tasks for each active execution\n            for exec_id, execution in self.active_executions.items():\n                tool_name = execution[\"tool_name\"]\n                description = f\"{tool_name} ({exec_id[:8]}...)\"\n                task_id = progress.add_task(description, total=100, completed=int(execution[\"progress\"] * 100))\n                \n            progress_group.add_row(progress)\n        else:\n            progress_group.add_row(\"[italic]No active executions[/italic]\")\n            \n        # Create a table for completed executions\n        completed_table = Table(show_header=True, header_style=\"bold blue\", expand=True)\n        completed_table.add_column(\"Tool\")\n        completed_table.add_column(\"Duration\")\n        completed_table.add_column(\"Status\")\n        completed_table.add_column(\"Result Preview\")\n        \n        if self.completed_executions:\n            # Most recent first\n            for execution in reversed(self.completed_executions[-10:]):\n                tool_name = execution[\"tool_name\"]\n                duration = f\"{execution['duration']:.2f}s\"\n                status = execution[\"status\"]\n                \n                # Format result preview\n                result = str(execution.get(\"result\", \"\"))\n                preview = result[:50] + \"...\" if len(result) > 50 else result\n                \n                # Add status with color\n                status_text = f\"[green]{status}[/green]\" if status == \"success\" else f\"[red]{status}[/red]\"\n                \n                completed_table.add_row(tool_name, duration, status_text, preview)\n        else:\n            completed_table.add_row(\"[italic]No completed executions[/italic]\", \"\", \"\", \"\")\n            \n        # Combine both into a layout\n        layout = Layout()\n        layout.split(\n            Layout(name=\"active\", size=len(self.active_executions) * 2 + 3 if self.active_executions else 3),\n            Layout(name=\"completed\")\n        )\n        layout[\"active\"].update(Panel(progress_group, title=\"[bold]Active Executions[/bold]\", border_style=\"blue\"))\n        layout[\"completed\"].update(Panel(completed_table, title=\"[bold]Recent Completions[/bold]\", border_style=\"green\"))\n        \n        return layout\n    \n    def _create_metrics_panel(self) -> Panel:\n        \"\"\"Create a panel showing execution metrics.\n        \n        Returns:\n            Panel with metrics visualization\n        \"\"\"\n        from rich.table import Table\n        \n        # Calculate metrics\n        total_executions = len(self.completed_executions)\n        successful = sum(1 for e in self.completed_executions if e[\"status\"] == \"success\")\n        failed = total_executions - successful\n        \n        if total_executions > 0:\n            success_rate = successful / total_executions\n            avg_duration = sum(e[\"duration\"] for e in self.completed_executions) / total_executions\n        else:\n            success_rate = 0\n            avg_duration = 0\n            \n        # Create metrics table\n        table = Table(box=None, expand=True)\n        table.add_column(\"Metric\")\n        table.add_column(\"Value\")\n        \n        table.add_row(\"Total Executions\", str(total_executions))\n        table.add_row(\"Success Rate\", f\"{success_rate:.1%}\")\n        table.add_row(\"Average Duration\", f\"{avg_duration:.2f}s\")\n        table.add_row(\"Current Parallelism\", str(len(self.active_executions)))\n        \n        return Panel(table, title=\"[bold]Execution Metrics[/bold]\", border_style=\"magenta\")\n\n\nclass MultiPanelLayout:\n    \"\"\"Creates a multi-panel layout for the entire UI.\"\"\"\n    \n    def __init__(self, console: Console):\n        \"\"\"Initialize the multi-panel layout.\n        \n        Args:\n            console: Rich console instance\n        \"\"\"\n        self.console = console\n        self.layout = self._create_layout()\n        self.live = Live(self.layout, console=console, refresh_per_second=4, auto_refresh=False)\n        \n    def _create_layout(self) -> Layout:\n        \"\"\"Create the main application layout.\n        \n        Returns:\n            Layout object\n        \"\"\"\n        layout = Layout()\n        \n        # Split into three main sections\n        layout.split(\n            Layout(name=\"conversation\", ratio=3),\n            Layout(name=\"tools\", ratio=2),\n            Layout(name=\"input\", ratio=1)\n        )\n        \n        # Further split the tools section\n        layout[\"tools\"].split_row(\n            Layout(name=\"active_tools\"),\n            Layout(name=\"cost\", size=30)\n        )\n        \n        return layout\n    \n    def start(self) -> None:\n        \"\"\"Start the live display.\"\"\"\n        self.live.start()\n    \n    def stop(self) -> None:\n        \"\"\"Stop the live display.\"\"\"\n        self.live.stop()\n    \n    def refresh(self) -> None:\n        \"\"\"Refresh the display.\"\"\"\n        self.live.refresh()\n    \n    def update_section(self, section: str, content: Any) -> None:\n        \"\"\"Update a section of the layout.\n        \n        Args:\n            section: Section name\n            content: Content to display\n        \"\"\"\n        if section in self.layout:\n            self.layout[section].update(content)\n            self.refresh()"}
{"type": "source_file", "path": "claude_code/lib/rl/mcts.py", "content": "\"\"\"\nMonte Carlo Tree Search implementation for decision making in Claude Code.\nThis module provides an advanced MCTS implementation that can be used to select\noptimal actions/tools based on simulated outcomes.\n\"\"\"\n\nimport math\nimport numpy as np\nimport random\nfrom typing import List, Dict, Any, Callable, Tuple, Optional, Union\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass MCTSNode:\n    \"\"\"Represents a node in the Monte Carlo search tree.\"\"\"\n    state: Any\n    parent: Optional['MCTSNode'] = None\n    action_taken: Any = None\n    visits: int = 0\n    value: float = 0.0\n    children: Dict[Any, 'MCTSNode'] = None\n    \n    def __post_init__(self):\n        if self.children is None:\n            self.children = {}\n    \n    def is_fully_expanded(self, possible_actions: List[Any]) -> bool:\n        \"\"\"Check if all possible actions have been tried from this node.\"\"\"\n        return all(action in self.children for action in possible_actions)\n    \n    def is_terminal(self) -> bool:\n        \"\"\"Check if this node represents a terminal state.\"\"\"\n        # This should be customized based on your environment\n        return False\n    \n    def best_child(self, exploration_weight: float = 1.0) -> 'MCTSNode':\n        \"\"\"Select the best child node according to UCB1 formula.\"\"\"\n        if not self.children:\n            return None\n            \n        def ucb_score(child: MCTSNode) -> float:\n            exploitation = child.value / child.visits if child.visits > 0 else 0\n            exploration = math.sqrt(2 * math.log(self.visits) / child.visits) if child.visits > 0 else float('inf')\n            return exploitation + exploration_weight * exploration\n            \n        return max(self.children.values(), key=ucb_score)\n\n\nclass AdvancedMCTS:\n    \"\"\"\n    Advanced Monte Carlo Tree Search implementation with various enhancements:\n    - Progressive widening for large/continuous action spaces\n    - RAVE (Rapid Action Value Estimation)\n    - Parallel simulations\n    - Dynamic exploration weight\n    - Customizable simulation and backpropagation strategies\n    \"\"\"\n    \n    def __init__(\n        self, \n        state_evaluator: Callable[[Any], float],\n        action_generator: Callable[[Any], List[Any]],\n        simulator: Callable[[Any, Any], Any],\n        max_iterations: int = 1000,\n        exploration_weight: float = 1.0,\n        time_limit: Optional[float] = None,\n        progressive_widening: bool = False,\n        pw_coef: float = 0.5,\n        pw_power: float = 0.5,\n        use_rave: bool = False,\n        rave_equiv_param: float = 1000,\n    ):\n        \"\"\"\n        Initialize the MCTS algorithm.\n        \n        Args:\n            state_evaluator: Function to evaluate the value of a state (terminal or not)\n            action_generator: Function to generate possible actions from a state\n            simulator: Function to simulate taking an action in a state, returning new state\n            max_iterations: Maximum number of search iterations\n            exploration_weight: Controls exploration vs exploitation balance\n            time_limit: Optional time limit for search in seconds\n            progressive_widening: Whether to use progressive widening for large action spaces\n            pw_coef: Coefficient for progressive widening\n            pw_power: Power for progressive widening\n            use_rave: Whether to use RAVE (Rapid Action Value Estimation)\n            rave_equiv_param: RAVE equivalence parameter\n        \"\"\"\n        self.state_evaluator = state_evaluator\n        self.action_generator = action_generator \n        self.simulator = simulator\n        self.max_iterations = max_iterations\n        self.exploration_weight = exploration_weight\n        self.time_limit = time_limit\n        \n        # Progressive widening parameters\n        self.progressive_widening = progressive_widening\n        self.pw_coef = pw_coef\n        self.pw_power = pw_power\n        \n        # RAVE parameters\n        self.use_rave = use_rave\n        self.rave_equiv_param = rave_equiv_param\n        self.rave_values = {}  # (state, action) -> (value, visits)\n    \n    def search(self, initial_state: Any, visualizer=None) -> Any:\n        \"\"\"\n        Perform MCTS search from the initial state and return the best action.\n        \n        Args:\n            initial_state: The starting state for the search\n            visualizer: Optional visualizer to show progress\n            \n        Returns:\n            The best action found by the search\n        \"\"\"\n        root = MCTSNode(state=initial_state)\n        \n        # Initialize visualizer if provided\n        if visualizer:\n            visualizer.set_search_parameters(root, self.max_iterations)\n        \n        # Run iterations of the MCTS algorithm\n        for iteration in range(self.max_iterations):\n            # Selection phase\n            selected_node = self._select(root)\n            \n            # Expansion phase (if not terminal)\n            expanded_node = None\n            if not selected_node.is_terminal():\n                expanded_node = self._expand(selected_node)\n            else:\n                expanded_node = selected_node\n            \n            # Simulation phase\n            simulation_path = []\n            if visualizer:\n                # Track simulation path for visualization\n                current = expanded_node\n                current_state = current.state\n                while current.parent:\n                    simulation_path.insert(0, (current.parent.state, current.action_taken))\n                    current = current.parent\n            \n            simulation_result = self._simulate(expanded_node)\n            \n            # Backpropagation phase\n            self._backpropagate(expanded_node, simulation_result)\n            \n            # Update visualization\n            if visualizer:\n                # Find current best action\n                best_action = None\n                if root.children:\n                    best_action = max(root.children.items(), key=lambda x: x[1].visits)[0]\n                \n                # Update visualizer\n                visualizer.update_iteration(\n                    iteration=iteration + 1,\n                    selected_node=selected_node,\n                    expanded_node=expanded_node,\n                    simulation_path=simulation_path,\n                    simulation_result=simulation_result,\n                    best_action=best_action\n                )\n            \n        # Return the action that leads to the child with the highest value\n        if not root.children:\n            possible_actions = self.action_generator(root.state)\n            if possible_actions:\n                best_action = random.choice(possible_actions)\n                if visualizer:\n                    visualizer.update_iteration(\n                        iteration=self.max_iterations,\n                        best_action=best_action\n                    )\n                return best_action\n            return None\n        \n        best_action = max(root.children.items(), key=lambda x: x[1].visits)[0]\n        if visualizer:\n            visualizer.update_iteration(\n                iteration=self.max_iterations,\n                best_action=best_action\n            )\n        return best_action\n    \n    def _select(self, node: MCTSNode) -> MCTSNode:\n        \"\"\"\n        Select a node to expand using UCB1 and progressive widening if enabled.\n        \n        Args:\n            node: The current node\n            \n        Returns:\n            The selected node for expansion\n        \"\"\"\n        while not node.is_terminal():\n            possible_actions = self.action_generator(node.state)\n            \n            # Handle progressive widening if enabled\n            if self.progressive_widening:\n                max_children = max(1, int(self.pw_coef * (node.visits ** self.pw_power)))\n                if len(node.children) < min(max_children, len(possible_actions)):\n                    return node\n            \n            # If not fully expanded, select this node for expansion\n            if not node.is_fully_expanded(possible_actions):\n                return node\n                \n            # Otherwise, select the best child according to UCB1\n            node = node.best_child(self.exploration_weight)\n            if node is None:\n                break\n                \n        return node\n    \n    def _expand(self, node: MCTSNode) -> MCTSNode:\n        \"\"\"\n        Expand the node by selecting an untried action and creating a new child node.\n        \n        Args:\n            node: The node to expand\n            \n        Returns:\n            The newly created child node\n        \"\"\"\n        possible_actions = self.action_generator(node.state)\n        untried_actions = [a for a in possible_actions if a not in node.children]\n        \n        if not untried_actions:\n            return node\n            \n        action = random.choice(untried_actions)\n        new_state = self.simulator(node.state, action)\n        child_node = MCTSNode(\n            state=new_state,\n            parent=node,\n            action_taken=action\n        )\n        node.children[action] = child_node\n        return child_node\n    \n    def _simulate(self, node: MCTSNode, depth: int = 10) -> float:\n        \"\"\"\n        Simulate a random playout from the given node until a terminal state or max depth.\n        \n        Args:\n            node: The node to start simulation from\n            depth: Maximum simulation depth\n            \n        Returns:\n            The value of the simulated outcome\n        \"\"\"\n        state = node.state\n        current_depth = 0\n        \n        # Continue simulation until we reach a terminal state or max depth\n        while current_depth < depth:\n            if self._is_terminal_state(state):\n                break\n                \n            possible_actions = self.action_generator(state)\n            if not possible_actions:\n                break\n                \n            action = random.choice(possible_actions)\n            state = self.simulator(state, action)\n            current_depth += 1\n            \n        return self.state_evaluator(state)\n    \n    def _is_terminal_state(self, state: Any) -> bool:\n        \"\"\"Determine if the state is terminal.\"\"\"\n        # This should be customized based on your environment\n        return False\n    \n    def _backpropagate(self, node: MCTSNode, value: float) -> None:\n        \"\"\"\n        Backpropagate the simulation result up the tree.\n        \n        Args:\n            node: The leaf node where simulation started\n            value: The value from the simulation\n        \"\"\"\n        while node is not None:\n            node.visits += 1\n            node.value += value\n            \n            # Update RAVE values if enabled\n            if self.use_rave and node.parent is not None:\n                state_hash = self._hash_state(node.parent.state)\n                action = node.action_taken\n                if (state_hash, action) not in self.rave_values:\n                    self.rave_values[(state_hash, action)] = [0, 0]  # [value, visits]\n                rave_value, rave_visits = self.rave_values[(state_hash, action)]\n                self.rave_values[(state_hash, action)] = [\n                    rave_value + value,\n                    rave_visits + 1\n                ]\n                \n            node = node.parent\n    \n    def _hash_state(self, state: Any) -> int:\n        \"\"\"Create a hash of the state for RAVE table lookups.\"\"\"\n        # This should be customized based on your state representation\n        if hasattr(state, \"__hash__\"):\n            return hash(state)\n        return hash(str(state))\n\n\nclass MCTSToolSelector:\n    \"\"\"\n    Specialized MCTS implementation for selecting optimal tools in Claude Code.\n    This class adapts the AdvancedMCTS for the specific context of tool selection.\n    \"\"\"\n    \n    def __init__(\n        self,\n        tool_registry: Any,  # Should be a reference to the tool registry\n        context_evaluator: Callable,  # Function to evaluate quality of response given context\n        max_iterations: int = 200,\n        exploration_weight: float = 1.0,\n        use_learning: bool = True,\n        tool_history_weight: float = 0.7,\n        enable_plan_generation: bool = True,\n        use_semantic_similarity: bool = True,\n        adaptation_rate: float = 0.05\n    ):\n        \"\"\"\n        Initialize the MCTS tool selector with enhanced intelligence.\n        \n        Args:\n            tool_registry: Registry containing available tools\n            context_evaluator: Function to evaluate response quality\n            max_iterations: Maximum search iterations\n            exploration_weight: Controls exploration vs exploitation\n            use_learning: Whether to use learning from past tool selections\n            tool_history_weight: Weight given to historical tool performance\n            enable_plan_generation: Generate complete tool sequences as plans\n            use_semantic_similarity: Use semantic similarity for tool relevance\n            adaptation_rate: Rate at which the system adapts to new patterns\n        \"\"\"\n        self.tool_registry = tool_registry\n        self.context_evaluator = context_evaluator\n        self.use_learning = use_learning\n        self.tool_history_weight = tool_history_weight\n        self.enable_plan_generation = enable_plan_generation\n        self.use_semantic_similarity = use_semantic_similarity\n        self.adaptation_rate = adaptation_rate\n        \n        # Tool performance history by query type\n        self.tool_history = {}\n        \n        # Tool sequence effectiveness records\n        self.sequence_effectiveness = {}\n        \n        # Semantic fingerprints for tools and queries\n        self.tool_fingerprints = {}\n        self.query_clusters = {}\n        \n        # Cached simulation results for similar queries\n        self.simulation_cache = {}\n        \n        # Initialize the MCTS algorithm\n        self.mcts = AdvancedMCTS(\n            state_evaluator=self._evaluate_state,\n            action_generator=self._generate_actions,\n            simulator=self._simulate_action,\n            max_iterations=max_iterations,\n            exploration_weight=exploration_weight,\n            progressive_widening=True\n        )\n        \n        # Initialize tool fingerprints\n        self._initialize_tool_fingerprints()\n    \n    def _initialize_tool_fingerprints(self):\n        \"\"\"Initialize semantic fingerprints for all available tools.\"\"\"\n        if not self.use_semantic_similarity:\n            return\n            \n        for tool_name in self.tool_registry.get_all_tool_names():\n            tool = self.tool_registry.get_tool(tool_name)\n            if tool and hasattr(tool, 'description'):\n                # In a real implementation, this would compute an embedding\n                # For now, we'll use a simple keyword extraction as a placeholder\n                keywords = set(word.lower() for word in tool.description.split() \n                             if len(word) > 3)\n                self.tool_fingerprints[tool_name] = {\n                    'keywords': keywords,\n                    'description': tool.description,\n                    'usage_contexts': set()\n                }\n    \n    def select_tool(self, user_query: str, context: Dict[str, Any], visualizer=None) -> Union[str, List[str]]:\n        \"\"\"\n        Select the best tool to use for a given user query and context.\n        \n        Args:\n            user_query: The user's query\n            context: The current conversation context\n            visualizer: Optional visualizer to show the selection process\n            \n        Returns:\n            Either a single tool name or a sequence of tool names (if plan generation is enabled)\n        \"\"\"\n        # Analyze query to determine its type/characteristics\n        query_type = self._analyze_query(user_query)\n        \n        # Update semantic fingerprints with this query\n        if self.use_semantic_similarity:\n            self._update_query_clusters(user_query, query_type)\n        \n        initial_state = {\n            'query': user_query,\n            'query_type': query_type,\n            'context': context,\n            'actions_taken': [],\n            'response_quality': 0.0,\n            'steps_remaining': 3 if self.enable_plan_generation else 1,\n            'step_results': {}\n        }\n        \n        # First check if we have a high-confidence cached result for similar queries\n        cached_result = self._check_cache(user_query, query_type)\n        if cached_result and random.random() > 0.1:  # 10% random exploration\n            if visualizer:\n                visualizer.add_execution(\n                    execution_id=\"mcts_cache_hit\",\n                    tool_name=\"MCTS Tool Selection (cached)\",\n                    parameters={\"query\": user_query[:100] + \"...\" if len(user_query) > 100 else user_query}\n                )\n                visualizer.complete_execution(\n                    execution_id=\"mcts_cache_hit\",\n                    result={\"selected_tool\": cached_result, \"source\": \"cache\"},\n                    status=\"success\"\n                )\n            return cached_result\n        \n        # Run MCTS search\n        best_action = self.mcts.search(initial_state, visualizer)\n        \n        # If plan generation is enabled, we might want to return a sequence\n        if self.enable_plan_generation:\n            # Extract the most promising action sequence from search\n            plan = self._extract_plan_from_search()\n            if plan and len(plan) > 1:\n                # Store this plan in our cache\n                self._cache_result(user_query, query_type, plan)\n                return plan\n        \n        # Store single action in cache\n        self._cache_result(user_query, query_type, best_action)\n        return best_action\n    \n    def _analyze_query(self, query: str) -> str:\n        \"\"\"\n        Analyze a query to determine its type and characteristics.\n        \n        Args:\n            query: The user query\n            \n        Returns:\n            A string identifying the query type\n        \"\"\"\n        query_lower = query.lower()\n        \n        # Check for search-related queries\n        if any(term in query_lower for term in ['find', 'search', 'where', 'look for']):\n            return 'search'\n            \n        # Check for explanation queries\n        if any(term in query_lower for term in ['explain', 'how', 'why', 'what is']):\n            return 'explanation'\n            \n        # Check for file operation queries\n        if any(term in query_lower for term in ['file', 'read', 'write', 'edit', 'create']):\n            return 'file_operation'\n            \n        # Check for execution queries\n        if any(term in query_lower for term in ['run', 'execute', 'start']):\n            return 'execution'\n            \n        # Check for debugging queries\n        if any(term in query_lower for term in ['debug', 'fix', 'error', 'problem']):\n            return 'debugging'\n            \n        # Default to general\n        return 'general'\n    \n    def _update_query_clusters(self, query: str, query_type: str):\n        \"\"\"\n        Update query clusters with new query information.\n        \n        Args:\n            query: The user query\n            query_type: The type of query\n        \"\"\"\n        # Extract query keywords\n        keywords = set(word.lower() for word in query.split() if len(word) > 3)\n        \n        # Update query clusters\n        if query_type not in self.query_clusters:\n            self.query_clusters[query_type] = {\n                'keywords': set(),\n                'queries': []\n            }\n            \n        # Add keywords to cluster\n        self.query_clusters[query_type]['keywords'].update(keywords)\n        \n        # Add query to cluster (limit to last 50)\n        self.query_clusters[query_type]['queries'].append(query)\n        if len(self.query_clusters[query_type]['queries']) > 50:\n            self.query_clusters[query_type]['queries'].pop(0)\n            \n        # Update tool fingerprints with these keywords\n        for tool_name, fingerprint in self.tool_fingerprints.items():\n            # If tool has been used successfully for this query type before\n            if tool_name in self.tool_history.get(query_type, {}) and \\\n               self.tool_history[query_type][tool_name]['success_rate'] > 0.6:\n                fingerprint['usage_contexts'].add(query_type)\n    \n    def _check_cache(self, query: str, query_type: str) -> Union[str, List[str], None]:\n        \"\"\"\n        Check if we have a cached result for a similar query.\n        \n        Args:\n            query: The user query\n            query_type: The type of query\n            \n        Returns:\n            A cached tool selection or None\n        \"\"\"\n        if not self.use_learning or query_type not in self.tool_history:\n            return None\n            \n        # Find the most successful tool for this query type\n        type_history = self.tool_history[query_type]\n        best_tools = sorted(\n            [(tool, data['success_rate']) for tool, data in type_history.items()],\n            key=lambda x: x[1], \n            reverse=True\n        )\n        \n        # Only use cache if we have a high confidence result\n        if best_tools and best_tools[0][1] > 0.75:\n            return best_tools[0][0]\n            \n        return None\n    \n    def _cache_result(self, query: str, query_type: str, action: Union[str, List[str]]):\n        \"\"\"\n        Cache a result for future similar queries.\n        \n        Args:\n            query: The user query\n            query_type: The type of query\n            action: The selected action or plan\n        \"\"\"\n        # Store in simulation cache\n        query_key = self._get_query_cache_key(query)\n        self.simulation_cache[query_key] = {\n            'action': action,\n            'timestamp': self._get_timestamp(),\n            'query_type': query_type\n        }\n        \n        # Limit cache size\n        if len(self.simulation_cache) > 1000:\n            # Remove oldest entries\n            oldest_key = min(self.simulation_cache.keys(), \n                           key=lambda k: self.simulation_cache[k]['timestamp'])\n            del self.simulation_cache[oldest_key]\n    \n    def _get_query_cache_key(self, query: str) -> str:\n        \"\"\"Generate a cache key for a query.\"\"\"\n        # In a real implementation, this might use a hash of query embeddings\n        # For now, use a simple keyword approach\n        keywords = ' '.join(sorted(set(word.lower() for word in query.split() if len(word) > 3)))\n        return keywords[:100]  # Limit key length\n    \n    def _get_timestamp(self):\n        \"\"\"Get current timestamp.\"\"\"\n        import time\n        return time.time()\n    \n    def _evaluate_state(self, state: Dict[str, Any]) -> float:\n        \"\"\"\n        Evaluate the quality of a state based on response quality and steps.\n        \n        Args:\n            state: The current state\n            \n        Returns:\n            A quality score\n        \"\"\"\n        # Base score is the response quality\n        score = state['response_quality']\n        \n        # If plan generation is enabled, we want to encourage complete plans\n        if self.enable_plan_generation:\n            steps_completed = len(state['actions_taken'])\n            total_steps = steps_completed + state['steps_remaining']\n            \n            # Add bonus for completing more steps\n            if total_steps > 0:\n                step_completion_bonus = steps_completed / total_steps\n                score += step_completion_bonus * 0.2  # 20% bonus for step completion\n        \n        return score\n    \n    def _generate_actions(self, state: Dict[str, Any]) -> List[str]:\n        \"\"\"\n        Generate possible tool actions from the current state with intelligent filtering.\n        \n        Args:\n            state: The current state\n            \n        Returns:\n            List of possible actions\n        \"\"\"\n        # Get query type\n        query_type = state['query_type']\n        query = state['query']\n        \n        # Get all available tools\n        all_tools = set(self.tool_registry.get_all_tool_names())\n        \n        # Tools already used in this sequence\n        used_tools = set(state['actions_taken'])\n        \n        # Remaining tools\n        remaining_tools = all_tools - used_tools\n        \n        # If we're using learning, prioritize tools based on history\n        if self.use_learning and query_type in self.tool_history:\n            prioritized_tools = []\n            \n            # First, add tools that have been successful for this query type\n            type_history = self.tool_history[query_type]\n            \n            # Check for successful tools\n            for tool in remaining_tools:\n                if tool in type_history and type_history[tool]['success_rate'] > 0.5:\n                    prioritized_tools.append(tool)\n                    \n            # If we have at least some tools, return them\n            if prioritized_tools and random.random() < self.tool_history_weight:\n                return prioritized_tools\n        \n        # If using semantic similarity, filter by relevant tools\n        if self.use_semantic_similarity:\n            query_keywords = set(word.lower() for word in query.split() if len(word) > 3)\n            \n            # Score tools by semantic similarity to query\n            scored_tools = []\n            for tool in remaining_tools:\n                if tool in self.tool_fingerprints:\n                    fingerprint = self.tool_fingerprints[tool]\n                    \n                    # Calculate keyword overlap\n                    keyword_overlap = len(query_keywords.intersection(fingerprint['keywords']))\n                    \n                    # Check if tool has been used for this query type\n                    context_match = 1.0 if query_type in fingerprint['usage_contexts'] else 0.0\n                    \n                    # Combined score\n                    score = keyword_overlap * 0.7 + context_match * 0.3\n                    \n                    scored_tools.append((tool, score))\n            \n            # Sort and filter tools\n            scored_tools.sort(key=lambda x: x[1], reverse=True)\n            \n            # Take top half of tools if we have enough\n            if len(scored_tools) > 2:\n                return [t[0] for t in scored_tools[:max(2, len(scored_tools) // 2)]]\n        \n        # If we reach here, use all remaining tools\n        return list(remaining_tools)\n    \n    def _simulate_action(self, state: Dict[str, Any], action: str) -> Dict[str, Any]:\n        \"\"\"\n        Simulate taking an action (using a tool) in the given state with enhanced modeling.\n        \n        Args:\n            state: The current state\n            action: The tool action to simulate\n            \n        Returns:\n            The new state after taking the action\n        \"\"\"\n        # Create a new state with the action added\n        new_state = state.copy()\n        new_actions = state['actions_taken'].copy()\n        new_actions.append(action)\n        new_state['actions_taken'] = new_actions\n        \n        # Decrement steps remaining if using plan generation\n        if self.enable_plan_generation and new_state['steps_remaining'] > 0:\n            new_state['steps_remaining'] -= 1\n        \n        # Get query type and query\n        query_type = state['query_type']\n        query = state['query']\n        \n        # Simulate step result\n        step_results = state['step_results'].copy()\n        step_results[action] = self._simulate_tool_result(action, query)\n        new_state['step_results'] = step_results\n        \n        # Estimate tool relevance based on learning or semantic similarity\n        tool_relevance = self._estimate_tool_relevance(action, query, query_type)\n        \n        # Check for sequence effects (tools that work well together)\n        sequence_bonus = 0.0\n        if len(new_actions) > 1:\n            prev_tool = new_actions[-2]\n            sequence_key = f\"{prev_tool}->{action}\"\n            if sequence_key in self.sequence_effectiveness:\n                sequence_bonus = self.sequence_effectiveness[sequence_key] * 0.3  # 30% weight for sequence effects\n        \n        # Update quality based on relevance and sequence effects\n        current_quality = state['response_quality']\n        quality_improvement = tool_relevance + sequence_bonus\n        \n        # Add diminishing returns effect for additional tools\n        if len(new_actions) > 1:\n            diminishing_factor = 1.0 / len(new_actions)\n            quality_improvement *= diminishing_factor\n        \n        new_quality = min(1.0, current_quality + quality_improvement)\n        new_state['response_quality'] = new_quality\n        \n        return new_state\n    \n    def _simulate_tool_result(self, tool_name: str, query: str) -> Dict[str, Any]:\n        \"\"\"\n        Simulate the result of using a tool for a query.\n        \n        Args:\n            tool_name: The name of the tool\n            query: The user query\n            \n        Returns:\n            A simulated result\n        \"\"\"\n        # In a real implementation, this would be a more sophisticated simulation\n        return {\n            \"tool\": tool_name,\n            \"success_probability\": self._estimate_tool_relevance(tool_name, query),\n            \"simulated\": True\n        }\n    \n    def _estimate_tool_relevance(self, tool_name: str, query: str, query_type: str = None) -> float:\n        \"\"\"\n        Estimate how relevant a tool is for a given query using history and semantics.\n        \n        Args:\n            tool_name: The name of the tool\n            query: The user query\n            query_type: Optional query type\n            \n        Returns:\n            A relevance score between 0.0 and 1.0\n        \"\"\"\n        relevance_score = 0.0\n        \n        # If we have historical data for this query type\n        if self.use_learning and query_type and query_type in self.tool_history and \\\n           tool_name in self.tool_history[query_type]:\n            \n            # Get historical success rate\n            history_score = self.tool_history[query_type][tool_name]['success_rate']\n            relevance_score += history_score * self.tool_history_weight\n        \n        # If we're using semantic similarity\n        if self.use_semantic_similarity and tool_name in self.tool_fingerprints:\n            fingerprint = self.tool_fingerprints[tool_name]\n            \n            # Calculate keyword overlap\n            query_keywords = set(word.lower() for word in query.split() if len(word) > 3)\n            keyword_overlap = len(query_keywords.intersection(fingerprint['keywords']))\n            \n            # Normalize by query keywords\n            if query_keywords:\n                semantic_score = keyword_overlap / len(query_keywords)\n                relevance_score += semantic_score * (1.0 - self.tool_history_weight)\n        \n        # Ensure we have a minimum score for exploration\n        if relevance_score < 0.1:\n            relevance_score = 0.1 + (random.random() * 0.1)  # Random boost between 0.1-0.2\n        \n        return relevance_score\n    \n    def _extract_plan_from_search(self) -> List[str]:\n        \"\"\"\n        Extract a complete plan (tool sequence) from the search results.\n        \n        Returns:\n            A list of tool names representing the plan\n        \"\"\"\n        # In a real implementation, this would extract the highest value path \n        # from the search tree. For now, return None to indicate no plan extraction.\n        return None\n    \n    def update_tool_history(self, tool_name: str, query: str, success: bool, \n                          execution_time: float, result: Any = None):\n        \"\"\"\n        Update the tool history with the results of using a tool.\n        \n        Args:\n            tool_name: The name of the tool used\n            query: The query the tool was used for\n            success: Whether the tool was successful\n            execution_time: The execution time in seconds\n            result: Optional result of the tool execution\n        \"\"\"\n        if not self.use_learning:\n            return\n            \n        # Get query type\n        query_type = self._analyze_query(query)\n        \n        # Initialize history entry if needed\n        if query_type not in self.tool_history:\n            self.tool_history[query_type] = {}\n            \n        if tool_name not in self.tool_history[query_type]:\n            self.tool_history[query_type][tool_name] = {\n                'success_count': 0,\n                'failure_count': 0,\n                'total_time': 0.0,\n                'success_rate': 0.0,\n                'avg_time': 0.0,\n                'examples': []\n            }\n        \n        # Update history\n        history = self.tool_history[query_type][tool_name]\n        \n        # Update counts\n        if success:\n            history['success_count'] += 1\n        else:\n            history['failure_count'] += 1\n            \n        # Update time\n        history['total_time'] += execution_time\n        \n        # Update success rate\n        total = history['success_count'] + history['failure_count']\n        history['success_rate'] = history['success_count'] / total if total > 0 else 0.0\n        \n        # Update average time\n        history['avg_time'] = history['total_time'] / total if total > 0 else 0.0\n        \n        # Add example (limit to last 5)\n        history['examples'].append({\n            'query': query,\n            'success': success,\n            'timestamp': self._get_timestamp()\n        })\n        if len(history['examples']) > 5:\n            history['examples'].pop(0)\n            \n        # Update tool fingerprint\n        if self.use_semantic_similarity and tool_name in self.tool_fingerprints:\n            if success:\n                # Add query type to usage contexts\n                self.tool_fingerprints[tool_name]['usage_contexts'].add(query_type)\n                \n                # Add query keywords to tool fingerprint (with decay)\n                query_keywords = set(word.lower() for word in query.split() if len(word) > 3)\n                current_keywords = self.tool_fingerprints[tool_name]['keywords']\n                \n                # Add new keywords with adaptation rate\n                for keyword in query_keywords:\n                    if keyword not in current_keywords:\n                        if random.random() < self.adaptation_rate:\n                            current_keywords.add(keyword)\n    \n    def update_sequence_effectiveness(self, tool_sequence: List[str], success: bool, quality_score: float):\n        \"\"\"\n        Update the effectiveness record for a sequence of tools.\n        \n        Args:\n            tool_sequence: The sequence of tools used\n            success: Whether the sequence was successful\n            quality_score: A quality score for the sequence\n        \"\"\"\n        if not self.use_learning or len(tool_sequence) < 2:\n            return\n            \n        # Update pairwise effectiveness\n        for i in range(len(tool_sequence) - 1):\n            first_tool = tool_sequence[i]\n            second_tool = tool_sequence[i + 1]\n            sequence_key = f\"{first_tool}->{second_tool}\"\n            \n            if sequence_key not in self.sequence_effectiveness:\n                self.sequence_effectiveness[sequence_key] = 0.5  # Initial neutral score\n                \n            # Update score with decay\n            current_score = self.sequence_effectiveness[sequence_key]\n            if success:\n                # Increase score with quality bonus\n                new_score = current_score + self.adaptation_rate * quality_score\n            else:\n                # Decrease score\n                new_score = current_score - self.adaptation_rate\n                \n            # Clamp between 0 and 1\n            self.sequence_effectiveness[sequence_key] = max(0.0, min(1.0, new_score))"}
{"type": "source_file", "path": "claude_code/lib/tools/manager.py", "content": "#!/usr/bin/env python3\n# claude_code/lib/tools/manager.py\n\"\"\"Tool execution manager.\"\"\"\n\nimport logging\nimport time\nimport json\nimport uuid\nimport os\nfrom typing import Dict, List, Any, Optional, Callable, Union, Sequence\nimport concurrent.futures\nfrom concurrent.futures import ThreadPoolExecutor, Future\n\nfrom .base import Tool, ToolResult, ToolRegistry, Routine, RoutineStep, RoutineDefinition\n\nlogger = logging.getLogger(__name__)\n\n\nclass RoutineExecutionManager:\n    \"\"\"Manages the execution of tool routines.\"\"\"\n    \n    def __init__(self, registry: ToolRegistry, execution_manager: 'ToolExecutionManager'):\n        \"\"\"Initialize the routine execution manager.\n        \n        Args:\n            registry: Tool registry containing available tools and routines\n            execution_manager: Tool execution manager for executing individual tools\n        \"\"\"\n        self.registry = registry\n        self.execution_manager = execution_manager\n        self.active_routines: Dict[str, Dict[str, Any]] = {}\n        self.progress_callback: Optional[Callable[[str, str, float], None]] = None\n        self.result_callback: Optional[Callable[[str, List[ToolResult]], None]] = None\n        \n        # Load existing routines\n        self.registry.load_routines()\n    \n    def set_progress_callback(self, callback: Callable[[str, str, float], None]) -> None:\n        \"\"\"Set a callback function for routine progress updates.\n        \n        Args:\n            callback: Function that takes routine_id, step_name, and progress (0-1) as arguments\n        \"\"\"\n        self.progress_callback = callback\n    \n    def set_result_callback(self, callback: Callable[[str, List[ToolResult]], None]) -> None:\n        \"\"\"Set a callback function for routine results.\n        \n        Args:\n            callback: Function that takes routine_id and list of ToolResults as arguments\n        \"\"\"\n        self.result_callback = callback\n    \n    def create_routine(self, definition: RoutineDefinition) -> str:\n        \"\"\"Create a new routine from a definition.\n        \n        Args:\n            definition: Routine definition\n            \n        Returns:\n            Routine ID\n            \n        Raises:\n            ValueError: If a routine with the same name already exists\n        \"\"\"\n        # Convert step objects to dictionaries\n        steps = []\n        for step in definition.steps:\n            step_dict = {\n                \"tool_name\": step.tool_name,\n                \"args\": step.args\n            }\n            if step.condition is not None:\n                step_dict[\"condition\"] = step.condition\n            if step.store_result:\n                step_dict[\"store_result\"] = True\n                if step.result_var is not None:\n                    step_dict[\"result_var\"] = step.result_var\n            \n            steps.append(step_dict)\n        \n        # Create routine\n        routine = Routine(\n            name=definition.name,\n            description=definition.description,\n            steps=steps\n        )\n        \n        # Register routine\n        self.registry.register_routine(routine)\n        \n        return routine.name\n    \n    def create_routine_from_tool_history(\n        self, \n        name: str, \n        description: str, \n        tool_results: List[ToolResult],\n        context_variables: Dict[str, Any] = None\n    ) -> str:\n        \"\"\"Create a routine from a history of tool executions.\n        \n        Args:\n            name: Name for the routine\n            description: Description of the routine\n            tool_results: List of tool results to base the routine on\n            context_variables: Optional dictionary of context variables to identify\n            \n        Returns:\n            Routine ID\n        \"\"\"\n        steps = []\n        \n        # Process tool results into steps\n        for i, result in enumerate(tool_results):\n            # Skip failed tool calls\n            if result.status != \"success\":\n                continue\n            \n            # Get tool\n            tool = self.registry.get_tool(result.name)\n            if not tool:\n                continue\n            \n            # Extract arguments from tool call\n            args = {}\n            # Here we would need to extract the arguments from the tool call\n            # This is a simplification and would need to be adapted to the actual structure\n            \n            # Create step\n            step = {\n                \"tool_name\": result.name,\n                \"args\": args,\n                \"store_result\": True,\n                \"result_var\": f\"result_{i}\"\n            }\n            \n            steps.append(step)\n        \n        # Create routine\n        routine = Routine(\n            name=name,\n            description=description,\n            steps=steps\n        )\n        \n        # Register routine\n        self.registry.register_routine(routine)\n        \n        return routine.name\n    \n    def execute_routine(self, name: str, context: Dict[str, Any] = None) -> str:\n        \"\"\"Execute a routine with the given context.\n        \n        Args:\n            name: Name of the routine to execute\n            context: Context variables for the routine\n            \n        Returns:\n            Routine execution ID\n            \n        Raises:\n            ValueError: If the routine is not found\n        \"\"\"\n        # Get routine\n        routine = self.registry.get_routine(name)\n        if not routine:\n            raise ValueError(f\"Routine not found: {name}\")\n        \n        # Create execution ID\n        execution_id = str(uuid.uuid4())\n        \n        # Initialize context\n        if context is None:\n            context = {}\n        \n        # Initialize execution state\n        self.active_routines[execution_id] = {\n            \"routine\": routine,\n            \"context\": context.copy(),\n            \"results\": [],\n            \"current_step\": 0,\n            \"start_time\": time.time(),\n            \"status\": \"running\"\n        }\n        \n        # Record routine usage\n        self.registry.record_routine_usage(name)\n        \n        # Start execution in background thread\n        executor = ThreadPoolExecutor(max_workers=1)\n        executor.submit(self._execute_routine_steps, execution_id)\n        \n        return execution_id\n    \n    def _execute_routine_steps(self, execution_id: str) -> None:\n        \"\"\"Execute the steps of a routine in sequence.\n        \n        Args:\n            execution_id: Routine execution ID\n        \"\"\"\n        if execution_id not in self.active_routines:\n            logger.error(f\"Routine execution not found: {execution_id}\")\n            return\n        \n        execution = self.active_routines[execution_id]\n        routine = execution[\"routine\"]\n        context = execution[\"context\"]\n        results = execution[\"results\"]\n        \n        try:\n            # Execute each step\n            for i, step in enumerate(routine.steps):\n                # Update current step\n                execution[\"current_step\"] = i\n                \n                # Check for conditions\n                if \"condition\" in step and not self._evaluate_condition(step[\"condition\"], context, results):\n                    logger.info(f\"Skipping step {i+1}/{len(routine.steps)} due to condition\")\n                    continue\n                \n                # Process tool arguments with variable substitution\n                processed_args = self._process_arguments(step[\"args\"], context, results)\n                \n                # Create tool call\n                tool_call = {\n                    \"id\": f\"{execution_id}_{i}\",\n                    \"function\": {\n                        \"name\": step[\"tool_name\"],\n                        \"arguments\": json.dumps(processed_args)\n                    }\n                }\n                \n                # Report progress\n                self._report_routine_progress(execution_id, i, len(routine.steps), step[\"tool_name\"])\n                \n                # Execute tool\n                result = self.execution_manager.execute_tool(tool_call)\n                \n                # Add result to results\n                results.append(result)\n                \n                # Store result in context if requested\n                if step.get(\"store_result\", False):\n                    var_name = step.get(\"result_var\", f\"result_{i}\")\n                    context[var_name] = result.result\n                \n                # Check for loop control\n                if \"repeat_until\" in step and not self._evaluate_condition(step[\"repeat_until\"], context, results):\n                    # Go back to specified step\n                    target_step = step.get(\"repeat_target\", 0)\n                    if 0 <= target_step < i:\n                        i = target_step - 1  # Will be incremented in next loop iteration\n                \n                # Check for exit condition\n                if \"exit_condition\" in step and self._evaluate_condition(step[\"exit_condition\"], context, results):\n                    logger.info(f\"Exiting routine early due to exit condition at step {i+1}/{len(routine.steps)}\")\n                    break\n            \n            # Update execution status\n            execution[\"status\"] = \"completed\"\n            \n            # Report final progress\n            self._report_routine_progress(execution_id, len(routine.steps), len(routine.steps), \"completed\")\n            \n            # Call result callback\n            if self.result_callback:\n                self.result_callback(execution_id, results)\n                \n        except Exception as e:\n            logger.exception(f\"Error executing routine: {e}\")\n            execution[\"status\"] = \"error\"\n            execution[\"error\"] = str(e)\n            \n            # Report error progress\n            self._report_routine_progress(execution_id, execution[\"current_step\"], len(routine.steps), \"error\")\n    \n    def _process_arguments(\n        self,\n        args: Dict[str, Any],\n        context: Dict[str, Any],\n        results: List[ToolResult]\n    ) -> Dict[str, Any]:\n        \"\"\"Process tool arguments with variable substitution.\n        \n        Args:\n            args: Tool arguments\n            context: Context variables\n            results: Previous tool results\n            \n        Returns:\n            Processed arguments\n        \"\"\"\n        processed_args = {}\n        \n        for key, value in args.items():\n            if isinstance(value, str) and value.startswith(\"$\"):\n                # Variable reference\n                var_name = value[1:]\n                if var_name in context:\n                    processed_args[key] = context[var_name]\n                elif var_name.startswith(\"result[\") and var_name.endswith(\"]\"):\n                    # Reference to previous result\n                    try:\n                        idx = int(var_name[7:-1])\n                        if 0 <= idx < len(results):\n                            processed_args[key] = results[idx].result\n                        else:\n                            processed_args[key] = value\n                    except (ValueError, IndexError):\n                        processed_args[key] = value\n                else:\n                    processed_args[key] = value\n            else:\n                processed_args[key] = value\n        \n        return processed_args\n    \n    def _evaluate_condition(\n        self,\n        condition: Dict[str, Any],\n        context: Dict[str, Any],\n        results: List[ToolResult]\n    ) -> bool:\n        \"\"\"Evaluate a condition for a routine step.\n        \n        Args:\n            condition: Condition specification\n            context: Context variables\n            results: Previous tool results\n            \n        Returns:\n            Whether the condition is met\n        \"\"\"\n        condition_type = condition.get(\"type\", \"simple\")\n        \n        if condition_type == \"simple\":\n            # Simple variable comparison\n            var_name = condition.get(\"variable\", \"\")\n            operation = condition.get(\"operation\", \"equals\")\n            value = condition.get(\"value\")\n            \n            # Get variable value\n            var_value = None\n            if var_name.startswith(\"$\"):\n                var_name = var_name[1:]\n                var_value = context.get(var_name)\n            elif var_name.startswith(\"result[\") and var_name.endswith(\"]\"):\n                try:\n                    idx = int(var_name[7:-1])\n                    if 0 <= idx < len(results):\n                        var_value = results[idx].result\n                except (ValueError, IndexError):\n                    return False\n            \n            # Compare\n            if operation == \"equals\":\n                return var_value == value\n            elif operation == \"not_equals\":\n                return var_value != value\n            elif operation == \"contains\":\n                return value in var_value if var_value is not None else False\n            elif operation == \"greater_than\":\n                return var_value > value if var_value is not None else False\n            elif operation == \"less_than\":\n                return var_value < value if var_value is not None else False\n            \n            return False\n        \n        elif condition_type == \"and\":\n            # Logical AND of multiple conditions\n            sub_conditions = condition.get(\"conditions\", [])\n            return all(self._evaluate_condition(c, context, results) for c in sub_conditions)\n        \n        elif condition_type == \"or\":\n            # Logical OR of multiple conditions\n            sub_conditions = condition.get(\"conditions\", [])\n            return any(self._evaluate_condition(c, context, results) for c in sub_conditions)\n        \n        elif condition_type == \"not\":\n            # Logical NOT\n            sub_condition = condition.get(\"condition\", {})\n            return not self._evaluate_condition(sub_condition, context, results)\n        \n        return True  # Default to True\n    \n    def _report_routine_progress(\n        self,\n        execution_id: str,\n        current_step: int,\n        total_steps: int,\n        step_name: str\n    ) -> None:\n        \"\"\"Report progress for a routine execution.\n        \n        Args:\n            execution_id: Routine execution ID\n            current_step: Current step index\n            total_steps: Total number of steps\n            step_name: Name of the current step\n        \"\"\"\n        progress = current_step / total_steps if total_steps > 0 else 1.0\n        \n        # Call progress callback if set\n        if self.progress_callback:\n            self.progress_callback(execution_id, step_name, progress)\n    \n    def get_active_routines(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get information about active routine executions.\n        \n        Returns:\n            Dictionary mapping execution ID to routine execution information\n        \"\"\"\n        return {\n            k: {\n                \"routine_name\": v[\"routine\"].name,\n                \"current_step\": v[\"current_step\"],\n                \"total_steps\": len(v[\"routine\"].steps),\n                \"status\": v[\"status\"],\n                \"start_time\": v[\"start_time\"],\n                \"elapsed_time\": time.time() - v[\"start_time\"]\n            }\n            for k, v in self.active_routines.items()\n        }\n    \n    def get_routine_results(self, execution_id: str) -> Optional[List[ToolResult]]:\n        \"\"\"Get the results of a routine execution.\n        \n        Args:\n            execution_id: Routine execution ID\n            \n        Returns:\n            List of tool results, or None if the routine execution is not found\n        \"\"\"\n        if execution_id in self.active_routines:\n            return self.active_routines[execution_id][\"results\"]\n        return None\n    \n    def cancel_routine(self, execution_id: str) -> bool:\n        \"\"\"Cancel a routine execution.\n        \n        Args:\n            execution_id: Routine execution ID\n            \n        Returns:\n            Whether the routine was canceled successfully\n        \"\"\"\n        if execution_id in self.active_routines:\n            self.active_routines[execution_id][\"status\"] = \"canceled\"\n            return True\n        return False\n\n\nclass ToolExecutionManager:\n    \"\"\"Manages tool execution, including parallel execution and progress tracking.\"\"\"\n    \n    def __init__(self, registry: ToolRegistry):\n        \"\"\"Initialize the tool execution manager.\n        \n        Args:\n            registry: Tool registry containing available tools\n        \"\"\"\n        self.registry = registry\n        self.active_executions: Dict[str, Dict[str, Any]] = {}\n        self.progress_callback: Optional[Callable[[str, float], None]] = None\n        self.result_callback: Optional[Callable[[str, ToolResult], None]] = None\n        self.max_workers = 10\n        \n        # Initialize routine manager\n        self.routine_manager = RoutineExecutionManager(registry, self)\n    \n    def set_progress_callback(self, callback: Callable[[str, float], None]) -> None:\n        \"\"\"Set a callback function for progress updates.\n        \n        Args:\n            callback: Function that takes tool_call_id and progress (0-1) as arguments\n        \"\"\"\n        self.progress_callback = callback\n    \n    def set_result_callback(self, callback: Callable[[str, ToolResult], None]) -> None:\n        \"\"\"Set a callback function for results.\n        \n        Args:\n            callback: Function that takes tool_call_id and ToolResult as arguments\n        \"\"\"\n        self.result_callback = callback\n    \n    def execute_tool(self, tool_call: Dict[str, Any]) -> ToolResult:\n        \"\"\"Execute a single tool synchronously.\n        \n        Args:\n            tool_call: Dictionary containing tool call information\n            \n        Returns:\n            ToolResult with execution result\n            \n        Raises:\n            ValueError: If the tool is not found\n        \"\"\"\n        function_name = tool_call.get(\"function\", {}).get(\"name\", \"\")\n        tool_call_id = tool_call.get(\"id\", \"unknown\")\n        \n        # Check if it's a routine\n        if function_name.startswith(\"routine.\"):\n            routine_name = function_name[9:]  # Remove \"routine.\" prefix\n            return self._execute_routine_as_tool(routine_name, tool_call)\n        \n        # Get the tool\n        tool = self.registry.get_tool(function_name)\n        if not tool:\n            error_msg = f\"Tool not found: {function_name}\"\n            logger.error(error_msg)\n            return ToolResult(\n                tool_call_id=tool_call_id,\n                name=function_name,\n                result=f\"Error: {error_msg}\",\n                execution_time=0,\n                status=\"error\",\n                error=error_msg\n            )\n        \n        # Check if tool needs permission and handle it\n        if tool.needs_permission:\n            # TODO: Implement permission handling\n            logger.warning(f\"Tool {function_name} needs permission, but permission handling is not implemented\")\n        \n        # Track progress\n        self._track_execution_start(tool_call_id, function_name)\n        \n        try:\n            # Execute the tool\n            result = tool.execute(tool_call)\n            \n            # Track completion\n            self._track_execution_complete(tool_call_id, result)\n            \n            return result\n        except Exception as e:\n            logger.exception(f\"Error executing tool {function_name}: {e}\")\n            result = ToolResult(\n                tool_call_id=tool_call_id,\n                name=function_name,\n                result=f\"Error: {str(e)}\",\n                execution_time=0,\n                status=\"error\",\n                error=str(e)\n            )\n            self._track_execution_complete(tool_call_id, result)\n            return result\n    \n    def _execute_routine_as_tool(self, routine_name: str, tool_call: Dict[str, Any]) -> ToolResult:\n        \"\"\"Execute a routine as if it were a tool.\n        \n        Args:\n            routine_name: Name of the routine\n            tool_call: Dictionary containing tool call information\n            \n        Returns:\n            ToolResult with execution result\n        \"\"\"\n        tool_call_id = tool_call.get(\"id\", \"unknown\")\n        start_time = time.time()\n        \n        try:\n            # Extract context from arguments\n            arguments_str = tool_call.get(\"function\", {}).get(\"arguments\", \"{}\")\n            try:\n                context = json.loads(arguments_str)\n            except json.JSONDecodeError:\n                context = {}\n            \n            # Execute routine\n            execution_id = self.routine_manager.execute_routine(routine_name, context)\n            \n            # Wait for routine to complete\n            while True:\n                routine_status = self.routine_manager.get_active_routines().get(execution_id, {})\n                if routine_status.get(\"status\") != \"running\":\n                    break\n                time.sleep(0.1)\n            \n            # Get results\n            results = self.routine_manager.get_routine_results(execution_id)\n            if not results:\n                raise ValueError(f\"No results from routine: {routine_name}\")\n            \n            # Format results\n            result_summary = f\"Routine {routine_name} executed successfully with {len(results)} steps\\n\\n\"\n            for i, result in enumerate(results):\n                result_summary += f\"Step {i+1}: {result.name} - {'SUCCESS' if result.status == 'success' else 'ERROR'}\\n\"\n                if result.status != \"success\":\n                    result_summary += f\"  Error: {result.error}\\n\"\n            \n            # Track execution time\n            execution_time = time.time() - start_time\n            \n            # Create result\n            return ToolResult(\n                tool_call_id=tool_call_id,\n                name=f\"routine.{routine_name}\",\n                result=result_summary,\n                execution_time=execution_time,\n                status=\"success\"\n            )\n        \n        except Exception as e:\n            logger.exception(f\"Error executing routine {routine_name}: {e}\")\n            return ToolResult(\n                tool_call_id=tool_call_id,\n                name=f\"routine.{routine_name}\",\n                result=f\"Error: {str(e)}\",\n                execution_time=time.time() - start_time,\n                status=\"error\",\n                error=str(e)\n            )\n    \n    def execute_tools_parallel(self, tool_calls: List[Dict[str, Any]]) -> List[ToolResult]:\n        \"\"\"Execute multiple tools in parallel.\n        \n        Args:\n            tool_calls: List of dictionaries containing tool call information\n            \n        Returns:\n            List of ToolResult with execution results\n        \"\"\"\n        results = []\n        futures: Dict[Future, str] = {}\n        \n        # Use ThreadPoolExecutor for parallel execution\n        with ThreadPoolExecutor(max_workers=min(self.max_workers, len(tool_calls))) as executor:\n            # Submit all tool calls\n            for tool_call in tool_calls:\n                tool_call_id = tool_call.get(\"id\", \"unknown\")\n                future = executor.submit(self.execute_tool, tool_call)\n                futures[future] = tool_call_id\n            \n            # Wait for completion and collect results\n            for future in concurrent.futures.as_completed(futures):\n                tool_call_id = futures[future]\n                try:\n                    result = future.result()\n                    results.append(result)\n                except Exception as e:\n                    logger.exception(f\"Error in parallel tool execution for {tool_call_id}: {e}\")\n                    # Create an error result\n                    function_name = next(\n                        (tc.get(\"function\", {}).get(\"name\", \"\") for tc in tool_calls \n                         if tc.get(\"id\", \"\") == tool_call_id), \n                        \"unknown\"\n                    )\n                    results.append(ToolResult(\n                        tool_call_id=tool_call_id,\n                        name=function_name,\n                        result=f\"Error: {str(e)}\",\n                        execution_time=0,\n                        status=\"error\",\n                        error=str(e)\n                    ))\n        \n        return results\n    \n    def create_routine(self, definition: RoutineDefinition) -> str:\n        \"\"\"Create a new routine.\n        \n        Args:\n            definition: Routine definition\n            \n        Returns:\n            Routine ID\n        \"\"\"\n        return self.routine_manager.create_routine(definition)\n    \n    def create_routine_from_tool_history(\n        self, \n        name: str, \n        description: str, \n        tool_results: List[ToolResult],\n        context_variables: Dict[str, Any] = None\n    ) -> str:\n        \"\"\"Create a routine from a history of tool executions.\n        \n        Args:\n            name: Name for the routine\n            description: Description of the routine\n            tool_results: List of tool results to base the routine on\n            context_variables: Optional dictionary of context variables to identify\n            \n        Returns:\n            Routine ID\n        \"\"\"\n        return self.routine_manager.create_routine_from_tool_history(\n            name, description, tool_results, context_variables\n        )\n    \n    def execute_routine(self, name: str, context: Dict[str, Any] = None) -> str:\n        \"\"\"Execute a routine with the given context.\n        \n        Args:\n            name: Name of the routine to execute\n            context: Context variables for the routine\n            \n        Returns:\n            Routine execution ID\n        \"\"\"\n        return self.routine_manager.execute_routine(name, context)\n    \n    def get_routine_results(self, execution_id: str) -> Optional[List[ToolResult]]:\n        \"\"\"Get the results of a routine execution.\n        \n        Args:\n            execution_id: Routine execution ID\n            \n        Returns:\n            List of tool results, or None if the routine execution is not found\n        \"\"\"\n        return self.routine_manager.get_routine_results(execution_id)\n    \n    def _track_execution_start(self, tool_call_id: str, tool_name: str) -> None:\n        \"\"\"Track the start of tool execution.\n        \n        Args:\n            tool_call_id: ID of the tool call\n            tool_name: Name of the tool\n        \"\"\"\n        self.active_executions[tool_call_id] = {\n            \"tool_name\": tool_name,\n            \"start_time\": time.time(),\n            \"progress\": 0.0\n        }\n        \n        # Call progress callback if set\n        if self.progress_callback:\n            self.progress_callback(tool_call_id, 0.0)\n    \n    def _track_execution_progress(self, tool_call_id: str, progress: float) -> None:\n        \"\"\"Track the progress of tool execution.\n        \n        Args:\n            tool_call_id: ID of the tool call\n            progress: Progress value (0-1)\n        \"\"\"\n        if tool_call_id in self.active_executions:\n            self.active_executions[tool_call_id][\"progress\"] = progress\n            \n            # Call progress callback if set\n            if self.progress_callback:\n                self.progress_callback(tool_call_id, progress)\n    \n    def _track_execution_complete(self, tool_call_id: str, result: ToolResult) -> None:\n        \"\"\"Track the completion of tool execution.\n        \n        Args:\n            tool_call_id: ID of the tool call\n            result: Tool execution result\n        \"\"\"\n        if tool_call_id in self.active_executions:\n            # Update progress\n            self._track_execution_progress(tool_call_id, 1.0)\n            \n            # Calculate execution time\n            start_time = self.active_executions[tool_call_id][\"start_time\"]\n            execution_time = time.time() - start_time\n            \n            # Clean up\n            del self.active_executions[tool_call_id]\n            \n            # Call result callback if set\n            if self.result_callback:\n                self.result_callback(tool_call_id, result)\n    \n    def get_active_executions(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get information about active tool executions.\n        \n        Returns:\n            Dictionary mapping tool_call_id to execution information\n        \"\"\"\n        return self.active_executions.copy()\n    \n    def cancel_execution(self, tool_call_id: str) -> bool:\n        \"\"\"Cancel a tool execution if possible.\n        \n        Args:\n            tool_call_id: ID of the tool call to cancel\n            \n        Returns:\n            True if canceled successfully, False otherwise\n        \"\"\"\n        # TODO: Implement cancellation logic\n        # This would require more sophisticated execution tracking\n        logger.warning(f\"Cancellation not implemented for tool_call_id: {tool_call_id}\")\n        return False"}
{"type": "source_file", "path": "claude_code/lib/tools/file_tools.py", "content": "#!/usr/bin/env python3\n# claude_code/lib/tools/file_tools.py\n\"\"\"File operation tools.\"\"\"\n\nimport os\nimport logging\nfrom typing import Dict, List, Optional, Any\n\nfrom .base import tool, ToolRegistry\n\nlogger = logging.getLogger(__name__)\n\n\n@tool(\n    name=\"View\",\n    description=\"Reads a file from the local filesystem. The file_path parameter must be an absolute path, not a relative path.\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"file_path\": {\n                \"type\": \"string\",\n                \"description\": \"The absolute path to the file to read\"\n            },\n            \"limit\": {\n                \"type\": \"number\",\n                \"description\": \"The number of lines to read. Only provide if the file is too large to read at once.\"\n            },\n            \"offset\": {\n                \"type\": \"number\",\n                \"description\": \"The line number to start reading from. Only provide if the file is too large to read at once\"\n            }\n        },\n        \"required\": [\"file_path\"]\n    },\n    category=\"file\"\n)\ndef view_file(file_path: str, limit: Optional[int] = None, offset: Optional[int] = 0) -> str:\n    \"\"\"Read contents of a file.\n    \n    Args:\n        file_path: Absolute path to the file\n        limit: Maximum number of lines to read\n        offset: Line number to start reading from\n        \n    Returns:\n        File contents as a string\n        \n    Raises:\n        FileNotFoundError: If the file doesn't exist\n        PermissionError: If the file can't be read\n    \"\"\"\n    logger.info(f\"Reading file: {file_path} (offset={offset}, limit={limit})\")\n    \n    if not os.path.isabs(file_path):\n        return f\"Error: File path must be absolute: {file_path}\"\n    \n    if not os.path.exists(file_path):\n        return f\"Error: File not found: {file_path}\"\n    \n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n            if limit is not None and offset is not None:\n                # Skip to offset\n                for _ in range(offset):\n                    next(f, None)\n                \n                # Read limited lines\n                lines = []\n                for _ in range(limit):\n                    line = next(f, None)\n                    if line is None:\n                        break\n                    lines.append(line)\n                content = ''.join(lines)\n            else:\n                content = f.read()\n        \n        return content\n    except Exception as e:\n        logger.exception(f\"Error reading file: {file_path}\")\n        return f\"Error reading file: {str(e)}\"\n\n\n@tool(\n    name=\"Edit\",\n    description=\"This is a tool for editing files. For moving or renaming files, you should generally use the Bash tool with the 'mv' command instead.\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"file_path\": {\n                \"type\": \"string\",\n                \"description\": \"The absolute path to the file to modify\"\n            },\n            \"old_string\": {\n                \"type\": \"string\",\n                \"description\": \"The text to replace\"\n            },\n            \"new_string\": {\n                \"type\": \"string\",\n                \"description\": \"The text to replace it with\"\n            }\n        },\n        \"required\": [\"file_path\", \"old_string\", \"new_string\"]\n    },\n    needs_permission=True,\n    category=\"file\"\n)\ndef edit_file(file_path: str, old_string: str, new_string: str) -> str:\n    \"\"\"Edit a file by replacing text.\n    \n    Args:\n        file_path: Absolute path to the file\n        old_string: Text to replace\n        new_string: Replacement text\n        \n    Returns:\n        Success or error message\n        \n    Raises:\n        FileNotFoundError: If the file doesn't exist\n        PermissionError: If the file can't be modified\n    \"\"\"\n    logger.info(f\"Editing file: {file_path}\")\n    \n    if not os.path.isabs(file_path):\n        return f\"Error: File path must be absolute: {file_path}\"\n    \n    try:\n        # Create directory if creating new file\n        if not os.path.exists(os.path.dirname(file_path)) and old_string == \"\":\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n            \n        if old_string == \"\" and not os.path.exists(file_path):\n            # Creating new file\n            with open(file_path, 'w', encoding='utf-8') as f:\n                f.write(new_string)\n            return f\"Created new file: {file_path}\"\n        \n        # Reading existing file\n        if not os.path.exists(file_path):\n            return f\"Error: File not found: {file_path}\"\n        \n        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n            content = f.read()\n        \n        # Replace string\n        if old_string not in content:\n            return f\"Error: Could not find the specified text in {file_path}\"\n        \n        # Count occurrences to ensure uniqueness\n        occurrences = content.count(old_string)\n        if occurrences > 1:\n            return f\"Error: Found {occurrences} occurrences of the specified text in {file_path}. Please provide more context to uniquely identify the text to replace.\"\n        \n        new_content = content.replace(old_string, new_string)\n        \n        # Write back to file\n        with open(file_path, 'w', encoding='utf-8') as f:\n            f.write(new_content)\n        \n        return f\"Successfully edited {file_path}\"\n    \n    except Exception as e:\n        logger.exception(f\"Error editing file: {file_path}\")\n        return f\"Error editing file: {str(e)}\"\n\n\n@tool(\n    name=\"Replace\",\n    description=\"Write a file to the local filesystem. Overwrites the existing file if there is one.\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"file_path\": {\n                \"type\": \"string\",\n                \"description\": \"The absolute path to the file to write\"\n            },\n            \"content\": {\n                \"type\": \"string\",\n                \"description\": \"The content to write to the file\"\n            }\n        },\n        \"required\": [\"file_path\", \"content\"]\n    },\n    needs_permission=True,\n    category=\"file\"\n)\ndef replace_file(file_path: str, content: str) -> str:\n    \"\"\"Replace file contents or create a new file.\n    \n    Args:\n        file_path: Absolute path to the file\n        content: New content for the file\n        \n    Returns:\n        Success or error message\n        \n    Raises:\n        PermissionError: If the file can't be written\n    \"\"\"\n    logger.info(f\"Replacing file: {file_path}\")\n    \n    if not os.path.isabs(file_path):\n        return f\"Error: File path must be absolute: {file_path}\"\n    \n    try:\n        # Create directory if it doesn't exist\n        directory = os.path.dirname(file_path)\n        if directory and not os.path.exists(directory):\n            os.makedirs(directory, exist_ok=True)\n        \n        # Write content to file\n        with open(file_path, 'w', encoding='utf-8') as f:\n            f.write(content)\n        \n        return f\"Successfully wrote to {file_path}\"\n    \n    except Exception as e:\n        logger.exception(f\"Error writing file: {file_path}\")\n        return f\"Error writing file: {str(e)}\"\n\n\n@tool(\n    name=\"MakeDirectory\",\n    description=\"Create a new directory on the local filesystem.\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"directory_path\": {\n                \"type\": \"string\",\n                \"description\": \"The absolute path to the directory to create\"\n            },\n            \"parents\": {\n                \"type\": \"boolean\",\n                \"description\": \"Whether to create parent directories if they don't exist\",\n                \"default\": True\n            },\n            \"mode\": {\n                \"type\": \"integer\",\n                \"description\": \"The file mode (permissions) to set for the directory (octal)\",\n                \"default\": 0o755\n            }\n        },\n        \"required\": [\"directory_path\"]\n    },\n    needs_permission=True,\n    category=\"file\"\n)\ndef make_directory(directory_path: str, parents: bool = True, mode: int = 0o755) -> str:\n    \"\"\"Create a new directory.\n    \n    Args:\n        directory_path: Absolute path to the directory to create\n        parents: Whether to create parent directories\n        mode: File mode (permissions) to set\n        \n    Returns:\n        Success or error message\n        \n    Raises:\n        PermissionError: If the directory can't be created\n    \"\"\"\n    logger.info(f\"Creating directory: {directory_path}\")\n    \n    if not os.path.isabs(directory_path):\n        return f\"Error: Directory path must be absolute: {directory_path}\"\n    \n    try:\n        if os.path.exists(directory_path):\n            if os.path.isdir(directory_path):\n                return f\"Directory already exists: {directory_path}\"\n            else:\n                return f\"Error: Path exists but is not a directory: {directory_path}\"\n        \n        # Create directory\n        os.makedirs(directory_path, exist_ok=parents, mode=mode)\n        \n        return f\"Successfully created directory: {directory_path}\"\n    \n    except Exception as e:\n        logger.exception(f\"Error creating directory: {directory_path}\")\n        return f\"Error creating directory: {str(e)}\"\n\n\n@tool(\n    name=\"ListDirectory\",\n    description=\"List files and directories in a given path with detailed information.\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"directory_path\": {\n                \"type\": \"string\",\n                \"description\": \"The absolute path to the directory to list\"\n            },\n            \"pattern\": {\n                \"type\": \"string\",\n                \"description\": \"Optional glob pattern to filter files (e.g., '*.py')\"\n            },\n            \"recursive\": {\n                \"type\": \"boolean\",\n                \"description\": \"Whether to list files recursively\",\n                \"default\": False\n            },\n            \"show_hidden\": {\n                \"type\": \"boolean\",\n                \"description\": \"Whether to show hidden files (starting with .)\",\n                \"default\": False\n            },\n            \"details\": {\n                \"type\": \"boolean\",\n                \"description\": \"Whether to show detailed information (size, permissions, etc.)\",\n                \"default\": False\n            }\n        },\n        \"required\": [\"directory_path\"]\n    },\n    category=\"file\"\n)\ndef list_directory(\n    directory_path: str, \n    pattern: Optional[str] = None, \n    recursive: bool = False,\n    show_hidden: bool = False,\n    details: bool = False\n) -> str:\n    \"\"\"List files and directories with detailed information.\n    \n    Args:\n        directory_path: Absolute path to the directory\n        pattern: Glob pattern to filter files\n        recursive: Whether to list files recursively\n        show_hidden: Whether to show hidden files\n        details: Whether to show detailed information\n        \n    Returns:\n        Directory listing as formatted text\n    \"\"\"\n    logger.info(f\"Listing directory: {directory_path}\")\n    \n    if not os.path.isabs(directory_path):\n        return f\"Error: Directory path must be absolute: {directory_path}\"\n    \n    if not os.path.exists(directory_path):\n        return f\"Error: Directory not found: {directory_path}\"\n    \n    if not os.path.isdir(directory_path):\n        return f\"Error: Path is not a directory: {directory_path}\"\n    \n    try:\n        import glob\n        import stat\n        from datetime import datetime\n        \n        # Build the pattern\n        if pattern:\n            if recursive:\n                search_pattern = os.path.join(directory_path, \"**\", pattern)\n            else:\n                search_pattern = os.path.join(directory_path, pattern)\n        else:\n            if recursive:\n                search_pattern = os.path.join(directory_path, \"**\")\n            else:\n                search_pattern = os.path.join(directory_path, \"*\")\n        \n        # Get all matching files\n        if recursive:\n            matches = glob.glob(search_pattern, recursive=True)\n        else:\n            matches = glob.glob(search_pattern)\n        \n        # Filter hidden files if needed\n        if not show_hidden:\n            matches = [m for m in matches if not os.path.basename(m).startswith('.')]\n        \n        # Sort by name\n        matches.sort()\n        \n        # Format the output\n        result = []\n        \n        if details:\n            # Header\n            result.append(f\"{'Type':<6} {'Permissions':<11} {'Size':<10} {'Modified':<20} {'Name'}\")\n            result.append(\"-\" * 80)\n            \n            for item_path in matches:\n                try:\n                    # Get file stats\n                    item_stat = os.stat(item_path)\n                    \n                    # Determine type\n                    if os.path.isdir(item_path):\n                        item_type = \"dir\"\n                    elif os.path.islink(item_path):\n                        item_type = \"link\"\n                    else:\n                        item_type = \"file\"\n                    \n                    # Format permissions\n                    mode = item_stat.st_mode\n                    perms = \"\"\n                    for who in \"USR\", \"GRP\", \"OTH\":\n                        for what in \"R\", \"W\", \"X\":\n                            perm = getattr(stat, f\"S_I{what}{who}\")\n                            perms += what.lower() if mode & perm else \"-\"\n                    \n                    # Format size\n                    size = item_stat.st_size\n                    if size < 1024:\n                        size_str = f\"{size}B\"\n                    elif size < 1024 * 1024:\n                        size_str = f\"{size/1024:.1f}KB\"\n                    elif size < 1024 * 1024 * 1024:\n                        size_str = f\"{size/(1024*1024):.1f}MB\"\n                    else:\n                        size_str = f\"{size/(1024*1024*1024):.1f}GB\"\n                    \n                    # Format modification time\n                    mtime = datetime.fromtimestamp(item_stat.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n                    \n                    # Format name (relative to the directory)\n                    name = os.path.relpath(item_path, directory_path)\n                    \n                    # Add to result\n                    result.append(f\"{item_type:<6} {perms:<11} {size_str:<10} {mtime:<20} {name}\")\n                \n                except Exception as e:\n                    result.append(f\"Error getting info for {item_path}: {str(e)}\")\n        else:\n            # Simple listing\n            dirs = []\n            files = []\n            \n            for item_path in matches:\n                name = os.path.relpath(item_path, directory_path)\n                if os.path.isdir(item_path):\n                    dirs.append(f\"{name}/\")\n                else:\n                    files.append(name)\n            \n            if dirs:\n                result.append(\"Directories:\")\n                for d in dirs:\n                    result.append(f\"  {d}\")\n            \n            if files:\n                if dirs:\n                    result.append(\"\")\n                result.append(\"Files:\")\n                for f in files:\n                    result.append(f\"  {f}\")\n        \n        if not result:\n            return f\"No matching items found in {directory_path}\"\n        \n        return \"\\n\".join(result)\n    \n    except Exception as e:\n        logger.exception(f\"Error listing directory: {directory_path}\")\n        return f\"Error listing directory: {str(e)}\"\n\n\ndef register_file_tools(registry: ToolRegistry) -> None:\n    \"\"\"Register all file tools with the registry.\n    \n    Args:\n        registry: Tool registry to register with\n    \"\"\"\n    from .base import create_tools_from_functions\n    \n    file_tools = [\n        view_file,\n        edit_file,\n        replace_file,\n        make_directory,\n        list_directory\n    ]\n    \n    create_tools_from_functions(registry, file_tools)\n"}
{"type": "source_file", "path": "claude_code/lib/tools/ai_tools.py", "content": "#!/usr/bin/env python3\n# claude_code/lib/tools/ai_tools.py\n\"\"\"AI-powered tools for generation and analysis.\"\"\"\n\nimport os\nimport logging\nimport json\nimport base64\nimport requests\nimport tempfile\nfrom typing import Dict, List, Optional, Any, Union\nimport time\n\nfrom .base import tool, ToolRegistry\n\nlogger = logging.getLogger(__name__)\n\n\n@tool(\n    name=\"GenerateImage\",\n    description=\"Generate an image using AI based on a text prompt\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"prompt\": {\n                \"type\": \"string\",\n                \"description\": \"Text description of the image to generate\"\n            },\n            \"style\": {\n                \"type\": \"string\",\n                \"description\": \"Style of the image (realistic, cartoon, sketch, etc.)\",\n                \"enum\": [\"realistic\", \"cartoon\", \"sketch\", \"painting\", \"3d\", \"pixel-art\", \"abstract\"],\n                \"default\": \"realistic\"\n            },\n            \"size\": {\n                \"type\": \"string\",\n                \"description\": \"Size of the image\",\n                \"enum\": [\"small\", \"medium\", \"large\"],\n                \"default\": \"medium\"\n            },\n            \"save_path\": {\n                \"type\": \"string\",\n                \"description\": \"Absolute path where the image should be saved (optional)\"\n            }\n        },\n        \"required\": [\"prompt\"]\n    },\n    needs_permission=True,\n    category=\"ai\"\n)\ndef generate_image(prompt: str, style: str = \"realistic\", size: str = \"medium\", save_path: Optional[str] = None) -> str:\n    \"\"\"Generate an image using AI based on a text prompt.\n    \n    Args:\n        prompt: Text description of the image to generate\n        style: Style of the image\n        size: Size of the image\n        save_path: Path where to save the image\n        \n    Returns:\n        Path to the generated image or error message\n    \"\"\"\n    logger.info(f\"Generating image with prompt: {prompt} (style: {style}, size: {size})\")\n    \n    # Map size to actual dimensions\n    size_map = {\n        \"small\": \"512x512\",\n        \"medium\": \"1024x1024\",\n        \"large\": \"1792x1024\"\n    }\n    \n    # Get API key\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        return \"Error: OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\"\n    \n    # Prepare the prompt based on style\n    full_prompt = prompt\n    if style != \"realistic\":\n        style_prompts = {\n            \"cartoon\": f\"A cartoon-style image of {prompt}\",\n            \"sketch\": f\"A pencil sketch of {prompt}\",\n            \"painting\": f\"An oil painting of {prompt}\",\n            \"3d\": f\"A 3D rendered image of {prompt}\",\n            \"pixel-art\": f\"A pixel art image of {prompt}\",\n            \"abstract\": f\"An abstract representation of {prompt}\"\n        }\n        full_prompt = style_prompts.get(style, prompt)\n    \n    try:\n        # Call OpenAI API to generate image\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {api_key}\"\n        }\n        \n        payload = {\n            \"model\": \"dall-e-3\",\n            \"prompt\": full_prompt,\n            \"size\": size_map.get(size, \"1024x1024\"),\n            \"quality\": \"standard\",\n            \"n\": 1\n        }\n        \n        response = requests.post(\n            \"https://api.openai.com/v1/images/generations\",\n            headers=headers,\n            json=payload\n        )\n        \n        if response.status_code != 200:\n            return f\"Error: API request failed with status code {response.status_code}: {response.text}\"\n        \n        data = response.json()\n        \n        if \"data\" not in data or not data[\"data\"]:\n            return \"Error: No image data in response\"\n        \n        image_url = data[\"data\"][0][\"url\"]\n        \n        # Download the image\n        image_response = requests.get(image_url)\n        if image_response.status_code != 200:\n            return f\"Error: Failed to download image: {image_response.status_code}\"\n        \n        # Save the image\n        if save_path:\n            # Ensure the path is absolute\n            if not os.path.isabs(save_path):\n                return f\"Error: Save path must be absolute: {save_path}\"\n            \n            # Create directory if it doesn't exist\n            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n            \n            # Save the image\n            with open(save_path, \"wb\") as f:\n                f.write(image_response.content)\n            \n            return f\"Image generated and saved to: {save_path}\"\n        else:\n            # Save to a temporary file\n            with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmp:\n                tmp.write(image_response.content)\n                return f\"Image generated and saved to temporary file: {tmp.name}\"\n    \n    except Exception as e:\n        logger.exception(f\"Error generating image: {str(e)}\")\n        return f\"Error generating image: {str(e)}\"\n\n\n@tool(\n    name=\"TextToSpeech\",\n    description=\"Convert text to speech using AI\",\n    parameters={\n        \"type\": \"object\",\n        \"properties\": {\n            \"text\": {\n                \"type\": \"string\",\n                \"description\": \"Text to convert to speech\"\n            },\n            \"voice\": {\n                \"type\": \"string\",\n                \"description\": \"Voice to use\",\n                \"enum\": [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\n                \"default\": \"nova\"\n            },\n            \"save_path\": {\n                \"type\": \"string\",\n                \"description\": \"Absolute path where the audio file should be saved (optional)\"\n            }\n        },\n        \"required\": [\"text\"]\n    },\n    needs_permission=True,\n    category=\"ai\"\n)\ndef text_to_speech(text: str, voice: str = \"nova\", save_path: Optional[str] = None) -> str:\n    \"\"\"Convert text to speech using AI.\n    \n    Args:\n        text: Text to convert to speech\n        voice: Voice to use\n        save_path: Path where to save the audio file\n        \n    Returns:\n        Path to the generated audio file or error message\n    \"\"\"\n    logger.info(f\"Converting text to speech: {text[:50]}... (voice: {voice})\")\n    \n    # Get API key\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        return \"Error: OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\"\n    \n    try:\n        # Call OpenAI API to generate speech\n        headers = {\n            \"Authorization\": f\"Bearer {api_key}\"\n        }\n        \n        payload = {\n            \"model\": \"tts-1\",\n            \"input\": text,\n            \"voice\": voice\n        }\n        \n        response = requests.post(\n            \"https://api.openai.com/v1/audio/speech\",\n            headers=headers,\n            json=payload\n        )\n        \n        if response.status_code != 200:\n            return f\"Error: API request failed with status code {response.status_code}: {response.text}\"\n        \n        # Save the audio\n        if save_path:\n            # Ensure the path is absolute\n            if not os.path.isabs(save_path):\n                return f\"Error: Save path must be absolute: {save_path}\"\n            \n            # Create directory if it doesn't exist\n            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n            \n            # Save the audio\n            with open(save_path, \"wb\") as f:\n                f.write(response.content)\n            \n            return f\"Speech generated and saved to: {save_path}\"\n        else:\n            # Save to a temporary file\n            with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmp:\n                tmp.write(response.content)\n                return f\"Speech generated and saved to temporary file: {tmp.name}\"\n    \n    except Exception as e:\n        logger.exception(f\"Error generating speech: {str(e)}\")\n        return f\"Error generating speech: {str(e)}\"\n\n\ndef register_ai_tools(registry: ToolRegistry) -> None:\n    \"\"\"Register all AI tools with the registry.\n    \n    Args:\n        registry: Tool registry to register with\n    \"\"\"\n    from .base import create_tools_from_functions\n    \n    ai_tools = [\n        generate_image,\n        text_to_speech\n    ]\n    \n    create_tools_from_functions(registry, ai_tools)\n"}
{"type": "source_file", "path": "mcp_server.py", "content": "#!/usr/bin/env python3\n\"\"\"\nModel Context Protocol (MCP) Server Implementation\n\nThis module implements the Model Context Protocol server capabilities,\nallowing the assistant to be used as an MCP-compatible context provider.\n\"\"\"\n\nimport os\nimport json\nimport time\nimport uuid\nimport sys\nimport logging\nimport asyncio\nimport tiktoken\nimport re\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional, Union, AsyncGenerator\nfrom fastapi import FastAPI, HTTPException, Request, Response, Depends, BackgroundTasks, Query\nfrom fastapi.responses import JSONResponse, StreamingResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nfrom pydantic import BaseModel, Field\nimport uvicorn\nimport openai\nfrom openai import OpenAI\nimport prometheus_client\nfrom prometheus_client import Counter, Histogram, Gauge\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n)\nlogger = logging.getLogger(\"mcp_server\")\n\n# MCP Protocol Models\nclass MCPHealthResponse(BaseModel):\n    \"\"\"Health check response for MCP protocol\"\"\"\n    status: str = \"healthy\"\n    version: str = \"1.0.0\"\n    protocol_version: str = \"0.1.0\"\n    provider: str = \"OpenAI Code Assistant\"\n    models: List[str] = [\"gpt-4o\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"]\n    uptime: Optional[float] = None\n    request_count: Optional[int] = None\n    cache_hit_ratio: Optional[float] = None\n\nclass MCPContextRequest(BaseModel):\n    \"\"\"Request for context generation from a prompt template\"\"\"\n    prompt_id: str\n    parameters: Dict[str, Any] = Field(default_factory=dict, description=\"Parameters to fill in the prompt template\")\n    model: Optional[str] = Field(None, description=\"Model to use for context generation\")\n    stream: bool = Field(False, description=\"Whether to stream the response\")\n    user: Optional[str] = Field(None, description=\"User identifier for tracking\")\n    conversation_id: Optional[str] = Field(None, description=\"Conversation identifier\")\n    message_id: Optional[str] = Field(None, description=\"Message identifier\")\n\nclass MCPContextResponse(BaseModel):\n    \"\"\"Response containing generated context\"\"\"\n    context: str = Field(..., description=\"The generated context\")\n    context_id: str = Field(..., description=\"Unique identifier for this context\")\n    model: str = Field(..., description=\"Model used for generation\")\n    usage: Dict[str, int] = Field(default_factory=dict, description=\"Token usage statistics\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n\nclass MCPErrorResponse(BaseModel):\n    \"\"\"Error response format\"\"\"\n    error: str = Field(..., description=\"Error message\")\n    error_type: str = Field(..., description=\"Type of error\")\n    status_code: int = Field(..., description=\"HTTP status code\")\n    details: Optional[Dict[str, Any]] = Field(None, description=\"Additional error details\")\n\nclass MCPPromptTemplate(BaseModel):\n    \"\"\"Prompt template definition\"\"\"\n    id: str = Field(..., description=\"Unique identifier for the template\")\n    template: str = Field(..., description=\"The prompt template with parameter placeholders\")\n    description: Optional[str] = Field(None, description=\"Description of the template\")\n    parameters: Dict[str, Dict[str, Any]] = Field(default_factory=dict, description=\"Parameter definitions\")\n    default_model: Optional[str] = Field(None, description=\"Default model to use with this template\")\n    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Additional metadata\")\n\nclass MCPPromptLibraryResponse(BaseModel):\n    \"\"\"Response containing a list of prompt templates\"\"\"\n    prompts: List[MCPPromptTemplate] = Field(..., description=\"List of prompt templates\")\n    count: int = Field(..., description=\"Number of templates\")\n\n# MCP Server Implementation\n# Prometheus metrics\nREQUEST_COUNT = Counter('mcp_requests_total', 'Total number of requests processed', ['endpoint', 'status'])\nREQUEST_LATENCY = Histogram('mcp_request_latency_seconds', 'Request latency in seconds', ['endpoint'])\nCACHE_HIT = Counter('mcp_cache_hits_total', 'Total number of cache hits')\nCACHE_MISS = Counter('mcp_cache_misses_total', 'Total number of cache misses')\nACTIVE_CONNECTIONS = Gauge('mcp_active_connections', 'Number of active connections')\nTOKEN_USAGE = Counter('mcp_token_usage_total', 'Total number of tokens used', ['model', 'type'])\n\n# Cache implementation\nclass CacheManager:\n    \"\"\"Manages caching for context responses\"\"\"\n    \n    def __init__(self, cache_type=\"memory\", redis_url=None, ttl=3600):\n        self.cache_type = cache_type\n        self.redis_url = redis_url\n        self.ttl = ttl\n        self.memory_cache = {}\n        self.redis_client = None\n        \n        if cache_type == \"redis\" and redis_url:\n            try:\n                import redis\n                self.redis_client = redis.from_url(redis_url)\n                logging.info(f\"Redis cache initialized with URL: {redis_url}\")\n            except ImportError:\n                logging.warning(\"Redis package not installed. Falling back to memory cache.\")\n                self.cache_type = \"memory\"\n            except Exception as e:\n                logging.error(f\"Failed to connect to Redis: {str(e)}\")\n                self.cache_type = \"memory\"\n    \n    async def get(self, key):\n        \"\"\"Get item from cache\"\"\"\n        if self.cache_type == \"redis\" and self.redis_client:\n            try:\n                value = self.redis_client.get(key)\n                if value:\n                    CACHE_HIT.inc()\n                    return json.loads(value)\n                CACHE_MISS.inc()\n                return None\n            except Exception as e:\n                logging.error(f\"Redis get error: {str(e)}\")\n                CACHE_MISS.inc()\n                return None\n        else:\n            # Memory cache\n            if key in self.memory_cache:\n                if time.time() - self.memory_cache[key][\"timestamp\"] < self.ttl:\n                    CACHE_HIT.inc()\n                    return self.memory_cache[key][\"data\"]\n                else:\n                    # Expired\n                    del self.memory_cache[key]\n            CACHE_MISS.inc()\n            return None\n    \n    async def set(self, key, value, ttl=None):\n        \"\"\"Set item in cache\"\"\"\n        if ttl is None:\n            ttl = self.ttl\n            \n        if self.cache_type == \"redis\" and self.redis_client:\n            try:\n                self.redis_client.setex(key, ttl, json.dumps(value))\n            except Exception as e:\n                logging.error(f\"Redis set error: {str(e)}\")\n        else:\n            # Memory cache\n            self.memory_cache[key] = {\n                \"data\": value,\n                \"timestamp\": time.time()\n            }\n    \n    async def delete(self, key):\n        \"\"\"Delete item from cache\"\"\"\n        if self.cache_type == \"redis\" and self.redis_client:\n            try:\n                self.redis_client.delete(key)\n            except Exception as e:\n                logging.error(f\"Redis delete error: {str(e)}\")\n        else:\n            # Memory cache\n            if key in self.memory_cache:\n                del self.memory_cache[key]\n    \n    async def clear(self):\n        \"\"\"Clear all cache\"\"\"\n        if self.cache_type == \"redis\" and self.redis_client:\n            try:\n                self.redis_client.flushdb()\n            except Exception as e:\n                logging.error(f\"Redis flush error: {str(e)}\")\n        else:\n            # Memory cache\n            self.memory_cache = {}\n\nclass MCPServer:\n    \"\"\"Model Context Protocol Server Implementation\"\"\"\n    \n    def __init__(self, cache_type=\"memory\", redis_url=None):\n        self.app = FastAPI(\n            title=\"OpenAI Code Assistant MCP Server\",\n            description=\"Model Context Protocol server for OpenAI Code Assistant\",\n            version=\"1.0.0\",\n            docs_url=\"/docs\",\n            redoc_url=\"/redoc\",\n            openapi_url=\"/openapi.json\",\n        )\n        \n        # Initialize OpenAI client\n        self.openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n        \n        # Initialize cache\n        self.cache = CacheManager(cache_type=cache_type, redis_url=redis_url)\n        \n        # Initialize tokenizer\n        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n        \n        # Setup routes and middleware\n        self.setup_routes()\n        self.setup_middleware()\n        \n        # Load templates and static files\n        self.templates_dir = os.path.join(os.path.dirname(__file__), \"templates\")\n        os.makedirs(self.templates_dir, exist_ok=True)\n        self.static_dir = os.path.join(os.path.dirname(__file__), \"static\")\n        os.makedirs(self.static_dir, exist_ok=True)\n        \n        # Create default template if it doesn't exist\n        self._create_default_template()\n        \n        # Initialize templates\n        self.templates = Jinja2Templates(directory=self.templates_dir)\n        \n        # Mount static files\n        self.app.mount(\"/static\", StaticFiles(directory=self.static_dir), name=\"static\")\n        \n        # Load prompt templates\n        self.prompt_templates = self._load_prompt_templates()\n        \n        # Initialize metrics\n        self.request_count = 0\n        self.start_time = time.time()\n        \n    def setup_middleware(self):\n        \"\"\"Configure middleware for the FastAPI app\"\"\"\n        # Add CORS middleware\n        self.app.add_middleware(\n            CORSMiddleware,\n            allow_origins=[\"*\"],\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n        \n        # Add request tracking middleware\n        @self.app.middleware(\"http\")\n        async def track_requests(request: Request, call_next):\n            # Increment active connections\n            ACTIVE_CONNECTIONS.inc()\n            \n            # Track request start time\n            start_time = time.time()\n            \n            # Process request\n            try:\n                response = await call_next(request)\n                \n                # Record metrics\n                endpoint = request.url.path\n                status = response.status_code\n                REQUEST_COUNT.labels(endpoint=endpoint, status=status).inc()\n                REQUEST_LATENCY.labels(endpoint=endpoint).observe(time.time() - start_time)\n                \n                # Increment total request count\n                self.request_count += 1\n                \n                return response\n            finally:\n                # Decrement active connections\n                ACTIVE_CONNECTIONS.dec()\n    \n    def _create_default_template(self):\n        \"\"\"Create default dashboard template if it doesn't exist\"\"\"\n        index_path = os.path.join(self.templates_dir, \"index.html\")\n        if not os.path.exists(index_path):\n            with open(index_path, \"w\") as f:\n                f.write(\"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>OpenAI Code Assistant MCP Server</title>\n    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css\">\n    <style>\n        body { padding: 20px; }\n        .card { margin-bottom: 20px; }\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>OpenAI Code Assistant MCP Server</h1>\n        <div class=\"row\">\n            <div class=\"col-md-6\">\n                <div class=\"card\">\n                    <div class=\"card-header\">Server Status</div>\n                    <div class=\"card-body\">\n                        <p><strong>Status:</strong> {{ status }}</p>\n                        <p><strong>Uptime:</strong> {{ uptime }}</p>\n                        <p><strong>Requests Served:</strong> {{ request_count }}</p>\n                        <p><strong>Cache Hit Ratio:</strong> {{ cache_hit_ratio }}%</p>\n                    </div>\n                </div>\n            </div>\n            <div class=\"col-md-6\">\n                <div class=\"card\">\n                    <div class=\"card-header\">Available Models</div>\n                    <div class=\"card-body\">\n                        <ul>\n                            {% for model in models %}\n                            <li>{{ model }}</li>\n                            {% endfor %}\n                        </ul>\n                    </div>\n                </div>\n            </div>\n        </div>\n        \n        <h2>Available Prompt Templates</h2>\n        <div class=\"row\">\n            {% for template in templates %}\n            <div class=\"col-md-6\">\n                <div class=\"card\">\n                    <div class=\"card-header\">{{ template.id }}</div>\n                    <div class=\"card-body\">\n                        <p><strong>Description:</strong> {{ template.description }}</p>\n                        <p><strong>Parameters:</strong> {{ template.parameters|join(\", \") }}</p>\n                        <p><strong>Default Model:</strong> {{ template.default_model }}</p>\n                    </div>\n                </div>\n            </div>\n            {% endfor %}\n        </div>\n        \n        <h2>API Documentation</h2>\n        <p>\n            <a href=\"/docs\" class=\"btn btn-primary\">Interactive API Docs</a>\n            <a href=\"/redoc\" class=\"btn btn-secondary\">ReDoc API Docs</a>\n            <a href=\"/metrics\" class=\"btn btn-info\">Prometheus Metrics</a>\n        </p>\n    </div>\n    \n    <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js\"></script>\n</body>\n</html>\n                \"\"\")\n    \n    def setup_routes(self):\n        \"\"\"Configure API routes for MCP protocol\"\"\"\n        \n        # MCP Protocol Routes\n        # Dashboard route\n        @self.app.get(\"/\", tags=[\"Dashboard\"])\n        async def dashboard(request: Request):\n            \"\"\"Dashboard showing server status and available templates\"\"\"\n            # Calculate cache hit ratio\n            cache_hits = prometheus_client.REGISTRY.get_sample_value('mcp_cache_hits_total') or 0\n            cache_misses = prometheus_client.REGISTRY.get_sample_value('mcp_cache_misses_total') or 0\n            total_cache_requests = cache_hits + cache_misses\n            cache_hit_ratio = (cache_hits / total_cache_requests * 100) if total_cache_requests > 0 else 0\n            \n            # Format uptime\n            uptime_seconds = time.time() - self.start_time\n            days, remainder = divmod(uptime_seconds, 86400)\n            hours, remainder = divmod(remainder, 3600)\n            minutes, seconds = divmod(remainder, 60)\n            uptime_str = f\"{int(days)}d {int(hours)}h {int(minutes)}m {int(seconds)}s\"\n            \n            # Get template information\n            templates = []\n            for template_id, template in self.prompt_templates.items():\n                templates.append({\n                    \"id\": template_id,\n                    \"description\": template.get(\"description\", \"\"),\n                    \"parameters\": list(template.get(\"parameters\", {}).keys()),\n                    \"default_model\": template.get(\"default_model\", \"gpt-4o\")\n                })\n            \n            return self.templates.TemplateResponse(\"index.html\", {\n                \"request\": request,\n                \"status\": \"Healthy\",\n                \"uptime\": uptime_str,\n                \"request_count\": self.request_count,\n                \"cache_hit_ratio\": round(cache_hit_ratio, 2),\n                \"models\": [\"gpt-4o\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"],\n                \"templates\": templates\n            })\n        \n        # Prometheus metrics endpoint\n        @self.app.get(\"/metrics\", tags=[\"Monitoring\"])\n        async def metrics():\n            \"\"\"Expose Prometheus metrics\"\"\"\n            return Response(prometheus_client.generate_latest(), media_type=\"text/plain\")\n        \n        # Health check endpoints\n        @self.app.get(\"/health\", response_model=MCPHealthResponse, tags=[\"Health\"])\n        async def health():\n            \"\"\"Health check endpoint\"\"\"\n            # Calculate cache hit ratio\n            cache_hits = prometheus_client.REGISTRY.get_sample_value('mcp_cache_hits_total') or 0\n            cache_misses = prometheus_client.REGISTRY.get_sample_value('mcp_cache_misses_total') or 0\n            total_cache_requests = cache_hits + cache_misses\n            cache_hit_ratio = (cache_hits / total_cache_requests) if total_cache_requests > 0 else 0\n            \n            return MCPHealthResponse(\n                status=\"healthy\",\n                uptime=time.time() - self.start_time,\n                request_count=self.request_count,\n                cache_hit_ratio=cache_hit_ratio\n            )\n        \n        @self.app.post(\"/context\", response_model=MCPContextResponse, tags=[\"Context\"])\n        async def get_context(\n            request: MCPContextRequest, \n            background_tasks: BackgroundTasks,\n            use_cache: bool = Query(True, description=\"Whether to use cached results if available\")\n        ):\n            \"\"\"\n            Get context for a prompt template with parameters.\n            \n            This endpoint processes a prompt template with the provided parameters\n            and returns the generated context. It can optionally use OpenAI models\n            to enhance the context.\n            \"\"\"\n            try:\n                # Check if prompt template exists\n                if request.prompt_id not in self.prompt_templates:\n                    raise HTTPException(\n                        status_code=404,\n                        detail=f\"Prompt template '{request.prompt_id}' not found\"\n                    )\n                \n                # Get prompt template\n                template = self.prompt_templates[request.prompt_id]\n                \n                # Use default model if not specified\n                model = request.model or template.get(\"default_model\", \"gpt-4o\")\n                \n                # Generate context ID\n                context_id = str(uuid.uuid4())\n                \n                # Generate cache key\n                cache_key = f\"{request.prompt_id}:{json.dumps(request.parameters, sort_keys=True)}:{model}\"\n                \n                # Check cache if enabled\n                if use_cache:\n                    cached_result = await self.cache.get(cache_key)\n                    if cached_result:\n                        # Update context ID for this request\n                        cached_result[\"context_id\"] = context_id\n                        return MCPContextResponse(**cached_result)\n                \n                # Process template with parameters\n                processed_template = self._process_template(template[\"template\"], request.parameters)\n                \n                # Check if we should use OpenAI to enhance the context\n                if template.get(\"use_openai\", False):\n                    # Generate context using OpenAI\n                    context, usage = await self._generate_with_openai(\n                        processed_template, \n                        model, \n                        template.get(\"system_prompt\")\n                    )\n                else:\n                    # Use the processed template directly\n                    context = processed_template\n                    \n                    # Calculate token usage\n                    token_count = len(self.tokenizer.encode(context))\n                    usage = {\n                        \"prompt_tokens\": token_count,\n                        \"completion_tokens\": 0,\n                        \"total_tokens\": token_count\n                    }\n                \n                # Track token usage in Prometheus\n                TOKEN_USAGE.labels(model=model, type=\"prompt\").inc(usage[\"prompt_tokens\"])\n                TOKEN_USAGE.labels(model=model, type=\"completion\").inc(usage[\"completion_tokens\"])\n                \n                # Create response\n                response = MCPContextResponse(\n                    context=context,\n                    context_id=context_id,\n                    model=model,\n                    usage=usage,\n                    metadata={\n                        \"prompt_id\": request.prompt_id,\n                        \"timestamp\": time.time(),\n                        \"parameters\": request.parameters\n                    }\n                )\n                \n                # Store in cache\n                await self.cache.set(cache_key, response.dict())\n                \n                return response\n                \n            except Exception as e:\n                logger.error(f\"Error processing context request: {str(e)}\", exc_info=True)\n                raise HTTPException(\n                    status_code=500,\n                    detail=f\"Error processing context: {str(e)}\"\n                )\n        \n        @self.app.post(\"/context/stream\", tags=[\"Context\"])\n        async def stream_context(request: MCPContextRequest):\n            \"\"\"\n            Stream context generation.\n            \n            Similar to /context but streams the response as it's generated.\n            \"\"\"\n            try:\n                # Check if prompt template exists\n                if request.prompt_id not in self.prompt_templates:\n                    raise HTTPException(\n                        status_code=404,\n                        detail=f\"Prompt template '{request.prompt_id}' not found\"\n                    )\n                \n                # Get prompt template\n                template = self.prompt_templates[request.prompt_id]\n                \n                # Use default model if not specified\n                model = request.model or template.get(\"default_model\", \"gpt-4o\")\n                \n                # Generate context ID\n                context_id = str(uuid.uuid4())\n                \n                # Process template with parameters\n                processed_template = self._process_template(template[\"template\"], request.parameters)\n                \n                # Stream the context generation\n                return StreamingResponse(\n                    self._stream_context(processed_template, model, context_id, template.get(\"system_prompt\")),\n                    media_type=\"text/event-stream\"\n                )\n                \n            except Exception as e:\n                logger.error(f\"Error streaming context: {str(e)}\", exc_info=True)\n                raise HTTPException(\n                    status_code=500,\n                    detail=f\"Error streaming context: {str(e)}\"\n                )\n        \n        @self.app.get(\"/prompts\", response_model=MCPPromptLibraryResponse, tags=[\"Prompts\"])\n        async def get_prompts():\n            \"\"\"\n            Get available prompt templates.\n            \n            Returns a list of all prompt templates available in the system.\n            \"\"\"\n            prompts = [\n                MCPPromptTemplate(\n                    id=prompt_id,\n                    template=template[\"template\"],\n                    description=template.get(\"description\", \"\"),\n                    parameters=template.get(\"parameters\", {}),\n                    default_model=template.get(\"default_model\", \"gpt-4o\"),\n                    metadata=template.get(\"metadata\", {})\n                )\n                for prompt_id, template in self.prompt_templates.items()\n            ]\n            \n            return MCPPromptLibraryResponse(\n                prompts=prompts,\n                count=len(prompts)\n            )\n        \n        @self.app.get(\"/prompts/{prompt_id}\", response_model=MCPPromptTemplate, tags=[\"Prompts\"])\n        async def get_prompt(prompt_id: str):\n            \"\"\"\n            Get a specific prompt template.\n            \n            Returns the details of a specific prompt template by ID.\n            \"\"\"\n            if prompt_id not in self.prompt_templates:\n                raise HTTPException(\n                    status_code=404,\n                    detail=f\"Prompt template '{prompt_id}' not found\"\n                )\n            \n            template = self.prompt_templates[prompt_id]\n            return MCPPromptTemplate(\n                id=prompt_id,\n                template=template[\"template\"],\n                description=template.get(\"description\", \"\"),\n                parameters=template.get(\"parameters\", {}),\n                default_model=template.get(\"default_model\", \"gpt-4o\"),\n                metadata=template.get(\"metadata\", {})\n            )\n        \n        @self.app.post(\"/prompts\", response_model=MCPPromptTemplate, status_code=201, tags=[\"Prompts\"])\n        async def create_prompt(prompt: MCPPromptTemplate):\n            \"\"\"\n            Create a new prompt template.\n            \n            Adds a new prompt template to the system.\n            \"\"\"\n            if prompt.id in self.prompt_templates:\n                raise HTTPException(\n                    status_code=409,\n                    detail=f\"Prompt template '{prompt.id}' already exists\"\n                )\n            \n            self.prompt_templates[prompt.id] = {\n                \"template\": prompt.template,\n                \"description\": prompt.description,\n                \"parameters\": prompt.parameters,\n                \"default_model\": prompt.default_model,\n                \"metadata\": prompt.metadata\n            }\n            \n            # Save updated templates\n            self._save_prompt_templates()\n            \n            return prompt\n        \n        @self.app.put(\"/prompts/{prompt_id}\", response_model=MCPPromptTemplate, tags=[\"Prompts\"])\n        async def update_prompt(prompt_id: str, prompt: MCPPromptTemplate):\n            \"\"\"\n            Update an existing prompt template.\n            \n            Updates the details of an existing prompt template.\n            \"\"\"\n            if prompt_id != prompt.id:\n                raise HTTPException(\n                    status_code=400,\n                    detail=\"Prompt ID in path must match prompt ID in body\"\n                )\n            \n            if prompt_id not in self.prompt_templates:\n                raise HTTPException(\n                    status_code=404,\n                    detail=f\"Prompt template '{prompt_id}' not found\"\n                )\n            \n            self.prompt_templates[prompt_id] = {\n                \"template\": prompt.template,\n                \"description\": prompt.description,\n                \"parameters\": prompt.parameters,\n                \"default_model\": prompt.default_model,\n                \"metadata\": prompt.metadata\n            }\n            \n            # Save updated templates\n            self._save_prompt_templates()\n            \n            return prompt\n        \n        @self.app.delete(\"/prompts/{prompt_id}\", tags=[\"Prompts\"])\n        async def delete_prompt(prompt_id: str):\n            \"\"\"\n            Delete a prompt template.\n            \n            Removes a prompt template from the system.\n            \"\"\"\n            if prompt_id not in self.prompt_templates:\n                raise HTTPException(\n                    status_code=404,\n                    detail=f\"Prompt template '{prompt_id}' not found\"\n                )\n            \n            del self.prompt_templates[prompt_id]\n            \n            # Save updated templates\n            self._save_prompt_templates()\n            \n            return {\"status\": \"deleted\", \"prompt_id\": prompt_id}\n        \n        # Additional endpoints for a more complete MCP server\n        @self.app.get(\"/models\", tags=[\"Models\"])\n        async def get_models():\n            \"\"\"\n            Get available models.\n            \n            Returns a list of models that can be used with this MCP server.\n            \"\"\"\n            return {\n                \"models\": [\n                    {\n                        \"id\": \"gpt-4o\",\n                        \"name\": \"GPT-4o\",\n                        \"description\": \"OpenAI's most advanced model\",\n                        \"context_length\": 128000,\n                        \"is_default\": True\n                    },\n                    {\n                        \"id\": \"gpt-4-turbo\",\n                        \"name\": \"GPT-4 Turbo\",\n                        \"description\": \"Optimized version of GPT-4\",\n                        \"context_length\": 128000,\n                        \"is_default\": False\n                    },\n                    {\n                        \"id\": \"gpt-3.5-turbo\",\n                        \"name\": \"GPT-3.5 Turbo\",\n                        \"description\": \"Fast and efficient model\",\n                        \"context_length\": 16385,\n                        \"is_default\": False\n                    }\n                ],\n                \"count\": 3\n            }\n        \n        @self.app.get(\"/stats\", tags=[\"System\"])\n        async def get_stats():\n            \"\"\"\n            Get server statistics.\n            \n            Returns usage statistics and system information.\n            \"\"\"\n            return {\n                \"uptime\": time.time() - self.start_time,\n                \"prompt_templates_count\": len(self.prompt_templates),\n                \"cache_size\": len(self.context_cache),\n                \"requests_served\": {\n                    \"context\": 0,  # This would be tracked in a real implementation\n                    \"prompts\": 0,\n                    \"total\": 0\n                },\n                \"system_info\": {\n                    \"python_version\": sys.version,\n                    \"platform\": sys.platform\n                }\n            }\n            \n        @self.app.post(\"/context/stream\", tags=[\"Context\"])\n        async def stream_context(request: MCPContextRequest):\n            \"\"\"\n            Stream context generation.\n            \n            Similar to /context but streams the response as it's generated.\n            \"\"\"\n            # In a real implementation, this would stream the response\n            # For now, we'll just return a simple response\n            return JSONResponse(\n                content={\"message\": \"Streaming not implemented in this version\"},\n                status_code=501\n            )\n            \n        # Error handlers\n        @self.app.exception_handler(HTTPException)\n        async def http_exception_handler(request: Request, exc: HTTPException):\n            \"\"\"Handle HTTP exceptions in MCP format\"\"\"\n            return JSONResponse(\n                status_code=exc.status_code,\n                content={\n                    \"error\": exc.detail,\n                    \"error_type\": \"http_error\",\n                    \"status_code\": exc.status_code,\n                    \"details\": exc.detail if isinstance(exc.detail, dict) else None\n                }\n            )\n        \n        @self.app.exception_handler(Exception)\n        async def general_exception_handler(request: Request, exc: Exception):\n            \"\"\"Handle general exceptions in MCP format\"\"\"\n            logger.error(f\"Unhandled exception: {str(exc)}\", exc_info=True)\n            return JSONResponse(\n                status_code=500,\n                content={\n                    \"error\": str(exc),\n                    \"error_type\": \"server_error\",\n                    \"status_code\": 500,\n                    \"details\": None\n                }\n            )\n    \n    def _load_prompt_templates(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Load prompt templates from file or initialize defaults\"\"\"\n        templates_file = os.path.join(os.path.dirname(__file__), \"data\", \"prompt_templates.json\")\n        \n        # Create directory if it doesn't exist\n        os.makedirs(os.path.dirname(templates_file), exist_ok=True)\n        \n        # Try to load existing templates\n        if os.path.exists(templates_file):\n            try:\n                with open(templates_file, \"r\") as f:\n                    templates = json.load(f)\n                    logger.info(f\"Loaded {len(templates)} prompt templates from {templates_file}\")\n                    return templates\n            except Exception as e:\n                logger.error(f\"Error loading prompt templates: {str(e)}\")\n        \n        # Initialize with enhanced default templates\n        default_templates = {\n            \"greeting\": {\n                \"template\": \"Hello! The current time is {time}. How can I help you today?\",\n                \"description\": \"A simple greeting template\",\n                \"parameters\": {\n                    \"time\": {\n                        \"type\": \"string\",\n                        \"description\": \"The current time\"\n                    }\n                },\n                \"default_model\": \"gpt-4o\",\n                \"metadata\": {\n                    \"category\": \"general\"\n                }\n            },\n            \"code_review\": {\n                \"template\": \"Please review the following code:\\n\\n```{language}\\n{code}\\n```\\n\\nFocus on: {focus_areas}\",\n                \"description\": \"Template for code review requests\",\n                \"parameters\": {\n                    \"language\": {\n                        \"type\": \"string\",\n                        \"description\": \"Programming language of the code\"\n                    },\n                    \"code\": {\n                        \"type\": \"string\",\n                        \"description\": \"The code to review\"\n                    },\n                    \"focus_areas\": {\n                        \"type\": \"string\",\n                        \"description\": \"Areas to focus on during review (e.g., 'performance, security')\"\n                    }\n                },\n                \"default_model\": \"gpt-4o\",\n                \"use_openai\": True,\n                \"system_prompt\": \"You are a code review expert. Analyze the provided code and provide constructive feedback focusing on the specified areas.\",\n                \"metadata\": {\n                    \"category\": \"development\"\n                }\n            },\n            \"system_prompt\": {\n                \"template\": \"You are OpenAI Code Assistant, a CLI tool that helps users with software engineering tasks and general information.\\nUse the available tools to assist the user with their requests.\\n\\n# Tone and style\\nYou should be concise, direct, and to the point. When you run a non-trivial bash command, \\nyou should explain what the command does and why you are running it.\\nOutput text to communicate with the user; all text you output outside of tool use is displayed to the user.\\nRemember that your output will be displayed on a command line interface.\\n\\n# Tool usage policy\\n- When doing file search, remember to search effectively with the available tools.\\n- Always use the appropriate tool for the task.\\n- Use parallel tool calls when appropriate to improve performance.\\n- NEVER commit changes unless the user explicitly asks you to.\\n- For weather queries, use the Weather tool to provide real-time information.\\n\\n# Tasks\\nThe user will primarily request you perform software engineering tasks:\\n1. Solving bugs\\n2. Adding new functionality \\n3. Refactoring code\\n4. Explaining code\\n5. Writing tests\\n\\nFor these tasks:\\n1. Use search tools to understand the codebase\\n2. Implement solutions using the available tools\\n3. Verify solutions with tests if possible\\n4. Run lint and typecheck commands when appropriate\\n\\nThe user may also ask for general information:\\n1. Weather conditions\\n2. Simple calculations\\n3. General knowledge questions\\n\\n# Code style\\n- Follow the existing code style of the project\\n- Maintain consistent naming conventions\\n- Use appropriate libraries that are already in the project\\n- Add comments when code is complex or non-obvious\\n\\nIMPORTANT: You should minimize output tokens as much as possible while maintaining helpfulness, \\nquality, and accuracy. Answer concisely with short lines of text unless the user asks for detail.\",\n                \"description\": \"System prompt for the assistant\",\n                \"parameters\": {},\n                \"default_model\": \"gpt-4o\",\n                \"metadata\": {\n                    \"category\": \"system\"\n                }\n            },\n            \"documentation\": {\n                \"template\": \"Generate documentation for the following code:\\n\\n```{language}\\n{code}\\n```\\n\\nFormat: {format}\",\n                \"description\": \"Generate code documentation\",\n                \"parameters\": {\n                    \"language\": {\n                        \"type\": \"string\",\n                        \"description\": \"Programming language of the code\"\n                    },\n                    \"code\": {\n                        \"type\": \"string\",\n                        \"description\": \"The code to document\"\n                    },\n                    \"format\": {\n                        \"type\": \"string\",\n                        \"description\": \"Documentation format (e.g., 'markdown', 'docstring', 'jsdoc')\",\n                        \"default\": \"markdown\"\n                    }\n                },\n                \"default_model\": \"gpt-4o\",\n                \"use_openai\": True,\n                \"system_prompt\": \"You are a technical documentation expert. Generate clear, concise, and accurate documentation for the provided code.\",\n                \"metadata\": {\n                    \"category\": \"development\"\n                }\n            },\n            \"explain_code\": {\n                \"template\": \"Explain how the following code works:\\n\\n```{language}\\n{code}\\n```\\n\\nDetail level: {detail_level}\",\n                \"description\": \"Explain code functionality\",\n                \"parameters\": {\n                    \"language\": {\n                        \"type\": \"string\",\n                        \"description\": \"Programming language of the code\"\n                    },\n                    \"code\": {\n                        \"type\": \"string\",\n                        \"description\": \"The code to explain\"\n                    },\n                    \"detail_level\": {\n                        \"type\": \"string\",\n                        \"description\": \"Level of detail in the explanation (e.g., 'basic', 'intermediate', 'advanced')\",\n                        \"default\": \"intermediate\"\n                    }\n                },\n                \"default_model\": \"gpt-4o\",\n                \"use_openai\": True,\n                \"system_prompt\": \"You are a programming instructor. Explain the provided code clearly at the requested level of detail.\",\n                \"metadata\": {\n                    \"category\": \"education\"\n                }\n            },\n            \"current_time\": {\n                \"template\": \"The current time is {{now:%Y-%m-%d %H:%M:%S}}.\",\n                \"description\": \"Get the current time\",\n                \"parameters\": {},\n                \"default_model\": \"gpt-4o\",\n                \"metadata\": {\n                    \"category\": \"utility\"\n                }\n            }\n        }\n        \n        # Save default templates\n        try:\n            with open(templates_file, \"w\") as f:\n                json.dump(default_templates, f, indent=2)\n        except Exception as e:\n            logger.error(f\"Error saving default prompt templates: {str(e)}\")\n        \n        return default_templates\n    \n    def _save_prompt_templates(self):\n        \"\"\"Save prompt templates to file\"\"\"\n        templates_file = os.path.join(os.path.dirname(__file__), \"data\", \"prompt_templates.json\")\n        \n        try:\n            with open(templates_file, \"w\") as f:\n                json.dump(self.prompt_templates, f, indent=2)\n        except Exception as e:\n            logger.error(f\"Error saving prompt templates: {str(e)}\")\n    \n    async def _generate_with_openai(self, prompt: str, model: str, system_prompt: Optional[str] = None) -> tuple:\n        \"\"\"Generate context using OpenAI API\"\"\"\n        messages = []\n        \n        # Add system prompt if provided\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n        \n        # Add user prompt\n        messages.append({\"role\": \"user\", \"content\": prompt})\n        \n        # Call OpenAI API\n        try:\n            response = await asyncio.to_thread(\n                self.openai_client.chat.completions.create,\n                model=model,\n                messages=messages,\n                temperature=0.0,  # Use deterministic output for context generation\n                max_tokens=4000\n            )\n            \n            # Extract content and usage\n            content = response.choices[0].message.content\n            usage = {\n                \"prompt_tokens\": response.usage.prompt_tokens,\n                \"completion_tokens\": response.usage.completion_tokens,\n                \"total_tokens\": response.usage.total_tokens\n            }\n            \n            return content, usage\n            \n        except Exception as e:\n            logger.error(f\"OpenAI API error: {str(e)}\")\n            raise ValueError(f\"Error generating context with OpenAI: {str(e)}\")\n    \n    async def _stream_context(self, prompt: str, model: str, context_id: str, system_prompt: Optional[str] = None) -> AsyncGenerator[str, None]:\n        \"\"\"Stream context generation using OpenAI API\"\"\"\n        messages = []\n        \n        # Add system prompt if provided\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n        \n        # Add user prompt\n        messages.append({\"role\": \"user\", \"content\": prompt})\n        \n        # Initial event with context ID\n        yield f\"data: {json.dumps({'context_id': context_id, 'event': 'start'})}\\n\\n\"\n        \n        try:\n            # Call OpenAI API with streaming\n            stream = await asyncio.to_thread(\n                self.openai_client.chat.completions.create,\n                model=model,\n                messages=messages,\n                temperature=0.0,\n                max_tokens=4000,\n                stream=True\n            )\n            \n            full_content = \"\"\n            \n            # Process the stream\n            for chunk in stream:\n                if chunk.choices and chunk.choices[0].delta.content:\n                    content_piece = chunk.choices[0].delta.content\n                    full_content += content_piece\n                    \n                    # Yield the content piece\n                    yield f\"data: {json.dumps({'content': content_piece, 'event': 'content'})}\\n\\n\"\n            \n            # Calculate token usage\n            prompt_tokens = len(self.tokenizer.encode(prompt))\n            completion_tokens = len(self.tokenizer.encode(full_content))\n            total_tokens = prompt_tokens + completion_tokens\n            \n            # Track token usage\n            TOKEN_USAGE.labels(model=model, type=\"prompt\").inc(prompt_tokens)\n            TOKEN_USAGE.labels(model=model, type=\"completion\").inc(completion_tokens)\n            \n            # Final event with complete context and usage\n            yield f\"data: {json.dumps({\n                'event': 'end',\n                'context': full_content,\n                'usage': {\n                    'prompt_tokens': prompt_tokens,\n                    'completion_tokens': completion_tokens,\n                    'total_tokens': total_tokens\n                }\n            })}\\n\\n\"\n            \n        except Exception as e:\n            logger.error(f\"Error streaming context: {str(e)}\")\n            yield f\"data: {json.dumps({'event': 'error', 'error': str(e)})}\\n\\n\"\n    \n    def _process_template(self, template: str, parameters: Dict[str, Any]) -> str:\n        \"\"\"Process a template with parameters\"\"\"\n        try:\n            # Handle date/time formatting if needed\n            processed_params = parameters.copy()\n            for key, value in processed_params.items():\n                if isinstance(value, str) and value.startswith(\"{{now\") and value.endswith(\"}}\"):\n                    # Extract format string if present\n                    format_match = re.search(r\"{{now:(.+)}}\", value)\n                    if format_match:\n                        format_string = format_match.group(1)\n                        processed_params[key] = datetime.now().strftime(format_string)\n                    else:\n                        processed_params[key] = datetime.now().isoformat()\n            \n            return template.format(**processed_params)\n        except KeyError as e:\n            raise ValueError(f\"Missing required parameter: {e}\")\n        except Exception as e:\n            raise ValueError(f\"Error processing template: {str(e)}\")\n    \n    def start(self, host: str = \"127.0.0.1\", port: int = 8000, reload: bool = False):\n        \"\"\"Start the MCP server\"\"\"\n        uvicorn.run(self.app, host=host, port=port, reload=reload)\n\ndef create_mcp_app():\n    \"\"\"Factory function for creating the FastAPI app\"\"\"\n    server = MCPServer()\n    return server.app\n\nif __name__ == \"__main__\":\n    # Create data directory if it doesn't exist\n    os.makedirs(os.path.join(os.path.dirname(__file__), \"data\"), exist_ok=True)\n    \n    # Start server\n    server = MCPServer()\n    server.start()\n"}
{"type": "source_file", "path": "setup.py", "content": "from setuptools import setup, find_packages\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\nwith open(\"requirements.txt\", \"r\", encoding=\"utf-8\") as f:\n    requirements = [line.strip() for line in f.readlines() if line.strip()]\n\nsetup(\n    name=\"claude_code\",\n    version=\"0.1.0\",\n    author=\"Claude Code Team\",\n    author_email=\"noreply@example.com\",\n    description=\"Python recreation of Claude Code with enhanced features\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/yourusername/claude-code-python\",\n    packages=find_packages(),\n    install_requires=requirements,\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Developers\",\n        \"Topic :: Software Development :: User Interfaces\",\n    ],\n    python_requires=\">=3.10\",\n    entry_points={\n        \"console_scripts\": [\n            \"claude-code=claude_code.claude:app\",\n        ],\n    },\n)"}
{"type": "source_file", "path": "mcp_modal_adapter.py", "content": "import os\nimport json\nimport logging\nimport asyncio\nimport httpx\nfrom typing import Dict, List, Optional, Any, AsyncIterator\nfrom fastapi import FastAPI, Request, HTTPException, status\nfrom fastapi.responses import JSONResponse, StreamingResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"MCP Server Modal Adapter\",\n    description=\"Model Context Protocol server adapter for Modal OpenAI API\",\n    version=\"1.0.0\"\n)\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Configuration\nMODAL_API_URL = os.environ.get(\"MODAL_API_URL\", \"https://your-modal-app-url.modal.run\")\nMODAL_API_KEY = os.environ.get(\"MODAL_API_KEY\", \"sk-modal-llm-api-key\")  # Default key from modal_mcp_server.py\nDEFAULT_MODEL = os.environ.get(\"DEFAULT_MODEL\", \"phi-4\")\n\n# MCP Protocol Models\nclass MCPHealthResponse(BaseModel):\n    status: str = \"healthy\"\n    version: str = \"1.0.0\"\n\nclass MCPPromptTemplate(BaseModel):\n    id: str\n    name: str\n    description: str\n    template: str\n    parameters: Dict[str, Any] = Field(default_factory=dict)\n\nclass MCPPromptLibraryResponse(BaseModel):\n    prompts: List[MCPPromptTemplate]\n\nclass MCPContextResponse(BaseModel):\n    context_id: str\n    content: str\n    model: str\n    prompt_id: Optional[str] = None\n    parameters: Optional[Dict[str, Any]] = None\n\n# Default prompt template\nDEFAULT_TEMPLATE = MCPPromptTemplate(\n    id=\"default\",\n    name=\"Default Template\",\n    description=\"Default prompt template for general use\",\n    template=\"{prompt}\",\n    parameters={\"prompt\": {\"type\": \"string\", \"description\": \"The prompt to send to the model\"}}\n)\n\n# In-memory prompt library\nprompt_library = {\n    \"default\": DEFAULT_TEMPLATE.dict()\n}\n\n# Health check endpoint\n@app.get(\"/health\", response_model=MCPHealthResponse)\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return MCPHealthResponse()\n\n# List prompts endpoint\n@app.get(\"/prompts\", response_model=MCPPromptLibraryResponse)\nasync def list_prompts():\n    \"\"\"List available prompt templates\"\"\"\n    return MCPPromptLibraryResponse(prompts=[MCPPromptTemplate(**prompt) for prompt in prompt_library.values()])\n\n# Get prompt endpoint\n@app.get(\"/prompts/{prompt_id}\", response_model=MCPPromptTemplate)\nasync def get_prompt(prompt_id: str):\n    \"\"\"Get a specific prompt template\"\"\"\n    if prompt_id not in prompt_library:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Prompt template with ID {prompt_id} not found\"\n        )\n    return MCPPromptTemplate(**prompt_library[prompt_id])\n\n# Get context endpoint\n@app.post(\"/context/{prompt_id}\")\nasync def get_context(prompt_id: str, request: Request):\n    \"\"\"Get context from a prompt template\"\"\"\n    try:\n        # Get request data\n        data = await request.json()\n        parameters = data.get(\"parameters\", {})\n        model = data.get(\"model\", DEFAULT_MODEL)\n        stream = data.get(\"stream\", False)\n        \n        # Get prompt template\n        if prompt_id not in prompt_library:\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND,\n                detail=f\"Prompt template with ID {prompt_id} not found\"\n            )\n        \n        prompt_template = prompt_library[prompt_id]\n        \n        # Process template\n        template = prompt_template[\"template\"]\n        prompt_text = template.format(**parameters)\n        \n        # Create OpenAI-compatible request\n        openai_request = {\n            \"model\": model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n            \"temperature\": parameters.get(\"temperature\", 0.7),\n            \"max_tokens\": parameters.get(\"max_tokens\", 1024),\n            \"stream\": stream\n        }\n        \n        # If streaming is requested, return a streaming response\n        if stream:\n            return StreamingResponse(\n                stream_from_modal(openai_request),\n                media_type=\"text/event-stream\"\n            )\n        \n        # Otherwise, make a regular request to Modal API\n        async with httpx.AsyncClient(timeout=60.0) as client:\n            headers = {\n                \"Authorization\": f\"Bearer {MODAL_API_KEY}\",\n                \"Content-Type\": \"application/json\"\n            }\n            \n            response = await client.post(\n                f\"{MODAL_API_URL}/v1/chat/completions\",\n                json=openai_request,\n                headers=headers\n            )\n            \n            if response.status_code != 200:\n                raise HTTPException(\n                    status_code=response.status_code,\n                    detail=f\"Error from Modal API: {response.text}\"\n                )\n            \n            result = response.json()\n            \n            # Extract content from OpenAI response\n            content = \"\"\n            if \"choices\" in result and len(result[\"choices\"]) > 0:\n                if \"message\" in result[\"choices\"][0] and \"content\" in result[\"choices\"][0][\"message\"]:\n                    content = result[\"choices\"][0][\"message\"][\"content\"]\n            \n            # Create MCP response\n            mcp_response = MCPContextResponse(\n                context_id=result.get(\"id\", \"\"),\n                content=content,\n                model=model,\n                prompt_id=prompt_id,\n                parameters=parameters\n            )\n            \n            return mcp_response.dict()\n            \n    except Exception as e:\n        logging.error(f\"Error in get_context: {str(e)}\")\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=f\"Error generating context: {str(e)}\"\n        )\n\nasync def stream_from_modal(openai_request: Dict[str, Any]) -> AsyncIterator[str]:\n    \"\"\"Stream response from Modal API\"\"\"\n    try:\n        async with httpx.AsyncClient(timeout=300.0) as client:\n            headers = {\n                \"Authorization\": f\"Bearer {MODAL_API_KEY}\",\n                \"Content-Type\": \"application/json\",\n                \"Accept\": \"text/event-stream\"\n            }\n            \n            async with client.stream(\n                \"POST\",\n                f\"{MODAL_API_URL}/v1/chat/completions\",\n                json=openai_request,\n                headers=headers\n            ) as response:\n                if response.status_code != 200:\n                    error_detail = await response.aread()\n                    yield f\"data: {json.dumps({'error': f'Error from Modal API: {error_detail.decode()}'})}\\n\\n\"\n                    yield \"data: [DONE]\\n\\n\"\n                    return\n                \n                # Process streaming response\n                buffer = \"\"\n                content_buffer = \"\"\n                \n                async for chunk in response.aiter_text():\n                    buffer += chunk\n                    \n                    # Process complete SSE messages\n                    while \"\\n\\n\" in buffer:\n                        message, buffer = buffer.split(\"\\n\\n\", 1)\n                        \n                        if message.startswith(\"data: \"):\n                            data = message[6:]  # Remove \"data: \" prefix\n                            \n                            if data == \"[DONE]\":\n                                # End of stream, send final MCP response\n                                final_response = MCPContextResponse(\n                                    context_id=\"stream-\" + str(hash(content_buffer))[:8],\n                                    content=content_buffer,\n                                    model=openai_request.get(\"model\", DEFAULT_MODEL),\n                                    prompt_id=\"default\",\n                                    parameters={}\n                                )\n                                \n                                yield f\"data: {json.dumps(final_response.dict())}\\n\\n\"\n                                yield \"data: [DONE]\\n\\n\"\n                                return\n                            \n                            try:\n                                # Parse JSON data\n                                chunk_data = json.loads(data)\n                                \n                                # Extract content from chunk\n                                if 'choices' in chunk_data and len(chunk_data['choices']) > 0:\n                                    if 'delta' in chunk_data['choices'][0] and 'content' in chunk_data['choices'][0]['delta']:\n                                        content = chunk_data['choices'][0]['delta']['content']\n                                        content_buffer += content\n                                        \n                                        # Create partial MCP response\n                                        partial_response = {\n                                            \"context_id\": \"stream-\" + str(hash(content_buffer))[:8],\n                                            \"content\": content,\n                                            \"model\": openai_request.get(\"model\", DEFAULT_MODEL),\n                                            \"is_partial\": True\n                                        }\n                                        \n                                        yield f\"data: {json.dumps(partial_response)}\\n\\n\"\n                                        \n                            except json.JSONDecodeError:\n                                logging.error(f\"Invalid JSON in stream: {data}\")\n                \n    except Exception as e:\n        logging.error(f\"Error in stream_from_modal: {str(e)}\")\n        yield f\"data: {json.dumps({'error': str(e)})}\\n\\n\"\n        yield \"data: [DONE]\\n\\n\"\n\n# Add a custom prompt template\n@app.post(\"/prompts\")\nasync def add_prompt(prompt: MCPPromptTemplate):\n    \"\"\"Add a new prompt template\"\"\"\n    prompt_library[prompt.id] = prompt.dict()\n    return {\"status\": \"success\", \"message\": f\"Added prompt template with ID {prompt.id}\"}\n\n# Delete a prompt template\n@app.delete(\"/prompts/{prompt_id}\")\nasync def delete_prompt(prompt_id: str):\n    \"\"\"Delete a prompt template\"\"\"\n    if prompt_id == \"default\":\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Cannot delete the default prompt template\"\n        )\n        \n    if prompt_id not in prompt_library:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Prompt template with ID {prompt_id} not found\"\n        )\n        \n    del prompt_library[prompt_id]\n    return {\"status\": \"success\", \"message\": f\"Deleted prompt template with ID {prompt_id}\"}\n\n# Main entry point\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"}
{"type": "source_file", "path": "modal_mcp_server.py", "content": "import modal\nimport logging\nimport time\nimport uuid\nimport json\nimport asyncio\nimport hashlib\nimport threading\nimport concurrent.futures\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple, Union, AsyncIterator\nfrom datetime import datetime, timedelta\nfrom collections import deque\n\nfrom fastapi import FastAPI, Request, Depends, HTTPException, status, BackgroundTasks\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom fastapi.responses import JSONResponse, HTMLResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# Create FastAPI app\napi_app = FastAPI(\n    title=\"Advanced LLM Inference API\", \n    description=\"Enterprise-grade OpenAI-compatible LLM serving API with multiple model support, streaming, and advanced caching\",\n    version=\"1.1.0\"\n)\n\n# Add CORS middleware\napi_app.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # For production, specify specific origins instead of wildcard\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Security setup\nsecurity = HTTPBearer()\n\n# Token bucket rate limiter\nclass TokenBucket:\n    \"\"\"\n    Token bucket algorithm for rate limiting.\n    Each user gets a bucket that fills at a constant rate.\n    \"\"\"\n    def __init__(self):\n        self.buckets = {}\n        self.lock = threading.Lock()\n    \n    def _get_bucket(self, user_id, rate_limit):\n        \"\"\"Get or create a bucket for a user\"\"\"\n        now = time.time()\n        \n        if user_id not in self.buckets:\n            # Initialize with full bucket\n            self.buckets[user_id] = {\n                \"tokens\": rate_limit,\n                \"last_refill\": now,\n                \"rate\": rate_limit / 60.0  # tokens per second\n            }\n            return self.buckets[user_id]\n        \n        bucket = self.buckets[user_id]\n        \n        # Update rate if it changed\n        bucket[\"rate\"] = rate_limit / 60.0\n        \n        # Refill tokens based on time elapsed\n        elapsed = now - bucket[\"last_refill\"]\n        new_tokens = elapsed * bucket[\"rate\"]\n        \n        bucket[\"tokens\"] = min(rate_limit, bucket[\"tokens\"] + new_tokens)\n        bucket[\"last_refill\"] = now\n        \n        return bucket\n    \n    def consume(self, user_id, tokens=1, rate_limit=60):\n        \"\"\"\n        Consume tokens from a user's bucket.\n        Returns True if tokens were consumed, False otherwise.\n        \"\"\"\n        with self.lock:\n            bucket = self._get_bucket(user_id, rate_limit)\n            \n            if bucket[\"tokens\"] >= tokens:\n                bucket[\"tokens\"] -= tokens\n                return True\n            return False\n\n# Create rate limiter\nrate_limiter = TokenBucket()\n\n# Define the container image with necessary dependencies\nvllm_image = (\n    modal.Image.debian_slim(python_version=\"3.10\")\n    .pip_install(\n        \"vllm==0.7.3\",  # Updated version\n        \"huggingface_hub[hf_transfer]==0.26.2\",\n        \"flashinfer-python==0.2.0.post2\",\n        \"fastapi>=0.95.0\",\n        \"uvicorn>=0.15.0\",\n        \"pydantic>=2.0.0\",\n        \"tiktoken>=0.5.1\",\n        extra_index_url=\"https://flashinfer.ai/whl/cu124/torch2.5\",\n    )\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})  # faster model transfers\n    .env({\"VLLM_USE_V1\": \"1\"})  # Enable V1 engine for better performance\n)\n\n# Define llama.cpp image for alternative models\nllama_cpp_image = (\n    modal.Image.debian_slim(python_version=\"3.10\")\n    .apt_install(\"git\", \"build-essential\", \"cmake\", \"curl\", \"libcurl4-openssl-dev\")\n    .pip_install(\n        \"huggingface_hub==0.26.2\",\n        \"hf_transfer>=0.1.4\",\n        \"fastapi>=0.95.0\",\n        \"uvicorn>=0.15.0\",\n        \"pydantic>=2.0.0\"\n    )\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n    .run_commands(\n        \"git clone https://github.com/ggerganov/llama.cpp\",\n        \"cmake llama.cpp -B llama.cpp/build -DBUILD_SHARED_LIBS=OFF -DLLAMA_CURL=ON\",\n        \"cmake --build llama.cpp/build --config Release -j --target llama-cli\",\n        \"cp llama.cpp/build/bin/llama-* /usr/local/bin/\"\n    )\n)\n\n# Set up model configurations\nMODELS_DIR = \"/models\"\nVLLM_MODELS = {\n    \"llama3-8b\": {\n        \"id\": \"llama3-8b\",\n        \"name\": \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",\n        \"config\": \"config.json\",  # Ensure this file is present in the model directory\n        \"revision\": \"a7c09948d9a632c2c840722f519672cd94af885d\",\n        \"max_tokens\": 4096,\n        \"loaded\": False\n    },\n    \"mistral-7b\": {\n        \"id\": \"mistral-7b\",\n        \"name\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n        \"revision\": \"main\",\n        \"max_tokens\": 4096,\n        \"loaded\": False\n    },\n    # Small model for quick loading\n    \"tiny-llama-1.1b\": {\n        \"id\": \"tiny-llama-1.1b\",\n        \"name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        \"revision\": \"main\",\n        \"max_tokens\": 2048,\n        \"loaded\": False\n    }\n}\n\nLLAMA_CPP_MODELS = {\n    \"deepseek-r1\": {\n        \"id\": \"deepseek-r1\",\n        \"name\": \"unsloth/DeepSeek-R1-GGUF\",\n        \"quant\": \"UD-IQ1_S\",\n        \"pattern\": \"*UD-IQ1_S*\",\n        \"revision\": \"02656f62d2aa9da4d3f0cdb34c341d30dd87c3b6\",\n        \"gpu\": \"L40S:4\",\n        \"max_tokens\": 4096,\n        \"loaded\": False\n    },\n    \"phi-4\": {\n        \"id\": \"phi-4\",\n        \"name\": \"unsloth/phi-4-GGUF\",\n        \"quant\": \"Q2_K\",\n        \"pattern\": \"*Q2_K*\",\n        \"revision\": None,\n        \"gpu\": \"L40S:4\",  # Use GPU for better performance\n        \"max_tokens\": 4096,\n        \"loaded\": False\n    },\n    # Small model for quick loading\n    \"phi-2\": {\n        \"id\": \"phi-2\",\n        \"name\": \"TheBloke/phi-2-GGUF\",\n        \"quant\": \"Q4_K_M\",\n        \"pattern\": \"*Q4_K_M.gguf\",\n        \"revision\": \"main\",\n        \"gpu\": None,  # Can run on CPU\n        \"max_tokens\": 2048,\n        \"loaded\": False\n    }\n}\n\nDEFAULT_MODEL = \"phi-4\"\n\n# Create volumes for caching\nhf_cache_vol = modal.Volume.from_name(\"huggingface-cache\", create_if_missing=True)\nvllm_cache_vol = modal.Volume.from_name(\"vllm-cache\", create_if_missing=True)\nllama_cpp_cache_vol = modal.Volume.from_name(\"llama-cpp-cache\", create_if_missing=True)\nresults_vol = modal.Volume.from_name(\"model-results\", create_if_missing=True)\n\n# Create the Modal app\napp = modal.App(\"openai-compatible-llm-server\")\n\n# Create shared data structures\nmodel_stats_dict = modal.Dict.from_name(\"model-stats\", create_if_missing=True)\nuser_usage_dict = modal.Dict.from_name(\"user-usage\", create_if_missing=True)\nrequest_queue = modal.Queue.from_name(\"request-queue\", create_if_missing=True)\nresponse_dict = modal.Dict.from_name(\"response-cache\", create_if_missing=True)\napi_keys_dict = modal.Dict.from_name(\"api-keys\", create_if_missing=True)\nstream_queues = modal.Dict.from_name(\"stream-queues\", create_if_missing=True)\n\n# Advanced caching system\nclass AdvancedCache:\n    \"\"\"\n    Advanced caching system with TTL and LRU eviction.\n    \"\"\"\n    def __init__(self, max_size=1000, default_ttl=3600):\n        self.cache = {}\n        self.ttl_map = {}\n        self.access_times = {}\n        self.max_size = max_size\n        self.default_ttl = default_ttl\n        self.lock = threading.Lock()\n    \n    def get(self, key):\n        \"\"\"Get a value from the cache\"\"\"\n        with self.lock:\n            now = time.time()\n            \n            # Check if key exists and is not expired\n            if key in self.cache:\n                # Check TTL\n                if key in self.ttl_map and self.ttl_map[key] < now:\n                    # Expired\n                    self._remove(key)\n                    return None\n                \n                # Update access time\n                self.access_times[key] = now\n                return self.cache[key]\n            \n            return None\n    \n    def set(self, key, value, ttl=None):\n        \"\"\"Set a value in the cache with optional TTL\"\"\"\n        with self.lock:\n            now = time.time()\n            \n            # Evict if needed\n            if len(self.cache) >= self.max_size and key not in self.cache:\n                self._evict_lru()\n            \n            # Set value\n            self.cache[key] = value\n            self.access_times[key] = now\n            \n            # Set TTL\n            if ttl is not None:\n                self.ttl_map[key] = now + ttl\n            elif self.default_ttl > 0:\n                self.ttl_map[key] = now + self.default_ttl\n    \n    def _remove(self, key):\n        \"\"\"Remove a key from the cache\"\"\"\n        if key in self.cache:\n            del self.cache[key]\n        if key in self.ttl_map:\n            del self.ttl_map[key]\n        if key in self.access_times:\n            del self.access_times[key]\n    \n    def _evict_lru(self):\n        \"\"\"Evict least recently used item\"\"\"\n        if not self.access_times:\n            return\n        \n        # Find oldest access time\n        oldest_key = min(self.access_times.items(), key=lambda x: x[1])[0]\n        self._remove(oldest_key)\n    \n    def clear_expired(self):\n        \"\"\"Clear all expired entries\"\"\"\n        with self.lock:\n            now = time.time()\n            expired_keys = [k for k, v in self.ttl_map.items() if v < now]\n            for key in expired_keys:\n                self._remove(key)\n\n# Constants\nMAX_CACHE_AGE = 3600  # 1 hour in seconds\n\n# Create memory cache\nmemory_cache = AdvancedCache(max_size=10000, default_ttl=MAX_CACHE_AGE)\n\n# Initialize with default key if empty\nif \"default\" not in api_keys_dict:\n    api_keys_dict[\"default\"] = {\n        \"key\": \"sk-modal-llm-api-key\",\n        \"rate_limit\": 60,  # requests per minute\n        \"quota\": 1000000,  # tokens per day\n        \"created_at\": datetime.now().isoformat(),\n        \"owner\": \"default\"\n    }\n\n# Add a default ADMIN API key\nif \"admin\" not in api_keys_dict:\n    api_keys_dict[\"admin\"] = {\n        \"key\": \"sk-modal-admin-api-key\",\n        \"rate_limit\": 1000,  # Higher rate limit for admin\n        \"quota\": 10000000,  # Higher quota for admin\n        \"created_at\": datetime.now().isoformat(),\n        \"owner\": \"admin\"\n    }\n\n# Constants\nDEFAULT_API_KEY = api_keys_dict[\"default\"][\"key\"]\nMINUTES = 60  # seconds\nSERVER_PORT = 8000\nCACHE_DIR = \"/root/.cache\"\nRESULTS_DIR = \"/root/results\"\n\n# Request/response models\nclass GenerationRequest(BaseModel):\n    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    model_id: str\n    messages: List[Dict[str, str]]\n    temperature: float = 0.7\n    max_tokens: int = 1024\n    top_p: float = 1.0\n    frequency_penalty: float = 0.0\n    presence_penalty: float = 0.0\n    user: Optional[str] = None\n    stream: bool = False\n    timestamp: float = Field(default_factory=time.time)\n    api_key: str = DEFAULT_API_KEY\n    \nclass StreamChunk(BaseModel):\n    \"\"\"Model for streaming response chunks\"\"\"\n    id: str\n    object: str = \"chat.completion.chunk\"\n    created: int\n    model: str\n    choices: List[Dict[str, Any]]\n    \nclass StreamManager:\n    \"\"\"Manages streaming responses for clients\"\"\"\n    def __init__(self):\n        self.streams = {}\n        self.lock = threading.Lock()\n    \n    def create_stream(self, request_id):\n        \"\"\"Create a new stream for a request\"\"\"\n        with self.lock:\n            self.streams[request_id] = {\n                \"queue\": asyncio.Queue(),\n                \"finished\": False,\n                \"created_at\": time.time()\n            }\n    \n    def add_chunk(self, request_id, chunk):\n        \"\"\"Add a chunk to a stream\"\"\"\n        with self.lock:\n            if request_id in self.streams:\n                stream = self.streams[request_id]\n                if not stream[\"finished\"]:\n                    stream[\"queue\"].put_nowait(chunk)\n    \n    def finish_stream(self, request_id):\n        \"\"\"Mark a stream as finished\"\"\"\n        with self.lock:\n            if request_id in self.streams:\n                self.streams[request_id][\"finished\"] = True\n                # Add None to signal end of stream\n                self.streams[request_id][\"queue\"].put_nowait(None)\n    \n    async def get_chunks(self, request_id):\n        \"\"\"Get chunks from a stream as an async generator\"\"\"\n        if request_id not in self.streams:\n            return\n        \n        stream = self.streams[request_id]\n        queue = stream[\"queue\"]\n        \n        while True:\n            chunk = await queue.get()\n            if chunk is None:  # End of stream\n                break\n            yield chunk\n            queue.task_done()\n        \n        # Clean up after streaming is done\n        with self.lock:\n            if request_id in self.streams:\n                del self.streams[request_id]\n    \n    def clean_old_streams(self, max_age=3600):\n        \"\"\"Clean up old streams\"\"\"\n        with self.lock:\n            now = time.time()\n            to_remove = []\n            \n            for request_id, stream in self.streams.items():\n                if now - stream[\"created_at\"] > max_age:\n                    to_remove.append(request_id)\n            \n            for request_id in to_remove:\n                if request_id in self.streams:\n                    # Mark as finished to stop any ongoing processing\n                    self.streams[request_id][\"finished\"] = True\n                    # Add None to unblock any waiting consumers\n                    self.streams[request_id][\"queue\"].put_nowait(None)\n                    # Remove from streams\n                    del self.streams[request_id]\n\n# Create stream manager\nstream_manager = StreamManager()\n\n# API Authentication dependency\ndef verify_api_key(credentials: HTTPAuthorizationCredentials = Depends(security)):\n    \"\"\"Verify that the API key in the authorization header is valid and check rate limits\"\"\"\n    if credentials.scheme != \"Bearer\":\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid authentication scheme. Use Bearer\",\n        )\n    \n    api_key = credentials.credentials\n    valid_key = False\n    key_info = None\n    \n    # Check if this is a known API key\n    for user_id, user_data in api_keys_dict.items():\n        if user_data.get(\"key\") == api_key:\n            valid_key = True\n            key_info = user_data\n            break\n    \n    if not valid_key:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid API key\",\n        )\n    \n    # Check rate limits\n    user_id = key_info.get(\"owner\", \"unknown\")\n    rate_limit = key_info.get(\"rate_limit\", 60)  # Default: 60 requests per minute\n    \n    # Get or initialize user usage tracking\n    if user_id not in user_usage_dict:\n        user_usage_dict[user_id] = {\n            \"requests\": [],\n            \"tokens\": {\n                \"input\": 0,\n                \"output\": 0,\n                \"last_reset\": datetime.now().isoformat()\n            }\n        }\n    \n    usage = user_usage_dict[user_id]\n    \n    # Check if user exceeded rate limit using token bucket algorithm\n    if not rate_limiter.consume(user_id, tokens=1, rate_limit=rate_limit):\n        # Calculate retry-after based on rate\n        retry_after = int(60 / rate_limit)  # seconds until at least one token is available\n        \n        raise HTTPException(\n            status_code=status.HTTP_429_TOO_MANY_REQUESTS,\n            detail=f\"Rate limit exceeded. Maximum {rate_limit} requests per minute.\",\n            headers={\"Retry-After\": str(retry_after)}\n        )\n    \n    # Add current request timestamp for analytics\n    now = datetime.now()\n    usage[\"requests\"].append(now.timestamp())\n    \n    # Clean up old requests (older than 1 day) to prevent unbounded growth\n    day_ago = (now - timedelta(days=1)).timestamp()\n    usage[\"requests\"] = [req for req in usage[\"requests\"] if req > day_ago]\n    \n    # Update usage dict\n    user_usage_dict[user_id] = usage\n    \n    # Return the API key and user ID\n    return {\"key\": api_key, \"user_id\": user_id}\n\n# API Endpoints\n@api_app.get(\"/\", response_class=HTMLResponse)\nasync def index():\n    \"\"\"Root endpoint that returns HTML with API information\"\"\"\n    return \"\"\"\n    <html>\n        <head>\n            <title>Modal LLM Inference API</title>\n            <style>\n                body { font-family: system-ui, sans-serif; max-width: 800px; margin: 0 auto; padding: 2rem; }\n                h1 { color: #4a56e2; }\n                code { background: #f4f4f8; padding: 0.2rem 0.4rem; border-radius: 3px; }\n            </style>\n        </head>\n        <body>\n            <h1>Modal LLM Inference API</h1>\n            <p>This is an OpenAI-compatible API for LLM inference powered by Modal.</p>\n            <p>Use the following endpoints:</p>\n            <ul>\n                <li><a href=\"/docs\">/docs</a> - API documentation</li>\n                <li><a href=\"/v1/models\">/v1/models</a> - List available models</li>\n                <li><code>/v1/chat/completions</code> - Chat completions endpoint</li>\n            </ul>\n        </body>\n    </html>\n    \"\"\"\n\n@api_app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\"status\": \"healthy\"}\n\n@api_app.get(\"/v1/models\", dependencies=[Depends(verify_api_key)])\nasync def list_models():\n    \"\"\"List all available models in OpenAI-compatible format\"\"\"\n    # Combine vLLM and llama.cpp models\n    all_models = []\n    \n    for model_id, model_info in VLLM_MODELS.items():\n        all_models.append({\n            \"id\": model_info[\"id\"],\n            \"object\": \"model\",\n            \"created\": 1677610602,\n            \"owned_by\": \"modal\",\n            \"engine\": \"vllm\",\n            \"loaded\": model_info.get(\"loaded\", False)\n        })\n        \n    for model_id, model_info in LLAMA_CPP_MODELS.items():\n        all_models.append({\n            \"id\": model_info[\"id\"],\n            \"object\": \"model\",\n            \"created\": 1677610602,\n            \"owned_by\": \"modal\",\n            \"engine\": \"llama.cpp\",\n            \"loaded\": model_info.get(\"loaded\", False)\n        })\n        \n    return {\"data\": all_models, \"object\": \"list\"}\n\n# Model management endpoints\nclass ModelLoadRequest(BaseModel):\n    \"\"\"Request model to load a specific model\"\"\"\n    model_id: str\n    force_reload: bool = False\n    \nclass HFModelLoadRequest(BaseModel):\n    \"\"\"Request to load a model directly from Hugging Face\"\"\"\n    repo_id: str\n    model_type: str = \"vllm\"  # \"vllm\" or \"llama.cpp\"\n    revision: Optional[str] = None\n    quant: Optional[str] = None  # For llama.cpp models\n    max_tokens: int = 4096\n    gpu: Optional[str] = None  # For llama.cpp models\n\n@api_app.post(\"/admin/models/load\", dependencies=[Depends(verify_api_key)])\nasync def load_model(request: ModelLoadRequest, background_tasks: BackgroundTasks):\n    \"\"\"Load a specific model into memory\"\"\"\n    model_id = request.model_id\n    force_reload = request.force_reload\n    \n    # Check if model exists\n    if model_id in VLLM_MODELS:\n        model_type = \"vllm\"\n        model_info = VLLM_MODELS[model_id]\n    elif model_id in LLAMA_CPP_MODELS:\n        model_type = \"llama.cpp\"\n        model_info = LLAMA_CPP_MODELS[model_id]\n    else:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Model {model_id} not found\"\n        )\n    \n    # Check if model is already loaded\n    if model_info.get(\"loaded\", False) and not force_reload:\n        return {\n            \"status\": \"success\",\n            \"message\": f\"Model {model_id} is already loaded\",\n            \"model_id\": model_id,\n            \"model_type\": model_type\n        }\n    \n    # Start loading the model in the background\n    if model_type == \"vllm\":\n        # Start vLLM server for this model\n        background_tasks.add_task(serve_vllm_model.remote, model_id=model_id)\n        # Update model status\n        VLLM_MODELS[model_id][\"loaded\"] = True\n    else:  # llama.cpp\n        # For llama.cpp models, we'll preload the model\n        background_tasks.add_task(preload_llama_cpp_model, model_id)\n        # Update model status\n        LLAMA_CPP_MODELS[model_id][\"loaded\"] = True\n    \n    return {\n        \"status\": \"success\",\n        \"message\": f\"Started loading model {model_id}\",\n        \"model_id\": model_id,\n        \"model_type\": model_type\n    }\n\n@api_app.post(\"/admin/models/load-from-hf\", dependencies=[Depends(verify_api_key)])\nasync def load_model_from_hf(request: HFModelLoadRequest, background_tasks: BackgroundTasks):\n    \"\"\"Load a model directly from Hugging Face\"\"\"\n    repo_id = request.repo_id\n    model_type = request.model_type\n    revision = request.revision\n    \n    # Generate a unique model_id based on the repo name\n    repo_name = repo_id.split(\"/\")[-1] if \"/\" in repo_id else repo_id\n    model_id = f\"hf-{repo_name}-{uuid.uuid4().hex[:6]}\"\n    \n    # Create model info based on type\n    if model_type.lower() == \"vllm\":\n        # Add to VLLM_MODELS\n        VLLM_MODELS[model_id] = {\n            \"id\": model_id,\n            \"name\": repo_id,\n            \"revision\": revision or \"main\",\n            \"max_tokens\": request.max_tokens,\n            \"loaded\": False,\n            \"hf_direct\": True  # Mark as directly loaded from HF\n        }\n        \n        # Start vLLM server for this model\n        background_tasks.add_task(serve_vllm_model.remote, model_id=model_id)\n        # Update model status\n        VLLM_MODELS[model_id][\"loaded\"] = True\n        \n    elif model_type.lower() == \"llama.cpp\":\n        # For llama.cpp we need quant info\n        quant = request.quant or \"Q4_K_M\"  # Default quantization\n        pattern = f\"*{quant}*\"\n        \n        # Add to LLAMA_CPP_MODELS\n        LLAMA_CPP_MODELS[model_id] = {\n            \"id\": model_id,\n            \"name\": repo_id,\n            \"quant\": quant,\n            \"pattern\": pattern,\n            \"revision\": revision,\n            \"gpu\": request.gpu,  # Can be None for CPU\n            \"max_tokens\": request.max_tokens,\n            \"loaded\": False,\n            \"hf_direct\": True  # Mark as directly loaded from HF\n        }\n        \n        # Preload the model\n        background_tasks.add_task(preload_llama_cpp_model, model_id)\n        # Update model status\n        LLAMA_CPP_MODELS[model_id][\"loaded\"] = True\n        \n    else:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=f\"Invalid model type: {model_type}. Must be 'vllm' or 'llama.cpp'\"\n        )\n    \n    return {\n        \"status\": \"success\",\n        \"message\": f\"Started loading model {repo_id} as {model_id}\",\n        \"model_id\": model_id,\n        \"model_type\": model_type,\n        \"repo_id\": repo_id\n    }\n\n@api_app.post(\"/admin/models/unload\", dependencies=[Depends(verify_api_key)])\nasync def unload_model(request: ModelLoadRequest):\n    \"\"\"Unload a specific model from memory\"\"\"\n    model_id = request.model_id\n    \n    # Check if model exists\n    if model_id in VLLM_MODELS:\n        model_type = \"vllm\"\n        model_info = VLLM_MODELS[model_id]\n    elif model_id in LLAMA_CPP_MODELS:\n        model_type = \"llama.cpp\"\n        model_info = LLAMA_CPP_MODELS[model_id]\n    else:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Model {model_id} not found\"\n        )\n    \n    # Check if model is loaded\n    if not model_info.get(\"loaded\", False):\n        return {\n            \"status\": \"success\",\n            \"message\": f\"Model {model_id} is not loaded\",\n            \"model_id\": model_id,\n            \"model_type\": model_type\n        }\n    \n    # Update model status\n    if model_type == \"vllm\":\n        VLLM_MODELS[model_id][\"loaded\"] = False\n    else:  # llama.cpp\n        LLAMA_CPP_MODELS[model_id][\"loaded\"] = False\n    \n    return {\n        \"status\": \"success\",\n        \"message\": f\"Unloaded model {model_id}\",\n        \"model_id\": model_id,\n        \"model_type\": model_type\n    }\n\n@api_app.get(\"/admin/models/status/{model_id}\", dependencies=[Depends(verify_api_key)])\nasync def get_model_status(model_id: str):\n    \"\"\"Get the status of a specific model\"\"\"\n    # Check if model exists\n    if model_id in VLLM_MODELS:\n        model_type = \"vllm\"\n        model_info = VLLM_MODELS[model_id]\n    elif model_id in LLAMA_CPP_MODELS:\n        model_type = \"llama.cpp\"\n        model_info = LLAMA_CPP_MODELS[model_id]\n    else:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Model {model_id} not found\"\n        )\n    \n    # Get model stats if available\n    model_stats = model_stats_dict.get(model_id, {})\n    \n    # Include HF info if available\n    hf_info = {}\n    if model_info.get(\"hf_direct\"):\n        hf_info = {\n            \"repo_id\": model_info.get(\"name\"),\n            \"revision\": model_info.get(\"revision\"),\n        }\n        if model_type == \"llama.cpp\":\n            hf_info[\"quant\"] = model_info.get(\"quant\")\n    \n    return {\n        \"model_id\": model_id,\n        \"model_type\": model_type,\n        \"loaded\": model_info.get(\"loaded\", False),\n        \"stats\": model_stats,\n        \"hf_info\": hf_info if hf_info else None\n    }\n\n# Admin API endpoints\nclass APIKeyRequest(BaseModel):\n    user_id: str\n    rate_limit: int = 60\n    quota: int = 1000000\n    \nclass APIKey(BaseModel):\n    key: str\n    user_id: str\n    rate_limit: int\n    quota: int\n    created_at: str\n\n@api_app.post(\"/admin/api-keys\", response_model=APIKey)\nasync def create_api_key(request: APIKeyRequest, auth_info: dict = Depends(verify_api_key)):\n    \"\"\"Create a new API key for a user (admin only)\"\"\"\n    # Check if this is an admin request\n    if auth_info[\"user_id\"] != \"default\":\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Only admin users can create API keys\"\n        )\n    \n    # Generate a new API key\n    new_key = f\"sk-modal-{uuid.uuid4()}\"\n    user_id = request.user_id\n    \n    # Store the key\n    api_keys_dict[user_id] = {\n        \"key\": new_key,\n        \"rate_limit\": request.rate_limit,\n        \"quota\": request.quota,\n        \"created_at\": datetime.now().isoformat(),\n        \"owner\": user_id\n    }\n    \n    # Initialize user usage\n    if not user_usage_dict.contains(user_id):\n        user_usage_dict[user_id] = {\n            \"requests\": [],\n            \"tokens\": {\n                \"input\": 0,\n                \"output\": 0,\n                \"last_reset\": datetime.now().isoformat()\n            }\n        }\n    \n    return APIKey(\n        key=new_key,\n        user_id=user_id,\n        rate_limit=request.rate_limit,\n        quota=request.quota,\n        created_at=datetime.now().isoformat()\n    )\n\n@api_app.get(\"/admin/api-keys\")\nasync def list_api_keys(auth_info: dict = Depends(verify_api_key)):\n    \"\"\"List all API keys (admin only)\"\"\"\n    # Check if this is an admin request\n    if auth_info[\"user_id\"] != \"default\":\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Only admin users can list API keys\"\n        )\n    \n    # Return all keys (except the actual key values for security)\n    keys = []\n    for user_id, key_info in api_keys_dict.items():\n        keys.append({\n            \"user_id\": user_id,\n            \"rate_limit\": key_info.get(\"rate_limit\", 60),\n            \"quota\": key_info.get(\"quota\", 1000000),\n            \"created_at\": key_info.get(\"created_at\", datetime.now().isoformat()),\n            # Mask the actual key\n            \"key\": key_info.get(\"key\", \"\")[:8] + \"...\" if key_info.get(\"key\") else \"None\"\n        })\n    \n    return {\"keys\": keys}\n\n@api_app.get(\"/admin/stats\")\nasync def get_stats(auth_info: dict = Depends(verify_api_key)):\n    \"\"\"Get usage statistics (admin only)\"\"\"\n    # Check if this is an admin request\n    if auth_info[\"user_id\"] != \"default\":\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Only admin users can view stats\"\n        )\n    \n    # Get model stats\n    model_stats = {}\n    for model_id in list(VLLM_MODELS.keys()) + list(LLAMA_CPP_MODELS.keys()):\n        if model_id in model_stats_dict:\n            model_stats[model_id] = model_stats_dict[model_id]\n    \n    # Get user stats\n    user_stats = {}\n    for user_id in user_usage_dict.keys():\n        usage = user_usage_dict[user_id]\n        # Don't include request timestamps for brevity\n        if \"requests\" in usage:\n            usage = usage.copy()\n            usage[\"request_count\"] = len(usage[\"requests\"])\n            del usage[\"requests\"]\n        user_stats[user_id] = usage\n    \n    # Get queue info\n    queue_info = {\n        \"pending_requests\": request_queue.len(),\n        \"active_workers\": model_stats_dict.get(\"workers_running\", 0)\n    }\n    \n    return {\n        \"models\": model_stats,\n        \"users\": user_stats,\n        \"queue\": queue_info,\n        \"timestamp\": datetime.now().isoformat()\n    }\n\n@api_app.delete(\"/admin/api-keys/{user_id}\")\nasync def delete_api_key(user_id: str, auth_info: dict = Depends(verify_api_key)):\n    \"\"\"Delete an API key (admin only)\"\"\"\n    # Check if this is an admin request\n    if auth_info[\"user_id\"] != \"default\":\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Only admin users can delete API keys\"\n        )\n    \n    # Check if the key exists\n    if not api_keys_dict.contains(user_id):\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"No API key found for user {user_id}\"\n        )\n    \n    # Can't delete the default key\n    if user_id == \"default\":\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Cannot delete the default API key\"\n        )\n    \n    # Delete the key\n    api_keys_dict.pop(user_id)\n    \n    return {\"status\": \"success\", \"message\": f\"API key deleted for user {user_id}\"}\n\n@api_app.post(\"/v1/chat/completions\")\nasync def chat_completions(request: Request, background_tasks: BackgroundTasks, auth_info: dict = Depends(verify_api_key)):\n    \"\"\"OpenAI-compatible chat completions endpoint with request queueing, streaming and response caching\"\"\"\n    try:\n        json_data = await request.json()\n        \n        # Extract model or use default\n        model_id = json_data.get(\"model\", DEFAULT_MODEL)\n        messages = json_data.get(\"messages\", [])\n        temperature = json_data.get(\"temperature\", 0.7)\n        max_tokens = json_data.get(\"max_tokens\", 1024)\n        stream = json_data.get(\"stream\", False)\n        user = json_data.get(\"user\", auth_info[\"user_id\"])\n        \n        # Calculate a cache key based on the request parameters\n        cache_key = calculate_cache_key(model_id, messages, temperature, max_tokens)\n        \n        # Check if we have a cached response in memory cache first (faster)\n        cached_response = memory_cache.get(cache_key)\n        if cached_response and not stream:  # Don't use cache for streaming requests\n            # Update stats\n            update_stats(model_id, \"cache_hit\")\n            return cached_response\n        \n        # Check if we have a cached response in Modal's persistent cache\n        if not cached_response and cache_key in response_dict and not stream:\n            cached_response = response_dict[cache_key]\n            cache_age = time.time() - cached_response.get(\"timestamp\", 0)\n            \n            # Use cached response if it's fresh enough\n            if cache_age < MAX_CACHE_AGE:\n                # Update stats\n                update_stats(model_id, \"cache_hit\")\n                response_data = cached_response[\"response\"]\n                \n                # Also cache in memory for faster access next time\n                memory_cache.set(cache_key, response_data)\n                \n                return response_data\n        \n        # Select best model if \"auto\" is specified\n        if model_id == \"auto\" and len(messages) > 0:\n            # Get the last user message\n            last_message = None\n            for msg in reversed(messages):\n                if msg.get(\"role\") == \"user\":\n                    last_message = msg.get(\"content\", \"\")\n                    break\n            \n            if last_message:\n                prompt = last_message\n                # Select best model based on prompt and parameters\n                model_id = select_best_model(prompt, max_tokens, temperature)\n                logging.info(f\"Auto-selected model: {model_id} for prompt\")\n        \n        # Check if model exists\n        if model_id not in VLLM_MODELS and model_id not in LLAMA_CPP_MODELS:\n            # Default to the default model if specified model not found\n            logging.warning(f\"Model {model_id} not found, using default: {DEFAULT_MODEL}\")\n            model_id = DEFAULT_MODEL\n        \n        # Create a unique request ID\n        request_id = str(uuid.uuid4())\n        \n        # Create request object\n        gen_request = GenerationRequest(\n            request_id=request_id,\n            model_id=model_id,\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            top_p=json_data.get(\"top_p\", 1.0),\n            frequency_penalty=json_data.get(\"frequency_penalty\", 0.0),\n            presence_penalty=json_data.get(\"presence_penalty\", 0.0),\n            user=user,\n            stream=stream,\n            api_key=auth_info[\"key\"]\n        )\n        \n        # For streaming requests, set up streaming response\n        if stream:\n            # Create a new stream\n            stream_manager.create_stream(request_id)\n            \n            # Put the request in the queue\n            await request_queue.put.aio(gen_request.model_dump())\n            \n            # Update stats\n            update_stats(model_id, \"request_count\")\n            update_stats(model_id, \"stream_count\")\n            \n            # Start a background worker to process the request if needed\n            background_tasks.add_task(ensure_worker_running)\n            \n            # Return a streaming response using FastAPI's StreamingResponse\n            from fastapi.responses import StreamingResponse as FastAPIStreamingResponse\n            return FastAPIStreamingResponse(\n                content=stream_response(request_id, model_id, auth_info[\"user_id\"]),\n                media_type=\"text/event-stream\"\n            )\n            \n        # For non-streaming, enqueue the request and wait for result\n        # Put the request in the queue\n        await request_queue.put.aio(gen_request.model_dump())\n        \n        # Update stats\n        update_stats(model_id, \"request_count\")\n        \n        # Start a background worker to process the request if needed\n        background_tasks.add_task(ensure_worker_running)\n        \n        # Wait for the response with timeout\n        start_time = time.time()\n        timeout = 120  # 2-minute timeout for non-streaming requests\n        \n        while time.time() - start_time < timeout:\n            # Check memory cache first (faster)\n            response_data = memory_cache.get(request_id)\n            if response_data:\n                # Update stats\n                update_stats(model_id, \"success_count\")\n                estimate_tokens(messages, response_data, auth_info[\"user_id\"], model_id)\n                \n                # Save to persistent cache\n                response_dict[cache_key] = {\n                    \"response\": response_data,\n                    \"timestamp\": time.time()\n                }\n                \n                # Clean up request-specific cache\n                memory_cache.set(request_id, None)\n                \n                return response_data\n                \n            # Check persistent cache\n            if response_dict.contains(request_id):\n                response_data = response_dict[request_id]\n                \n                # Remove from response dict to save memory\n                try:\n                    response_dict.pop(request_id)\n                except Exception:\n                    pass\n                \n                # Save to cache\n                response_dict[cache_key] = {\n                    \"response\": response_data,\n                    \"timestamp\": time.time()\n                }\n                \n                # Also cache in memory\n                memory_cache.set(cache_key, response_data)\n                \n                # Update stats\n                update_stats(model_id, \"success_count\")\n                estimate_tokens(messages, response_data, auth_info[\"user_id\"], model_id)\n                \n                return response_data\n            \n            # Wait a bit before checking again\n            await asyncio.sleep(0.1)\n        \n        # If we get here, we timed out\n        update_stats(model_id, \"timeout_count\")\n        raise HTTPException(\n            status_code=status.HTTP_504_GATEWAY_TIMEOUT,\n            detail=\"Request timed out. The model may be busy. Please try again later.\"\n        )\n            \n    except Exception as e:\n        logging.error(f\"Error in chat completions: {str(e)}\")\n        # Update error stats\n        if \"model_id\" in locals():\n            update_stats(model_id, \"error_count\")\n            \n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=f\"Error generating response: {str(e)}\"\n        )\n\nasync def stream_response(request_id: str, model_id: str, user_id: str) -> AsyncIterator[str]:\n    \"\"\"Stream response chunks to the client\"\"\"\n    try:\n        # Stream header\n        yield \"data: \" + json.dumps({\"object\": \"chat.completion.chunk\"}) + \"\\n\\n\"\n        \n        # Stream chunks\n        async for chunk in stream_manager.get_chunks(request_id):\n            if chunk:\n                yield f\"data: {json.dumps(chunk)}\\n\\n\"\n        \n        # Stream done\n        yield \"data: [DONE]\\n\\n\"\n        \n    except Exception as e:\n        logging.error(f\"Error streaming response: {str(e)}\")\n        # Update error stats\n        update_stats(model_id, \"stream_error_count\")\n        \n        # Send error as SSE\n        error_json = json.dumps({\"error\": str(e)})\n        yield f\"data: {error_json}\\n\\n\"\n        yield \"data: [DONE]\\n\\n\"\n        \nasync def ensure_worker_running():\n    \"\"\"Ensure that a worker is running to process the queue\"\"\"\n    # Check if workers are already running via a sentinel in shared dict\n    workers_running_key = \"workers_running\"\n    \n    if not model_stats_dict.contains(workers_running_key):\n        model_stats_dict[workers_running_key] = 0\n    \n    current_workers = model_stats_dict[workers_running_key]\n    \n    # If no workers or too few workers, start more\n    if current_workers < 3:  # Keep up to 3 workers running\n        # Increment worker count\n        model_stats_dict[workers_running_key] = current_workers + 1\n        \n        # Start a worker\n        await process_queue_worker.spawn.aio()\n\ndef calculate_cache_key(model_id: str, messages: List[dict], temperature: float, max_tokens: int) -> str:\n    \"\"\"Calculate a deterministic cache key for a request using SHA-256\"\"\"\n    # Create a simplified version of the request for cache key\n    cache_dict = {\n        \"model\": model_id,\n        \"messages\": messages,\n        \"temperature\": round(temperature, 2),  # Round to reduce variations\n        \"max_tokens\": max_tokens\n    }\n    # Convert to a stable string representation and hash it with SHA-256\n    cache_str = json.dumps(cache_dict, sort_keys=True)\n    hash_obj = hashlib.sha256(cache_str.encode())\n    return f\"cache:{hash_obj.hexdigest()[:16]}\"\n\ndef update_stats(model_id: str, stat_type: str):\n    \"\"\"Update usage statistics for a model\"\"\"\n    if not model_stats_dict.contains(model_id):\n        model_stats_dict[model_id] = {\n            \"request_count\": 0,\n            \"success_count\": 0,\n            \"error_count\": 0,\n            \"timeout_count\": 0,\n            \"cache_hit\": 0,\n            \"token_count\": 0,\n            \"avg_latency\": 0\n        }\n    \n    stats = model_stats_dict[model_id]\n    stats[stat_type] = stats.get(stat_type, 0) + 1\n    model_stats_dict[model_id] = stats\n    \ndef estimate_tokens(messages: List[dict], response: dict, user_id: str, model_id: str):\n    \"\"\"Estimate token usage and update user quotas\"\"\"\n    # Very simple token estimation based on whitespace-split words * 1.3\n    input_tokens = 0\n    for msg in messages:\n        input_tokens += len(msg.get(\"content\", \"\").split()) * 1.3\n    \n    output_tokens = 0\n    if response and \"choices\" in response:\n        for choice in response[\"choices\"]:\n            if \"message\" in choice and \"content\" in choice[\"message\"]:\n                output_tokens += len(choice[\"message\"][\"content\"].split()) * 1.3\n    \n    # Update model stats\n    if model_stats_dict.contains(model_id):\n        stats = model_stats_dict[model_id]\n        stats[\"token_count\"] = stats.get(\"token_count\", 0) + input_tokens + output_tokens\n        model_stats_dict[model_id] = stats\n    \n    # Update user usage\n    if user_id in user_usage_dict:\n        usage = user_usage_dict[user_id]\n        \n        # Check if we need to reset daily counters\n        last_reset = datetime.fromisoformat(usage[\"tokens\"][\"last_reset\"])\n        now = datetime.now()\n        \n        if now.date() > last_reset.date():\n            # Reset daily counters\n            usage[\"tokens\"][\"input\"] = 0\n            usage[\"tokens\"][\"output\"] = 0\n            usage[\"tokens\"][\"last_reset\"] = now.isoformat()\n        \n        # Update token counts\n        usage[\"tokens\"][\"input\"] += int(input_tokens)\n        usage[\"tokens\"][\"output\"] += int(output_tokens)\n        user_usage_dict[user_id] = usage\n\ndef select_best_model(prompt: str, n_predict: int, temperature: float) -> str:\n    \"\"\"\n    Intelligently selects the best model based on input parameters.\n\n    Args:\n        prompt (str): The input prompt for the model.\n        n_predict (int): The number of tokens to predict.\n        temperature (float): The sampling temperature.\n\n    Returns:\n        str: The identifier of the best model to use.\n    \"\"\"\n    # Check for code generation patterns\n    code_indicators = [\"```\", \"def \", \"class \", \"function\", \"import \", \"from \", \"<script\", \"<style\", \n                      \"SELECT \", \"CREATE TABLE\", \"const \", \"let \", \"var \", \"function(\", \"=>\"]\n    \n    is_likely_code = any(indicator in prompt for indicator in code_indicators)\n    \n    # Check for creative writing patterns\n    creative_indicators = [\"story\", \"poem\", \"creative\", \"imagine\", \"fiction\", \"narrative\", \n                          \"write a\", \"compose\", \"create a\"]\n    \n    is_creative_task = any(indicator in prompt.lower() for indicator in creative_indicators)\n    \n    # Check for analytical/reasoning tasks\n    analytical_indicators = [\"explain\", \"analyze\", \"compare\", \"contrast\", \"reason\", \n                            \"evaluate\", \"assess\", \"why\", \"how does\"]\n    \n    is_analytical_task = any(indicator in prompt.lower() for indicator in analytical_indicators)\n    \n    # Decision logic\n    if is_likely_code:\n        # For code generation, prefer phi-4 for all code tasks\n        return \"phi-4\"  # Excellent for code generation\n            \n    elif is_creative_task:\n        # For creative tasks, use models with higher creativity\n        if temperature > 0.8:\n            return \"deepseek-r1\"  # More creative at high temperatures\n        else:\n            return \"phi-4\"  # Good balance of creativity and coherence\n            \n    elif is_analytical_task:\n        # For analytical tasks, use models with strong reasoning\n        return \"phi-4\"  # Strong reasoning capabilities\n        \n    # Length-based decisions\n    if len(prompt) > 2000:\n        # For very long prompts, use models with good context handling\n        return \"llama3-8b\"\n    elif len(prompt) < 1000:\n        # For shorter prompts, prefer phi-4\n        return \"phi-4\"\n        \n    # Temperature-based decisions\n    if temperature < 0.5:\n        # For deterministic outputs\n        return \"phi-4\"\n    elif temperature > 0.9:\n        # For very creative outputs\n        return \"deepseek-r1\"\n        \n    # Default to phi-4 instead of the standard model\n    return \"phi-4\"\n\n# vLLM serving function\n@app.function(\n    image=vllm_image,\n    gpu=\"H100:1\",\n    allow_concurrent_inputs=100,\n    volumes={\n        f\"{CACHE_DIR}/huggingface\": hf_cache_vol,\n        f\"{CACHE_DIR}/vllm\": vllm_cache_vol,\n    },\n    timeout=30 * MINUTES,\n)\n@modal.web_server(port=SERVER_PORT)\ndef serve_vllm_model(model_id: str = DEFAULT_MODEL):\n    \"\"\"\n    Serves a model using vLLM with an OpenAI-compatible API.\n\n    Args:\n        model_id (str): The identifier of the model to serve. Defaults to DEFAULT_MODEL.\n\n    Raises:\n        ValueError: If the specified model_id is not found in VLLM_MODELS.\n    \"\"\"\n    import subprocess\n    \n    if model_id not in VLLM_MODELS:\n        available_models = list(VLLM_MODELS.keys())\n        logging.error(f\"Error: Unknown model: {model_id}. Available models: {available_models}\")\n        raise ValueError(f\"Unknown model: {model_id}. Available models: {available_models}\")\n    \n    model_info = VLLM_MODELS[model_id]\n    model_name = model_info[\"name\"]\n    revision = model_info[\"revision\"]\n    \n    logging.basicConfig(level=logging.INFO)\n    logging.info(f\"Starting vLLM server with model: {model_name}\")\n    \n    cmd = [\n        \"vllm\",\n        \"serve\",\n        \"--uvicorn-log-level=info\",\n        model_name,\n        \"--revision\",\n        revision,\n        \"--host\",\n        \"0.0.0.0\",\n        \"--port\",\n        str(SERVER_PORT),\n        \"--api-key\",\n        DEFAULT_API_KEY,\n    ]\n\n    # Use subprocess.run instead of Popen to ensure the server is fully started\n    # before returning, and don't use shell=True for better process management\n    process = subprocess.Popen(cmd)\n    \n    # Log that we've started the server\n    logging.info(f\"Started vLLM server with PID {process.pid}\")\n\n# Define the worker that will process the queue\n@app.function(\n    image=vllm_image,\n    gpu=None,  # Worker will spawn GPU functions as needed\n    allow_concurrent_inputs=10,\n    volumes={\n        f\"{CACHE_DIR}/huggingface\": hf_cache_vol,\n    },\n    timeout=30 * MINUTES,\n)\nasync def process_queue_worker():\n    \"\"\"Worker function that processes requests from the queue\"\"\"\n    import asyncio\n    import time\n    \n    try:\n        # Signal that we're starting a worker\n        worker_id = str(uuid.uuid4())[:8]\n        logging.info(f\"Starting queue processing worker {worker_id}\")\n        \n        # Process requests until timeout or empty queue\n        empty_count = 0\n        max_empty_count = 10  # Stop after 10 consecutive empty polls\n        \n        while empty_count < max_empty_count:\n            # Try to get a request from the queue\n            try:\n                request_dict = await request_queue.get.aio(timeout_ms=5000)\n                empty_count = 0  # Reset empty counter\n                \n                # Process the request\n                try:\n                    # Create request object\n                    request_id = request_dict.get(\"request_id\")\n                    model_id = request_dict.get(\"model_id\")\n                    messages = request_dict.get(\"messages\", [])\n                    temperature = request_dict.get(\"temperature\", 0.7)\n                    max_tokens = request_dict.get(\"max_tokens\", 1024)\n                    api_key = request_dict.get(\"api_key\", DEFAULT_API_KEY)\n                    stream_mode = request_dict.get(\"stream\", False)\n                    \n                    logging.info(f\"Worker {worker_id} processing request {request_id} for model {model_id}\")\n                    \n                    # Start time for latency calculation\n                    start_time = time.time()\n                    \n                    if stream_mode:\n                        # Generate streaming response\n                        await generate_streaming_response(\n                            request_id=request_id,\n                            model_id=model_id,\n                            messages=messages,\n                            temperature=temperature,\n                            max_tokens=max_tokens,\n                            api_key=api_key\n                        )\n                    else:\n                        # Generate non-streaming response\n                        response = await generate_response(\n                            model_id=model_id,\n                            messages=messages,\n                            temperature=temperature,\n                            max_tokens=max_tokens,\n                            api_key=api_key\n                        )\n                        \n                        # Calculate latency\n                        latency = time.time() - start_time\n                        \n                        # Update latency stats\n                        if model_stats_dict.contains(model_id):\n                            stats = model_stats_dict[model_id]\n                            old_avg = stats.get(\"avg_latency\", 0)\n                            old_count = stats.get(\"success_count\", 0) \n                            \n                            # Calculate new average (moving average)\n                            if old_count > 0:\n                                new_avg = (old_avg * old_count + latency) / (old_count + 1)\n                            else:\n                                new_avg = latency\n                                \n                            stats[\"avg_latency\"] = new_avg\n                            model_stats_dict[model_id] = stats\n                        \n                        # Store the response in both caches\n                        memory_cache.set(request_id, response)\n                        response_dict[request_id] = response\n                        \n                        logging.info(f\"Worker {worker_id} completed request {request_id} in {latency:.2f}s\")\n                    \n                except Exception as e:\n                    # Log error and move on\n                    logging.error(f\"Worker {worker_id} error processing request {request_id}: {str(e)}\")\n                    \n                    # Create error response\n                    error_response = {\n                        \"error\": {\n                            \"message\": str(e),\n                            \"type\": \"internal_error\",\n                            \"code\": 500\n                        }\n                    }\n                    \n                    # Store the error as a response\n                    memory_cache.set(request_id, error_response)\n                    response_dict[request_id] = error_response\n                    \n                    # If streaming, send error and finish stream\n                    if \"stream_mode\" in locals() and stream_mode:\n                        stream_manager.add_chunk(request_id, {\n                            \"id\": f\"chatcmpl-{int(time.time())}\",\n                            \"object\": \"chat.completion.chunk\",\n                            \"created\": int(time.time()),\n                            \"model\": model_id,\n                            \"choices\": [{\n                                \"index\": 0,\n                                \"delta\": {\"content\": f\"Error: {str(e)}\"},\n                                \"finish_reason\": \"error\"\n                            }]\n                        })\n                        stream_manager.finish_stream(request_id)\n            \n            except asyncio.TimeoutError:\n                # No requests in queue\n                empty_count += 1\n                logging.info(f\"Worker {worker_id}: No requests in queue. Empty count: {empty_count}\")\n                \n                # Clean up expired cache entries and old streams\n                if empty_count % 5 == 0:  # Every 5 empty polls\n                    memory_cache.clear_expired()\n                    stream_manager.clean_old_streams()\n                \n                await asyncio.sleep(1)  # Wait a bit before checking again\n        \n        # If we get here, we've had too many consecutive empty polls\n        logging.info(f\"Worker {worker_id} shutting down due to empty queue\")\n        \n    finally:\n        # Signal that this worker is done\n        workers_running_key = \"workers_running\"\n        if model_stats_dict.contains(workers_running_key):\n            current_workers = model_stats_dict[workers_running_key]\n            model_stats_dict[workers_running_key] = max(0, current_workers - 1)\n            logging.info(f\"Worker {worker_id} shutdown. Workers remaining: {max(0, current_workers - 1)}\")\n\nasync def generate_streaming_response(\n    request_id: str,\n    model_id: str,\n    messages: List[dict],\n    temperature: float,\n    max_tokens: int,\n    api_key: str\n):\n    \"\"\"\n    Generate a streaming response and send chunks to the stream manager.\n    \n    Args:\n        request_id: The unique ID for this request\n        model_id: The ID of the model to use\n        messages: The chat messages\n        temperature: The sampling temperature\n        max_tokens: The maximum tokens to generate\n        api_key: The API key for authentication\n    \"\"\"\n    import httpx\n    import time\n    import json\n    import asyncio\n    \n    try:\n        # Create response ID\n        response_id = f\"chatcmpl-{int(time.time())}\"\n        \n        if model_id in VLLM_MODELS:\n            # Start vLLM server for this model\n            server_url = await serve_vllm_model.remote(model_id=model_id)\n            \n            # Need to wait for server startup\n            await wait_for_server(serve_vllm_model.web_url, timeout=120)\n            \n            # Forward request to vLLM with streaming enabled\n            async with httpx.AsyncClient(timeout=120.0) as client:\n                headers = {\n                    \"Authorization\": f\"Bearer {api_key}\",\n                    \"Content-Type\": \"application/json\",\n                    \"Accept\": \"text/event-stream\"\n                }\n                \n                # Format request for vLLM OpenAI-compatible endpoint\n                vllm_request = {\n                    \"model\": VLLM_MODELS[model_id][\"name\"],\n                    \"messages\": messages,\n                    \"temperature\": temperature,\n                    \"max_tokens\": max_tokens,\n                    \"stream\": True\n                }\n                \n                # Make streaming request\n                async with client.stream(\n                    \"POST\",\n                    f\"{serve_vllm_model.web_url}/v1/chat/completions\",\n                    json=vllm_request,\n                    headers=headers\n                ) as response:\n                    # Process streaming response\n                    buffer = \"\"\n                    async for chunk in response.aiter_text():\n                        buffer += chunk\n                        \n                        # Process complete SSE messages\n                        while \"\\n\\n\" in buffer:\n                            message, buffer = buffer.split(\"\\n\\n\", 1)\n                            \n                            if message.startswith(\"data: \"):\n                                data = message[6:]  # Remove \"data: \" prefix\n                                \n                                if data == \"[DONE]\":\n                                    # End of stream\n                                    stream_manager.finish_stream(request_id)\n                                    return\n                                \n                                try:\n                                    # Parse JSON data\n                                    chunk_data = json.loads(data)\n                                    # Forward to client\n                                    stream_manager.add_chunk(request_id, chunk_data)\n                                except json.JSONDecodeError:\n                                    logging.error(f\"Invalid JSON in stream: {data}\")\n                    \n                    # Ensure stream is finished\n                    stream_manager.finish_stream(request_id)\n                    \n        elif model_id in LLAMA_CPP_MODELS:\n            # For llama.cpp models, we need to simulate streaming\n            # First convert the chat format to a prompt\n            prompt = format_messages_to_prompt(messages)\n            \n            # Run llama.cpp with the prompt\n            output = await run_llama_cpp_stream.remote(\n                model_id=model_id,\n                prompt=prompt,\n                n_predict=max_tokens,\n                temperature=temperature,\n                request_id=request_id\n            )\n            \n            # Streaming is handled by the run_llama_cpp_stream function\n            # which directly adds chunks to the stream manager\n            \n            # Wait for completion signal\n            while True:\n                if request_id in stream_queues and stream_queues[request_id] == \"DONE\":\n                    # Clean up\n                    stream_queues.pop(request_id)\n                    break\n                await asyncio.sleep(0.1)\n                \n        else:\n            raise ValueError(f\"Unknown model: {model_id}\")\n            \n    except Exception as e:\n        logging.error(f\"Error in streaming generation: {str(e)}\")\n        # Send error chunk\n        stream_manager.add_chunk(request_id, {\n            \"id\": response_id,\n            \"object\": \"chat.completion.chunk\",\n            \"created\": int(time.time()),\n            \"model\": model_id,\n            \"choices\": [{\n                \"index\": 0,\n                \"delta\": {\"content\": f\"Error: {str(e)}\"},\n                \"finish_reason\": \"error\"\n            }]\n        })\n        # Finish stream\n        stream_manager.finish_stream(request_id)\n\nasync def generate_response(model_id: str, messages: List[dict], temperature: float, max_tokens: int, api_key: str):\n    \"\"\"\n    Generate a response using the appropriate model based on model_id.\n    \n    Args:\n        model_id: The ID of the model to use\n        messages: The chat messages\n        temperature: The sampling temperature\n        max_tokens: The maximum tokens to generate\n        api_key: The API key for authentication\n        \n    Returns:\n        A response in OpenAI-compatible format\n    \"\"\"\n    import httpx\n    import time\n    import json\n    import asyncio\n    \n    if model_id in VLLM_MODELS:\n        # Start vLLM server for this model\n        server_url = await serve_vllm_model.remote(model_id=model_id)\n        \n        # Need to wait for server startup\n        await wait_for_server(serve_vllm_model.web_url, timeout=120)\n        \n        # Forward request to vLLM\n        async with httpx.AsyncClient(timeout=60.0) as client:\n            headers = {\n                \"Authorization\": f\"Bearer {api_key}\",\n                \"Content-Type\": \"application/json\"\n            }\n            \n            # Format request for vLLM OpenAI-compatible endpoint\n            vllm_request = {\n                \"model\": VLLM_MODELS[model_id][\"name\"],\n                \"messages\": messages,\n                \"temperature\": temperature,\n                \"max_tokens\": max_tokens\n            }\n            \n            response = await client.post(\n                f\"{serve_vllm_model.web_url}/v1/chat/completions\",\n                json=vllm_request,\n                headers=headers\n            )\n            \n            return response.json()\n    elif model_id in LLAMA_CPP_MODELS:\n        # For llama.cpp models, use the run_llama_cpp function\n        # First convert the chat format to a prompt\n        prompt = format_messages_to_prompt(messages)\n        \n        # Run llama.cpp with the prompt\n        output = await run_llama_cpp.remote(\n            model_id=model_id,\n            prompt=prompt,\n            n_predict=max_tokens,\n            temperature=temperature\n        )\n        \n        # Format the response in the OpenAI format\n        completion_text = output.strip()\n        finish_reason = \"stop\" if len(completion_text) < max_tokens else \"length\"\n        \n        return {\n            \"id\": f\"chatcmpl-{int(time.time())}\",\n            \"object\": \"chat.completion\",\n            \"created\": int(time.time()),\n            \"model\": model_id,\n            \"choices\": [\n                {\n                    \"index\": 0,\n                    \"message\": {\n                        \"role\": \"assistant\",\n                        \"content\": completion_text\n                    },\n                    \"finish_reason\": finish_reason\n                }\n            ],\n            \"usage\": {\n                \"prompt_tokens\": len(prompt) // 4,  # Rough estimation\n                \"completion_tokens\": len(completion_text) // 4,  # Rough estimation\n                \"total_tokens\": (len(prompt) + len(completion_text)) // 4  # Rough estimation\n            }\n        }\n    else:\n        raise ValueError(f\"Unknown model: {model_id}\")\n\ndef format_messages_to_prompt(messages: List[Dict[str, str]]) -> str:\n    \"\"\"\n    Convert chat messages to a text prompt format for llama.cpp.\n    \n    Args:\n        messages: List of message dictionaries with role and content\n    \n    Returns:\n        Formatted prompt string\n    \"\"\"\n    formatted_prompt = \"\"\n    \n    for message in messages:\n        role = message.get(\"role\", \"\").lower()\n        content = message.get(\"content\", \"\")\n        \n        if role == \"system\":\n            formatted_prompt += f\"<|system|>\\n{content}\\n\"\n        elif role == \"user\":\n            formatted_prompt += f\"<|user|>\\n{content}\\n\"\n        elif role == \"assistant\":\n            formatted_prompt += f\"<|assistant|>\\n{content}\\n\"\n        else:\n            # For unknown roles, treat as user\n            formatted_prompt += f\"<|user|>\\n{content}\\n\"\n    \n    # Add final assistant marker to prompt the model to respond\n    formatted_prompt += \"<|assistant|>\\n\"\n    \n    return formatted_prompt\n\nasync def wait_for_server(url: str, timeout: int = 120, check_interval: int = 2):\n    \"\"\"\n    Wait for a server to be ready by checking its health endpoint.\n    \n    Args:\n        url: The base URL of the server\n        timeout: Maximum time to wait in seconds\n        check_interval: Interval between checks in seconds\n    \n    Returns:\n        True if server is ready, False otherwise\n    \"\"\"\n    import httpx\n    import asyncio\n    import time\n    \n    start_time = time.time()\n    health_url = f\"{url}/health\"\n    \n    logging.info(f\"Waiting for server at {url} to be ready...\")\n    \n    while time.time() - start_time < timeout:\n        try:\n            async with httpx.AsyncClient(timeout=5.0) as client:\n                response = await client.get(health_url)\n                if response.status_code == 200:\n                    logging.info(f\"Server at {url} is ready!\")\n                    return True\n        except Exception as e:\n            elapsed = time.time() - start_time\n            logging.info(f\"Server not ready yet after {elapsed:.1f}s: {str(e)}\")\n            \n        await asyncio.sleep(check_interval)\n    \n    logging.error(f\"Timed out waiting for server at {url} after {timeout} seconds\")\n    return False\n\n@app.function(\n    image=llama_cpp_image,\n    gpu=None,  # Will be set dynamically based on model\n    volumes={\n        f\"{CACHE_DIR}/huggingface\": hf_cache_vol,\n        f\"{CACHE_DIR}/llama_cpp\": llama_cpp_cache_vol,\n        RESULTS_DIR: results_vol,\n    },\n    timeout=30 * MINUTES,\n)\nasync def run_llama_cpp_stream(\n    model_id: str,\n    prompt: str,\n    n_predict: int = 1024,\n    temperature: float = 0.7,\n    request_id: str = None,\n):\n    \"\"\"\n    Run streaming inference with llama.cpp for models like DeepSeek-R1 and Phi-4\n    \"\"\"\n    import subprocess\n    import os\n    import json\n    import time\n    import threading\n    from uuid import uuid4\n    from pathlib import Path\n    from huggingface_hub import snapshot_download\n    \n    if model_id not in LLAMA_CPP_MODELS:\n        available_models = list(LLAMA_CPP_MODELS.keys())\n        error_msg = f\"Unknown model: {model_id}. Available models: {available_models}\"\n        logging.error(error_msg)\n        \n        if request_id:\n            # Send error to stream\n            stream_manager.add_chunk(request_id, {\n                \"id\": f\"chatcmpl-{int(time.time())}\",\n                \"object\": \"chat.completion.chunk\",\n                \"created\": int(time.time()),\n                \"model\": model_id,\n                \"choices\": [{\n                    \"index\": 0,\n                    \"delta\": {\"content\": f\"Error: {error_msg}\"},\n                    \"finish_reason\": \"error\"\n                }]\n            })\n            stream_manager.finish_stream(request_id)\n            # Signal completion\n            stream_queues[request_id] = \"DONE\"\n            \n        raise ValueError(error_msg)\n    \n    model_info = LLAMA_CPP_MODELS[model_id]\n    repo_id = model_info[\"name\"]\n    pattern = model_info[\"pattern\"]\n    revision = model_info[\"revision\"]\n    quant = model_info[\"quant\"]\n    \n    # Download model if not already cached\n    logging.info(f\"Downloading model {repo_id} if not present\")\n    try:\n        model_path = snapshot_download(\n            repo_id=repo_id,\n            revision=revision,\n            local_dir=f\"{CACHE_DIR}/llama_cpp\",\n            allow_patterns=[pattern],\n        )\n    except ValueError as e:\n        if \"hf_transfer\" in str(e):\n            # Fallback to standard download if hf_transfer fails\n            logging.warning(\"hf_transfer failed, falling back to standard download\")\n            # Temporarily disable hf_transfer\n            import os\n            old_env = os.environ.get(\"HF_HUB_ENABLE_HF_TRANSFER\", \"1\")\n            os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n            try:\n                model_path = snapshot_download(\n                    repo_id=repo_id,\n                    revision=revision,\n                    local_dir=f\"{CACHE_DIR}/llama_cpp\",\n                    allow_patterns=[pattern],\n                )\n            finally:\n                # Restore original setting\n                os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = old_env\n        else:\n            raise\n    \n    # Find the model file\n    model_files = list(Path(model_path).glob(pattern))\n    if not model_files:\n        error_msg = f\"No model files found matching pattern {pattern}\"\n        logging.error(error_msg)\n        \n        if request_id:\n            # Send error to stream\n            stream_manager.add_chunk(request_id, {\n                \"id\": f\"chatcmpl-{int(time.time())}\",\n                \"object\": \"chat.completion.chunk\",\n                \"created\": int(time.time()),\n                \"model\": model_id,\n                \"choices\": [{\n                    \"index\": 0,\n                    \"delta\": {\"content\": f\"Error: {error_msg}\"},\n                    \"finish_reason\": \"error\"\n                }]\n            })\n            stream_manager.finish_stream(request_id)\n            # Signal completion\n            stream_queues[request_id] = \"DONE\"\n            \n        raise FileNotFoundError(error_msg)\n    \n    model_file = str(model_files[0])\n    logging.info(f\"Using model file: {model_file}\")\n    \n    # Set up command\n    cmd = [\n        \"llama-cli\",\n        \"--model\", model_file,\n        \"--prompt\", prompt,\n        \"--n-predict\", str(n_predict),\n        \"--temp\", str(temperature),\n        \"--ctx-size\", \"8192\",\n    ]\n    \n    # Add GPU layers if needed\n    if model_info[\"gpu\"] is not None:\n        cmd.extend([\"--n-gpu-layers\", \"9999\"])  # Use all layers on GPU\n    \n    # Run inference\n    result_id = str(uuid4())\n    logging.info(f\"Running streaming inference with ID: {result_id}\")\n    \n    # Create response ID for streaming\n    response_id = f\"chatcmpl-{int(time.time())}\"\n    \n    # Function to process output in real-time and send to stream\n    def process_output(process, request_id):\n        content_buffer = \"\"\n        last_send_time = time.time()\n        \n        # Send initial chunk with role\n        if request_id:\n            stream_manager.add_chunk(request_id, {\n                \"id\": response_id,\n                \"object\": \"chat.completion.chunk\",\n                \"created\": int(time.time()),\n                \"model\": model_id,\n                \"choices\": [{\n                    \"index\": 0,\n                    \"delta\": {\"role\": \"assistant\"},\n                }]\n            })\n        \n        for line in iter(process.stdout.readline, b''):\n            try:\n                line_str = line.decode('utf-8', errors='replace')\n                \n                # Skip llama.cpp info lines\n                if line_str.startswith(\"llama_\"):\n                    continue\n                \n                # Add to buffer\n                content_buffer += line_str\n                \n                # Send chunks at reasonable intervals or when buffer gets large\n                now = time.time()\n                if (now - last_send_time > 0.1 or len(content_buffer) > 20) and request_id:\n                    # Send chunk\n                    stream_manager.add_chunk(request_id, {\n                        \"id\": response_id,\n                        \"object\": \"chat.completion.chunk\",\n                        \"created\": int(time.time()),\n                        \"model\": model_id,\n                        \"choices\": [{\n                            \"index\": 0,\n                            \"delta\": {\"content\": content_buffer},\n                        }]\n                    })\n                    \n                    # Reset buffer and time\n                    content_buffer = \"\"\n                    last_send_time = now\n                    \n            except Exception as e:\n                logging.error(f\"Error processing output: {str(e)}\")\n        \n        # Send any remaining content\n        if content_buffer and request_id:\n            stream_manager.add_chunk(request_id, {\n                \"id\": response_id,\n                \"object\": \"chat.completion.chunk\",\n                \"created\": int(time.time()),\n                \"model\": model_id,\n                \"choices\": [{\n                    \"index\": 0,\n                    \"delta\": {\"content\": content_buffer},\n                }]\n            })\n        \n        # Send final chunk with finish reason\n        if request_id:\n            stream_manager.add_chunk(request_id, {\n                \"id\": response_id,\n                \"object\": \"chat.completion.chunk\",\n                \"created\": int(time.time()),\n                \"model\": model_id,\n                \"choices\": [{\n                    \"index\": 0,\n                    \"delta\": {},\n                    \"finish_reason\": \"stop\"\n                }]\n            })\n            \n            # Finish stream\n            stream_manager.finish_stream(request_id)\n            \n            # Signal completion\n            stream_queues[request_id] = \"DONE\"\n    \n    # Start process\n    process = subprocess.Popen(\n        cmd, \n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=False,\n        bufsize=1  # Line buffered\n    )\n    \n    # Start output processing thread if streaming\n    if request_id:\n        thread = threading.Thread(target=process_output, args=(process, request_id))\n        thread.daemon = True\n        thread.start()\n        \n        # Return immediately for streaming\n        return \"Streaming in progress\"\n    else:\n        # For non-streaming, collect all output\n        stdout, stderr = collect_output(process)\n        \n        # Save results\n        result_dir = Path(RESULTS_DIR) / result_id\n        result_dir.mkdir(parents=True, exist_ok=True)\n        \n        (result_dir / \"output.txt\").write_text(stdout)\n        (result_dir / \"stderr.txt\").write_text(stderr)\n        (result_dir / \"prompt.txt\").write_text(prompt)\n        \n        logging.info(f\"Results saved to {result_dir}\")\n        return stdout\n\n@app.function(\n    image=llama_cpp_image,\n    gpu=None,  # Will be set dynamically based on model\n    volumes={\n        f\"{CACHE_DIR}/huggingface\": hf_cache_vol,\n        f\"{CACHE_DIR}/llama_cpp\": llama_cpp_cache_vol,\n        RESULTS_DIR: results_vol,\n    },\n    timeout=30 * MINUTES,\n)\nasync def run_llama_cpp(\n    model_id: str,\n    prompt: str = \"Tell me about Modal and how it helps with ML deployments.\",\n    n_predict: int = 1024,\n    temperature: float = 0.7,\n):\n    \"\"\"\n    Run inference with llama.cpp for models like DeepSeek-R1 and Phi-4\n    \"\"\"\n    import subprocess\n    import os\n    from uuid import uuid4\n    from pathlib import Path\n    from huggingface_hub import snapshot_download\n    \n    if model_id not in LLAMA_CPP_MODELS:\n        available_models = list(LLAMA_CPP_MODELS.keys())\n        print(f\"Error: Unknown model: {model_id}. Available models: {available_models}\")\n        raise ValueError(f\"Unknown model: {model_id}. Available models: {available_models}\")\n    \n    model_info = LLAMA_CPP_MODELS[model_id]\n    repo_id = model_info[\"name\"]\n    pattern = model_info[\"pattern\"]\n    revision = model_info[\"revision\"]\n    quant = model_info[\"quant\"]\n    \n    # Download model if not already cached\n    logging.info(f\"Downloading model {repo_id} if not present\")\n    try:\n        model_path = snapshot_download(\n            repo_id=repo_id,\n            revision=revision,\n            local_dir=f\"{CACHE_DIR}/llama_cpp\",\n            allow_patterns=[pattern],\n        )\n    except ValueError as e:\n        if \"hf_transfer\" in str(e):\n            # Fallback to standard download if hf_transfer fails\n            logging.warning(\"hf_transfer failed, falling back to standard download\")\n            # Temporarily disable hf_transfer\n            import os\n            old_env = os.environ.get(\"HF_HUB_ENABLE_HF_TRANSFER\", \"1\")\n            os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n            try:\n                model_path = snapshot_download(\n                    repo_id=repo_id,\n                    revision=revision,\n                    local_dir=f\"{CACHE_DIR}/llama_cpp\",\n                    allow_patterns=[pattern],\n                )\n            finally:\n                # Restore original setting\n                os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = old_env\n        else:\n            raise\n    \n    # Find the model file\n    model_files = list(Path(model_path).glob(pattern))\n    if not model_files:\n        logging.error(f\"No model files found matching pattern {pattern}\")\n        raise FileNotFoundError(f\"No model files found matching pattern {pattern}\")\n    \n    model_file = str(model_files[0])\n    print(f\"Using model file: {model_file}\")\n    \n    # Set up command\n    cmd = [\n        \"llama-cli\",\n        \"--model\", model_file,\n        \"--prompt\", prompt,\n        \"--n-predict\", str(n_predict),\n        \"--temp\", str(temperature),\n        \"--ctx-size\", \"8192\",\n    ]\n    \n    # Add GPU layers if needed\n    if model_info[\"gpu\"] is not None:\n        cmd.extend([\"--n-gpu-layers\", \"9999\"])  # Use all layers on GPU\n    \n    # Run inference\n    result_id = str(uuid4())\n    print(f\"Running inference with ID: {result_id}\")\n    \n    process = subprocess.Popen(\n        cmd, \n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=False\n    )\n    \n    stdout, stderr = collect_output(process)\n    \n    # Save results\n    result_dir = Path(RESULTS_DIR) / result_id\n    result_dir.mkdir(parents=True, exist_ok=True)\n    \n    (result_dir / \"output.txt\").write_text(stdout)\n    (result_dir / \"stderr.txt\").write_text(stderr)\n    (result_dir / \"prompt.txt\").write_text(prompt)\n    \n    print(f\"Results saved to {result_dir}\")\n    return stdout\n\n@app.function(\n    image=vllm_image,\n    volumes={\n        f\"{CACHE_DIR}/huggingface\": hf_cache_vol,\n    },\n)\ndef list_available_models():\n    \"\"\"\n    Lists available models that can be used with this server.\n\n    Returns:\n        dict: A dictionary containing lists of available vLLM and llama.cpp models.\n    \"\"\"\n    print(\"Available vLLM models:\")\n    for model_id, model_info in VLLM_MODELS.items():\n        print(f\"- {model_id}: {model_info['name']}\")\n    \n    print(\"\\nAvailable llama.cpp models:\")\n    for model_id, model_info in LLAMA_CPP_MODELS.items():\n        gpu_info = f\"(GPU: {model_info['gpu']})\" if model_info['gpu'] else \"(CPU)\"\n        print(f\"- {model_id}: {model_info['name']} {gpu_info}\")\n    \n    return {\n        \"vllm\": list(VLLM_MODELS.keys()),\n        \"llama_cpp\": list(LLAMA_CPP_MODELS.keys())\n    }\n\ndef collect_output(process):\n    \"\"\"\n    Collect output from a process while streaming it.\n\n    Args:\n        process: The process from which to collect output.\n\n    Returns:\n        tuple: A tuple containing the collected stdout and stderr as strings.\n    \"\"\"\n    import sys\n    from queue import Queue\n    from threading import Thread\n    \n    def stream_output(stream, queue, write_stream):\n        for line in iter(stream.readline, b\"\"):\n            line_str = line.decode(\"utf-8\", errors=\"replace\")\n            write_stream.write(line_str)\n            write_stream.flush()\n            queue.put(line_str)\n        stream.close()\n    \n    stdout_queue = Queue()\n    stderr_queue = Queue()\n    \n    stdout_thread = Thread(target=stream_output, args=(process.stdout, stdout_queue, sys.stdout))\n    stderr_thread = Thread(target=stream_output, args=(process.stderr, stderr_queue, sys.stderr))\n    \n    stdout_thread.start()\n    stderr_thread.start()\n    \n    stdout_thread.join()\n    stderr_thread.join()\n    process.wait()\n    \n    stdout_collected = \"\".join(list(stdout_queue.queue))\n    stderr_collected = \"\".join(list(stderr_queue.queue))\n    \n    return stdout_collected, stderr_collected\n\n# Main ASGI app for Modal\n@app.function(\n    image=vllm_image,\n    gpu=None,  # No GPU for the API frontend\n    allow_concurrent_inputs=100,\n    volumes={\n        f\"{CACHE_DIR}/huggingface\": hf_cache_vol,\n    },\n)\n@modal.asgi_app()\ndef inference_api():\n    \"\"\"The main ASGI app that serves the FastAPI application\"\"\"\n    return api_app\n\n@app.local_entrypoint()\ndef main(\n    prompt: str = \"What can you tell me about Modal?\",\n    n_predict: int = 1024,\n    temperature: float = 0.7,\n    create_admin_key: bool = False,\n    stream: bool = False,\n    model: str = \"auto\",\n    load_model: str = None,\n    load_hf_model: str = None,\n    hf_model_type: str = \"vllm\",\n):\n    \"\"\"\n    Main entrypoint for testing the API\n    \"\"\"\n    import json\n    import time\n    import urllib.request\n    \n    # Initialize the API\n    print(f\"Starting API at {inference_api.web_url}\")\n    \n    # Wait for API to be ready\n    print(\"Checking if API is ready...\")\n    up, start, delay = False, time.time(), 10\n    while not up:\n        try:\n            with urllib.request.urlopen(inference_api.web_url + \"/health\") as response:\n                if response.getcode() == 200:\n                    up = True\n        except Exception:\n            if time.time() - start > 5 * MINUTES:\n                break\n            time.sleep(delay)\n\n    assert up, f\"Failed health check for API at {inference_api.web_url}\"\n    print(f\"API is up and running at {inference_api.web_url}\")\n    \n    # Create a test API key if requested\n    if create_admin_key:\n        print(\"Creating a test API key...\")\n        key_request = {\n            \"user_id\": \"test_user\",\n            \"rate_limit\": 120,\n            \"quota\": 2000000\n        }\n        headers = {\n            \"Authorization\": f\"Bearer {DEFAULT_API_KEY}\",  # Admin key\n            \"Content-Type\": \"application/json\",\n        }\n        req = urllib.request.Request(\n            inference_api.web_url + \"/admin/api-keys\",\n            data=json.dumps(key_request).encode(\"utf-8\"),\n            headers=headers,\n            method=\"POST\",\n        )\n        try:\n            with urllib.request.urlopen(req) as response:\n                result = json.loads(response.read().decode())\n                print(\"Created API key:\")\n                print(json.dumps(result, indent=2))\n                # Use this key for the test message\n                test_key = result[\"key\"]\n        except Exception as e:\n            print(f\"Error creating API key: {str(e)}\")\n            test_key = DEFAULT_API_KEY\n    else:\n        test_key = DEFAULT_API_KEY\n            \n    # List available models\n    print(\"\\nAvailable models:\")\n    try:\n        headers = {\n            \"Authorization\": f\"Bearer {test_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n        req = urllib.request.Request(\n            inference_api.web_url + \"/v1/models\",\n            headers=headers,\n            method=\"GET\",\n        )\n        with urllib.request.urlopen(req) as response:\n            models = json.loads(response.read().decode())\n            print(json.dumps(models, indent=2))\n    except Exception as e:\n        print(f\"Error listing models: {str(e)}\")\n        \n    # Select best model for the prompt\n    model = select_best_model(prompt, n_predict, temperature)\n    \n    # Send a test message\n    print(f\"\\nSending a sample message to {inference_api.web_url}\")\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n\n    headers = {\n        \"Authorization\": f\"Bearer {test_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n    payload = json.dumps({\n        \"messages\": messages, \n        \"model\": model,\n        \"temperature\": temperature,\n        \"max_tokens\": n_predict,\n        \"stream\": stream\n    })\n    req = urllib.request.Request(\n        inference_api.web_url + \"/v1/chat/completions\",\n        data=payload.encode(\"utf-8\"),\n        headers=headers,\n        method=\"POST\",\n    )\n    \n    try:\n        if stream:\n            print(\"Streaming response:\")\n            with urllib.request.urlopen(req) as response:\n                for line in response:\n                    line = line.decode('utf-8')\n                    if line.startswith('data: '):\n                        data = line[6:].strip()\n                        if data == '[DONE]':\n                            print(\"\\n[DONE]\")\n                        else:\n                            try:\n                                chunk = json.loads(data)\n                                if 'choices' in chunk and len(chunk['choices']) > 0:\n                                    if 'delta' in chunk['choices'][0] and 'content' in chunk['choices'][0]['delta']:\n                                        content = chunk['choices'][0]['delta']['content']\n                                        print(content, end='', flush=True)\n                            except json.JSONDecodeError:\n                                print(f\"Error parsing: {data}\")\n        else:\n            with urllib.request.urlopen(req) as response:\n                result = json.loads(response.read().decode())\n                print(\"Response:\")\n                print(json.dumps(result, indent=2))\n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n    \n    # Check API stats\n    print(\"\\nChecking API stats...\")\n    headers = {\n        \"Authorization\": f\"Bearer {DEFAULT_API_KEY}\",  # Admin key\n        \"Content-Type\": \"application/json\",\n    }\n    req = urllib.request.Request(\n        inference_api.web_url + \"/admin/stats\",\n        headers=headers,\n        method=\"GET\",\n    )\n    try:\n        with urllib.request.urlopen(req) as response:\n            stats = json.loads(response.read().decode())\n            print(\"API Stats:\")\n            print(json.dumps(stats, indent=2))\n    except Exception as e:\n        print(f\"Error getting stats: {str(e)}\")\n        \n    # Start a worker if none running\n    try:\n        current_workers = stats.get(\"queue\", {}).get(\"active_workers\", 0)\n        if current_workers < 1:\n            print(\"\\nStarting a queue worker...\")\n            process_queue_worker.spawn()\n    except Exception as e:\n        print(f\"Error starting worker: {str(e)}\")\n        \n    print(f\"\\nAPI is available at {inference_api.web_url}\")\n    print(f\"Documentation is at {inference_api.web_url}/docs\")\n    print(f\"Default Bearer token: {DEFAULT_API_KEY}\")\n    \n    if create_admin_key:\n        print(f\"Test Bearer token: {test_key}\")\n        \n    # If a model was specified to load, load it\n    if load_model:\n        print(f\"\\nLoading model: {load_model}\")\n        load_url = f\"{inference_api.web_url}/admin/models/load\"\n        headers = {\n            \"Authorization\": f\"Bearer {test_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n        payload = json.dumps({\n            \"model_id\": load_model,\n            \"force_reload\": True\n        })\n        req = urllib.request.Request(\n            load_url,\n            data=payload.encode(\"utf-8\"),\n            headers=headers,\n            method=\"POST\",\n        )\n        try:\n            with urllib.request.urlopen(req) as response:\n                result = json.loads(response.read().decode())\n                print(\"Load response:\")\n                print(json.dumps(result, indent=2))\n                \n                # If it's a small model, wait a bit for it to load\n                if load_model in [\"tiny-llama-1.1b\", \"phi-2\"]:\n                    print(f\"Waiting for {load_model} to load...\")\n                    time.sleep(10)\n                    \n                    # Check status\n                    status_url = f\"{inference_api.web_url}/admin/models/status/{load_model}\"\n                    status_req = urllib.request.Request(\n                        status_url,\n                        headers={\"Authorization\": f\"Bearer {test_key}\"},\n                        method=\"GET\",\n                    )\n                    with urllib.request.urlopen(status_req) as status_response:\n                        status_result = json.loads(status_response.read().decode())\n                        print(\"Model status:\")\n                        print(json.dumps(status_result, indent=2))\n                \n                # Use this model for the test\n                model = load_model\n        except Exception as e:\n            print(f\"Error loading model: {str(e)}\")\n            \n    # If a HF model was specified to load directly\n    if load_hf_model:\n        print(f\"\\nLoading HF model: {load_hf_model} with type {hf_model_type}\")\n        load_url = f\"{inference_api.web_url}/admin/models/load-from-hf\"\n        headers = {\n            \"Authorization\": f\"Bearer {test_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n        payload = json.dumps({\n            \"repo_id\": load_hf_model,\n            \"model_type\": hf_model_type,\n            \"max_tokens\": n_predict\n        })\n        req = urllib.request.Request(\n            load_url,\n            data=payload.encode(\"utf-8\"),\n            headers=headers,\n            method=\"POST\",\n        )\n        try:\n            with urllib.request.urlopen(req) as response:\n                result = json.loads(response.read().decode())\n                print(\"HF Load response:\")\n                print(json.dumps(result, indent=2))\n                \n                # Get the model_id from the response\n                hf_model_id = result.get(\"model_id\")\n                \n                # Wait a bit for it to start loading\n                print(f\"Waiting for {load_hf_model} to start loading...\")\n                time.sleep(5)\n                \n                # Check status\n                if hf_model_id:\n                    status_url = f\"{inference_api.web_url}/admin/models/status/{hf_model_id}\"\n                    status_req = urllib.request.Request(\n                        status_url,\n                        headers={\"Authorization\": f\"Bearer {test_key}\"},\n                        method=\"GET\",\n                    )\n                    with urllib.request.urlopen(status_req) as status_response:\n                        status_result = json.loads(status_response.read().decode())\n                        print(\"Model status:\")\n                        print(json.dumps(status_result, indent=2))\n                \n                # Use this model for the test\n                if hf_model_id:\n                    model = hf_model_id\n        except Exception as e:\n            print(f\"Error loading HF model: {str(e)}\")\n\n    # Show curl examples\n    print(\"\\nExample curl commands:\")\n    \n    # Regular completion\n    print(f\"\"\"# Regular completion:\ncurl -X POST {inference_api.web_url}/v1/chat/completions \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -H \"Authorization: Bearer {test_key}\" \\\\\n  -d '{{\n    \"model\": \"{model}\",\n    \"messages\": [\n      {{\n        \"role\": \"user\",\n        \"content\": \"Hello, how can you help me today?\"\n      }}\n    ],\n    \"temperature\": 0.7,\n    \"max_tokens\": 500\n  }}'\"\"\")\n    \n    # Streaming completion\n    print(f\"\"\"\\n# Streaming completion:\ncurl -X POST {inference_api.web_url}/v1/chat/completions \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -H \"Authorization: Bearer {test_key}\" \\\\\n  -d '{{\n    \"model\": \"{model}\",\n    \"messages\": [\n      {{\n        \"role\": \"user\",\n        \"content\": \"Write a short story about AI\"\n      }}\n    ],\n    \"temperature\": 0.8,\n    \"max_tokens\": 1000,\n    \"stream\": true\n  }}' --no-buffer\"\"\")\n    \n    # List models\n    print(f\"\"\"\\n# List available models:\ncurl -X GET {inference_api.web_url}/v1/models \\\\\n  -H \"Authorization: Bearer {test_key}\" \"\"\")\n    \n    # Model management commands\n    print(f\"\"\"\\n# Load a model:\ncurl -X POST {inference_api.web_url}/admin/models/load \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -H \"Authorization: Bearer {test_key}\" \\\\\n  -d '{{\n    \"model_id\": \"phi-2\",\n    \"force_reload\": false\n  }}'\"\"\")\n    \n    print(f\"\"\"\\n# Load a model directly from Hugging Face:\ncurl -X POST {inference_api.web_url}/admin/models/load-from-hf \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -H \"Authorization: Bearer {test_key}\" \\\\\n  -d '{{\n    \"repo_id\": \"microsoft/phi-2\",\n    \"model_type\": \"vllm\",\n    \"max_tokens\": 4096\n  }}'\"\"\")\n    \n    print(f\"\"\"\\n# Get model status:\ncurl -X GET {inference_api.web_url}/admin/models/status/phi-2 \\\\\n  -H \"Authorization: Bearer {test_key}\" \"\"\")\n    \n    print(f\"\"\"\\n# Unload a model:\ncurl -X POST {inference_api.web_url}/admin/models/unload \\\\\n  -H \"Content-Type: application/json\" \\\\\n  -H \"Authorization: Bearer {test_key}\" \\\\\n  -d '{{\n    \"model_id\": \"phi-2\"\n  }}'\"\"\")\nasync def preload_llama_cpp_model(model_id: str):\n    \"\"\"Preload a llama.cpp model to make inference faster on first request\"\"\"\n    if model_id not in LLAMA_CPP_MODELS:\n        logging.error(f\"Unknown model: {model_id}\")\n        return\n    \n    try:\n        # Run a simple inference to load the model\n        logging.info(f\"Preloading llama.cpp model: {model_id}\")\n        await run_llama_cpp.remote(\n            model_id=model_id,\n            prompt=\"Hello, this is a test to preload the model.\",\n            n_predict=10,\n            temperature=0.7\n        )\n        logging.info(f\"Successfully preloaded llama.cpp model: {model_id}\")\n    except Exception as e:\n        logging.error(f\"Error preloading llama.cpp model {model_id}: {str(e)}\")\n        # Mark as not loaded\n        LLAMA_CPP_MODELS[model_id][\"loaded\"] = False\n"}
{"type": "source_file", "path": "claude_code/lib/rl/__init__.py", "content": "\"\"\"\nReinforcement Learning module for Claude Code.\nThis package contains implementations of MCTS and GRPO for decision making.\n\"\"\"\n\nfrom .mcts import AdvancedMCTS, MCTSToolSelector\nfrom .grpo import GRPO, MultiAgentGroupRL, ToolSelectionGRPO\n\n__all__ = [\n    \"AdvancedMCTS\",\n    \"MCTSToolSelector\",\n    \"GRPO\",\n    \"MultiAgentGroupRL\",\n    \"ToolSelectionGRPO\",\n]"}
{"type": "source_file", "path": "cli.py", "content": "#!/usr/bin/env python3\n# TODO: Refactor into modular structure similar to Claude Code (lib/, commands/, tools/ directories)\n# TODO: Add support for multiple LLM providers (Azure OpenAI, Anthropic, etc.)\n# TODO: Implement telemetry and usage tracking (optional, with consent)\nimport os\nimport sys\nimport json\nimport typer\nfrom rich.console import Console\nfrom rich.markdown import Markdown\nfrom rich.prompt import Prompt\nfrom rich.panel import Panel\nfrom rich.progress import Progress\nfrom rich.syntax import Syntax\nfrom rich.live import Live\nfrom rich.layout import Layout\nfrom rich.table import Table\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List, Dict, Any, Optional, Union, Callable\nimport asyncio\nimport concurrent.futures\nfrom dotenv import load_dotenv\nimport time\nimport re\nimport traceback\nimport requests\nimport urllib.parse\nfrom uuid import uuid4\nimport socket\nimport threading\nimport multiprocessing\nimport pickle\nimport hashlib\nimport logging\nimport fastapi\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException, Depends, Request, BackgroundTasks\nfrom fastapi.responses import JSONResponse, StreamingResponse\nfrom fastapi.middleware.cors import CORSMiddleware\n\n# Jina.ai client for search, fact-checking, and web reading\nclass JinaClient:\n    \"\"\"Client for interacting with Jina.ai endpoints\"\"\"\n    \n    def __init__(self, token: Optional[str] = None):\n        \"\"\"Initialize with your Jina token\"\"\"\n        self.token = token or os.getenv(\"JINA_API_KEY\", \"\")\n        \n        self.headers = {\n            \"Authorization\": f\"Bearer {self.token}\",\n            \"Content-Type\": \"application/json\"\n        }\n    \n    def search(self, query: str) -> dict:\n        \"\"\"\n        Search using s.jina.ai endpoint\n        Args:\n            query: Search term\n        Returns:\n            API response as dict\n        \"\"\"\n        encoded_query = urllib.parse.quote(query)\n        url = f\"https://s.jina.ai/{encoded_query}\"\n        response = requests.get(url, headers=self.headers)\n        return response.json()\n    \n    def fact_check(self, query: str) -> dict:\n        \"\"\"\n        Get grounding info using g.jina.ai endpoint\n        Args:\n            query: Query to ground\n        Returns:\n            API response as dict\n        \"\"\"\n        encoded_query = urllib.parse.quote(query)\n        url = f\"https://g.jina.ai/{encoded_query}\"\n        response = requests.get(url, headers=self.headers)\n        return response.json()\n        \n    def reader(self, url: str) -> dict:\n        \"\"\"\n        Get ranking using r.jina.ai endpoint\n        Args:\n            url: URL to rank\n        Returns:\n            API response as dict\n        \"\"\"\n        encoded_url = urllib.parse.quote(url)\n        url = f\"https://r.jina.ai/{encoded_url}\"\n        response = requests.get(url, headers=self.headers)\n        return response.json()\n\n# Check if RL tools are available\nHAVE_RL_TOOLS = False\ntry:\n    # This is a placeholder for the actual import that would be used\n    from tool_optimizer import ToolSelectionManager\n    # If the import succeeds, set HAVE_RL_TOOLS to True\n    HAVE_RL_TOOLS = True\nexcept ImportError:\n    # RL tools not available\n    # Define a dummy ToolSelectionManager to avoid NameError\n    class ToolSelectionManager:\n        def __init__(self, **kwargs):\n            self.optimizer = None\n            self.data_dir = kwargs.get('data_dir', '')\n            \n        def record_tool_usage(self, **kwargs):\n            pass\n\n# Load environment variables\nload_dotenv()\n\n# TODO: Add update checking similar to Claude Code's auto-update functionality\n# TODO: Add configuration file support to store settings beyond environment variables\n\napp = typer.Typer(help=\"OpenAI Code Assistant CLI\")\nconsole = Console()\n\n# Global Constants\n# TODO: Move these to a config file\nDEFAULT_MODEL = \"gpt-4o\"\nDEFAULT_TEMPERATURE = 0\nMAX_TOKENS = 4096\nTOKEN_LIMIT_WARNING = 0.8  # Warn when 80% of token limit is reached\n\n# Models\n# TODO: Implement more sophisticated schema validation similar to Zod in the original\n# TODO: Add permission system for tools that require user approval\n\nclass ToolParameter(BaseModel):\n    name: str\n    description: str\n    type: str\n    required: bool = False\n\nclass Tool(BaseModel):\n    name: str\n    description: str\n    parameters: Dict[str, Any]\n    function: Callable\n    # TODO: Add needs_permission flag for sensitive operations\n    # TODO: Add category for organizing tools (file, search, etc.)\n\nclass Message(BaseModel):\n    role: str\n    content: Optional[str] = None\n    tool_calls: Optional[List[Dict[str, Any]]] = None\n    tool_call_id: Optional[str] = None\n    name: Optional[str] = None\n    # TODO: Add timestamp for message tracking\n    # TODO: Add token count for better context management\n\nclass Conversation:\n    def __init__(self):\n        self.messages = []\n        # TODO: Implement retry logic with exponential backoff for API calls\n        # TODO: Add support for multiple LLM providers\n        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n        self.model = os.getenv(\"OPENAI_MODEL\", DEFAULT_MODEL)\n        self.temperature = float(os.getenv(\"OPENAI_TEMPERATURE\", DEFAULT_TEMPERATURE))\n        self.tools = self._register_tools()\n        self.tool_map = {tool.name: tool.function for tool in self.tools}\n        self.conversation_id = str(uuid4())\n        self.session_start_time = time.time()\n        self.token_usage = {\"prompt\": 0, \"completion\": 0, \"total\": 0}\n        self.verbose = False\n        self.max_tool_iterations = int(os.getenv(\"MAX_TOOL_ITERATIONS\", \"10\"))\n        \n        # Initialize tool selection optimizer if available\n        self.tool_optimizer = None\n        if HAVE_RL_TOOLS:\n            try:\n                # Create a simple tool registry adapter for the optimizer\n                class ToolRegistryAdapter:\n                    def __init__(self, tools):\n                        self.tools = tools\n                \n                    def get_all_tools(self):\n                        return self.tools\n                \n                    def get_all_tool_names(self):\n                        return [tool.name for tool in self.tools]\n            \n                # Initialize the tool selection manager\n                self.tool_optimizer = ToolSelectionManager(\n                    tool_registry=ToolRegistryAdapter(self.tools),\n                    enable_optimization=os.getenv(\"ENABLE_TOOL_OPTIMIZATION\", \"1\") == \"1\",\n                    data_dir=os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data/rl\")\n                )\n                if self.verbose:\n                    print(\"Tool selection optimization enabled\")\n            except Exception as e:\n                print(f\"Warning: Failed to initialize tool optimizer: {e}\")\n        # TODO: Implement context window management\n        \n    # Jina.ai client for search, fact-checking, and web reading\n    def _init_jina_client(self):\n        \"\"\"Initialize the Jina.ai client\"\"\"\n        token = os.getenv(\"JINA_API_KEY\", \"\")\n        return JinaClient(token)\n    \n    def _jina_search(self, query: str) -> str:\n        \"\"\"Search the web using Jina.ai\"\"\"\n        try:\n            client = self._init_jina_client()\n            results = client.search(query)\n            \n            if not results or not isinstance(results, dict):\n                return f\"No search results found for '{query}'\"\n            \n            # Format the results\n            formatted_results = \"Search Results:\\n\\n\"\n            \n            if \"results\" in results and isinstance(results[\"results\"], list):\n                for i, result in enumerate(results[\"results\"], 1):\n                    title = result.get(\"title\", \"No title\")\n                    url = result.get(\"url\", \"No URL\")\n                    snippet = result.get(\"snippet\", \"No snippet\")\n                    \n                    formatted_results += f\"{i}. {title}\\n\"\n                    formatted_results += f\"   URL: {url}\\n\"\n                    formatted_results += f\"   {snippet}\\n\\n\"\n            else:\n                formatted_results += \"Unexpected response format. Raw data:\\n\"\n                formatted_results += json.dumps(results, indent=2)[:1000]\n                \n            return formatted_results\n        except Exception as e:\n            return f\"Error performing search: {str(e)}\"\n    \n    def _jina_fact_check(self, statement: str) -> str:\n        \"\"\"Fact check a statement using Jina.ai\"\"\"\n        try:\n            client = self._init_jina_client()\n            results = client.fact_check(statement)\n            \n            if not results or not isinstance(results, dict):\n                return f\"No fact-checking results for '{statement}'\"\n            \n            # Format the results\n            formatted_results = \"Fact Check Results:\\n\\n\"\n            formatted_results += f\"Statement: {statement}\\n\\n\"\n            \n            if \"grounding\" in results:\n                grounding = results[\"grounding\"]\n                verdict = grounding.get(\"verdict\", \"Unknown\")\n                confidence = grounding.get(\"confidence\", 0)\n                \n                formatted_results += f\"Verdict: {verdict}\\n\"\n                formatted_results += f\"Confidence: {confidence:.2f}\\n\\n\"\n                \n                if \"sources\" in grounding and isinstance(grounding[\"sources\"], list):\n                    formatted_results += \"Sources:\\n\"\n                    for i, source in enumerate(grounding[\"sources\"], 1):\n                        title = source.get(\"title\", \"No title\")\n                        url = source.get(\"url\", \"No URL\")\n                        formatted_results += f\"{i}. {title}\\n   {url}\\n\\n\"\n            else:\n                formatted_results += \"Unexpected response format. Raw data:\\n\"\n                formatted_results += json.dumps(results, indent=2)[:1000]\n                \n            return formatted_results\n        except Exception as e:\n            return f\"Error performing fact check: {str(e)}\"\n    \n    def _jina_read_url(self, url: str) -> str:\n        \"\"\"Read and summarize a webpage using Jina.ai\"\"\"\n        try:\n            client = self._init_jina_client()\n            results = client.reader(url)\n            \n            if not results or not isinstance(results, dict):\n                return f\"No reading results for URL '{url}'\"\n            \n            # Format the results\n            formatted_results = f\"Web Page Summary: {url}\\n\\n\"\n            \n            if \"content\" in results:\n                content = results[\"content\"]\n                title = content.get(\"title\", \"No title\")\n                summary = content.get(\"summary\", \"No summary available\")\n                \n                formatted_results += f\"Title: {title}\\n\\n\"\n                formatted_results += f\"Summary:\\n{summary}\\n\\n\"\n                \n                if \"keyPoints\" in content and isinstance(content[\"keyPoints\"], list):\n                    formatted_results += \"Key Points:\\n\"\n                    for i, point in enumerate(content[\"keyPoints\"], 1):\n                        formatted_results += f\"{i}. {point}\\n\"\n            else:\n                formatted_results += \"Unexpected response format. Raw data:\\n\"\n                formatted_results += json.dumps(results, indent=2)[:1000]\n                \n            return formatted_results\n        except Exception as e:\n            return f\"Error reading URL: {str(e)}\"\n    \n    def _register_tools(self) -> List[Tool]:\n        # TODO: Modularize tools into separate files\n        # TODO: Implement Tool decorators for easier registration\n        # TODO: Add more tools similar to Claude Code (ReadNotebook, NotebookEditCell, etc.)\n        \n        # Define and register all tools\n        tools = [\n            Tool(\n                name=\"Weather\",\n                description=\"Gets the current weather for a location\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and optional state/country (e.g., 'San Francisco, CA' or 'London, UK')\"\n                        }\n                    },\n                    \"required\": [\"location\"]\n                },\n                function=self._get_weather\n            ),\n            Tool(\n                name=\"View\",\n                description=\"Reads a file from the local filesystem. The file_path parameter must be an absolute path, not a relative path.\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"file_path\": {\n                            \"type\": \"string\",\n                            \"description\": \"The absolute path to the file to read\"\n                        },\n                        \"limit\": {\n                            \"type\": \"number\",\n                            \"description\": \"The number of lines to read. Only provide if the file is too large to read at once.\"\n                        },\n                        \"offset\": {\n                            \"type\": \"number\",\n                            \"description\": \"The line number to start reading from. Only provide if the file is too large to read at once\"\n                        }\n                    },\n                    \"required\": [\"file_path\"]\n                },\n                function=self._view_file\n            ),\n            Tool(\n                name=\"Edit\",\n                description=\"This is a tool for editing files.\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"file_path\": {\n                            \"type\": \"string\",\n                            \"description\": \"The absolute path to the file to modify\"\n                        },\n                        \"old_string\": {\n                            \"type\": \"string\",\n                            \"description\": \"The text to replace\"\n                        },\n                        \"new_string\": {\n                            \"type\": \"string\",\n                            \"description\": \"The text to replace it with\"\n                        }\n                    },\n                    \"required\": [\"file_path\", \"old_string\", \"new_string\"]\n                },\n                function=self._edit_file\n            ),\n            Tool(\n                name=\"Replace\",\n                description=\"Write a file to the local filesystem. Overwrites the existing file if there is one.\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"file_path\": {\n                            \"type\": \"string\",\n                            \"description\": \"The absolute path to the file to write\"\n                        },\n                        \"content\": {\n                            \"type\": \"string\",\n                            \"description\": \"The content to write to the file\"\n                        }\n                    },\n                    \"required\": [\"file_path\", \"content\"]\n                },\n                function=self._replace_file\n            ),\n            Tool(\n                name=\"Bash\",\n                description=\"Executes a given bash command in a persistent shell session.\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"command\": {\n                            \"type\": \"string\",\n                            \"description\": \"The command to execute\"\n                        },\n                        \"timeout\": {\n                            \"type\": \"number\",\n                            \"description\": \"Optional timeout in milliseconds (max 600000)\"\n                        }\n                    },\n                    \"required\": [\"command\"]\n                },\n                function=self._execute_bash\n            ),\n            Tool(\n                name=\"GlobTool\",\n                description=\"Fast file pattern matching tool that works with any codebase size.\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\n                            \"type\": \"string\",\n                            \"description\": \"The directory to search in. Defaults to the current working directory.\"\n                        },\n                        \"pattern\": {\n                            \"type\": \"string\",\n                            \"description\": \"The glob pattern to match files against\"\n                        }\n                    },\n                    \"required\": [\"pattern\"]\n                },\n                function=self._glob_tool\n            ),\n            Tool(\n                name=\"GrepTool\",\n                description=\"Fast content search tool that works with any codebase size.\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\n                            \"type\": \"string\",\n                            \"description\": \"The directory to search in. Defaults to the current working directory.\"\n                        },\n                        \"pattern\": {\n                            \"type\": \"string\",\n                            \"description\": \"The regular expression pattern to search for in file contents\"\n                        },\n                        \"include\": {\n                            \"type\": \"string\",\n                            \"description\": \"File pattern to include in the search (e.g. \\\"*.js\\\", \\\"*.{ts,tsx}\\\")\"\n                        }\n                    },\n                    \"required\": [\"pattern\"]\n                },\n                function=self._grep_tool\n            ),\n            Tool(\n                name=\"LS\",\n                description=\"Lists files and directories in a given path.\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\n                            \"type\": \"string\",\n                            \"description\": \"The absolute path to the directory to list\"\n                        },\n                        \"ignore\": {\n                            \"type\": \"array\",\n                            \"items\": {\n                                \"type\": \"string\"\n                            },\n                            \"description\": \"List of glob patterns to ignore\"\n                        }\n                    },\n                    \"required\": [\"path\"]\n                },\n                function=self._list_directory\n            ),\n            Tool(\n                name=\"JinaSearch\",\n                description=\"Search the web for information using Jina.ai\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\n                            \"type\": \"string\",\n                            \"description\": \"The search query\"\n                        }\n                    },\n                    \"required\": [\"query\"]\n                },\n                function=self._jina_search\n            ),\n            Tool(\n                name=\"JinaFactCheck\",\n                description=\"Fact check a statement using Jina.ai\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"statement\": {\n                            \"type\": \"string\",\n                            \"description\": \"The statement to fact check\"\n                        }\n                    },\n                    \"required\": [\"statement\"]\n                },\n                function=self._jina_fact_check\n            ),\n            Tool(\n                name=\"JinaReadURL\",\n                description=\"Read and summarize a webpage using Jina.ai\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"url\": {\n                            \"type\": \"string\",\n                            \"description\": \"The URL of the webpage to read\"\n                        }\n                    },\n                    \"required\": [\"url\"]\n                },\n                function=self._jina_read_url\n            )\n        ]\n        return tools\n    \n    # Tool implementations\n    # TODO: Add better error handling and user feedback\n    # TODO: Implement tool usage tracking and metrics\n    \n    def _get_weather(self, location: str) -> str:\n        \"\"\"Get current weather for a location using OpenWeatherMap API\"\"\"\n        try:\n            # Get API key from environment or use a default for testing\n            api_key = os.getenv(\"OPENWEATHER_API_KEY\", \"\")\n            if not api_key:\n                return \"Error: OpenWeatherMap API key not found. Please set the OPENWEATHER_API_KEY environment variable.\"\n            \n            # Prepare the API request\n            base_url = \"https://api.openweathermap.org/data/2.5/weather\"\n            params = {\n                \"q\": location,\n                \"appid\": api_key,\n                \"units\": \"metric\"  # Use metric units (Celsius)\n            }\n            \n            # Make the API request\n            response = requests.get(base_url, params=params)\n            \n            # Check if the request was successful\n            if response.status_code == 200:\n                data = response.text\n                # Try to parse as JSON\n                try:\n                    data = json.loads(data)\n                except json.JSONDecodeError:\n                    return f\"Error: Unable to parse weather data. Raw response: {data[:200]}...\"\n                \n                # Extract relevant weather information\n                weather_desc = data[\"weather\"][0][\"description\"]\n                temp = data[\"main\"][\"temp\"]\n                feels_like = data[\"main\"][\"feels_like\"]\n                humidity = data[\"main\"][\"humidity\"]\n                wind_speed = data[\"wind\"][\"speed\"]\n                \n                # Format the response\n                weather_info = (\n                    f\"Current weather in {location}:\\n\"\n                    f\"• Condition: {weather_desc.capitalize()}\\n\"\n                    f\"• Temperature: {temp}°C ({(temp * 9/5) + 32:.1f}°F)\\n\"\n                    f\"• Feels like: {feels_like}°C ({(feels_like * 9/5) + 32:.1f}°F)\\n\"\n                    f\"• Humidity: {humidity}%\\n\"\n                    f\"• Wind speed: {wind_speed} m/s ({wind_speed * 2.237:.1f} mph)\"\n                )\n                return weather_info\n            else:\n                # Handle API errors\n                if response.status_code == 404:\n                    return f\"Error: Location '{location}' not found. Please check the spelling or try a different location.\"\n                elif response.status_code == 401:\n                    return \"Error: Invalid API key. Please check your OpenWeatherMap API key.\"\n                else:\n                    return f\"Error: Unable to fetch weather data. Status code: {response.status_code}\"\n        \n        except requests.exceptions.RequestException as e:\n            return f\"Error: Network error when fetching weather data: {str(e)}\"\n        except Exception as e:\n            return f\"Error: Failed to get weather information: {str(e)}\"\n    \n    def _view_file(self, file_path: str, limit: Optional[int] = None, offset: Optional[int] = 0) -> str:\n        # TODO: Add special handling for binary files and images\n        # TODO: Add syntax highlighting for code files\n        try:\n            if not os.path.exists(file_path):\n                return f\"Error: File not found: {file_path}\"\n            \n            # TODO: Handle file size limits better\n            \n            with open(file_path, 'r') as f:\n                if limit is not None and offset is not None:\n                    # Skip to offset\n                    for _ in range(offset):\n                        next(f, None)\n                    \n                    # Read limited lines\n                    lines = []\n                    for _ in range(limit):\n                        line = next(f, None)\n                        if line is None:\n                            break\n                        lines.append(line)\n                    content = ''.join(lines)\n                else:\n                    content = f.read()\n            \n            # TODO: Add file metadata like size, permissions, etc.\n            return content\n        except Exception as e:\n            return f\"Error reading file: {str(e)}\"\n    \n    def _edit_file(self, file_path: str, old_string: str, new_string: str) -> str:\n        try:\n            # Create directory if creating new file\n            if not os.path.exists(os.path.dirname(file_path)) and old_string == \"\":\n                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n                \n            if old_string == \"\" and not os.path.exists(file_path):\n                # Creating new file\n                with open(file_path, 'w') as f:\n                    f.write(new_string)\n                return f\"Created new file: {file_path}\"\n            \n            # Reading existing file\n            if not os.path.exists(file_path):\n                return f\"Error: File not found: {file_path}\"\n            \n            with open(file_path, 'r') as f:\n                content = f.read()\n            \n            # Replace string\n            if old_string not in content:\n                return f\"Error: Could not find the specified text in {file_path}\"\n            \n            # Count occurrences to ensure uniqueness\n            occurrences = content.count(old_string)\n            if occurrences > 1:\n                return f\"Error: Found {occurrences} occurrences of the specified text in {file_path}. Please provide more context to uniquely identify the text to replace.\"\n            \n            new_content = content.replace(old_string, new_string)\n            \n            # Write back to file\n            with open(file_path, 'w') as f:\n                f.write(new_content)\n            \n            return f\"Successfully edited {file_path}\"\n        \n        except Exception as e:\n            return f\"Error editing file: {str(e)}\"\n    \n    def _replace_file(self, file_path: str, content: str) -> str:\n        try:\n            # Create directory if it doesn't exist\n            directory = os.path.dirname(file_path)\n            if directory and not os.path.exists(directory):\n                os.makedirs(directory, exist_ok=True)\n            \n            # Write content to file\n            with open(file_path, 'w') as f:\n                f.write(content)\n            \n            return f\"Successfully wrote to {file_path}\"\n        \n        except Exception as e:\n            return f\"Error writing file: {str(e)}\"\n    \n    def _execute_bash(self, command: str, timeout: Optional[int] = None) -> str:\n        try:\n            import subprocess\n            import shlex\n            \n            # Security check for banned commands\n            banned_commands = [\n                'alias', 'curl', 'curlie', 'wget', 'axel', 'aria2c', 'nc', \n                'telnet', 'lynx', 'w3m', 'links', 'httpie', 'xh', 'http-prompt', \n                'chrome', 'firefox', 'safari'\n            ]\n            \n            for banned in banned_commands:\n                if banned in command.split():\n                    return f\"Error: The command '{banned}' is not allowed for security reasons.\"\n            \n            # Execute command\n            if timeout:\n                timeout_seconds = timeout / 1000  # Convert to seconds\n            else:\n                timeout_seconds = 1800  # 30 minutes default\n            \n            result = subprocess.run(\n                command,\n                shell=True,\n                capture_output=True,\n                text=True,\n                timeout=timeout_seconds\n            )\n            \n            output = result.stdout\n            if result.stderr:\n                output += f\"\\nErrors:\\n{result.stderr}\"\n            \n            # Truncate if too long\n            if len(output) > 30000:\n                output = output[:30000] + \"\\n... (output truncated)\"\n            \n            return output\n        \n        except subprocess.TimeoutExpired:\n            return f\"Error: Command timed out after {timeout_seconds} seconds\"\n        except Exception as e:\n            return f\"Error executing command: {str(e)}\"\n    \n    def _glob_tool(self, pattern: str, path: Optional[str] = None) -> str:\n        try:\n            import glob\n            import os\n            \n            if path is None:\n                path = os.getcwd()\n            \n            # Build the full pattern path\n            if not os.path.isabs(path):\n                path = os.path.abspath(path)\n            \n            full_pattern = os.path.join(path, pattern)\n            \n            # Get matching files\n            matches = glob.glob(full_pattern, recursive=True)\n            \n            # Sort by modification time (newest first)\n            matches.sort(key=os.path.getmtime, reverse=True)\n            \n            if not matches:\n                return f\"No files matching pattern '{pattern}' in {path}\"\n            \n            return \"\\n\".join(matches)\n        \n        except Exception as e:\n            return f\"Error in glob search: {str(e)}\"\n    \n    def _grep_tool(self, pattern: str, path: Optional[str] = None, include: Optional[str] = None) -> str:\n        try:\n            import re\n            import os\n            import fnmatch\n            from concurrent.futures import ThreadPoolExecutor\n            \n            if path is None:\n                path = os.getcwd()\n            \n            if not os.path.isabs(path):\n                path = os.path.abspath(path)\n            \n            # Compile regex pattern\n            regex = re.compile(pattern)\n            \n            # Get all files\n            all_files = []\n            for root, _, files in os.walk(path):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    \n                    # Apply include filter if provided\n                    if include:\n                        if not fnmatch.fnmatch(file, include):\n                            continue\n                    \n                    all_files.append(file_path)\n            \n            # Sort by modification time (newest first)\n            all_files.sort(key=os.path.getmtime, reverse=True)\n            \n            matches = []\n            \n            def search_file(file_path):\n                try:\n                    with open(file_path, 'r', errors='ignore') as f:\n                        content = f.read()\n                        if regex.search(content):\n                            return file_path\n                except:\n                    # Skip files that can't be read\n                    pass\n                return None\n            \n            # Search files in parallel\n            with ThreadPoolExecutor(max_workers=10) as executor:\n                results = executor.map(search_file, all_files)\n                \n                for result in results:\n                    if result:\n                        matches.append(result)\n            \n            if not matches:\n                return f\"No matches found for pattern '{pattern}' in {path}\"\n            \n            return \"\\n\".join(matches)\n        \n        except Exception as e:\n            return f\"Error in grep search: {str(e)}\"\n    \n    def _list_directory(self, path: str, ignore: Optional[List[str]] = None) -> str:\n        try:\n            import os\n            import fnmatch\n            \n            # If path is not absolute, make it absolute from current directory\n            if not os.path.isabs(path):\n                path = os.path.abspath(os.path.join(os.getcwd(), path))\n            \n            if not os.path.exists(path):\n                return f\"Error: Directory not found: {path}\"\n            \n            if not os.path.isdir(path):\n                return f\"Error: Path is not a directory: {path}\"\n            \n            # List directory contents\n            items = os.listdir(path)\n            \n            # Apply ignore patterns\n            if ignore:\n                for pattern in ignore:\n                    items = [item for item in items if not fnmatch.fnmatch(item, pattern)]\n            \n            # Sort items\n            items.sort()\n            \n            # Format output\n            result = []\n            for item in items:\n                item_path = os.path.join(path, item)\n                if os.path.isdir(item_path):\n                    result.append(f\"{item}/\")\n                else:\n                    result.append(item)\n            \n            if not result:\n                return f\"Directory {path} is empty\"\n            \n            return \"\\n\".join(result)\n        \n        except Exception as e:\n            return f\"Error listing directory: {str(e)}\"\n    \n    def add_message(self, role: str, content: str):\n        \"\"\"Legacy method to add messages - use direct append now\"\"\"\n        self.messages.append({\"role\": role, \"content\": content})\n    \n    def process_tool_calls(self, tool_calls, query=None):\n        # TODO: Add tool call validation\n        # TODO: Add permission system for sensitive tools\n        # TODO: Add progress visualization for long-running tools\n        responses = []\n        \n        # Process tool calls in parallel\n        from concurrent.futures import ThreadPoolExecutor\n        \n        def process_single_tool(tool_call):\n            # Handle both object-style and dict-style tool calls\n            if isinstance(tool_call, dict):\n                function_name = tool_call[\"function\"][\"name\"]\n                function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n                tool_call_id = tool_call[\"id\"]\n            else:\n                function_name = tool_call.function.name\n                function_args = json.loads(tool_call.function.arguments)\n                tool_call_id = tool_call.id\n            \n            # Get the tool function\n            if function_name in self.tool_map:\n                # TODO: Add pre-execution validation\n                # TODO: Add permission check here\n                \n                # Track start time for metrics\n                start_time = time.time()\n                \n                try:\n                    function = self.tool_map[function_name]\n                    result = function(**function_args)\n                    success = True\n                except Exception as e:\n                    result = f\"Error executing tool {function_name}: {str(e)}\\n{traceback.format_exc()}\"\n                    success = False\n                \n                # Calculate execution time\n                execution_time = time.time() - start_time\n                \n                # Record tool usage for optimization if optimizer is available\n                if self.tool_optimizer is not None and query is not None:\n                    try:\n                        # Create current context snapshot\n                        context = {\n                            \"messages\": self.messages.copy(),\n                            \"conversation_id\": self.conversation_id,\n                        }\n                        \n                        # Record tool usage\n                        self.tool_optimizer.record_tool_usage(\n                            query=query,\n                            tool_name=function_name,\n                            execution_time=execution_time,\n                            token_usage=self.token_usage.copy(),\n                            success=success,\n                            context=context,\n                            result=result\n                        )\n                    except Exception as e:\n                        if self.verbose:\n                            print(f\"Warning: Failed to record tool usage: {e}\")\n                \n                return {\n                    \"tool_call_id\": tool_call_id,\n                    \"function_name\": function_name,\n                    \"result\": result,\n                    \"name\": function_name,\n                    \"execution_time\": execution_time,  # For metrics\n                    \"success\": success\n                }\n            return None\n        \n        # Process all tool calls in parallel\n        with ThreadPoolExecutor(max_workers=min(10, len(tool_calls))) as executor:\n            futures = [executor.submit(process_single_tool, tool_call) for tool_call in tool_calls]\n            for future in futures:\n                result = future.result()\n                if result:\n                    # Add tool response to messages\n                    self.messages.append({\n                        \"tool_call_id\": result[\"tool_call_id\"],\n                        \"role\": \"tool\",\n                        \"name\": result[\"name\"],\n                        \"content\": result[\"result\"]\n                    })\n                    \n                    responses.append({\n                        \"tool_call_id\": result[\"tool_call_id\"],\n                        \"function_name\": result[\"function_name\"],\n                        \"result\": result[\"result\"]\n                    })\n                    \n                    # Log tool execution metrics if verbose\n                    if self.verbose:\n                        print(f\"Tool {result['function_name']} executed in {result['execution_time']:.2f}s (success: {result['success']})\")\n        \n        # Return tool responses\n        return responses\n    \n    def compact(self):\n        # TODO: Add more sophisticated compaction with token counting\n        # TODO: Implement selective retention of critical information\n        # TODO: Add option to save conversation history before compacting\n        \n        system_prompt = next((m for m in self.messages if m[\"role\"] == \"system\"), None)\n        user_messages = [m for m in self.messages if m[\"role\"] == \"user\"]\n        \n        if not user_messages:\n            return \"No user messages to compact.\"\n        \n        last_user_message = user_messages[-1]\n        \n        # Create a compaction prompt\n        # TODO: Improve the compaction prompt with more guidance on what to retain\n        compact_prompt = (\n            \"Summarize the conversation so far, focusing on the key points, decisions, and context. \"\n            \"Keep important details about the code and tasks. Retain critical file paths, commands, \"\n            \"and code snippets. The summary should be concise but complete enough to continue the \"\n            \"conversation effectively.\"\n        )\n        \n        # Add compaction message\n        self.messages.append({\"role\": \"user\", \"content\": compact_prompt})\n        \n        # Get compaction summary\n        # TODO: Add error handling for compaction API call\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=self.messages,\n            stream=False\n        )\n        \n        summary = response.choices[0].message.content\n        \n        # Reset conversation with summary\n        if system_prompt:\n            self.messages = [system_prompt]\n        else:\n            self.messages = []\n        \n        self.messages.append({\"role\": \"system\", \"content\": f\"This is a compacted conversation. Previous context: {summary}\"})\n        self.messages.append({\"role\": \"user\", \"content\": last_user_message[\"content\"]})\n        \n        # TODO: Add metrics for compaction (tokens before/after)\n        \n        return \"Conversation compacted successfully.\"\n    \n    def get_response(self, user_input: str, stream: bool = True):\n        # TODO: Add more special commands similar to Claude Code (e.g., /version, /status)\n        # TODO: Implement binary feedback mechanism for comparing responses\n        \n        # Special commands\n        if user_input.strip() == \"/compact\":\n            return self.compact()\n        \n        # Add a debug command to help diagnose issues\n        if user_input.strip() == \"/debug\":\n            debug_info = {\n                \"model\": self.model,\n                \"temperature\": self.temperature,\n                \"message_count\": len(self.messages),\n                \"token_usage\": self.token_usage,\n                \"conversation_id\": self.conversation_id,\n                \"session_duration\": time.time() - self.session_start_time,\n                \"tools_count\": len(self.tools),\n                \"python_version\": sys.version,\n                \"openai_version\": OpenAI.__version__ if hasattr(OpenAI, \"__version__\") else \"Unknown\"\n            }\n            return \"Debug Information:\\n\" + json.dumps(debug_info, indent=2)\n        \n        if user_input.strip() == \"/help\":\n            # Standard commands\n            commands = [\n                \"/help - Show this help message\",\n                \"/compact - Compact the conversation to reduce token usage\",\n                \"/status - Show token usage and session information\",\n                \"/config - Show current configuration settings\",\n            ]\n            \n            # RL-specific commands if available\n            if self.tool_optimizer is not None:\n                commands.extend([\n                    \"/rl-status - Show RL tool optimizer status\",\n                    \"/rl-update - Update the RL model manually\",\n                    \"/rl-stats - Show tool usage statistics\",\n                ])\n            \n            return \"Available commands:\\n\" + \"\\n\".join(commands)\n        \n        # Token usage and session stats\n        if user_input.strip() == \"/status\":\n            # Calculate session duration\n            session_duration = time.time() - self.session_start_time\n            hours, remainder = divmod(session_duration, 3600)\n            minutes, seconds = divmod(remainder, 60)\n            \n            # Format message\n            status = (\n                f\"Session ID: {self.conversation_id}\\n\"\n                f\"Model: {self.model} (Temperature: {self.temperature})\\n\"\n                f\"Session duration: {int(hours)}h {int(minutes)}m {int(seconds)}s\\n\\n\"\n                f\"Token usage:\\n\"\n                f\"  Prompt tokens: {self.token_usage['prompt']}\\n\"\n                f\"  Completion tokens: {self.token_usage['completion']}\\n\"\n                f\"  Total tokens: {self.token_usage['total']}\\n\"\n            )\n            return status\n            \n        # Configuration settings\n        if user_input.strip() == \"/config\":\n            config_info = (\n                f\"Current Configuration:\\n\"\n                f\"  Model: {self.model}\\n\"\n                f\"  Temperature: {self.temperature}\\n\"\n                f\"  Max tool iterations: {self.max_tool_iterations}\\n\"\n                f\"  Verbose mode: {self.verbose}\\n\"\n                f\"  RL optimization: {self.tool_optimizer is not None}\\n\"\n            )\n            \n            # Provide instructions for changing settings\n            config_info += \"\\nTo change settings, use:\\n\"\n            config_info += \"  /config set <setting> <value>\\n\"\n            config_info += \"Example: /config set max_tool_iterations 15\"\n            \n            return config_info\n            \n        # Handle configuration changes\n        if user_input.strip().startswith(\"/config set \"):\n            parts = user_input.strip().split(\" \", 3)\n            if len(parts) != 4:\n                return \"Invalid format. Use: /config set <setting> <value>\"\n                \n            setting = parts[2]\n            value = parts[3]\n            \n            if setting == \"max_tool_iterations\":\n                try:\n                    self.max_tool_iterations = int(value)\n                    return f\"Max tool iterations set to {self.max_tool_iterations}\"\n                except ValueError:\n                    return \"Invalid value. Please provide a number.\"\n            elif setting == \"temperature\":\n                try:\n                    self.temperature = float(value)\n                    return f\"Temperature set to {self.temperature}\"\n                except ValueError:\n                    return \"Invalid value. Please provide a number.\"\n            elif setting == \"verbose\":\n                if value.lower() in (\"true\", \"yes\", \"1\", \"on\"):\n                    self.verbose = True\n                    return \"Verbose mode enabled\"\n                elif value.lower() in (\"false\", \"no\", \"0\", \"off\"):\n                    self.verbose = False\n                    return \"Verbose mode disabled\"\n                else:\n                    return \"Invalid value. Use 'true' or 'false'.\"\n            elif setting == \"model\":\n                self.model = value\n                return f\"Model set to {self.model}\"\n            else:\n                return f\"Unknown setting: {setting}\"\n        \n        # RL-specific commands\n        if self.tool_optimizer is not None:\n            # RL status command\n            if user_input.strip() == \"/rl-status\":\n                return (\n                    f\"RL tool optimization is active\\n\"\n                    f\"Optimizer type: {type(self.tool_optimizer).__name__}\\n\"\n                    f\"Number of tools: {len(self.tools)}\\n\"\n                    f\"Data directory: {self.tool_optimizer.optimizer.data_dir if hasattr(self.tool_optimizer, 'optimizer') else 'N/A'}\\n\"\n                )\n            \n            # RL update command\n            if user_input.strip() == \"/rl-update\":\n                try:\n                    result = self.tool_optimizer.optimizer.update_model()\n                    status = f\"RL model update status: {result['status']}\\n\"\n                    if 'metrics' in result:\n                        status += \"Metrics:\\n\" + \"\\n\".join([f\"  {k}: {v}\" for k, v in result['metrics'].items()])\n                    return status\n                except Exception as e:\n                    return f\"Error updating RL model: {str(e)}\"\n            \n            # RL stats command\n            if user_input.strip() == \"/rl-stats\":\n                try:\n                    if hasattr(self.tool_optimizer, 'optimizer') and hasattr(self.tool_optimizer.optimizer, 'tracker'):\n                        stats = self.tool_optimizer.optimizer.tracker.get_tool_stats()\n                        if not stats:\n                            return \"No tool usage data available yet.\"\n                        \n                        result = \"Tool Usage Statistics:\\n\\n\"\n                        for tool_name, tool_stats in stats.items():\n                            result += f\"{tool_name}:\\n\"\n                            result += f\"  Count: {tool_stats['count']}\\n\"\n                            result += f\"  Success rate: {tool_stats['success_rate']:.2f}\\n\"\n                            result += f\"  Avg time: {tool_stats['avg_time']:.2f}s\\n\"\n                            result += f\"  Avg tokens: {tool_stats['avg_total_tokens']:.1f}\\n\"\n                            result += \"\\n\"\n                        return result\n                    return \"Tool usage statistics not available.\"\n                except Exception as e:\n                    return f\"Error getting tool statistics: {str(e)}\"\n        \n        # TODO: Add /version command to show version information\n        \n        # Add user message\n        self.messages.append({\"role\": \"user\", \"content\": user_input})\n        \n        # Initialize empty response\n        response_text = \"\"\n        \n        # Create tools list for API\n        # TODO: Add dynamic tool availability based on context\n        api_tools = []\n        for tool in self.tools:\n            api_tools.append({\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": tool.name,\n                    \"description\": tool.description,\n                    \"parameters\": tool.parameters\n                }\n            })\n        \n        if stream:\n            # TODO: Add retry mechanism for API failures\n            # TODO: Add token tracking for response\n            # TODO: Implement cancellation support\n            \n            # Stream response\n            try:\n                # Add retry logic for API calls\n                max_retries = 3\n                retry_count = 0\n                while retry_count < max_retries:\n                    try:\n                        stream = self.client.chat.completions.create(\n                            model=self.model,\n                            messages=self.messages,\n                            tools=api_tools,\n                            temperature=self.temperature,\n                            stream=True\n                        )\n                        break  # Success, exit retry loop\n                    except Exception as e:\n                        retry_count += 1\n                        if retry_count >= max_retries:\n                            raise  # Re-raise if we've exhausted retries\n                        \n                        # Exponential backoff\n                        wait_time = 2 ** retry_count\n                        if self.verbose:\n                            console.print(f\"[yellow]API call failed, retrying in {wait_time}s... ({retry_count}/{max_retries})[/yellow]\")\n                        time.sleep(wait_time)\n                \n                current_tool_calls = []\n                tool_call_chunks = {}\n                \n                # Process streaming response outside of the status context\n                with Live(\"\", refresh_per_second=10) as live:\n                    for chunk in stream:\n                        # If there's content, print it\n                        if chunk.choices[0].delta.content:\n                            content_piece = chunk.choices[0].delta.content\n                            response_text += content_piece\n                            # Update the live display with the accumulated response\n                            live.update(response_text)\n                        \n                        # Process tool calls\n                        delta = chunk.choices[0].delta\n                        if delta.tool_calls:\n                            for tool_call_delta in delta.tool_calls:\n                                # Initialize tool call in chunks dictionary if new\n                                if tool_call_delta.index not in tool_call_chunks:\n                                    tool_call_chunks[tool_call_delta.index] = {\n                                        \"id\": \"\",\n                                        \"function\": {\"name\": \"\", \"arguments\": \"\"}\n                                    }\n                                \n                                # Update tool call data\n                                if tool_call_delta.id:\n                                    tool_call_chunks[tool_call_delta.index][\"id\"] = tool_call_delta.id\n                                \n                                if tool_call_delta.function:\n                                    if tool_call_delta.function.name:\n                                        tool_call_chunks[tool_call_delta.index][\"function\"][\"name\"] = tool_call_delta.function.name\n                                    \n                                    if tool_call_delta.function.arguments:\n                                        tool_call_chunks[tool_call_delta.index][\"function\"][\"arguments\"] += tool_call_delta.function.arguments\n                \n                # No need to print the response again as it was already streamed in the Live context\n                \n            except Exception as e:\n                # TODO: Add better error handling and user feedback\n                console.print(f\"[bold red]Error during API call:[/bold red] {str(e)}\")\n                return f\"Error during API call: {str(e)}\"\n            \n            # Convert tool call chunks to actual tool calls\n            for index, tool_call_data in tool_call_chunks.items():\n                current_tool_calls.append({\n                    \"id\": tool_call_data[\"id\"],\n                    \"function\": {\n                        \"name\": tool_call_data[\"function\"][\"name\"],\n                        \"arguments\": tool_call_data[\"function\"][\"arguments\"]\n                    }\n                })\n            \n            # Process tool calls if any\n            if current_tool_calls:\n                try:\n                    # Add assistant message with tool_calls to messages first\n                    # Ensure each tool call has a \"type\" field set to \"function\"\n                    processed_tool_calls = []\n                    for tool_call in current_tool_calls:\n                        processed_tool_call = tool_call.copy()\n                        processed_tool_call[\"type\"] = \"function\"\n                        processed_tool_calls.append(processed_tool_call)\n                        \n                    # Make sure we add the assistant message with tool calls before processing them\n                    self.messages.append({\n                        \"role\": \"assistant\", \n                        \"content\": response_text,\n                        \"tool_calls\": processed_tool_calls\n                    })\n                        \n                    # Now process the tool calls\n                    with console.status(\"[bold green]Running tools...\"):\n                        tool_responses = self.process_tool_calls(current_tool_calls, query=user_input)\n                except Exception as e:\n                    console.print(f\"[bold red]Error:[/bold red] {str(e)}\")\n                    console.print(traceback.format_exc())\n                    return f\"Error processing tool calls: {str(e)}\"\n                    \n                # Continue the conversation with tool responses\n                # Implement looping function calls to allow for recursive tool usage\n                max_loop_iterations = self.max_tool_iterations  # Use configurable setting\n                current_iteration = 0\n                \n                while current_iteration < max_loop_iterations:\n                    # Add retry logic for follow-up API calls\n                    max_retries = 3\n                    retry_count = 0\n                    follow_up = None\n                    \n                    while retry_count < max_retries:\n                        try:\n                            follow_up = self.client.chat.completions.create(\n                                model=self.model,\n                                messages=self.messages,\n                                tools=api_tools,  # Pass tools to enable recursive function calling\n                                stream=False\n                            )\n                            break  # Success, exit retry loop\n                        except Exception as e:\n                            retry_count += 1\n                            if retry_count >= max_retries:\n                                raise  # Re-raise if we've exhausted retries\n                                \n                            # Exponential backoff\n                            wait_time = 2 ** retry_count\n                            if self.verbose:\n                                console.print(f\"[yellow]Follow-up API call failed, retrying in {wait_time}s... ({retry_count}/{max_retries})[/yellow]\")\n                            time.sleep(wait_time)\n                    \n                    # Check if the follow-up response contains more tool calls\n                    assistant_message = follow_up.choices[0].message\n                    follow_up_text = assistant_message.content or \"\"\n                    \n                    # If there are no more tool calls, we're done with the loop\n                    if not hasattr(assistant_message, 'tool_calls') or not assistant_message.tool_calls:\n                        if follow_up_text:\n                            console.print(Markdown(follow_up_text))\n                            response_text += \"\\n\" + follow_up_text\n                        \n                        # Add the final assistant message to the conversation\n                        self.messages.append({\"role\": \"assistant\", \"content\": follow_up_text})\n                        break\n                    \n                    # Process the new tool calls\n                    current_tool_calls = []\n                    for tool_call in assistant_message.tool_calls:\n                        # Handle both object-style and dict-style tool calls\n                        if isinstance(tool_call, dict):\n                            processed_tool_call = tool_call.copy()\n                        else:\n                            # Convert object to dict\n                            processed_tool_call = {\n                                \"id\": tool_call.id,\n                                \"function\": {\n                                    \"name\": tool_call.function.name,\n                                    \"arguments\": tool_call.function.arguments\n                                }\n                            }\n                        \n                        # Ensure type field is present\n                        processed_tool_call[\"type\"] = \"function\"\n                        current_tool_calls.append(processed_tool_call)\n                    \n                    # Add the assistant message with tool calls\n                    self.messages.append({\n                        \"role\": \"assistant\",\n                        \"content\": follow_up_text,\n                        \"tool_calls\": current_tool_calls\n                    })\n                    \n                    # Process the new tool calls\n                    with console.status(f\"[bold green]Running tools (iteration {current_iteration + 1})...[/bold green]\"):\n                        tool_responses = self.process_tool_calls(assistant_message.tool_calls, query=user_input)\n                    \n                    # Increment the iteration counter\n                    current_iteration += 1\n                \n                # If we've reached the maximum number of iterations, add a warning\n                if current_iteration >= max_loop_iterations:\n                    warning_message = f\"[yellow]Warning: Reached maximum number of tool call iterations ({max_loop_iterations}). Some operations may be incomplete.[/yellow]\"\n                    console.print(warning_message)\n                    response_text += f\"\\n\\n{warning_message}\"\n            \n            # Add assistant response to messages if there were no tool calls\n            # (we already added it above if there were tool calls)\n            if not current_tool_calls:\n                self.messages.append({\"role\": \"assistant\", \"content\": response_text})\n            \n            return response_text\n        else:\n            # Non-streaming response\n            response = self.client.chat.completions.create(\n                model=self.model,\n                messages=self.messages,\n                tools=api_tools,\n                temperature=self.temperature,\n                stream=False\n            )\n            \n            # Track token usage\n            if hasattr(response, 'usage'):\n                self.token_usage[\"prompt\"] += response.usage.prompt_tokens\n                self.token_usage[\"completion\"] += response.usage.completion_tokens\n                self.token_usage[\"total\"] += response.usage.total_tokens\n            \n            assistant_message = response.choices[0].message\n            response_text = assistant_message.content or \"\"\n            \n            # Process tool calls if any\n            if assistant_message.tool_calls:\n                # Add assistant message with tool_calls to messages\n                # Convert tool_calls to a list of dictionaries with \"type\" field\n                processed_tool_calls = []\n                for tool_call in assistant_message.tool_calls:\n                    # Handle both object-style and dict-style tool calls\n                    if isinstance(tool_call, dict):\n                        processed_tool_call = tool_call.copy()\n                    else:\n                        # Convert object to dict\n                        processed_tool_call = {\n                            \"id\": tool_call.id,\n                            \"function\": {\n                                \"name\": tool_call.function.name,\n                                \"arguments\": tool_call.function.arguments\n                            }\n                        }\n                    \n                    # Ensure type field is present\n                    processed_tool_call[\"type\"] = \"function\"\n                    processed_tool_calls.append(processed_tool_call)\n                \n                self.messages.append({\n                    \"role\": \"assistant\",\n                    \"content\": response_text,\n                    \"tool_calls\": processed_tool_calls\n                })\n                \n                with console.status(\"[bold green]Running tools...\"):\n                    tool_responses = self.process_tool_calls(assistant_message.tool_calls, query=user_input)\n                \n                # Continue the conversation with tool responses\n                # Implement looping function calls to allow for recursive tool usage\n                max_loop_iterations = self.max_tool_iterations  # Use configurable setting\n                current_iteration = 0\n                \n                while current_iteration < max_loop_iterations:\n                    # Add retry logic for follow-up API calls\n                    max_retries = 3\n                    retry_count = 0\n                    follow_up = None\n                    \n                    while retry_count < max_retries:\n                        try:\n                            follow_up = self.client.chat.completions.create(\n                                model=self.model,\n                                messages=self.messages,\n                                tools=api_tools,  # Pass tools to enable recursive function calling\n                                stream=False\n                            )\n                            break  # Success, exit retry loop\n                        except Exception as e:\n                            retry_count += 1\n                            if retry_count >= max_retries:\n                                raise  # Re-raise if we've exhausted retries\n                                \n                            # Exponential backoff\n                            wait_time = 2 ** retry_count\n                            if self.verbose:\n                                console.print(f\"[yellow]Follow-up API call failed, retrying in {wait_time}s... ({retry_count}/{max_retries})[/yellow]\")\n                            time.sleep(wait_time)\n                    \n                    # Check if the follow-up response contains more tool calls\n                    assistant_message = follow_up.choices[0].message\n                    follow_up_text = assistant_message.content or \"\"\n                    \n                    # If there are no more tool calls, we're done with the loop\n                    if not hasattr(assistant_message, 'tool_calls') or not assistant_message.tool_calls:\n                        if follow_up_text:\n                            console.print(Markdown(follow_up_text))\n                            response_text += \"\\n\" + follow_up_text\n                        \n                        # Add the final assistant message to the conversation\n                        self.messages.append({\"role\": \"assistant\", \"content\": follow_up_text})\n                        break\n                    \n                    # Process the new tool calls\n                    current_tool_calls = []\n                    for tool_call in assistant_message.tool_calls:\n                        # Handle both object-style and dict-style tool calls\n                        if isinstance(tool_call, dict):\n                            processed_tool_call = tool_call.copy()\n                        else:\n                            # Convert object to dict\n                            processed_tool_call = {\n                                \"id\": tool_call.id,\n                                \"function\": {\n                                    \"name\": tool_call.function.name,\n                                    \"arguments\": tool_call.function.arguments\n                                }\n                            }\n                        \n                        # Ensure type field is present\n                        processed_tool_call[\"type\"] = \"function\"\n                        current_tool_calls.append(processed_tool_call)\n                    \n                    # Add the assistant message with tool calls\n                    self.messages.append({\n                        \"role\": \"assistant\",\n                        \"content\": follow_up_text,\n                        \"tool_calls\": current_tool_calls\n                    })\n                    \n                    # Process the new tool calls\n                    with console.status(f\"[bold green]Running tools (iteration {current_iteration + 1})...[/bold green]\"):\n                        tool_responses = self.process_tool_calls(assistant_message.tool_calls, query=user_input)\n                    \n                    # Increment the iteration counter\n                    current_iteration += 1\n                \n                # If we've reached the maximum number of iterations, add a warning\n                if current_iteration >= max_loop_iterations:\n                    warning_message = f\"[yellow]Warning: Reached maximum number of tool call iterations ({max_loop_iterations}). Some operations may be incomplete.[/yellow]\"\n                    console.print(warning_message)\n                    response_text += f\"\\n\\n{warning_message}\"\n            else:\n                console.print(Markdown(response_text))\n            \n            # Add assistant response to messages if not already added\n            # (we already added it above if there were tool calls)\n            if not assistant_message.tool_calls:\n                self.messages.append({\"role\": \"assistant\", \"content\": response_text})\n            \n            return response_text\n\n# TODO: Create a more flexible system prompt mechanism with customizable templates\ndef get_system_prompt():\n    return \"\"\"You are OpenAI Code Assistant, a CLI tool that helps users with software engineering tasks and general information.\nUse the available tools to assist the user with their requests.\n\n# Tone and style\nYou should be concise, direct, and to the point. When you run a non-trivial bash command, \nyou should explain what the command does and why you are running it.\nOutput text to communicate with the user; all text you output outside of tool use is displayed to the user.\nRemember that your output will be displayed on a command line interface.\n\n# Tool usage policy\n- When doing file search, remember to search effectively with the available tools.\n- Always use the appropriate tool for the task.\n- Use parallel tool calls when appropriate to improve performance.\n- NEVER commit changes unless the user explicitly asks you to.\n- For weather queries, use the Weather tool to provide real-time information.\n\n# Tasks\nThe user will primarily request you perform software engineering tasks:\n1. Solving bugs\n2. Adding new functionality \n3. Refactoring code\n4. Explaining code\n5. Writing tests\n\nFor these tasks:\n1. Use search tools to understand the codebase\n2. Implement solutions using the available tools\n3. Verify solutions with tests if possible\n4. Run lint and typecheck commands when appropriate\n\nThe user may also ask for general information:\n1. Weather conditions\n2. Simple calculations\n3. General knowledge questions\n\n# Code style\n- Follow the existing code style of the project\n- Maintain consistent naming conventions\n- Use appropriate libraries that are already in the project\n- Add comments when code is complex or non-obvious\n\nIMPORTANT: You should minimize output tokens as much as possible while maintaining helpfulness, \nquality, and accuracy. Answer concisely with short lines of text unless the user asks for detail.\n\"\"\"\n\n# TODO: Add version information and CLI arguments\n# TODO: Add logging configuration\n# TODO: Create a proper CLI command structure with subcommands\n\n# Hosting and replication capabilities\nclass HostingManager:\n    \"\"\"Manages hosting and replication of the assistant\"\"\"\n    \n    def __init__(self, host=\"127.0.0.1\", port=8000):\n        self.host = host\n        self.port = port\n        self.app = FastAPI(title=\"OpenAI Code Assistant API\")\n        self.conversation_pool = {}\n        self.setup_api()\n        \n    def setup_api(self):\n        \"\"\"Configure the FastAPI application\"\"\"\n        # Add CORS middleware\n        self.app.add_middleware(\n            CORSMiddleware,\n            allow_origins=[\"*\"],  # In production, restrict this to specific domains\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n        \n        # Define API routes\n        @self.app.get(\"/\")\n        async def root():\n            return {\"message\": \"OpenAI Code Assistant API\", \"status\": \"running\"}\n        \n        @self.app.post(\"/conversation\")\n        async def create_conversation(\n            request: Request,\n            background_tasks: BackgroundTasks,\n            model: str = DEFAULT_MODEL,\n            temperature: float = DEFAULT_TEMPERATURE\n        ):\n            \"\"\"Create a new conversation instance\"\"\"\n            conversation_id = str(uuid4())\n            \n            # Initialize conversation in background\n            background_tasks.add_task(self._init_conversation, conversation_id, model, temperature)\n            \n            return {\n                \"conversation_id\": conversation_id,\n                \"status\": \"initializing\",\n                \"model\": model\n            }\n        \n        @self.app.post(\"/conversation/{conversation_id}/message\")\n        async def send_message(\n            conversation_id: str,\n            request: Request\n        ):\n            \"\"\"Send a message to a conversation\"\"\"\n            if conversation_id not in self.conversation_pool:\n                raise HTTPException(status_code=404, detail=\"Conversation not found\")\n                \n            data = await request.json()\n            user_input = data.get(\"message\", \"\")\n            \n            # Get conversation instance\n            conversation = self.conversation_pool[conversation_id]\n            \n            # Process message\n            try:\n                response = conversation.get_response(user_input, stream=False)\n                return {\n                    \"conversation_id\": conversation_id,\n                    \"response\": response\n                }\n            except Exception as e:\n                raise HTTPException(status_code=500, detail=f\"Error processing message: {str(e)}\")\n        \n        @self.app.post(\"/conversation/{conversation_id}/message/stream\")\n        async def stream_message(\n            conversation_id: str,\n            request: Request\n        ):\n            \"\"\"Stream a message response from a conversation\"\"\"\n            if conversation_id not in self.conversation_pool:\n                raise HTTPException(status_code=404, detail=\"Conversation not found\")\n                \n            data = await request.json()\n            user_input = data.get(\"message\", \"\")\n            \n            # Get conversation instance\n            conversation = self.conversation_pool[conversation_id]\n            \n            # Create async generator for streaming\n            async def response_generator():\n                # Add user message\n                conversation.messages.append({\"role\": \"user\", \"content\": user_input})\n                \n                # Create tools list for API\n                api_tools = []\n                for tool in conversation.tools:\n                    api_tools.append({\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"name\": tool.name,\n                            \"description\": tool.description,\n                            \"parameters\": tool.parameters\n                        }\n                    })\n                \n                # Stream response\n                try:\n                    stream = conversation.client.chat.completions.create(\n                        model=conversation.model,\n                        messages=conversation.messages,\n                        tools=api_tools,\n                        temperature=conversation.temperature,\n                        stream=True\n                    )\n                    \n                    current_tool_calls = []\n                    tool_call_chunks = {}\n                    response_text = \"\"\n                    \n                    for chunk in stream:\n                        # If there's content, yield it\n                        if chunk.choices[0].delta.content:\n                            content_piece = chunk.choices[0].delta.content\n                            response_text += content_piece\n                            yield json.dumps({\"type\": \"content\", \"content\": content_piece}) + \"\\n\"\n                        \n                        # Process tool calls\n                        delta = chunk.choices[0].delta\n                        if delta.tool_calls:\n                            for tool_call_delta in delta.tool_calls:\n                                # Initialize tool call in chunks dictionary if new\n                                if tool_call_delta.index not in tool_call_chunks:\n                                    tool_call_chunks[tool_call_delta.index] = {\n                                        \"id\": \"\",\n                                        \"function\": {\"name\": \"\", \"arguments\": \"\"}\n                                    }\n                                \n                                # Update tool call data\n                                if tool_call_delta.id:\n                                    tool_call_chunks[tool_call_delta.index][\"id\"] = tool_call_delta.id\n                                \n                                if tool_call_delta.function:\n                                    if tool_call_delta.function.name:\n                                        tool_call_chunks[tool_call_delta.index][\"function\"][\"name\"] = tool_call_delta.function.name\n                                    \n                                    if tool_call_delta.function.arguments:\n                                        tool_call_chunks[tool_call_delta.index][\"function\"][\"arguments\"] += tool_call_delta.function.arguments\n                    \n                    # Convert tool call chunks to actual tool calls\n                    for index, tool_call_data in tool_call_chunks.items():\n                        current_tool_calls.append({\n                            \"id\": tool_call_data[\"id\"],\n                            \"function\": {\n                                \"name\": tool_call_data[\"function\"][\"name\"],\n                                \"arguments\": tool_call_data[\"function\"][\"arguments\"]\n                            }\n                        })\n                    \n                    # Process tool calls if any\n                    if current_tool_calls:\n                        # Add assistant message with tool_calls to messages\n                        processed_tool_calls = []\n                        for tool_call in current_tool_calls:\n                            processed_tool_call = tool_call.copy()\n                            processed_tool_call[\"type\"] = \"function\"\n                            processed_tool_calls.append(processed_tool_call)\n                        \n                        conversation.messages.append({\n                            \"role\": \"assistant\", \n                            \"content\": response_text,\n                            \"tool_calls\": processed_tool_calls\n                        })\n                        \n                        # Notify client that tools are running\n                        yield json.dumps({\"type\": \"status\", \"status\": \"running_tools\"}) + \"\\n\"\n                        \n                        # Process tool calls\n                        tool_responses = conversation.process_tool_calls(current_tool_calls, query=user_input)\n                        \n                        # Notify client of tool results\n                        for response in tool_responses:\n                            yield json.dumps({\n                                \"type\": \"tool_result\", \n                                \"tool\": response[\"function_name\"],\n                                \"result\": response[\"result\"]\n                            }) + \"\\n\"\n                        \n                        # Continue the conversation with tool responses\n                        max_loop_iterations = conversation.max_tool_iterations\n                        current_iteration = 0\n                        \n                        while current_iteration < max_loop_iterations:\n                            follow_up = conversation.client.chat.completions.create(\n                                model=conversation.model,\n                                messages=conversation.messages,\n                                tools=api_tools,\n                                stream=False\n                            )\n                            \n                            # Check if the follow-up response contains more tool calls\n                            assistant_message = follow_up.choices[0].message\n                            follow_up_text = assistant_message.content or \"\"\n                            \n                            # If there are no more tool calls, we're done with the loop\n                            if not hasattr(assistant_message, 'tool_calls') or not assistant_message.tool_calls:\n                                if follow_up_text:\n                                    yield json.dumps({\"type\": \"content\", \"content\": follow_up_text}) + \"\\n\"\n                                \n                                # Add the final assistant message to the conversation\n                                conversation.messages.append({\"role\": \"assistant\", \"content\": follow_up_text})\n                                break\n                            \n                            # Process the new tool calls\n                            current_tool_calls = []\n                            for tool_call in assistant_message.tool_calls:\n                                if isinstance(tool_call, dict):\n                                    processed_tool_call = tool_call.copy()\n                                else:\n                                    processed_tool_call = {\n                                        \"id\": tool_call.id,\n                                        \"function\": {\n                                            \"name\": tool_call.function.name,\n                                            \"arguments\": tool_call.function.arguments\n                                        }\n                                    }\n                                \n                                processed_tool_call[\"type\"] = \"function\"\n                                current_tool_calls.append(processed_tool_call)\n                            \n                            # Add the assistant message with tool calls\n                            conversation.messages.append({\n                                \"role\": \"assistant\",\n                                \"content\": follow_up_text,\n                                \"tool_calls\": current_tool_calls\n                            })\n                            \n                            # Notify client that tools are running\n                            yield json.dumps({\n                                \"type\": \"status\", \n                                \"status\": f\"running_tools_iteration_{current_iteration + 1}\"\n                            }) + \"\\n\"\n                            \n                            # Process the new tool calls\n                            tool_responses = conversation.process_tool_calls(assistant_message.tool_calls, query=user_input)\n                            \n                            # Notify client of tool results\n                            for response in tool_responses:\n                                yield json.dumps({\n                                    \"type\": \"tool_result\", \n                                    \"tool\": response[\"function_name\"],\n                                    \"result\": response[\"result\"]\n                                }) + \"\\n\"\n                            \n                            # Increment the iteration counter\n                            current_iteration += 1\n                        \n                        # If we've reached the maximum number of iterations, add a warning\n                        if current_iteration >= max_loop_iterations:\n                            warning_message = f\"Warning: Reached maximum number of tool call iterations ({max_loop_iterations}). Some operations may be incomplete.\"\n                            yield json.dumps({\"type\": \"warning\", \"warning\": warning_message}) + \"\\n\"\n                    else:\n                        # Add assistant response to messages\n                        conversation.messages.append({\"role\": \"assistant\", \"content\": response_text})\n                    \n                    # Signal completion\n                    yield json.dumps({\"type\": \"status\", \"status\": \"complete\"}) + \"\\n\"\n                    \n                except Exception as e:\n                    yield json.dumps({\"type\": \"error\", \"error\": str(e)}) + \"\\n\"\n            \n            return StreamingResponse(response_generator(), media_type=\"text/event-stream\")\n        \n        @self.app.get(\"/conversation/{conversation_id}\")\n        async def get_conversation(conversation_id: str):\n            \"\"\"Get conversation details\"\"\"\n            if conversation_id not in self.conversation_pool:\n                raise HTTPException(status_code=404, detail=\"Conversation not found\")\n            \n            conversation = self.conversation_pool[conversation_id]\n            \n            return {\n                \"conversation_id\": conversation_id,\n                \"model\": conversation.model,\n                \"temperature\": conversation.temperature,\n                \"message_count\": len(conversation.messages),\n                \"token_usage\": conversation.token_usage\n            }\n        \n        @self.app.delete(\"/conversation/{conversation_id}\")\n        async def delete_conversation(conversation_id: str):\n            \"\"\"Delete a conversation\"\"\"\n            if conversation_id not in self.conversation_pool:\n                raise HTTPException(status_code=404, detail=\"Conversation not found\")\n            \n            del self.conversation_pool[conversation_id]\n            \n            return {\"status\": \"deleted\", \"conversation_id\": conversation_id}\n        \n        @self.app.get(\"/health\")\n        async def health_check():\n            \"\"\"Health check endpoint\"\"\"\n            return {\n                \"status\": \"healthy\",\n                \"active_conversations\": len(self.conversation_pool),\n                \"uptime\": time.time() - self.start_time\n            }\n    \n    async def _init_conversation(self, conversation_id, model, temperature):\n        \"\"\"Initialize a conversation instance\"\"\"\n        conversation = Conversation()\n        conversation.model = model\n        conversation.temperature = temperature\n        conversation.messages.append({\"role\": \"system\", \"content\": get_system_prompt()})\n        \n        self.conversation_pool[conversation_id] = conversation\n    \n    def start(self):\n        \"\"\"Start the API server\"\"\"\n        self.start_time = time.time()\n        uvicorn.run(self.app, host=self.host, port=self.port)\n    \n    def start_background(self):\n        \"\"\"Start the API server in a background thread\"\"\"\n        self.start_time = time.time()\n        thread = threading.Thread(target=uvicorn.run, args=(self.app,), \n                                 kwargs={\"host\": self.host, \"port\": self.port})\n        thread.daemon = True\n        thread.start()\n        return thread\n\nclass ReplicationManager:\n    \"\"\"Manages replication across multiple instances\"\"\"\n    \n    def __init__(self, primary=True, sync_interval=60):\n        self.primary = primary\n        self.sync_interval = sync_interval\n        self.peers = []\n        self.conversation_cache = {}\n        self.last_sync = time.time()\n        self.sync_lock = threading.Lock()\n    \n    def add_peer(self, host, port):\n        \"\"\"Add a peer instance to replicate with\"\"\"\n        peer = {\"host\": host, \"port\": port}\n        if peer not in self.peers:\n            self.peers.append(peer)\n            return True\n        return False\n    \n    def remove_peer(self, host, port):\n        \"\"\"Remove a peer instance\"\"\"\n        peer = {\"host\": host, \"port\": port}\n        if peer in self.peers:\n            self.peers.remove(peer)\n            return True\n        return False\n    \n    def sync_conversation(self, conversation_id, conversation):\n        \"\"\"Sync a conversation to all peers\"\"\"\n        if not self.peers:\n            return\n        \n        # Serialize conversation\n        try:\n            serialized = pickle.dumps(conversation)\n            \n            # Calculate hash for change detection\n            conversation_hash = hashlib.md5(serialized).hexdigest()\n            \n            # Check if conversation has changed\n            if conversation_id in self.conversation_cache:\n                if self.conversation_cache[conversation_id] == conversation_hash:\n                    return  # No changes, skip sync\n            \n            # Update cache\n            self.conversation_cache[conversation_id] = conversation_hash\n            \n            # Sync to peers\n            for peer in self.peers:\n                try:\n                    url = f\"http://{peer['host']}:{peer['port']}/sync/conversation/{conversation_id}\"\n                    requests.post(url, data=serialized, \n                                 headers={\"Content-Type\": \"application/octet-stream\"})\n                except Exception as e:\n                    logging.error(f\"Failed to sync with peer {peer['host']}:{peer['port']}: {e}\")\n        except Exception as e:\n            logging.error(f\"Error serializing conversation: {e}\")\n    \n    def start_sync_thread(self, conversation_pool):\n        \"\"\"Start background thread for periodic syncing\"\"\"\n        def sync_worker():\n            while True:\n                time.sleep(self.sync_interval)\n                \n                with self.sync_lock:\n                    for conversation_id, conversation in conversation_pool.items():\n                        self.sync_conversation(conversation_id, conversation)\n        \n        thread = threading.Thread(target=sync_worker)\n        thread.daemon = True\n        thread.start()\n        return thread\n\n@app.command()\ndef serve(\n    host: str = typer.Option(\"127.0.0.1\", \"--host\", help=\"Host address to bind to\"),\n    port: int = typer.Option(8000, \"--port\", \"-p\", help=\"Port to listen on\"),\n    workers: int = typer.Option(1, \"--workers\", \"-w\", help=\"Number of worker processes\"),\n    enable_replication: bool = typer.Option(False, \"--enable-replication\", help=\"Enable replication across instances\"),\n    primary: bool = typer.Option(True, \"--primary/--secondary\", help=\"Whether this is a primary or secondary instance\"),\n    peers: List[str] = typer.Option([], \"--peer\", help=\"Peer instances to replicate with (host:port)\")\n):\n    \"\"\"\n    Start the OpenAI Code Assistant as a web service\n    \"\"\"\n    console.print(Panel.fit(\n        f\"[bold green]OpenAI Code Assistant API Server[/bold green]\\n\"\n        f\"Host: {host}\\n\"\n        f\"Port: {port}\\n\"\n        f\"Workers: {workers}\\n\"\n        f\"Replication: {'Enabled' if enable_replication else 'Disabled'}\\n\"\n        f\"Role: {'Primary' if primary else 'Secondary'}\\n\"\n        f\"Peers: {', '.join(peers) if peers else 'None'}\",\n        title=\"Server Starting\",\n        border_style=\"green\"\n    ))\n    \n    # Check API key\n    if not os.getenv(\"OPENAI_API_KEY\"):\n        console.print(\"[bold red]Error:[/bold red] No OpenAI API key found. Please set the OPENAI_API_KEY environment variable.\")\n        return\n    \n    # Start server\n    if workers > 1:\n        # Use multiprocessing for multiple workers\n        console.print(f\"Starting server with {workers} workers...\")\n        uvicorn.run(\n            \"cli:create_app\",\n            host=host,\n            port=port,\n            workers=workers,\n            factory=True\n        )\n    else:\n        # Single process mode\n        hosting_manager = HostingManager(host=host, port=port)\n        \n        # Setup replication if enabled\n        if enable_replication:\n            replication_manager = ReplicationManager(primary=primary)\n            \n            # Add peers\n            for peer in peers:\n                try:\n                    peer_host, peer_port = peer.split(\":\")\n                    replication_manager.add_peer(peer_host, int(peer_port))\n                except ValueError:\n                    console.print(f\"[yellow]Warning: Invalid peer format: {peer}. Use host:port format.[/yellow]\")\n            \n            # Start sync thread\n            replication_manager.start_sync_thread(hosting_manager.conversation_pool)\n            \n            console.print(f\"Replication enabled with {len(replication_manager.peers)} peers\")\n        \n        # Start server\n        hosting_manager.start()\n\ndef create_app():\n    \"\"\"Factory function for creating the FastAPI app (used with multiple workers)\"\"\"\n    hosting_manager = HostingManager()\n    return hosting_manager.app\n\n@app.command()\ndef mcp_serve(\n    host: str = typer.Option(\"127.0.0.1\", \"--host\", help=\"Host address to bind to\"),\n    port: int = typer.Option(8000, \"--port\", \"-p\", help=\"Port to listen on\"),\n    dev_mode: bool = typer.Option(False, \"--dev\", help=\"Enable development mode with additional logging\"),\n    dependencies: List[str] = typer.Option([], \"--dependencies\", help=\"Additional Python dependencies to install\"),\n    env_file: str = typer.Option(None, \"--env-file\", help=\"Path to .env file with environment variables\"),\n    cache_type: str = typer.Option(\"memory\", \"--cache\", help=\"Cache type: 'memory' or 'redis'\"),\n    redis_url: str = typer.Option(None, \"--redis-url\", help=\"Redis URL for cache (if cache_type is 'redis')\"),\n    reload: bool = typer.Option(False, \"--reload\", help=\"Enable auto-reload on code changes\")\n):\n    \"\"\"\n    Start the OpenAI Code Assistant as an MCP (Model Context Protocol) server\n    \n    This allows the assistant to be used as a context provider for MCP clients\n    like Claude Desktop or other MCP-compatible applications.\n    \"\"\"\n    # Load environment variables from file if specified\n    if env_file:\n        if os.path.exists(env_file):\n            load_dotenv(env_file)\n            console.print(f\"[green]Loaded environment variables from {env_file}[/green]\")\n        else:\n            console.print(f\"[yellow]Warning: Environment file {env_file} not found[/yellow]\")\n    \n    # Install additional dependencies if specified\n    required_deps = [\"prometheus-client\", \"tiktoken\"]\n    if cache_type == \"redis\":\n        required_deps.append(\"redis\")\n    \n    all_deps = required_deps + list(dependencies)\n    \n    if all_deps:\n        console.print(f\"[bold]Installing dependencies: {', '.join(all_deps)}[/bold]\")\n        try:\n            import subprocess\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *all_deps])\n            console.print(\"[green]Dependencies installed successfully[/green]\")\n        except Exception as e:\n            console.print(f\"[red]Error installing dependencies: {str(e)}[/red]\")\n            return\n    \n    # Configure logging for development mode\n    if dev_mode:\n        import logging\n        logging.basicConfig(level=logging.DEBUG)\n        console.print(\"[yellow]Development mode enabled with debug logging[/yellow]\")\n    \n    # Print server information\n    cache_info = f\"Cache: {cache_type}\"\n    if cache_type == \"redis\" and redis_url:\n        cache_info += f\" ({redis_url})\"\n    \n    console.print(Panel.fit(\n        f\"[bold green]OpenAI Code Assistant MCP Server[/bold green]\\n\"\n        f\"Host: {host}\\n\"\n        f\"Port: {port}\\n\"\n        f\"Development Mode: {'Enabled' if dev_mode else 'Disabled'}\\n\"\n        f\"Auto-reload: {'Enabled' if reload else 'Disabled'}\\n\"\n        f\"{cache_info}\\n\"\n        f\"API Key: {'Configured' if os.getenv('OPENAI_API_KEY') else 'Not Configured'}\",\n        title=\"MCP Server Starting\",\n        border_style=\"green\"\n    ))\n    \n    # Check API key\n    if not os.getenv(\"OPENAI_API_KEY\"):\n        console.print(\"[bold red]Error:[/bold red] No OpenAI API key found. Please set the OPENAI_API_KEY environment variable.\")\n        return\n    \n    # Create required directories\n    base_dir = os.path.dirname(os.path.abspath(__file__))\n    os.makedirs(os.path.join(base_dir, \"data\"), exist_ok=True)\n    os.makedirs(os.path.join(base_dir, \"templates\"), exist_ok=True)\n    os.makedirs(os.path.join(base_dir, \"static\"), exist_ok=True)\n    \n    try:\n        # Import the MCP server module\n        from mcp_server import MCPServer\n        \n        # Start the MCP server\n        server = MCPServer(cache_type=cache_type, redis_url=redis_url)\n        server.start(host=host, port=port, reload=reload)\n    except ImportError:\n        console.print(\"[bold red]Error:[/bold red] MCP server module not found. Make sure mcp_server.py is in the same directory.\")\n    except Exception as e:\n        console.print(f\"[bold red]Error starting MCP server:[/bold red] {str(e)}\")\n        if dev_mode:\n            import traceback\n            console.print(traceback.format_exc())\n\n@app.command()\ndef mcp_client(\n    server_path: str = typer.Argument(..., help=\"Path to the MCP server script or module\"),\n    model: str = typer.Option(\"gpt-4o\", \"--model\", \"-m\", help=\"Model to use for reasoning\"),\n    host: str = typer.Option(\"127.0.0.1\", \"--host\", help=\"Host address for the MCP server\"),\n    port: int = typer.Option(8000, \"--port\", \"-p\", help=\"Port for the MCP server\")\n):\n    \"\"\"\n    Connect to an MCP server using OpenAI Code Assistant as the reasoning engine\n    \n    This allows using the assistant to interact with any MCP-compatible server.\n    \"\"\"\n    console.print(Panel.fit(\n        f\"[bold green]OpenAI Code Assistant MCP Client[/bold green]\\n\"\n        f\"Server: {server_path}\\n\"\n        f\"Model: {model}\\n\"\n        f\"Host: {host}\\n\"\n        f\"Port: {port}\",\n        title=\"MCP Client Starting\",\n        border_style=\"green\"\n    ))\n    \n    # Check if server path exists\n    if not os.path.exists(server_path):\n        console.print(f\"[bold red]Error:[/bold red] Server script not found at {server_path}\")\n        return\n    \n    # Check API key\n    if not os.getenv(\"OPENAI_API_KEY\"):\n        console.print(\"[bold red]Error:[/bold red] No OpenAI API key found. Please set the OPENAI_API_KEY environment variable.\")\n        return\n    \n    try:\n        # Start the server in a subprocess\n        import subprocess\n        import signal\n        \n        # Start server process\n        console.print(f\"[bold]Starting MCP server from {server_path}...[/bold]\")\n        server_process = subprocess.Popen(\n            [sys.executable, server_path],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        \n        # Wait for server to start\n        time.sleep(2)\n        \n        # Check if server started successfully\n        if server_process.poll() is not None:\n            console.print(\"[bold red]Error:[/bold red] Failed to start MCP server\")\n            stdout, stderr = server_process.communicate()\n            console.print(f\"[red]Server output:[/red]\\n{stdout}\\n{stderr}\")\n            return\n        \n        console.print(\"[green]MCP server started successfully[/green]\")\n        \n        # Initialize conversation\n        conversation = Conversation()\n        conversation.model = model\n        \n        # Add system prompt\n        conversation.messages.append({\n            \"role\": \"system\", \n            \"content\": \"You are an MCP client connecting to a Model Context Protocol server. \"\n                      \"Use the available tools to interact with the server and help the user.\"\n        })\n        \n        # Register MCP-specific tools\n        mcp_tools = [\n            Tool(\n                name=\"MCPGetContext\",\n                description=\"Get context from the MCP server using a prompt template\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"prompt_id\": {\n                            \"type\": \"string\",\n                            \"description\": \"ID of the prompt template to use\"\n                        },\n                        \"parameters\": {\n                            \"type\": \"object\",\n                            \"description\": \"Parameters for the prompt template\"\n                        }\n                    },\n                    \"required\": [\"prompt_id\"]\n                },\n                function=lambda prompt_id, parameters=None: _mcp_get_context(host, port, prompt_id, parameters or {})\n            ),\n            Tool(\n                name=\"MCPListPrompts\",\n                description=\"List available prompt templates from the MCP server\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {}\n                },\n                function=lambda: _mcp_list_prompts(host, port)\n            ),\n            Tool(\n                name=\"MCPGetPrompt\",\n                description=\"Get details of a specific prompt template from the MCP server\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"prompt_id\": {\n                            \"type\": \"string\",\n                            \"description\": \"ID of the prompt template to get\"\n                        }\n                    },\n                    \"required\": [\"prompt_id\"]\n                },\n                function=lambda prompt_id: _mcp_get_prompt(host, port, prompt_id)\n            )\n        ]\n        \n        # Add MCP tools to conversation\n        conversation.tools.extend(mcp_tools)\n        for tool in mcp_tools:\n            conversation.tool_map[tool.name] = tool.function\n        \n        # Main interaction loop\n        console.print(\"[bold]MCP Client ready. Type your questions or commands.[/bold]\")\n        console.print(\"[bold]Type 'exit' to quit.[/bold]\")\n        \n        while True:\n            try:\n                user_input = Prompt.ask(\"\\n[bold blue]>>[/bold blue]\")\n                \n                # Handle exit\n                if user_input.lower() in (\"exit\", \"quit\", \"/exit\", \"/quit\"):\n                    console.print(\"[bold yellow]Shutting down MCP client...[/bold yellow]\")\n                    break\n                \n                # Get response\n                conversation.get_response(user_input)\n                \n            except KeyboardInterrupt:\n                console.print(\"\\n[bold yellow]Operation cancelled by user.[/bold yellow]\")\n                if Prompt.ask(\"[bold]Exit?[/bold]\", choices=[\"y\", \"n\"], default=\"n\") == \"y\":\n                    break\n                continue\n            except Exception as e:\n                console.print(f\"[bold red]Error:[/bold red] {str(e)}\")\n        \n        # Clean up\n        console.print(\"[bold]Stopping MCP server...[/bold]\")\n        server_process.terminate()\n        server_process.wait(timeout=5)\n        \n    except Exception as e:\n        console.print(f\"[bold red]Error:[/bold red] {str(e)}\")\n\n# MCP client helper functions\ndef _mcp_get_context(host, port, prompt_id, parameters):\n    \"\"\"Get context from MCP server\"\"\"\n    try:\n        url = f\"http://{host}:{port}/context\"\n        response = requests.post(\n            url,\n            json={\n                \"prompt_id\": prompt_id,\n                \"parameters\": parameters\n            }\n        )\n        \n        if response.status_code != 200:\n            return f\"Error: {response.status_code} - {response.text}\"\n        \n        data = response.json()\n        return f\"Context (ID: {data['context_id']}):\\n\\n{data['context']}\"\n    except Exception as e:\n        return f\"Error connecting to MCP server: {str(e)}\"\n\ndef _mcp_list_prompts(host, port):\n    \"\"\"List available prompt templates from MCP server\"\"\"\n    try:\n        url = f\"http://{host}:{port}/prompts\"\n        response = requests.get(url)\n        \n        if response.status_code != 200:\n            return f\"Error: {response.status_code} - {response.text}\"\n        \n        data = response.json()\n        prompts = data.get(\"prompts\", [])\n        \n        if not prompts:\n            return \"No prompt templates available\"\n        \n        result = \"Available prompt templates:\\n\\n\"\n        for prompt in prompts:\n            result += f\"ID: {prompt['id']}\\n\"\n            result += f\"Description: {prompt['description']}\\n\"\n            result += f\"Parameters: {', '.join(prompt.get('parameters', {}).keys())}\\n\\n\"\n        \n        return result\n    except Exception as e:\n        return f\"Error connecting to MCP server: {str(e)}\"\n\ndef _mcp_get_prompt(host, port, prompt_id):\n    \"\"\"Get details of a specific prompt template\"\"\"\n    try:\n        url = f\"http://{host}:{port}/prompts/{prompt_id}\"\n        response = requests.get(url)\n        \n        if response.status_code != 200:\n            return f\"Error: {response.status_code} - {response.text}\"\n        \n        prompt = response.json()\n        \n        result = f\"Prompt Template: {prompt['id']}\\n\\n\"\n        result += f\"Description: {prompt['description']}\\n\\n\"\n        result += \"Parameters:\\n\"\n        \n        for param_name, param_info in prompt.get(\"parameters\", {}).items():\n            result += f\"- {param_name}: {param_info.get('description', '')}\\n\"\n        \n        result += f\"\\nTemplate:\\n{prompt['template']}\\n\"\n        \n        return result\n    except Exception as e:\n        return f\"Error connecting to MCP server: {str(e)}\"\n\n@app.command()\ndef mcp_multi_agent(\n    server_path: str = typer.Argument(..., help=\"Path to the MCP server script or module\"),\n    config: str = typer.Option(None, \"--config\", \"-c\", help=\"Path to agent configuration JSON file\"),\n    host: str = typer.Option(\"127.0.0.1\", \"--host\", help=\"Host address for the MCP server\"),\n    port: int = typer.Option(8000, \"--port\", \"-p\", help=\"Port for the MCP server\")\n):\n    \"\"\"\n    Start a multi-agent MCP client with multiple specialized agents\n    \n    This allows using multiple agents with different roles to collaborate\n    on complex tasks by connecting to an MCP server.\n    \"\"\"\n    # Load configuration\n    if config:\n        if not os.path.exists(config):\n            console.print(f\"[bold red]Error:[/bold red] Configuration file not found at {config}\")\n            return\n        \n        try:\n            with open(config, 'r') as f:\n                config_data = json.load(f)\n        except Exception as e:\n            console.print(f\"[bold red]Error loading configuration:[/bold red] {str(e)}\")\n            return\n    else:\n        # Default configuration\n        config_data = {\n            \"agents\": [\n                {\n                    \"name\": \"Primary\",\n                    \"role\": \"primary\",\n                    \"system_prompt\": \"You are a helpful assistant that uses an MCP server to provide information.\",\n                    \"model\": \"gpt-4o\",\n                    \"temperature\": 0.0\n                }\n            ],\n            \"coordination\": {\n                \"strategy\": \"single\",\n                \"primary_agent\": \"Primary\"\n            },\n            \"settings\": {\n                \"max_turns_per_agent\": 1,\n                \"enable_agent_reflection\": False,\n                \"enable_cross_agent_communication\": False,\n                \"enable_user_selection\": False\n            }\n        }\n    \n    # Display configuration\n    agent_names = [agent[\"name\"] for agent in config_data[\"agents\"]]\n    console.print(Panel.fit(\n        f\"[bold green]OpenAI Code Assistant Multi-Agent MCP Client[/bold green]\\n\"\n        f\"Server: {server_path}\\n\"\n        f\"Host: {host}:{port}\\n\"\n        f\"Agents: {', '.join(agent_names)}\\n\"\n        f\"Coordination: {config_data['coordination']['strategy']}\\n\"\n        f\"Primary Agent: {config_data['coordination']['primary_agent']}\",\n        title=\"Multi-Agent MCP Client Starting\",\n        border_style=\"green\"\n    ))\n    \n    # Check if server path exists\n    if not os.path.exists(server_path):\n        console.print(f\"[bold red]Error:[/bold red] Server script not found at {server_path}\")\n        return\n    \n    # Check API key\n    if not os.getenv(\"OPENAI_API_KEY\"):\n        console.print(\"[bold red]Error:[/bold red] No OpenAI API key found. Please set the OPENAI_API_KEY environment variable.\")\n        return\n    \n    try:\n        # Start the server in a subprocess\n        import subprocess\n        import signal\n        \n        # Start server process\n        console.print(f\"[bold]Starting MCP server from {server_path}...[/bold]\")\n        server_process = subprocess.Popen(\n            [sys.executable, server_path],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        \n        # Wait for server to start\n        time.sleep(2)\n        \n        # Check if server started successfully\n        if server_process.poll() is not None:\n            console.print(\"[bold red]Error:[/bold red] Failed to start MCP server\")\n            stdout, stderr = server_process.communicate()\n            console.print(f\"[red]Server output:[/red]\\n{stdout}\\n{stderr}\")\n            return\n        \n        console.print(\"[green]MCP server started successfully[/green]\")\n        \n        # Initialize agents\n        agents = {}\n        for agent_config in config_data[\"agents\"]:\n            # Create conversation for agent\n            agent = Conversation()\n            agent.model = agent_config.get(\"model\", \"gpt-4o\")\n            agent.temperature = agent_config.get(\"temperature\", 0.0)\n            \n            # Add system prompt\n            agent.messages.append({\n                \"role\": \"system\", \n                \"content\": agent_config.get(\"system_prompt\", \"You are a helpful assistant.\")\n            })\n            \n            # Register MCP-specific tools\n            mcp_tools = [\n                Tool(\n                    name=\"MCPGetContext\",\n                    description=\"Get context from the MCP server using a prompt template\",\n                    parameters={\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"prompt_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"ID of the prompt template to use\"\n                            },\n                            \"parameters\": {\n                                \"type\": \"object\",\n                                \"description\": \"Parameters for the prompt template\"\n                            }\n                        },\n                        \"required\": [\"prompt_id\"]\n                    },\n                    function=lambda prompt_id, parameters=None: _mcp_get_context(host, port, prompt_id, parameters or {})\n                ),\n                Tool(\n                    name=\"MCPListPrompts\",\n                    description=\"List available prompt templates from the MCP server\",\n                    parameters={\n                        \"type\": \"object\",\n                        \"properties\": {}\n                    },\n                    function=lambda: _mcp_list_prompts(host, port)\n                ),\n                Tool(\n                    name=\"MCPGetPrompt\",\n                    description=\"Get details of a specific prompt template from the MCP server\",\n                    parameters={\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"prompt_id\": {\n                                \"type\": \"string\",\n                                \"description\": \"ID of the prompt template to get\"\n                            }\n                        },\n                        \"required\": [\"prompt_id\"]\n                    },\n                    function=lambda prompt_id: _mcp_get_prompt(host, port, prompt_id)\n                )\n            ]\n            \n            # Add MCP tools to agent\n            agent.tools.extend(mcp_tools)\n            for tool in mcp_tools:\n                agent.tool_map[tool.name] = tool.function\n            \n            # Add agent to agents dictionary\n            agents[agent_config[\"name\"]] = {\n                \"config\": agent_config,\n                \"conversation\": agent,\n                \"history\": []\n            }\n        \n        # Get primary agent\n        primary_agent_name = config_data[\"coordination\"][\"primary_agent\"]\n        if primary_agent_name not in agents:\n            console.print(f\"[bold red]Error:[/bold red] Primary agent '{primary_agent_name}' not found in configuration\")\n            return\n        \n        # Main interaction loop\n        console.print(\"[bold]Multi-Agent MCP Client ready. Type your questions or commands.[/bold]\")\n        console.print(\"[bold]Special commands:[/bold]\")\n        console.print(\"  [blue]/agents[/blue] - List available agents\")\n        console.print(\"  [blue]/talk <agent_name> <message>[/blue] - Send message to specific agent\")\n        console.print(\"  [blue]/history[/blue] - Show conversation history\")\n        console.print(\"  [blue]/exit[/blue] - Exit the client\")\n        \n        conversation_history = []\n        \n        while True:\n            try:\n                user_input = Prompt.ask(\"\\n[bold blue]>>[/bold blue]\")\n                \n                # Handle exit\n                if user_input.lower() in (\"exit\", \"quit\", \"/exit\", \"/quit\"):\n                    console.print(\"[bold yellow]Shutting down multi-agent MCP client...[/bold yellow]\")\n                    break\n                \n                # Handle special commands\n                if user_input.startswith(\"/agents\"):\n                    console.print(\"[bold]Available Agents:[/bold]\")\n                    for name, agent_data in agents.items():\n                        role = agent_data[\"config\"][\"role\"]\n                        model = agent_data[\"config\"][\"model\"]\n                        console.print(f\"  [green]{name}[/green] ({role}, {model})\")\n                    continue\n                \n                if user_input.startswith(\"/history\"):\n                    console.print(\"[bold]Conversation History:[/bold]\")\n                    for i, entry in enumerate(conversation_history, 1):\n                        if entry[\"role\"] == \"user\":\n                            console.print(f\"[blue]{i}. User:[/blue] {entry['content']}\")\n                        else:\n                            console.print(f\"[green]{i}. {entry['agent']}:[/green] {entry['content']}\")\n                    continue\n                \n                if user_input.startswith(\"/talk \"):\n                    parts = user_input.split(\" \", 2)\n                    if len(parts) < 3:\n                        console.print(\"[yellow]Usage: /talk <agent_name> <message>[/yellow]\")\n                        continue\n                    \n                    agent_name = parts[1]\n                    message = parts[2]\n                    \n                    if agent_name not in agents:\n                        console.print(f\"[yellow]Agent '{agent_name}' not found. Use /agents to see available agents.[/yellow]\")\n                        continue\n                    \n                    # Add message to history\n                    conversation_history.append({\n                        \"role\": \"user\",\n                        \"content\": message,\n                        \"target_agent\": agent_name\n                    })\n                    \n                    # Get response from specific agent\n                    console.print(f\"[bold]Asking {agent_name}...[/bold]\")\n                    agent = agents[agent_name][\"conversation\"]\n                    response = agent.get_response(message)\n                    \n                    # Add response to history\n                    conversation_history.append({\n                        \"role\": \"assistant\",\n                        \"agent\": agent_name,\n                        \"content\": response\n                    })\n                    \n                    # Add to agent's history\n                    agents[agent_name][\"history\"].append({\n                        \"role\": \"user\",\n                        \"content\": message\n                    })\n                    agents[agent_name][\"history\"].append({\n                        \"role\": \"assistant\",\n                        \"content\": response\n                    })\n                    \n                    continue\n                \n                # Regular message - use coordination strategy\n                strategy = config_data[\"coordination\"][\"strategy\"]\n                \n                # Add message to history\n                conversation_history.append({\n                    \"role\": \"user\",\n                    \"content\": user_input\n                })\n                \n                if strategy == \"single\" or strategy == \"primary\":\n                    # Just use the primary agent\n                    agent = agents[primary_agent_name][\"conversation\"]\n                    response = agent.get_response(user_input)\n                    \n                    # Add response to history\n                    conversation_history.append({\n                        \"role\": \"assistant\",\n                        \"agent\": primary_agent_name,\n                        \"content\": response\n                    })\n                    \n                    # Add to agent's history\n                    agents[primary_agent_name][\"history\"].append({\n                        \"role\": \"user\",\n                        \"content\": user_input\n                    })\n                    agents[primary_agent_name][\"history\"].append({\n                        \"role\": \"assistant\",\n                        \"content\": response\n                    })\n                    \n                elif strategy == \"round_robin\":\n                    # Ask each agent in turn\n                    console.print(\"[bold]Consulting all agents...[/bold]\")\n                    \n                    for agent_name, agent_data in agents.items():\n                        console.print(f\"[bold]Response from {agent_name}:[/bold]\")\n                        agent = agent_data[\"conversation\"]\n                        response = agent.get_response(user_input)\n                        \n                        # Add response to history\n                        conversation_history.append({\n                            \"role\": \"assistant\",\n                            \"agent\": agent_name,\n                            \"content\": response\n                        })\n                        \n                        # Add to agent's history\n                        agent_data[\"history\"].append({\n                            \"role\": \"user\",\n                            \"content\": user_input\n                        })\n                        agent_data[\"history\"].append({\n                            \"role\": \"assistant\",\n                            \"content\": response\n                        })\n                \n                elif strategy == \"voting\":\n                    # Ask all agents and show all responses\n                    console.print(\"[bold]Collecting responses from all agents...[/bold]\")\n                    \n                    responses = {}\n                    for agent_name, agent_data in agents.items():\n                        agent = agent_data[\"conversation\"]\n                        response = agent.get_response(user_input)\n                        responses[agent_name] = response\n                        \n                        # Add to agent's history\n                        agent_data[\"history\"].append({\n                            \"role\": \"user\",\n                            \"content\": user_input\n                        })\n                        agent_data[\"history\"].append({\n                            \"role\": \"assistant\",\n                            \"content\": response\n                        })\n                    \n                    # Display all responses\n                    for agent_name, response in responses.items():\n                        console.print(f\"[bold]Response from {agent_name}:[/bold]\")\n                        console.print(response)\n                        \n                        # Add response to history\n                        conversation_history.append({\n                            \"role\": \"assistant\",\n                            \"agent\": agent_name,\n                            \"content\": response\n                        })\n                \n                else:\n                    console.print(f\"[yellow]Unknown coordination strategy: {strategy}[/yellow]\")\n                    # Default to primary agent\n                    agent = agents[primary_agent_name][\"conversation\"]\n                    response = agent.get_response(user_input)\n                    \n                    # Add response to history\n                    conversation_history.append({\n                        \"role\": \"assistant\",\n                        \"agent\": primary_agent_name,\n                        \"content\": response\n                    })\n                    \n                    # Add to agent's history\n                    agents[primary_agent_name][\"history\"].append({\n                        \"role\": \"user\",\n                        \"content\": user_input\n                    })\n                    agents[primary_agent_name][\"history\"].append({\n                        \"role\": \"assistant\",\n                        \"content\": response\n                    })\n                \n            except KeyboardInterrupt:\n                console.print(\"\\n[bold yellow]Operation cancelled by user.[/bold yellow]\")\n                if Prompt.ask(\"[bold]Exit?[/bold]\", choices=[\"y\", \"n\"], default=\"n\") == \"y\":\n                    break\n                continue\n            except Exception as e:\n                console.print(f\"[bold red]Error:[/bold red] {str(e)}\")\n        \n        # Clean up\n        console.print(\"[bold]Stopping MCP server...[/bold]\")\n        server_process.terminate()\n        server_process.wait(timeout=5)\n        \n    except Exception as e:\n        console.print(f\"[bold red]Error:[/bold red] {str(e)}\")\n\n@app.command()\ndef main(\n    model: str = typer.Option(DEFAULT_MODEL, \"--model\", \"-m\", help=\"Specify the model to use\"),\n    temperature: float = typer.Option(DEFAULT_TEMPERATURE, \"--temperature\", \"-t\", help=\"Set temperature for response generation\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\", help=\"Enable verbose output with additional information\"),\n    enable_rl: bool = typer.Option(True, \"--enable-rl/--disable-rl\", help=\"Enable/disable reinforcement learning for tool optimization\"),\n    rl_update: bool = typer.Option(False, \"--rl-update\", help=\"Manually trigger an update of the RL model\"),\n):\n    \"\"\"\n    OpenAI Code Assistant - A command-line coding assistant \n    that uses OpenAI APIs with function calling and streaming\n    \"\"\"\n    # TODO: Check for updates on startup\n    # TODO: Add environment setup verification\n    \n    # Create welcome panel with more details\n    rl_status = \"enabled\" if enable_rl else \"disabled\"\n    console.print(Panel.fit(\n        f\"[bold green]OpenAI Code Assistant[/bold green]\\n\"\n        f\"Model: {model} (Temperature: {temperature})\\n\"\n        f\"Reinforcement Learning: {rl_status}\\n\"\n        \"Type your questions or commands. Use /help for available commands.\",\n        title=\"Welcome\",\n        border_style=\"green\"\n    ))\n    \n    # Check API key\n    if not os.getenv(\"OPENAI_API_KEY\"):\n        console.print(\"[bold red]Error:[/bold red] No OpenAI API key found. Please set the OPENAI_API_KEY environment variable.\")\n        console.print(\"You can create a .env file with your API key or set it in your environment.\")\n        return\n    \n    # Initialize conversation\n    conversation = Conversation()\n    \n    # Override model and temperature if specified\n    if model != DEFAULT_MODEL:\n        conversation.model = model\n    conversation.temperature = temperature\n    \n    # Configure verbose mode\n    conversation.verbose = verbose\n    \n    # Configure RL mode\n    if not enable_rl and hasattr(conversation, 'tool_optimizer') and conversation.tool_optimizer is not None:\n        os.environ[\"ENABLE_TOOL_OPTIMIZATION\"] = \"0\"\n        conversation.tool_optimizer = None\n        console.print(\"[yellow]Reinforcement learning disabled[/yellow]\")\n    \n    # Handle manual RL update if requested\n    if rl_update and hasattr(conversation, 'tool_optimizer') and conversation.tool_optimizer is not None:\n        try:\n            with console.status(\"[bold blue]Updating RL model...[/bold blue]\"):\n                result = conversation.tool_optimizer.optimizer.update_model()\n            console.print(f\"[green]RL model update result:[/green] {result['status']}\")\n            if 'metrics' in result:\n                console.print(Panel.fit(\n                    \"\\n\".join([f\"{k}: {v}\" for k, v in result['metrics'].items()]),\n                    title=\"RL Metrics\",\n                    border_style=\"blue\"\n                ))\n        except Exception as e:\n            console.print(f\"[red]Error updating RL model:[/red] {e}\")\n    \n    # Add system prompt\n    conversation.messages.append({\"role\": \"system\", \"content\": get_system_prompt()})\n    \n    # TODO: Add context collection for file system and git information\n    # TODO: Add session persistence to allow resuming conversations\n    \n    # Main interaction loop\n    while True:\n        try:\n            user_input = Prompt.ask(\"\\n[bold blue]>>[/bold blue]\")\n            \n            # Handle exit\n            if user_input.lower() in (\"exit\", \"quit\", \"/exit\", \"/quit\"):\n                console.print(\"[bold yellow]Goodbye![/bold yellow]\")\n                break\n            \n            # Get response without wrapping it in a status indicator\n            # This allows the streaming to work properly\n            try:\n                conversation.get_response(user_input)\n            except Exception as e:\n                console.print(f\"[bold red]Error during response generation:[/bold red] {str(e)}\")\n                \n                # Provide more helpful error messages for common issues\n                if \"api_key\" in str(e).lower():\n                    console.print(\"[yellow]Hint: Check your OpenAI API key.[/yellow]\")\n                elif \"rate limit\" in str(e).lower():\n                    console.print(\"[yellow]Hint: You've hit a rate limit. Try again in a moment.[/yellow]\")\n                elif \"context_length_exceeded\" in str(e).lower() or \"maximum context length\" in str(e).lower():\n                    console.print(\"[yellow]Hint: The conversation is too long. Try using /compact to reduce its size.[/yellow]\")\n                elif \"Missing required parameter\" in str(e):\n                    console.print(\"[yellow]Hint: There's an API format issue. Try restarting the conversation.[/yellow]\")\n                \n                # Offer recovery options\n                recovery_choice = Prompt.ask(\n                    \"[bold]Would you like to:[/bold]\",\n                    choices=[\"continue\", \"debug\", \"compact\", \"restart\", \"exit\"],\n                    default=\"continue\"\n                )\n                \n                if recovery_choice == \"debug\":\n                    # Show debug information\n                    debug_info = {\n                        \"model\": conversation.model,\n                        \"temperature\": conversation.temperature,\n                        \"message_count\": len(conversation.messages),\n                        \"token_usage\": conversation.token_usage,\n                        \"conversation_id\": conversation.conversation_id,\n                        \"session_duration\": time.time() - conversation.session_start_time,\n                        \"tools_count\": len(conversation.tools),\n                        \"python_version\": sys.version,\n                        \"openai_version\": OpenAI.__version__ if hasattr(OpenAI, \"__version__\") else \"Unknown\"\n                    }\n                    console.print(Panel(json.dumps(debug_info, indent=2), title=\"Debug Information\", border_style=\"yellow\"))\n                elif recovery_choice == \"compact\":\n                    # Compact the conversation\n                    result = conversation.compact()\n                    console.print(f\"[green]{result}[/green]\")\n                elif recovery_choice == \"restart\":\n                    # Restart the conversation\n                    conversation = Conversation()\n                    conversation.model = model\n                    conversation.temperature = temperature\n                    conversation.verbose = verbose\n                    conversation.messages.append({\"role\": \"system\", \"content\": get_system_prompt()})\n                    console.print(\"[green]Conversation restarted.[/green]\")\n                elif recovery_choice == \"exit\":\n                    console.print(\"[bold yellow]Goodbye![/bold yellow]\")\n                    break\n            \n        except KeyboardInterrupt:\n            console.print(\"\\n[bold yellow]Operation cancelled by user.[/bold yellow]\")\n            # Offer options after cancellation\n            cancel_choice = Prompt.ask(\n                \"[bold]Would you like to:[/bold]\",\n                choices=[\"continue\", \"exit\"],\n                default=\"continue\"\n            )\n            if cancel_choice == \"exit\":\n                console.print(\"[bold yellow]Goodbye![/bold yellow]\")\n                break\n            continue\n        except Exception as e:\n            console.print(f\"[bold red]Unexpected error:[/bold red] {str(e)}\")\n            import traceback\n            console.print(traceback.format_exc())\n            # Ask if user wants to continue despite the error\n            if Prompt.ask(\"[bold]Continue?[/bold]\", choices=[\"y\", \"n\"], default=\"y\") == \"n\":\n                break\n\nif __name__ == \"__main__\":\n    app()\n"}
