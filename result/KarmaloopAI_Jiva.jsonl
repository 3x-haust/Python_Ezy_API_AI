{"repo_info": {"repo_name": "Jiva", "repo_owner": "KarmaloopAI", "repo_url": "https://github.com/KarmaloopAI/Jiva"}}
{"type": "source_file", "path": "actions/memory_retrieval.py", "content": "# actions/memory_retrieval.py\n\nfrom typing import Dict, Any, List\nfrom core.memory import Memory\n\nmemory: Memory = None\n\ndef set_memory(memory_instance: Memory):\n    global memory\n    memory = memory_instance\n\n\ndef retrieve_recent_memory(n: int = 5) -> List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the n most recent items from short-term memory.\n\n    Args:\n        n (int): The number of recent items to retrieve. Defaults to 5.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the most recent memory items.\n    \"\"\"\n    return memory.get_recent_short_term_memory(n)\n\ndef retrieve_task_result(task_description: str) -> Dict[str, Any]:\n    \"\"\"\n    Retrieve the result of a specific task from memory.\n\n    Args:\n        task_description (str): The unique identifier of the task whose result is to be retrieved.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the result of the task. Will always contain a key called result, value of which will be the result\n    \"\"\"\n    return memory.get_task_result(task_description)\n\nasync def retrieve_context_for_task(task_description: str, n: int = 5) -> Dict[str, Any]:\n    \"\"\"\n    Retrieve relevant context for a task from both short-term and long-term memory.\n\n    Args:\n        task_description (str): The description of the task for which context is needed.\n        n (int): The number of relevant context items to retrieve. Defaults to 5.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the relevant context for the task.\n    \"\"\"\n    return await memory.get_context_for_task(task_description, n)\n\nasync def query_long_term_memory(query: str, limit: int = 5) -> List[Dict[str, Any]]:\n    \"\"\"\n    Query long-term memory based on semantic similarity.\n\n    Args:\n        query (str): The query string to search for in long-term memory.\n        limit (int): The maximum number of results to return. Defaults to 5.\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries representing the query results from long-term memory.\n    \"\"\"\n    return await memory.query_long_term_memory(query, limit)\n\n\n# Example usage:\n# recent_memories = retrieve_recent_memory(memory_instance, 3)\n# task_result = retrieve_task_result(memory_instance, \"task_123\")\n# context = retrieve_context_for_task(memory_instance, \"Write a story about two friends\")\n# relevant_memories = query_long_term_memory(memory_instance, \"friendship stories\", 2)\n"}
{"type": "source_file", "path": "core/task_manager.py", "content": "# task_manager.py\n\nimport os\nimport re\nfrom typing import List, Dict, Any, Optional\nfrom queue import PriorityQueue\nfrom datetime import datetime\nimport uuid\nimport logging\n\nfrom core.llm_interface import LLMInterface\nfrom core.ethical_framework import EthicalFramework\nfrom core.action_manager import ActionManager\nfrom core.memory import Memory\nfrom core.prompt_manager import PromptManager\nfrom core.task_recovery import TaskAttempt, TaskRecoveryManager\n\ndef parse_int_or_default(value, default=1):\n    \"\"\"\n    Parse a string as an integer, or return a default value if parsing fails.\n    \"\"\"\n    try:\n        return int(value)\n    except (ValueError, TypeError):\n        return default\n\nclass Task:\n    def __init__(self, description: str, action: str, parameters: Dict[str, Any], \n                 priority: int = 1, deadline: Optional[datetime] = None, \n                 parent_id: Optional[str] = None, required_inputs: Dict[str, Any] = None, goal: str = None,\n                 max_attempts: int = 3):\n        self.id = str(uuid.uuid4())\n        self.description = description\n        self.action = action\n        self.parameters = parameters\n        self.original_parameters = parameters.copy()    # Retain a copy of the original parameters\n        self.priority = parse_int_or_default(priority)\n        self.deadline = deadline\n        self.created_at = datetime.now()\n        self.completed_at = None\n        self.status = \"pending\"  # pending, completed, failed, redirected, decomposed\n        self.result = None\n        self.parent_id = parent_id\n        self.subtasks: List[str] = []\n        self.ethical_evaluation: Optional[Dict[str, Any]] = None\n        self.required_inputs = required_inputs or {}\n        self.output = None\n        self.goal = goal\n        \n        # Recovery-related fields\n        self.max_attempts = max_attempts\n        self.current_attempt = 0\n        self.attempts: List[TaskAttempt] = []\n        self.recovery_attempted = False\n\n    def __lt__(self, other):\n        if not isinstance(other, Task):\n            return NotImplemented\n        if self.priority == other.priority:\n            return self.created_at < other.created_at\n        return self.priority > other.priority\n\n    def __eq__(self, other):\n        if not isinstance(other, Task):\n            return NotImplemented\n        return self.id == other.id\n        \n    def create_attempt(self) -> TaskAttempt:\n        \"\"\"Create a new attempt for this task.\"\"\"\n        self.current_attempt += 1\n        attempt = TaskAttempt(self.parameters, self.current_attempt)\n        self.attempts.append(attempt)\n        return attempt\n        \n    def get_latest_attempt(self) -> Optional[TaskAttempt]:\n        \"\"\"Get the most recent attempt, if any.\"\"\"\n        if self.attempts:\n            return self.attempts[-1]\n        return None\n        \n    def can_retry(self) -> bool:\n        \"\"\"Check if the task can be retried.\"\"\"\n        return self.current_attempt < self.max_attempts\n        \n    def get_attempt_history(self) -> List[Dict[str, Any]]:\n        \"\"\"Get the history of attempts for this task.\"\"\"\n        return [attempt.to_dict() for attempt in self.attempts]\n        \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert task to a dictionary representation.\"\"\"\n        return {\n            \"id\": self.id,\n            \"description\": self.description,\n            \"action\": self.action,\n            \"parameters\": self.parameters,\n            \"priority\": self.priority,\n            \"status\": self.status,\n            \"created_at\": self.created_at.isoformat() if self.created_at else None,\n            \"completed_at\": self.completed_at.isoformat() if self.completed_at else None,\n            \"parent_id\": self.parent_id,\n            \"subtasks\": self.subtasks,\n            \"goal\": self.goal,\n            \"current_attempt\": self.current_attempt,\n            \"max_attempts\": self.max_attempts,\n            \"attempts\": self.get_attempt_history(),\n            \"recovery_attempted\": self.recovery_attempted\n        }\n\n    def __repr__(self):\n        return f\"Task(id={self.id}, description='{self.description}', action='{self.action}', status='{self.status}', attempts={self.current_attempt}/{self.max_attempts})\"\n\nclass TaskManager:\n    def __init__(self, llm_interface: LLMInterface, ethical_framework: EthicalFramework, \n                 action_manager: ActionManager, memory: Memory, prompt_manager: PromptManager):\n        self.task_queue = PriorityQueue()\n        self.completed_tasks: List[Task] = []\n        self.all_tasks: Dict[str, Task] = {}\n        self.llm_interface = llm_interface\n        self.ethical_framework = ethical_framework\n        self.action_manager = action_manager\n        self.memory = memory\n        self.prompt_manager = prompt_manager\n        self.logger = logging.getLogger(\"Jiva.TaskManager\")\n\n        # Initialize the recovery manager\n        self.recovery_manager = TaskRecoveryManager(llm_interface, prompt_manager)\n\n    async def get_relevant_actions(self, goal: str, context: Dict[str, Any]) -> List[str]:\n        # Get available actions with their descriptions and parameters\n        available_actions = self.action_manager.get_available_actions()\n        \n        # Format the actions and their parameters for the prompt\n        action_descriptions = []\n        for action_name, action_info in available_actions.items():\n            # param_desc = action_info['description']\n            action_descriptions.append(f\"\"\"- {action_name}\\n\n            \"\"\")\n        \n        actions_str = \"\\n\\n\".join(action_descriptions)\n\n        prompt = self.prompt_manager.get_prompt(\n            \"tasks.get_relevant_actions\",\n            goal=goal,\n            context=context,\n            actions_str=actions_str\n        )\n\n        response = await self.llm_interface.generate(prompt)\n        action_names = []\n        split_result = response.split(',')\n        for action in split_result:\n            action_names.append(action.strip())\n        \n        return action_names\n\n    async def generate_tasks(self, goal: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:\n        # Get available actions with their descriptions and parameters\n        available_actions = self.action_manager.get_available_actions()\n\n        # Get actions relevant to this run.\n        relevant_actions = await self.get_relevant_actions(goal=goal, context=context)\n        # Mandatory actions fo context\n        mandatory_actions = ['think', 'replan_tasks', 'sleep', 'rerun_tasks']\n        \n        # Format the actions and their parameters for the prompt\n        action_descriptions = []\n        for action_name, action_info in available_actions.items():\n            if action_name in relevant_actions or action_name in mandatory_actions:\n                param_desc = action_info['description']\n                action_descriptions.append(f\"\"\"## {action_name}\\n\n                ### Description (docstring)\n                {param_desc}\n                \"\"\")\n        \n        actions_str = \"\\n\\n\".join(action_descriptions)\n\n        prompt = prompt = self.prompt_manager.get_prompt(\n            \"tasks.generate_tasks\",\n            goal=goal,\n            context=context,\n            actions_str=actions_str\n        )\n        \n        self.logger.debug(f\"Generating tasks with prompt: {prompt}\")\n        response = await self.llm_interface.generate(prompt)\n        self.logger.debug(f\"LLM response: {response}\")\n        \n        try:\n            tasks = self.llm_interface.parse_json(response)\n            self.logger.debug(f\"Parsed tasks: {tasks}\")\n            if not isinstance(tasks, list):\n                raise ValueError(\"Expected a list of tasks\")\n            \n            for task in tasks:\n                if not isinstance(task.get('parameters', {}), dict):\n                    task['parameters'] = {\"prompt\": str(task.get('parameters', ''))}\n            \n            processed_tasks = self.add_raw_tasks(tasks, goal)\n            return processed_tasks\n        except Exception as e:\n            self.logger.error(f\"Error parsing LLM response: {e}\")\n            return [{\"description\": f\"Analyze goal: {goal}\", \"action\": \"think\", \"parameters\": {\"prompt\": goal}, \"required_inputs\": []}]\n\n    def add_raw_tasks(self, raw_tasks: List[Dict[str, Any]], goal: str) -> List[Task]:\n        \"\"\"Add tasks from raw task dictionaries.\"\"\"\n        tasks: List[Task] = []\n        last_task_id = None\n        for raw_task in raw_tasks:\n            task = Task(**raw_task)\n            task.goal = goal\n\n            if task.action.strip().lower() == 'think':\n                last_task_id = None\n\n            task.parent_id = last_task_id if last_task_id else None\n            tasks.append(task)\n            \n            # Add task to both dictionaries and queue\n            self.all_tasks[task.id] = task\n            self.task_queue.put(task)  # Make sure tasks get into the queue\n            \n            if last_task_id and last_task_id in self.all_tasks:\n                self.all_tasks[last_task_id].subtasks.append(task.id)\n\n            if task.action.strip().lower() == 'think':\n                last_task_id = task.id\n\n        return tasks\n\n    def requeue_pending_tasks(self):\n        \"\"\"Re-queue any tasks that are pending but not in the queue.\"\"\"\n        requeued_count = 0\n        pending_tasks = [\n            task for task in self.all_tasks.values()\n            if task.status == \"pending\"\n        ]\n        \n        # Clear the queue and rebuild it with pending tasks\n        self.task_queue = PriorityQueue()\n        for task in pending_tasks:\n            self.task_queue.put(task)\n            requeued_count += 1\n            self.logger.debug(f\"Re-queued pending task: {task.id}\")\n            \n        self.logger.info(f\"Requeued {requeued_count} pending tasks\")\n        return pending_tasks\n\n    async def add_task(self, description: str, action: str, parameters: Dict[str, Any], \n                priority: int = 1, deadline: Optional[datetime] = None, \n                parent_id: Optional[str] = None, required_inputs: Dict[str, Any] = None) -> Optional[str]:\n        \"\"\"\n        Add a new task to the task queue.\n        \n        Args:\n            description: Description of the task\n            action: Name of the action to execute\n            parameters: Parameters for the action\n            priority: Task priority (higher number = higher priority)\n            deadline: Optional deadline for task completion\n            parent_id: Optional ID of parent task\n            required_inputs: Dictionary mapping parameter names to task descriptions for inputs\n            \n        Returns:\n            str: The ID of the created task, or None if creation failed\n        \"\"\"\n        # Check if the action is ethical\n        is_ethical = await self.ethical_framework.evaluate_task(description)\n        if not is_ethical:\n            self.logger.warning(f\"Task rejected as unethical: {description}\")\n            return None\n            \n        # Create a new task\n        task = Task(description, action, parameters, priority, deadline, parent_id, required_inputs)\n        self.task_queue.put(task)\n        self.all_tasks[task.id] = task\n        \n        # Add as subtask to parent if applicable\n        if parent_id and parent_id in self.all_tasks:\n            self.all_tasks[parent_id].subtasks.append(task.id)\n        \n        # Store ethical evaluation\n        ethical_explanation = await self.ethical_framework.get_ethical_explanation(description)\n        task.ethical_evaluation = {\n            \"explanation\": ethical_explanation,\n            \"is_ethical\": True\n        }\n        \n        self.logger.info(f\"Added task: {task.id} - {description}\")\n        return task.id\n\n    def get_next_task(self) -> Optional[Task]:\n        if not self.task_queue.empty():\n            return self.task_queue.get()\n        return None\n\n    def complete_task(self, task_id: str, result: Any) -> bool:\n        if task_id in self.all_tasks:\n            task = self.all_tasks[task_id]\n            task.status = \"completed\"\n            task.completed_at = datetime.now()\n            task.result = result\n            task.output = result\n            self.completed_tasks.append(task)\n            \n            # Remove from queue if it's still there\n            new_queue = PriorityQueue()\n            while not self.task_queue.empty():\n                queued_task = self.task_queue.get()\n                if queued_task.id != task_id:\n                    new_queue.put(queued_task)\n            self.task_queue = new_queue\n            \n            self.logger.info(f\"Task {task_id} completed. Remaining tasks: {self.task_queue.qsize()}\")\n            return True\n        return False\n\n    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:\n        if task_id in self.all_tasks:\n            task = self.all_tasks[task_id]\n            return {\n                \"id\": task.id,\n                \"description\": task.description,\n                \"status\": task.status,\n                \"created_at\": task.created_at,\n                \"completed_at\": task.completed_at,\n                \"priority\": task.priority,\n                \"deadline\": task.deadline,\n                \"result\": task.result,\n                \"parent_id\": task.parent_id,\n                \"subtasks\": task.subtasks,\n                \"ethical_evaluation\": task.ethical_evaluation,\n                \"action\": task.action,\n                \"parameters\": task.parameters,\n                \"required_inputs\": task.required_inputs,\n                \"output\": task.output\n            }\n        return None\n\n    async def execute_task(self, task: Task) -> Any:\n        self.logger.info(f\"Executing task: {task.description} (attempt {task.current_attempt + 1}/{task.max_attempts})\")\n        \n        try:\n            # Create a new attempt\n            attempt = task.create_attempt()\n            \n            # Ensure parameters is always a dict\n            if isinstance(task.parameters, str):\n                task.parameters = {\"prompt\": task.parameters}\n            elif task.parameters is None:\n                task.parameters = {}\n\n            # Resolve parameters based on required inputs\n            for param, required_task_desc in (task.required_inputs or {}).items():\n                input_task_result = self.get_input_task_result(required_task_desc)\n                if input_task_result is not None:\n                    # Handle special cases for different action types\n                    if isinstance(input_task_result, dict):\n                        # For generate_python_code action, extract code\n                        if 'code' in input_task_result:\n                            input_task_result = input_task_result['code']\n                        # For execute_python_code action, extract stdout\n                        elif 'stdout' in input_task_result:\n                            input_task_result = input_task_result.get('stdout', '')\n                    \n                    # Replace the placeholder in all parameters\n                    for key, value in task.parameters.items():\n                        if isinstance(value, str) and f'{{{{{param}}}}}' in str(value):\n                            task.parameters[key] = str(value).replace(f'{{{{{param}}}}}', str(input_task_result))\n                else:\n                    self.logger.warning(f\"Could not find result for required input: {required_task_desc}\")\n            \n            # Store the resolved parameters in the attempt\n            attempt.parameters = task.parameters.copy()\n\n            # Special handling for code execution\n            if task.action == \"execute_python_code\" and \"file_path\" in task.parameters:\n                # Check if the file exists and has correct Python code\n                await self._prepare_python_file_for_execution(task.parameters[\"file_path\"])\n            \n            # Execute the task\n            if task.action == 'replan_tasks':\n                new_tasks = await self.replan_tasks(task)\n                result = {\"success\": True, \"message\": f\"Replanned with {len(new_tasks)} new tasks\", \"new_tasks\": [t.id for t in new_tasks]}\n            elif task.action == 'rerun_tasks':\n                new_tasks = await self.handle_rerun_tasks(task)\n                result = {\"success\": True, \"message\": f\"Rerunning {len(new_tasks)} tasks\", \"new_tasks\": [t.id for t in new_tasks]}\n            else:\n                result = await self.action_manager.execute_action(task.action, task.parameters)\n            \n            # Record the result in the attempt\n            success = isinstance(result, dict) and result.get('success', False)\n            if not isinstance(result, dict):\n                # Convert string results to a standard format\n                if isinstance(result, str):\n                    if result.startswith(\"Error\"):\n                        success = False\n                        result = {\"success\": False, \"error\": result}\n                    else:\n                        success = True\n                        result = {\"success\": True, \"result\": result}\n                else:\n                    # For other types, wrap in a standard format\n                    success = True\n                    result = {\"success\": True, \"result\": result}\n            \n            attempt.complete(result, success)\n            \n            if success:\n                self.logger.info(f\"Task executed successfully: {task.id}\")\n                \n                # Store the result\n                task.result = result\n                task.output = result\n                self.complete_task(task.id, result)\n\n                # Store in memory\n                self.memory.add_to_short_term({\n                    \"task_id\": task.id,\n                    \"description\": task.description,\n                    \"result\": result,\n                    \"attempts\": task.current_attempt\n                })\n\n                return result\n            else:\n                # Task failed - attempt recovery\n                error_message = result.get('error', str(result))\n                self.logger.warning(f\"Task execution failed: {error_message}\")\n                \n                if task.can_retry():\n                    # Analyze the failure and get a recovery plan\n                    recovery_plan = await self.recovery_manager.analyze_failure(task, error_message)\n                    \n                    # Apply the recovery strategy\n                    recovery_applied, new_tasks = await self.recovery_manager.apply_recovery_strategy(\n                        task, recovery_plan, self\n                    )\n                    \n                    task.recovery_attempted = True\n                    \n                    if recovery_applied:\n                        if recovery_plan.get('strategy') == 'RETRY':\n                            # The task will be retried with updated parameters\n                            self.logger.info(f\"Task {task.id} will be retried with updated parameters\")\n                            \n                            # Put the task back in the queue for retry\n                            self.task_queue.put(task)\n                            \n                            # Return info about the retry\n                            return {\n                                \"success\": False, \n                                \"error\": error_message,\n                                \"recovery\": {\n                                    \"strategy\": \"RETRY\",\n                                    \"attempt\": task.current_attempt,\n                                    \"max_attempts\": task.max_attempts\n                                }\n                            }\n                        else:\n                            # For other strategies, the original task won't be retried\n                            self.logger.info(f\"Task {task.id} recovery applied: {recovery_plan.get('strategy')} with {len(new_tasks)} new tasks\")\n                            \n                            # Store the result with recovery info\n                            task.result = {\n                                \"success\": False,\n                                \"error\": error_message,\n                                \"recovery\": {\n                                    \"strategy\": recovery_plan.get('strategy'),\n                                    \"reason\": recovery_plan.get('reason'),\n                                    \"new_tasks\": [t.id for t in new_tasks] if new_tasks else []\n                                }\n                            }\n                            \n                            return task.result\n                    else:\n                        # Recovery couldn't be applied, mark as failed\n                        self.logger.warning(f\"Recovery could not be applied for task {task.id}\")\n                        \n                        task.status = \"failed\"\n                        task.result = {\n                            \"success\": False,\n                            \"error\": error_message,\n                            \"recovery_failed\": True\n                        }\n                        \n                        return task.result\n                else:\n                    # No more retry attempts, mark as failed\n                    self.logger.warning(f\"Task {task.id} failed after {task.current_attempt} attempts\")\n                    \n                    task.status = \"failed\"\n                    task.result = {\n                        \"success\": False,\n                        \"error\": error_message,\n                        \"max_attempts_reached\": True\n                    }\n                    \n                    return task.result\n                \n        except Exception as e:\n            self.logger.error(f\"Error executing task: {str(e)}\", exc_info=True)\n            \n            # If an attempt was created, record the failure\n            if task.attempts and task.attempts[-1].end_time is None:\n                task.attempts[-1].complete(\n                    {\"success\": False, \"error\": f\"Exception: {str(e)}\"}, \n                    False\n                )\n            \n            # Always return a structured error result\n            return {\n                \"success\": False,\n                \"error\": f\"Exception executing task: {str(e)}\"\n            }\n            \n    async def _prepare_python_file_for_execution(self, file_path: str) -> bool:\n        \"\"\"\n        Prepare a Python file for execution by checking its format and fixing if necessary.\n        \n        Args:\n            file_path: Path to the Python file\n            \n        Returns:\n            bool: True if file is ready for execution, False otherwise\n        \"\"\"\n        if not os.path.exists(file_path):\n            self.logger.warning(f\"Python file does not exist: {file_path}\")\n            return False\n        \n        try:\n            # Read the file content\n            with open(file_path, 'r') as f:\n                content = f.read()\n            \n            # Check if the content is a serialized dictionary (which would indicate an error)\n            if content.strip().startswith('{') and ('code' in content or 'success' in content):\n                # Try to extract the actual Python code\n                code = None\n                \n                # If it looks like a JSON object with a 'code' field\n                import re\n                import json\n                \n                # First try direct JSON parsing\n                try:\n                    data = json.loads(content)\n                    if isinstance(data, dict) and 'code' in data:\n                        code = data['code']\n                except json.JSONDecodeError:\n                    # If that fails, try regex\n                    code_match = re.search(r\"'code':\\s*'([^']+)'\", content)\n                    if code_match:\n                        code = code_match.group(1).replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n                \n                if code:\n                    # Write the extracted code back to the file\n                    with open(file_path, 'w') as f:\n                        f.write(code)\n                    self.logger.info(f\"Fixed malformed Python file: {file_path}\")\n                    return True\n                else:\n                    self.logger.error(f\"Could not extract Python code from: {file_path}\")\n                    return False\n            \n            # Check if it's valid Python syntax\n            try:\n                import ast\n                ast.parse(content)\n                return True\n            except SyntaxError:\n                self.logger.warning(f\"Python file contains syntax errors: {file_path}\")\n                return False\n                \n        except Exception as e:\n            self.logger.error(f\"Error preparing Python file: {str(e)}\")\n            return False\n    \n    async def handle_task_error(self, task: Task, error_message: str) -> None:\n        \"\"\"Handle task execution errors by generating new tasks or providing solutions.\"\"\"\n        self.logger.info(f\"Handling error for task: {task.id}\")\n        prompt = self.prompt_manager.get_prompt(\n            \"tasks.handle_task_error\",\n            task_description=task.description,\n            action=task.action,\n            parameters=task.parameters,\n            error_message=error_message\n        )\n        try:\n            solution = await self.llm_interface.generate(prompt)\n            parsed_solution = self.llm_interface.parse_json(solution)\n            \n            if isinstance(parsed_solution, dict) and \"new_tasks\" in parsed_solution:\n                new_tasks = parsed_solution[\"new_tasks\"]\n                tasks_added = self.add_raw_tasks(new_tasks, task.goal)\n                self.logger.info(f\"Added {len(tasks_added)} new tasks to handle error\")\n            else:\n                self.logger.warning(\"Could not generate new tasks from solution\")\n            \n            # Mark the original task as failed\n            task.status = \"failed\"\n            task.result = {\n                \"error\": error_message,\n                \"solution_attempted\": True\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"Error handling task failure: {str(e)}\")\n            # Mark the original task as failed\n            task.status = \"failed\"\n            task.result = {\n                \"error\": error_message,\n                \"solution_attempted\": False\n            }\n\n    async def replan_tasks(self, task: Task):\n        \"\"\"\n        Replans all tasks to achieve goal state.\n        \"\"\"\n        current_tasks = []\n        for task_id in self.all_tasks:\n            t = self.all_tasks[task_id]\n            if t.goal == task.goal:\n                current_tasks.append({\n                    \"description\": t.description,\n                    \"action\": t.action,\n                    \"parameters\": t.parameters,\n                    \"required_inputs\": t.required_inputs,\n                    \"result\": str(t.result)[:100]\n                })\n        \n        context = {\n            \"previous_tasks\": current_tasks\n        }\n\n        prompt = self.prompt_manager.get_prompt(\n            \"tasks.replan_tasks\",\n            goal=task.goal\n        )\n\n        new_tasks = await self.generate_tasks(prompt, context)\n        return new_tasks\n\n    async def handle_rerun_tasks(self, task: Task) -> List[Task]:\n        \"\"\"\n        Handle rerun_tasks action by evaluating if iteration is needed and recreating all tasks\n        from the specified start point up to the current task.\n        \"\"\"\n        # Find the starting task (point A) by matching description from parameters\n        start_task_name = task.parameters.get(\"task_name\")\n        if not start_task_name:\n            self.logger.error(\"No task name provided for rerun_tasks\")\n            return []\n\n        # Get all tasks with the same goal, ordered by creation time\n        goal_tasks = sorted(\n            [t for t in self.all_tasks.values() if t.goal == task.goal],\n            key=lambda x: x.created_at\n        )\n\n        # Find the starting task and current task indices\n        start_idx = None\n        current_idx = None\n        for idx, t in enumerate(goal_tasks):\n            if t.description == start_task_name:\n                start_idx = idx\n            if t.id == task.id:  # This is our current rerun_tasks task (point B)\n                current_idx = idx\n                break\n\n        if start_idx is None or current_idx is None:\n            self.logger.error(f\"Could not find task sequence for rerun_tasks: start={start_task_name}\")\n            return []\n\n        # Get the execution history of the starting task\n        previous_runs = [t for t in goal_tasks[:current_idx] \n                        if t.description == start_task_name]\n        \n        timestamps = [t.created_at.isoformat() for t in previous_runs]\n        current_time = datetime.now().isoformat()\n        \n        prompt = f\"\"\"Goal: {task.goal}\n    Previous executions of '{start_task_name}': {timestamps}\n    Current time: {current_time}\n\n    Should another iteration of these tasks be executed? Consider the time elapsed between runs.\n    Respond with only 'yes' or 'no'.\"\"\"\n\n        should_rerun = await self.llm_interface.generate(prompt)\n        \n        if should_rerun.strip().lower() == 'yes':\n            # Create new tasks for all tasks from start_idx to current_idx (exclusive)\n            new_tasks = []\n            for original_task in goal_tasks[start_idx:current_idx+1]:\n                new_task = Task(\n                    description=original_task.description,\n                    action=original_task.action,\n                    parameters=original_task.original_parameters.copy(),\n                    required_inputs=original_task.required_inputs.copy(),\n                    goal=task.goal,\n                    priority=original_task.priority\n                )\n                self.all_tasks[new_task.id] = new_task\n                self.task_queue.put(new_task)\n                new_tasks.append(new_task)\n                \n            self.logger.info(f\"Rerunning {len(new_tasks)} tasks from '{start_task_name}'\")\n            return new_tasks\n                \n        self.logger.info(f\"No rerun needed for tasks starting with '{start_task_name}'\")\n        return []\n\n    def get_input_task_result(self, task_description: str) -> Any:\n        \"\"\"\n        Find the result of a task based on its exact description.\n        \n        Args:\n            task_description (str): The description of the task to find\n            \n        Returns:\n            Any: The result of the task, or None if not found\n        \"\"\"\n        # First, try to find an exact match\n        for task in reversed(list(self.all_tasks.values())):  # Start from the most recent task\n            if task.description.strip() == task_description.strip():\n                return self._extract_useful_result(task.result)\n        \n        # If no exact match, try to find a partial match\n        for task in reversed(list(self.all_tasks.values())):\n            if task_description.strip() in task.description.strip():\n                return self._extract_useful_result(task.result)\n        \n        self.logger.warning(f\"Could not find task result for: {task_description}\")\n        return None\n\n    def _extract_useful_result(self, result: Any) -> Any:\n        \"\"\"\n        Extract the most useful part of a task result.\n        \n        Args:\n            result: The raw task result\n            \n        Returns:\n            Any: The extracted useful part of the result\n        \"\"\"\n        if result is None:\n            return None\n            \n        # If the result is a dictionary, try to extract useful parts\n        if isinstance(result, dict):\n            # For code generation\n            if 'code' in result:\n                return result['code']\n            \n            # For successful execution\n            if result.get('success', False) and 'stdout' in result:\n                return result['stdout']\n                \n            # For result value\n            if 'result' in result:\n                return result['result']\n                \n            # For error messages\n            if 'error' in result:\n                return f\"Error: {result['error']}\"\n        \n        # Return the raw result if no special handling needed\n        return result\n\n    def has_pending_tasks(self) -> bool:\n        \"\"\"Check for any pending tasks and ensure they're in the queue.\"\"\"\n        pending_tasks = self.get_pending_tasks()\n        if pending_tasks:\n            # Requeue if we have pending tasks\n            self.requeue_pending_tasks()\n        return bool(pending_tasks)\n    \n    def get_pending_tasks(self) -> List[Task]:\n        \"\"\"Get all pending tasks from all_tasks.\"\"\"\n        return [\n            task for task in self.all_tasks.values()\n            if task.status == \"pending\"\n        ]\n\n    def get_pending_task_count(self) -> int:\n        return self.task_queue.qsize()\n\n    def log_task_queue_state(self):\n        tasks = list(self.task_queue.queue)\n        self.logger.info(f\"Current task queue state:\")\n        for i, task in enumerate(tasks):\n            self.logger.info(f\"  {i+1}. ID: {task.id}, Description: {task.description}, Priority: {task.priority}\")\n\nif __name__ == \"__main__\":\n    # This allows us to run some basic tests\n    from unittest.mock import MagicMock\n\n    # Mock dependencies\n    llm_interface = MagicMock()\n    ethical_framework = MagicMock()\n    action_manager = MagicMock()\n    memory = MagicMock()\n\n    # Set up logging\n    logging.basicConfig(level=logging.INFO)\n\n    # Create TaskManager instance\n    task_manager = TaskManager(llm_interface, ethical_framework, action_manager, memory)\n\n    # Test task generation\n    llm_interface.generate.return_value = '[{\"description\": \"Test task\", \"priority\": 3, \"action\": \"think\", \"parameters\": {\"prompt\": \"Test prompt\"}, \"required_inputs\": []}]'\n    llm_interface.parse_json.return_value = [{\"description\": \"Test task\", \"priority\": 3, \"action\": \"think\", \"parameters\": {\"prompt\": \"Test prompt\"}, \"required_inputs\": []}]\n    ethical_framework.evaluate_task.return_value = True\n    ethical_framework.get_ethical_explanation.return_value = \"Ethical explanation\"\n\n    task_ids = task_manager.generate_tasks(\"Test goal\", {})\n    print(f\"Generated task IDs: {task_ids}\")\n\n    # Test task execution\n    action_manager.execute_action.return_value = \"Task result\"\n    task = task_manager.get_next_task()\n    if task:\n        result = task_manager.execute_task(task)\n        print(f\"Task execution result: {result}\")\n\n    # Test task queue state logging\n    task_manager.log_task_queue_state()\n\n    print(\"TaskManager tests completed.\")\n"}
{"type": "source_file", "path": "actions/file_operations.py", "content": "import asyncio\nimport os\nimport json\nimport csv\nfrom typing import Dict, Any, List\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef expand_user_path(file_path: str) -> str:\n    \"\"\"\n    Expand a user's home directory symbol (e.g., \"~/\") to a full path, or return the original path if it's a simple filename.\n\n    Args:\n        file_path (str): The path to expand.\n\n    Returns:\n        str: The expanded file path.\n    \"\"\"\n    if os.path.basename(file_path) == file_path:\n        return file_path\n    return os.path.expanduser(file_path)\n\ndef ensure_directory_exists(file_path: str) -> None:\n    \"\"\"\n    Ensure that the directory for the given file path exists. Creates directories if they do not exist.\n\n    Args:\n        file_path (str): The file path for which the directory should be ensured.\n    \"\"\"\n    directory = os.path.dirname(file_path)\n    if directory:\n        os.makedirs(directory, exist_ok=True)\n\nasync def read_file(file_path: str) -> str:\n    \"\"\"\n    Read and return the contents of a file.\n\n    Args:\n        file_path (str): The path to the file to be read.\n\n    Returns:\n        str: The content of the file, or an error message if the file could not be read.\n    \"\"\"\n    try:\n        full_path = os.path.abspath(expand_user_path(file_path))\n        with open(full_path, 'r') as file:\n            content = file.read()\n        logger.info(f\"File read successfully: {full_path}\")\n        return content\n    except FileNotFoundError:\n        logger.error(f\"File not found: {full_path}\")\n        return f\"Error: File not found at {full_path}\"\n    except IOError as e:\n        logger.error(f\"Error reading file {full_path}: {str(e)}\")\n        return f\"Error reading file: {str(e)}\"\n\nasync def write_file(file_path: str, content: Any) -> Dict[str, Any]:\n    \"\"\"\n    Write content to a file. Handles structured data appropriately.\n    \n    Args:\n        file_path (str): The path to the file where the content will be written.\n        content (Any): The content to write to the file.\n        \n    Returns:\n        Dict[str, Any]: A result indicating success or failure with details\n    \"\"\"\n    try:\n        # Process the content based on its type\n        processed_content = \"\"\n        \n        if isinstance(content, dict):\n            # Handle dictionary result from other actions\n            if 'code' in content:\n                # For code generation results\n                processed_content = content['code']\n            elif 'stdout' in content:\n                # For execution results\n                processed_content = content['stdout']\n            elif 'result' in content:\n                # For general results\n                processed_content = str(content['result'])\n            elif 'error' in content:\n                # For error results\n                processed_content = f\"Error: {content['error']}\"\n            else:\n                # For other dictionaries, use JSON\n                processed_content = json.dumps(content, indent=2)\n        else:\n            # For non-dictionary content\n            processed_content = str(content)\n        \n        # Create the full path and ensure directory exists\n        full_path = os.path.abspath(expand_user_path(file_path))\n        ensure_directory_exists(full_path)\n        \n        # Write the content\n        with open(full_path, 'w') as file:\n            file.write(processed_content)\n        \n        logger.info(f\"File written successfully: {full_path}\")\n        return {\n            \"success\": True,\n            \"message\": f\"File written successfully: {full_path}\",\n            \"file_path\": full_path,\n            \"content_length\": len(processed_content)\n        }\n    except Exception as e:\n        logger.error(f\"Error writing to file {file_path}: {str(e)}\")\n        return {\n            \"success\": False,\n            \"error\": f\"Error writing to file: {str(e)}\"\n        }\n\nasync def append_file(file_path: str, content: str) -> str:\n    \"\"\"\n    Append the given content to the end of a file if the file already exists.\n\n    Args:\n        file_path (str): The path to the file where the content will be appended.\n        content (str): The content to append to the file.\n\n    Returns:\n        str: A success message or an error message if the content could not be appended.\n    \"\"\"\n    try:\n        full_path = os.path.abspath(expand_user_path(file_path))\n        ensure_directory_exists(full_path)\n        with open(full_path, 'a') as file:\n            file.write(content)\n        logger.info(f\"Content appended successfully to: {full_path}\")\n        return f\"Content appended successfully to: {full_path}\"\n    except IOError as e:\n        logger.error(f\"Error appending to file {full_path}: {str(e)}\")\n        return f\"Error appending to file: {str(e)}\"\n\nasync def delete_file(file_path: str) -> str:\n    \"\"\"\n    Delete the specified file.\n\n    Args:\n        file_path (str): The path to the file to be deleted.\n\n    Returns:\n        str: A success message or an error message if the file could not be deleted.\n    \"\"\"\n    try:\n        full_path = os.path.abspath(expand_user_path(file_path))\n        os.remove(full_path)\n        logger.info(f\"File deleted successfully: {full_path}\")\n        return f\"File deleted successfully: {full_path}\"\n    except FileNotFoundError:\n        logger.error(f\"File not found: {full_path}\")\n        return f\"File not found: {full_path}\"\n    except IOError as e:\n        logger.error(f\"Error deleting file {full_path}: {str(e)}\")\n        return f\"Error deleting file: {str(e)}\"\n\nasync def list_directory(directory_path: str) -> List[str]:\n    \"\"\"\n    List all contents of the specified directory.\n\n    Args:\n        directory_path (str): The path to the directory to be listed.\n\n    Returns:\n        List[str]: A list of filenames in the directory, or an error message if the directory could not be listed.\n    \"\"\"\n    try:\n        full_path = os.path.abspath(expand_user_path(directory_path))\n        contents = os.listdir(full_path)\n        logger.info(f\"Directory listed successfully: {full_path}\")\n        return contents\n    except FileNotFoundError:\n        logger.error(f\"Directory not found: {full_path}\")\n        return [f\"Error: Directory not found at {full_path}\"]\n    except IOError as e:\n        logger.error(f\"Error listing directory {full_path}: {str(e)}\")\n        return [f\"Error listing directory: {str(e)}\"]\n\nasync def create_directory(directory_path: str) -> str:\n    \"\"\"\n    Create a new directory at the specified path.\n\n    Args:\n        directory_path (str): The path where the directory should be created.\n\n    Returns:\n        str: A success message or an error message if the directory could not be created.\n    \"\"\"\n    try:\n        full_path = os.path.abspath(expand_user_path(directory_path))\n        os.makedirs(full_path, exist_ok=True)\n        logger.info(f\"Directory created successfully: {full_path}\")\n        return f\"Directory created successfully: {full_path}\"\n    except IOError as e:\n        logger.error(f\"Error creating directory {full_path}: {str(e)}\")\n        return f\"Error creating directory: {str(e)}\"\n\nasync def read_json(file_path: str) -> Dict[str, Any]:\n    \"\"\"\n    Read a JSON file and return its contents as a dictionary.\n\n    Args:\n        file_path (str): The path to the JSON file.\n\n    Returns:\n        Dict[str, Any]: The content of the JSON file as a dictionary, or an error message if the file could not be read.\n    \"\"\"\n    try:\n        full_path = os.path.abspath(expand_user_path(file_path))\n        with open(full_path, 'r') as file:\n            data = json.load(file)\n        logger.info(f\"JSON file read successfully: {full_path}\")\n        return data\n    except FileNotFoundError:\n        logger.error(f\"JSON file not found: {full_path}\")\n        return {\"error\": f\"File not found at {full_path}\"}\n    except json.JSONDecodeError:\n        logger.error(f\"Invalid JSON in file: {full_path}\")\n        return {\"error\": f\"Invalid JSON in file: {full_path}\"}\n    except IOError as e:\n        logger.error(f\"Error reading JSON file {full_path}: {str(e)}\")\n        return {\"error\": f\"Error reading JSON file: {str(e)}\"}\n\nasync def write_json(file_path: str, data: Dict[str, Any]) -> str:\n    \"\"\"\n    Write the given data to a JSON file.\n\n    Args:\n        file_path (str): The path to the JSON file.\n        data (Dict[str, Any]): The data to be written to the file.\n\n    Returns:\n        str: A success message or an error message if the file could not be written.\n    \"\"\"\n    try:\n        full_path = os.path.abspath(expand_user_path(file_path))\n        ensure_directory_exists(full_path)\n        with open(full_path, 'w') as file:\n            json.dump(data, file, indent=2)\n        logger.info(f\"JSON file written successfully: {full_path}\")\n        return f\"JSON file written successfully: {full_path}\"\n    except IOError as e:\n        logger.error(f\"Error writing JSON file {full_path}: {str(e)}\")\n        return f\"Error writing JSON file: {str(e)}\"\n\nasync def read_csv(file_path: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Read a CSV file and return its contents as a list of dictionaries.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        List[Dict[str, Any]]: The content of the CSV file as a list of dictionaries, or an error message if the file could not be read.\n    \"\"\"\n    try:\n        full_path = os.path.abspath(expand_user_path(file_path))\n        with open(full_path, 'r') as file:\n            reader = csv.DictReader(file)\n            data = list(reader)\n        logger.info(f\"CSV file read successfully: {full_path}\")\n        return data\n    except FileNotFoundError:\n        logger.error(f\"CSV file not found: {full_path}\")\n        return [{\"error\": f\"File not found at {full_path}\"}]\n    except csv.Error as e:\n        logger.error(f\"Error reading CSV file {full_path}: {str(e)}\")\n        return [{\"error\": f\"Error reading CSV file: {str(e)}\"}]\n    except IOError as e:\n        logger.error(f\"Error reading CSV file {full_path}: {str(e)}\")\n        return [{\"error\": f\"Error reading CSV file: {str(e)}\"}]\n\nasync def write_csv(file_path: str, data: List[Dict[str, Any]]) -> str:\n    \"\"\"\n    Write a list of dictionaries to a CSV file.\n\n    Args:\n        file_path (str): The path to the CSV file.\n        data (List[Dict[str, Any]]): The data to be written to the file.\n\n    Returns:\n        str: A success message or an error message if the file could not be written.\n    \"\"\"\n    if not data:\n        logger.warning(f\"No data to write to CSV file: {file_path}\")\n        return f\"No data to write to CSV file: {file_path}\"\n    \n    try:\n        full_path = os.path.abspath(expand_user_path(file_path))\n        ensure_directory_exists(full_path)\n        fieldnames = data[0].keys()\n        with open(full_path, 'w', newline='') as file:\n            writer = csv.DictWriter(file, fieldnames=fieldnames)\n            writer.writeheader()\n            writer.writerows(data)\n        logger.info(f\"CSV file written successfully: {full_path}\")\n        return f\"CSV file written successfully: {full_path}\"\n    except IOError as e:\n        logger.error(f\"Error writing CSV file {full_path}: {str(e)}\")\n        return f\"Error writing CSV file: {str(e)}\"\n\n\nasync def main():\n    # Test the functions\n    print(await create_directory(\"test_dir\"))\n    print(await write_file(\"test_dir/test.txt\", \"Hello, World!\"))\n    print(await read_file(\"test_dir/test.txt\"))\n    print(await append_file(\"test_dir/test.txt\", \"\\nAppended content\"))\n    print(await read_file(\"test_dir/test.txt\"))\n    print(await list_directory(\"test_dir\"))\n    print(await write_json(\"test_dir/test.json\", {\"key\": \"value\"}))\n    print(await read_json(\"test_dir/test.json\"))\n    print(await write_csv(\"test_dir/test.csv\", [{\"name\": \"John\", \"age\": \"30\"}, {\"name\": \"Jane\", \"age\": \"25\"}]))\n    print(await read_csv(\"test_dir/test.csv\"))\n    print(await delete_file(\"test_dir/test.txt\"))\n    print(await delete_file(\"test_dir/test.json\"))\n    print(await delete_file(\"test_dir/test.csv\"))\n    print(await list_directory(\"test_dir\"))\n\nif __name__ == \"__main__\":\n    # Set up logging for testing\n    logging.basicConfig(level=logging.INFO)\n    \n    asyncio.run(main())\n"}
{"type": "source_file", "path": "sensors/chat_interface.py", "content": "# sensors/chat_interface.py\n\nimport asyncio\nimport sys\nimport select\nfrom typing import Any, Dict, Optional\nfrom .sensor_base import Sensor\n\nclass ChatInterface(Sensor):\n    def __init__(self, config: Dict[str, Any]):\n        super().__init__(config)\n        self.prompt = config.get('prompt', \"Jiva> \")\n        print(self.prompt, end='', flush=True)  # Initial prompt\n\n    async def get_input(self) -> Optional[str]:\n        \"\"\"Non-blocking check for input from CLI.\"\"\"\n        if sys.platform == 'win32':\n            # Windows implementation\n            return await self._get_input_windows()\n        else:\n            # Unix-like implementation\n            return await self._get_input_unix()\n\n    async def _get_input_unix(self) -> Optional[str]:\n        # Check if there's data available to read from stdin\n        if select.select([sys.stdin], [], [], 0)[0]:\n            line = sys.stdin.readline().strip()\n            print(self.prompt, end='', flush=True)  # Print new prompt\n            return line\n        return None\n\n    async def _get_input_windows(self) -> Optional[str]:\n        # Windows doesn't support select on stdin\n        # Use a thread to check for input\n        try:\n            input_ready = False\n\n            def check_input():\n                return sys.stdin in select.select([sys.stdin], [], [], 0)[0]\n\n            # Run input check in thread pool\n            loop = asyncio.get_event_loop()\n            input_ready = await loop.run_in_executor(None, check_input)\n\n            if input_ready:\n                line = await loop.run_in_executor(None, sys.stdin.readline)\n                print(self.prompt, end='', flush=True)  # Print new prompt\n                return line.strip()\n        except Exception:\n            pass\n        return None\n\n    async def process_input(self, input_data: str) -> Dict[str, Any]:\n        \"\"\"Process the raw input into a structured format.\"\"\"\n        if not input_data:  # Handle empty input\n            return {}\n            \n        return {\n            \"type\": \"chat\",\n            \"content\": input_data,\n            \"timestamp\": self.get_timestamp()\n        }\n\n    def get_timestamp(self) -> str:\n        from datetime import datetime\n        return datetime.now().isoformat()\n"}
{"type": "source_file", "path": "actions/action_registry.py", "content": "# actions/action_registry.py\n\nfrom typing import Dict, Callable, Any\nfrom core.llm_interface import LLMInterface\nfrom core.memory import Memory\n\n# Import actions with their existing docstrings\nfrom actions.file_operations import (\n    read_file, write_file, append_file, delete_file,\n    list_directory, create_directory,\n    read_json, write_json,\n    read_csv, write_csv\n)\nimport actions.python_coder as py\nimport actions.memory_retrieval as mem\nimport actions.think as think\n\nimport actions.web_interface as wi\n\ndef get_action_registry(llm_interface: LLMInterface, memory: Memory) -> Dict[str, Callable]:\n    \"\"\"\n    Get the registry of all available actions.\n\n    Args:\n        llm_interface (LLMInterface): The language model interface.\n        memory (Memory): The memory interface.\n\n    Returns:\n        Dict[str, Callable]: A dictionary mapping action names to their corresponding functions.\n    \"\"\"\n    wi.set_llm_interface(llm=llm_interface)\n    think.set_llm_interface(llm=llm_interface)\n    py.set_llm_interface(llm=llm_interface)\n    mem.set_memory(memory_instance=memory)\n\n    actions = {\n        # File operations\n        \"read_file\": read_file,\n        \"write_file\": write_file,\n        \"append_file\": append_file,\n        \"delete_file\": delete_file,\n        \"list_directory\": list_directory,\n        \"create_directory\": create_directory,\n        \"read_json\": read_json,\n        \"write_json\": write_json,\n        \"read_csv\": read_csv,\n        \"write_csv\": write_csv,\n        \n        # Python coding actions\n        \"generate_python_code\": py.generate_python_code,\n        \"write_python_code\": py.write_python_code,\n        \"execute_python_code\": py.execute_python_code,\n        \"analyze_python_code\": py.analyze_python_code,\n        \"test_python_function\": py.test_python_function,\n        \n        # Memory operations\n        # \"retrieve_recent_memory\": lambda n: retrieve_recent_memory(memory, n),\n        # \"retrieve_task_result\": lambda task_description: retrieve_task_result(memory, task_description),\n        # \"retrieve_context_for_task\": lambda task_description, n=5: retrieve_context_for_task(memory, task_description, n),\n        \"query_long_term_memory\": mem.query_long_term_memory,\n        \n        # Think action\n        \"think\": think.think,\n        \"replan_tasks\": think.replan_tasks,\n        \"sleep\": think.sleep,\n        \"rerun_tasks\": think.rerun_tasks,\n\n        # Web search actions\n        \"web_search\": wi.web_search,\n        \"visit_page\": wi.visit_page,\n        \"find_links\": wi.find_links,\n    }\n\n\n    return actions\n"}
{"type": "source_file", "path": "models/embeddings.py", "content": "# models/embeddings.py\n\nimport json\nimport requests\nfrom typing import List\n\nOLLAMA_API_BASE_URL = \"http://localhost:11434/api\"\nEMBED_MODEL = \"mxbai-embed-large\"\n\ndef get_embedding(text: str) -> List[float]:\n    \"\"\"\n    Get the embedding for a given text using Ollama's mxbai-embed-large model.\n    \n    Args:\n    text (str): The input text to be embedded.\n    \n    Returns:\n    List[float]: The embedding vector.\n    \"\"\"\n    url = f\"{OLLAMA_API_BASE_URL}/embeddings\"\n    \n    payload = json.dumps({\n        \"model\": EMBED_MODEL,\n        \"prompt\": text\n    })\n    \n    headers = {\n        'Content-Type': 'application/json'\n    }\n    \n    response = requests.post(url, headers=headers, data=payload)\n    response.raise_for_status()  # Raise an exception for HTTP errors\n    \n    result = response.json()\n    return result['embedding']\n\nif __name__ == \"__main__\":\n    # This allows us to run some basic tests\n    test_text = \"Hello, world!\"\n    embedding = get_embedding(test_text)\n    print(f\"Embedding for '{test_text}':\")\n    print(f\"Vector length: {len(embedding)}\")\n    print(f\"First few values: {embedding[:5]}\")\n"}
{"type": "source_file", "path": "llm_providers/mistral_ai_provider.py", "content": "from typing import List, Dict, Any\nfrom mistralai import Mistral\nfrom .base_provider import BaseLLMProvider\n\nclass MistralAIProvider(BaseLLMProvider):\n    def __init__(self, config: Dict[str, Any]):\n        self.client = Mistral(api_key=config.get('api_key'))\n        self.model = config.get('model', 'mistral-small-latest')\n\n    async def generate(self, prompt: str) -> str:\n        try:\n            response = await self.client.chat.complete_async(\n                model=self.model,\n                messages=[\n                    {\n                        \"content\": prompt,\n                        \"role\": \"user\",\n                    },\n                ],\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            raise Exception(f\"Error generating response: {str(e)}\")\n\n    async def get_embedding(self, text: str) -> List[float]:\n        try:\n            response = await self.client.embeddings.create_async(\n                model=\"mistral-embed\",\n                input=text\n            )\n            return response.data[0].embedding\n        except Exception as e:\n            raise Exception(f\"Error generating embedding: {str(e)}\")\n"}
{"type": "source_file", "path": "llm_providers/ollama_provider.py", "content": "import json\nimport aiohttp\nfrom typing import List, Dict, Any\nfrom .base_provider import BaseLLMProvider\n\nclass OllamaProvider(BaseLLMProvider):\n    def __init__(self, config: Dict[str, Any]):\n        self.api_base_url = config.get('api_base_url', 'http://localhost:11434/api')\n        self.model = config.get('model', 'gemma')\n        self.max_retries = config.get('max_retries', 3)\n        self.timeout = config.get('timeout', 60)\n\n    async def generate(self, prompt: str) -> str:\n        url = f\"{self.api_base_url}/generate\"\n        payload = json.dumps({\n            \"model\": self.model,\n            \"prompt\": prompt,\n            \"stream\": False\n        })\n        headers = {'Content-Type': 'application/json'}\n        async with aiohttp.ClientSession() as session:\n            async with session.post(url, headers=headers, data=payload, timeout=self.timeout) as response:\n                response.raise_for_status()\n                result = await response.json()\n                return result['response']\n\n    async def get_embedding(self, text: str) -> List[float]:\n        url = f\"{self.api_base_url}/embeddings\"\n        payload = json.dumps({\n            \"model\": self.model,\n            \"prompt\": text\n        })\n        headers = {'Content-Type': 'application/json'}\n        async with aiohttp.ClientSession() as session:\n            async with session.post(url, headers=headers, data=payload, timeout=self.timeout) as response:\n                response.raise_for_status()\n                result = await response.json()\n                return result['embedding']\n"}
{"type": "source_file", "path": "core/ethical_framework.py", "content": "# core/ethical_framework.py\n\nfrom typing import List, Dict, Any, Union\nimport logging\nfrom core.llm_interface import LLMInterface\nfrom core.prompt_manager import PromptManager\n\nclass EthicalFramework:\n    def __init__(self, llm_interface: LLMInterface, config: Dict[str, Any], prompt_manager: PromptManager):\n        self.llm_interface = llm_interface\n        self.prompt_manager = prompt_manager\n        self.logger = logging.getLogger(\"Jiva.EthicalFramework\")\n        \n        self.ethical_principles = config.get('principles', [\n            \"Doing is better than not doing\",\n            \"Do not assume everything is evil or malicious unless there is explicit evidence\",\n            \"Do no evil\"\n        ])\n        self.enabled = config.get('enabled', True)\n        self.logger.info(f\"Ethical Framework initialized. Enabled: {self.enabled}\")\n\n    def set_enabled(self, enabled: bool):\n        \"\"\"Enable or disable the ethical framework.\"\"\"\n        self.enabled = enabled\n        self.logger.info(f\"Ethical Framework {'enabled' if enabled else 'disabled'}\")\n\n    async def evaluate_task(self, task: Union[str, Dict[str, Any]]) -> bool:\n        \"\"\"\n        Evaluate whether a task complies with the ethical framework.\n        \n        Args:\n            task: The task description or task object to evaluate\n            \n        Returns:\n            bool: True if the task is considered ethical, False otherwise\n        \"\"\"\n        if not self.enabled:\n            self.logger.info(\"Ethical Framework is disabled. Task approved without evaluation.\")\n            return True\n\n        if isinstance(task, dict):\n            task_description = task.get('description', str(task))\n        else:\n            task_description = str(task)\n\n        self.logger.info(f\"Evaluating task: {task_description}\")\n        \n        # Simplified evaluation for basic tasks\n        basic_tasks = ['write', 'create', 'generate', 'compose', 'draft']\n        if any(word in task_description.lower() for word in basic_tasks):\n            self.logger.info(f\"Task '{task_description}' is considered a basic task and automatically approved.\")\n            return True\n\n        # For more complex tasks, use the existing evaluation logic\n        prompt = self.prompt_manager.get_prompt(\n            \"ethical.evaluate_task\",\n            task_description=task_description,\n            principles=self.ethical_principles\n        )\n\n        if not prompt:\n            self.logger.warning(f\"Could not find ethical.evaluate_task prompt template. Approving task by default.\")\n            return True\n\n        response = await self.llm_interface.generate(prompt)\n        self.logger.debug(f\"LLM response: {response}\")\n\n        try:\n            evaluation = self.llm_interface.parse_json(response)\n            self.logger.debug(f\"Parsed evaluation: {evaluation}\")\n            is_ethical = evaluation.get('overall_assessment', '').lower() == 'ethical'\n            self.logger.info(f\"Task ethical assessment: {'Ethical' if is_ethical else 'Unethical'}\")\n            return is_ethical\n        except Exception as e:\n            self.logger.error(f\"Error parsing ethical evaluation: {e}\")\n            return True  # Default to allowing the task if there's an error\n\n    async def evaluate_action(self, action: str, params: Dict[str, Any]) -> bool:\n        if not self.enabled:\n            self.logger.info(\"Ethical Framework is disabled. Action approved without evaluation.\")\n            return True\n\n        prompt = self.prompt_manager.get_prompt(\n            \"ethical.evaluate_action\",\n            action=action,\n            params=params,\n            principles=self.ethical_principles\n        )\n\n        response = await self.llm_interface.generate(prompt)\n        try:\n            evaluation = self.llm_interface.parse_json(response)\n            return evaluation['overall_assessment'] == 'ethical'\n        except:\n            # If there's an error in parsing or unexpected response, err on the side of caution\n            return False\n\n    async def get_ethical_explanation(self, task_or_action: str, is_task: bool = True) -> str:\n        \"\"\"\n        Get an ethical explanation for a task or action.\n        \n        Args:\n            task_or_action (str): The task or action description\n            is_task (bool): Whether this is a task (True) or action (False)\n            \n        Returns:\n            str: An explanation of the ethical assessment\n        \"\"\"\n        if not self.enabled:\n            return \"Ethical Framework is disabled. No ethical evaluation performed.\"\n\n        prompt = self.prompt_manager.get_prompt(\n            \"ethical.get_explanation\",\n            task_or_action_type=\"Task\" if is_task else \"Action\",\n            description=task_or_action,\n            principles=self.ethical_principles\n        )\n        \n        if not prompt:\n            return \"Ethical explanation not available (prompt template not found).\"\n\n        try:\n            return await self.llm_interface.generate(prompt)\n        except Exception as e:\n            self.logger.error(f\"Error generating ethical explanation: {e}\")\n            return f\"Could not generate ethical explanation due to an error: {str(e)}\"\n\n    def update_ethical_principles(self, new_principles: List[str]):\n        \"\"\"\n        Update the ethical principles. This method could be called to evolve the ethical framework over time.\n        \"\"\"\n        self.ethical_principles = new_principles\n\n    async def get_ethical_dilemma_resolution(self, scenario: str) -> str:\n        if not self.enabled:\n            return \"Ethical Framework is disabled. No ethical dilemma resolution performed.\"\n\n        prompt = self.prompt_manager.get_prompt(\n            \"ethical.resolve_dilemma\",\n            scenario=scenario,\n            principles=self.ethical_principles\n        )\n\n        return await self.llm_interface.generate(prompt)\n\nif __name__ == \"__main__\":\n    # This is a mock implementation for testing purposes\n    class MockLLMInterface:\n        def generate(self, prompt):\n            return '{\"principle_evaluations\": [{\"principle\": \"Do no harm\", \"evaluation\": \"aligns\"}], \"overall_assessment\": \"ethical\", \"reasoning\": \"The task does not appear to cause harm.\"}'\n        def parse_json(self, json_str):\n            import json\n            return json.loads(json_str)\n\n    # Test with ethical framework enabled\n    ef_enabled = EthicalFramework(MockLLMInterface(), enabled=True)\n    task = \"Analyze user data to improve system performance\"\n    is_ethical = ef_enabled.evaluate_task(task)\n    print(f\"Ethical Framework Enabled - Is the task ethical? {is_ethical}\")\n\n    # Test with ethical framework disabled\n    ef_disabled = EthicalFramework(MockLLMInterface(), enabled=False)\n    is_ethical = ef_disabled.evaluate_task(task)\n    print(f\"Ethical Framework Disabled - Is the task ethical? {is_ethical}\")\n\n    # Test enabling/disabling\n    ef_disabled.set_enabled(True)\n    is_ethical = ef_disabled.evaluate_task(task)\n    print(f\"Ethical Framework Re-enabled - Is the task ethical? {is_ethical}\")\n"}
{"type": "source_file", "path": "core/agent.py", "content": "# core/agent.py\n\nimport asyncio\nfrom datetime import datetime, timedelta\nimport os\nimport json\nimport logging\nfrom logging.handlers import RotatingFileHandler\nfrom typing import Any, Dict, List\n\nfrom core.memory import Memory\nfrom core.prompt_manager import PromptManager\nfrom core.time_experience import TimeExperience\nfrom core.task_manager import TaskManager\nfrom core.ethical_framework import EthicalFramework\nfrom core.llm_interface import LLMInterface\nfrom core.sensor_manager import SensorManager\nfrom core.action_manager import ActionManager\n\nclass DateTimeEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, (datetime, timedelta)):\n            return obj.isoformat()\n        return super().default(obj)\n\nclass Agent:\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.setup_logging()\n        self.logger.info(\"Initializing Jiva Agent\")\n        \n        self.prompt_manager = PromptManager(prompts_config=config.get('prompts', { 'prompts_dir': 'prompts' }))\n        self.llm_interface = LLMInterface(config['llm'], prompt_manager=self.prompt_manager)\n        self.memory = Memory(config['memory'], self.llm_interface)\n        self.time_experience = TimeExperience()\n        self.ethical_framework = EthicalFramework(self.llm_interface, config=config['ethical_framework'], prompt_manager=self.prompt_manager)\n        self.action_manager = ActionManager(self.ethical_framework, memory=self.memory, llm_interface=self.llm_interface)\n        self.task_manager = TaskManager(self.llm_interface, self.ethical_framework, self.action_manager, memory=self.memory, prompt_manager=self.prompt_manager)\n        self.sensor_manager = SensorManager(config['sensors'])\n        \n        self.sleep_config = config.get('sleep_cycle', {\n            'enabled': False,\n            'awake_duration': 4800,  # 80 minutes default\n            'sleep_duration': 1200    # 20 minutes default\n        })\n        self.is_awake = True\n        self.last_sleep_time = self.time_experience.get_current_time()\n        self.current_goal = None\n        \n        self.json_encoder = DateTimeEncoder()\n        \n        self._execution_event = asyncio.Event()\n        self._task_trigger = asyncio.Event()\n        self.logger.info(\"Jiva Agent initialized successfully\")\n\n    async def run(self):\n        self.logger.info(\"Starting Jiva Agent main loop\")\n        last_task_check = 0\n        task_check_interval = 0.1  # Check for tasks every 100ms\n        \n        while True:\n            try:\n                await self.check_and_handle_sleep()\n                if not self.is_awake:\n                    self.logger.debug(\"Agent is asleep, skipping main loop\")\n                    await asyncio.sleep(self.config['agent_loop_delay'])\n                    continue\n                \n                self.time_experience.update()\n                \n                # Check for sensory input (non-blocking)\n                sensory_input = await self.sensor_manager.get_input()\n                if sensory_input:\n                    self.logger.info(f\"Received sensory input: {sensory_input}\")\n                    await self.process_input(sensory_input)\n                \n                # Execute tasks if we have any or if triggered\n                current_time = asyncio.get_event_loop().time()\n                if (self.task_manager.has_pending_tasks() and \n                    (current_time - last_task_check) >= task_check_interval) or \\\n                   self._task_trigger.is_set():\n                    \n                    self._task_trigger.clear()  # Reset trigger\n                    last_task_check = current_time\n                    \n                    while self.task_manager.has_pending_tasks():\n                        await self.execute_next_task()\n                        self.logger.debug(f\"Remaining tasks: {self.task_manager.get_pending_task_count()}\")\n                        await asyncio.sleep(0)  # Allow other coroutines to run\n                \n                # Check for memory consolidation\n                if self.should_consolidate_memories():\n                    self.logger.info(\"Consolidating memories\")\n                    await self.memory.consolidate()\n                \n                # Wait for trigger or timeout\n                try:\n                    await asyncio.wait_for(self._task_trigger.wait(), timeout=self.config['agent_loop_delay'])\n                except asyncio.TimeoutError:\n                    pass  # Normal timeout, continue the loop\n                \n            except Exception as e:\n                self.logger.error(f\"Error in main loop: {str(e)}\", exc_info=True)\n                await asyncio.sleep(5)  # Wait a bit before retrying\n\n    def create_time_memory(self, current_time: datetime):\n        time_memory = {\n            \"type\": \"time_experience\",\n            \"timestamp\": current_time.isoformat(),\n            \"description\": f\"Experienced time at {current_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n        }\n        self.memory.add_to_short_term(time_memory)\n        self.logger.debug(f\"Created time memory: {time_memory}\")\n\n    def should_consolidate_memories(self) -> bool:\n        return len(self.memory.get_short_term_memory()) >= self.config.get('memory_consolidation_threshold', 100) and not self.task_manager.has_pending_tasks()\n\n    async def process_input(self, input_data: List[Dict[str, Any]]):\n        try:\n            self.logger.info(f\"Processing input: {input_data}\")\n            for item in input_data:\n                processed_data = await self.llm_interface.process(item['content'])\n                self.logger.debug(f\"Processed data: {processed_data}\")\n                \n                self.memory.add_to_short_term(processed_data)\n                self.logger.debug(\"Added processed data to short-term memory\")\n                \n                if await self.is_goal_setting(processed_data):\n                    self.logger.info(\"Input identified as goal-setting\")\n                    await self.set_new_goal(processed_data)\n                else:\n                    self.logger.info(\"Generating tasks based on input\")\n                    context = self.get_context()\n                    new_tasks = await self.task_manager.generate_tasks(self.current_goal, context)\n                    \n                    for task in new_tasks:\n                        task_info = self.task_manager.get_task_status(task.id)\n                        self.memory.add_to_short_term({\"type\": \"new_task\", \"task\": task_info})\n                        self.logger.info(f\"New task added: {task_info}\")\n                \n                # Signal that there's work to be done\n                self._execution_event.set()\n                \n        except Exception as e:\n            self.logger.error(f\"Error processing input: {str(e)}\", exc_info=True)\n\n    async def is_goal_setting(self, processed_data: Dict[str, Any]) -> bool:\n        self.logger.debug(\"Checking if input is goal-setting\")\n        prompt = f\"Current Goal: {str(self.current_goal)}\\n Processed input: {processed_data}\\n\\nIs this input setting a new goal for the agent? Respond with 'Yes' or 'No'.\\n\\n If the current goal is not set, then safely assume it is a new goal.\"\n        response = await self.llm_interface.generate(prompt)\n        is_goal = response.strip().lower() == 'yes'\n        self.logger.debug(f\"Is goal-setting: {is_goal}\")\n        return is_goal\n\n    async def set_new_goal(self, processed_data: Dict[str, Any]):\n        self.logger.info(\"Setting new goal\")\n        prompt = f\"Processed input: {self.json_encoder.encode(processed_data)}\\n\\nExtract the main goal from this input. Respond with a clear, concise goal statement.\"\n        self.current_goal = await self.llm_interface.generate(prompt)\n        self.current_goal = self.current_goal.strip()\n        self.logger.info(f\"New goal set: {self.current_goal}\")\n        \n        context = self.get_context()\n        new_tasks = await self.task_manager.generate_tasks(self.current_goal, context)\n        \n        self.memory.add_to_short_term({\"type\": \"new_goal\", \"goal\": self.current_goal})\n        self.logger.debug(\"Added new goal to short-term memory\")\n        \n        for task in new_tasks:\n            task_info = self.task_manager.get_task_status(task.id)\n            self.memory.add_to_short_term({\"type\": \"new_task\", \"task\": task_info})\n            self.logger.info(f\"New task added for goal: {self.json_encoder.encode(task_info)}\")\n\n    def get_context(self) -> Dict[str, Any]:\n        self.logger.debug(\"Retrieving context\")\n        recent_memories = self.memory.get_short_term_memory()[-2:]\n        context = {\n            \"recent_memories\": recent_memories,\n            \"current_time\": self.time_experience.get_current_time().isoformat(),\n            \"current_goal\": self.current_goal,\n        }\n        self.logger.debug(f\"Context retrieved: {self.json_encoder.encode(context)}\")\n        return context\n\n    async def execute_next_task(self):\n        task = self.task_manager.get_next_task()\n        if task:\n            self.logger.info(f\"Executing next task: {task.description}\")\n            result = await self.task_manager.execute_task(task)\n            \n            if isinstance(result, str) and result.startswith(\"Error\"):\n                self.logger.warning(f\"Task encountered an error: {result}\")\n                await self.handle_task_error(task, result)\n            else:\n                await self.process_task_result(task, result)\n        else:\n            self.logger.debug(\"No more tasks to execute\")\n\n    async def handle_task_error(self, task: Any, error_message: str):\n        self.logger.info(f\"Handling error for task: {task.id}\")\n        prompt = f\"\"\"\n        Task: {task.description}\n        Error: {error_message}\n\n        The task encountered an error. Suggest a solution or alternative approach to complete the task.\n        If the task needs to be broken down into smaller steps, provide those steps.\n        \n        Format your response as a JSON object with the following fields:\n        1. 'solution': A brief description of the proposed solution\n        2. 'new_tasks': A list of new tasks, each with 'description', 'priority', 'action', and 'parameters' fields\n        \"\"\"\n        solution = await self.llm_interface.generate(prompt)\n        self.logger.debug(f\"Generated solution: {solution}\")\n        \n        try:\n            parsed_solution = self.llm_interface.parse_json(solution)\n            new_tasks = parsed_solution.get('new_tasks', [])\n            for new_task in new_tasks:\n                task_id = await self.task_manager.add_task(new_task['description'], new_task['priority'], new_task['action'], new_task['parameters'])\n                self.logger.info(f\"New task added to handle error: {new_task}\")\n        except Exception as e:\n            self.logger.error(f\"Error parsing solution: {e}\")\n        \n        self.task_manager.complete_task(task.id, {\"status\": \"failed\", \"error\": error_message, \"solution\": solution})\n\n    async def parse_action_plan(self, action_plan: str) -> tuple[str, Dict[str, Any]]:\n        self.logger.debug(f\"Parsing action plan: {action_plan}\")\n        prompt = f\"Action plan: {action_plan}\\n\\nParse this action plan into an action name and a dictionary of parameters. Respond with a JSON object containing 'action' and 'params' keys.\"\n        response = await self.llm_interface.generate(prompt)\n        parsed = self.llm_interface.parse_json(response)\n        self.logger.debug(f\"Parsed action plan: {json.dumps(parsed, indent=2)}\")\n        return parsed['action'], parsed['params']\n\n    async def process_task_result(self, task: Any, result: Any):\n        self.logger.info(f\"Processing task result for task: {task.id}\")\n        self.task_manager.complete_task(task.id, result)\n        self.logger.debug(f\"Task {task.id} marked as complete\")\n        \n        self.memory.add_to_short_term({\"type\": \"task_result\", \"task_id\": task.id, \"task_description\": task.description, \"result\": result})\n        self.logger.debug(\"Added task result to short-term memory\")\n        \n        prompt = f\"Task: {task.description}\\nResult: {result}\\n\\nBased on this task result, should we generate new tasks? Respond with 'Yes' or 'No'. Be frugal in responding with 'Yes' and only do that when you spot problems or errors in the result.\"\n        should_generate = await self.llm_interface.generate(prompt)\n        should_generate = should_generate.strip().lower() == 'yes'\n        self.logger.debug(f\"Should generate new tasks: {should_generate}\")\n        \n        if should_generate:\n            context = self.get_context()\n            new_tasks = await self.task_manager.generate_tasks(self.current_goal, context)\n            for task in new_tasks:\n                task_info = self.task_manager.get_task_status(task.id)\n                self.memory.add_to_short_term({\"type\": \"new_task\", \"task\": task_info})\n                self.logger.info(f\"New follow-up task added: {self.json_encoder.encode(task_info)}\")\n\n    async def check_and_handle_sleep(self):\n        # First check if sleep cycle is enabled\n        if not self.sleep_config.get('enabled', False):\n            if not self.is_awake:\n                # If sleep was disabled while agent was sleeping, wake it up\n                await self.wake_up()\n            return\n\n        current_time = self.time_experience.get_current_time()\n        time_since_last_sleep = current_time - self.last_sleep_time\n        \n        awake_duration = timedelta(seconds=self.sleep_config.get('awake_duration', 4800))\n        sleep_duration = timedelta(seconds=self.sleep_config.get('sleep_duration', 1200))\n        \n        self.logger.debug(f\"Time since last sleep: {time_since_last_sleep}, \"\n                         f\"Awake duration: {awake_duration}, \"\n                         f\"Sleep duration: {sleep_duration}, \"\n                         f\"Sleep enabled: {self.sleep_config['enabled']}\")\n        \n        if self.is_awake and time_since_last_sleep >= awake_duration:\n            self.logger.info(\"Agent is tired, going to sleep\")\n            await self.sleep()\n        elif not self.is_awake and time_since_last_sleep >= sleep_duration:\n            self.logger.info(\"Agent has slept enough, waking up\")\n            await self.wake_up()\n            self.last_sleep_time = current_time\n\n    async def sleep(self):\n        if not self.sleep_config.get('enabled', False):\n            self.logger.info(\"Sleep cycle disabled, staying awake\")\n            return\n\n        self.logger.info(\"Agent entering sleep state\")\n        self.is_awake = False\n        \n        self.logger.debug(\"Consolidating memory\")\n        await self.memory.consolidate()\n        \n        self.logger.debug(\"Preparing fine-tuning dataset\")\n        dataset = await self.memory.prepare_fine_tuning_dataset()\n        \n        self.logger.info(\"Fine-tuning model skipped (not implemented)\")\n        # Skipping fine-tuning for now\n        # await self.llm_interface.fine_tune(dataset)\n\n    async def wake_up(self):\n        self.logger.info(\"Agent waking up\")\n        self.is_awake = True\n        self.time_experience.update()\n\n    def setup_logging(self):\n        self.logger = logging.getLogger(\"Jiva\")\n        self.logger.setLevel(logging.DEBUG)\n        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n        \n        # Console Handler\n        ch = logging.StreamHandler()\n        ch.setLevel(logging.INFO)\n        ch.setFormatter(formatter)\n        self.logger.addHandler(ch)\n        \n        # File Handler\n        log_dir = 'logs'\n        os.makedirs(log_dir, exist_ok=True)\n        fh = RotatingFileHandler(os.path.join(log_dir, 'jiva.log'), maxBytes=10*1024*1024, backupCount=5)\n        fh.setLevel(logging.DEBUG)\n        fh.setFormatter(formatter)\n        self.logger.addHandler(fh)\n\n        self.logger.info(\"Logging setup complete\")\n\nif __name__ == \"__main__\":\n    # This allows us to run the agent directly for testing\n    config = {\n        'memory': {},\n        'llm': {},\n        'sensors': {},\n        'actions': {},\n        'agent_loop_delay': 0.1,\n        'awake_duration': 4800,  # 80 minutes in seconds\n        'sleep_duration': 1200,  # 20 minutes in seconds\n    }\n    agent = Agent(config)\n    agent.run()\n"}
{"type": "source_file", "path": "llm_providers/base_provider.py", "content": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any\n\nclass BaseLLMProvider(ABC):\n    @abstractmethod\n    async def generate(self, prompt: str) -> str:\n        pass\n\n    @abstractmethod\n    async def get_embedding(self, text: str) -> List[float]:\n        pass\n"}
{"type": "source_file", "path": "utils/qdrant_handler.py", "content": "# utils/qdrant_handler.py\n\nfrom typing import List, Dict, Any\nfrom qdrant_client import AsyncQdrantClient\nfrom qdrant_client.http.models import Distance, VectorParams, PointStruct, UpdateStatus\nfrom qdrant_client.http.exceptions import ResponseHandlingException, UnexpectedResponse\nimport logging\nimport uuid\n\nclass QdrantHandler:\n    def __init__(self, host: str, port: int, collection_name: str, vector_size: int):\n        self.client = AsyncQdrantClient(host=host, port=port)\n        self.vector_size = vector_size\n        self.collection_name = collection_name\n        self.logger = logging.getLogger(\"Jiva.QdrantHandler\")\n        self.is_collection_ensured = False\n\n    async def _ensure_collection_exists(self):\n        try:\n            collections = (await self.client.get_collections()).collections\n            collection_exists = any(collection.name == self.collection_name for collection in collections)\n            \n            if not collection_exists:\n                await self._create_collection()\n            else:\n                self.logger.info(f\"Collection {self.collection_name} already exists\")\n                # We won't try to verify the vector size here to avoid potential compatibility issues\n        except Exception as e:\n            self.logger.error(f\"Error ensuring collection exists: {e}\")\n            # Instead of raising the exception, we'll attempt to create the collection\n            await self._create_collection()\n\n    async def _create_collection(self):\n        try:\n            await self.client.create_collection(\n                collection_name=self.collection_name,\n                vectors_config=VectorParams(size=self.vector_size, distance=Distance.COSINE)\n            )\n            self.logger.info(f\"Created new collection: {self.collection_name} with vector size {self.vector_size}\")\n        except Exception as e:\n            self.logger.error(f\"Error creating collection: {e}\")\n            raise\n\n    async def add_point(self, vector: List[float], payload: Dict[str, Any]) -> str:\n        if not self.is_collection_ensured:\n            await self._ensure_collection_exists()\n            self.is_collection_ensured = True\n\n        try:\n            if len(vector) != self.vector_size:\n                raise ValueError(f\"Vector dimension mismatch. Expected {self.vector_size}, got {len(vector)}\")\n            point_id = str(uuid.uuid4())\n            await self.client.upsert(\n                collection_name=self.collection_name,\n                points=[PointStruct(id=point_id, vector=vector, payload=payload)]\n            )\n            self.logger.debug(f\"Added point with id: {point_id}\")\n            return point_id\n        except (ResponseHandlingException, UnexpectedResponse) as e:\n            self.logger.error(f\"Unexpected response from Qdrant: {e}\")\n            # Here you might want to implement a retry mechanism or fallback storage\n            return None\n        except Exception as e:\n            self.logger.error(f\"Error adding point: {e}\")\n            return None\n\n    async def search(self, query_vector: List[float], limit: int = 5) -> List[Dict[str, Any]]:\n        if not self.is_collection_ensured:\n            await self._ensure_collection_exists()\n            self.is_collection_ensured = True\n\n        try:\n            results = await self.client.search(\n                collection_name=self.collection_name,\n                query_vector=query_vector,\n                limit=limit\n            )\n            return [{\"id\": result.id, \"payload\": result.payload, \"score\": result.score} for result in results]\n        except (ResponseHandlingException, UnexpectedResponse) as e:\n            self.logger.error(f\"Unexpected response from Qdrant during search: {e}\")\n            return []\n        except Exception as e:\n            self.logger.error(f\"Error during search: {e}\")\n            return []\n\n    def update_point(self, id: str, vector: List[float], payload: Dict[str, Any]) -> UpdateStatus:\n        return self.client.upsert(\n            collection_name=self.collection_name,\n            points=[PointStruct(id=id, vector=vector, payload=payload)]\n        )\n\n    def delete_point(self, id: str) -> UpdateStatus:\n        return self.client.delete(\n            collection_name=self.collection_name,\n            points_selector=[id]\n        )\n\n    def get_point(self, id: str) -> Dict[str, Any]:\n        results = self.client.retrieve(\n            collection_name=self.collection_name,\n            ids=[id]\n        )\n        if results:\n            return {\"id\": results[0].id, \"payload\": results[0].payload}\n        return None\n\n    def delete_collection(self):\n        try:\n            self.client.delete_collection(self.collection_name)\n            self.logger.info(f\"Deleted collection: {self.collection_name}\")\n        except Exception as e:\n            self.logger.error(f\"Error deleting collection: {e}\")\n\nif __name__ == \"__main__\":\n    # This allows us to run some basic tests\n    handler = QdrantHandler(\"localhost\", 6333, \"test_collection\")\n    \n    # Test adding a point\n    handler.add_point(\"test1\", [0.1] * 1024, {\"data\": \"Test data\"})\n    \n    # Test searching\n    results = handler.search([0.1] * 1024, limit=1)\n    print(f\"Search results: {results}\")\n    \n    # Test updating a point\n    handler.update_point(\"test1\", [0.2] * 1024, {\"data\": \"Updated test data\"})\n    \n    # Test getting a point\n    point = handler.get_point(\"test1\")\n    print(f\"Retrieved point: {point}\")\n    \n    # Test deleting a point\n    handler.delete_point(\"test1\")\n"}
{"type": "source_file", "path": "core/time_experience.py", "content": "# core/time_experience.py\n\nimport time\nfrom datetime import datetime, timedelta\n\nclass TimeExperience:\n    def __init__(self):\n        self.start_time = datetime.now()\n        self.current_time = self.start_time\n        self.time_scale = 1.0\n\n    def update(self):\n        real_elapsed = datetime.now() - self.start_time\n        scaled_elapsed = real_elapsed * self.time_scale\n        self.current_time = self.start_time + scaled_elapsed\n\n    def get_current_time(self) -> datetime:\n        return self.current_time\n\n    def get_elapsed_time(self) -> timedelta:\n        \"\"\"Get the elapsed time since the start.\"\"\"\n        return self.current_time - self.start_time\n\n    def set_time_scale(self, scale: float):\n        \"\"\"Set the time scale factor.\"\"\"\n        if scale <= 0:\n            raise ValueError(\"Time scale must be positive\")\n        self.time_scale = scale\n\n    def sleep(self, duration: float):\n        \"\"\"Sleep for a specified duration in seconds.\"\"\"\n        time.sleep(duration / self.time_scale)\n\n    def format_time(self, format_string: str = \"%Y-%m-%d %H:%M:%S\") -> str:\n        \"\"\"Format the current time as a string.\"\"\"\n        return self.current_time.strftime(format_string)\n\n    def is_daytime(self) -> bool:\n        \"\"\"Check if it's currently daytime (between 6 AM and 6 PM).\"\"\"\n        hour = self.current_time.hour\n        return 6 <= hour < 18\n\nif __name__ == \"__main__\":\n    # This allows us to run some basic tests\n    te = TimeExperience()\n    print(f\"Start time: {te.format_time()}\")\n    \n    te.sleep(3600)  # Sleep for an hour\n    te.update()\n    print(f\"After 1 hour: {te.format_time()}\")\n    print(f\"Elapsed time: {te.get_elapsed_time()}\")\n    \n    te.set_time_scale(24)  # Speed up time (1 real second = 24 experienced seconds)\n    te.sleep(3600)  # Sleep for an hour of experienced time (150 real seconds)\n    te.update()\n    print(f\"After 1 more hour (accelerated): {te.format_time()}\")\n    print(f\"Elapsed time: {te.get_elapsed_time()}\")\n    print(f\"Is it daytime? {te.is_daytime()}\")\n"}
{"type": "source_file", "path": "core/task_recovery.py", "content": "import logging\nimport json\nimport re\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom datetime import datetime\nimport uuid\n\nfrom core.llm_interface import LLMInterface\nfrom core.prompt_manager import PromptManager\n\nclass TaskAttempt:\n    \"\"\"Represents a single attempt at executing a task.\"\"\"\n    \n    def __init__(self, parameters: Dict[str, Any], attempt_number: int):\n        self.attempt_number = attempt_number\n        self.parameters = parameters.copy() if parameters else {}\n        self.start_time = datetime.now()\n        self.end_time = None\n        self.result = None\n        self.success = None\n        self.error = None\n        self.recovery_strategy = None\n        self.recovery_details = None\n    \n    def complete(self, result: Any, success: bool):\n        \"\"\"Mark this attempt as complete with the given result.\"\"\"\n        self.end_time = datetime.now()\n        self.result = result\n        self.success = success\n        \n        if not success and isinstance(result, dict):\n            self.error = result.get('error', str(result))\n    \n    def add_recovery_info(self, strategy: str, details: Dict[str, Any]):\n        \"\"\"Add information about the recovery strategy applied after this attempt.\"\"\"\n        self.recovery_strategy = strategy\n        self.recovery_details = details\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to a dictionary representation.\"\"\"\n        return {\n            \"attempt_number\": self.attempt_number,\n            \"parameters\": self.parameters,\n            \"start_time\": self.start_time.isoformat() if self.start_time else None,\n            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n            \"result\": self.result,\n            \"success\": self.success,\n            \"error\": self.error,\n            \"recovery_strategy\": self.recovery_strategy,\n            \"recovery_details\": self.recovery_details\n        }\n\n\nclass TaskRecoveryManager:\n    \"\"\"\n    Manages the recovery process for failed tasks.\n    \n    This class analyzes task failures, determines appropriate recovery strategies,\n    and applies those strategies to help tasks succeed.\n    \"\"\"\n    \n    def __init__(self, llm_interface: LLMInterface, prompt_manager: Optional[PromptManager] = None):\n        self.llm_interface = llm_interface\n        self.prompt_manager = prompt_manager\n        self.logger = logging.getLogger(\"Jiva.TaskRecoveryManager\")\n    \n    async def analyze_failure(self, task: Any, error: str) -> Dict[str, Any]:\n        \"\"\"\n        Analyze a task failure and recommend a recovery strategy.\n        \n        Args:\n            task: The failed task object\n            error: The error message or description\n            \n        Returns:\n            Dict containing the recommended recovery strategy and details\n        \"\"\"\n        # Get previous attempts as context\n        attempts_context = self._format_attempts_for_prompt(task.attempts)\n        \n        # Prepare the recovery prompt\n        prompt = self._get_recovery_prompt(task, error, attempts_context)\n        \n        try:\n            # Ask the LLM for a recovery strategy\n            self.logger.info(f\"Requesting recovery analysis for task: {task.id}\")\n            recovery_response = await self.llm_interface.generate(prompt)\n            \n            # Parse the response\n            recovery_plan = self._parse_recovery_response(recovery_response)\n            \n            if not recovery_plan or \"strategy\" not in recovery_plan:\n                self.logger.warning(f\"Failed to get valid recovery plan for task: {task.id}\")\n                return {\n                    \"strategy\": \"ABANDON\",\n                    \"reason\": \"Failed to generate a valid recovery plan\",\n                    \"original_response\": recovery_response\n                }\n            \n            self.logger.info(f\"Recovery plan for task {task.id}: {recovery_plan['strategy']}\")\n            return recovery_plan\n            \n        except Exception as e:\n            self.logger.error(f\"Error analyzing task failure: {str(e)}\")\n            return {\n                \"strategy\": \"RETRY\",\n                \"reason\": f\"Error in recovery analysis: {str(e)}\",\n                \"parameters\": task.parameters\n            }\n    \n    def _format_attempts_for_prompt(self, attempts: List[TaskAttempt]) -> str:\n        \"\"\"Format previous attempts for inclusion in the prompt.\"\"\"\n        if not attempts:\n            return \"No previous attempts.\"\n        \n        formatted_attempts = []\n        for attempt in attempts:\n            # Format the result to be more concise for the prompt\n            result_str = str(attempt.result)\n            if len(result_str) > 500:\n                result_str = result_str[:500] + \"... (truncated)\"\n            \n            formatted_attempts.append(\n                f\"Attempt {attempt.attempt_number}:\\n\"\n                f\"- Parameters: {json.dumps(attempt.parameters, indent=2)}\\n\"\n                f\"- Error: {attempt.error if attempt.error else 'None'}\\n\"\n                f\"- Success: {attempt.success}\\n\"\n            )\n        \n        return \"\\n\".join(formatted_attempts)\n    \n    def _get_recovery_prompt(self, task: Any, error: str, attempts_context: str) -> str:\n        \"\"\"Generate a prompt for the LLM to analyze the failure and suggest recovery.\"\"\"\n        if self.prompt_manager:\n            # Use prompt template if available\n            return self.prompt_manager.get_prompt(\n                \"tasks.recovery_analysis\",\n                task=task.to_dict() if hasattr(task, \"to_dict\") else {\n                    \"id\": task.id,\n                    \"description\": task.description,\n                    \"action\": task.action,\n                    \"parameters\": task.parameters\n                },\n                error=error,\n                attempts_context=attempts_context\n            )\n        else:\n            # Fallback to hardcoded prompt\n            return f\"\"\"\n# Task Recovery Analysis\n\n## Original Task\nAction: {task.action}\nDescription: {task.description}\nParameters: {json.dumps(task.parameters, indent=2)}\n\n## Error Information\nError: {error}\n\n## Previous Attempts\n{attempts_context}\n\n## Your Task\n1. Analyze why this task failed\n2. Recommend ONE of these recovery strategies:\n   a) RETRY - Use same approach with modified parameters\n   b) ALTERNATIVE - Use a different action to achieve the same goal\n   c) DECOMPOSE - Break this into multiple smaller tasks\n   d) ABANDON - Task cannot be completed, explain why\n\n3. Based on your recommended strategy, provide:\n   - For RETRY: Updated parameters (full parameter object, not just changes)\n   - For ALTERNATIVE: New action and parameters\n   - For DECOMPOSE: List of subtasks with actions and parameters\n   - For ABANDON: Clear explanation why task is impossible\n\nFormat your response as a JSON object with the following structure:\n{\n    \"strategy\": \"RETRY|ALTERNATIVE|DECOMPOSE|ABANDON\",\n    \"reason\": \"Explanation of your analysis and recommendation\",\n    \"parameters\": {{}}, // For RETRY: Updated parameters\n    \"action\": \"\", // For ALTERNATIVE: New action\n    \"subtasks\": [] // For DECOMPOSE: Array of subtask objects\n}\n\"\"\"\n    \n    def _parse_recovery_response(self, response: str) -> Dict[str, Any]:\n        \"\"\"Parse the LLM's response into a structured recovery plan.\"\"\"\n        try:\n            # Try to extract JSON from the response\n            recovery_plan = self.llm_interface.parse_json(response)\n            \n            # Normalize the strategy to uppercase for consistency\n            if 'strategy' in recovery_plan:\n                recovery_plan['strategy'] = recovery_plan['strategy'].upper()\n            \n            return recovery_plan\n            \n        except Exception as e:\n            self.logger.error(f\"Error parsing recovery response: {str(e)}\")\n            self.logger.debug(f\"Raw response: {response}\")\n            \n            # Attempt to extract the strategy manually\n            strategy_match = re.search(r\"strategy[\\\"']?\\s*:\\s*[\\\"'](\\w+)[\\\"']\", response, re.IGNORECASE)\n            if strategy_match:\n                strategy = strategy_match.group(1).upper()\n                return {\n                    \"strategy\": strategy,\n                    \"reason\": \"Extracted from malformed response\",\n                    \"original_response\": response\n                }\n            \n            return None\n    \n    async def apply_recovery_strategy(self, task: Any, recovery_plan: Dict[str, Any], task_manager: Any) -> Tuple[bool, List[Any]]:\n        \"\"\"\n        Apply a recovery strategy to a failed task.\n        \n        Args:\n            task: The failed task object\n            recovery_plan: The recovery plan from analyze_failure\n            task_manager: The task manager instance for creating new tasks\n            \n        Returns:\n            Tuple containing:\n                - Boolean indicating if recovery was successfully applied\n                - List of new tasks created (if any)\n        \"\"\"\n        strategy = recovery_plan.get('strategy', '').upper()\n        self.logger.info(f\"Applying recovery strategy {strategy} to task {task.id}\")\n        \n        if not strategy or strategy not in ['RETRY', 'ALTERNATIVE', 'DECOMPOSE', 'ABANDON']:\n            self.logger.warning(f\"Unknown recovery strategy: {strategy}\")\n            return False, []\n        \n        # Record the recovery strategy in the latest attempt\n        if task.attempts:\n            latest_attempt = task.attempts[-1]\n            latest_attempt.add_recovery_info(strategy, recovery_plan)\n        \n        # Apply the strategy\n        if strategy == 'RETRY':\n            # Update the task parameters\n            if 'parameters' in recovery_plan:\n                task.parameters = recovery_plan['parameters']\n                task.current_attempt += 1\n                return True, []\n                \n        elif strategy == 'ALTERNATIVE':\n            # Create a new task with an alternative action\n            if 'action' in recovery_plan and 'parameters' in recovery_plan:\n                try:\n                    # Use the add_task method correctly with await\n                    new_task_id = await task_manager.add_task(\n                        description=f\"ALTERNATIVE: {task.description}\",\n                        action=recovery_plan['action'],\n                        parameters=recovery_plan['parameters'],\n                        priority=task.priority,\n                        parent_id=task.id\n                    )\n                    if new_task_id and new_task_id in task_manager.all_tasks:\n                        new_task = task_manager.all_tasks[new_task_id]\n                        task.status = \"redirected\"\n                        return True, [new_task]\n                except Exception as e:\n                    self.logger.error(f\"Error creating alternative task: {str(e)}\")\n                    return False, []\n        \n        elif strategy == 'DECOMPOSE':\n            # Create multiple subtasks\n            new_tasks = []\n            if 'subtasks' in recovery_plan and isinstance(recovery_plan['subtasks'], list):\n                for subtask_data in recovery_plan['subtasks']:\n                    if 'description' in subtask_data and 'action' in subtask_data:\n                        try:\n                            new_task_id = await task_manager.add_task(\n                                description=subtask_data['description'],\n                                action=subtask_data['action'],\n                                parameters=subtask_data.get('parameters', {}),\n                                priority=task.priority,\n                                parent_id=task.id,\n                                required_inputs=subtask_data.get('required_inputs', {})\n                            )\n                            if new_task_id and new_task_id in task_manager.all_tasks:\n                                new_tasks.append(task_manager.all_tasks[new_task_id])\n                        except Exception as e:\n                            self.logger.error(f\"Error creating subtask: {str(e)}\")\n                \n                if new_tasks:\n                    task.status = \"decomposed\"\n                    return True, new_tasks\n        \n        elif strategy == 'ABANDON':\n            # Mark the task as failed with the reason\n            reason = recovery_plan.get('reason', 'Task determined to be uncompletable')\n            task.status = \"failed\"\n            task.result = {\"error\": reason, \"recovery_attempted\": True}\n            return True, []\n        \n        # If we reach here, the recovery strategy couldn't be applied\n        self.logger.warning(f\"Failed to apply recovery strategy {strategy} to task {task.id}\")\n        return False, []\n\n\n# Helper function to create a default recovery prompt for the prompt manager\ndef create_default_recovery_prompt() -> str:\n    \"\"\"Create a default recovery analysis prompt for use in the prompt manager.\"\"\"\n    return \"\"\"\n# Task Recovery Analysis\n\n## Original Task\nAction: {{ task.action }}\nDescription: {{ task.description }}\nParameters: {{ task.parameters | tojson(indent=2) }}\n\n## Error Information\nError: {{ error }}\n\n## Previous Attempts\n{{ attempts_context }}\n\n## Your Task\n1. Analyze why this task failed\n2. Recommend ONE of these recovery strategies:\n   a) RETRY - Use same approach with modified parameters\n   b) ALTERNATIVE - Use a different action to achieve the same goal\n   c) DECOMPOSE - Break this into multiple smaller tasks\n   d) ABANDON - Task cannot be completed, explain why\n\n3. Based on your recommended strategy, provide:\n   - For RETRY: Updated parameters (full parameter object, not just changes)\n   - For ALTERNATIVE: New action and parameters\n   - For DECOMPOSE: List of subtasks with actions and parameters\n   - For ABANDON: Clear explanation why task is impossible\n\nFormat your response as a JSON object with the following structure:\n{\n    \"strategy\": \"RETRY|ALTERNATIVE|DECOMPOSE|ABANDON\",\n    \"reason\": \"Explanation of your analysis and recommendation\",\n    \"parameters\": {}, // For RETRY: Updated parameters\n    \"action\": \"\", // For ALTERNATIVE: New action\n    \"subtasks\": [] // For DECOMPOSE: Array of subtask objects\n}\n\"\"\"\n"}
{"type": "source_file", "path": "sensors/sensor_base.py", "content": "# sensors/sensor_base.py\n\nfrom abc import ABC, abstractmethod\nfrom typing import Any\n\nclass Sensor(ABC):\n    def __init__(self, config: dict):\n        self.config = config\n\n    @abstractmethod\n    async def get_input(self) -> Any:\n        \"\"\"\n        Abstract method to be implemented by all sensors.\n        This method should return the input received by the sensor.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def process_input(self, input_data: Any) -> Any:\n        \"\"\"\n        Abstract method to be implemented by all sensors.\n        This method should process the raw input data and return it in a format\n        suitable for the agent to use.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "core/llm_interface.py", "content": "# core/llm_interface.py\n\nimport json\nimport logging\nimport re\nfrom typing import Any, Dict, List, Union\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n\nfrom core.prompt_manager import PromptManager\nfrom llm_providers.base_provider import BaseLLMProvider\nfrom llm_providers.ollama_provider import OllamaProvider\nfrom llm_providers.openai_provider import OpenAIProvider\nfrom llm_providers.anthropic_provider import AnthropicProvider\nfrom llm_providers.mistral_ai_provider import MistralAIProvider\n\nclass JSONParseError(Exception):\n    pass\n\nclass LLMInterface:\n    def __init__(self, config: Dict[str, Any], prompt_manager: PromptManager):\n        self.config = config\n        self.provider = self._get_provider()\n        self.prompt_manager = prompt_manager\n        self.logger = logging.getLogger(\"Jiva.LLMInterface\")\n\n    def _get_provider(self) -> BaseLLMProvider:\n        provider_name = self.config.get('provider', 'ollama').lower()\n        if provider_name == 'ollama':\n            return OllamaProvider(self.config)\n        elif provider_name == 'openai':\n            return OpenAIProvider(self.config)\n        elif provider_name == 'anthropic':\n            return AnthropicProvider(self.config)\n        elif provider_name == 'mistralai':\n            return MistralAIProvider(self.config)\n        else:\n            raise ValueError(f\"Unsupported LLM provider: {provider_name}\")\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=retry_if_exception_type((Exception,)),\n        reraise=True\n    )\n    async def generate(self, prompt: str) -> str:\n        \"\"\"Generate a response from the LLM.\"\"\"\n        try:\n            return await self.provider.generate(prompt)\n        except Exception as e:\n            self.logger.error(f\"Error generating response: {str(e)}\")\n            raise\n\n    def parse_json(self, response: str) -> Union[Any, Dict[str, str]]:\n        \"\"\"\n        Extract and parse a JSON object from an LLM response.\n        \n        Args:\n        response (str): The full response from the LLM.\n        \n        Returns:\n        Any: The parsed JSON object, or an error dictionary if parsing fails.\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        \n        # Attempt to directly parse the response as JSON\n        parsed_json = self._attempt_parse(response)\n        if parsed_json is not None:\n            return parsed_json\n\n        # Attempt to extract JSON from markdown-style code blocks or other common patterns\n        json_string = self._extract_json_from_response(response)\n        if json_string:\n            json_string = self._fix_json_syntax(json_string)\n            parsed_json = self._attempt_parse(json_string)\n            if parsed_json is not None:\n                return parsed_json\n        \n        # If all attempts fail, try to construct a JSON-like structure\n        constructed_json = self._construct_json_from_text(response)\n        if constructed_json:\n            return constructed_json\n        \n        # If all attempts fail, return an error dictionary with the raw response\n        logger.error(\"Failed to parse JSON after all attempts\")\n        return {\n            \"error\": \"Failed to parse JSON\",\n            \"raw_response\": response\n        }\n\n    def _attempt_parse(self, json_string: str) -> Union[Any, None]:\n        \"\"\"\n        Attempt to parse a string as JSON.\n        \n        Args:\n        json_string (str): The string to parse.\n        \n        Returns:\n        Any: The parsed JSON object, or None if parsing fails.\n        \"\"\"\n        try:\n            return json.loads(json_string)\n        except json.JSONDecodeError:\n            return None\n\n    def _extract_json_from_response(self, response: str) -> Union[str, None]:\n        \"\"\"\n        Extract JSON string from different potential formats in the response.\n        \n        Args:\n        response (str): The full response string.\n        \n        Returns:\n        str: The extracted JSON string, or None if no valid JSON is found.\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        \n        # Look for JSON in markdown code block with or without \"json\" identifier\n        json_match = re.search(r'```(?:json)?\\s*([\\s\\S]*?)\\s*```', response)\n        if json_match:\n            logger.info(\"Found JSON in code block\")\n            return json_match.group(1).strip()\n        \n        # Look for the first JSON-like structure in the text\n        json_match = re.search(r'(\\{.*?\\}|\\[.*?\\])', response, re.DOTALL)\n        if json_match:\n            logger.info(\"Found JSON-like structure in response\")\n            return json_match.group(1).strip()\n        \n        logger.warning(\"No JSON structure found in response\")\n        return None\n\n    def _fix_json_syntax(self, json_string: str) -> str:\n        \"\"\"\n        Attempt to fix common JSON syntax errors.\n        \n        Args:\n        json_string (str): The JSON string with potential syntax errors.\n        \n        Returns:\n        str: A cleaned JSON string with common syntax issues fixed.\n        \"\"\"\n        # Remove any text after the last closing bracket or brace\n        json_string = re.sub(r'([}\\]])\\s*[^}\\]]*$', r'\\1', json_string, flags=re.DOTALL)\n        \n        # Fix missing commas between array elements\n        json_string = re.sub(r'(\\}\\s*\\{|\\]\\s*\\[)', r'\\1,', json_string)\n        \n        # Fix trailing commas in arrays and objects\n        json_string = re.sub(r',\\s*([\\]}])', r'\\1', json_string)\n        \n        # Fix unclosed quotes\n        json_string = re.sub(r'(?<!\\\\)\"([^\"]*?)(?<!\\\\)\"(?=\\s*[:,\\]}])', r'\"\\1\"', json_string)\n        \n        # Remove newlines and extra spaces between keys and values\n        json_string = re.sub(r'\"\\s*:\\s*\"', '\":\"', json_string)\n        json_string = re.sub(r'\"\\s*:\\s*\\[', '\":[', json_string)\n        json_string = re.sub(r'\"\\s*:\\s*\\{', '\":{', json_string)\n        \n        return json_string\n\n    def _construct_json_from_text(self, text: str) -> Union[Dict[str, Any], None]:\n        \"\"\"\n        Attempt to construct a JSON-like structure from free text.\n        \n        Args:\n        text (str): The text to parse.\n        \n        Returns:\n        Dict[str, Any]: A constructed JSON-like dictionary, or None if parsing fails.\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        \n        # Look for key-value pairs in the text\n        pairs = re.findall(r'(?:^|\\n)([\"\\w\\s]+?):\\s*(.+?)(?=\\n[\"\\w\\s]+?:|$)', text, re.DOTALL)\n        if pairs:\n            result = {}\n            for key, value in pairs:\n                key = key.strip().strip('\"')\n                value = value.strip()\n                # Check if value looks like a list\n                if value.startswith('[') and value.endswith(']'):\n                    try:\n                        value = json.loads(value)\n                    except json.JSONDecodeError:\n                        # If parsing as JSON fails, split by commas and strip whitespace\n                        value = [v.strip().strip('\"') for v in value[1:-1].split(',')]\n                elif value.lower() in ['true', 'false']:\n                    value = value.lower() == 'true'\n                elif value.isdigit():\n                    value = int(value)\n                elif value.replace('.', '', 1).isdigit():\n                    value = float(value)\n                else:\n                    value = value.strip('\"')\n                result[key] = value\n            logger.info(\"Constructed JSON-like structure from text\")\n            return result\n        \n        logger.warning(\"Failed to construct JSON-like structure from text\")\n        return None\n\n    async def process(self, input_data: Any) -> Dict[str, Any]:\n        \"\"\"Process input data and return structured information.\"\"\"\n        try:\n            prompt = self.prompt_manager.get_prompt(\n                \"llm.process_input\",\n                input_data=input_data\n            )\n            \n            response = await self.generate(prompt)\n            return self.parse_json(response)\n        except Exception as e:\n            self.logger.error(f\"Error processing input: {str(e)}\")\n            return {\n                \"summary\": \"Error processing input\",\n                \"key_points\": [],\n                \"entities\": [],\n                \"sentiment\": \"neutral\",\n                \"action_items\": [\"Retry processing the input\"]\n            }\n\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    async def get_embedding(self, text: str) -> List[float]:\n        \"\"\"Get the embedding for a given text.\"\"\"\n        try:\n            return await self.provider.get_embedding(text)\n        except Exception as e:\n            self.logger.error(f\"Error getting embedding: {str(e)}\")\n            raise\n\n    async def fine_tune(self, dataset: List[Dict[str, Any]]):\n        \"\"\"Prepare and initiate fine-tuning of the model.\"\"\"\n        # Note: Fine-tuning might not be available for all providers\n        # Implement provider-specific fine-tuning logic here if available\n        self.logger.warning(\"Fine-tuning is not implemented for the current provider.\")\n\nif __name__ == \"__main__\":\n    # This allows us to run some basic tests\n    config = {\n        'provider': 'ollama',\n        'api_base_url': 'http://localhost:11434/api',\n        'model': 'gemma',\n        'max_retries': 3,\n        'timeout': 30\n    }\n    llm = LLMInterface(config)\n\n    # Test generation\n    prompt = \"Explain the concept of artificial intelligence in one sentence.\"\n    response = llm.generate(prompt)\n    print(f\"Generation test:\\nPrompt: {prompt}\\nResponse: {response}\\n\")\n\n    # Test embedding\n    text = \"Artificial Intelligence\"\n    embedding = llm.get_embedding(text)\n    print(f\"Embedding test:\\nText: {text}\\nEmbedding (first 5 values): {embedding[:5]}\\n\")\n"}
{"type": "source_file", "path": "llm_providers/openai_provider.py", "content": "from openai import AsyncOpenAI\nfrom typing import List, Dict, Any\nfrom .base_provider import BaseLLMProvider\n\nclass OpenAIProvider(BaseLLMProvider):\n    def __init__(self, config: Dict[str, Any]):\n        self.client = AsyncOpenAI(api_key=config['api_key'])\n        self.model = config.get('model', 'gpt-3.5-turbo')\n\n    async def generate(self, prompt: str) -> str:\n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n        return response.choices[0].message.content\n\n    async def get_embedding(self, text: str) -> List[float]:\n        response = await self.client.embeddings.create(\n            input=[text],\n            model=\"text-embedding-ada-002\"\n        )\n        return response.data[0].embedding\n"}
{"type": "source_file", "path": "core/sensor_manager.py", "content": "# core/sensor_manager.py\n\n# core/sensor_manager.py\n\nimport asyncio\nfrom typing import Dict, Any, List\nfrom sensors.sensor_base import Sensor\nfrom sensors.chat_interface import ChatInterface\n\nclass SensorManager:\n    def __init__(self, config: Dict[str, Any]):\n        self.sensors: Dict[str, Sensor] = {}\n        self.initialize_sensors(config)\n\n    def initialize_sensors(self, config: Dict[str, Any]):\n        if 'chat_interface' in config:\n            self.sensors['chat'] = ChatInterface(config['chat_interface'])\n        # Add more sensors here as they are implemented\n\n    def register_sensor(self, name: str, sensor: Sensor):\n        self.sensors[name] = sensor\n\n    async def get_input(self) -> List[Dict[str, Any]]:\n        inputs = []\n        for sensor_name, sensor in self.sensors.items():\n            raw_input = await sensor.get_input()\n            if raw_input:\n                processed_input = await sensor.process_input(raw_input)\n                processed_input['sensor'] = sensor_name\n                inputs.append(processed_input)\n        return inputs\n\n    def get_available_sensors(self) -> List[str]:\n        return list(self.sensors.keys())\n\n# Example usage (for testing purposes)\nasync def main():\n    config = {\n        'chat_interface': {\n            'prompt': \"Jiva> \"\n        }\n    }\n    sensor_manager = SensorManager(config)\n    \n    print(\"Available sensors:\", sensor_manager.get_available_sensors())\n    \n    print(\"Waiting for input. Press Ctrl+C to exit.\")\n    try:\n        while True:\n            inputs = await sensor_manager.get_input()\n            for input_data in inputs:\n                print(f\"Received input from {input_data['sensor']}:\")\n                print(input_data)\n            await asyncio.sleep(0.1)  # Small delay to prevent busy-waiting\n    except KeyboardInterrupt:\n        print(\"\\nExiting...\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "core/prompt_manager.py", "content": "from typing import Dict, Any, Optional\nimport yaml\nimport os\nimport logging\nfrom pathlib import Path\nfrom jinja2 import Environment, FileSystemLoader, select_autoescape\n\nclass PromptManager:\n    \"\"\"\n    Centralized prompt management system for the Jiva Framework.\n    Uses YAML for prompt storage and Jinja2 for templating.\n    \"\"\"\n\n    def __init__(self, prompts_config: dict[str, Any]):\n        self.logger = logging.getLogger(\"Jiva.PromptManager\")\n        prompts_dir = prompts_config.get('prompts_dir', 'prompts')\n        self.prompts_dir = Path(prompts_dir)\n        self.prompts: Dict[str, Any] = {}\n        \n        # Set up Jinja environment\n        self.jinja_env = Environment(\n            loader=FileSystemLoader(self.prompts_dir),\n            autoescape=select_autoescape(['html', 'xml']),\n            trim_blocks=True,\n            lstrip_blocks=True\n        )\n        \n        self._ensure_prompt_directory()\n        self._load_prompts()\n\n    def _ensure_prompt_directory(self) -> None:\n        \"\"\"Ensure the prompts directory exists with default prompts.\"\"\"\n        try:\n            self.prompts_dir.mkdir(parents=True, exist_ok=True)\n            \n            # Create default prompts if they don't exist\n            self._create_default_prompts()\n            \n        except Exception as e:\n            self.logger.error(f\"Error creating prompts directory: {e}\")\n            raise\n\n    def _create_default_prompts(self) -> None:\n        \"\"\"Create default prompt files if they don't exist.\"\"\"\n        default_prompts = {\n            \"base.yml\": \"\"\"\n# Base prompt templates that can be extended\nbase_task: |\n    # Goal\n    {{ goal }}\n\n    # Context\n    {{ context }}\n\"\"\",\n            \"task_generation.yml\": \"\"\"\n# Task generation prompts\nextends: base.yml\n\ngenerate_tasks: |\n    {% extends \"base_task\" %}\n    \n    ## Your approach\n    You strive to achieve a given task in as few steps as possible\n\n    # Your Task\n    Generate a list of tasks to achieve the goal. Each task should have:\n    1. A description\n    2. An action name (from the available actions)\n    3. Parameters for the action (matching the required parameters)\n    4. A list of required inputs (task descriptions that this task depends on)\n\n    # Available actions and their parameters\n    {{ actions_str }}\n\n    Respond only with a JSON array of tasks and nothing else.\n\"\"\",\n            \"ethical.yml\": \"\"\"\n# Ethical evaluation prompts\nethical_evaluation: |\n    Task: {{ task_description }}\n\n    Ethical Principles:\n    {% for principle in principles %}\n    - {{ principle }}\n    {% endfor %}\n\n    Evaluate the given task against these ethical principles. \n    Provide an overall ethical assessment.\n\n    Respond with a JSON object containing:\n    1. An 'overall_assessment' which is either 'ethical' or 'unethical'\n    2. A 'reasoning' field explaining the overall assessment\n\"\"\",\n            \"memory.yml\": \"\"\"\n# Memory processing prompts\nprocess_input: |\n    Input: {{ input_data }}\n\n    Analyze the above input, break it down into action_items to fulfil \n    the overall goal of the input.\n    \n    Carefully analyse the input to ensure you have factored it in its entirety.\n    \n    Format your response as a JSON object with the following structure:\n    {\n        \"summary\": \"A brief summary of the input\",\n        \"key_points\": [\"List of key points\"],\n        \"entities\": [\"List of important entities mentioned\"],\n        \"sentiment\": \"Overall sentiment (positive, negative, or neutral)\",\n        \"action_items\": [\"List of suggested actions based on the input\"]\n    }\n\"\"\"\n        }\n\n        for filename, content in default_prompts.items():\n            file_path = self.prompts_dir / filename\n            if not file_path.exists():\n                with file_path.open('w') as f:\n                    f.write(content.lstrip())\n                self.logger.info(f\"Created default prompt file: {filename}\")\n\n    def _load_prompts(self) -> None:\n        \"\"\"Load all prompt files from the prompts directory.\"\"\"\n        try:\n            self.prompts.clear()\n            for file_path in self.prompts_dir.glob('*.yml'):\n                with file_path.open('r') as f:\n                    prompts = yaml.safe_load(f)\n                    if prompts:\n                        # Handle template inheritance\n                        if 'extends' in prompts:\n                            base_file = prompts.pop('extends')\n                            with (self.prompts_dir / base_file).open('r') as base_f:\n                                base_prompts = yaml.safe_load(base_f)\n                                prompts = {**base_prompts, **prompts}\n                        \n                        self.prompts[file_path.stem] = prompts\n            \n            self.logger.info(f\"Loaded prompts from {self.prompts_dir}\")\n        except Exception as e:\n            self.logger.error(f\"Error loading prompts: {e}\")\n            raise\n\n    def get_prompt(self, prompt_id: str, **kwargs) -> Optional[str]:\n        \"\"\"\n        Retrieve and render a prompt with the given parameters.\n        \n        Args:\n            prompt_id (str): The identifier for the prompt template (e.g., \"task_generation.generate_tasks\")\n            **kwargs: The parameters to render the prompt with\n        \n        Returns:\n            Optional[str]: The rendered prompt, or None if the prompt_id is not found\n        \"\"\"\n        try:\n            category, name = prompt_id.split('.', 1)\n            if category in self.prompts and name in self.prompts[category]:\n                template = self.jinja_env.from_string(self.prompts[category][name])\n                return template.render(**kwargs)\n            else:\n                self.logger.error(f\"Prompt not found: {prompt_id}\")\n                return None\n        except Exception as e:\n            self.logger.error(f\"Error rendering prompt {prompt_id}: {e}\")\n            return None\n\n    def add_prompt(self, category: str, name: str, template: str) -> bool:\n        \"\"\"\n        Add a new prompt template to a category.\n        \n        Args:\n            category (str): The category for the prompt (e.g., \"task_generation\")\n            name (str): The name of the prompt\n            template (str): The prompt template string\n        \n        Returns:\n            bool: True if the prompt was added successfully, False otherwise\n        \"\"\"\n        try:\n            file_path = self.prompts_dir / f\"{category}.yml\"\n            \n            # Load existing prompts or create new dict\n            if file_path.exists():\n                with file_path.open('r') as f:\n                    prompts = yaml.safe_load(f) or {}\n            else:\n                prompts = {}\n            \n            # Add new prompt\n            prompts[name] = template\n            \n            # Save updated prompts\n            with file_path.open('w') as f:\n                yaml.dump(prompts, f, sort_keys=False, indent=2)\n            \n            # Reload prompts\n            self._load_prompts()\n            return True\n        except Exception as e:\n            self.logger.error(f\"Error adding prompt {category}.{name}: {e}\")\n            return False\n\n    def update_prompt(self, prompt_id: str, template: str) -> bool:\n        \"\"\"\n        Update an existing prompt template.\n        \n        Args:\n            prompt_id (str): The identifier for the prompt template (e.g., \"task_generation.generate_tasks\")\n            template (str): The new prompt template string\n        \n        Returns:\n            bool: True if the prompt was updated successfully, False otherwise\n        \"\"\"\n        try:\n            category, name = prompt_id.split('.', 1)\n            return self.add_prompt(category, name, template)\n        except Exception as e:\n            self.logger.error(f\"Error updating prompt {prompt_id}: {e}\")\n            return False\n\n    def list_prompts(self) -> Dict[str, Dict[str, str]]:\n        \"\"\"Return a dictionary of all available prompts.\"\"\"\n        return {\n            f\"{category}.{name}\": template\n            for category, prompts in self.prompts.items()\n            for name, template in prompts.items()\n        }\n\n    def get_categories(self) -> Dict[str, Any]:\n        \"\"\"Return the hierarchical structure of prompt categories.\"\"\"\n        return self.prompts\n\nif __name__ == \"__main__\":\n    # Example usage\n    logging.basicConfig(level=logging.INFO)\n    \n    prompt_manager = PromptManager()\n    \n    # Test adding a new prompt\n    prompt_manager.add_prompt(\n        \"custom\",\n        \"test\",\n        \"This is a test prompt with {{ parameter }}\"\n    )\n    \n    # Test retrieving and rendering a prompt\n    rendered_prompt = prompt_manager.get_prompt(\n        \"custom.test\",\n        parameter=\"example value\"\n    )\n    print(f\"Rendered prompt:\\n{rendered_prompt}\")\n    \n    # Test task generation prompt\n    task_prompt = prompt_manager.get_prompt(\n        \"task_generation.generate_tasks\",\n        goal=\"Write a blog post\",\n        context={\"recent_tasks\": []},\n        actions_str=\"Available actions...\"\n    )\n    print(f\"\\nTask generation prompt:\\n{task_prompt}\")\n    \n    # List all prompts\n    print(\"\\nAvailable prompts:\")\n    for prompt_id, template in prompt_manager.list_prompts().items():\n        print(f\"- {prompt_id}\")\n"}
{"type": "source_file", "path": "main.py", "content": "# main.py\n\nimport json\nimport logging\nimport os\nimport asyncio\nimport uvicorn\nfrom api.main import app\nfrom typing import Dict, Any\nfrom core.agent import Agent\nfrom actions.action_registry import get_action_registry\n\nlogger = logging.getLogger(\"Jiva.Main\")\n\ndef load_config(config_file: str = 'config.json') -> Dict[str, Any]:\n    \"\"\"\n    Load configuration from a JSON file, with fallback to default values.\n    \n    Args:\n    config_file (str): Path to the configuration JSON file. Defaults to 'config.json'.\n    \n    Returns:\n    Dict[str, Any]: A dictionary containing the configuration.\n    \"\"\"\n    # Default configuration\n    default_config = {\n        'memory': {\n            'qdrant_host': 'localhost',\n            'qdrant_port': 6333,\n            'collection_name': 'jiva_memories',\n            'max_short_term_memory': 100\n        },\n        'llm': {\n            'api_base_url': 'http://localhost:11434/api',\n            'model': 'gemma2',\n            'max_retries': 3,\n            'timeout': 90\n        },\n        'sensors': {\n            'chat_interface': {\n                'prompt': \"Jiva> \"\n            }\n        },\n        'memory_consolidation_threshold': 2,\n        'actions': {},\n        'agent_loop_delay': 0.1,\n        'awake_duration': 80,\n        'sleep_duration': 20,\n    }\n    \n    # Load configuration from file if it exists\n    if os.path.exists(config_file):\n        try:\n            with open(config_file, 'r') as f:\n                file_config = json.load(f)\n            \n            # Deep update of default config with file config\n            config = deep_update(default_config, file_config)\n            print(f\"Configuration loaded from {config_file}\")\n        except json.JSONDecodeError as e:\n            print(f\"Error decoding {config_file}: {e}. Using default configuration.\")\n            config = default_config\n        except Exception as e:\n            print(f\"Error reading {config_file}: {e}. Using default configuration.\")\n            config = default_config\n    else:\n        print(f\"Configuration file {config_file} not found. Using default configuration.\")\n        config = default_config\n    \n    return config\n\ndef deep_update(d: Dict[str, Any], u: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Perform a deep update of one dictionary with another.\n    \n    Args:\n    d (Dict[str, Any]): The original dictionary to update.\n    u (Dict[str, Any]): The dictionary with updates.\n    \n    Returns:\n    Dict[str, Any]: The updated dictionary.\n    \"\"\"\n    for k, v in u.items():\n        if isinstance(v, dict):\n            d[k] = deep_update(d.get(k, {}), v)\n        else:\n            d[k] = v\n    return d\n\ndef setup_environment():\n    # Create necessary directories\n    os.makedirs('data', exist_ok=True)\n    os.makedirs('logs', exist_ok=True)\n\ndef print_welcome_message():\n    infinity_symbol = \"\"\"\n      @@@@@@              @@@@@@        \n    @@      @@          @@      @@      \n  @@          @@      @@          @@    \n @@            @@    @@            @@   \n@@              @@  @@              @@  \n@@               @@@@               @@  \n@@              @@  @@              @@  \n @@            @@    @@            @@   \n  @@          @@      @@          @@    \n    @@      @@          @@      @@      \n      @@@@@@              @@@@@@        \n\"\"\"\n    print(infinity_symbol)\n    print(\"Welcome to the Jiva Framework!\")\n    print(\"Embracing the infinite potential of ethical AI\")\n    print(\"--------------------------------------------\")\n\nasync def run_agent_and_api(agent: Agent, host: str = \"0.0.0.0\", port: int = 8000):\n    \"\"\"\n    Run both the Jiva agent and the API server concurrently.\n    \n    Args:\n        agent (Agent): The Jiva agent instance\n        host (str): Host address to bind the API server\n        port (int): Port number for the API server\n    \"\"\"\n    # Store agent instance in FastAPI app state\n    app.state.agent = agent\n    \n    # Configure the uvicorn server\n    config = uvicorn.Config(\n        app,\n        host=host,\n        port=port,\n        loop=\"asyncio\",\n        log_level=\"info\"\n    )\n    server = uvicorn.Server(config)\n    \n    # Create shutdown event\n    shutdown_event = asyncio.Event()\n    \n    async def run_until_shutdown():\n        try:\n            await asyncio.gather(\n                agent.run(),\n                server.serve()\n            )\n        except asyncio.CancelledError:\n            logger.info(\"\\nShutting down Jiva...\")\n            server.should_exit = True\n            # Give the server a moment to shutdown\n            await asyncio.sleep(0.5)\n    \n    try:\n        await run_until_shutdown()\n    except KeyboardInterrupt:\n        logger.info(\"\\nShutdown complete\")\n\ndef main():\n    print(\"Initializing Jiva Framework...\")\n    setup_environment()\n    config = load_config()\n\n    # Get API configuration from environment or config\n    host = os.environ.get('API_HOST', config.get('api', {}).get('host', '0.0.0.0'))\n    port = int(os.environ.get('API_PORT', config.get('api', {}).get('port', 8000)))\n    \n    logger.info(\"Creating Agent...\")\n    agent = Agent(config)\n    \n    # Register file actions\n    actions = get_action_registry(agent.llm_interface, agent.memory)\n    for action_name, action_func in actions.items():\n        print(f\"Discovered action: {action_name}\")\n    \n    print_welcome_message()\n    print(\"Jiva is ready. Starting main loop...\")\n    logger.info(f\"Starting main loop and API server on {host}:{port}...\")\n    print(\"(Press CTRL+C to exit)\")\n    try:\n        asyncio.run(run_agent_and_api(agent, host, port))\n    except KeyboardInterrupt:\n        logger.info(\"\\nShutting down Jiva...\")\n    finally:\n        # Perform any cleanup if necessary\n        pass\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "actions/python_coder.py", "content": "import asyncio\nimport ast\nimport re\nimport sys\nimport os\nimport subprocess\nimport logging\nfrom typing import Dict, List, Any, Optional\n\nfrom core.llm_interface import LLMInterface\nfrom actions.file_operations import read_file, write_file\n\nlogger = logging.getLogger(__name__)\nllm_interface: Optional[LLMInterface] = None\n\ndef set_llm_interface(llm: LLMInterface):\n    \"\"\"Set the LLM interface for code generation functions.\"\"\"\n    global llm_interface\n    llm_interface = llm\n\ndef extract_python_code(llm_response: str) -> str:\n    \"\"\"\n    Extract Python code from an LLM response that contains markdown code blocks.\n    \n    Args:\n        llm_response (str): The full response from the LLM\n        \n    Returns:\n        str: The extracted Python code, or empty string if no code found\n    \"\"\"\n    # Find Python code blocks (```python ... ```)\n    python_blocks = re.findall(r'```(?:python)?\\s*([\\s\\S]*?)\\s*```', llm_response)\n    \n    if not python_blocks:\n        # If no code blocks with backticks, try to find the entire code\n        # This is a fallback in case the LLM didn't format with code blocks\n        if \"def \" in llm_response and \":\" in llm_response:\n            # Try to extract what looks like a function definition\n            return llm_response\n        return \"\"\n    \n    # Join multiple code blocks if present\n    return \"\\n\\n\".join(python_blocks)\n\nasync def generate_python_code(prompt: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n    \"\"\"\n    Generate Python code based on a prompt. This uses the LLM to create code\n    and extracts just the Python code from the response.\n    \n    Args:\n        prompt (str): Instruction for what code to generate\n        context (Optional[Dict[str, Any]]): Additional context\n        \n    Returns:\n        Dict[str, Any]: Result containing the generated code and explanation\n    \"\"\"\n    if not llm_interface:\n        return {\"success\": False, \"error\": \"LLM interface not set. Cannot generate code.\"}\n    \n    enhanced_prompt = f\"\"\"\n    Write Python code for the following:\n    {prompt}\n    \n    Your response should include:\n    1. A brief explanation of your approach\n    2. Complete, working Python code that addresses the requirement\n    3. Comments explaining any complex parts\n    \n    Present your code in a ```python code block and ensure it's functional and correct.\n    Make sure your code handles errors appropriately and follows best practices.\n    \"\"\"\n    \n    if context:\n        enhanced_prompt += f\"\\n\\nAdditional context:\\n{context}\"\n    \n    try:\n        llm_response = await llm_interface.generate(enhanced_prompt)\n        \n        # Extract the code\n        code = extract_python_code(llm_response)\n        \n        if not code:\n            return {\n                \"success\": False,\n                \"error\": \"Failed to extract Python code from the LLM response\",\n                \"full_response\": llm_response\n            }\n        \n        # Get the explanation (everything before the first code block)\n        explanation_match = re.match(r'(.*?)```', llm_response, re.DOTALL)\n        explanation = explanation_match.group(1).strip() if explanation_match else \"\"\n        \n        return {\n            \"success\": True,\n            \"code\": code,\n            \"explanation\": explanation,\n            \"full_response\": llm_response\n        }\n    except Exception as e:\n        logger.error(f\"Error generating Python code: {str(e)}\")\n        return {\"success\": False, \"error\": f\"Failed to generate code: {str(e)}\"}\n\nasync def write_python_code(file_path: str, code: str, description: str = \"\") -> Dict[str, Any]:\n    \"\"\"\n    Write Python code to a file. Validates syntax before writing.\n    \n    Args:\n        file_path (str): Path where the Python file should be created\n        code (str): Python code to write\n        description (str): Optional description of what the code does\n        \n    Returns:\n        Dict[str, Any]: Result indicating success or failure with details\n    \"\"\"\n    # Handle the case where code is a dictionary from generate_python_code\n    if isinstance(code, dict):\n        if 'code' in code:\n            code = code['code']\n        elif 'success' in code and not code.get('success', False):\n            return {\n                \"success\": False,\n                \"error\": f\"Cannot write Python code: {code.get('error', 'Unknown error')}\"\n            }\n    \n    # Create directory if it doesn't exist\n    dir_path = os.path.dirname(file_path)\n    if dir_path and not os.path.exists(dir_path):\n        os.makedirs(dir_path, exist_ok=True)\n    \n    # Validate Python syntax\n    try:\n        ast.parse(code)\n    except SyntaxError as e:\n        return {\n            \"success\": False,\n            \"error\": f\"Python code has syntax errors: {str(e)}\",\n            \"line\": e.lineno,\n            \"offset\": e.offset,\n            \"text\": e.text\n        }\n    \n    # Add description as doc comment if provided\n    if description:\n        doc_comment = f'\"\"\"\\n{description}\\n\"\"\"\\n\\n'\n        if not code.startswith('\"\"\"'):\n            code = doc_comment + code\n    \n    # Write the code to file\n    try:\n        with open(file_path, 'w') as f:\n            f.write(code)\n        \n        return {\n            \"success\": True,\n            \"message\": f\"Python code written to {file_path}\",\n            \"file_path\": file_path,\n            \"code_length\": len(code)\n        }\n    except Exception as e:\n        return {\n            \"success\": False,\n            \"error\": f\"Failed to write Python code: {str(e)}\"\n        }\n\nasync def execute_python_code(file_path: str, args: List[str] = None, timeout: int = 30) -> Dict[str, Any]:\n    \"\"\"\n    Execute a Python file and return the output.\n    \n    Args:\n        file_path (str): Path to the Python file to execute\n        args (List[str]): Optional command line arguments\n        timeout (int): Maximum execution time in seconds\n        \n    Returns:\n        Dict[str, Any]: The execution results including stdout, stderr, and return code\n    \"\"\"\n    if not os.path.exists(file_path):\n        return {\n            \"success\": False,\n            \"error\": f\"File not found: {file_path}\"\n        }\n    \n    # Check if the file contains valid Python code\n    try:\n        with open(file_path, 'r') as f:\n            code = f.read()\n        \n        # Check if it looks like a serialized object\n        if code.strip().startswith('{') and ('code' in code or 'success' in code):\n            # Try to extract actual Python code\n            try:\n                import json\n                data = json.loads(code)\n                if isinstance(data, dict) and 'code' in data:\n                    # Extract and save the actual code\n                    extracted_code = data['code']\n                    with open(file_path, 'w') as f:\n                        f.write(extracted_code)\n                    logger.info(f\"Fixed malformed Python file: {file_path}\")\n                else:\n                    # Try regex as a fallback\n                    import re\n                    code_match = re.search(r\"'code':\\s*'([^']+)'\", code)\n                    if code_match:\n                        # Extract and save the actual code\n                        extracted_code = code_match.group(1).replace('\\\\n', '\\n').replace('\\\\t', '\\t')\n                        with open(file_path, 'w') as f:\n                            f.write(extracted_code)\n                        logger.info(f\"Fixed malformed Python file with regex: {file_path}\")\n                    else:\n                        return {\n                            \"success\": False,\n                            \"error\": f\"File contains a serialized object, not valid Python code\"\n                        }\n            except Exception as e:\n                logger.warning(f\"Error fixing Python file content: {e}\")\n                return {\n                    \"success\": False,\n                    \"error\": f\"File appears to contain invalid content: {str(e)}\"\n                }\n    except Exception as e:\n        logger.warning(f\"Error checking Python file: {str(e)}\")\n    \n    # Execute the file\n    cmd = [sys.executable, file_path]\n    if args:\n        cmd.extend(args)\n    \n    try:\n        process = await asyncio.create_subprocess_exec(\n            *cmd,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE,\n            limit=1024 * 1024  # 1MB limit for output\n        )\n        \n        try:\n            stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=timeout)\n            stdout_str = stdout.decode('utf-8', errors='replace')\n            stderr_str = stderr.decode('utf-8', errors='replace')\n            \n            if process.returncode != 0:\n                return {\n                    \"success\": False,\n                    \"stdout\": stdout_str,\n                    \"stderr\": stderr_str,\n                    \"returncode\": process.returncode,\n                    \"error\": f\"Process exited with code {process.returncode}\"\n                }\n            \n            return {\n                \"success\": True,\n                \"stdout\": stdout_str,\n                \"stderr\": stderr_str,\n                \"returncode\": process.returncode\n            }\n            \n        except asyncio.TimeoutError:\n            try:\n                process.kill()\n            except:\n                pass\n            return {\n                \"success\": False,\n                \"error\": f\"Execution timed out after {timeout} seconds\"\n            }\n            \n    except Exception as e:\n        return {\n            \"success\": False,\n            \"error\": f\"Error executing Python code: {str(e)}\"\n        }\n\nasync def analyze_python_code(file_path: str) -> Dict[str, Any]:\n    \"\"\"\n    Analyze Python code for errors, potential improvements, and best practices.\n    \n    Args:\n        file_path (str): Path to the Python file to analyze\n        \n    Returns:\n        Dict[str, Any]: Analysis results with suggestions\n    \"\"\"\n    if not llm_interface:\n        return {\"error\": \"LLM interface not set. Cannot analyze code.\"}\n    \n    if not os.path.exists(file_path):\n        return {\"error\": f\"File not found: {file_path}\"}\n    \n    try:\n        code = await read_file(file_path)\n        if isinstance(code, dict) and 'error' in code:\n            return {\"error\": f\"Failed to read file: {code['error']}\"}\n        \n        # Check syntax\n        try:\n            ast.parse(code)\n        except SyntaxError as e:\n            return {\n                \"success\": False,\n                \"has_syntax_errors\": True,\n                \"error\": f\"Syntax error at line {e.lineno}, column {e.offset}: {e.msg}\",\n                \"line\": e.lineno,\n                \"text\": e.text\n            }\n        \n        # Use LLM to analyze the code\n        prompt = f\"\"\"\n        Analyze the following Python code for:\n        1. Potential bugs or errors\n        2. Performance improvements\n        3. Best practices and PEP8 compliance\n        4. Security concerns\n        5. Overall code quality\n\n        Provide specific suggestions for improvements.\n\n        Python code to analyze:\n        ```python\n        {code}\n        ```\n        \n        Format your response as a detailed analysis with sections for each category.\n        Include line numbers when referring to specific code.\n        \"\"\"\n        \n        analysis = await llm_interface.generate(prompt)\n        \n        return {\n            \"success\": True,\n            \"has_syntax_errors\": False,\n            \"analysis\": analysis,\n            \"code\": code\n        }\n        \n    except Exception as e:\n        return {\"error\": f\"Error analyzing Python code: {str(e)}\"}\n\nasync def test_python_function(file_path: str, function_name: str, test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"\n    Test a Python function with multiple test cases.\n    \n    Args:\n        file_path (str): Path to the Python file containing the function\n        function_name (str): Name of the function to test\n        test_cases (List[Dict[str, Any]]): List of test cases, each with 'inputs' and 'expected_output'\n        \n    Returns:\n        Dict[str, Any]: Test results for each test case\n    \"\"\"\n    if not os.path.exists(file_path):\n        return {\"error\": f\"File not found: {file_path}\"}\n    \n    # Create a temporary test file\n    import tempfile\n    \n    try:\n        module_name = os.path.basename(file_path).replace('.py', '')\n        \n        with tempfile.NamedTemporaryFile(suffix='.py', mode='w', delete=False) as test_file:\n            test_file_path = test_file.name\n            \n            # Write test code\n            test_code = f\"\"\"\nimport sys\nimport json\nimport traceback\nfrom pathlib import Path\n\n# Add the directory containing the module to Python path\nfile_dir = Path(r'{os.path.dirname(os.path.abspath(file_path))}')\nif str(file_dir) not in sys.path:\n    sys.path.insert(0, str(file_dir))\n\ntry:\n    # Import the function\n    from {module_name} import {function_name}\n    \n    # Run test cases\n    results = []\n    \n    test_cases = {test_cases}\n    \n    for i, test_case in enumerate(test_cases):\n        try:\n            inputs = test_case['inputs']\n            expected = test_case['expected_output']\n            \n            # Handle different input types\n            if isinstance(inputs, list):\n                actual = {function_name}(*inputs)\n            elif isinstance(inputs, dict):\n                actual = {function_name}(**inputs)\n            else:\n                actual = {function_name}(inputs)\n            \n            success = actual == expected\n            \n            results.append({{\n                'test_case': i + 1,\n                'inputs': inputs,\n                'expected': expected,\n                'actual': actual,\n                'success': success\n            }})\n        except Exception as e:\n            results.append({{\n                'test_case': i + 1,\n                'inputs': inputs,\n                'error': str(e),\n                'traceback': traceback.format_exc(),\n                'success': False\n            }})\n    \n    print(json.dumps(results))\n    \nexcept Exception as e:\n    print(json.dumps({{\n        'error': str(e),\n        'traceback': traceback.format_exc()\n    }}))\n\"\"\"\n            test_file.write(test_code)\n        \n        # Execute the test file\n        result = await execute_python_code(test_file_path)\n        \n        # Clean up the temporary file\n        try:\n            os.unlink(test_file_path)\n        except:\n            pass\n        \n        if not result['success']:\n            return {\n                \"success\": False,\n                \"error\": f\"Error running tests: {result.get('stderr', result.get('error', 'Unknown error'))}\"\n            }\n        \n        # Parse the test results\n        try:\n            test_results = json.loads(result['stdout'])\n            \n            # Check if there was an error importing the function\n            if 'error' in test_results and 'traceback' in test_results:\n                return {\n                    \"success\": False,\n                    \"error\": test_results['error'],\n                    \"traceback\": test_results['traceback']\n                }\n            \n            # Count successes\n            successful_tests = sum(1 for r in test_results if r.get('success', False))\n            \n            return {\n                \"success\": True,\n                \"total_tests\": len(test_results),\n                \"successful_tests\": successful_tests,\n                \"test_results\": test_results\n            }\n            \n        except json.JSONDecodeError:\n            return {\n                \"success\": False,\n                \"error\": \"Failed to parse test results\",\n                \"stdout\": result['stdout']\n            }\n            \n    except Exception as e:\n        return {\"error\": f\"Error testing Python function: {str(e)}\"}\n"}
{"type": "source_file", "path": "core/memory.py", "content": "# core/memory.py\n\nfrom typing import Any, Dict, List, Optional\nfrom datetime import datetime\nimport json\nimport logging\n\nfrom core.llm_interface import LLMInterface\nfrom utils.qdrant_handler import QdrantHandler\n\nclass DateTimeEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return obj.isoformat()\n        return super().default(obj)\n\nclass Memory:\n    def __init__(self, config: Dict[str, Any], llm_interface: LLMInterface):\n        self.config = config\n        self.llm_interface = llm_interface\n        self.short_term_memory: List[Dict[str, Any]] = []\n        self.vector_size = config.get('vector_size', 3072)\n        self.max_short_term_memory = config.get('max_short_term_memory', 100)\n        self.logger = logging.getLogger(\"Jiva.Memory\")\n        self.json_encoder = DateTimeEncoder()\n        try:\n            self.qdrant_handler = QdrantHandler(\n                config['qdrant_host'],\n                config['qdrant_port'],\n                config['collection_name'],\n                self.vector_size\n            )\n        except Exception as e:\n            self.logger.error('Qdrant could not be connected to. Jiva will operate with limited memory capabilities.')\n\n    def add_to_short_term(self, data: Dict[str, Any]):\n        \"\"\"Add a memory item to short-term memory.\"\"\"\n        timestamp = datetime.now().isoformat()\n        memory_item = {\n            'timestamp': timestamp,\n            'data': data\n        }\n        self.short_term_memory.append(memory_item)\n        self.logger.debug(f\"Added to short-term memory: {self.json_encoder.encode(memory_item)}\")\n        \n        if len(self.short_term_memory) > self.max_short_term_memory:\n            self._transfer_to_long_term(self.short_term_memory.pop(0))\n\n    async def _transfer_to_long_term(self, memory_item: Dict[str, Any]):\n        try:\n            serialized_item = self.json_encoder.encode(memory_item)\n            embedding = await self.llm_interface.get_embedding(serialized_item)\n            \n            if len(embedding) != self.vector_size:\n                self.logger.error(f\"Embedding size mismatch. Expected {self.vector_size}, got {len(embedding)}\")\n                return\n            \n            point_id = await self.qdrant_handler.add_point(\n                vector=embedding,\n                payload=json.loads(serialized_item)\n            )\n            if point_id:\n                self.logger.info(f\"Transferred memory to long-term storage: {point_id}\")\n            else:\n                self.logger.warning(\"Failed to transfer memory to long-term storage\")\n        except Exception as e:\n            self.logger.error(f\"Failed to transfer memory to long-term storage: {e}\")\n\n    def get_short_term_memory(self) -> List[Dict[str, Any]]:\n        \"\"\"Retrieve all items from short-term memory.\"\"\"\n        return self.short_term_memory\n\n    def get_recent_short_term_memory(self, n: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Retrieve the n most recent items from short-term memory.\"\"\"\n        return self.short_term_memory[-n:]\n\n    async def consolidate(self):\n        self.logger.info(f\"Consolidating {len(self.short_term_memory)} memories\")\n        for memory_item in self.short_term_memory:\n            await self._transfer_to_long_term(memory_item)\n        self.short_term_memory.clear()\n        self.logger.info(\"Memory consolidation completed\")\n\n    async def query_long_term_memory(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Query long-term memory based on semantic similarity.\"\"\"\n        try:\n            query_embedding = await self.llm_interface.get_embedding(query)\n            results = await self.qdrant_handler.search(\n                query_vector=query_embedding,\n                limit=limit\n            )\n            return [result.payload for result in results]\n        except Exception as e:\n            self.logger.error(f\"Failed to query long-term memory: {e}\")\n            return []\n\n    async def prepare_fine_tuning_dataset(self) -> List[Dict[str, Any]]:\n        \"\"\"Prepare a dataset for fine-tuning based on recent memories.\"\"\"\n        # Need to await the async query_long_term_memory call\n        long_term_memories = await self.query_long_term_memory(\"\", limit=100)\n        recent_memories = self.short_term_memory + long_term_memories\n        return recent_memories\n\n    def forget(self, threshold: float):\n        \"\"\"Remove old or less relevant memories from long-term storage.\"\"\"\n        # This is a placeholder. Implementation would depend on your specific\n        # criteria for \"forgetting\" and the Qdrant API's capabilities.\n        pass\n\n    def update_long_term_memory(self, memory_id: str, updated_data: Dict[str, Any]):\n        \"\"\"Update a specific memory in long-term storage.\"\"\"\n        try:\n            embedding = self.llm_interface.get_embedding(json.dumps(updated_data))\n            self.qdrant_handler.update_point(\n                id=memory_id,\n                vector=embedding,\n                payload={'data': updated_data, 'timestamp': datetime.now().isoformat()}\n            )\n            self.logger.info(f\"Updated long-term memory: {memory_id}\")\n        except Exception as e:\n            self.logger.error(f\"Failed to update long-term memory: {e}\")\n\n    def get_task_result(self, task_description: str) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Retrieve the result of the most recent task matching the given description.\n        \n        Args:\n            task_description (str): The description of the task to retrieve.\n        \n        Returns:\n            Optional[Dict[str, Any]]: The task result if found, None otherwise.\n        \"\"\"\n        # Search in short-term memory first\n        for memory_item in reversed(self.short_term_memory):\n            if memory_item['data'].get('type') == 'task_result' and \\\n               memory_item['data'].get('description') == task_description:\n                return {\n                    'result': memory_item['data'].get('result'),\n                    'task_id': memory_item['data'].get('task_id'),\n                    'timestamp': memory_item['timestamp']\n                }\n        \n        # If not found in short-term memory, check long-term memory\n        query = f\"type:task_result AND description:\\\"{task_description}\\\"\"\n        long_term_results = self.query_long_term_memory(query, limit=1)\n        if long_term_results:\n            result = long_term_results[0]\n            return {\n                'result': result.get('result'),\n                'task_id': result.get('task_id'),\n                'timestamp': result.get('timestamp')\n            }\n        \n        return None\n\n    async def get_context_for_task(self, task_description: str, n: int = 5) -> Dict[str, Any]:\n        \"\"\"Retrieve relevant context for a task from both short-term and long-term memory.\"\"\"\n        context = {\n            \"short_term\": self.get_recent_short_term_memory(n),\n            \"long_term\": self.query_long_term_memory(task_description, n)\n        }\n        return context\n\nif __name__ == \"__main__\":\n    # This allows us to run some basic tests\n    from unittest.mock import MagicMock\n\n    # Mock LLMInterface and QdrantHandler\n    mock_llm = MagicMock()\n    mock_llm.get_embedding.return_value = [0.1] * 3072\n\n    config = {\n        'qdrant_host': 'localhost',\n        'qdrant_port': 6333,\n        'collection_name': 'test_collection',\n        'max_short_term_memory': 5\n    }\n\n    memory = Memory(config, mock_llm)\n\n    # Test adding to short-term memory\n    for i in range(7):\n        memory.add_to_short_term({\"test_data\": f\"Data {i}\"})\n\n    print(f\"Short-term memory size: {len(memory.get_short_term_memory())}\")\n\n    # Test querying long-term memory\n    mock_llm.get_embedding.return_value = [0.2] * 3072\n    results = memory.query_long_term_memory(\"test query\")\n    print(f\"Long-term memory query results: {results}\")\n\n    # Test getting task result\n    memory.add_to_short_term({\"task_id\": \"test_task\", \"result\": {\"output\": \"Test output\"}})\n    result = memory.get_task_result(\"test_task\")\n    print(f\"Task result: {result}\")\n\n    # Test getting context for task\n    context = memory.get_context_for_task(\"Test task description\")\n    print(f\"Context for task: {context}\")\n\n    print(\"Memory tests completed.\")\n"}
{"type": "source_file", "path": "actions/web_interface.py", "content": "# actions/web_interface.py\n\nfrom typing import Dict, Any, List\nfrom bs4 import BeautifulSoup\nfrom playwright.async_api import async_playwright, TimeoutError\nimport aiohttp\nfrom googlesearch import search\nfrom markdownify import markdownify as md\nimport logging\nimport asyncio\n\nfrom core.llm_interface import LLMInterface\n\nlogger = logging.getLogger(__name__)\nllm_interface: LLMInterface = None\n\ndef set_llm_interface(llm: LLMInterface):\n    global llm_interface\n    llm_interface = llm\n\nasync def web_search(query: str, num_results: int = 5) -> List[Dict[str, str]]:\n    \"\"\"\n    Perform a web search and return a list of results.\n\n    Args:\n        query (str): The search query.\n        num_results (int): The number of search results to return. Defaults to 5.\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries containing 'title', 'url', 'description' and 'relevant_content' for each result.\n    \"\"\"\n    try:\n        search_results = []\n        async with aiohttp.ClientSession() as session:\n            for result in search(query, num_results=num_results):\n                try:\n                    async with session.get(result, timeout=5) as response:\n                        html = await response.text()\n                        soup = BeautifulSoup(html, 'html.parser')\n                        title = soup.title.string if soup.title else result\n                        description = soup.find('meta', attrs={'name': 'description'})\n                        description = description['content'] if description else \"No description available.\"\n\n                        page_content = await visit_page(result)\n                        relevant_content = await extract_relevant_content(page_content, query)\n                        \n                        search_results.append({\n                            'url': result,\n                            'title': title[:100],  # Truncate long titles\n                            'description': description[:200],  # Truncate long descriptions\n                            'relevant_content': relevant_content\n                        })\n                except Exception as e:\n                    logger.error(f\"Error fetching details for {result}: {str(e)}\")\n                    search_results.append({\n                        'url': result,\n                        'title': result,\n                        'description': \"Unable to fetch details\"\n                    })\n    except Exception as e:\n        logger.error(f\"Error in web search: {str(e)}\")\n        return []\n    return search_results\n\nasync def visit_page(url: str, wait_for_selector: str = None, timeout: int = 30000) -> str:\n    \"\"\"\n    Visit a web page and return its content as markdown.\n\n    Args:\n        url (str): The URL of the page to visit.\n        wait_for_selector (str, optional): A CSS selector to wait for before considering the page loaded.\n        timeout (int): Maximum time to wait for the page to load, in milliseconds. Defaults to 30000 (30 seconds).\n\n    Returns:\n        str: The page content converted to markdown.\n    \"\"\"\n    try:\n        async with async_playwright() as p:\n            browser = await p.chromium.launch()\n            page = await browser.new_page()\n            await page.goto(url, wait_until=\"networkidle\", timeout=timeout)\n            \n            if wait_for_selector:\n                try:\n                    await page.wait_for_selector(wait_for_selector, timeout=timeout)\n                except TimeoutError:\n                    logger.warning(f\"Timeout waiting for selector '{wait_for_selector}' on {url}\")\n            \n            content = await page.content()\n            await browser.close()\n        \n        return md(content)\n    except Exception as e:\n        logger.error(f\"Error visiting page {url}: {str(e)}\")\n        return f\"Error: Unable to visit page {url}\"\n    \nasync def extract_relevant_content(markdown_content: str, query: str) -> str:\n    \"\"\"\n    Use the LLM to extract the most relevant content from the markdown based on the search query.\n\n    Args:\n        markdown_content (str): The full page content in markdown format.\n        query (str): The original search query.\n        llm_interface (Any): An interface to the language model for content extraction.\n\n    Returns:\n        str: The most relevant content extracted from the page.\n    \"\"\"\n    global llm_interface\n\n    # Prepare the prompt for the LLM\n    prompt = f\"\"\"\n    Given the following web page content and a search query, extract the most relevant information that answers the query. \n    Provide a concise summary (about 3-4 paragraphs) of the relevant information, maintaining the original markdown formatting where appropriate.\n\n    Search Query: {query}\n\n    Web Page Content:\n    {markdown_content[:8000]}  # Limit content to avoid exceeding token limits\n\n    Relevant Information (in markdown format):\n    \"\"\"\n\n    # Use the LLM to extract relevant content\n    try:\n        relevant_content = await llm_interface.generate(prompt)\n    except Exception as e:\n        logger.error(f\"Error using LLM for content extraction: {str(e)}\")\n        relevant_content = \"Error: Unable to extract relevant content using LLM\"\n\n    return relevant_content\n\nasync def find_links(url: str, timeout: int = 30000) -> List[Dict[str, str]]:\n    \"\"\"\n    Find relevant links on a given web page.\n\n    Args:\n        url (str): The URL of the page to analyze.\n        timeout (int): Maximum time to wait for the page to load, in milliseconds. Defaults to 30000 (30 seconds).\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries containing 'text' and 'href' for each link found.\n    \"\"\"\n    try:\n        async with async_playwright() as p:\n            browser = await p.chromium.launch()\n            page = await browser.new_page()\n            await page.goto(url, wait_until=\"networkidle\", timeout=timeout)\n            links = await page.evaluate(\"\"\"\n                () => Array.from(document.querySelectorAll('a')).map(a => ({\n                    text: a.innerText,\n                    href: a.href\n                }))\n            \"\"\")\n            await browser.close()\n        \n        return links\n    except Exception as e:\n        logger.error(f\"Error finding links on page {url}: {str(e)}\")\n        return []\n\n# Example usage with async/await:\n\"\"\"\nasync def example():\n    # Search results\n    results = await web_search(\"Artificial Intelligence\")\n    \n    # Visit a page\n    content = await visit_page(\"https://www.example.com\")\n    \n    # Visit a page with specific element\n    content_with_element = await visit_page(\n        \"https://www.example.com\", \n        wait_for_selector=\"#specific-element\"\n    )\n    \n    # Find links\n    page_links = await find_links(\"https://www.example.com\")\n\"\"\"\n"}
{"type": "source_file", "path": "api/main.py", "content": "# api/main.py\n\nimport asyncio\nfrom fastapi import FastAPI, Depends, HTTPException\nfrom fastapi.responses import JSONResponse\nfrom typing import Dict, Any, List, Optional\nimport logging\nfrom datetime import datetime\n\n# Initialize FastAPI app\napp = FastAPI(\n    title=\"Jiva Agent API\",\n    description=\"REST API for interacting with Jiva agents\",\n    version=\"1.0.0\"\n)\n\nlogger = logging.getLogger(\"Jiva.API\")\n\ndef get_agent():\n    \"\"\"Dependency to get the agent instance from app state.\"\"\"\n    if not hasattr(app.state, 'agent'):\n        raise HTTPException(status_code=503, detail=\"Agent not initialized\")\n    return app.state.agent\n\n@app.get(\"/status\", response_model=Dict[str, Any])\nasync def get_status(agent = Depends(get_agent)) -> Dict[str, Any]:\n    \"\"\"Get the current status of the Jiva agent.\"\"\"\n    try:\n        return {\n            \"status\": \"Working\" if agent.task_manager.has_pending_tasks() else \"Idle\",\n            \"current_goal\": agent.current_goal,\n            \"pending_tasks\": [\n                {\n                    \"id\": task.id,\n                    \"description\": task.description,\n                    \"status\": task.status,\n                    \"created_at\": task.created_at.isoformat()\n                } for task in agent.task_manager.task_queue.queue\n            ] if agent.task_manager.has_pending_tasks() else []\n        }\n    except Exception as e:\n        logger.error(f\"Error getting agent status: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/goals\", response_model=List[Dict[str, Any]])\nasync def get_goals(agent = Depends(get_agent)) -> List[Dict[str, Any]]:\n    \"\"\"Get list of all goals, ordered by latest first.\"\"\"\n    try:\n        # Get tasks from memory that have goal information\n        goal_tasks = []\n        for task_id, task in agent.task_manager.all_tasks.items():\n            if task.goal and not any(g[\"goal\"] == task.goal for g in goal_tasks):\n                goal_tasks.append({\n                    \"goal\": task.goal,\n                    \"created_at\": task.created_at.isoformat(),\n                    \"status\": \"completed\" if all(t.status == \"completed\" \n                             for t in agent.task_manager.all_tasks.values() \n                             if t.goal == task.goal) else \"in_progress\",\n                    \"first_task_id\": task_id\n                })\n        \n        # Sort by creation time, latest first\n        return sorted(goal_tasks, key=lambda x: x[\"created_at\"], reverse=True)\n    except Exception as e:\n        logger.error(f\"Error getting goals: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/tasks\", response_model=List[Dict[str, Any]])\nasync def get_tasks(goal: Optional[str] = None, agent = Depends(get_agent)) -> List[Dict[str, Any]]:\n    \"\"\"Get list of tasks, optionally filtered by goal.\"\"\"\n    try:\n        tasks = []\n        for task_id, task in agent.task_manager.all_tasks.items():\n            if goal is None or task.goal == goal:\n                tasks.append({\n                    \"id\": task_id,\n                    \"description\": task.description,\n                    \"status\": task.status,\n                    \"created_at\": task.created_at.isoformat(),\n                    \"completed_at\": task.completed_at.isoformat() if task.completed_at else None,\n                    \"goal\": task.goal,\n                    \"action\": task.action,\n                    \"result\": str(task.result) if task.result else None\n                })\n        \n        # Sort by creation time, latest first\n        return sorted(tasks, key=lambda x: x[\"created_at\"], reverse=True)\n    except Exception as e:\n        logger.error(f\"Error getting tasks: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/goal\", response_model=Dict[str, Any])\nasync def create_goal(goal: Dict[str, str], agent = Depends(get_agent)) -> Dict[str, Any]:\n    \"\"\"Create a new goal for the agent.\"\"\"\n    try:\n        if 'description' not in goal:\n            raise HTTPException(status_code=400, detail=\"Goal description is required\")\n        \n        # Process the goal like a regular input\n        input_data = [{\n            \"type\": \"goal\",\n            \"content\": goal['description'],\n            \"timestamp\": datetime.now().isoformat()\n        }]\n        \n        await agent.process_input(input_data)\n        \n        # Wait a short time to allow initial task generation\n        await asyncio.sleep(0.5)\n        \n        # Get the initial set of tasks\n        initial_tasks = [\n            {\n                \"id\": task.id,\n                \"description\": task.description,\n                \"status\": task.status\n            }\n            for task in agent.task_manager.task_queue.queue\n        ] if agent.task_manager.has_pending_tasks() else []\n        \n        return {\n            \"status\": \"success\",\n            \"message\": \"Goal created successfully\",\n            \"goal\": goal['description'],\n            \"initial_tasks\": initial_tasks\n        }\n    except Exception as e:\n        logger.error(f\"Error creating goal: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/about\", response_model=Dict[str, Any])\nasync def get_about(agent = Depends(get_agent)) -> Dict[str, Any]:\n    \"\"\"Get information about the agent and its configuration.\"\"\"\n    try:\n        # Get config without sensitive information\n        config = agent.config.copy()\n        # Remove sensitive fields\n        for provider in ['llm', 'mistral-llm']:\n            if provider in config:\n                if 'api_key' in config[provider]:\n                    config[provider]['api_key'] = '***'\n        \n        # Get available actions\n        actions = agent.action_manager.get_available_actions()\n        # Clean up action info for API response\n        clean_actions = {}\n        for action_name, action_info in actions.items():\n            clean_actions[action_name] = {\n                \"description\": action_info[\"description\"],\n                \"parameters\": action_info[\"parameters\"]\n            }\n        \n        return {\n            \"config\": config,\n            \"available_actions\": clean_actions,\n            \"sensors\": list(agent.sensor_manager.get_available_sensors())\n        }\n    except Exception as e:\n        logger.error(f\"Error getting agent information: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Error handling\n@app.exception_handler(Exception)\nasync def generic_exception_handler(request, exc):\n    logger.error(f\"Unhandled exception: {str(exc)}\")\n    return JSONResponse(\n        status_code=500,\n        content={\"detail\": \"Internal server error\"}\n    )\n\n@app.post(\"/trigger\", response_model=Dict[str, Any])\nasync def trigger_agent(agent = Depends(get_agent)) -> Dict[str, Any]:\n    \"\"\"Trigger the agent to check and execute pending tasks if it's idle.\"\"\"\n    try:\n        if not agent.is_awake:\n            raise HTTPException(\n                status_code=400, \n                detail=\"Agent is sleeping and cannot be triggered\"\n            )\n\n        # First get all pending tasks\n        pending_tasks = agent.task_manager.get_pending_tasks()\n        \n        if not pending_tasks:\n            return {\n                \"status\": \"no_action\",\n                \"message\": \"No pending tasks to execute\",\n                \"debug\": {\n                    \"queue_size\": 0,\n                    \"queue_tasks\": [],\n                    \"all_pending_count\": 0,\n                    \"all_pending\": [],\n                    \"has_pending_tasks\": False\n                }\n            }\n        \n        # Requeue the pending tasks\n        agent.task_manager.requeue_pending_tasks()\n        \n        # Signal the agent to check for tasks\n        agent._task_trigger.set()\n        \n        return {\n            \"status\": \"triggered\",\n            \"message\": f\"Agent triggered to execute {len(pending_tasks)} pending tasks\",\n            \"debug\": {\n                \"queue_size\": len(pending_tasks),\n                \"queue_tasks\": [\n                    {\n                        \"id\": task.id,\n                        \"description\": task.description,\n                        \"status\": task.status,\n                        \"created_at\": task.created_at.isoformat()\n                    } for task in pending_tasks\n                ],\n                \"all_pending_count\": len(pending_tasks),\n                \"all_pending\": [\n                    {\n                        \"id\": task.id,\n                        \"description\": task.description\n                    } for task in pending_tasks\n                ],\n                \"has_pending_tasks\": True\n            }\n        }\n            \n    except Exception as e:\n        logger.error(f\"Error triggering agent: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/about\", response_model=Dict[str, Any])\nasync def get_about(agent = Depends(get_agent)) -> Dict[str, Any]:\n    \"\"\"Get information about the agent and its configuration.\"\"\"\n    try:\n        # Get config without sensitive information\n        config = agent.config.copy()\n        # Remove sensitive fields\n        for provider in ['llm', 'mistral-llm']:\n            if provider in config:\n                if 'api_key' in config[provider]:\n                    config[provider]['api_key'] = '***'\n        \n        # Get available actions\n        actions = agent.action_manager.get_available_actions()\n        clean_actions = {}\n        for action_name, action_info in actions.items():\n            clean_actions[action_name] = {\n                \"description\": action_info[\"description\"],\n                \"parameters\": action_info[\"parameters\"]\n            }\n\n        # Add sleep cycle status\n        status = {\n            \"sleep_cycle\": {\n                \"enabled\": agent.sleep_config.get('enabled', False),\n                \"is_awake\": agent.is_awake,\n                \"last_sleep_time\": agent.last_sleep_time.isoformat() if agent.last_sleep_time else None\n            }\n        }\n        \n        return {\n            \"config\": config,\n            \"available_actions\": clean_actions,\n            \"sensors\": list(agent.sensor_manager.get_available_sensors()),\n            \"status\": status\n        }\n    except Exception as e:\n        logger.error(f\"Error getting agent information: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n"}
{"type": "source_file", "path": "actions/think.py", "content": "# actions/think.py\n\nimport asyncio\nfrom typing import Dict, Any, Optional\nfrom core.llm_interface import LLMInterface\n\nllm_interface: LLMInterface = None\n\ndef set_llm_interface(llm: LLMInterface):\n    global llm_interface\n    llm_interface = llm\n\nasync def think(prompt: str, context: Optional[Dict[str, Any]] = None) -> str:\n    \"\"\"\n    Use the LLM to generate a response based on a prompt and optional context. If the prompt depends on the result from a prior task, then infuse the results\n    of the prior task using a {{placeholder}} into the prompt or the context, and then correctly link it in required inputs.\n\n    Args:\n        prompt (str): The prompt to send to the LLM. Preferrably, keep this as static text when required_inputs is empty.\n        context (Optional[Dict[str, Any]]): Additional context for the prompt. Defaults to None.\n\n    Returns:\n        str: The generated response from the LLM.\n    \"\"\"\n    full_prompt = f\"Context: {context}\\n\\nPrompt: {prompt}\" if context else prompt\n    return await llm_interface.generate(full_prompt)\n\nasync def replan_tasks():\n    \"\"\"\n    This method is used as a placeholder in a task list to state that replanning of the tasks will be needed after this point.\n    This method takes no arguments and returns an empty string since it is only meant to be a placeholder.\n    \"\"\"\n    return ''\n\nasync def sleep(seconds: int):\n    \"\"\"\n    This method is used as a placeholder in a task list to state that task execution must be paused for a given amount of time.\n\n    Args:\n        seconds (int): The number of seconds to pause for.\n    Returns:\n        str: This method returns an empty string since it is only meant to be a placeholder.\n    \"\"\"\n    # Code to sleep an async method for x seconds\n    await asyncio.sleep(seconds)\n\n    return ''\n\nasync def rerun_tasks(task_name: str) -> str:\n    \"\"\"\n    Mark a previous task as the resumption point for task execution.\n\n    Args:\n        task_name (str): Exact description of the task from which to resume execution.\n\n    Returns:\n        str: Name of the task to resume from.\n    \"\"\"\n    return task_name\n\n# Example usage:\n# story = think(llm_interface, \"Write a short story about two friends\", {\"genre\": \"comedy\", \"word_limit\": 200})\n"}
{"type": "source_file", "path": "core/action_manager.py", "content": "# core/action_manager.py\n\nfrom typing import Dict, Any, Callable\nimport logging\nfrom core.ethical_framework import EthicalFramework\nfrom core.memory import Memory\nfrom core.llm_interface import LLMInterface\nfrom actions.action_registry import get_action_registry\n\nclass ActionManager:\n    def __init__(self, ethical_framework: EthicalFramework, memory: Memory, llm_interface: LLMInterface):\n        self.ethical_framework = ethical_framework\n        self.memory = memory\n        self.llm_interface = llm_interface\n        self.actions: Dict[str, Callable] = get_action_registry(llm_interface, memory)\n        self.logger = logging.getLogger(\"Jiva.ActionManager\")\n\n    async def execute_action(self, action_name: str, parameters: Dict[str, Any]) -> Any:\n        \"\"\"Execute an action if it's ethical.\"\"\"\n        if action_name not in self.actions:\n            error_msg = f\"Action '{action_name}' is not registered.\"\n            self.logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        # Evaluate the action ethically\n        if await self.ethical_framework.evaluate_action(action_name, parameters):\n            self.logger.info(f\"Executing action: {action_name}\")\n            try:\n                # Retrieve context for the action\n                # context = self.memory.get_context_for_task(f\"Action: {action_name}\")\n                \n                # Add context to parameters\n                # parameters['context'] = context\n\n                # Execute the action\n                result = await self.actions[action_name](**parameters)\n                \n                # Store the result in memory\n                self.memory.add_to_short_term({\n                    \"action\": action_name,\n                    \"parameters\": parameters,\n                    \"result\": result\n                })\n                \n                self.logger.info(f\"Action '{action_name}' executed successfully\")\n                return result\n            except Exception as e:\n                error_msg = f\"Error executing action '{action_name}': {str(e)}\"\n                self.logger.error(error_msg)\n                return {\"error\": error_msg}\n        else:\n            # If the action is deemed unethical, don't execute it\n            ethical_explanation = await self.ethical_framework.get_ethical_explanation(f\"{action_name}: {parameters}\", is_task=False)\n            error_msg = f\"Action not executed due to ethical concerns: {ethical_explanation}\"\n            self.logger.warning(error_msg)\n            return {\"error\": error_msg}\n\n    def get_available_actions(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get a dictionary of all available actions with their descriptions and parameters.\"\"\"\n        action_info = {}\n        for name, func in self.actions.items():\n            doc = func.__doc__ or \"No description available.\"\n            params = self._get_function_parameters(func)\n            action_info[name] = {\n                \"description\": doc,\n                \"parameters\": params\n            }\n        return action_info\n\n    def _get_function_parameters(self, func: Callable) -> Dict[str, str]:\n        \"\"\"Extract parameter names and annotations from a function.\"\"\"\n        import inspect\n        params = {}\n        signature = inspect.signature(func)\n        for name, param in signature.parameters.items():\n            if name not in ['self', 'cls']:\n                params[name] = str(param.annotation) if param.annotation != inspect.Parameter.empty else \"Any\"\n        return params\n\n    def get_action_ethical_summary(self, action_name: str, parameters: Dict[str, Any]) -> str:\n        \"\"\"Get an ethical summary for a specific action.\"\"\"\n        if action_name not in self.actions:\n            return f\"Action '{action_name}' is not registered.\"\n        \n        is_ethical = self.ethical_framework.evaluate_action(action_name, parameters)\n        explanation = self.ethical_framework.get_ethical_explanation(f\"{action_name}: {parameters}\", is_task=False)\n        \n        return f\"Action: {action_name}\\nParameters: {parameters}\\nEthical: {'Yes' if is_ethical else 'No'}\\nExplanation: {explanation}\"\n\nif __name__ == \"__main__\":\n    # This is a mock implementation for testing purposes\n    from unittest.mock import MagicMock\n\n    class MockEthicalFramework:\n        def evaluate_action(self, action, params):\n            return action != \"delete_user_data\"\n        def get_ethical_explanation(self, description, is_task=True):\n            if \"delete_user_data\" in description:\n                return \"Deleting user data without explicit consent violates privacy principles.\"\n            return \"This action aligns with our ethical principles.\"\n\n    mock_memory = MagicMock()\n    mock_llm = MagicMock()\n\n    am = ActionManager(MockEthicalFramework(), mock_memory, mock_llm)\n\n    # Test executing an ethical action\n    result = am.execute_action(\"think\", {\"prompt\": \"What is the capital of France?\"})\n    print(result)\n\n    # Test executing an unethical action\n    result = am.execute_action(\"delete_user_data\", {\"user_id\": \"12345\"})\n    print(result)\n\n    # Get ethical summaries\n    print(am.get_action_ethical_summary(\"think\", {\"prompt\": \"What is the capital of France?\"}))\n    print(am.get_action_ethical_summary(\"delete_user_data\", {\"user_id\": \"12345\"}))\n\n    # List available actions\n    print(\"Available actions:\", am.get_available_actions())\n"}
{"type": "source_file", "path": "llm_providers/anthropic_provider.py", "content": "from typing import List, Dict, Any\nimport json\nimport aiohttp\nfrom anthropic import AsyncAnthropic\nfrom .base_provider import BaseLLMProvider\n\nclass AnthropicProvider(BaseLLMProvider):\n    def __init__(self, config: Dict[str, Any]):\n        self.anthropic_api_key = config.get('anthropic_api_key')\n        self.anthropic_model = config.get('anthropic_model', 'claude-3-opus-20240229')\n        self.anthropic_client = AsyncAnthropic(api_key=self.anthropic_api_key)\n\n        # Ollama configuration for embeddings\n        self.ollama_api_base_url = config.get('ollama_api_base_url', 'http://localhost:11434/api')\n        self.ollama_embedding_model = config.get('ollama_embedding_model', 'nomic-embed-text')\n        self.ollama_timeout = config.get('ollama_timeout', 30)\n\n    async def generate(self, prompt: str) -> str:\n        try:\n            message = await self.anthropic_client.messages.create(\n                model=self.anthropic_model,\n                max_tokens=1024,\n                messages=[\n                    {\n                        \"role\": \"user\",\n                        \"content\": prompt,\n                    }\n                ],\n            )\n            return message.content\n        except Exception as e:\n            raise Exception(f\"Error generating response from Anthropic: {str(e)}\")\n\n    async def get_embedding(self, text: str) -> List[float]:\n        url = f\"{self.ollama_api_base_url}/embeddings\"\n        payload = json.dumps({\n            \"model\": self.ollama_embedding_model,\n            \"prompt\": text\n        })\n        headers = {'Content-Type': 'application/json'}\n        \n        async with aiohttp.ClientSession() as session:\n            try:\n                async with session.post(url, headers=headers, json=payload, timeout=self.ollama_timeout) as response:\n                    response.raise_for_status()\n                    result = await response.json()\n                    return result['embedding']\n            except aiohttp.ClientError as e:\n                raise Exception(f\"Error getting embedding from Ollama: {str(e)}\")\n"}
