{"repo_info": {"repo_name": "LLMRec", "repo_owner": "HKUDS", "repo_url": "https://github.com/HKUDS/LLMRec"}}
{"type": "source_file", "path": "LATTICE/codes/utility/load_data.py", "content": "import numpy as np\nimport random as rd\nimport scipy.sparse as sp\nfrom time import time\nimport json\nfrom utility.parser import parse_args\nargs = parse_args()\n\nclass Data(object):\n    def __init__(self, path, batch_size):\n        # self.path = path + '/%d-core' % args.core\n        # self.batch_size = batch_size\n\n        # train_file = path + '/%d-core/train.json' % (args.core)\n        # val_file = path + '/%d-core/val.json' % (args.core)\n        # test_file = path + '/%d-core/test.json'  % (args.core)\n\n        self.path = path #+ '/%d-core' % args.core\n        self.batch_size = batch_size\n\n        train_file = path + '/train.json'#+ '/%d-core/train.json' % (args.core)\n        val_file = path + '/val.json' #+ '/%d-core/val.json' % (args.core)\n        test_file = path + '/test.json' #+ '/%d-core/test.json'  % (args.core)\n\n        #get number of users and items\n        self.n_users, self.n_items = 0, 0\n        self.n_train, self.n_test = 0, 0\n        self.neg_pools = {}\n\n        self.exist_users = []\n\n        train = json.load(open(train_file))\n        test = json.load(open(test_file))\n        val = json.load(open(val_file))\n        for uid, items in train.items():\n            if len(items) == 0:\n                continue\n            uid = int(uid)\n            self.exist_users.append(uid)\n            self.n_items = max(self.n_items, max(items))\n            self.n_users = max(self.n_users, uid)\n            self.n_train += len(items)\n\n        for uid, items in test.items():\n            uid = int(uid)\n            try:\n                self.n_items = max(self.n_items, max(items))\n                self.n_test += len(items)\n            except:\n                continue\n\n        for uid, items in val.items():\n            uid = int(uid)\n            try:\n                self.n_items = max(self.n_items, max(items))\n                self.n_val += len(items)\n            except:\n                continue\n\n        self.n_items += 1\n        self.n_users += 1\n\n        text_feats = np.load(args.data_path + '{}/text_feat.npy'.format(args.dataset))\n        self.n_items = text_feats.shape[0]\n\n        self.print_statistics()\n\n        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n        self.R_Item_Interacts = sp.dok_matrix((self.n_items, self.n_items), dtype=np.float32)\n\n        self.train_items, self.test_set, self.val_set = {}, {}, {}\n        for uid, train_items in train.items():\n            if len(train_items) == 0:\n                continue\n            uid = int(uid)\n            for idx, i in enumerate(train_items):\n                self.R[uid, i] = 1.\n\n            self.train_items[uid] = train_items\n\n        for uid, test_items in test.items():\n            uid = int(uid)\n            if len(test_items) == 0:\n                continue\n            try:\n                self.test_set[uid] = test_items\n            except:\n                continue\n\n        for uid, val_items in val.items():\n            uid = int(uid)\n            if len(val_items) == 0:\n                continue\n            try:\n                self.val_set[uid] = val_items\n            except:\n                continue            \n\n    def get_adj_mat(self):\n        try:\n            t1 = time()\n            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n            norm_adj_mat = sp.load_npz(self.path + '/s_norm_adj_mat.npz')\n            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')\n            print('already load adj matrix', adj_mat.shape, time() - t1)\n\n        except Exception:\n            adj_mat, norm_adj_mat, mean_adj_mat = self.create_adj_mat()\n            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n            sp.save_npz(self.path + '/s_norm_adj_mat.npz', norm_adj_mat)\n            sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)\n        return adj_mat, norm_adj_mat, mean_adj_mat\n\n    def create_adj_mat(self):\n        t1 = time()\n        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n        adj_mat = adj_mat.tolil()\n        R = self.R.tolil()\n\n        adj_mat[:self.n_users, self.n_users:] = R\n        adj_mat[self.n_users:, :self.n_users] = R.T\n        adj_mat = adj_mat.todok()\n        print('already create adjacency matrix', adj_mat.shape, time() - t1)\n\n        t2 = time()\n\n        def normalized_adj_single(adj):\n            rowsum = np.array(adj.sum(1))\n\n            d_inv = np.power(rowsum, -1).flatten()\n            d_inv[np.isinf(d_inv)] = 0.\n            d_mat_inv = sp.diags(d_inv)\n\n            norm_adj = d_mat_inv.dot(adj)\n            # norm_adj = adj.dot(d_mat_inv)\n            print('generate single-normalized adjacency matrix.')\n            return norm_adj.tocoo()\n\n        def get_D_inv(adj):\n            rowsum = np.array(adj.sum(1))\n\n            d_inv = np.power(rowsum, -1).flatten()\n            d_inv[np.isinf(d_inv)] = 0.\n            d_mat_inv = sp.diags(d_inv)\n            return d_mat_inv\n\n        def check_adj_if_equal(adj):\n            dense_A = np.array(adj.todense())\n            degree = np.sum(dense_A, axis=1, keepdims=False)\n\n            temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n            print('check normalized adjacency matrix whether equal to this laplacian matrix.')\n            return temp\n\n        norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n        mean_adj_mat = normalized_adj_single(adj_mat)\n\n        print('already normalize adjacency matrix', time() - t2)\n        return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n\n\n    def sample(self):\n        if self.batch_size <= self.n_users:\n            users = rd.sample(self.exist_users, self.batch_size)\n        else:\n            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n        # users = self.exist_users[:]\n\n        def sample_pos_items_for_u(u, num):\n            pos_items = self.train_items[u]\n            n_pos_items = len(pos_items)\n            pos_batch = []\n            while True:\n                if len(pos_batch) == num: break\n                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n                pos_i_id = pos_items[pos_id]\n\n                if pos_i_id not in pos_batch:\n                    pos_batch.append(pos_i_id)\n            return pos_batch\n\n        def sample_neg_items_for_u(u, num):\n            neg_items = []\n            while True:\n                if len(neg_items) == num: break\n                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n                    neg_items.append(neg_id)\n            return neg_items\n\n        def sample_neg_items_for_u_from_pools(u, num):\n            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n            return rd.sample(neg_items, num)\n\n        pos_items, neg_items = [], []\n        for u in users:\n            pos_items += sample_pos_items_for_u(u, 1)\n            neg_items += sample_neg_items_for_u(u, 1)\n            # neg_items += sample_neg_items_for_u(u, 3)\n        return users, pos_items, neg_items\n\n\n\n    def print_statistics(self):\n        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n        print('n_interactions=%d' % (self.n_train + self.n_test))\n        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))\n\n"}
{"type": "source_file", "path": "LATTICE/codes/main.py", "content": "import datetime\nimport math\nimport os\nimport random\nimport sys\nfrom time import time\nfrom tqdm import tqdm\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.sparse as sparse\n\nfrom utility.parser import parse_args\nfrom Models import LATTICE\nfrom utility.batch_test import *\n\nargs = parse_args()\n\n\nclass Trainer(object):\n    def __init__(self, data_config):\n        # argument settings\n        self.n_users = data_config['n_users']\n        self.n_items = data_config['n_items']\n\n        self.model_name = args.model_name\n        self.mess_dropout = eval(args.mess_dropout)\n        self.lr = args.lr\n        self.emb_dim = args.embed_size\n        self.batch_size = args.batch_size\n        self.weight_size = eval(args.weight_size)\n        self.n_layers = len(self.weight_size)\n        self.regs = eval(args.regs)\n        self.decay = self.regs[0]\n\n        self.norm_adj = data_config['norm_adj']\n        self.norm_adj = self.sparse_mx_to_torch_sparse_tensor(self.norm_adj).float().cuda()\n        \n        image_feats = np.load(args.data_path + '{}/image_feat.npy'.format(args.dataset))\n        text_feats = np.load(args.data_path + '{}/text_feat.npy'.format(args.dataset))\n\n        self.model = LATTICE(self.n_users, self.n_items, self.emb_dim, self.weight_size, self.mess_dropout, image_feats, text_feats)\n        self.model = self.model.cuda()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n        self.lr_scheduler = self.set_lr_scheduler()\n\n    def set_lr_scheduler(self):\n        fac = lambda epoch: 0.96 ** (epoch / 50)\n        scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=fac)\n        return scheduler\n\n    def test(self, users_to_test, is_val):\n        self.model.eval()\n        with torch.no_grad():\n            ua_embeddings, ia_embeddings = self.model(self.norm_adj, build_item_graph=True)\n        result = test_torch(ua_embeddings, ia_embeddings, users_to_test, is_val)\n        return result\n\n    def train(self):\n        training_time_list = []\n        loss_loger, pre_loger, rec_loger, ndcg_loger, hit_loger = [], [], [], [], []\n        stopping_step = 0\n        should_stop = False\n        cur_best_pre_0 = 0.\n\n        n_batch = data_generator.n_train // args.batch_size + 1\n        best_recall = 0\n        for epoch in (range(args.epoch)):\n            t1 = time()\n            loss, mf_loss, emb_loss, reg_loss = 0., 0., 0., 0.\n            n_batch = data_generator.n_train // args.batch_size + 1\n            f_time, b_time, loss_time, opt_time, clip_time, emb_time = 0., 0., 0., 0., 0., 0.\n            sample_time = 0.\n            build_item_graph = True\n            for idx in (range(n_batch)):\n                self.model.train()\n                self.optimizer.zero_grad()\n                sample_t1 = time()\n                users, pos_items, neg_items = data_generator.sample()\n                sample_time += time() - sample_t1                                                 \n                ua_embeddings, ia_embeddings = self.model(self.norm_adj, build_item_graph=build_item_graph)\n                build_item_graph = False\n                u_g_embeddings = ua_embeddings[users]\n                pos_i_g_embeddings = ia_embeddings[pos_items]\n                neg_i_g_embeddings = ia_embeddings[neg_items]\n\n\n                batch_mf_loss, batch_emb_loss, batch_reg_loss = self.bpr_loss(u_g_embeddings, pos_i_g_embeddings,\n                                                                              neg_i_g_embeddings)\n\n                batch_loss = batch_mf_loss + batch_emb_loss + batch_reg_loss\n\n                batch_loss.backward(retain_graph=True)\n                self.optimizer.step()\n\n                loss += float(batch_loss)\n                mf_loss += float(batch_mf_loss)\n                emb_loss += float(batch_emb_loss)\n                reg_loss += float(batch_reg_loss)\n\n\n            self.lr_scheduler.step()\n\n            del ua_embeddings, ia_embeddings, u_g_embeddings, neg_i_g_embeddings, pos_i_g_embeddings\n\n            if math.isnan(loss) == True:\n                print('ERROR: loss is nan.')\n                sys.exit()\n\n            perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f]' % (\n                epoch, time() - t1, loss, mf_loss, emb_loss)\n            training_time_list.append(time() - t1)\n            print(perf_str)\n\n            if epoch % args.verbose != 0:\n                continue\n\n\n            t2 = time()\n            users_to_test = list(data_generator.test_set.keys())\n            users_to_val = list(data_generator.val_set.keys())\n            ret = self.test(users_to_val, is_val=True)\n            training_time_list.append(t2 - t1)\n\n            t3 = time()\n\n            loss_loger.append(loss)\n            rec_loger.append(ret['recall'])\n            pre_loger.append(ret['precision'])\n            ndcg_loger.append(ret['ndcg'])\n            hit_loger.append(ret['hit_ratio'])\n            if args.verbose > 0:\n                perf_str = 'Epoch %d [%.1fs + %.1fs]:  val==[%.5f=%.5f + %.5f + %.5f], recall=[%.5f, %.5f], ' \\\n                           'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f]' % \\\n                           (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, reg_loss, ret['recall'][0],\n                            ret['recall'][-1],\n                            ret['precision'][0], ret['precision'][-1], ret['hit_ratio'][0], ret['hit_ratio'][-1],\n                            ret['ndcg'][0], ret['ndcg'][-1])\n                print(perf_str)\n\n            if ret['recall'][1] > best_recall:\n                best_recall = ret['recall'][1]\n                test_ret = self.test(users_to_test, is_val=False)\n                perf_str = 'Epoch %d [%.1fs + %.1fs]: test==[%.5f=%.5f + %.5f + %.5f], recall=[%.5f, %.5f], ' \\\n                           'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f]' % \\\n                           (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, reg_loss, test_ret['recall'][0],\n                            test_ret['recall'][-1],\n                            test_ret['precision'][0], test_ret['precision'][-1], test_ret['hit_ratio'][0], test_ret['hit_ratio'][-1],\n                            test_ret['ndcg'][0], test_ret['ndcg'][-1])\n                print(perf_str)                \n                stopping_step = 0\n            elif stopping_step < args.early_stopping_patience:\n                stopping_step += 1\n                print('#####Early stopping steps: %d #####' % stopping_step)\n            else:\n                print('#####Early stop! #####')\n                break\n\n        print(test_ret)\n\n    def bpr_loss(self, users, pos_items, neg_items):\n        pos_scores = torch.sum(torch.mul(users, pos_items), dim=1)\n        neg_scores = torch.sum(torch.mul(users, neg_items), dim=1)\n\n        regularizer = 1./2*(users**2).sum() + 1./2*(pos_items**2).sum() + 1./2*(neg_items**2).sum()\n        regularizer = regularizer / self.batch_size\n\n        maxi = F.logsigmoid(pos_scores - neg_scores)\n        mf_loss = -torch.mean(maxi)\n\n        emb_loss = self.decay * regularizer\n        reg_loss = 0.0\n        return mf_loss, emb_loss, reg_loss\n\n    def sparse_mx_to_torch_sparse_tensor(self, sparse_mx):\n        \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n        sparse_mx = sparse_mx.tocoo().astype(np.float32)\n        indices = torch.from_numpy(\n            np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n        values = torch.from_numpy(sparse_mx.data)\n        shape = torch.Size(sparse_mx.shape)\n        return torch.sparse.FloatTensor(indices, values, shape)\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed) # cpu\n    torch.cuda.manual_seed_all(seed)  # gpu\n\nif __name__ == '__main__':\n    set_seed(args.seed)\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n\n    config = dict()\n    config['n_users'] = data_generator.n_users\n    config['n_items'] = data_generator.n_items\n\n    plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()\n    config['norm_adj'] = norm_adj\n\n    trainer = Trainer(data_config=config)\n    trainer.train()\n\n"}
{"type": "source_file", "path": "LATTICE/codes/Models.py", "content": "import os\nimport numpy as np\nfrom time import time\n\nimport torch\nimport torch.nn as nn\nimport torch.sparse as sparse\nimport torch.nn.functional as F\n\nfrom utility.parser import parse_args\nargs = parse_args()\n\ndef build_knn_neighbourhood(adj, topk):\n    knn_val, knn_ind = torch.topk(adj, topk, dim=-1)\n    weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n    return weighted_adjacency_matrix\ndef compute_normalized_laplacian(adj):\n    rowsum = torch.sum(adj, -1)\n    d_inv_sqrt = torch.pow(rowsum, -0.5)\n    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n    L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n    return L_norm\ndef build_sim(context):\n    context_norm = context.div(torch.norm(context, p=2, dim=-1, keepdim=True))\n    sim = torch.mm(context_norm, context_norm.transpose(1, 0))\n    return sim\n\nclass LATTICE(nn.Module):\n    def __init__(self, n_users, n_items, embedding_dim, weight_size, dropout_list, image_feats, text_feats):\n        super().__init__()\n        self.n_users = n_users\n        self.n_items = n_items\n        self.embedding_dim = embedding_dim\n        self.weight_size = weight_size\n        self.n_ui_layers = len(self.weight_size)\n        self.weight_size = [self.embedding_dim] + self.weight_size\n        self.user_embedding = nn.Embedding(n_users, self.embedding_dim)\n        self.item_id_embedding = nn.Embedding(n_items, self.embedding_dim)\n        nn.init.xavier_uniform_(self.user_embedding.weight)\n        nn.init.xavier_uniform_(self.item_id_embedding.weight)\n\n        if args.cf_model == 'ngcf':\n            self.GC_Linear_list = nn.ModuleList()\n            self.Bi_Linear_list = nn.ModuleList()\n            self.dropout_list = nn.ModuleList()\n            for i in range(self.n_ui_layers):\n                self.GC_Linear_list.append(nn.Linear(self.weight_size[i], self.weight_size[i+1]))\n                self.Bi_Linear_list.append(nn.Linear(self.weight_size[i], self.weight_size[i+1]))\n                self.dropout_list.append(nn.Dropout(dropout_list[i]))\n\n\n        self.image_embedding = nn.Embedding.from_pretrained(torch.Tensor(image_feats), freeze=False)\n        self.text_embedding = nn.Embedding.from_pretrained(torch.Tensor(text_feats), freeze=False)\n            \n    \n        if os.path.exists(args.data_path + 'image_adj_%d.pt'%(args.topk)):\n            image_adj = torch.load(args.data_path + 'image_adj_%d.pt'%(args.topk))\n        else:\n            image_adj = build_sim(self.image_embedding.weight.detach())\n            image_adj = build_knn_neighbourhood(image_adj, topk=args.topk)\n            image_adj = compute_normalized_laplacian(image_adj)\n            torch.save(image_adj, args.data_path + 'image_adj_%d.pt'%(args.topk))\n\n        if os.path.exists(args.data_path + 'text_adj_%d.pt'%(args.topk)):\n            text_adj = torch.load(args.data_path + 'text_adj_%d.pt'%(args.topk))        \n        else:\n            text_adj = build_sim(self.text_embedding.weight.detach())\n            text_adj = build_knn_neighbourhood(text_adj, topk=args.topk)\n            text_adj = compute_normalized_laplacian(text_adj)\n            torch.save(text_adj, args.data_path + 'text_adj_%d.pt'%(args.topk))\n\n        self.text_original_adj = text_adj.cuda()\n        self.image_original_adj = image_adj.cuda()\n        \n        self.image_trs = nn.Linear(image_feats.shape[1], args.feat_embed_dim)\n        self.text_trs = nn.Linear(text_feats.shape[1], args.feat_embed_dim)\n\n\n        self.modal_weight = nn.Parameter(torch.Tensor([0.5, 0.5]))\n        self.softmax = nn.Softmax(dim=0)\n\n    def forward(self, adj, build_item_graph=False):\n        image_feats = self.image_trs(self.image_embedding.weight)\n        text_feats = self.text_trs(self.text_embedding.weight)\n        if build_item_graph:\n            weight = self.softmax(self.modal_weight)\n            self.image_adj = build_sim(image_feats)\n            self.image_adj = build_knn_neighbourhood(self.image_adj, topk=args.topk)\n\n            self.text_adj = build_sim(text_feats)\n            self.text_adj = build_knn_neighbourhood(self.text_adj, topk=args.topk)   \n\n            \n            learned_adj = weight[0] * self.image_adj + weight[1] * self.text_adj\n            learned_adj = compute_normalized_laplacian(learned_adj)\n            original_adj = weight[0] * self.image_original_adj + weight[1] * self.text_original_adj\n            self.item_adj = (1 - args.lambda_coeff) * learned_adj + args.lambda_coeff * original_adj\n        else:\n            self.item_adj = self.item_adj.detach()\n\n        h = self.item_id_embedding.weight\n        for i in range(args.n_layers):\n            h = torch.mm(self.item_adj, h)\n\n        if args.cf_model == 'ngcf':\n            ego_embeddings = torch.cat((self.user_embedding.weight, self.item_id_embedding.weight), dim=0)\n            all_embeddings = [ego_embeddings]\n            for i in range(self.n_ui_layers):\n                side_embeddings = torch.sparse.mm(adj, ego_embeddings)\n                sum_embeddings = F.leaky_relu(self.GC_Linear_list[i](side_embeddings))\n                bi_embeddings = torch.mul(ego_embeddings, side_embeddings)\n                bi_embeddings = F.leaky_relu(self.Bi_Linear_list[i](bi_embeddings))\n                ego_embeddings = sum_embeddings + bi_embeddings\n                ego_embeddings = self.dropout_list[i](ego_embeddings)\n\n                norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n                all_embeddings += [norm_embeddings]\n\n            all_embeddings = torch.stack(all_embeddings, dim=1)\n            all_embeddings = all_embeddings.mean(dim=1, keepdim=False)            \n            u_g_embeddings, i_g_embeddings = torch.split(all_embeddings, [self.n_users, self.n_items], dim=0)\n            i_g_embeddings = i_g_embeddings + F.normalize(h, p=2, dim=1)\n            return u_g_embeddings, i_g_embeddings\n        elif args.cf_model == 'lightgcn':\n            ego_embeddings = torch.cat((self.user_embedding.weight, self.item_id_embedding.weight), dim=0)\n            all_embeddings = [ego_embeddings]\n            for i in range(self.n_ui_layers):\n                side_embeddings = torch.sparse.mm(adj, ego_embeddings)\n                ego_embeddings = side_embeddings\n                all_embeddings += [ego_embeddings]\n            all_embeddings = torch.stack(all_embeddings, dim=1)\n            all_embeddings = all_embeddings.mean(dim=1, keepdim=False)\n            u_g_embeddings, i_g_embeddings = torch.split(all_embeddings, [self.n_users, self.n_items], dim=0)\n            i_g_embeddings = i_g_embeddings + F.normalize(h, p=2, dim=1)\n            return u_g_embeddings, i_g_embeddings\n        elif args.cf_model == 'mf':\n                return self.user_embedding.weight, self.item_id_embedding.weight + F.normalize(h, p=2, dim=1)\n\n"}
{"type": "source_file", "path": "MMSSL/utility/parser.py", "content": "import argparse\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"\")\n\n    #use less\n    parser.add_argument('--verbose', type=int, default=5, help='Interval of evaluation.')    \n    parser.add_argument('--core', type=int, default=5, help='5-core for warm-start; 0-core for cold start')\n    parser.add_argument('--lambda_coeff', type=float, default=0.9, help='Lambda value of skip connection')\n\n    parser.add_argument('--early_stopping_patience', type=int, default=7, help='') \n    parser.add_argument('--layers', type=int, default=1, help='Number of feature graph conv layers')  \n    parser.add_argument('--mess_dropout', nargs='?', default='[0.1, 0.1]', help='Keep probability w.r.t. message dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n    parser.add_argument('--sparse', type=int, default=1, help='Sparse or dense adjacency matrix')   \n\n    parser.add_argument('--test_flag', nargs='?', default='part', help='Specify the test type from {part, full}, indicating whether the reference is done in mini-batch')\n    parser.add_argument('--metapath_threshold', default=2, type=int, help='metapath_threshold') \n    parser.add_argument('--sc', type=float, default=1.0, help='GCN self connection')\n    parser.add_argument('--ssl_c_rate', type=float, default=1.3, help='ssl_c_rate')\n    parser.add_argument('--ssl_s_rate', type=float, default=0.8, help='ssl_s_rate')\n    parser.add_argument('--g_rate', type=float, default=0.000029, help='ssl_s_rate')\n    parser.add_argument('--sample_num', default=1, type=int, help='sample_num') \n    parser.add_argument('--sample_num_neg', default=1, type=int, help='sample_num') \n    parser.add_argument('--sample_num_ii', default=8, type=int, help='sample_num') \n    parser.add_argument('--sample_num_co', default=2, type=int, help='sample_num') \n    parser.add_argument('--mask_rate', default=0.75, type=float, help='sample_num') \n    parser.add_argument('--gss_rate', default=0.85, type=float, help='gene_self_subgraph_rate') \n    parser.add_argument('--anchor_rate', default=0.75, type=float, help='anchor_rate') \n    parser.add_argument('--feat_reg_decay', default=1e-5, type=float, help='feat_reg_decay') \n    parser.add_argument('--ad1_rate', default=0.2, type=float, help='ad1_rate') \n    parser.add_argument('--ad2_rate', default=0.2, type=float, help='ad1_rate')     \n    parser.add_argument('--ad_sampNum', type=int, default=1, help='ad topk')  \n    parser.add_argument('--ad_topk_multi_num', type=int, default=100, help='ad topk')  \n    parser.add_argument('--fake_gene_rate', default=0.0001, type=float, help='fake_gene_rate') \n    parser.add_argument('--ID_layers', type=int, default=1, help='Number of item graph conv layers')  \n    parser.add_argument('--reward_rate', default=1, type=float, help='fake_gene_rate') \n    parser.add_argument('--G_embed_size', type=int, default=64, help='Embedding size.')   \n    parser.add_argument('--model_num', default=2, type=float, help='fake_gene_rate') \n    parser.add_argument('--negrate', default=0.01, type=float, help='item_neg_sample_rate')\n    parser.add_argument('--cis', default=25, type=int, help='') \n    parser.add_argument('--confidence', default=0.5, type=float, help='') \n    parser.add_argument('--ii_it', default=15, type=int, help='') \n    parser.add_argument('--isload', default=False , type=bool, help='whether load model')  #\n    parser.add_argument('--isJustTest', default=False , type=bool, help='whether load model')\n    parser.add_argument('--loadModelPath', default='/home/ww/Code/work3/BSTRec/Model/retailrocket/for_meta_hidden_dim_dim__8_retailrocket_2021_07_10__18_35_32_lr_0.0003_reg_0.01_batch_size_1024_gnn_layer_[16,16,16].pth', type=str, help='loadModelPath')\n    parser.add_argument('--title', default=\"try_to_draw_line\", type=str, help='')  #\n\n\n    #train\n    parser.add_argument('--data_path', nargs='?', default='', help='Input data path.')\n    parser.add_argument('--seed', type=int, default=2022, help='Random seed')\n    parser.add_argument('--dataset', nargs='?', default='baby', help='Choose a dataset from {sports, baby, clothing, tiktok, allrecipes}')\n    parser.add_argument('--epoch', type=int, default=1000, help='Number of epoch.')  #default: 1000\n    parser.add_argument('--batch_size', type=int, default=1024, help='Batch size.')\n    parser.add_argument('--embed_size', type=int, default=64,help='Embedding size.')                     \n    parser.add_argument('--D_lr', type=float, default=3e-4, help='Learning rate.')\n    parser.add_argument('--topk', type=int, default=10, help='K value of k-NN sparsification')  \n    parser.add_argument('--cf_model', nargs='?', default='slmrec', help='Downstream Collaborative Filtering model {mf, ngcf, lightgcn, vbpr, hafr, clcrec, slmrec}')   \n    parser.add_argument('--debug', action='store_true')  \n    parser.add_argument('--cl_rate', type=float, default=0.03, help='Control the effect of the contrastive auxiliary task')        \n    parser.add_argument('--norm_type', nargs='?', default='sym', help='Adjacency matrix normalization operation') \n    parser.add_argument('--gpu_id', type=int, default=0, help='GPU id')\n    parser.add_argument('--Ks', nargs='?', default='[10, 20, 50]', help='K value of ndcg/recall @ k')\n    parser.add_argument('--regs', nargs='?', default='[1e-5,1e-5,1e-2]', help='for emb_loss.')  #default: '[1e-5,1e-5,1e-2]'\n    parser.add_argument('--lr', type=float, default=0.00055, help='Learning rate.')\n    parser.add_argument('--emm', default=1e-3, type=float, help='for feature embedding bpr')  #\n    parser.add_argument('--L2_alpha', default=1e-3, type=float, help='')  #\n    parser.add_argument('--weight_decay', default=1e-4, type=float, help='for opt_D')  #\n\n\n    #GNN\n    parser.add_argument('--drop_rate', type=float, default=0.2, help='dropout rate')\n    parser.add_argument('--model_cat_rate', type=float, default=0.55, help='model_cat_rate')\n    parser.add_argument('--gnn_cat_rate', type=float, default=0.55, help='gnn_cat_rate')\n    parser.add_argument('--id_cat_rate', type=float, default=0.36, help='before GNNs')\n    parser.add_argument('--id_cat_rate1', type=float, default=0.36, help='id_cat_rate')\n    parser.add_argument('--head_num', default=4, type=int, help='head_num_of_multihead_attention. For multi-model relation.')  #\n    parser.add_argument('--dgl_nei_num', default=8, type=int, help='dgl_nei_num')  #\n\n\n    #GAN\n    parser.add_argument('--weight_size', nargs='?', default='[64, 64]', help='Output sizes of every layer')  #default: '[64, 64]'\n    parser.add_argument('--G_rate', default=0.0001, type=float, help='for D model1')  #\n    parser.add_argument('--G_drop1', default=0.31, type=float, help='for D model2')  #\n    parser.add_argument('--G_drop2', default=0.5, type=float, help='')  #\n    parser.add_argument('--gp_rate', default=1, type=float, help='gradient penal')  #\n\n    parser.add_argument('--real_data_tau', default=0.005, type=float, help='for real_data soft')  #\n    parser.add_argument('--ui_pre_scale', default=100, type=int, help='ui_pre_scale')  \n\n\n    #cl\n    parser.add_argument('--T', default=1, type=int, help='it for ui update')  \n    parser.add_argument('--tau', default=0.5, type=float, help='')  #\n    parser.add_argument('--geneGraph_rate', default=0.1, type=float, help='')  #\n    parser.add_argument('--geneGraph_rate_pos', default=2, type=float, help='')  #\n    parser.add_argument('--geneGraph_rate_neg', default=-1, type=float, help='')  #     \n    parser.add_argument('--m_topk_rate', default=0.0001, type=float, help='for reconstruct')  \n    parser.add_argument('--log_log_scale', default=0.00001, type=int, help='log_log_scale')  \n    parser.add_argument('--point', default='', type=str, help='point')  \n\n    return parser.parse_args()\n\n\n"}
{"type": "source_file", "path": "MMSSL/utility/logging.py", "content": "import os\nfrom datetime import datetime\n\nclass Logger():\n    def __init__(self, filename, is_debug, path='/home/ww/Code/work5/MICRO2Ours/logs/'):\n        self.filename = filename\n        self.path = path\n        self.log_ = not is_debug\n    def logging(self, s):\n        s = str(s)\n        print(datetime.now().strftime('%Y-%m-%d %H:%M: '), s)\n        if self.log_:\n            with open(os.path.join(os.path.join(self.path, self.filename)), 'a+') as f_log:\n                f_log.write(str(datetime.now().strftime('%Y-%m-%d %H:%M:  ')) + s + '\\n')\n"}
{"type": "source_file", "path": "utility/parser.py", "content": "import argparse\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"\")\n\n    parser.add_argument('--data_path', nargs='?', default='./data/', help='Input data path')  \n    parser.add_argument('--seed', type=int, default=2022, help='Random seed')\n    parser.add_argument('--dataset', nargs='?', default='netflix', help='Choose a dataset from {movieLens, netflix}')\n    parser.add_argument('--verbose', type=int, default=5, help='Interval of evaluation.')\n    parser.add_argument('--epoch', type=int, default=1000, help='Number of epoch.')  \n    parser.add_argument('--regs', nargs='?', default='[1e-5,1e-5,1e-2]', help='Regularizations.')  \n    parser.add_argument('--embed_size', type=int, default=64, help='Embedding size.')                     \n    parser.add_argument('--weight_size', nargs='?', default='[64, 64]', help='Output sizes of every layer')\n    parser.add_argument('--early_stopping_patience', type=int, default=7, help='Early Stop Patience') \n    parser.add_argument('--mess_dropout', nargs='?', default='[0.1, 0.1]',help='Keep probability w.r.t. message dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n    parser.add_argument('--sparse', type=int, default=1, help='Sparse or dense adjacency matrix')   \n    parser.add_argument('--debug', action='store_true')  \n    parser.add_argument('--norm_type', nargs='?', default='sym', help='Adjacency matrix normalization operation') \n    parser.add_argument('--gpu_id', type=int, default=0, help='GPU ID')  \n    parser.add_argument('--Ks', nargs='?', default='[10, 20, 50]', help='K value of ndcg/recall @ k')\n    parser.add_argument('--test_flag', nargs='?', default='part', help='Specify the test type from {part, full}, indicating whether the reference is done in mini-batch')\n    parser.add_argument('--sc', type=float, default=1.0, help='GCN self connection')\n    parser.add_argument('--feat_reg_decay', default=1e-5, type=float, help='Feature Reg Decay') \n    parser.add_argument('--title', default=\"try_to_draw_line\", type=str, help='')  \n    parser.add_argument('--cf_model', nargs='?', default='lightgcn', help='Downstream Collaborative Filtering model {mf, ngcf, lightgcn, mmgcn, vbpr, hafr, bm3}')   \n    parser.add_argument('--point', default=\"\", type=str, help='')  #\n\n    # train\n    parser.add_argument('--batch_size', type=int, default=1024, help='Batch size.')\n    parser.add_argument('--lr', type=float, default=0.0001, help='Learning rate.')  \n    parser.add_argument('--de_lr', type=float, default=0.0002, help='Decoder learning rate.')  \n    parser.add_argument('--weight_decay', default=1e-4, type=float, help='Weight_decay')  #\n\n    # model\n    parser.add_argument('--layers', type=int, default=1, help='Number of graph conv layers')  \n    parser.add_argument('--drop_rate', type=float, default=0.0, help='Dropout rate')\n    parser.add_argument('--mask_rate', type=float, default=0.0, help='Mask rate')   \n    parser.add_argument('--mask', type=bool, default=False, help='If mask')   \n    parser.add_argument('--user_cat_rate', type=float, default=2.8, help='User cat rate')\n    parser.add_argument('--item_cat_rate', type=float, default=0.005, help='Item cat rate')\n    parser.add_argument('--model_cat_rate', type=float, default=0.02, help='Model cat rate')\n    parser.add_argument('--de_drop1', default=0.31, type=float, help='for D model2')  #\n    parser.add_argument('--de_drop2', default=0.5, type=float, help='')  #\n\n    # loss\n    parser.add_argument('--aug_mf_rate', type=float, default=0.012, help='Augmentation mf rate')      # \n    parser.add_argument('--prune_loss_drop_rate', type=float, default=0.71, help='Prune loss drop rate')    \n    parser.add_argument('--mm_mf_rate', type=float, default=0.0001, help='MM mf rate')    \n    parser.add_argument('--feat_loss_type', default=\"sce\", type=str, help='Feature loss type')  #\n    parser.add_argument('--att_re_rate', type=float, default=0.00000, help='Attribute restoration rate')      # \n    parser.add_argument(\"--alpha_l\", type=float, default=2, help=\"`pow`inddex for `sce` loss\")\n    parser.add_argument('--aug_sample_rate', type=float, default=0.1, help='Augmentation sample rate')\n    parser.add_argument('--mf_emb_rate', type=float, default=0.0, help='MF embedding rate')\n\n    return parser.parse_args()\n"}
{"type": "source_file", "path": "MMSSL/MMD.py", "content": "import torch\n\n\n#guassian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None)\n#mmd_rbf(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n\n\ndef guassian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n    '''\n    将源域数据和目标域数据转化为核矩阵，即上文中的K\n    Params: \n\t    source: 源域数据（n * len(x))\n\t    target: 目标域数据（m * len(y))\n\t    kernel_mul: \n\t    kernel_num: 取不同高斯核的数量\n\t    fix_sigma: 不同高斯核的sigma值\n\tReturn:\n\t\tsum(kernel_val): 多个核矩阵之和\n    '''\n    n_samples = int(source.size()[0])+int(target.size()[0])# 求矩阵的行数，一般source和target的尺度是一样的，这样便于计算\n    total = torch.cat([source, target], dim=0)#将source,target按列方向合并\n    #将total复制（n+m）份\n    total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n    #将total的每一行都复制成（n+m）行，即每个数据都扩展成（n+m）份\n    total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n    #求任意两个数据之间的和，得到的矩阵中坐标（i,j）代表total中第i行数据和第j行数据之间的l2 distance(i==j时为0）\n\n    L2_distance = ((total0-total1)**2).sum(2) \n    #调整高斯核函数的sigma值\n    if fix_sigma:\n        bandwidth = fix_sigma\n    else:\n        bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n    #以fix_sigma为中值，以kernel_mul为倍数取kernel_num个bandwidth值（比如fix_sigma为1时，得到[0.25,0.5,1,2,4]\n    bandwidth /= kernel_mul ** (kernel_num // 2)\n    bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n    #高斯核函数的数学表达式\n    kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]\n    #得到最终的核矩阵\n    return sum(kernel_val)#/len(kernel_val)\n\ndef mmd_rbf(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n    '''\n    计算源域数据和目标域数据的MMD距离\n    Params: \n\t    source: 源域数据（n * len(x))\n\t    target: 目标域数据（m * len(y))\n\t    kernel_mul: \n\t    kernel_num: 取不同高斯核的数量\n\t    fix_sigma: 不同高斯核的sigma值\n\tReturn:\n\t\tloss: MMD loss\n    '''\n    batch_size = int(source.size()[0])#一般默认为源域和目标域的batchsize相同\n    kernels = guassian_kernel(source, target,\n        kernel_mul=kernel_mul, kernel_num=kernel_num, fix_sigma=fix_sigma)\n    #根据式（3）将核矩阵分成4部分\n    XX = kernels[:batch_size, :batch_size]\n    YY = kernels[batch_size:, batch_size:]\n    XY = kernels[:batch_size, batch_size:]\n    YX = kernels[batch_size:, :batch_size]\n    loss = torch.mean(XX + YY - XY -YX)\n    return loss#因为一般都是n==m，所以L矩阵一般不加入计算\n"}
{"type": "source_file", "path": "MMSSL/utility/norm.py", "content": "import torch\nimport numpy as np\nfrom scipy.sparse import csr_matrix \n\ndef build_sim(context):\n    context_norm = context.div(torch.norm(context, p=2, dim=-1, keepdim=True))\n    sim = torch.sparse.mm(context_norm, context_norm.transpose(1, 0))\n    # a, b = context_norm.shape\n    # b, c = context_norm.transpose(1, 0).shape\n    # ab = context_norm.unsqueeze(-1)  #.repeat(1,1,c)\n    # bc = context_norm.transpose(1, 0).unsqueeze(0)  #.repeat(a, 1,1)\n    # sim = torch.mul(ab, bc).sum(dim=1, keepdim=False)\n\n    return sim\n\n# def build_knn_normalized_graph(adj, topk, is_sparse, norm_type):\n#     device = adj.device\n#     knn_val, knn_ind = torch.topk(adj, topk, dim=-1)\n#     if is_sparse:\n#         tuple_list = [[row, int(col)] for row in range(len(knn_ind)) for col in knn_ind[row]]\n#         row = [i[0] for i in tuple_list]\n#         col = [i[1] for i in tuple_list]\n#         i = torch.LongTensor([row, col]).to(device)\n#         v = knn_val.flatten()\n#         edge_index, edge_weight = get_sparse_laplacian(i, v, normalization=norm_type, num_nodes=adj.shape[0])\n#         return torch.sparse_coo_tensor(edge_index, edge_weight, adj.shape)\n#     else:\n#         weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n#         return get_dense_laplacian(weighted_adjacency_matrix, normalization=norm_type)\n\ndef build_knn_normalized_graph(adj, topk, is_sparse, norm_type):\n    device = adj.device\n    knn_val, knn_ind = torch.topk(adj, topk, dim=-1)  #[7050, 10] [7050, 10]\n    n_item = knn_val.shape[0]\n    n_data = knn_val.shape[0]*knn_val.shape[1]\n    data = np.ones(n_data)\n    if is_sparse:\n        tuple_list = [[row, int(col)] for row in range(len(knn_ind)) for col in knn_ind[row]]  #[70500]\n        # data = np.array(knn_val.flatten().cpu())  #args.topk_rate*\n        row = [i[0] for i in tuple_list]  #[70500]\n        col = [i[1] for i in tuple_list]  #[70500]\n        # #-----------------------------------------------------------------------------------------------------\n        # i = torch.LongTensor([row, col]).to(device)\n        # v = knn_val.flatten()\n        # edge_index, edge_weight = get_sparse_laplacian(i, v, normalization=norm_type, num_nodes=adj.shape[0])\n        # #-----------------------------------------------------------------------------------------------------\n        ii_graph = csr_matrix((data, (row, col)) ,shape=(n_item, n_item))\n        # return torch.sparse_coo_tensor(edge_index, edge_weight, adj.shape)\n        return ii_graph\n    else:\n        weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n        return get_dense_laplacian(weighted_adjacency_matrix, normalization=norm_type)\n\n\ndef get_sparse_laplacian(edge_index, edge_weight, num_nodes, normalization='none'):  #[2, 70500], [70500]\n    from torch_scatter import scatter_add\n    row, col = edge_index[0], edge_index[1]  #[70500] [70500]\n    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)  #[7050]\n\n    if normalization == 'sym':\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    elif normalization == 'rw':\n        deg_inv = 1.0 / deg\n        deg_inv.masked_fill_(deg_inv == float('inf'), 0)\n        edge_weight = deg_inv[row] * edge_weight\n    return edge_index, edge_weight\n\n\ndef get_dense_laplacian(adj, normalization='none'):\n    if normalization == 'sym':\n        rowsum = torch.sum(adj, -1)\n        d_inv_sqrt = torch.pow(rowsum, -0.5)\n        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n        d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n        L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n    elif normalization == 'rw':\n        rowsum = torch.sum(adj, -1)\n        d_inv = torch.pow(rowsum, -1)\n        d_inv[torch.isinf(d_inv)] = 0.\n        d_mat_inv = torch.diagflat(d_inv)\n        L_norm = torch.mm(d_mat_inv, adj)\n    elif normalization == 'none':\n        L_norm = adj\n    return L_norm\n"}
{"type": "source_file", "path": "LATTICE/codes/utility/parser.py", "content": "import argparse\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"\")\n\n    parser.add_argument('--data_path', nargs='?', default='',\n                        help='Input data path.')\n    parser.add_argument('--seed', type=int, default=123,\n                        help='Random seed')\n    parser.add_argument('--dataset', nargs='?', default='cloth',\n                        help='Choose a dataset from {grocery, cloth, sport, netflix, sports, baby, clothing}')\n    parser.add_argument('--verbose', type=int, default=5,\n                        help='Interval of evaluation.')\n    parser.add_argument('--epoch', type=int, default=200,\n                        help='Number of epoch.')\n    parser.add_argument('--batch_size', type=int, default=1024,\n                        help='Batch size.')\n    parser.add_argument('--regs', nargs='?', default='[1e-5,1e-5,1e-2]',\n                        help='Regularizations.')\n    parser.add_argument('--lr', type=float, default=0.0005,\n                        help='Learning rate.')\n    parser.add_argument('--model_name', nargs='?', default='lattice',\n                        help='Specify the model name.')\n\n    parser.add_argument('--embed_size', type=int, default=64,\n                        help='Embedding size.')\n    parser.add_argument('--feat_embed_dim', type=int, default=64,\n                        help='')                        \n    parser.add_argument('--weight_size', nargs='?', default='[64,64]',\n                        help='Output sizes of every layer')\n    parser.add_argument('--core', type=int, default=5,\n                        help='5-core for warm-start; 0-core for cold start')\n    parser.add_argument('--topk', type=int, default=10,\n                        help='K value of k-NN sparsification')  \n    parser.add_argument('--lambda_coeff', type=float, default=0.9,\n                        help='Lambda value of skip connection')\n    parser.add_argument('--cf_model', nargs='?', default='lightgcn',\n                        help='Downstream Collaborative Filtering model {mf, ngcf, lightgcn}')   \n    parser.add_argument('--n_layers', type=int, default=1,\n                        help='Number of item graph conv layers')  \n    parser.add_argument('--mess_dropout', nargs='?', default='[0.1, 0.1]',\n                        help='Keep probability w.r.t. message dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n\n    parser.add_argument('--early_stopping_patience', type=int, default=10,\n                        help='') \n    parser.add_argument('--gpu_id', type=int, default=0,\n                        help='GPU id')\n    parser.add_argument('--Ks', nargs='?', default='[10, 20, 50]',\n                        help='K value of ndcg/recall @ k')\n    parser.add_argument('--test_flag', nargs='?', default='part',\n                        help='Specify the test type from {part, full}, indicating whether the reference is done in mini-batch')\n\n\n    return parser.parse_args()\n"}
{"type": "source_file", "path": "MMSSL/utility/metrics.py", "content": "import numpy as np\nfrom sklearn.metrics import roc_auc_score\n\ndef recall(rank, ground_truth, N):\n    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n\n\ndef precision_at_k(r, k):\n    \"\"\"Score is precision @ k\n    Relevance is binary (nonzero is relevant).\n    Returns:\n        Precision @ k\n    Raises:\n        ValueError: len(r) must be >= k\n    \"\"\"\n    assert k >= 1\n    r = np.asarray(r)[:k]\n    return np.mean(r)\n\n\ndef average_precision(r,cut):\n    \"\"\"Score is average precision (area under PR curve)\n    Relevance is binary (nonzero is relevant).\n    Returns:\n        Average precision\n    \"\"\"\n    r = np.asarray(r)\n    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n    if not out:\n        return 0.\n    return np.sum(out)/float(min(cut, np.sum(r)))\n\n\ndef mean_average_precision(rs):\n    \"\"\"Score is mean average precision\n    Relevance is binary (nonzero is relevant).\n    Returns:\n        Mean average precision\n    \"\"\"\n    return np.mean([average_precision(r) for r in rs])\n\n\ndef dcg_at_k(r, k, method=1):\n    \"\"\"Score is discounted cumulative gain (dcg)\n    Relevance is positive real values.  Can use binary\n    as the previous methods.\n    Returns:\n        Discounted cumulative gain\n    \"\"\"\n    r = np.asfarray(r)[:k]\n    if r.size:\n        if method == 0:\n            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n        elif method == 1:\n            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n        else:\n            raise ValueError('method must be 0 or 1.')\n    return 0.\n\n\ndef ndcg_at_k(r, k, method=1):\n    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n    Relevance is positive real values.  Can use binary\n    as the previous methods.\n    Returns:\n        Normalized discounted cumulative gain\n    \"\"\"\n    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n    if not dcg_max:\n        return 0.\n    return dcg_at_k(r, k, method) / dcg_max\n\n\ndef recall_at_k(r, k, all_pos_num):\n    r = np.asfarray(r)[:k]\n    if all_pos_num == 0:\n        return 0\n    else:\n        return np.sum(r) / all_pos_num\n\n\ndef hit_at_k(r, k):\n    r = np.array(r)[:k]\n    if np.sum(r) > 0:\n        return 1.\n    else:\n        return 0.\n\ndef F1(pre, rec):\n    if pre + rec > 0:\n        return (2.0 * pre * rec) / (pre + rec)\n    else:\n        return 0.\n\ndef auc(ground_truth, prediction):\n    try:\n        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n    except Exception:\n        res = 0.\n    return res"}
{"type": "source_file", "path": "LLM_augmentation_construct_prompt/gpt_user_profiling.py", "content": "\nimport threading\nimport openai\nimport time\nimport pandas as pd\nimport csv\nimport requests\nimport concurrent.futures\nimport pickle\nimport torch\nimport os\nimport threading\nimport time\nimport numpy as np\n\n\nopenai.api_base = \"http://llms-se.baidu-int.com:8200\"\n\n\nimport requests\n\n\nfile_path = \"\"\nmax_threads = 5\ncnt = 0 \n\n# MovieLens\ndef construct_prompting(item_attribute, item_list): \n    # make history string\n    history_string = \"User history:\\n\" \n    for index in item_list:\n        title = item_attribute['title'][index]\n        genre = item_attribute['genre'][index]\n        history_string += \"[\"\n        history_string += str(index)\n        history_string += \"] \"\n        history_string += title + \", \"\n        history_string += genre + \"\\n\"\n    # output format\n    output_format = \"Please output the following infomation of user, output format:\\n{\\'age\\':age, \\'gender\\':gender, \\'liked genre\\':liked genre, \\'disliked genre\\':disliked genre, \\'liked directors\\':liked directors, \\'country\\':country\\, 'language\\':language}\\nPlease do not fill in \\'unknown\\', but make an educated guess based on the available information and fill in the specific content.\\nplease output only the content in format above, but no other thing else, no reasoning, no analysis, no Chinese. Reiterating once again!! Please only output the content after \\\"output format: \\\", and do not include any other content such as introduction or acknowledgments.\\n\\n\"\n    # make prompt\n    prompt = \"You are required to generate user profile based on the history of user, that each movie with title, year, genre.\\n\"\n    prompt += history_string\n    prompt += output_format\n    return prompt\n\n# # Netflix\n# def construct_prompting(item_attribute, item_list): \n#     # make history string\n#     history_string = \"User history:\\n\" \n#     for index in item_list:\n#         year = item_attribute['year'][index]\n#         title = item_attribute['title'][index]\n#         history_string += \"[\"\n#         history_string += str(index)\n#         history_string += \"] \"\n#         history_string += str(year) + \", \"\n#         history_string += title + \"\\n\"\n#     # output format\n#     output_format = \"Please output the following infomation of user, output format:\\n{\\'age\\':age, \\'gender\\':gender, \\'liked genre\\':liked genre, \\'disliked genre\\':disliked genre, \\'liked directors\\':liked directors, \\'country\\':country\\, 'language\\':language}\\nPlease do not fill in \\'unknown\\', but make an educated guess based on the available information and fill in the specific content.\\nplease output only the content in format above, but no other thing else, no reasoning, no analysis, no Chinese. Reiterating once again!! Please only output the content after \\\"output format: \\\", and do not include any other content such as introduction or acknowledgments.\\n\\n\"\n#     # make prompt\n#     prompt = \"You are required to generate user profile based on the history of user, that each movie with title, year, genre.\\n\"\n#     prompt += history_string\n#     prompt += output_format\n#     return prompt\n\n\n# # embedding\n# def LLM_request(augmented_user_profiling_dict, index, model_type, augmented_user_init_embedding):\n\n#     if index in augmented_user_init_embedding:\n#         return 0\n#     else:\n#         try: \n#             # print(f\"{index}\")\n#             # prompt = construct_prompting(augmented_user_init_embedding, index)\n#             url = \"https://api.openai.com/v1/embeddings\"\n#             headers={\n#                 # \"Content-Type\": \"application/json\",\n#                 \"Authorization\": \"Bearer your key\"\n\n#             }\n#             params={\n#             \"model\": \"text-embedding-ada-002\",\n#             \"input\": augmented_user_profiling_dict[index]\n#             }\n\n#             response = requests.post(url=url, headers=headers,json=params)\n#             message = response.json()\n#             content = message['data'][0]['embedding']\n#             # print(content)\n#             print(index)\n\n#             augmented_user_init_embedding[index] = np.array(content)\n#             # pickle.dump(augmented_sample_dict, open('augmented_sample_dict','wb'))\n#             pickle.dump(augmented_user_init_embedding, open(file_path + 'augmented_user_init_embedding','wb'))\n        \n\n#         # except ValueError as e:\n#         except requests.exceptions.RequestException as e:\n#             print(\"An HTTP error occurred:\", str(e))\n#             time.sleep(5)\n#         except ValueError as ve:\n#             print(\"An error occurred while parsing the response:\", str(ve))\n#             time.sleep(5)\n#             LLM_request(augmented_user_profiling_dict, index, \"text-embedding-ada-002\", augmented_user_init_embedding)\n#         except KeyError as ke:\n#             print(\"An error occurred while accessing the response:\", str(ke))\n#             time.sleep(5)\n#             LLM_request(augmented_user_profiling_dict, index, \"text-embedding-ada-002\", augmented_user_init_embedding)\n#         except Exception as ex:\n#             print(\"An unknown error occurred:\", str(ex))\n#             time.sleep(5)\n        \n#         return 1\n    \n\n### user profile #################################################################################################################################\ndef get_gpt_response_w_system(model_type, prompt):\n    # global system_prompt\n    completion = openai.ChatCompletion.create(\n        model=model_type,\n        messages=[\n            # {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n    )\n    response = completion.choices[0].message.content\n    # print(response)  \n    return response\n\nstart_id = 0\ng_model_type = \"gpt-3.5-turbo-0613\" \n# # \"claude\", \"chatglm-6b\", \"hambuger-13b\", \"baichuan-7B\", \"gpt-4\", \"gpt-4-0613\"\n\n\n# toy_item_attribute, adjacency_list_dict, index, \"gpt-4\", augmented_user_profiling_dict\ndef file_reading():\n    augmented_user_profiling_dict = pickle.load(open(file_path + 'augmented_user_profiling_dict','rb')) \n    return augmented_user_profiling_dict\n## baidu user profile generate\ndef LLM_request(toy_item_attribute, adjacency_list_dict, index, model_type, _, error_cnt):    \n    # try:\n    #     augmented_user_profiling_dict = file_reading()\n    # except pickle.UnpicklingError as e:\n    #     # Handle the unpickling error\n    #     # time.sleep(0.001)\n    #     augmented_user_profiling_dict = file_reading()\n    #     print(\"Error occurred while unpickling:\", e)  \n\n    # if index in augmented_user_profiling_dict:\n    #     return 0\n    # else:\n        # try: \n    print(f\"{index}\")\n    prompt = construct_prompting(toy_item_attribute, adjacency_list_dict[index])\n    url = \"http://llms-se.baidu-int.com:8200/chat/completions\"\n    headers={\n        # \"Authorization\": \"Bearer your key\"\n    }\n    prompt = \"Please output the following infomation of user, output format:\\n{\\'age\\':age, \\'gender\\':gender, \\'liked genre\\':liked genre, \\'disliked genre\\':disliked genre, \\'liked directors\\':liked directors, \\'country\\':country\\, 'language\\':language}\\nPlease do not fill in \\'unknown\\', but make an educated guess based on the available information and fill in the specific content.\\nplease output only the content in format above, but no other thing else, no reasoning, no analysis, no Chinese. Reiterating once again!! Please only output the content after \\\"output format: \\\", and do not include any other content such as introduction or acknowledgments.\\n\\n\" + \"User history:\\n\" + \"[332]\" + \"title: Heart and Souls (1993), \" + \"genre: Comedy|Fantasy\\n\" + \"[364]\" + \"title: Men with Brooms (2002), \" + \"genre: Comedy|Drama|Romance\\n\" + \"You are required to generate user profile based on the history of user, that each movie with title, year, genre.\\n\"\n    params={\n        \"model\": model_type,\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        \"temperature\":0.8,\n        \"max_tokens\": 1000,\n        \"stream\": False, \n        \"top_p\": 0.1\n    }\n    response = requests.post(url=url, headers=headers,json=params)\n    message = response.json()\n    content = message['choices'][0]['message']['content']\n    # content = get_gpt_response_w_system(model_type, prompt)\n\n    print(f\"content: {content}, model_type: {model_type}\")\n\n    #     augmented_user_profiling_dict[index] = content\n    #     pickle.dump(augmented_user_profiling_dict, open(file_path + 'augmented_user_profiling_dict','wb'))\n    #     error_cnt = 0\n    #     time.sleep(8)\n    # # # except ValueError as e:\n    # # except requests.exceptions.RequestException as e:\n    # #     print(\"An HTTP error occurred:\", str(e))\n    # #     time.sleep(25)\n    # #     # print(content)\n    # #     LLM_request(toy_item_attribute, adjacency_list_dict, index, \"gpt-4\", augmented_user_profiling_dict)\n    # # except ValueError as ve:\n    # #     print(\"An error occurred while parsing the response:\", str(ve))\n    # #     time.sleep(25)\n    # #     # print(content)\n    # #     LLM_request(toy_item_attribute, adjacency_list_dict, index, \"gpt-4\", augmented_user_profiling_dict)\n    # # except KeyError as ke:\n    # #     print(\"An error occurred while accessing the response:\", str(ke))\n    # #     time.sleep(25)\n    # #     # print(content)\n    # #     LLM_request(toy_item_attribute, adjacency_list_dict, index, \"gpt-4\", augmented_user_profiling_dict)\n    # # except Exception as ex:\n    # #     print(\"An unknown error occurred:\", str(ex))\n    # #     time.sleep(25)\n    # #     # print(content)\n    # #     LLM_request(toy_item_attribute, adjacency_list_dict, index, \"gpt-4\", augmented_user_profiling_dict)\n    # # return 1\n    \n    # # except ValueError as e:\n    # except requests.exceptions.RequestException as e:\n    #     print(\"An HTTP error occurred:\", str(e))\n    #     time.sleep(5)\n    #     # print(content)\n    #     # error_cnt += 1\n    #     # if error_cnt==5:\n    #     #     return 1\n    #     # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n    #     LLM_request(toy_item_attribute, adjacency_list_dict, index, model_type, augmented_user_profiling_dict, error_cnt)\n    # except ValueError as ve:\n    #     print(\"ValueError error occurred while parsing the response:\", str(ve))\n    #     time.sleep(5)\n    #     # error_cnt += 1\n    #     # if error_cnt==5:\n    #     #     return 1\n    #     # print(content)\n    #     # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n    #     LLM_request(toy_item_attribute, adjacency_list_dict, index, model_type, augmented_user_profiling_dict, error_cnt)\n    # except KeyError as ke:\n    #     print(\"KeyError error occurred while accessing the response:\", str(ke))\n    #     time.sleep(5)\n    #     # error_cnt += 1\n    #     # if error_cnt==5:\n    #     #     return 1\n    #     # print(content)\n    #     # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n    #     LLM_request(toy_item_attribute, adjacency_list_dict, index, model_type, augmented_user_profiling_dict, error_cnt)\n    # except IndexError as ke:\n    #     print(\"IndexError error occurred while accessing the response:\", str(ke))\n    #     time.sleep(5)\n    #     # error_cnt += 1\n    #     # if error_cnt==5:\n    #     #     return 1\n    #     # # print(content)\n    #     # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict)\n    #     # return 1\n    #     LLM_request(toy_item_attribute, adjacency_list_dict, index, model_type, augmented_user_profiling_dict, error_cnt)\n    # except EOFError as ke:\n    #     print(\"EOFError: : Ran out of input error occurred while accessing the response:\", str(ke))\n    #     time.sleep(5)\n    #     # error_cnt += 1\n    #     # if error_cnt==5:\n    #     #     return 1\n    #     # print(content)\n    #     # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n    #     LLM_request(toy_item_attribute, adjacency_list_dict, index, model_type, augmented_user_profiling_dict, error_cnt)\n    # except Exception as ex:\n    #     print(\"An unknown error occurred:\", str(ex))\n    #     time.sleep(5)\n    #     # error_cnt += 1\n    #     # if error_cnt==5:\n    #     #     return 1\n    #     # print(content)\n    #     # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n    #     LLM_request(toy_item_attribute, adjacency_list_dict, index, model_type, augmented_user_profiling_dict, error_cnt)\n    # return 1\n### user profile #################################################################################################################################\n\n\n\n\n# ## chatgpt user profile generate\n# def LLM_request(toy_item_attribute, adjacency_list_dict, index, model_type, augmented_user_profiling_dict, error_cnt):\n#     if index in augmented_user_profiling_dict:\n#         return 0\n#     else:\n#         # try: \n#         print(f\"{index}\")\n#         prompt = construct_prompting(toy_item_attribute, adjacency_list_dict[index])\n#         url = \"http://llms-se.baidu-int.com:8200/chat/completions\"\n#         headers={\n#             # \"Content-Type\": \"application/json\",\n#             # \"Authorization\": \"Bearer your key\"\n#           \n#         }\n#         params={\n#             \"model\": model_type,\n#             \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n#             \"temperature\":0.6,\n#             \"max_tokens\": 1000,\n#             \"stream\": False, \n#             \"top_p\": 0.1\n#         }\n\n#         response = requests.post(url=url, headers=headers,json=params)\n#         message = response.json()\n\n#         content = message['choices'][0]['message']['content']\n#         print(f\"content: {content}, model_type: {model_type}\")\n\n#         augmented_user_profiling_dict[index] = content\n#         pickle.dump(augmented_user_profiling_dict, open(file_path + 'augmented_user_profiling_dict','wb'))\n#         error_cnt = 0\n#         # # except ValueError as e:\n#         # except requests.exceptions.RequestException as e:\n#         #     print(\"An HTTP error occurred:\", str(e))\n#         #     time.sleep(25)\n#         #     # print(content)\n#         #     LLM_request(toy_item_attribute, adjacency_list_dict, index, \"gpt-4\", augmented_user_profiling_dict)\n#         # except ValueError as ve:\n#         #     print(\"An error occurred while parsing the response:\", str(ve))\n#         #     time.sleep(25)\n#         #     # print(content)\n#         #     LLM_request(toy_item_attribute, adjacency_list_dict, index, \"gpt-4\", augmented_user_profiling_dict)\n#         # except KeyError as ke:\n#         #     print(\"An error occurred while accessing the response:\", str(ke))\n#         #     time.sleep(25)\n#         #     # print(content)\n#         #     LLM_request(toy_item_attribute, adjacency_list_dict, index, \"gpt-4\", augmented_user_profiling_dict)\n#         # except Exception as ex:\n#         #     print(\"An unknown error occurred:\", str(ex))\n#         #     time.sleep(25)\n#         #     # print(content)\n#         #     LLM_request(toy_item_attribute, adjacency_list_dict, index, \"gpt-4\", augmented_user_profiling_dict)\n#         # return 1\n     \n#         # # except ValueError as e:\n#         # except requests.exceptions.RequestException as e:\n#         #     print(\"An HTTP error occurred:\", str(e))\n#         #     # time.sleep(25)\n#         #     # print(content)\n#         #     error_cnt += 1\n#         #     if error_cnt==5:\n#         #         return 1\n#         #     # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n#         #     LLM_request(toy_item_attribute, adjacency_list_dict, index, \"gpt-4\", augmented_user_profiling_dict, error_cnt)\n#         # except ValueError as ve:\n#         #     print(\"ValueError error occurred while parsing the response:\", str(ve))\n#         #     # time.sleep(25)\n#         #     error_cnt += 1\n#         #     if error_cnt==5:\n#         #         return 1\n#         #     # print(content)\n#         #     # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n#         #     LLM_request(toy_item_attribute, adjacency_list_dict, index, \"gpt-4\", augmented_user_profiling_dict, error_cnt)\n#         # except KeyError as ke:\n#         #     print(\"KeyError error occurred while accessing the response:\", str(ke))\n#         #     # time.sleep(25)\n#         #     error_cnt += 1\n#         #     if error_cnt==5:\n#         #         return 1\n#         #     # print(content)\n#         #     # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n#         #     LLM_request(toy_item_attribute, adjacency_list_dict, index, \"gpt-4\", augmented_user_profiling_dict, error_cnt)\n#         # except IndexError as ke:\n#         #     print(\"IndexError error occurred while accessing the response:\", str(ke))\n#         #     # time.sleep(25)\n#         #     error_cnt += 1\n#         #     if error_cnt==5:\n#         #         return 1\n#         #     # # print(content)\n#         #     # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict)\n#         #     # return 1\n#         # except Exception as ex:\n#         #     print(\"An unknown error occurred:\", str(ex))\n#         #     # time.sleep(25)\n#         #     # error_cnt += 1\n#         #     # if error_cnt==5:\n#         #     #     return 1\n#         #     # print(content)\n#         #     # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n#         #     LLM_request(toy_item_attribute, adjacency_list_dict, index, \"gpt-4\", augmented_user_profiling_dict, error_cnt)\n#         # return 1\n\n\n\n\n\n\n\n\n\n\nerror_cnt = 0\n\n\n### step1: generate user profiling ################################################################################## \n### read item_attribute\ntoy_item_attribute = pd.read_csv(file_path + '/item_attribute.csv', names=['id','title', 'genre'])\n### write augmented dict\naugmented_user_profiling_dict = {}  \nif os.path.exists(file_path + \"augmented_user_profiling_dict\"): \n    print(f\"The file augmented_user_profiling_dict exists.\")\n    augmented_user_profiling_dict = pickle.load(open(file_path + 'augmented_user_profiling_dict','rb')) \nelse:\n    print(f\"The file augmented_user_profiling_dict does not exist.\")\n    pickle.dump(augmented_user_profiling_dict, open(file_path + 'augmented_user_profiling_dict','wb'))\n\n### read adjacency_list\nadjacency_list_dict = {}\ntrain_mat = pickle.load(open(file_path + 'train_mat','rb'))\nfor index in range(train_mat.shape[0]):\n    data_x, data_y = train_mat[index].nonzero()\n    adjacency_list_dict[index] = data_y\n\nfor index in range(start_id, len(adjacency_list_dict.keys())):\n    print(index)\n    # # make prompting\n    re = LLM_request(toy_item_attribute, adjacency_list_dict, index, g_model_type, augmented_user_profiling_dict, error_cnt)\n# \"claude\", \"chatglm-6b\", \"hambuger-13b\", \"baichuan-7B\", \"gpt-4\", \"gpt-4-0613\"\n### step1: generate user profiling ################################################################################## \n\n\n\n\n\n# ### step2: generate user embedding ################################################################################## \n\n# ### read user_profile\n# augmented_user_profiling_dict = pickle.load(open(file_path + 'augmented_user_profiling_dict','rb'))\n# ### write augmented_user_init_embedding\n# augmented_user_init_embedding = {}  \n# if os.path.exists(file_path + \"augmented_user_init_embedding\"): \n#     print(f\"The file augmented_user_init_embedding exists.\")\n#     augmented_user_init_embedding = pickle.load(open(file_path + 'augmented_user_init_embedding','rb')) \n# else:\n#     print(f\"The file augmented_user_init_embedding does not exist.\")\n#     pickle.dump(augmented_user_init_embedding, open(file_path + 'augmented_user_init_embedding','wb'))\n\n# for index,value in enumerate(augmented_user_profiling_dict.keys()):\n#     # # make prompting\n#     # prompt = construct_prompting(toy_item_attribute, adjacency_list_dict[index], candidate_indices_dict[index])\n#     re = LLM_request(augmented_user_profiling_dict, index, \"text-embedding-ada-002\", augmented_user_init_embedding)\n#     # print(f\"{index}\")\n#     # if re:\n#     #     time.sleep(0.5)\n# ### step2: generate user embedding ################################################################################## \n\n\n\n\n# # ### step3: get user embedding ################################################################################## \n# augmented_user_init_embedding = pickle.load(open('/Users/weiwei/Documents/Datasets/ml-10m/ml-10M100K/preprocessed_raw_MovieLens/toy_MovieLens1000/augmented_user_init_embedding','rb'))\n# augmented_user_init_embedding_list = []\n# for i in range(len(augmented_user_init_embedding)):\n#     augmented_user_init_embedding_list.append(augmented_user_init_embedding[i])\n# augmented_user_init_embedding_final = np.array(augmented_user_init_embedding_list)\n# pickle.dump(augmented_user_init_embedding_final, open('/Users/weiwei/Documents/Datasets/ml-10m/ml-10M100K/preprocessed_raw_MovieLens/toy_MovieLens1000/augmented_user_init_embedding_final','wb'))\n# # ### step3: get user embedding ################################################################################## \n\n\n\n\n# ### clean keys  ########################################################################################################\n# In [196]: new_augmented_user_profiling_dict = {}\n#      ...: for index,value in enumerate(augmented_user_profiling_dict.keys()):\n#      ...:     if type(value) == str:\n#      ...:         if int(value.strip(\"'\")) in augmented_user_profiling_dict:\n#      ...:             continue\n#      ...:         else:\n#      ...:             new_augmented_user_profiling_dict[int(value.strip(\"'\"))] = augmented_user_profiling_dict[value]\n#      ...:     else:\n#      ...:         new_augmented_user_profiling_dict[value] = augmented_user_profiling_dict[value]\n# ### clean keys  ########################################################################################################\n"}
{"type": "source_file", "path": "utility/norm.py", "content": "import torch\nimport numpy as np\nfrom scipy.sparse import csr_matrix \n\ndef build_sim(context):\n    context_norm = context.div(torch.norm(context, p=2, dim=-1, keepdim=True))\n    sim = torch.sparse.mm(context_norm, context_norm.transpose(1, 0))\n    return sim\n\ndef build_knn_normalized_graph(adj, topk, is_sparse, norm_type):\n    device = adj.device\n    knn_val, knn_ind = torch.topk(adj, topk, dim=-1)  \n    n_item = knn_val.shape[0]\n    n_data = knn_val.shape[0]*knn_val.shape[1]\n    data = np.ones(n_data)\n    if is_sparse:\n        tuple_list = [[row, int(col)] for row in range(len(knn_ind)) for col in knn_ind[row]]  #\n        row = [i[0] for i in tuple_list]  #\n        col = [i[1] for i in tuple_list]  #\n        ii_graph = csr_matrix((data, (row, col)) ,shape=(n_item, n_item))\n        return ii_graph\n    else:\n        weighted_adjacency_matrix = (torch.zeros_like(adj)).scatter_(-1, knn_ind, knn_val)\n        return get_dense_laplacian(weighted_adjacency_matrix, normalization=norm_type)\n\n\ndef get_sparse_laplacian(edge_index, edge_weight, num_nodes, normalization='none'):  \n    from torch_scatter import scatter_add\n    row, col = edge_index[0], edge_index[1]  \n    deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)  \n\n    if normalization == 'sym':\n        deg_inv_sqrt = deg.pow_(-0.5)\n        deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    elif normalization == 'rw':\n        deg_inv = 1.0 / deg\n        deg_inv.masked_fill_(deg_inv == float('inf'), 0)\n        edge_weight = deg_inv[row] * edge_weight\n    return edge_index, edge_weight\n\n\ndef get_dense_laplacian(adj, normalization='none'):\n    if normalization == 'sym':\n        rowsum = torch.sum(adj, -1)\n        d_inv_sqrt = torch.pow(rowsum, -0.5)\n        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.\n        d_mat_inv_sqrt = torch.diagflat(d_inv_sqrt)\n        L_norm = torch.mm(torch.mm(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n    elif normalization == 'rw':\n        rowsum = torch.sum(adj, -1)\n        d_inv = torch.pow(rowsum, -1)\n        d_inv[torch.isinf(d_inv)] = 0.\n        d_mat_inv = torch.diagflat(d_inv)\n        L_norm = torch.mm(d_mat_inv, adj)\n    elif normalization == 'none':\n        L_norm = adj\n    return L_norm\n"}
{"type": "source_file", "path": "MMSSL/main.py", "content": "from datetime import datetime\nimport math\nimport os\nimport random\nimport sys\nfrom time import time\nfrom tqdm import tqdm\nimport dgl\nimport pickle\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.sparse import csr_matrix\nimport  visdom\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.sparse as sparse\nfrom torch import autograd\n\n\nimport copy\n\n\n\nfrom utility.parser import parse_args\nfrom Models import G_Model, D_Model, Discriminator\nfrom utility.batch_test import *\nfrom utility.logging import Logger\nfrom utility.norm import build_sim, build_knn_normalized_graph\nfrom torch.utils.tensorboard import SummaryWriter\n\nargs = parse_args()\n\n\nclass Trainer(object):\n    def __init__(self, data_config):\n       \n        self.task_name = \"%s_%s_%s\" % (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), args.dataset, args.cf_model,)\n        self.logger = Logger(filename=self.task_name, is_debug=args.debug)\n        self.logger.logging(\"PID: %d\" % os.getpid())\n        self.logger.logging(str(args))\n\n        self.mess_dropout = eval(args.mess_dropout)\n        self.lr = args.lr\n        self.emb_dim = args.embed_size\n        self.batch_size = args.batch_size\n        self.weight_size = eval(args.weight_size)\n        self.n_layers = len(self.weight_size)\n        self.regs = eval(args.regs)\n        self.decay = self.regs[0]\n \n        self.image_feats = np.load('/home/ww/Code/work5/MMSSL/data/{}/image_feat.npy'.format(args.dataset))\n        self.text_feats = np.load('/home/ww/Code/work5/MMSSL/data/{}/text_feat.npy'.format(args.dataset))\n        self.image_feat_dim = self.image_feats.shape[-1]\n        self.text_feat_dim = self.text_feats.shape[-1]\n\n        self.ui_graph = self.ui_graph_raw = pickle.load(open('/home/ww/Code/work5/MMSSL/data/' + args.dataset + '/train_mat','rb'))\n\n        self.image_ui_graph_tmp = self.text_ui_graph_tmp = torch.tensor(self.ui_graph_raw.todense()).cuda()\n        self.image_iu_graph_tmp = self.text_iu_graph_tmp = torch.tensor(self.ui_graph_raw.T.todense()).cuda()\n\n        self.image_ui_index = {'x':[], 'y':[]}\n        self.text_ui_index = {'x':[], 'y':[]}\n\n        self.n_users = self.ui_graph.shape[0]\n        self.n_items = self.ui_graph.shape[1]        \n        self.iu_graph = self.ui_graph.T\n  \n        self.ui_graph_dgl = dgl.heterograph({('user','ui','item'):self.ui_graph.nonzero()})\n        self.iu_graph_dgl = dgl.heterograph({('user','ui','item'):self.iu_graph.nonzero()})\n\n        self.ui_graph = self.csr_norm(self.ui_graph, mean_flag=True)\n        self.iu_graph = self.csr_norm(self.iu_graph, mean_flag=True)\n        self.ui_graph = self.matrix_to_tensor(self.ui_graph)\n        self.iu_graph = self.matrix_to_tensor(self.iu_graph)\n        self.image_ui_graph = self.text_ui_graph = self.ui_graph\n        self.image_iu_graph = self.text_iu_graph = self.iu_graph\n\n        self.model_g = G_Model(self.n_users, self.n_items, self.emb_dim, self.weight_size, self.mess_dropout, self.image_feats, self.text_feats)      \n        self.model_d = D_Model(self.n_users, self.n_items, self.emb_dim, self.weight_size, self.mess_dropout, self.image_feats, self.text_feats)      \n        self.model_g = self.model_g.cuda()\n        self.model_d = self.model_d.cuda()\n\n        self.D = Discriminator(self.n_items).cuda()\n        self.D.apply(self.weights_init)\n        self.optim_D = optim.Adam(self.D.parameters(), lr=args.D_lr, betas=(0.5, 0.9), weight_decay=args.weight_decay)  \n        self.optim_D = optim.Adam(self.D.parameters(), lr=args.D_lr, betas=(0.5, 0.9))  \n\n        self.bce = nn.BCEWithLogitsLoss()\n        self.bce_loss = nn.BCELoss()        \n        self.feature_classifier_image = nn.Sequential()  \n        self.feature_classifier_image.add_module('d_fc1', nn.Linear(self.image_feat_dim, int(self.image_feat_dim/2)))\n        self.feature_classifier_image.add_module('d_bn1', nn.BatchNorm1d(int(self.image_feat_dim/2)))\n        self.feature_classifier_image.add_module('d_relu1', nn.ReLU(True))\n        self.feature_classifier_image.add_module('d_fc2', nn.Linear(int(self.image_feat_dim/2), 1))  \n        self.feature_classifier_image.add_module('d_sigmoid', nn.Sigmoid())\n        self.feature_classifier_image = self.feature_classifier_image.cuda()\n        self.feature_classifier_text = nn.Sequential()\n        self.feature_classifier_text.add_module('d_fc1', nn.Linear(self.text_feat_dim, int(self.text_feat_dim/2)))\n        self.feature_classifier_text.add_module('d_bn1', nn.BatchNorm1d(int(self.text_feat_dim/2)))\n        self.feature_classifier_text.add_module('d_relu1', nn.ReLU(True))\n        self.feature_classifier_text.add_module('d_fc2', nn.Linear(int(self.text_feat_dim/2), 1))  \n        self.feature_classifier_text.add_module('d_sigmoid', nn.Sigmoid())\n        self.feature_classifier_text = self.feature_classifier_text.cuda()\n\n        self.feature_classifier_common = nn.Sequential()\n        self.feature_classifier_common.add_module('d_fc1', nn.Linear(self.emb_dim, int(self.emb_dim/2)))\n        self.feature_classifier_common.add_module('d_bn1', nn.BatchNorm1d(int(self.emb_dim/2)))\n        self.feature_classifier_common.add_module('d_relu1', nn.ReLU(True))\n        self.feature_classifier_common.add_module('d_fc2', nn.Linear(int(self.emb_dim/2), 1))  \n        self.feature_classifier_common.add_module('d_sigmoid', nn.Sigmoid())\n        self.feature_classifier_common = self.feature_classifier_common.cuda()\n\n        self.optimizer_D = optim.AdamW(\n        [\n            {'params':self.model_d.parameters()},      #\n        ]\n            , lr=self.lr)  #\n\n        self.optimizer_G = optim.AdamW(\n        [\n            {'params':self.model_g.parameters()},\n        ]\n            , lr=self.lr)  #\n\n        self.scheduler_D, self.scheduler_G = self.set_lr_scheduler()\n\n\n    def set_lr_scheduler(self):\n        fac = lambda epoch: 0.96 ** (epoch / 50)\n\n        scheduler_D = optim.lr_scheduler.LambdaLR(self.optimizer_D, lr_lambda=fac)\n        scheduler_G = optim.lr_scheduler.LambdaLR(self.optimizer_G, lr_lambda=fac)\n\n        return scheduler_D, scheduler_G\n\n    def csr_norm(self, csr_mat, mean_flag=False):\n        rowsum = np.array(csr_mat.sum(1))\n        rowsum = np.power(rowsum+1e-8, -0.5).flatten()\n        rowsum[np.isinf(rowsum)] = 0.\n        rowsum_diag = sp.diags(rowsum)\n\n        colsum = np.array(csr_mat.sum(0))\n        colsum = np.power(colsum+1e-8, -0.5).flatten()\n        colsum[np.isinf(colsum)] = 0.\n        colsum_diag = sp.diags(colsum)\n\n        if mean_flag == False:\n            return rowsum_diag*csr_mat*colsum_diag\n        else:\n            return rowsum_diag*csr_mat\n\n    def matrix_to_tensor(self, cur_matrix):\n        if type(cur_matrix) != sp.coo_matrix:\n            cur_matrix = cur_matrix.tocoo()  #\n        indices = torch.from_numpy(np.vstack((cur_matrix.row, cur_matrix.col)).astype(np.int64))  #\n        values = torch.from_numpy(cur_matrix.data)  #\n        shape = torch.Size(cur_matrix.shape)\n\n        return torch.sparse.FloatTensor(indices, values, shape).to(torch.float32).cuda()  #\n\n    def innerProduct(self, u_pos, i_pos, u_neg, j_neg):  \n        pred_i = torch.sum(torch.mul(u_pos,i_pos), dim=-1) \n        pred_j = torch.sum(torch.mul(u_neg,j_neg), dim=-1)  \n        return pred_i, pred_j\n\n    def sampleTrainBatch_dgl(self, batIds, pos_id=None, g=None, g_neg=None, sample_num=None, sample_num_neg=None):\n\n        sub_g = dgl.sampling.sample_neighbors(g.cpu(), {'user':batIds}, sample_num, edge_dir='out', replace=True)\n        row, col = sub_g.edges()\n        row = row.reshape(len(batIds), sample_num)\n        col = col.reshape(len(batIds), sample_num)\n\n        if g_neg==None:\n            return row, col\n        else: \n            sub_g_neg = dgl.sampling.sample_neighbors(g_neg, {'user':batIds}, sample_num_neg, edge_dir='out', replace=True)\n            row_neg, col_neg = sub_g_neg.edges()\n            row_neg = row_neg.reshape(len(batIds), sample_num_neg)\n            col_neg = col_neg.reshape(len(batIds), sample_num_neg)\n            return row, col, col_neg \n\n    def weights_init(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.kaiming_normal_(m.weight)\n            m.bias.data.fill_(0)\n\n    def gradient_penalty(self, D, xr, xf):\n\n        LAMBDA = 0.3\n\n        xf = xf.detach()\n        xr = xr.detach()\n\n        alpha = torch.rand(args.batch_size*2, 1).cuda()\n        alpha = alpha.expand_as(xr)\n\n        interpolates = alpha * xr + ((1 - alpha) * xf)\n        interpolates.requires_grad_()\n\n        disc_interpolates = D(interpolates)\n\n        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                                grad_outputs=torch.ones_like(disc_interpolates),\n                                create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n        gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n\n        return gp\n\n    def weighted_sum(self, anchor, nei, co):  \n\n        ac = torch.multiply(anchor, co).sum(-1).sum(-1)  \n        nc = torch.multiply(nei, co).sum(-1).sum(-1)  \n\n        an = (anchor.permute(1, 0, 2)[0])\n        ne = (nei.permute(1, 0, 2)[0])\n\n        an_w = an*(ac.unsqueeze(-1).repeat(1, args.embed_size))\n        ne_w = ne*(nc.unsqueeze(-1).repeat(1, args.embed_size))                                     \n  \n        res = (args.anchor_rate*an_w + (1-args.anchor_rate)*ne_w).reshape(-1, args.sample_num_ii, args.embed_size).sum(1)\n\n        return res\n\n\n    def sample_topk(self, u_sim, users, emb_type=None):\n        topk_p, topk_id = torch.topk(u_sim, args.ad_topk*10, dim=-1)  \n        topk_data = topk_p.reshape(-1).cpu()\n        topk_col = topk_id.reshape(-1).cpu().int()\n        topk_row = torch.tensor(np.array(users)).unsqueeze(1).repeat(1, args.ad_topk*args.ad_topk_multi_num).reshape(-1).int()  #\n        topk_csr = csr_matrix((topk_data.detach().numpy(), (topk_row.detach().numpy(), topk_col.detach().numpy())), shape=(self.n_users, self.n_items))\n        topk_g = dgl.heterograph({('user','ui','item'):topk_csr.nonzero()})\n        _, topk_id = self.sampleTrainBatch_dgl(users, g=topk_g, sample_num=args.ad_topk, pos_id=None, g_neg=None, sample_num_neg=None)\n        self.gene_fake[emb_type] = topk_id\n\n        topk_id_u = torch.arange(len(users)).unsqueeze(1).repeat(1, args.ad_topk)\n        topk_p = u_sim[topk_id_u, topk_id]\n        return topk_p, topk_id\n\n    def ssl_loss_calculation(self, ssl_image_logit, ssl_text_logit, ssl_common_logit):\n        ssl_label_1_s2 = torch.ones(1, self.n_items).cuda()\n        ssl_label_0_s2 = torch.zeros(1, self.n_items).cuda()\n        ssl_label_s2 = torch.cat((ssl_label_1_s2, ssl_label_0_s2), 1)\n        ssl_image_s2 = self.bce(ssl_image_logit, ssl_label_s2)\n        ssl_text_s2 = self.bce(ssl_text_logit, ssl_label_s2)\n        ssl_loss_s2 = ssl_image_s2 + ssl_text_s2\n\n        ssl_label_1_c2 = torch.ones(1, self.n_items*2).cuda()\n        ssl_label_0_c2 = torch.zeros(1, self.n_items*2).cuda()\n        ssl_label_c2 = torch.cat((ssl_label_1_c2, ssl_label_0_c2), 1)\n        ssl_result_c2 = self.bce(ssl_common_logit, ssl_label_c2)  \n        ssl_loss_c2 = ssl_result_c2\n\n        ssl_loss2 = args.ssl_s_rate*ssl_loss_s2 + args.ssl_c_rate*ssl_loss_c2 \n        return ssl_loss2\n\n\n    def sim(self, z1, z2):\n        z1 = F.normalize(z1)  \n        z2 = F.normalize(z2)\n        # z1 = z1/((z1**2).sum(-1) + 1e-8)\n        # z2 = z2/((z2**2).sum(-1) + 1e-8)\n        return torch.mm(z1, z2.t())\n\n    def batched_contrastive_loss(self, z1, z2, batch_size=1024):\n\n        device = z1.device\n        num_nodes = z1.size(0)\n        num_batches = (num_nodes - 1) // batch_size + 1\n        f = lambda x: torch.exp(x / args.tau)   #       \n\n        indices = torch.arange(0, num_nodes).to(device)\n        losses = []\n\n        for i in range(num_batches):\n            tmp_i = indices[i * batch_size:(i + 1) * batch_size]\n\n            tmp_refl_sim_list = []\n            tmp_between_sim_list = []\n            for j in range(num_batches):\n                tmp_j = indices[j * batch_size:(j + 1) * batch_size]\n                tmp_refl_sim = f(self.sim(z1[tmp_i], z1[tmp_j]))  \n                tmp_between_sim = f(self.sim(z1[tmp_i], z2[tmp_j]))  \n\n                tmp_refl_sim_list.append(tmp_refl_sim)\n                tmp_between_sim_list.append(tmp_between_sim)\n\n            refl_sim = torch.cat(tmp_refl_sim_list, dim=-1)\n            between_sim = torch.cat(tmp_between_sim_list, dim=-1)\n\n            losses.append(-torch.log(between_sim[:, i * batch_size:(i + 1) * batch_size].diag()/ (refl_sim.sum(1) + between_sim.sum(1) - refl_sim[:, i * batch_size:(i + 1) * batch_size].diag())+1e-8))\n\n            del refl_sim, between_sim, tmp_refl_sim_list, tmp_between_sim_list\n                   \n        loss_vec = torch.cat(losses)\n        return loss_vec.mean()\n\n\n    def feat_reg_loss_calculation(self, g_item_image, g_item_text, g_user_image, g_user_text):\n        feat_reg = 1./2*(g_item_image**2).sum() + 1./2*(g_item_text**2).sum() \\\n            + 1./2*(g_user_image**2).sum() + 1./2*(g_user_text**2).sum()        \n        feat_reg = feat_reg / self.n_items\n        feat_emb_loss = args.feat_reg_decay * feat_reg\n        return feat_emb_loss\n\n\n    def fake_gene_loss_calculation(self, u_emb, i_emb, emb_type=None):\n        if self.gene_u!=None:\n            gene_real_loss = (-F.logsigmoid((u_emb[self.gene_u]*i_emb[self.gene_real]).sum(-1)+1e-8)).mean()\n            gene_fake_loss = (1-(-F.logsigmoid((u_emb[self.gene_u]*i_emb[self.gene_fake[emb_type]]).sum(-1)+1e-8))).mean()\n\n            gene_loss = gene_real_loss + gene_fake_loss\n        else:\n            gene_loss = 0\n\n        return gene_loss\n\n    def reward_loss_calculation(self, users, re_u, re_i, topk_id, topk_p):\n        self.gene_u = torch.tensor(np.array(users)).unsqueeze(1).repeat(1, args.ad_topk)\n        reward_u = re_u[self.gene_u]\n        reward_i = re_i[topk_id]\n        reward_value = (reward_u*reward_i).sum(-1)\n\n        reward_loss = -(((topk_p*reward_value).sum(-1)).mean()+1e-8).log()\n        \n        return reward_loss\n\n\n\n    def u_sim_calculation(self, users, user_final, item_final):\n        topk_u = user_final[users]\n        u_ui = torch.tensor(self.ui_graph_raw[users].todense()).cuda()\n\n        num_batches = (self.n_items - 1) // args.batch_size + 1\n        indices = torch.arange(0, self.n_items).cuda()\n        u_sim_list = []\n\n        for i_b in range(num_batches):\n            index = indices[i_b * args.batch_size:(i_b + 1) * args.batch_size]\n            sim = torch.mm(topk_u, item_final[index].T)\n            sim_gt = torch.multiply(sim, (1-u_ui[:, index]))\n            u_sim_list.append(sim_gt)\n                \n        u_sim = F.normalize(torch.cat(u_sim_list, dim=-1), p=2, dim=1)   \n        return u_sim\n\n\n    def test(self, users_to_test, is_val):\n        self.model_d.eval()\n        with torch.no_grad():\n            ua_embeddings, ia_embeddings, *rest = self.model_d(self.ui_graph, self.iu_graph, self.image_ui_graph, self.image_iu_graph, self.text_ui_graph, self.text_iu_graph)\n        result = test_torch(ua_embeddings, ia_embeddings, users_to_test, is_val)\n        return result\n\n    def train(self):\n\n        now_time = datetime.now()\n        run_time = datetime.strftime(now_time,'%Y_%m_%d__%H_%M_%S')\n\n        training_time_list = []\n        loss_loger, pre_loger, rec_loger, ndcg_loger, hit_loger = [], [], [], [], []\n        line_var_loss, line_g_loss, line_d_loss, line_cl_loss, line_var_recall, line_var_precision, line_var_ndcg = [], [], [], [], [], [], []\n        stopping_step = 0\n        should_stop = False\n        cur_best_pre_0 = 0. \n        # tb_writer = SummaryWriter(log_dir=\"/home/ww/Code/work5/MICRO2Ours/tensorboard/\")\n        # tensorboard_cnt = 0\n\n        n_batch = data_generator.n_train // args.batch_size + 1\n        best_recall = 0\n        for epoch in range(args.epoch):\n            t1 = time()\n            loss, mf_loss, emb_loss, reg_loss = 0., 0., 0., 0.\n            contrastive_loss = 0.\n            n_batch = data_generator.n_train // args.batch_size + 1\n            f_time, b_time, loss_time, opt_time, clip_time, emb_time = 0., 0., 0., 0., 0., 0.\n            sample_time = 0.\n            build_item_graph = True\n\n            self.gene_u, self.gene_real, self.gene_fake = None, None, {}\n            self.topk_p_dict, self.topk_id_dict = {}, {}\n\n            for idx in tqdm(range(n_batch)):\n                self.model_d.train()\n                self.model_g.train()\n                sample_t1 = time()\n                users, pos_items, neg_items = data_generator.sample()\n                sample_time += time() - sample_t1       \n\n                with torch.no_grad():\n                    ua_embeddings, ia_embeddings, image_item_embeds, text_item_embeds, image_user_embeds, text_user_embeds \\\n                                    , user_emb, item_emb, image_user_id, text_user_id, image_item_id, text_item_id \\\n                            = self.model_d(self.ui_graph, self.iu_graph, self.image_ui_graph, self.image_iu_graph, self.text_ui_graph, self.text_iu_graph)\n\n                ui_u_sim = self.u_sim_calculation(users, ua_embeddings, ia_embeddings)\n                image_u_sim = self.u_sim_calculation(users, image_user_embeds, image_item_embeds)\n                text_u_sim = self.u_sim_calculation(users, text_user_embeds, text_item_embeds)\n                ui_u_sim_detach = ui_u_sim.detach() \n                image_u_sim_detach = image_u_sim.detach() \n                text_u_sim_detach = text_u_sim.detach()\n\n\n\n                inputf = torch.cat((image_u_sim_detach, text_u_sim_detach), dim=0)\n                predf = (self.D(inputf))\n\n                lossf = (predf.mean())\n                u_ui = torch.tensor(self.ui_graph_raw[users].todense()).cuda()\n                noise = torch.empty((u_ui.shape[0], u_ui.shape[1]), dtype=torch.float32).uniform_(0,1).cuda()\n                logits_with_noise = u_ui - args.log_log_scale*torch.log(-torch.log(noise+1e-8)+1e-8)\n                u_ui = F.softmax(logits_with_noise/args.real_data_tau, dim=1) #0.002  \n                u_ui += ui_u_sim_detach*args.ui_pre_scale                  \n                u_ui = F.normalize(u_ui, dim=1)  \n\n\n                write_path = \"/home/ww/Code/work5/MICRO2Ours/t_SNE_G/distribution/dir_draw\"\n                write_data = [\"u_ui\", \"image_u_sim_detach\", \"text_u_sim_detach\"]\n\n                \"\"\"\n                u_ui\n                noise\n                u_ui - log_log_noise\n                \n                \"\"\"\n\n                inputr = torch.cat((u_ui, u_ui), dim=0)\n                predr = (self.D(inputr))\n\n                lossr = - (predr.mean())\n\n                gp = self.gradient_penalty(self.D, inputr, inputf.detach())\n\n                loss_D = lossr + lossf + args.gp_rate*gp \n        \n                self.optim_D.zero_grad()\n                loss_D.backward()\n                self.optim_D.step()\n                line_d_loss.append(loss_D.detach().data)\n\n                G_ua_embeddings, G_ia_embeddings, G_image_item_embeds, G_text_item_embeds, G_image_user_embeds, G_text_user_embeds \\\n                                , G_user_emb, G_item_emb, G_image_user_id, G_text_user_id, G_image_item_id, G_text_item_id \\\n                        = self.model_d(self.ui_graph, self.iu_graph, self.image_ui_graph, self.image_iu_graph, self.text_ui_graph, self.text_iu_graph)\n\n\n                G_u_g_embeddings = G_ua_embeddings[users]\n                G_pos_i_g_embeddings = G_ia_embeddings[pos_items]\n                G_neg_i_g_embeddings = G_ia_embeddings[neg_items]\n                G_batch_mf_loss, G_batch_emb_loss, G_batch_reg_loss = self.bpr_loss(G_u_g_embeddings, G_pos_i_g_embeddings, G_neg_i_g_embeddings)\n       \n                G_image_u_g_embeddings = G_image_user_embeds[users]\n                G_image_pos_i_g_embeddings = G_image_item_embeds[pos_items]\n                G_image_neg_i_g_embeddings = G_image_item_embeds[neg_items]\n                G_image_batch_mf_loss, G_image_batch_emb_loss, G_image_batch_reg_loss = self.bpr_loss(G_image_u_g_embeddings, G_image_pos_i_g_embeddings, G_image_neg_i_g_embeddings)\n\n                G_text_u_g_embeddings = G_text_user_embeds[users]\n                G_text_pos_i_g_embeddings = G_text_item_embeds[pos_items]\n                G_text_neg_i_g_embeddings = G_text_item_embeds[neg_items]\n                G_text_batch_mf_loss, G_text_batch_emb_loss, G_text_batch_reg_loss = self.bpr_loss(G_text_u_g_embeddings, G_text_pos_i_g_embeddings, G_text_neg_i_g_embeddings)\n\n                G_ui_u_sim = self.u_sim_calculation(users, G_ua_embeddings, G_ia_embeddings)\n                G_image_u_sim = self.u_sim_calculation(users, G_image_user_embeds, G_image_item_embeds)\n                G_text_u_sim = self.u_sim_calculation(users, G_text_user_embeds, G_text_item_embeds)\n                G_image_u_sim_detach = G_image_u_sim.detach() \n                G_text_u_sim_detach = G_text_u_sim.detach()\n\n\n                if idx%args.T==0 and idx!=0:\n                    self.image_ui_graph_tmp = csr_matrix((torch.ones(len(self.image_ui_index['x'])),(self.image_ui_index['x'], self.image_ui_index['y'])), shape=(self.n_users, self.n_items))\n                    self.text_ui_graph_tmp = csr_matrix((torch.ones(len(self.text_ui_index['x'])),(self.text_ui_index['x'], self.text_ui_index['y'])), shape=(self.n_users, self.n_items))\n                    self.image_iu_graph_tmp = self.image_ui_graph_tmp.T\n                    self.text_iu_graph_tmp = self.text_ui_graph_tmp.T\n                    self.image_ui_graph = self.sparse_mx_to_torch_sparse_tensor( \\\n                        self.csr_norm(self.image_ui_graph_tmp, mean_flag=True)\n                        ).cuda() \n                    self.text_ui_graph = self.sparse_mx_to_torch_sparse_tensor(\n                        self.csr_norm(self.text_ui_graph_tmp, mean_flag=True)\n                        ).cuda()\n                    self.image_iu_graph = self.sparse_mx_to_torch_sparse_tensor(\n                        self.csr_norm(self.image_iu_graph_tmp, mean_flag=True)\n                        ).cuda()\n                    self.text_iu_graph = self.sparse_mx_to_torch_sparse_tensor(\n                        self.csr_norm(self.text_iu_graph_tmp, mean_flag=True)\n                        ).cuda()\n\n                    self.image_ui_index = {'x':[], 'y':[]}\n                    self.text_ui_index = {'x':[], 'y':[]}\n\n                else:\n                    image_ui_v, image_ui_id = torch.topk(G_image_u_sim_detach, int(self.n_items*args.m_topk_rate), dim=-1)\n                    self.image_ui_index['x'] += np.array(torch.tensor(users).repeat(1, int(self.n_items*args.m_topk_rate)).view(-1)).tolist()\n                    self.image_ui_index['y'] += np.array(image_ui_id.cpu().view(-1)).tolist()\n                    text_ui_v, text_ui_id = torch.topk(G_text_u_sim_detach, int(self.n_items*args.m_topk_rate), dim=-1)\n                    self.text_ui_index['x'] += np.array(torch.tensor(users).repeat(1, int(self.n_items*args.m_topk_rate)).view(-1)).tolist()\n                    self.text_ui_index['y'] += np.array(text_ui_id.cpu().view(-1)).tolist()\n\n\n                feat_emb_loss = self.feat_reg_loss_calculation(G_image_item_embeds, G_text_item_embeds, G_image_user_embeds, G_text_user_embeds)\n\n                batch_contrastive_loss = 0\n                batch_contrastive_loss1 = self.batched_contrastive_loss(G_image_user_id[users],G_user_emb[users])\n                batch_contrastive_loss2 = self.batched_contrastive_loss(G_text_user_id[users],G_user_emb[users])\n  \n                batch_contrastive_loss = batch_contrastive_loss1 + batch_contrastive_loss2 \n    \n                G_inputf = torch.cat((G_image_u_sim, G_text_u_sim), dim=0)\n                G_predf = (self.D(G_inputf))\n\n                G_lossf = -(G_predf.mean())\n                batch_loss = G_batch_mf_loss + G_batch_emb_loss + G_batch_reg_loss + feat_emb_loss + args.G_rate*G_lossf+ args.cl_rate*batch_contrastive_loss  \n\n                line_var_loss.append(batch_loss.detach().data)\n                line_g_loss.append(G_lossf.detach().data)\n                line_cl_loss.append(batch_contrastive_loss.detach().data)\n                             \n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           #+ ssl_loss2 #+ batch_contrastive_loss\n                self.optimizer_D.zero_grad()  \n                batch_loss.backward(retain_graph=False)\n                self.optimizer_D.step()\n\n                loss += float(batch_loss)\n                mf_loss += float(G_batch_mf_loss)\n                emb_loss += float(G_batch_emb_loss)\n                reg_loss += float(G_batch_reg_loss)\n    \n    \n            del ua_embeddings, ia_embeddings, G_ua_embeddings, G_ia_embeddings, G_u_g_embeddings, G_neg_i_g_embeddings, G_pos_i_g_embeddings\n\n\n            if math.isnan(loss) == True:\n                self.logger.logging('ERROR: loss is nan.')\n                sys.exit()\n\n            if (epoch + 1) % args.verbose != 0:\n                perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f + %.5f  + %.5f]' % (\n                    epoch, time() - t1, loss, mf_loss, emb_loss, reg_loss, contrastive_loss)\n                training_time_list.append(time() - t1)\n                self.logger.logging(perf_str)\n\n            t2 = time()\n            users_to_test = list(data_generator.test_set.keys())\n            users_to_val = list(data_generator.val_set.keys())\n            ret = self.test(users_to_test, is_val=False)  #^-^\n            training_time_list.append(t2 - t1)\n\n            t3 = time()\n\n            loss_loger.append(loss)\n            rec_loger.append(ret['recall'].data)\n            pre_loger.append(ret['precision'].data)\n            ndcg_loger.append(ret['ndcg'].data)\n            hit_loger.append(ret['hit_ratio'].data)\n\n            line_var_recall.append(ret['recall'][1])\n            line_var_precision.append(ret['precision'][1])\n            line_var_ndcg.append(ret['ndcg'][1])\n\n            tags = [\"recall\", \"precision\", \"ndcg\"]\n            # tb_writer.add_scalar(tags[0], ret['recall'][1], epoch)\n            # tb_writer.add_scalar(tags[1], ret['precision'][1], epoch)\n            # tb_writer.add_scalar(tags[2], ret['ndcg'][1], epoch)\n\n\n            if args.verbose > 0:\n                perf_str = 'Epoch %d [%.1fs + %.1fs]: train==[%.5f=%.5f + %.5f + %.5f], recall=[%.5f, %.5f, %.5f], ' \\\n                           'precision=[%.5f, %.5f, %.5f], hit=[%.5f, %.5f, %.5f], ndcg=[%.5f, %.5f, %.5f]' % \\\n                           (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, reg_loss, ret['recall'][0], ret['recall'][1],\n                            ret['recall'][-1],\n                            ret['precision'][0], ret['precision'][1], ret['precision'][-1], ret['hit_ratio'][0], ret['hit_ratio'][1], ret['hit_ratio'][-1],\n                            ret['ndcg'][0], ret['ndcg'][1], ret['ndcg'][-1])\n                self.logger.logging(perf_str)\n\n            if ret['recall'][1] > best_recall:\n                best_recall = ret['recall'][1]\n                test_ret = self.test(users_to_test, is_val=False)\n                self.logger.logging(\"Test_Recall@%d: %.5f,  precision=[%.5f], ndcg=[%.5f]\" % (eval(args.Ks)[1], test_ret['recall'][1], test_ret['precision'][1], test_ret['ndcg'][1]))\n                stopping_step = 0\n            elif stopping_step < args.early_stopping_patience:\n                stopping_step += 1\n                self.logger.logging('#####Early stopping steps: %d #####' % stopping_step)\n            else:\n                self.logger.logging('#####Early stop! #####')\n                break\n        self.logger.logging(str(test_ret))\n\n        return best_recall, run_time \n\n\n    def bpr_loss(self, users, pos_items, neg_items):\n        pos_scores = torch.sum(torch.mul(users, pos_items), dim=1)\n        neg_scores = torch.sum(torch.mul(users, neg_items), dim=1)\n\n        regularizer = 1./2*(users**2).sum() + 1./2*(pos_items**2).sum() + 1./2*(neg_items**2).sum()        \n        regularizer = regularizer / self.batch_size\n\n        maxi = F.logsigmoid(pos_scores - neg_scores)\n        mf_loss = -torch.mean(maxi)\n\n        emb_loss = self.decay * regularizer\n        reg_loss = 0.0\n        return mf_loss, emb_loss, reg_loss\n\n    def sparse_mx_to_torch_sparse_tensor(self, sparse_mx):\n        \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n        sparse_mx = sparse_mx.tocoo().astype(np.float32)\n        indices = torch.from_numpy(\n            np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n        values = torch.from_numpy(sparse_mx.data)\n        shape = torch.Size(sparse_mx.shape)\n        return torch.sparse.FloatTensor(indices, values, shape)\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed) \n    torch.cuda.manual_seed_all(seed)  \n\nif __name__ == '__main__':\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n    set_seed(args.seed)\n    config = dict()\n    config['n_users'] = data_generator.n_users\n    config['n_items'] = data_generator.n_items\n\n    trainer = Trainer(data_config=config)\n    trainer.train()\n"}
{"type": "source_file", "path": "utility/metrics.py", "content": "import numpy as np\nfrom sklearn.metrics import roc_auc_score\n\ndef recall(rank, ground_truth, N):\n    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n\n\ndef precision_at_k(r, k):\n    \"\"\"Score is precision @ k\n    Relevance is binary (nonzero is relevant).\n    Returns:\n        Precision @ k\n    Raises:\n        ValueError: len(r) must be >= k\n    \"\"\"\n    assert k >= 1\n    r = np.asarray(r)[:k]\n    return np.mean(r)\n\n\ndef average_precision(r,cut):\n    \"\"\"Score is average precision (area under PR curve)\n    Relevance is binary (nonzero is relevant).\n    Returns:\n        Average precision\n    \"\"\"\n    r = np.asarray(r)\n    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n    if not out:\n        return 0.\n    return np.sum(out)/float(min(cut, np.sum(r)))\n\n\ndef mean_average_precision(rs):\n    \"\"\"Score is mean average precision\n    Relevance is binary (nonzero is relevant).\n    Returns:\n        Mean average precision\n    \"\"\"\n    return np.mean([average_precision(r) for r in rs])\n\n\ndef dcg_at_k(r, k, method=1):\n    \"\"\"Score is discounted cumulative gain (dcg)\n    Relevance is positive real values.  Can use binary\n    as the previous methods.\n    Returns:\n        Discounted cumulative gain\n    \"\"\"\n    r = np.asfarray(r)[:k]\n    if r.size:\n        if method == 0:\n            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n        elif method == 1:\n            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n        else:\n            raise ValueError('method must be 0 or 1.')\n    return 0.\n\n\ndef ndcg_at_k(r, k, method=1):\n    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n    Relevance is positive real values.  Can use binary\n    as the previous methods.\n    Returns:\n        Normalized discounted cumulative gain\n    \"\"\"\n    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n    if not dcg_max:\n        return 0.\n    return dcg_at_k(r, k, method) / dcg_max\n\n\ndef recall_at_k(r, k, all_pos_num):\n    r = np.asfarray(r)[:k]\n    if all_pos_num == 0:\n        return 0\n    else:\n        return np.sum(r) / all_pos_num\n\n\ndef hit_at_k(r, k):\n    r = np.array(r)[:k]\n    if np.sum(r) > 0:\n        return 1.\n    else:\n        return 0.\n\ndef F1(pre, rec):\n    if pre + rec > 0:\n        return (2.0 * pre * rec) / (pre + rec)\n    else:\n        return 0.\n\ndef auc(ground_truth, prediction):\n    try:\n        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n    except Exception:\n        res = 0.\n    return res"}
{"type": "source_file", "path": "utility/load_data.py", "content": "\nimport numpy as np\nimport random as rd\nimport scipy.sparse as sp\nfrom time import time\nimport json\nfrom utility.parser import parse_args\nargs = parse_args()\n\nclass Data(object):\n    def __init__(self, path, batch_size):\n        self.path = path\n        self.batch_size = batch_size\n\n        train_file = path + '/train.json'\n        val_file = path + '/val.json' \n        test_file = path + '/test.json'\n\n        #get number of users and items\n        self.n_users, self.n_items = 0, 0\n        self.n_train, self.n_test = 0, 0\n        self.neg_pools = {}\n\n        self.exist_users = []\n\n        train = json.load(open(train_file))\n        test = json.load(open(test_file))\n        val = json.load(open(val_file))\n        for uid, items in train.items():\n            if len(items) == 0:\n                continue\n            uid = int(uid)\n            self.exist_users.append(uid)\n            self.n_items = max(self.n_items, max(items))\n            self.n_users = max(self.n_users, uid)\n            self.n_train += len(items)\n\n        for uid, items in test.items():\n            uid = int(uid)\n            try:\n                self.n_items = max(self.n_items, max(items))\n                self.n_test += len(items)\n            except:\n                continue\n\n        for uid, items in val.items():\n            uid = int(uid)\n            try:\n                self.n_items = max(self.n_items, max(items))\n                self.n_val += len(items)\n            except:\n                continue\n\n        self.n_items += 1\n        self.n_users += 1\n\n        text_feats = np.load(args.data_path + args.dataset + '/text_feat.npy')\n        self.n_items = text_feats.shape[0]\n\n\n        self.print_statistics()\n\n        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n        self.R_Item_Interacts = sp.dok_matrix((self.n_items, self.n_items), dtype=np.float32)\n\n        self.train_items, self.test_set, self.val_set = {}, {}, {}\n        for uid, train_items in train.items():\n            if len(train_items) == 0:\n                continue\n            uid = int(uid)\n            for idx, i in enumerate(train_items):\n                self.R[uid, i] = 1.\n\n            self.train_items[uid] = train_items\n\n        for uid, test_items in test.items():\n            uid = int(uid)\n            if len(test_items) == 0:\n                continue\n            try:\n                self.test_set[uid] = test_items\n            except:\n                continue\n\n        for uid, val_items in val.items():\n            uid = int(uid)\n            if len(val_items) == 0:\n                continue\n            try:\n                self.val_set[uid] = val_items\n            except:\n                continue            \n\n    def get_adj_mat(self):\n        try:\n            t1 = time()\n            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n            norm_adj_mat = sp.load_npz(self.path + '/s_norm_adj_mat.npz')\n            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')\n            print('already load adj matrix', adj_mat.shape, time() - t1)\n\n        except Exception:\n            adj_mat, norm_adj_mat, mean_adj_mat = self.create_adj_mat()\n            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n            sp.save_npz(self.path + '/s_norm_adj_mat.npz', norm_adj_mat)\n            sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)\n        return adj_mat, norm_adj_mat, mean_adj_mat\n\n    def create_adj_mat(self):\n        t1 = time()\n        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n        adj_mat = adj_mat.tolil()\n        R = self.R.tolil()\n\n        adj_mat[:self.n_users, self.n_users:] = R\n        adj_mat[self.n_users:, :self.n_users] = R.T\n        adj_mat = adj_mat.todok()\n        print('already create adjacency matrix', adj_mat.shape, time() - t1)\n\n        t2 = time()\n\n        def normalized_adj_single(adj):\n            rowsum = np.array(adj.sum(1))\n\n            d_inv = np.power(rowsum, -1).flatten()\n            d_inv[np.isinf(d_inv)] = 0.\n            d_mat_inv = sp.diags(d_inv)\n\n            norm_adj = d_mat_inv.dot(adj)\n            # norm_adj = adj.dot(d_mat_inv)\n            print('generate single-normalized adjacency matrix.')\n            return norm_adj.tocoo()\n\n        def get_D_inv(adj):\n            rowsum = np.array(adj.sum(1))\n\n            d_inv = np.power(rowsum, -1).flatten()\n            d_inv[np.isinf(d_inv)] = 0.\n            d_mat_inv = sp.diags(d_inv)\n            return d_mat_inv\n\n        def check_adj_if_equal(adj):\n            dense_A = np.array(adj.todense())\n            degree = np.sum(dense_A, axis=1, keepdims=False)\n\n            temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n            print('check normalized adjacency matrix whether equal to this laplacian matrix.')\n            return temp\n\n        norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n        mean_adj_mat = normalized_adj_single(adj_mat)\n\n        print('already normalize adjacency matrix', time() - t2)\n        return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n\n\n    def sample(self):\n        if self.batch_size <= self.n_users:\n            users = rd.sample(self.exist_users, self.batch_size)\n        else:\n            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n        # users = self.exist_users[:]\n\n        def sample_pos_items_for_u(u, num):\n            pos_items = self.train_items[u]\n            n_pos_items = len(pos_items)\n            pos_batch = []\n            while True:\n                if len(pos_batch) == num: break\n                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n                pos_i_id = pos_items[pos_id]\n\n                if pos_i_id not in pos_batch:\n                    pos_batch.append(pos_i_id)\n            return pos_batch\n\n        def sample_neg_items_for_u(u, num):\n            neg_items = []\n            while True:\n                if len(neg_items) == num: break\n                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n                    neg_items.append(neg_id)\n            return neg_items\n\n        def sample_neg_items_for_u_from_pools(u, num):\n            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n            return rd.sample(neg_items, num)\n\n        pos_items, neg_items = [], []\n        for u in users:\n            pos_items += sample_pos_items_for_u(u, 1)\n            neg_items += sample_neg_items_for_u(u, 1)\n            # neg_items += sample_neg_items_for_u(u, 3)\n        return users, pos_items, neg_items\n        \n\n    def print_statistics(self):\n        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n        print('n_interactions=%d' % (self.n_train + self.n_test))\n        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))\n\n"}
{"type": "source_file", "path": "LLM_augmentation_construct_prompt/gpt_i_attribute_generate_aug.py", "content": "\nimport threading\nimport openai\nimport time\nimport pandas as pd\nimport pickle\nimport os\nimport numpy as np\nimport torch\n\n# openai.api_key = \"\"\nopenai.api_key = \"\"\n\nimport requests\n\nfile_path = \"\"\n\n\n# # MovieLens\n# def construct_prompting(item_attribute, indices): \n#     # pre string\n#     pre_string = \"You are now a search engines, and required to provide the inquired information of the given movies bellow:\\n\"\n#     # make item list\n#     item_list_string = \"\"\n#     for index in indices:\n#         title = item_attribute['title'][index]\n#         genre = item_attribute['genre'][index]\n#         item_list_string += \"[\"\n#         item_list_string += str(index)\n#         item_list_string += \"] \"\n#         item_list_string += title + \", \"\n#         item_list_string += genre + \"\\n\"\n#     # output format\n#     output_format = \"The inquired information is : director, country, language.\\nAnd please output them in form of: \\ndirector::country::language\\nplease output only the content in the form above, i.e., director::country::language\\n, but no other thing else, no reasoning, no index.\\n\\n\"\n#     # make prompt\n#     prompt = pre_string + item_list_string + output_format\n#     return prompt \n\n# Netflix\ndef construct_prompting(item_attribute, indices): \n    # pre string\n    pre_string = \"You are now a search engines, and required to provide the inquired information of the given movies bellow:\\n\"\n    # make item list\n    item_list_string = \"\"\n    for index in indices:\n        year = item_attribute['year'][index]\n        title = item_attribute['title'][index]\n        item_list_string += \"[\"\n        item_list_string += str(index)\n        item_list_string += \"] \"\n        item_list_string += str(year) + \", \"\n        item_list_string += title + \"\\n\"\n    # output format\n    output_format = \"The inquired information is : director, country, language.\\nAnd please output them in form of: \\ndirector::country::language\\nplease output only the content in the form above, i.e., director::country::language\\n, but no other thing else, no reasoning, no index.\\n\\n\"\n    # make prompt\n    prompt = pre_string + item_list_string + output_format\n    return prompt \n\n\n# def file_reading():\n#     augmented_attribute_dict = pickle.load(open(file_path + 'augmented_attribute_dict','rb')) \n#     return augmented_attribute_dict\n\n# ## baidu attribute generate\n# # error_cnt = 0\n# global error_cnt\n# error_cnt = 0\n# def LLM_request(toy_item_attribute, indices, model_type, augmented_attribute_dict, error_cnt):\n\n#     # try:\n#     #     augmented_attribute_dict = file_reading()\n#     # except pickle.UnpicklingError as e:\n#     #     # Handle the unpickling error\n#     #     # time.sleep(0.001)\n#     #     # augmented_attribute_dict = file_reading()\n#     #     print(\"Error occurred while unpickling:\", e) \n#     #     # return\n#     #     LLM_request(toy_item_attribute, indices, model_type, augmented_attribute_dict, error_cnt)\n\n#     if indices[0] in augmented_attribute_dict:\n#         return 0\n#     else:\n#         try: \n#             print(f\"{indices}\")\n#             prompt = construct_prompting(toy_item_attribute, indices)\n#             url = \"http://llms-se.baidu-int.com:8200/chat/completions\"\n#             headers={\n#                 # \"Content-Type\": \"application/json\",\n#                 # \"Authorization\": \"Bearer your key\"\n#          \n#             }\n#             params={\n#                 \"model\": model_type,\n#                 \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n#                 \"temperature\":1,\n#                 \"max_tokens\": 1000,\n#                 \"stream\": False, \n#                 \"top_p\": 0.1\n#             }\n\n#             response = requests.post(url=url, headers=headers,json=params)\n#             message = response.json()\n\n#             content = message['choices'][0]['message']['content']\n#             print(f\"content: {content}, model_type: {model_type}\")\n\n#             rows = content.strip().split(\"\\n\")  # Split the content into rows\n#             for i,row in enumerate(rows):\n#                 elements = row.split(\"::\")  # Split each row into elements using \"::\" as the delimiter\n#                 director = elements[0]\n#                 country = elements[1]\n#                 language = elements[2]\n#                 augmented_attribute_dict[indices[i]] = {}\n#                 augmented_attribute_dict[indices[i]][0] = director\n#                 augmented_attribute_dict[indices[i]][1] = country\n#                 augmented_attribute_dict[indices[i]][2] = language\n#             # pickle.dump(augmented_sample_dict, open('augmented_sample_dict','wb'))\n#             pickle.dump(augmented_attribute_dict, open(file_path + 'augmented_attribute_dict','wb'))\n#             # time.sleep(5)\n\n#             error_cnt = 0\n#         # except ValueError as e:\n#         except requests.exceptions.RequestException as e:\n#             print(\"An HTTP error occurred:\", str(e))\n#             time.sleep(25)\n#             # print(content)\n#             error_cnt += 1\n#             if error_cnt==5:\n#                 return 1\n#             LLM_request(toy_item_attribute, indices, model_type, augmented_attribute_dict, error_cnt)\n#         except ValueError as ve:\n#             print(\"ValueError error occurred while parsing the response:\", str(ve))\n#             time.sleep(25)\n#             error_cnt += 1\n#             if error_cnt==5:\n#                 return 1\n#             # print(content)\n#             LLM_request(toy_item_attribute, indices, model_type, augmented_attribute_dict, error_cnt)\n#         except KeyError as ke:\n#             print(\"KeyError error occurred while accessing the response:\", str(ke))\n#             time.sleep(25)\n#             error_cnt += 1\n#             if error_cnt==5:\n#                 return 1\n#             # print(content)\n#             LLM_request(toy_item_attribute, indices, model_type, augmented_attribute_dict, error_cnt)\n#         except IndexError as ke:\n#             print(\"IndexError error occurred while accessing the response:\", str(ke))\n#             time.sleep(25)\n#             # error_cnt += 1\n#             # if error_cnt==5:\n#             #     return 1\n#             # # print(content)\n#             LLM_request(toy_item_attribute, indices, model_type, augmented_attribute_dict, error_cnt)\n#             return 1\n#         except Exception as ex:\n#             print(\"An unknown error occurred:\", str(ex))\n#             time.sleep(25)\n#             error_cnt += 1\n#             if error_cnt==5:\n#                 return 1\n#             # print(content)\n#             LLM_request(toy_item_attribute, indices, model_type, augmented_attribute_dict, error_cnt)\n#         return 1\n\n\n### chatgpt attribute generate\ndef LLM_request(toy_item_attribute, indices, model_type, augmented_attribute_dict, error_cnt):\n    if indices[0] in augmented_attribute_dict:\n        return 0\n    else:\n        try: \n            print(f\"{indices}\")\n            prompt = construct_prompting(toy_item_attribute, indices)\n            url = \"https://api.openai.com/v1/completions\"\n            headers={\n                # \"Content-Type\": \"application/json\",\n                \"Authorization\": \"Bearer your key\"\n            }\n\n            params={\n                \"model\": \"text-davinci-003\",\n                \"prompt\": prompt,\n                \"max_tokens\": 1024,\n                \"temperature\": 0.6,\n                \"stream\": False,\n            } \n\n            response = requests.post(url=url, headers=headers,json=params)\n            message = response.json()\n\n            content = message['choices'][0]['text']\n            print(f\"content: {content}, model_type: {model_type}\")\n\n            rows = content.strip().split(\"\\n\")  # Split the content into rows\n            for i,row in enumerate(rows):\n                elements = row.split(\"::\")  # Split each row into elements using \"::\" as the delimiter\n                director = elements[0]\n                country = elements[1]\n                language = elements[2]\n                augmented_attribute_dict[indices[i]] = {}\n                augmented_attribute_dict[indices[i]][0] = director\n                augmented_attribute_dict[indices[i]][1] = country\n                augmented_attribute_dict[indices[i]][2] = language\n            # pickle.dump(augmented_sample_dict, open('augmented_sample_dict','wb'))\n            pickle.dump(augmented_attribute_dict, open(file_path + 'augmented_attribute_dict','wb'))\n        \n        # except ValueError as e:\n        except requests.exceptions.RequestException as e:\n            print(\"An HTTP error occurred:\", str(e))\n            # time.sleep(25)\n            # print(content)\n            error_cnt += 1\n            if error_cnt==5:\n                return 1\n            LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n        except ValueError as ve:\n            print(\"ValueError error occurred while parsing the response:\", str(ve))\n            # time.sleep(25)\n            error_cnt += 1\n            if error_cnt==5:\n                return 1\n            # print(content)\n            LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n        except KeyError as ke:\n            print(\"KeyError error occurred while accessing the response:\", str(ke))\n            # time.sleep(25)\n            error_cnt += 1\n            if error_cnt==5:\n                return 1\n            # print(content)\n            LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n        except IndexError as ke:\n            print(\"IndexError error occurred while accessing the response:\", str(ke))\n            # time.sleep(25)\n            # error_cnt += 1\n            # if error_cnt==5:\n            #     return 1\n            # # print(content)\n            # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict)\n            return 1\n        except Exception as ex:\n            print(\"An unknown error occurred:\", str(ex))\n            # time.sleep(25)\n            error_cnt += 1\n            if error_cnt==5:\n                return 1\n            # print(content)\n            LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n        return 1\n\n\n\n\n\n\n\n### chatgpt attribute embedding\ndef LLM_request(toy_augmented_item_attribute, indices, model_type, augmented_atttribute_embedding_dict, error_cnt):\n    for value in augmented_atttribute_embedding_dict.keys():\n        print(value)\n        if indices[0] in augmented_atttribute_embedding_dict[value]:\n            # return 0\n            continue \n        else:\n            try: \n                print(f\"{indices}\")\n                # prompt = construct_prompting(toy_item_attribute, indices)\n                url = \"https://api.openai.com/v1/embeddings\"\n                headers={\n                    # \"Content-Type\": \"application/json\",\n                    \"Authorization\": \"Bearer your key\"\n                }\n                params={\n                \"model\": \"text-embedding-ada-002\",\n                \"input\": toy_augmented_item_attribute[value][indices].values[0]\n                }\n\n                response = requests.post(url=url, headers=headers,json=params)\n                message = response.json()\n\n                content = message['data'][0]['embedding']\n\n                augmented_atttribute_embedding_dict[value][indices[0]] = content\n                # pickle.dump(augmented_sample_dict, open('augmented_sample_dict','wb'))\n                pickle.dump(augmented_atttribute_embedding_dict, open(file_path + 'augmented_atttribute_embedding_dict','wb'))\n            \n            # except ValueError as e:\n            except requests.exceptions.RequestException as e:\n                print(\"An HTTP error occurred:\", str(e))\n                time.sleep(5)\n                # print(content)\n                LLM_request(toy_augmented_item_attribute, indices, \"text-embedding-ada-002\", augmented_atttribute_embedding_dict, error_cnt)\n            except ValueError as ve:\n                print(\"An error occurred while parsing the response:\", str(ve))\n                time.sleep(5)\n                # print(content)\n                LLM_request(toy_augmented_item_attribute, indices, \"text-embedding-ada-002\", augmented_atttribute_embedding_dict, error_cnt)\n            except KeyError as ke:\n                print(\"An error occurred while accessing the response:\", str(ke))\n                time.sleep(5)\n                # print(content)\n                LLM_request(toy_augmented_item_attribute, indices, \"text-embedding-ada-002\", augmented_atttribute_embedding_dict, error_cnt)\n            except Exception as ex:\n                print(\"An unknown error occurred:\", str(ex))\n                time.sleep(5)\n                # print(content)\n                LLM_request(toy_augmented_item_attribute, indices, \"text-embedding-ada-002\", augmented_atttribute_embedding_dict, error_cnt)\n            # return 1\n\n\n\n\n\n\n\n\n\n\ndef file_reading():\n    augmented_atttribute_embedding_dict = pickle.load(open(file_path + 'augmented_atttribute_embedding_dict','rb')) \n    return augmented_atttribute_embedding_dict\n\n### baidu attribute embedding\ndef LLM_request(toy_augmented_item_attribute, indices, model_type, augmented_atttribute_embedding_dict, error_cnt, key, file_name):\n    for value in augmented_atttribute_embedding_dict.keys():\n        if indices[0] in augmented_atttribute_embedding_dict[value]:\n            # return 0\n            continue\n        else:\n            try: \n                print(f\"{indices}\")\n                print(value)\n\n                ### chatgpt #############################################################################################################################\n                # prompt = construct_prompting(toy_item_attribute, indices)\n                url = \"https://api.openai.com/v1/embeddings\"\n                headers={\n                    # \"Content-Type\": \"application/json\",\n                    \"Authorization\": \"Bearer your key\"\n                }\n                ### chatgpt #############################################################################################################################\n\n\n                params={\n                \"model\": \"text-embedding-ada-002\",\n                \"input\": str(toy_augmented_item_attribute[value][indices].values[0])\n                }\n                response = requests.post(url=url, headers=headers,json=params)\n                message = response.json()\n\n                content = message['data'][0]['embedding']\n\n                augmented_atttribute_embedding_dict[value][indices[0]] = content\n                pickle.dump(augmented_atttribute_embedding_dict, open(file_path + file_name,'wb'))\n            \n            # except ValueError as e:\n            except requests.exceptions.RequestException as e:\n                print(\"An HTTP error occurred:\", str(e))\n                time.sleep(5)\n                # print(content)\n                LLM_request(toy_augmented_item_attribute, indices, \"text-embedding-ada-002\", augmented_atttribute_embedding_dict, error_cnt, key, file_name)\n            except ValueError as ve:\n                print(\"An error occurred while parsing the response:\", str(ve))\n                time.sleep(5)\n                # print(content)\n                LLM_request(toy_augmented_item_attribute, indices, \"text-embedding-ada-002\", augmented_atttribute_embedding_dict, error_cnt, key, file_name)\n            except KeyError as ke:\n                print(\"An error occurred while accessing the response:\", str(ke))\n                time.sleep(5)\n                # print(content)\n                LLM_request(toy_augmented_item_attribute, indices, \"text-embedding-ada-002\", augmented_atttribute_embedding_dict, error_cnt, key, file_name)\n            except Exception as ex:\n                print(\"An unknown error occurred:\", str(ex))\n                time.sleep(5)\n                # print(content)\n                LLM_request(toy_augmented_item_attribute, indices, \"text-embedding-ada-002\", augmented_atttribute_embedding_dict, error_cnt, key, file_name)\n            # return 1\n\n\n\n\n# error_cnt = 0\n# ############################# step 1: built item attribute ##########################################################\n# ### write augmented dict\n# augmented_attribute_dict = {}\n# if os.path.exists(file_path + \"augmented_attribute_dict\"): \n#     print(f\"The file augmented_attribute_dict exists.\")\n#     augmented_attribute_dict = pickle.load(open(file_path + 'augmented_attribute_dict','rb')) \n# else:\n#     print(f\"The file augmented_attribute_dict does not exist.\")\n#     pickle.dump(augmented_attribute_dict, open(file_path + 'augmented_attribute_dict','wb'))\n\n# ### read item attribute file\n# # toy_item_attribute = pd.read_csv(file_path + 'item_attribute.csv', names=['id','title', 'genre'])\n# toy_item_attribute = pd.read_csv(file_path + 'item_attribute.csv', names=['id','year', 'title'])\n\n# for i in range(0, toy_item_attribute.shape[0], 1):\n#     batch_start = i\n#     batch_end = min(i + 1, toy_item_attribute.shape[0])\n#     indices = list(range(batch_start, batch_end))\n#     print(f\"###i###: {i}\")\n#     print(f\"#######: {indices}\")\n#     re = LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-16k\", augmented_attribute_dict, error_cnt)\n#     # if re:\n#     #     time.sleep(0.3)\n\n# # # \n# # for i in range(4189, toy_item_attribute.shape[0], 1):\n# #     batch_start = i\n# #     batch_end = min(i + 1, toy_item_attribute.shape[0])\n# #     indices = list(range(batch_start, batch_end))\n# #     print(f\"###i###: {i}\")\n# #     print(f\"#######: {indices}\")\n# #     re = LLM_request(toy_item_attribute, indices, \"gpt-4\", augmented_attribute_dict, error_cnt)\n# #     # # if re:\n#     # time.sleep(1)\n#     # \"gpt-3.5-turbo\", \"gpt-3.5-turbo-0613\", \"gpt-3.5-turbo-16k\"\n# ############################# step 1: built item attribute ##########################################################\n\n\n\n# ############################# step 2: generate new csv ##########################################################\n# import pandas as pd\n# # raw_item_attribute = pd.read_csv(file_path + 'item_attribute.csv', names=['id','title','genre'])\n# raw_item_attribute = pd.read_csv(file_path + 'item_attribute_filter.csv', names=['id','year','title'])\n# augmented_attribute_dict = pickle.load(open(file_path + 'augmented_attribute_dict','rb'))\n# director_list, country_list, language_list = [], [], []\n# for i in range(len(augmented_attribute_dict)):\n#     director_list.append(augmented_attribute_dict[i][0])\n#     country_list.append(augmented_attribute_dict[i][1])\n#     language_list.append(augmented_attribute_dict[i][2])\n# director_series = pd.Series(director_list)\n# country_series = pd.Series(country_list)\n# language_series = pd.Series(language_list)\n# raw_item_attribute['director'] = director_series\n# raw_item_attribute['country'] = country_series\n# raw_item_attribute['language'] = language_series\n# raw_item_attribute.to_csv(file_path + 'augmented_item_attribute_agg.csv', index=False, header=None)\n# ############################# step 2: generate new csv ##########################################################\n\n\n# ############################# step 3: generate item atttribute embedding ##########################################################\n# ### write augmented dict\n# # emb_dict_name = ['title_embedding_dict', 'genre_embedding_dict', 'director_embedding_dict', 'country_embedding_dict', 'language_embedding_dict']  # TODO: add total\n# emb_dict_name = ['year_embedding_dict', 'title_embedding_dict', 'director_embedding_dict', 'country_embedding_dict', 'language_embedding_dict']  # TODO: add total\n# title_embedding_dict, genre_embedding_dict, director_embedding_dict, country_embedding_dict, language_embedding_dict = {}, {}, {}, {}, {}\n# # augmented_atttribute_embedding_dict = {'title':title_embedding_dict, 'genre':genre_embedding_dict, 'director':director_embedding_dict, 'country':country_embedding_dict, 'language':language_embedding_dict}\n# augmented_atttribute_embedding_dict = {'year':genre_embedding_dict, 'title':title_embedding_dict, 'director':director_embedding_dict, 'country':country_embedding_dict, 'language':language_embedding_dict}\n\n# augmented_atttribute_embedding_dict1 = augmented_atttribute_embedding_dict2 = augmented_atttribute_embedding_dict3 = augmented_atttribute_embedding_dict4 = augmented_atttribute_embedding_dict5 = augmented_atttribute_embedding_dict6 = augmented_atttribute_embedding_dict7 = augmented_atttribute_embedding_dict8 = augmented_atttribute_embedding_dict9 = augmented_atttribute_embedding_dict10 = augmented_atttribute_embedding_dict11 = augmented_atttribute_embedding_dict12 = augmented_atttribute_embedding_dict13 = augmented_atttribute_embedding_dict14 = augmented_atttribute_embedding_dict15 = augmented_atttribute_embedding_dict16 = augmented_atttribute_embedding_dict17 = augmented_atttribute_embedding_dict18 = augmented_atttribute_embedding_dict19 = augmented_atttribute_embedding_dict20 = augmented_atttribute_embedding_dict21 = augmented_atttribute_embedding_dict22 = augmented_atttribute_embedding_dict23 = augmented_atttribute_embedding_dict24 = augmented_atttribute_embedding_dict25 = augmented_atttribute_embedding_dict26 = augmented_atttribute_embedding_dict27 = augmented_atttribute_embedding_dict28 = augmented_atttribute_embedding_dict29 = augmented_atttribute_embedding_dict30 = augmented_atttribute_embedding_dict31 = augmented_atttribute_embedding_dict32 = augmented_atttribute_embedding_dict33 = augmented_atttribute_embedding_dict34 = augmented_atttribute_embedding_dict35 = augmented_atttribute_embedding_dict\n# # if os.path.exists(file_path + \"augmented_atttribute_embedding_dict\"): \n# #     print(f\"The file augmented_atttribute_embedding_dict exists.\")\n# #     augmented_atttribute_embedding_dict = pickle.load(open(file_path + 'augmented_atttribute_embedding_dict','rb')) \n# # else:\n# #     print(f\"The file augmented_atttribute_embedding_dict does not exist.\")\n# #     pickle.dump(augmented_atttribute_embedding_dict, open(file_path + 'augmented_atttribute_embedding_dict','wb'))\n\n# file_name = \"augmented_atttribute_embedding_dict12\"\n# if os.path.exists(file_path + file_name): \n#     print(f\"The file augmented_atttribute_embedding_dict exists.\")\n#     augmented_atttribute_embedding_dict = pickle.load(open(file_path + file_name,'rb')) \n# else:\n#     print(f\"The file augmented_atttribute_embedding_dict does not exist.\")\n#     pickle.dump(augmented_atttribute_embedding_dict, open(file_path + file_name,'wb'))\n\n\n\n# error_cnt=0\n# ### read augmented item attribute file\n# # toy_augmented_item_attribute = pd.read_csv(file_path + 'augmented_item_attribute_agg.csv', names=['id','title', 'genre', 'director', 'country', 'language'])\n# toy_augmented_item_attribute = pd.read_csv(file_path + 'augmented_item_attribute_agg.csv', names=['id', 'year','title', 'director', 'country', 'language'])\n\n\n# g_key = \"\"\n\n# for i in range(5500, 6000, 1):\n#     batch_start = i\n#     batch_end = min(i + 1, toy_augmented_item_attribute.shape[0])\n#     indices = list(range(batch_start, batch_end))\n#     # print(f\"###i###: {i}\")\n#     print(f\"#######: {indices}\")\n#     LLM_request(toy_augmented_item_attribute, indices, \"text-embedding-ada-002\", augmented_atttribute_embedding_dict, error_cnt, g_key, file_name)\n\n\n# # ### get separate embedding matrix\n# # import pandas as pd\n# # augmented_atttribute_embedding_dict = pickle.load(open(file_path + 'augmented_atttribute_embedding_dict','rb')) \n# # for value in augmented_atttribute_embedding_dict.keys():\n# #     augmented_atttribute_embedding_dict[value] = np.array(augmented_atttribute_embedding_dict[value])\n\n\n# # raw_item_attribute = pd.read_csv(file_path + 'toy_item_attribute.csv', names=['id','title','genre'])\n\n\n# # augmented_attribute_dict = pickle.load(open('augmented_attribute_dict','rb'))\n# # director_list, country_list, language_list = [], [], []\n# # for i in range(len(augmented_attribute_dict)):\n# #     director_list.append(augmented_attribute_dict[i][0])\n# #     country_list.append(augmented_attribute_dict[i][1])\n# #     language_list.append(augmented_attribute_dict[i][2])\n# # director_series = pd.Series(director_list)\n# # country_series = pd.Series(country_list)\n# # language_series = pd.Series(language_list)\n# # raw_item_attribute['director'] = director_series\n# # raw_item_attribute['country'] = country_series\n# # raw_item_attribute['language'] = language_series\n# # raw_item_attribute.to_csv(file_path + 'toy_augmented_item_attribute.csv', index=False, header=None)\n# ############################# step 3: generate item atttribute embedding ##########################################################\n\n\n\n# ############################# step 4: get separate embedding matrix ##########################################################\n# augmented_total_embed_dict = {'title':[] , 'genre':[], 'director':[], 'country':[], 'language':[]}   \n# augmented_atttribute_embedding_dict = pickle.load(open('augmented_atttribute_embedding_dict','rb'))\n# for value in augmented_atttribute_embedding_dict.keys():\n#     for i in range(len(augmented_atttribute_embedding_dict[value])):\n#         augmented_total_embed_dict[value].append(augmented_atttribute_embedding_dict[value][i])   \n#     augmented_total_embed_dict[value] = np.array(augmented_total_embed_dict[value])    \n# pickle.dump(augmented_total_embed_dict, open(file_path + 'augmented_total_embed_dict','wb'))\n# ############################# step 4: get separate embedding matrix ##########################################################\n\n\n\n# ############################# step 5: i-i relation struction:  (constructured when start) ##########################################################\n# # augmented_total_embed_dict = pickle.load(open(file_path + 'augmented_total_embed_dict','rb'))\n# # for value in augmented_atttribute_embedding_dict.keys():\n# #     augmented_atttribute_embedding_dict[value] = torch.tensor(augmented_atttribute_embedding_dict[value])\n# pass\n# ############################# step 5: i-i relation struction:  (constructured when start) ##########################################################\n\n\n\n# # ############################# step 6: agg file ##########################################################\n# dict_list = []\n# for i in range(1,36):\n#     tmp_dict = pickle.load(open('augmented_atttribute_embedding_dict'+str(i),'rb'))\n#     dict_list.append(tmp_dict)\n# total_dict = {'year':{}, 'title':{}, 'director':{}, 'country':{}, 'language':{}}\n# for value in dict_list:\n#     for key in total_dict.keys():\n#         total_dict[key].update(value[key])\n# # ############################# step 6: agg file ##########################################################\n\n\n\n\n"}
{"type": "source_file", "path": "LLM_augmentation_construct_prompt/gpt_ui_aug.py", "content": "\nimport threading\nimport openai\nimport time\nimport pandas as pd\nimport csv\nimport requests\nimport concurrent.futures\nimport pickle\nimport torch\nimport os\nimport threading\nimport time\nimport tqdm\nimport requests\n\nfile_path = \"\"\nmax_threads = 5\ncnt = 0 \n\n# MovieLens\ndef construct_prompting(item_attribute, item_list, candidate_list): \n    # make history string\n    history_string = \"User history:\\n\" \n    for index in item_list:\n        title = item_attribute['title'][index]\n        genre = item_attribute['genre'][index]\n        history_string += \"[\"\n        history_string += str(index)\n        history_string += \"] \"\n        history_string += title + \", \"\n        history_string += genre + \"\\n\"\n    # make candidates\n    candidate_string = \"Candidates:\\n\" \n    for index in candidate_list:\n        title = item_attribute['title'][index.item()]\n        genre = item_attribute['genre'][index.item()]\n        candidate_string += \"[\"\n        candidate_string += str(index.item())\n        candidate_string += \"] \"\n        candidate_string += title + \", \"\n        candidate_string += genre + \"\\n\"\n    # output format\n    output_format = \"Please output the index of user\\'s favorite and least favorite movie only from candidate, but not user history. Please get the index from candidate, at the beginning of each line.\\nOutput format:\\nTwo numbers separated by '::'. Nothing else.Plese just give the index of candicates, remove [] (just output the digital value), please do not output other thing else, do not give reasoning.\\n\\n\"\n    # make prompt\n    prompt = \"You are a movie recommendation system and required to recommend user with movies based on user history that each movie with title(same topic/doctor), year(similar years), genre(similar genre).\\n\"\n    prompt += history_string\n    prompt += candidate_string\n    prompt += output_format\n    return prompt\n\n# # Netflix\n# def construct_prompting(item_attribute, item_list, candidate_list): \n#     # make history string\n#     history_string = \"User history:\\n\" \n#     for index in item_list:\n#         year = item_attribute['year'][index]\n#         title = item_attribute['title'][index]\n#         history_string += \"[\"\n#         history_string += str(index)\n#         history_string += \"] \"\n#         history_string += str(year) + \", \"\n#         history_string += title + \"\\n\"\n#     # make candidates\n#     candidate_string = \"Candidates:\\n\" \n#     for index in candidate_list:\n#         year = item_attribute['year'][index.item()]\n#         title = item_attribute['title'][index.item()]\n#         candidate_string += \"[\"\n#         candidate_string += str(index.item())\n#         candidate_string += \"] \"\n#         candidate_string += str(year) + \", \"\n#         candidate_string += title + \"\\n\"\n#     # output format\n#     output_format = \"Please output the index of user\\'s favorite and least favorite movie only from candidate, but not user history. Please get the index from candidate, at the beginning of each line.\\nOutput format:\\nTwo numbers separated by '::'. Nothing else.Plese just give the index of candicates, remove [] (just output the digital value), please do not output other thing else, do not give reasoning.\\n\\n\"\n#     # make prompt\n#     # prompt = \"You are a movie recommendation system and required to recommend user with movies based on user history that each movie with title(same topic/doctor), year(similar years), genre(similar genre).\\n\"\n#     prompt = \"\"\n#     prompt += history_string\n#     prompt += candidate_string\n#     prompt += output_format\n#     return prompt\n\n### read candidate \ncandidate_indices = pickle.load(open(file_path + 'candidate_indices','rb'))\ncandidate_indices_dict = {}\nfor index in range(candidate_indices.shape[0]):\n    candidate_indices_dict[index] = candidate_indices[index]\n### read adjacency_list\nadjacency_list_dict = {}\ntrain_mat = pickle.load(open(file_path + 'train_mat','rb'))\nfor index in range(train_mat.shape[0]):\n    data_x, data_y = train_mat[index].nonzero()\n    adjacency_list_dict[index] = data_y\n### read item_attribute\ntoy_item_attribute = pd.read_csv(file_path + 'item_attribute.csv', names=['id','title', 'genre'])\n### write augmented dict\naugmented_sample_dict = {}\nif os.path.exists(file_path + \"augmented_sample_dict\"): \n    print(f\"The file augmented_sample_dict exists.\")\n    augmented_sample_dict = pickle.load(open(file_path + 'augmented_sample_dict','rb')) \nelse:\n    print(f\"The file augmented_sample_dict does not exist.\")\n    pickle.dump(augmented_sample_dict, open(file_path + 'augmented_sample_dict','wb')) \n\ndef file_reading():\n    augmented_attribute_dict = pickle.load(open(file_path + 'augmented_sample_dict','rb')) \n    return augmented_attribute_dict\n\n# baidu\ndef LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, model_type, augmented_sample_dict):\n\n    try:\n        augmented_sample_dict = file_reading()\n    except pickle.UnpicklingError as e:\n        print(\"Error occurred while unpickling:\", e) \n        LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, model_type, augmented_sample_dict)\n    if index in augmented_sample_dict:\n        return 0\n    else:\n        try: \n            print(f\"{index}\")\n            prompt = construct_prompting(toy_item_attribute, adjacency_list_dict[index], candidate_indices_dict[index])\n            url = \"http://llms-se.baidu-int.com:8200/chat/completions\"\n            headers={\n                # \"Content-Type\": \"application/json\",\n                \"Authorization\": \"Bearer your key\"\n            \n            }\n            params={\n                \"model\": model_type,\n                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n                \"temperature\":0.6,\n                \"max_tokens\": 1000,\n                \"stream\": False, \n                \"top_p\": 0.1\n            }\n\n            response = requests.post(url=url, headers=headers,json=params)\n            message = response.json()\n\n            content = message['choices'][0]['message']['content']\n            print(f\"content: {content}, model_type: {model_type}\")\n            samples = content.split(\"::\")\n            pos_sample = int(samples[0])\n            neg_sample = int(samples[1])\n            augmented_sample_dict[index] = {}\n            augmented_sample_dict[index][0] = pos_sample\n            augmented_sample_dict[index][1] = neg_sample\n            pickle.dump(augmented_sample_dict, open(file_path + 'augmented_sample_dict','wb'))\n\n        # except ValueError as e:\n        except requests.exceptions.RequestException as e:\n            print(\"An HTTP error occurred:\", str(e))\n            time.sleep(10)\n        except ValueError as ve:\n            print(\"An error occurred while parsing the response:\", str(ve))\n            time.sleep(10)\n            LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, \"gpt-3.5-turbo-0613\", augmented_sample_dict)\n        except KeyError as ke:\n            print(\"An error occurred while accessing the response:\", str(ke))\n            time.sleep(10)\n            LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, \"gpt-3.5-turbo-0613\", augmented_sample_dict)\n        except Exception as ex:\n            print(\"An unknown error occurred:\", str(ex))\n            time.sleep(10)\n        \n        return 1\n\n\n\n\n\n# # chatgpt\n# def LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, model_type, augmented_sample_dict):\n\n#     if index in augmented_sample_dict:\n#         print(f\"g:{index}\")\n#         return 0\n#     else:\n#         try: \n#             print(f\"{index}\")\n#             prompt = construct_prompting(toy_item_attribute, adjacency_list_dict[index], candidate_indices_dict[index])\n#             # url = \"http://llms-se.baidu-int.com:8200/chat/completions\"\n#             # url = \"https://api.openai.com/v1/completions\"\n#             url = \"https://api.openai.com/v1/chat/completions\"\n\n#             headers={\n#                 # \"Content-Type\": \"application/json\",\n#                 # \"Authorization\": \"Bearer your key\"\n#                \n#             }\n#             # params={\n#             #     \"model\": model_type,\n#             #     \"prompt\": prompt,\n#             #     \"max_tokens\": 1024,\n#             #     \"temperature\": 0.6,\n#             #     \"stream\": False,\n#             # } \n\n#             params = {\n#                 \"model\": \"gpt-3.5-turbo\",\n#                 \"messages\": [{\"role\": \"system\", \"content\": \"You are a movie recommendation system and required to recommend user with movies based on user history that each movie with title(same topic/doctor), year(similar years), genre(similar genre).\\n\"}, {\"role\": \"user\", \"content\": prompt}]\n#             }\n\n#             response = requests.post(url=url, headers=headers,json=params)\n#             message = response.json()\n\n#             content = message['choices'][0]['message']['content']\n#             # content = message['choices'][0]['text']\n#             print(f\"content: {content}, model_type: {model_type}\")\n#             samples = content.split(\"::\")\n#             pos_sample = int(samples[0])\n#             neg_sample = int(samples[1])\n#             augmented_sample_dict[index] = {}\n#             augmented_sample_dict[index][0] = pos_sample\n#             augmented_sample_dict[index][1] = neg_sample\n#             # pickle.dump(augmented_sample_dict, open('augmented_sample_dict','wb'))\n#             # pickle.dump(augmented_sample_dict, open('/Users/weiwei/Documents/Datasets/ml-10m/ml-10M100K/preprocessed_raw_MovieLens/toy_MovieLens1000/augmented_sample_dict','wb'))\n#             pickle.dump(augmented_sample_dict, open(file_path + 'augmented_sample_dict','wb'))\n\n#         # # except ValueError as e:\n#         # except requests.exceptions.RequestException as e:\n#         #     print(\"An HTTP error occurred:\", str(e))\n#         #     # time.sleep(40)\n#         # except ValueError as ve:\n#         #     print(\"An error occurred while parsing the response:\", str(ve))\n#         #     # time.sleep(40)\n#         #     LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, \"gpt-3.5-turbo-0613\", augmented_sample_dict)\n#         # except KeyError as ke:\n#         #     print(\"An error occurred while accessing the response:\", str(ke))\n#         #     # time.sleep(40)\n#         #     LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, \"gpt-3.5-turbo-0613\", augmented_sample_dict)\n#         # except Exception as ex:\n#         #     print(\"An unknown error occurred:\", str(ex))\n#         #     # time.sleep(40)\n        \n#         # return 1\n\n#         # except ValueError as e:\n#         except requests.exceptions.RequestException as e:\n#             print(\"An HTTP error occurred:\", str(e))\n#             time.sleep(8)\n#             # print(content)\n#             # error_cnt += 1\n#             # if error_cnt==5:\n#             #     return 1\n#             # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n#             LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, model_type, augmented_sample_dict)\n#         except ValueError as ve:\n#             print(\"ValueError error occurred while parsing the response:\", str(ve))\n#             time.sleep(10)\n#             # error_cnt += 1\n#             # if error_cnt==5:\n#             #     return 1\n#             # print(content)\n#             # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n#             LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, model_type, augmented_sample_dict)\n#         except KeyError as ke:\n#             print(\"KeyError error occurred while accessing the response:\", str(ke))\n#             time.sleep(10)\n#             # error_cnt += 1\n#             # if error_cnt==5:\n#             #     return 1\n#             # print(content)\n#             # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n#             LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, model_type, augmented_sample_dict)\n#         except IndexError as ke:\n#             print(\"IndexError error occurred while accessing the response:\", str(ke))\n#             time.sleep(10)\n#             # error_cnt += 1\n#             # if error_cnt==5:\n#             #     return 1\n#             # # print(content)\n#             # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict)\n#             # return 1\n#             LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, model_type, augmented_sample_dict)\n#         except EOFError as ke:\n#             print(\"EOFError: : Ran out of input error occurred while accessing the response:\", str(ke))\n#             time.sleep(10)\n#             # error_cnt += 1\n#             # if error_cnt==5:\n#             #     return 1\n#             # print(content)\n#             # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n#             LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, model_type, augmented_sample_dict)\n#         except Exception as ex:\n#             print(\"An unknown error occurred:\", str(ex))\n#             time.sleep(10)\n#             # error_cnt += 1\n#             # if error_cnt==5:\n#             #     return 1\n#             # print(content)\n#             # LLM_request(toy_item_attribute, indices, \"gpt-3.5-turbo-0613\", augmented_attribute_dict, error_cnt)\n#             LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, model_type, augmented_sample_dict)\n#         return 1\n\n\nfor index in range(0, len(adjacency_list_dict)):\n    # # make prompting\n    re = LLM_request(toy_item_attribute, adjacency_list_dict, candidate_indices_dict, index, \"gpt-3.5-turbo\", augmented_sample_dict)\n\n\n\n\n"}
{"type": "source_file", "path": "MMSSL/utility/load_data.py", "content": "\nimport numpy as np\nimport random as rd\nimport scipy.sparse as sp\nfrom time import time\nimport json\nfrom utility.parser import parse_args\nargs = parse_args()\n\nclass Data(object):\n    def __init__(self, path, batch_size):\n        self.path = path #+ '/%d-core' % args.core\n        self.batch_size = batch_size\n\n        train_file = path + '/train.json'#+ '/%d-core/train.json' % (args.core)\n        val_file = path + '/val.json' #+ '/%d-core/val.json' % (args.core)\n        test_file = path + '/test.json' #+ '/%d-core/test.json'  % (args.core)\n\n        #get number of users and items\n        self.n_users, self.n_items = 0, 0\n        self.n_train, self.n_test = 0, 0\n        self.neg_pools = {}\n\n        self.exist_users = []\n\n        train = json.load(open(train_file))\n        test = json.load(open(test_file))\n        val = json.load(open(val_file))\n        for uid, items in train.items():\n            if len(items) == 0:\n                continue\n            uid = int(uid)\n            self.exist_users.append(uid)\n            self.n_items = max(self.n_items, max(items))\n            self.n_users = max(self.n_users, uid)\n            self.n_train += len(items)\n\n        for uid, items in test.items():\n            uid = int(uid)\n            try:\n                self.n_items = max(self.n_items, max(items))\n                self.n_test += len(items)\n            except:\n                continue\n\n        for uid, items in val.items():\n            uid = int(uid)\n            try:\n                self.n_items = max(self.n_items, max(items))\n                self.n_val += len(items)\n            except:\n                continue\n\n        self.n_items += 1\n        self.n_users += 1\n\n        self.print_statistics()\n\n        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n        self.R_Item_Interacts = sp.dok_matrix((self.n_items, self.n_items), dtype=np.float32)\n\n        self.train_items, self.test_set, self.val_set = {}, {}, {}\n        for uid, train_items in train.items():\n            if len(train_items) == 0:\n                continue\n            uid = int(uid)\n            for idx, i in enumerate(train_items):\n                self.R[uid, i] = 1.\n\n            self.train_items[uid] = train_items\n\n        for uid, test_items in test.items():\n            uid = int(uid)\n            if len(test_items) == 0:\n                continue\n            try:\n                self.test_set[uid] = test_items\n            except:\n                continue\n\n        for uid, val_items in val.items():\n            uid = int(uid)\n            if len(val_items) == 0:\n                continue\n            try:\n                self.val_set[uid] = val_items\n            except:\n                continue            \n\n    def get_adj_mat(self):\n        try:\n            t1 = time()\n            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n            norm_adj_mat = sp.load_npz(self.path + '/s_norm_adj_mat.npz')\n            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')\n            print('already load adj matrix', adj_mat.shape, time() - t1)\n\n        except Exception:\n            adj_mat, norm_adj_mat, mean_adj_mat = self.create_adj_mat()\n            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n            sp.save_npz(self.path + '/s_norm_adj_mat.npz', norm_adj_mat)\n            sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)\n        return adj_mat, norm_adj_mat, mean_adj_mat\n\n    def create_adj_mat(self):\n        t1 = time()\n        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n        adj_mat = adj_mat.tolil()\n        R = self.R.tolil()\n\n        adj_mat[:self.n_users, self.n_users:] = R\n        adj_mat[self.n_users:, :self.n_users] = R.T\n        adj_mat = adj_mat.todok()\n        print('already create adjacency matrix', adj_mat.shape, time() - t1)\n\n        t2 = time()\n\n        def normalized_adj_single(adj):\n            rowsum = np.array(adj.sum(1))\n\n            d_inv = np.power(rowsum, -1).flatten()\n            d_inv[np.isinf(d_inv)] = 0.\n            d_mat_inv = sp.diags(d_inv)\n\n            norm_adj = d_mat_inv.dot(adj)\n            # norm_adj = adj.dot(d_mat_inv)\n            print('generate single-normalized adjacency matrix.')\n            return norm_adj.tocoo()\n\n        def get_D_inv(adj):\n            rowsum = np.array(adj.sum(1))\n\n            d_inv = np.power(rowsum, -1).flatten()\n            d_inv[np.isinf(d_inv)] = 0.\n            d_mat_inv = sp.diags(d_inv)\n            return d_mat_inv\n\n        def check_adj_if_equal(adj):\n            dense_A = np.array(adj.todense())\n            degree = np.sum(dense_A, axis=1, keepdims=False)\n\n            temp = np.dot(np.diag(np.power(degree, -1)), dense_A)\n            print('check normalized adjacency matrix whether equal to this laplacian matrix.')\n            return temp\n\n        norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))\n        mean_adj_mat = normalized_adj_single(adj_mat)\n\n        print('already normalize adjacency matrix', time() - t2)\n        return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()\n\n\n    def sample(self):\n        if self.batch_size <= self.n_users:\n            users = rd.sample(self.exist_users, self.batch_size)\n        else:\n            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n        # users = self.exist_users[:]\n\n        def sample_pos_items_for_u(u, num):\n            pos_items = self.train_items[u]\n            n_pos_items = len(pos_items)\n            pos_batch = []\n            while True:\n                if len(pos_batch) == num: break\n                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n                pos_i_id = pos_items[pos_id]\n\n                if pos_i_id not in pos_batch:\n                    pos_batch.append(pos_i_id)\n            return pos_batch\n\n        def sample_neg_items_for_u(u, num):\n            neg_items = []\n            while True:\n                if len(neg_items) == num: break\n                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n                    neg_items.append(neg_id)\n            return neg_items\n\n        def sample_neg_items_for_u_from_pools(u, num):\n            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n            return rd.sample(neg_items, num)\n\n        pos_items, neg_items = [], []\n        for u in users:\n            pos_items += sample_pos_items_for_u(u, 1)\n            neg_items += sample_neg_items_for_u(u, 1)\n            # neg_items += sample_neg_items_for_u(u, 3)\n        return users, pos_items, neg_items\n        \n\n    def print_statistics(self):\n        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n        print('n_interactions=%d' % (self.n_train + self.n_test))\n        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))\n\n"}
{"type": "source_file", "path": "utility/logging.py", "content": "import os\nfrom datetime import datetime\n\nclass Logger():\n    def __init__(self, filename, is_debug, path='./logs/'):\n        self.filename = filename\n        self.path = path\n        self.log_ = not is_debug\n    def logging(self, s):\n        s = str(s)\n        print(datetime.now().strftime('%Y-%m-%d %H:%M: '), s)\n        if self.log_:\n            with open(os.path.join(os.path.join(self.path, self.filename)), 'a+') as f_log:\n                f_log.write(str(datetime.now().strftime('%Y-%m-%d %H:%M:  ')) + s + '\\n')\n\n\n\n"}
{"type": "source_file", "path": "Models.py", "content": "import os\nimport numpy as np\nfrom time import time\nimport pickle\nimport scipy.sparse as sp\nfrom scipy.sparse import csr_matrix\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\n\nfrom utility.parser import parse_args\nfrom utility.norm import build_sim, build_knn_normalized_graph\nargs = parse_args()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass MM_Model(nn.Module):\n    def __init__(self, n_users, n_items, embedding_dim, weight_size, dropout_list, image_feats, text_feats, user_init_embedding, item_attribute_dict):\n\n        super().__init__()\n        self.n_users = n_users\n        self.n_items = n_items\n        self.embedding_dim = embedding_dim\n        self.weight_size = weight_size\n        self.n_ui_layers = len(self.weight_size)\n        self.weight_size = [self.embedding_dim] + self.weight_size\n\n        self.image_trans = nn.Linear(image_feats.shape[1], args.embed_size)\n        self.text_trans = nn.Linear(text_feats.shape[1], args.embed_size)\n        self.user_trans = nn.Linear(user_init_embedding.shape[1], args.embed_size)  \n        self.item_trans = nn.Linear(item_attribute_dict['title'].shape[1], args.embed_size)  \n        nn.init.xavier_uniform_(self.image_trans.weight)\n        nn.init.xavier_uniform_(self.text_trans.weight)   \n        nn.init.xavier_uniform_(self.user_trans.weight)\n        nn.init.xavier_uniform_(self.item_trans.weight)\n\n        self.user_id_embedding = nn.Embedding(n_users, self.embedding_dim)\n        self.item_id_embedding = nn.Embedding(n_items, self.embedding_dim)\n        nn.init.xavier_uniform_(self.user_id_embedding.weight)\n        nn.init.xavier_uniform_(self.item_id_embedding.weight)\n        self.image_feats = torch.tensor(image_feats).float().cuda()\n        self.text_feats = torch.tensor(text_feats).float().cuda()\n        self.user_feats = torch.tensor(user_init_embedding).float().cuda()\n        self.item_feats = {}\n        for key in item_attribute_dict.keys():                                   \n            self.item_feats[key] = torch.tensor(item_attribute_dict[key]).float().cuda() \n\n        self.softmax = nn.Softmax(dim=-1)\n        self.act = nn.Sigmoid()  \n        self.sigmoid = nn.Sigmoid()\n        self.dropout = nn.Dropout(p=args.drop_rate)\n        self.batch_norm = nn.BatchNorm1d(args.embed_size)\n        self.tau = 0.5\n\n    def mm(self, x, y):\n        if args.sparse:\n            return torch.sparse.mm(x, y)\n        else:\n            return torch.mm(x, y)\n    def sim(self, z1, z2):\n        z1 = F.normalize(z1)\n        z2 = F.normalize(z2)\n        return torch.mm(z1, z2.t())\n\n    def batched_contrastive_loss(self, z1, z2, batch_size=4096):\n        device = z1.device\n        num_nodes = z1.size(0)\n        num_batches = (num_nodes - 1) // batch_size + 1\n        f = lambda x: torch.exp(x / self.tau)\n        indices = torch.arange(0, num_nodes).to(device)\n        losses = []\n\n        for i in range(num_batches):\n            mask = indices[i * batch_size:(i + 1) * batch_size]\n            refl_sim = f(self.sim(z1[mask], z1))  \n            between_sim = f(self.sim(z1[mask], z2))  \n\n            losses.append(-torch.log(\n                between_sim[:, i * batch_size:(i + 1) * batch_size].diag()\n                / (refl_sim.sum(1) + between_sim.sum(1)\n                   - refl_sim[:, i * batch_size:(i + 1) * batch_size].diag())))\n                   \n        loss_vec = torch.cat(losses)\n        return loss_vec.mean()\n\n    def csr_norm(self, csr_mat, mean_flag=False):\n        rowsum = np.array(csr_mat.sum(1))\n        rowsum = np.power(rowsum+1e-8, -0.5).flatten()\n        rowsum[np.isinf(rowsum)] = 0.\n        rowsum_diag = sp.diags(rowsum)\n\n        colsum = np.array(csr_mat.sum(0))\n        colsum = np.power(colsum+1e-8, -0.5).flatten()\n        colsum[np.isinf(colsum)] = 0.\n        colsum_diag = sp.diags(colsum)\n\n        if mean_flag == False:\n            return rowsum_diag*csr_mat*colsum_diag\n        else:\n            return rowsum_diag*csr_mat\n\n    def matrix_to_tensor(self, cur_matrix):\n        if type(cur_matrix) != sp.coo_matrix:\n            cur_matrix = cur_matrix.tocoo()  #\n        indices = torch.from_numpy(np.vstack((cur_matrix.row, cur_matrix.col)).astype(np.int64))  #\n        values = torch.from_numpy(cur_matrix.data)  #\n        shape = torch.Size(cur_matrix.shape)\n\n        return torch.sparse.FloatTensor(indices, values, shape).to(torch.float32).cuda()  #\n\n    def para_dict_to_tenser(self, para_dict):  \n        \"\"\"\n        :param para_dict: nn.ParameterDict()\n        :return: tensor\n        \"\"\"\n        tensors = []\n\n        for beh in para_dict.keys():\n            tensors.append(para_dict[beh])\n        tensors = torch.stack(tensors, dim=0)\n\n        return tensors\n\n\n    def forward(self, ui_graph, iu_graph, image_ui_graph, image_iu_graph, text_ui_graph, text_iu_graph):\n\n\n        # feature mask \n        i_mask_nodes, u_mask_nodes = None, None\n        if args.mask:\n            i_perm = torch.randperm(self.n_items)\n            i_num_mask_nodes = int(args.mask_rate * self.n_items)\n            i_mask_nodes = i_perm[: i_num_mask_nodes]\n            for key in self.item_feats.keys():\n                self.item_feats[key][i_mask_nodes] = self.item_feats[key].mean(0)\n\n        u_perm = torch.randperm(self.n_users)\n        u_num_mask_nodes = int(args.mask_rate * self.n_users)\n        u_mask_nodes = u_perm[: u_num_mask_nodes]\n        self.user_feats[u_mask_nodes] = self.user_feats.mean(0) \n\n\n        image_feats = self.dropout(self.image_trans(self.image_feats))\n        text_feats = self.dropout(self.text_trans(self.text_feats))\n        user_feats = self.dropout(self.user_trans(self.user_feats.to(torch.float32)))\n        item_feats = {}\n        for key in self.item_feats.keys():\n            item_feats[key] = self.dropout(self.item_trans(self.item_feats[key]))\n\n        for i in range(args.layers):\n            image_user_feats = self.mm(ui_graph, image_feats)\n            image_item_feats = self.mm(iu_graph, image_user_feats)\n\n            text_user_feats = self.mm(ui_graph, text_feats)\n            text_item_feats = self.mm(iu_graph, text_user_feats)\n\n        # aug item attribute\n        user_feat_from_item = {}\n        for key in self.item_feats.keys():\n            user_feat_from_item[key] = self.mm(ui_graph, item_feats[key])\n            item_feats[key] = self.mm(iu_graph, user_feat_from_item[key])\n\n        # aug user profile\n        item_prof_feat = self.mm(iu_graph, user_feats)\n        user_prof_feat = self.mm(ui_graph,  item_prof_feat)\n\n        u_g_embeddings = self.user_id_embedding.weight\n        i_g_embeddings = self.item_id_embedding.weight             \n\n        user_emb_list = [u_g_embeddings]\n        item_emb_list = [i_g_embeddings]\n        for i in range(self.n_ui_layers):    \n            if i == (self.n_ui_layers-1):\n                u_g_embeddings = self.softmax( torch.mm(ui_graph, i_g_embeddings) ) \n                i_g_embeddings = self.softmax( torch.mm(iu_graph, u_g_embeddings) )\n            else:\n                u_g_embeddings = torch.mm(ui_graph, i_g_embeddings) \n                i_g_embeddings = torch.mm(iu_graph, u_g_embeddings) \n\n            user_emb_list.append(u_g_embeddings)\n            item_emb_list.append(i_g_embeddings)\n\n        u_g_embeddings = torch.mean(torch.stack(user_emb_list), dim=0)\n        i_g_embeddings = torch.mean(torch.stack(item_emb_list), dim=0) \n\n        u_g_embeddings = u_g_embeddings + args.model_cat_rate*F.normalize(image_user_feats, p=2, dim=1) + args.model_cat_rate*F.normalize(text_user_feats, p=2, dim=1)\n        i_g_embeddings = i_g_embeddings + args.model_cat_rate*F.normalize(image_item_feats, p=2, dim=1) + args.model_cat_rate*F.normalize(text_item_feats, p=2, dim=1)\n        # profile\n        u_g_embeddings += args.user_cat_rate*F.normalize(user_prof_feat, p=2, dim=1)\n        i_g_embeddings += args.user_cat_rate*F.normalize(item_prof_feat, p=2, dim=1)\n\n        # attribute \n        for key in self.item_feats.keys():\n            u_g_embeddings += args.item_cat_rate*F.normalize(user_feat_from_item[key], p=2, dim=1)\n            i_g_embeddings += args.item_cat_rate*F.normalize(item_feats[key], p=2, dim=1) \n\n        return u_g_embeddings, i_g_embeddings, image_item_feats, text_item_feats, image_user_feats, text_user_feats, user_feats, item_feats, user_prof_feat, item_prof_feat, user_feat_from_item, item_feats, i_mask_nodes, u_mask_nodes\n\n\n\nclass Decoder(nn.Module):\n    def __init__(self, feat_size):\n        super(Decoder, self).__init__()\n        self.feat_size=feat_size\n\n        self.u_net = nn.Sequential(\n            nn.Linear(args.embed_size, int(self.feat_size)),\n            nn.LeakyReLU(True),\n        )\n\n        self.i_net = nn.Sequential(\n            nn.Linear(args.embed_size, int(self.feat_size)),\n            nn.LeakyReLU(True),\n        )\n\n    def forward(self, u, i):\n        u_output = self.u_net(u.float())  \n        tensor_list = []\n        for index,value in enumerate(i.keys()):  \n            tensor_list.append(i[value]) \n        i_tensor = torch.stack(tensor_list)\n        i_output = self.i_net(i_tensor.float())  \n        return u_output, i_output  \n\n\n"}
{"type": "source_file", "path": "MMSSL/Models.py", "content": "import os\nimport numpy as np\nfrom time import time\nimport pickle\nimport scipy.sparse as sp\nfrom scipy.sparse import csr_matrix\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\n\nfrom utility.parser import parse_args\nfrom utility.norm import build_sim, build_knn_normalized_graph\nargs = parse_args()\n\nclass G_Model(nn.Module):\n    def __init__(self, n_users, n_items, embedding_dim, weight_size, dropout_list, image_feats, text_feats):\n\n        super().__init__()\n        self.n_users = n_users\n        self.n_items = n_items\n        self.embedding_dim = embedding_dim\n        self.weight_size = weight_size\n        self.n_ui_layers = len(self.weight_size)\n        self.weight_size = [self.embedding_dim] + self.weight_size\n        self.image_feats = image_feats\n        self.text_feats = text_feats\n\n        initializer = nn.init.xavier_uniform_\n        self.act = nn.ReLU()  #nn.PReLU(), nn.LeakyReLU(negative_slope=args.slope), nn.Sigmoid()\n        self.sigmoid = nn.Sigmoid()\n        self.dropout = nn.Dropout(p=args.drop_rate)\n        self.batch_norm = nn.BatchNorm1d(self.embedding_dim)\n        self.softmax = nn.Softmax(dim=1)\n\n        self.image_embedding = nn.Embedding.from_pretrained(torch.Tensor(image_feats), freeze=False)\n        self.text_embedding = nn.Embedding.from_pretrained(torch.Tensor(text_feats), freeze=False)\n\n        self.image_common_transformation = nn.Linear(image_feats.shape[1], args.embed_size)  #\n        self.text_common_transformation = nn.Linear(text_feats.shape[1], args.embed_size)  #\n        init.xavier_uniform_(self.image_common_transformation.weight, gain=1.414)  #\n        init.xavier_uniform_(self.text_common_transformation.weight, gain=1.414)  #\n\n        self.user_common_feature_embedding = {\"image\":None,\"text\":None}\n        self.item_common_feature_embedding = {\"image\":None,\"text\":None}\n        self.user_common_feature_embedding_f = {\"image\":None,\"text\":None}\n        self.item_common_feature_embedding_f = {\"image\":None,\"text\":None}\n\n        self.ssl_common = nn.Bilinear(self.embedding_dim, self.embedding_dim, 1)\n        self.ssl_image = nn.Bilinear(self.embedding_dim, self.embedding_dim, 1)\n        self.ssl_text = nn.Bilinear(self.embedding_dim, self.embedding_dim, 1)\n        init.xavier_uniform_(self.ssl_common.weight, gain=1.414)\n        init.xavier_uniform_(self.ssl_image.weight, gain=1.414)\n        init.xavier_uniform_(self.ssl_text.weight, gain=1.414)\n\n    def mm(self, x, y):\n        if args.sparse:\n            return torch.sparse.mm(x, y)\n        else:\n            return torch.mm(x, y)\n\n    def sim(self, z1, z2):\n        z1 = F.normalize(z1)\n        z2 = F.normalize(z2)\n        return torch.mm(z1, z2.t())\n\n    def batched_contrastive_loss(self, z1, z2, batch_size=4096):\n        device = z1.device\n        num_nodes = z1.size(0)\n        num_batches = (num_nodes - 1) // batch_size + 1\n        f = lambda x: torch.exp(x / self.tau)\n        indices = torch.arange(0, num_nodes).to(device)\n        losses = []\n\n        for i in range(num_batches):\n            mask = indices[i * batch_size:(i + 1) * batch_size]\n            refl_sim = f(self.sim(z1[mask], z1)) \n            between_sim = f(self.sim(z1[mask], z2))  \n\n            losses.append(-torch.log(\n                between_sim[:, i * batch_size:(i + 1) * batch_size].diag()\n                / (refl_sim.sum(1) + between_sim.sum(1)\n                   - refl_sim[:, i * batch_size:(i + 1) * batch_size].diag())))\n                   \n        loss_vec = torch.cat(losses)\n        return loss_vec.mean()\n\n\n    def para_dict_to_tenser(self, para_dict):  \n        \"\"\"\n        :param para_dict: nn.ParameterDict()\n        :return: tensor\n        \"\"\"\n        tensors = []\n\n        for k in para_dict.keys():\n            tensors.append(para_dict[k])\n\n        tensors = torch.stack(tensors, dim=0)\n\n        return tensors\n\n\n    def behavior_attention(self, embedding_input):  \n        embedding = self.para_dict_to_tenser(embedding_input)  \n        attention = torch.matmul(embedding, self.weight_dict['w_d_d'])  \n        attention = F.softmax(attention, dim=0)*2.5 \n\n        Z = torch.mul(attention, embedding)  \n        Z = torch.sum(Z, dim=0)  #\n        Z = self.act(Z)\n\n        return Z\n\n    def forward(self, image_adj_norm, image_adj, text_adj_norm, text_adj, ui_graph, iu_graph):\n\n        image_embedding = self.image_embedding.weight\n        text_embedding = self.text_embedding.weight\n        self.item_common_feature_embedding[\"image\"] = self.image_common_transformation(image_embedding)  \n        self.item_common_feature_embedding[\"text\"]  =  self.text_common_transformation(text_embedding)  \n\n        idx_image = np.random.permutation(self.n_items)\n        idx_text = np.random.permutation(self.n_items)\n        shuffle_image = self.image_feats[idx_image, :]\n        shuffle_text = self.text_feats[idx_text, :]\n        false_item_feature_embedding_image =  self.image_common_transformation(torch.tensor(shuffle_image).float().cuda())\n        false_item_feature_embedding_text = self.text_common_transformation(torch.tensor(shuffle_text).float().cuda())\n\n\n        for i in range(args.layers):\n            #ii\n            self.item_common_feature_embedding[\"image\"] = torch.mm(image_adj, self.item_common_feature_embedding['image'])\n            self.item_common_feature_embedding[\"text\"] = torch.mm(text_adj, self.item_common_feature_embedding['text'])\n\n            self.item_common_feature_embedding_f[\"image\"] = torch.mm(image_adj, false_item_feature_embedding_image)\n            self.item_common_feature_embedding_f[\"text\"] = torch.mm(text_adj, false_item_feature_embedding_text)\n\n            #ui\n            self.user_common_feature_embedding[\"image\"] = torch.mm(ui_graph, self.item_common_feature_embedding[\"image\"])\n            self.user_common_feature_embedding[\"text\"] = torch.mm(ui_graph, self.item_common_feature_embedding[\"text\"])\n\n            self.user_common_feature_embedding_f[\"image\"] = torch.mm(ui_graph, self.item_common_feature_embedding_f[\"image\"])\n            self.user_common_feature_embedding_f[\"text\"] = torch.mm(ui_graph, self.item_common_feature_embedding_f[\"text\"])\n\n\n        item_common_feature_embedding = (self.item_common_feature_embedding[\"image\"] + self.item_common_feature_embedding[\"text\"]) / 2\n\n\n        global_item_common_feature_embedding_image = torch.sum(self.item_common_feature_embedding[\"image\"], dim=0)  #\n        global_item_common_feature_embedding_text = torch.sum(self.item_common_feature_embedding[\"text\"], dim=0)  #\n\n        global_item_image_specific_feature_embedding = self.sigmoid(global_item_common_feature_embedding_image)\n        global_item_text_specific_feature_embedding = self.sigmoid(global_item_common_feature_embedding_text)\n\n        global_item_common_feature_embedding = torch.sum(item_common_feature_embedding, dim=0) \n        global_item_common_feature_embedding = self.sigmoid(global_item_common_feature_embedding)\n\n        global_item_common_feature_embedding = torch.unsqueeze(global_item_common_feature_embedding, 0)\n        global_item_common_feature_embedding = global_item_common_feature_embedding.repeat(self.n_items*2, 1)  #\n\n        local_item_common_feature_embedding_t = torch.cat((self.item_common_feature_embedding[\"image\"], self.item_common_feature_embedding[\"text\"]), 0)\n        local_item_common_feature_embedding_f = torch.cat((false_item_feature_embedding_image, false_item_feature_embedding_text), 0)\n\n        ssl_common_image_t = torch.unsqueeze(torch.squeeze(self.ssl_common(global_item_common_feature_embedding, local_item_common_feature_embedding_t), 1), 0)\n        ssl_common_image_f = torch.unsqueeze(torch.squeeze(self.ssl_common(global_item_common_feature_embedding, local_item_common_feature_embedding_f), 1), 0)\n\n        ssl_common_logit = torch.cat((ssl_common_image_t, ssl_common_image_f),1)\n\n        global_item_image_specific_feature_embedding = self.sigmoid(global_item_common_feature_embedding_image)\n        global_item_text_specific_feature_embedding = self.sigmoid(global_item_common_feature_embedding_text)\n\n        global_item_image_specific_feature_embedding = torch.unsqueeze(global_item_image_specific_feature_embedding, 0)\n        global_item_image_specific_feature_embedding = global_item_image_specific_feature_embedding.repeat(self.n_items, 1)  \n        global_item_text_specific_feature_embedding = torch.unsqueeze(global_item_text_specific_feature_embedding, 0)\n        global_item_text_specific_feature_embedding = global_item_text_specific_feature_embedding.repeat(self.n_items, 1)  \n\n\n        ssl_image_t = torch.unsqueeze(torch.squeeze(self.ssl_image(global_item_image_specific_feature_embedding, self.item_common_feature_embedding[\"image\"]), 1), 0)\n        ssl_image_f = torch.unsqueeze(torch.squeeze(self.ssl_image(global_item_image_specific_feature_embedding, false_item_feature_embedding_image), 1), 0)\n        ssl_text_t = torch.unsqueeze(torch.squeeze(self.ssl_text(global_item_text_specific_feature_embedding, self.item_common_feature_embedding[\"text\"]), 1), 0)\n        ssl_text_f = torch.unsqueeze(torch.squeeze(self.ssl_text(global_item_text_specific_feature_embedding, false_item_feature_embedding_text), 1), 0)\n\n        ssl_image_logit = torch.cat((ssl_image_t, ssl_image_f),1)\n        ssl_text_logit = torch.cat((ssl_text_t, ssl_text_f),1)        \n \n        item_final = (self.item_common_feature_embedding[\"image\"] + self.item_common_feature_embedding[\"text\"]) / 2\n        user_final = (self.user_common_feature_embedding[\"image\"] + self.user_common_feature_embedding[\"text\"]) / 2\n\n        return item_final, user_final, self.item_common_feature_embedding[\"image\"], self.item_common_feature_embedding[\"text\"], self.user_common_feature_embedding[\"image\"], self.user_common_feature_embedding[\"text\"], ssl_common_logit, ssl_image_logit, ssl_text_logit\n\n\nclass D_Model(nn.Module):\n    def __init__(self, n_users, n_items, embedding_dim, weight_size, dropout_list, image_feats, text_feats):\n\n        super().__init__()\n        self.n_users = n_users\n        self.n_items = n_items\n        self.embedding_dim = embedding_dim\n        self.weight_size = weight_size\n        self.n_ui_layers = len(self.weight_size)\n        self.weight_size = [self.embedding_dim] + self.weight_size\n\n        self.image_decoder = nn.Linear(args.embed_size, image_feats.shape[1])\n        self.text_decoder = nn.Linear(args.embed_size, text_feats.shape[1])\n        nn.init.xavier_uniform_(self.image_decoder.weight)\n        nn.init.xavier_uniform_(self.text_decoder.weight)\n\n        self.decoder = nn.ModuleDict() \n        self.decoder['image_decoder'] = self.image_decoder\n        self.decoder['text_decoder'] = self.text_decoder\n\n        self.image_trans = nn.Linear(image_feats.shape[1], args.embed_size)\n        self.text_trans = nn.Linear(text_feats.shape[1], args.embed_size)\n        nn.init.xavier_uniform_(self.image_trans.weight)\n        nn.init.xavier_uniform_(self.text_trans.weight)             \n        self.encoder = nn.ModuleDict() \n        self.encoder['image_encoder'] = self.image_trans\n        self.encoder['text_encoder'] = self.text_trans\n\n        self.common_trans = nn.Linear(args.embed_size, args.embed_size)\n        nn.init.xavier_uniform_(self.common_trans.weight)\n        self.align = nn.ModuleDict() \n        self.align['common_trans'] = self.common_trans\n\n        self.user_id_embedding = nn.Embedding(n_users, self.embedding_dim)\n        self.item_id_embedding = nn.Embedding(n_items, self.embedding_dim)\n\n        nn.init.xavier_uniform_(self.user_id_embedding.weight)\n        nn.init.xavier_uniform_(self.item_id_embedding.weight)\n        self.image_feats = torch.tensor(image_feats).float().cuda()\n        self.text_feats = torch.tensor(text_feats).float().cuda()\n        self.image_embedding = nn.Embedding.from_pretrained(torch.Tensor(image_feats), freeze=False)\n        self.text_embedding = nn.Embedding.from_pretrained(torch.Tensor(text_feats), freeze=False)\n\n        self.image_gnn_trans = nn.Linear(args.embed_size, args.embed_size)\n        self.text_gnn_trans = nn.Linear(args.embed_size, args.embed_size)\n        nn.init.xavier_uniform_(self.image_gnn_trans.weight)\n        nn.init.xavier_uniform_(self.text_gnn_trans.weight)\n   \n        self.gnn = nn.ModuleDict() \n        self.gnn['user_id_embedding'] = self.user_id_embedding\n        self.gnn['item_id_embedding'] = self.item_id_embedding\n        self.gnn['image_embedding'] = self.image_embedding\n        self.gnn['text_embedding'] = self.text_embedding\n        self.gnn['image_trans'] = self.image_trans\n        self.gnn['text_trans'] = self.text_trans\n        self.gnn['image_gnn_trans'] = self.image_gnn_trans\n        self.gnn['text_gnn_trans'] = self.text_gnn_trans\n\n        self.softmax = nn.Softmax(dim=-1)\n        self.act = nn.Sigmoid()  #nn.LeakyReLU(0.1) nn.ReLU() nn.PReLU(), nn.LeakyReLU(negative_slope=args.slope), nn.Sigmoid()\n        self.sigmoid = nn.Sigmoid()\n        self.dropout = nn.Dropout(p=args.drop_rate)\n        self.batch_norm = nn.BatchNorm1d(args.embed_size)\n        self.tau = 0.5\n\n        self.other = nn.ModuleDict() \n        self.other['softmax'] = self.softmax\n        self.other['act'] = self.act\n        self.other['sigmoid'] = self.sigmoid\n        self.other['dropout'] = self.dropout\n        self.other['batch_norm'] = self.batch_norm\n\n        initializer = nn.init.xavier_uniform_\n        self.weight_dict = nn.ParameterDict({\n            'w_q': nn.Parameter(initializer(torch.empty([args.embed_size, args.embed_size]))),\n            'w_k': nn.Parameter(initializer(torch.empty([args.embed_size, args.embed_size]))),\n            'w_v': nn.Parameter(initializer(torch.empty([args.embed_size, args.embed_size]))),\n            'w_self_attention_item': nn.Parameter(initializer(torch.empty([args.embed_size, args.embed_size]))),\n            'w_self_attention_user': nn.Parameter(initializer(torch.empty([args.embed_size, args.embed_size]))),\n            'w_self_attention_cat': nn.Parameter(initializer(torch.empty([args.head_num*args.embed_size, args.embed_size]))),\n        })\n        self.embedding_dict = {'user':{}, 'item':{}}\n\n    def mm(self, x, y):\n        if args.sparse:\n            return torch.sparse.mm(x, y)\n        else:\n            return torch.mm(x, y)\n    def sim(self, z1, z2):\n        z1 = F.normalize(z1)\n        z2 = F.normalize(z2)\n        return torch.mm(z1, z2.t())\n\n    def batched_contrastive_loss(self, z1, z2, batch_size=4096):\n        device = z1.device\n        num_nodes = z1.size(0)\n        num_batches = (num_nodes - 1) // batch_size + 1\n        f = lambda x: torch.exp(x / self.tau)\n        indices = torch.arange(0, num_nodes).to(device)\n        losses = []\n\n        for i in range(num_batches):\n            mask = indices[i * batch_size:(i + 1) * batch_size]\n            refl_sim = f(self.sim(z1[mask], z1))  \n            between_sim = f(self.sim(z1[mask], z2))  \n\n            losses.append(-torch.log(\n                between_sim[:, i * batch_size:(i + 1) * batch_size].diag()\n                / (refl_sim.sum(1) + between_sim.sum(1)\n                   - refl_sim[:, i * batch_size:(i + 1) * batch_size].diag())))\n                   \n        loss_vec = torch.cat(losses)\n        return loss_vec.mean()\n\n    def csr_norm(self, csr_mat, mean_flag=False):\n        rowsum = np.array(csr_mat.sum(1))\n        rowsum = np.power(rowsum+1e-8, -0.5).flatten()\n        rowsum[np.isinf(rowsum)] = 0.\n        rowsum_diag = sp.diags(rowsum)\n\n        colsum = np.array(csr_mat.sum(0))\n        colsum = np.power(colsum+1e-8, -0.5).flatten()\n        colsum[np.isinf(colsum)] = 0.\n        colsum_diag = sp.diags(colsum)\n\n        if mean_flag == False:\n            return rowsum_diag*csr_mat*colsum_diag\n        else:\n            return rowsum_diag*csr_mat\n\n    def matrix_to_tensor(self, cur_matrix):\n        if type(cur_matrix) != sp.coo_matrix:\n            cur_matrix = cur_matrix.tocoo()  #\n        indices = torch.from_numpy(np.vstack((cur_matrix.row, cur_matrix.col)).astype(np.int64))  #\n        values = torch.from_numpy(cur_matrix.data)  #\n        shape = torch.Size(cur_matrix.shape)\n\n        return torch.sparse.FloatTensor(indices, values, shape).to(torch.float32).cuda()  #\n\n    def para_dict_to_tenser(self, para_dict):  \n        \"\"\"\n        :param para_dict: nn.ParameterDict()\n        :return: tensor\n        \"\"\"\n        tensors = []\n\n        for beh in para_dict.keys():\n            tensors.append(para_dict[beh])\n        tensors = torch.stack(tensors, dim=0)\n\n        return tensors\n\n\n    def multi_head_self_attention(self, trans_w, embedding_t_1, embedding_t):  \n       \n        q = self.para_dict_to_tenser(embedding_t)\n        v = k = self.para_dict_to_tenser(embedding_t_1)\n        beh, N, d_h = q.shape[0], q.shape[1], args.embed_size/args.head_num\n\n        Q = torch.matmul(q, trans_w['w_q'])  \n        K = torch.matmul(k, trans_w['w_k'])\n        V = v\n\n        Q = Q.reshape(beh, N, args.head_num, int(d_h)).permute(2, 0, 1, 3)  \n        K = Q.reshape(beh, N, args.head_num, int(d_h)).permute(2, 0, 1, 3)\n\n        Q = torch.unsqueeze(Q, 2) \n        K = torch.unsqueeze(K, 1)  \n        V = torch.unsqueeze(V, 1)  \n\n        att = torch.mul(Q, K) / torch.sqrt(torch.tensor(d_h))  \n        att = torch.sum(att, dim=-1) \n        att = torch.unsqueeze(att, dim=-1)  \n        att = F.softmax(att, dim=2)  \n\n        Z = torch.mul(att, V)  \n        Z = torch.sum(Z, dim=2)  \n\n        Z_list = [value for value in Z]\n        Z = torch.cat(Z_list, -1)\n        Z = torch.matmul(Z, self.weight_dict['w_self_attention_cat'])\n\n        args.model_cat_rate*F.normalize(Z, p=2, dim=2)\n        return Z, att.detach()\n\n    def forward(self, ui_graph, iu_graph, image_ui_graph, image_iu_graph, text_ui_graph, text_iu_graph):\n\n        image_feats = image_item_feats = image_feats_encode = self.dropout(self.image_trans(self.image_feats))\n        text_feats = text_item_feats = text_feats_encode = self.dropout(self.text_trans(self.text_feats))\n\n        image_feats_decode = self.image_decoder(image_feats_encode)\n        text_feats_decode = self.text_decoder(text_feats_encode)\n\n        idx_image = np.random.permutation(self.n_items)\n        idx_text = np.random.permutation(self.n_items)\n        shuffle_image = self.image_feats[idx_image, :]\n        shuffle_text = self.text_feats[idx_text, :]\n        image_feats_f =  self.dropout(self.image_trans(torch.tensor(shuffle_image).float().cuda()))\n        text_feats_f = self.dropout(self.text_trans(torch.tensor(shuffle_text).float().cuda()))\n\n        for i in range(args.layers):\n            image_user_feats = self.mm(ui_graph, image_feats)\n            image_item_feats = self.mm(iu_graph, image_user_feats)\n            image_user_id = self.mm(image_ui_graph, self.item_id_embedding.weight)\n            image_item_id = self.mm(image_iu_graph, self.user_id_embedding.weight)\n\n            text_user_feats = self.mm(ui_graph, text_feats)\n            text_item_feats = self.mm(iu_graph, text_user_feats)\n\n            text_user_id = self.mm(text_ui_graph, self.item_id_embedding.weight)\n            text_item_id = self.mm(text_iu_graph, self.user_id_embedding.weight)\n\n\n        self.embedding_dict['user']['image'] = image_user_id\n        self.embedding_dict['user']['text'] = text_user_id\n        self.embedding_dict['item']['image'] = image_item_id\n        self.embedding_dict['item']['text'] = text_item_id\n        user_z, att_u = self.multi_head_self_attention(self.weight_dict, self.embedding_dict['user'], self.embedding_dict['user'])\n        item_z, att_i = self.multi_head_self_attention(self.weight_dict, self.embedding_dict['item'], self.embedding_dict['item'])\n        user_emb = user_z.mean(0)\n        item_emb = item_z.mean(0)\n        u_g_embeddings = self.user_id_embedding.weight + args.id_cat_rate*F.normalize(user_emb, p=2, dim=1)\n        i_g_embeddings = self.item_id_embedding.weight + args.id_cat_rate*F.normalize(item_emb, p=2, dim=1)\n\n        user_emb_list = [u_g_embeddings]\n        item_emb_list = [i_g_embeddings]\n        for i in range(self.n_ui_layers):    \n            if i == (self.n_ui_layers-1):\n                u_g_embeddings = self.softmax( torch.mm(ui_graph, i_g_embeddings) ) \n                i_g_embeddings = self.softmax( torch.mm(iu_graph, u_g_embeddings) )\n\n            else:\n                u_g_embeddings = torch.mm(ui_graph, i_g_embeddings) \n                i_g_embeddings = torch.mm(iu_graph, u_g_embeddings) \n\n            user_emb_list.append(u_g_embeddings)\n            item_emb_list.append(i_g_embeddings)\n\n        u_g_embeddings = torch.mean(torch.stack(user_emb_list), dim=0)\n        i_g_embeddings = torch.mean(torch.stack(item_emb_list), dim=0)\n\n\n        u_g_embeddings = u_g_embeddings + args.model_cat_rate*F.normalize(image_user_feats, p=2, dim=1) + args.model_cat_rate*F.normalize(text_user_feats, p=2, dim=1)\n        i_g_embeddings = i_g_embeddings + args.model_cat_rate*F.normalize(image_item_feats, p=2, dim=1) + args.model_cat_rate*F.normalize(text_item_feats, p=2, dim=1)\n\n        return u_g_embeddings, i_g_embeddings, image_item_feats, text_item_feats, image_user_feats, text_user_feats, u_g_embeddings, i_g_embeddings, image_user_id, text_user_id, image_item_id, text_item_id\n\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, dim):\n        super(Discriminator, self).__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(dim, int(dim/4)),\n            nn.LeakyReLU(True),\n            nn.BatchNorm1d(int(dim/4)),\n    \t\tnn.Dropout(args.G_drop1),\n\n            nn.Linear(int(dim/4), int(dim/8)),\n            nn.LeakyReLU(True),\n            nn.BatchNorm1d(int(dim/8)),\n    \t\tnn.Dropout(args.G_drop2),\n\n            nn.Linear(int(dim/8), 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        output = 100*self.net(x.float())  \n        return output.view(-1)\n\n"}
{"type": "source_file", "path": "LATTICE/codes/utility/metrics.py", "content": "import numpy as np\nfrom sklearn.metrics import roc_auc_score\n\ndef recall(rank, ground_truth, N):\n    return len(set(rank[:N]) & set(ground_truth)) / float(len(set(ground_truth)))\n\n\ndef precision_at_k(r, k):\n    \"\"\"Score is precision @ k\n    Relevance is binary (nonzero is relevant).\n    Returns:\n        Precision @ k\n    Raises:\n        ValueError: len(r) must be >= k\n    \"\"\"\n    assert k >= 1\n    r = np.asarray(r)[:k]\n    return np.mean(r)\n\n\ndef average_precision(r,cut):\n    \"\"\"Score is average precision (area under PR curve)\n    Relevance is binary (nonzero is relevant).\n    Returns:\n        Average precision\n    \"\"\"\n    r = np.asarray(r)\n    out = [precision_at_k(r, k + 1) for k in range(cut) if r[k]]\n    if not out:\n        return 0.\n    return np.sum(out)/float(min(cut, np.sum(r)))\n\n\ndef mean_average_precision(rs):\n    \"\"\"Score is mean average precision\n    Relevance is binary (nonzero is relevant).\n    Returns:\n        Mean average precision\n    \"\"\"\n    return np.mean([average_precision(r) for r in rs])\n\n\ndef dcg_at_k(r, k, method=1):\n    \"\"\"Score is discounted cumulative gain (dcg)\n    Relevance is positive real values.  Can use binary\n    as the previous methods.\n    Returns:\n        Discounted cumulative gain\n    \"\"\"\n    r = np.asfarray(r)[:k]\n    if r.size:\n        if method == 0:\n            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n        elif method == 1:\n            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n        else:\n            raise ValueError('method must be 0 or 1.')\n    return 0.\n\n\ndef ndcg_at_k(r, k, method=1):\n    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n    Relevance is positive real values.  Can use binary\n    as the previous methods.\n    Returns:\n        Normalized discounted cumulative gain\n    \"\"\"\n    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n    if not dcg_max:\n        return 0.\n    return dcg_at_k(r, k, method) / dcg_max\n\n\ndef recall_at_k(r, k, all_pos_num):\n    r = np.asfarray(r)[:k]\n    if all_pos_num == 0:\n        return 0\n    else:\n        return np.sum(r) / all_pos_num\n\n\ndef hit_at_k(r, k):\n    r = np.array(r)[:k]\n    if np.sum(r) > 0:\n        return 1.\n    else:\n        return 0.\n\ndef F1(pre, rec):\n    if pre + rec > 0:\n        return (2.0 * pre * rec) / (pre + rec)\n    else:\n        return 0.\n\ndef auc(ground_truth, prediction):\n    try:\n        res = roc_auc_score(y_true=ground_truth, y_score=prediction)\n    except Exception:\n        res = 0.\n    return res"}
{"type": "source_file", "path": "main.py", "content": "from datetime import datetime\nimport math\nimport os\nimport random\nimport sys\nfrom time import time\nfrom tqdm import tqdm\nimport pickle\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.sparse import csr_matrix\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.sparse as sparse\nfrom torch import autograd\nimport random\n\nimport copy\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nfrom utility.parser import parse_args\nfrom Models import MM_Model, Decoder  \nfrom utility.batch_test import *\nfrom utility.logging import Logger\nfrom utility.norm import build_sim, build_knn_normalized_graph\n\nimport setproctitle\n\nargs = parse_args()\n\n\nclass Trainer(object):\n    def __init__(self, data_config):\n       \n        self.task_name = \"%s_%s_%s\" % (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), args.dataset, args.cf_model,)\n        self.logger = Logger(filename=self.task_name, is_debug=args.debug)\n        self.logger.logging(\"PID: %d\" % os.getpid())\n        self.logger.logging(str(args))\n\n        self.mess_dropout = eval(args.mess_dropout)\n        self.lr = args.lr\n        self.emb_dim = args.embed_size\n        self.batch_size = args.batch_size\n        self.weight_size = eval(args.weight_size)\n        self.n_layers = len(self.weight_size)\n        self.regs = eval(args.regs)\n        self.decay = self.regs[0]\n \n        self.image_feats = np.load(args.data_path + '{}/image_feat.npy'.format(args.dataset))\n        self.text_feats = np.load(args.data_path + '{}/text_feat.npy'.format(args.dataset))\n        self.image_feat_dim = self.image_feats.shape[-1]\n        self.text_feat_dim = self.text_feats.shape[-1]\n\n        self.ui_graph = self.ui_graph_raw = pickle.load(open(args.data_path + args.dataset + '/train_mat','rb'))\n        # get user embedding  \n        augmented_user_init_embedding = pickle.load(open(args.data_path + args.dataset + '/augmented_user_init_embedding','rb'))\n        augmented_user_init_embedding_list = []\n        for i in range(len(augmented_user_init_embedding)):\n            augmented_user_init_embedding_list.append(augmented_user_init_embedding[i])\n        augmented_user_init_embedding_final = np.array(augmented_user_init_embedding_list)\n        pickle.dump(augmented_user_init_embedding_final, open(args.data_path + args.dataset + '/augmented_user_init_embedding_final','wb'))\n        self.user_init_embedding = pickle.load(open(args.data_path + args.dataset + '/augmented_user_init_embedding_final','rb'))\n        # get separate embedding matrix \n        if args.dataset=='preprocessed_raw_MovieLens':\n            augmented_total_embed_dict = {'title':[] , 'genre':[], 'director':[], 'country':[], 'language':[]}   \n        elif args.dataset=='netflix_valid_item':\n            augmented_total_embed_dict = {'year':[] , 'title':[], 'director':[], 'country':[], 'language':[]}   \n        augmented_atttribute_embedding_dict = pickle.load(open(args.data_path + args.dataset + '/augmented_atttribute_embedding_dict','rb'))\n        for value in augmented_atttribute_embedding_dict.keys():\n            for i in range(len(augmented_atttribute_embedding_dict[value])):\n                augmented_total_embed_dict[value].append(augmented_atttribute_embedding_dict[value][i])   \n            augmented_total_embed_dict[value] = np.array(augmented_total_embed_dict[value])    \n        pickle.dump(augmented_total_embed_dict, open(args.data_path + args.dataset + '/augmented_total_embed_dict','wb'))\n        self.item_attribute_embedding = pickle.load(open(args.data_path + args.dataset + '/augmented_total_embed_dict','rb'))       \n\n        self.image_ui_index = {'x':[], 'y':[]}\n        self.text_ui_index = {'x':[], 'y':[]}\n\n        self.n_users = self.ui_graph.shape[0]\n        self.n_items = self.ui_graph.shape[1]        \n        self.iu_graph = self.ui_graph.T\n  \n        self.ui_graph = self.csr_norm(self.ui_graph, mean_flag=True)\n        self.iu_graph = self.csr_norm(self.iu_graph, mean_flag=True)\n        self.ui_graph = self.matrix_to_tensor(self.ui_graph)\n        self.iu_graph = self.matrix_to_tensor(self.iu_graph)\n        self.image_ui_graph = self.text_ui_graph = self.ui_graph\n        self.image_iu_graph = self.text_iu_graph = self.iu_graph\n\n        self.model_mm = MM_Model(self.n_users, self.n_items, self.emb_dim, self.weight_size, self.mess_dropout, self.image_feats, self.text_feats, self.user_init_embedding, self.item_attribute_embedding)      \n        self.model_mm = self.model_mm.cuda()  \n        self.decoder = Decoder(self.user_init_embedding.shape[1]).cuda()\n\n\n        self.optimizer = optim.AdamW(\n        [\n            {'params':self.model_mm.parameters()},      \n        ]\n            , lr=self.lr)  \n        \n        self.de_optimizer = optim.AdamW(\n        [\n            {'params':self.decoder.parameters()},      \n        ]\n            , lr=args.de_lr)  \n\n\n\n    def csr_norm(self, csr_mat, mean_flag=False):\n        rowsum = np.array(csr_mat.sum(1))\n        rowsum = np.power(rowsum+1e-8, -0.5).flatten()\n        rowsum[np.isinf(rowsum)] = 0.\n        rowsum_diag = sp.diags(rowsum)\n        colsum = np.array(csr_mat.sum(0))\n        colsum = np.power(colsum+1e-8, -0.5).flatten()\n        colsum[np.isinf(colsum)] = 0.\n        colsum_diag = sp.diags(colsum)\n        if mean_flag == False:\n            return rowsum_diag*csr_mat*colsum_diag\n        else:\n            return rowsum_diag*csr_mat\n\n    def matrix_to_tensor(self, cur_matrix):\n        if type(cur_matrix) != sp.coo_matrix:\n            cur_matrix = cur_matrix.tocoo()  #\n        indices = torch.from_numpy(np.vstack((cur_matrix.row, cur_matrix.col)).astype(np.int64))  #\n        values = torch.from_numpy(cur_matrix.data)  #\n        shape = torch.Size(cur_matrix.shape)\n        return torch.sparse.FloatTensor(indices, values, shape).to(torch.float32).cuda()  #\n\n    def innerProduct(self, u_pos, i_pos, u_neg, j_neg):  \n        pred_i = torch.sum(torch.mul(u_pos,i_pos), dim=-1) \n        pred_j = torch.sum(torch.mul(u_neg,j_neg), dim=-1)  \n        return pred_i, pred_j\n\n    def weights_init(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.kaiming_normal_(m.weight)\n            m.bias.data.fill_(0)\n\n    def sim(self, z1, z2):\n        z1 = F.normalize(z1)  \n        z2 = F.normalize(z2)\n        return torch.mm(z1, z2.t())\n\n    def feat_reg_loss_calculation(self, g_item_image, g_item_text, g_user_image, g_user_text):\n        feat_reg = 1./2*(g_item_image**2).sum() + 1./2*(g_item_text**2).sum() \\\n            + 1./2*(g_user_image**2).sum() + 1./2*(g_user_text**2).sum()        \n        feat_reg = feat_reg / self.n_items\n        feat_emb_loss = args.feat_reg_decay * feat_reg\n        return feat_emb_loss\n\n    def prune_loss(self, pred, drop_rate):\n        ind_sorted = np.argsort(pred.cpu().data).cuda()\n        loss_sorted = pred[ind_sorted]\n        remember_rate = 1 - drop_rate\n        num_remember = int(remember_rate * len(loss_sorted))\n        ind_update = ind_sorted[:num_remember]\n        loss_update = pred[ind_update]\n        return loss_update.mean()\n\n    def mse_criterion(self, x, y, alpha=3):\n        x = F.normalize(x, p=2, dim=-1)\n        y = F.normalize(y, p=2, dim=-1)\n        tmp_loss = (1 - (x * y).sum(dim=-1)).pow_(alpha)\n        tmp_loss = tmp_loss.mean()\n        loss = F.mse_loss(x, y)\n        return loss\n\n    def sce_criterion(self, x, y, alpha=1):\n        x = F.normalize(x, p=2, dim=-1)\n        y = F.normalize(y, p=2, dim=-1)\n        loss = (1-(x*y).sum(dim=-1)).pow_(alpha)\n        loss = loss.mean() \n        return loss\n\n    def test(self, users_to_test, is_val):\n        self.model_mm.eval()\n        with torch.no_grad():\n            ua_embeddings, ia_embeddings, *rest = self.model_mm(self.ui_graph, self.iu_graph, self.image_ui_graph, self.image_iu_graph, self.text_ui_graph, self.text_iu_graph)\n        result = test_torch(ua_embeddings, ia_embeddings, users_to_test, is_val)\n        return result\n\n    def train(self):\n\n        now_time = datetime.now()\n        run_time = datetime.strftime(now_time,'%Y_%m_%d__%H_%M_%S')\n\n        training_time_list = []\n        stopping_step = 0\n\n        n_batch = data_generator.n_train // args.batch_size + 1\n        best_recall = 0\n        for epoch in range(args.epoch):\n            t1 = time()\n            loss, mf_loss, emb_loss, reg_loss = 0., 0., 0., 0.\n            contrastive_loss = 0.\n            n_batch = data_generator.n_train // args.batch_size + 1\n            sample_time = 0.\n            build_item_graph = True\n\n            self.gene_u, self.gene_real, self.gene_fake = None, None, {}\n            self.topk_p_dict, self.topk_id_dict = {}, {}\n\n            for idx in tqdm(range(n_batch)):\n                self.model_mm.train()\n                sample_t1 = time()\n                users, pos_items, neg_items = data_generator.sample()\n\n                # augment samples \n                augmented_sample_dict = pickle.load(open(args.data_path + args.dataset + '/augmented_sample_dict','rb'))\n                users_aug = random.sample(users, int(len(users)*args.aug_sample_rate))\n                pos_items_aug = [augmented_sample_dict[user][0] for user in users_aug if (augmented_sample_dict[user][0]<self.n_items and augmented_sample_dict[user][1]<self.n_items)]\n                neg_items_aug = [augmented_sample_dict[user][1] for user in users_aug if (augmented_sample_dict[user][0]<self.n_items and augmented_sample_dict[user][1]<self.n_items)]\n                users_aug = [user for user in users_aug if (augmented_sample_dict[user][0]<self.n_items and augmented_sample_dict[user][1]<self.n_items)]\n                self.new_batch_size = len(users_aug)\n                users += users_aug\n                pos_items += pos_items_aug\n                neg_items += neg_items_aug\n\n\n                sample_time += time() - sample_t1       \n                user_presentation_h, item_presentation_h, image_i_feat, text_i_feat, image_u_feat, text_u_feat \\\n                                , user_prof_feat_pre, item_prof_feat_pre, user_prof_feat, item_prof_feat, user_att_feats, item_att_feats, i_mask_nodes, u_mask_nodes \\\n                        = self.model_mm(self.ui_graph, self.iu_graph, self.image_ui_graph, self.image_iu_graph, self.text_ui_graph, self.text_iu_graph)\n                \n                u_bpr_emb = user_presentation_h[users]\n                i_bpr_pos_emb = item_presentation_h[pos_items]\n                i_bpr_neg_emb = item_presentation_h[neg_items]\n                batch_mf_loss, batch_emb_loss, batch_reg_loss = self.bpr_loss(u_bpr_emb, i_bpr_pos_emb, i_bpr_neg_emb)\n       \n                # modal feat\n                image_u_bpr_emb = image_u_feat[users]\n                image_i_bpr_pos_emb = image_i_feat[pos_items]\n                image_i_bpr_neg_emb = image_i_feat[neg_items]\n                image_batch_mf_loss, image_batch_emb_loss, image_batch_reg_loss = self.bpr_loss(image_u_bpr_emb, image_i_bpr_pos_emb, image_i_bpr_neg_emb)\n                text_u_bpr_emb = text_u_feat[users]\n                text_i_bpr_pos_emb = text_i_feat[pos_items]\n                text_i_bpr_neg_emb = text_i_feat[neg_items]\n                text_batch_mf_loss, text_batch_emb_loss, text_batch_reg_loss = self.bpr_loss(text_u_bpr_emb, text_i_bpr_pos_emb, text_i_bpr_neg_emb)\n                mm_mf_loss = image_batch_mf_loss + text_batch_mf_loss\n\n                batch_mf_loss_aug = 0 \n                for index,value in enumerate(item_att_feats):  # \n                    u_g_embeddings_aug = user_prof_feat[users]\n                    pos_i_g_embeddings_aug = item_att_feats[value][pos_items]\n                    neg_i_g_embeddings_aug = item_att_feats[value][neg_items]\n                    tmp_batch_mf_loss_aug, batch_emb_loss_aug, batch_reg_loss_aug = self.bpr_loss(u_g_embeddings_aug, pos_i_g_embeddings_aug, neg_i_g_embeddings_aug)\n                    batch_mf_loss_aug += tmp_batch_mf_loss_aug\n\n                feat_emb_loss = self.feat_reg_loss_calculation(image_i_feat, text_i_feat, image_u_feat, text_u_feat)\n\n                att_re_loss = 0\n                if args.mask:\n                    input_i = {} \n                    for index,value in enumerate(item_att_feats.keys()):  \n                        input_i[value] = item_att_feats[value][i_mask_nodes]\n                    decoded_u, decoded_i = self.decoder(torch.tensor(user_prof_feat[u_mask_nodes]), input_i)\n                    if args.feat_loss_type=='mse':\n                        att_re_loss += self.mse_criterion(decoded_u, torch.tensor(self.user_init_embedding[u_mask_nodes]).cuda(), alpha=args.alpha_l)\n                        for index,value in enumerate(item_att_feats.keys()):  \n                            att_re_loss += self.mse_criterion(decoded_i[index], torch.tensor(self.item_attribute_embedding[value][i_mask_nodes]).cuda(), alpha=args.alpha_l)\n                    elif args.feat_loss_type=='sce':\n                        att_re_loss += self.sce_criterion(decoded_u, torch.tensor(self.user_init_embedding[u_mask_nodes]).cuda(), alpha=args.alpha_l) \n                        for index,value in enumerate(item_att_feats.keys()):  \n                            att_re_loss += self.sce_criterion(decoded_i[index], torch.tensor(self.item_attribute_embedding[value][i_mask_nodes]).cuda(), alpha=args.alpha_l)\n\n                batch_loss = batch_mf_loss + batch_emb_loss + batch_reg_loss + feat_emb_loss + args.aug_mf_rate*batch_mf_loss_aug + args.mm_mf_rate*mm_mf_loss + args.att_re_rate*att_re_loss\n                nn.utils.clip_grad_norm_(self.model_mm.parameters(), max_norm=1.0)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      #+ ssl_loss2 #+ batch_contrastive_loss\n                self.optimizer.zero_grad()  \n                batch_loss.backward(retain_graph=False)\n                \n                self.optimizer.step()\n\n                loss += float(batch_loss)\n                mf_loss += float(batch_mf_loss)\n                emb_loss += float(batch_emb_loss)\n                reg_loss += float(batch_reg_loss)\n    \n            del user_presentation_h, item_presentation_h, u_bpr_emb, i_bpr_neg_emb, i_bpr_pos_emb\n\n            if math.isnan(loss) == True:\n                self.logger.logging('ERROR: loss is nan.')\n                sys.exit()\n\n            if (epoch + 1) % args.verbose != 0:\n                perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f + %.5f  + %.5f]' % (\n                    epoch, time() - t1, loss, mf_loss, emb_loss, reg_loss, contrastive_loss)\n                training_time_list.append(time() - t1)\n                self.logger.logging(perf_str)\n\n            t2 = time()\n            users_to_test = list(data_generator.test_set.keys())\n            users_to_val = list(data_generator.val_set.keys())\n            ret = self.test(users_to_test, is_val=False)  #^-^\n            training_time_list.append(t2 - t1)\n\n            t3 = time()\n\n            if args.verbose > 0:\n                perf_str = 'Epoch %d [%.1fs + %.1fs]: train==[%.5f=%.5f + %.5f + %.5f], recall=[%.5f, %.5f, %.5f, %.5f], ' \\\n                           'precision=[%.5f, %.5f, %.5f, %.5f], hit=[%.5f, %.5f, %.5f, %.5f], ndcg=[%.5f, %.5f, %.5f, %.5f]' % \\\n                           (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, reg_loss, ret['recall'][0], ret['recall'][1], ret['recall'][2],\n                            ret['recall'][-1],\n                            ret['precision'][0], ret['precision'][1], ret['precision'][2], ret['precision'][-1], ret['hit_ratio'][0], ret['hit_ratio'][1], ret['hit_ratio'][2], ret['hit_ratio'][-1],\n                            ret['ndcg'][0], ret['ndcg'][1], ret['ndcg'][2], ret['ndcg'][-1])\n                self.logger.logging(perf_str)\n\n            if ret['recall'][1] > best_recall:\n                best_recall = ret['recall'][1]\n                test_ret = self.test(users_to_test, is_val=False)\n                self.logger.logging(\"Test_Recall@%d: %.5f,  precision=[%.5f], ndcg=[%.5f]\" % (eval(args.Ks)[1], test_ret['recall'][1], test_ret['precision'][1], test_ret['ndcg'][1]))\n                stopping_step = 0\n            elif stopping_step < args.early_stopping_patience:\n                stopping_step += 1\n                self.logger.logging('#####Early stopping steps: %d #####' % stopping_step)\n            else:\n                self.logger.logging('#####Early stop! #####')\n                break\n        self.logger.logging(str(test_ret))\n\n        return best_recall, run_time \n\n\n    def bpr_loss(self, users, pos_items, neg_items):\n        pos_scores = torch.sum(torch.mul(users, pos_items), dim=1)\n        neg_scores = torch.sum(torch.mul(users, neg_items), dim=1)\n\n        regularizer = 1./(2*(users**2).sum()+1e-8) + 1./(2*(pos_items**2).sum()+1e-8) + 1./(2*(neg_items**2).sum()+1e-8)        \n        regularizer = regularizer / self.batch_size\n\n        maxi = F.logsigmoid(pos_scores - neg_scores+1e-8)\n        mf_loss = - self.prune_loss(maxi, args.prune_loss_drop_rate)\n\n        emb_loss = self.decay * regularizer\n        reg_loss = 0.0\n        return mf_loss, emb_loss, reg_loss\n    \n\n    def sparse_mx_to_torch_sparse_tensor(self, sparse_mx):\n        \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n        sparse_mx = sparse_mx.tocoo().astype(np.float32)\n        indices = torch.from_numpy(\n            np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n        values = torch.from_numpy(sparse_mx.data)\n        shape = torch.Size(sparse_mx.shape)\n        return torch.sparse.FloatTensor(indices, values, shape)\n    \n\ndef set_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed) \n    torch.cuda.manual_seed_all(seed)  \n\nif __name__ == '__main__':\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n    set_seed(args.seed)\n    config = dict()\n    config['n_users'] = data_generator.n_users\n    config['n_items'] = data_generator.n_items\n\n    trainer = Trainer(data_config=config)\n    trainer.train()\n\n\n\n\n\n\n"}
