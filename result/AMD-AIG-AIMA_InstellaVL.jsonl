{"repo_info": {"repo_name": "InstellaVL", "repo_owner": "AMD-AIG-AIMA", "repo_url": "https://github.com/AMD-AIG-AIMA/InstellaVL"}}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/constants.py", "content": "OPENAI_DATASET_MEAN = (0.48145466, 0.4578275, 0.40821073)\nOPENAI_DATASET_STD = (0.26862954, 0.26130258, 0.27577711)\n"}
{"type": "source_file", "path": "assets/patches/encodings.py", "content": "# Modification CopyrightÂ© 2025 Advanced Micro Devices, Inc. All rights reserved.\n# Copyright 2022-2024 MosaicML Streaming authors\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Encode and Decode samples in a supported MDS format.\"\"\"\n\nimport json\nimport pickle\nfrom abc import ABC, abstractmethod\nfrom decimal import Decimal\nfrom io import BytesIO\nfrom typing import Any, Optional, List\n\nimport numpy as np\nfrom numpy import typing as npt\nfrom PIL import Image\nfrom PIL.JpegImagePlugin import JpegImageFile\nfrom typing_extensions import Self\n\n__all__ = [\n    'get_mds_encoded_size', 'get_mds_encodings', 'is_mds_encoding', 'mds_decode', 'mds_encode',\n    'is_mds_encoding_safe'\n]\n\n\nclass Encoding(ABC):\n    \"\"\"Encodes and decodes between objects of a certain type and raw bytes.\"\"\"\n\n    size: Optional[int] = None  # Fixed size in bytes of encoded data (None if variable size).\n\n    @abstractmethod\n    def encode(self, obj: Any) -> bytes:\n        \"\"\"Encode the given data from the original object to bytes.\n\n        Args:\n            obj (Any): Decoded object.\n\n        Returns:\n            bytes: Encoded data.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def decode(self, data: bytes) -> Any:\n        \"\"\"Decode the given data from bytes to the original object.\n\n        Args:\n            data (bytes): Encoded data.\n\n        Returns:\n            Any: Decoded object.\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def _validate(data: Any, expected_type: Any) -> None:\n        if not isinstance(data, expected_type):\n            raise AttributeError(\n                f'data should be of type {expected_type}, but instead, found as {type(data)}')\n\n\nclass Bytes(Encoding):\n    \"\"\"Store bytes (no-op encoding).\"\"\"\n\n    def encode(self, obj: bytes) -> bytes:\n        self._validate(obj, bytes)\n        return obj\n\n    def decode(self, data: bytes) -> bytes:\n        return data\n\n\nclass Str(Encoding):\n    \"\"\"Store UTF-8.\"\"\"\n\n    def encode(self, obj: str) -> bytes:\n        self._validate(obj, str)\n        return obj.encode('utf-8')\n\n    def decode(self, data: bytes) -> str:\n        return data.decode('utf-8')\n\n\nclass Int(Encoding):\n    \"\"\"Store int64.\"\"\"\n\n    size = 8\n\n    def encode(self, obj: int) -> bytes:\n        self._validate(obj, int)\n        return np.int64(obj).tobytes()\n\n    def decode(self, data: bytes) -> int:\n        return int(np.frombuffer(data, np.int64)[0])\n\n\nclass NDArray(Encoding):\n    \"\"\"Store NumPy NDArray.\n\n    The dtype and shape may be either static or dynamic.\n\n    Accordingly, there are four serialized formats:\n      * Static dtype:\n          * Static shape:\n              [values: size * dtype]\n          * Dynamic shape:\n              [ndim | shape dtype: 1] [shape: ndim * shape dtype] [values: size * dtype]\n      * Dynamic dtype:\n          * Static shape:\n              [dtype: 1] [values: size * dtype]\n          * Dynamic shape:\n              [dtype: 1] [ndim | shape dtype: 1] [shape: ndim * shape dtype] [values: size * dtype]\n\n    Args:\n        dtype (str, optional): The dtype, if fixed. Defaults to ``None``.\n        shape (Tuple[int], optional): The shape, if fixed. Defaults to ``None``.\n    \"\"\"\n\n    # Integer <4 -> shape dtype.\n    _int2shape_dtype = {\n        0: 'uint8',\n        1: 'uint16',\n        2: 'uint32',\n        3: 'uint64',\n    }\n\n    # Shape dtype -> integer <4.\n    _shape_dtype2int = {v: k for k, v in _int2shape_dtype.items()}\n\n    # Integer <256 -> value dtype.\n    _int2value_dtype = {\n        8: 'uint8',\n        9: 'int8',\n        16: 'uint16',\n        17: 'int16',\n        18: 'float16',\n        32: 'uint32',\n        33: 'int32',\n        34: 'float32',\n        64: 'uint64',\n        65: 'int64',\n        66: 'float64',\n    }\n\n    # Value dtype -> integer <256.\n    _value_dtype2int = {v: k for k, v in _int2value_dtype.items()}\n\n    @classmethod\n    def _get_static_size(cls, dtype: Optional[str], shape: Optional[tuple[int]]) -> Optional[int]:\n        \"\"\"Get the fixed size of the column in bytes, if applicable.\n\n        Args:\n            dtype (str, optional): The dtype, if fixed.\n            shape (Tuple[int], optional): The shape, if fixed.\n\n        Returns:\n            int: The fixed size in bytes, if there is one.\n        \"\"\"\n        if dtype is None or shape is None:\n            return None\n        return int(np.prod(shape)) * getattr(np, dtype)().nbytes\n\n    def __init__(self, dtype: Optional[str] = None, shape: Optional[tuple[int]] = None):\n        if dtype is not None:\n            assert dtype in self._value_dtype2int\n        if shape is not None:\n            for dim in shape:\n                assert 1 <= dim\n        self.dtype = dtype\n        self.shape = shape\n        self.size = self._get_static_size(dtype, shape)\n\n    @classmethod\n    def from_str(cls, text: str) -> Self:\n        \"\"\"Parse this encoding from string.\n\n        Args:\n            text (str): The string to parse.\n\n        Returns:\n            Self: The initialized Encoding.\n        \"\"\"\n        args = text.split(':') if text else []\n        assert len(args) in {0, 1, 2}\n        if 1 <= len(args):\n            dtype = args[0]\n        else:\n            dtype = None\n        if 2 <= len(args):\n            shape = tuple(map(int, args[1].split(',')))\n        else:\n            shape = None\n        return cls(dtype, shape)\n\n    @classmethod\n    def _rightsize_shape_dtype(cls, shape: npt.NDArray[np.int64]) -> str:\n        \"\"\"Get the smallest unsigned int dtype that will accept the given shape.\n\n        Args:\n            shape (NDArray[np.int64]): The shape.\n\n        Returns:\n            str: The smallest acceptable uint* dtype.\n        \"\"\"\n        if len(shape) == 0:\n            raise ValueError(\n                'Attempting to encode a scalar with NDArray encoding. Please use a scalar encoding.'\n            )\n\n        if shape.min() <= 0:\n            raise ValueError('All dimensions must be greater than zero.')\n        x = shape.max()\n        if x < (1 << 8):\n            return 'uint8'\n        elif x < (1 << 16):\n            return 'uint16'\n        elif x < (1 << 32):\n            return 'uint32'\n        else:\n            return 'uint64'\n\n    def encode(self, obj: npt.NDArray) -> bytes:\n        \"\"\"Encode the given data from the original object to bytes.\n\n        Args:\n            obj (NDArray): Decoded object.\n\n        Returns:\n            bytes: Encoded data.\n        \"\"\"\n        parts = []\n\n        # Encode dtype, if not given in header.\n        dtype_int = self._value_dtype2int.get(obj.dtype.name)\n        if dtype_int is None:\n            raise ValueError(f'Unsupported dtype: {obj.dtype.name}.')\n        if self.dtype is None:\n            part = bytes([dtype_int])\n            parts.append(part)\n        else:\n            if obj.dtype != self.dtype:\n                raise ValueError(f'Wrong dtype: expected {self.dtype}, got {obj.dtype.name}.')\n\n        if obj.size == 0:\n            raise ValueError('Attempting to encode a numpy array with 0 elements.')\n\n        # Encode shape, if not given in header.\n        if self.shape is None:\n            ndim = len(obj.shape)\n            if 64 <= ndim:\n                raise ValueError('Array has too many axes: maximum 63, got {ndim}.')\n            shape_arr = np.array(obj.shape, np.int64)\n            shape_dtype = self._rightsize_shape_dtype(shape_arr)\n            shape_dtype_int = self._shape_dtype2int[shape_dtype]\n            byte = (ndim << 2) | shape_dtype_int\n            part = bytes([byte])\n            parts.append(part)\n            part = shape_arr.astype(shape_dtype).tobytes()\n            parts.append(part)\n        else:\n            if obj.shape != self.shape:\n                raise ValueError('Wrong shape: expected {self.shape}, got {obj.shape}.')\n\n        # Encode the array values.\n        part = obj.tobytes()\n        parts.append(part)\n\n        return b''.join(parts)\n\n    def decode(self, data: bytes) -> npt.NDArray:\n        \"\"\"Decode the given data from bytes to the original object.\n\n        Args:\n            data (bytes): Encoded data.\n\n        Returns:\n            NDArray: Decoded object.\n        \"\"\"\n        index = 0\n\n        # Decode dtype, if not given in header.\n        if self.dtype:\n            dtype = self.dtype\n        else:\n            dtype_int = data[index]\n            index += 1\n            dtype = self._int2value_dtype[dtype_int]\n\n        # Decode shape, if not given in header.\n        if self.shape:\n            shape = self.shape\n        else:\n            byte = data[index]\n            index += 1\n            ndim = byte >> 2\n            shape_dtype_int = byte % 4\n            shape_dtype = self._int2shape_dtype[shape_dtype_int]\n            shape_dtype_nbytes = 2**shape_dtype_int\n            size = ndim * shape_dtype_nbytes\n            shape = np.frombuffer(data[index:index + size], shape_dtype)\n            index += size\n\n        # Decode the array values.\n        arr = np.frombuffer(data[index:], dtype)\n        return arr.reshape(shape)  # pyright: ignore\n\n\nclass Scalar(Encoding):\n    \"\"\"Store scalar.\"\"\"\n\n    def __init__(self, dtype: type) -> None:\n        self.dtype = dtype\n        self.size = self.dtype().nbytes\n\n    def encode(self, obj: Any) -> bytes:\n        return self.dtype(obj).tobytes()\n\n    def decode(self, data: bytes) -> Any:\n        return np.frombuffer(data, self.dtype)[0]\n\n\nclass UInt8(Scalar):\n    \"\"\"Store uint8.\"\"\"\n\n    def __init__(self):\n        super().__init__(np.uint8)\n\n\nclass UInt16(Scalar):\n    \"\"\"Store uint16.\"\"\"\n\n    def __init__(self):\n        super().__init__(np.uint16)\n\n\nclass UInt32(Scalar):\n    \"\"\"Store uint32.\"\"\"\n\n    def __init__(self):\n        super().__init__(np.uint32)\n\n\nclass UInt64(Scalar):\n    \"\"\"Store uint64.\"\"\"\n\n    def __init__(self):\n        super().__init__(np.uint64)\n\n\nclass Int8(Scalar):\n    \"\"\"Store int8.\"\"\"\n\n    def __init__(self):\n        super().__init__(np.int8)\n\n\nclass Int16(Scalar):\n    \"\"\"Store int16.\"\"\"\n\n    def __init__(self):\n        super().__init__(np.int16)\n\n\nclass Int32(Scalar):\n    \"\"\"Store int32.\"\"\"\n\n    def __init__(self):\n        super().__init__(np.int32)\n\n\nclass Int64(Scalar):\n    \"\"\"Store int64.\"\"\"\n\n    def __init__(self):\n        super().__init__(np.int64)\n\n\nclass Float16(Scalar):\n    \"\"\"Store float16.\"\"\"\n\n    def __init__(self):\n        super().__init__(np.float16)\n\n\nclass Float32(Scalar):\n    \"\"\"Store float32.\"\"\"\n\n    def __init__(self):\n        super().__init__(np.float32)\n\n\nclass Float64(Scalar):\n    \"\"\"Store float64.\"\"\"\n\n    def __init__(self):\n        super().__init__(np.float64)\n\n\nclass StrEncoding(Encoding):\n    \"\"\"Base class for stringified types.\n\n    Using variable-length strings allows us to store scalars with arbitrary precision.\n\n    The encode/decode methods of subclasses are the same except for typing specializations.\n    \"\"\"\n\n    pass\n\n\nclass StrInt(StrEncoding):\n    \"\"\"Store int as variable-length digits str.\"\"\"\n\n    def encode(self, obj: int) -> bytes:\n        self._validate(obj, int)\n        return str(obj).encode('utf-8')\n\n    def decode(self, data: bytes) -> int:\n        return int(data.decode('utf-8'))\n\n\nclass StrFloat(Encoding):\n    \"\"\"Store float as variable-length digits str.\"\"\"\n\n    def encode(self, obj: float) -> bytes:\n        self._validate(obj, float)\n        return str(obj).encode('utf-8')\n\n    def decode(self, data: bytes) -> float:\n        return float(data.decode('utf-8'))\n\n\nclass StrDecimal(Encoding):\n    \"\"\"Store decimal as variable-length digits str.\"\"\"\n\n    def encode(self, obj: Decimal) -> bytes:\n        self._validate(obj, Decimal)\n        return str(obj).encode('utf-8')\n\n    def decode(self, data: bytes) -> Decimal:\n        return Decimal(data.decode('utf-8'))\n\n\nclass PIL(Encoding):\n    \"\"\"Store PIL image raw.\n\n    Format: [width: 4] [height: 4] [mode size: 4] [mode] [raw image].\n    \"\"\"\n\n    def encode(self, obj: Image.Image) -> bytes:\n        self._validate(obj, Image.Image)\n        mode = obj.mode.encode('utf-8')\n        width, height = obj.size\n        raw = obj.tobytes()\n        ints = np.array([width, height, len(mode)], np.uint32)\n        return ints.tobytes() + mode + raw\n\n    def decode(self, data: bytes) -> Image.Image:\n        idx = 3 * 4\n        width, height, mode_size = np.frombuffer(data[:idx], np.uint32)\n        idx2 = idx + mode_size\n        mode = data[idx:idx2].decode('utf-8')\n        size = width, height\n        raw = data[idx2:]\n        return Image.frombytes(mode, size, raw)  # pyright: ignore\n\n\nclass JPEG(Encoding):\n    \"\"\"Store PIL image as JPEG. Optionally specify quality.\"\"\"\n\n    def __init__(self, quality: int = 75):\n        if not isinstance(quality, int):\n            raise ValueError('JPEG quality must be an integer')\n        if not (0 <= quality <= 100):\n            raise ValueError('JPEG quality must be between 0 and 100')\n        self.quality = quality\n\n    @classmethod\n    def from_str(cls, config: str) -> Self:\n        \"\"\"Parse this encoding from string.\n\n        Args:\n            text (str): The string to parse.\n\n        Returns:\n            Self: The initialized Encoding.\n        \"\"\"\n        if config == '':\n            return cls()\n        else:\n            return cls(int(config))\n\n    def encode(self, obj: Image.Image) -> bytes:\n        self._validate(obj, Image.Image)\n        if isinstance(obj, JpegImageFile) and hasattr(obj, 'filename'):\n            # read the source file to prevent lossy re-encoding\n            with open(obj.filename, 'rb') as f:\n                return f.read()\n        else:\n            out = BytesIO()\n            obj.save(out, format='JPEG', quality=self.quality)\n            return out.getvalue()\n\n    def decode(self, data: bytes) -> Image.Image:\n        inp = BytesIO(data)\n        return Image.open(inp)\n\n\n# Streaming data type for encoding and decoding list of JPEG images\nclass JPEGList(Encoding):\n    \n    def __init__(self, quality: int = 75):\n        if not isinstance(quality, int):\n            raise ValueError('JPEG quality must be an integer')\n        if not (0 <= quality <= 100):\n            raise ValueError('JPEG quality must be between 0 and 100')\n        self.quality = quality\n\n    @classmethod\n    def from_str(cls, config: str) -> Self:\n        \"\"\"Parse this encoding from string.\n\n        Args:\n            text (str): The string to parse.\n\n        Returns:\n            Self: The initialized Encoding.\n        \"\"\"\n        if config == '':\n            return cls()\n        else:\n            return cls(int(config))\n\n    def encode(self, jpeg_list: List):\n        output = b''\n        for jpeg in jpeg_list:\n            o = BytesIO()\n            jpeg.save(o, format='JPEG',  quality=self.quality)\n            byte = o.getvalue()\n\n            # We need to append the length of the image to the front of the image\n            leng = len(byte)\n            leng = leng.to_bytes(3, byteorder='big')\n            output += leng + byte\n        return output\n    \n    def decode(self, data: bytes):\n        output = []\n        while len(data) > 0:\n            leng = int.from_bytes(data[:3], byteorder='big')\n            data = data[3:]\n            byte = data[:leng]\n            data = data[leng:]\n            output.append(Image.open(BytesIO(byte)))\n        return output\n\nclass PNG(Encoding):\n    \"\"\"Store PIL image as PNG.\"\"\"\n\n    def encode(self, obj: Image.Image) -> bytes:\n        self._validate(obj, Image.Image)\n        out = BytesIO()\n        obj.save(out, format='PNG')\n        return out.getvalue()\n\n    def decode(self, data: bytes) -> Image.Image:\n        inp = BytesIO(data)\n        return Image.open(inp)\n\n\nclass Pickle(Encoding):\n    \"\"\"Store arbitrary data as pickle.\"\"\"\n\n    def encode(self, obj: Any) -> bytes:\n        return pickle.dumps(obj)\n\n    def decode(self, data: bytes) -> Any:\n        return pickle.loads(data)\n\n\nclass JSON(Encoding):\n    \"\"\"Store arbitrary data as JSON.\"\"\"\n\n    def encode(self, obj: Any) -> bytes:\n        if isinstance(obj, np.ndarray):\n            obj = obj.tolist()\n        data = json.dumps(obj)\n        self._is_valid(obj, data)\n        return data.encode('utf-8')\n\n    def decode(self, data: bytes) -> Any:\n        return json.loads(data.decode('utf-8'))\n\n    def _is_valid(self, original: Any, converted: Any) -> None:\n        try:\n            json.loads(converted)\n        except json.decoder.JSONDecodeError as e:\n            e.msg = f'Invalid JSON data: {original}'\n            raise\n\n\n# Encodings (name -> class).\n_encodings = {\n    'bytes': Bytes,\n    'str': Str,\n    'int': Int,\n    'ndarray': NDArray,\n    'uint8': UInt8,\n    'uint16': UInt16,\n    'uint32': UInt32,\n    'uint64': UInt64,\n    'int8': Int8,\n    'int16': Int16,\n    'int32': Int32,\n    'int64': Int64,\n    'float16': Float16,\n    'float32': Float32,\n    'float64': Float64,\n    'str_int': StrInt,\n    'str_float': StrFloat,\n    'str_decimal': StrDecimal,\n    'pil': PIL,\n    'jpeg': JPEG,\n    'png': PNG,\n    'pkl': Pickle,\n    'json': JSON,\n}\n_encodings[\"jpeg_list\"] = JPEGList\n\n_unsafe_encodings = {'pkl'}\n\n\ndef get_mds_encodings() -> set[str]:\n    \"\"\"List supported encodings.\n\n    Returns:\n        Set[str]: Encoding names.\n    \"\"\"\n    return set(_encodings)\n\n\ndef _get_coder(encoding: str) -> Optional[Encoding]:\n    \"\"\"Get an object that encodes/decodes.\n\n    Args:\n        encoding (str): The encoding details.\n\n    Returns:\n        Encoding: The coder.\n    \"\"\"\n    index = encoding.find(':')\n    if index == -1:\n        cls = _encodings.get(encoding)\n        if cls is None:\n            return None\n        return cls()\n    name = encoding[:index]\n    config = encoding[index + 1:]\n    return _encodings[name].from_str(config)\n\n\ndef is_mds_encoding(encoding: str) -> bool:\n    \"\"\"Get whether the given encoding is supported.\n\n    Args:\n        encoding (str): Encoding.\n\n    Returns:\n        bool: Whether the encoding is valid.\n    \"\"\"\n    coder = _get_coder(encoding)\n    return coder is not None\n\n\ndef is_mds_encoding_safe(encoding: str) -> bool:\n    \"\"\"Get whether the given encoding is safe (does not allow arbitrary code execution).\n\n    Args:\n        encoding (str): Encoding.\n\n    Returns:\n        bool: Whether the encoding is safe.\n    \"\"\"\n    return encoding not in _unsafe_encodings\n\n\ndef mds_encode(encoding: str, obj: Any) -> bytes:\n    \"\"\"Encode the given data from the original object to bytes.\n\n    Args:\n        encoding (str): Encoding.\n        obj (Any): Decoded object.\n\n    Returns:\n        bytes: Encoded data.\n    \"\"\"\n    if isinstance(obj, bytes):\n        return obj\n    coder = _get_coder(encoding)\n    if coder is None:\n        raise ValueError(f'Unsupported encoding: {encoding}.')\n    return coder.encode(obj)\n\n\ndef mds_decode(encoding: str, data: bytes) -> Any:\n    \"\"\"Decode the given data from bytes to the original object.\n\n    Args:\n        encoding (str): Encoding.\n        data (bytes): Encoded data.\n\n    Returns:\n        Any: Decoded object.\n    \"\"\"\n    coder = _get_coder(encoding)\n    if coder is None:\n        raise ValueError(f'Unsupported encoding: {encoding}.')\n    return coder.decode(data)\n\n\ndef get_mds_encoded_size(encoding: str) -> Optional[int]:\n    \"\"\"Get the fixed size of all encodings of this type, or None if N/A.\n\n    Args:\n        encoding (str): Encoding.\n\n    Returns:\n        Optional[int]: Size of encoded data.\n    \"\"\"\n    coder = _get_coder(encoding)\n    if coder is None:\n        raise ValueError(f'Unsupported encoding: {encoding}.')\n    return coder.size\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/eva_vit_model.py", "content": "# --------------------------------------------------------\n# Adapted from  https://github.com/microsoft/unilm/tree/master/beit\n# --------------------------------------------------------\nimport math\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from timm.models.layers import drop_path, to_2tuple, trunc_normal_\nexcept:\n    from timm.layers import drop_path, to_2tuple, trunc_normal_\n\nfrom .transformer import PatchDropout\nfrom .rope import VisionRotaryEmbedding, VisionRotaryEmbeddingFast\n\nif os.getenv(\"ENV_TYPE\") == \"deepspeed\":\n    try:\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return \"p={}\".format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        drop=0.0,\n        subln=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.ffn_ln(x)\n\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass SwiGLU(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.SiLU, drop=0.0, norm_layer=nn.LayerNorm, subln=False):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        self.w1 = nn.Linear(in_features, hidden_features)\n        self.w2 = nn.Linear(in_features, hidden_features)\n\n        self.act = act_layer()\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n        self.w3 = nn.Linear(hidden_features, out_features)\n\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x1 = self.w1(x)\n        x2 = self.w2(x)\n        hidden = self.act(x1) * x2\n        x = self.ffn_ln(hidden)\n        x = self.w3(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0, window_size=None, attn_head_dim=None, xattn=False, rope=None, subln=False, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.subln = subln\n        if self.subln:\n            self.q_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.k_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.v_proj = nn.Linear(dim, all_head_dim, bias=False)\n        else:\n            self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.inner_attn_ln = norm_layer(all_head_dim) if subln else nn.Identity()\n        # self.proj = nn.Linear(all_head_dim, all_head_dim)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n\n        self.rope = rope\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        B, N, C = x.shape\n        if self.subln:\n            q = F.linear(input=x, weight=self.q_proj.weight, bias=self.q_bias)\n            k = F.linear(input=x, weight=self.k_proj.weight, bias=None)\n            v = F.linear(input=x, weight=self.v_proj.weight, bias=self.v_bias)\n\n            q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)  # B, num_heads, N, C\n            k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n            v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n        else:\n\n            qkv_bias = None\n            if self.q_bias is not None:\n                qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n\n            qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n            qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)  # 3, B, num_heads, N, C\n            q, k, v = qkv[0], qkv[1], qkv[2]\n\n        if self.rope:\n            # slightly fast impl\n            q_t = q[:, :, 1:, :]\n            ro_q_t = self.rope(q_t)\n            q = torch.cat((q[:, :, :1, :], ro_q_t), -2).type_as(v)\n\n            k_t = k[:, :, 1:, :]\n            ro_k_t = self.rope(k_t)\n            k = torch.cat((k[:, :, :1, :], ro_k_t), -2).type_as(v)\n\n        if self.xattn:\n            q = q.permute(0, 2, 1, 3)  # B, num_heads, N, C -> B, N, num_heads, C\n            k = k.permute(0, 2, 1, 3)\n            v = v.permute(0, 2, 1, 3)\n\n            x = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                p=self.xattn_drop,\n                scale=self.scale,\n            )\n            x = x.reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n\n            if self.relative_position_bias_table is not None:\n                relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0).type_as(attn)\n\n            if rel_pos_bias is not None:\n                attn = attn + rel_pos_bias.type_as(attn)\n\n            if attn_mask is not None:\n                attn_mask = attn_mask.bool()\n                attn = attn.masked_fill(~attn_mask[:, None, None, :], float(\"-inf\"))\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        init_values=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        window_size=None,\n        attn_head_dim=None,\n        xattn=False,\n        rope=None,\n        postnorm=False,\n        subln=False,\n        naiveswiglu=False,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim, xattn=xattn, rope=rope, subln=subln, norm_layer=norm_layer\n        )\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n\n        if naiveswiglu:\n            self.mlp = SwiGLU(\n                in_features=dim,\n                hidden_features=mlp_hidden_dim,\n                subln=subln,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, subln=subln, drop=drop)\n\n        if init_values is not None and init_values > 0:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n        self.postnorm = postnorm\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        if self.gamma_1 is None:\n            if self.postnorm:\n                x = x + self.drop_path(self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            if self.postnorm:\n                x = x + self.drop_path(self.gamma_1 * self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.gamma_2 * self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self):\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass EVAVisionTransformer(nn.Module):\n    \"\"\"Vision Transformer with support for patch or hybrid CNN input stage\"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=1000,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        init_values=None,\n        patch_dropout=0.0,\n        use_abs_pos_emb=True,\n        use_rel_pos_bias=False,\n        use_shared_rel_pos_bias=False,\n        rope=False,\n        use_mean_pooling=True,\n        init_scale=0.001,\n        grad_checkpointing=False,\n        xattn=False,\n        postnorm=False,\n        pt_hw_seq_len=16,\n        intp_freq=False,\n        naiveswiglu=False,\n        subln=False,\n    ):\n        super().__init__()\n        self.image_size = img_size\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        if rope:\n            half_head_dim = embed_dim // num_heads // 2\n            hw_seq_len = img_size // patch_size\n            self.rope = VisionRotaryEmbeddingFast(\n                dim=half_head_dim,\n                pt_seq_len=pt_hw_seq_len,\n                ft_seq_len=hw_seq_len if intp_freq else None,\n                # patch_dropout=patch_dropout\n            )\n        else:\n            self.rope = None\n\n        self.naiveswiglu = naiveswiglu\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    init_values=init_values,\n                    window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None,\n                    xattn=xattn,\n                    rope=self.rope,\n                    postnorm=postnorm,\n                    subln=subln,\n                    naiveswiglu=naiveswiglu,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n        self.head = nn.Linear(embed_dim, num_classes, bias=qkv_bias) if num_classes > 0 else nn.Identity()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=0.02)\n\n        trunc_normal_(self.cls_token, std=0.02)\n\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=0.02)\n            self.head.weight.data.mul_(init_scale)\n            if self.head.bias is not None:\n                self.head.bias.data.mul_(init_scale)\n\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0.0 else nn.Identity()\n\n        self.grad_checkpointing = grad_checkpointing\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            if self.naiveswiglu:\n                rescale(layer.mlp.w3.weight.data, layer_id + 1)\n            else:\n                rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.blocks[0].mlp.fc2.weight.dtype\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=\"\"):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x, return_all_features=False):\n\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        # if os.getenv(\"RoPE\") == \"1\":\n        #     if self.training and not isinstance(self.patch_dropout, nn.Identity):\n        #         x, patch_indices_keep = self.patch_dropout(x)\n        #         self.rope.forward = partial(self.rope.forward, patch_indices_keep=patch_indices_keep)\n        #     else:\n        #         self.rope.forward = partial(self.rope.forward, patch_indices_keep=None)\n        #         x = self.patch_dropout(x)\n        # else:\n        x = self.patch_dropout(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for blk in self.blocks:\n            if self.grad_checkpointing:\n                x = checkpoint(blk, x, (rel_pos_bias,))\n            else:\n                x = blk(x, rel_pos_bias=rel_pos_bias)\n\n        if not return_all_features:\n            x = self.norm(x)\n            if self.fc_norm is not None:\n                return self.fc_norm(x.mean(1))\n            else:\n                return x[:, 0]\n        return x\n\n    def forward(self, x, return_all_features=False):\n        if return_all_features:\n            return self.forward_features(x, return_all_features)\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n"}
{"type": "source_file", "path": "instellavl/model/builder.py", "content": "#    Modification CopyrightÂ© 2025 Advanced Micro Devices, Inc. All rights reserved.\n#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nimport os\nimport warnings\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n\nfrom instellavl.model import *\nfrom instellavl.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom instellavl.utils import rank0_print\n\n\ndef load_pretrained_model(model_path: str,\n                          model_base: str,\n                          model_name: str,\n                          load_8bit: bool = False,\n                          load_4bit: bool = False,\n                          device_map: str = \"auto\",\n                          attn_implementation: str = \"flash_attention_2\",\n                          customized_config: dict = None,\n                          overwrite_config: dict = None,\n                          **kwargs\n    ) -> tuple:\n    r\"\"\"\n    Load a pretrained model with various configurations and options.\n\n    Args:\n        - model_path (str): Path to the pretrained model.\n        - model_base (str): Base model to use for loading LoRA weights or multimodal models.\n        - model_name (str): Name of the model to load.\n        - load_8bit (bool, optional): Whether to load the model in 8-bit precision. Defaults to False.\n        - load_4bit (bool, optional): Whether to load the model in 4-bit precision. Defaults to False.\n        - device_map (str, optional): Device map for model loading. Defaults to \"auto\".\n        - attn_implementation (str, optional): Attention implementation to use. Defaults to \"flash_attention_2\".\n        - customized_config (dict, optional): Custom configuration for the model. Defaults to None.\n        - overwrite_config (dict, optional): Configuration to overwrite the default or customized configuration. Defaults to None.\n        - **kwargs: Additional keyword arguments for model loading.\n\n    Returns:\n        tuple: A tuple containing the tokenizer, model, image processor (if applicable), and context length.\n    \"\"\"\n    kwargs[\"device_map\"] = device_map\n\n    if load_8bit:\n        kwargs[\"load_in_8bit\"] = True\n    elif load_4bit:\n        kwargs[\"load_in_4bit\"] = True\n        kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n    else:\n        kwargs[\"torch_dtype\"] = torch.float16\n\n    if customized_config is not None:\n        kwargs[\"config\"] = customized_config\n\n    if \"multimodal\" in kwargs:\n        if kwargs[\"multimodal\"] is True:\n            is_multimodal = True\n            kwargs.pop(\"multimodal\")\n    else:\n        is_multimodal = False\n\n    if \"instellavl\" in model_name.lower() or is_multimodal:\n        # Load InstellaVL model\n        if \"lora\" in model_name.lower() and model_base is None:\n            warnings.warn(\n                \"There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.\"\n            )\n        if \"lora\" in model_name.lower() and model_base is not None:\n            lora_cfg_pretrained = AutoConfig.from_pretrained(model_path)\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            rank0_print(\"Loading InstellaVL from base model...\")\n            \n            from instellavl.model.language_model.instellavl import InstellaVLConfig\n            \n            lora_cfg_pretrained = InstellaVLConfig.from_pretrained(model_path)\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True)\n            model = InstellaVLForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            \n            token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\n            if model.lm_head.weight.shape[0] != token_num:\n                model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n                model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n\n            rank0_print(\"Loading additional InstellaVL weights...\")\n            if os.path.exists(os.path.join(model_path, \"non_lora_trainables.bin\")):\n                non_lora_trainables = torch.load(os.path.join(model_path, \"non_lora_trainables.bin\"), map_location=\"cpu\")\n            else:\n                # this is probably from HF Hub\n                from huggingface_hub import hf_hub_download\n\n                def load_from_hf(repo_id, filename, subfolder=None):\n                    cache_file = hf_hub_download(repo_id=repo_id, filename=filename, subfolder=subfolder)\n                    return torch.load(cache_file, map_location=\"cpu\")\n\n                non_lora_trainables = load_from_hf(model_path, \"non_lora_trainables.bin\")\n            non_lora_trainables = {(k[11:] if k.startswith(\"base_model.\") else k): v for k, v in non_lora_trainables.items()}\n            if any(k.startswith(\"model.model.\") for k in non_lora_trainables):\n                non_lora_trainables = {(k[6:] if k.startswith(\"model.\") else k): v for k, v in non_lora_trainables.items()}\n            model.load_state_dict(non_lora_trainables, strict=False)\n\n            from peft import PeftModel\n\n            rank0_print(\"Loading LoRA weights...\")\n            model = PeftModel.from_pretrained(model, model_path)\n            rank0_print(\"Merging LoRA weights...\")\n            model = model.merge_and_unload()\n            rank0_print(\"Model is loaded...\")\n        elif model_base is not None:  # this may be mm projector only, loading projector with preset language mdoel\n            rank0_print(f\"\\n\\nLoading InstellaVL from base model {model_base}...\\n\")\n            if \"instellavl\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = InstellaVLForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs) \n            else:\n                raise ValueError(f\"Model {model_name} not supported\")\n\n            mm_projector_weights = torch.load(os.path.join(model_path, \"mm_projector.bin\"), map_location=\"cpu\")\n            mm_projector_weights = {k: v.to(torch.float16) for k, v in mm_projector_weights.items()}\n            model.load_state_dict(mm_projector_weights, strict=False)\n        else:\n            rank0_print(f\"\\n\\nLoading InstellaVL model: {model_path}\\n\\n\")\n            if \"instellavl\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                from instellavl.model.language_model.instellavl import InstellaVLConfig\n                if overwrite_config is not None:\n                    instellavl_cfg = InstellaVLConfig.from_pretrained(model_path)\n                    rank0_print(f\"Overwriting config with {overwrite_config}\")\n                    for k, v in overwrite_config.items():\n                        setattr(instellavl_cfg, k, v)\n                    model = InstellaVLForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=instellavl_cfg, **kwargs)\n                else:\n                    model = InstellaVLForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, **kwargs)\n                rank0_print(f\"\\nModel loaded!!!\\n\")\n\n            else:\n                raise ValueError(f\"Model {model_name} not supported\")\n\n    else:\n        # Load language model\n        if model_base is not None:\n            # PEFT model\n            from peft import PeftModel\n\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            model = AutoModelForCausalLM.from_pretrained(model_base, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\n            print(f\"Loading LoRA weights from {model_path}\")\n            model = PeftModel.from_pretrained(model, model_path)\n            print(f\"Merging weights\")\n            model = model.merge_and_unload()\n            print(\"Convert to FP16...\")\n            model.to(torch.float16)\n        else:\n            if \"mpt\" in model_name.lower().replace(\"prompt\", \"\"):\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\n            else:\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n\n    rank0_print(f\"Model Class: {model.__class__.__name__}\")\n    image_processor = None\n\n    if \"instellavl\" in model_name.lower() or is_multimodal:\n        rank0_print(f\"\\nAdding extra Image Tokens\\n\")\n        mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n        mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\n        if mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n        if mm_use_im_start_end:\n            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n        model.resize_token_embeddings(len(tokenizer))\n\n        vision_tower = model.get_vision_tower()\n        if not vision_tower.is_loaded:\n            vision_tower.load_model(device_map=device_map)\n        if device_map != \"auto\":\n            vision_tower.to(device=\"cuda\", dtype=torch.float16)\n        image_processor = vision_tower.image_processor\n\n    if hasattr(model.config, \"max_sequence_length\"):\n        context_len = model.config.max_sequence_length\n    elif hasattr(model.config, \"max_position_embeddings\"):\n        context_len = model.config.max_position_embeddings\n    elif hasattr(model.config, \"tokenizer_model_max_length\"):\n        context_len = model.config.tokenizer_model_max_length\n    else:\n        context_len = 2048\n\n    return tokenizer, model, image_processor, context_len\n"}
{"type": "source_file", "path": "instellavl/__init__.py", "content": "# from .model import InstellaVLForCausalLM\n"}
{"type": "source_file", "path": "assets/patches/stream.py", "content": "# Copyright 2022-2024 MosaicML Streaming authors\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"A dataset, or sub-dataset if mixing, from which we stream/cache samples.\"\"\"\n\nimport hashlib\nimport json\nimport os\nimport tempfile\nfrom typing import Optional, Sequence\n\nimport numpy as np\nfrom numpy.typing import NDArray\nfrom typing_extensions import Self\n\nfrom streaming.base.compression import decompress\nfrom streaming.base.constant import TICK\nfrom streaming.base.distributed import barrier, get_local_rank\nfrom streaming.base.format import FileInfo, Reader, get_index_basename, reader_from_json\nfrom streaming.base.hashing import get_hash\nfrom streaming.base.registry_utils import create_registry\nfrom streaming.base.storage import CloudDownloader\nfrom streaming.base.util import retry, wait_for_file_to_exist\nfrom streaming.base.world import World\n\n\nclass Stream:\n    \"\"\"A dataset, or sub-dataset if mixing, from which we stream/cache samples.\n\n    We initialize a StreamingDataset with one or more Streams. Streams may be resampled to achieve\n    different mixtures of samples.\n\n    Stream init takes three kinds of arguments:\n\n    * At least one of ``remote`` and ``local`` must exist. If no ``remote``, the data must be\n      local. If no ``local``, we cache to a temp directory.\n\n      * ``remote``\n      * ``local``\n\n    * At most one of ``proportion``, ``repeat``, or ``choose`` may exist. If provided one of these,\n      we derive the rest. Note that ``proportion`` (relative) and ``repeat``/``choose`` (absolute)\n      are mutually incompatible -- you must entirely use one or the other (or neither) for all\n      sub-datasets. If none are provided for all streams and ``epoch_size`` is unspecified, then\n      each sample from each stream is seen once per epoch. If none are provided for all streams\n      and ``epoch_size`` is specified, then streams are sampled in proportion to their size.\n\n      * ``proportion``\n      * ``repeat``\n      * ``choose``\n\n    * The remaining arguments are optional knobs for controlling downloading behavior and default\n      to ``None``. If ``None``, they take a default value provided to or by the StreamingDataset\n      init.\n\n      * ``split``\n      * ``download_retry``\n      * ``download_timeout``\n      * ``validate_hash``\n      * ``keep_zip``\n\n    Args:\n        remote (str, optional): Remote path or directory to download the dataset from. If ``None``,\n            its data must exist locally. Defaults to ``None``.\n        local (str, optional): Local working directory to download shards to. This is where shards\n            are cached while they are being used. Uses a temp directory if not set. Defaults to\n            ``None``.\n        split (str, optional): Which dataset split to use, if any. If provided, we stream from/to\n            the ``split`` subdirs of  ``remote`` and ``local``. Defaults to ``None``.\n        proportion (float, optional): How much to upsample or downsample this sub-dataset, as the\n            proportion of the total combined dataset that consists of this sub-dataset. If\n            using proportions, all sub-datasets provided together to the StreamingDataset init must\n            define their proportions. The total combined number of samples is either the\n            StreamingDataset argument \"epoch_size\" if provided, or kept the same total size as the\n            underlying data if not. If provided, must be non-negative. Defaults to ``None``.\n        repeat (float, optional): How much to upsample or downsample this sub-dataset, as a\n            multipler on the number of samples. If provided, must be non-negative. Defaults to\n            ``None``.\n        choose (int, optional): How much to upsample or downsample this sub-dataset, as the exact\n            number of resulting samples. If provided, must be non-negative. Defaults to ``None``.\n        download_retry (int, optional): Number of download re-attempts before giving up. Defaults\n            to ``None``.\n        download_timeout (float, optional): Number of seconds to wait for a shard to download\n            before raising an exception. Defaults to ``None``.\n        validate_hash (str, optional): Optional hash or checksum algorithm to use to validate\n            shards. Defaults to ``None``.\n        keep_zip (bool, optional): Whether to keep or delete the compressed form when decompressing\n            downloaded shards. If ``False``, keep if and only if remote is local or no remote.\n            Defaults to ``None``.\n    \"\"\"\n\n    def __init__(self,\n                 *,\n                 remote: Optional[str] = None,\n                 local: Optional[str] = None,\n                 split: Optional[str] = None,\n                 proportion: Optional[float] = None,\n                 repeat: Optional[float] = None,\n                 choose: Optional[int] = None,\n                 download_retry: Optional[int] = None,\n                 download_timeout: Optional[float] = None,\n                 validate_hash: Optional[str] = None,\n                 keep_zip: Optional[bool] = None) -> None:\n        self.remote = remote\n        self._local = local\n        self.split = split or ''\n\n        has_proportion = proportion is not None\n        has_repeat = repeat is not None\n        has_choose = choose is not None\n        if not (0 <= has_proportion + has_repeat + has_choose <= 1):\n            raise ValueError('At most one of `proportion`, `repeat`, and `choose` may be ' +\n                             'specified; the others are derived')\n\n        self._proportion = proportion\n        if proportion is not None:\n            if proportion < 0:\n                raise ValueError('`proportion` must be non-negative')\n            self.proportion = proportion\n\n        self._repeat = repeat\n        if repeat is not None:\n            if repeat < 0:\n                raise ValueError('`repeat` must be non-negative')\n            self.repeat = repeat\n\n        self._choose = choose\n        if choose is not None:\n            if choose < 0:\n                raise ValueError('`choose` must be non-negative')\n            self.choose = choose\n\n        self._download_retry = download_retry\n        if download_retry is not None:\n            if download_retry < 0:\n                raise ValueError('`download_retry` must be non-negative')\n            self.download_retry = download_retry\n\n        self._download_timeout = download_timeout\n        if download_timeout is not None:\n            if download_timeout <= 0:\n                raise ValueError('`download_timeout` must be positive')\n            self.download_timeout = download_timeout\n\n        self.validate_hash = validate_hash\n\n        if local is None:\n            self.local = self._get_temporary_directory()\n            if get_local_rank() == 0:\n                if os.path.exists(self.local):\n                    raise FileExistsError(\n                        f'Could not create a temporary local directory {self.local} because it ' +\n                        f'already exists. If you want to reuse the locally cached dataset, ' +\n                        f'explicitly pass in a unique local directory with the `local` argument.' +\n                        f' Otherwise, delete this directory and retry.')\n                os.makedirs(self.local)\n            barrier()\n        else:\n            self.local = local\n\n        self._keep_zip = keep_zip\n        if keep_zip is not None:\n            self.keep_zip = keep_zip\n            self.safe_keep_zip = self.keep_zip or self.remote in {None, self.local}\n\n        self._downloader = CloudDownloader.get(remote)\n\n    def _get_temporary_directory(self) -> str:\n        \"\"\"Construct a path to a temporary directory based on remote and split.\"\"\"\n        root = tempfile.gettempdir()\n        hash = ''\n        if self.remote is not None:\n            hash = hashlib.blake2s(self.remote.encode('utf-8'), digest_size=16).hexdigest()\n        return os.path.join(root, hash, self.split)\n\n    def apply_default(self, default: dict) -> None:\n        \"\"\"Apply defaults, setting any unset fields.\n\n        We use pairs of (name, _name) in order to make type checking happy.\n\n        Args:\n            default (Self): Stream containing default values for all optional fields.\n        \"\"\"\n        if not (self.remote or self._local):\n            raise ValueError('`remote` and/or `local` path must be provided')\n\n        if not self.split:\n            self.split = default['split'] or ''\n        if self._download_retry is None:\n            self.download_retry = default['download_retry']\n        if self._download_timeout is None:\n            self.download_timeout = default['download_timeout']\n        if self.validate_hash is None:\n            self.validate_hash = default['validate_hash'] or None\n        if self._keep_zip is None:\n            self.keep_zip = default['keep_zip']\n            self.safe_keep_zip = default['keep_zip'] or self.remote in {None, self.local}\n\n    @classmethod\n    def validate_weights(cls, streams: Sequence[Self]) -> tuple[bool, bool]:\n        \"\"\"Validate stream weights, returning whether relative or absolute weighting was used.\n\n        Args:\n            streams (Sequence[Stream]): Every stream comprising the dataset.\n\n        Returns:\n            bool: Whether streams are weighted relatively (proportionally).\n        \"\"\"\n        # Validate stream weights (\"proportion\", \"repeat\", \"choose\", or none).\n        is_proportional = hasattr(streams[0], 'proportion')\n        is_unspecified = True\n        for stream_id, stream in enumerate(streams):\n            has_proportion = hasattr(stream, 'proportion')\n            has_repeat = hasattr(stream, 'repeat')\n            has_choose = hasattr(stream, 'choose')\n            if not (0 <= has_proportion + has_repeat + has_choose <= 1):\n                raise ValueError(f'Streams must provide at most one of `proportion`, `repeat`, ' +\n                                 f'or `choose` (error in stream {stream_id})')\n            if is_proportional != has_proportion:\n                raise ValueError(f'Relative (`proportion`) and absolute (`repeat`, `choose`, ' +\n                                 f'none) stream weights are incompatible with each other (error ' +\n                                 f'in stream {stream_id})')\n            if has_proportion or has_repeat or has_choose:\n                is_unspecified = False\n        return is_proportional, is_unspecified\n\n    @classmethod\n    def apply_weights(cls, streams: Sequence[Self], samples_per_stream: NDArray[np.int64],\n                      choose_per_epoch: Optional[int], seed: int) -> int:\n        \"\"\"Given samples per stream, derive each stream's proportion/repeat/samples.\n\n        Modifies streams to save the derived weights.\n\n        Args:\n            streams (Sequence[Stream]): The list of streams which comprise the dataset.\n            samples_per_stream (NDArray[np.int64]): Underlying samples of each stream.\n            choose_per_epoch (int, optional): Absolute epoch size if weighting relatively.\n            seed (int): Random number generator seed used to sample evenly.\n\n        Returns:\n            int: Number of samples to draw per epoch.\n        \"\"\"\n        # Validate provided weights, determining whether they are relative or absolute.\n        are_weights_relative, are_weights_unspecified = cls.validate_weights(streams)\n\n        # Derive weights.\n        if are_weights_relative:\n            # Relative.\n            if not choose_per_epoch:\n                choose_per_epoch = sum(samples_per_stream)\n            proportion_per_stream = np.array([stream.proportion for stream in streams], np.float64)\n            proportion_per_stream /= proportion_per_stream.sum()\n            choose_per_stream = (choose_per_epoch * proportion_per_stream).astype(np.int64)\n            shortfall = choose_per_epoch - choose_per_stream.sum()\n            rng = np.random.default_rng(seed)\n            indices = rng.choice(len(streams), shortfall, False)\n            choose_per_stream[indices] += 1\n            repeat_per_stream = choose_per_stream / samples_per_stream\n        elif are_weights_unspecified and choose_per_epoch:\n            # weights are unspecified, but epoch size (choose_per_epoch) is provided.\n            # sample from each stream in proportion stream's samples\n            proportion_per_stream = samples_per_stream.copy().astype(np.float64)\n            proportion_per_stream /= proportion_per_stream.sum()\n            choose_per_stream = (choose_per_epoch * proportion_per_stream).astype(np.int64)\n            shortfall = choose_per_epoch - choose_per_stream.sum()\n            rng = np.random.default_rng(seed)\n            indices = rng.choice(len(streams), shortfall, False)\n            choose_per_stream[indices] += 1\n            repeat_per_stream = choose_per_stream / samples_per_stream\n        else:\n            # Absolute.\n            if choose_per_epoch:\n                raise ValueError('Only provide `choose` when weighting streams relatively')\n            choose_per_stream = np.zeros(len(streams), np.int64)\n            for stream_id, stream in enumerate(streams):\n                if hasattr(stream, 'repeat'):\n                    choose = int(stream.repeat * samples_per_stream[stream_id])\n                elif hasattr(stream, 'choose'):\n                    choose = stream.choose\n                else:\n                    choose = samples_per_stream[stream_id]\n                choose_per_stream[stream_id] = choose\n            repeat_per_stream = choose_per_stream / samples_per_stream\n            proportion_per_stream = choose_per_stream / choose_per_stream.sum()\n            choose_per_epoch = sum(choose_per_stream)\n\n        # Now that we know the true props/reps/choices, inject those back into the streams.\n        for stream, proportion, repeat, choose in zip(streams, proportion_per_stream,\n                                                      repeat_per_stream, choose_per_stream):\n            stream.proportion = proportion\n            stream.repeat = repeat\n            stream.choose = choose\n\n        return int(choose_per_epoch)\n\n    def _download_file(self, from_basename: str, to_basename: Optional[str] = None) -> str:\n        \"\"\"Safely download a file from remote to local cache.\n\n        Args:\n            from_basename (str): Source basename.\n            to_basename (str, optional): Destination basename, if different.\n\n        Returns:\n            str: Local cache filename.\n        \"\"\"\n        # Calculate paths.\n        if self.remote is None:\n            remote = None\n        else:\n            remote = os.path.join(self.remote, self.split, from_basename)\n        local = os.path.join(self.local, self.split, to_basename or from_basename)\n\n        # Attempt to download, possibly repeating on failure.\n        retry(clean_up_fn=self._downloader.clean_up, num_attempts=self.download_retry)(\n            lambda: self._downloader.download(remote, local, self.download_timeout))()\n\n        return local\n\n    def _decompress_shard_part(self, zip_info: FileInfo, zip_filename: str, raw_filename: str,\n                               compression: Optional[str]) -> None:\n        \"\"\"Validate and decompress shard data.\n\n        Args:\n            zip_info (FileInfo): Compressed file info.\n            zip_filename (str): Compressed filename.\n            raw_filename (str): Decompressed filename.\n            compression (str, optional): Compression algorithm.\n        \"\"\"\n        # Load compressed.\n        data = open(zip_filename, 'rb').read()\n\n        # Validate what was downloaded.\n        if self.validate_hash:\n            if self.validate_hash not in zip_info.hashes:\n                raise ValueError(\n                    f'Hash algorithm `{self.validate_hash}` chosen for data ' +\n                    f'validation does not match with those provided during dataset ' +\n                    f'creation `{sorted(zip_info.hashes.keys())}`. Provide one of those.')\n            if get_hash(self.validate_hash, data) != zip_info.hashes[self.validate_hash]:\n                raise ValueError(f'Checksum failure: {zip_filename}')\n\n        # Decompress and save that.\n        data = decompress(compression, data)  # pyright: ignore\n        tmp_filename = raw_filename + '.tmp'\n        with open(tmp_filename, 'wb') as out:\n            out.write(data)\n        try:\n            os.rename(tmp_filename, raw_filename)\n        except FileNotFoundError:\n            pass\n\n        # Maybe remove compressed to save space.\n        if not self.safe_keep_zip:\n            os.remove(zip_filename)\n\n    def _prepare_shard_part(self,\n                            raw_info: FileInfo,\n                            zip_info: Optional[FileInfo] = None,\n                            compression: Optional[str] = None) -> int:\n        \"\"\"Get shard data given metadata for the raw and compressed versions of it.\n\n        MDS format uses joint shards (ie, one file per shard). Other formats supported by streaming\n        use split shards (ie, shard data lives in two files per shard: the raw data itself and\n        metadata in a separate file).\n\n        Args:\n            raw_info (FileInfo): Raw file info.\n            zip_info (FileInfo, optional): Zip file info. Defaults to ``None``.\n            compression (str, optional): Compression algorithm used for zip_info. Defaults to\n                ``None``.\n\n        Returns:\n            int: Change in cache usage.\n        \"\"\"\n        # Has raw?\n        delta = 0\n        raw_filename = os.path.join(self.local, self.split, raw_info.basename)\n        if os.path.isfile(raw_filename):\n            # Has raw.\n            if zip_info and not self.safe_keep_zip:\n                zip_filename = os.path.join(self.local, self.split, zip_info.basename)\n                if os.path.isfile(zip_filename):\n                    # If don't keep zip and it has a zip, drop the zip.\n                    os.remove(zip_filename)\n                    delta -= zip_info.bytes\n        else:\n            # Missing raw. Uses zip?\n            if zip_info:\n                # Ensure has zip.\n                zip_filename = os.path.join(self.local, self.split, zip_info.basename)\n                if not os.path.isfile(zip_filename):\n                    self._download_file(zip_info.basename)\n                    delta += zip_info.bytes\n\n                # Validate and decompress.\n                self._decompress_shard_part(zip_info, zip_filename, raw_filename, compression)\n                delta += raw_info.bytes\n                if not self.safe_keep_zip:\n                    delta -= zip_info.bytes\n            else:\n                # Download raw.\n                self._download_file(raw_info.basename)\n                delta += raw_info.bytes\n\n                # Validate.\n                if self.validate_hash:\n                    if self.validate_hash not in raw_info.hashes:\n                        raise ValueError(\n                            f'Hash algorithm `{self.validate_hash}` chosen for data ' +\n                            f'validation does not match with those provided during dataset ' +\n                            f'creation `{sorted(raw_info.hashes.keys())}`. Provide one of those.')\n                    data = open(raw_filename, 'rb').read()\n                    if get_hash(self.validate_hash, data) != raw_info.hashes[self.validate_hash]:\n                        raise ValueError(f'Checksum failure: {raw_filename}')\n        return delta\n\n    def prepare_shard(self, shard: Reader) -> int:\n        \"\"\"Ensure (download, validate, extract, etc.) that we have the given shard.\n\n        Args:\n            shard (Reader): Which shard.\n\n        Returns:\n            int: Change in cache usage.\n        \"\"\"\n        delta = 0\n        for raw_info, zip_info in shard.file_pairs:\n            delta += self._prepare_shard_part(raw_info, zip_info, shard.compression)\n        return delta\n\n    def get_shards(self, world: World, allow_unsafe_types: bool) -> list[Reader]:\n        \"\"\"Load this Stream's index, retrieving its shard readers.\n\n        Args:\n            world (World): Distributed context.\n            allow_unsafe_types (bool): If a shard contains Pickle, which allows arbitrary code\n                execution during deserialization, whether to keep going if ``True`` or raise an\n                error.\n\n        Returns:\n            `List[Reader]: Shard readers.\n        \"\"\"\n        # Download the index file if it does not exist locally.\n        basename = get_index_basename()\n        filename = os.path.join(self.local, self.split, basename)  # pyright: ignore\n        if not os.path.exists(filename):\n            if world.is_local_leader:\n                if self.remote:\n                    # Downloads the `index.json` as `index.json.tmp` fully and then rename it to\n                    # `index.json` since only one process downloads the `index.json` file while\n                    # other processes wait for it to get downloaded. Hence, It avoids loading the\n                    # in-progress downloading `index.json`.\n                    tmp_filename = self._download_file(basename, basename + '.tmp')\n                    try:\n                        os.rename(tmp_filename, filename)\n                    except FileNotFoundError:\n                        pass\n                else:\n                    if not os.path.exists(filename):\n                        raise RuntimeError(f'No `remote` provided, but local file {filename} ' +\n                                           'does not exist either')\n            else:\n                wait_for_file_to_exist(\n                    filename, TICK, self.download_timeout,\n                    f'Index file {os.path.join(self.remote or \"\", self.split or \"\", basename)} ' +\n                    f'-> {filename} took too long to download or failed to download. Either increase the '\n                    + f'`download_timeout` value or check the local rank 0 traceback.')\n\n        # Load the index.\n        try:\n            obj = json.load(open(filename))\n        except json.decoder.JSONDecodeError as error:\n            error.args = (f'Index file at {filename} is empty or corrupted. ' + error.args[0],)\n            raise error\n\n        # Version check.\n        if obj['version'] != 2:\n            raise ValueError(f'Unsupported streaming data version: {obj[\"version\"]}. ' +\n                             f'Expected version 2.')\n\n        # Initialize shard readers according to the loaded info.\n        shards = []\n        for info in obj['shards']:\n            shard = reader_from_json(self.local, self.split, info)\n            shard.validate(allow_unsafe_types)\n            shards.append(shard)\n\n        return shards\n\n    def set_up_local(self, shards: list[Reader], cache_usage_per_shard: NDArray[np.int64]) -> None:\n        \"\"\"Bring a local directory into a consistent state, getting which shards are present.\n\n        Args:\n            shards (List[Reader]): List of this stream's shards.\n            cache_usage_per_shard (NDArray[np.int64]): Cache usage per shard of this stream.\n        \"\"\"\n        # List the cache directory (so that we hit the filesystem once).\n        local_dirname = os.path.join(self.local, self.split)\n        listing = set()\n        for dirname, _, subfiles in os.walk(local_dirname):\n            for subfile in subfiles:\n                filename = os.path.join(dirname, subfile)\n                listing.add(filename)\n\n        # Determine which shards are present, making local dir consistent.\n        for i, shard in enumerate(shards):\n            cache_usage_per_shard[i] = shard.set_up_local(listing, self.safe_keep_zip)\n\n    def get_index_size(self) -> int:\n        \"\"\"Get the size of the index file in bytes.\n\n        Returns:\n            int: Size in bytes.\n        \"\"\"\n        filename = os.path.join(self.local, self.split, get_index_basename())\n        return os.stat(filename).st_size\n\n\nstreams_registry = create_registry(\n    'streaming',\n    'streams_registry',\n    generic_type=type[Stream],\n    entry_points=True,\n    description='The streams registry is used for registering Stream classes.')\n\nstreams_registry.register('stream', func=Stream)\n"}
{"type": "source_file", "path": "assets/patches/reader.py", "content": "# Copyright 2022-2024 MosaicML Streaming authors\n# SPDX-License-Identifier: Apache-2.0\n\n\"\"\"Read and decode sample from shards.\"\"\"\n\nimport os\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Any, Iterator, Optional, Union\n\nfrom streaming.base.array import Array\nfrom streaming.base.util import bytes_to_int\n\n__all__ = ['FileInfo', 'Reader', 'JointReader', 'SplitReader']\n\n\n@dataclass\nclass FileInfo(object):\n    \"\"\"File validation info.\n\n    Args:\n        basename (str): File basename.\n        bytes (int): File size in bytes.\n        hashes (Dict[str, str]): Mapping of hash algorithm to hash value.\n    \"\"\"\n    basename: str\n    bytes: int\n    hashes: dict[str, str]\n\n\nclass Reader(Array, ABC):\n    \"\"\"Provides random access to the samples of a shard.\n\n    Args:\n        dirname (str): Local dataset directory.\n        split (str, optional): Which dataset split to use, if any.\n        compression (str, optional): Optional compression or compression:level.\n        hashes (List[str]): Optional list of hash algorithms to apply to shard files.\n        samples (int): Number of samples in this shard.\n        size_limit (Union[int, str], optional): Optional shard size limit, after which\n            point to start a new shard. If None, puts everything in one shard. Can\n            specify bytes in human-readable format as well, for example ``\"100kb\"``\n            for 100 kilobyte (100*1024) and so on.\n    \"\"\"\n\n    def __init__(\n        self,\n        dirname: str,\n        split: Optional[str],\n        compression: Optional[str],\n        hashes: list[str],\n        samples: int,\n        size_limit: Optional[Union[int, str]],\n    ) -> None:\n\n        if size_limit:\n            if (isinstance(size_limit, str)):\n                size_limit = bytes_to_int(size_limit)\n            if size_limit < 0:\n                raise ValueError(f'`size_limit` must be greater than zero, instead, ' +\n                                 f'found as {size_limit}.')\n\n        self.dirname = dirname\n        self.split = split or ''\n        self.compression = compression\n        self.hashes = hashes\n        self.samples = samples\n        self.size_limit = size_limit\n\n        self.file_pairs = []\n\n    def validate(self, allow_unsafe_types: bool) -> None:\n        \"\"\"Check whether this shard is acceptable to be part of some Stream.\n\n        Args:\n            allow_unsafe_types (bool): If a shard contains Pickle, which allows arbitrary code\n                execution during deserialization, whether to keep going if ``True`` or raise an\n                error if ``False``.\n        \"\"\"\n        pass\n\n    @property\n    def size(self):\n        \"\"\"Get the number of samples in this shard.\n\n        Returns:\n            int: Sample count.\n        \"\"\"\n        return self.samples\n\n    def __len__(self) -> int:\n        \"\"\"Get the number of samples in this shard.\n\n        Returns:\n            int: Sample count.\n        \"\"\"\n        return self.samples\n\n    def _evict_raw(self) -> int:\n        \"\"\"Remove all raw files belonging to this shard.\n\n        Returns:\n            int: Bytes evicted from cache.\n        \"\"\"\n        size = 0\n        for raw_info, _ in self.file_pairs:\n            filename = os.path.join(self.dirname, self.split, raw_info.basename)\n            if os.path.exists(filename):\n                try:\n                    os.remove(filename)\n                except FileNotFoundError:\n                    pass\n                size += raw_info.bytes\n        return size\n\n    def _evict_zip(self) -> int:\n        \"\"\"Remove all zip files belonging to this shard.\n\n        Returns:\n            int: Bytes evicted from cache.\n        \"\"\"\n        size = 0\n        for _, zip_info in self.file_pairs:\n            if zip_info:\n                filename = os.path.join(self.dirname, self.split, zip_info.basename)\n                if os.path.exists(filename):\n                    try:\n                        os.remove(filename)\n                    except FileNotFoundError:\n                        pass\n                    size += zip_info.bytes\n        return size\n\n    def evict(self) -> int:\n        \"\"\"Remove all files belonging to this shard.\n\n        Returns:\n            int: Bytes evicted from cache.\n        \"\"\"\n        return self._evict_raw() + self._evict_zip()\n\n    def set_up_local(self, listing: set[str], safe_keep_zip: bool) -> int:\n        \"\"\"Bring what shard files are present to a consistent state, returning whether present.\n\n        Args:\n            listing (Set[str]): The listing of all files under dirname/[split/]. This is listed\n                once and then saved because there could potentially be very many shard files.\n            safe_keep_zip (bool): Whether to keep zip files when decompressing. Possible when\n                compression was used. Necessary when local is the remote or there is no remote.\n\n        Returns:\n            bool: Whether the shard is present.\n        \"\"\"\n        # For raw/zip to be considered present, each raw/zip file must be present.\n        raw_files_present = 0\n        zip_files_present = 0\n        for raw_info, zip_info in self.file_pairs:\n            if raw_info:\n                filename = os.path.join(self.dirname, self.split, raw_info.basename)\n                if filename in listing:\n                    raw_files_present += 1\n            if zip_info:\n                filename = os.path.join(self.dirname, self.split, zip_info.basename)\n                if filename in listing:\n                    zip_files_present += 1\n\n        # If the shard raw files are partially present, garbage collect the present ones and mark\n        # the shard raw as not present, in order to achieve consistency.\n        if not raw_files_present:\n            has_raw = False\n        elif raw_files_present < len(self.file_pairs):\n            has_raw = False\n            self._evict_raw()\n        else:\n            has_raw = True\n\n        # Same as the above, but for shard zip files.\n        if not zip_files_present:\n            has_zip = False\n        elif zip_files_present < len(self.file_pairs):\n            has_zip = False\n            self._evict_zip()\n        else:\n            has_zip = True\n\n        # Enumerate cases of raw/zip presence.\n        if self.compression:\n            if safe_keep_zip:\n                if has_raw:\n                    if has_zip:\n                        # Present (normalized).\n                        pass\n                    else:\n                        # Missing: there is no natural way to arrive at this state, so drop raw.\n                        has_raw = False\n                        self._evict_raw()\n                else:\n                    if has_zip:\n                        # Present: but missing raw, so need to decompress upon use.\n                        pass\n                    else:\n                        # Missing (normalized).\n                        pass\n            else:\n                if has_raw:\n                    if has_zip:\n                        # Present: zip is unnecessary, so evict it.\n                        has_zip = False\n                        self._evict_raw()\n                    else:\n                        # Present (normalized).\n                        pass\n                else:\n                    if has_zip:\n                        # Present: but missing raw, so need to decompress and evict zip upon use.\n                        pass\n                    else:\n                        # Missing (normalized).\n                        pass\n        else:\n            if has_zip:\n                raise ValueError('Shard is invalid: compression was not used, but has a ' +\n                                 'compressed form.')\n\n        # Get cache usage. Shard is present if either raw or zip are present.\n        size = 0\n        if has_raw:\n            size += self.get_raw_size()\n        if has_zip:\n            size += self.get_zip_size() or 0\n        return size\n\n    def get_raw_size(self) -> int:\n        \"\"\"Get the raw (uncompressed) size of this shard.\n\n        Returns:\n            int: Size in bytes.\n        \"\"\"\n        size = 0\n        for info, _ in self.file_pairs:\n            size += info.bytes\n        return size\n\n    def get_zip_size(self) -> Optional[int]:\n        \"\"\"Get the zip (compressed) size of this shard, if compression was used.\n\n        Returns:\n            Optional[int]: Size in bytes, or ``None`` if does not exist.\n        \"\"\"\n        size = 0\n        for _, info in self.file_pairs:\n            if info is None:\n                return None\n            size += info.bytes\n        return size\n\n    def get_max_size(self) -> int:\n        \"\"\"Get the full size of this shard.\n\n        \"Max\" in this case means both the raw (decompressed) and zip (compressed) versions are\n        resident (assuming it has a zip form). This is the maximum disk usage the shard can reach.\n        When compressed was used, even if keep_zip is ``False``, the zip form must still be\n        resident at the same time as the raw form during shard decompression.\n\n        Returns:\n            int: Size in bytes.\n        \"\"\"\n        return self.get_raw_size() + (self.get_zip_size() or 0)\n\n    def get_persistent_size(self, keep_zip: bool) -> int:\n        \"\"\"Get the persistent size of this shard.\n\n        \"Persistent\" in this case means whether both raw and zip are present is subject to\n        keep_zip. If we are not keeping zip files after decompression, they don't count to the\n        shard's persistent size on disk.\n\n        Args:\n            keep_zip (bool): Whether to keep zip files after decompressing.\n\n        Returns:\n            int: Size in bytes.\n        \"\"\"\n        if self.compression:\n            if keep_zip:\n                size = self.get_max_size()\n            else:\n                size = self.get_raw_size()\n        else:\n            size = self.get_raw_size()\n        return size\n\n    @abstractmethod\n    def decode_sample(self, data: bytes) -> dict[str, Any]:\n        \"\"\"Decode a sample dict from bytes.\n\n        Args:\n            data (bytes): The sample encoded as bytes.\n\n        Returns:\n            Dict[str, Any]: Sample dict.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_sample_data(self, idx: int) -> bytes:\n        \"\"\"Get the raw sample data at the index.\n\n        Args:\n            idx (int): Sample index.\n\n        Returns:\n            bytes: Sample data.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_item(self, idx: int) -> dict[str, Any]:\n        \"\"\"Get the sample at the index.\n\n        Args:\n            idx (int): Sample index.\n\n        Returns:\n            Dict[str, Any]: Sample dict.\n        \"\"\"\n        data = self.get_sample_data(idx)\n        return self.decode_sample(data)\n\n    def __iter__(self) -> Iterator[dict[str, Any]]:\n        \"\"\"Iterate over the samples of this shard.\n\n        Returns:\n            Iterator[Dict[str, Any]]: Iterator over samples.\n        \"\"\"\n        for i in range(len(self)):\n            yield self[i]\n\n\nclass JointReader(Reader):\n    \"\"\"Provides random access to the samples of a joint shard.\n\n    Args:\n        dirname (str): Local dataset directory.\n        split (str, optional): Which dataset split to use, if any.\n        compression (str, optional): Optional compression or compression:level.\n        hashes (List[str]): Optional list of hash algorithms to apply to shard files.\n        raw_data (FileInfo): Uncompressed data file info.\n        samples (int): Number of samples in this shard.\n        size_limit (Union[int, str], optional): Optional shard size limit, after which\n        point to start a new shard. If None, puts everything in one shard.\n        zip_data (FileInfo, optional): Compressed data file info.\n    \"\"\"\n\n    def __init__(\n        self,\n        dirname: str,\n        split: Optional[str],\n        compression: Optional[str],\n        hashes: list[str],\n        raw_data: FileInfo,\n        samples: int,\n        size_limit: Optional[Union[int, str]],\n        zip_data: Optional[FileInfo],\n    ) -> None:\n        super().__init__(dirname, split, compression, hashes, samples, size_limit)\n        self.raw_data = raw_data\n        self.zip_data = zip_data\n        self.file_pairs.append((raw_data, zip_data))\n\n\nclass SplitReader(Reader):\n    \"\"\"Provides random access to the samples of a split shard.\n\n    Args:\n        dirname (str): Local dataset directory.\n        split (str, optional): Which dataset split to use, if any.\n        compression (str, optional): Optional compression or compression:level.\n        hashes (List[str]): Optional list of hash algorithms to apply to shard files.\n        raw_data (FileInfo): Uncompressed data file info.\n        raw_meta (FileInfo): Uncompressed meta file info.\n        samples (int): Number of samples in this shard.\n        size_limit (Union[int, str], optional): Optional shard size limit, after which\n            point to start a new shard. If None, puts everything in one shard.\n        zip_data (FileInfo, optional): Compressed data file info.\n        zip_meta (FileInfo, optional): Compressed meta file info.\n    \"\"\"\n\n    def __init__(\n        self,\n        dirname: str,\n        split: Optional[str],\n        compression: Optional[str],\n        hashes: list[str],\n        raw_data: FileInfo,\n        raw_meta: FileInfo,\n        samples: int,\n        size_limit: Optional[Union[int, str]],\n        zip_data: Optional[FileInfo],\n        zip_meta: Optional[FileInfo],\n    ) -> None:\n        super().__init__(dirname, split, compression, hashes, samples, size_limit)\n        self.raw_data = raw_data\n        self.raw_meta = raw_meta\n        self.zip_data = zip_data\n        self.zip_meta = zip_meta\n        self.file_pairs.append((raw_meta, zip_meta))\n        self.file_pairs.append((raw_data, zip_data))\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/loss.py", "content": "import math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ntry:\n    import torch.distributed.nn\n    from torch import distributed as dist\n\n    has_distributed = True\nexcept ImportError:\n    has_distributed = False\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    hvd = None\n\nfrom timm.loss import LabelSmoothingCrossEntropy\n\n\ndef gather_features(image_features, text_features, local_loss=False, gather_with_grad=False, rank=0, world_size=1, use_horovod=False):\n    assert has_distributed, \"torch.distributed did not import correctly, please use a PyTorch version with support.\"\n    if use_horovod:\n        assert hvd is not None, \"Please install horovod\"\n        if gather_with_grad:\n            all_image_features = hvd.allgather(image_features)\n            all_text_features = hvd.allgather(text_features)\n        else:\n            with torch.no_grad():\n                all_image_features = hvd.allgather(image_features)\n                all_text_features = hvd.allgather(text_features)\n            if not local_loss:\n                # ensure grads for local rank when all_* features don't have a gradient\n                gathered_image_features = list(all_image_features.chunk(world_size, dim=0))\n                gathered_text_features = list(all_text_features.chunk(world_size, dim=0))\n                gathered_image_features[rank] = image_features\n                gathered_text_features[rank] = text_features\n                all_image_features = torch.cat(gathered_image_features, dim=0)\n                all_text_features = torch.cat(gathered_text_features, dim=0)\n    else:\n        # We gather tensors from all gpus\n        if gather_with_grad:\n            all_image_features = torch.cat(torch.distributed.nn.all_gather(image_features), dim=0)\n            all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)\n            # all_image_features = torch.cat(torch.distributed.nn.all_gather(image_features, async_op=True), dim=0)\n            # all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features, async_op=True), dim=0)\n        else:\n            gathered_image_features = [torch.zeros_like(image_features) for _ in range(world_size)]\n            gathered_text_features = [torch.zeros_like(text_features) for _ in range(world_size)]\n            dist.all_gather(gathered_image_features, image_features)\n            dist.all_gather(gathered_text_features, text_features)\n            if not local_loss:\n                # ensure grads for local rank when all_* features don't have a gradient\n                gathered_image_features[rank] = image_features\n                gathered_text_features[rank] = text_features\n            all_image_features = torch.cat(gathered_image_features, dim=0)\n            all_text_features = torch.cat(gathered_text_features, dim=0)\n\n    return all_image_features, all_text_features\n\n\nclass ClipLoss(nn.Module):\n\n    def __init__(\n        self,\n        local_loss=False,\n        gather_with_grad=False,\n        cache_labels=False,\n        rank=0,\n        world_size=1,\n        use_horovod=False,\n        smoothing=0.0,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.rank = rank\n        self.world_size = world_size\n        self.use_horovod = use_horovod\n        self.label_smoothing_cross_entropy = LabelSmoothingCrossEntropy(smoothing=smoothing) if smoothing > 0 else None\n\n        # cache state\n        self.prev_num_logits = 0\n        self.labels = {}\n\n    def forward(self, image_features, text_features, logit_scale=1.0):\n        device = image_features.device\n        if self.world_size > 1:\n            all_image_features, all_text_features = gather_features(image_features, text_features, self.local_loss, self.gather_with_grad, self.rank, self.world_size, self.use_horovod)\n\n            if self.local_loss:\n                logits_per_image = logit_scale * image_features @ all_text_features.T\n                logits_per_text = logit_scale * text_features @ all_image_features.T\n            else:\n                logits_per_image = logit_scale * all_image_features @ all_text_features.T\n                logits_per_text = logits_per_image.T\n        else:\n            logits_per_image = logit_scale * image_features @ text_features.T\n            logits_per_text = logit_scale * text_features @ image_features.T\n        # calculated ground-truth and cache if enabled\n        num_logits = logits_per_image.shape[0]\n        if self.prev_num_logits != num_logits or device not in self.labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n            if self.world_size > 1 and self.local_loss:\n                labels = labels + num_logits * self.rank\n            if self.cache_labels:\n                self.labels[device] = labels\n                self.prev_num_logits = num_logits\n        else:\n            labels = self.labels[device]\n\n        if self.label_smoothing_cross_entropy:\n            total_loss = (self.label_smoothing_cross_entropy(logits_per_image, labels) + self.label_smoothing_cross_entropy(logits_per_text, labels)) / 2\n        else:\n            total_loss = (F.cross_entropy(logits_per_image, labels) + F.cross_entropy(logits_per_text, labels)) / 2\n\n        acc = None\n        i2t_acc = (logits_per_image.argmax(-1) == labels).sum() / len(logits_per_image)\n        t2i_acc = (logits_per_text.argmax(-1) == labels).sum() / len(logits_per_text)\n        acc = {\"i2t\": i2t_acc, \"t2i\": t2i_acc}\n        return total_loss, acc\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/hf_model.py", "content": "\"\"\" huggingface model adapter\n\nWraps HuggingFace transformers (https://github.com/huggingface/transformers) models for use as a text tower in CLIP model.\n\"\"\"\n\nimport re\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch import TensorType\n\ntry:\n    import transformers\n    from transformers import AutoModel, AutoModelForMaskedLM, AutoTokenizer, AutoConfig, PretrainedConfig\n    from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, BaseModelOutputWithPoolingAndCrossAttentions\nexcept ImportError as e:\n    transformers = None\n\n    class BaseModelOutput:\n        pass\n\n    class PretrainedConfig:\n        pass\n\n\nfrom .hf_configs import arch_dict\n\n\n# utils\ndef _camel2snake(s):\n    return re.sub(r\"(?<!^)(?=[A-Z])\", \"_\", s).lower()\n\n\n# TODO: ?last - for gpt-like models\n_POOLERS = {}\n\n\ndef register_pooler(cls):\n    \"\"\"Decorator registering pooler class\"\"\"\n    _POOLERS[_camel2snake(cls.__name__)] = cls\n    return cls\n\n\n@register_pooler\nclass MeanPooler(nn.Module):\n    \"\"\"Mean pooling\"\"\"\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n        masked_output = x.last_hidden_state * attention_mask.unsqueeze(-1)\n        return masked_output.sum(dim=1) / attention_mask.sum(-1, keepdim=True)\n\n\n@register_pooler\nclass MaxPooler(nn.Module):\n    \"\"\"Max pooling\"\"\"\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n        masked_output = x.last_hidden_state.masked_fill(attention_mask.unsqueeze(-1), -torch.inf)\n        return masked_output.max(1).values\n\n\n@register_pooler\nclass ClsPooler(nn.Module):\n    \"\"\"CLS token pooling\"\"\"\n\n    def __init__(self, use_pooler_output=True):\n        super().__init__()\n        self.cls_token_position = 0\n        self.use_pooler_output = use_pooler_output\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n\n        if self.use_pooler_output and isinstance(x, (BaseModelOutputWithPooling, BaseModelOutputWithPoolingAndCrossAttentions)) and (x.pooler_output is not None):\n            return x.pooler_output\n\n        return x.last_hidden_state[:, self.cls_token_position, :]\n\n\nclass HFTextEncoder(nn.Module):\n    \"\"\"HuggingFace model adapter\"\"\"\n\n    def __init__(self, model_name_or_path: str, output_dim: int, tokenizer_name: str = None, config: PretrainedConfig = None, pooler_type: str = None, proj: str = None, pretrained: bool = True, masked_language_modeling: bool = False):\n        super().__init__()\n\n        self.output_dim = output_dim\n\n        # TODO: find better way to get this information\n        uses_transformer_pooler = pooler_type == \"cls_pooler\"\n\n        if transformers is None:\n            raise RuntimeError(\"Please `pip install transformers` to use pre-trained HuggingFace models\")\n        if config is None:\n            self.config = AutoConfig.from_pretrained(model_name_or_path)\n            if masked_language_modeling:\n                create_func, model_args = (AutoModelForMaskedLM.from_pretrained, model_name_or_path) if pretrained else (AutoModelForMaskedLM.from_config, self.config)\n            else:\n                create_func, model_args = (AutoModel.from_pretrained, model_name_or_path) if pretrained else (AutoModel.from_config, self.config)\n            # TODO: do all model configs have this attribute? PretrainedConfig does so yes??\n            if hasattr(self.config, \"is_encoder_decoder\") and self.config.is_encoder_decoder:\n                self.transformer = create_func(model_args)\n                self.transformer = self.transformer.encoder\n            else:\n                self.transformer = create_func(model_args, add_pooling_layer=uses_transformer_pooler)\n        else:\n            self.config = config\n            if masked_language_modeling:\n                self.transformer = AutoModelForMaskedLM.from_config(config)\n            else:\n                self.transformer = AutoModel.from_config(config)\n\n        if pooler_type is None:  # get default arch pooler\n            self.pooler = _POOLERS[(arch_dict[self.config.model_type][\"pooler\"])]()\n        else:\n            self.pooler = _POOLERS[pooler_type]()\n\n        d_model = getattr(self.config, arch_dict[self.config.model_type][\"config_names\"][\"width\"])\n        if (d_model == output_dim) and (proj is None):  # do we always need a proj?\n            self.proj = nn.Identity()\n        elif proj == \"linear\":\n            self.proj = nn.Linear(d_model, output_dim, bias=False)\n        elif proj == \"mlp\":\n            hidden_size = (d_model + output_dim) // 2\n            self.proj = nn.Sequential(\n                nn.Linear(d_model, hidden_size, bias=False),\n                nn.GELU(),\n                nn.Linear(hidden_size, output_dim, bias=False),\n            )\n\n        # self.itm_proj = nn.Linear(d_model, 2, bias=False)\n        # self.mlm_proj = nn.Linear(d_model, self.config.vocab_size), bias=False)\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\n    # def forward_itm(self, x:TensorType, image_embeds:TensorType) -> TensorType:\n    #     image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(x.device)\n    #     attn_mask = (x != self.config.pad_token_id).long()\n    #     out = self.transformer(\n    #         input_ids=x,\n    #         attention_mask=attn_mask,\n    #         encoder_hidden_states = image_embeds,\n    #         encoder_attention_mask = image_atts,\n    #         )\n    #     pooled_out = self.pooler(out, attn_mask)\n\n    #     return self.itm_proj(pooled_out)\n\n    def mask(self, input_ids, vocab_size, device, targets=None, masked_indices=None, probability_matrix=None):\n        if masked_indices is None:\n            masked_indices = torch.bernoulli(probability_matrix).bool()\n\n        masked_indices[input_ids == self.tokenizer.pad_token_id] = False\n        masked_indices[input_ids == self.tokenizer.cls_token_id] = False\n\n        if targets is not None:\n            targets[~masked_indices] = -100  # We only compute loss on masked tokens\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool() & masked_indices\n        input_ids[indices_replaced] = self.tokenizer.mask_token_id\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(input_ids.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(vocab_size, input_ids.shape, dtype=torch.long).to(device)\n        input_ids[indices_random] = random_words[indices_random]\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n\n        if targets is not None:\n            return input_ids, targets\n        else:\n            return input_ids\n\n    def forward_mlm(self, input_ids, image_embeds, mlm_probability=0.25):\n        labels = input_ids.clone()\n        attn_mask = (input_ids != self.config.pad_token_id).long()\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(input_ids.device)\n        vocab_size = getattr(self.config, arch_dict[self.config.model_type][\"config_names\"][\"vocab_size\"])\n        probability_matrix = torch.full(labels.shape, mlm_probability)\n        input_ids, labels = self.mask(input_ids, vocab_size, input_ids.device, targets=labels, probability_matrix=probability_matrix)\n        mlm_output = self.transformer(\n            input_ids,\n            attention_mask=attn_mask,\n            encoder_hidden_states=image_embeds,\n            encoder_attention_mask=image_atts,\n            return_dict=True,\n            labels=labels,\n        )\n        return mlm_output.loss\n        # mlm_output = self.transformer(input_ids,\n        #                 attention_mask = attn_mask,\n        #                 encoder_hidden_states = image_embeds,\n        #                 encoder_attention_mask = image_atts,\n        #                 return_dict = True,\n        #             ).last_hidden_state\n        # logits = self.mlm_proj(mlm_output)\n\n        # # logits = logits[:, :-1, :].contiguous().view(-1, vocab_size)\n        # logits = logits[:, 1:, :].contiguous().view(-1, vocab_size)\n        # labels = labels[:, 1:].contiguous().view(-1)\n\n        # mlm_loss = F.cross_entropy(\n        #     logits,\n        #     labels,\n        #     # label_smoothing=0.1,\n        # )\n        # return mlm_loss\n\n    def forward(self, x: TensorType) -> TensorType:\n        attn_mask = (x != self.config.pad_token_id).long()\n        out = self.transformer(input_ids=x, attention_mask=attn_mask)\n        pooled_out = self.pooler(out, attn_mask)\n\n        return self.proj(pooled_out)\n\n    def lock(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):\n        if not unlocked_layers:  # full freezing\n            for n, p in self.transformer.named_parameters():\n                p.requires_grad = (not freeze_layer_norm) if \"LayerNorm\" in n.split(\".\") else False\n            return\n\n        encoder = self.transformer.encoder if hasattr(self.transformer, \"encoder\") else self.transformer\n        layer_list = getattr(encoder, arch_dict[self.config.model_type][\"config_names\"][\"layer_attr\"])\n        print(f\"Unlocking {unlocked_layers}/{len(layer_list) + 1} layers of hf model\")\n        embeddings = getattr(self.transformer, arch_dict[self.config.model_type][\"config_names\"][\"token_embeddings_attr\"])\n        modules = [embeddings, *layer_list][:-unlocked_layers]\n        # freeze layers\n        for module in modules:\n            for n, p in module.named_parameters():\n                p.requires_grad = (not freeze_layer_norm) if \"LayerNorm\" in n.split(\".\") else False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.gradient_checkpointing_enable()\n\n    def get_num_layers(self):\n        encoder = self.transformer.encoder if hasattr(self.transformer, \"encoder\") else self.transformer\n        layer_list = getattr(encoder, arch_dict[self.config.model_type][\"config_names\"][\"layer_attr\"])\n        return len(layer_list)\n\n    def init_parameters(self):\n        pass\n"}
{"type": "source_file", "path": "instellavl/model/language_model/instellavl.py", "content": "#    Modification CopyrightÂ© 2025 Advanced Micro Devices, Inc. All rights reserved.\n#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nimport torch\nimport torch.nn as nn\n\nfrom typing import List, Optional, Tuple, Union\n\nfrom transformers import (AutoConfig, AutoModelForCausalLM,\n                          OlmoConfig, OlmoModel, OlmoForCausalLM)\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom instellavl.model.instellavl_arch import InstellaVLMetaModel, InstellaVLMetaForCausalLM\n\n\nclass InstellaVLConfig(OlmoConfig):\n    \"\"\"\n    Configuration class for the InstellaVL model.\n    Attributes:\n        model_type (str): The type of the model, set to \"instellavl\".\n    \"\"\"\n\n    model_type = \"instellavl\"\n\n\nclass InstellaVLModel(InstellaVLMetaModel, OlmoModel):\n    config_class = InstellaVLConfig\n\n    def __init__(self, config: OlmoConfig):\n        super(InstellaVLModel, self).__init__(config)\n\n\nclass InstellaVLForCausalLM(OlmoForCausalLM, InstellaVLMetaForCausalLM):\n    r\"\"\"\n    InstellaVLForCausalLM is a class that extends OlmoForCausalLM and InstellaVLMetaForCausalLM to provide\n    a language model with multimodal capabilities, specifically for handling images along with text.\n    \n    1. Attributes:\n        - config_class (type): The configuration class to use for this model.\n        - model (InstellaVLModel): The underlying model.\n        - lm_head (nn.Linear): The linear layer for language modeling head.\n    \n    2. Methods:\n        \n        1. `__init__(config: InstellaVLConfig)`:\n            Initializes the InstellaVLForCausalLM model with the given configuration.\n\n        2. `get_model() -> InstellaVLModel`:\n            Returns the underlying model.\n\n        3. `forward() -> Union[Tuple, CausalLMOutputWithPast]`:\n            Performs a forward pass through the model.\n            \n        4. `generate() -> Union[GenerateOutput, torch.LongTensor]`:\n            Generates text based on the input.\n            \n        5. `prepare_inputs_for_generation(input_ids: torch.LongTensor,) -> dict`:\n            Prepares inputs for text generation.\n            \n    \"\"\"\n\n    config_class = InstellaVLConfig\n\n    def __init__(self, config: OlmoConfig):\n        r\"\"\"\n        Initializes the InstellaVLForCausalLM model.\n\n        Args:\n            - config (OlmoConfig): Configuration object for the model.\n\n        Attributes:\n            - model (InstellaVLModel): The main model instance.\n            - lm_head (torch.nn.Linear): Linear layer that maps hidden states to vocabulary size.\n        \"\"\"\n        super(OlmoForCausalLM, self).__init__(config)\n        config.model_type = \"instellavl\"\n        self.model = InstellaVLModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            - input_ids (torch.LongTensor, optional): Input token IDs.\n            - attention_mask (torch.Tensor, optional): Attention mask.\n            - position_ids (torch.LongTensor, optional): Position IDs.\n            - past_key_values (List[torch.FloatTensor], optional): Past key values for caching.\n            - inputs_embeds (torch.FloatTensor, optional): Input embeddings.\n            - labels (torch.LongTensor, optional): Labels for language modeling.\n            - use_cache (bool, optional): Whether to use cache.\n            - output_attentions (bool, optional): Whether to output attentions.\n            - output_hidden_states (bool, optional): Whether to output hidden states.\n            - images (torch.FloatTensor, optional): Input images.\n            - image_sizes (List[List[int]], optional): Sizes of input images.\n            - return_dict (bool, optional): Whether to return a dictionary.\n            - modalities (List[str], optional): List of modalities.\n            - cache_position (optional): Cache position.\n        \n        Returns:\n            Union[Tuple, CausalLMOutputWithPast]: The output of the forward pass.\n        \"\"\"\n        if inputs_embeds is None:\n            (\n                input_ids,\n                position_ids,\n                attention_mask,\n                past_key_values,\n                inputs_embeds,\n                labels\n            ) = self.prepare_inputs_labels_for_multimodal(\n                input_ids,\n                position_ids,\n                attention_mask,\n                past_key_values,\n                labels,\n                images,\n                modalities,\n                image_sizes\n            )\n\n        return super().forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        r\"\"\"\n        Args:\n            - inputs (torch.Tensor, optional): Input tensor.\n            - images (torch.Tensor, optional): Input images.\n            - image_sizes (torch.Tensor, optional): Sizes of input images.\n            - modalities (List[str], optional): List of modalities.\n            - **kwargs: Additional arguments.\n        \n        Returns:\n            Union[GenerateOutput, torch.LongTensor]: The generated text.\n        \"\"\"\n        modalities = kwargs.pop(\"modalities\", None) if \"modalities\" in kwargs and modalities is None else modalities\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (\n                inputs,\n                position_ids,\n                attention_mask,\n                _,\n                inputs_embeds,\n                _\n            ) = self.prepare_inputs_labels_for_multimodal(\n                inputs,\n                position_ids,\n                attention_mask,\n                None,\n                None,\n                images,\n                image_sizes=image_sizes\n            )\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n        return super().generate(\n            position_ids=position_ids,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            **kwargs\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None,\n                                      inputs_embeds=None, **kwargs):\n        r\"\"\"\n        Args:\n            - input_ids (torch.LongTensor): Input token IDs.\n            - past_key_values (List[torch.FloatTensor], optional): Past key values for caching.\n            - inputs_embeds (torch.FloatTensor, optional): Input embeddings.\n            - **kwargs: Additional arguments.\n        \n        Returns:\n            dict: Prepared inputs for generation.\n        \"\"\"\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(\n            input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs\n        )\n        if images is not None:\n            inputs['images'] = images\n        if image_sizes is not None:\n            inputs['image_sizes'] = image_sizes\n        return inputs\n\nAutoConfig.register(\"instellavl\", InstellaVLConfig)\nAutoModelForCausalLM.register(InstellaVLConfig, InstellaVLForCausalLM)\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/clip_encoder.py", "content": "import torch\nimport torch.nn as nn\n\nfrom instellavl.utils import rank0_print\nfrom transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n\ntry:\n    from s2wrapper import forward as multiscale_forward\nexcept:\n    pass\n\n\nclass CLIPVisionTower(nn.Module):\n    r\"\"\"\n    A class to represent the CLIP Vision Tower model.\n    \n    Attributes :\n    ------------\n        - is_loaded (bool): A flag indicating whether the model is loaded.\n        - vision_tower_name (str): The name of the vision tower model.\n        - select_layer (int): The layer to select features from.\n        - select_feature (str): The type of feature to select.\n\n    Methods :\n    ------------\n        - `__init__(vision_tower: str, args: Namespace, delay_load: bool = False)`: Initializes the CLIPVisionTower with the given vision tower name and arguments.\n        - `load_model(device_map: Optional[dict] = None)`: Loads the vision tower model and image processor.\n        - `feature_select(image_forward_outs: Any) -> torch.Tensor`: Selects features from the image forward outputs based on the specified feature type.\n        - `forward(images: Union[torch.Tensor, List[torch.Tensor]]) -> torch.Tensor`: Forward pass for the vision tower model.\n        - `dummy_feature() -> torch.Tensor`: Returns a dummy feature tensor.\n        - `dtype() -> torch.dtype`: Returns the data type of the vision tower model.\n        - `device() -> torch.device`: Returns the device of the vision tower model.\n        - `config() -> Any`: Returns the configuration of the vision tower model.\n        - `hidden_size() -> int`: Returns the hidden size of the vision tower model.\n        - `num_patches_per_side() -> int`: Returns the number of patches per side of the image.\n        - `num_patches() -> int`: Returns the total number of patches in the image.\n        - `image_size() -> int`: Returns the size of the image.\n    \"\"\"\n\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.vision_tower_name = vision_tower\n        self.select_layer = args.mm_vision_select_layer\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n        else:\n            self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n        self.vision_tower.requires_grad_(False)\n\n        self.is_loaded = True\n\n    def feature_select(self, image_forward_outs):\n        select_feature_type = self.select_feature\n\n        if self.select_feature in [\"slicefour_patch\", \"slicefour_cls_patch\"]:\n            select_every_k_layer = len(image_forward_outs.hidden_states) // 4\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in range(select_every_k_layer + self.select_layer, len(image_forward_outs.hidden_states), select_every_k_layer)], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slicefour_\", \"\")\n        elif self.select_feature in [\"slice_m25811_f6_patch\", \"slice_m25811_f6_cls_patch\"]:\n            select_layers = [-2, -5, -8, -11, 6]\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in select_layers], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slice_m25811_f6_\", \"\")\n        else:\n            image_features = image_forward_outs.hidden_states[self.select_layer]\n\n        if select_feature_type == \"patch\":\n            image_features = image_features[:, 1:]\n        elif select_feature_type == \"cls_patch\":\n            image_features = image_features\n        else:\n            raise ValueError(f\"Unexpected select feature: {select_feature_type}\")\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        return self.vision_tower.dtype\n\n    @property\n    def device(self):\n        return self.vision_tower.device\n\n    @property\n    def config(self):\n        if self.is_loaded:\n            return self.vision_tower.config\n        else:\n            return self.cfg_only\n\n    @property\n    def hidden_size(self):\n        _hidden_size = self.config.hidden_size\n        if \"slicefour\" in self.select_feature:\n            _hidden_size *= 4\n        if \"slice_m25811_f6\" in self.select_feature:\n            _hidden_size *= 5\n        return _hidden_size\n\n    @property\n    def num_patches_per_side(self):\n        return self.config.image_size // self.config.patch_size\n\n    @property\n    def num_patches(self):\n        _num_patches = (self.config.image_size // self.config.patch_size) ** 2\n        if \"cls_patch\" in self.select_feature:\n            _num_patches += 1\n        return _num_patches\n\n    @property\n    def image_size(self):\n        return self.config.image_size\n\n\nclass CLIPVisionTowerS2(CLIPVisionTower):\n    r\"\"\"\n    CLIPVisionTowerS2 is a subclass of CLIPVisionTower designed to handle multi-scale image inputs for vision processing.\n    \n    Attributes:\n    ------------\n        - s2_scales (list): List of scales for multi-scale image processing.\n        - s2_split_size (int): The smallest scale size used for splitting images.\n        - s2_image_size (int): The largest scale size used for image resizing and cropping.\n        - image_processor (CLIPImageProcessor): Processor for handling image preprocessing.\n        - vision_tower (CLIPVisionModel): Pretrained vision model for image feature extraction.\n        - is_loaded (bool): Flag indicating whether the model is loaded.\n    \n    Methods:\n    ------------\n        - `__init__(vision_tower, args, delay_load=False)`: Initializes the CLIPVisionTowerS2 with given vision tower and arguments.\n        - `load_model(device_map=None)`: Loads the vision model and image processor, and sets the preprocessing sizes.\n        - `forward_feature(images)`: Extracts features from the given images using the vision model.\n        - `forward(images)`: Processes the given images through the vision model using multi-scale forward pass.\n        - `hidden_size`: Returns the hidden size of the model multiplied by the number of scales.\n    \"\"\"\n\n    def __init__(self, vision_tower, args, delay_load=False):\n\n        self.s2_scales = getattr(args, \"s2_scales\", \"336,672,1008\")\n        self.s2_scales = list(map(int, self.s2_scales.split(\",\")))\n        self.s2_scales.sort()\n        self.s2_split_size = self.s2_scales[0]\n        self.s2_image_size = self.s2_scales[-1]\n\n        super().__init__(vision_tower, args, delay_load)\n\n        # change resize/crop size in preprocessing to the largest image size in s2_scale\n        if not delay_load or getattr(args, \"unfreeze_mm_vision_tower\", False):\n            self.image_processor.size[\"shortest_edge\"] = self.s2_image_size\n            self.image_processor.crop_size[\"height\"] = self.image_processor.crop_size[\"width\"] = self.s2_image_size\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n        self.vision_tower.requires_grad_(False)\n\n        self.image_processor.size[\"shortest_edge\"] = self.s2_image_size\n        self.image_processor.crop_size[\"height\"] = self.image_processor.crop_size[\"width\"] = self.s2_image_size\n\n        self.is_loaded = True\n\n    def forward_feature(self, images):\n        image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n        image_features = self.feature_select(image_forward_outs).to(images.dtype)\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_feature = multiscale_forward(self.forward_feature, image.unsqueeze(0), img_sizes=self.s2_scales, max_split_size=self.s2_split_size, split_forward=True)\n                image_features.append(image_feature)\n        else:\n            image_features = multiscale_forward(self.forward_feature, images, img_sizes=self.s2_scales, max_split_size=self.s2_split_size, split_forward=True)\n\n        return image_features\n\n    @property\n    def hidden_size(self):\n        return self.config.hidden_size * len(self.s2_scales)\n"}
{"type": "source_file", "path": "assets/patches/instellavl.py", "content": "# CopyrightÂ© 2025 Advanced Micro Devices, Inc. All rights reserved.\n\nimport json\nimport logging\nimport warnings\nfrom datetime import timedelta\nfrom typing import List, Optional, Tuple, Union\n\nimport PIL\nimport torch\nimport numpy as np\nfrom accelerate import Accelerator, DistributedType, InitProcessGroupKwargs\nfrom accelerate.state import AcceleratorState\nfrom decord import VideoReader, cpu\nfrom packaging import version\nfrom tqdm import tqdm\nfrom transformers import AutoConfig\n\nfrom lmms_eval import utils\nfrom lmms_eval.api.instance import Instance\nfrom lmms_eval.api.model import lmms\nfrom lmms_eval.api.registry import register_model\nfrom lmms_eval.models.model_utils.load_video import read_video_pyav\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Configure logging\neval_logger = logging.getLogger(\"lmms-eval\")\n\n# Enable TF32 for CUDA\ntorch.backends.cuda.matmul.allow_tf32 = True\n\n# Import instellavl modules\ntry:\n    from instellavl.constants import (\n        DEFAULT_IM_END_TOKEN,\n        DEFAULT_IM_START_TOKEN,\n        DEFAULT_IMAGE_TOKEN,\n        IGNORE_INDEX,\n        IMAGE_TOKEN_INDEX,\n    )\n    from instellavl.conversation import SeparatorStyle, conv_templates\n    from instellavl.mm_utils import (\n        KeywordsStoppingCriteria,\n        get_model_name_from_path,\n        process_images,\n        tokenizer_image_token,\n    )\n    from instellavl.model.builder import load_pretrained_model\nexcept ImportError as e:\n    eval_logger.debug(f\"InstellaVL is not installed. Please install InstellaVL to use this model.\\nError: {e}\")\n\n\n# Determine best attention implementation\nif version.parse(torch.__version__) >= version.parse(\"2.1.2\"):\n    best_fit_attn_implementation = \"sdpa\"\nelse:\n    best_fit_attn_implementation = \"eager\"\n\n\n@register_model(\"instellavl\")\nclass InstellaVL(lmms):\n    \"\"\"\n    InstellaVL Model\n    \"\"\"\n\n    def __init__(\n        self,\n        pretrained: str = \"\",  #TODO: add our own huggingface model path \n        truncation: Optional[bool] = True,\n        device: Optional[str] = \"cuda:0\",\n        batch_size: Optional[Union[int, str]] = 1,\n        model_name: Optional[str] = None,\n        attn_implementation: Optional[str] = best_fit_attn_implementation,\n        device_map: Optional[str] = \"cuda:0\",\n        conv_template: Optional[str] = \"instella\",\n        use_cache: Optional[bool] = True,\n        truncate_context: Optional[bool] = False,  \n        customized_config: Optional[str] = None,  # ends in json\n        max_frames_num: Optional[int] = 32,\n        mm_spatial_pool_stride: Optional[int] = 2,\n        mm_spatial_pool_mode: Optional[str] = \"bilinear\",\n        token_strategy: Optional[str] = \"single\",  # could be \"single\" or \"multiple\", \"multiple\" denotes adding multiple <image> tokens for each frame\n        video_decode_backend: str = \"decord\",\n        **kwargs,\n    ) -> None:\n        super().__init__()\n        # Do not use kwargs for now\n        assert kwargs == {}, f\"Unexpected kwargs: {kwargs}\"\n\n        accelerator_kwargs = InitProcessGroupKwargs(timeout=timedelta(weeks=52))\n        accelerator = Accelerator(kwargs_handlers=[accelerator_kwargs])\n        if accelerator.num_processes > 1:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            self._device = torch.device(device)\n            self.device_map = device_map\n        else:\n            self._device = torch.device(f\"cuda:{accelerator.local_process_index}\")\n            self.device_map = f\"cuda:{accelerator.local_process_index}\"\n\n        instellavl_model_args = {\n            \"multimodal\": True,\n        }\n        if customized_config is not None:\n            instellavl_model_args[\"customized_config\"] = customized_config\n        if attn_implementation is not None:\n            instellavl_model_args[\"attn_implementation\"] = attn_implementation\n        if \"use_flash_attention_2\" in kwargs:\n            instellavl_model_args[\"use_flash_attention_2\"] = kwargs[\"use_flash_attention_2\"]\n\n        self.pretrained = pretrained\n        self.token_strategy = token_strategy\n        self.max_frames_num = max_frames_num\n        self.mm_spatial_pool_stride = mm_spatial_pool_stride\n        self.mm_spatial_pool_mode = mm_spatial_pool_mode\n        self.video_decode_backend = video_decode_backend\n\n        overwrite_config = {}\n        overwrite_config[\"mm_spatial_pool_stride\"] = self.mm_spatial_pool_stride\n        overwrite_config[\"mm_spatial_pool_mode\"] = self.mm_spatial_pool_mode\n        cfg_pretrained = AutoConfig.from_pretrained(self.pretrained)\n\n\n        # Try to load the model with the multimodal argument\n        self._tokenizer, self._model, self._image_processor, self._max_length = load_pretrained_model(pretrained, None, model_name, device_map=self.device_map, **instellavl_model_args)\n\n        self._config = self._model.config\n        # self._config.mm_anyres_choose_method = 'best_fit'\n        self.model.eval()\n        self.truncation = truncation\n        self.batch_size_per_gpu = int(batch_size)\n        self.conv_template = conv_template\n        self.use_cache = use_cache\n        self.truncate_context = truncate_context\n        assert self.batch_size_per_gpu == 1, \"InstellaVL currently does not support batched generation.\"\n\n        if accelerator.num_processes > 1:\n            assert accelerator.distributed_type in [DistributedType.FSDP, DistributedType.MULTI_GPU, DistributedType.DEEPSPEED], \"Unsupported distributed type provided. Only DDP and FSDP are supported.\"\n            # If you want to use DistributedType.DEEPSPEED, you have to run accelerate config before using the model\n            # Also, you have to select zero stage 0 (equivalent to DDP) in order to make the prepare model works\n            # I tried to set different parameters in the kwargs to let default zero 2 stage works, but it didn't work.\n            if accelerator.distributed_type == DistributedType.DEEPSPEED:\n                kwargs = {\n                    \"train_micro_batch_size_per_gpu\": self.batch_size_per_gpu,\n                    \"train_batch_size\": self.batch_size_per_gpu * accelerator.num_processes,\n                }\n                AcceleratorState().deepspeed_plugin.deepspeed_config_process(must_match=True, **kwargs)\n                eval_logger.info(\"Detected that you are using DistributedType.DEEPSPEED. Make sure you run `accelerate config` and set zero stage to 0\")\n\n            if accelerator.distributed_type == DistributedType.FSDP or accelerator.distributed_type == DistributedType.DEEPSPEED:\n                self._model = accelerator.prepare(self.model)\n            else:\n                self._model = accelerator.prepare_model(self.model, evaluation_mode=True)\n            self.accelerator = accelerator\n            if self.accelerator.is_local_main_process:\n                eval_logger.info(f\"Using {accelerator.num_processes} devices with data parallelism\")\n            self._rank = self.accelerator.local_process_index\n            self._world_size = self.accelerator.num_processes\n\n        elif accelerator.num_processes == 1 and device_map == \"auto\":\n            eval_logger.info(f\"Using {accelerator.num_processes} devices with tensor parallelism\")\n            self._rank = 0\n            self._world_size = 1\n\n        else:\n            eval_logger.info(f\"Using single device: {self._device}\")\n            self.model.to(self._device)\n            self._rank = 0\n            self._world_size = 1\n\n    @property\n    def config(self):\n        # return the associated transformers.AutoConfig for the given pretrained model.\n        return self._config\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def model(self):\n        # returns the model, unwrapping it if using Accelerate\n        if hasattr(self, \"accelerator\"):\n            return self.accelerator.unwrap_model(self._model)\n        else:\n            return self._model\n\n    @property\n    def eot_token_id(self):\n        # we use EOT because end of *text* is more accurate for what we're doing than end of *sentence*\n        return self.tokenizer.eos_token_id\n\n    @property\n    def max_length(self):\n        return self._max_length\n\n    def pad_sequence(self, input_ids, batch_first, padding_value):\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = [torch.flip(_input_ids, [0]) for _input_ids in input_ids]\n        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=batch_first, padding_value=padding_value)\n        if self.tokenizer.padding_side == \"left\":\n            input_ids = torch.flip(input_ids, [1])\n        return input_ids\n\n    @property\n    def batch_size(self):\n        return self.batch_size_per_gpu\n\n    @property\n    def device(self):\n        return self._device\n\n    @property\n    def rank(self):\n        return self._rank\n\n    @property\n    def world_size(self):\n        return self._world_size\n\n    def tok_encode(self, string: str, left_truncate_len=None, add_special_tokens=None) -> List[int]:\n        \"\"\" \"\"\"\n        add_special_tokens = False if add_special_tokens is None else add_special_tokens\n        encoding = self.tokenizer.encode(string, add_special_tokens=add_special_tokens)\n        # left-truncate the encoded context to be at most `left_truncate_len` tokens long\n        if left_truncate_len:\n            encoding = encoding[-left_truncate_len:]\n        return encoding\n\n    def tok_decode(self, tokens):\n        try:\n            return self.tokenizer.decode(tokens)\n        except:\n            return self.tokenizer.decode([tokens])\n\n    def loglikelihood(self, requests: List[Instance]) -> List[Tuple[float, bool]]:\n        res = []\n        pbar = tqdm(total=len(requests), disable=(self.rank != 0), desc=\"Model Responding\")\n\n        for contexts, doc_to_target, doc_to_visual, doc_id, task, split in [reg.args for reg in requests]:\n            visual = doc_to_visual(self.task_dict[task][split][doc_id])\n\n            if visual is None or visual == []:\n                visual = None\n                task_type = \"text\"\n                image_tensor = None\n            else:\n                if len(visual) > 1 or \"image_aspect_ratio\" not in self._config.__dict__:\n                    self._config.image_aspect_ratio = \"pad\"\n                    eval_logger.info(f\"In Multi-Image setting, image aspect ratio: {self._config.image_aspect_ratio}\")\n\n                if \"task_type\" in self.metadata and self.metadata[\"task_type\"] == \"video\" and \"sample_frames\" in self.metadata:\n                    assert type(visual) == list, \"sample_frames must be specified for video task\"\n                    sample_indices = np.linspace(0, len(visual) - 1, self.metadata[\"sample_frames\"], dtype=int)\n                    visual = [visual[i] for i in sample_indices]\n                    assert len(visual) == self.metadata[\"sample_frames\"]\n\n                    image_tensor = process_images(visual, self._image_processor, self._config)\n                    if type(image_tensor) is list:\n                        image_tensor = [_image.to(dtype=torch.float16, device=self.device) for _image in image_tensor]\n                    else:\n                        image_tensor = image_tensor.to(dtype=torch.float16, device=self.device)\n\n                    task_type = \"video\"\n\n                elif type(visual[0]) == PIL.Image.Image:\n                    image_tensor = process_images(visual, self._image_processor, self._config)\n                    if type(image_tensor) is list:\n                        image_tensor = [_image.to(dtype=torch.float16, device=self.device) for _image in image_tensor]\n                    else:\n                        image_tensor = image_tensor.to(dtype=torch.float16, device=self.device)\n\n                    task_type = \"image\"\n\n                elif type(visual[0]) == str:\n                    image_tensor = []\n                    try:\n                        if self.video_decode_backend == \"decord\":\n                            frames = self.load_video(visual, self.max_frames_num)\n                        elif self.video_decode_backend == \"pyav\":\n                            frames = read_video_pyav(visual[0], num_frm=self.max_frames_num)\n                        frames = self._image_processor.preprocess(frames, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\n                        image_tensor.append(frames)\n                    except Exception as e:\n                        eval_logger.error(f\"Error {e} in loading video\")\n                        image_tensor = None\n\n                    task_type = \"video\"\n\n            if image_tensor is not None and len(image_tensor) != 0 and DEFAULT_IMAGE_TOKEN not in contexts:\n                placeholder_count = len(visual) if isinstance(visual, list) else 1\n                if task_type == \"video\":\n                    placeholder_count = len(frames) if self.token_strategy == \"multiple\" else 1\n                image_tokens = [DEFAULT_IMAGE_TOKEN] * placeholder_count\n                image_tokens = \" \".join(image_tokens)\n                prompts_input = image_tokens + \"\\n\" + contexts\n            else:\n                prompts_input = contexts\n\n\n            conv = conv_templates[self.conv_template].copy()\n\n            conv.append_message(conv.roles[0], prompts_input)\n            conv.append_message(conv.roles[1], None)\n            prompt = conv.get_prompt()\n\n            input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(self.device)\n\n            if type(doc_to_target) == str:\n                continuation = doc_to_target\n            else:\n                continuation = doc_to_target(self.task_dict[task][split][doc_id])\n\n            conv.messages[-1][1] = continuation\n            full_prompt = conv.get_prompt()\n            full_input_ids = tokenizer_image_token(full_prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(self.device)\n\n            labels = full_input_ids.clone()\n            labels[0, : input_ids.shape[1]] = -100\n\n            kwargs = {}\n            if task_type == \"image\":\n                kwargs[\"image_sizes\"] = [[v.size[0], v.size[1]] for v in visual] if isinstance(visual, list) else [[visual.size[0], visual.size[1]]]\n            elif task_type == \"video\":\n                kwargs[\"modalities\"] = [\"video\"]\n                self._config.mm_spatial_pool_stride = self.mm_spatial_pool_stride\n                self._config.mm_spatial_pool_mode = self.mm_spatial_pool_mode\n\n            with torch.inference_mode():\n                outputs = self.model(input_ids=full_input_ids, labels=labels, images=image_tensor, use_cache=True, **kwargs)\n\n            loss = outputs[\"loss\"]\n            logits = outputs[\"logits\"]\n            greedy_tokens = logits.argmax(dim=-1)\n            cont_toks = full_input_ids[:, input_ids.shape[1] :]\n            greedy_tokens = greedy_tokens[:, input_ids.shape[1] : full_input_ids.shape[1]]\n            max_equal = (greedy_tokens == cont_toks).all()\n\n            res.append((float(loss.item()), bool(max_equal)))\n            pbar.update(1)\n\n        pbar.close()\n        return res\n\n    def flatten(self, input):\n        new_list = []\n        for i in input:\n            for j in i:\n                new_list.append(j)\n        return new_list\n\n    def load_video(self, video_path, max_frames_num):\n        if type(video_path) == str:\n            vr = VideoReader(video_path, ctx=cpu(0))\n        else:\n            vr = VideoReader(video_path[0], ctx=cpu(0))\n        total_frame_num = len(vr)\n        uniform_sampled_frames = np.linspace(0, total_frame_num - 1, max_frames_num, dtype=int)\n        frame_idx = uniform_sampled_frames.tolist()\n        spare_frames = vr.get_batch(frame_idx).asnumpy()\n        return spare_frames  # (frames, height, width, channels)\n\n    def generate_until(self, requests: List[Instance]) -> List[str]:\n        res = []\n\n        def _collate(x):\n            # the negative sign on len(toks) sorts descending - this has a few advantages:\n            # - time estimates will always be over not underestimates, which is more useful for planning\n            # - to know the size of a batch when going through the list, you know the first one is always the batch\n            #   padded context length. this is useful to simplify the batching logic and more importantly to make\n            #   automatic adaptive batches much much easier to implement\n            # - any OOMs will happen right away rather than near the end\n            toks = self.tok_encode(x[0])\n            return -len(toks), x[0]\n\n        # we group requests by their generation_kwargs,\n        # so that we don't try to execute e.g. greedy sampling and temp=0.8 sampling\n        # in the same batch.\n        metadata = requests[0].metadata\n        re_ords = utils.Collator([reg.args for reg in requests], _collate, grouping=True)\n        chunks = re_ords.get_batched(n=self.batch_size, batch_fn=None)\n        num_iters = len(requests) // self.batch_size if len(requests) % self.batch_size == 0 else len(requests) // self.batch_size + 1\n        pbar = tqdm(total=num_iters, disable=(self.rank != 0), desc=\"Model Responding\")\n        for chunk in chunks:\n            batched_contexts, all_gen_kwargs, batched_doc_to_visual, batched_doc_id, batched_task, batched_split = zip(*chunk)\n            task = batched_task[0]\n            split = batched_split[0]\n            batched_visuals = [batched_doc_to_visual[0](self.task_dict[task][split][ids]) for ids in batched_doc_id]  # [B, N]\n            assert len(batched_visuals) == 1\n\n            # we assume all gen kwargs in the batch are the same\n            # this is safe to assume because the `grouper` object ensures it.\n            gen_kwargs = all_gen_kwargs[0]\n            if \"until\" in gen_kwargs:\n                gen_kwargs.pop(\"until\")\n\n            question_input = []\n\n            for visual, context in zip(batched_visuals, batched_contexts):\n                if visual is None or visual == []:  # for text-only tasks.\n                    visual = None\n                    task_type = \"text\"\n                    placeholder_count = 0\n                    image_tensor = None\n                else:\n                    if len(visual) > 1 or \"image_aspect_ratio\" not in self._config.__dict__:  # for multi image case, we treat per image aspect ratio as \"pad\" by default.\n                        self._config.image_aspect_ratio = getattr(gen_kwargs, \"image_aspect_ratio\", \"pad\")\n                        eval_logger.info(f\"In Multi-Image setting, image aspect ratio: {self._config.image_aspect_ratio}\")\n\n                    if \"task_type\" in metadata and metadata[\"task_type\"] == \"video\" and \"sample_frames\" in metadata:  # overwrite logic for video task with multiple static image frames\n                        assert type(visual) == list, \"sample_frames must be specified for video task\"\n                        sample_indices = np.linspace(0, len(visual) - 1, metadata[\"sample_frames\"], dtype=int)\n                        visual = [visual[i] for i in sample_indices]\n                        assert len(visual) == metadata[\"sample_frames\"]\n\n                        image_tensor = process_images(visual, self._image_processor, self._config)\n                        if type(image_tensor) is list:\n                            image_tensor = [_image.to(dtype=torch.float16, device=self.device) for _image in image_tensor]\n                        else:\n                            image_tensor = image_tensor.to(dtype=torch.float16, device=self.device)\n\n                        task_type = \"video\"\n                        placeholder_count = 1\n\n                    elif type(visual[0]) == PIL.Image.Image:  # For image, multi-image tasks\n                        image_tensor = process_images(visual, self._image_processor, self._config)\n                        if type(image_tensor) is list:\n                            image_tensor = [_image.to(dtype=torch.float16, device=self.device) for _image in image_tensor]\n                        else:\n                            image_tensor = image_tensor.to(dtype=torch.float16, device=self.device)\n\n                        task_type = \"image\"\n                        placeholder_count = len(visual) if isinstance(visual, list) else 1\n\n                    elif type(visual[0]) == str:  # For video task\n                        image_tensor = []\n                        try:\n                            if self.video_decode_backend == \"decord\":\n                                frames = self.load_video(visual, self.max_frames_num)\n                            elif self.video_decode_backend == \"pyav\":\n                                frames = read_video_pyav(visual[0], num_frm=self.max_frames_num)\n                            frames = self._image_processor.preprocess(frames, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\n                            image_tensor.append(frames)\n                        except Exception as e:\n                            eval_logger.error(f\"Error {e} in loading video\")\n                            image_tensor = None\n\n                        task_type = \"video\"\n                        placeholder_count = len(frames) if self.token_strategy == \"multiple\" else 1\n\n                if image_tensor is not None and len(image_tensor) != 0 and DEFAULT_IMAGE_TOKEN not in context:\n                    \"\"\"\n                    Three senarios:\n                    1. No image, and there for, no image token should be added.\n                    2. image token is already specified in the context, so we don't need to add it.\n                    3. image token is not specified in the context and there is image inputs, so we need to add it. In this case, we add the image token at the beginning of the context and add a new line.\n                    4. For video tasks, we could add a <image> token or multiple <image> tokens for each frame in the context. This depends on the training strategy and should balance in test to decide which is better\n                    \"\"\"\n                    image_tokens = [DEFAULT_IMAGE_TOKEN] * placeholder_count\n                    image_tokens = \" \".join(image_tokens)\n                    question = image_tokens + \"\\n\" + context\n                else:\n                    question = context\n\n\n                conv = conv_templates[self.conv_template].copy()\n\n                if utils.is_json(question):  # conversational question input\n                    question = json.loads(question)\n                    for idx, item in enumerate(question):\n                        role = conv.roles[idx % 2]\n                        message = item[\"value\"]\n                        conv.append_message(role, message)\n\n                    assert len(conv.messages) % 2 == 1\n                    conv.append_message(conv.roles[1], None)\n                    prompt_question = conv.get_prompt()\n                    question_input.append(prompt_question)\n                else:  # only simple string for question\n                    conv.append_message(conv.roles[0], question)\n                    conv.append_message(conv.roles[1], None)\n                    prompt_question = conv.get_prompt()\n                    question_input.append(prompt_question)\n\n            # preconfigure gen_kwargs with defaults\n            if \"max_new_tokens\" not in gen_kwargs:\n                gen_kwargs[\"max_new_tokens\"] = 1024\n            if \"temperature\" not in gen_kwargs:\n                gen_kwargs[\"temperature\"] = 0\n            if \"do_sample\" not in gen_kwargs:\n                gen_kwargs[\"do_sample\"] = False\n            if \"top_p\" not in gen_kwargs:\n                gen_kwargs[\"top_p\"] = None\n            if \"num_beams\" not in gen_kwargs:\n                gen_kwargs[\"num_beams\"] = 1\n\n            input_ids_list = [tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\") for prompt in question_input]\n            pad_token_ids = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n            input_ids = self.pad_sequence(input_ids_list, batch_first=True, padding_value=pad_token_ids).to(self.device)\n            attention_masks = input_ids.ne(pad_token_ids).to(self.device)\n\n            if task_type == \"image\":\n                gen_kwargs[\"image_sizes\"] = [batched_visuals[0][idx].size for idx in range(len(batched_visuals[0]))]\n            elif task_type == \"video\":\n                stopping_criteria = KeywordsStoppingCriteria([conv.sep], self.tokenizer, input_ids)\n                gen_kwargs[\"modalities\"] = [\"video\"]\n                gen_kwargs[\"stopping_criteria\"] = [stopping_criteria]\n                self._config.mm_spatial_pool_stride = self.mm_spatial_pool_stride\n                self._config.mm_spatial_pool_mode = self.mm_spatial_pool_mode\n\n            # TODO: attention to this major generation step...\n            if \"image_aspect_ratio\" in gen_kwargs.keys():\n                gen_kwargs.pop(\"image_aspect_ratio\")\n            try:\n                with torch.inference_mode():\n                    cont = self.model.generate(input_ids, attention_mask=attention_masks, pad_token_id=pad_token_ids, images=image_tensor, use_cache=self.use_cache, **gen_kwargs)\n\n                text_outputs = self.tokenizer.batch_decode(cont, skip_special_tokens=True)\n            except Exception as e:\n                raise e\n\n            text_outputs = [response.strip() for response in text_outputs]\n            res.extend(text_outputs)\n            self.cache_hook.add_partial(\"generate_until\", (context, gen_kwargs), text_outputs)\n            pbar.update(1)\n            # reorder this group of results back to original unsorted form\n        res = re_ords.get_original(res)\n\n        pbar.close()\n        return res\n\n    def generate_until_multi_round(self, requests: List[Instance]) -> List[str]:\n        return self.generate_until(requests)"}
{"type": "source_file", "path": "instellavl/model/instellavl_arch.py", "content": "#    Modification CopyrightÂ© 2025 Advanced Micro Devices, Inc. All rights reserved.\n#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom abc import ABC, abstractmethod\n\nimport re\nimport os\nimport math\nimport random\nimport shutil\n\nimport torch\nimport torch.nn as nn\n\nfrom .multimodal_encoder.builder import build_vision_tower\nfrom .multimodal_resampler.builder import build_vision_resampler\nfrom .multimodal_projector.builder import build_vision_projector\n\nfrom instellavl.mm_utils import get_anyres_image_grid_shape\nfrom instellavl.utils import rank0_print\n\nfrom instellavl.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n\nclass InstellaVLMetaModel:\n\n    def __init__(self, config):\n        super(InstellaVLMetaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            delay_load = getattr(config, \"delay_load\", False)\n            self.vision_tower = build_vision_tower(config, delay_load=delay_load)\n            self.vision_resampler = build_vision_resampler(config, vision_tower=self.vision_tower)\n            self.mm_projector = build_vision_projector(config, vision_cfg=self.vision_tower.config)\n\n            if \"unpad\" in getattr(config, \"mm_patch_merge_type\", \"\"):\n                self.image_newline = nn.Parameter(torch.empty(config.hidden_size, dtype=self.dtype))\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, \"vision_tower\", None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        vision_tower = model_args.vision_tower\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        mm_patch_merge_type = model_args.mm_patch_merge_type\n\n        self.config.mm_vision_tower = vision_tower\n        self.config.vision_tower_pretrained = getattr(model_args, \"vision_tower_pretrained\", \"\")\n\n        if self.get_vision_tower() is None:\n            vision_tower = build_vision_tower(model_args)\n            vision_resampler = build_vision_resampler(model_args, vision_tower=vision_tower)\n            for k, v in vision_resampler.config.items():\n                setattr(self.config, k, v)\n\n            if fsdp is not None and len(fsdp) > 0:\n                self.vision_tower = [vision_tower]\n                self.vision_resampler = [vision_resampler]\n            else:\n                self.vision_tower = vision_tower\n                self.vision_resampler = vision_resampler\n        else:\n            if fsdp is not None and len(fsdp) > 0:\n                vision_resampler = self.vision_resampler[0]\n                vision_tower = self.vision_tower[0]\n            else:\n                vision_resampler = self.vision_resampler\n                vision_tower = self.vision_tower\n            vision_tower.load_model()\n\n            # In case it is frozen by LoRA\n            for p in self.vision_resampler.parameters():\n                p.requires_grad = True\n\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, \"mm_projector_type\", \"linear\")\n        self.config.mm_hidden_size = getattr(vision_resampler, \"hidden_size\", vision_tower.hidden_size)\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        self.config.mm_patch_merge_type = mm_patch_merge_type\n        self.config.online_training = model_args.online_training\n\n        if getattr(self, \"mm_projector\", None) is None:\n            self.mm_projector = build_vision_projector(self.config, vision_cfg=vision_tower.config)\n\n            if \"unpad\" in mm_patch_merge_type:\n                embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype))\n                self.image_newline = nn.Parameter(torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std)\n        else:\n            # In case it is frozen by LoRA\n            for p in self.mm_projector.parameters():\n                p.requires_grad = True\n\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location=\"cpu\")\n\n            def get_w(weights, keyword):\n                return {k.split(keyword + \".\")[1]: v for k, v in weights.items() if keyword in k}\n\n            incompatible_keys = self.mm_projector.load_state_dict(get_w(mm_projector_weights, \"mm_projector\"))\n            rank0_print(f\"Loaded mm projector weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}\")\n            incompatible_keys = self.vision_resampler.load_state_dict(get_w(mm_projector_weights, \"vision_resampler\"), strict=False)\n            rank0_print(f\"Loaded vision resampler weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}\") \n        \n            if 'tmp-' in pretrain_mm_mlp_adapter:\n                pretrain_mm_mlp_adapter_folder = os.path.dirname(pretrain_mm_mlp_adapter)\n                shutil.rmtree(pretrain_mm_mlp_adapter_folder, ignore_errors=True)\n            \n\n\ndef unpad_image(tensor, original_size):\n    \"\"\"\n    Unpads a PyTorch tensor of a padded and resized image.\n\n    Args:\n    tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.\n    original_size (tuple): The original size of the image (height, width).\n\n    Returns:\n    torch.Tensor: The unpadded image tensor.\n    \"\"\"\n    original_width, original_height = original_size\n    current_height, current_width = tensor.shape[1:]\n\n    # Compute aspect ratios\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    # Determine padding size and direction\n    if original_aspect_ratio > current_aspect_ratio:\n        # Padding was added to the height\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        padding = (current_height - new_height) // 2\n        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n    else:\n        # Padding was added to the width\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        padding = (current_width - new_width) // 2\n        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n\n    return unpadded_tensor\n\n\nclass InstellaVLMetaForCausalLM(ABC):\n\n    @abstractmethod\n    def get_model(self):\n        pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def get_2dPool(self, image_feature):\n        height = width = self.get_vision_tower().num_patches_per_side\n        num_frames, num_tokens, num_dim = image_feature.shape\n        image_feature = image_feature.view(num_frames, height, width, -1)\n        image_feature = image_feature.permute(0, 3, 1, 2).contiguous()\n        # image_feature = nn.functional.max_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        if self.config.mm_spatial_pool_mode == \"average\":\n            image_feature = nn.functional.avg_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        elif self.config.mm_spatial_pool_mode == \"max\":\n            image_feature = nn.functional.max_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        elif self.config.mm_spatial_pool_mode == \"bilinear\":\n            height, weight = image_feature.shape[2:]\n            scaled_shape = [math.ceil(height / 2), math.ceil(weight / 2)]\n            image_feature = nn.functional.interpolate(image_feature, size=scaled_shape, mode='bilinear')\n\n        else:\n            raise ValueError(f\"Unexpected mm_spatial_pool_mode: {self.config.mm_spatial_pool_mode}\")\n        image_feature = image_feature.permute(0, 2, 3, 1)\n        image_feature = image_feature.view(num_frames, -1, num_dim)\n        return image_feature\n\n    def encode_images(self, images):\n        image_features = self.get_model().get_vision_tower()(images)\n        # image_features = self.get_model().vision_resampler(image_features, images=images)\n        image_features = self.get_model().mm_projector(image_features)\n        return image_features\n    \n    def encode_multimodals(self, videos_or_images, video_idx_in_batch, split_sizes=None):\n        videos_or_images_features = self.get_model().get_vision_tower()(videos_or_images)\n        per_videos_or_images_features = torch.split(videos_or_images_features, split_sizes, dim=0)  # tuple, (dim_1, 576, 4096)\n        all_videos_or_images_features = []\n\n        for idx, feat in enumerate(per_videos_or_images_features):\n            feat = self.get_model().mm_projector(feat)\n            if idx in video_idx_in_batch:\n                feat = self.get_2dPool(feat)\n            all_videos_or_images_features.append(feat)\n        return all_videos_or_images_features\n\n    def add_token_per_grid(self, image_feature):\n        resize_h = int(math.sqrt(image_feature.shape[1]))\n        num_frames = image_feature.shape[0]\n        image_feature = image_feature.view(num_frames, 1, resize_h, resize_h, -1)\n        image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n        image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n        image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n        image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n        return image_feature\n\n    def add_token_per_frame(self, image_feature):\n        image_feature = image_feature.permute(2, 0, 1).contiguous()\n        image_feature =  torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n        image_feature = image_feature.permute(1, 2, 0).contiguous()\n        return image_feature\n\n    def prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities=[\"image\"], image_sizes=None):\n        vision_tower = self.get_vision_tower()\n        # rank_print(modalities)\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n            return input_ids, position_ids, attention_mask, past_key_values, None, labels\n\n        if isinstance(modalities, str):\n            modalities = [modalities]\n\n        if type(images) is list or images.ndim == 5:\n            if type(images) is list:\n                images = [x.unsqueeze(0) if x.ndim == 3 else x for x in images]\n\n            video_idx_in_batch = []\n            for _ in range(len(modalities)):\n                if modalities[_] == \"video\":\n                    video_idx_in_batch.append(_)\n\n            # print(video_idx_in_batch)\n\n            images_list = []\n            for image in images:\n                if image.ndim == 4:\n                    images_list.append(image)\n                else:\n                    images_list.append(image.unsqueeze(0))\n\n            # import pdb;pdb.set_trace()\n            concat_images = torch.cat([image for image in images_list], dim=0)\n            split_sizes = [image.shape[0] for image in images_list]\n            encoded_image_features = self.encode_images(concat_images)\n            # import pdb\n            # pdb.set_trace()\n\n            # This is a list, each element is [num_images, patch * patch, dim]\n            # rank_print(f\"Concat images : {concat_images.shape}\")\n            encoded_image_features = torch.split(encoded_image_features, split_sizes)\n            image_features = []\n            for idx, image_feat in enumerate(encoded_image_features):\n                if idx in video_idx_in_batch:\n                    image_features.append(self.get_2dPool(image_feat))\n                else:\n                    image_features.append(image_feat)\n            # image_features = self.encode_multimodals(concat_images, video_idx_in_batch, split_sizes)\n            # rank_print(f\"Encoded image feats : {[x.shape for x in image_features]}\")\n            # image_features = torch.split(image_features, split_sizes, dim=0)\n            mm_patch_merge_type = getattr(self.config, \"mm_patch_merge_type\", \"flat\")\n            image_aspect_ratio = getattr(self.config, \"image_aspect_ratio\", \"square\")\n\n            if mm_patch_merge_type == \"flat\":\n                image_features = [x.flatten(0, 1) for x in image_features]\n\n            elif mm_patch_merge_type.startswith(\"spatial\"):\n                new_image_features = []\n                for image_idx, image_feature in enumerate(image_features):\n                    # FIXME: now assume the image is square, and split to 2x2 patches\n                    # num_patches = h * w, where h = w = sqrt(num_patches)\n                    # currently image_feature is a tensor of shape (4, num_patches, hidden_size)\n                    # we want to first unflatten it to (2, 2, h, w, hidden_size)\n                    # rank0_print(\"At least we are reaching here\")\n                    if image_idx in video_idx_in_batch:  # video operations\n                        # rank0_print(\"Video\")\n                        if self.config.mm_newline_position == \"grid\":\n                            # Grid-wise\n                            image_feature = self.add_token_per_grid(image_feature)\n                        \n                            new_image_features.append(image_feature)\n                        elif self.config.mm_newline_position == \"frame\":\n                            # Frame-wise\n                            image_feature = self.add_token_per_frame(image_feature)\n\n                            new_image_features.append(image_feature.flatten(0, 1))\n                            \n                        elif self.config.mm_newline_position == \"one_token\":\n                            # one-token\n                            image_feature = image_feature.flatten(0, 1)\n                            if 'unpad' in mm_patch_merge_type:\n                                image_feature = torch.cat((\n                                    image_feature,\n                                    self.model.image_newline[None].to(image_feature.device)\n                                ), dim=0)\n                            new_image_features.append(image_feature)      \n                        elif self.config.mm_newline_position == \"no_token\":\n                            new_image_features.append(image_feature.flatten(0, 1))\n                        else:\n                            raise ValueError(f\"Unexpected mm_newline_position: {self.config.mm_newline_position}\")\n\n\n                    elif image_feature.shape[0] > 1:  # multi patches and multi images operations\n                        base_image_feature = image_feature[0]\n                        image_feature = image_feature[1:]\n                        height = width = self.get_vision_tower().num_patches_per_side\n\n                        assert height * width == base_image_feature.shape[0]\n\n                        if \"anyres_max\" in image_aspect_ratio:\n                            matched_anyres_max_num_patches = re.match(r\"anyres_max_(\\d+)\", image_aspect_ratio)\n                            if matched_anyres_max_num_patches:\n                                max_num_patches = int(matched_anyres_max_num_patches.group(1))\n\n                        if image_aspect_ratio == \"anyres\" or \"anyres_max\" in image_aspect_ratio:\n                            if hasattr(self.get_vision_tower(), \"image_size\"):\n                                vision_tower_image_size = self.get_vision_tower().image_size\n                            else:\n                                raise ValueError(\"vision_tower_image_size is not found in the vision tower.\")\n                            try:\n                                num_patch_width, num_patch_height = get_anyres_image_grid_shape(image_sizes[image_idx], self.config.image_grid_pinpoints, vision_tower_image_size)\n                            except Exception as e:\n                                rank0_print(f\"Error: {e}\")\n                                num_patch_width, num_patch_height = 2, 2\n                            image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1)\n                        else:\n                            image_feature = image_feature.view(2, 2, height, width, -1)\n\n                        if \"maxpool2x2\" in mm_patch_merge_type:\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = nn.functional.max_pool2d(image_feature, 2)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        elif \"unpad\" in mm_patch_merge_type and \"anyres_max\" in image_aspect_ratio and matched_anyres_max_num_patches:\n                            unit = image_feature.shape[2]\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = unpad_image(image_feature, image_sizes[image_idx])\n                            c, h, w = image_feature.shape\n                            times = math.sqrt(h * w / (max_num_patches * unit**2))\n                            if times > 1.1:\n                                image_feature = image_feature[None]\n                                image_feature = nn.functional.interpolate(image_feature, [int(h // times), int(w // times)], mode=\"bilinear\")[0]\n                            image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        elif \"unpad\" in mm_patch_merge_type:\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = unpad_image(image_feature, image_sizes[image_idx])\n                            image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        else:\n                            image_feature = image_feature.permute(0, 2, 1, 3, 4).contiguous()\n                            image_feature = image_feature.flatten(0, 3)\n                        if \"nobase\" in mm_patch_merge_type:\n                            pass\n                        else:\n                            image_feature = torch.cat((base_image_feature, image_feature), dim=0)\n                    else:  # single image operations\n                        image_feature = image_feature[0]\n                        if \"unpad\" in mm_patch_merge_type:\n                            image_feature = torch.cat((image_feature, self.model.image_newline[None]), dim=0)\n\n                    new_image_features.append(image_feature)\n                image_features = new_image_features\n            else:\n                raise ValueError(f\"Unexpected mm_patch_merge_type: {self.config.mm_patch_merge_type}\")\n        else:\n            image_features = self.encode_images(images)\n\n        # TODO: image start / end is not implemented here to support pretraining.\n        if getattr(self.config, \"tune_mm_mlp_adapter\", False) and getattr(self.config, \"mm_use_im_start_end\", False):\n            raise NotImplementedError\n        # rank_print(f\"Total images : {len(image_features)}\")\n\n        # Let's just add dummy tensors if they do not exist,\n        # it is a headache to deal with None all the time.\n        # But it is not ideal, and if you have a better idea,\n        # please open an issue / submit a PR, thanks.\n        _labels = labels\n        _position_ids = position_ids\n        _attention_mask = attention_mask\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n        else:\n            attention_mask = attention_mask.bool()\n        if position_ids is None:\n            position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n        if labels is None:\n            labels = torch.full_like(input_ids, IGNORE_INDEX)\n\n        # remove the padding using attention_mask -- FIXME\n        _input_ids = input_ids\n        input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n        labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n\n        new_input_embeds = []\n        new_labels = []\n        cur_image_idx = 0\n        # rank_print(\"Inserting Images embedding\")\n        for batch_idx, cur_input_ids in enumerate(input_ids):\n            num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n            # rank0_print(num_images)\n            if num_images == 0:\n                try:\n                    cur_image_features = image_features[cur_image_idx]\n                except IndexError:\n                    try:\n                        cur_image_features = image_features[cur_image_idx - 1]\n                    except IndexError:\n                        pass\n                cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\n                cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n                new_input_embeds.append(cur_input_embeds)\n                new_labels.append(labels[batch_idx])\n                cur_image_idx += 1\n                continue\n\n            image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\n            cur_input_ids_noim = []\n            cur_labels = labels[batch_idx]\n            cur_labels_noim = []\n            for i in range(len(image_token_indices) - 1):\n                cur_input_ids_noim.append(cur_input_ids[image_token_indices[i] + 1 : image_token_indices[i + 1]])\n                cur_labels_noim.append(cur_labels[image_token_indices[i] + 1 : image_token_indices[i + 1]])\n            split_sizes = [x.shape[0] for x in cur_labels_noim]\n            cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\n            cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n            cur_new_input_embeds = []\n            cur_new_labels = []\n\n            for i in range(num_images + 1):\n                cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n                cur_new_labels.append(cur_labels_noim[i])\n                if i < num_images:\n                    try:\n                        cur_image_features = image_features[cur_image_idx]\n                    except IndexError:\n                        cur_image_features = image_features[cur_image_idx - 1]\n                    cur_image_idx += 1\n                    cur_new_input_embeds.append(cur_image_features)\n                    cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n\n            cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n\n            # import pdb; pdb.set_trace()\n            cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n            cur_new_labels = torch.cat(cur_new_labels)\n\n            new_input_embeds.append(cur_new_input_embeds)\n            new_labels.append(cur_new_labels)\n\n        # Truncate sequences to max length as image embeddings can make the sequence longer\n        tokenizer_model_max_length = getattr(self.config, \"tokenizer_model_max_length\", None)\n        # rank_print(\"Finishing Inserting\")\n\n        new_input_embeds = [x[:tokenizer_model_max_length] for x, modality in zip(new_input_embeds, modalities)]\n        new_labels = [x[:tokenizer_model_max_length] for x, modality in zip(new_labels, modalities)]\n        # TODO: Hard code for control loss spike\n        # if tokenizer_model_max_length is not None:\n        #     new_input_embeds = [x[:4096] if modality != \"video\" else x[:tokenizer_model_max_length] for x, modality in zip(new_input_embeds, modalities)]\n        #     new_labels = [x[:4096] if modality != \"video\" else x[:tokenizer_model_max_length] for x, modality in zip(new_labels, modalities)]\n\n        # Combine them\n        max_len = max(x.shape[0] for x in new_input_embeds)\n        batch_size = len(new_input_embeds)\n\n        new_input_embeds_padded = []\n        new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n        attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n        position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\n        # rank0_print(\"Prepare pos id\")\n\n        for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n            cur_len = cur_new_embed.shape[0]\n            if getattr(self.config, \"tokenizer_padding_side\", \"right\") == \"left\":\n                new_input_embeds_padded.append(torch.cat((torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device), cur_new_embed), dim=0))\n                if cur_len > 0:\n                    new_labels_padded[i, -cur_len:] = cur_new_labels\n                    attention_mask[i, -cur_len:] = True\n                    position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n            else:\n                new_input_embeds_padded.append(torch.cat((cur_new_embed, torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)), dim=0))\n                if cur_len > 0:\n                    new_labels_padded[i, :cur_len] = cur_new_labels\n                    attention_mask[i, :cur_len] = True\n                    position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n\n        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n        # rank0_print(\"tokenizer padding\")\n\n        if _labels is None:\n            new_labels = None\n        else:\n            new_labels = new_labels_padded\n\n        if _attention_mask is None:\n            attention_mask = None\n        else:\n            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n\n        if _position_ids is None:\n            position_ids = None\n        if getattr(self.config, \"use_pos_skipping\", False) and self.training:\n            position_ids = torch.arange(new_input_embeds.size(1), device=new_input_embeds.device).unsqueeze(0).to(new_input_embeds.device)\n            split_position = random.randint(0, new_input_embeds.size(1))\n            left_add = random.randint(0, self.config.pos_skipping_range)\n            right_add = random.randint(left_add, self.config.pos_skipping_range)\n            position_ids[:, :split_position] += left_add\n            position_ids[:, split_position:] += right_add\n        # rank0_print(\"Finish preparing\")\n        return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels\n\n    def initialize_vision_tokenizer(self, model_args, tokenizer):\n        if model_args.mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n\n        if model_args.mm_use_im_start_end:\n            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n\n            if num_new_tokens > 0:\n                input_embeddings = self.get_input_embeddings().weight.data\n                output_embeddings = self.get_output_embeddings().weight.data\n\n                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\n            if model_args.tune_mm_mlp_adapter:\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = True\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n\n            if model_args.pretrain_mm_mlp_adapter:\n                mm_projector_weights = torch.load(model_args.pretrain_mm_mlp_adapter, map_location=\"cpu\")\n                embed_tokens_weight = mm_projector_weights[\"model.embed_tokens.weight\"]\n                assert num_new_tokens == 2\n                if input_embeddings.shape == embed_tokens_weight.shape:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n                elif embed_tokens_weight.shape[0] == num_new_tokens:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n                else:\n                    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n        elif model_args.mm_use_im_patch_token:\n            if model_args.tune_mm_mlp_adapter:\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = False\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/__init__.py", "content": "from .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\nfrom .factory import create_model, create_model_and_transforms, create_model_from_pretrained, get_tokenizer\nfrom .factory import list_models, add_model_config, get_model_config, load_checkpoint\nfrom .loss import ClipLoss\nfrom .model import CLIP, CustomCLIP, CLIPTextCfg, CLIPVisionCfg, convert_weights_to_lp, convert_weights_to_fp16, trace_model, get_cast_dtype\nfrom .openai import load_openai_model, list_openai_models\nfrom .pretrained import list_pretrained, list_pretrained_models_by_tag, list_pretrained_tags_by_model, get_pretrained_url, download_pretrained_from_url, is_pretrained_cfg, get_pretrained_cfg, download_pretrained\nfrom .tokenizer import SimpleTokenizer, tokenize\nfrom .transform import image_transform\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/factory.py", "content": "import json\nimport logging\nimport os\nimport pathlib\nimport re\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union, Dict, Any\nimport torch\n\ntry:\n    import deepspeed\nexcept ImportError:\n    deepspeed = None\n\nfrom .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\nfrom .model import CLIP, CustomCLIP, convert_weights_to_lp, convert_to_custom_text_state_dict, get_cast_dtype\nfrom .openai import load_openai_model\nfrom .pretrained import is_pretrained_cfg, get_pretrained_cfg, download_pretrained, list_pretrained_tags_by_model\nfrom .transform import image_transform\nfrom .tokenizer import HFTokenizer, tokenize\nfrom .utils import resize_clip_pos_embed, resize_evaclip_pos_embed, resize_visual_pos_embed, resize_eva_pos_embed\n\n\n_MODEL_CONFIG_PATHS = [Path(__file__).parent / f\"model_configs/\"]\n_MODEL_CONFIGS = {}  # directory (model_name: config) of model architecture configs\n\n\ndef _natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\n\n\ndef _rescan_model_configs():\n    global _MODEL_CONFIGS\n\n    config_ext = (\".json\",)\n    config_files = []\n    for config_path in _MODEL_CONFIG_PATHS:\n        if config_path.is_file() and config_path.suffix in config_ext:\n            config_files.append(config_path)\n        elif config_path.is_dir():\n            for ext in config_ext:\n                config_files.extend(config_path.glob(f\"*{ext}\"))\n\n    for cf in config_files:\n        with open(cf, \"r\", encoding=\"utf8\") as f:\n            model_cfg = json.load(f)\n            if all(a in model_cfg for a in (\"embed_dim\", \"vision_cfg\", \"text_cfg\")):\n                _MODEL_CONFIGS[cf.stem] = model_cfg\n\n    _MODEL_CONFIGS = dict(sorted(_MODEL_CONFIGS.items(), key=lambda x: _natural_key(x[0])))\n\n\n_rescan_model_configs()  # initial populate of model config registry\n\n\ndef list_models():\n    \"\"\"enumerate available model architectures based on config files\"\"\"\n    return list(_MODEL_CONFIGS.keys())\n\n\ndef add_model_config(path):\n    \"\"\"add model config path or file and update registry\"\"\"\n    if not isinstance(path, Path):\n        path = Path(path)\n    _MODEL_CONFIG_PATHS.append(path)\n    _rescan_model_configs()\n\n\ndef get_model_config(model_name):\n    if model_name in _MODEL_CONFIGS:\n        return deepcopy(_MODEL_CONFIGS[model_name])\n    else:\n        return None\n\n\ndef get_tokenizer(model_name):\n    config = get_model_config(model_name)\n    tokenizer = HFTokenizer(config[\"text_cfg\"][\"hf_tokenizer_name\"]) if \"hf_tokenizer_name\" in config[\"text_cfg\"] else tokenize\n    return tokenizer\n\n\n# loading openai CLIP weights when is_openai=True for training\ndef load_state_dict(checkpoint_path: str, map_location: str = \"cpu\", model_key: str = \"model|module|state_dict\", is_openai: bool = False, skip_list: list = []):\n    if is_openai:\n        model = torch.jit.load(checkpoint_path, map_location=\"cpu\").eval()\n        state_dict = model.state_dict()\n        for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n            state_dict.pop(key, None)\n    else:\n        checkpoint = torch.load(checkpoint_path, map_location=map_location)\n        for mk in model_key.split(\"|\"):\n            if isinstance(checkpoint, dict) and mk in checkpoint:\n                state_dict = checkpoint[mk]\n                break\n            else:\n                state_dict = checkpoint\n        if next(iter(state_dict.items()))[0].startswith(\"module\"):\n            state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n    for k in skip_list:\n        if k in list(state_dict.keys()):\n            logging.info(f\"Removing key {k} from pretrained checkpoint\")\n            del state_dict[k]\n\n    if os.getenv(\"RoPE\") == \"1\":\n        for k in list(state_dict.keys()):\n            if \"freqs_cos\" in k or \"freqs_sin\" in k:\n                del state_dict[k]\n    return state_dict\n\n\ndef load_checkpoint(model, checkpoint_path, model_key=\"model|module|state_dict\", strict=True):\n    state_dict = load_state_dict(checkpoint_path, model_key=model_key, is_openai=False)\n    # detect old format and make compatible with new format\n    if \"positional_embedding\" in state_dict and not hasattr(model, \"positional_embedding\"):\n        state_dict = convert_to_custom_text_state_dict(state_dict)\n    if \"text.logit_scale\" in state_dict and hasattr(model, \"logit_scale\"):\n        state_dict[\"logit_scale\"] = state_dict[\"text.logit_scale\"]\n        del state_dict[\"text.logit_scale\"]\n\n    # resize_clip_pos_embed for CLIP and open CLIP\n    if \"visual.positional_embedding\" in state_dict:\n        resize_clip_pos_embed(state_dict, model)\n    # specified to eva_vit_model\n    elif \"visual.pos_embed\" in state_dict:\n        resize_evaclip_pos_embed(state_dict, model)\n\n    # resize_clip_pos_embed(state_dict, model)\n    incompatible_keys = model.load_state_dict(state_dict, strict=strict)\n    logging.info(f\"incompatible_keys.missing_keys: {incompatible_keys.missing_keys}\")\n    return incompatible_keys\n\n\ndef load_clip_visual_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n\n    for k in list(state_dict.keys()):\n        if not k.startswith(\"visual.\"):\n            del state_dict[k]\n    for k in list(state_dict.keys()):\n        if k.startswith(\"visual.\"):\n            new_k = k[7:]\n            state_dict[new_k] = state_dict[k]\n            del state_dict[k]\n    return state_dict\n\n\ndef load_clip_text_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n\n    for k in list(state_dict.keys()):\n        if k.startswith(\"visual.\"):\n            del state_dict[k]\n    return state_dict\n\n\ndef get_pretrained_tag(pretrained_model):\n    pretrained_model = pretrained_model.lower()\n    if \"laion\" in pretrained_model or \"open_clip\" in pretrained_model:\n        return \"open_clip\"\n    elif \"openai\" in pretrained_model:\n        return \"clip\"\n    elif \"eva\" in pretrained_model and \"clip\" in pretrained_model:\n        return \"eva_clip\"\n    else:\n        return \"other\"\n\n\ndef load_zero_partitions(model, state_dict, is_deepspeed_zero3_enabled, pretrained_model_path, ignore_mismatched_sizes=False):\n    \"\"\"\n    adept from pytorch lightning and transformers\n    with deepspeed.zero.Init():\n        model = MyModel()\n    state_dict = torch.load(model_path, map_location=\"cpu\")\n    load_zero_partitions(model, prefix=\"\")\n    \"\"\"\n\n    # because zero3 puts placeholders in model params, this context\n    # manager gathers (unpartitions) the params of the current layer, then loads from\n    # the state dict and then re-partitions them again\n    model_state_dict = model.state_dict()\n    expected_keys = list(model_state_dict.keys())\n    loaded_keys = list(state_dict.keys())\n    missing_keys = list(set(expected_keys) - set(loaded_keys))\n    unexpected_keys = list(set(loaded_keys) - set(expected_keys))\n\n    # Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\n    # matching the weights in the model.\n    mismatched_keys = []\n    if ignore_mismatched_sizes:\n        for checkpoint_key in loaded_keys:\n            model_key = checkpoint_key\n\n            if model_key in model_state_dict and state_dict[checkpoint_key].shape != model_state_dict[model_key].shape:\n                mismatched_keys.append((checkpoint_key, state_dict[checkpoint_key].shape, model_state_dict[model_key].shape))\n                del state_dict[checkpoint_key]\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(state_dict, \"_metadata\", None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    error_msgs = []\n\n    # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants\n    # so we need to apply the function recursively.\n    def load(module, prefix=\"\"):\n        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n        args = (state_dict, prefix, local_metadata, True, [], [], error_msgs)\n        if is_deepspeed_zero3_enabled:\n            # because zero3 puts placeholders in model params, this context\n            # manager gathers (unpartitions) the params of the current layer, then loads from\n            # the state dict and then re-partitions them again\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\n                if torch.distributed.get_rank() == 0:\n                    module._load_from_state_dict(*args)\n        else:\n            module._load_from_state_dict(*args)\n\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + \".\")\n\n    # Make sure we are able to load base models as well as derived models (with heads)\n    start_prefix = \"\"\n    model_to_load = model\n    load(model_to_load, prefix=start_prefix)\n    del state_dict\n    if len(error_msgs) > 0:\n        error_msg = \"\\n\\t\".join(error_msgs)\n        if \"size mismatch\" in error_msg:\n            error_msg += \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n        raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n    if len(unexpected_keys) > 0:\n        logging.warning(\n            f\"Some weights of the model checkpoint at {pretrained_model_path} were not used when\"\n            f\" initializing {model.__class__.__name__}: {unexpected_keys}\\n- This IS expected if you are\"\n            f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task or\"\n            \" with another architecture (e.g. initializing a BertForSequenceClassification model from a\"\n            \" BertForPreTraining model).\\n- This IS NOT expected if you are initializing\"\n            f\" {model.__class__.__name__} from the checkpoint of a model that you expect to be exactly identical\"\n            \" (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n        )\n    else:\n        logging.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n    if len(missing_keys) > 0:\n        logging.warning(\n            f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n            f\" {pretrained_model_path} and are newly initialized: {missing_keys}\\nYou should probably\"\n            \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n        )\n    elif len(mismatched_keys) == 0:\n        logging.info(\n            f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at\"\n            f\" {pretrained_model_path}.\\nIf your task is similar to the task the model of the checkpoint\"\n            f\" was trained on, you can already use {model.__class__.__name__} for predictions without further\"\n            \" training.\"\n        )\n    if len(mismatched_keys) > 0:\n        mismatched_warning = \"\\n\".join([f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\" for key, shape1, shape2 in mismatched_keys])\n        logging.warning(\n            f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n            f\" {pretrained_model_path} and are newly initialized because the shapes did not\"\n            f\" match:\\n{mismatched_warning}\\nYou should probably TRAIN this model on a down-stream task to be able\"\n            \" to use it for predictions and inference.\"\n        )\n\n\ndef load_pretrained_checkpoint(model, visual_checkpoint_path, text_checkpoint_path, strict=True, visual_model=None, text_model=None, model_key=\"model|module|state_dict\", skip_list=[]):\n    visual_tag = get_pretrained_tag(visual_model)\n    text_tag = get_pretrained_tag(text_model)\n\n    logging.info(f\"num of model state_dict keys: {len(model.state_dict().keys())}\")\n    visual_incompatible_keys, text_incompatible_keys = None, None\n    if visual_checkpoint_path:\n        if visual_tag == \"eva_clip\" or visual_tag == \"open_clip\":\n            visual_state_dict = load_clip_visual_state_dict(visual_checkpoint_path, is_openai=False, skip_list=skip_list)\n        elif visual_tag == \"clip\":\n            visual_state_dict = load_clip_visual_state_dict(visual_checkpoint_path, is_openai=True, skip_list=skip_list)\n        else:\n            visual_state_dict = load_state_dict(visual_checkpoint_path, model_key=model_key, is_openai=False, skip_list=skip_list)\n\n        # resize_clip_pos_embed for CLIP and open CLIP\n        if \"positional_embedding\" in visual_state_dict:\n            resize_visual_pos_embed(visual_state_dict, model)\n        # specified to EVA model\n        elif \"pos_embed\" in visual_state_dict:\n            resize_eva_pos_embed(visual_state_dict, model)\n\n        visual_incompatible_keys = model.visual.load_state_dict(visual_state_dict, strict=strict)\n        logging.info(f\"num of loaded visual_state_dict keys: {len(visual_state_dict.keys())}\")\n        logging.info(f\"visual_incompatible_keys.missing_keys: {visual_incompatible_keys.missing_keys}\")\n\n    if text_checkpoint_path:\n        if text_tag == \"eva_clip\" or text_tag == \"open_clip\":\n            text_state_dict = load_clip_text_state_dict(text_checkpoint_path, is_openai=False, skip_list=skip_list)\n        elif text_tag == \"clip\":\n            text_state_dict = load_clip_text_state_dict(text_checkpoint_path, is_openai=True, skip_list=skip_list)\n        else:\n            text_state_dict = load_state_dict(visual_checkpoint_path, model_key=model_key, is_openai=False, skip_list=skip_list)\n\n        text_incompatible_keys = model.text.load_state_dict(text_state_dict, strict=strict)\n\n        logging.info(f\"num of loaded text_state_dict keys: {len(text_state_dict.keys())}\")\n        logging.info(f\"text_incompatible_keys.missing_keys: {text_incompatible_keys.missing_keys}\")\n\n    return visual_incompatible_keys, text_incompatible_keys\n\n\ndef create_model(\n    model_name: str,\n    pretrained: Optional[str] = None,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    pretrained_image: str = \"\",\n    pretrained_text: str = \"\",\n    pretrained_hf: bool = True,\n    pretrained_visual_model: str = None,\n    pretrained_text_model: str = None,\n    cache_dir: Optional[str] = None,\n    skip_list: list = [],\n):\n    model_name = model_name.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n    if isinstance(device, str):\n        device = torch.device(device)\n\n    if pretrained and pretrained.lower() == \"openai\":\n        logging.info(f\"Loading pretrained {model_name} from OpenAI.\")\n        model = load_openai_model(\n            model_name,\n            precision=precision,\n            device=device,\n            jit=jit,\n            cache_dir=cache_dir,\n        )\n    else:\n        model_cfg = get_model_config(model_name)\n        if model_cfg is not None:\n            logging.info(f\"Loaded {model_name} model config.\")\n        else:\n            logging.error(f\"Model config for {model_name} not found; available models {list_models()}.\")\n            raise RuntimeError(f\"Model config for {model_name} not found.\")\n\n        if \"rope\" in model_cfg.get(\"vision_cfg\", {}):\n            if model_cfg[\"vision_cfg\"][\"rope\"]:\n                os.environ[\"RoPE\"] = \"1\"\n        else:\n            os.environ[\"RoPE\"] = \"0\"\n\n        if force_quick_gelu:\n            # override for use of QuickGELU on non-OpenAI transformer models\n            model_cfg[\"quick_gelu\"] = True\n\n        if force_patch_dropout is not None:\n            # override the default patch dropout value\n            model_cfg[\"vision_cfg\"][\"patch_dropout\"] = force_patch_dropout\n\n        cast_dtype = get_cast_dtype(precision)\n        custom_clip = model_cfg.pop(\"custom_text\", False) or force_custom_clip or (\"hf_model_name\" in model_cfg[\"text_cfg\"])\n\n        if custom_clip:\n            if \"hf_model_name\" in model_cfg.get(\"text_cfg\", {}):\n                model_cfg[\"text_cfg\"][\"hf_model_pretrained\"] = pretrained_hf\n            model = CustomCLIP(**model_cfg, cast_dtype=cast_dtype)\n        else:\n            model = CLIP(**model_cfg, cast_dtype=cast_dtype)\n\n        pretrained_cfg = {}\n        if pretrained:\n            checkpoint_path = \"\"\n            pretrained_cfg = get_pretrained_cfg(model_name, pretrained)\n            if pretrained_cfg:\n                checkpoint_path = download_pretrained(pretrained_cfg, cache_dir=cache_dir)\n            elif os.path.exists(pretrained):\n                checkpoint_path = pretrained\n\n            if checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name} weights ({pretrained}).\")\n                load_checkpoint(model, checkpoint_path, model_key=\"model|module|state_dict\", strict=False)\n            else:\n                error_str = f\"Pretrained weights ({pretrained}) not found for model {model_name}.\" f\"Available pretrained tags ({list_pretrained_tags_by_model(model_name)}.\"\n                logging.warning(error_str)\n                raise RuntimeError(error_str)\n        else:\n            visual_checkpoint_path = \"\"\n            text_checkpoint_path = \"\"\n\n            if pretrained_image:\n                pretrained_visual_model = pretrained_visual_model.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n                pretrained_image_cfg = get_pretrained_cfg(pretrained_visual_model, pretrained_image)\n                if \"timm_model_name\" in model_cfg.get(\"vision_cfg\", {}):\n                    # pretrained weight loading for timm models set via vision_cfg\n                    model_cfg[\"vision_cfg\"][\"timm_model_pretrained\"] = True\n                elif pretrained_image_cfg:\n                    visual_checkpoint_path = download_pretrained(pretrained_image_cfg, cache_dir=cache_dir)\n                elif os.path.exists(pretrained_image):\n                    visual_checkpoint_path = pretrained_image\n                else:\n                    logging.warning(f\"Pretrained weights ({visual_checkpoint_path}) not found for model {model_name}.visual.\")\n                    raise RuntimeError(f\"Pretrained weights ({visual_checkpoint_path}) not found for model {model_name}.visual.\")\n\n            if pretrained_text:\n                pretrained_text_model = pretrained_text_model.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n                pretrained_text_cfg = get_pretrained_cfg(pretrained_text_model, pretrained_text)\n                if pretrained_image_cfg:\n                    text_checkpoint_path = download_pretrained(pretrained_text_cfg, cache_dir=cache_dir)\n                elif os.path.exists(pretrained_text):\n                    text_checkpoint_path = pretrained_text\n                else:\n                    logging.warning(f\"Pretrained weights ({text_checkpoint_path}) not found for model {model_name}.text.\")\n                    raise RuntimeError(f\"Pretrained weights ({text_checkpoint_path}) not found for model {model_name}.text.\")\n\n            if visual_checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name}.visual weights ({visual_checkpoint_path}).\")\n            if text_checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name}.text weights ({text_checkpoint_path}).\")\n\n            if visual_checkpoint_path or text_checkpoint_path:\n                load_pretrained_checkpoint(model, visual_checkpoint_path, text_checkpoint_path, strict=False, visual_model=pretrained_visual_model, text_model=pretrained_text_model, model_key=\"model|module|state_dict\", skip_list=skip_list)\n\n        if \"fp16\" in precision or \"bf16\" in precision:\n            logging.info(f\"convert precision to {precision}\")\n            model = model.to(torch.bfloat16) if \"bf16\" in precision else model.to(torch.float16)\n\n        # model.to(device=device)\n\n        # set image / mean metadata from pretrained_cfg if available, or use default\n        model.visual.image_mean = pretrained_cfg.get(\"mean\", None) or OPENAI_DATASET_MEAN\n        model.visual.image_std = pretrained_cfg.get(\"std\", None) or OPENAI_DATASET_STD\n\n        if jit:\n            model = torch.jit.script(model)\n\n    return model\n\n\ndef create_model_and_transforms(\n    model_name: str,\n    pretrained: Optional[str] = None,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    pretrained_image: str = \"\",\n    pretrained_text: str = \"\",\n    pretrained_hf: bool = True,\n    pretrained_visual_model: str = None,\n    pretrained_text_model: str = None,\n    image_mean: Optional[Tuple[float, ...]] = None,\n    image_std: Optional[Tuple[float, ...]] = None,\n    cache_dir: Optional[str] = None,\n    skip_list: list = [],\n):\n    model = create_model(\n        model_name,\n        pretrained,\n        precision=precision,\n        device=device,\n        jit=jit,\n        force_quick_gelu=force_quick_gelu,\n        force_custom_clip=force_custom_clip,\n        force_patch_dropout=force_patch_dropout,\n        pretrained_image=pretrained_image,\n        pretrained_text=pretrained_text,\n        pretrained_hf=pretrained_hf,\n        pretrained_visual_model=pretrained_visual_model,\n        pretrained_text_model=pretrained_text_model,\n        cache_dir=cache_dir,\n        skip_list=skip_list,\n    )\n\n    image_mean = image_mean or getattr(model.visual, \"image_mean\", None)\n    image_std = image_std or getattr(model.visual, \"image_std\", None)\n    preprocess_train = image_transform(model.visual.image_size, is_train=True, mean=image_mean, std=image_std)\n    preprocess_val = image_transform(model.visual.image_size, is_train=False, mean=image_mean, std=image_std)\n\n    return model, preprocess_train, preprocess_val\n\n\ndef create_model_from_pretrained(\n    model_name: str,\n    pretrained: str,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    return_transform: bool = True,\n    image_mean: Optional[Tuple[float, ...]] = None,\n    image_std: Optional[Tuple[float, ...]] = None,\n    cache_dir: Optional[str] = None,\n    is_frozen: bool = False,\n):\n    if not is_pretrained_cfg(model_name, pretrained) and not os.path.exists(pretrained):\n        raise RuntimeError(f\"{pretrained} is not a valid pretrained cfg or checkpoint for {model_name}.\" f\" Use open_clip.list_pretrained() to find one.\")\n\n    model = create_model(\n        model_name,\n        pretrained,\n        precision=precision,\n        device=device,\n        jit=jit,\n        force_quick_gelu=force_quick_gelu,\n        force_custom_clip=force_custom_clip,\n        force_patch_dropout=force_patch_dropout,\n        cache_dir=cache_dir,\n    )\n\n    if is_frozen:\n        for param in model.parameters():\n            param.requires_grad = False\n\n    if not return_transform:\n        return model\n\n    image_mean = image_mean or getattr(model.visual, \"image_mean\", None)\n    image_std = image_std or getattr(model.visual, \"image_std\", None)\n    preprocess = image_transform(model.visual.image_size, is_train=False, mean=image_mean, std=image_std)\n\n    return model, preprocess\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/builder.py", "content": "import os\nfrom .clip_encoder import CLIPVisionTower, CLIPVisionTowerS2\n# from .imagebind import ImageBindWrapper\n# from .open_clip_encoder import OpenCLIPVisionTower\n# from .hf_vision import HFVisionTower\n# from .siglip_encoder import SigLipVisionTower\n# from .internvit_encoder import InternVisionTower\n# from .eva_clip.eva_clip_encoder import EvaClipVisionTower\n# from .dev_eva_clip.eva_vit import EvaViTWrapper\n\n\ndef build_vision_tower(vision_tower_cfg, **kwargs):\n    vision_tower = getattr(vision_tower_cfg, \"mm_vision_tower\", getattr(vision_tower_cfg, \"vision_tower\", None))\n    is_absolute_path_exists = os.path.exists(vision_tower)\n    use_s2 = getattr(vision_tower_cfg, \"s2\", False)\n    if is_absolute_path_exists or vision_tower.startswith(\"openai\") or vision_tower.startswith(\"laion\") or \"ShareGPT4V\" in vision_tower:\n        if use_s2:\n            return CLIPVisionTowerS2(vision_tower, args=vision_tower_cfg, **kwargs)\n        else:\n            return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    # elif \"siglip\" in vision_tower:\n    #     return SigLipVisionTower(vision_tower, vision_tower_cfg=vision_tower_cfg, **kwargs)\n    # elif vision_tower.startswith(\"hf:\"):\n    #     return HFVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    # elif vision_tower in [\"imagebind_huge\"]:\n    #     return ImageBindWrapper(vision_tower, args=vision_tower_cfg, **kwargs)\n    # elif vision_tower.startswith(\"open_clip_hub\"):\n    #     return OpenCLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    # elif 'internvit' in vision_tower.lower():\n    #     return InternVisionTower(vision_tower, vision_tower_cfg=vision_tower_cfg, **kwargs)\n    # elif \"internal-eva\" in vision_tower.lower() or \"eva02\" in vision_tower.lower():\n    #     return EvaClipVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    # elif vision_tower in [\"EVA-CLIP-8B\", \"EVA-CLIP-8B-plus\"]:\n    #     return EvaViTWrapper(vision_tower, args=vision_tower_cfg, **kwargs)\n\n    raise ValueError(f\"Unknown vision tower: {vision_tower}\")\n"}
{"type": "source_file", "path": "instellavl/constants.py", "content": "CONTROLLER_HEART_BEAT_EXPIRATION = 30\nWORKER_HEART_BEAT_INTERVAL = 15\n\nLOGDIR = \".\"\n\n# Model Constants\nIGNORE_INDEX = -100\nIMAGE_TOKEN_INDEX = -200\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/modified_resnet.py", "content": "from collections import OrderedDict\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .utils import freeze_batch_norm_2d\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.act1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.act2 = nn.ReLU(inplace=True)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.act3 = nn.ReLU(inplace=True)\n\n        self.downsample = None\n        self.stride = stride\n\n        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([(\"-1\", nn.AvgPool2d(stride)), (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)), (\"1\", nn.BatchNorm2d(planes * self.expansion))]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n\n        out = self.act1(self.bn1(self.conv1(x)))\n        out = self.act2(self.bn2(self.conv2(out)))\n        out = self.avgpool(out)\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.act3(out)\n        return out\n\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim**2 + 1, embed_dim) / embed_dim**0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x,\n            key=x,\n            value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0.0,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False,\n        )\n\n        return x[0]\n\n\nclass ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n\n    def __init__(self, layers, output_dim, heads, image_size=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim\n        self.image_size = image_size\n\n        # the 3-layer stem\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.act2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.act3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n        # residual layers\n        self._inplanes = width  # this is a *mutable* variable used during construction\n        self.layer1 = self._make_layer(width, layers[0])\n        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n\n        embed_dim = width * 32  # the ResNet feature dimension\n        self.attnpool = AttentionPool2d(image_size // 32, embed_dim, heads, output_dim)\n\n        self.init_parameters()\n\n    def _make_layer(self, planes, blocks, stride=1):\n        layers = [Bottleneck(self._inplanes, planes, stride)]\n\n        self._inplanes = planes * Bottleneck.expansion\n        for _ in range(1, blocks):\n            layers.append(Bottleneck(self._inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def init_parameters(self):\n        if self.attnpool is not None:\n            std = self.attnpool.c_proj.in_features**-0.5\n            nn.init.normal_(self.attnpool.q_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.k_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.v_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.c_proj.weight, std=std)\n\n        for resnet_block in [self.layer1, self.layer2, self.layer3, self.layer4]:\n            for name, param in resnet_block.named_parameters():\n                if name.endswith(\"bn3.weight\"):\n                    nn.init.zeros_(param)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n        if freeze_bn_stats:\n            freeze_batch_norm_2d(self)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        # FIXME support for non-transformer\n        pass\n\n    def stem(self, x):\n        x = self.act1(self.bn1(self.conv1(x)))\n        x = self.act2(self.bn2(self.conv2(x)))\n        x = self.act3(self.bn3(self.conv3(x)))\n        x = self.avgpool(x)\n        return x\n\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.attnpool(x)\n\n        return x\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/openai.py", "content": "\"\"\" OpenAI pretrained model functions\n\nAdapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport os\nimport warnings\nfrom typing import List, Optional, Union\n\nimport torch\n\nfrom .model import build_model_from_openai_state_dict, convert_weights_to_lp, get_cast_dtype\nfrom .pretrained import get_pretrained_url, list_pretrained_models_by_tag, download_pretrained_from_url\n\n__all__ = [\"list_openai_models\", \"load_openai_model\"]\n\n\ndef list_openai_models() -> List[str]:\n    \"\"\"Returns the names of available CLIP models\"\"\"\n    return list_pretrained_models_by_tag(\"openai\")\n\n\ndef load_openai_model(\n    name: str,\n    precision: Optional[str] = None,\n    device: Optional[Union[str, torch.device]] = None,\n    jit: bool = True,\n    cache_dir: Optional[str] = None,\n):\n    \"\"\"Load a CLIP model\n\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n    precision: str\n        Model precision, if None defaults to 'fp32' if device == 'cpu' else 'fp16'.\n    device : Union[str, torch.device]\n        The device to put the loaded model\n    jit : bool\n        Whether to load the optimized JIT model (default) or more hackable non-JIT model.\n    cache_dir : Optional[str]\n        The directory to cache the downloaded model weights\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The CLIP model\n    preprocess : Callable[[PIL.Image], torch.Tensor]\n        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n    \"\"\"\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if precision is None:\n        precision = \"fp32\" if device == \"cpu\" else \"fp16\"\n\n    if get_pretrained_url(name, \"openai\"):\n        model_path = download_pretrained_from_url(get_pretrained_url(name, \"openai\"), cache_dir=cache_dir)\n    elif os.path.isfile(name):\n        model_path = name\n    else:\n        raise RuntimeError(f\"Model {name} not found; available models = {list_openai_models()}\")\n\n    try:\n        # loading JIT archive\n        model = torch.jit.load(model_path, map_location=device if jit else \"cpu\").eval()\n        state_dict = None\n    except RuntimeError:\n        # loading saved state dict\n        if jit:\n            warnings.warn(f\"File {model_path} is not a JIT archive. Loading as a state dict instead\")\n            jit = False\n        state_dict = torch.load(model_path, map_location=\"cpu\")\n\n    if not jit:\n        # Build a non-jit model from the OpenAI jitted model state dict\n        cast_dtype = get_cast_dtype(precision)\n        try:\n            model = build_model_from_openai_state_dict(state_dict or model.state_dict(), cast_dtype=cast_dtype)\n        except KeyError:\n            sd = {k[7:]: v for k, v in state_dict[\"state_dict\"].items()}\n            model = build_model_from_openai_state_dict(sd, cast_dtype=cast_dtype)\n\n        # model from OpenAI state dict is in manually cast fp16 mode, must be converted for AMP/fp32/bf16 use\n        model = model.to(device)\n        if precision.startswith(\"amp\") or precision == \"fp32\":\n            model.float()\n        elif precision == \"bf16\":\n            convert_weights_to_lp(model, dtype=torch.bfloat16)\n\n        return model\n\n    # patch the device names\n    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])\n    device_node = [n for n in device_holder.graph.findAllNodes(\"prim::Constant\") if \"Device\" in repr(n)][-1]\n\n    def patch_device(module):\n        try:\n            graphs = [module.graph] if hasattr(module, \"graph\") else []\n        except RuntimeError:\n            graphs = []\n\n        if hasattr(module, \"forward1\"):\n            graphs.append(module.forward1.graph)\n\n        for graph in graphs:\n            for node in graph.findAllNodes(\"prim::Constant\"):\n                if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\"cuda\"):\n                    node.copyAttributes(device_node)\n\n    model.apply(patch_device)\n    patch_device(model.encode_image)\n    patch_device(model.encode_text)\n\n    # patch dtype to float32 (typically for CPU)\n    if precision == \"fp32\":\n        float_holder = torch.jit.trace(lambda: torch.ones([]).float(), example_inputs=[])\n        float_input = list(float_holder.graph.findNode(\"aten::to\").inputs())[1]\n        float_node = float_input.node()\n\n        def patch_float(module):\n            try:\n                graphs = [module.graph] if hasattr(module, \"graph\") else []\n            except RuntimeError:\n                graphs = []\n\n            if hasattr(module, \"forward1\"):\n                graphs.append(module.forward1.graph)\n\n            for graph in graphs:\n                for node in graph.findAllNodes(\"aten::to\"):\n                    inputs = list(node.inputs())\n                    for i in [1, 2]:  # dtype can be the second or third argument to aten::to()\n                        if inputs[i].node()[\"value\"] == 5:\n                            inputs[i].node().copyAttributes(float_node)\n\n        model.apply(patch_float)\n        patch_float(model.encode_image)\n        patch_float(model.encode_text)\n        model.float()\n\n    # ensure image_size attr available at consistent location for both jit and non-jit\n    model.visual.image_size = model.input_resolution.item()\n    return model\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/model.py", "content": "\"\"\" CLIP Model\n\nAdapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nfrom functools import partial\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\ntry:\n    from .hf_model import HFTextEncoder\nexcept:\n    HFTextEncoder = None\nfrom .modified_resnet import ModifiedResNet\nfrom .timm_model import TimmModel\nfrom .eva_vit_model import EVAVisionTransformer\nfrom .transformer import LayerNorm, QuickGELU, Attention, VisionTransformer, TextTransformer\n\ntry:\n    from apex.normalization import FusedLayerNorm\nexcept:\n    FusedLayerNorm = LayerNorm\n    # print(\"Please 'pip install apex'\")\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass RMSnorm(nn.Module):\n    \"\"\"\n    adepted from transformers T5LayerNorm\n    \"\"\"\n\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Construct a layernorm module in the T5 style. No bias and no subtraction of mean.\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        # T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n        # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n        # half-precision inputs is done in fp32\n\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into half-precision if necessary\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return self.weight * hidden_states\n\n\n@dataclass\nclass CLIPVisionCfg:\n    layers: Union[Tuple[int, int, int, int], int] = 12\n    width: int = 768\n    head_width: int = 64\n    mlp_ratio: float = 4.0\n    patch_size: int = 16\n    image_size: Union[Tuple[int, int], int] = 224\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    patch_dropout: float = 0.0  # what fraction of patches to dropout during training (0 would mean disabled and no patches dropped) - 0.5 to 0.75 recommended in the paper for optimal results\n    global_average_pool: bool = False  # whether to global average pool the last embedding layer, instead of using CLS token (https://arxiv.org/abs/2205.01580)\n    drop_path_rate: Optional[float] = None  # drop path rate\n    timm_model_name: str = None  # a valid model name overrides layers, width, patch_size\n    timm_model_pretrained: bool = False  # use (imagenet) pretrained weights for named model\n    timm_pool: str = \"avg\"  # feature pooling for timm model ('abs_attn', 'rot_attn', 'avg', '')\n    timm_proj: str = \"linear\"  # linear projection for timm model output ('linear', 'mlp', '')\n    timm_proj_bias: bool = False  # enable bias final projection\n    eva_model_name: str = None  # a valid eva model name overrides layers, width, patch_size\n    qkv_bias: bool = True\n    fusedLN: bool = False\n    xattn: bool = False\n    postnorm: bool = False\n    rope: bool = False\n    pt_hw_seq_len: int = 16  # 224/14\n    intp_freq: bool = False\n    naiveswiglu: bool = False\n    subln: bool = False\n    use_rms_norm: bool = False\n\n\n@dataclass\nclass CLIPTextCfg:\n    context_length: int = 77\n    vocab_size: int = 49408\n    width: int = 512\n    heads: int = 8\n    layers: int = 12\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    hf_model_name: str = None\n    hf_tokenizer_name: str = None\n    hf_model_pretrained: bool = True\n    proj: str = \"mlp\"\n    pooler_type: str = \"mean_pooler\"\n    masked_language_modeling: bool = False\n    fusedLN: bool = False\n    xattn: bool = False\n    attn_mask: bool = True\n\n\ndef get_cast_dtype(precision: str):\n    cast_dtype = None\n    if precision == \"bf16\":\n        cast_dtype = torch.bfloat16\n    elif precision == \"fp16\":\n        cast_dtype = torch.float16\n    return cast_dtype\n\n\ndef _build_vision_tower(embed_dim: int, vision_cfg: CLIPVisionCfg, quick_gelu: bool = False, cast_dtype: Optional[torch.dtype] = None):\n    if isinstance(vision_cfg, dict):\n        vision_cfg = CLIPVisionCfg(**vision_cfg)\n\n    # OpenAI models are pretrained w/ QuickGELU but native nn.GELU is both faster and more\n    # memory efficient in recent PyTorch releases (>= 1.10).\n    # NOTE: timm models always use native GELU regardless of quick_gelu flag.\n    act_layer = QuickGELU if quick_gelu else nn.GELU\n\n    if vision_cfg.eva_model_name:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n\n        norm_layer = RMSnorm if vision_cfg.use_rms_norm else LayerNorm\n\n        visual = EVAVisionTransformer(\n            img_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            num_classes=embed_dim,\n            use_mean_pooling=vision_cfg.global_average_pool,  # False\n            init_values=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            embed_dim=vision_cfg.width,\n            depth=vision_cfg.layers,\n            num_heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            qkv_bias=vision_cfg.qkv_bias,\n            drop_path_rate=vision_cfg.drop_path_rate,\n            norm_layer=partial(norm_layer, eps=1e-6),\n            xattn=vision_cfg.xattn,\n            rope=vision_cfg.rope,\n            postnorm=vision_cfg.postnorm,\n            pt_hw_seq_len=vision_cfg.pt_hw_seq_len,  # 224/14\n            intp_freq=vision_cfg.intp_freq,\n            naiveswiglu=vision_cfg.naiveswiglu,\n            subln=vision_cfg.subln,\n        )\n    elif vision_cfg.timm_model_name:\n        visual = TimmModel(\n            vision_cfg.timm_model_name, pretrained=vision_cfg.timm_model_pretrained, pool=vision_cfg.timm_pool, proj=vision_cfg.timm_proj, proj_bias=vision_cfg.timm_proj_bias, embed_dim=embed_dim, image_size=vision_cfg.image_size\n        )\n        act_layer = nn.GELU  # so that text transformer doesn't use QuickGELU w/ timm models\n    elif isinstance(vision_cfg.layers, (tuple, list)):\n        vision_heads = vision_cfg.width * 32 // vision_cfg.head_width\n        visual = ModifiedResNet(layers=vision_cfg.layers, output_dim=embed_dim, heads=vision_heads, image_size=vision_cfg.image_size, width=vision_cfg.width)\n    else:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n        norm_layer = LayerNormFp32 if cast_dtype in (torch.float16, torch.bfloat16) else LayerNorm\n        visual = VisionTransformer(\n            image_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            width=vision_cfg.width,\n            layers=vision_cfg.layers,\n            heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            ls_init_value=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            global_average_pool=vision_cfg.global_average_pool,\n            output_dim=embed_dim,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n        )\n\n    return visual\n\n\ndef _build_text_tower(\n    embed_dim: int,\n    text_cfg: CLIPTextCfg,\n    quick_gelu: bool = False,\n    cast_dtype: Optional[torch.dtype] = None,\n):\n    if isinstance(text_cfg, dict):\n        text_cfg = CLIPTextCfg(**text_cfg)\n\n    if text_cfg.hf_model_name:\n        text = HFTextEncoder(text_cfg.hf_model_name, output_dim=embed_dim, tokenizer_name=text_cfg.hf_tokenizer_name, proj=text_cfg.proj, pooler_type=text_cfg.pooler_type, masked_language_modeling=text_cfg.masked_language_modeling)\n    else:\n        act_layer = QuickGELU if quick_gelu else nn.GELU\n        norm_layer = LayerNorm\n\n        text = TextTransformer(\n            context_length=text_cfg.context_length,\n            vocab_size=text_cfg.vocab_size,\n            width=text_cfg.width,\n            heads=text_cfg.heads,\n            layers=text_cfg.layers,\n            ls_init_value=text_cfg.ls_init_value,\n            output_dim=embed_dim,\n            act_layer=act_layer,\n            norm_layer=FusedLayerNorm if text_cfg.fusedLN else norm_layer,\n            xattn=text_cfg.xattn,\n            attn_mask=text_cfg.attn_mask,\n        )\n    return text\n\n\nclass CLIP(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        vision_cfg: CLIPVisionCfg,\n        text_cfg: CLIPTextCfg,\n        quick_gelu: bool = False,\n        cast_dtype: Optional[torch.dtype] = None,\n    ):\n        super().__init__()\n        self.visual = _build_vision_tower(embed_dim, vision_cfg, quick_gelu, cast_dtype)\n\n        text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n        self.transformer = text.transformer\n        self.vocab_size = text.vocab_size\n        self.token_embedding = text.token_embedding\n        self.positional_embedding = text.positional_embedding\n        self.ln_final = text.ln_final\n        self.text_projection = text.text_projection\n        self.register_buffer(\"attn_mask\", text.attn_mask, persistent=False)\n\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):\n        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n        self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.visual.set_grad_checkpointing(enable)\n        self.transformer.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"logit_scale\"}\n\n    def encode_image(self, image, normalize: bool = False):\n        features = self.visual(image)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def encode_text(self, text, normalize: bool = False):\n        cast_dtype = self.transformer.get_cast_dtype()\n\n        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.to(cast_dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x, attn_mask=self.attn_mask)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n        return F.normalize(x, dim=-1) if normalize else x\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image, normalize=True)\n        text_features = self.encode_text(text, normalize=True)\n        return image_features, text_features, self.logit_scale.exp()\n\n\nclass CustomCLIP(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        vision_cfg: CLIPVisionCfg,\n        text_cfg: CLIPTextCfg,\n        quick_gelu: bool = False,\n        cast_dtype: Optional[torch.dtype] = None,\n        itm_task: bool = False,\n    ):\n        super().__init__()\n        self.visual = _build_vision_tower(embed_dim, vision_cfg, quick_gelu, cast_dtype)\n        self.text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):\n        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n        self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)\n\n    def lock_text_tower(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):\n        self.text.lock(unlocked_layers, freeze_layer_norm)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.visual.set_grad_checkpointing(enable)\n        self.text.set_grad_checkpointing(enable)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"logit_scale\"}\n\n    def encode_image(self, image, normalize: bool = False):\n        features = self.visual(image)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def encode_text(self, text, normalize: bool = False):\n        features = self.text(text)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image, normalize=True)\n        text_features = self.encode_text(text, normalize=True)\n        return image_features, text_features, self.logit_scale.exp()\n\n\ndef convert_weights_to_lp(model: nn.Module, dtype=torch.float16):\n    \"\"\"Convert applicable model parameters to low-precision (bf16 or fp16)\"\"\"\n\n    def _convert_weights(l):\n\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.to(dtype)\n            if l.bias is not None:\n                l.bias.data = l.bias.data.to(dtype)\n\n        if isinstance(l, (nn.MultiheadAttention, Attention)):\n            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n                tensor = getattr(l, attr, None)\n                if tensor is not None:\n                    tensor.data = tensor.data.to(dtype)\n\n        if isinstance(l, nn.Parameter):\n            l.data = l.data.to(dtype)\n\n        for name in [\"text_projection\", \"proj\"]:\n            if hasattr(l, name) and isinstance(l, nn.Parameter):\n                attr = getattr(l, name, None)\n                if attr is not None:\n                    attr.data = attr.data.to(dtype)\n\n    model.apply(_convert_weights)\n\n\nconvert_weights_to_fp16 = convert_weights_to_lp  # backwards compat\n\n\n# used to maintain checkpoint compatibility\ndef convert_to_custom_text_state_dict(state_dict: dict):\n    if \"text_projection\" in state_dict:\n        # old format state_dict, move text tower -> .text\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            if any(k.startswith(p) for p in (\"text_projection\", \"positional_embedding\", \"token_embedding\", \"transformer\", \"ln_final\", \"logit_scale\")):\n                k = \"text.\" + k\n            new_state_dict[k] = v\n        return new_state_dict\n    return state_dict\n\n\ndef build_model_from_openai_state_dict(\n    state_dict: dict,\n    quick_gelu=True,\n    cast_dtype=torch.float16,\n):\n    vit = \"visual.proj\" in state_dict\n\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n        image_size = vision_patch_size * grid_size\n    else:\n        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n        vision_layers = tuple(counts)\n        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n        vision_patch_size = None\n        assert output_width**2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n        image_size = output_width * 32\n\n    embed_dim = state_dict[\"text_projection\"].shape[1]\n    context_length = state_dict[\"positional_embedding\"].shape[0]\n    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n    transformer_heads = transformer_width // 64\n    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"transformer.resblocks\")))\n\n    vision_cfg = CLIPVisionCfg(\n        layers=vision_layers,\n        width=vision_width,\n        patch_size=vision_patch_size,\n        image_size=image_size,\n    )\n    text_cfg = CLIPTextCfg(context_length=context_length, vocab_size=vocab_size, width=transformer_width, heads=transformer_heads, layers=transformer_layers)\n    model = CLIP(\n        embed_dim,\n        vision_cfg=vision_cfg,\n        text_cfg=text_cfg,\n        quick_gelu=quick_gelu,  # OpenAI models were trained with QuickGELU\n        cast_dtype=cast_dtype,\n    )\n\n    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n        state_dict.pop(key, None)\n\n    convert_weights_to_fp16(model)  # OpenAI state dicts are partially converted to float16\n    model.load_state_dict(state_dict)\n    return model.eval()\n\n\ndef trace_model(model, batch_size=256, device=torch.device(\"cpu\")):\n    model.eval()\n    image_size = model.visual.image_size\n    example_images = torch.ones((batch_size, 3, image_size, image_size), device=device)\n    example_text = torch.zeros((batch_size, model.context_length), dtype=torch.int, device=device)\n    model = torch.jit.trace_module(model, inputs=dict(forward=(example_images, example_text), encode_text=(example_text,), encode_image=(example_images,)))\n    model.visual.image_size = image_size\n    return model\n"}
{"type": "source_file", "path": "instellavl/model/__init__.py", "content": "\nAVAILABLE_MODELS = {\n    \"instellavl\": \"InstellaVLForCausalLM, InstellaVLConfig\"\n    # Add other models as needed\n}\n\nfor model_name, model_classes in AVAILABLE_MODELS.items():\n    try:\n        exec(f\"from .language_model.{model_name} import {model_classes}\")\n    except Exception as e:\n        print(f\"Failed to import {model_name} from instellavl.language_model.{model_name}. Error: {e}\")\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/hf_configs.py", "content": "# HF architecture dict:\narch_dict = {\n    # https://huggingface.co/docs/transformers/model_doc/roberta#roberta\n    \"roberta\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    # https://huggingface.co/docs/transformers/model_doc/xlm-roberta#transformers.XLMRobertaConfig\n    \"xlm-roberta\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    # https://huggingface.co/docs/transformers/model_doc/mt5#mt5\n    \"mt5\": {\n        \"config_names\": {\n            # unlimited seqlen\n            # https://github.com/google-research/text-to-text-transfer-transformer/issues/273\n            # https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/t5/modeling_t5.py#L374\n            \"context_length\": \"\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"d_model\",\n            \"heads\": \"num_heads\",\n            \"layers\": \"num_layers\",\n            \"layer_attr\": \"block\",\n            \"token_embeddings_attr\": \"embed_tokens\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    \"bert\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n}\n"}
{"type": "source_file", "path": "instellavl/mm_utils.py", "content": "# Modification CopyrightÂ© 2025 Advanced Micro Devices, Inc. All rights reserved.\n\nr\"\"\"This module provides various utility functions for processing images, including resizing, cropping, padding, \nand extracting patches. It also includes functions for processing images with different resolutions and \ntokenizing image prompts.\"\"\"\n\nimport re\nimport ast\nimport math\nimport torch\nimport base64\n\nfrom PIL import Image\nfrom io import BytesIO\nfrom typing import List, Tuple, Union, Any\nfrom instellavl.constants import IMAGE_TOKEN_INDEX\nfrom transformers import StoppingCriteria, PreTrainedTokenizer\n\ndef resize_and_center_crop(image: Image.Image, shortest_edge_length: int) -> Image.Image:\n    r\"\"\"\n    Resize the given image such that its shortest edge matches the specified length,\n    and then center crop it to a square of the same size.\n    \n    Args:\n        - image (`Image.Image`): The input image to be resized and cropped.\n        - shortest_edge_length (`int`): The length of the shortest edge after resizing.\n    \n    Returns:\n        `Image.Image`: The resized and center-cropped image.\n    \"\"\"\n    \n    # Calculate new dimensions and resize\n    aspect_ratio = float(image.width) / float(image.height)\n    if (aspect_ratio > 1):\n        new_width = int(shortest_edge_length * aspect_ratio)\n        new_height = shortest_edge_length\n    else:\n        new_width = shortest_edge_length\n        new_height = int(shortest_edge_length / aspect_ratio)\n    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)\n\n    # Calculate the position and perform the center crop\n    left = (new_width - shortest_edge_length) / 2\n    top = (new_height - shortest_edge_length) / 2\n    right = (new_width + shortest_edge_length) / 2\n    bottom = (new_height + shortest_edge_length) / 2\n    cropped_image = resized_image.crop((left, top, right, bottom))\n\n    return cropped_image\n\n\ndef auto_pad_images(image: Image.Image, grid_params: list) -> Image.Image:\n    r\"\"\"\n    Automatically pads an input image to match the closest aspect ratio from a list of grid parameters.\n    \n    Args:\n        - image (`Image.Image`): The input image to be padded. Must be a Pillow Image object.\n        - grid_params (`list`): A list of integers representing the grid parameters to determine the target aspect ratio.\n    \n    Returns:\n        `Image.Image`: The padded image with the closest aspect ratio from the grid parameters.\n    \n    Raises:\n        `AssertionError`: If the input is not a Pillow Image object or if the grid parameters list is empty.\n    \"\"\"\n\n    assert isinstance(image, Image.Image), \"Input should be a Pillow Image\"\n    assert len(grid_params) > 0, \"Grid parameters should not be empty\"\n\n    # Step 1: Calculate and find the closest aspect ratio\n    input_width, input_height = image.size\n    input_aspect_ratio = input_width / input_height\n    candidate_resolutions = [(w / h, w, h) for w in grid_params for h in grid_params]\n    closest_aspect_ratio = min(candidate_resolutions, key=lambda x: abs(input_aspect_ratio - x[0]))\n\n    candidate_resolutions = [(x[1], x[2]) for x in candidate_resolutions if abs(x[0] - closest_aspect_ratio[0]) < 1e-3]\n\n    target_resolution = min(candidate_resolutions, key=lambda res: abs(max(input_width, input_height) / max(res) - 1))\n\n    resize_width, resize_height = target_resolution\n    if input_width > input_height:\n        resize_height = int(resize_width / input_aspect_ratio)\n    else:\n        resize_width = int(resize_height * input_aspect_ratio)\n    resized_image = image.resize((resize_width, resize_height), Image.ANTIALIAS)\n\n    # Step 5: Pad the resized image if necessary to match the target resolution\n    pad_width = target_resolution[0] - resize_width\n    pad_height = target_resolution[1] - resize_height\n    padded_image = Image.new(\"RGB\", target_resolution, color=(0, 0, 0))\n    padded_image.paste(resized_image, (pad_width // 2, pad_height // 2))\n\n    return padded_image\n\n\ndef extract_patches(image: Image.Image, patch_size: int, overlap_ratio: float) -> List[Image.Image]:\n    r\"\"\"\n    Extracts patches from a given image with specified patch size and overlap ratio.\n    \n    Args:\n        - image (`Image.Image`): The input image from which patches are to be extracted. Must be a Pillow Image.\n        - patch_size (`int`): The size of each patch (both width and height). Must be greater than 0.\n        - overlap_ratio (`float`): The ratio of overlap between adjacent patches. Must be between 0 and 1 (exclusive).\n    \n    Returns:\n        `List[Image.Image]`: A list of extracted patches as Pillow Images.\n    \n    Raises:\n        `AssertionError`: If the input image is not a Pillow Image.\n        `AssertionError`: If the patch size is not greater than 0.\n        `AssertionError`: If the overlap ratio is not between 0 and 1.\n    \"\"\"\n\n    assert isinstance(image, Image.Image), \"Input should be a Pillow Image\"\n    assert patch_size > 0, \"Patch size should be greater than 0\"\n    assert 0 <= overlap_ratio < 1, \"Overlap ratio should be between 0 and 1\"\n\n    W, H = image.size\n    patches = []\n\n    stride = int(patch_size * (1 - overlap_ratio))\n\n    num_patches_y = (H - patch_size) // stride + 1\n    num_patches_x = (W - patch_size) // stride + 1\n\n    y_start = (H - (num_patches_y - 1) * stride - patch_size) // 2\n    x_start = (W - (num_patches_x - 1) * stride - patch_size) // 2\n\n    for y in range(y_start, y_start + num_patches_y * stride, stride):\n        for x in range(x_start, x_start + num_patches_x * stride, stride):\n            patch = image.crop((x, y, x + patch_size, y + patch_size))\n            patches.append(patch)\n\n    return patches\n\n\ndef process_highres_image_crop_split(image: Image.Image, data_args, processor=None) -> torch.Tensor:\n    \"\"\"\n    Process a high-resolution image by cropping and splitting it into patches.\n\n    Args:\n        - image (`PIL.Image.Image`): The input image to be processed.\n        - data_args: The data arguments containing crop and split resolutions.\n        - processor: The image processor object. If None, it will be taken from data_args.\n\n    Returns:\n        `torch.Tensor`: A tensor containing the processed image patches.\n    \"\"\"\n    crop_resolution = data_args.image_crop_resolution\n    split_resolution = data_args.image_split_resolution\n    if processor is None:\n        processor = data_args.image_processor\n    image_crop = resize_and_center_crop(image, crop_resolution)\n    image_patches = extract_patches(image_crop, patch_size=split_resolution, overlap_ratio=0)\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef process_highres_image(image: Image.Image, processor, grid_pinpoints: str) -> torch.Tensor:\n    r\"\"\"\n    Processes a high-resolution image by resizing, padding, and extracting patches.\n    \n    Args:\n        - image (`Image.Image`): The input image to be processed.\n        - processor: An object that contains image processing parameters and methods.\n        - grid_pinpoints (`str`): A comma-separated string of grid sizes to consider for resizing.\n    \n    Returns:\n        torch.Tensor: A tensor containing the processed image patches.\n    \"\"\"\n\n    grid_params = [int(x) for x in grid_pinpoints.split(\",\")]\n    width_height = max(image.size)\n    fit_grid_params = [x for x in grid_params if x >= width_height]\n    if len(fit_grid_params) == 0:\n        select_size = max(grid_params)\n    else:\n        select_size = min(fit_grid_params)\n    # FIXME: always select the 448\n    select_size = max(grid_params)\n    image_padded = expand2square(image, tuple(int(x * 255) for x in processor.image_mean))\n\n    # FIXME: this seems to be a bug that it always resizes instead of padding\n    image_original_resize = image.resize((processor.size[\"shortest_edge\"], processor.size[\"shortest_edge\"]))\n    image_padded = image_padded.resize((select_size, select_size))\n    image_patches = extract_patches(image_padded, patch_size=processor.size[\"shortest_edge\"], overlap_ratio=0)\n    image_patches = [image_original_resize] + image_patches\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef select_best_resolution(original_size: tuple, possible_resolutions: List[Tuple[int, int]]) -> tuple:\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        - original_size (`tuple`): The original size of the image in the format (width, height).\n        - possible_resolutions (`List[Tuple[int, int]]`): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        `tuple`: The best fit resolution in the format (width, height).\n    \"\"\"\n    original_width, original_height = original_size\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float(\"inf\")\n\n    for width, height in possible_resolutions:\n        # Calculate the downscaled size to keep the aspect ratio\n        scale = min(width / original_width, height / original_height)\n        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n\n        # Calculate effective and wasted resolutions\n        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n        wasted_resolution = (width * height) - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (width, height)\n\n    return best_fit\n\n\ndef resize_and_pad_image(image: Image.Image, target_resolution: tuple) -> Image.Image:\n    r\"\"\"\n    Resize and pad an image to a target resolution while maintaining aspect ratio.\n\n    Args:\n        - image (`Image.Image`): The input image.\n        - target_resolution (`tuple`): The target resolution (width, height) of the image.\n\n    Returns:\n        `Image.Image`: The resized and padded image.\n    \"\"\"\n    original_width, original_height = image.size\n    target_width, target_height = target_resolution\n\n    # Determine which dimension (width or height) to fill\n    scale_w = target_width / original_width\n    scale_h = target_height / original_height\n\n    if scale_w < scale_h:\n        # Width will be filled completely\n        new_width = target_width\n        new_height = min(math.ceil(original_height * scale_w), target_height)\n    else:\n        # Height will be filled completely\n        new_height = target_height\n        new_width = min(math.ceil(original_width * scale_h), target_width)\n\n    # Resize the image\n    resized_image = image.resize((new_width, new_height))\n\n    # Create a new image with the target size and paste the resized image onto it\n    new_image = Image.new(\"RGB\", (target_width, target_height), (0, 0, 0))\n    paste_x = (target_width - new_width) // 2\n    paste_y = (target_height - new_height) // 2\n    new_image.paste(resized_image, (paste_x, paste_y))\n\n    return new_image\n\n\ndef divide_to_patches(image: Image.Image, patch_size: int) -> list:\n    \"\"\"\n    Divides an image into patches of a specified size.\n\n    Args:\n        - image (`Image.Image`): The input image.\n        - patch_size (`int`): The size of each patch.\n\n    Returns:\n        `list`: A list of Image.Image objects representing the patches.\n    \"\"\"\n    patches = []\n    width, height = image.size\n    for i in range(0, height, patch_size):\n        for j in range(0, width, patch_size):\n            box = (j, i, j + patch_size, i + patch_size)\n            patch = image.crop(box)\n            patches.append(patch)\n\n    return patches\n\n\ndef get_anyres_image_grid_shape(image_size: Tuple[int, int], grid_pinpoints: Union[str, list], patch_size: int) -> Tuple[int, int]:\n    r\"\"\"\n    Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n\n    Args:\n        - image_size (`tuple`): The size of the input image in the format (width, height).\n        - grid_pinpoints (`str` or `list`): A string representation of a list of possible resolutions.\n        - patch_size (`int`): The size of each image patch.\n\n    Returns:\n        `tuple`: The shape of the image patch grid in the format (width, height).\n    \"\"\"\n    if isinstance(grid_pinpoints, str) and \"x\" in grid_pinpoints:\n        assert patch_size in [224, 336, 384, 448, 512], \"patch_size should be in [224, 336, 384, 448, 512]\"\n        # Use regex to extract the range from the input string\n        matches = re.findall(r\"\\((\\d+)x(\\d+)\\)\", grid_pinpoints)\n        range_start = tuple(map(int, matches[0]))\n        range_end = tuple(map(int, matches[-1]))\n        # Generate a matrix of tuples from (range_start[0], range_start[1]) to (range_end[0], range_end[1])\n        grid_pinpoints = [(i, j) for i in range(range_start[0], range_end[0] + 1) for j in range(range_start[1], range_end[1] + 1)]\n        # Multiply all elements by patch_size\n        grid_pinpoints = [[dim * patch_size for dim in pair] for pair in grid_pinpoints]\n    if type(grid_pinpoints) is list:\n        possible_resolutions = grid_pinpoints\n    else:\n        possible_resolutions = ast.literal_eval(grid_pinpoints)\n    width, height = select_best_resolution(image_size, possible_resolutions)\n    return width // patch_size, height // patch_size\n\n\ndef process_anyres_image(image: Image.Image, processor: Any, grid_pinpoints: Union[str, List[Tuple[int, int]]]) -> torch.Tensor:\n    r\"\"\"\n    Process an image with variable resolutions.\n\n    Args:\n        - image (`Image.Image`): The input image to be processed.\n        - processor: The image processor object.\n        - grid_pinpoints (`str`): A string representation of a list of possible resolutions.\n\n    Returns:\n        `torch.Tensor`: A tensor containing the processed image patches.\n    \"\"\"\n    # Convert grid_pinpoints from string to list\n    if isinstance(grid_pinpoints, str) and \"x\" in grid_pinpoints:\n        try:\n            patch_size = processor.size[0]\n        except Exception as e:\n            patch_size = processor.size[\"shortest_edge\"]\n        assert patch_size in [224, 336, 384, 448, 512], \"patch_size should be in [224, 336, 384, 448, 512]\"\n        # Use regex to extract the range from the input string\n        matches = re.findall(r\"\\((\\d+)x(\\d+)\\)\", grid_pinpoints)\n        range_start = tuple(map(int, matches[0]))\n        range_end = tuple(map(int, matches[-1]))\n        # Generate a matrix of tuples from (range_start[0], range_start[1]) to (range_end[0], range_end[1])\n        grid_pinpoints = [(i, j) for i in range(range_start[0], range_end[0] + 1) for j in range(range_start[1], range_end[1] + 1)]\n        # Multiply all elements by patch_size\n        grid_pinpoints = [[dim * patch_size for dim in pair] for pair in grid_pinpoints]\n\n    if type(grid_pinpoints) is list:\n        possible_resolutions = grid_pinpoints\n    else:\n        possible_resolutions = ast.literal_eval(grid_pinpoints)\n    best_resolution = select_best_resolution(image.size, possible_resolutions)\n    image_padded = resize_and_pad_image(image, best_resolution)\n\n    patches = divide_to_patches(image_padded, processor.crop_size[\"height\"])\n\n    # FIXME: this seems to be a bug that it resizes instead of pad. # FIXME\n    # but to keep it consistent with previous, i will keep it as it is\n    # TODO: uncomment below to ablate with the padding\n    if isinstance(processor.size, dict):\n        shortest_edge = processor.size[\"shortest_edge\"]\n    else:\n        shortest_edge = min(processor.size)\n    image_original_resize = image.resize((shortest_edge, shortest_edge))\n    # image_padded_square = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\n    # image_original_resize = image_padded_square.resize((processor.size['shortest_edge'], processor.size['shortest_edge']))\n\n    image_patches = [image_original_resize] + patches\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    image_patches = torch.stack(image_patches, dim=0)\n    return image_patches\n\n\ndef load_image_from_base64(image):\n    return Image.open(BytesIO(base64.b64decode(image)))\n\n\ndef expand2square(pil_img: Image.Image, background_color: tuple) -> Image.Image:\n    r\"\"\"\n    Expands a given PIL image to a square by adding a background color.\n\n    Args:\n        - pil_img (`Image.Image`): The input PIL image to be expanded.\n        - background_color (`tuple`): The background color to use for expansion, specified as an RGB tuple.\n\n    Returns:\n        `Image.Image`: The expanded square PIL image.\n    \"\"\"\n    width, height = pil_img.size\n    if width == height:\n        return pil_img\n    elif width > height:\n        result = Image.new(pil_img.mode, (width, width), background_color)\n        result.paste(pil_img, (0, (width - height) // 2))\n        return result\n    else:\n        result = Image.new(pil_img.mode, (height, height), background_color)\n        result.paste(pil_img, ((height - width) // 2, 0))\n        return result\n\n\ndef process_images(images: List[Image.Image], image_processor: Any, model_cfg: Any) -> Union[torch.Tensor, List[torch.Tensor]]:\n    r\"\"\"\n    Processes a list of images based on the specified model configuration.\n\n    Args:\n        - images (`list`): A list of images to be processed.\n        - image_processor (`ImageProcessor`): An instance of the image processor to be used.\n        - model_cfg (`ModelConfig`): Configuration object containing model settings.\n\n    Returns:\n        `torch.Tensor` or list: Processed images as a tensor if all images have the same shape, \n                              otherwise a list of processed images.\n    \"\"\"\n    image_aspect_ratio = getattr(model_cfg, \"image_aspect_ratio\", None)\n    new_images = []\n    if image_aspect_ratio == \"highres\":\n        for image in images:\n            image = process_highres_image(image, image_processor, model_cfg.image_grid_pinpoints)\n            new_images.append(image)\n    elif image_aspect_ratio == \"anyres\" or \"anyres_max\" in image_aspect_ratio:\n        for image in images:\n            image = process_anyres_image(image, image_processor, model_cfg.image_grid_pinpoints)\n            new_images.append(image)\n    elif image_aspect_ratio == \"crop_split\":\n        for image in images:\n            image = process_highres_image_crop_split(image, model_cfg, image_processor)\n            new_images.append(image)\n    elif image_aspect_ratio == \"pad\":\n        for image in images:\n            image = expand2square(image, tuple(int(x * 255) for x in image_processor.image_mean))\n            image = image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0]\n            new_images.append(image)\n    else:\n        return image_processor.preprocess(images, return_tensors=\"pt\")[\"pixel_values\"]\n    if all(x.shape == new_images[0].shape for x in new_images):\n        new_images = torch.stack(new_images, dim=0)\n    return new_images\n\n\ndef tokenizer_image_token(prompt: str, tokenizer: PreTrainedTokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None)->Union[torch.Tensor, List[torch.Tensor]]:\n    r\"\"\"\n    Tokenizes a prompt containing image tokens and inserts the specified image token index at the appropriate positions.\n\n    Args:\n        - prompt (str): The input prompt string containing text and \"<image>\" placeholders.\n        - tokenizer (PreTrainedTokenizer): The tokenizer to use for tokenizing the text chunks.\n        - image_token_index (int): The token index to use for the image placeholders. Default is IMAGE_TOKEN_INDEX.\n        - return_tensors (str, optional): The type of tensor to return. If \"pt\", returns a PyTorch tensor. Default is None.\n\n    Returns:\n        list or torch.Tensor: The tokenized input IDs as a list or a PyTorch tensor if return_tensors is specified.\n    \"\"\"\n    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(\"<image>\")]\n    # FIXME: prompt_chunks = [tokenizer(chunk, return_tensors=\"pt\", padding=\"longest\", max_length=tokenizer.model_max_length, truncation=True).input_ids for chunk in prompt.split(\"<image>\")]\n\n    def insert_separator(X, sep):\n        return [ele for sublist in zip(X, [sep] * len(X)) for ele in sublist][:-1]\n\n    input_ids = []\n    offset = 0\n    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n        offset = 1\n        input_ids.append(prompt_chunks[0][0])\n\n    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n        input_ids.extend(x[offset:])\n\n    if return_tensors is not None:\n        if return_tensors == \"pt\":\n            return torch.tensor(input_ids, dtype=torch.long)\n        raise ValueError(f\"Unsupported tensor type: {return_tensors}\")\n    return input_ids\n\n\ndef get_model_name_from_path(model_path: str)->str:\n    model_path = model_path.strip(\"/\")\n    model_paths = model_path.split(\"/\")\n    if model_paths[-1].startswith(\"checkpoint-\"):\n        return model_paths[-2] + \"_\" + model_paths[-1]\n    else:\n        return model_paths[-1]\n\n\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, keywords, tokenizer, input_ids):\n        self.keywords = keywords\n        self.keyword_ids = []\n        for keyword in keywords:\n            cur_keyword_ids = tokenizer(keyword).input_ids\n            if len(cur_keyword_ids) > 1 and cur_keyword_ids[0] == tokenizer.bos_token_id:\n                cur_keyword_ids = cur_keyword_ids[1:]\n            self.keyword_ids.append(torch.tensor(cur_keyword_ids))\n        self.tokenizer = tokenizer\n        self.start_len = input_ids.shape[1]\n\n    def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        assert output_ids.shape[0] == 1, \"Only support batch size 1 (yet)\"  # TODO\n        offset = min(output_ids.shape[1] - self.start_len, 3)\n        self.keyword_ids = [keyword_id.to(output_ids.device) for keyword_id in self.keyword_ids]\n        for keyword_id in self.keyword_ids:\n            if output_ids[0, -keyword_id.shape[0] :] == keyword_id:\n                return True\n        outputs = self.tokenizer.batch_decode(output_ids[:, -offset:], skip_special_tokens=True)[0]\n        for keyword in self.keywords:\n            if keyword in outputs:\n                return True\n        return False\n"}
{"type": "source_file", "path": "instellavl/conversation.py", "content": "# Modification CopyrightÂ© 2025 Advanced Micro Devices, Inc. All rights reserved.\n\nimport re\nimport base64\nimport dataclasses\n\nfrom PIL import Image\nfrom io import BytesIO\nfrom enum import auto, Enum\nfrom typing import List, Any, Dict, Union, Tuple\n\nfrom transformers import AutoTokenizer\n\n\nclass SeparatorStyle(Enum):\n    \"\"\"Different separator style.\"\"\"\n\n    SINGLE = auto()\n    MPT = auto()\n    INSTELLA = auto()\n\n\n@dataclasses.dataclass\nclass Conversation:\n    r\"\"\"A class that keeps all conversation history.\"\"\"\n\n    system: str\n    roles: List[str]\n    messages: List[List[str]]\n    offset: int\n    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n    sep: str = \"###\"\n    sep2: str = None\n    version: str = \"Unknown\"\n\n    tokenizer_id: str = \"\"\n    tokenizer: Any = None\n    # Stop criteria (the default one is EOS token)\n    stop_str: Union[str, List[str]] = None\n    # Stops generation if meeting any token in this list\n    stop_token_ids: List[int] = None\n\n    skip_next: bool = False\n\n    def get_prompt(self):\n        \"\"\"\n        Generates a formatted prompt string based on the messages and separator style.\n        The function processes the messages stored in the instance, applies specific formatting rules \n        based on the separator style, and returns the resulting prompt string.\n        \n        Returns:\n            `str`: The formatted prompt string.\n        \n        Raises:\n            `ValueError`: If an invalid separator style is specified.\n        \"\"\"\n\n        messages = self.messages\n        if len(messages) > 0 and type(messages[0][1]) is tuple:\n            messages = self.messages.copy()\n            init_role, init_msg = messages[0].copy()\n            init_msg = init_msg[0]\n            if \"mmtag\" in self.version:\n                init_msg = init_msg.replace(\"<image>\", \"\").strip()\n                messages[0] = (init_role, init_msg)\n                messages.insert(0, (self.roles[0], \"<Image><image></Image>\"))\n                messages.insert(1, (self.roles[1], \"Received.\"))\n            elif not init_msg.startswith(\"<image>\"):\n                init_msg = init_msg.replace(\"<image>\", \"\").strip()\n                messages[0] = (init_role, \"<image>\\n\" + init_msg)\n            else:\n                messages[0] = (init_role, init_msg)\n\n        if self.sep_style == SeparatorStyle.SINGLE:\n            ret = self.system + self.sep\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n\n        elif self.sep_style == SeparatorStyle.MPT:\n            ret = self.system + self.sep\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n\n        elif self.sep_style == SeparatorStyle.INSTELLA:\n            seps = [self.sep, self.sep2]\n            ret = \"|||IP_ADDRESS|||\" \n            for i, (role, message) in enumerate(messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    if i % 2 == 1:\n                        message = message.strip()\n                    ret += role + message + seps[i % 2]\n                else:\n                    ret += role \n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\n        return ret\n\n    def append_message(self, role, message):\n        self.messages.append([role, message])\n\n    def process_image(self, image: Union[str, Image.Image], image_process_mode: str, return_pil: bool=False, image_format: str=\"PNG\")->Union[str, Image.Image]:\n        r\"\"\"\n        Processes an image according to the specified mode and returns either a PIL image or a base64 encoded string.\n        \n        Args:\n            - image (Union[str, Image.Image]): The image to be processed. Can be a file path or a PIL Image object.\n            - image_process_mode (str): The mode of image processing. Options are \"Pad\", \"Default\", \"Crop\", or \"Resize\".\n            - return_pil (bool, optional): If True, returns a PIL Image object. If False, returns a base64 encoded string. Defaults to False.\n            - image_format (str, optional): The format to save the image in if returning a base64 encoded string. Defaults to \"PNG\".\n        \n        Returns:\n            Union[str, Image.Image]: The processed image, either as a PIL Image object or a base64 encoded string.\n        \n        Raises:\n            ValueError: If an invalid image_process_mode is provided.\n        \"\"\"\n        \n        if image_process_mode == \"Pad\":\n\n            def expand2square(pil_img, background_color=(122, 116, 104)):\n                width, height = pil_img.size\n                if width == height:\n                    return pil_img\n                elif width > height:\n                    result = Image.new(pil_img.mode, (width, width), background_color)\n                    result.paste(pil_img, (0, (width - height) // 2))\n                    return result\n                else:\n                    result = Image.new(pil_img.mode, (height, height), background_color)\n                    result.paste(pil_img, ((height - width) // 2, 0))\n                    return result\n\n            image = expand2square(image)\n        elif image_process_mode in [\"Default\", \"Crop\"]:\n            pass\n        elif image_process_mode == \"Resize\":\n            image = image.resize((336, 336))\n        else:\n            raise ValueError(f\"Invalid image_process_mode: {image_process_mode}\")\n\n        if type(image) is not Image.Image:\n            image = Image.open(image).convert(\"RGB\")\n\n        max_hw, min_hw = max(image.size), min(image.size)\n        aspect_ratio = max_hw / min_hw\n        max_len, min_len = 672, 448\n        shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n        longest_edge = int(shortest_edge * aspect_ratio)\n        W, H = image.size\n        if H > W:\n            H, W = longest_edge, shortest_edge\n        else:\n            H, W = shortest_edge, longest_edge\n        image = image.resize((W, H))\n        if return_pil:\n            return image\n        else:\n            buffered = BytesIO()\n            image.save(buffered, format=image_format)\n            img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n            return img_b64_str\n\n    def get_images(self, return_pil: bool=False, return_path: bool=False) -> List[Union[str, Image.Image]]:\n        \"\"\"\n        Retrieve images from the conversation messages.\n\n        Args:\n            return_pil (bool): If True, return images as PIL objects. Defaults to False.\n            return_path (bool): If True, return the image file paths instead of processing them. Defaults to False.\n\n        Returns:\n            list: A list of images or image paths depending on the arguments.\n        \"\"\"\n        images = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    msg, image, image_process_mode = msg\n                    if type(image) != list:\n                        image = [image]\n                    for img in image:\n                        if not return_path and self.is_image_file(img):\n                            img = self.process_image(img, image_process_mode, return_pil=return_pil)\n                        else:\n                            images.append(img)\n        return images\n\n    def is_image_file(self, filename: str)->bool:\n        image_extensions = [\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\", \".webp\"]\n        return any(filename.lower().endswith(ext) for ext in image_extensions)\n\n    def is_video_file(self, filename: str)->bool:\n        video_extensions = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\", \".mpeg\", \".mpg\"]\n        return any(filename.lower().endswith(ext) for ext in video_extensions)\n\n    def to_gradio_chatbot(self)->list:\n        ret = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    msg, image, image_process_mode = msg\n                    if type(image) != list:\n                        image = [image]\n                    if len(image) == 1:\n                        msg = \"<image>\\n\" + msg.replace(\"<image>\", \"\").strip()\n                    else:\n                        msg = re.sub(r\"(<image>)\\n(?=<image>)\", r\"\\1 \", msg)\n\n                    img_str_list = []                         \n                    for img in image:\n                        if self.is_image_file(img):\n                            img_b64_str = self.process_image(img, \"Default\", return_pil=False, image_format=\"JPEG\")\n                            img_str = f'<img src=\"data:image/jpeg;base64,{img_b64_str}\" style=\"max-width: 256px; max-height: 256px; width: auto; height: auto; object-fit: contain;\"/>'\n                            img_str_list.append(img_str)\n                        elif self.is_video_file(img):\n                            ret.append(((img,), None))\n\n                    msg = msg.strip()\n                    img_place_holder = \"\"\n                    for img_str in img_str_list:\n                        img_place_holder += f\"{img_str}\\n\\n\"\n\n                    if len(img_str_list) > 0:\n                        msg = f\"{img_place_holder}\\n\\n{msg}\"\n\n                    if len(msg) > 0:\n                        ret.append([msg, None])\n                else:\n                    ret.append([msg, None])\n            else:\n                ret[-1][-1] = msg\n        return ret\n\n    def copy(self)->\"Conversation\":\n        return Conversation(system=self.system, roles=self.roles, messages=[[x, y] for x, y in self.messages], offset=self.offset, sep_style=self.sep_style, sep=self.sep, sep2=self.sep2, version=self.version)\n\n    def dict(self)->Dict[str, Any]:\n        if len(self.get_images()) > 0:\n            return {\n                \"system\": self.system,\n                \"roles\": self.roles,\n                \"messages\": [[x, y[0] if type(y) is tuple else y] for x, y in self.messages],\n                \"offset\": self.offset,\n                \"sep\": self.sep,\n                \"sep2\": self.sep2,\n            }\n        return {\n            \"system\": self.system,\n            \"roles\": self.roles,\n            \"messages\": self.messages,\n            \"offset\": self.offset,\n            \"sep\": self.sep,\n            \"sep2\": self.sep2,\n        }\n\n\nconv_vicuna_v0 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=[\n        [\"Human\", \"What are the key differences between renewable and non-renewable energy sources?\"],\n        [\n            \"Assistant\",\n            \"Renewable energy sources are those that can be replenished naturally in a relatively \"\n            \"short amount of time, such as solar, wind, hydro, geothermal, and biomass. \"\n            \"Non-renewable energy sources, on the other hand, are finite and will eventually be \"\n            \"depleted, such as coal, oil, and natural gas. Here are some key differences between \"\n            \"renewable and non-renewable energy sources:\\n\"\n            \"1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable \"\n            \"energy sources are finite and will eventually run out.\\n\"\n            \"2. Environmental impact: Renewable energy sources have a much lower environmental impact \"\n            \"than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, \"\n            \"and other negative effects.\\n\"\n            \"3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically \"\n            \"have lower operational costs than non-renewable sources.\\n\"\n            \"4. Reliability: Renewable energy sources are often more reliable and can be used in more remote \"\n            \"locations than non-renewable sources.\\n\"\n            \"5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different \"\n            \"situations and needs, while non-renewable sources are more rigid and inflexible.\\n\"\n            \"6. Sustainability: Renewable energy sources are more sustainable over the long term, while \"\n            \"non-renewable sources are not, and their depletion can lead to economic and social instability.\\n\",\n        ],\n    ],\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_mpt = Conversation(\n    system=\"\"\"<|im_start|>system\nA conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_instella = Conversation(\n    system=\"\",\n    roles=(\"<|user|>\\n\", \"<|assistant|>\\n\"),\n    version=\"instella\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.INSTELLA,\n    sep=\"\\n\",\n    sep2='|||IP_ADDRESS|||\\n'\n)\n\n\ndefault_conversation = conv_vicuna_v0\nconv_templates = {\n    \"default\": conv_vicuna_v0,\n    \"mpt\": conv_mpt,\n    \"instella\": conv_instella,\n}\n\n\nif __name__ == \"__main__\":\n    print(default_conversation.get_prompt())\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/hf_vision.py", "content": "import torch\nimport torch.nn as nn\n\nfrom transformers import AutoModel, AutoImageProcessor, AutoConfig, CLIPImageProcessor\nfrom llava.utils import rank0_print\n\n\nclass HFVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.vision_tower_name = vision_tower.replace(\"hf:\", \"\", 1)\n        self.select_layer = args.mm_vision_select_layer\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        if not delay_load:\n            self.load_model()\n        else:\n            self.cfg_only = AutoConfig.from_pretrained(self.vision_tower_name)\n\n    def load_model(self):\n        try:\n            self.image_processor = AutoImageProcessor.from_pretrained(self.vision_tower_name)\n        except Exception as e:\n            if \"448\" in self.vision_tower_name:\n                image_size = 448\n                # use image processor with conig\n                self.image_processor = CLIPImageProcessor(size={\"shortest_edge\": image_size}, do_center_crop=True, crop_size=image_size)\n            else:\n                self.image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower = AutoModel.from_pretrained(self.vision_tower_name, torch_dtype=torch.bfloat16, trust_remote_code=True).to(\"cuda\")\n        self.device = self.vision_tower.device\n        self.dtype = self.vision_tower.dtype\n        self.config = self.vision_tower.config\n\n        if hasattr(self.vision_tower, \"vision_model\"):\n            self.vision_tower = self.vision_tower.vision_model\n        self.vision_tower.requires_grad_(False)\n        # self.vision_tower.eval()\n        self.is_loaded = True\n\n    def feature_select(self, image_forward_outs):\n        select_feature_type = self.select_feature\n\n        if self.select_feature in [\"slicefour_patch\", \"slicefour_cls_patch\"]:\n            select_every_k_layer = len(image_forward_outs.hidden_states) // 4\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in range(select_every_k_layer + self.select_layer, len(image_forward_outs.hidden_states), select_every_k_layer)], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slicefour_\", \"\")\n        else:\n            image_features = image_forward_outs.hidden_states[self.select_layer]\n\n        if select_feature_type == \"patch\":\n            image_features = image_features[:, 1:]\n        elif select_feature_type == \"cls_patch\":\n            image_features = image_features\n        else:\n            raise ValueError(f\"Unexpected select feature: {select_feature_type}\")\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    # @property\n    # def dtype(self):\n    #     return self.vision_tower.dtype\n\n    # @property\n    # def device(self):\n    #     return self.vision_tower.device\n\n    @property\n    def hidden_size(self):\n        try:\n            _hidden_size = self.config.hidden_size\n        except:\n            _hidden_size = self.config.vision_config.hidden_size\n        if \"slicefour\" in self.select_feature:\n            _hidden_size *= 4\n        return _hidden_size\n\n    @property\n    def num_patches(self):\n        _num_patches = (self.config.image_size // self.config.patch_size) ** 2\n        if \"cls_patch\" in self.select_feature:\n            _num_patches += 1\n        return _num_patches\n\n    @property\n    def num_patches_per_side(self):\n        return self.config.image_size // self.config.patch_size\n\n    @property\n    def image_size(self):\n        return self.config.image_size\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_vit.py", "content": "# Based on EVA, BEIT, timm and DeiT code bases\n# https://github.com/baaivision/EVA\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/facebookresearch/deit/\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\n# not tested yet\nimport math\nfrom transformers import CLIPImageProcessor\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import drop_path, to_2tuple, trunc_normal_\nfrom .eva_clip import create_model_and_transforms, get_model_config\nimport torch\nimport torchvision\nimport time\n\nfrom llava.utils import rank0_print\n\n\nclass EvaViTWrapper(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n        self.vision_tower_name = vision_tower\n        self.pretrained = args.vision_tower_pretrained\n        self.args = args\n\n        self.select_layer = args.mm_vision_select_layer\n        if self.select_layer < -1:\n            self.select_layer += 1\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        self.model_config = get_model_config(self.vision_tower_name)\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n\n    def load_model(self):\n        rank0_print(f\"Loading: {self.vision_tower_name}\")\n        rank0_print(f\"Pretrained: {self.pretrained}\")\n        time_start = time.time()\n        model, _, image_processor = create_model_and_transforms(self.vision_tower_name, self.pretrained, force_custom_clip=True, precision=\"fp16\")\n        time_end = time.time()\n        rank0_print(f\"Loaded: {self.vision_tower_name} in {time_end - time_start:.2f}s\")\n        self.device = next(model.parameters()).device\n        self.dtype = next(model.parameters()).dtype\n        if self.device.type != \"meta\":\n            model = model.to(\"cuda\")\n        self.vision_tower = model.visual\n        resize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Resize)][0]\n        normalize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Normalize)][0]\n        self.resize_transform_size = resize_transform.size\n        self.image_processor = CLIPImageProcessor.from_pretrained(\n            \"openai/clip-vit-large-patch14\",\n            crop_size=resize_transform.size,\n            size={\"shortest_edge\": resize_transform.size},\n            image_mean=list(normalize_transform.mean),\n            image_std=list(normalize_transform.std),\n        )\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower.requires_grad_(False)\n        self.is_loaded = True\n\n    def feature_select(self, image_features):\n        select_feature_type = self.select_feature\n\n        # if self.select_feature in [\"slicefour_patch\", \"slicefour_cls_patch\"]:\n        #     select_every_k_layer = len(image_features) // 4\n        #     image_features = torch.cat([image_features[i] for i in range(select_every_k_layer + self.select_layer, len(image_features), select_every_k_layer)], dim=-1)\n        #     select_feature_type = select_feature_type.replace(\"slicefour_\", \"\")\n        # elif self.select_feature in [\"slice_m25811_f6_patch\", \"slice_m25811_f6_cls_patch\"]:\n        #     select_layers = [-1, -4, -7, -10, 6]\n        #     image_features = torch.cat([image_features[i] for i in select_layers], dim=-1)\n        #     select_feature_type = select_feature_type.replace(\"slice_m25811_f6_\", \"\")\n        # else:\n        #     image_features = image_features[self.select_layer]\n\n        if select_feature_type == \"patch\":\n            image_features = image_features[:, 1:]\n        elif select_feature_type == \"cls_patch\":\n            image_features = image_features\n        else:\n            raise ValueError(f\"Unexpected select feature: {select_feature_type}\")\n        return image_features\n\n    def train(self, mode=True):\n        self.training = mode\n\n        if self.is_loaded:\n            self.vision_tower.eval()\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_features = self.vision_tower.forward_features(image.to(self.dtype), return_all_features=True)\n                image_features = self.feature_select(image_features).to(self.dtype)\n                image_features.append(image_features)\n        else:\n            image_features = self.vision_tower.forward_features(images.to(self.dtype), return_all_features=True)\n            image_features = self.feature_select(image_features).to(self.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def hidden_size(self):\n        return self.model_config[\"vision_cfg\"][\"width\"]\n\n    @property\n    def num_patches(self):\n        return (self.model_config[\"vision_cfg\"][\"image_size\"] // self.model_config[\"vision_cfg\"][\"patch_size\"]) ** 2\n\n    @property\n    def num_patches_per_side(self):\n        return self.model_config[\"vision_cfg\"][\"image_size\"] // self.model_config[\"vision_cfg\"][\"patch_size\"]\n\n    @property\n    def config(self):\n        return self.model_config\n\n    @property\n    def image_size(self):\n        return self.model_config[\"vision_cfg\"][\"image_size\"]\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/pretrained.py", "content": "import hashlib\nimport os\nimport urllib\nimport warnings\nfrom typing import Dict, Union\n\nfrom tqdm import tqdm\n\ntry:\n    from huggingface_hub import hf_hub_download\n\n    _has_hf_hub = True\nexcept ImportError:\n    hf_hub_download = None\n    _has_hf_hub = False\n\n\ndef _pcfg(url=\"\", hf_hub=\"\", filename=\"\", mean=None, std=None):\n    return dict(\n        url=url,\n        hf_hub=hf_hub,\n        mean=mean,\n        std=std,\n    )\n\n\n_VITB32 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e32-46683a32.pt\"),\n    laion2b_e16=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-laion2b_e16-af8dbd0c.pth\"),\n    laion2b_s34b_b79k=_pcfg(hf_hub=\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K/\"),\n)\n\n_VITB32_quickgelu = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e32-46683a32.pt\"),\n)\n\n_VITB16 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16-laion400m_e31-00efa78f.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16-laion400m_e32-55e67d44.pt\"),\n    laion2b_s34b_b88k=_pcfg(hf_hub=\"laion/CLIP-ViT-B-16-laion2B-s34B-b88K/\"),\n)\n\n_EVAB16 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_B_psz14to16.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_B_psz14to16.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_B_psz16_s8B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_B_psz16_s8B.pt\"),\n)\n\n_VITB16_PLUS_240 = dict(\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16_plus_240-laion400m_e31-8fb26589.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16_plus_240-laion400m_e32-699c4b84.pt\"),\n)\n\n_VITL14 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_l_14-laion400m_e31-69988bb6.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_l_14-laion400m_e32-3d133497.pt\"),\n    laion2b_s32b_b82k=_pcfg(hf_hub=\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K/\", mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n)\n\n_EVAL14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_L_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_L_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_s4B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_s4B.pt\"),\n)\n\n_VITL14_336 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\"),\n)\n\n_EVAL14_336 = dict(\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_336_psz14_s6B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_336_psz14_s6B.pt\"),\n    eva_clip_224to336=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_224to336.pt\"),\n    eva02_clip_224to336=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_224to336.pt\"),\n)\n\n_VITH14 = dict(\n    laion2b_s32b_b79k=_pcfg(hf_hub=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K/\"),\n)\n\n_VITg14 = dict(\n    laion2b_s12b_b42k=_pcfg(hf_hub=\"laion/CLIP-ViT-g-14-laion2B-s12B-b42K/\"),\n    laion2b_s34b_b88k=_pcfg(hf_hub=\"laion/CLIP-ViT-g-14-laion2B-s34B-b88K/\"),\n)\n\n_EVAg14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/\"),\n    eva01=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_g_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_psz14_s11B.pt\"),\n    eva01_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_psz14_s11B.pt\"),\n)\n\n_EVAg14_PLUS = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/\"),\n    eva01=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_g_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_plus_psz14_s11B.pt\"),\n    eva01_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_plus_psz14_s11B.pt\"),\n)\n\n_VITbigG14 = dict(\n    laion2b_s39b_b160k=_pcfg(hf_hub=\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k/\"),\n)\n\n_EVAbigE14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_s4B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_s4B.pt\"),\n)\n\n_EVAbigE14_PLUS = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_plus_s9B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_plus_s9B.pt\"),\n)\n\n_EVA_8B = dict(\n    eva=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B/EVA_8B_psz14.bin\"),\n    eva_clip=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B/EVA_CLIP_8B_psz14_s9B.pt\"),\n)\n\n_EVA_8B_PLUS = dict(\n    eva_clip=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B-448/EVA_CLIP_8B_psz14_plus_s0.6B.pt\"),\n)\n\n\n_PRETRAINED = {\n    # \"ViT-B-32\": _VITB32,\n    \"OpenaiCLIP-B-32\": _VITB32,\n    \"OpenCLIP-B-32\": _VITB32,\n    # \"ViT-B-32-quickgelu\": _VITB32_quickgelu,\n    \"OpenaiCLIP-B-32-quickgelu\": _VITB32_quickgelu,\n    \"OpenCLIP-B-32-quickgelu\": _VITB32_quickgelu,\n    # \"ViT-B-16\": _VITB16,\n    \"OpenaiCLIP-B-16\": _VITB16,\n    \"OpenCLIP-B-16\": _VITB16,\n    \"EVA02-B-16\": _EVAB16,\n    \"EVA02-CLIP-B-16\": _EVAB16,\n    # \"ViT-B-16-plus-240\": _VITB16_PLUS_240,\n    \"OpenCLIP-B-16-plus-240\": _VITB16_PLUS_240,\n    # \"ViT-L-14\": _VITL14,\n    \"OpenaiCLIP-L-14\": _VITL14,\n    \"OpenCLIP-L-14\": _VITL14,\n    \"EVA02-L-14\": _EVAL14,\n    \"EVA02-CLIP-L-14\": _EVAL14,\n    # \"ViT-L-14-336\": _VITL14_336,\n    \"OpenaiCLIP-L-14-336\": _VITL14_336,\n    \"EVA02-CLIP-L-14-336\": _EVAL14_336,\n    # \"ViT-H-14\": _VITH14,\n    # \"ViT-g-14\": _VITg14,\n    \"OpenCLIP-H-14\": _VITH14,\n    \"OpenCLIP-g-14\": _VITg14,\n    \"EVA01-CLIP-g-14\": _EVAg14,\n    \"EVA01-CLIP-g-14-plus\": _EVAg14_PLUS,\n    # \"ViT-bigG-14\": _VITbigG14,\n    \"OpenCLIP-bigG-14\": _VITbigG14,\n    \"EVA02-CLIP-bigE-14\": _EVAbigE14,\n    \"EVA02-CLIP-bigE-14-plus\": _EVAbigE14_PLUS,\n    \"EVA-CLIP-8B\": _EVA_8B,\n    \"EVA-CLIP-8B-448\": _EVA_8B_PLUS,\n    \"EVA-CLIP-8B-plus\": _EVA_8B_PLUS,\n}\n\n\ndef _clean_tag(tag: str):\n    # normalize pretrained tags\n    return tag.lower().replace(\"-\", \"_\")\n\n\ndef list_pretrained(as_str: bool = False):\n    \"\"\"returns list of pretrained models\n    Returns a tuple (model_name, pretrain_tag) by default or 'name:tag' if as_str == True\n    \"\"\"\n    return [\":\".join([k, t]) if as_str else (k, t) for k in _PRETRAINED.keys() for t in _PRETRAINED[k].keys()]\n\n\ndef list_pretrained_models_by_tag(tag: str):\n    \"\"\"return all models having the specified pretrain tag\"\"\"\n    models = []\n    tag = _clean_tag(tag)\n    for k in _PRETRAINED.keys():\n        if tag in _PRETRAINED[k]:\n            models.append(k)\n    return models\n\n\ndef list_pretrained_tags_by_model(model: str):\n    \"\"\"return all pretrain tags for the specified model architecture\"\"\"\n    tags = []\n    if model in _PRETRAINED:\n        tags.extend(_PRETRAINED[model].keys())\n    return tags\n\n\ndef is_pretrained_cfg(model: str, tag: str):\n    if model not in _PRETRAINED:\n        return False\n    return _clean_tag(tag) in _PRETRAINED[model]\n\n\ndef get_pretrained_cfg(model: str, tag: str):\n    if model not in _PRETRAINED:\n        return {}\n    model_pretrained = _PRETRAINED[model]\n    return model_pretrained.get(_clean_tag(tag), {})\n\n\ndef get_pretrained_url(model: str, tag: str):\n    cfg = get_pretrained_cfg(model, _clean_tag(tag))\n    return cfg.get(\"url\", \"\")\n\n\ndef download_pretrained_from_url(\n    url: str,\n    cache_dir: Union[str, None] = None,\n):\n    if not cache_dir:\n        cache_dir = os.path.expanduser(\"~/.cache/clip\")\n    os.makedirs(cache_dir, exist_ok=True)\n    filename = os.path.basename(url)\n\n    if \"openaipublic\" in url:\n        expected_sha256 = url.split(\"/\")[-2]\n    elif \"mlfoundations\" in url:\n        expected_sha256 = os.path.splitext(filename)[0].split(\"-\")[-1]\n    else:\n        expected_sha256 = \"\"\n\n    download_target = os.path.join(cache_dir, filename)\n\n    if os.path.exists(download_target) and not os.path.isfile(download_target):\n        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n    if os.path.isfile(download_target):\n        if expected_sha256:\n            if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest().startswith(expected_sha256):\n                return download_target\n            else:\n                warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n        else:\n            return download_target\n\n    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n        with tqdm(total=int(source.headers.get(\"Content-Length\")), ncols=80, unit=\"iB\", unit_scale=True) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n\n                output.write(buffer)\n                loop.update(len(buffer))\n\n    if expected_sha256 and not hashlib.sha256(open(download_target, \"rb\").read()).hexdigest().startswith(expected_sha256):\n        raise RuntimeError(f\"Model has been downloaded but the SHA256 checksum does not not match\")\n\n    return download_target\n\n\ndef has_hf_hub(necessary=False):\n    if not _has_hf_hub and necessary:\n        # if no HF Hub module installed, and it is necessary to continue, raise error\n        raise RuntimeError(\"Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.\")\n    return _has_hf_hub\n\n\ndef download_pretrained_from_hf(\n    model_id: str,\n    filename: str = \"open_clip_pytorch_model.bin\",\n    revision=None,\n    cache_dir: Union[str, None] = None,\n):\n    has_hf_hub(True)\n    cached_file = hf_hub_download(model_id, filename, revision=revision, cache_dir=cache_dir)\n    return cached_file\n\n\ndef download_pretrained(\n    cfg: Dict,\n    force_hf_hub: bool = False,\n    cache_dir: Union[str, None] = None,\n):\n    target = \"\"\n    if not cfg:\n        return target\n\n    download_url = cfg.get(\"url\", \"\")\n    download_hf_hub = cfg.get(\"hf_hub\", \"\")\n    if download_hf_hub and force_hf_hub:\n        # use HF hub even if url exists\n        download_url = \"\"\n\n    if download_url:\n        target = download_pretrained_from_url(download_url, cache_dir=cache_dir)\n    elif download_hf_hub:\n        has_hf_hub(True)\n        # we assume the hf_hub entries in pretrained config combine model_id + filename in\n        # 'org/model_name/filename.pt' form. To specify just the model id w/o filename and\n        # use 'open_clip_pytorch_model.bin' default, there must be a trailing slash 'org/model_name/'.\n        model_id, filename = os.path.split(download_hf_hub)\n        if filename:\n            target = download_pretrained_from_hf(model_id, filename=filename, cache_dir=cache_dir)\n        else:\n            target = download_pretrained_from_hf(model_id, cache_dir=cache_dir)\n\n    return target\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/internvit_encoder.py", "content": "# Modification CopyrightÂ© 2025 Advanced Micro Devices, Inc. All rights reserved.\n\n# --------------------------------------------------------\n# InternVL\n# Copyright (c) 2023 OpenGVLab\n# Licensed under The MIT License [see LICENSE for details]\n# --------------------------------------------------------\nfrom typing import Optional, Tuple, Union\nimport os\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom einops import rearrange\nfrom timm.models.layers import DropPath\nfrom torch import nn\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import (BaseModelOutput,\n                                           BaseModelOutputWithPooling)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import logging\nfrom transformers import PretrainedConfig\nfrom llava.utils import rank0_print\nfrom transformers import  CLIPImageProcessor\n\n\n\n# https://github.com/Dao-AILab/flash-attention/blob/v0.2.8/flash_attn/flash_attention.py\n\nfrom flash_attn.flash_attn_interface import flash_attn_varlen_qkvpacked_func as flash_attn_unpadded_qkvpacked_func\n\nfrom flash_attn.bert_padding import pad_input, unpad_input\n\n\nclass FlashAttention(nn.Module):\n    \"\"\"Implement the scaled dot product attention with softmax.\n    Arguments\n    ---------\n        softmax_scale: The temperature to use for the softmax attention.\n                      (default: 1/sqrt(d_keys) where d_keys is computed at\n                      runtime)\n        attention_dropout: The dropout rate to apply to the attention\n                           (default: 0.0)\n    \"\"\"\n\n    def __init__(self, softmax_scale=None, attention_dropout=0.0, device=None, dtype=None):\n        super().__init__()\n        self.softmax_scale = softmax_scale\n        self.dropout_p = attention_dropout\n\n    def forward(self, qkv, key_padding_mask=None, causal=False, cu_seqlens=None,\n                max_s=None, need_weights=False):\n        \"\"\"Implements the multihead softmax attention.\n        Arguments\n        ---------\n            qkv: The tensor containing the query, key, and value. (B, S, 3, H, D) if key_padding_mask is None\n                if unpadded: (nnz, 3, h, d)\n            key_padding_mask: a bool tensor of shape (B, S)\n        \"\"\"\n        assert not need_weights\n        assert qkv.dtype in [torch.float16, torch.bfloat16]\n        assert qkv.is_cuda\n\n        if cu_seqlens is None:\n            batch_size = qkv.shape[0]\n            seqlen = qkv.shape[1]\n            if key_padding_mask is None:\n                qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n                max_s = seqlen\n                cu_seqlens = torch.arange(0, (batch_size + 1) * seqlen, step=seqlen, dtype=torch.int32,\n                                          device=qkv.device)\n                output = flash_attn_unpadded_qkvpacked_func(\n                    qkv, cu_seqlens, max_s, self.dropout_p if self.training else 0.0,\n                    softmax_scale=self.softmax_scale, causal=causal\n                )\n                output = rearrange(output, '(b s) ... -> b s ...', b=batch_size)\n            else:\n                nheads = qkv.shape[-2]\n                x = rearrange(qkv, 'b s three h d -> b s (three h d)')\n                x_unpad, indices, cu_seqlens, max_s = unpad_input(x, key_padding_mask)\n                x_unpad = rearrange(x_unpad, 'nnz (three h d) -> nnz three h d', three=3, h=nheads)\n                output_unpad = flash_attn_unpadded_qkvpacked_func(\n                    x_unpad, cu_seqlens, max_s, self.dropout_p if self.training else 0.0,\n                    softmax_scale=self.softmax_scale, causal=causal\n                )\n                output = rearrange(pad_input(rearrange(output_unpad, 'nnz h d -> nnz (h d)'),\n                                             indices, batch_size, seqlen),\n                                   'b s (h d) -> b s h d', h=nheads)\n        else:\n            assert max_s is not None\n            output = flash_attn_unpadded_qkvpacked_func(\n                qkv, cu_seqlens, max_s, self.dropout_p if self.training else 0.0,\n                softmax_scale=self.softmax_scale, causal=causal\n            )\n\n        return output, None\n\nclass InternVisionConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`InternVisionModel`]. It is used to\n    instantiate a vision encoder according to the specified arguments, defining the model architecture.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n    Args:\n        num_channels (`int`, *optional*, defaults to 3):\n            Number of color channels in the input images (e.g., 3 for RGB).\n        patch_size (`int`, *optional*, defaults to 14):\n            The size (resolution) of each patch.\n        image_size (`int`, *optional*, defaults to 224):\n            The size (resolution) of each image.\n        qkv_bias (`bool`, *optional*, defaults to `False`):\n            Whether to add a bias to the queries and values in the self-attention layers.\n        hidden_size (`int`, *optional*, defaults to 3200):\n            Dimensionality of the encoder layers and the pooler layer.\n        num_attention_heads (`int`, *optional*, defaults to 25):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        intermediate_size (`int`, *optional*, defaults to 12800):\n            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n        qk_normalization (`bool`, *optional*, defaults to `True`):\n            Whether to normalize the queries and keys in the self-attention layers.\n        num_hidden_layers (`int`, *optional*, defaults to 48):\n            Number of hidden layers in the Transformer encoder.\n        use_flash_attn (`bool`, *optional*, defaults to `True`):\n            Whether to use flash attention mechanism.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` ``\"gelu\"` are supported.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-6):\n            The epsilon used by the layer normalization layers.\n        dropout (`float`, *optional*, defaults to 0.0):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        drop_path_rate (`float`, *optional*, defaults to 0.0):\n            Dropout rate for stochastic depth.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        initializer_factor (`float`, *optional*, defaults to 0.1):\n            A factor for layer scale.\n    \"\"\"\n\n    model_type = 'intern_vit_6b'\n\n    def __init__(\n            self,\n            num_channels=3,\n            patch_size=14,\n            image_size=224,\n            qkv_bias=False,\n            hidden_size=3200,\n            num_attention_heads=25,\n            intermediate_size=12800,\n            qk_normalization=True,\n            num_hidden_layers=48,\n            use_flash_attn=True,\n            hidden_act='gelu',\n            layer_norm_eps=1e-6,\n            dropout=0.0,\n            drop_path_rate=0.0,\n            attention_dropout=0.0,\n            initializer_range=0.02,\n            initializer_factor=0.1,\n            **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.dropout = dropout\n        self.drop_path_rate = drop_path_rate\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.num_channels = num_channels\n        self.patch_size = patch_size\n        self.image_size = image_size\n        self.initializer_range = initializer_range\n        self.initializer_factor = initializer_factor\n        self.attention_dropout = attention_dropout\n        self.layer_norm_eps = layer_norm_eps\n        self.hidden_act = hidden_act\n        self.qkv_bias = qkv_bias\n        self.qk_normalization = qk_normalization\n        self.use_flash_attn = use_flash_attn\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> 'PretrainedConfig':\n        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n\n        if 'vision_config' in config_dict:\n            config_dict = config_dict['vision_config']\n\n        if 'model_type' in config_dict and hasattr(cls, 'model_type') and config_dict['model_type'] != cls.model_type:\n            print(\n                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n                f'{cls.model_type}. This is not supported for all configurations of models and can yield errors.'\n            )\n\n        return cls.from_dict(config_dict, **kwargs)\n\nclass InternRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\ntry:\n    from apex.normalization import FusedRMSNorm\n\n    InternRMSNorm = FusedRMSNorm  # noqa\n\n    rank0_print('Discovered apex.normalization.FusedRMSNorm - will use it instead of InternRMSNorm')\nexcept ImportError:\n    # using the normal InternRMSNorm\n    pass\nexcept Exception:\n    rank0_print('discovered apex but it failed to load, falling back to InternRMSNorm')\n    pass\n\n\nNORM2FN = {\n    'rms_norm': InternRMSNorm,\n    'layer_norm': nn.LayerNorm,\n}\n\n\nclass InternVisionEmbeddings(nn.Module):\n    def __init__(self, config: InternVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.image_size = config.image_size\n        self.patch_size = config.patch_size\n\n        self.class_embedding = nn.Parameter(\n            torch.randn(1, 1, self.embed_dim),\n        )\n\n        self.patch_embedding = nn.Conv2d(\n            in_channels=3, out_channels=self.embed_dim, kernel_size=self.patch_size, stride=self.patch_size\n        )\n\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        self.num_positions = self.num_patches + 1\n\n        self.position_embedding = nn.Parameter(torch.randn(1, self.num_positions, self.embed_dim))\n\n    def _get_pos_embed(self, pos_embed, H, W):\n        target_dtype = pos_embed.dtype\n        pos_embed = pos_embed.float().reshape(\n            1, self.image_size // self.patch_size, self.image_size // self.patch_size, -1).permute(0, 3, 1, 2)\n        pos_embed = F.interpolate(pos_embed, size=(H, W), mode='bicubic', align_corners=False).\\\n            reshape(1, -1, H * W).permute(0, 2, 1).to(target_dtype)\n        return pos_embed\n\n    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n        target_dtype = self.patch_embedding.weight.dtype\n        patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, channel, width, height]\n        batch_size, _, height, width = patch_embeds.shape\n        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n        class_embeds = self.class_embedding.expand(batch_size, 1, -1).to(target_dtype)\n        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n        position_embedding = torch.cat([\n            self.position_embedding[:, :1, :],\n            self._get_pos_embed(self.position_embedding[:, 1:, :], height, width)\n        ], dim=1)\n        embeddings = embeddings + position_embedding.to(target_dtype)\n        return embeddings\n\n\nclass InternAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: InternVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.use_flash_attn = config.use_flash_attn\n        # if config.use_flash_attn and not has_flash_attn:\n        #     print('Warning: Flash Attention is not available, use_flash_attn is set to False.')\n        self.head_dim = self.embed_dim // self.num_heads\n        if self.head_dim * self.num_heads != self.embed_dim:\n            raise ValueError(\n                f'embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:'\n                f' {self.num_heads}).'\n            )\n\n        self.scale = self.head_dim ** -0.5\n        self.qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, bias=config.qkv_bias)\n        self.attn_drop = nn.Dropout(config.attention_dropout)\n        self.proj_drop = nn.Dropout(config.dropout)\n\n        self.qk_normalization = config.qk_normalization\n\n        if self.qk_normalization:\n            self.q_norm = InternRMSNorm(self.embed_dim, eps=config.layer_norm_eps)\n            self.k_norm = InternRMSNorm(self.embed_dim, eps=config.layer_norm_eps)\n\n        if self.use_flash_attn:\n            self.inner_attn = FlashAttention(attention_dropout=config.attention_dropout)\n        self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n\n    def _naive_attn(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n\n        if self.qk_normalization:\n            B_, H_, N_, D_ = q.shape\n            q = self.q_norm(q.transpose(1, 2).flatten(-2, -1)).view(B_, N_, H_, D_).transpose(1, 2)\n            k = self.k_norm(k.transpose(1, 2).flatten(-2, -1)).view(B_, N_, H_, D_).transpose(1, 2)\n\n        attn = ((q * self.scale) @ k.transpose(-2, -1))\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    def _flash_attn(self, x, key_padding_mask=None, need_weights=False):\n        qkv = self.qkv(x)\n        qkv = rearrange(qkv, 'b s (three h d) -> b s three h d', three=3, h=self.num_heads)\n\n        if self.qk_normalization:\n            q, k, v = qkv.unbind(2)\n            q = self.q_norm(q.flatten(-2, -1)).view(q.shape)\n            k = self.k_norm(k.flatten(-2, -1)).view(k.shape)\n            qkv = torch.stack([q, k, v], dim=2)\n\n        context, _ = self.inner_attn(\n            qkv, key_padding_mask=key_padding_mask, need_weights=need_weights, causal=False\n        )\n        outs = self.proj(rearrange(context, 'b s h d -> b s (h d)'))\n        outs = self.proj_drop(outs)\n        return outs\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        x = self._naive_attn(hidden_states) if not self.use_flash_attn else self._flash_attn(hidden_states)\n        return x\n\n\nclass InternMLP(nn.Module):\n    def __init__(self, config: InternVisionConfig):\n        super().__init__()\n        self.config = config\n        self.act = ACT2FN[config.hidden_act]\n        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n        return hidden_states\n\n\nclass InternVisionEncoderLayer(nn.Module):\n    def __init__(self, config: InternVisionConfig, drop_path_rate: float):\n        super().__init__()\n        self.embed_dim = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.norm_type = config.norm_type\n\n        self.attn = InternAttention(config)\n        self.mlp = InternMLP(config)\n        self.norm1 = NORM2FN[self.norm_type](self.embed_dim, eps=config.layer_norm_eps)\n        self.norm2 = NORM2FN[self.norm_type](self.embed_dim, eps=config.layer_norm_eps)\n\n        self.ls1 = nn.Parameter(config.initializer_factor * torch.ones(self.embed_dim))\n        self.ls2 = nn.Parameter(config.initializer_factor * torch.ones(self.embed_dim))\n        self.drop_path1 = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n        self.drop_path2 = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n\n    def forward(\n            self,\n            hidden_states: torch.Tensor,\n    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor], Optional[Tuple[torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]`): input to the layer of shape `(batch, seq_len, embed_dim)`\n        \"\"\"\n        hidden_states = hidden_states + self.drop_path1(self.attn(self.norm1(hidden_states)) * self.ls1)\n\n        hidden_states = hidden_states + self.drop_path2(self.mlp(self.norm2(hidden_states)) * self.ls2)\n\n        return hidden_states\n\n\nclass InternVisionEncoder(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n    [`InternEncoderLayer`].\n\n    Args:\n        config (`InternConfig`):\n            The corresponding vision configuration for the `InternEncoder`.\n    \"\"\"\n\n    def __init__(self, config: InternVisionConfig):\n        super().__init__()\n        self.config = config\n        # stochastic depth decay rule\n        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers)]\n        self.layers = nn.ModuleList([\n            InternVisionEncoderLayer(config, dpr[idx]) for idx in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = True\n\n    def forward(\n            self,\n            inputs_embeds,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutput]:\n        r\"\"\"\n        Args:\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n                Embedded representation of the inputs. Should be float, not int tokens.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        encoder_states = () if output_hidden_states else None\n        hidden_states = inputs_embeds\n\n        for idx, encoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                encoder_states = encoder_states + (hidden_states,)\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    encoder_layer,\n                    hidden_states)\n            else:\n                layer_outputs = encoder_layer(\n                    hidden_states,\n                )\n            hidden_states = layer_outputs\n\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, encoder_states] if v is not None)\n        return BaseModelOutput(\n            last_hidden_state=hidden_states, hidden_states=encoder_states\n        )\n\n\nclass InternVisionModel(PreTrainedModel):\n    main_input_name = 'pixel_values'\n    config_class = InternVisionConfig\n    _no_split_modules = ['InternVisionEncoderLayer']\n\n    def __init__(self, config: InternVisionConfig):\n        super().__init__(config)\n        self.config = config\n\n        self.embeddings = InternVisionEmbeddings(config)\n        self.encoder = InternVisionEncoder(config)\n\n    def resize_pos_embeddings(self, old_size, new_size, patch_size):\n        pos_emb = self.embeddings.position_embedding\n        _, num_positions, embed_dim = pos_emb.shape\n        cls_emb = pos_emb[:, :1, :]\n        pos_emb = pos_emb[:, 1:, :].reshape(1, old_size // patch_size, old_size // patch_size, -1).permute(0, 3, 1, 2)\n        pos_emb = F.interpolate(pos_emb.float(), size=new_size // patch_size, mode='bicubic', align_corners=False)\n        pos_emb = pos_emb.to(cls_emb.dtype).reshape(1, embed_dim, -1).permute(0, 2, 1)\n        pos_emb = torch.cat([cls_emb, pos_emb], dim=1)\n        self.embeddings.position_embedding = nn.Parameter(pos_emb)\n        self.embeddings.image_size = new_size\n        rank0_print('Resized position embeddings from {} to {}'.format(old_size, new_size))\n\n    def get_input_embeddings(self):\n        return self.embeddings\n\n    def forward(\n            self,\n            pixel_values: Optional[torch.FloatTensor] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            pixel_embeds: Optional[torch.FloatTensor] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if pixel_values is None and pixel_embeds is None:\n            raise ValueError('You have to specify pixel_values or pixel_embeds')\n\n        if pixel_embeds is not None:\n            hidden_states = pixel_embeds\n        else:\n            if len(pixel_values.shape) == 4:\n                hidden_states = self.embeddings(pixel_values)\n            else:\n                raise ValueError(f'wrong pixel_values size: {pixel_values.shape}')\n        encoder_outputs = self.encoder(\n            inputs_embeds=hidden_states,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        last_hidden_state = encoder_outputs.last_hidden_state\n        pooled_output = last_hidden_state[:, 0, :]\n\n        if not return_dict:\n            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPooling(\n            last_hidden_state=last_hidden_state,\n            pooler_output=pooled_output,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n    \nclass InternVisionTower(nn.Module):\n    def __init__(self, vision_tower, vision_tower_cfg, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.vision_tower_name = vision_tower\n        self.config = InternVisionConfig.from_pretrained(self.vision_tower_name)\n\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(vision_tower_cfg, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(vision_tower_cfg, \"mm_tunable_parts\") and \"mm_vision_tower\" in vision_tower_cfg.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n        else:\n            self.cfg_only = self.config\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n        \n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n        self.vision_tower = InternVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n\n        # del self.vision_tower.encoder.layers[-1:]\n        self.vision_tower.head = nn.Identity()\n        self.vision_tower.requires_grad_(False)\n\n        self.is_loaded = True\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = image_forward_out.hidden_states[-1].to(image.dtype)\n                assert image_features.shape[-2] == 1025\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n            image_features = image_forward_outs.hidden_states[-1].to(images.dtype)\n            assert image_features.shape[-2] == 1025\n        return image_features[:, 1:]\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        for p in self.vision_tower.parameters():\n            return p.dtype\n\n    @property\n    def device(self):\n        for p in self.vision_tower.parameters():\n            return p.device\n\n    @property\n    def hidden_size(self):\n        return self.config.hidden_size\n\n    @property\n    def num_patches(self):\n        return (self.config.image_size // self.config.patch_size) ** 2\n\n    @property\n    def num_patches_per_side(self):\n        return self.config.image_size // self.config.patch_size\n        # return self.model_config[\"vision_cfg\"][\"image_size\"] // self.model_config[\"vision_cfg\"][\"patch_size\"]\n\n    @property\n    def image_size(self):\n        return self.config.image_size\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/eva_clip/eva_vit.py", "content": "\"\"\"\n# Adapted from https://github.com/baaivision/EVA/tree/master/EVA-CLIP\n\"\"\"\n\nfrom math import pi\nimport torch\nfrom torch import nn\nfrom einops import rearrange, repeat\nimport logging\nfrom llava.utils import rank0_print\n\n\ndef broadcat(tensors, dim=-1):\n    num_tensors = len(tensors)\n    shape_lens = set(list(map(lambda t: len(t.shape), tensors)))\n    assert len(shape_lens) == 1, \"tensors must all have the same number of dimensions\"\n    shape_len = list(shape_lens)[0]\n    dim = (dim + shape_len) if dim < 0 else dim\n    dims = list(zip(*map(lambda t: list(t.shape), tensors)))\n    expandable_dims = [(i, val) for i, val in enumerate(dims) if i != dim]\n    assert all([*map(lambda t: len(set(t[1])) <= 2, expandable_dims)]), \"invalid dimensions for broadcastable concatentation\"\n    max_dims = list(map(lambda t: (t[0], max(t[1])), expandable_dims))\n    expanded_dims = list(map(lambda t: (t[0], (t[1],) * num_tensors), max_dims))\n    expanded_dims.insert(dim, (dim, dims[dim]))\n    expandable_shapes = list(zip(*map(lambda t: t[1], expanded_dims)))\n    tensors = list(map(lambda t: t[0].expand(*t[1]), zip(tensors, expandable_shapes)))\n    return torch.cat(tensors, dim=dim)\n\n\ndef rotate_half(x):\n    x = rearrange(x, \"... (d r) -> ... d r\", r=2)\n    x1, x2 = x.unbind(dim=-1)\n    x = torch.stack((-x2, x1), dim=-1)\n    return rearrange(x, \"... d r -> ... (d r)\")\n\n\nclass VisionRotaryEmbeddingFast(nn.Module):\n    def __init__(self, dim, pt_seq_len, ft_seq_len=None, custom_freqs=None, freqs_for=\"lang\", theta=10000, max_freq=10, num_freqs=1, patch_dropout=0.0):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs = repeat(freqs, \"... n -> ... (n r)\", r=2)\n        freqs = broadcat((freqs[:, None, :], freqs[None, :, :]), dim=-1)\n\n        freqs_cos = freqs.cos().view(-1, freqs.shape[-1])\n        freqs_sin = freqs.sin().view(-1, freqs.shape[-1])\n\n        self.patch_dropout = patch_dropout\n\n        self.register_buffer(\"freqs_cos\", freqs_cos)\n        self.register_buffer(\"freqs_sin\", freqs_sin)\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, patch_indices_keep=None):\n        if patch_indices_keep is not None:\n            batch = t.size()[0]\n            batch_indices = torch.arange(batch)\n            batch_indices = batch_indices[..., None]\n\n            freqs_cos = repeat(self.freqs_cos, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n            freqs_sin = repeat(self.freqs_sin, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n\n            freqs_cos = freqs_cos[batch_indices, patch_indices_keep]\n            freqs_cos = rearrange(freqs_cos, \"n i m j -> n m i j\")\n            freqs_sin = freqs_sin[batch_indices, patch_indices_keep]\n            freqs_sin = rearrange(freqs_sin, \"n i m j -> n m i j\")\n\n            return t * freqs_cos + rotate_half(t) * freqs_sin\n\n        return t * self.freqs_cos + rotate_half(t) * self.freqs_sin\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm (with cast back to input dtype).\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        return x.to(orig_type)\n\n\nclass PatchDropout(nn.Module):\n    \"\"\"\n    https://arxiv.org/abs/2212.00794\n    \"\"\"\n\n    def __init__(self, prob, exclude_first_token=True):\n        super().__init__()\n        assert 0 <= prob < 1.0\n        self.prob = prob\n        self.exclude_first_token = exclude_first_token  # exclude CLS token\n        logging.info(f\"os.getenv('RoPE')={os.getenv('RoPE')}\")\n\n    def forward(self, x):\n        if not self.training or self.prob == 0.0:\n            return x\n\n        if self.exclude_first_token:\n            cls_tokens, x = x[:, :1], x[:, 1:]\n        else:\n            cls_tokens = torch.jit.annotate(torch.Tensor, x[:, :1])\n\n        batch = x.size()[0]\n        num_tokens = x.size()[1]\n\n        batch_indices = torch.arange(batch)\n        batch_indices = batch_indices[..., None]\n\n        keep_prob = 1 - self.prob\n        num_patches_keep = max(1, int(num_tokens * keep_prob))\n\n        rand = torch.randn(batch, num_tokens)\n        patch_indices_keep = rand.topk(num_patches_keep, dim=-1).indices\n\n        x = x[batch_indices, patch_indices_keep]\n\n        if self.exclude_first_token:\n            x = torch.cat((cls_tokens, x), dim=1)\n\n        if self.training and os.getenv(\"RoPE\") == \"1\":\n            return x, patch_indices_keep\n\n        return x\n\n\n# --------------------------------------------------------\n# Adapted from  https://github.com/microsoft/unilm/tree/master/beit\n# --------------------------------------------------------\nimport math\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from timm.models.layers import drop_path, to_2tuple, trunc_normal_\nexcept:\n    from timm.layers import drop_path, to_2tuple, trunc_normal_\n\nif os.getenv(\"ENV_TYPE\") == \"deepspeed\":\n    try:\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return \"p={}\".format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        drop=0.0,\n        subln=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.ffn_ln(x)\n\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass SwiGLU(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.SiLU, drop=0.0, norm_layer=nn.LayerNorm, subln=False):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        self.w1 = nn.Linear(in_features, hidden_features)\n        self.w2 = nn.Linear(in_features, hidden_features)\n\n        self.act = act_layer()\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n        self.w3 = nn.Linear(hidden_features, out_features)\n\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x1 = self.w1(x)\n        x2 = self.w2(x)\n        hidden = self.act(x1) * x2\n        x = self.ffn_ln(hidden)\n        x = self.w3(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0, window_size=None, attn_head_dim=None, xattn=False, rope=None, subln=False, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.subln = subln\n        if self.subln:\n            self.q_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.k_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.v_proj = nn.Linear(dim, all_head_dim, bias=False)\n        else:\n            self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.inner_attn_ln = norm_layer(all_head_dim) if subln else nn.Identity()\n        # self.proj = nn.Linear(all_head_dim, all_head_dim)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n\n        self.rope = rope\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        B, N, C = x.shape\n        if self.subln:\n            q = F.linear(input=x, weight=self.q_proj.weight, bias=self.q_bias)\n            k = F.linear(input=x, weight=self.k_proj.weight, bias=None)\n            v = F.linear(input=x, weight=self.v_proj.weight, bias=self.v_bias)\n\n            q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)  # B, num_heads, N, C\n            k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n            v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n        else:\n\n            qkv_bias = None\n            if self.q_bias is not None:\n                qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n\n            qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n            qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)  # 3, B, num_heads, N, C\n            q, k, v = qkv[0], qkv[1], qkv[2]\n\n        if self.rope:\n            # slightly fast impl\n            q_t = q[:, :, 1:, :]\n            ro_q_t = self.rope(q_t)\n            q = torch.cat((q[:, :, :1, :], ro_q_t), -2).type_as(v)\n\n            k_t = k[:, :, 1:, :]\n            ro_k_t = self.rope(k_t)\n            k = torch.cat((k[:, :, :1, :], ro_k_t), -2).type_as(v)\n\n        if self.xattn and xops is not None:\n            q = q.permute(0, 2, 1, 3)  # B, num_heads, N, C -> B, N, num_heads, C\n            k = k.permute(0, 2, 1, 3)\n            v = v.permute(0, 2, 1, 3)\n\n            x = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                p=self.xattn_drop,\n                scale=self.scale,\n            )\n            x = x.reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n\n            if self.relative_position_bias_table is not None:\n                relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0).type_as(attn)\n\n            if rel_pos_bias is not None:\n                attn = attn + rel_pos_bias.type_as(attn)\n\n            if attn_mask is not None:\n                attn_mask = attn_mask.bool()\n                attn = attn.masked_fill(~attn_mask[:, None, None, :], float(\"-inf\"))\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        init_values=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        window_size=None,\n        attn_head_dim=None,\n        xattn=False,\n        rope=None,\n        postnorm=False,\n        subln=False,\n        naiveswiglu=False,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim, xattn=xattn, rope=rope, subln=subln, norm_layer=norm_layer\n        )\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n\n        if naiveswiglu:\n            self.mlp = SwiGLU(\n                in_features=dim,\n                hidden_features=mlp_hidden_dim,\n                subln=subln,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, subln=subln, drop=drop)\n\n        if init_values is not None and init_values > 0:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n        self.postnorm = postnorm\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        if self.gamma_1 is None:\n            if self.postnorm:\n                x = x + self.drop_path(self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            if self.postnorm:\n                x = x + self.drop_path(self.gamma_1 * self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.gamma_2 * self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self):\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass EVAVisionTransformer(nn.Module):\n    \"\"\"Vision Transformer with support for patch or hybrid CNN input stage\"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=1000,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        init_values=None,\n        patch_dropout=0.0,\n        use_abs_pos_emb=True,\n        use_rel_pos_bias=False,\n        use_shared_rel_pos_bias=False,\n        rope=False,\n        use_mean_pooling=True,\n        init_scale=0.001,\n        grad_checkpointing=False,\n        xattn=False,\n        postnorm=False,\n        pt_hw_seq_len=16,\n        intp_freq=False,\n        naiveswiglu=False,\n        subln=False,\n    ):\n        super().__init__()\n        self.image_size = img_size\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        if rope:\n            half_head_dim = embed_dim // num_heads // 2\n            hw_seq_len = img_size // patch_size\n            self.rope = VisionRotaryEmbeddingFast(\n                dim=half_head_dim,\n                pt_seq_len=pt_hw_seq_len,\n                ft_seq_len=hw_seq_len if intp_freq else None,\n                # patch_dropout=patch_dropout\n            )\n        else:\n            self.rope = None\n\n        self.naiveswiglu = naiveswiglu\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    init_values=init_values,\n                    window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None,\n                    xattn=xattn,\n                    rope=self.rope,\n                    postnorm=postnorm,\n                    subln=subln,\n                    naiveswiglu=naiveswiglu,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=0.02)\n\n        trunc_normal_(self.cls_token, std=0.02)\n        # trunc_normal_(self.mask_token, std=.02)\n\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=0.02)\n            self.head.weight.data.mul_(init_scale)\n            self.head.bias.data.mul_(init_scale)\n\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0.0 else nn.Identity()\n\n        self.grad_checkpointing = grad_checkpointing\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            if self.naiveswiglu:\n                rescale(layer.mlp.w3.weight.data, layer_id + 1)\n            else:\n                rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.blocks[0].mlp.fc2.weight.dtype\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=\"\"):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x, return_all_features=False):\n\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        if os.getenv(\"RoPE\") == \"1\":\n            if self.training and not isinstance(self.patch_dropout, nn.Identity):\n                x, patch_indices_keep = self.patch_dropout(x)\n                # Directly pass patch_indices_keep to self.rope.forward\n                x = self.rope.forward(x, patch_indices_keep=patch_indices_keep)\n            else:\n                # Pass None or omit the patch_indices_keep argument for default behavior\n                x = self.rope.forward(x, patch_indices_keep=None)\n                x = self.patch_dropout(x)\n        else:\n            x = self.patch_dropout(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for i, blk in enumerate(self.blocks):\n            if i == len(self.blocks) - 1:\n                continue\n            if self.grad_checkpointing:\n                x = checkpoint(blk, x, (rel_pos_bias,))\n            else:\n                x = blk(x, rel_pos_bias=rel_pos_bias)\n\n        if not return_all_features:\n            x = self.norm(x)\n            if self.fc_norm is not None:\n                return self.fc_norm(x.mean(1))\n            else:\n                return x[:, 0]\n        return x\n\n    def forward(self, x, return_all_features=False):\n        if return_all_features:\n            return self.forward_features(x, return_all_features)\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\n\ndef load_state_dict(checkpoint_path: str, map_location: str = \"cpu\", model_key: str = \"model|module|state_dict\", is_openai: bool = False, skip_list: list = []):\n    if is_openai:\n        model = torch.jit.load(checkpoint_path, map_location=\"cpu\").eval()\n        state_dict = model.state_dict()\n        for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n            state_dict.pop(key, None)\n    else:\n        checkpoint = torch.load(checkpoint_path, map_location=map_location)\n        for mk in model_key.split(\"|\"):\n            if isinstance(checkpoint, dict) and mk in checkpoint:\n                state_dict = checkpoint[mk]\n                break\n            else:\n                state_dict = checkpoint\n        if next(iter(state_dict.items()))[0].startswith(\"module\"):\n            state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n    for k in skip_list:\n        if k in list(state_dict.keys()):\n            logging.info(f\"Removing key {k} from pretrained checkpoint\")\n            del state_dict[k]\n\n    if os.getenv(\"RoPE\") == \"1\":\n        for k in list(state_dict.keys()):\n            if \"freqs_cos\" in k or \"freqs_sin\" in k:\n                del state_dict[k]\n    return state_dict\n\n\ndef load_clip_visual_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n    # for k in list(state_dict.keys()):\n    #     if not k.startswith(\"visual.\"):\n    #         del state_dict[k]\n    # for k in list(state_dict.keys()):\n    #     if k.startswith(\"visual.\"):\n    #         new_k = k[7:]\n    #         state_dict[new_k] = state_dict[k]\n    #         del state_dict[k]\n    return state_dict\n\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\n\ntry:\n    from apex.normalization import FusedLayerNorm\nexcept:\n    FusedLayerNorm = LayerNorm\n    # print(\"Please build and install Nvidia apex package with option '--cuda_ext' according to https://github.com/NVIDIA/apex#from-source .\")\n\n\n@dataclass\nclass CLIPVisionCfg:\n    layers: Union[Tuple[int, int, int, int], int] = 12\n    width: int = 768\n    head_width: int = 64\n    mlp_ratio: float = 4.0\n    patch_size: int = 16\n    image_size: Union[Tuple[int, int], int] = 224\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    patch_dropout: float = 0.0  # what fraction of patches to dropout during training (0 would mean disabled and no patches dropped) - 0.5 to 0.75 recommended in the paper for optimal results\n    global_average_pool: bool = False  # whether to global average pool the last embedding layer, instead of using CLS token (https://arxiv.org/abs/2205.01580)\n    drop_path_rate: Optional[float] = None  # drop path rate\n    timm_model_name: str = None  # a valid model name overrides layers, width, patch_size\n    timm_model_pretrained: bool = False  # use (imagenet) pretrained weights for named model\n    timm_pool: str = \"avg\"  # feature pooling for timm model ('abs_attn', 'rot_attn', 'avg', '')\n    timm_proj: str = \"linear\"  # linear projection for timm model output ('linear', 'mlp', '')\n    timm_proj_bias: bool = False  # enable bias final projection\n    eva_model_name: str = None  # a valid eva model name overrides layers, width, patch_size\n    qkv_bias: bool = True\n    fusedLN: bool = False\n    xattn: bool = False\n    postnorm: bool = False\n    rope: bool = False\n    pt_hw_seq_len: int = 16  # 224/14\n    intp_freq: bool = False\n    naiveswiglu: bool = False\n    subln: bool = False\n\n\ndef create_norm_layer_factory(use_fused_ln, eps=1e-6):\n    # Otherwise, use the standard LayerNorm\n    return lambda num_features: nn.LayerNorm(num_features, eps=eps)\n\n\ndef _build_vision_tower(vision_tower_path: str, embed_dim: int, vision_cfg: CLIPVisionCfg, **kwargs):\n    if isinstance(vision_cfg, dict):\n        vision_cfg = CLIPVisionCfg(**vision_cfg)\n\n    if vision_cfg.eva_model_name:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n        # Determine the appropriate norm layer factory based on the configuration\n        norm_layer_factory = create_norm_layer_factory(vision_cfg.fusedLN, eps=1e-6)\n\n        visual = EVAVisionTransformer(\n            img_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            num_classes=embed_dim,\n            use_mean_pooling=vision_cfg.global_average_pool,  # False\n            init_values=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            embed_dim=vision_cfg.width,\n            depth=vision_cfg.layers,\n            num_heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            qkv_bias=vision_cfg.qkv_bias,\n            drop_path_rate=vision_cfg.drop_path_rate,\n            norm_layer=norm_layer_factory,\n            xattn=vision_cfg.xattn,\n            rope=vision_cfg.rope,\n            postnorm=vision_cfg.postnorm,\n            pt_hw_seq_len=vision_cfg.pt_hw_seq_len,  # 224/14\n            intp_freq=vision_cfg.intp_freq,\n            naiveswiglu=vision_cfg.naiveswiglu,\n            subln=vision_cfg.subln,\n        )\n\n        state_dict = load_clip_visual_state_dict(vision_tower_path)\n        incompatible_keys = visual.load_state_dict(state_dict, strict=False)\n        rank0_print(\"EVA-CLIP incompatible_keys:\", incompatible_keys)\n\n    return visual\n\n\nclass EVAEncoderWrapper(nn.Module):\n    def __init__(self, vision_tower_pretrained, config):\n        super(EVAEncoderWrapper, self).__init__()\n        self.config = config\n        self.config[\"vision_tower_path\"] = vision_tower_pretrained\n        self.model = _build_vision_tower(**self.config)\n\n    def forward(self, image, **kwargs):\n        encode = self.model(image, return_all_features=True)[:, 1:, :]  # remove the CLS token\n        return encode\n\n    @property\n    def dtype(self):\n        return list(self.parameters())[-1].dtype\n\n    @property\n    def device(self):\n        return list(self.parameters())[-1].device\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/open_clip_encoder.py", "content": "import torch\nimport torch.nn as nn\nfrom transformers import CLIPImageProcessor\nfrom llava.utils import rank0_print\n\ntry:\n    import open_clip\n    import torchvision\n    from open_clip.transformer import _expand_token\nexcept ImportError:\n    print(\"OpenCLIP not installed\")\n    open_clip = None\n\nHIDDEN_SIZE_DICT = {\n    \"ViT-H-14-378-quickgelu\": 1280,\n}\n\n\nclass OpenCLIPVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n        self.model_name = vision_tower.replace(\"open_clip_hub:\", \"\")\n        self.pretrained = args.vision_tower_pretrained\n        self.select_layer = args.mm_vision_select_layer\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n\n    def load_model(self, device_map=\"auto\"):\n        rank0_print(f\"Loading OpenCLIP model: {self.model_name}\")\n        rank0_print(f\"Pretrained: {self.pretrained}\")\n        vision_tower, _, image_processor = open_clip.create_model_and_transforms(model_name=self.model_name, pretrained=self.pretrained, precision=\"fp32\", device=\"cuda\")\n\n        resize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Resize)][0]\n        normalize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Normalize)][0]\n        self.resize_transform_size = resize_transform.size  # 224 or 384\n        self.patch_size = vision_tower.visual.conv1.kernel_size[0]  # 14 or 16\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(\n            \"openai/clip-vit-large-patch14\",\n            crop_size=resize_transform.size,\n            size={\"shortest_edge\": resize_transform.size},\n            image_mean=list(normalize_transform.mean),\n            image_std=list(normalize_transform.std),\n        )\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower = vision_tower.visual\n        self.vision_tower.requires_grad_(False)\n\n        self.is_loaded = True\n\n    def feature_select(self, image_forward_outs):\n        image_features = image_forward_outs[self.select_layer]\n        if self.select_feature == \"patch\":\n            image_features = image_features[:, 1:]\n        elif self.select_feature == \"cls_patch\":\n            image_features = image_features\n        elif self.select_feature == \"conv_flatten\":\n            image_features = image_features.flatten(2).transpose(1, 2)\n        else:\n            raise ValueError(f\"Unexpected select feature: {self.select_feature}\")\n        return image_features\n\n    def forward_visual(self, x, output_hidden_states=False):\n        if hasattr(self.vision_tower, \"trunk\") and hasattr(self.vision_tower.trunk, \"_intermediate_layers\"):\n            return self.vision_tower.trunk._intermediate_layers(x, abs(self.select_layer))\n        else:\n\n            def forward_openclip(self, x: torch.Tensor):\n                features = []\n                x = self.conv1(x)  # shape = [*, width, grid, grid]\n                x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n                x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n\n                # class embeddings and positional embeddings\n                x = torch.cat(\n                    [_expand_token(self.class_embedding, x.shape[0]).to(x.dtype), x],\n                    dim=1,\n                )\n                # shape = [*, grid ** 2 + 1, width]\n                x = x + self.positional_embedding.to(x.dtype)\n\n                x = self.patch_dropout(x)\n                x = self.ln_pre(x)\n\n                x = x.permute(1, 0, 2)  # NLD -> LND\n                for r in self.transformer.resblocks:\n                    x = r(x, attn_mask=None)\n                    features.append(x)\n                return features\n\n            return forward_openclip(self.vision_tower, x)\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.forward_visual(image.to(self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.forward_visual(images.to(self.dtype), output_hidden_states=True)\n            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        if hasattr(self.vision_tower, \"conv1\"):\n            return self.vision_tower.conv1.weight.dtype\n        if hasattr(self.vision_tower, \"trunk\"):\n            return self.vision_tower.trunk.patch_embed.proj.weight.dtype\n        raise NotImplementedError\n\n    @property\n    def device(self):\n        if hasattr(self.vision_tower, \"conv1\"):\n            return self.vision_tower.conv1.weight.device\n        if hasattr(self.vision_tower, \"trunk\"):\n            return self.vision_tower.trunk.patch_embed.proj.weight.device\n        raise NotImplementedError\n\n    @property\n    def config(self):\n        return None\n\n    @property\n    def hidden_size(self):\n        if self.model_name in HIDDEN_SIZE_DICT:\n            return HIDDEN_SIZE_DICT[self.model_name]\n        else:\n            raise NotImplementedError\n\n    @property\n    def num_patches(self):\n        image_size = self.resize_transform_size if isinstance(self.resize_transform_size, int) else self.resize_transform_size[0]\n        _num_patches = (image_size // self.patch_size) ** 2\n        if \"cls_patch\" in self.select_feature:\n            _num_patches += 1\n        return _num_patches\n\n    @property\n    def image_size(self):\n        return self.resize_transform_size\n\n    @property\n    def num_patches_per_side(self):\n        return self.resize_transform_size // self.patch_size\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_resampler/builder.py", "content": "import torch\n\nfrom .masked_drop import MaskedDrop\nfrom .spatial_pool import SpatialPool\nfrom .perceiver import PerceiverResampler\nfrom .qformer import Qformer\n\n\nclass IdentityMap(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n    @property\n    def config(self):\n        return {\"mm_resampler_type\": None}\n\n\ndef build_vision_resampler(model_args, delay_load=False, **kwargs):\n    resampler_type = getattr(model_args, \"mm_resampler_type\", None)\n    if resampler_type == \"masked_drop\":\n        return MaskedDrop(model_args)\n    elif resampler_type == \"spatial_pool\":\n        return SpatialPool(model_args, **kwargs)\n    elif resampler_type == \"perceiver\":\n        return PerceiverResampler(model_args, **kwargs)\n    elif resampler_type == \"qformer\":\n        return Qformer(model_args, **kwargs)\n    elif resampler_type is None:\n        return IdentityMap()\n\n    raise ValueError(f\"Unknown resampler type: {resampler_type}\")\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/timm_model.py", "content": "\"\"\" timm model adapter\n\nWraps timm (https://github.com/rwightman/pytorch-image-models) models for use as a vision tower in CLIP model.\n\"\"\"\n\nimport logging\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\ntry:\n    import timm\n    from timm.models.layers import Mlp, to_2tuple\n\n    try:\n        # old timm imports < 0.8.1\n        from timm.models.layers.attention_pool2d import RotAttentionPool2d\n        from timm.models.layers.attention_pool2d import AttentionPool2d as AbsAttentionPool2d\n    except ImportError:\n        # new timm imports >= 0.8.1\n        from timm.layers import RotAttentionPool2d\n        from timm.layers import AttentionPool2d as AbsAttentionPool2d\nexcept ImportError:\n    timm = None\n\nfrom .utils import freeze_batch_norm_2d\n\n\nclass TimmModel(nn.Module):\n    \"\"\"timm model adapter\n    # FIXME this adapter is a work in progress, may change in ways that break weight compat\n    \"\"\"\n\n    def __init__(self, model_name, embed_dim, image_size=224, pool=\"avg\", proj=\"linear\", proj_bias=False, drop=0.0, pretrained=False):\n        super().__init__()\n        if timm is None:\n            raise RuntimeError(\"Please `pip install timm` to use timm models.\")\n\n        self.image_size = to_2tuple(image_size)\n        self.trunk = timm.create_model(model_name, pretrained=pretrained)\n        feat_size = self.trunk.default_cfg.get(\"pool_size\", None)\n        feature_ndim = 1 if not feat_size else 2\n        if pool in (\"abs_attn\", \"rot_attn\"):\n            assert feature_ndim == 2\n            # if attn pooling used, remove both classifier and default pool\n            self.trunk.reset_classifier(0, global_pool=\"\")\n        else:\n            # reset global pool if pool config set, otherwise leave as network default\n            reset_kwargs = dict(global_pool=pool) if pool else {}\n            self.trunk.reset_classifier(0, **reset_kwargs)\n        prev_chs = self.trunk.num_features\n\n        head_layers = OrderedDict()\n        if pool == \"abs_attn\":\n            head_layers[\"pool\"] = AbsAttentionPool2d(prev_chs, feat_size=feat_size, out_features=embed_dim)\n            prev_chs = embed_dim\n        elif pool == \"rot_attn\":\n            head_layers[\"pool\"] = RotAttentionPool2d(prev_chs, out_features=embed_dim)\n            prev_chs = embed_dim\n        else:\n            assert proj, \"projection layer needed if non-attention pooling is used.\"\n\n        # NOTE attention pool ends with a projection layer, so proj should usually be set to '' if such pooling is used\n        if proj == \"linear\":\n            head_layers[\"drop\"] = nn.Dropout(drop)\n            head_layers[\"proj\"] = nn.Linear(prev_chs, embed_dim, bias=proj_bias)\n        elif proj == \"mlp\":\n            head_layers[\"mlp\"] = Mlp(prev_chs, 2 * embed_dim, embed_dim, drop=drop, bias=(True, proj_bias))\n\n        self.head = nn.Sequential(head_layers)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        \"\"\"lock modules\n        Args:\n            unlocked_groups (int): leave last n layer groups unlocked (default: 0)\n        \"\"\"\n        if not unlocked_groups:\n            # lock full model\n            for param in self.trunk.parameters():\n                param.requires_grad = False\n            if freeze_bn_stats:\n                freeze_batch_norm_2d(self.trunk)\n        else:\n            # NOTE: partial freeze requires latest timm (master) branch and is subject to change\n            try:\n                # FIXME import here until API stable and in an official release\n                from timm.models.helpers import group_parameters, group_modules\n            except ImportError:\n                raise RuntimeError(\"Please install latest timm `pip install git+https://github.com/rwightman/pytorch-image-models`\")\n            matcher = self.trunk.group_matcher()\n            gparams = group_parameters(self.trunk, matcher)\n            max_layer_id = max(gparams.keys())\n            max_layer_id = max_layer_id - unlocked_groups\n            for group_idx in range(max_layer_id + 1):\n                group = gparams[group_idx]\n                for param in group:\n                    self.trunk.get_parameter(param).requires_grad = False\n            if freeze_bn_stats:\n                gmodules = group_modules(self.trunk, matcher, reverse=True)\n                gmodules = {k for k, v in gmodules.items() if v <= max_layer_id}\n                freeze_batch_norm_2d(self.trunk, gmodules)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        try:\n            self.trunk.set_grad_checkpointing(enable)\n        except Exception as e:\n            logging.warning(\"grad checkpointing not supported for this timm image tower, continuing without...\")\n\n    def forward(self, x):\n        x = self.trunk(x)\n        x = self.head(x)\n        return x\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/utils.py", "content": "from itertools import repeat\nimport collections.abc\nimport logging\nimport math\nimport numpy as np\n\nimport torch\nfrom torch import nn as nn\nfrom torchvision.ops.misc import FrozenBatchNorm2d\nimport torch.nn.functional as F\n\n\n# open CLIP\ndef resize_clip_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    # Rescale the grid of position embeddings when loading from state_dict\n    old_pos_embed = state_dict.get(\"visual.positional_embedding\", None)\n    if old_pos_embed is None or not hasattr(model.visual, \"grid_size\"):\n        return\n    grid_size = to_2tuple(model.visual.grid_size)\n    extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n    new_seq_len = grid_size[0] * grid_size[1] + extra_tokens\n    if new_seq_len == old_pos_embed.shape[0]:\n        return\n\n    if extra_tokens:\n        pos_emb_tok, pos_emb_img = old_pos_embed[:extra_tokens], old_pos_embed[extra_tokens:]\n    else:\n        pos_emb_tok, pos_emb_img = None, old_pos_embed\n    old_grid_size = to_2tuple(int(math.sqrt(len(pos_emb_img))))\n\n    logging.info(\"Resizing position embedding grid-size from %s to %s\", old_grid_size, grid_size)\n    pos_emb_img = pos_emb_img.reshape(1, old_grid_size[0], old_grid_size[1], -1).permute(0, 3, 1, 2)\n    pos_emb_img = F.interpolate(\n        pos_emb_img,\n        size=grid_size,\n        mode=interpolation,\n        align_corners=True,\n    )\n    pos_emb_img = pos_emb_img.permute(0, 2, 3, 1).reshape(1, grid_size[0] * grid_size[1], -1)[0]\n    if pos_emb_tok is not None:\n        new_pos_embed = torch.cat([pos_emb_tok, pos_emb_img], dim=0)\n    else:\n        new_pos_embed = pos_emb_img\n    state_dict[\"visual.positional_embedding\"] = new_pos_embed\n\n\ndef resize_visual_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    # Rescale the grid of position embeddings when loading from state_dict\n    old_pos_embed = state_dict.get(\"positional_embedding\", None)\n    if old_pos_embed is None or not hasattr(model.visual, \"grid_size\"):\n        return\n    grid_size = to_2tuple(model.visual.grid_size)\n    extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n    new_seq_len = grid_size[0] * grid_size[1] + extra_tokens\n    if new_seq_len == old_pos_embed.shape[0]:\n        return\n\n    if extra_tokens:\n        pos_emb_tok, pos_emb_img = old_pos_embed[:extra_tokens], old_pos_embed[extra_tokens:]\n    else:\n        pos_emb_tok, pos_emb_img = None, old_pos_embed\n    old_grid_size = to_2tuple(int(math.sqrt(len(pos_emb_img))))\n\n    logging.info(\"Resizing position embedding grid-size from %s to %s\", old_grid_size, grid_size)\n    pos_emb_img = pos_emb_img.reshape(1, old_grid_size[0], old_grid_size[1], -1).permute(0, 3, 1, 2)\n    pos_emb_img = F.interpolate(\n        pos_emb_img,\n        size=grid_size,\n        mode=interpolation,\n        align_corners=True,\n    )\n    pos_emb_img = pos_emb_img.permute(0, 2, 3, 1).reshape(1, grid_size[0] * grid_size[1], -1)[0]\n    if pos_emb_tok is not None:\n        new_pos_embed = torch.cat([pos_emb_tok, pos_emb_img], dim=0)\n    else:\n        new_pos_embed = pos_emb_img\n    state_dict[\"positional_embedding\"] = new_pos_embed\n\n\ndef resize_evaclip_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    all_keys = list(state_dict.keys())\n    # interpolate position embedding\n    if \"visual.pos_embed\" in state_dict:\n        pos_embed_checkpoint = state_dict[\"visual.pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.visual.patch_embed.num_patches\n        # num_extra_tokens = model.visual.pos_embed.shape[-2] - num_patches\n        num_extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches**0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(pos_tokens, size=(new_size, new_size), mode=\"bicubic\", align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[\"visual.pos_embed\"] = new_pos_embed\n\n            patch_embed_proj = state_dict[\"visual.patch_embed.proj.weight\"]\n            patch_size = model.visual.patch_embed.patch_size\n            state_dict[\"visual.patch_embed.proj.weight\"] = torch.nn.functional.interpolate(patch_embed_proj.float(), size=patch_size, mode=\"bicubic\", align_corners=False)\n\n\ndef resize_eva_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    all_keys = list(state_dict.keys())\n    # interpolate position embedding\n    if \"pos_embed\" in state_dict:\n        pos_embed_checkpoint = state_dict[\"pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.visual.patch_embed.num_patches\n        # num_extra_tokens = model.visual.pos_embed.shape[-2] - num_patches\n        num_extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches**0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(pos_tokens, size=(new_size, new_size), mode=\"bicubic\", align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[\"pos_embed\"] = new_pos_embed\n\n            patch_embed_proj = state_dict[\"patch_embed.proj.weight\"]\n            patch_size = model.visual.patch_embed.patch_size\n            state_dict[\"patch_embed.proj.weight\"] = torch.nn.functional.interpolate(patch_embed_proj.float(), size=patch_size, mode=\"bicubic\", align_corners=False)\n\n\ndef resize_rel_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    all_keys = list(state_dict.keys())\n    for key in all_keys:\n        if \"relative_position_index\" in key:\n            state_dict.pop(key)\n\n        if \"relative_position_bias_table\" in key:\n            rel_pos_bias = state_dict[key]\n            src_num_pos, num_attn_heads = rel_pos_bias.size()\n            dst_num_pos, _ = model.visual.state_dict()[key].size()\n            dst_patch_shape = model.visual.patch_embed.patch_shape\n            if dst_patch_shape[0] != dst_patch_shape[1]:\n                raise NotImplementedError()\n            num_extra_tokens = dst_num_pos - (dst_patch_shape[0] * 2 - 1) * (dst_patch_shape[1] * 2 - 1)\n            src_size = int((src_num_pos - num_extra_tokens) ** 0.5)\n            dst_size = int((dst_num_pos - num_extra_tokens) ** 0.5)\n            if src_size != dst_size:\n                print(\"Position interpolate for %s from %dx%d to %dx%d\" % (key, src_size, src_size, dst_size, dst_size))\n                extra_tokens = rel_pos_bias[-num_extra_tokens:, :]\n                rel_pos_bias = rel_pos_bias[:-num_extra_tokens, :]\n\n                def geometric_progression(a, r, n):\n                    return a * (1.0 - r**n) / (1.0 - r)\n\n                left, right = 1.01, 1.5\n                while right - left > 1e-6:\n                    q = (left + right) / 2.0\n                    gp = geometric_progression(1, q, src_size // 2)\n                    if gp > dst_size // 2:\n                        right = q\n                    else:\n                        left = q\n\n                # if q > 1.090307:\n                #     q = 1.090307\n\n                dis = []\n                cur = 1\n                for i in range(src_size // 2):\n                    dis.append(cur)\n                    cur += q ** (i + 1)\n\n                r_ids = [-_ for _ in reversed(dis)]\n\n                x = r_ids + [0] + dis\n                y = r_ids + [0] + dis\n\n                t = dst_size // 2.0\n                dx = np.arange(-t, t + 0.1, 1.0)\n                dy = np.arange(-t, t + 0.1, 1.0)\n\n                print(\"Original positions = %s\" % str(x))\n                print(\"Target positions = %s\" % str(dx))\n\n                all_rel_pos_bias = []\n\n                for i in range(num_attn_heads):\n                    z = rel_pos_bias[:, i].view(src_size, src_size).float().numpy()\n                    f = F.interpolate.interp2d(x, y, z, kind=\"cubic\")\n                    all_rel_pos_bias.append(torch.Tensor(f(dx, dy)).contiguous().view(-1, 1).to(rel_pos_bias.device))\n\n                rel_pos_bias = torch.cat(all_rel_pos_bias, dim=-1)\n\n                new_rel_pos_bias = torch.cat((rel_pos_bias, extra_tokens), dim=0)\n                state_dict[key] = new_rel_pos_bias\n\n    # interpolate position embedding\n    if \"pos_embed\" in state_dict:\n        pos_embed_checkpoint = state_dict[\"pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.visual.patch_embed.num_patches\n        num_extra_tokens = model.visual.pos_embed.shape[-2] - num_patches\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches**0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(pos_tokens, size=(new_size, new_size), mode=\"bicubic\", align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[\"pos_embed\"] = new_pos_embed\n\n            patch_embed_proj = state_dict[\"patch_embed.proj.weight\"]\n            patch_size = model.visual.patch_embed.patch_size\n            state_dict[\"patch_embed.proj.weight\"] = torch.nn.functional.interpolate(patch_embed_proj.float(), size=patch_size, mode=\"bicubic\", align_corners=False)\n\n\ndef freeze_batch_norm_2d(module, module_match={}, name=\"\"):\n    \"\"\"\n    Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`. If `module` is\n    itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into `FrozenBatchNorm2d` and\n    returned. Otherwise, the module is walked recursively and submodules are converted in place.\n\n    Args:\n        module (torch.nn.Module): Any PyTorch module.\n        module_match (dict): Dictionary of full module names to freeze (all if empty)\n        name (str): Full module name (prefix)\n\n    Returns:\n        torch.nn.Module: Resulting module\n\n    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762\n    \"\"\"\n    res = module\n    is_match = True\n    if module_match:\n        is_match = name in module_match\n    if is_match and isinstance(module, (nn.modules.batchnorm.BatchNorm2d, nn.modules.batchnorm.SyncBatchNorm)):\n        res = FrozenBatchNorm2d(module.num_features)\n        res.num_features = module.num_features\n        res.affine = module.affine\n        if module.affine:\n            res.weight.data = module.weight.data.clone().detach()\n            res.bias.data = module.bias.data.clone().detach()\n        res.running_mean.data = module.running_mean.data\n        res.running_var.data = module.running_var.data\n        res.eps = module.eps\n    else:\n        for child_name, child in module.named_children():\n            full_child_name = \".\".join([name, child_name]) if name else child_name\n            new_child = freeze_batch_norm_2d(child, module_match, full_child_name)\n            if new_child is not child:\n                res.add_module(child_name, new_child)\n    return res\n\n\n# From PyTorch internals\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return x\n        return tuple(repeat(x, n))\n\n    return parse\n\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = lambda n, x: _ntuple(n)(x)\n\n\ndef is_logging(args):\n    def is_global_master(args):\n        return args.rank == 0\n\n    def is_local_master(args):\n        return args.local_rank == 0\n\n    def is_master(args, local=False):\n        return is_local_master(args) if local else is_global_master(args)\n\n    return is_master\n\n\nclass AllGather(torch.autograd.Function):\n    \"\"\"An autograd function that performs allgather on a tensor.\n    Performs all_gather operation on the provided tensors.\n    *** Warning ***: torch.distributed.all_gather has no gradient.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, tensor, rank, world_size):\n        tensors_gather = [torch.empty_like(tensor) for _ in range(world_size)]\n        torch.distributed.all_gather(tensors_gather, tensor)\n        ctx.rank = rank\n        ctx.batch_size = tensor.shape[0]\n        return torch.cat(tensors_gather, 0)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return (grad_output[ctx.batch_size * ctx.rank : ctx.batch_size * (ctx.rank + 1)], None, None)\n\n\nallgather = AllGather.apply\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/transform.py", "content": "from typing import Optional, Sequence, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as F\n\nfrom torchvision.transforms import Normalize, Compose, RandomResizedCrop, InterpolationMode, ToTensor, Resize, CenterCrop\n\nfrom .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\n\n\nclass ResizeMaxSize(nn.Module):\n\n    def __init__(self, max_size, interpolation=InterpolationMode.BICUBIC, fn=\"max\", fill=0):\n        super().__init__()\n        if not isinstance(max_size, int):\n            raise TypeError(f\"Size should be int. Got {type(max_size)}\")\n        self.max_size = max_size\n        self.interpolation = interpolation\n        self.fn = min if fn == \"min\" else min\n        self.fill = fill\n\n    def forward(self, img):\n        if isinstance(img, torch.Tensor):\n            height, width = img.shape[:2]\n        else:\n            width, height = img.size\n        scale = self.max_size / float(max(height, width))\n        if scale != 1.0:\n            new_size = tuple(round(dim * scale) for dim in (height, width))\n            img = F.resize(img, new_size, self.interpolation)\n            pad_h = self.max_size - new_size[0]\n            pad_w = self.max_size - new_size[1]\n            img = F.pad(img, padding=[pad_w // 2, pad_h // 2, pad_w - pad_w // 2, pad_h - pad_h // 2], fill=self.fill)\n        return img\n\n\ndef _convert_to_rgb(image):\n    return image.convert(\"RGB\")\n\n\n# class CatGen(nn.Module):\n#     def __init__(self, num=4):\n#         self.num = num\n#     def mixgen_batch(image, text):\n#         batch_size = image.shape[0]\n#         index = np.random.permutation(batch_size)\n\n#         cat_images = []\n#         for i in range(batch_size):\n#             # image mixup\n#             image[i,:] = lam * image[i,:] + (1 - lam) * image[index[i],:]\n#             # text concat\n#             text[i] = tokenizer((str(text[i]) + \" \" + str(text[index[i]])))[0]\n#         text = torch.stack(text)\n#         return image, text\n\n\ndef image_transform(\n    image_size: int,\n    is_train: bool,\n    mean: Optional[Tuple[float, ...]] = None,\n    std: Optional[Tuple[float, ...]] = None,\n    resize_longest_max: bool = False,\n    fill_color: int = 0,\n):\n    mean = mean or OPENAI_DATASET_MEAN\n    if not isinstance(mean, (list, tuple)):\n        mean = (mean,) * 3\n\n    std = std or OPENAI_DATASET_STD\n    if not isinstance(std, (list, tuple)):\n        std = (std,) * 3\n\n    if isinstance(image_size, (list, tuple)) and image_size[0] == image_size[1]:\n        # for square size, pass size as int so that Resize() uses aspect preserving shortest edge\n        image_size = image_size[0]\n\n    normalize = Normalize(mean=mean, std=std)\n    if is_train:\n        return Compose(\n            [\n                RandomResizedCrop(image_size, scale=(0.9, 1.0), interpolation=InterpolationMode.BICUBIC),\n                _convert_to_rgb,\n                ToTensor(),\n                normalize,\n            ]\n        )\n    else:\n        if resize_longest_max:\n            transforms = [ResizeMaxSize(image_size, fill=fill_color)]\n        else:\n            transforms = [\n                Resize(image_size, interpolation=InterpolationMode.BICUBIC),\n                CenterCrop(image_size),\n            ]\n        transforms.extend(\n            [\n                _convert_to_rgb,\n                ToTensor(),\n                normalize,\n            ]\n        )\n        return Compose(transforms)\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_resampler/perceiver.py", "content": "\"\"\"\nTaken from https://github.com/lucidrains/flamingo-pytorch\n\"\"\"\n\nimport torch\nfrom einops import rearrange, repeat\n\ntry:\n    from einops_exts import rearrange_many\nexcept:\n    pass\n\nfrom torch import einsum, nn\n\n\ndef exists(val):\n    return val is not None\n\n\ndef FeedForward(dim, mult=4):\n    inner_dim = int(dim * mult)\n    return nn.Sequential(\n        nn.LayerNorm(dim),\n        nn.Linear(dim, inner_dim, bias=False),\n        nn.GELU(),\n        nn.Linear(inner_dim, dim, bias=False),\n    )\n\n\nclass PerceiverAttention(nn.Module):\n    def __init__(self, *, dim, dim_head=64, heads=8):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm_media = nn.LayerNorm(dim)\n        self.norm_latents = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n\n    def forward(self, x, latents):\n        \"\"\"\n        Args:\n            x (torch.Tensor): image features\n                shape (b, T, n1, D)\n            latent (torch.Tensor): latent features\n                shape (b, T, n2, D)\n        \"\"\"\n        x = self.norm_media(x)\n        latents = self.norm_latents(latents)\n\n        h = self.heads\n\n        q = self.to_q(latents)\n        kv_input = torch.cat((x, latents), dim=-2)\n        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n        q, k, v = rearrange_many((q, k, v), \"b t n (h d) -> b h t n d\", h=h)\n        q = q * self.scale\n\n        # attention\n        sim = einsum(\"... i d, ... j d  -> ... i j\", q, k)\n        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n        attn = sim.softmax(dim=-1)\n\n        out = einsum(\"... i j, ... j d -> ... i d\", attn, v)\n        out = rearrange(out, \"b h t n d -> b t n (h d)\", h=h)\n        return self.to_out(out)\n\n\nclass PerceiverResamplerModule(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth=6,\n        dim_head=64,\n        heads=8,\n        num_latents=64,\n        max_num_media=None,\n        max_num_frames=None,\n        ff_mult=4,\n    ):\n        super().__init__()\n        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n        self.frame_embs = nn.Parameter(torch.randn(max_num_frames, dim)) if exists(max_num_frames) else None\n        self.media_time_embs = nn.Parameter(torch.randn(max_num_media, 1, dim)) if exists(max_num_media) else None\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                nn.ModuleList(\n                    [\n                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n                        FeedForward(dim=dim, mult=ff_mult) if ff_mult > 0 else nn.Identity(),\n                    ]\n                )\n            )\n\n        self.norm = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): image features\n                shape (b, T, F, v, D)\n        Returns:\n            shape (b, T, n, D) where n is self.num_latents\n        \"\"\"\n        b, T, F, v = x.shape[:4]\n\n        # frame and media time embeddings\n        if exists(self.frame_embs):\n            frame_embs = repeat(self.frame_embs[:F], \"F d -> b T F v d\", b=b, T=T, v=v)\n            x = x + frame_embs\n        x = rearrange(x, \"b T F v d -> b T (F v) d\")  # flatten the frame and spatial dimensions\n        if exists(self.media_time_embs):\n            x = x + self.media_time_embs[:T]\n\n        # blocks\n        latents = repeat(self.latents, \"n d -> b T n d\", b=b, T=T)\n        for attn, ff in self.layers:\n            latents = attn(x, latents) + latents\n            latents = ff(latents) + latents\n        return self.norm(latents)\n\n\nclass PerceiverResampler(nn.Module):\n    def __init__(self, model_args, vision_tower):\n        super().__init__()\n\n        self.depth = model_args.mm_perceiver_depth\n        self.num_latents = model_args.mm_perceiver_latents\n        self.ff_mult = model_args.mm_perceiver_ff_mult\n        self.pretrained = model_args.mm_perceiver_pretrained\n\n        self.perceiver = PerceiverResamplerModule(dim=vision_tower.hidden_size, depth=self.depth, num_latents=self.num_latents, ff_mult=self.ff_mult)\n\n        if self.pretrained is not None:\n            self.load_state_dict(torch.load(self.pretrained))\n\n    def forward(self, image_features, *args, **kwargs):\n        return self.perceiver(image_features[:, None, None]).squeeze(1)\n\n    @property\n    def config(self):\n        return {\n            \"mm_resampler_type\": \"perceiver\",\n            \"mm_perceiver_depth\": self.depth,\n            \"mm_perceiver_latents\": self.num_latents,\n            \"mm_perceiver_ff_mult\": self.ff_mult,\n            \"mm_perceiver_pretrained\": self.pretrained,\n        }\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_projector/pooler_projector.py", "content": "import torch.nn as nn\n\nclass PoolerProjector(nn.Module):\n    def __init__(self, config, vision_cfg):\n        super().__init__()\n        self._config = config\n        self.hw = vision_cfg.image_size // vision_cfg.patch_size\n\n        self.conv_pool = nn.Conv2d(config.mm_hidden_size, config.hidden_size, kernel_size=2, stride=2)\n\n        self.proj = nn.Sequential(\n            nn.GELU(),\n            nn.Linear(config.hidden_size, config.hidden_size),\n        )\n\n    def forward(self, x, *args, **kwargs):\n        height = width = self.hw\n        assert height * width == x.shape[1]\n        x = x.view(x.shape[0], height, width, -1).permute(0, 3, 1, 2)\n        x = self.conv_pool(x)\n        x = x.flatten(2).transpose(1, 2)\n        x = self.proj(x)\n        return x\n\n    @property\n    def config(self):\n        return {\"mm_projector_type\": \"pooler\"}\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/imagebind.py", "content": "import torch\nimport torch.nn as nn\n\nfrom transformers import CLIPImageProcessor\n\ntry:\n    from imagebind.models import imagebind_model\n    from imagebind.models.imagebind_model import ModalityType\n    from imagebind.data import load_and_transform_audio_data\nexcept ImportError:\n    pass\n\n\nclass ImageBindWrapper(nn.Module):\n    def __init__(self, vision_tower, select_layer, select_feature=\"patch\", delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.vision_tower_name = vision_tower\n        self.select_layer = select_layer\n        self.select_feature = select_feature\n\n        if not delay_load:\n            self.load_model()\n\n    def load_model(self):\n        self.image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n        self.vision_tower = imagebind_model.imagebind_huge(pretrained=True)\n        for p in self.vision_tower.parameters():\n            p.requires_grad = False\n        self.vision_tower.eval()\n        self.is_loaded = True\n\n    def train(self, mode=True):\n        self.training = mode\n\n        if self.is_loaded:\n            self.vision_tower.eval()\n\n    @torch.no_grad()\n    def forward(self, x):\n        if type(x) == dict:\n            if x[\"audios\"] is not None:\n                inputs = {ModalityType.AUDIO: load_and_transform_audio_data(x[\"audios\"], device=self.device).half()}\n                embeddings = self.vision_tower(inputs)\n                audio_embedding = embeddings[ModalityType.AUDIO]\n                return audio_embedding.unsqueeze(1)\n        else:\n            inputs = {ModalityType.VISION: x.to(dtype=self.dtype)}\n            embeddings = self.vision_tower(inputs)\n            vision_embedding = embeddings[ModalityType.VISION]\n            if vision_embedding.ndim == 2:\n                return vision_embedding.unsqueeze(1)\n            if vision_embedding.shape[1] == 257:\n                return vision_embedding[:, 1:]\n            raise ValueError(f\"Unexpected shape: {vision_embedding.shape}\")\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, 1024, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        return self.vision_tower.modality_preprocessors.vision.cls_token.dtype\n\n    @property\n    def device(self):\n        return self.vision_tower.modality_preprocessors.vision.cls_token.device\n\n    @property\n    def hidden_size(self):\n        return 1024\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_resampler/masked_drop.py", "content": "import torch\nimport torch.nn as nn\n\nimport random\n\n\nclass MaskedDrop(nn.Module):\n    def __init__(self, model_args):\n        super().__init__()\n\n        self.mode = model_args.mm_mask_drop_mode\n        self.skip_percentage = model_args.mm_mask_drop_skip_percentage\n        self.ratio = model_args.mm_mask_drop_ratio\n        self.ratio_upper = model_args.mm_mask_drop_ratio_upper\n        self.ratio_lower = model_args.mm_mask_drop_ratio_lower\n\n    def forward(self, image_features, *args, **kwargs):\n\n        if not self.training:\n            return image_features\n\n        if self.skip_percentage > random.random():\n            return image_features\n\n        masked_features = []\n\n        for image_feature in image_features:\n            num_tokens = image_feature.shape[0]\n            if self.mode == \"fixed\":\n                num_keep = int(num_tokens * self.ratio)\n                masked_features.append(self.random_masking(image_feature.unsqueeze(0), num_keep)[0][0])\n            elif self.mode == \"range\":\n                num_keep = int(num_tokens * random.uniform(self.ratio_lower, self.ratio_upper))\n                masked_features.append(self.random_masking(image_feature.unsqueeze(0), num_keep)[0])\n            elif self.mode == \"cls_only\":\n                masked_features.append(image_feature[0:1])\n            else:\n                raise ValueError(f\"Unexpected masked drop mode: {self.mode}\")\n\n        if self.mode not in [\"range\"] and (type(image_features) is not list or self.mode in [\"cls_only\"]):\n            masked_features = torch.stack(masked_features, dim=0)\n\n        return masked_features\n\n    @property\n    def config(self):\n        return {\n            \"mm_resampler_type\": \"masked_drop\",\n            \"mm_mask_drop_mode\": self.mode,\n            \"mm_mask_drop_skip_percentage\": self.skip_percentage,\n            \"mm_mask_drop_ratio\": self.ratio,\n            \"mm_mask_drop_ratio_upper\": self.ratio_upper,\n            \"mm_mask_drop_ratio_lower\": self.ratio_lower,\n        }\n\n    def random_masking(self, x, len_keep):\n        \"\"\"\n        Perform per-sample random masking by per-sample shuffling.\n        Per-sample shuffling is done by argsort random noise.\n        x: [N, L, D], sequence\n        \"\"\"\n        N, L, D = x.shape  # batch, length, dim\n\n        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n\n        # sort noise for each sample\n        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n        ids_restore = torch.argsort(ids_shuffle, dim=1)\n\n        # keep the first subset\n        ids_keep = ids_shuffle[:, :len_keep]\n        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n\n        # generate the binary mask: 0 is keep, 1 is remove\n        mask = torch.ones([N, L], device=x.device)\n        mask[:, :len_keep] = 0\n        # unshuffle to get the binary mask\n        mask = torch.gather(mask, dim=1, index=ids_restore)\n\n        return x_masked, mask, ids_restore\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/tokenizer.py", "content": "\"\"\" CLIP tokenizer\n\nCopied from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport gzip\nimport html\nimport os\nfrom functools import lru_cache\nfrom typing import Union, List\n\nimport ftfy\nimport regex as re\nimport torch\n\n# https://stackoverflow.com/q/62691279\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n@lru_cache()\ndef default_bpe():\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"bpe_simple_vocab_16e6.txt.gz\")\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"Â¡\"), ord(\"Â¬\") + 1)) + list(range(ord(\"Â®\"), ord(\"Ã¿\") + 1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n\n\ndef whitespace_clean(text):\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = text.strip()\n    return text\n\n\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe(), special_tokens=None):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split(\"\\n\")\n        merges = merges[1 : 49152 - 256 - 2 + 1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v + \"</w>\" for v in vocab]\n        for merge in merges:\n            vocab.append(\"\".join(merge))\n        if not special_tokens:\n            special_tokens = [\"<start_of_text>\", \"<end_of_text>\"]\n        else:\n            special_tokens = [\"<start_of_text>\", \"<end_of_text>\"] + special_tokens\n        vocab.extend(special_tokens)\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {t: t for t in special_tokens}\n        special = \"|\".join(special_tokens)\n        self.pat = re.compile(special + r\"\"\"|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n\n        self.vocab_size = len(self.encoder)\n        self.all_special_ids = [self.encoder[t] for t in special_tokens]\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token + \"</w>\"\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \" \".join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \"\".join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=\"replace\").replace(\"</w>\", \" \")\n        return text\n\n\n_tokenizer = SimpleTokenizer()\n\n\ndef tokenize(texts: Union[str, List[str]], context_length: int = 77) -> torch.LongTensor:\n    \"\"\"\n    Returns the tokenized representation of given input string(s)\n\n    Parameters\n    ----------\n    texts : Union[str, List[str]]\n        An input string or a list of input strings to tokenize\n    context_length : int\n        The context length to use; all CLIP models use 77 as the context length\n\n    Returns\n    -------\n    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<start_of_text>\"]\n    eot_token = _tokenizer.encoder[\"<end_of_text>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        if len(tokens) > context_length:\n            tokens = tokens[:context_length]  # Truncate\n            tokens[-1] = eot_token\n        result[i, : len(tokens)] = torch.tensor(tokens)\n\n    return result\n\n\nclass HFTokenizer:\n    \"HuggingFace tokenizer wrapper\"\n\n    def __init__(self, tokenizer_name: str):\n        from transformers import AutoTokenizer\n\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\n    def __call__(self, texts: Union[str, List[str]], context_length: int = 77) -> torch.Tensor:\n        # same cleaning as for default tokenizer, except lowercasing\n        # adding lower (for case-sensitive tokenizers) will make it more robust but less sensitive to nuance\n        if isinstance(texts, str):\n            texts = [texts]\n        texts = [whitespace_clean(basic_clean(text)) for text in texts]\n        input_ids = self.tokenizer(texts, return_tensors=\"pt\", max_length=context_length, padding=\"max_length\", truncation=True).input_ids\n        return input_ids\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/eva_clip/eva_clip_processors.py", "content": "\"\"\"\n# Adapted from https://github.com/baaivision/EVA/tree/master/EVA-CLIP\n\"\"\"\n\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\nfrom transformers.image_processing_utils import BatchFeature\nfrom PIL import Image\nfrom transformers.image_transforms import convert_to_rgb\n\n\nclass BaseProcessor:\n    def __init__(self):\n        self.transform = lambda x: x\n        return\n\n    def __call__(self, item):\n        return self.transform(item)\n\n\nclass EvaClipImageBaseProcessor(BaseProcessor):\n    def __init__(self, mean=None, std=None):\n        self.mean = (0.48145466, 0.4578275, 0.40821073) if mean is None else mean\n        self.std = (0.26862954, 0.26130258, 0.27577711) if std is None else std\n\n        self.normalize = transforms.Normalize(self.mean, self.std)\n\n    @property\n    def image_mean(self):\n        return self.mean\n\n\nclass EvaClipImageTrainProcessor(EvaClipImageBaseProcessor):\n    def __init__(self, image_size=224, mean=None, std=None, min_scale=0.5, max_scale=1.0):\n        super().__init__(mean=mean, std=std)\n\n        self.transform = transforms.Compose(\n            [\n                convert_to_rgb,\n                transforms.Resize(\n                    image_size,\n                    interpolation=InterpolationMode.BICUBIC,\n                ),\n                transforms.CenterCrop(image_size),\n                transforms.ToTensor(),\n                self.normalize,\n            ]\n        )\n\n        self.image_size = image_size\n\n    def preprocess(self, images, return_tensors):\n        if isinstance(images, Image.Image):\n            images = [images]\n        else:\n            assert isinstance(images, list)\n\n        transformed_images = [self.transform(image).numpy() for image in images]\n        data = {\"pixel_values\": transformed_images}\n\n        return BatchFeature(data=data, tensor_type=return_tensors)\n\n    def __call__(self, item):\n        return self.transform(item)\n\n    @property\n    def crop_size(self):\n        return {\"height\": self.image_size, \"width\": self.image_size}\n\n    @property\n    def size(self):\n        return {\"shortest_edge\": self.image_size}\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_resampler/spatial_pool.py", "content": "import math\nimport torch.nn as nn\n\n\nclass SpatialPool(nn.Module):\n    def __init__(self, model_args, vision_tower):\n        super().__init__()\n\n        self.mode = model_args.mm_spatial_pool_mode\n        self.stride = model_args.mm_spatial_pool_stride\n        self.out_channels = getattr(model_args, \"mm_spatial_pool_out_channels\", vision_tower.hidden_size)\n\n        if self.mode == \"average\":\n            self.pool = nn.AvgPool2d(kernel_size=self.stride, stride=self.stride)\n        elif self.mode == \"max\":\n            self.pool = nn.MaxPool2d(kernel_size=self.stride, stride=self.stride)\n        elif self.mode == \"conv\":\n            self.pool = nn.Conv2d(in_channels=vision_tower.hidden_size, out_channels=self.out_channels, kernel_size=self.stride, stride=self.stride)\n        else:\n            raise ValueError(f\"Unknown pooling mode: {self.pool}.\")\n\n    def forward(self, image_features, images, *args, **kwargs):\n        ori_W = int(math.sqrt(image_features.shape[1] * images.shape[3] // images.shape[2]))\n        ori_H = int(ori_W * images.shape[2] // images.shape[3])\n\n        B, _, F = image_features.shape\n\n        image_features_spatial = image_features.view(B, ori_H, ori_H, F).permute(0, 3, 1, 2)\n        image_features_spatial_pool = self.pool(image_features_spatial)\n\n        return image_features_spatial_pool.flatten(2).transpose(1, 2).contiguous()\n\n    @property\n    def config(self):\n        return {\n            \"mm_resampler_type\": \"spatial_pool\",\n            \"mm_spatial_pool_stride\": self.stride,\n            \"mm_spatial_pool_mode\": self.mode,\n            \"mm_spatial_pool_out_channels\": self.out_channels,\n        }\n\n    @property\n    def hidden_size(self):\n        return self.out_channels\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_projector/builder.py", "content": "r\"\"\"\nThis module defines various neural network components and a factory function to build vision projectors.\n\nClasses:\n------------\n    1. IdentityMap(nn.Module):\n        A neural network module that returns the input as is.\n        \n    2. SimpleResBlock(nn.Module):\n        A simple residual block with LayerNorm and a linear projection followed by GELU activation.\n        \nFunctions:\n------------\n    - build_vision_projector(config, delay_load=False, **kwargs):\n        Factory function to build different types of vision projectors based on the configuration.\n        \n\"\"\"\n\nimport re\nimport torch.nn as nn\n\nfrom .pooler_projector import PoolerProjector\n\n\nclass IdentityMap(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n    @property\n    def config(self):\n        return {\"mm_projector_type\": \"identity\"}\n\n\nclass SimpleResBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.pre_norm = nn.LayerNorm(channels)\n\n        self.proj = nn.Sequential(nn.Linear(channels, channels), nn.GELU(), nn.Linear(channels, channels))\n\n    def forward(self, x):\n        x = self.pre_norm(x)\n        return x + self.proj(x)\n\n\ndef build_vision_projector(config, delay_load=False, **kwargs):\n    projector_type = getattr(config, \"mm_projector_type\", \"linear\")\n\n    if projector_type == \"linear\":\n        return nn.Linear(config.mm_hidden_size, config.hidden_size)\n\n    if projector_type == \"pooler\":\n        return PoolerProjector(config, kwargs[\"vision_cfg\"])\n\n    mlp_gelu_match = re.match(r\"^mlp(\\d+)x_gelu$\", projector_type)\n    if mlp_gelu_match:\n        mlp_depth = int(mlp_gelu_match.group(1))\n        modules = [nn.Linear(config.mm_hidden_size, config.hidden_size)]\n        for _ in range(1, mlp_depth):\n            modules.append(nn.GELU())\n            modules.append(nn.Linear(config.hidden_size, config.hidden_size))\n        return nn.Sequential(*modules)\n\n    mlp_gelu_resnet_match = re.match(r\"^mlp(\\d+)x_res(\\d+)x_gelu$\", projector_type)\n    if mlp_gelu_resnet_match:\n        mlp_depth = int(mlp_gelu_resnet_match.group(1))\n        res_depth = int(mlp_gelu_resnet_match.group(2))\n        modules = [nn.Linear(config.mm_hidden_size, config.hidden_size)]\n        for _ in range(1, mlp_depth):\n            modules.append(nn.GELU())\n            modules.append(nn.Linear(config.hidden_size, config.hidden_size))\n        for _ in range(res_depth):\n            modules.append(SimpleResBlock(config.hidden_size))\n        return nn.Sequential(*modules)\n\n    if projector_type == \"identity\":\n        return IdentityMap()\n\n    raise ValueError(f\"Unknown projector type: {projector_type}\")\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/rope.py", "content": "from math import pi\nimport torch\nfrom torch import nn\nfrom einops import rearrange, repeat\nimport logging\n\n\ndef broadcat(tensors, dim=-1):\n    num_tensors = len(tensors)\n    shape_lens = set(list(map(lambda t: len(t.shape), tensors)))\n    assert len(shape_lens) == 1, \"tensors must all have the same number of dimensions\"\n    shape_len = list(shape_lens)[0]\n    dim = (dim + shape_len) if dim < 0 else dim\n    dims = list(zip(*map(lambda t: list(t.shape), tensors)))\n    expandable_dims = [(i, val) for i, val in enumerate(dims) if i != dim]\n    assert all([*map(lambda t: len(set(t[1])) <= 2, expandable_dims)]), \"invalid dimensions for broadcastable concatentation\"\n    max_dims = list(map(lambda t: (t[0], max(t[1])), expandable_dims))\n    expanded_dims = list(map(lambda t: (t[0], (t[1],) * num_tensors), max_dims))\n    expanded_dims.insert(dim, (dim, dims[dim]))\n    expandable_shapes = list(zip(*map(lambda t: t[1], expanded_dims)))\n    tensors = list(map(lambda t: t[0].expand(*t[1]), zip(tensors, expandable_shapes)))\n    return torch.cat(tensors, dim=dim)\n\n\ndef rotate_half(x):\n    x = rearrange(x, \"... (d r) -> ... d r\", r=2)\n    x1, x2 = x.unbind(dim=-1)\n    x = torch.stack((-x2, x1), dim=-1)\n    return rearrange(x, \"... d r -> ... (d r)\")\n\n\nclass VisionRotaryEmbedding(nn.Module):\n    def __init__(\n        self,\n        dim,\n        pt_seq_len,\n        ft_seq_len=None,\n        custom_freqs=None,\n        freqs_for=\"lang\",\n        theta=10000,\n        max_freq=10,\n        num_freqs=1,\n    ):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs_h = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs_h = repeat(freqs_h, \"... n -> ... (n r)\", r=2)\n\n        freqs_w = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs_w = repeat(freqs_w, \"... n -> ... (n r)\", r=2)\n\n        freqs = broadcat((freqs_h[:, None, :], freqs_w[None, :, :]), dim=-1)\n\n        self.register_buffer(\"freqs_cos\", freqs.cos())\n        self.register_buffer(\"freqs_sin\", freqs.sin())\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, start_index=0):\n        rot_dim = self.freqs_cos.shape[-1]\n        end_index = start_index + rot_dim\n        assert rot_dim <= t.shape[-1], f\"feature dimension {t.shape[-1]} is not of sufficient size to rotate in all the positions {rot_dim}\"\n        t_left, t, t_right = t[..., :start_index], t[..., start_index:end_index], t[..., end_index:]\n        t = (t * self.freqs_cos) + (rotate_half(t) * self.freqs_sin)\n\n        return torch.cat((t_left, t, t_right), dim=-1)\n\n\nclass VisionRotaryEmbeddingFast(nn.Module):\n    def __init__(self, dim, pt_seq_len, ft_seq_len=None, custom_freqs=None, freqs_for=\"lang\", theta=10000, max_freq=10, num_freqs=1, patch_dropout=0.0):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs = repeat(freqs, \"... n -> ... (n r)\", r=2)\n        freqs = broadcat((freqs[:, None, :], freqs[None, :, :]), dim=-1)\n\n        freqs_cos = freqs.cos().view(-1, freqs.shape[-1])\n        freqs_sin = freqs.sin().view(-1, freqs.shape[-1])\n\n        self.patch_dropout = patch_dropout\n\n        self.register_buffer(\"freqs_cos\", freqs_cos)\n        self.register_buffer(\"freqs_sin\", freqs_sin)\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, patch_indices_keep=None):\n        if patch_indices_keep is not None:\n            batch = t.size()[0]\n            batch_indices = torch.arange(batch)\n            batch_indices = batch_indices[..., None]\n\n            freqs_cos = repeat(self.freqs_cos, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n            freqs_sin = repeat(self.freqs_sin, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n\n            freqs_cos = freqs_cos[batch_indices, patch_indices_keep]\n            freqs_cos = rearrange(freqs_cos, \"n i m j -> n m i j\")\n            freqs_sin = freqs_sin[batch_indices, patch_indices_keep]\n            freqs_sin = rearrange(freqs_sin, \"n i m j -> n m i j\")\n\n            return t * freqs_cos + rotate_half(t) * freqs_sin\n\n        return t * self.freqs_cos + rotate_half(t) * self.freqs_sin\n"}
{"type": "source_file", "path": "instellavl/serve/__init__.py", "content": ""}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/dev_eva_clip/eva_clip/transformer.py", "content": "import os\nimport logging\nfrom collections import OrderedDict\nimport math\nfrom typing import Callable, Optional, Sequence\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ntry:\n    from timm.models.layers import trunc_normal_\nexcept:\n    from timm.layers import trunc_normal_\n\nfrom .rope import VisionRotaryEmbedding, VisionRotaryEmbeddingFast\nfrom .utils import to_2tuple\n\nif os.getenv(\"ENV_TYPE\") == \"deepspeed\":\n    try:\n        import deepspeed\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        print(\"Please 'pip install deepspeed'\")\n        deepspeed = None\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass LayerNormFp32(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16 (by casting to float32 and back).\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, x: torch.Tensor):\n        output = F.layer_norm(\n            x.float(),\n            self.normalized_shape,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,\n        )\n        return output.type_as(x)\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm (with cast back to input dtype).\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        return x.to(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    # NOTE This is slower than nn.GELU or nn.SiLU and uses more GPU memory\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass LayerScale(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n\n\nclass PatchDropout(nn.Module):\n    \"\"\"\n    https://arxiv.org/abs/2212.00794\n    \"\"\"\n\n    def __init__(self, prob, exclude_first_token=True):\n        super().__init__()\n        assert 0 <= prob < 1.0\n        self.prob = prob\n        self.exclude_first_token = exclude_first_token  # exclude CLS token\n        logging.info(f\"os.getenv('RoPE')={os.getenv('RoPE')}\")\n\n    def forward(self, x):\n        if not self.training or self.prob == 0.0:\n            return x\n\n        if self.exclude_first_token:\n            cls_tokens, x = x[:, :1], x[:, 1:]\n        else:\n            cls_tokens = torch.jit.annotate(torch.Tensor, x[:, :1])\n\n        batch = x.size()[0]\n        num_tokens = x.size()[1]\n\n        batch_indices = torch.arange(batch)\n        batch_indices = batch_indices[..., None]\n\n        keep_prob = 1 - self.prob\n        num_patches_keep = max(1, int(num_tokens * keep_prob))\n\n        rand = torch.randn(batch, num_tokens)\n        patch_indices_keep = rand.topk(num_patches_keep, dim=-1).indices\n\n        x = x[batch_indices, patch_indices_keep]\n\n        if self.exclude_first_token:\n            x = torch.cat((cls_tokens, x), dim=1)\n\n        if self.training and os.getenv(\"RoPE\") == \"1\":\n            return x, patch_indices_keep\n\n        return x\n\n\ndef _in_projection_packed(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    w: torch.Tensor,\n    b: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    https://github.com/pytorch/pytorch/blob/db2a237763eb8693a20788be94f8c192e762baa8/torch/nn/functional.py#L4726\n    \"\"\"\n    E = q.size(-1)\n    if k is v:\n        if q is k:\n            # self-attention\n            return F.linear(q, w, b).chunk(3, dim=-1)\n        else:\n            # encoder-decoder attention\n            w_q, w_kv = w.split([E, E * 2])\n            if b is None:\n                b_q = b_kv = None\n            else:\n                b_q, b_kv = b.split([E, E * 2])\n            return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).chunk(2, dim=-1)\n    else:\n        w_q, w_k, w_v = w.chunk(3)\n        if b is None:\n            b_q = b_k = b_v = None\n        else:\n            b_q, b_k, b_v = b.chunk(3)\n        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=True, scaled_cosine=False, scale_heads=False, logit_scale_max=math.log(1.0 / 0.01), attn_drop=0.0, proj_drop=0.0, xattn=False, rope=False):\n        super().__init__()\n        self.scaled_cosine = scaled_cosine\n        self.scale_heads = scale_heads\n        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim**-0.5\n        self.logit_scale_max = logit_scale_max\n\n        # keeping in_proj in this form (instead of nn.Linear) to match weight scheme of original\n        self.in_proj_weight = nn.Parameter(torch.randn((dim * 3, dim)) * self.scale)\n        if qkv_bias:\n            self.in_proj_bias = nn.Parameter(torch.zeros(dim * 3))\n        else:\n            self.in_proj_bias = None\n\n        if self.scaled_cosine:\n            self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n        else:\n            self.logit_scale = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        if self.scale_heads:\n            self.head_scale = nn.Parameter(torch.ones((num_heads, 1, 1)))\n        else:\n            self.head_scale = None\n        self.out_proj = nn.Linear(dim, dim)\n        self.out_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n        self.rope = rope\n\n    def forward(self, x, attn_mask: Optional[torch.Tensor] = None):\n        L, N, C = x.shape\n        q, k, v = F.linear(x, self.in_proj_weight, self.in_proj_bias).chunk(3, dim=-1)\n        if self.xattn:\n            q = q.contiguous().view(L, N, self.num_heads, -1).transpose(0, 1)\n            k = k.contiguous().view(L, N, self.num_heads, -1).transpose(0, 1)\n            v = v.contiguous().view(L, N, self.num_heads, -1).transpose(0, 1)\n\n            x = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                p=self.xattn_drop,\n                scale=self.scale if self.logit_scale is None else None,\n                attn_bias=xops.LowerTriangularMask() if attn_mask is not None else None,\n            )\n        else:\n            q = q.contiguous().view(L, N * self.num_heads, -1).transpose(0, 1)\n            k = k.contiguous().view(L, N * self.num_heads, -1).transpose(0, 1)\n            v = v.contiguous().view(L, N * self.num_heads, -1).transpose(0, 1)\n\n            if self.logit_scale is not None:\n                attn = torch.bmm(F.normalize(q, dim=-1), F.normalize(k, dim=-1).transpose(-1, -2))\n                logit_scale = torch.clamp(self.logit_scale, max=self.logit_scale_max).exp()\n                attn = attn.view(N, self.num_heads, L, L) * logit_scale\n                attn = attn.view(-1, L, L)\n            else:\n                q = q * self.scale\n                attn = torch.bmm(q, k.transpose(-1, -2))\n\n            if attn_mask is not None:\n                if attn_mask.dtype == torch.bool:\n                    new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n                    new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n                    attn_mask = new_attn_mask\n                attn += attn_mask\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = torch.bmm(attn, v)\n\n        if self.head_scale is not None:\n            x = x.view(N, self.num_heads, L, C) * self.head_scale\n            x = x.view(-1, L, C)\n        x = x.transpose(0, 1).reshape(L, N, C)\n        x = self.out_proj(x)\n        x = self.out_drop(x)\n        return x\n\n\nclass CustomAttention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=True, scaled_cosine=True, scale_heads=False, logit_scale_max=math.log(1.0 / 0.01), attn_drop=0.0, proj_drop=0.0, xattn=False):\n        super().__init__()\n        self.scaled_cosine = scaled_cosine\n        self.scale_heads = scale_heads\n        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim**-0.5\n        self.logit_scale_max = logit_scale_max\n\n        # keeping in_proj in this form (instead of nn.Linear) to match weight scheme of original\n        self.in_proj_weight = nn.Parameter(torch.randn((dim * 3, dim)) * self.scale)\n        if qkv_bias:\n            self.in_proj_bias = nn.Parameter(torch.zeros(dim * 3))\n        else:\n            self.in_proj_bias = None\n\n        if self.scaled_cosine:\n            self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n        else:\n            self.logit_scale = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        if self.scale_heads:\n            self.head_scale = nn.Parameter(torch.ones((num_heads, 1, 1)))\n        else:\n            self.head_scale = None\n        self.out_proj = nn.Linear(dim, dim)\n        self.out_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n\n    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        q, k, v = _in_projection_packed(query, key, value, self.in_proj_weight, self.in_proj_bias)\n        N_q, B_q, C_q = q.shape\n        N_k, B_k, C_k = k.shape\n        N_v, B_v, C_v = v.shape\n        if self.xattn:\n            # B, N, C -> B, N, num_heads, C\n            q = q.permute(1, 0, 2).reshape(B_q, N_q, self.num_heads, -1)\n            k = k.permute(1, 0, 2).reshape(B_k, N_k, self.num_heads, -1)\n            v = v.permute(1, 0, 2).reshape(B_v, N_v, self.num_heads, -1)\n\n            x = xops.memory_efficient_attention(q, k, v, p=self.xattn_drop, scale=self.scale if self.logit_scale is None else None, attn_bias=xops.LowerTriangularMask() if attn_mask is not None else None)\n        else:\n            # B*H, L, C\n            q = q.contiguous().view(N_q, B_q * self.num_heads, -1).transpose(0, 1)\n            k = k.contiguous().view(N_k, B_k * self.num_heads, -1).transpose(0, 1)\n            v = v.contiguous().view(N_v, B_v * self.num_heads, -1).transpose(0, 1)\n\n            if self.logit_scale is not None:\n                # B*H, N_q, N_k\n                attn = torch.bmm(F.normalize(q, dim=-1), F.normalize(k, dim=-1).transpose(-1, -2))\n                logit_scale = torch.clamp(self.logit_scale, max=self.logit_scale_max).exp()\n                attn = attn.view(B_q, self.num_heads, N_q, N_k) * logit_scale\n                attn = attn.view(-1, N_q, N_k)\n            else:\n                q = q * self.scale\n                attn = torch.bmm(q, k.transpose(-1, -2))\n\n            if attn_mask is not None:\n                if attn_mask.dtype == torch.bool:\n                    new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n                    new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n                    attn_mask = new_attn_mask\n                attn += attn_mask\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = torch.bmm(attn, v)\n\n        if self.head_scale is not None:\n            x = x.view(B_q, self.num_heads, N_q, C_q) * self.head_scale\n            x = x.view(-1, N_q, C_q)\n        x = x.transpose(0, 1).reshape(N_q, B_q, C_q)\n        x = self.out_proj(x)\n        x = self.out_drop(x)\n        return x\n\n\nclass CustomResidualAttentionBlock(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        n_head: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        scale_cosine_attn: bool = False,\n        scale_heads: bool = False,\n        scale_attn: bool = False,\n        scale_fc: bool = False,\n        cross_attn: bool = False,\n        xattn: bool = False,\n    ):\n        super().__init__()\n\n        self.ln_1 = norm_layer(d_model)\n        self.ln_1_k = norm_layer(d_model) if cross_attn else self.ln_1\n        self.ln_1_v = norm_layer(d_model) if cross_attn else self.ln_1\n        self.attn = CustomAttention(d_model, n_head, qkv_bias=True, attn_drop=0.0, proj_drop=0.0, scaled_cosine=scale_cosine_attn, scale_heads=scale_heads, xattn=xattn)\n\n        self.ln_attn = norm_layer(d_model) if scale_attn else nn.Identity()\n        self.ls_1 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n\n        self.ln_2 = norm_layer(d_model)\n        mlp_width = int(d_model * mlp_ratio)\n        self.mlp = nn.Sequential(OrderedDict([(\"c_fc\", nn.Linear(d_model, mlp_width)), (\"ln\", norm_layer(mlp_width) if scale_fc else nn.Identity()), (\"gelu\", act_layer()), (\"c_proj\", nn.Linear(mlp_width, d_model))]))\n\n        self.ls_2 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n\n    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        q = q + self.ls_1(self.ln_attn(self.attn(self.ln_1(q), self.ln_1_k(k), self.ln_1_v(v), attn_mask=attn_mask)))\n        q = q + self.ls_2(self.mlp(self.ln_2(q)))\n        return q\n\n\nclass CustomTransformer(nn.Module):\n    def __init__(\n        self,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        scale_cosine_attn: bool = True,\n        scale_heads: bool = False,\n        scale_attn: bool = False,\n        scale_fc: bool = False,\n        cross_attn: bool = False,\n        xattn: bool = False,\n    ):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.grad_checkpointing = False\n        self.xattn = xattn\n\n        self.resblocks = nn.ModuleList(\n            [\n                CustomResidualAttentionBlock(\n                    width,\n                    heads,\n                    mlp_ratio,\n                    ls_init_value=ls_init_value,\n                    act_layer=act_layer,\n                    norm_layer=norm_layer,\n                    scale_cosine_attn=scale_cosine_attn,\n                    scale_heads=scale_heads,\n                    scale_attn=scale_attn,\n                    scale_fc=scale_fc,\n                    cross_attn=cross_attn,\n                    xattn=xattn,\n                )\n                for _ in range(layers)\n            ]\n        )\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.resblocks[0].mlp.c_fc.weight.dtype\n\n    def forward(self, q: torch.Tensor, k: torch.Tensor = None, v: torch.Tensor = None, attn_mask: Optional[torch.Tensor] = None):\n        if k is None and v is None:\n            k = v = q\n        for r in self.resblocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                q = checkpoint(r, q, k, v, attn_mask)\n            else:\n                q = r(q, k, v, attn_mask=attn_mask)\n        return q\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        n_head: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n    ):\n        super().__init__()\n\n        self.ln_1 = norm_layer(d_model)\n        if xattn:\n            self.attn = Attention(d_model, n_head, xattn=True)\n        else:\n            self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ls_1 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n\n        self.ln_2 = norm_layer(d_model)\n        mlp_width = int(d_model * mlp_ratio)\n        self.mlp = nn.Sequential(OrderedDict([(\"c_fc\", nn.Linear(d_model, mlp_width)), (\"gelu\", act_layer()), (\"c_proj\", nn.Linear(mlp_width, d_model))]))\n\n        self.ls_2 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n        self.xattn = xattn\n\n    def attention(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        attn_mask = attn_mask.to(x.dtype) if attn_mask is not None else None\n        if self.xattn:\n            return self.attn(x, attn_mask=attn_mask)\n        return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\n\n    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        x = x + self.ls_1(self.attention(self.ln_1(x), attn_mask=attn_mask))\n        x = x + self.ls_2(self.mlp(self.ln_2(x)))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n    ):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.grad_checkpointing = False\n\n        self.resblocks = nn.ModuleList([ResidualAttentionBlock(width, heads, mlp_ratio, ls_init_value=ls_init_value, act_layer=act_layer, norm_layer=norm_layer, xattn=xattn) for _ in range(layers)])\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.resblocks[0].mlp.c_fc.weight.dtype\n\n    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        for r in self.resblocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(r, x, attn_mask)\n            else:\n                x = r(x, attn_mask=attn_mask)\n        return x\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(\n        self,\n        image_size: int,\n        patch_size: int,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float,\n        ls_init_value: float = None,\n        patch_dropout: float = 0.0,\n        global_average_pool: bool = False,\n        output_dim: int = 512,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n    ):\n        super().__init__()\n        self.image_size = to_2tuple(image_size)\n        self.patch_size = to_2tuple(patch_size)\n        self.grid_size = (self.image_size[0] // self.patch_size[0], self.image_size[1] // self.patch_size[1])\n        self.output_dim = output_dim\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n\n        scale = width**-0.5\n        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n        self.positional_embedding = nn.Parameter(scale * torch.randn(self.grid_size[0] * self.grid_size[1] + 1, width))\n\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0.0 else nn.Identity()\n        self.ln_pre = norm_layer(width)\n\n        self.transformer = Transformer(width, layers, heads, mlp_ratio, ls_init_value=ls_init_value, act_layer=act_layer, norm_layer=norm_layer, xattn=xattn)\n\n        self.global_average_pool = global_average_pool\n        self.ln_post = norm_layer(width)\n        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        for param in self.parameters():\n            param.requires_grad = False\n\n        if unlocked_groups != 0:\n            groups = [\n                [\n                    self.conv1,\n                    self.class_embedding,\n                    self.positional_embedding,\n                    self.ln_pre,\n                ],\n                *self.transformer.resblocks[:-1],\n                [\n                    self.transformer.resblocks[-1],\n                    self.ln_post,\n                ],\n                self.proj,\n            ]\n\n            def _unlock(x):\n                if isinstance(x, Sequence):\n                    for g in x:\n                        _unlock(g)\n                else:\n                    if isinstance(x, torch.nn.Parameter):\n                        x.requires_grad = True\n                    else:\n                        for p in x.parameters():\n                            p.requires_grad = True\n\n            _unlock(groups[-unlocked_groups:])\n\n    def get_num_layers(self):\n        return self.transformer.layers\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"positional_embedding\", \"class_embedding\"}\n\n    def forward(self, x: torch.Tensor, return_all_features: bool = False):\n        x = self.conv1(x)  # shape = [*, width, grid, grid]\n        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n        x = x + self.positional_embedding.to(x.dtype)\n\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        x = self.patch_dropout(x)\n        x = self.ln_pre(x)\n\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n\n        if not return_all_features:\n            if self.global_average_pool:\n                x = x.mean(dim=1)  # x = x[:,1:,:].mean(dim=1)\n            else:\n                x = x[:, 0]\n\n            x = self.ln_post(x)\n\n            if self.proj is not None:\n                x = x @ self.proj\n\n        return x\n\n\nclass TextTransformer(nn.Module):\n    def __init__(\n        self,\n        context_length: int = 77,\n        vocab_size: int = 49408,\n        width: int = 512,\n        heads: int = 8,\n        layers: int = 12,\n        ls_init_value: float = None,\n        output_dim: int = 512,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n        attn_mask: bool = True,\n    ):\n        super().__init__()\n        self.context_length = context_length\n        self.vocab_size = vocab_size\n        self.width = width\n        self.output_dim = output_dim\n\n        self.token_embedding = nn.Embedding(vocab_size, width)\n        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, width))\n        self.transformer = Transformer(width=width, layers=layers, heads=heads, ls_init_value=ls_init_value, act_layer=act_layer, norm_layer=norm_layer, xattn=xattn)\n\n        self.xattn = xattn\n        self.ln_final = norm_layer(width)\n        self.text_projection = nn.Parameter(torch.empty(width, output_dim))\n\n        if attn_mask:\n            self.register_buffer(\"attn_mask\", self.build_attention_mask(), persistent=False)\n        else:\n            self.attn_mask = None\n\n        self.init_parameters()\n\n    def init_parameters(self):\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n        proj_std = (self.transformer.width**-0.5) * ((2 * self.transformer.layers) ** -0.5)\n        attn_std = self.transformer.width**-0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n        if self.text_projection is not None:\n            nn.init.normal_(self.text_projection, std=self.transformer.width**-0.5)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        # return {'positional_embedding', 'token_embedding'}\n        return {\"positional_embedding\"}\n\n    def get_num_layers(self):\n        return self.transformer.layers\n\n    def build_attention_mask(self):\n        # lazily create causal attention mask, with full attention between the vision tokens\n        # pytorch uses additive attention mask; fill with -inf\n        mask = torch.empty(self.context_length, self.context_length)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)  # zero out the lower diagonal\n        return mask\n\n    def forward(self, text, return_all_features: bool = False):\n        cast_dtype = self.transformer.get_cast_dtype()\n        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.to(cast_dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x, attn_mask=self.attn_mask)\n        # x = self.transformer(x) # no attention mask is applied\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x)\n\n        if not return_all_features:\n            # x.shape = [batch_size, n_ctx, transformer.width]\n            # take features from the eot embedding (eot_token is the highest number in each sequence)\n            x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n        return x\n"}
{"type": "source_file", "path": "instellavl/serve/cli.py", "content": "import torch\nimport argparse\nimport requests\n\nfrom PIL import Image\nfrom io import BytesIO\nfrom transformers import TextStreamer\nfrom huggingface_hub import login\nlogin(token = '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')\n\nfrom instellavl.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom instellavl.conversation import conv_templates, SeparatorStyle\nfrom instellavl.model.builder import load_pretrained_model\nfrom instellavl.utils import disable_torch_init\nfrom instellavl.mm_utils import tokenizer_image_token,  KeywordsStoppingCriteria\n\n\ndef load_image(image_file):\n    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n        response = requests.get(image_file)\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n    else:\n        image = Image.open(image_file).convert(\"RGB\")\n    return image\n\n\ndef main(args):\n    # Model\n    disable_torch_init()\n\n    tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, model_base=None, model_name=args.model_name, load_4bit=args.load_8bit, load_8bit=args.load_4bit)\n\n    conv_mode = 'instella'\n \n\n    if args.conv_mode is not None and conv_mode != args.conv_mode:\n        print(\"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(conv_mode, args.conv_mode, args.conv_mode))\n    else:\n        args.conv_mode = conv_mode\n\n    conv = conv_templates[args.conv_mode].copy()\n\n    roles = conv.roles\n\n    image = load_image(args.image_file)\n    image_tensor = image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\n\n    while True:\n        try:\n            inp = input(f\"{roles[0]}: \")\n        except EOFError:\n            inp = \"\"\n        if not inp:\n            print(\"exit...\")\n            break\n\n        print(f\"{roles[1]}: \", end=\"\")\n\n        if image is not None:\n            # first message\n            if model.config.mm_use_im_start_end:\n                inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + inp\n            else:\n                inp = DEFAULT_IMAGE_TOKEN + \"\\n\" + inp\n            conv.append_message(conv.roles[0], inp)\n            image = None\n        else:\n            # later messages\n            conv.append_message(conv.roles[0], inp)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n\n        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.INSTELLA else conv.sep2\n        if 'instellavl' in args.model_name.lower():\n            keywords = []\n        else:\n            keywords = [stop_str]\n        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n        terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"|||IP_ADDRESS|||\")]\n        with torch.inference_mode():\n            output_ids = model.generate(input_ids, images=image_tensor, do_sample=True, temperature=0.2, max_new_tokens=1024, streamer=streamer, use_cache=True, stopping_criteria=[stopping_criteria], eos_token_id=terminators)\n\n        outputs = tokenizer.decode(output_ids[0, input_ids.shape[1] :]).strip()\n        conv.messages[-1][-1] = outputs\n\n        if args.debug:\n            print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-path\", type=str, default=\"AIG-GenAI/Instella-VL-1B\")\n    parser.add_argument(\"--model-name\", type=str, default=\"instellavl-1b\")\n\n    parser.add_argument(\"--image-file\", type=str, required=True)\n    parser.add_argument(\"--num-gpus\", type=int, default=1)\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\n    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\n    parser.add_argument(\"--load-8bit\", action=\"store_true\")\n    parser.add_argument(\"--load-4bit\", action=\"store_true\")\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    args = parser.parse_args()\n    main(args)\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/eva_clip/eva_clip_encoder.py", "content": "import torch\nimport torch.nn as nn\n\nfrom .eva_clip_processors import EvaClipImageTrainProcessor\nfrom .eva_vit import EVAEncoderWrapper\nfrom .factory import list_models, add_model_config, get_model_config\n\nfrom llava.utils import rank0_print\n\n\nclass EvaClipVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n        self.vision_tower_name = vision_tower\n        self.vision_tower_pretrained = args.vision_tower_pretrained\n        self.config = get_model_config(vision_tower)\n\n        if not delay_load:\n            rank0_print(f\"Loading EVA ViT: {self.vision_tower_name}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n        else:\n            self.cfg_only = self.config\n\n    def load_model(self, device_map=None):\n        rank0_print(f\"Pretrained: {self.vision_tower_pretrained}\")\n        self.image_processor = EvaClipImageTrainProcessor(self.config[\"vision_cfg\"][\"image_size\"])\n        self.vision_tower = EVAEncoderWrapper(self.vision_tower_pretrained, self.config)\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower.requires_grad_(False)\n        self.is_loaded = True\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_feature = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0)).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_features = self.vision_tower(images.to(device=self.device, dtype=self.dtype)).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dtype(self):\n        return self.vision_tower.dtype\n\n    @property\n    def device(self):\n        return self.vision_tower.device\n\n    @property\n    def hidden_size(self):\n        return self.config[\"vision_cfg\"][\"width\"]\n\n    @property\n    def num_patches(self):\n        return (self.config[\"vision_cfg\"][\"image_size\"] // self.config[\"vision_cfg\"][\"patch_size\"]) ** 2\n\n    @property\n    def num_patches_per_side(self):\n        return self.config[\"vision_cfg\"][\"image_size\"] // self.config[\"vision_cfg\"][\"patch_size\"]\n\n    @property\n    def image_size(self):\n        return self.config[\"vision_cfg\"][\"image_size\"]\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/eva_clip/factory.py", "content": "import json\nimport logging\nimport os\nimport pathlib\nimport re\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union, Dict, Any\nimport torch\n\n_MODEL_CONFIG_PATHS = [Path(__file__).parent / f\"model_configs/\"]\n_MODEL_CONFIGS = {}  # directory (model_name: config) of model architecture configs\n\n\ndef _natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\n\n\ndef _rescan_model_configs():\n    global _MODEL_CONFIGS\n\n    config_ext = (\".json\",)\n    config_files = []\n    for config_path in _MODEL_CONFIG_PATHS:\n        if config_path.is_file() and config_path.suffix in config_ext:\n            config_files.append(config_path)\n        elif config_path.is_dir():\n            for ext in config_ext:\n                config_files.extend(config_path.glob(f\"*{ext}\"))\n\n    for cf in config_files:\n        with open(cf, \"r\", encoding=\"utf8\") as f:\n            model_cfg = json.load(f)\n            if all(a in model_cfg for a in (\"embed_dim\", \"vision_cfg\", \"text_cfg\")):\n                _MODEL_CONFIGS[cf.stem] = model_cfg\n\n    _MODEL_CONFIGS = dict(sorted(_MODEL_CONFIGS.items(), key=lambda x: _natural_key(x[0])))\n\n\n_rescan_model_configs()  # initial populate of model config registry\n\n\ndef list_models():\n    \"\"\"enumerate available model architectures based on config files\"\"\"\n    return list(_MODEL_CONFIGS.keys())\n\n\ndef add_model_config(path):\n    \"\"\"add model config path or file and update registry\"\"\"\n    if not isinstance(path, Path):\n        path = Path(path)\n    _MODEL_CONFIG_PATHS.append(path)\n    _rescan_model_configs()\n\n\ndef get_model_config(model_name):\n    if model_name in _MODEL_CONFIGS:\n        return deepcopy(_MODEL_CONFIGS[model_name])\n    else:\n        return None\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_encoder/siglip_encoder.py", "content": "\"\"\"\n# Adapted from https://huggingface.co/MILVLG/imp-v1-3b/blob/main/vision_encoder.py\n\"\"\"\n\nfrom typing import Optional, Tuple, Union, Dict\nfrom dataclasses import dataclass\nfrom functools import partial, reduce\nfrom PIL import Image\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nimport os\nfrom transformers.image_processing_utils import BatchFeature, get_size_dict\nfrom transformers.image_transforms import (\n    convert_to_rgb,\n    normalize,\n    rescale,\n    resize,\n    to_channel_dimension_format,\n)\nfrom transformers.image_utils import (\n    ChannelDimension,\n    PILImageResampling,\n    to_numpy_array,\n)\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers import PretrainedConfig\nfrom transformers.utils import ModelOutput\nfrom llava.utils import rank0_print\n\n\nclass SigLipImageProcessor:\n    def __init__(self, image_mean=(0.5, 0.5, 0.5), image_std=(0.5, 0.5, 0.5), size=(384, 384), crop_size: Dict[str, int] = None, resample=PILImageResampling.BICUBIC, rescale_factor=1 / 255, data_format=ChannelDimension.FIRST):\n        crop_size = crop_size if crop_size is not None else {\"height\": 384, \"width\": 384}\n        crop_size = get_size_dict(crop_size, default_to_square=True, param_name=\"crop_size\")\n\n        self.image_mean = image_mean\n        self.image_std = image_std\n        self.size = size\n        self.resample = resample\n        self.rescale_factor = rescale_factor\n        self.data_format = data_format\n        self.crop_size = crop_size\n\n    def preprocess(self, images, return_tensors):\n        if isinstance(images, Image.Image):\n            images = [images]\n        else:\n            # to adapt video data\n            images = [to_numpy_array(image) for image in images]\n            assert isinstance(images, list)\n\n        transforms = [\n            convert_to_rgb,\n            to_numpy_array,\n            partial(resize, size=self.size, resample=self.resample, data_format=self.data_format),\n            partial(rescale, scale=self.rescale_factor, data_format=self.data_format),\n            partial(normalize, mean=self.image_mean, std=self.image_std, data_format=self.data_format),\n            partial(to_channel_dimension_format, channel_dim=self.data_format, input_channel_dim=self.data_format),\n        ]\n\n        images = reduce(lambda x, f: [*map(f, x)], transforms, images)\n        data = {\"pixel_values\": images}\n\n        return BatchFeature(data=data, tensor_type=return_tensors)\n\n\nclass SigLipVisionConfig(PretrainedConfig):\n    model_type = \"siglip_vision_model\"\n\n    def __init__(\n        self,\n        hidden_size=1152,\n        image_mean=(0.5, 0.5, 0.5),\n        intermediate_size=4304,\n        num_hidden_layers=27,\n        num_attention_heads=16,\n        num_channels=3,\n        image_size=384,\n        patch_size=14,\n        hidden_act=\"gelu_pytorch_tanh\",\n        layer_norm_eps=1e-6,\n        attention_dropout=0.0,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.num_channels = num_channels\n        self.patch_size = patch_size\n        self.image_size = image_size\n        self.attention_dropout = attention_dropout\n        self.layer_norm_eps = layer_norm_eps\n        self.hidden_act = hidden_act\n        self.image_mean = image_mean\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n        cls._set_token_in_kwargs(kwargs)\n        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n\n        # get the vision config dict if we are loading from SigLipConfig\n        if config_dict.get(\"model_type\") == \"siglip\":\n            config_dict = config_dict[\"vision_config\"]\n\n        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n            print(f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \" f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\")\n\n        print(f'loading vision checkpoints from {pretrained_model_name_or_path}')\n        return cls.from_dict(config_dict, **kwargs)\n\n\n@dataclass\n# Copied from transformers.models.clip.modeling_clip.CLIPVisionModelOutput with CLIP->SigLip\nclass SigLipVisionModelOutput(ModelOutput):\n    \"\"\"\n    Base class for vision model's outputs that also contains image embeddings of the pooling of the last hidden states.\n\n    Args:\n        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n            The image embeddings obtained by applying the projection layer to the pooler_output.\n        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n            Sequence of hidden-states at the output of the last layer of the model.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n\n    image_embeds: Optional[torch.FloatTensor] = None\n    last_hidden_state: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None\n\n\nclass SigLipVisionEmbeddings(nn.Module):\n    def __init__(self, config: SigLipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.image_size = config.image_size\n        self.patch_size = config.patch_size\n\n        self.patch_embedding = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=self.embed_dim,\n            kernel_size=self.patch_size,\n            stride=self.patch_size,\n            padding=\"valid\",\n        )\n\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        self.num_positions = self.num_patches\n        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n        self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n\n    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n        patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]\n        embeddings = patch_embeds.flatten(2).transpose(1, 2)\n\n        embeddings = embeddings + self.position_embedding(self.position_ids)\n        return embeddings\n\n\nclass SigLipAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    # Copied from transformers.models.clip.modeling_clip.CLIPAttention.__init__\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n        if self.head_dim * self.num_heads != self.embed_dim:\n            raise ValueError(f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\" f\" {self.num_heads}).\")\n        self.scale = self.head_dim**-0.5\n        self.dropout = config.attention_dropout\n\n        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        batch_size, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        k_v_seq_len = key_states.shape[-2]\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n\n        if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n            raise ValueError(f\"Attention weights should be of size {(batch_size, self.num_heads, q_len, k_v_seq_len)}, but is\" f\" {attn_weights.size()}\")\n\n        if attention_mask is not None:\n            if attention_mask.size() != (batch_size, 1, q_len, k_v_seq_len):\n                raise ValueError(f\"Attention mask should be of size {(batch_size, 1, q_len, k_v_seq_len)}, but is {attention_mask.size()}\")\n            attn_weights = attn_weights + attention_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n            raise ValueError(f\"`attn_output` should be of size {(batch_size, self.num_heads, q_len, self.head_dim)}, but is\" f\" {attn_output.size()}\")\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim)\n\n        attn_output = self.out_proj(attn_output)\n\n        return attn_output, attn_weights\n\n\n# Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->SigLip\nclass SigLipMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.activation_fn = ACT2FN[config.hidden_act]\n        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.activation_fn(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n        return hidden_states\n\n\n# Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer with CLIP->SigLip\nclass SigLipEncoderLayer(nn.Module):\n    def __init__(self, config: SigLipVisionConfig):\n        super().__init__()\n        self.embed_dim = config.hidden_size\n        self.self_attn = SigLipAttention(config)\n        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n        self.mlp = SigLipMLP(config)\n        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n\n    # Ignore copy\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: torch.Tensor,\n        output_attentions: Optional[bool] = False,\n    ) -> Tuple[torch.FloatTensor]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`):\n                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n            attention_mask (`torch.FloatTensor`):\n                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*, defaults to `False`):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n        \"\"\"\n        residual = hidden_states\n\n        hidden_states = self.layer_norm1(hidden_states)\n        hidden_states, attn_weights = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n        )\n        hidden_states = residual + hidden_states\n\n        residual = hidden_states\n        hidden_states = self.layer_norm2(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs\n\n\nclass SigLipPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = SigLipVisionConfig\n    base_model_prefix = \"siglip\"\n    supports_gradient_checkpointing = True\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        pass\n\n\n# Copied from transformers.models.clip.modeling_clip.CLIPEncoder with CLIP->SigLip\nclass SigLipEncoder(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n    [`SigLipEncoderLayer`].\n\n    Args:\n        config: SigLipVisionConfig\n    \"\"\"\n\n    def __init__(self, config: SigLipVisionConfig):\n        super().__init__()\n        self.config = config\n        self.layers = nn.ModuleList([SigLipEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = False\n\n    # Ignore copy\n    def forward(\n        self,\n        inputs_embeds,\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutput]:\n        r\"\"\"\n        Args:\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n                than the model's internal embedding lookup matrix.\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        encoder_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n\n        hidden_states = inputs_embeds\n        for encoder_layer in self.layers:\n            if output_hidden_states:\n                encoder_states = encoder_states + (hidden_states,)\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    encoder_layer.__call__,\n                    hidden_states,\n                    attention_mask,\n                    output_attentions,\n                )\n            else:\n                layer_outputs = encoder_layer(\n                    hidden_states,\n                    attention_mask,\n                    output_attentions=output_attentions,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n        return BaseModelOutput(last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions)\n\n\nclass SigLipVisionTransformer(nn.Module):\n    def __init__(self, config: SigLipVisionConfig):\n        super().__init__()\n        self.config = config\n        embed_dim = config.hidden_size\n\n        self.embeddings = SigLipVisionEmbeddings(config)\n        self.encoder = SigLipEncoder(config)\n        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n        self.head = SigLipMultiheadAttentionPoolingHead(config)\n\n    def forward(\n        self,\n        pixel_values,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n        r\"\"\"\n        Returns:\n\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        hidden_states = self.embeddings(pixel_values)\n\n        encoder_outputs = self.encoder(\n            inputs_embeds=hidden_states,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        last_hidden_state = encoder_outputs[0]\n        last_hidden_state = self.post_layernorm(last_hidden_state)\n\n        pooled_output = self.head(last_hidden_state)\n\n        if not return_dict:\n            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPooling(\n            last_hidden_state=last_hidden_state,\n            pooler_output=pooled_output,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n\n\nclass SigLipMultiheadAttentionPoolingHead(nn.Module):\n    \"\"\"Multihead Attention Pooling.\"\"\"\n\n    def __init__(self, config: SigLipVisionConfig):\n        super().__init__()\n\n        self.probe = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n        self.attention = torch.nn.MultiheadAttention(config.hidden_size, config.num_attention_heads, batch_first=True)\n        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.mlp = SigLipMLP(config)\n\n    def forward(self, hidden_state):\n        batch_size = hidden_state.shape[0]\n        probe = self.probe.repeat(batch_size, 1, 1)\n\n        hidden_state = self.attention(probe, hidden_state, hidden_state)[0]\n\n        residual = hidden_state\n        hidden_state = self.layernorm(hidden_state)\n        hidden_state = residual + self.mlp(hidden_state)\n\n        return hidden_state[:, 0]\n\n\nclass SigLipVisionModel(SigLipPreTrainedModel):\n    config_class = SigLipVisionConfig\n    main_input_name = \"pixel_values\"\n    _no_split_modules = [\"SigLipEncoderLayer\"]\n\n    def __init__(self, config: SigLipVisionConfig):\n        super().__init__(config)\n\n        self.vision_model = SigLipVisionTransformer(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self) -> nn.Module:\n        return self.vision_model.embeddings.patch_embedding\n\n    def forward(\n        self,\n        pixel_values,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n        r\"\"\"\n        Returns:\n\n        Examples:\n\n        ```python\n        >>> from PIL import Image\n        >>> import requests\n        >>> from transformers import AutoProcessor, SigLipVisionModel\n\n        >>> model = SigLipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\")\n        >>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n\n        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n        >>> image = Image.open(requests.get(url, stream=True).raw)\n\n        >>> inputs = processor(images=image, return_tensors=\"pt\")\n\n        >>> outputs = model(**inputs)\n        >>> last_hidden_state = outputs.last_hidden_state\n        >>> pooled_output = outputs.pooler_output  # pooled features\n        ```\"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        return self.vision_model(\n            pixel_values=pixel_values,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n\nclass SigLipVisionTower(nn.Module):\n    def __init__(self, vision_tower, vision_tower_cfg, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.config = SigLipVisionConfig()\n\n        self.vision_tower_name = vision_tower\n\n        self.image_processor = SigLipImageProcessor()\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(vision_tower_cfg, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(vision_tower_cfg, \"mm_tunable_parts\") and \"mm_vision_tower\" in vision_tower_cfg.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n        else:\n            self.cfg_only = self.config\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n\n        self.vision_tower = SigLipVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n\n        del self.vision_tower.vision_model.encoder.layers[-1:]\n        self.vision_tower.vision_model.head = nn.Identity()\n        self.vision_tower.requires_grad_(False)\n\n        self.is_loaded = True\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = image_forward_out.hidden_states[-1].to(image.dtype)\n                assert image_features.shape[-2] == 729\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n            image_features = image_forward_outs.hidden_states[-1].to(images.dtype)\n            assert image_features.shape[-2] == 729\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        for p in self.vision_tower.parameters():\n            return p.dtype\n\n    @property\n    def device(self):\n        for p in self.vision_tower.parameters():\n            return p.device\n\n    @property\n    def hidden_size(self):\n        return self.config.hidden_size\n\n    @property\n    def num_patches(self):\n        return (self.config.image_size // self.config.patch_size) ** 2\n\n    @property\n    def num_patches_per_side(self):\n        return self.config.image_size // self.config.patch_size\n        # return self.model_config[\"vision_cfg\"][\"image_size\"] // self.model_config[\"vision_cfg\"][\"patch_size\"]\n\n    @property\n    def image_size(self):\n        return self.config.image_size\n"}
{"type": "source_file", "path": "instellavl/model/multimodal_resampler/qformer.py", "content": "\"\"\"\n * Copyright (c) 2023, salesforce.com, inc.\n * All rights reserved.\n * SPDX-License-Identifier: BSD-3-Clause\n * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n * By Junnan Li\n * Based on huggingface code base\n * https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert\n\"\"\"\n\nimport math\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Dict, Any\n\nimport torch\nfrom torch import Tensor, device, nn\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nimport torch.nn.functional as F\n\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions,\n    BaseModelOutputWithPoolingAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,\n)\nfrom transformers.modeling_utils import (\n    PreTrainedModel,\n    apply_chunking_to_forward,\n    find_pruneable_heads_and_indices,\n    prune_linear_layer,\n)\nfrom transformers.utils import logging\nfrom transformers.models.bert.configuration_bert import BertConfig\n\nlogger = logging.get_logger(__name__)\n\n\ndef disabled_train(self, mode=True):\n    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n    does not change anymore.\"\"\"\n    return self\n\n\nclass BertEmbeddings(nn.Module):\n    \"\"\"Construct the embeddings from word and position embeddings.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n\n        self.config = config\n\n    def forward(\n        self,\n        input_ids=None,\n        position_ids=None,\n        query_embeds=None,\n        past_key_values_length=0,\n    ):\n        if input_ids is not None:\n            seq_length = input_ids.size()[1]\n        else:\n            seq_length = 0\n\n        if position_ids is None:\n            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length].clone()\n\n        if input_ids is not None:\n            embeddings = self.word_embeddings(input_ids)\n            if self.position_embedding_type == \"absolute\":\n                position_embeddings = self.position_embeddings(position_ids)\n                embeddings = embeddings + position_embeddings\n\n            if query_embeds is not None:\n                embeddings = torch.cat((query_embeds, embeddings), dim=1)\n        else:\n            embeddings = query_embeds\n\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config, is_cross_attention):\n        super().__init__()\n        self.config = config\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n            raise ValueError(\"The hidden size (%d) is not a multiple of the number of attention \" \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        if is_cross_attention:\n            self.key = nn.Linear(config.encoder_width, self.all_head_size)\n            self.value = nn.Linear(config.encoder_width, self.all_head_size)\n        else:\n            self.key = nn.Linear(config.hidden_size, self.all_head_size)\n            self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            self.max_position_embeddings = config.max_position_embeddings\n            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n        self.save_attention = False\n\n    def save_attn_gradients(self, attn_gradients):\n        self.attn_gradients = attn_gradients\n\n    def get_attn_gradients(self):\n        return self.attn_gradients\n\n    def save_attention_map(self, attention_map):\n        self.attention_map = attention_map\n\n    def get_attention_map(self):\n        return self.attention_map\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (\n            self.num_attention_heads,\n            self.attention_head_size,\n        )\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n\n        mixed_query_layer = self.query(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n        past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n\n        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n            seq_length = hidden_states.size()[1]\n            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n            distance = position_ids_l - position_ids_r\n            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n\n            if self.position_embedding_type == \"relative_key\":\n                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores\n            elif self.position_embedding_type == \"relative_key_query\":\n                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        if is_cross_attention and self.save_attention:\n            self.save_attention_map(attention_probs)\n            attention_probs.register_hook(self.save_attn_gradients)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs_dropped = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs_dropped = attention_probs_dropped * head_mask\n\n        context_layer = torch.matmul(attention_probs_dropped, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n\n        outputs = outputs + (past_key_value,)\n        return outputs\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config, is_cross_attention=False):\n        super().__init__()\n        self.self = BertSelfAttention(config, is_cross_attention)\n        self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(\n            heads,\n            self.self.num_attention_heads,\n            self.self.attention_head_size,\n            self.pruned_heads,\n        )\n\n        # Prune linear layers\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            past_key_value,\n            output_attentions,\n        )\n        attention_output = self.output(self_outputs[0], hidden_states)\n\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n        return outputs\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config, layer_num):\n        super().__init__()\n        self.config = config\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n        self.attention = BertAttention(config)\n        self.layer_num = layer_num\n        if self.config.add_cross_attention and layer_num % self.config.cross_attention_freq == 0:\n            self.crossattention = BertAttention(config, is_cross_attention=self.config.add_cross_attention)\n            self.has_cross_attention = True\n        else:\n            self.has_cross_attention = False\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n        self.intermediate_query = BertIntermediate(config)\n        self.output_query = BertOutput(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n        query_length=0,\n    ):\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n            past_key_value=self_attn_past_key_value,\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[1:-1]\n\n        present_key_value = self_attention_outputs[-1]\n\n        if query_length > 0:\n            query_attention_output = attention_output[:, :query_length, :]\n\n            if self.has_cross_attention:\n                assert encoder_hidden_states is not None, \"encoder_hidden_states must be given for cross-attention layers\"\n                cross_attention_outputs = self.crossattention(\n                    query_attention_output,\n                    attention_mask,\n                    head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    output_attentions=output_attentions,\n                )\n                query_attention_output = cross_attention_outputs[0]\n                outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n\n            layer_output = apply_chunking_to_forward(\n                self.feed_forward_chunk_query,\n                self.chunk_size_feed_forward,\n                self.seq_len_dim,\n                query_attention_output,\n            )\n            if attention_output.shape[1] > query_length:\n                layer_output_text = apply_chunking_to_forward(\n                    self.feed_forward_chunk,\n                    self.chunk_size_feed_forward,\n                    self.seq_len_dim,\n                    attention_output[:, query_length:, :],\n                )\n                layer_output = torch.cat([layer_output, layer_output_text], dim=1)\n        else:\n            layer_output = apply_chunking_to_forward(\n                self.feed_forward_chunk,\n                self.chunk_size_feed_forward,\n                self.seq_len_dim,\n                attention_output,\n            )\n        outputs = (layer_output,) + outputs\n\n        outputs = outputs + (present_key_value,)\n\n        return outputs\n\n    def feed_forward_chunk(self, attention_output):\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output\n\n    def feed_forward_chunk_query(self, attention_output):\n        intermediate_output = self.intermediate_query(attention_output)\n        layer_output = self.output_query(intermediate_output, attention_output)\n        return layer_output\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.layer = nn.ModuleList([BertLayer(config, i) for i in range(config.num_hidden_layers)])\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict=True,\n        query_length=0,\n    ):\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attentions = () if output_attentions else None\n        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n\n        next_decoder_cache = () if use_cache else None\n\n        for i in range(self.config.num_hidden_layers):\n            layer_module = self.layer[i]\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_head_mask = head_mask[i] if head_mask is not None else None\n            past_key_value = past_key_values[i] if past_key_values is not None else None\n\n            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n\n                if use_cache:\n                    logger.warn(\"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\")\n                    use_cache = False\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs, past_key_value, output_attentions, query_length)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    past_key_value,\n                    output_attentions,\n                    query_length,\n                )\n\n            hidden_states = layer_outputs[0]\n            if use_cache:\n                next_decoder_cache += (layer_outputs[-1],)\n            if output_attentions:\n                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    next_decoder_cache,\n                    all_hidden_states,\n                    all_self_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=next_decoder_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n            cross_attentions=all_cross_attentions,\n        )\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We \"pool\" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n        self.decoder.bias = self.bias\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states)\n        return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = BertConfig\n    base_model_prefix = \"bert\"\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n\nclass BertModel(BertPreTrainedModel):\n    \"\"\"\n    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n    input to the forward pass.\n    \"\"\"\n\n    def __init__(self, config, add_pooling_layer=False):\n        super().__init__(config)\n        self.config = config\n\n        self.embeddings = BertEmbeddings(config)\n\n        self.encoder = BertEncoder(config)\n\n        self.pooler = BertPooler(config) if add_pooling_layer else None\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def get_extended_attention_mask(\n        self,\n        attention_mask: Tensor,\n        input_shape: Tuple[int],\n        device: device,\n        is_decoder: bool,\n        has_query: bool = False,\n    ) -> Tensor:\n        \"\"\"\n        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\n        Arguments:\n            attention_mask (:obj:`torch.Tensor`):\n                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n            input_shape (:obj:`Tuple[int]`):\n                The shape of the input to the model.\n            device: (:obj:`torch.device`):\n                The device of the input to the model.\n\n        Returns:\n            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n        \"\"\"\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if is_decoder:\n                batch_size, seq_length = input_shape\n\n                seq_ids = torch.arange(seq_length, device=device)\n                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n\n                # add a prefix ones mask to the causal mask\n                # causal and attention masks must have same type with pytorch version < 1.3\n                causal_mask = causal_mask.to(attention_mask.dtype)\n\n                if causal_mask.shape[1] < attention_mask.shape[1]:\n                    prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n                    if has_query:  # UniLM style attention mask\n                        causal_mask = torch.cat(\n                            [\n                                torch.zeros(\n                                    (batch_size, prefix_seq_len, seq_length),\n                                    device=device,\n                                    dtype=causal_mask.dtype,\n                                ),\n                                causal_mask,\n                            ],\n                            axis=1,\n                        )\n                    causal_mask = torch.cat(\n                        [\n                            torch.ones(\n                                (batch_size, causal_mask.shape[1], prefix_seq_len),\n                                device=device,\n                                dtype=causal_mask.dtype,\n                            ),\n                            causal_mask,\n                        ],\n                        axis=-1,\n                    )\n                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(input_shape, attention_mask.shape))\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        return extended_attention_mask\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        position_ids=None,\n        head_mask=None,\n        query_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        is_decoder=False,\n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n        if input_ids is None:\n            assert query_embeds is not None, \"You have to specify query_embeds when input_ids is None\"\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] - self.config.query_length if past_key_values is not None else 0\n\n        query_length = query_embeds.shape[1] if query_embeds is not None else 0\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            query_embeds=query_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n\n        input_shape = embedding_output.size()[:-1]\n        batch_size, seq_length = input_shape\n        device = embedding_output.device\n\n        if attention_mask is None:\n            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if is_decoder:\n            extended_attention_mask = self.get_extended_attention_mask(\n                attention_mask,\n                input_ids.shape,\n                device,\n                is_decoder,\n                has_query=(query_embeds is not None),\n            )\n        else:\n            extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape, device, is_decoder)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if encoder_hidden_states is not None:\n            if type(encoder_hidden_states) == list:\n                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states[0].size()\n            else:\n                (\n                    encoder_batch_size,\n                    encoder_sequence_length,\n                    _,\n                ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n\n            if type(encoder_attention_mask) == list:\n                encoder_extended_attention_mask = [self.invert_attention_mask(mask) for mask in encoder_attention_mask]\n            elif encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n            else:\n                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            query_length=query_length,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n\n\nclass BertLMHeadModel(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        position_ids=None,\n        head_mask=None,\n        query_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        past_key_values=None,\n        use_cache=True,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        return_logits=False,\n        is_decoder=True,\n        reduction=\"mean\",\n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        Returns:\n        Example::\n            >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n            >>> import torch\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n            >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\n            >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> prediction_logits = outputs.logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if labels is not None:\n            use_cache = False\n        if past_key_values is not None:\n            query_embeds = None\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            query_embeds=query_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            is_decoder=is_decoder,\n        )\n\n        sequence_output = outputs[0]\n        if query_embeds is not None:\n            sequence_output = outputs[0][:, query_embeds.shape[1] :, :]\n\n        prediction_scores = self.cls(sequence_output)\n\n        if return_logits:\n            return prediction_scores[:, :-1, :].contiguous()\n\n        lm_loss = None\n        if labels is not None:\n            # we are doing next-token prediction; shift prediction scores and input ids by one\n            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n            labels = labels[:, 1:].contiguous()\n            loss_fct = CrossEntropyLoss(reduction=reduction, label_smoothing=0.1)\n            lm_loss = loss_fct(\n                shifted_prediction_scores.view(-1, self.config.vocab_size),\n                labels.view(-1),\n            )\n            if reduction == \"none\":\n                lm_loss = lm_loss.view(prediction_scores.size(0), -1).sum(1)\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((lm_loss,) + output) if lm_loss is not None else output\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=lm_loss,\n            logits=prediction_scores,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, query_embeds, past=None, attention_mask=None, **model_kwargs):\n        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n        if attention_mask is None:\n            attention_mask = input_ids.new_ones(input_ids.shape)\n        query_mask = input_ids.new_ones(query_embeds.shape[:-1])\n        attention_mask = torch.cat([query_mask, attention_mask], dim=-1)\n\n        # cut decoder_input_ids if past is used\n        if past is not None:\n            input_ids = input_ids[:, -1:]\n\n        return {\n            \"input_ids\": input_ids,\n            \"query_embeds\": query_embeds,\n            \"attention_mask\": attention_mask,\n            \"past_key_values\": past,\n            \"encoder_hidden_states\": model_kwargs.get(\"encoder_hidden_states\", None),\n            \"encoder_attention_mask\": model_kwargs.get(\"encoder_attention_mask\", None),\n            \"is_decoder\": True,\n        }\n\n    def _reorder_cache(self, past, beam_idx):\n        reordered_past = ()\n        for layer_past in past:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n        return reordered_past\n\n\nclass BertForMaskedLM(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        position_ids=None,\n        head_mask=None,\n        query_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        return_logits=False,\n        is_decoder=False,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            query_embeds=query_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            is_decoder=is_decoder,\n        )\n\n        if query_embeds is not None:\n            sequence_output = outputs[0][:, query_embeds.shape[1] :, :]\n        prediction_scores = self.cls(sequence_output)\n\n        if return_logits:\n            return prediction_scores\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return MaskedLMOutput(\n            loss=masked_lm_loss,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n\nclass Qformer(nn.Module):\n    def __init__(self, model_args, vision_tower):\n        super().__init__()\n\n        self.depth = model_args.mm_qformer_depth\n        self.num_latents = model_args.mm_qformer_latents\n        self.pretrained = model_args.mm_qformer_pretrained\n\n        self.Qformer, self.query_tokens, self.ln_vision = self.build_Qformer(vision_tower.hidden_size, self.depth, self.num_latents)\n\n        if self.pretrained is not None:\n            pretrained_dict = torch.load(self.pretrained, map_location=\"cpu\")[\"model\"]\n            pretrained_dict = {k: v for k, v in pretrained_dict.items() if not k.startswith(\"t5_proj\")}\n            self.load_state_dict(pretrained_dict)\n\n    def build_Qformer(self, vision_width, cross_attention_freq, num_query_token):\n        encoder_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n        encoder_config.encoder_width = vision_width\n        # insert cross-attention layer every other block\n        encoder_config.add_cross_attention = True\n        encoder_config.cross_attention_freq = cross_attention_freq\n        encoder_config.query_length = num_query_token\n        Qformer = BertLMHeadModel(config=encoder_config)\n        query_tokens = nn.Parameter(torch.zeros(1, num_query_token, encoder_config.hidden_size))\n        query_tokens.data.normal_(mean=0.0, std=encoder_config.initializer_range)\n        Qformer.cls = None\n        Qformer.bert.embeddings.word_embeddings = None\n        Qformer.bert.embeddings.position_embeddings = None\n        for layer in Qformer.bert.encoder.layer:\n            layer.output = None\n            layer.intermediate = None\n        return Qformer, query_tokens, nn.LayerNorm(vision_width)\n\n    def forward(self, image_features, *args, **kwargs):\n        x = self.ln_vision(image_features)\n        image_atts = torch.ones(x.size()[:-1], dtype=torch.long).to(x.device)\n\n        query_tokens = self.query_tokens.expand(x.shape[0], -1, -1)\n        query_output = self.Qformer.bert(\n            query_embeds=query_tokens,\n            encoder_hidden_states=x,\n            encoder_attention_mask=image_atts,\n            return_dict=True,\n        )\n\n        return query_output.last_hidden_state\n\n    @property\n    def hidden_size(self):\n        return 768\n\n    @property\n    def config(self):\n        return {\n            \"mm_resampler_type\": \"qformer\",\n            \"mm_qformer_depth\": self.depth,\n            \"mm_qformer_latents\": self.num_latents,\n            \"mm_qformer_pretrained\": self.pretrained,\n        }\n"}
