{"repo_info": {"repo_name": "ID-Sculpt", "repo_owner": "jinkun-hao", "repo_url": "https://github.com/jinkun-hao/ID-Sculpt"}}
{"type": "test_file", "path": "threestudio/scripts/test_dreambooth_lora.py", "content": "import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n\n\n# model_base = \"stabilityai/stable-diffusion-2-1-base\"\n\n# pipe = DiffusionPipeline.from_pretrained(model_base, torch_dtype=torch.float16, cache_dir=CACHE_DIR, local_files_only=True)\n# pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, cache_dir=CACHE_DIR, local_files_only=True)\n# lora_model_path = \"load/checkpoints/sd_21_base_bear_dreambooth_lora\"\n# pipe.unet.load_attn_procs(lora_model_path)\n\n# pipe.to(\"cuda\")\n\n\n# image = pipe(\"A picture of a sks bear in the sky\", num_inference_steps=50, guidance_scale=7.5).images[0]\n# image.save(\"bear_dreambooth_lora.png\")\n\n\npipe = DiffusionPipeline.from_pretrained(\"DeepFloyd/IF-I-XL-v1.0\", local_files_only=True, safety_checker=None)\npipe.load_lora_weights(\"if_dreambooth_mushroom\")\npipe.scheduler = pipe.scheduler.__class__.from_config(pipe.scheduler.config, variance_type=\"fixed_small\")\npipe.to(\"cuda:7\")\n\nimage = pipe(\"A photo of a sks mushroom, front view\", num_inference_steps=50, guidance_scale=7.5).images[0]\nimage.save(\"mushroom_dreambooth_lora.png\")"}
{"type": "test_file", "path": "IP_Adapter/ip_adapter/test_resampler.py", "content": "import torch\nfrom resampler import Resampler\nfrom transformers import CLIPVisionModel\n\nBATCH_SIZE = 2\nOUTPUT_DIM = 1280\nNUM_QUERIES = 8\nNUM_LATENTS_MEAN_POOLED = 4  # 0 for no mean pooling (previous behavior)\nAPPLY_POS_EMB = True  # False for no positional embeddings (previous behavior)\nIMAGE_ENCODER_NAME_OR_PATH = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n\n\ndef main():\n    image_encoder = CLIPVisionModel.from_pretrained(IMAGE_ENCODER_NAME_OR_PATH)\n    embedding_dim = image_encoder.config.hidden_size\n    print(f\"image_encoder hidden size: \", embedding_dim)\n\n    image_proj_model = Resampler(\n        dim=1024,\n        depth=2,\n        dim_head=64,\n        heads=16,\n        num_queries=NUM_QUERIES,\n        embedding_dim=embedding_dim,\n        output_dim=OUTPUT_DIM,\n        ff_mult=2,\n        max_seq_len=257,\n        apply_pos_emb=APPLY_POS_EMB,\n        num_latents_mean_pooled=NUM_LATENTS_MEAN_POOLED,\n    )\n\n    dummy_images = torch.randn(BATCH_SIZE, 3, 224, 224)\n    with torch.no_grad():\n        image_embeds = image_encoder(dummy_images, output_hidden_states=True).hidden_states[-2]\n    print(\"image_embds shape: \", image_embeds.shape)\n\n    with torch.no_grad():\n        ip_tokens = image_proj_model(image_embeds)\n    print(\"ip_tokens shape:\", ip_tokens.shape)\n    assert ip_tokens.shape == (BATCH_SIZE, NUM_QUERIES + NUM_LATENTS_MEAN_POOLED, OUTPUT_DIM)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "test_file", "path": "threestudio/scripts/test_dreambooth.py", "content": "from diffusers import StableDiffusionPipeline, DDIMScheduler\nimport torch\n\n# model_id = \"load/checkpoints/sd_21_base_mushroom_vd_prompt\"\n# model_id = \"load/checkpoints/sd_base_mushroom\"\nmodel_id = \".cache/checkpoints/sd_21_base_rabbit\"\n# scheduler = DDIMScheduler()\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\nguidance_scale = 7.5\n\nprompt = \"a sks rabbit, front view\"\nimage = pipe(prompt, num_inference_steps=50, guidance_scale=guidance_scale).images[0]\n\nimage.save(\"debug.png\")\n\n\n# import os\n# import cv2\n# import glob\n# import torch\n# import argparse\n# import numpy as np\n# from tqdm import tqdm\n# import pytorch_lightning as pl\n# from torchvision.utils import save_image\n\n# import threestudio\n# from threestudio.utils.config import load_config\n\n\n# if __name__ == \"__main__\":\n#     parser = argparse.ArgumentParser()\n#     parser.add_argument(\"--config\", required=True, help=\"path to config file\")\n#     parser.add_argument(\"--view_dependent_noise\", action=\"store_true\", help=\"use view depdendent noise strength\")\n\n#     args, extras = parser.parse_known_args()\n\n#     cfg = load_config(args.config, cli_args=extras, n_gpus=1)\n#     guidance = threestudio.find(cfg.system.guidance_type)(cfg.system.guidance)\n#     prompt_processor = threestudio.find(cfg.system.prompt_processor_type)(cfg.system.prompt_processor)\n#     prompt_utils = prompt_processor()\n\n#     guidance.update_step(epoch=0, global_step=0)\n#     elevation, azimuth = torch.zeros(1).cuda(), torch.zeros(1).cuda()\n#     camera_distances = torch.tensor([3.0]).cuda()\n#     c2w = torch.zeros(4,4).cuda()\n#     a = guidance.sample(prompt_utils, elevation, azimuth, camera_distances) # sample_lora\n#     from torchvision.utils import save_image\n#     save_image(a.permute(0,3,1,2), \"debug.png\", normalize=True, value_range=(0,1))\n\n\n\n# python threestudio/scripts/test_dreambooth.py --config configs/experimental/stablediffusion.yaml system.prompt_processor.prompt=\"a sks mushroom growing on a log\" \\\n#     system.guidance.pretrained_model_name_or_path_lora=\"load/checkpoints/sd_21_base_mushroom_camera_condition\""}
{"type": "source_file", "path": "IP_Adapter/ip_adapter/__init__.py", "content": "from .ip_adapter import IPAdapter, IPAdapterPlus, IPAdapterPlusXL, IPAdapterXL, IPAdapterFull\nfrom .ip_adapter_faceid import IPAdapterFaceID\n\n__all__ = [\n    \"IPAdapter\",\n    \"IPAdapterPlus\",\n    \"IPAdapterPlusXL\",\n    \"IPAdapterXL\",\n    \"IPAdapterFull\",\n    \"IPAdapterFaceID\",\n]\n"}
{"type": "source_file", "path": "IP_Adapter/ip_adapter/attention_processor.py", "content": "# modified from https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass AttnProcessor(nn.Module):\n    r\"\"\"\n    Default processor for performing attention-related computations.\n    \"\"\"\n\n    def __init__(\n        self,\n        hidden_size=None,\n        cross_attention_dim=None,\n    ):\n        super().__init__()\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.norm_cross:\n            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\nclass IPAttnProcessor(nn.Module):\n    r\"\"\"\n    Attention processor for IP-Adapater.\n    Args:\n        hidden_size (`int`):\n            The hidden size of the attention layer.\n        cross_attention_dim (`int`):\n            The number of channels in the `encoder_hidden_states`.\n        scale (`float`, defaults to 1.0):\n            the weight scale of image prompt.\n        num_tokens (`int`, defaults to 4 when do ip_adapter_plus it should be 16):\n            The context length of the image features.\n    \"\"\"\n\n    def __init__(self, hidden_size, cross_attention_dim=None, scale=1.0, num_tokens=4):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.cross_attention_dim = cross_attention_dim\n        self.scale = scale\n        self.num_tokens = num_tokens\n\n        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        else:\n            # get encoder_hidden_states, ip_hidden_states\n            end_pos = encoder_hidden_states.shape[1] - self.num_tokens\n            encoder_hidden_states, ip_hidden_states = (\n                encoder_hidden_states[:, :end_pos, :],\n                encoder_hidden_states[:, end_pos:, :],\n            )\n            if attn.norm_cross:\n                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # for ip-adapter\n        ip_key = self.to_k_ip(ip_hidden_states)\n        ip_value = self.to_v_ip(ip_hidden_states)\n\n        ip_key = attn.head_to_batch_dim(ip_key)\n        ip_value = attn.head_to_batch_dim(ip_value)\n\n        ip_attention_probs = attn.get_attention_scores(query, ip_key, None)\n        self.attn_map = ip_attention_probs\n        ip_hidden_states = torch.bmm(ip_attention_probs, ip_value)\n        ip_hidden_states = attn.batch_to_head_dim(ip_hidden_states)\n\n        hidden_states = hidden_states + self.scale * ip_hidden_states\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\nclass AttnProcessor2_0(torch.nn.Module):\n    r\"\"\"\n    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).\n    \"\"\"\n\n    def __init__(\n        self,\n        hidden_size=None,\n        cross_attention_dim=None,\n    ):\n        super().__init__()\n        if not hasattr(F, \"scaled_dot_product_attention\"):\n            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n\n        if attention_mask is not None:\n            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n            # scaled_dot_product_attention expects attention_mask shape to be\n            # (batch, heads, source_length, target_length)\n            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.norm_cross:\n            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        inner_dim = key.shape[-1]\n        head_dim = inner_dim // attn.heads\n\n        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n        # TODO: add support for attn.scale when we move to Torch 2.1\n        hidden_states = F.scaled_dot_product_attention(\n            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n        )\n\n        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n        hidden_states = hidden_states.to(query.dtype)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\nclass IPAttnProcessor2_0(torch.nn.Module):\n    r\"\"\"\n    Attention processor for IP-Adapater for PyTorch 2.0.\n    Args:\n        hidden_size (`int`):\n            The hidden size of the attention layer.\n        cross_attention_dim (`int`):\n            The number of channels in the `encoder_hidden_states`.\n        scale (`float`, defaults to 1.0):\n            the weight scale of image prompt.\n        num_tokens (`int`, defaults to 4 when do ip_adapter_plus it should be 16):\n            The context length of the image features.\n    \"\"\"\n\n    def __init__(self, hidden_size, cross_attention_dim=None, scale=1.0, num_tokens=4):\n        super().__init__()\n\n        if not hasattr(F, \"scaled_dot_product_attention\"):\n            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n\n        self.hidden_size = hidden_size\n        self.cross_attention_dim = cross_attention_dim\n        self.scale = scale\n        self.num_tokens = num_tokens\n\n        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n\n        if attention_mask is not None:\n            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n            # scaled_dot_product_attention expects attention_mask shape to be\n            # (batch, heads, source_length, target_length)\n            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        else:\n            # get encoder_hidden_states, ip_hidden_states\n            end_pos = encoder_hidden_states.shape[1] - self.num_tokens\n            encoder_hidden_states, ip_hidden_states = (\n                encoder_hidden_states[:, :end_pos, :],\n                encoder_hidden_states[:, end_pos:, :],\n            )\n            if attn.norm_cross:\n                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        inner_dim = key.shape[-1]\n        head_dim = inner_dim // attn.heads\n\n        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n        # TODO: add support for attn.scale when we move to Torch 2.1\n        hidden_states = F.scaled_dot_product_attention(\n            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n        )\n\n        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n        hidden_states = hidden_states.to(query.dtype)\n\n        # for ip-adapter\n        ip_key = self.to_k_ip(ip_hidden_states)\n        ip_value = self.to_v_ip(ip_hidden_states)\n\n        ip_key = ip_key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        ip_value = ip_value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n        # TODO: add support for attn.scale when we move to Torch 2.1\n        ip_hidden_states = F.scaled_dot_product_attention(\n            query, ip_key, ip_value, attn_mask=None, dropout_p=0.0, is_causal=False\n        )\n        with torch.no_grad():\n            self.attn_map = query @ ip_key.transpose(-2, -1).softmax(dim=-1)\n            #print(self.attn_map.shape)\n\n        ip_hidden_states = ip_hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n        ip_hidden_states = ip_hidden_states.to(query.dtype)\n\n        hidden_states = hidden_states + self.scale * ip_hidden_states\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\n## for controlnet\nclass CNAttnProcessor:\n    r\"\"\"\n    Default processor for performing attention-related computations.\n    \"\"\"\n\n    def __init__(self, num_tokens=4):\n        self.num_tokens = num_tokens\n\n    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None, temb=None):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        else:\n            end_pos = encoder_hidden_states.shape[1] - self.num_tokens\n            encoder_hidden_states = encoder_hidden_states[:, :end_pos]  # only use text\n            if attn.norm_cross:\n                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\nclass CNAttnProcessor2_0:\n    r\"\"\"\n    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).\n    \"\"\"\n\n    def __init__(self, num_tokens=4):\n        if not hasattr(F, \"scaled_dot_product_attention\"):\n            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n        self.num_tokens = num_tokens\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n\n        if attention_mask is not None:\n            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n            # scaled_dot_product_attention expects attention_mask shape to be\n            # (batch, heads, source_length, target_length)\n            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        else:\n            end_pos = encoder_hidden_states.shape[1] - self.num_tokens\n            encoder_hidden_states = encoder_hidden_states[:, :end_pos]  # only use text\n            if attn.norm_cross:\n                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states)\n\n        inner_dim = key.shape[-1]\n        head_dim = inner_dim // attn.heads\n\n        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n        # TODO: add support for attn.scale when we move to Torch 2.1\n        hidden_states = F.scaled_dot_product_attention(\n            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n        )\n\n        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n        hidden_states = hidden_states.to(query.dtype)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n"}
{"type": "source_file", "path": "IP_Adapter/ip_adapter/attention_processor_faceid.py", "content": "# modified from https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom diffusers.models.lora import LoRALinearLayer\n\n\nclass LoRAAttnProcessor(nn.Module):\n    r\"\"\"\n    Default processor for performing attention-related computations.\n    \"\"\"\n\n    def __init__(\n        self,\n        hidden_size=None,\n        cross_attention_dim=None,\n        rank=4,\n        network_alpha=None,\n        lora_scale=1.0,\n    ):\n        super().__init__()\n\n        self.rank = rank\n        self.lora_scale = lora_scale\n        \n        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states) + self.lora_scale * self.to_q_lora(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.norm_cross:\n            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states) + self.lora_scale * self.to_k_lora(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states) + self.lora_scale * self.to_v_lora(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states) + self.lora_scale * self.to_out_lora(hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\nclass LoRAIPAttnProcessor(nn.Module):\n    r\"\"\"\n    Attention processor for IP-Adapater.\n    Args:\n        hidden_size (`int`):\n            The hidden size of the attention layer.\n        cross_attention_dim (`int`):\n            The number of channels in the `encoder_hidden_states`.\n        scale (`float`, defaults to 1.0):\n            the weight scale of image prompt.\n        num_tokens (`int`, defaults to 4 when do ip_adapter_plus it should be 16):\n            The context length of the image features.\n    \"\"\"\n\n    def __init__(self, hidden_size, cross_attention_dim=None, rank=4, network_alpha=None, lora_scale=1.0, scale=1.0, num_tokens=4):\n        super().__init__()\n\n        self.rank = rank\n        self.lora_scale = lora_scale\n        \n        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n\n        self.hidden_size = hidden_size\n        self.cross_attention_dim = cross_attention_dim\n        self.scale = scale\n        self.num_tokens = num_tokens\n\n        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states) + self.lora_scale * self.to_q_lora(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        else:\n            # get encoder_hidden_states, ip_hidden_states\n            end_pos = encoder_hidden_states.shape[1] - self.num_tokens\n            encoder_hidden_states, ip_hidden_states = (\n                encoder_hidden_states[:, :end_pos, :],\n                encoder_hidden_states[:, end_pos:, :],\n            )\n            if attn.norm_cross:\n                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states) + self.lora_scale * self.to_k_lora(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states) + self.lora_scale * self.to_v_lora(encoder_hidden_states)\n\n        query = attn.head_to_batch_dim(query)\n        key = attn.head_to_batch_dim(key)\n        value = attn.head_to_batch_dim(value)\n\n        attention_probs = attn.get_attention_scores(query, key, attention_mask)\n        hidden_states = torch.bmm(attention_probs, value)\n        hidden_states = attn.batch_to_head_dim(hidden_states)\n\n        # for ip-adapter\n        ip_key = self.to_k_ip(ip_hidden_states)\n        ip_value = self.to_v_ip(ip_hidden_states)\n\n        ip_key = attn.head_to_batch_dim(ip_key)\n        ip_value = attn.head_to_batch_dim(ip_value)\n\n        ip_attention_probs = attn.get_attention_scores(query, ip_key, None)\n        self.attn_map = ip_attention_probs\n        ip_hidden_states = torch.bmm(ip_attention_probs, ip_value)\n        ip_hidden_states = attn.batch_to_head_dim(ip_hidden_states)\n\n        hidden_states = hidden_states + self.scale * ip_hidden_states\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states) + self.lora_scale * self.to_out_lora(hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\nclass LoRAAttnProcessor2_0(nn.Module):\n    \n    r\"\"\"\n    Default processor for performing attention-related computations.\n    \"\"\"\n    \n    def __init__(\n        self,\n        hidden_size=None,\n        cross_attention_dim=None,\n        rank=4,\n        network_alpha=None,\n        lora_scale=1.0,\n    ):\n        super().__init__()\n        \n        self.rank = rank\n        self.lora_scale = lora_scale\n        \n        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n\n    def __call__(\n        self,\n        attn,\n        hidden_states,\n        encoder_hidden_states=None,\n        attention_mask=None,\n        temb=None,\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states) + self.lora_scale * self.to_q_lora(hidden_states)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        elif attn.norm_cross:\n            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        key = attn.to_k(encoder_hidden_states) + self.lora_scale * self.to_k_lora(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states) + self.lora_scale * self.to_v_lora(encoder_hidden_states)\n\n        inner_dim = key.shape[-1]\n        head_dim = inner_dim // attn.heads\n\n        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n        # TODO: add support for attn.scale when we move to Torch 2.1\n        hidden_states = F.scaled_dot_product_attention(\n            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n        )\n\n        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n        hidden_states = hidden_states.to(query.dtype)\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states) + self.lora_scale * self.to_out_lora(hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n\n\nclass LoRAIPAttnProcessor2_0(nn.Module):\n    r\"\"\"\n    Processor for implementing the LoRA attention mechanism.\n\n    Args:\n        hidden_size (`int`, *optional*):\n            The hidden size of the attention layer.\n        cross_attention_dim (`int`, *optional*):\n            The number of channels in the `encoder_hidden_states`.\n        rank (`int`, defaults to 4):\n            The dimension of the LoRA update matrices.\n        network_alpha (`int`, *optional*):\n            Equivalent to `alpha` but it's usage is specific to Kohya (A1111) style LoRAs.\n    \"\"\"\n\n    def __init__(self, hidden_size, cross_attention_dim=None, rank=4, network_alpha=None, lora_scale=1.0, scale=1.0, num_tokens=4):\n        super().__init__()\n        \n        self.rank = rank\n        self.lora_scale = lora_scale\n        self.num_tokens = num_tokens\n        \n        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)\n        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)\n        \n        \n        self.hidden_size = hidden_size\n        self.cross_attention_dim = cross_attention_dim\n        self.scale = scale\n\n        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)\n\n    def __call__(\n        self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None, scale=1.0, temb=None\n    ):\n        residual = hidden_states\n\n        if attn.spatial_norm is not None:\n            hidden_states = attn.spatial_norm(hidden_states, temb)\n\n        input_ndim = hidden_states.ndim\n\n        if input_ndim == 4:\n            batch_size, channel, height, width = hidden_states.shape\n            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n\n        batch_size, sequence_length, _ = (\n            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n        )\n        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n\n        if attn.group_norm is not None:\n            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n\n        query = attn.to_q(hidden_states) + self.lora_scale * self.to_q_lora(hidden_states)\n        #query = attn.head_to_batch_dim(query)\n\n        if encoder_hidden_states is None:\n            encoder_hidden_states = hidden_states\n        else:\n            # get encoder_hidden_states, ip_hidden_states\n            end_pos = encoder_hidden_states.shape[1] - self.num_tokens\n            encoder_hidden_states, ip_hidden_states = (\n                encoder_hidden_states[:, :end_pos, :],\n                encoder_hidden_states[:, end_pos:, :],\n            )\n            if attn.norm_cross:\n                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n\n        # for text\n        key = attn.to_k(encoder_hidden_states) + self.lora_scale * self.to_k_lora(encoder_hidden_states)\n        value = attn.to_v(encoder_hidden_states) + self.lora_scale * self.to_v_lora(encoder_hidden_states)\n\n        inner_dim = key.shape[-1]\n        head_dim = inner_dim // attn.heads\n\n        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n        # TODO: add support for attn.scale when we move to Torch 2.1\n        hidden_states = F.scaled_dot_product_attention(\n            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n        )\n\n        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n        hidden_states = hidden_states.to(query.dtype)\n        \n        # for ip\n        ip_key = self.to_k_ip(ip_hidden_states)\n        ip_value = self.to_v_ip(ip_hidden_states)\n        \n        ip_key = ip_key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n        ip_value = ip_value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n\n        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n        # TODO: add support for attn.scale when we move to Torch 2.1\n        ip_hidden_states = F.scaled_dot_product_attention(\n            query, ip_key, ip_value, attn_mask=None, dropout_p=0.0, is_causal=False\n        )\n        \n\n        ip_hidden_states = ip_hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n        ip_hidden_states = ip_hidden_states.to(query.dtype)\n        \n        hidden_states = hidden_states + self.scale * ip_hidden_states\n\n        # linear proj\n        hidden_states = attn.to_out[0](hidden_states) + self.lora_scale * self.to_out_lora(hidden_states)\n        # dropout\n        hidden_states = attn.to_out[1](hidden_states)\n\n        if input_ndim == 4:\n            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n\n        if attn.residual_connection:\n            hidden_states = hidden_states + residual\n\n        hidden_states = hidden_states / attn.rescale_output_factor\n\n        return hidden_states\n"}
{"type": "source_file", "path": "IP_Adapter/ip_adapter/custom_pipelines.py", "content": "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\nfrom diffusers.pipelines.stable_diffusion_xl import StableDiffusionXLPipelineOutput\nfrom diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import rescale_noise_cfg\n\nfrom .utils import is_torch2_available\n\nif is_torch2_available():\n    from .attention_processor import IPAttnProcessor2_0 as IPAttnProcessor\nelse:\n    from .attention_processor import IPAttnProcessor\n\n\nclass StableDiffusionXLCustomPipeline(StableDiffusionXLPipeline):\n    def set_scale(self, scale):\n        for attn_processor in self.unet.attn_processors.values():\n            if isinstance(attn_processor, IPAttnProcessor):\n                attn_processor.scale = scale\n\n    @torch.no_grad()\n    def __call__(  # noqa: C901\n        self,\n        prompt: Optional[Union[str, List[str]]] = None,\n        prompt_2: Optional[Union[str, List[str]]] = None,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 50,\n        denoising_end: Optional[float] = None,\n        guidance_scale: float = 5.0,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n        pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n        negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: int = 1,\n        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n        guidance_rescale: float = 0.0,\n        original_size: Optional[Tuple[int, int]] = None,\n        crops_coords_top_left: Tuple[int, int] = (0, 0),\n        target_size: Optional[Tuple[int, int]] = None,\n        negative_original_size: Optional[Tuple[int, int]] = None,\n        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n        negative_target_size: Optional[Tuple[int, int]] = None,\n        control_guidance_start: float = 0.0,\n        control_guidance_end: float = 1.0,\n    ):\n        r\"\"\"\n        Function invoked when calling the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n                instead.\n            prompt_2 (`str` or `List[str]`, *optional*):\n                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is\n                used in both text-encoders\n            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n                Anything below 512 pixels won't work well for\n                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n                and checkpoints that are not specifically fine-tuned on low resolutions.\n            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n                Anything below 512 pixels won't work well for\n                [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n                and checkpoints that are not specifically fine-tuned on low resolutions.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            denoising_end (`float`, *optional*):\n                When specified, determines the fraction (between 0.0 and 1.0) of the total denoising process to be\n                completed before it is intentionally prematurely terminated. As a result, the returned sample will\n                still retain a substantial amount of noise as determined by the discrete timesteps selected by the\n                scheduler. The denoising_end parameter should ideally be utilized when this pipeline forms a part of a\n                \"Mixture of Denoisers\" multi-pipeline setup, as elaborated in [**Refining the Image\n                Output**](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl#refining-the-image-output)\n            guidance_scale (`float`, *optional*, defaults to 5.0):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n                less than `1`).\n            negative_prompt_2 (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation to be sent to `tokenizer_2` and\n                `text_encoder_2`. If not defined, `negative_prompt` is used in both text-encoders\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta () in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n                [`schedulers.DDIMScheduler`], will be ignored for others.\n            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n                to make generation deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor will ge generated by sampling using the supplied random `generator`.\n            prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n                provided, text embeddings will be generated from `prompt` input argument.\n            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n                argument.\n            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.\n                If not provided, pooled text embeddings will be generated from `prompt` input argument.\n            negative_pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n                Pre-generated negative pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n                weighting. If not provided, pooled negative_prompt_embeds will be generated from `negative_prompt`\n                input argument.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] instead\n                of a plain tuple.\n            callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. The function will be\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\n                called at every step.\n            cross_attention_kwargs (`dict`, *optional*):\n                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n                `self.processor` in\n                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n            guidance_rescale (`float`, *optional*, defaults to 0.7):\n                Guidance rescale factor proposed by [Common Diffusion Noise Schedules and Sample Steps are\n                Flawed](https://arxiv.org/pdf/2305.08891.pdf) `guidance_scale` is defined as `` in equation 16. of\n                [Common Diffusion Noise Schedules and Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf).\n                Guidance rescale factor should fix overexposure when using zero terminal SNR.\n            original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                If `original_size` is not the same as `target_size` the image will appear to be down- or upsampled.\n                `original_size` defaults to `(width, height)` if not specified. Part of SDXL's micro-conditioning as\n                explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n            crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):\n                `crops_coords_top_left` can be used to generate an image that appears to be \"cropped\" from the position\n                `crops_coords_top_left` downwards. Favorable, well-centered images are usually achieved by setting\n                `crops_coords_top_left` to (0, 0). Part of SDXL's micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n            target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                For most cases, `target_size` should be set to the desired height and width of the generated image. If\n                not specified it will default to `(width, height)`. Part of SDXL's micro-conditioning as explained in\n                section 2.2 of [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952).\n            negative_original_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                To negatively condition the generation process based on a specific image resolution. Part of SDXL's\n                micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n            negative_crops_coords_top_left (`Tuple[int]`, *optional*, defaults to (0, 0)):\n                To negatively condition the generation process based on a specific crop coordinates. Part of SDXL's\n                micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n            negative_target_size (`Tuple[int]`, *optional*, defaults to (1024, 1024)):\n                To negatively condition the generation process based on a target image resolution. It should be as same\n                as the `target_size` for most cases. Part of SDXL's micro-conditioning as explained in section 2.2 of\n                [https://huggingface.co/papers/2307.01952](https://huggingface.co/papers/2307.01952). For more\n                information, refer to this issue thread: https://github.com/huggingface/diffusers/issues/4208.\n            control_guidance_start (`float`, *optional*, defaults to 0.0):\n                The percentage of total steps at which the ControlNet starts applying.\n            control_guidance_end (`float`, *optional*, defaults to 1.0):\n                The percentage of total steps at which the ControlNet stops applying.\n\n        Examples:\n\n        Returns:\n            [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] or `tuple`:\n            [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] if `return_dict` is True, otherwise a\n            `tuple`. When returning a tuple, the first element is a list with the generated images.\n        \"\"\"\n        # 0. Default height and width to unet\n        height = height or self.default_sample_size * self.vae_scale_factor\n        width = width or self.default_sample_size * self.vae_scale_factor\n\n        original_size = original_size or (height, width)\n        target_size = target_size or (height, width)\n\n        # 1. Check inputs. Raise error if not correct\n        self.check_inputs(\n            prompt,\n            prompt_2,\n            height,\n            width,\n            callback_steps,\n            negative_prompt,\n            negative_prompt_2,\n            prompt_embeds,\n            negative_prompt_embeds,\n            pooled_prompt_embeds,\n            negative_pooled_prompt_embeds,\n        )\n\n        # 2. Define call parameters\n        if prompt is not None and isinstance(prompt, str):\n            batch_size = 1\n        elif prompt is not None and isinstance(prompt, list):\n            batch_size = len(prompt)\n        else:\n            batch_size = prompt_embeds.shape[0]\n\n        device = self._execution_device\n\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n\n        # 3. Encode input prompt\n        text_encoder_lora_scale = (\n            cross_attention_kwargs.get(\"scale\", None) if cross_attention_kwargs is not None else None\n        )\n        (\n            prompt_embeds,\n            negative_prompt_embeds,\n            pooled_prompt_embeds,\n            negative_pooled_prompt_embeds,\n        ) = self.encode_prompt(\n            prompt=prompt,\n            prompt_2=prompt_2,\n            device=device,\n            num_images_per_prompt=num_images_per_prompt,\n            do_classifier_free_guidance=do_classifier_free_guidance,\n            negative_prompt=negative_prompt,\n            negative_prompt_2=negative_prompt_2,\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n            lora_scale=text_encoder_lora_scale,\n        )\n\n        # 4. Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n\n        timesteps = self.scheduler.timesteps\n\n        # 5. Prepare latent variables\n        num_channels_latents = self.unet.config.in_channels\n        latents = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            prompt_embeds.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        # 7. Prepare added time ids & embeddings\n        add_text_embeds = pooled_prompt_embeds\n        if self.text_encoder_2 is None:\n            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n        else:\n            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n\n        add_time_ids = self._get_add_time_ids(\n            original_size,\n            crops_coords_top_left,\n            target_size,\n            dtype=prompt_embeds.dtype,\n            text_encoder_projection_dim=text_encoder_projection_dim,\n        )\n        if negative_original_size is not None and negative_target_size is not None:\n            negative_add_time_ids = self._get_add_time_ids(\n                negative_original_size,\n                negative_crops_coords_top_left,\n                negative_target_size,\n                dtype=prompt_embeds.dtype,\n                text_encoder_projection_dim=text_encoder_projection_dim,\n            )\n        else:\n            negative_add_time_ids = add_time_ids\n\n        if do_classifier_free_guidance:\n            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n\n        prompt_embeds = prompt_embeds.to(device)\n        add_text_embeds = add_text_embeds.to(device)\n        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n\n        # 8. Denoising loop\n        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n\n        # 7.1 Apply denoising_end\n        if denoising_end is not None and isinstance(denoising_end, float) and denoising_end > 0 and denoising_end < 1:\n            discrete_timestep_cutoff = int(\n                round(\n                    self.scheduler.config.num_train_timesteps\n                    - (denoising_end * self.scheduler.config.num_train_timesteps)\n                )\n            )\n            num_inference_steps = len(list(filter(lambda ts: ts >= discrete_timestep_cutoff, timesteps)))\n            timesteps = timesteps[:num_inference_steps]\n\n        # get init conditioning scale\n        for attn_processor in self.unet.attn_processors.values():\n            if isinstance(attn_processor, IPAttnProcessor):\n                conditioning_scale = attn_processor.scale\n                break\n\n        with self.progress_bar(total=num_inference_steps) as progress_bar:\n            for i, t in enumerate(timesteps):\n                if (i / len(timesteps) < control_guidance_start) or ((i + 1) / len(timesteps) > control_guidance_end):\n                    self.set_scale(0.0)\n                else:\n                    self.set_scale(conditioning_scale)\n\n                # expand the latents if we are doing classifier free guidance\n                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n\n                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n                # predict the noise residual\n                added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n                noise_pred = self.unet(\n                    latent_model_input,\n                    t,\n                    encoder_hidden_states=prompt_embeds,\n                    cross_attention_kwargs=cross_attention_kwargs,\n                    added_cond_kwargs=added_cond_kwargs,\n                    return_dict=False,\n                )[0]\n\n                # perform guidance\n                if do_classifier_free_guidance:\n                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n                if do_classifier_free_guidance and guidance_rescale > 0.0:\n                    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=guidance_rescale)\n\n                # compute the previous noisy sample x_t -> x_t-1\n                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n\n                # call the callback, if provided\n                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n                    progress_bar.update()\n                    if callback is not None and i % callback_steps == 0:\n                        callback(i, t, latents)\n\n        if not output_type == \"latent\":\n            # make sure the VAE is in float32 mode, as it overflows in float16\n            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n\n            if needs_upcasting:\n                self.upcast_vae()\n                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n\n            image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n\n            # cast back to fp16 if needed\n            if needs_upcasting:\n                self.vae.to(dtype=torch.float16)\n        else:\n            image = latents\n\n        if output_type != \"latent\":\n            # apply watermark if available\n            if self.watermark is not None:\n                image = self.watermark.apply_watermark(image)\n\n            image = self.image_processor.postprocess(image, output_type=output_type)\n\n        # Offload all models\n        self.maybe_free_model_hooks()\n\n        if not return_dict:\n            return (image,)\n\n        return StableDiffusionXLPipelineOutput(images=image)\n"}
{"type": "source_file", "path": "load/make_prompt_library.py", "content": "import json\n\ndreamfusion_gallery_video_names = [\n    \"a_20-sided_die_made_out_of_glass.mp4\",\n    \"a_bald_eagle_carved_out_of_wood.mp4\",\n    \"a_banana_peeling_itself.mp4\",\n    \"a_beagle_in_a_detective's_outfit.mp4\",\n    \"a_beautiful_dress_made_out_of_fruit,_on_a_mannequin._Studio_lighting,_high_quality,_high_resolution.mp4\",\n    \"a_beautiful_dress_made_out_of_garbage_bags,_on_a_mannequin._Studio_lighting,_high_quality,_high_resolution.mp4\",\n    \"a_beautiful_rainbow_fish.mp4\",\n    \"a_bichon_frise_wearing_academic_regalia.mp4\",\n    \"a_blue_motorcycle.mp4\",\n    \"a_blue_poison-dart_frog_sitting_on_a_water_lily.mp4\",\n    \"a_brightly_colored_mushroom_growing_on_a_log.mp4\",\n    \"a_bumblebee_sitting_on_a_pink_flower.mp4\",\n    \"a_bunch_of_colorful_marbles_spilling_out_of_a_red_velvet_bag.mp4\",\n    \"a_capybara_wearing_a_top_hat,_low_poly.mp4\",\n    \"a_cat_with_a_mullet.mp4\",\n    \"a_ceramic_lion.mp4\",\n    \"a_ceramic_upside_down_yellow_octopus_holding_a_blue_green_ceramic_cup.mp4\",\n    \"a_chihuahua_wearing_a_tutu.mp4\",\n    \"a_chimpanzee_holding_a_peeled_banana.mp4\",\n    \"a_chimpanzee_looking_through_a_telescope.mp4\",\n    \"a_chimpanzee_stirring_a_bubbling_purple_potion_in_a_cauldron.mp4\",\n    \"a_chimpanzee_with_a_big_grin.mp4\",\n    \"a_completely_destroyed_car.mp4\",\n    \"a_confused_beagle_sitting_at_a_desk_working_on_homework.mp4\",\n    \"a_corgi_taking_a_selfie.mp4\",\n    \"a_crab,_low_poly.mp4\",\n    \"a_crocodile_playing_a_drum_set.mp4\",\n    \"a_cute_steampunk_elephant.mp4\",\n    \"a_dachsund_dressed_up_in_a_hotdog_costume.mp4\",\n    \"a_delicious_hamburger.mp4\",\n    \"a_dragon-cat_hybrid.mp4\",\n    \"a_DSLR_photo_of_a_baby_dragon_drinking_boba.mp4\",\n    \"a_DSLR_photo_of_a_baby_dragon_hatching_out_of_a_stone_egg.mp4\",\n    \"a_DSLR_photo_of_a_baby_grand_piano_viewed_from_far_away.mp4\",\n    \"a_DSLR_photo_of_a_bagel_filled_with_cream_cheese_and_lox.mp4\",\n    \"a_DSLR_photo_of_a_bald_eagle.mp4\",\n    \"a_DSLR_photo_of_a_barbecue_grill_cooking_sausages_and_burger_patties.mp4\",\n    \"a_DSLR_photo_of_a_basil_plant.mp4\",\n    \"a_DSLR_photo_of_a_bear_dancing_ballet.mp4\",\n    \"a_DSLR_photo_of_a_bear_dressed_as_a_lumberjack.mp4\",\n    \"a_DSLR_photo_of_a_bear_dressed_in_medieval_armor.mp4\",\n    \"a_DSLR_photo_of_a_beautiful_violin_sitting_flat_on_a_table.mp4\",\n    \"a_DSLR_photo_of_a_blue_jay_standing_on_a_large_basket_of_rainbow_macarons.mp4\",\n    \"a_DSLR_photo_of_a_bulldozer_clearing_away_a_pile_of_snow.mp4\",\n    \"a_DSLR_photo_of_a_bulldozer.mp4\",\n    \"a_DSLR_photo_of_a_cake_covered_in_colorful_frosting_with_a_slice_being_taken_out,_high_resolution.mp4\",\n    \"a_DSLR_photo_of_a_candelabra_with_many_candles_on_a_red_velvet_tablecloth.mp4\",\n    \"a_DSLR_photo_of_a_car_made_out_of_cheese.mp4\",\n    \"a_DSLR_photo_of_A_car_made_out_of_sushi.mp4\",\n    \"a_DSLR_photo_of_a_car_made_out_pizza.mp4\",\n    \"a_DSLR_photo_of_a_cat_lying_on_its_side_batting_at_a_ball_of_yarn.mp4\",\n    \"a_DSLR_photo_of_a_cat_magician_making_a_white_dove_appear.mp4\",\n    \"a_DSLR_photo_of_a_cat_wearing_a_bee_costume.mp4\",\n    \"a_DSLR_photo_of_a_cat_wearing_a_lion_costume.mp4\",\n    \"a_DSLR_photo_of_a_cauldron_full_of_gold_coins.mp4\",\n    \"a_DSLR_photo_of_a_chimpanzee_dressed_like_Henry_VIII_king_of_England.mp4\",\n    \"a_DSLR_photo_of_a_chimpanzee_dressed_like_Napoleon_Bonaparte.mp4\",\n    \"a_DSLR_photo_of_a_chow_chow_puppy.mp4\",\n    \"a_DSLR_photo_of_a_Christmas_tree_with_donuts_as_decorations.mp4\",\n    \"a_DSLR_photo_of_a_chrome-plated_duck_with_a_golden_beak_arguing_with_an_angry_turtle_in_a_forest.mp4\",\n    \"a_DSLR_photo_of_a_classic_Packard_car.mp4\",\n    \"a_DSLR_photo_of_a_cocker_spaniel_wearing_a_crown.mp4\",\n    \"a_DSLR_photo_of_a_corgi_lying_on_its_back_with_its_tongue_lolling_out.mp4\",\n    \"a_DSLR_photo_of_a_corgi_puppy.mp4\",\n    \"a_DSLR_photo_of_a_corgi_sneezing.mp4\",\n    \"a_DSLR_photo_of_a_corgi_standing_up_drinking_boba.mp4\",\n    \"a_DSLR_photo_of_a_corgi_taking_a_selfie.mp4\",\n    \"a_DSLR_photo_of_a_corgi_wearing_a_beret_and_holding_a_baguette,_standing_up_on_two_hind_legs.mp4\",\n    \"a_DSLR_photo_of_a_covered_wagon.mp4\",\n    \"a_DSLR_photo_of_a_cracked_egg_with_the_yolk_spilling_out_on_a_wooden_table.mp4\",\n    \"a_DSLR_photo_of_a_cup_full_of_pens_and_pencils.mp4\",\n    \"a_DSLR_photo_of_a_dalmation_wearing_a_fireman's_hat.mp4\",\n    \"a_DSLR_photo_of_a_delicious_chocolate_brownie_dessert_with_ice_cream_on_the_side.mp4\",\n    \"a_DSLR_photo_of_a_delicious_croissant.mp4\",\n    \"a_DSLR_photo_of_A_DMC_Delorean_car.mp4\",\n    \"a_DSLR_photo_of_a_dog_made_out_of_salad.mp4\",\n    \"a_DSLR_photo_of_a_drum_set_made_of_cheese.mp4\",\n    \"a_DSLR_photo_of_a_drying_rack_covered_in_clothes.mp4\",\n    \"a_DSLR_photo_of_aerial_view_of_a_ruined_castle.mp4\",\n    \"a_DSLR_photo_of_a_football_helmet.mp4\",\n    \"a_DSLR_photo_of_a_fox_holding_a_videogame_controller.mp4\",\n    \"a_DSLR_photo_of_a_fox_taking_a_photograph_using_a_DSLR.mp4\",\n    \"a_DSLR_photo_of_a_frazer_nash_super_sport_car.mp4\",\n    \"a_DSLR_photo_of_a_frog_wearing_a_sweater.mp4\",\n    \"a_DSLR_photo_of_a_ghost_eating_a_hamburger.mp4\",\n    \"a_DSLR_photo_of_a_giant_worm_emerging_from_the_sand_in_the_middle_of_the_desert.mp4\",\n    \"a_DSLR_photo_of_a_goose_made_out_of_gold.mp4\",\n    \"a_DSLR_photo_of_a_green_monster_truck.mp4\",\n    \"a_DSLR_photo_of_a_group_of_dogs_eating_pizza.mp4\",\n    \"a_DSLR_photo_of_a_group_of_dogs_playing_poker.mp4\",\n    \"a_DSLR_photo_of_a_gummy_bear_playing_the_saxophone.mp4\",\n    \"a_DSLR_photo_of_a_hippo_wearing_a_sweater.mp4\",\n    \"a_DSLR_photo_of_a_humanoid_robot_holding_a_human_brain.mp4\",\n    \"a_DSLR_photo_of_a_humanoid_robot_playing_solitaire.mp4\",\n    \"a_DSLR_photo_of_a_humanoid_robot_playing_the_cello.mp4\",\n    \"a_DSLR_photo_of_a_humanoid_robot_using_a_laptop.mp4\",\n    \"a_DSLR_photo_of_a_humanoid_robot_using_a_rolling_pin_to_roll_out_dough.mp4\",\n    \"a_DSLR_photo_of_a_human_skull.mp4\",\n    \"a_DSLR_photo_of_a_kitten_standing_on_top_of_a_giant_tortoise.mp4\",\n    \"a_DSLR_photo_of_a_knight_chopping_wood.mp4\",\n    \"a_DSLR_photo_of_a_knight_holding_a_lance_and_sitting_on_an_armored_horse.mp4\",\n    \"a_DSLR_photo_of_a_koala_wearing_a_party_hat_and_blowing_out_birthday_candles_on_a_cake.mp4\",\n    \"a_DSLR_photo_of_a_lemur_taking_notes_in_a_journal.mp4\",\n    \"a_DSLR_photo_of_a_lion_reading_the_newspaper.mp4\",\n    \"a_DSLR_photo_of_a_mandarin_duck_swimming_in_a_pond.mp4\",\n    \"a_DSLR_photo_of_a_model_of_the_eiffel_tower_made_out_of_toothpicks.mp4\",\n    \"a_DSLR_photo_of_a_mouse_playing_the_tuba.mp4\",\n    \"a_DSLR_photo_of_a_mug_of_hot_chocolate_with_whipped_cream_and_marshmallows.mp4\",\n    \"a_DSLR_photo_of_an_adorable_piglet_in_a_field.mp4\",\n    \"a_DSLR_photo_of_an_airplane_taking_off_from_the_runway.mp4\",\n    \"a_DSLR_photo_of_an_astronaut_standing_on_the_surface_of_mars.mp4\",\n    \"a_DSLR_photo_of_an_eggshell_broken_in_two_with_an_adorable_chick_standing_next_to_it.mp4\",\n    \"a_DSLR_photo_of_an_elephant_skull.mp4\",\n    \"a_DSLR_photo_of_an_exercise_bike_in_a_well_lit_room.mp4\",\n    \"a_DSLR_photo_of_an_extravagant_mansion,_aerial_view.mp4\",\n    \"a_DSLR_photo_of_an_ice_cream_sundae.mp4\",\n    \"a_DSLR_photo_of_an_iguana_holding_a_balloon.mp4\",\n    \"a_DSLR_photo_of_an_intricate_and_complex_dish_from_a_michelin_star_restaurant.mp4\",\n    \"a_DSLR_photo_of_An_iridescent_steampunk_patterned_millipede_with_bison_horns.mp4\",\n    \"a_DSLR_photo_of_an_octopus_playing_the_piano.mp4\",\n    \"a_DSLR_photo_of_an_old_car_overgrown_by_vines_and_weeds.mp4\",\n    \"a_DSLR_photo_of_an_old_vintage_car.mp4\",\n    \"a_DSLR_photo_of_an_orangutan_making_a_clay_bowl_on_a_throwing_wheel.mp4\",\n    \"a_DSLR_photo_of_an_orc_forging_a_hammer_on_an_anvil.mp4\",\n    \"a_DSLR_photo_of_an_origami_motorcycle.mp4\",\n    \"a_DSLR_photo_of_an_ornate_silver_gravy_boat_sitting_on_a_patterned_tablecloth.mp4\",\n    \"a_DSLR_photo_of_an_overstuffed_pastrami_sandwich.mp4\",\n    \"a_DSLR_photo_of_an_unstable_rock_cairn_in_the_middle_of_a_stream.mp4\",\n    \"a_DSLR_photo_of_a_pair_of_headphones_sitting_on_a_desk.mp4\",\n    \"a_DSLR_photo_of_a_pair_of_tan_cowboy_boots,_studio_lighting,_product_photography.mp4\",\n    \"a_DSLR_photo_of_a_peacock_on_a_surfboard.mp4\",\n    \"a_DSLR_photo_of_a_pigeon_reading_a_book.mp4\",\n    \"a_DSLR_photo_of_a_piglet_sitting_in_a_teacup.mp4\",\n    \"a_DSLR_photo_of_a_pig_playing_a_drum_set.mp4\",\n    \"a_DSLR_photo_of_a_pile_of_dice_on_a_green_tabletop_next_to_some_playing_cards.mp4\",\n    \"a_DSLR_photo_of_a_pirate_collie_dog,_high_resolution.mp4\",\n    \"a_DSLR_photo_of_a_plate_of_fried_chicken_and_waffles_with_maple_syrup_on_them.mp4\",\n    \"a_DSLR_photo_of_a_plate_piled_high_with_chocolate_chip_cookies.mp4\",\n    \"a_DSLR_photo_of_a_plush_t-rex_dinosaur_toy,_studio_lighting,_high_resolution.mp4\",\n    \"a_DSLR_photo_of_a_plush_triceratops_toy,_studio_lighting,_high_resolution.mp4\",\n    \"a_DSLR_photo_of_a_pomeranian_dog.mp4\",\n    \"a_DSLR_photo_of_a_porcelain_dragon.mp4\",\n    \"a_DSLR_photo_of_a_praying_mantis_wearing_roller_skates.mp4\",\n    \"a_DSLR_photo_of_a_puffin_standing_on_a_rock.mp4\",\n    \"a_DSLR_photo_of_a_pug_made_out_of_metal.mp4\",\n    \"a_DSLR_photo_of_a_pug_wearing_a_bee_costume.mp4\",\n    \"a_DSLR_photo_of_a_quill_and_ink_sitting_on_a_desk.mp4\",\n    \"a_DSLR_photo_of_a_raccoon_stealing_a_pie.mp4\",\n    \"a_DSLR_photo_of_a_red_cardinal_bird_singing.mp4\",\n    \"a_DSLR_photo_of_a_red_convertible_car_with_the_top_down.mp4\",\n    \"a_DSLR_photo_of_a_red-eyed_tree_frog.mp4\",\n    \"a_DSLR_photo_of_a_red_pickup_truck_driving_across_a_stream.mp4\",\n    \"a_DSLR_photo_of_a_red_wheelbarrow_with_a_shovel_in_it.mp4\",\n    \"a_DSLR_photo_of_a_roast_turkey_on_a_platter.mp4\",\n    \"a_DSLR_photo_of_a_robot_and_dinosaur_playing_chess,_high_resolution.mp4\",\n    \"a_DSLR_photo_of_a_robot_arm_picking_up_a_colorful_block_from_a_table.mp4\",\n    \"a_DSLR_photo_of_a_robot_cat_knocking_over_a_chess_piece_on_a_board.mp4\",\n    \"a_DSLR_photo_of_a_robot_dinosaur.mp4\",\n    \"a_DSLR_photo_of_a_robot_made_out_of_vegetables.mp4\",\n    \"a_DSLR_photo_of_a_robot_stegosaurus.mp4\",\n    \"a_DSLR_photo_of_a_robot_tiger.mp4\",\n    \"a_DSLR_photo_of_a_rolling_pin_on_top_of_bread_dough.mp4\",\n    \"a_DSLR_photo_of_a_sheepdog_running.mp4\",\n    \"a_DSLR_photo_of_a_shiba_inu_playing_golf_wearing_tartan_golf_clothes_and_hat.mp4\",\n    \"a_DSLR_photo_of_a_shiny_silver_robot_cat.mp4\",\n    \"a_DSLR_photo_of_a_silverback_gorilla_holding_a_golden_trophy.mp4\",\n    \"a_DSLR_photo_of_a_silver_humanoid_robot_flipping_a_coin.mp4\",\n    \"a_DSLR_photo_of_a_small_cherry_tomato_plant_in_a_pot_with_a_few_red_tomatoes_growing_on_it.mp4\",\n    \"a_DSLR_photo_of_a_small_saguaro_cactus_planted_in_a_clay_pot.mp4\",\n    \"a_DSLR_photo_of_a_Space_Shuttle.mp4\",\n    \"a_DSLR_photo_of_a_squirrel_dressed_like_a_clown.mp4\",\n    \"a_DSLR_photo_of_a_squirrel_flying_a_biplane.mp4\",\n    \"a_DSLR_photo_of_a_squirrel_giving_a_lecture_writing_on_a_chalkboard.mp4\",\n    \"a_DSLR_photo_of_a_squirrel_holding_a_bowling_ball.mp4\",\n    \"a_DSLR_photo_of_a_squirrel-lizard_hybrid.mp4\",\n    \"a_DSLR_photo_of_a_squirrel_made_out_of_fruit.mp4\",\n    \"a_DSLR_photo_of_a_squirrel-octopus_hybrid.mp4\",\n    \"a_DSLR_photo_of_a_stack_of_pancakes_covered_in_maple_syrup.mp4\",\n    \"a_DSLR_photo_of_a_steam_engine_train,_high_resolution.mp4\",\n    \"a_DSLR_photo_of_a_steaming_basket_full_of_dumplings.mp4\",\n    \"a_DSLR_photo_of_a_steaming_hot_plate_piled_high_with_spaghetti_and_meatballs.mp4\",\n    \"a_DSLR_photo_of_a_steampunk_space_ship_designed_in_the_18th_century.mp4\",\n    \"a_DSLR_photo_of_a_straw_basket_with_a_cobra_coming_out_of_it.mp4\",\n    \"a_DSLR_photo_of_a_swan_and_its_cygnets_swimming_in_a_pond.mp4\",\n    \"a_DSLR_photo_of_a_tarantula,_highly_detailed.mp4\",\n    \"a_DSLR_photo_of_a_teal_moped.mp4\",\n    \"a_DSLR_photo_of_a_teapot_shaped_like_an_elephant_head_where_its_snout_acts_as_the_spout.mp4\",\n    \"a_DSLR_photo_of_a_teddy_bear_taking_a_selfie.mp4\",\n    \"a_DSLR_photo_of_a_terracotta_bunny.mp4\",\n    \"a_DSLR_photo_of_a_tiger_dressed_as_a_doctor.mp4\",\n    \"a_DSLR_photo_of_a_tiger_made_out_of_yarn.mp4\",\n    \"a_DSLR_photo_of_a_toilet_made_out_of_gold.mp4\",\n    \"a_DSLR_photo_of_a_toy_robot.mp4\",\n    \"a_DSLR_photo_of_a_train_engine_made_out_of_clay.mp4\",\n    \"a_DSLR_photo_of_a_tray_of_Sushi_containing_pugs.mp4\",\n    \"a_DSLR_photo_of_a_tree_stump_with_an_axe_buried_in_it.mp4\",\n    \"a_DSLR_photo_of_a_turtle_standing_on_its_hind_legs,_wearing_a_top_hat_and_holding_a_cane.mp4\",\n    \"a_DSLR_photo_of_a_very_beautiful_small_organic_sculpture_made_of_fine_clockwork_and_gears_with_tiny_ruby_bearings,_very_intricate,_caved,_curved._Studio_lighting,_High_resolution,_white_background.mp4\",\n    \"a_DSLR_photo_of_A_very_beautiful_tiny_human_heart_organic_sculpture_made_of_copper_wire_and_threaded_pipes,_very_intricate,_curved,_Studio_lighting,_high_resolution.mp4\",\n    \"a_DSLR_photo_of_a_very_cool_and_trendy_pair_of_sneakers,_studio_lighting.mp4\",\n    \"a_DSLR_photo_of_a_vintage_record_player.mp4\",\n    \"a_DSLR_photo_of_a_wine_bottle_and_full_wine_glass_on_a_chessboard.mp4\",\n    \"a_DSLR_photo_of_a_wooden_desk_and_chair_from_an_elementary_school.mp4\",\n    \"a_DSLR_photo_of_a_yorkie_dog_eating_a_donut.mp4\",\n    \"a_DSLR_photo_of_a_yorkie_dog_wearing_extremely_cool_sneakers.mp4\",\n    \"a_DSLR_photo_of_baby_elephant_jumping_on_a_trampoline.mp4\",\n    \"a_DSLR_photo_of_cat_wearing_virtual_reality_headset_in_renaissance_oil_painting_high_detail_caravaggio.mp4\",\n    \"a_DSLR_photo_of_edible_typewriter_made_out_of_vegetables.mp4\",\n    \"a_DSLR_photo_of_Mont_Saint-Michel,_France,_aerial_view.mp4\",\n    \"a_DSLR_photo_of_Mount_Fuji,_aerial_view.mp4\",\n    \"a_DSLR_photo_of_Neuschwanstein_Castle,_aerial_view.mp4\",\n    \"A_DSLR_photo_of___pyramid_shaped_burrito_with_a_slice_cut_out_of_it.mp4\",\n    \"a_DSLR_photo_of_the_Imperial_State_Crown_of_England.mp4\",\n    \"a_DSLR_photo_of_the_leaning_tower_of_Pisa,_aerial_view.mp4\",\n    \"a_DSLR_photo_of_the_Statue_of_Liberty,_aerial_view.mp4\",\n    \"a_DSLR_photo_of_Two_locomotives_playing_tug_of_war.mp4\",\n    \"a_DSLR_photo_of_two_macaw_parrots_sharing_a_milkshake_with_two_straws.mp4\",\n    \"a_DSLR_photo_of_Westminster_Abbey,_aerial_view.mp4\",\n    \"a_ficus_planted_in_a_pot.mp4\",\n    \"a_flower_made_out_of_metal.mp4\",\n    \"a_fluffy_cat_lying_on_its_back_in_a_patch_of_sunlight.mp4\",\n    \"a_fox_and_a_hare_tangoing_together.mp4\",\n    \"a_fox_holding_a_videogame_controller.mp4\",\n    \"a_fox_playing_the_cello.mp4\",\n    \"a_frazer_nash_super_sport_car.mp4\",\n    \"a_freshly_baked_loaf_of_sourdough_bread_on_a_cutting_board.mp4\",\n    \"a_goat_drinking_beer.mp4\",\n    \"a_golden_goblet,_low_poly.mp4\",\n    \"a_green_dragon_breathing_fire.mp4\",\n    \"a_green_tractor_farming_corn_fields.mp4\",\n    \"a_highland_cow.mp4\",\n    \"a_hotdog_in_a_tutu_skirt.mp4\",\n    \"a_humanoid_robot_laying_on_the_couch_while_on_a_laptop.mp4\",\n    \"a_humanoid_robot_playing_the_violin.mp4\",\n    \"a_humanoid_robot_sitting_looking_at_a_Go_board_with_some_pieces_on_it.mp4\",\n    \"a_human_skeleton_drinking_a_glass_of_red_wine.mp4\",\n    \"a_human_skull_with_a_vine_growing_through_one_of_the_eye_sockets.mp4\",\n    \"a_kitten_looking_at_a_goldfish_in_a_bowl.mp4\",\n    \"a_lemur_drinking_boba.mp4\",\n    \"a_lemur_taking_notes_in_a_journal.mp4\",\n    \"a_lionfish.mp4\",\n    \"a_llama_wearing_a_suit.mp4\",\n    \"a_marble_bust_of_a_mouse.mp4\",\n    \"a_metal_sculpture_of_a_lion's_head,_highly_detailed.mp4\",\n    \"a_mojito_in_a_beach_chair.mp4\",\n    \"a_monkey-rabbit_hybrid.mp4\",\n    \"an_airplane_made_out_of_wood.mp4\",\n    \"an_amigurumi_bulldozer.mp4\",\n    \"An_anthropomorphic_tomato_eating_another_tomato.mp4\",\n    \"an_astronaut_playing_the_violin.mp4\",\n    \"an_astronaut_riding_a_kangaroo.mp4\",\n    \"an_English_castle,_aerial_view.mp4\",\n    \"an_erupting_volcano,_aerial_view.mp4\",\n    \"a_nest_with_a_few_white_eggs_and_one_golden_egg.mp4\",\n    \"an_exercise_bike.mp4\",\n    \"an_iridescent_metal_scorpion.mp4\",\n    \"An_octopus_and_a_giraffe_having_cheesecake.mp4\",\n    \"an_octopus_playing_the_harp.mp4\",\n    \"an_old_vintage_car.mp4\",\n    \"an_opulent_couch_from_the_palace_of_Versailles.mp4\",\n    \"an_orange_road_bike.mp4\",\n    \"an_orangutan_holding_a_paint_palette_in_one_hand_and_a_paintbrush_in_the_other.mp4\",\n    \"an_orangutan_playing_accordion_with_its_hands_spread_wide.mp4\",\n    \"an_orangutan_using_chopsticks_to_eat_ramen.mp4\",\n    \"an_orchid_flower_planted_in_a_clay_pot.mp4\",\n    \"a_palm_tree,_low_poly_3d_model.mp4\",\n    \"a_panda_rowing_a_boat_in_a_pond.mp4\",\n    \"a_panda_wearing_a_necktie_and_sitting_in_an_office_chair.mp4\",\n    \"A_Panther_De_Ville_car.mp4\",\n    \"a_pig_wearing_a_backpack.mp4\",\n    \"a_plate_of_delicious_tacos.mp4\",\n    \"a_plush_dragon_toy.mp4\",\n    \"a_plush_toy_of_a_corgi_nurse.mp4\",\n    \"a_rabbit,_animated_movie_character,_high_detail_3d_model.mp4\",\n    \"a_rabbit_cutting_grass_with_a_lawnmower.mp4\",\n    \"a_red_eyed_tree_frog,_low_poly.mp4\",\n    \"a_red_panda.mp4\",\n    \"a_ripe_strawberry.mp4\",\n    \"a_roulette_wheel.mp4\",\n    \"a_shiny_red_stand_mixer.mp4\",\n    \"a_silver_platter_piled_high_with_fruits.mp4\",\n    \"a_sliced_loaf_of_fresh_bread.mp4\",\n    \"a_snail_on_a_leaf.mp4\",\n    \"a_spanish_galleon_sailing_on_the_open_sea.mp4\",\n    \"a_squirrel_dressed_like_Henry_VIII_king_of_England.mp4\",\n    \"a_squirrel_gesturing_in_front_of_an_easel_showing_colorful_pie_charts.mp4\",\n    \"a_squirrel_wearing_a_tuxedo_and_holding_a_conductor's_baton.mp4\",\n    \"a_team_of_butterflies_playing_soccer_on_a_field.mp4\",\n    \"a_teddy_bear_pushing_a_shopping_cart_full_of_fruits_and_vegetables.mp4\",\n    \"a_tiger_dressed_as_a_military_general.mp4\",\n    \"a_tiger_karate_master.mp4\",\n    \"a_tiger_playing_the_violin.mp4\",\n    \"a_tiger_waiter_at_a_fancy_restaurant.mp4\",\n    \"a_tiger_wearing_a_tuxedo.mp4\",\n    \"a_t-rex_roaring_up_into_the_air.mp4\",\n    \"a_turtle_standing_on_its_hind_legs,_wearing_a_top_hat_and_holding_a_cane.mp4\",\n    \"a_typewriter.mp4\",\n    \"a_walrus_smoking_a_pipe.mp4\",\n    \"a_wedge_of_cheese_on_a_silver_platter.mp4\",\n    \"a_wide_angle_DSLR_photo_of_a_colorful_rooster.mp4\",\n    \"a_wide_angle_DSLR_photo_of_a_humanoid_banana_sitting_at_a_desk_doing_homework.mp4\",\n    \"a_wide_angle_DSLR_photo_of_a_mythical_troll_stirring_a_cauldron.mp4\",\n    \"a_wide_angle_DSLR_photo_of_a_squirrel_in_samurai_armor_wielding_a_katana.mp4\",\n    \"a_wide_angle_zoomed_out_DSLR_photo_of_A_red_dragon_dressed_in_a_tuxedo_and_playing_chess._The_chess_pieces_are_fashioned_after_robots.mp4\",\n    \"a_wide_angle_zoomed_out_DSLR_photo_of_a_skiing_penguin_wearing_a_puffy_jacket.mp4\",\n    \"a_wide_angle_zoomed_out_DSLR_photo_of_zoomed_out_view_of_Tower_Bridge_made_out_of_gingerbread_and_candy.mp4\",\n    \"a_woolly_mammoth_standing_on_ice.mp4\",\n    \"a_yellow_schoolbus.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_3d_model_of_an_adorable_cottage_with_a_thatched_roof.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_baby_bunny_sitting_on_top_of_a_stack_of_pancakes.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_baby_dragon.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_baby_monkey_riding_on_a_pig.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_badger_wearing_a_party_hat_and_blowing_out_birthday_candles_on_a_cake.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_beagle_eating_a_donut.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_bear_playing_electric_bass.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_beautifully_carved_wooden_knight_chess_piece.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_beautiful_suit_made_out_of_moss,_on_a_mannequin._Studio_lighting,_high_quality,_high_resolution.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_blue_lobster.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_blue_tulip.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_bowl_of_cereal_and_milk_with_a_spoon_in_it.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_brain_in_a_jar.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_bulldozer_made_out_of_toy_bricks.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_cake_in_the_shape_of_a_train.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_chihuahua_lying_in_a_pool_ring.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_chimpanzee_dressed_as_a_football_player.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_chimpanzee_holding_a_cup_of_hot_coffee.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_chimpanzee_wearing_headphones.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_colorful_camping_tent_in_a_patch_of_grass.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_complex_movement_from_an_expensive_watch_with_many_shiny_gears,_sitting_on_a_table.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_construction_excavator.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_corgi_wearing_a_top_hat.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_corn_cob_and_a_banana_playing_poker.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_dachsund_riding_a_unicycle.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_dachsund_wearing_a_boater_hat.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_few_pool_balls_sitting_on_a_pool_table.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_fox_working_on_a_jigsaw_puzzle.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_fresh_cinnamon_roll_covered_in_glaze.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_green_tractor.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_greyhound_dog_racing_down_the_track.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_group_of_squirrels_rowing_crew.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_gummy_bear_driving_a_convertible.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_hermit_crab_with_a_colorful_shell.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_hippo_biting_through_a_watermelon.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_hippo_made_out_of_chocolate.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_humanoid_robot_lying_on_a_couch_using_a_laptop.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_humanoid_robot_sitting_on_a_chair_drinking_a_cup_of_coffee.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_human_skeleton_relaxing_in_a_lounge_chair.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_kangaroo_sitting_on_a_bench_playing_the_accordion.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_kingfisher_bird.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_ladybug.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_lion's_mane_jellyfish.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_lobster_playing_the_saxophone.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_majestic_sailboat.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_marble_bust_of_a_cat,_a_real_mouse_is_sitting_on_its_head.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_marble_bust_of_a_fox_head.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_model_of_a_house_in_Tudor_style.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_monkey-rabbit_hybrid.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_monkey_riding_a_bike.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_mountain_goat_standing_on_a_boulder.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_mouse_holding_a_candlestick.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_an_adorable_kitten_lying_next_to_a_flower.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_an_all-utility_vehicle_driving_across_a_stream.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_an_amigurumi_motorcycle.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_an_astronaut_chopping_vegetables_in_a_sunlit_kitchen.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_an_egg_cracked_open_with_a_newborn_chick_hatching_out_of_it.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_an_expensive_office_chair.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_an_origami_bulldozer_sitting_on_the_ground.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_an_origami_crane.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_an_origami_hippo_in_a_river.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_an_otter_lying_on_its_back_in_the_water_holding_a_flower.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_pair_of_floating_chopsticks_picking_up_noodles_out_of_a_bowl_of_ramen.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_panda_throwing_wads_of_cash_into_the_air.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_panda_wearing_a_chef's_hat_and_kneading_bread_dough_on_a_countertop.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_pigeon_standing_on_a_manhole_cover.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_pig_playing_the_saxophone.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_pile_of_dice_on_a_green_tabletop.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_pita_bread_full_of_hummus_and_falafel_and_vegetables.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_pug_made_out_of_modeling_clay.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_A_punk_rock_squirrel_in_a_studded_leather_jacket_shouting_into_a_microphone_while_standing_on_a_stump_and_holding_a_beer.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_rabbit_cutting_grass_with_a_lawnmower.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_rabbit_digging_a_hole_with_a_shovel.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_raccoon_astronaut_holding_his_helmet.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_rainforest_bird_mating_ritual_dance.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_recliner_chair.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_red_rotary_telephone.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_robot_couple_fine_dining.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_rotary_telephone_carved_out_of_wood.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_shiny_beetle.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_silver_candelabra_sitting_on_a_red_velvet_tablecloth,_only_one_candle_is_lit.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_squirrel_DJing.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_squirrel_dressed_up_like_a_Victorian_woman.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_table_with_dim_sum_on_it.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_tiger_dressed_as_a_maid.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_tiger_dressed_as_a_military_general.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_tiger_eating_an_ice_cream_cone.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_tiger_wearing_sunglasses_and_a_leather_jacket,_riding_a_motorcycle.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_toad_catching_a_fly_with_its_tongue.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_wizard_raccoon_casting_a_spell.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_a_yorkie_dog_dressed_as_a_maid.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_cats_wearing_eyeglasses.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_miniature_schnauzer_wooden_sculpture,_high_quality_studio_photo.mp4\",\n    \"A_zoomed_out_DSLR_photo_of___phoenix_made_of_splashing_water_.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_Sydney_opera_house,_aerial_view.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_two_foxes_tango_dancing.mp4\",\n    \"a_zoomed_out_DSLR_photo_of_two_raccoons_playing_poker.mp4\",\n    \"Chichen_Itza,_aerial_view.mp4\",\n    \"__Coffee_cup_with_many_holes.mp4\",\n    \"fries_and_a_hamburger.mp4\",\n    \"__Luminescent_wild_horses.mp4\",\n    \"Michelangelo_style_statue_of_an_astronaut.mp4\",\n    \"Michelangelo_style_statue_of_dog_reading_news_on_a_cellphone.mp4\",\n    \"the_titanic,_aerial_view.mp4\",\n    \"two_gummy_bears_playing_dominoes.mp4\",\n    \"two_macaw_parrots_playing_chess.mp4\",\n    \"Wedding_dress_made_of_tentacles.mp4\",\n]\n\n\ndef main():\n    prompt_library = {\n        \"dreamfusion\": [\n            p.replace(\".mp4\", \"\").replace(\"_\", \" \")\n            for p in dreamfusion_gallery_video_names\n        ]\n    }\n    with open(\"load/prompt_library.json\", \"w\") as f:\n        json.dump(prompt_library, f, indent=2)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "metric_utils.py", "content": "# * evaluate use laion/CLIP-ViT-H-14-laion2B-s32B-b79K\n# best open source clip so far: laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\n# code adapted from NeuralLift-360\n\nimport torch\nimport torch.nn as nn\nimport os\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nimport matplotlib.pyplot as plt\n# import clip\nfrom transformers import CLIPFeatureExtractor, CLIPModel, CLIPTokenizer, CLIPProcessor\nfrom torchvision import transforms\nimport numpy as np\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport cv2\nfrom PIL import Image\n# import torchvision.transforms as transforms\nimport glob\nfrom skimage.metrics import peak_signal_noise_ratio as compare_psnr\nimport lpips\nfrom os.path import join as osp\nimport argparse\nimport pandas as pd\nimport contextual_loss as cl\n\ncriterion = cl.ContextualLoss(use_vgg=True, vgg_layer='relu5_4')\n\nclass CLIP(nn.Module):\n\n    def __init__(self,\n                 device,\n                 clip_name='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k',\n                 size=224):  #'laion/CLIP-ViT-B-32-laion2B-s34B-b79K'):\n        super().__init__()\n        self.size = size\n        self.device = f\"cuda:{device}\"\n\n        clip_name = clip_name\n\n        self.feature_extractor = CLIPFeatureExtractor.from_pretrained(\n            clip_name)\n        self.clip_model = CLIPModel.from_pretrained(clip_name).to(self.device)\n        self.tokenizer = CLIPTokenizer.from_pretrained(\n            'openai/clip-vit-base-patch32')\n\n        self.normalize = transforms.Normalize(\n            mean=self.feature_extractor.image_mean,\n            std=self.feature_extractor.image_std)\n\n        self.resize = transforms.Resize(224)\n        self.to_tensor = transforms.ToTensor()\n\n        # image augmentation\n        self.aug = T.Compose([\n            T.Resize((224, 224)),\n            T.Normalize((0.48145466, 0.4578275, 0.40821073),\n                        (0.26862954, 0.26130258, 0.27577711)),\n        ])\n\n    # * recommend to use this function for evaluation\n    @torch.no_grad()\n    def score_gt(self, ref_img_path, novel_views):\n        # assert len(novel_views) == 100\n        clip_scores = []\n        for novel in novel_views:\n            clip_scores.append(self.score_from_path(ref_img_path, [novel]))\n        return np.mean(clip_scores)\n\n    # * recommend to use this function for evaluation\n    # def score_gt(self, ref_paths, novel_paths):\n    #     clip_scores = []\n    #     for img1_path, img2_path in zip(ref_paths, novel_paths):\n    #         clip_scores.append(self.score_from_path(img1_path, img2_path))\n\n    #     return np.mean(clip_scores)\n\n    def similarity(self, image1_features: torch.Tensor,\n                   image2_features: torch.Tensor) -> float:\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            y = image1_features.T.view(image1_features.T.shape[1],\n                                       image1_features.T.shape[0])\n            similarity = torch.matmul(y, image2_features.T)\n            # print(similarity)\n            return similarity[0][0].item()\n\n    def get_img_embeds(self, img):\n        if img.shape[0] == 4:\n            img = img[:3, :, :]\n\n        img = self.aug(img).to(self.device)\n        img = img.unsqueeze(0)  # b,c,h,w\n\n        # plt.imshow(img.cpu().squeeze(0).permute(1, 2, 0).numpy())\n        # plt.show()\n        # print(img)\n\n        image_z = self.clip_model.get_image_features(img)\n        image_z = image_z / image_z.norm(dim=-1,\n                                         keepdim=True)  # normalize features\n        return image_z\n\n    def score_from_feature(self, img1, img2):\n        img1_feature, img2_feature = self.get_img_embeds(\n            img1), self.get_img_embeds(img2)\n        # for debug\n        return self.similarity(img1_feature, img2_feature)\n\n    def read_img_list(self, img_list):\n        size = self.size\n        images = []\n        # white_background = np.ones((size, size, 3), dtype=np.uint8) * 255\n\n        for img_path in img_list:\n            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n            # print(img_path)\n            if img.shape[2] == 4:  # Handle BGRA images\n                alpha = img[:, :, 3]  # Extract alpha channel\n                img = cv2.cvtColor(img,cv2.COLOR_BGRA2RGB)  # Convert BGRA to BGR\n                img[np.where(alpha == 0)] = [\n                    255, 255, 255\n                ]  # Set transparent pixels to white\n            else:  # Handle other image formats like JPG and PNG\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            img = cv2.resize(img, (size, size), interpolation=cv2.INTER_AREA)\n\n            # plt.imshow(img)\n            # plt.show()\n\n            images.append(img)\n\n        images = np.stack(images, axis=0)\n        # images[np.where(images == 0)] = 255  # Set black pixels to white\n        # images = np.where(images == 0, white_background, images)  # Set transparent pixels to white\n        # images = images.astype(np.float32)\n\n        return images\n\n    def score_from_path(self, img1_path, img2_path):\n        img1, img2 = self.read_img_list(img1_path), self.read_img_list(img2_path)\n        img1 = np.squeeze(img1)\n        img2 = np.squeeze(img2)\n        # plt.imshow(img1)\n        # plt.show()\n        # plt.imshow(img2)\n        # plt.show()\n\n        img1, img2 = self.to_tensor(img1), self.to_tensor(img2)\n        # print(\"img1 to tensor \",img1)\n        return self.score_from_feature(img1, img2)\n\n\ndef numpy_to_torch(images):\n    images = images * 2.0 - 1.0\n    images = torch.from_numpy(images.transpose((0, 3, 1, 2))).float()\n    return images.cuda()\n\n\nclass LPIPSMeter:\n\n    def __init__(self,\n                 net='alex',\n                 device=None,\n                 size=224):  # or we can use 'alex', 'vgg' as network\n        self.size = size\n        self.net = net\n        self.results = []\n        self.device = device if device is not None else torch.device(\n            'cuda' if torch.cuda.is_available() else 'cpu')\n        self.fn = lpips.LPIPS(net=net).eval().to(self.device)\n\n    def measure(self):\n        return np.mean(self.results)\n\n    def report(self):\n        return f'LPIPS ({self.net}) = {self.measure():.6f}'\n\n    def read_img_list(self, img_list):\n        size = self.size\n        images = []\n        white_background = np.ones((size, size, 3), dtype=np.uint8) * 255\n\n        for img_path in img_list:\n            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n\n            if img.shape[2] == 4:  # Handle BGRA images\n                alpha = img[:, :, 3]  # Extract alpha channel\n                img = cv2.cvtColor(img,\n                                   cv2.COLOR_BGRA2BGR)  # Convert BGRA to BGR\n\n                img = cv2.cvtColor(img,\n                                   cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n                img[np.where(alpha == 0)] = [\n                    255, 255, 255\n                ]  # Set transparent pixels to white\n            else:  # Handle other image formats like JPG and PNG\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            img = cv2.resize(img, (size, size), interpolation=cv2.INTER_AREA)\n            images.append(img)\n\n        images = np.stack(images, axis=0)\n        # images[np.where(images == 0)] = 255  # Set black pixels to white\n        # images = np.where(images == 0, white_background, images)  # Set transparent pixels to white\n        images = images.astype(np.float32) / 255.0\n\n        return images\n\n    # * recommend to use this function for evaluation\n    @torch.no_grad()\n    def score_gt(self, ref_paths, novel_paths):\n        self.results = []\n        for path0, path1 in zip(ref_paths, novel_paths):\n            # Load images\n            # img0 = lpips.im2tensor(lpips.load_image(path0)).cuda() # RGB image from [-1,1]\n            # img1 = lpips.im2tensor(lpips.load_image(path1)).cuda()\n            img0, img1 = self.read_img_list([path0]), self.read_img_list(\n                [path1])\n            img0, img1 = numpy_to_torch(img0), numpy_to_torch(img1)\n            # print(img0.shape,img1.shape)\n            img0 = F.interpolate(img0,\n                                    size=(self.size, self.size),\n                                    mode='area')\n            img1 = F.interpolate(img1,\n                                    size=(self.size, self.size),\n                                    mode='area')\n\n            # for debug vis\n            # plt.imshow(img0.cpu().squeeze(0).permute(1, 2, 0).numpy())\n            # plt.show()\n            # plt.imshow(img1.cpu().squeeze(0).permute(1, 2, 0).numpy())\n            # plt.show()\n            # equivalent to cv2.resize(rgba, (w, h), interpolation=cv2.INTER_AREA\n\n            # print(img0.shape,img1.shape)\n\n            self.results.append(self.fn.forward(img0, img1).cpu().numpy())\n\n        return self.measure()\n\nclass CXMeter:\n\n    def __init__(self,\n                 net='vgg',\n                 device=None,\n                 size=512):  # or we can use 'alex', 'vgg' as network\n        self.size = size\n        self.net = net\n        self.results = []\n        self.device = device if device is not None else torch.device(\n            'cuda' if torch.cuda.is_available() else 'cpu')\n        self.fn = lpips.LPIPS(net=net).eval().to(self.device)\n\n    def measure(self):\n        return np.mean(self.results)\n\n    def report(self):\n        return f'LPIPS ({self.net}) = {self.measure():.6f}'\n\n    def read_img_list(self, img_list):\n        size = self.size\n        images = []\n        white_background = np.ones((size, size, 3), dtype=np.uint8) * 255\n\n        for img_path in img_list:\n            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n\n            if img.shape[2] == 4:  # Handle BGRA images\n                alpha = img[:, :, 3]  # Extract alpha channel\n                img = cv2.cvtColor(img,\n                                   cv2.COLOR_BGRA2BGR)  # Convert BGRA to BGR\n\n                img = cv2.cvtColor(img,\n                                   cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n                img[np.where(alpha == 0)] = [\n                    255, 255, 255\n                ]  # Set transparent pixels to white\n            else:  # Handle other image formats like JPG and PNG\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            img = cv2.resize(img, (size, size), interpolation=cv2.INTER_AREA)\n            images.append(img)\n\n        images = np.stack(images, axis=0)\n        # images[np.where(images == 0)] = 255  # Set black pixels to white\n        # images = np.where(images == 0, white_background, images)  # Set transparent pixels to white\n        images = images.astype(np.float32) / 255.0\n\n        return images\n\n    # * recommend to use this function for evaluation\n    @torch.no_grad()\n    def score_gt(self, ref_paths, novel_paths):\n        self.results = []\n        path0 = ref_paths[0]\n        print('calculating CX loss')\n        for path1 in tqdm(novel_paths):\n            # Load images\n            img0, img1 = self.read_img_list([path0]), self.read_img_list(\n                [path1])\n            img0, img1 = numpy_to_torch(img0), numpy_to_torch(img1)\n            img0, img1 = img0 * 0.5 + 0.5, img1 * 0.5 + 0.5\n            img0 = F.interpolate(img0,\n                                    size=(self.size, self.size),\n                                    mode='area')\n            img1 = F.interpolate(img1,\n                                    size=(self.size, self.size),\n                                    mode='area')\n            loss = criterion(img0.cpu(), img1.cpu())\n            self.results.append(loss.cpu().numpy())\n\n        return self.measure()\n\nclass PSNRMeter:\n\n    def __init__(self, size=800):\n        self.results = []\n        self.size = size\n\n    def read_img_list(self, img_list):\n        size = self.size\n        images = []\n        white_background = np.ones((size, size, 3), dtype=np.uint8) * 255\n        for img_path in img_list:\n            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n\n            if img.shape[2] == 4:  # Handle BGRA images\n                alpha = img[:, :, 3]  # Extract alpha channel\n                img = cv2.cvtColor(img,\n                                   cv2.COLOR_BGRA2BGR)  # Convert BGRA to BGR\n\n                img = cv2.cvtColor(img,\n                                   cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n                img[np.where(alpha == 0)] = [\n                    255, 255, 255\n                ]  # Set transparent pixels to white\n            else:  # Handle other image formats like JPG and PNG\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            img = cv2.resize(img, (size, size), interpolation=cv2.INTER_AREA)\n            images.append(img)\n\n        images = np.stack(images, axis=0)\n        # images[np.where(images == 0)] = 255  # Set black pixels to white\n        # images = np.where(images == 0, white_background, images)  # Set transparent pixels to white\n        images = images.astype(np.float32) / 255.0\n        # print(images.shape)\n        return images\n\n    def update(self, preds, truths):\n        # print(preds.shape)\n\n        psnr_values = []\n        # For each pair of images in the batches\n        for img1, img2 in zip(preds, truths):\n            # Compute the PSNR and add it to the list\n            # print(img1.shape,img2.shape)\n\n            # for debug\n            # plt.imshow(img1)\n            # plt.show()\n            # plt.imshow(img2)\n            # plt.show()\n\n            psnr = compare_psnr(\n                img1, img2,\n                data_range=1.0)  # assuming your images are scaled to [0,1]\n            # print(f\"temp psnr {psnr}\")\n            psnr_values.append(psnr)\n\n        # Convert the list of PSNR values to a numpy array\n        self.results = psnr_values\n\n    def measure(self):\n        return np.mean(self.results)\n\n    def report(self):\n        return f'PSNR = {self.measure():.6f}'\n\n    # * recommend to use this function for evaluation\n    def score_gt(self, ref_paths, novel_paths):\n        self.results = []\n        # [B, N, 3] or [B, H, W, 3], range[0, 1]\n        preds = self.read_img_list(ref_paths)\n        print('novel_paths', novel_paths)\n        truths = self.read_img_list(novel_paths)\n        self.update(preds, truths)\n        return self.measure()\n\n# all_inputs = 'data'\n# nerf_dataset = os.listdir(osp(all_inputs, 'nerf4'))\n# realfusion_dataset = os.listdir(osp(all_inputs, 'realfusion15'))\n# meta_examples = {\n#    'nerf4': nerf_dataset, \n#    'realfusion15': realfusion_dataset, \n# }\n# all_datasets = meta_examples.keys()\n\n# organization 1\ndef deprecated_score_from_method_for_dataset(my_scorer,\n                                  method,\n                                  dataset,\n                                  input,\n                                  output,\n                                  score_type='clip', \n                                  ):  # psnr, lpips\n    # print(\"\\n\\n\\n\")\n    # print(f\"______{method}___{dataset}___{score_type}_________\")\n    scores = {}\n    final_res = 0\n    examples = meta_examples[dataset]\n    for i in range(len(examples)):\n        \n        # compare entire folder for clip\n        if score_type == 'clip':\n            novel_view = osp(pred_path, examples[i], 'colors')\n        # compare first image for other metrics\n        else:\n            if method == '3d_fuse': method = '3d_fuse_0'\n            novel_view = list(\n                glob.glob(\n                    osp(pred_path, examples[i], 'colors',\n                        'step_0000*')))[0]\n\n        score_i = my_scorer.score_gt(\n            [], [novel_view])\n        scores[examples[i]] = score_i\n        final_res += score_i\n    # print(scores, \" Avg : \", final_res / len(examples))\n    # print(\"``````````````````````\")\n    return scores\n\n# results organization 2\ndef score_from_method_for_dataset(my_scorer,\n                                  input_path,\n                                  pred_path,\n                                  score_type='clip', \n                                  rgb_name='lambertian', \n                                  result_folder='results/images', \n                                  first_str='*0000*'\n                                  ):  # psnr, lpips\n    scores = {}\n    final_res = 0\n    examples = os.listdir(input_path)\n    for i in range(len(examples)):\n        # ref path\n        ref_path = osp(input_path, examples[i], 'rgba.png') \n        # compare entire folder for clip\n        print(pred_path,'*'+examples[i]+'*', result_folder, f'*{rgb_name}*')\n        exit(0)\n        if score_type == 'clip':\n            novel_view = glob.glob(osp(pred_path,'*'+examples[i]+'*', result_folder, f'*{rgb_name}*'))\n            print(f'[INOF] {score_type} loss for example {examples[i]} between 1 GT and {len(novel_view)} predictions')\n        # compare first image for other metrics\n        else:\n            novel_view = glob.glob(osp(pred_path, '*'+examples[i]+'*/', result_folder, f'{first_str}{rgb_name}*'))\n            print(f'[INOF] {score_type} loss for example {examples[i]} between {ref_path} and {novel_view}')\n        # breakpoint()\n        score_i = my_scorer.score_gt([ref_path], novel_view)\n        scores[examples[i]] = score_i\n        final_res += score_i\n    avg_score = final_res / len(examples)\n    scores['average'] = avg_score\n    return scores\n\n\n# results organization 2\ndef score_from_my_method_for_dataset(my_scorer,\n                                  input_path, dataset,\n                                  score_type='clip'\n                                  ):  # psnr, lpips\n    scores = {}\n    final_res = 0\n    input_path = osp(input_path, dataset)\n    ref_path = glob.glob(osp(input_path, \"*_rgba.png\"))\n    novel_view = [osp(input_path, '%d.png' % i) for i in range(120)]\n    # print(ref_path)\n    # print(novel_view)\n    for i in tqdm(range(120)):\n        if os.path.exists(osp(input_path, '%d_color.png' % i)):\n            continue\n        img = cv2.imread(novel_view[i])\n        H = img.shape[0]\n        img = img[:, :H]\n        cv2.imwrite(osp(input_path, '%d_color.png' % i), img)\n    if score_type == 'clip' or score_type == 'cx':\n        novel_view = [osp(input_path, '%d_color.png' % i) for i in range(120)]\n    else:\n        novel_view = [osp(input_path, '%d_color.png' % i) for i in range(1)]\n    print(novel_view)\n    scores['%s_average' % dataset] = my_scorer.score_gt(ref_path, novel_view)\n    return scores\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Script to accept three string arguments\")\n    parser.add_argument(\"--input_path\",\n                        default=None,\n                        help=\"Specify the input path\")\n    parser.add_argument(\"--pred_pattern\",\n                        default=\"out/magic123*\", \n                        help=\"Specify the pattern of predition paths\")\n    parser.add_argument(\"--results_folder\",\n                        default=\"results/images\", \n                        help=\"where are the results under each pred_path\")\n    parser.add_argument(\"--rgb_name\",\n                        default=\"lambertian\", \n                        help=\"the postfix of the image\")\n    parser.add_argument(\"--first_str\",\n                        default=\"*0000*\", \n                        help=\"the str to indicate the first view\")\n    parser.add_argument(\"--datasets\",\n                        default=None,\n                        nargs='*',\n                        help=\"Specify the output path\")\n    parser.add_argument(\"--device\",\n                        type=int,\n                        default=0,\n                        help=\"Specify the GPU device to be used\")\n    parser.add_argument(\"--save_dir\", type=str, default='all_metrics/results')\n    args = parser.parse_args()\n\n    clip_scorer = CLIP(args.device)\n    lpips_scorer = LPIPSMeter()\n    psnr_scorer = PSNRMeter()\n    CX_scorer = CXMeter()\n    # criterion = criterion.to(args.device)\n\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    for dataset in os.listdir(args.input_path):\n        print(dataset)\n        results_dict = {}\n        results_dict['clip'] = score_from_my_method_for_dataset(\n            clip_scorer, args.input_path, dataset, 'clip')\n        \n        results_dict['psnr'] = score_from_my_method_for_dataset(\n            psnr_scorer, args.input_path, dataset,  'psnr')\n        \n        results_dict['lpips'] = score_from_my_method_for_dataset(\n            lpips_scorer, args.input_path, dataset,  'lpips')\n        \n        results_dict['CX'] = score_from_my_method_for_dataset(\n            CX_scorer, args.input_path, dataset,  'cx')\n        \n    df = pd.DataFrame(results_dict)\n    print(df)\n    df.to_csv(f\"{args.save_dir}/result.csv\")\n        \n\n    # for dataset in args.datasets:\n    #     input_path = osp(args.input_path, dataset)\n        \n    #     # assume the pred_path is organized as: pred_path/methods/dataset\n    #     pred_pattern = osp(args.pred_pattern, dataset)\n    #     pred_paths = glob.glob(pred_pattern)\n    #     print(f\"[INFO] Following the pattern {pred_pattern}, find {len(pred_paths)} pred_paths: \\n\", pred_paths)\n    #     if len(pred_paths) == 0:\n    #         raise IOError\n    #     for pred_path in pred_paths:\n    #         if not os.path.exists(pred_path):\n    #             print(f'[WARN] prediction does not exit for {pred_path}')\n    #         else:\n    #             print(f'[INFO] evaluate {pred_path}')\n            "}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/c++/mobilenet_v1.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nfrom __future__ import division\n\n\"\"\" \nCreates a MobileNet Model as defined in:\nAndrew G. Howard Menglong Zhu Bo Chen, et.al. (2017). \nMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. \nCopyright (c) Yang Lu, 2017\n\nModified By cleardusk\n\"\"\"\nimport math\nimport torch.nn as nn\n\n__all__ = ['mobilenet_2', 'mobilenet_1', 'mobilenet_075', 'mobilenet_05', 'mobilenet_025']\n\n\nclass DepthWiseBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, prelu=False):\n        super(DepthWiseBlock, self).__init__()\n        inplanes, planes = int(inplanes), int(planes)\n        self.conv_dw = nn.Conv2d(inplanes, inplanes, kernel_size=3, padding=1, stride=stride, groups=inplanes,\n                                 bias=False)\n        self.bn_dw = nn.BatchNorm2d(inplanes)\n        self.conv_sep = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_sep = nn.BatchNorm2d(planes)\n        if prelu:\n            self.relu = nn.PReLU()\n        else:\n            self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        out = self.conv_dw(x)\n        out = self.bn_dw(out)\n        out = self.relu(out)\n\n        out = self.conv_sep(out)\n        out = self.bn_sep(out)\n        out = self.relu(out)\n\n        return out\n\n\nclass MobileNet(nn.Module):\n    def __init__(self, widen_factor=1.0, num_classes=1000, prelu=False, input_channel=3):\n        \"\"\" Constructor\n        Args:\n            widen_factor: config of widen_factor\n            num_classes: number of classes\n        \"\"\"\n        super(MobileNet, self).__init__()\n\n        block = DepthWiseBlock\n        self.conv1 = nn.Conv2d(input_channel, int(32 * widen_factor), kernel_size=3, stride=2, padding=1,\n                               bias=False)\n\n        self.bn1 = nn.BatchNorm2d(int(32 * widen_factor))\n        if prelu:\n            self.relu = nn.PReLU()\n        else:\n            self.relu = nn.ReLU(inplace=True)\n\n        self.dw2_1 = block(32 * widen_factor, 64 * widen_factor, prelu=prelu)\n        self.dw2_2 = block(64 * widen_factor, 128 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw3_1 = block(128 * widen_factor, 128 * widen_factor, prelu=prelu)\n        self.dw3_2 = block(128 * widen_factor, 256 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw4_1 = block(256 * widen_factor, 256 * widen_factor, prelu=prelu)\n        self.dw4_2 = block(256 * widen_factor, 512 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw5_1 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_2 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_3 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_4 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_5 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_6 = block(512 * widen_factor, 1024 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw6 = block(1024 * widen_factor, 1024 * widen_factor, prelu=prelu)\n\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(int(1024 * widen_factor), num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.dw2_1(x)\n        x = self.dw2_2(x)\n        x = self.dw3_1(x)\n        x = self.dw3_2(x)\n        x = self.dw4_1(x)\n        x = self.dw4_2(x)\n        x = self.dw5_1(x)\n        x = self.dw5_2(x)\n        x = self.dw5_3(x)\n        x = self.dw5_4(x)\n        x = self.dw5_5(x)\n        x = self.dw5_6(x)\n        x = self.dw6(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef mobilenet(widen_factor=1.0, num_classes=1000):\n    \"\"\"\n    Construct MobileNet.\n    widen_factor=1.0  for mobilenet_1\n    widen_factor=0.75 for mobilenet_075\n    widen_factor=0.5  for mobilenet_05\n    widen_factor=0.25 for mobilenet_025\n    \"\"\"\n    model = MobileNet(widen_factor=widen_factor, num_classes=num_classes)\n    return model\n\n\ndef mobilenet_2(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=2.0, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_1(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=1.0, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_075(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.75, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_05(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.5, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_025(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.25, num_classes=num_classes, input_channel=input_channel)\n    return model\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/benchmark_aflw2000.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\n\"\"\"\nNotation (2019.09.15): two versions of spliting AFLW2000-3D:\n 1) AFLW2000-3D.pose.npy: according to the fitted pose\n 2) AFLW2000-3D-new.pose: according to AFLW labels \nThere is no obvious difference between these two splits.\n\"\"\"\n\nimport os.path as osp\nimport numpy as np\nfrom math import sqrt\nfrom utils.io import _load\n\nd = 'test.configs'\n\n# [1312, 383, 305], current version\nyaws_list = _load(osp.join(d, 'AFLW2000-3D.pose.npy'))\n\n# [1306, 462, 232], same as paper\n# yaws_list = _load(osp.join(d, 'AFLW2000-3D-new.pose.npy'))\n\n# origin\npts68_all_ori = _load(osp.join(d, 'AFLW2000-3D.pts68.npy'))\n\n# reannonated\npts68_all_re = _load(osp.join(d, 'AFLW2000-3D-Reannotated.pts68.npy'))\nroi_boxs = _load(osp.join(d, 'AFLW2000-3D_crop.roi_box.npy'))\n\n\ndef ana(nme_list):\n    yaw_list_abs = np.abs(yaws_list)\n    ind_yaw_1 = yaw_list_abs <= 30\n    ind_yaw_2 = np.bitwise_and(yaw_list_abs > 30, yaw_list_abs <= 60)\n    ind_yaw_3 = yaw_list_abs > 60\n\n    nme_1 = nme_list[ind_yaw_1]\n    nme_2 = nme_list[ind_yaw_2]\n    nme_3 = nme_list[ind_yaw_3]\n\n    mean_nme_1 = np.mean(nme_1) * 100\n    mean_nme_2 = np.mean(nme_2) * 100\n    mean_nme_3 = np.mean(nme_3) * 100\n    # mean_nme_all = np.mean(nme_list) * 100\n\n    std_nme_1 = np.std(nme_1) * 100\n    std_nme_2 = np.std(nme_2) * 100\n    std_nme_3 = np.std(nme_3) * 100\n    # std_nme_all = np.std(nme_list) * 100\n\n    mean_all = [mean_nme_1, mean_nme_2, mean_nme_3]\n    mean = np.mean(mean_all)\n    std = np.std(mean_all)\n\n    s1 = '[ 0, 30]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_1, std_nme_1)\n    s2 = '[30, 60]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_2, std_nme_2)\n    s3 = '[60, 90]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_3, std_nme_3)\n    # s4 = '[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_all, std_nme_all)\n    s5 = '[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: \\x1b[31m{:.3f}\\x1b[0m'.format(mean, std)\n\n    s = '\\n'.join([s1, s2, s3, s5])\n    print(s)\n\n    return mean_nme_1, mean_nme_2, mean_nme_3, mean, std\n\n\ndef convert_to_ori(lms, i):\n    std_size = 120\n    sx, sy, ex, ey = roi_boxs[i]\n    scale_x = (ex - sx) / std_size\n    scale_y = (ey - sy) / std_size\n    lms[0, :] = lms[0, :] * scale_x + sx\n    lms[1, :] = lms[1, :] * scale_y + sy\n    return lms\n\n\ndef calc_nme(pts68_fit_all, option='ori'):\n    if option == 'ori':\n        pts68_all = pts68_all_ori\n    elif option == 're':\n        pts68_all = pts68_all_re\n    std_size = 120\n\n    nme_list = []\n\n    for i in range(len(roi_boxs)):\n        pts68_fit = pts68_fit_all[i]\n        pts68_gt = pts68_all[i]\n\n        sx, sy, ex, ey = roi_boxs[i]\n        scale_x = (ex - sx) / std_size\n        scale_y = (ey - sy) / std_size\n        pts68_fit[0, :] = pts68_fit[0, :] * scale_x + sx\n        pts68_fit[1, :] = pts68_fit[1, :] * scale_y + sy\n\n        # build bbox\n        minx, maxx = np.min(pts68_gt[0, :]), np.max(pts68_gt[0, :])\n        miny, maxy = np.min(pts68_gt[1, :]), np.max(pts68_gt[1, :])\n        llength = sqrt((maxx - minx) * (maxy - miny))\n\n        #\n        dis = pts68_fit - pts68_gt[:2, :]\n        dis = np.sqrt(np.sum(np.power(dis, 2), 0))\n        dis = np.mean(dis)\n        nme = dis / llength\n        nme_list.append(nme)\n\n    nme_list = np.array(nme_list, dtype=np.float32)\n    return nme_list\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/demo@obama/rendering.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport sys\n\nsys.path.append('../')\nimport os\nimport os.path as osp\nfrom glob import glob\n\nfrom utils.lighting import RenderPipeline\nimport numpy as np\nimport scipy.io as sio\nimport imageio\n\ncfg = {\n    'intensity_ambient': 0.3,\n    'color_ambient': (1, 1, 1),\n    'intensity_directional': 0.6,\n    'color_directional': (1, 1, 1),\n    'intensity_specular': 0.1,\n    'specular_exp': 5,\n    'light_pos': (0, 0, 5),\n    'view_pos': (0, 0, 5)\n}\n\n\ndef _to_ctype(arr):\n    if not arr.flags.c_contiguous:\n        return arr.copy(order='C')\n    return arr\n\n\ndef obama_demo():\n    wd = 'obama_res@dense_py'\n    if not osp.exists(wd):\n        os.mkdir(wd)\n\n    app = RenderPipeline(**cfg)\n    img_fps = sorted(glob('obama/*.jpg'))\n    triangles = sio.loadmat('tri_refine.mat')['tri']  # mx3\n    triangles = _to_ctype(triangles).astype(np.int32)  # for type compatible\n\n    for img_fp in img_fps[:]:\n        vertices = sio.loadmat(img_fp.replace('.jpg', '_0.mat'))['vertex'].T  # mx3\n        img = imageio.imread(img_fp).astype(np.float32) / 255.\n\n        # end = time.clock()\n        img_render = app(vertices, triangles, img)\n        # print('Elapse: {:.1f}ms'.format((time.clock() - end) * 1000))\n\n        img_wfp = osp.join(wd, osp.basename(img_fp))\n        imageio.imwrite(img_wfp, img_render)\n        print('Writing to {}'.format(img_wfp))\n\n\nif __name__ == '__main__':\n    obama_demo()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/demo@obama/rendering_demo.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\n\"\"\"\nA demo for rendering mesh generated by `main.py`\n\"\"\"\n\nfrom rendering import cfg, _to_ctype, RenderPipeline\nimport scipy.io as sio\nimport imageio\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef test():\n    # 1. first, using main.py to generate dense vertices, like emma_input_0.mat\n    fp = '../samples/emma_input_0.mat'\n    vertices = sio.loadmat(fp)['vertex'].T  # 3xm\n    print(vertices.shape)\n    img = imageio.imread('../samples/emma_input.jpg').astype(np.float32) / 255.\n\n    # 2. render it\n    # triangles = sio.loadmat('tri_refine.mat')['tri']  # mx3\n    triangles = sio.loadmat('../visualize/tri.mat')['tri'].T - 1  # mx3\n    print(triangles.shape)\n    triangles = _to_ctype(triangles).astype(np.int32)  # for type compatible\n    app = RenderPipeline(**cfg)\n    img_render = app(vertices, triangles, img)\n\n    plt.imshow(img_render)\n    plt.show()\n\n\ndef main():\n    test()\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/mobilenet_v1.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nfrom __future__ import division\n\n\"\"\" \nCreates a MobileNet Model as defined in:\nAndrew G. Howard Menglong Zhu Bo Chen, et.al. (2017). \nMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. \nCopyright (c) Yang Lu, 2017\n\nModified By cleardusk\n\"\"\"\nimport math\nimport torch.nn as nn\n\n__all__ = ['mobilenet_2', 'mobilenet_1', 'mobilenet_075', 'mobilenet_05', 'mobilenet_025']\n\n\nclass DepthWiseBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, prelu=False):\n        super(DepthWiseBlock, self).__init__()\n        inplanes, planes = int(inplanes), int(planes)\n        self.conv_dw = nn.Conv2d(inplanes, inplanes, kernel_size=3, padding=1, stride=stride, groups=inplanes,\n                                 bias=False)\n        self.bn_dw = nn.BatchNorm2d(inplanes)\n        self.conv_sep = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_sep = nn.BatchNorm2d(planes)\n        if prelu:\n            self.relu = nn.PReLU()\n        else:\n            self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        out = self.conv_dw(x)\n        out = self.bn_dw(out)\n        out = self.relu(out)\n\n        out = self.conv_sep(out)\n        out = self.bn_sep(out)\n        out = self.relu(out)\n\n        return out\n\n\nclass MobileNet(nn.Module):\n    def __init__(self, widen_factor=1.0, num_classes=1000, prelu=False, input_channel=3):\n        \"\"\" Constructor\n        Args:\n            widen_factor: config of widen_factor\n            num_classes: number of classes\n        \"\"\"\n        super(MobileNet, self).__init__()\n\n        block = DepthWiseBlock\n        self.conv1 = nn.Conv2d(input_channel, int(32 * widen_factor), kernel_size=3, stride=2, padding=1,\n                               bias=False)\n\n        self.bn1 = nn.BatchNorm2d(int(32 * widen_factor))\n        if prelu:\n            self.relu = nn.PReLU()\n        else:\n            self.relu = nn.ReLU(inplace=True)\n\n        self.dw2_1 = block(32 * widen_factor, 64 * widen_factor, prelu=prelu)\n        self.dw2_2 = block(64 * widen_factor, 128 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw3_1 = block(128 * widen_factor, 128 * widen_factor, prelu=prelu)\n        self.dw3_2 = block(128 * widen_factor, 256 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw4_1 = block(256 * widen_factor, 256 * widen_factor, prelu=prelu)\n        self.dw4_2 = block(256 * widen_factor, 512 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw5_1 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_2 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_3 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_4 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_5 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_6 = block(512 * widen_factor, 1024 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw6 = block(1024 * widen_factor, 1024 * widen_factor, prelu=prelu)\n\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(int(1024 * widen_factor), num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.dw2_1(x)\n        x = self.dw2_2(x)\n        x = self.dw3_1(x)\n        x = self.dw3_2(x)\n        x = self.dw4_1(x)\n        x = self.dw4_2(x)\n        x = self.dw5_1(x)\n        x = self.dw5_2(x)\n        x = self.dw5_3(x)\n        x = self.dw5_4(x)\n        x = self.dw5_5(x)\n        x = self.dw5_6(x)\n        x = self.dw6(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef mobilenet(widen_factor=1.0, num_classes=1000):\n    \"\"\"\n    Construct MobileNet.\n    widen_factor=1.0  for mobilenet_1\n    widen_factor=0.75 for mobilenet_075\n    widen_factor=0.5  for mobilenet_05\n    widen_factor=0.25 for mobilenet_025\n    \"\"\"\n    model = MobileNet(widen_factor=widen_factor, num_classes=num_classes)\n    return model\n\n\ndef mobilenet_2(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=2.0, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_1(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=1.0, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_075(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.75, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_05(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.5, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_025(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.25, num_classes=num_classes, input_channel=input_channel)\n    return model\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/train.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport os.path as osp\nfrom pathlib import Path\nimport numpy as np\nimport argparse\nimport time\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport mobilenet_v1\nimport torch.backends.cudnn as cudnn\n\nfrom utils.ddfa import DDFADataset, ToTensorGjz, NormalizeGjz\nfrom utils.ddfa import str2bool, AverageMeter\nfrom utils.io import mkdir\nfrom vdc_loss import VDCLoss\nfrom wpdc_loss import WPDCLoss\n\n# global args (configuration)\nargs = None\nlr = None\narch_choices = ['mobilenet_2', 'mobilenet_1', 'mobilenet_075', 'mobilenet_05', 'mobilenet_025']\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='3DMM Fitting')\n    parser.add_argument('-j', '--workers', default=6, type=int)\n    parser.add_argument('--epochs', default=40, type=int)\n    parser.add_argument('--start-epoch', default=1, type=int)\n    parser.add_argument('-b', '--batch-size', default=128, type=int)\n    parser.add_argument('-vb', '--val-batch-size', default=32, type=int)\n    parser.add_argument('--base-lr', '--learning-rate', default=0.001, type=float)\n    parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n                        help='momentum')\n    parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float)\n    parser.add_argument('--print-freq', '-p', default=20, type=int)\n    parser.add_argument('--resume', default='', type=str, metavar='PATH')\n    parser.add_argument('--devices-id', default='0,1', type=str)\n    parser.add_argument('--filelists-train',\n                        default='', type=str)\n    parser.add_argument('--filelists-val',\n                        default='', type=str)\n    parser.add_argument('--root', default='')\n    parser.add_argument('--snapshot', default='', type=str)\n    parser.add_argument('--log-file', default='output.log', type=str)\n    parser.add_argument('--log-mode', default='w', type=str)\n    parser.add_argument('--size-average', default='true', type=str2bool)\n    parser.add_argument('--num-classes', default=62, type=int)\n    parser.add_argument('--arch', default='mobilenet_1', type=str,\n                        choices=arch_choices)\n    parser.add_argument('--frozen', default='false', type=str2bool)\n    parser.add_argument('--milestones', default='15,25,30', type=str)\n    parser.add_argument('--task', default='all', type=str)\n    parser.add_argument('--test_initial', default='false', type=str2bool)\n    parser.add_argument('--warmup', default=-1, type=int)\n    parser.add_argument('--param-fp-train',\n                        default='',\n                        type=str)\n    parser.add_argument('--param-fp-val',\n                        default='')\n    parser.add_argument('--opt-style', default='resample', type=str)  # resample\n    parser.add_argument('--resample-num', default=132, type=int)\n    parser.add_argument('--loss', default='vdc', type=str)\n\n    global args\n    args = parser.parse_args()\n\n    # some other operations\n    args.devices_id = [int(d) for d in args.devices_id.split(',')]\n    args.milestones = [int(m) for m in args.milestones.split(',')]\n\n    snapshot_dir = osp.split(args.snapshot)[0]\n    mkdir(snapshot_dir)\n\n\ndef print_args(args):\n    for arg in vars(args):\n        s = arg + ': ' + str(getattr(args, arg))\n        logging.info(s)\n\n\ndef adjust_learning_rate(optimizer, epoch, milestones=None):\n    \"\"\"Sets the learning rate: milestone is a list/tuple\"\"\"\n\n    def to(epoch):\n        if epoch <= args.warmup:\n            return 1\n        elif args.warmup < epoch <= milestones[0]:\n            return 0\n        for i in range(1, len(milestones)):\n            if milestones[i - 1] < epoch <= milestones[i]:\n                return i\n        return len(milestones)\n\n    n = to(epoch)\n\n    global lr\n    lr = args.base_lr * (0.2 ** n)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef save_checkpoint(state, filename='checkpoint.pth.tar'):\n    torch.save(state, filename)\n    logging.info(f'Save checkpoint to {filename}')\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    model.train()\n\n    end = time.time()\n    # loader is batch style\n    # for i, (input, target) in enumerate(train_loader):\n    for i, (input, target) in enumerate(train_loader):\n        target.requires_grad = False\n        target = target.cuda(non_blocking=True)\n        output = model(input)\n\n        data_time.update(time.time() - end)\n\n        if args.loss.lower() == 'vdc':\n            loss = criterion(output, target)\n        elif args.loss.lower() == 'wpdc':\n            loss = criterion(output, target)\n        elif args.loss.lower() == 'pdc':\n            loss = criterion(output, target)\n        else:\n            raise Exception(f'Unknown loss {args.loss}')\n\n        losses.update(loss.item(), input.size(0))\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # log\n        if i % args.print_freq == 0:\n            logging.info(f'Epoch: [{epoch}][{i}/{len(train_loader)}]\\t'\n                         f'LR: {lr:8f}\\t'\n                         f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                         # f'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                         f'Loss {losses.val:.4f} ({losses.avg:.4f})')\n\n\ndef validate(val_loader, model, criterion, epoch):\n    model.eval()\n\n    end = time.time()\n    with torch.no_grad():\n        losses = []\n        for i, (input, target) in enumerate(val_loader):\n            # compute output\n            target.requires_grad = False\n            target = target.cuda(non_blocking=True)\n            output = model(input)\n\n            loss = criterion(output, target)\n            losses.append(loss.item())\n\n        elapse = time.time() - end\n        loss = np.mean(losses)\n        logging.info(f'Val: [{epoch}][{len(val_loader)}]\\t'\n                     f'Loss {loss:.4f}\\t'\n                     f'Time {elapse:.3f}')\n\n\ndef main():\n    parse_args()  # parse global argsl\n\n    # logging setup\n    logging.basicConfig(\n        format='[%(asctime)s] [p%(process)s] [%(pathname)s:%(lineno)d] [%(levelname)s] %(message)s',\n        level=logging.INFO,\n        handlers=[\n            logging.FileHandler(args.log_file, mode=args.log_mode),\n            logging.StreamHandler()\n        ]\n    )\n\n    print_args(args)  # print args\n\n    # step1: define the model structure\n    model = getattr(mobilenet_v1, args.arch)(num_classes=args.num_classes)\n\n    torch.cuda.set_device(args.devices_id[0])  # fix bug for `ERROR: all tensors must be on devices[0]`\n\n    model = nn.DataParallel(model, device_ids=args.devices_id).cuda()  # -> GPU\n\n    # step2: optimization: loss and optimization method\n    # criterion = nn.MSELoss(size_average=args.size_average).cuda()\n    if args.loss.lower() == 'wpdc':\n        print(args.opt_style)\n        criterion = WPDCLoss(opt_style=args.opt_style).cuda()\n        logging.info('Use WPDC Loss')\n    elif args.loss.lower() == 'vdc':\n        criterion = VDCLoss(opt_style=args.opt_style).cuda()\n        logging.info('Use VDC Loss')\n    elif args.loss.lower() == 'pdc':\n        criterion = nn.MSELoss(size_average=args.size_average).cuda()\n        logging.info('Use PDC loss')\n    else:\n        raise Exception(f'Unknown Loss {args.loss}')\n\n    optimizer = torch.optim.SGD(model.parameters(),\n                                lr=args.base_lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay,\n                                nesterov=True)\n    # step 2.1 resume\n    if args.resume:\n        if Path(args.resume).is_file():\n            logging.info(f'=> loading checkpoint {args.resume}')\n\n            checkpoint = torch.load(args.resume, map_location=lambda storage, loc: storage)['state_dict']\n            # checkpoint = torch.load(args.resume)['state_dict']\n            model.load_state_dict(checkpoint)\n\n        else:\n            logging.info(f'=> no checkpoint found at {args.resume}')\n\n    # step3: data\n    normalize = NormalizeGjz(mean=127.5, std=128)  # may need optimization\n\n    train_dataset = DDFADataset(\n        root=args.root,\n        filelists=args.filelists_train,\n        param_fp=args.param_fp_train,\n        transform=transforms.Compose([ToTensorGjz(), normalize])\n    )\n    val_dataset = DDFADataset(\n        root=args.root,\n        filelists=args.filelists_val,\n        param_fp=args.param_fp_val,\n        transform=transforms.Compose([ToTensorGjz(), normalize])\n    )\n\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.workers,\n                              shuffle=True, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_dataset, batch_size=args.val_batch_size, num_workers=args.workers,\n                            shuffle=False, pin_memory=True)\n\n    # step4: run\n    cudnn.benchmark = True\n    if args.test_initial:\n        logging.info('Testing from initial')\n        validate(val_loader, model, criterion, args.start_epoch)\n\n    for epoch in range(args.start_epoch, args.epochs + 1):\n        # adjust learning rate\n        adjust_learning_rate(optimizer, epoch, args.milestones)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n        filename = f'{args.snapshot}_checkpoint_epoch_{epoch}.pth.tar'\n        save_checkpoint(\n            {\n                'epoch': epoch,\n                'state_dict': model.state_dict(),\n                # 'optimizer': optimizer.state_dict()\n            },\n            filename\n        )\n\n        validate(val_loader, model, criterion, epoch)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/utils/cv_plot.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\n\n\"\"\"\nModified from: https://sourcegraph.com/github.com/YadiraF/PRNet@master/-/blob/utils/cv_plot.py\n\"\"\"\n\nimport numpy as np\nimport cv2\n\nfrom utils.inference import calc_hypotenuse\n\nend_list = np.array([17, 22, 27, 42, 48, 31, 36, 68], dtype=np.int32) - 1\n\n\ndef plot_kpt(image, kpt):\n    ''' Draw 68 key points\n    Args:\n        image: the input image\n        kpt: (68, 3).\n    '''\n    image = image.copy()\n    kpt = np.round(kpt).astype(np.int32)\n    for i in range(kpt.shape[0]):\n        st = kpt[i, :2]\n        image = cv2.circle(image, (st[0], st[1]), 1, (0, 0, 255), 2)\n        if i in end_list:\n            continue\n        ed = kpt[i + 1, :2]\n        image = cv2.line(image, (st[0], st[1]), (ed[0], ed[1]), (255, 255, 255), 1)\n    return image\n\n\ndef build_camera_box(rear_size=90):\n    point_3d = []\n    rear_depth = 0\n    point_3d.append((-rear_size, -rear_size, rear_depth))\n    point_3d.append((-rear_size, rear_size, rear_depth))\n    point_3d.append((rear_size, rear_size, rear_depth))\n    point_3d.append((rear_size, -rear_size, rear_depth))\n    point_3d.append((-rear_size, -rear_size, rear_depth))\n\n    front_size = int(4 / 3 * rear_size)\n    front_depth = int(4 / 3 * rear_size)\n    point_3d.append((-front_size, -front_size, front_depth))\n    point_3d.append((-front_size, front_size, front_depth))\n    point_3d.append((front_size, front_size, front_depth))\n    point_3d.append((front_size, -front_size, front_depth))\n    point_3d.append((-front_size, -front_size, front_depth))\n    point_3d = np.array(point_3d, dtype=np.float).reshape(-1, 3)\n\n    return point_3d\n\n\ndef plot_pose_box(image, Ps, pts68s, color=(40, 255, 0), line_width=2):\n    ''' Draw a 3D box as annotation of pose. Ref:https://github.com/yinguobing/head-pose-estimation/blob/master/pose_estimator.py\n    Args:\n        image: the input image\n        P: (3, 4). Affine Camera Matrix.\n        kpt: (2, 68) or (3, 68)\n    '''\n    image = image.copy()\n    if not isinstance(pts68s, list):\n        pts68s = [pts68s]\n    if not isinstance(Ps, list):\n        Ps = [Ps]\n    for i in range(len(pts68s)):\n        pts68 = pts68s[i]\n        llength = calc_hypotenuse(pts68)\n        point_3d = build_camera_box(llength)\n        P = Ps[i]\n\n        # Map to 2d image points\n        point_3d_homo = np.hstack((point_3d, np.ones([point_3d.shape[0], 1])))  # n x 4\n        point_2d = point_3d_homo.dot(P.T)[:, :2]\n\n        point_2d[:, 1] = - point_2d[:, 1]\n        point_2d[:, :2] = point_2d[:, :2] - np.mean(point_2d[:4, :2], 0) + np.mean(pts68[:2, :27], 1)\n        point_2d = np.int32(point_2d.reshape(-1, 2))\n\n        # Draw all the lines\n        cv2.polylines(image, [point_2d], True, color, line_width, cv2.LINE_AA)\n        cv2.line(image, tuple(point_2d[1]), tuple(\n            point_2d[6]), color, line_width, cv2.LINE_AA)\n        cv2.line(image, tuple(point_2d[2]), tuple(\n            point_2d[7]), color, line_width, cv2.LINE_AA)\n        cv2.line(image, tuple(point_2d[3]), tuple(\n            point_2d[8]), color, line_width, cv2.LINE_AA)\n\n    return image\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/utils/__init__.py", "content": ""}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/demo@obama/convert_imgs_to_video.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport os\nimport os.path as osp\nimport sys\nfrom glob import glob\nimport imageio\n\n\ndef main():\n    assert len(sys.argv) >= 2\n    d = sys.argv[1]\n\n    fps = glob(osp.join(d, '*.jpg'))\n    fps = sorted(fps, key=lambda x: int(x.split('/')[-1].replace('.jpg', '')))\n\n    imgs = []\n    for fp in fps:\n        img = imageio.imread(fp)\n        imgs.append(img)\n\n    if len(sys.argv) >= 3:\n        imageio.mimwrite(sys.argv[2], imgs, fps=24, macro_block_size=None)\n    else:\n        imageio.mimwrite(osp.basename(d) + '.mp4', imgs, fps=24, macro_block_size=None)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/training/train.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport os.path as osp\nfrom pathlib import Path\nimport numpy as np\nimport argparse\nimport time\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport mobilenet_v1\nimport torch.backends.cudnn as cudnn\n\nfrom utils.ddfa import DDFADataset, ToTensorGjz, NormalizeGjz\nfrom utils.ddfa import str2bool, AverageMeter\nfrom utils.io import mkdir\nfrom vdc_loss import VDCLoss\nfrom wpdc_loss import WPDCLoss\n\n# global args (configuration)\nargs = None\nlr = None\narch_choices = ['mobilenet_2', 'mobilenet_1', 'mobilenet_075', 'mobilenet_05', 'mobilenet_025']\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='3DMM Fitting')\n    parser.add_argument('-j', '--workers', default=6, type=int)\n    parser.add_argument('--epochs', default=40, type=int)\n    parser.add_argument('--start-epoch', default=1, type=int)\n    parser.add_argument('-b', '--batch-size', default=128, type=int)\n    parser.add_argument('-vb', '--val-batch-size', default=32, type=int)\n    parser.add_argument('--base-lr', '--learning-rate', default=0.001, type=float)\n    parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n                        help='momentum')\n    parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float)\n    parser.add_argument('--print-freq', '-p', default=20, type=int)\n    parser.add_argument('--resume', default='', type=str, metavar='PATH')\n    parser.add_argument('--devices-id', default='0,1', type=str)\n    parser.add_argument('--filelists-train',\n                        default='', type=str)\n    parser.add_argument('--filelists-val',\n                        default='', type=str)\n    parser.add_argument('--root', default='')\n    parser.add_argument('--snapshot', default='', type=str)\n    parser.add_argument('--log-file', default='output.log', type=str)\n    parser.add_argument('--log-mode', default='w', type=str)\n    parser.add_argument('--size-average', default='true', type=str2bool)\n    parser.add_argument('--num-classes', default=62, type=int)\n    parser.add_argument('--arch', default='mobilenet_1', type=str,\n                        choices=arch_choices)\n    parser.add_argument('--frozen', default='false', type=str2bool)\n    parser.add_argument('--milestones', default='15,25,30', type=str)\n    parser.add_argument('--task', default='all', type=str)\n    parser.add_argument('--test_initial', default='false', type=str2bool)\n    parser.add_argument('--warmup', default=-1, type=int)\n    parser.add_argument('--param-fp-train',\n                        default='',\n                        type=str)\n    parser.add_argument('--param-fp-val',\n                        default='')\n    parser.add_argument('--opt-style', default='resample', type=str)  # resample\n    parser.add_argument('--resample-num', default=132, type=int)\n    parser.add_argument('--loss', default='vdc', type=str)\n\n    global args\n    args = parser.parse_args()\n\n    # some other operations\n    args.devices_id = [int(d) for d in args.devices_id.split(',')]\n    args.milestones = [int(m) for m in args.milestones.split(',')]\n\n    snapshot_dir = osp.split(args.snapshot)[0]\n    mkdir(snapshot_dir)\n\n\ndef print_args(args):\n    for arg in vars(args):\n        s = arg + ': ' + str(getattr(args, arg))\n        logging.info(s)\n\n\ndef adjust_learning_rate(optimizer, epoch, milestones=None):\n    \"\"\"Sets the learning rate: milestone is a list/tuple\"\"\"\n\n    def to(epoch):\n        if epoch <= args.warmup:\n            return 1\n        elif args.warmup < epoch <= milestones[0]:\n            return 0\n        for i in range(1, len(milestones)):\n            if milestones[i - 1] < epoch <= milestones[i]:\n                return i\n        return len(milestones)\n\n    n = to(epoch)\n\n    global lr\n    lr = args.base_lr * (0.2 ** n)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef save_checkpoint(state, filename='checkpoint.pth.tar'):\n    torch.save(state, filename)\n    logging.info(f'Save checkpoint to {filename}')\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    model.train()\n\n    end = time.time()\n    # loader is batch style\n    # for i, (input, target) in enumerate(train_loader):\n    for i, (input, target) in enumerate(train_loader):\n        target.requires_grad = False\n        target = target.cuda(non_blocking=True)\n        output = model(input)\n\n        data_time.update(time.time() - end)\n\n        if args.loss.lower() == 'vdc':\n            loss = criterion(output, target)\n        elif args.loss.lower() == 'wpdc':\n            loss = criterion(output, target)\n        elif args.loss.lower() == 'pdc':\n            loss = criterion(output, target)\n        else:\n            raise Exception(f'Unknown loss {args.loss}')\n\n        losses.update(loss.item(), input.size(0))\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # log\n        if i % args.print_freq == 0:\n            logging.info(f'Epoch: [{epoch}][{i}/{len(train_loader)}]\\t'\n                         f'LR: {lr:8f}\\t'\n                         f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                         # f'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                         f'Loss {losses.val:.4f} ({losses.avg:.4f})')\n\n\ndef validate(val_loader, model, criterion, epoch):\n    model.eval()\n\n    end = time.time()\n    with torch.no_grad():\n        losses = []\n        for i, (input, target) in enumerate(val_loader):\n            # compute output\n            target.requires_grad = False\n            target = target.cuda(non_blocking=True)\n            output = model(input)\n\n            loss = criterion(output, target)\n            losses.append(loss.item())\n\n        elapse = time.time() - end\n        loss = np.mean(losses)\n        logging.info(f'Val: [{epoch}][{len(val_loader)}]\\t'\n                     f'Loss {loss:.4f}\\t'\n                     f'Time {elapse:.3f}')\n\n\ndef main():\n    parse_args()  # parse global argsl\n\n    # logging setup\n    logging.basicConfig(\n        format='[%(asctime)s] [p%(process)s] [%(pathname)s:%(lineno)d] [%(levelname)s] %(message)s',\n        level=logging.INFO,\n        handlers=[\n            logging.FileHandler(args.log_file, mode=args.log_mode),\n            logging.StreamHandler()\n        ]\n    )\n\n    print_args(args)  # print args\n\n    # step1: define the model structure\n    model = getattr(mobilenet_v1, args.arch)(num_classes=args.num_classes)\n\n    torch.cuda.set_device(args.devices_id[0])  # fix bug for `ERROR: all tensors must be on devices[0]`\n\n    model = nn.DataParallel(model, device_ids=args.devices_id).cuda()  # -> GPU\n\n    # step2: optimization: loss and optimization method\n    # criterion = nn.MSELoss(size_average=args.size_average).cuda()\n    if args.loss.lower() == 'wpdc':\n        print(args.opt_style)\n        criterion = WPDCLoss(opt_style=args.opt_style).cuda()\n        logging.info('Use WPDC Loss')\n    elif args.loss.lower() == 'vdc':\n        criterion = VDCLoss(opt_style=args.opt_style).cuda()\n        logging.info('Use VDC Loss')\n    elif args.loss.lower() == 'pdc':\n        criterion = nn.MSELoss(size_average=args.size_average).cuda()\n        logging.info('Use PDC loss')\n    else:\n        raise Exception(f'Unknown Loss {args.loss}')\n\n    optimizer = torch.optim.SGD(model.parameters(),\n                                lr=args.base_lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay,\n                                nesterov=True)\n    # step 2.1 resume\n    if args.resume:\n        if Path(args.resume).is_file():\n            logging.info(f'=> loading checkpoint {args.resume}')\n\n            checkpoint = torch.load(args.resume, map_location=lambda storage, loc: storage)['state_dict']\n            # checkpoint = torch.load(args.resume)['state_dict']\n            model.load_state_dict(checkpoint)\n\n        else:\n            logging.info(f'=> no checkpoint found at {args.resume}')\n\n    # step3: data\n    normalize = NormalizeGjz(mean=127.5, std=128)  # may need optimization\n\n    train_dataset = DDFADataset(\n        root=args.root,\n        filelists=args.filelists_train,\n        param_fp=args.param_fp_train,\n        transform=transforms.Compose([ToTensorGjz(), normalize])\n    )\n    val_dataset = DDFADataset(\n        root=args.root,\n        filelists=args.filelists_val,\n        param_fp=args.param_fp_val,\n        transform=transforms.Compose([ToTensorGjz(), normalize])\n    )\n\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.workers,\n                              shuffle=True, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_dataset, batch_size=args.val_batch_size, num_workers=args.workers,\n                            shuffle=False, pin_memory=True)\n\n    # step4: run\n    cudnn.benchmark = True\n    if args.test_initial:\n        logging.info('Testing from initial')\n        validate(val_loader, model, criterion, args.start_epoch)\n\n    for epoch in range(args.start_epoch, args.epochs + 1):\n        # adjust learning rate\n        adjust_learning_rate(optimizer, epoch, args.milestones)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n        filename = f'{args.snapshot}_checkpoint_epoch_{epoch}.pth.tar'\n        save_checkpoint(\n            {\n                'epoch': epoch,\n                'state_dict': model.state_dict(),\n                # 'optimizer': optimizer.state_dict()\n            },\n            filename\n        )\n\n        validate(val_loader, model, criterion, epoch)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "IP_Adapter/ip_adapter/ip_adapter.py", "content": "import os\nfrom typing import List\n\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.pipelines.controlnet import MultiControlNetModel\nfrom PIL import Image\nfrom safetensors import safe_open\nfrom transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n\nfrom .utils import is_torch2_available, get_generator\n\nif is_torch2_available():\n    from .attention_processor import (\n        AttnProcessor2_0 as AttnProcessor,\n    )\n    from .attention_processor import (\n        CNAttnProcessor2_0 as CNAttnProcessor,\n    )\n    from .attention_processor import (\n        IPAttnProcessor2_0 as IPAttnProcessor,\n    )\nelse:\n    from .attention_processor import AttnProcessor, CNAttnProcessor, IPAttnProcessor\nfrom .resampler import Resampler\n\n\nclass ImageProjModel(torch.nn.Module):\n    \"\"\"Projection Model\"\"\"\n\n    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n        super().__init__()\n\n        self.generator = None\n        self.cross_attention_dim = cross_attention_dim\n        self.clip_extra_context_tokens = clip_extra_context_tokens\n        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n\n    def forward(self, image_embeds):\n        embeds = image_embeds\n        clip_extra_context_tokens = self.proj(embeds).reshape(\n            -1, self.clip_extra_context_tokens, self.cross_attention_dim\n        )\n        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n        return clip_extra_context_tokens\n\n\nclass MLPProjModel(torch.nn.Module):\n    \"\"\"SD model with image prompt\"\"\"\n    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024):\n        super().__init__()\n        \n        self.proj = torch.nn.Sequential(\n            torch.nn.Linear(clip_embeddings_dim, clip_embeddings_dim),\n            torch.nn.GELU(),\n            torch.nn.Linear(clip_embeddings_dim, cross_attention_dim),\n            torch.nn.LayerNorm(cross_attention_dim)\n        )\n        \n    def forward(self, image_embeds):\n        clip_extra_context_tokens = self.proj(image_embeds)\n        return clip_extra_context_tokens\n\n\nclass IPAdapter:\n    def __init__(self, sd_pipe, image_encoder_path, ip_ckpt, device, num_tokens=4):\n        self.device = device\n        self.image_encoder_path = image_encoder_path\n        self.ip_ckpt = ip_ckpt\n        self.num_tokens = num_tokens\n\n        self.pipe = sd_pipe.to(self.device)\n        self.set_ip_adapter()\n\n        # load image encoder\n        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(self.image_encoder_path).to(\n            self.device, dtype=torch.float16\n        )\n        self.clip_image_processor = CLIPImageProcessor()\n        # image proj model\n        self.image_proj_model = self.init_proj()\n\n        self.load_ip_adapter()\n\n    def init_proj(self):\n        image_proj_model = ImageProjModel(\n            cross_attention_dim=self.pipe.unet.config.cross_attention_dim,\n            clip_embeddings_dim=self.image_encoder.config.projection_dim,\n            clip_extra_context_tokens=self.num_tokens,\n        ).to(self.device, dtype=torch.float16)\n        return image_proj_model\n\n    def set_ip_adapter(self):\n        unet = self.pipe.unet\n        attn_procs = {}\n        for name in unet.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = unet.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = unet.config.block_out_channels[block_id]\n            if cross_attention_dim is None:\n                attn_procs[name] = AttnProcessor()\n            else:\n                attn_procs[name] = IPAttnProcessor(\n                    hidden_size=hidden_size,\n                    cross_attention_dim=cross_attention_dim,\n                    scale=1.0,\n                    num_tokens=self.num_tokens,\n                ).to(self.device, dtype=torch.float16)\n        unet.set_attn_processor(attn_procs)\n        if hasattr(self.pipe, \"controlnet\"):\n            if isinstance(self.pipe.controlnet, MultiControlNetModel):\n                for controlnet in self.pipe.controlnet.nets:\n                    controlnet.set_attn_processor(CNAttnProcessor(num_tokens=self.num_tokens))\n            else:\n                self.pipe.controlnet.set_attn_processor(CNAttnProcessor(num_tokens=self.num_tokens))\n\n    def load_ip_adapter(self):\n        if os.path.splitext(self.ip_ckpt)[-1] == \".safetensors\":\n            state_dict = {\"image_proj\": {}, \"ip_adapter\": {}}\n            with safe_open(self.ip_ckpt, framework=\"pt\", device=\"cpu\") as f:\n                for key in f.keys():\n                    if key.startswith(\"image_proj.\"):\n                        state_dict[\"image_proj\"][key.replace(\"image_proj.\", \"\")] = f.get_tensor(key)\n                    elif key.startswith(\"ip_adapter.\"):\n                        state_dict[\"ip_adapter\"][key.replace(\"ip_adapter.\", \"\")] = f.get_tensor(key)\n        else:\n            state_dict = torch.load(self.ip_ckpt, map_location=\"cpu\")\n        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"])\n        ip_layers = torch.nn.ModuleList(self.pipe.unet.attn_processors.values())\n        ip_layers.load_state_dict(state_dict[\"ip_adapter\"])\n\n    @torch.inference_mode()\n    def get_image_embeds(self, pil_image=None, clip_image_embeds=None):\n        if pil_image is not None:\n            if isinstance(pil_image, Image.Image):\n                pil_image = [pil_image]\n            clip_image = self.clip_image_processor(images=pil_image, return_tensors=\"pt\").pixel_values\n            clip_image_embeds = self.image_encoder(clip_image.to(self.device, dtype=torch.float16)).image_embeds\n        else:\n            clip_image_embeds = clip_image_embeds.to(self.device, dtype=torch.float16)\n        image_prompt_embeds = self.image_proj_model(clip_image_embeds)\n        uncond_image_prompt_embeds = self.image_proj_model(torch.zeros_like(clip_image_embeds))\n        return image_prompt_embeds, uncond_image_prompt_embeds\n\n    def set_scale(self, scale):\n        for attn_processor in self.pipe.unet.attn_processors.values():\n            if isinstance(attn_processor, IPAttnProcessor):\n                attn_processor.scale = scale\n\n    def generate(\n        self,\n        pil_image=None,\n        clip_image_embeds=None,\n        prompt=None,\n        negative_prompt=None,\n        scale=1.0,\n        num_samples=4,\n        seed=None,\n        guidance_scale=7.5,\n        num_inference_steps=30,\n        **kwargs,\n    ):\n        self.set_scale(scale)\n\n        if pil_image is not None:\n            num_prompts = 1 if isinstance(pil_image, Image.Image) else len(pil_image)\n        else:\n            num_prompts = clip_image_embeds.size(0)\n\n        if prompt is None:\n            prompt = \"best quality, high quality\"\n        if negative_prompt is None:\n            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\n        if not isinstance(prompt, List):\n            prompt = [prompt] * num_prompts\n        if not isinstance(negative_prompt, List):\n            negative_prompt = [negative_prompt] * num_prompts\n\n        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(\n            pil_image=pil_image, clip_image_embeds=clip_image_embeds\n        )\n        bs_embed, seq_len, _ = image_prompt_embeds.shape\n        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n\n        with torch.inference_mode():\n            # import pdb\n            # pdb.set_trace()\n            prompt_embeds_temp = self.pipe._encode_prompt(\n                prompt,\n                device=self.device,\n                num_images_per_prompt=num_samples,\n                do_classifier_free_guidance=True,\n                negative_prompt=negative_prompt,\n            )\n            prompt_embeds_, negative_prompt_embeds_ = torch.chunk(prompt_embeds_temp, 2, dim=0)\n            prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)\n\n        generator = get_generator(seed, self.device)\n\n        images = self.pipe(\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            guidance_scale=guidance_scale,\n            num_inference_steps=num_inference_steps,\n            generator=generator,\n            **kwargs,\n        ).images\n\n        return images\n\n\nclass IPAdapterXL(IPAdapter):\n    \"\"\"SDXL\"\"\"\n\n    def generate(\n        self,\n        pil_image,\n        prompt=None,\n        negative_prompt=None,\n        scale=1.0,\n        num_samples=4,\n        seed=None,\n        num_inference_steps=30,\n        **kwargs,\n    ):\n        self.set_scale(scale)\n\n        num_prompts = 1 if isinstance(pil_image, Image.Image) else len(pil_image)\n\n        if prompt is None:\n            prompt = \"best quality, high quality\"\n        if negative_prompt is None:\n            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\n        if not isinstance(prompt, List):\n            prompt = [prompt] * num_prompts\n        if not isinstance(negative_prompt, List):\n            negative_prompt = [negative_prompt] * num_prompts\n\n        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(pil_image)\n        bs_embed, seq_len, _ = image_prompt_embeds.shape\n        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n\n        with torch.inference_mode():\n            (\n                prompt_embeds,\n                negative_prompt_embeds,\n                pooled_prompt_embeds,\n                negative_pooled_prompt_embeds,\n            ) = self.pipe.encode_prompt(\n                prompt,\n                num_images_per_prompt=num_samples,\n                do_classifier_free_guidance=True,\n                negative_prompt=negative_prompt,\n            )\n            prompt_embeds = torch.cat([prompt_embeds, image_prompt_embeds], dim=1)\n            negative_prompt_embeds = torch.cat([negative_prompt_embeds, uncond_image_prompt_embeds], dim=1)\n\n        self.generator = get_generator(seed, self.device)\n        \n        images = self.pipe(\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n            num_inference_steps=num_inference_steps,\n            generator=self.generator,\n            **kwargs,\n        ).images\n\n        return images\n\n\nclass IPAdapterPlus(IPAdapter):\n    \"\"\"IP-Adapter with fine-grained features\"\"\"\n\n    def init_proj(self):\n        image_proj_model = Resampler(\n            dim=self.pipe.unet.config.cross_attention_dim,\n            depth=4,\n            dim_head=64,\n            heads=12,\n            num_queries=self.num_tokens,\n            embedding_dim=self.image_encoder.config.hidden_size,\n            output_dim=self.pipe.unet.config.cross_attention_dim,\n            ff_mult=4,\n        ).to(self.device, dtype=torch.float16)\n        return image_proj_model\n\n    @torch.inference_mode()\n    def get_image_embeds(self, pil_image=None, clip_image_embeds=None):\n        if isinstance(pil_image, Image.Image):\n            pil_image = [pil_image]\n        clip_image = self.clip_image_processor(images=pil_image, return_tensors=\"pt\").pixel_values\n        clip_image = clip_image.to(self.device, dtype=torch.float16)\n        clip_image_embeds = self.image_encoder(clip_image, output_hidden_states=True).hidden_states[-2]\n        image_prompt_embeds = self.image_proj_model(clip_image_embeds)\n        uncond_clip_image_embeds = self.image_encoder(\n            torch.zeros_like(clip_image), output_hidden_states=True\n        ).hidden_states[-2]\n        uncond_image_prompt_embeds = self.image_proj_model(uncond_clip_image_embeds)\n        return image_prompt_embeds, uncond_image_prompt_embeds\n\n\nclass IPAdapterFull(IPAdapterPlus):\n    \"\"\"IP-Adapter with full features\"\"\"\n\n    def init_proj(self):\n        image_proj_model = MLPProjModel(\n            cross_attention_dim=self.pipe.unet.config.cross_attention_dim,\n            clip_embeddings_dim=self.image_encoder.config.hidden_size,\n        ).to(self.device, dtype=torch.float16)\n        return image_proj_model\n\n\nclass IPAdapterPlusXL(IPAdapter):\n    \"\"\"SDXL\"\"\"\n\n    def init_proj(self):\n        image_proj_model = Resampler(\n            dim=1280,\n            depth=4,\n            dim_head=64,\n            heads=20,\n            num_queries=self.num_tokens,\n            embedding_dim=self.image_encoder.config.hidden_size,\n            output_dim=self.pipe.unet.config.cross_attention_dim,\n            ff_mult=4,\n        ).to(self.device, dtype=torch.float16)\n        return image_proj_model\n\n    @torch.inference_mode()\n    def get_image_embeds(self, pil_image):\n        if isinstance(pil_image, Image.Image):\n            pil_image = [pil_image]\n        clip_image = self.clip_image_processor(images=pil_image, return_tensors=\"pt\").pixel_values\n        clip_image = clip_image.to(self.device, dtype=torch.float16)\n        clip_image_embeds = self.image_encoder(clip_image, output_hidden_states=True).hidden_states[-2]\n        image_prompt_embeds = self.image_proj_model(clip_image_embeds)\n        uncond_clip_image_embeds = self.image_encoder(\n            torch.zeros_like(clip_image), output_hidden_states=True\n        ).hidden_states[-2]\n        uncond_image_prompt_embeds = self.image_proj_model(uncond_clip_image_embeds)\n        return image_prompt_embeds, uncond_image_prompt_embeds\n\n    def generate(\n        self,\n        pil_image,\n        prompt=None,\n        negative_prompt=None,\n        scale=1.0,\n        num_samples=4,\n        seed=None,\n        num_inference_steps=30,\n        **kwargs,\n    ):\n        self.set_scale(scale)\n\n        num_prompts = 1 if isinstance(pil_image, Image.Image) else len(pil_image)\n\n        if prompt is None:\n            prompt = \"best quality, high quality\"\n        if negative_prompt is None:\n            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\n        if not isinstance(prompt, List):\n            prompt = [prompt] * num_prompts\n        if not isinstance(negative_prompt, List):\n            negative_prompt = [negative_prompt] * num_prompts\n\n        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(pil_image)\n        bs_embed, seq_len, _ = image_prompt_embeds.shape\n        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n\n        with torch.inference_mode():\n            (\n                prompt_embeds,\n                negative_prompt_embeds,\n                pooled_prompt_embeds,\n                negative_pooled_prompt_embeds,\n            ) = self.pipe.encode_prompt(\n                prompt,\n                num_images_per_prompt=num_samples,\n                do_classifier_free_guidance=True,\n                negative_prompt=negative_prompt,\n            )\n            prompt_embeds = torch.cat([prompt_embeds, image_prompt_embeds], dim=1)\n            negative_prompt_embeds = torch.cat([negative_prompt_embeds, uncond_image_prompt_embeds], dim=1)\n\n        generator = get_generator(seed, self.device)\n\n        images = self.pipe(\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n            num_inference_steps=num_inference_steps,\n            generator=generator,\n            **kwargs,\n        ).images\n\n        return images\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/speed_cpu.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport timeit\nimport numpy as np\n\nSETUP_CODE = '''\nimport mobilenet_v1\nimport torch\n\nmodel = mobilenet_v1.mobilenet_1()\nmodel.eval()\ndata = torch.rand(1, 3, 120, 120)\n'''\n\nTEST_CODE = '''\nwith torch.no_grad():\n    model(data)\n'''\n\n\ndef main():\n    repeat, number = 5, 100\n    res = timeit.repeat(setup=SETUP_CODE,\n                        stmt=TEST_CODE,\n                        repeat=repeat,\n                        number=number)\n    res = np.array(res, dtype=np.float32)\n    res /= number\n    mean, var = np.mean(res), np.std(res)\n    print('Inference speed: {:.2f}{:.2f} ms'.format(mean * 1000, var * 1000))\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "IP_Adapter/ip_adapter/ip_adapter_faceid_separate.py", "content": "import os\nfrom typing import List\n\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.pipelines.controlnet import MultiControlNetModel\nfrom PIL import Image\nfrom safetensors import safe_open\nfrom transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n\nfrom .utils import is_torch2_available, get_generator\n\nUSE_DAFAULT_ATTN = False # should be True for visualization_attnmap\nif is_torch2_available() and (not USE_DAFAULT_ATTN):\n    from .attention_processor import (\n        AttnProcessor2_0 as AttnProcessor,\n    )\n    from .attention_processor import (\n        IPAttnProcessor2_0 as IPAttnProcessor,\n    )\nelse:\n    from .attention_processor import AttnProcessor, IPAttnProcessor\nfrom .resampler import PerceiverAttention, FeedForward\n\n\nclass FacePerceiverResampler(torch.nn.Module):\n    def __init__(\n        self,\n        *,\n        dim=768,\n        depth=4,\n        dim_head=64,\n        heads=16,\n        embedding_dim=1280,\n        output_dim=768,\n        ff_mult=4,\n    ):\n        super().__init__()\n        \n        self.proj_in = torch.nn.Linear(embedding_dim, dim)\n        self.proj_out = torch.nn.Linear(dim, output_dim)\n        self.norm_out = torch.nn.LayerNorm(output_dim)\n        self.layers = torch.nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                torch.nn.ModuleList(\n                    [\n                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n                        FeedForward(dim=dim, mult=ff_mult),\n                    ]\n                )\n            )\n\n    def forward(self, latents, x):\n        x = self.proj_in(x)\n        for attn, ff in self.layers:\n            latents = attn(x, latents) + latents\n            latents = ff(latents) + latents\n        latents = self.proj_out(latents)\n        return self.norm_out(latents)\n\n\nclass MLPProjModel(torch.nn.Module):\n    def __init__(self, cross_attention_dim=768, id_embeddings_dim=512, num_tokens=4):\n        super().__init__()\n        \n        self.cross_attention_dim = cross_attention_dim\n        self.num_tokens = num_tokens\n        \n        self.proj = torch.nn.Sequential(\n            torch.nn.Linear(id_embeddings_dim, id_embeddings_dim*2),\n            torch.nn.GELU(),\n            torch.nn.Linear(id_embeddings_dim*2, cross_attention_dim*num_tokens),\n        )\n        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n        \n    def forward(self, id_embeds):\n        x = self.proj(id_embeds)\n        x = x.reshape(-1, self.num_tokens, self.cross_attention_dim)\n        x = self.norm(x)\n        return x\n\n\nclass ProjPlusModel(torch.nn.Module):\n    def __init__(self, cross_attention_dim=768, id_embeddings_dim=512, clip_embeddings_dim=1280, num_tokens=4):\n        super().__init__()\n        \n        self.cross_attention_dim = cross_attention_dim\n        self.num_tokens = num_tokens\n        \n        self.proj = torch.nn.Sequential(\n            torch.nn.Linear(id_embeddings_dim, id_embeddings_dim*2),\n            torch.nn.GELU(),\n            torch.nn.Linear(id_embeddings_dim*2, cross_attention_dim*num_tokens),\n        )\n        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n        \n        self.perceiver_resampler = FacePerceiverResampler(\n            dim=cross_attention_dim,\n            depth=4,\n            dim_head=64,\n            heads=cross_attention_dim // 64,\n            embedding_dim=clip_embeddings_dim,\n            output_dim=cross_attention_dim,\n            ff_mult=4,\n        )\n        \n    def forward(self, id_embeds, clip_embeds, shortcut=False, scale=1.0):\n        \n        x = self.proj(id_embeds)\n        x = x.reshape(-1, self.num_tokens, self.cross_attention_dim)\n        x = self.norm(x)\n        out = self.perceiver_resampler(x, clip_embeds)\n        if shortcut:\n            out = x + scale * out\n        return out\n\n\nclass IPAdapterFaceID:\n    def __init__(self, sd_pipe, ip_ckpt, device, num_tokens=4, n_cond=1, torch_dtype=torch.float16):\n        self.device = device\n        self.ip_ckpt = ip_ckpt\n        self.num_tokens = num_tokens\n        self.n_cond = n_cond\n        self.torch_dtype = torch_dtype\n\n        self.pipe = sd_pipe.to(self.device)\n        self.set_ip_adapter()\n\n        # image proj model\n        self.image_proj_model = self.init_proj()\n\n        self.load_ip_adapter()\n\n    def init_proj(self):\n        image_proj_model = MLPProjModel(\n            cross_attention_dim=self.pipe.unet.config.cross_attention_dim,\n            id_embeddings_dim=512,\n            num_tokens=self.num_tokens,\n        ).to(self.device, dtype=self.torch_dtype)\n        return image_proj_model\n\n    def set_ip_adapter(self):\n        unet = self.pipe.unet\n        attn_procs = {}\n        for name in unet.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = unet.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = unet.config.block_out_channels[block_id]\n            if cross_attention_dim is None:\n                attn_procs[name] = AttnProcessor()\n            else:\n                attn_procs[name] = IPAttnProcessor(\n                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, scale=1.0, num_tokens=self.num_tokens*self.n_cond,\n                ).to(self.device, dtype=self.torch_dtype)\n        unet.set_attn_processor(attn_procs)\n\n    def load_ip_adapter(self):\n        if os.path.splitext(self.ip_ckpt)[-1] == \".safetensors\":\n            state_dict = {\"image_proj\": {}, \"ip_adapter\": {}}\n            with safe_open(self.ip_ckpt, framework=\"pt\", device=\"cpu\") as f:\n                for key in f.keys():\n                    if key.startswith(\"image_proj.\"):\n                        state_dict[\"image_proj\"][key.replace(\"image_proj.\", \"\")] = f.get_tensor(key)\n                    elif key.startswith(\"ip_adapter.\"):\n                        state_dict[\"ip_adapter\"][key.replace(\"ip_adapter.\", \"\")] = f.get_tensor(key)\n        else:\n            state_dict = torch.load(self.ip_ckpt, map_location=\"cpu\")\n        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"])\n        ip_layers = torch.nn.ModuleList(self.pipe.unet.attn_processors.values())\n        ip_layers.load_state_dict(state_dict[\"ip_adapter\"], strict=False)\n\n    @torch.inference_mode()\n    def get_image_embeds(self, faceid_embeds):\n\n        multi_face = False\n        if faceid_embeds.dim() == 3:\n            multi_face = True\n            b, n, c = faceid_embeds.shape\n            faceid_embeds = faceid_embeds.reshape(b*n, c)\n\n        faceid_embeds = faceid_embeds.to(self.device, dtype=self.torch_dtype)\n        image_prompt_embeds = self.image_proj_model(faceid_embeds)\n        uncond_image_prompt_embeds = self.image_proj_model(torch.zeros_like(faceid_embeds))\n        if multi_face:\n            c = image_prompt_embeds.size(-1)\n            image_prompt_embeds = image_prompt_embeds.reshape(b, -1, c)\n            uncond_image_prompt_embeds = uncond_image_prompt_embeds.reshape(b, -1, c)\n        \n        return image_prompt_embeds, uncond_image_prompt_embeds\n\n    def set_scale(self, scale):\n        for attn_processor in self.pipe.unet.attn_processors.values():\n            if isinstance(attn_processor, IPAttnProcessor):\n                attn_processor.scale = scale\n\n    def generate(\n        self,\n        faceid_embeds=None,\n        prompt=None,\n        negative_prompt=None,\n        scale=1.0,\n        num_samples=4,\n        seed=None,\n        guidance_scale=7.5,\n        num_inference_steps=30,\n        **kwargs,\n    ):\n        self.set_scale(scale)\n\n       \n        num_prompts = faceid_embeds.size(0)\n\n        if prompt is None:\n            prompt = \"best quality, high quality\"\n        if negative_prompt is None:\n            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\n        if not isinstance(prompt, List):\n            prompt = [prompt] * num_prompts\n        if not isinstance(negative_prompt, List):\n            negative_prompt = [negative_prompt] * num_prompts\n\n        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(faceid_embeds)\n        \n        bs_embed, seq_len, _ = image_prompt_embeds.shape\n        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n\n        with torch.inference_mode():\n            prompt_temp = self.pipe._encode_prompt(\n                prompt,\n                device=self.device,\n                num_images_per_prompt=num_samples,\n                do_classifier_free_guidance=True,\n                negative_prompt=negative_prompt,\n            )\n            negative_prompt_embeds_, prompt_embeds_ = prompt_temp.chunk(2)\n            # [4, 16, 768], [4, 77, 768] -> [4, 77+16, 768]\n\n            prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)\n\n        generator = get_generator(seed, self.device)\n\n        images = self.pipe(\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            guidance_scale=guidance_scale,\n            num_inference_steps=num_inference_steps,\n            generator=generator,\n            **kwargs,\n        ).images\n\n        return images\n\n\nclass IPAdapterFaceIDPlus:\n    def __init__(self, sd_pipe, image_encoder_path, ip_ckpt, device, num_tokens=4, torch_dtype=torch.float16):\n        self.device = device\n        self.image_encoder_path = image_encoder_path\n        self.ip_ckpt = ip_ckpt\n        self.num_tokens = num_tokens\n        self.torch_dtype = torch_dtype\n\n        self.pipe = sd_pipe.to(self.device)\n        self.set_ip_adapter()\n\n        # load image encoder\n        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(self.image_encoder_path).to(\n            self.device, dtype=self.torch_dtype\n        )\n        self.clip_image_processor = CLIPImageProcessor()\n        # image proj model\n        self.image_proj_model = self.init_proj()\n\n        self.load_ip_adapter()\n\n    def init_proj(self):\n        image_proj_model = ProjPlusModel(\n            cross_attention_dim=self.pipe.unet.config.cross_attention_dim,\n            id_embeddings_dim=512,\n            clip_embeddings_dim=self.image_encoder.config.hidden_size,\n            num_tokens=self.num_tokens,\n        ).to(self.device, dtype=self.torch_dtype)\n        return image_proj_model\n\n    def set_ip_adapter(self):\n        unet = self.pipe.unet\n        attn_procs = {}\n        for name in unet.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = unet.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = unet.config.block_out_channels[block_id]\n            if cross_attention_dim is None:\n                attn_procs[name] = AttnProcessor()\n            else:\n                attn_procs[name] = IPAttnProcessor(\n                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, scale=1.0, num_tokens=self.num_tokens,\n                ).to(self.device, dtype=self.torch_dtype)\n        unet.set_attn_processor(attn_procs)\n\n    def load_ip_adapter(self):\n        if os.path.splitext(self.ip_ckpt)[-1] == \".safetensors\":\n            state_dict = {\"image_proj\": {}, \"ip_adapter\": {}}\n            with safe_open(self.ip_ckpt, framework=\"pt\", device=\"cpu\") as f:\n                for key in f.keys():\n                    if key.startswith(\"image_proj.\"):\n                        state_dict[\"image_proj\"][key.replace(\"image_proj.\", \"\")] = f.get_tensor(key)\n                    elif key.startswith(\"ip_adapter.\"):\n                        state_dict[\"ip_adapter\"][key.replace(\"ip_adapter.\", \"\")] = f.get_tensor(key)\n        else:\n            state_dict = torch.load(self.ip_ckpt, map_location=\"cpu\")\n        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"])\n        ip_layers = torch.nn.ModuleList(self.pipe.unet.attn_processors.values())\n        ip_layers.load_state_dict(state_dict[\"ip_adapter\"], strict=False)\n\n    @torch.inference_mode()\n    def get_image_embeds(self, faceid_embeds, face_image, s_scale, shortcut):\n        if isinstance(face_image, Image.Image):\n            pil_image = [face_image]\n        clip_image = self.clip_image_processor(images=face_image, return_tensors=\"pt\").pixel_values\n        clip_image = clip_image.to(self.device, dtype=self.torch_dtype)\n        clip_image_embeds = self.image_encoder(clip_image, output_hidden_states=True).hidden_states[-2]\n        uncond_clip_image_embeds = self.image_encoder(\n            torch.zeros_like(clip_image), output_hidden_states=True\n        ).hidden_states[-2]\n        \n        faceid_embeds = faceid_embeds.to(self.device, dtype=self.torch_dtype)\n        image_prompt_embeds = self.image_proj_model(faceid_embeds, clip_image_embeds, shortcut=shortcut, scale=s_scale)\n        uncond_image_prompt_embeds = self.image_proj_model(torch.zeros_like(faceid_embeds), uncond_clip_image_embeds, shortcut=shortcut, scale=s_scale)\n        return image_prompt_embeds, uncond_image_prompt_embeds\n\n    def set_scale(self, scale):\n        for attn_processor in self.pipe.unet.attn_processors.values():\n            if isinstance(attn_processor, LoRAIPAttnProcessor):\n                attn_processor.scale = scale\n\n    def generate(\n        self,\n        face_image=None,\n        faceid_embeds=None,\n        prompt=None,\n        negative_prompt=None,\n        scale=1.0,\n        num_samples=4,\n        seed=None,\n        guidance_scale=7.5,\n        num_inference_steps=30,\n        s_scale=1.0,\n        shortcut=False,\n        **kwargs,\n    ):\n        self.set_scale(scale)\n\n       \n        num_prompts = faceid_embeds.size(0)\n\n        if prompt is None:\n            prompt = \"best quality, high quality\"\n        if negative_prompt is None:\n            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\n        if not isinstance(prompt, List):\n            prompt = [prompt] * num_prompts\n        if not isinstance(negative_prompt, List):\n            negative_prompt = [negative_prompt] * num_prompts\n\n        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(faceid_embeds, face_image, s_scale, shortcut)\n\n        bs_embed, seq_len, _ = image_prompt_embeds.shape\n        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n\n        with torch.inference_mode():\n            prompt_embeds_, negative_prompt_embeds_ = self.pipe.encode_prompt(\n                prompt,\n                device=self.device,\n                num_images_per_prompt=num_samples,\n                do_classifier_free_guidance=True,\n                negative_prompt=negative_prompt,\n            )\n            prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)\n\n        generator = get_generator(seed, self.device)\n\n        images = self.pipe(\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            guidance_scale=guidance_scale,\n            num_inference_steps=num_inference_steps,\n            generator=generator,\n            **kwargs,\n        ).images\n\n        return images\n\n\nclass IPAdapterFaceIDXL(IPAdapterFaceID):\n    \"\"\"SDXL\"\"\"\n\n    def generate(\n        self,\n        faceid_embeds=None,\n        prompt=None,\n        negative_prompt=None,\n        scale=1.0,\n        num_samples=4,\n        seed=None,\n        num_inference_steps=30,\n        **kwargs,\n    ):\n        self.set_scale(scale)\n\n        num_prompts = faceid_embeds.size(0)\n\n        if prompt is None:\n            prompt = \"best quality, high quality\"\n        if negative_prompt is None:\n            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\n        if not isinstance(prompt, List):\n            prompt = [prompt] * num_prompts\n        if not isinstance(negative_prompt, List):\n            negative_prompt = [negative_prompt] * num_prompts\n\n        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(faceid_embeds)\n\n        bs_embed, seq_len, _ = image_prompt_embeds.shape\n        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n\n        with torch.inference_mode():\n            (\n                prompt_embeds,\n                negative_prompt_embeds,\n                pooled_prompt_embeds,\n                negative_pooled_prompt_embeds,\n            ) = self.pipe.encode_prompt(\n                prompt,\n                num_images_per_prompt=num_samples,\n                do_classifier_free_guidance=True,\n                negative_prompt=negative_prompt,\n            )\n            prompt_embeds = torch.cat([prompt_embeds, image_prompt_embeds], dim=1)\n            negative_prompt_embeds = torch.cat([negative_prompt_embeds, uncond_image_prompt_embeds], dim=1)\n\n        generator = get_generator(seed, self.device)\n\n        images = self.pipe(\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n            num_inference_steps=num_inference_steps,\n            generator=generator,\n            **kwargs,\n        ).images\n\n        return images\n\n\nclass IPAdapterFaceIDPlusXL(IPAdapterFaceIDPlus):\n    \"\"\"SDXL\"\"\"\n\n    def generate(\n        self,\n        face_image=None,\n        faceid_embeds=None,\n        prompt=None,\n        negative_prompt=None,\n        scale=1.0,\n        num_samples=4,\n        seed=None,\n        guidance_scale=7.5,\n        num_inference_steps=30,\n        s_scale=1.0,\n        shortcut=True,\n        **kwargs,\n    ):\n        self.set_scale(scale)\n\n        num_prompts = faceid_embeds.size(0)\n\n        if prompt is None:\n            prompt = \"best quality, high quality\"\n        if negative_prompt is None:\n            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\n        if not isinstance(prompt, List):\n            prompt = [prompt] * num_prompts\n        if not isinstance(negative_prompt, List):\n            negative_prompt = [negative_prompt] * num_prompts\n\n        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(faceid_embeds, face_image, s_scale, shortcut)\n\n        bs_embed, seq_len, _ = image_prompt_embeds.shape\n        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n\n        with torch.inference_mode():\n            (\n                prompt_embeds,\n                negative_prompt_embeds,\n                pooled_prompt_embeds,\n                negative_pooled_prompt_embeds,\n            ) = self.pipe.encode_prompt(\n                prompt,\n                num_images_per_prompt=num_samples,\n                do_classifier_free_guidance=True,\n                negative_prompt=negative_prompt,\n            )\n            prompt_embeds = torch.cat([prompt_embeds, image_prompt_embeds], dim=1)\n            negative_prompt_embeds = torch.cat([negative_prompt_embeds, uncond_image_prompt_embeds], dim=1)\n\n        generator = get_generator(seed, self.device)\n\n        images = self.pipe(\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n            num_inference_steps=num_inference_steps,\n            generator=generator,\n            guidance_scale=guidance_scale,\n            **kwargs,\n        ).images\n\n        return images\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/c++/convert_to_onnx.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport torch\nimport mobilenet_v1\n\n\ndef main():\n    # checkpoint_fp = 'weights/phase1_wpdc_vdc.pth.tar'\n    checkpoint_fp = 'weights/mb_1.p'\n    arch = 'mobilenet_1'\n    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)['state_dict']\n    model = getattr(mobilenet_v1, arch)(num_classes=62)  # 62 = 12(pose) + 40(shape) +10(expression)\n\n    model_dict = model.state_dict()\n    # because the model is trained by multiple gpus, prefix module should be removed\n    for k in checkpoint.keys():\n        kc = k.replace('module.', '')\n        if kc in model_dict.keys():\n            model_dict[kc] = checkpoint[k]\n        if kc in ['fc_param.bias', 'fc_param.weight']:\n            model_dict[kc.replace('_param', '')] = checkpoint[k]\n    model.load_state_dict(model_dict)\n\n    # conversion\n    batch_size = 1\n    dummy_input = torch.randn(batch_size, 3, 120, 120)\n    torch.onnx.export(model, dummy_input, checkpoint_fp.replace('.p', '.onnx'))\n    # torch.onnx.export(model, dummy_input, checkpoint_fp.replace('.pth.tar', '.onnx'))\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "IP_Adapter/ip_adapter/ip_adapter_faceid.py", "content": "import os\nfrom typing import List\n\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers.pipelines.controlnet import MultiControlNetModel\nfrom PIL import Image\nfrom safetensors import safe_open\nfrom transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n\nfrom .attention_processor_faceid import LoRAAttnProcessor, LoRAIPAttnProcessor\nfrom .utils import is_torch2_available, get_generator\n\nUSE_DAFAULT_ATTN = False # should be True for visualization_attnmap\nif is_torch2_available() and (not USE_DAFAULT_ATTN):\n    from .attention_processor_faceid import (\n        LoRAAttnProcessor2_0 as LoRAAttnProcessor,\n    )\n    from .attention_processor_faceid import (\n        LoRAIPAttnProcessor2_0 as LoRAIPAttnProcessor,\n    )\nelse:\n    from .attention_processor_faceid import LoRAAttnProcessor, LoRAIPAttnProcessor\nfrom .resampler import PerceiverAttention, FeedForward\n\n\nclass FacePerceiverResampler(torch.nn.Module):\n    def __init__(\n        self,\n        *,\n        dim=768,\n        depth=4,\n        dim_head=64,\n        heads=16,\n        embedding_dim=1280,\n        output_dim=768,\n        ff_mult=4,\n    ):\n        super().__init__()\n        \n        self.proj_in = torch.nn.Linear(embedding_dim, dim)\n        self.proj_out = torch.nn.Linear(dim, output_dim)\n        self.norm_out = torch.nn.LayerNorm(output_dim)\n        self.layers = torch.nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                torch.nn.ModuleList(\n                    [\n                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n                        FeedForward(dim=dim, mult=ff_mult),\n                    ]\n                )\n            )\n\n    def forward(self, latents, x):\n        x = self.proj_in(x)\n        for attn, ff in self.layers:\n            latents = attn(x, latents) + latents\n            latents = ff(latents) + latents\n        latents = self.proj_out(latents)\n        return self.norm_out(latents)\n\n\nclass MLPProjModel(torch.nn.Module):\n    def __init__(self, cross_attention_dim=768, id_embeddings_dim=512, num_tokens=4):\n        super().__init__()\n        \n        self.cross_attention_dim = cross_attention_dim\n        self.num_tokens = num_tokens\n        \n        self.proj = torch.nn.Sequential(\n            torch.nn.Linear(id_embeddings_dim, id_embeddings_dim*2),\n            torch.nn.GELU(),\n            torch.nn.Linear(id_embeddings_dim*2, cross_attention_dim*num_tokens),\n        )\n        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n        \n    def forward(self, id_embeds):\n        x = self.proj(id_embeds)\n        x = x.reshape(-1, self.num_tokens, self.cross_attention_dim)\n        x = self.norm(x)\n        return x\n\n\nclass ProjPlusModel(torch.nn.Module):\n    def __init__(self, cross_attention_dim=768, id_embeddings_dim=512, clip_embeddings_dim=1280, num_tokens=4):\n        super().__init__()\n        \n        self.cross_attention_dim = cross_attention_dim\n        self.num_tokens = num_tokens\n        \n        self.proj = torch.nn.Sequential(\n            torch.nn.Linear(id_embeddings_dim, id_embeddings_dim*2),\n            torch.nn.GELU(),\n            torch.nn.Linear(id_embeddings_dim*2, cross_attention_dim*num_tokens),\n        )\n        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n        \n        self.perceiver_resampler = FacePerceiverResampler(\n            dim=cross_attention_dim,\n            depth=4,\n            dim_head=64,\n            heads=cross_attention_dim // 64,\n            embedding_dim=clip_embeddings_dim,\n            output_dim=cross_attention_dim,\n            ff_mult=4,\n        )\n        \n    def forward(self, id_embeds, clip_embeds, shortcut=False, scale=1.0):\n        \n        x = self.proj(id_embeds)\n        x = x.reshape(-1, self.num_tokens, self.cross_attention_dim)\n        x = self.norm(x)\n        out = self.perceiver_resampler(x, clip_embeds)\n        if shortcut:\n            out = x + scale * out\n        return out\n\n\nclass IPAdapterFaceID:\n    def __init__(self, sd_pipe, ip_ckpt, device, lora_rank=128, num_tokens=4, torch_dtype=torch.float16):\n        self.device = device\n        self.ip_ckpt = ip_ckpt\n        self.lora_rank = lora_rank\n        self.num_tokens = num_tokens\n        self.torch_dtype = torch_dtype\n\n        self.pipe = sd_pipe.to(self.device)\n        self.set_ip_adapter()\n\n        # image proj model\n        self.image_proj_model = self.init_proj()\n\n        self.load_ip_adapter()\n\n    def init_proj(self):\n        image_proj_model = MLPProjModel(\n            cross_attention_dim=self.pipe.unet.config.cross_attention_dim,\n            id_embeddings_dim=512,\n            num_tokens=self.num_tokens,\n        ).to(self.device, dtype=self.torch_dtype)\n        return image_proj_model\n\n    def set_ip_adapter(self):\n        unet = self.pipe.unet\n        attn_procs = {}\n        for name in unet.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = unet.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = unet.config.block_out_channels[block_id]\n            if cross_attention_dim is None:\n                attn_procs[name] = LoRAAttnProcessor(\n                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, rank=self.lora_rank,\n                ).to(self.device, dtype=self.torch_dtype)\n            else:\n                attn_procs[name] = LoRAIPAttnProcessor(\n                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, scale=1.0, rank=self.lora_rank, num_tokens=self.num_tokens,\n                ).to(self.device, dtype=self.torch_dtype)\n        unet.set_attn_processor(attn_procs)\n\n    def load_ip_adapter(self):\n        if os.path.splitext(self.ip_ckpt)[-1] == \".safetensors\":\n            state_dict = {\"image_proj\": {}, \"ip_adapter\": {}}\n            with safe_open(self.ip_ckpt, framework=\"pt\", device=\"cpu\") as f:\n                for key in f.keys():\n                    if key.startswith(\"image_proj.\"):\n                        state_dict[\"image_proj\"][key.replace(\"image_proj.\", \"\")] = f.get_tensor(key)\n                    elif key.startswith(\"ip_adapter.\"):\n                        state_dict[\"ip_adapter\"][key.replace(\"ip_adapter.\", \"\")] = f.get_tensor(key)\n        else:\n            state_dict = torch.load(self.ip_ckpt, map_location=\"cpu\")\n        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"])\n        ip_layers = torch.nn.ModuleList(self.pipe.unet.attn_processors.values())\n        ip_layers.load_state_dict(state_dict[\"ip_adapter\"])\n\n    @torch.inference_mode()\n    def get_image_embeds(self, faceid_embeds):\n        \n        faceid_embeds = faceid_embeds.to(self.device, dtype=self.torch_dtype)\n        image_prompt_embeds = self.image_proj_model(faceid_embeds)\n        uncond_image_prompt_embeds = self.image_proj_model(torch.zeros_like(faceid_embeds))\n        return image_prompt_embeds, uncond_image_prompt_embeds\n\n    def set_scale(self, scale):\n        for attn_processor in self.pipe.unet.attn_processors.values():\n            if isinstance(attn_processor, LoRAIPAttnProcessor):\n                attn_processor.scale = scale\n\n    def generate(\n        self,\n        faceid_embeds=None,\n        prompt=None,\n        negative_prompt=None,\n        scale=1.0,\n        num_samples=4,\n        seed=None,\n        guidance_scale=7.5,\n        num_inference_steps=30,\n        **kwargs,\n    ):\n        self.set_scale(scale)\n\n       \n        num_prompts = faceid_embeds.size(0)\n\n        if prompt is None:\n            prompt = \"best quality, high quality\"\n        if negative_prompt is None:\n            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\n        if not isinstance(prompt, List):\n            prompt = [prompt] * num_prompts\n        if not isinstance(negative_prompt, List):\n            negative_prompt = [negative_prompt] * num_prompts\n\n        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(faceid_embeds)\n\n        bs_embed, seq_len, _ = image_prompt_embeds.shape\n        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n\n        with torch.inference_mode():\n            prompt_embeds_, negative_prompt_embeds_ = self.pipe.encode_prompt(\n                prompt,\n                device=self.device,\n                num_images_per_prompt=num_samples,\n                do_classifier_free_guidance=True,\n                negative_prompt=negative_prompt,\n            )\n            prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)\n\n        generator = get_generator(seed, self.device)\n\n        images = self.pipe(\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            guidance_scale=guidance_scale,\n            num_inference_steps=num_inference_steps,\n            generator=generator,\n            **kwargs,\n        ).images\n\n        return images\n\n\nclass IPAdapterFaceIDPlus:\n    def __init__(self, sd_pipe, image_encoder_path, ip_ckpt, device, lora_rank=128, num_tokens=4, torch_dtype=torch.float16):\n        self.device = device\n        self.image_encoder_path = image_encoder_path\n        self.ip_ckpt = ip_ckpt\n        self.lora_rank = lora_rank\n        self.num_tokens = num_tokens\n        self.torch_dtype = torch_dtype\n\n        self.pipe = sd_pipe.to(self.device)\n        self.set_ip_adapter()\n\n        # load image encoder\n        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(self.image_encoder_path).to(\n            self.device, dtype=self.torch_dtype\n        )\n        self.clip_image_processor = CLIPImageProcessor()\n        # image proj model\n        self.image_proj_model = self.init_proj()\n\n        self.load_ip_adapter()\n\n    def init_proj(self):\n        image_proj_model = ProjPlusModel(\n            cross_attention_dim=self.pipe.unet.config.cross_attention_dim,\n            id_embeddings_dim=512,\n            clip_embeddings_dim=self.image_encoder.config.hidden_size,\n            num_tokens=self.num_tokens,\n        ).to(self.device, dtype=self.torch_dtype)\n        return image_proj_model\n\n    def set_ip_adapter(self):\n        unet = self.pipe.unet\n        attn_procs = {}\n        for name in unet.attn_processors.keys():\n            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n            if name.startswith(\"mid_block\"):\n                hidden_size = unet.config.block_out_channels[-1]\n            elif name.startswith(\"up_blocks\"):\n                block_id = int(name[len(\"up_blocks.\")])\n                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n            elif name.startswith(\"down_blocks\"):\n                block_id = int(name[len(\"down_blocks.\")])\n                hidden_size = unet.config.block_out_channels[block_id]\n            if cross_attention_dim is None:\n                attn_procs[name] = LoRAAttnProcessor(\n                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, rank=self.lora_rank,\n                ).to(self.device, dtype=self.torch_dtype)\n            else:\n                attn_procs[name] = LoRAIPAttnProcessor(\n                    hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, scale=1.0, rank=self.lora_rank, num_tokens=self.num_tokens,\n                ).to(self.device, dtype=self.torch_dtype)\n        unet.set_attn_processor(attn_procs)\n\n    def load_ip_adapter(self):\n        if os.path.splitext(self.ip_ckpt)[-1] == \".safetensors\":\n            state_dict = {\"image_proj\": {}, \"ip_adapter\": {}}\n            with safe_open(self.ip_ckpt, framework=\"pt\", device=\"cpu\") as f:\n                for key in f.keys():\n                    if key.startswith(\"image_proj.\"):\n                        state_dict[\"image_proj\"][key.replace(\"image_proj.\", \"\")] = f.get_tensor(key)\n                    elif key.startswith(\"ip_adapter.\"):\n                        state_dict[\"ip_adapter\"][key.replace(\"ip_adapter.\", \"\")] = f.get_tensor(key)\n        else:\n            state_dict = torch.load(self.ip_ckpt, map_location=\"cpu\")\n        self.image_proj_model.load_state_dict(state_dict[\"image_proj\"])\n        ip_layers = torch.nn.ModuleList(self.pipe.unet.attn_processors.values())\n        ip_layers.load_state_dict(state_dict[\"ip_adapter\"])\n\n    @torch.inference_mode()\n    def get_image_embeds(self, faceid_embeds, face_image, s_scale, shortcut):\n        if isinstance(face_image, Image.Image):\n            pil_image = [face_image]\n        clip_image = self.clip_image_processor(images=face_image, return_tensors=\"pt\").pixel_values\n        clip_image = clip_image.to(self.device, dtype=self.torch_dtype)\n        clip_image_embeds = self.image_encoder(clip_image, output_hidden_states=True).hidden_states[-2]\n        uncond_clip_image_embeds = self.image_encoder(\n            torch.zeros_like(clip_image), output_hidden_states=True\n        ).hidden_states[-2]\n        \n        faceid_embeds = faceid_embeds.to(self.device, dtype=self.torch_dtype)\n        image_prompt_embeds = self.image_proj_model(faceid_embeds, clip_image_embeds, shortcut=shortcut, scale=s_scale)\n        uncond_image_prompt_embeds = self.image_proj_model(torch.zeros_like(faceid_embeds), uncond_clip_image_embeds, shortcut=shortcut, scale=s_scale)\n        return image_prompt_embeds, uncond_image_prompt_embeds\n\n    def set_scale(self, scale):\n        for attn_processor in self.pipe.unet.attn_processors.values():\n            if isinstance(attn_processor, LoRAIPAttnProcessor):\n                attn_processor.scale = scale\n\n    def generate(\n        self,\n        face_image=None,\n        faceid_embeds=None,\n        prompt=None,\n        negative_prompt=None,\n        scale=1.0,\n        num_samples=4,\n        seed=None,\n        guidance_scale=7.5,\n        num_inference_steps=30,\n        s_scale=1.0,\n        shortcut=False,\n        **kwargs,\n    ):\n        self.set_scale(scale)\n\n       \n        num_prompts = faceid_embeds.size(0)\n\n        if prompt is None:\n            prompt = \"best quality, high quality\"\n        if negative_prompt is None:\n            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\n        if not isinstance(prompt, List):\n            prompt = [prompt] * num_prompts\n        if not isinstance(negative_prompt, List):\n            negative_prompt = [negative_prompt] * num_prompts\n\n        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(faceid_embeds, face_image, s_scale, shortcut)\n\n        bs_embed, seq_len, _ = image_prompt_embeds.shape\n        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n\n        with torch.inference_mode():\n            prompt_embeds_, negative_prompt_embeds_ = self.pipe.encode_prompt(\n                prompt,\n                device=self.device,\n                num_images_per_prompt=num_samples,\n                do_classifier_free_guidance=True,\n                negative_prompt=negative_prompt,\n            )\n            prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)\n\n        generator = get_generator(seed, self.device)\n\n        images = self.pipe(\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            guidance_scale=guidance_scale,\n            num_inference_steps=num_inference_steps,\n            generator=generator,\n            **kwargs,\n        ).images\n\n        return images\n\n\nclass IPAdapterFaceIDXL(IPAdapterFaceID):\n    \"\"\"SDXL\"\"\"\n\n    def generate(\n        self,\n        faceid_embeds=None,\n        prompt=None,\n        negative_prompt=None,\n        scale=1.0,\n        num_samples=4,\n        seed=None,\n        num_inference_steps=30,\n        **kwargs,\n    ):\n        self.set_scale(scale)\n\n        num_prompts = faceid_embeds.size(0)\n\n        if prompt is None:\n            prompt = \"best quality, high quality\"\n        if negative_prompt is None:\n            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\n        if not isinstance(prompt, List):\n            prompt = [prompt] * num_prompts\n        if not isinstance(negative_prompt, List):\n            negative_prompt = [negative_prompt] * num_prompts\n\n        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(faceid_embeds)\n\n        bs_embed, seq_len, _ = image_prompt_embeds.shape\n        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n\n        with torch.inference_mode():\n            (\n                prompt_embeds,\n                negative_prompt_embeds,\n                pooled_prompt_embeds,\n                negative_pooled_prompt_embeds,\n            ) = self.pipe.encode_prompt(\n                prompt,\n                num_images_per_prompt=num_samples,\n                do_classifier_free_guidance=True,\n                negative_prompt=negative_prompt,\n            )\n            prompt_embeds = torch.cat([prompt_embeds, image_prompt_embeds], dim=1)\n            negative_prompt_embeds = torch.cat([negative_prompt_embeds, uncond_image_prompt_embeds], dim=1)\n\n        generator = get_generator(seed, self.device)\n\n        images = self.pipe(\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n            num_inference_steps=num_inference_steps,\n            generator=generator,\n            **kwargs,\n        ).images\n\n        return images\n\n\nclass IPAdapterFaceIDPlusXL(IPAdapterFaceIDPlus):\n    \"\"\"SDXL\"\"\"\n\n    def generate(\n        self,\n        face_image=None,\n        faceid_embeds=None,\n        prompt=None,\n        negative_prompt=None,\n        scale=1.0,\n        num_samples=4,\n        seed=None,\n        guidance_scale=7.5,\n        num_inference_steps=30,\n        s_scale=1.0,\n        shortcut=True,\n        **kwargs,\n    ):\n        self.set_scale(scale)\n\n        num_prompts = faceid_embeds.size(0)\n\n        if prompt is None:\n            prompt = \"best quality, high quality\"\n        if negative_prompt is None:\n            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n\n        if not isinstance(prompt, List):\n            prompt = [prompt] * num_prompts\n        if not isinstance(negative_prompt, List):\n            negative_prompt = [negative_prompt] * num_prompts\n\n        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(faceid_embeds, face_image, s_scale, shortcut)\n\n        bs_embed, seq_len, _ = image_prompt_embeds.shape\n        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n\n        with torch.inference_mode():\n            (\n                prompt_embeds,\n                negative_prompt_embeds,\n                pooled_prompt_embeds,\n                negative_pooled_prompt_embeds,\n            ) = self.pipe.encode_prompt(\n                prompt,\n                num_images_per_prompt=num_samples,\n                do_classifier_free_guidance=True,\n                negative_prompt=negative_prompt,\n            )\n            prompt_embeds = torch.cat([prompt_embeds, image_prompt_embeds], dim=1)\n            negative_prompt_embeds = torch.cat([negative_prompt_embeds, uncond_image_prompt_embeds], dim=1)\n\n        generator = get_generator(seed, self.device)\n\n        images = self.pipe(\n            prompt_embeds=prompt_embeds,\n            negative_prompt_embeds=negative_prompt_embeds,\n            pooled_prompt_embeds=pooled_prompt_embeds,\n            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n            num_inference_steps=num_inference_steps,\n            generator=generator,\n            guidance_scale=guidance_scale,\n            **kwargs,\n        ).images\n\n        return images\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/benchmark_aflw.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport os.path as osp\nimport numpy as np\nfrom math import sqrt\nfrom utils.io import _load\n\nd = 'test.configs'\nyaw_list = _load(osp.join(d, 'AFLW_GT_crop_yaws.npy'))\nroi_boxs = _load(osp.join(d, 'AFLW_GT_crop_roi_box.npy'))\npts68_all = _load(osp.join(d, 'AFLW_GT_pts68.npy'))\npts21_all = _load(osp.join(d, 'AFLW_GT_pts21.npy'))\n\n\ndef ana(nme_list):\n    yaw_list_abs = np.abs(yaw_list)\n    ind_yaw_1 = yaw_list_abs <= 30\n    ind_yaw_2 = np.bitwise_and(yaw_list_abs > 30, yaw_list_abs <= 60)\n    ind_yaw_3 = yaw_list_abs > 60\n\n    nme_1 = nme_list[ind_yaw_1]\n    nme_2 = nme_list[ind_yaw_2]\n    nme_3 = nme_list[ind_yaw_3]\n\n    mean_nme_1 = np.mean(nme_1) * 100\n    mean_nme_2 = np.mean(nme_2) * 100\n    mean_nme_3 = np.mean(nme_3) * 100\n    # mean_nme_all = np.mean(nme_list) * 100\n\n    std_nme_1 = np.std(nme_1) * 100\n    std_nme_2 = np.std(nme_2) * 100\n    std_nme_3 = np.std(nme_3) * 100\n    # std_nme_all = np.std(nme_list) * 100\n\n    mean_all = [mean_nme_1, mean_nme_2, mean_nme_3]\n    mean = np.mean(mean_all)\n    std = np.std(mean_all)\n\n    s1 = '[ 0, 30]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_1, std_nme_1)\n    s2 = '[30, 60]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_2, std_nme_2)\n    s3 = '[60, 90]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_3, std_nme_3)\n    # s4 = '[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_all, std_nme_all)\n    s5 = '[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: \\x1b[31m{:.3f}\\x1b[0m'.format(mean, std)\n\n    s = '\\n'.join([s1, s2, s3, s5])\n    print(s)\n\n    return mean_nme_1, mean_nme_2, mean_nme_3, mean, std\n\n\ndef calc_nme(pts68_fit_all):\n    std_size = 120\n    ind_68to21 = [[18], [20], [22], [23], [25], [27], [37], [37, 38, 39, 40, 41, 42], [40], [43],\n                  [43, 44, 45, 46, 47, 48],\n                  [46], [3], [32], [31], [36], [15], [49], [61, 62, 63, 64, 65, 66, 67, 68], [55], [9]]\n    for i in range(len(ind_68to21)):\n        for j in range(len(ind_68to21[i])):\n            ind_68to21[i][j] -= 1\n\n    nme_list = []\n\n    for i in range(len(roi_boxs)):\n        pts68_fit = pts68_fit_all[i]\n        pts68_gt = pts68_all[i]\n        pts21_gt = pts21_all[i]\n\n        # reconstruct 68 pts\n        sx, sy, ex, ey = roi_boxs[i]\n        scale_x = (ex - sx) / std_size\n        scale_y = (ey - sy) / std_size\n        pts68_fit[0, :] = pts68_fit[0, :] * scale_x + sx\n        pts68_fit[1, :] = pts68_fit[1, :] * scale_y + sy\n\n        # pts68 -> pts21\n        pts21_est = np.zeros_like(pts21_gt, dtype=np.float32)\n        for i in range(21):\n            ind = ind_68to21[i]\n            tmp = np.mean(pts68_fit[:, ind], 1)\n            pts21_est[:, i] = tmp\n\n        # build bbox\n        minx, maxx = np.min(pts68_gt[0, :]), np.max(pts68_gt[0, :])\n        miny, maxy = np.min(pts68_gt[1, :]), np.max(pts68_gt[1, :])\n        llength = sqrt((maxx - minx) * (maxy - miny))\n\n        # nme\n        pt_valid = (pts21_gt[0, :] != -1) & (pts21_gt[1, :] != -1)\n        dis = pts21_est[:, pt_valid] - pts21_gt[:, pt_valid]\n        dis = np.sqrt(np.sum(np.power(dis, 2), 0))\n        dis = np.mean(dis)\n        nme = dis / llength\n        nme_list.append(nme)\n\n    nme_list = np.array(nme_list, dtype=np.float32)\n    return nme_list\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/main.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\n__author__ = 'cleardusk'\n\n\"\"\"\nThe pipeline of 3DDFA prediction: given one image, predict the 3d face vertices, 68 landmarks and visualization.\n\n[todo]\n1. CPU optimization: https://pmchojnacki.wordpress.com/2018/10/07/slow-pytorch-cpu-performance\n\"\"\"\n\nimport torch\nimport torchvision.transforms as transforms\nimport mobilenet_v1\nimport numpy as np\nimport cv2\nimport dlib\nfrom utils.ddfa import ToTensorGjz, NormalizeGjz, str2bool\nimport scipy.io as sio\nfrom utils.inference import get_suffix, parse_roi_box_from_landmark, crop_img, predict_68pts, dump_to_ply, dump_vertex, \\\n    draw_landmarks, predict_dense, parse_roi_box_from_bbox, get_colors, write_obj_with_colors\nfrom utils.cv_plot import plot_pose_box\nfrom utils.estimate_pose import parse_pose\nfrom utils.render import get_depths_image, cget_depths_image, cpncc\nfrom utils.paf import gen_img_paf\nimport argparse\nimport torch.backends.cudnn as cudnn\n\nSTD_SIZE = 120\n\n\ndef main(args):\n    # 1. load pre-tained model\n    checkpoint_fp = 'models/phase1_wpdc_vdc.pth.tar'\n    arch = 'mobilenet_1'\n\n    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)['state_dict']\n    model = getattr(mobilenet_v1, arch)(num_classes=62)  # 62 = 12(pose) + 40(shape) +10(expression)\n\n    model_dict = model.state_dict()\n    # because the model is trained by multiple gpus, prefix module should be removed\n    for k in checkpoint.keys():\n        model_dict[k.replace('module.', '')] = checkpoint[k]\n    model.load_state_dict(model_dict)\n    if args.mode == 'gpu':\n        cudnn.benchmark = True\n        model = model.cuda()\n    model.eval()\n\n    # 2. load dlib model for face detection and landmark used for face cropping\n    if args.dlib_landmark:\n        dlib_landmark_model = 'models/shape_predictor_68_face_landmarks.dat'\n        face_regressor = dlib.shape_predictor(dlib_landmark_model)\n    if args.dlib_bbox:\n        face_detector = dlib.get_frontal_face_detector()\n\n    # 3. forward\n    tri = sio.loadmat('visualize/tri.mat')['tri']\n    transform = transforms.Compose([ToTensorGjz(), NormalizeGjz(mean=127.5, std=128)])\n    for img_fp in args.files:\n        img_ori = cv2.imread(img_fp)\n        if args.dlib_bbox:\n            rects = face_detector(img_ori, 1)\n        else:\n            rects = []\n\n        if len(rects) == 0:\n            rects = dlib.rectangles()\n            rect_fp = img_fp + '.bbox'\n            lines = open(rect_fp).read().strip().split('\\n')[1:]\n            for l in lines:\n                l, r, t, b = [int(_) for _ in l.split(' ')[1:]]\n                rect = dlib.rectangle(l, r, t, b)\n                rects.append(rect)\n\n        pts_res = []\n        Ps = []  # Camera matrix collection\n        poses = []  # pose collection, [todo: validate it]\n        vertices_lst = []  # store multiple face vertices\n        ind = 0\n        suffix = get_suffix(img_fp)\n        for rect in rects:\n            # whether use dlib landmark to crop image, if not, use only face bbox to calc roi bbox for cropping\n            if args.dlib_landmark:\n                # - use landmark for cropping\n                pts = face_regressor(img_ori, rect).parts()\n                pts = np.array([[pt.x, pt.y] for pt in pts]).T\n                roi_box = parse_roi_box_from_landmark(pts)\n            else:\n                # - use detected face bbox\n                bbox = [rect.left(), rect.top(), rect.right(), rect.bottom()]\n                roi_box = parse_roi_box_from_bbox(bbox)\n\n            img = crop_img(img_ori, roi_box)\n\n            # forward: one step\n            img = cv2.resize(img, dsize=(STD_SIZE, STD_SIZE), interpolation=cv2.INTER_LINEAR)\n            input = transform(img).unsqueeze(0)\n            with torch.no_grad():\n                if args.mode == 'gpu':\n                    input = input.cuda()\n                param = model(input)\n                param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n\n            # 68 pts\n            pts68 = predict_68pts(param, roi_box)\n\n            # two-step for more accurate bbox to crop face\n            if args.bbox_init == 'two':\n                roi_box = parse_roi_box_from_landmark(pts68)\n                img_step2 = crop_img(img_ori, roi_box)\n                img_step2 = cv2.resize(img_step2, dsize=(STD_SIZE, STD_SIZE), interpolation=cv2.INTER_LINEAR)\n                input = transform(img_step2).unsqueeze(0)\n                with torch.no_grad():\n                    if args.mode == 'gpu':\n                        input = input.cuda()\n                    param = model(input)\n                    param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n\n                pts68 = predict_68pts(param, roi_box)\n\n            pts_res.append(pts68)\n            P, pose = parse_pose(param)\n            Ps.append(P)\n            poses.append(pose)\n\n            # dense face 3d vertices\n            if args.dump_ply or args.dump_vertex or args.dump_depth or args.dump_pncc or args.dump_obj:\n                vertices = predict_dense(param, roi_box)\n                vertices_lst.append(vertices)\n            if args.dump_ply:\n                dump_to_ply(vertices, tri, '{}_{}.ply'.format(img_fp.replace(suffix, ''), ind))\n            if args.dump_vertex:\n                dump_vertex(vertices, '{}_{}.mat'.format(img_fp.replace(suffix, ''), ind))\n            if args.dump_pts:\n                wfp = '{}_{}.txt'.format(img_fp.replace(suffix, ''), ind)\n                np.savetxt(wfp, pts68, fmt='%.3f')\n                print('Save 68 3d landmarks to {}'.format(wfp))\n            if args.dump_roi_box:\n                wfp = '{}_{}.roibox'.format(img_fp.replace(suffix, ''), ind)\n                np.savetxt(wfp, roi_box, fmt='%.3f')\n                print('Save roi box to {}'.format(wfp))\n            if args.dump_paf:\n                wfp_paf = '{}_{}_paf.jpg'.format(img_fp.replace(suffix, ''), ind)\n                wfp_crop = '{}_{}_crop.jpg'.format(img_fp.replace(suffix, ''), ind)\n                paf_feature = gen_img_paf(img_crop=img, param=param, kernel_size=args.paf_size)\n\n                cv2.imwrite(wfp_paf, paf_feature)\n                cv2.imwrite(wfp_crop, img)\n                print('Dump to {} and {}'.format(wfp_crop, wfp_paf))\n            if args.dump_obj:\n                wfp = '{}_{}.obj'.format(img_fp.replace(suffix, ''), ind)\n                colors = get_colors(img_ori, vertices)\n                write_obj_with_colors(wfp, vertices, tri, colors)\n                print('Dump obj with sampled texture to {}'.format(wfp))\n            ind += 1\n\n        if args.dump_pose:\n            # P, pose = parse_pose(param)  # Camera matrix (without scale), and pose (yaw, pitch, roll, to verify)\n            img_pose = plot_pose_box(img_ori, Ps, pts_res)\n            wfp = img_fp.replace(suffix, '_pose.jpg')\n            cv2.imwrite(wfp, img_pose)\n            print('Dump to {}'.format(wfp))\n        if args.dump_depth:\n            wfp = img_fp.replace(suffix, '_depth.png')\n            # depths_img = get_depths_image(img_ori, vertices_lst, tri-1)  # python version\n            depths_img = cget_depths_image(img_ori, vertices_lst, tri - 1)  # cython version\n            cv2.imwrite(wfp, depths_img)\n            print('Dump to {}'.format(wfp))\n        if args.dump_pncc:\n            wfp = img_fp.replace(suffix, '_pncc.png')\n            pncc_feature = cpncc(img_ori, vertices_lst, tri - 1)  # cython version\n            cv2.imwrite(wfp, pncc_feature[:, :, ::-1])  # cv2.imwrite will swap RGB -> BGR\n            print('Dump to {}'.format(wfp))\n        if args.dump_res:\n            draw_landmarks(img_ori, pts_res, wfp=img_fp.replace(suffix, '_3DDFA.jpg'), show_flg=args.show_flg)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='3DDFA inference pipeline')\n    parser.add_argument('-f', '--files', nargs='+',\n                        help='image files paths fed into network, single or multiple images')\n    parser.add_argument('-m', '--mode', default='cpu', type=str, help='gpu or cpu mode')\n    parser.add_argument('--show_flg', default='true', type=str2bool, help='whether show the visualization result')\n    parser.add_argument('--bbox_init', default='one', type=str,\n                        help='one|two: one-step bbox initialization or two-step')\n    parser.add_argument('--dump_res', default='true', type=str2bool, help='whether write out the visualization image')\n    parser.add_argument('--dump_vertex', default='false', type=str2bool,\n                        help='whether write out the dense face vertices to mat')\n    parser.add_argument('--dump_ply', default='true', type=str2bool)\n    parser.add_argument('--dump_pts', default='true', type=str2bool)\n    parser.add_argument('--dump_roi_box', default='false', type=str2bool)\n    parser.add_argument('--dump_pose', default='true', type=str2bool)\n    parser.add_argument('--dump_depth', default='true', type=str2bool)\n    parser.add_argument('--dump_pncc', default='true', type=str2bool)\n    parser.add_argument('--dump_paf', default='false', type=str2bool)\n    parser.add_argument('--paf_size', default=3, type=int, help='PAF feature kernel size')\n    parser.add_argument('--dump_obj', default='true', type=str2bool)\n    parser.add_argument('--dlib_bbox', default='true', type=str2bool, help='whether use dlib to predict bbox')\n    parser.add_argument('--dlib_landmark', default='true', type=str2bool,\n                        help='whether use dlib landmark to crop image')\n\n    args = parser.parse_args()\n    main(args)\n"}
{"type": "source_file", "path": "IP_Adapter/ip_adapter/utils.py", "content": "import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom PIL import Image\n\nattn_maps = {}\ndef hook_fn(name):\n    def forward_hook(module, input, output):\n        if hasattr(module.processor, \"attn_map\"):\n            attn_maps[name] = module.processor.attn_map\n            del module.processor.attn_map\n\n    return forward_hook\n\ndef register_cross_attention_hook(unet):\n    for name, module in unet.named_modules():\n        if name.split('.')[-1].startswith('attn2'):\n            module.register_forward_hook(hook_fn(name))\n\n    return unet\n\ndef upscale(attn_map, target_size):\n    attn_map = torch.mean(attn_map, dim=0)\n    attn_map = attn_map.permute(1,0)\n    temp_size = None\n\n    for i in range(0,5):\n        scale = 2 ** i\n        if ( target_size[0] // scale ) * ( target_size[1] // scale) == attn_map.shape[1]*64:\n            temp_size = (target_size[0]//(scale*8), target_size[1]//(scale*8))\n            break\n\n    assert temp_size is not None, \"temp_size cannot is None\"\n\n    attn_map = attn_map.view(attn_map.shape[0], *temp_size)\n\n    attn_map = F.interpolate(\n        attn_map.unsqueeze(0).to(dtype=torch.float32),\n        size=target_size,\n        mode='bilinear',\n        align_corners=False\n    )[0]\n\n    attn_map = torch.softmax(attn_map, dim=0)\n    return attn_map\ndef get_net_attn_map(image_size, batch_size=2, instance_or_negative=False, detach=True):\n\n    idx = 0 if instance_or_negative else 1\n    net_attn_maps = []\n\n    for name, attn_map in attn_maps.items():\n        attn_map = attn_map.cpu() if detach else attn_map\n        attn_map = torch.chunk(attn_map, batch_size)[idx].squeeze()\n        attn_map = upscale(attn_map, image_size) \n        net_attn_maps.append(attn_map) \n\n    net_attn_maps = torch.mean(torch.stack(net_attn_maps,dim=0),dim=0)\n\n    return net_attn_maps\n\ndef attnmaps2images(net_attn_maps):\n\n    #total_attn_scores = 0\n    images = []\n\n    for attn_map in net_attn_maps:\n        attn_map = attn_map.cpu().numpy()\n        #total_attn_scores += attn_map.mean().item()\n\n        normalized_attn_map = (attn_map - np.min(attn_map)) / (np.max(attn_map) - np.min(attn_map)) * 255\n        normalized_attn_map = normalized_attn_map.astype(np.uint8)\n        #print(\"norm: \", normalized_attn_map.shape)\n        image = Image.fromarray(normalized_attn_map)\n\n        #image = fix_save_attn_map(attn_map)\n        images.append(image)\n\n    #print(total_attn_scores)\n    return images\ndef is_torch2_available():\n    return hasattr(F, \"scaled_dot_product_attention\")\n\ndef get_generator(seed, device):\n\n    if seed is not None:\n        if isinstance(seed, list):\n            generator = [torch.Generator(device).manual_seed(seed_item) for seed_item in seed]\n        else:\n            generator = torch.Generator(device).manual_seed(seed)\n    else:\n        generator = None\n\n    return generator"}
{"type": "source_file", "path": "IP_Adapter/ip_adapter/resampler.py", "content": "# modified from https://github.com/mlfoundations/open_flamingo/blob/main/open_flamingo/src/helpers.py\n# and https://github.com/lucidrains/imagen-pytorch/blob/main/imagen_pytorch/imagen_pytorch.py\n\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\nfrom einops.layers.torch import Rearrange\n\n\n# FFN\ndef FeedForward(dim, mult=4):\n    inner_dim = int(dim * mult)\n    return nn.Sequential(\n        nn.LayerNorm(dim),\n        nn.Linear(dim, inner_dim, bias=False),\n        nn.GELU(),\n        nn.Linear(inner_dim, dim, bias=False),\n    )\n\n\ndef reshape_tensor(x, heads):\n    bs, length, width = x.shape\n    # (bs, length, width) --> (bs, length, n_heads, dim_per_head)\n    x = x.view(bs, length, heads, -1)\n    # (bs, length, n_heads, dim_per_head) --> (bs, n_heads, length, dim_per_head)\n    x = x.transpose(1, 2)\n    # (bs, n_heads, length, dim_per_head) --> (bs*n_heads, length, dim_per_head)\n    x = x.reshape(bs, heads, length, -1)\n    return x\n\n\nclass PerceiverAttention(nn.Module):\n    def __init__(self, *, dim, dim_head=64, heads=8):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.dim_head = dim_head\n        self.heads = heads\n        inner_dim = dim_head * heads\n\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n\n    def forward(self, x, latents):\n        \"\"\"\n        Args:\n            x (torch.Tensor): image features\n                shape (b, n1, D)\n            latent (torch.Tensor): latent features\n                shape (b, n2, D)\n        \"\"\"\n        x = self.norm1(x)\n        latents = self.norm2(latents)\n\n        b, l, _ = latents.shape\n\n        q = self.to_q(latents)\n        kv_input = torch.cat((x, latents), dim=-2)\n        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n\n        q = reshape_tensor(q, self.heads)\n        k = reshape_tensor(k, self.heads)\n        v = reshape_tensor(v, self.heads)\n\n        # attention\n        scale = 1 / math.sqrt(math.sqrt(self.dim_head))\n        weight = (q * scale) @ (k * scale).transpose(-2, -1)  # More stable with f16 than dividing afterwards\n        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n        out = weight @ v\n\n        out = out.permute(0, 2, 1, 3).reshape(b, l, -1)\n\n        return self.to_out(out)\n\n\nclass Resampler(nn.Module):\n    def __init__(\n        self,\n        dim=1024,\n        depth=8,\n        dim_head=64,\n        heads=16,\n        num_queries=8,\n        embedding_dim=768,\n        output_dim=1024,\n        ff_mult=4,\n        max_seq_len: int = 257,  # CLIP tokens + CLS token\n        apply_pos_emb: bool = False,\n        num_latents_mean_pooled: int = 0,  # number of latents derived from mean pooled representation of the sequence\n    ):\n        super().__init__()\n        self.pos_emb = nn.Embedding(max_seq_len, embedding_dim) if apply_pos_emb else None\n\n        self.latents = nn.Parameter(torch.randn(1, num_queries, dim) / dim**0.5)\n\n        self.proj_in = nn.Linear(embedding_dim, dim)\n\n        self.proj_out = nn.Linear(dim, output_dim)\n        self.norm_out = nn.LayerNorm(output_dim)\n\n        self.to_latents_from_mean_pooled_seq = (\n            nn.Sequential(\n                nn.LayerNorm(dim),\n                nn.Linear(dim, dim * num_latents_mean_pooled),\n                Rearrange(\"b (n d) -> b n d\", n=num_latents_mean_pooled),\n            )\n            if num_latents_mean_pooled > 0\n            else None\n        )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                nn.ModuleList(\n                    [\n                        PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n                        FeedForward(dim=dim, mult=ff_mult),\n                    ]\n                )\n            )\n\n    def forward(self, x):\n        if self.pos_emb is not None:\n            n, device = x.shape[1], x.device\n            pos_emb = self.pos_emb(torch.arange(n, device=device))\n            x = x + pos_emb\n\n        latents = self.latents.repeat(x.size(0), 1, 1)\n\n        x = self.proj_in(x)\n\n        if self.to_latents_from_mean_pooled_seq:\n            meanpooled_seq = masked_mean(x, dim=1, mask=torch.ones(x.shape[:2], device=x.device, dtype=torch.bool))\n            meanpooled_latents = self.to_latents_from_mean_pooled_seq(meanpooled_seq)\n            latents = torch.cat((meanpooled_latents, latents), dim=-2)\n\n        for attn, ff in self.layers:\n            latents = attn(x, latents) + latents\n            latents = ff(latents) + latents\n\n        latents = self.proj_out(latents)\n        return self.norm_out(latents)\n\n\ndef masked_mean(t, *, dim, mask=None):\n    if mask is None:\n        return t.mean(dim=dim)\n\n    denom = mask.sum(dim=dim, keepdim=True)\n    mask = rearrange(mask, \"b n -> b n 1\")\n    masked_t = t.masked_fill(~mask, 0.0)\n\n    return masked_t.sum(dim=dim) / denom.clamp(min=1e-5)\n"}
{"type": "source_file", "path": "launch.py", "content": "import argparse\nimport contextlib\nimport importlib\nimport logging\nimport os\nimport sys\n\n\nclass ColoredFilter(logging.Filter):\n    \"\"\"\n    A logging filter to add color to certain log levels.\n    \"\"\"\n\n    RESET = \"\\033[0m\"\n    RED = \"\\033[31m\"\n    GREEN = \"\\033[32m\"\n    YELLOW = \"\\033[33m\"\n    BLUE = \"\\033[34m\"\n    MAGENTA = \"\\033[35m\"\n    CYAN = \"\\033[36m\"\n\n    COLORS = {\n        \"WARNING\": YELLOW,\n        \"INFO\": GREEN,\n        \"DEBUG\": BLUE,\n        \"CRITICAL\": MAGENTA,\n        \"ERROR\": RED,\n    }\n\n    RESET = \"\\x1b[0m\"\n\n    def __init__(self):\n        super().__init__()\n\n    def filter(self, record):\n        if record.levelname in self.COLORS:\n            color_start = self.COLORS[record.levelname]\n            record.levelname = f\"{color_start}[{record.levelname}]\"\n            record.msg = f\"{record.msg}{self.RESET}\"\n        return True\n\n\ndef main(args, extras) -> None:\n    # set CUDA_VISIBLE_DEVICES if needed, then import pytorch-lightning\n    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n    env_gpus_str = os.environ.get(\"CUDA_VISIBLE_DEVICES\", None)\n    env_gpus = list(env_gpus_str.split(\",\")) if env_gpus_str else []\n    selected_gpus = [0]\n\n    # Always rely on CUDA_VISIBLE_DEVICES if specific GPU ID(s) are specified.\n    # As far as Pytorch Lightning is concerned, we always use all available GPUs\n    # (possibly filtered by CUDA_VISIBLE_DEVICES).\n    devices = -1\n    if len(env_gpus) > 0:\n        # CUDA_VISIBLE_DEVICES was set already, e.g. within SLURM srun or higher-level script.\n        n_gpus = len(env_gpus)\n    else:\n        selected_gpus = list(args.gpu.split(\",\"))\n        n_gpus = len(selected_gpus)\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n\n    import pytorch_lightning as pl\n    import torch\n    from pytorch_lightning import Trainer\n    from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n    from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n    from pytorch_lightning.utilities.rank_zero import rank_zero_only\n\n    if args.typecheck:\n        from jaxtyping import install_import_hook\n\n        install_import_hook(\"threestudio\", \"typeguard.typechecked\")\n\n    import threestudio\n    from threestudio.systems.base import BaseSystem\n    from threestudio.utils.callbacks import (\n        CodeSnapshotCallback,\n        ConfigSnapshotCallback,\n        CustomProgressBar,\n        ProgressCallback,\n    )\n    from threestudio.utils.config import ExperimentConfig, load_config\n    from threestudio.utils.misc import get_rank\n    from threestudio.utils.typing import Optional\n\n    logger = logging.getLogger(\"pytorch_lightning\")\n    if args.verbose:\n        logger.setLevel(logging.DEBUG)\n\n    for handler in logger.handlers:\n        if handler.stream == sys.stderr:  # type: ignore\n            if not args.gradio:\n                handler.setFormatter(logging.Formatter(\"%(levelname)s %(message)s\"))\n                handler.addFilter(ColoredFilter())\n            else:\n                handler.setFormatter(logging.Formatter(\"[%(levelname)s] %(message)s\"))\n\n    # parse YAML config to OmegaConf\n    cfg: ExperimentConfig\n    cfg = load_config(args.config, cli_args=extras, n_gpus=n_gpus)\n\n    if len(cfg.custom_import) > 0:\n        print(cfg.custom_import)\n        for extension in cfg.custom_import:\n            importlib.import_module(extension)\n    # set a different seed for each device\n    pl.seed_everything(cfg.seed + get_rank(), workers=True)\n\n    dm = threestudio.find(cfg.data_type)(cfg.data)\n\n    # Auto check resume files during training\n    if args.train and cfg.resume is None:\n        import glob\n        resume_file_list = glob.glob(f\"{cfg.trial_dir}/ckpts/*\")\n        if len(resume_file_list) != 0:\n            print(sorted(resume_file_list))\n            cfg.resume = sorted(resume_file_list)[-1]\n            print(f\"Find resume file: {cfg.resume}\")\n\n    system: BaseSystem = threestudio.find(cfg.system_type)(\n        cfg.system, resumed=cfg.resume is not None\n    )\n    system.set_save_dir(os.path.join(cfg.trial_dir, \"save\"))\n\n    if args.gradio:\n        fh = logging.FileHandler(os.path.join(cfg.trial_dir, \"logs\"))\n        fh.setLevel(logging.INFO)\n        if args.verbose:\n            fh.setLevel(logging.DEBUG)\n        fh.setFormatter(logging.Formatter(\"[%(levelname)s] %(message)s\"))\n        logger.addHandler(fh)\n\n    callbacks = []\n    if args.train:\n        callbacks += [\n            ModelCheckpoint(\n                dirpath=os.path.join(cfg.trial_dir, \"ckpts\"), **cfg.checkpoint\n            ),\n            LearningRateMonitor(logging_interval=\"step\"),\n            CodeSnapshotCallback(\n                os.path.join(cfg.trial_dir, \"code\"), use_version=False\n            ),\n            ConfigSnapshotCallback(\n                args.config,\n                cfg,\n                os.path.join(cfg.trial_dir, \"configs\"),\n                use_version=False,\n            ),\n        ]\n        if args.gradio:\n            callbacks += [\n                ProgressCallback(save_path=os.path.join(cfg.trial_dir, \"progress\"))\n            ]\n        else:\n            callbacks += [CustomProgressBar(refresh_rate=1)]\n\n    def write_to_text(file, lines):\n        with open(file, \"w\") as f:\n            for line in lines:\n                f.write(line + \"\\n\")\n\n    loggers = []\n    if args.train:\n        # make tensorboard logging dir to suppress warning\n        rank_zero_only(\n            lambda: os.makedirs(os.path.join(cfg.trial_dir, \"tb_logs\"), exist_ok=True)\n        )()\n        loggers += [\n            TensorBoardLogger(cfg.trial_dir, name=\"tb_logs\"),\n            CSVLogger(cfg.trial_dir, name=\"csv_logs\"),\n        ] + system.get_loggers()\n        rank_zero_only(\n            lambda: write_to_text(\n                os.path.join(cfg.trial_dir, \"cmd.txt\"),\n                [\"python \" + \" \".join(sys.argv), str(args)],\n            )\n        )()\n    \n    trainer = Trainer(\n        callbacks=callbacks,\n        logger=loggers,\n        inference_mode=False,\n        accelerator=\"gpu\",\n        devices=devices,\n        **cfg.trainer,\n    )\n\n    def set_system_status(system: BaseSystem, ckpt_path: Optional[str]):\n        if ckpt_path is None:\n            return\n        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n        system.set_resume_status(ckpt[\"epoch\"], ckpt[\"global_step\"])\n\n    if args.train:\n        trainer.fit(system, datamodule=dm, ckpt_path=cfg.resume)\n        trainer.test(system, datamodule=dm)\n        if args.gradio:\n            # also export assets if in gradio mode\n            trainer.predict(system, datamodule=dm)\n    elif args.validate:\n        # manually set epoch and global_step as they cannot be automatically resumed\n        set_system_status(system, cfg.resume)\n        trainer.validate(system, datamodule=dm, ckpt_path=cfg.resume)\n    elif args.test:\n        # manually set epoch and global_step as they cannot be automatically resumed\n        set_system_status(system, cfg.resume)\n        trainer.test(system, datamodule=dm, ckpt_path=cfg.resume)\n    elif args.export:\n        set_system_status(system, cfg.resume)\n        trainer.predict(system, datamodule=dm, ckpt_path=cfg.resume)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", required=True, help=\"path to config file\")\n    parser.add_argument(\n        \"--gpu\",\n        default=\"0\",\n        help=\"GPU(s) to be used. 0 means use the 1st available GPU. \"\n        \"1,2 means use the 2nd and 3rd available GPU. \"\n        \"If CUDA_VISIBLE_DEVICES is set before calling `launch.py`, \"\n        \"this argument is ignored and all available GPUs are always used.\",\n    )\n\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--train\", action=\"store_true\")\n    group.add_argument(\"--validate\", action=\"store_true\")\n    group.add_argument(\"--test\", action=\"store_true\")\n    group.add_argument(\"--export\", action=\"store_true\")\n\n    parser.add_argument(\n        \"--gradio\", action=\"store_true\", help=\"if true, run in gradio mode\"\n    )\n\n    parser.add_argument(\n        \"--verbose\", action=\"store_true\", help=\"if true, set logging level to DEBUG\"\n    )\n\n    parser.add_argument(\n        \"--typecheck\",\n        action=\"store_true\",\n        help=\"whether to enable dynamic type checking\",\n    )\n\n    args, extras = parser.parse_known_args()\n\n    if args.gradio:\n        # FIXME: no effect, stdout is not captured\n        with contextlib.redirect_stdout(sys.stderr):\n            main(args, extras)\n    else:\n        main(args, extras)"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/vdc_loss.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nfrom utils.io import _load, _numpy_to_cuda, _numpy_to_tensor\nfrom utils.params import *\n\n_to_tensor = _numpy_to_cuda  # gpu\n\n\ndef _parse_param_batch(param):\n    \"\"\"Work for both numpy and tensor\"\"\"\n    N = param.shape[0]\n    p_ = param[:, :12].view(N, 3, -1)\n    p = p_[:, :, :3]\n    offset = p_[:, :, -1].view(N, 3, 1)\n    alpha_shp = param[:, 12:52].view(N, -1, 1)\n    alpha_exp = param[:, 52:].view(N, -1, 1)\n    return p, offset, alpha_shp, alpha_exp\n\n\nclass VDCLoss(nn.Module):\n    def __init__(self, opt_style='all'):\n        super(VDCLoss, self).__init__()\n\n        self.u = _to_tensor(u)\n        self.param_mean = _to_tensor(param_mean)\n        self.param_std = _to_tensor(param_std)\n        self.w_shp = _to_tensor(w_shp)\n        self.w_exp = _to_tensor(w_exp)\n\n        self.keypoints = _to_tensor(keypoints)\n        self.u_base = self.u[self.keypoints]\n        self.w_shp_base = self.w_shp[self.keypoints]\n        self.w_exp_base = self.w_exp[self.keypoints]\n\n        self.w_shp_length = self.w_shp.shape[0] // 3\n\n        self.opt_style = opt_style\n\n    def reconstruct_and_parse(self, input, target):\n        # reconstruct\n        param = input * self.param_std + self.param_mean\n        param_gt = target * self.param_std + self.param_mean\n\n        # parse param\n        p, offset, alpha_shp, alpha_exp = _parse_param_batch(param)\n        pg, offsetg, alpha_shpg, alpha_expg = _parse_param_batch(param_gt)\n\n        return (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg)\n\n    def forward_all(self, input, target):\n        (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg) \\\n            = self.reconstruct_and_parse(input, target)\n\n        N = input.shape[0]\n        offset[:, -1] = offsetg[:, -1]\n        gt_vertex = pg @ (self.u + self.w_shp @ alpha_shpg + self.w_exp @ alpha_expg) \\\n            .view(N, -1, 3).permute(0, 2, 1) + offsetg\n        vertex = p @ (self.u + self.w_shp @ alpha_shp + self.w_exp @ alpha_exp) \\\n            .view(N, -1, 3).permute(0, 2, 1) + offset\n\n        diff = (gt_vertex - vertex) ** 2\n        loss = torch.mean(diff)\n        return loss\n\n    def forward_resample(self, input, target, resample_num=132):\n        (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg) \\\n            = self.reconstruct_and_parse(input, target)\n\n        # resample index\n        index = torch.randperm(self.w_shp_length)[:resample_num].reshape(-1, 1)\n        keypoints_resample = torch.cat((3 * index, 3 * index + 1, 3 * index + 2), dim=1).view(-1).cuda()\n        keypoints_mix = torch.cat((self.keypoints, keypoints_resample))\n        w_shp_base = self.w_shp[keypoints_mix]\n        u_base = self.u[keypoints_mix]\n        w_exp_base = self.w_exp[keypoints_mix]\n\n        offset[:, -1] = offsetg[:, -1]\n\n        N = input.shape[0]\n        gt_vertex = pg @ (u_base + w_shp_base @ alpha_shpg + w_exp_base @ alpha_expg) \\\n            .view(N, -1, 3).permute(0, 2, 1) + offsetg\n        vertex = p @ (u_base + w_shp_base @ alpha_shp + w_exp_base @ alpha_exp) \\\n            .view(N, -1, 3).permute(0, 2, 1) + offset\n        diff = (gt_vertex - vertex) ** 2\n        loss = torch.mean(diff)\n        return loss\n\n    def forward(self, input, target):\n        if self.opt_style == 'all':\n            return self.forward_all(input, target)\n        elif self.opt_style == 'resample':\n            return self.forward_resample(input, target)\n        else:\n            raise Exception(f'Unknown opt style: f{opt_style}')\n\n\nif __name__ == '__main__':\n    pass\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/FaceBoxes/FaceBoxes_ONNX.py", "content": "# coding: utf-8\n\nimport os.path as osp\n\nimport torch\nimport numpy as np\nimport cv2\n\nfrom .utils.prior_box import PriorBox\nfrom .utils.nms_wrapper import nms\nfrom .utils.box_utils import decode\nfrom .utils.timer import Timer\nfrom .utils.config import cfg\nfrom .onnx import convert_to_onnx\n\nimport onnxruntime\n\n# some global configs\nconfidence_threshold = 0.05\ntop_k = 5000\nkeep_top_k = 750\nnms_threshold = 0.3\nvis_thres = 0.5\nresize = 1\n\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\n\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\nonnx_path = make_abs_path('weights/FaceBoxesProd.onnx')\n\n\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:\n            continue\n        text = \"{:.4f}\".format(b[4])\n        b = list(map(int, b))\n        cv2.rectangle(img, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n        cx = b[0]\n        cy = b[1] + 12\n        cv2.putText(img, text, (cx, cy), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n    cv2.imwrite(wfp, img)\n    print(f'Viz bbox to {wfp}')\n\n\nclass FaceBoxes_ONNX(object):\n    def __init__(self, timer_flag=False):\n        if not osp.exists(onnx_path):\n            convert_to_onnx(onnx_path)\n        self.session = onnxruntime.InferenceSession(onnx_path, None)\n\n        self.timer_flag = timer_flag\n\n    def __call__(self, img_):\n        img_raw = img_.copy()\n\n        # scaling to speed up\n        scale = 1\n        if scale_flag:\n            h, w = img_raw.shape[:2]\n            if h > HEIGHT:\n                scale = HEIGHT / h\n            if w * scale > WIDTH:\n                scale *= WIDTH / (w * scale)\n            # print(scale)\n            if scale == 1:\n                img_raw_scale = img_raw\n            else:\n                h_s = int(scale * h)\n                w_s = int(scale * w)\n                # print(h_s, w_s)\n                img_raw_scale = cv2.resize(img_raw, dsize=(w_s, h_s))\n                # print(img_raw_scale.shape)\n\n            img = np.float32(img_raw_scale)\n        else:\n            img = np.float32(img_raw)\n\n        # forward\n        _t = {'forward_pass': Timer(), 'misc': Timer()}\n        im_height, im_width, _ = img.shape\n        scale_bbox = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n\n        img -= (104, 117, 123)\n        img = img.transpose(2, 0, 1)\n        # img = torch.from_numpy(img).unsqueeze(0)\n        img = img[np.newaxis, ...]\n\n        _t['forward_pass'].tic()\n        # loc, conf = self.net(img)  # forward pass\n        out = self.session.run(None, {'input': img})\n        loc, conf = out[0], out[1]\n        # for compatibility, may need to optimize\n        loc = torch.from_numpy(loc)\n        _t['forward_pass'].toc()\n        _t['misc'].tic()\n\n        priorbox = PriorBox(image_size=(im_height, im_width))\n        priors = priorbox.forward()\n        prior_data = priors.data\n        boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n        if scale_flag:\n            boxes = boxes * scale_bbox / scale / resize\n        else:\n            boxes = boxes * scale_bbox / resize\n\n        boxes = boxes.cpu().numpy()\n        scores = conf[0][:, 1]\n        # scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n\n        # ignore low scores\n        inds = np.where(scores > confidence_threshold)[0]\n        boxes = boxes[inds]\n        scores = scores[inds]\n\n        # keep top-K before NMS\n        order = scores.argsort()[::-1][:top_k]\n        boxes = boxes[order]\n        scores = scores[order]\n\n        # do NMS\n        dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n        keep = nms(dets, nms_threshold)\n        dets = dets[keep, :]\n\n        # keep top-K faster NMS\n        dets = dets[:keep_top_k, :]\n        _t['misc'].toc()\n\n        if self.timer_flag:\n            print('Detection: {:d}/{:d} forward_pass_time: {:.4f}s misc: {:.4f}s'.format(1, 1, _t[\n                'forward_pass'].average_time, _t['misc'].average_time))\n\n        # filter using vis_thres\n        det_bboxes = []\n        for b in dets:\n            if b[4] > vis_thres:\n                xmin, ymin, xmax, ymax, score = b[0], b[1], b[2], b[3], b[4]\n                bbox = [xmin, ymin, xmax, ymax, score]\n                det_bboxes.append(bbox)\n\n        return det_bboxes\n\n\ndef main():\n    face_boxes = FaceBoxes_ONNX(timer_flag=True)\n\n    fn = 'trump_hillary.jpg'\n    img_fp = f'../examples/inputs/{fn}'\n    img = cv2.imread(img_fp)\n    print(f'input shape: {img.shape}')\n    dets = face_boxes(img)  # xmin, ymin, w, h\n    # print(dets)\n\n    # repeating inference for `n` times\n    n = 10\n    for i in range(n):\n        dets = face_boxes(img)\n\n    wfn = fn.replace('.jpg', '_det.jpg')\n    wfp = osp.join('../examples/results', wfn)\n    viz_bbox(img, dets, wfp)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/utils/ddfa.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport os.path as osp\nfrom pathlib import Path\nimport numpy as np\n\nimport torch\nimport torch.utils.data as data\nimport cv2\nimport pickle\nimport argparse\nfrom .io import _numpy_to_tensor, _load_cpu, _load_gpu\nfrom .params import *\n\n\ndef _parse_param(param):\n    \"\"\"Work for both numpy and tensor\"\"\"\n    p_ = param[:12].reshape(3, -1)\n    p = p_[:, :3]\n    offset = p_[:, -1].reshape(3, 1)\n    alpha_shp = param[12:52].reshape(-1, 1)\n    alpha_exp = param[52:].reshape(-1, 1)\n    return p, offset, alpha_shp, alpha_exp\n\n\ndef reconstruct_vertex(param, whitening=True, dense=False, transform=True):\n    \"\"\"Whitening param -> 3d vertex, based on the 3dmm param: u_base, w_shp, w_exp\n    dense: if True, return dense vertex, else return 68 sparse landmarks. All dense or sparse vertex is transformed to\n    image coordinate space, but without alignment caused by face cropping.\n    transform: whether transform to image space\n    \"\"\"\n    if len(param) == 12:\n        param = np.concatenate((param, [0] * 50))\n    if whitening:\n        if len(param) == 62:\n            param = param * param_std + param_mean\n        else:\n            param = np.concatenate((param[:11], [0], param[11:]))\n            param = param * param_std + param_mean\n\n    p, offset, alpha_shp, alpha_exp = _parse_param(param)\n\n    if dense:\n        vertex = p @ (u + w_shp @ alpha_shp + w_exp @ alpha_exp).reshape(3, -1, order='F') + offset\n\n        if transform:\n            # transform to image coordinate space\n            vertex[1, :] = std_size + 1 - vertex[1, :]\n    else:\n        \"\"\"For 68 pts\"\"\"\n        vertex = p @ (u_base + w_shp_base @ alpha_shp + w_exp_base @ alpha_exp).reshape(3, -1, order='F') + offset\n\n        if transform:\n            # transform to image coordinate space\n            vertex[1, :] = std_size + 1 - vertex[1, :]\n\n    return vertex\n\n\ndef img_loader(path):\n    return cv2.imread(path, cv2.IMREAD_COLOR)\n\n\ndef str2bool(v):\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected')\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass ToTensorGjz(object):\n    def __call__(self, pic):\n        if isinstance(pic, np.ndarray):\n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n            return img.float()\n\n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n\n\nclass NormalizeGjz(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        tensor.sub_(self.mean).div_(self.std)\n        return tensor\n\n\nclass DDFADataset(data.Dataset):\n    def __init__(self, root, filelists, param_fp, transform=None, **kargs):\n        self.root = root\n        self.transform = transform\n        self.lines = Path(filelists).read_text().strip().split('\\n')\n        self.params = _numpy_to_tensor(_load_cpu(param_fp))\n        self.img_loader = img_loader\n\n    def _target_loader(self, index):\n        target = self.params[index]\n\n        return target\n\n    def __getitem__(self, index):\n        path = osp.join(self.root, self.lines[index])\n        img = self.img_loader(path)\n\n        target = self._target_loader(index)\n\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, target\n\n    def __len__(self):\n        return len(self.lines)\n\n\nclass DDFATestDataset(data.Dataset):\n    def __init__(self, filelists, root='', transform=None):\n        self.root = root\n        self.transform = transform\n        self.lines = Path(filelists).read_text().strip().split('\\n')\n\n    def __getitem__(self, index):\n        path = osp.join(self.root, self.lines[index])\n        img = img_loader(path)\n\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n\n    def __len__(self):\n        return len(self.lines)\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/FaceBoxes/onnx.py", "content": "# coding: utf-8\n\n__author__ = 'cleardusk'\n\nimport torch\n\nfrom .models.faceboxes import FaceBoxesNet\nfrom .utils.functions import load_model\n\n\ndef convert_to_onnx(onnx_path):\n    pretrained_path = onnx_path.replace('.onnx', '.pth')\n    # 1. load model\n    torch.set_grad_enabled(False)\n    net = FaceBoxesNet(phase='test', size=None, num_classes=2)  # initialize detector\n    net = load_model(net, pretrained_path=pretrained_path, load_to_cpu=True)\n    net.eval()\n\n    # 2. convert\n    batch_size = 1\n    dummy_input = torch.randn(batch_size, 3, 720, 1080)\n    # export with dynamic axes for various input sizes\n    torch.onnx.export(\n        net,\n        (dummy_input,),\n        onnx_path,\n        input_names=['input'],\n        output_names=['output'],\n        dynamic_axes={\n            'input': [0, 2, 3],\n            'output': [0]\n        },\n        do_constant_folding=True\n    )\n    print(f'Convert {pretrained_path} to {onnx_path} done.')\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/FaceBoxes/models/faceboxes.py", "content": "# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=1e-5)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return F.relu(x, inplace=True)\n\n\nclass Inception(nn.Module):\n    def __init__(self):\n        super(Inception, self).__init__()\n        self.branch1x1 = BasicConv2d(128, 32, kernel_size=1, padding=0)\n        self.branch1x1_2 = BasicConv2d(128, 32, kernel_size=1, padding=0)\n        self.branch3x3_reduce = BasicConv2d(128, 24, kernel_size=1, padding=0)\n        self.branch3x3 = BasicConv2d(24, 32, kernel_size=3, padding=1)\n        self.branch3x3_reduce_2 = BasicConv2d(128, 24, kernel_size=1, padding=0)\n        self.branch3x3_2 = BasicConv2d(24, 32, kernel_size=3, padding=1)\n        self.branch3x3_3 = BasicConv2d(32, 32, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch1x1_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch1x1_2 = self.branch1x1_2(branch1x1_pool)\n\n        branch3x3_reduce = self.branch3x3_reduce(x)\n        branch3x3 = self.branch3x3(branch3x3_reduce)\n\n        branch3x3_reduce_2 = self.branch3x3_reduce_2(x)\n        branch3x3_2 = self.branch3x3_2(branch3x3_reduce_2)\n        branch3x3_3 = self.branch3x3_3(branch3x3_2)\n\n        outputs = [branch1x1, branch1x1_2, branch3x3, branch3x3_3]\n        return torch.cat(outputs, 1)\n\n\nclass CRelu(nn.Module):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(CRelu, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=1e-5)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = torch.cat([x, -x], 1)\n        x = F.relu(x, inplace=True)\n        return x\n\n\nclass FaceBoxesNet(nn.Module):\n\n    def __init__(self, phase, size, num_classes):\n        super(FaceBoxesNet, self).__init__()\n        self.phase = phase\n        self.num_classes = num_classes\n        self.size = size\n\n        self.conv1 = CRelu(3, 24, kernel_size=7, stride=4, padding=3)\n        self.conv2 = CRelu(48, 64, kernel_size=5, stride=2, padding=2)\n\n        self.inception1 = Inception()\n        self.inception2 = Inception()\n        self.inception3 = Inception()\n\n        self.conv3_1 = BasicConv2d(128, 128, kernel_size=1, stride=1, padding=0)\n        self.conv3_2 = BasicConv2d(128, 256, kernel_size=3, stride=2, padding=1)\n\n        self.conv4_1 = BasicConv2d(256, 128, kernel_size=1, stride=1, padding=0)\n        self.conv4_2 = BasicConv2d(128, 256, kernel_size=3, stride=2, padding=1)\n\n        self.loc, self.conf = self.multibox(self.num_classes)\n\n        if self.phase == 'test':\n            self.softmax = nn.Softmax(dim=-1)\n\n        if self.phase == 'train':\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    if m.bias is not None:\n                        nn.init.xavier_normal_(m.weight.data)\n                        m.bias.data.fill_(0.02)\n                    else:\n                        m.weight.data.normal_(0, 0.01)\n                elif isinstance(m, nn.BatchNorm2d):\n                    m.weight.data.fill_(1)\n                    m.bias.data.zero_()\n\n    def multibox(self, num_classes):\n        loc_layers = []\n        conf_layers = []\n        loc_layers += [nn.Conv2d(128, 21 * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(128, 21 * num_classes, kernel_size=3, padding=1)]\n        loc_layers += [nn.Conv2d(256, 1 * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(256, 1 * num_classes, kernel_size=3, padding=1)]\n        loc_layers += [nn.Conv2d(256, 1 * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(256, 1 * num_classes, kernel_size=3, padding=1)]\n        return nn.Sequential(*loc_layers), nn.Sequential(*conf_layers)\n\n    def forward(self, x):\n\n        detection_sources = list()\n        loc = list()\n        conf = list()\n\n        x = self.conv1(x)\n        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        x = self.inception1(x)\n        x = self.inception2(x)\n        x = self.inception3(x)\n        detection_sources.append(x)\n\n        x = self.conv3_1(x)\n        x = self.conv3_2(x)\n        detection_sources.append(x)\n\n        x = self.conv4_1(x)\n        x = self.conv4_2(x)\n        detection_sources.append(x)\n\n        for (x, l, c) in zip(detection_sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if self.phase == \"test\":\n            output = (loc.view(loc.size(0), -1, 4),\n                      self.softmax(conf.view(conf.size(0), -1, self.num_classes)))\n        else:\n            output = (loc.view(loc.size(0), -1, 4),\n                      conf.view(conf.size(0), -1, self.num_classes))\n\n        return output\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/FaceBoxes/utils/box_utils.py", "content": "# coding: utf-8\n\nimport torch\nimport numpy as np\n\n\ndef point_form(boxes):\n    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    \"\"\"\n    return torch.cat((boxes[:, :2] - boxes[:, 2:] / 2,  # xmin, ymin\n                      boxes[:, :2] + boxes[:, 2:] / 2), 1)  # xmax, ymax\n\n\ndef center_size(boxes):\n    \"\"\" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    \"\"\"\n    return torch.cat((boxes[:, 2:] + boxes[:, :2]) / 2,  # cx, cy\n                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n\n\ndef intersect(box_a, box_b):\n    \"\"\" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    \"\"\"\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A  B / A  B = A  B / (area(A) + area(B) - A  B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    \"\"\"\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2] - box_a[:, 0]) *\n              (box_a[:, 3] - box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2] - box_b[:, 0]) *\n              (box_b[:, 3] - box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\ndef matrix_iou(a, b):\n    \"\"\"\n    return iou of a and b, numpy version for data augenmentation\n    \"\"\"\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n\n    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    area_b = np.prod(b[:, 2:] - b[:, :2], axis=1)\n    return area_i / (area_a[:, np.newaxis] + area_b - area_i)\n\n\ndef matrix_iof(a, b):\n    \"\"\"\n    return iof of a and b, numpy version for data augenmentation\n    \"\"\"\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n\n    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    return area_i / np.maximum(area_a[:, np.newaxis], 1)\n\n\ndef match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n        threshold: (float) The overlap threshold used when mathing boxes.\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        variances: (tensor) Variances corresponding to each prior coord,\n            Shape: [num_priors, 4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n        idx: (int) current batch index\n    Return:\n        The matched indices corresponding to 1)location and 2)confidence preds.\n    \"\"\"\n    # jaccard index\n    overlaps = jaccard(\n        truths,\n        point_form(priors)\n    )\n    # (Bipartite Matching)\n    # [1,num_objects] best prior for each ground truth\n    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n\n    # ignore hard gt\n    valid_gt_idx = best_prior_overlap[:, 0] >= 0.2\n    best_prior_idx_filter = best_prior_idx[valid_gt_idx, :]\n    if best_prior_idx_filter.shape[0] <= 0:\n        loc_t[idx] = 0\n        conf_t[idx] = 0\n        return\n\n    # [1,num_priors] best ground truth for each prior\n    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n    best_truth_idx.squeeze_(0)\n    best_truth_overlap.squeeze_(0)\n    best_prior_idx.squeeze_(1)\n    best_prior_idx_filter.squeeze_(1)\n    best_prior_overlap.squeeze_(1)\n    best_truth_overlap.index_fill_(0, best_prior_idx_filter, 2)  # ensure best prior\n    # TODO refactor: index  best_prior_idx with long tensor\n    # ensure every gt matches with its prior of max overlap\n    for j in range(best_prior_idx.size(0)):\n        best_truth_idx[best_prior_idx[j]] = j\n    matches = truths[best_truth_idx]  # Shape: [num_priors,4]\n    conf = labels[best_truth_idx]  # Shape: [num_priors]\n    conf[best_truth_overlap < threshold] = 0  # label as background\n    loc = encode(matches, priors, variances)\n    loc_t[idx] = loc  # [num_priors,4] encoded offsets to learn\n    conf_t[idx] = conf  # [num_priors] top class label for each prior\n\n\ndef encode(matched, priors, variances):\n    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    \"\"\"\n\n    # dist b/t match center and prior's center\n    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\n    # encode variance\n    g_cxcy /= (variances[0] * priors[:, 2:])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n\n# Adapted from https://github.com/Hakuyume/chainer-ssd\ndef decode(loc, priors, variances):\n    \"\"\"Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    \"\"\"\n\n    boxes = torch.cat((\n        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\n\ndef log_sum_exp(x):\n    \"\"\"Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    \"\"\"\n    x_max = x.data.max()\n    return torch.log(torch.sum(torch.exp(x - x_max), 1, keepdim=True)) + x_max\n\n\n# Original author: Francisco Massa:\n# https://github.com/fmassa/object-detection.torch\n# Ported to PyTorch by Max deGroot (02/01/2017)\ndef nms(boxes, scores, overlap=0.5, top_k=200):\n    \"\"\"Apply non-maximum suppression at test time to avoid detecting too many\n    overlapping bounding boxes for a given object.\n    Args:\n        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n        top_k: (int) The Maximum number of box preds to consider.\n    Return:\n        The indices of the kept boxes with respect to num_priors.\n    \"\"\"\n\n    keep = torch.Tensor(scores.size(0)).fill_(0).long()\n    if boxes.numel() == 0:\n        return keep\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    # I = I[v >= 0.01]\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    # keep = torch.Tensor()\n    count = 0\n    while idx.numel() > 0:\n        i = idx[-1]  # index of current largest val\n        # keep.append(i)\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1:\n            break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w * h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter / union  # store result in iou\n        # keep only elements with an IoU <= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/utils/io.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport os\nimport numpy as np\nimport torch\nimport pickle\nimport scipy.io as sio\n\n\ndef mkdir(d):\n    \"\"\"only works on *nix system\"\"\"\n    if not os.path.isdir(d) and not os.path.exists(d):\n        os.system('mkdir -p {}'.format(d))\n\n\ndef _get_suffix(filename):\n    \"\"\"a.jpg -> jpg\"\"\"\n    pos = filename.rfind('.')\n    if pos == -1:\n        return ''\n    return filename[pos + 1:]\n\n\ndef _load(fp):\n    suffix = _get_suffix(fp)\n    if suffix == 'npy':\n        return np.load(fp)\n    elif suffix == 'pkl':\n        return pickle.load(open(fp, 'rb'))\n\n\ndef _dump(wfp, obj):\n    suffix = _get_suffix(wfp)\n    if suffix == 'npy':\n        np.save(wfp, obj)\n    elif suffix == 'pkl':\n        pickle.dump(obj, open(wfp, 'wb'))\n    else:\n        raise Exception('Unknown Type: {}'.format(suffix))\n\n\ndef _load_tensor(fp, mode='cpu'):\n    if mode.lower() == 'cpu':\n        return torch.from_numpy(_load(fp))\n    elif mode.lower() == 'gpu':\n        return torch.from_numpy(_load(fp)).cuda()\n\n\ndef _tensor_to_cuda(x):\n    if x.is_cuda:\n        return x\n    else:\n        return x.cuda()\n\n\ndef _load_gpu(fp):\n    return torch.from_numpy(_load(fp)).cuda()\n\n\ndef load_bfm(model_path):\n    suffix = _get_suffix(model_path)\n    if suffix == 'mat':\n        C = sio.loadmat(model_path)\n        model = C['model_refine']\n        model = model[0, 0]\n\n        model_new = {}\n        w_shp = model['w'].astype(np.float32)\n        model_new['w_shp_sim'] = w_shp[:, :40]\n        w_exp = model['w_exp'].astype(np.float32)\n        model_new['w_exp_sim'] = w_exp[:, :10]\n\n        u_shp = model['mu_shape']\n        u_exp = model['mu_exp']\n        u = (u_shp + u_exp).astype(np.float32)\n        model_new['mu'] = u\n        model_new['tri'] = model['tri'].astype(np.int32) - 1\n\n        # flatten it, pay attention to index value\n        keypoints = model['keypoints'].astype(np.int32) - 1\n        keypoints = np.concatenate((3 * keypoints, 3 * keypoints + 1, 3 * keypoints + 2), axis=0)\n\n        model_new['keypoints'] = keypoints.T.flatten()\n\n        #\n        w = np.concatenate((w_shp, w_exp), axis=1)\n        w_base = w[keypoints]\n        w_norm = np.linalg.norm(w, axis=0)\n        w_base_norm = np.linalg.norm(w_base, axis=0)\n\n        dim = w_shp.shape[0] // 3\n        u_base = u[keypoints].reshape(-1, 1)\n        w_shp_base = w_shp[keypoints]\n        w_exp_base = w_exp[keypoints]\n\n        model_new['w_norm'] = w_norm\n        model_new['w_base_norm'] = w_base_norm\n        model_new['dim'] = dim\n        model_new['u_base'] = u_base\n        model_new['w_shp_base'] = w_shp_base\n        model_new['w_exp_base'] = w_exp_base\n\n        _dump(model_path.replace('.mat', '.pkl'), model_new)\n        return model_new\n    else:\n        return _load(model_path)\n\n\n_load_cpu = _load\n_numpy_to_tensor = lambda x: torch.from_numpy(x)\n_tensor_to_numpy = lambda x: x.cpu()\n_numpy_to_cuda = lambda x: _tensor_to_cuda(torch.from_numpy(x))\n_cuda_to_tensor = lambda x: x.cpu()\n_cuda_to_numpy = lambda x: x.cpu().numpy()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/utils/render.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\n\n\"\"\"\nModified from https://raw.githubusercontent.com/YadiraF/PRNet/master/utils/render.py\n\"\"\"\n\n__author__ = 'cleardusk'\n\nimport numpy as np\nfrom .cython import mesh_core_cython\nfrom .params import pncc_code\n\n\ndef is_point_in_tri(point, tri_points):\n    ''' Judge whether the point is in the triangle\n    Method:\n        http://blackpawn.com/texts/pointinpoly/\n    Args:\n        point: [u, v] or [x, y]\n        tri_points: three vertices(2d points) of a triangle. 2 coords x 3 vertices\n    Returns:\n        bool: true for in triangle\n    '''\n    tp = tri_points\n\n    # vectors\n    v0 = tp[:, 2] - tp[:, 0]\n    v1 = tp[:, 1] - tp[:, 0]\n    v2 = point - tp[:, 0]\n\n    # dot products\n    dot00 = np.dot(v0.T, v0)\n    dot01 = np.dot(v0.T, v1)\n    dot02 = np.dot(v0.T, v2)\n    dot11 = np.dot(v1.T, v1)\n    dot12 = np.dot(v1.T, v2)\n\n    # barycentric coordinates\n    if dot00 * dot11 - dot01 * dot01 == 0:\n        inverDeno = 0\n    else:\n        inverDeno = 1 / (dot00 * dot11 - dot01 * dot01)\n\n    u = (dot11 * dot02 - dot01 * dot12) * inverDeno\n    v = (dot00 * dot12 - dot01 * dot02) * inverDeno\n\n    # check if point in triangle\n    return (u >= 0) & (v >= 0) & (u + v < 1)\n\n\ndef render_colors(vertices, colors, tri, h, w, c=3):\n    \"\"\" render mesh by z buffer\n    Args:\n        vertices: 3 x nver\n        colors: 3 x nver\n        tri: 3 x ntri\n        h: height\n        w: width\n    \"\"\"\n    # initial\n    image = np.zeros((h, w, c))\n\n    depth_buffer = np.zeros([h, w]) - 999999.\n    # triangle depth: approximate the depth to the average value of z in each vertex(v0, v1, v2), since the vertices are closed to each other\n    tri_depth = (vertices[2, tri[0, :]] + vertices[2, tri[1, :]] + vertices[2, tri[2, :]]) / 3.\n    tri_tex = (colors[:, tri[0, :]] + colors[:, tri[1, :]] + colors[:, tri[2, :]]) / 3.\n\n    for i in range(tri.shape[1]):\n        tri_idx = tri[:, i]  # 3 vertex indices\n\n        # the inner bounding box\n        umin = max(int(np.ceil(np.min(vertices[0, tri_idx]))), 0)\n        umax = min(int(np.floor(np.max(vertices[0, tri_idx]))), w - 1)\n\n        vmin = max(int(np.ceil(np.min(vertices[1, tri_idx]))), 0)\n        vmax = min(int(np.floor(np.max(vertices[1, tri_idx]))), h - 1)\n\n        if umax < umin or vmax < vmin:\n            continue\n\n        for u in range(umin, umax + 1):\n            for v in range(vmin, vmax + 1):\n                if tri_depth[i] > depth_buffer[v, u] and is_point_in_tri([u, v], vertices[:2, tri_idx]):\n                    depth_buffer[v, u] = tri_depth[i]\n                    image[v, u, :] = tri_tex[:, i]\n    return image\n\n\ndef get_depths_image(img, vertices_lst, tri):\n    h, w = img.shape[:2]\n    c = 1\n\n    depths_img = np.zeros((h, w, c))\n    for i in range(len(vertices_lst)):\n        vertices = vertices_lst[i]\n\n        z = vertices[2, :]\n        z_min, z_max = min(z), max(z)\n        vertices[2, :] = (z - z_min) / (z_max - z_min)\n\n        z = vertices[2:, :]\n        depth_img = render_colors(vertices.T, z.T, tri.T, h, w, 1)\n        depths_img[depth_img > 0] = depth_img[depth_img > 0]\n\n    depths_img = depths_img.squeeze() * 255\n    return depths_img\n\n\ndef crender_colors(vertices, triangles, colors, h, w, c=3, BG=None):\n    \"\"\" render mesh with colors\n    Args:\n        vertices: [nver, 3]\n        triangles: [ntri, 3]\n        colors: [nver, 3]\n        h: height\n        w: width\n        c: channel\n        BG: background image\n    Returns:\n        image: [h, w, c]. rendered image./rendering.\n    \"\"\"\n\n    if BG is None:\n        image = np.zeros((h, w, c), dtype=np.float32)\n    else:\n        assert BG.shape[0] == h and BG.shape[1] == w and BG.shape[2] == c\n        image = BG.astype(np.float32).copy(order='C')\n    depth_buffer = np.zeros([h, w], dtype=np.float32, order='C') - 999999.\n\n    # to C order\n    vertices = vertices.astype(np.float32).copy(order='C')\n    triangles = triangles.astype(np.int32).copy(order='C')\n    colors = colors.astype(np.float32).copy(order='C')\n\n    mesh_core_cython.render_colors_core(\n        image, vertices, triangles,\n        colors,\n        depth_buffer,\n        vertices.shape[0], triangles.shape[0],\n        h, w, c\n    )\n    return image\n\n\ndef cget_depths_image(img, vertices_lst, tri):\n    \"\"\"cython version for depth image render\"\"\"\n    h, w = img.shape[:2]\n    c = 1\n\n    depths_img = np.zeros((h, w, c))\n    for i in range(len(vertices_lst)):\n        vertices = vertices_lst[i]\n\n        z = vertices[2, :]\n        z_min, z_max = min(z), max(z)\n        vertices[2, :] = (z - z_min) / (z_max - z_min)\n        z = vertices[2:, :]\n\n        depth_img = crender_colors(vertices.T, tri.T, z.T, h, w, 1)\n        depths_img[depth_img > 0] = depth_img[depth_img > 0]\n\n    depths_img = depths_img.squeeze() * 255\n    return depths_img\n\n\ndef ncc(vertices):\n    ## simple version\n    # ncc_vertices = np.zeros_like(vertices)\n    # x = vertices[0, :]\n    # y = vertices[1, :]\n    # z = vertices[2, :]\n    #\n    # ncc_vertices[0, :] = (x - min(x)) / (max(x) - min(x))\n    # ncc_vertices[1, :] = (y - min(y)) / (max(y) - min(y))\n    # ncc_vertices[2, :] = (z - min(z)) / (max(z) - min(z))\n\n    # matrix version\n    v_min = np.min(vertices, axis=1).reshape(-1, 1)\n    v_max = np.max(vertices, axis=1).reshape(-1, 1)\n    ncc_vertices = (vertices - v_min) / (v_max - v_min)\n\n    return ncc_vertices\n\n\ndef cpncc(img, vertices_lst, tri):\n    \"\"\"cython version for PNCC render: original paper\"\"\"\n    h, w = img.shape[:2]\n    c = 3\n\n    pnccs_img = np.zeros((h, w, c))\n    for i in range(len(vertices_lst)):\n        vertices = vertices_lst[i]\n        pncc_img = crender_colors(vertices.T, tri.T, pncc_code.T, h, w, c)\n        pnccs_img[pncc_img > 0] = pncc_img[pncc_img > 0]\n\n    pnccs_img = pnccs_img.squeeze() * 255\n    return pnccs_img\n\n\ndef cpncc_v2(img, vertices_lst, tri):\n    \"\"\"cython version for PNCC render\"\"\"\n    h, w = img.shape[:2]\n    c = 3\n\n    pnccs_img = np.zeros((h, w, c))\n    for i in range(len(vertices_lst)):\n        vertices = vertices_lst[i]\n        ncc_vertices = ncc(vertices)\n        pncc_img = crender_colors(vertices.T, tri.T, ncc_vertices.T, h, w, c)\n        pnccs_img[pncc_img > 0] = pncc_img[pncc_img > 0]\n\n    pnccs_img = pnccs_img.squeeze() * 255\n    return pnccs_img\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/FaceBoxes/FaceBoxes.py", "content": "# coding: utf-8\n\nimport os.path as osp\n\nimport torch\nimport numpy as np\nimport cv2\n\nfrom .utils.prior_box import PriorBox\nfrom .utils.nms_wrapper import nms\nfrom .utils.box_utils import decode\nfrom .utils.timer import Timer\nfrom .utils.functions import check_keys, remove_prefix, load_model\nfrom .utils.config import cfg\nfrom .models.faceboxes import FaceBoxesNet\n\n# some global configs\nconfidence_threshold = 0.05\ntop_k = 5000\nkeep_top_k = 750\nnms_threshold = 0.3\nvis_thres = 0.5\nresize = 1\n\nscale_flag = True\nHEIGHT, WIDTH = 720, 1080\n\nmake_abs_path = lambda fn: osp.join(osp.dirname(osp.realpath(__file__)), fn)\npretrained_path = make_abs_path('weights/FaceBoxesProd.pth')\n\n\ndef viz_bbox(img, dets, wfp='out.jpg'):\n    # show\n    for b in dets:\n        if b[4] < vis_thres:\n            continue\n        text = \"{:.4f}\".format(b[4])\n        b = list(map(int, b))\n        cv2.rectangle(img, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n        cx = b[0]\n        cy = b[1] + 12\n        cv2.putText(img, text, (cx, cy), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n    cv2.imwrite(wfp, img)\n    print(f'Viz bbox to {wfp}')\n\n\nclass FaceBoxes:\n    def __init__(self, timer_flag=False):\n        torch.set_grad_enabled(False)\n\n        net = FaceBoxesNet(phase='test', size=None, num_classes=2)  # initialize detector\n        self.net = load_model(net, pretrained_path=pretrained_path, load_to_cpu=True)\n        self.net.eval()\n        # print('Finished loading model!')\n\n        self.timer_flag = timer_flag\n\n    def __call__(self, img_):\n        img_raw = img_.copy()\n\n        # scaling to speed up\n        scale = 1\n        if scale_flag:\n            h, w = img_raw.shape[:2]\n            if h > HEIGHT:\n                scale = HEIGHT / h\n            if w * scale > WIDTH:\n                scale *= WIDTH / (w * scale)\n            # print(scale)\n            if scale == 1:\n                img_raw_scale = img_raw\n            else:\n                h_s = int(scale * h)\n                w_s = int(scale * w)\n                # print(h_s, w_s)\n                img_raw_scale = cv2.resize(img_raw, dsize=(w_s, h_s))\n                # print(img_raw_scale.shape)\n\n            img = np.float32(img_raw_scale)\n        else:\n            img = np.float32(img_raw)\n\n        # forward\n        _t = {'forward_pass': Timer(), 'misc': Timer()}\n        im_height, im_width, _ = img.shape\n        scale_bbox = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n        img -= (104, 117, 123)\n        img = img.transpose(2, 0, 1)\n        img = torch.from_numpy(img).unsqueeze(0)\n\n        _t['forward_pass'].tic()\n        loc, conf = self.net(img)  # forward pass\n        _t['forward_pass'].toc()\n        _t['misc'].tic()\n        priorbox = PriorBox(image_size=(im_height, im_width))\n        priors = priorbox.forward()\n        prior_data = priors.data\n        boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n        if scale_flag:\n            boxes = boxes * scale_bbox / scale / resize\n        else:\n            boxes = boxes * scale_bbox / resize\n\n        boxes = boxes.cpu().numpy()\n        scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n\n        # ignore low scores\n        inds = np.where(scores > confidence_threshold)[0]\n        boxes = boxes[inds]\n        scores = scores[inds]\n\n        # keep top-K before NMS\n        order = scores.argsort()[::-1][:top_k]\n        boxes = boxes[order]\n        scores = scores[order]\n\n        # do NMS\n        dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n        keep = nms(dets, nms_threshold)\n        dets = dets[keep, :]\n\n        # keep top-K faster NMS\n        dets = dets[:keep_top_k, :]\n        _t['misc'].toc()\n\n        if self.timer_flag:\n            print('Detection: {:d}/{:d} forward_pass_time: {:.4f}s misc: {:.4f}s'.format(1, 1, _t[\n                'forward_pass'].average_time, _t['misc'].average_time))\n\n        # filter using vis_thres\n        det_bboxes = []\n        for b in dets:\n            if b[4] > vis_thres:\n                xmin, ymin, xmax, ymax, score = b[0], b[1], b[2], b[3], b[4]\n                bbox = [xmin, ymin, xmax, ymax, score]\n                det_bboxes.append(bbox)\n\n        return det_bboxes\n\n\ndef main():\n    face_boxes = FaceBoxes(timer_flag=True)\n\n    fn = 'trump_hillary.jpg'\n    img_fp = f'../examples/inputs/{fn}'\n    img = cv2.imread(img_fp)\n    print(f'input shape: {img.shape}')\n    dets = face_boxes(img)  # xmin, ymin, w, h\n    # print(dets)\n\n    # repeating inference for `n` times\n    n = 10\n    for i in range(n):\n        dets = face_boxes(img)\n\n    wfn = fn.replace('.jpg', '_det.jpg')\n    wfp = osp.join('../examples/results', wfn)\n    viz_bbox(img, dets, wfp)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/video_demo.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\nimport torch\nimport torchvision.transforms as transforms\nimport mobilenet_v1\nimport numpy as np\nimport cv2\nimport dlib\nfrom utils.ddfa import ToTensorGjz, NormalizeGjz\nimport scipy.io as sio\nfrom utils.inference import (\n    parse_roi_box_from_landmark,\n    crop_img,\n    predict_68pts,\n    predict_dense,\n)\nfrom utils.cv_plot import plot_kpt\nfrom utils.render import get_depths_image, cget_depths_image, cpncc\nfrom utils.paf import gen_img_paf\nimport argparse\nimport torch.backends.cudnn as cudnn\n\nSTD_SIZE = 120\n\n\ndef main(args):\n    # 0. open video\n    # vc = cv2.VideoCapture(str(args.video) if len(args.video) == 1 else args.video)\n    vc = cv2.VideoCapture(args.video if int(args.video) != 0 else 0)\n\n    # 1. load pre-tained model\n    checkpoint_fp = 'models/phase1_wpdc_vdc.pth.tar'\n    arch = 'mobilenet_1'\n\n    tri = sio.loadmat('visualize/tri.mat')['tri']\n    transform = transforms.Compose([ToTensorGjz(), NormalizeGjz(mean=127.5, std=128)])\n\n    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)[\n        'state_dict'\n    ]\n    model = getattr(mobilenet_v1, arch)(\n        num_classes=62\n    )  # 62 = 12(pose) + 40(shape) +10(expression)\n\n    model_dict = model.state_dict()\n    # because the model is trained by multiple gpus, prefix module should be removed\n    for k in checkpoint.keys():\n        model_dict[k.replace('module.', '')] = checkpoint[k]\n    model.load_state_dict(model_dict)\n    if args.mode == 'gpu':\n        cudnn.benchmark = True\n        model = model.cuda()\n    model.eval()\n\n    # 2. load dlib model for face detection and landmark used for face cropping\n    dlib_landmark_model = 'models/shape_predictor_68_face_landmarks.dat'\n    face_regressor = dlib.shape_predictor(dlib_landmark_model)\n    face_detector = dlib.get_frontal_face_detector()\n\n    # 3. forward\n    success, frame = vc.read()\n    last_frame_pts = []\n\n    while success:\n        if len(last_frame_pts) == 0:\n            rects = face_detector(frame, 1)\n            for rect in rects:\n                pts = face_regressor(frame, rect).parts()\n                pts = np.array([[pt.x, pt.y] for pt in pts]).T\n                last_frame_pts.append(pts)\n\n        vertices_lst = []\n        for lmk in last_frame_pts:\n            roi_box = parse_roi_box_from_landmark(lmk)\n            img = crop_img(frame, roi_box)\n            img = cv2.resize(\n                img, dsize=(STD_SIZE, STD_SIZE), interpolation=cv2.INTER_LINEAR\n            )\n            input = transform(img).unsqueeze(0)\n            with torch.no_grad():\n                if args.mode == 'gpu':\n                    input = input.cuda()\n                param = model(input)\n                param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n            pts68 = predict_68pts(param, roi_box)\n            vertex = predict_dense(param, roi_box)\n            lmk[:] = pts68[:2]\n            vertices_lst.append(vertex)\n\n        pncc = cpncc(frame, vertices_lst, tri - 1) / 255.0\n        frame = frame / 255.0 * (1.0 - pncc)\n        cv2.imshow('3ddfa', frame)\n        cv2.waitKey(1)\n        success, frame = vc.read()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='3DDFA inference pipeline')\n    parser.add_argument(\n        '-v',\n        '--video',\n        default='0',\n        type=str,\n        help='video file path or opencv cam index',\n    )\n    parser.add_argument('-m', '--mode', default='cpu', type=str, help='gpu or cpu mode')\n\n    args = parser.parse_args()\n    main(args)\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/utils/params.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport os.path as osp\nimport numpy as np\nfrom .io import _load\n\n\ndef make_abs_path(d):\n    return osp.join(osp.dirname(osp.realpath(__file__)), d)\n\n\nd = make_abs_path('../train.configs')\nkeypoints = _load(osp.join(d, 'keypoints_sim.npy'))\nw_shp = _load(osp.join(d, 'w_shp_sim.npy'))\nw_exp = _load(osp.join(d, 'w_exp_sim.npy'))  # simplified version\nmeta = _load(osp.join(d, 'param_whitening.pkl'))\n# param_mean and param_std are used for re-whitening\nparam_mean = meta.get('param_mean')\nparam_std = meta.get('param_std')\nu_shp = _load(osp.join(d, 'u_shp.npy'))\nu_exp = _load(osp.join(d, 'u_exp.npy'))\nu = u_shp + u_exp\nw = np.concatenate((w_shp, w_exp), axis=1)\nw_base = w[keypoints]\nw_norm = np.linalg.norm(w, axis=0)\nw_base_norm = np.linalg.norm(w_base, axis=0)\n\n# for inference\ndim = w_shp.shape[0] // 3\nu_base = u[keypoints].reshape(-1, 1)\nw_shp_base = w_shp[keypoints]\nw_exp_base = w_exp[keypoints]\nstd_size = 120\n\n# for paf (pac)\npaf = _load(osp.join(d, 'Model_PAF.pkl'))\nu_filter = paf.get('mu_filter')\nw_filter = paf.get('w_filter')\nw_exp_filter = paf.get('w_exp_filter')\n\n# pncc code (mean shape)\npncc_code = _load(osp.join(d, 'pncc_code.npy'))\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/FaceBoxes/models/__init__.py", "content": ""}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/utils/cython/__init__.py", "content": ""}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/utils/estimate_pose.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\n\"\"\"\nReference: https://github.com/YadiraF/PRNet/blob/master/utils/estimate_pose.py\n\"\"\"\n\nfrom math import cos, sin, atan2, asin, sqrt\nimport numpy as np\nfrom .params import param_mean, param_std\n\n\ndef parse_pose(param):\n    param = param * param_std + param_mean\n    Ps = param[:12].reshape(3, -1)  # camera matrix\n    # R = P[:, :3]\n    s, R, t3d = P2sRt(Ps)\n    P = np.concatenate((R, t3d.reshape(3, -1)), axis=1)  # without scale\n    # P = Ps / s\n    pose = matrix2angle(R)  # yaw, pitch, roll\n    # offset = p_[:, -1].reshape(3, 1)\n    return P, pose\n\n\ndef matrix2angle(R):\n    ''' compute three Euler angles from a Rotation Matrix. Ref: http://www.gregslabaugh.net/publications/euler.pdf\n    Args:\n        R: (3,3). rotation matrix\n    Returns:\n        x: yaw\n        y: pitch\n        z: roll\n    '''\n    # assert(isRotationMatrix(R))\n\n    if R[2, 0] != 1 and R[2, 0] != -1:\n        x = asin(R[2, 0])\n        y = atan2(R[2, 1] / cos(x), R[2, 2] / cos(x))\n        z = atan2(R[1, 0] / cos(x), R[0, 0] / cos(x))\n\n    else:  # Gimbal lock\n        z = 0  # can be anything\n        if R[2, 0] == -1:\n            x = np.pi / 2\n            y = z + atan2(R[0, 1], R[0, 2])\n        else:\n            x = -np.pi / 2\n            y = -z + atan2(-R[0, 1], -R[0, 2])\n\n    return x, y, z\n\n\ndef P2sRt(P):\n    ''' decompositing camera matrix P.\n    Args:\n        P: (3, 4). Affine Camera Matrix.\n    Returns:\n        s: scale factor.\n        R: (3, 3). rotation matrix.\n        t2d: (2,). 2d translation.\n    '''\n    t3d = P[:, 3]\n    R1 = P[0:1, :3]\n    R2 = P[1:2, :3]\n    s = (np.linalg.norm(R1) + np.linalg.norm(R2)) / 2.0\n    r1 = R1 / np.linalg.norm(R1)\n    r2 = R2 / np.linalg.norm(R2)\n    r3 = np.cross(r1, r2)\n\n    R = np.concatenate((r1, r2, r3), 0)\n    return s, R, t3d\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/utils/cython/setup.py", "content": "'''\npython setup.py build_ext -i\nto compile\n'''\n\n# setup.py\nfrom distutils.core import setup, Extension\n# from Cython.Build import cythonize\nfrom Cython.Distutils import build_ext\nimport numpy\n\nsetup(\n    name='mesh_core_cython',\n    cmdclass={'build_ext': build_ext},\n    ext_modules=[Extension(\"mesh_core_cython\",\n                           sources=[\"mesh_core_cython.pyx\", \"mesh_core.cpp\"],\n                           language='c++',\n                           include_dirs=[numpy.get_include()])],\n)\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/visualize.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nfrom benchmark import extract_param\nfrom utils.ddfa import reconstruct_vertex\nfrom utils.io import _dump, _load\nimport os.path as osp\nfrom skimage import io\nimport matplotlib.pyplot as plt\nfrom benchmark_aflw2000 import convert_to_ori\nimport scipy.io as sio\n\n\ndef aflw2000():\n    arch = 'mobilenet_1'\n    device_ids = [0]\n    checkpoint_fp = 'models/phase1_wpdc_vdc.pth.tar'\n\n    params = extract_param(\n        checkpoint_fp=checkpoint_fp,\n        root='test.data/AFLW2000-3D_crop',\n        filelists='test.data/AFLW2000-3D_crop.list',\n        arch=arch,\n        device_ids=device_ids,\n        batch_size=128)\n    _dump('res/params_aflw2000.npy', params)\n\n\ndef draw_landmarks():\n    filelists = 'test.data/AFLW2000-3D_crop.list'\n    root = 'AFLW-2000-3D/'\n    fns = open(filelists).read().strip().split('\\n')\n    params = _load('res/params_aflw2000.npy')\n\n    for i in range(2000):\n        plt.close()\n        img_fp = osp.join(root, fns[i])\n        img = io.imread(img_fp)\n        lms = reconstruct_vertex(params[i], dense=False)\n        lms = convert_to_ori(lms, i)\n\n        # print(lms.shape)\n        fig = plt.figure(figsize=plt.figaspect(.5))\n        # fig = plt.figure(figsize=(8, 4))\n        ax = fig.add_subplot(1, 2, 1)\n        ax.imshow(img)\n\n        alpha = 0.8\n        markersize = 4\n        lw = 1.5\n        color = 'w'\n        markeredgecolor = 'black'\n\n        nums = [0, 17, 22, 27, 31, 36, 42, 48, 60, 68]\n        for ind in range(len(nums) - 1):\n            l, r = nums[ind], nums[ind + 1]\n            ax.plot(lms[0, l:r], lms[1, l:r], color=color, lw=lw, alpha=alpha - 0.1)\n\n            ax.plot(lms[0, l:r], lms[1, l:r], marker='o', linestyle='None', markersize=markersize, color=color,\n                    markeredgecolor=markeredgecolor, alpha=alpha)\n\n        ax.axis('off')\n\n        # 3D\n        ax = fig.add_subplot(1, 2, 2, projection='3d')\n        lms[1] = img.shape[1] - lms[1]\n        lms[2] = -lms[2]\n\n        # print(lms)\n        ax.scatter(lms[0], lms[2], lms[1], c=\"cyan\", alpha=1.0, edgecolor='b')\n\n        for ind in range(len(nums) - 1):\n            l, r = nums[ind], nums[ind + 1]\n            ax.plot3D(lms[0, l:r], lms[2, l:r], lms[1, l:r], color='blue')\n\n        ax.view_init(elev=5., azim=-95)\n        # ax.set_xlabel('x')\n        # ax.set_ylabel('y')\n        # ax.set_zlabel('z')\n\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        ax.set_zticklabels([])\n\n        plt.tight_layout()\n        # plt.show()\n\n        wfp = f'res/AFLW-2000-3D/{osp.basename(img_fp)}'\n        plt.savefig(wfp, dpi=200)\n\n\ndef gen_3d_vertex():\n    filelists = 'test.data/AFLW2000-3D_crop.list'\n    root = 'AFLW-2000-3D/'\n    fns = open(filelists).read().strip().split('\\n')\n    params = _load('res/params_aflw2000.npy')\n\n    sel = ['00427', '00439', '00475', '00477', '00497', '00514', '00562', '00623', '01045', '01095', '01104', '01506',\n           '01621', '02214', '02244', '03906', '04157']\n    sel = list(map(lambda x: f'image{x}.jpg', sel))\n    for i in range(2000):\n        fn = fns[i]\n        if fn in sel:\n            vertex = reconstruct_vertex(params[i], dense=True)\n            wfp = osp.join('res/AFLW-2000-3D_vertex/', fn.replace('.jpg', '.mat'))\n            print(wfp)\n            sio.savemat(wfp, {'vertex': vertex})\n\n\ndef main():\n    # step1: extract params\n    # aflw2000()\n\n    # step2: draw landmarks\n    # draw_landmarks()\n\n    # step3: visual 3d vertex\n    gen_3d_vertex()\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/utils/lighting.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport sys\n\nsys.path.append('../')\nimport numpy as np\nfrom utils import render\nfrom utils.cython import mesh_core_cython\n\n_norm = lambda arr: arr / np.sqrt(np.sum(arr ** 2, axis=1))[:, None]\n\n\ndef norm_vertices(vertices):\n    vertices -= vertices.min(0)[None, :]\n    vertices /= vertices.max()\n    vertices *= 2\n    vertices -= vertices.max(0)[None, :] / 2\n    return vertices\n\n\ndef convert_type(obj):\n    if isinstance(obj, tuple) or isinstance(obj, list):\n        return np.array(obj, dtype=np.float32)[None, :]\n    return obj\n\n\nclass RenderPipeline(object):\n    def __init__(self, **kwargs):\n        self.intensity_ambient = convert_type(kwargs.get('intensity_ambient', 0.3))\n        self.intensity_directional = convert_type(kwargs.get('intensity_directional', 0.6))\n        self.intensity_specular = convert_type(kwargs.get('intensity_specular', 0.9))\n        self.specular_exp = kwargs.get('specular_exp', 5)\n        self.color_ambient = convert_type(kwargs.get('color_ambient', (1, 1, 1)))\n        self.color_directional = convert_type(kwargs.get('color_directional', (1, 1, 1)))\n        self.light_pos = convert_type(kwargs.get('light_pos', (0, 0, 1)))\n        self.view_pos = convert_type(kwargs.get('view_pos', (0, 0, 1)))\n\n    def update_light_pos(self, light_pos):\n        self.light_pos = convert_type(light_pos)\n\n    def __call__(self, vertices, triangles, background):\n        height, width = background.shape[:2]\n\n        # 1. compute triangle/face normals and vertex normals\n        # ## Old style: very slow\n        # normal = np.zeros((vertices.shape[0], 3), dtype=np.float32)\n        # # surface_count = np.zeros((vertices.shape[0], 1))\n        # for i in range(triangles.shape[0]):\n        #     i1, i2, i3 = triangles[i, :]\n        #     v1, v2, v3 = vertices[[i1, i2, i3], :]\n        #     surface_normal = np.cross(v2 - v1, v3 - v1)\n        #     normal[[i1, i2, i3], :] += surface_normal\n        #     # surface_count[[i1, i2, i3], :] += 1\n        #\n        # # normal /= surface_count\n        # # normal /= np.linalg.norm(normal, axis=1, keepdims=True)\n        # normal = _norm(normal)\n\n        # Cython style\n        normal = np.zeros((vertices.shape[0], 3), dtype=np.float32)\n        mesh_core_cython.get_normal(normal, vertices, triangles, vertices.shape[0], triangles.shape[0])\n\n        # 2. lighting\n        color = np.zeros_like(vertices, dtype=np.float32)\n        # ambient component\n        if self.intensity_ambient > 0:\n            color += self.intensity_ambient * self.color_ambient\n\n        vertices_n = norm_vertices(vertices.copy())\n        if self.intensity_directional > 0:\n            # diffuse component\n            direction = _norm(self.light_pos - vertices_n)\n            cos = np.sum(normal * direction, axis=1)[:, None]\n            # cos = np.clip(cos, 0, 1)\n            #  todo: check below\n            color += self.intensity_directional * (self.color_directional * np.clip(cos, 0, 1))\n\n            # specular component\n            if self.intensity_specular > 0:\n                v2v = _norm(self.view_pos - vertices_n)\n                reflection = 2 * cos * normal - direction\n                spe = np.sum((v2v * reflection) ** self.specular_exp, axis=1)[:, None]\n                spe = np.where(cos != 0, np.clip(spe, 0, 1), np.zeros_like(spe))\n                color += self.intensity_specular * self.color_directional * np.clip(spe, 0, 1)\n        color = np.clip(color, 0, 1)\n\n        # 2. rasterization, [0, 1]\n        render_img = render.crender_colors(vertices, triangles, color, height, width, BG=background)\n        render_img = (render_img * 255).astype(np.uint8)\n        return render_img\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/FaceBoxes/utils/__init__.py", "content": ""}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/FaceBoxes/__init__.py", "content": "from .FaceBoxes import FaceBoxes\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/utils/inference.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n__author__ = 'cleardusk'\n\nimport numpy as np\nfrom math import sqrt\nimport scipy.io as sio\nimport matplotlib.pyplot as plt\nfrom .ddfa import reconstruct_vertex\n\n\ndef get_suffix(filename):\n    \"\"\"a.jpg -> jpg\"\"\"\n    pos = filename.rfind('.')\n    if pos == -1:\n        return ''\n    return filename[pos:]\n\n\ndef crop_img(img, roi_box):\n    h, w = img.shape[:2]\n\n    sx, sy, ex, ey = [int(round(_)) for _ in roi_box]\n    dh, dw = ey - sy, ex - sx\n    if len(img.shape) == 3:\n        res = np.zeros((dh, dw, 3), dtype=np.uint8)\n    else:\n        res = np.zeros((dh, dw), dtype=np.uint8)\n    if sx < 0:\n        sx, dsx = 0, -sx\n    else:\n        dsx = 0\n\n    if ex > w:\n        ex, dex = w, dw - (ex - w)\n    else:\n        dex = dw\n\n    if sy < 0:\n        sy, dsy = 0, -sy\n    else:\n        dsy = 0\n\n    if ey > h:\n        ey, dey = h, dh - (ey - h)\n    else:\n        dey = dh\n\n    res[dsy:dey, dsx:dex] = img[sy:ey, sx:ex]\n    return res\n\n\ndef calc_hypotenuse(pts):\n    bbox = [min(pts[0, :]), min(pts[1, :]), max(pts[0, :]), max(pts[1, :])]\n    center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n    radius = max(bbox[2] - bbox[0], bbox[3] - bbox[1]) / 2\n    bbox = [center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius]\n    llength = sqrt((bbox[2] - bbox[0]) ** 2 + (bbox[3] - bbox[1]) ** 2)\n    return llength / 3\n\n\ndef parse_roi_box_from_landmark(pts):\n    \"\"\"calc roi box from landmark\"\"\"\n    bbox = [min(pts[0, :]), min(pts[1, :]), max(pts[0, :]), max(pts[1, :])]\n    center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n    radius = max(bbox[2] - bbox[0], bbox[3] - bbox[1]) / 2\n    bbox = [center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius]\n\n    llength = sqrt((bbox[2] - bbox[0]) ** 2 + (bbox[3] - bbox[1]) ** 2)\n    center_x = (bbox[2] + bbox[0]) / 2\n    center_y = (bbox[3] + bbox[1]) / 2\n\n    roi_box = [0] * 4\n    roi_box[0] = center_x - llength / 2\n    roi_box[1] = center_y - llength / 2\n    roi_box[2] = roi_box[0] + llength\n    roi_box[3] = roi_box[1] + llength\n\n    return roi_box\n\n\ndef parse_roi_box_from_bbox(bbox):\n    left, top, right, bottom = bbox\n    old_size = (right - left + bottom - top) / 2\n    center_x = right - (right - left) / 2.0\n    center_y = bottom - (bottom - top) / 2.0 + old_size * 0.14\n    size = int(old_size * 1.58)\n    roi_box = [0] * 4\n    roi_box[0] = center_x - size / 2\n    roi_box[1] = center_y - size / 2\n    roi_box[2] = roi_box[0] + size\n    roi_box[3] = roi_box[1] + size\n    return roi_box\n\n\ndef dump_to_ply(vertex, tri, wfp):\n    header = \"\"\"ply\n    format ascii 1.0\n    element vertex {}\n    property float x\n    property float y\n    property float z\n    element face {}\n    property list uchar int vertex_indices\n    end_header\"\"\"\n\n    n_vertex = vertex.shape[1]\n    n_face = tri.shape[1]\n    header = header.format(n_vertex, n_face)\n\n    with open(wfp, 'w') as f:\n        f.write(header + '\\n')\n        for i in range(n_vertex):\n            x, y, z = vertex[:, i]\n            f.write('{:.4f} {:.4f} {:.4f}\\n'.format(x, y, z))\n        for i in range(n_face):\n            idx1, idx2, idx3 = tri[:, i]\n            f.write('3 {} {} {}\\n'.format(idx1 - 1, idx2 - 1, idx3 - 1))\n    print('Dump tp {}'.format(wfp))\n\n\ndef dump_vertex(vertex, wfp):\n    sio.savemat(wfp, {'vertex': vertex})\n    print('Dump to {}'.format(wfp))\n\n\ndef _predict_vertices(param, roi_bbox, dense, transform=True):\n    vertex = reconstruct_vertex(param, dense=dense)\n    sx, sy, ex, ey = roi_bbox\n    scale_x = (ex - sx) / 120\n    scale_y = (ey - sy) / 120\n    vertex[0, :] = vertex[0, :] * scale_x + sx\n    vertex[1, :] = vertex[1, :] * scale_y + sy\n\n    s = (scale_x + scale_y) / 2\n    vertex[2, :] *= s\n\n    return vertex\n\n\ndef predict_68pts(param, roi_box):\n    return _predict_vertices(param, roi_box, dense=False)\n\n\ndef predict_dense(param, roi_box):\n    return _predict_vertices(param, roi_box, dense=True)\n\n\ndef draw_landmarks(img, pts, style='fancy', wfp=None, show_flg=False, **kwargs):\n    \"\"\"Draw landmarks using matplotlib\"\"\"\n    height, width = img.shape[:2]\n    plt.figure(figsize=(12, height / width * 12))\n    plt.imshow(img[:, :, ::-1])\n    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n    plt.axis('off')\n\n    if not type(pts) in [tuple, list]:\n        pts = [pts]\n    for i in range(len(pts)):\n        if style == 'simple':\n            plt.plot(pts[i][0, :], pts[i][1, :], 'o', markersize=4, color='g')\n\n        elif style == 'fancy':\n            alpha = 0.8\n            markersize = 4\n            lw = 1.5\n            color = kwargs.get('color', 'w')\n            markeredgecolor = kwargs.get('markeredgecolor', 'black')\n\n            nums = [0, 17, 22, 27, 31, 36, 42, 48, 60, 68]\n\n            # close eyes and mouths\n            plot_close = lambda i1, i2: plt.plot([pts[i][0, i1], pts[i][0, i2]], [pts[i][1, i1], pts[i][1, i2]],\n                                                 color=color, lw=lw, alpha=alpha - 0.1)\n            plot_close(41, 36)\n            plot_close(47, 42)\n            plot_close(59, 48)\n            plot_close(67, 60)\n\n            for ind in range(len(nums) - 1):\n                l, r = nums[ind], nums[ind + 1]\n                plt.plot(pts[i][0, l:r], pts[i][1, l:r], color=color, lw=lw, alpha=alpha - 0.1)\n\n                plt.plot(pts[i][0, l:r], pts[i][1, l:r], marker='o', linestyle='None', markersize=markersize,\n                         color=color,\n                         markeredgecolor=markeredgecolor, alpha=alpha)\n\n    if wfp is not None:\n        plt.savefig(wfp, dpi=200)\n        print('Save visualization result to {}'.format(wfp))\n    if show_flg:\n        plt.show()\n\n\ndef get_colors(image, vertices):\n    [h, w, _] = image.shape\n    vertices[0, :] = np.minimum(np.maximum(vertices[0, :], 0), w - 1)  # x\n    vertices[1, :] = np.minimum(np.maximum(vertices[1, :], 0), h - 1)  # y\n    ind = np.round(vertices).astype(np.int32)\n    colors = image[ind[1, :], ind[0, :], :]  # n x 3\n\n    return colors\n\n\ndef write_obj_with_colors(obj_name, vertices, triangles, colors):\n    triangles = triangles.copy() # meshlab start with 1\n\n    if obj_name.split('.')[-1] != 'obj':\n        obj_name = obj_name + '.obj'\n\n    # write obj\n    with open(obj_name, 'w') as f:\n        # write vertices & colors\n        for i in range(vertices.shape[1]):\n            s = 'v {:.4f} {:.4f} {:.4f} {} {} {}\\n'.format(vertices[1, i], vertices[0, i], vertices[2, i], colors[i, 2],\n                                               colors[i, 1], colors[i, 0])\n            f.write(s)\n\n        # write f: ver ind/ uv ind\n        for i in range(triangles.shape[1]):\n            s = 'f {} {} {}\\n'.format(triangles[0, i], triangles[1, i], triangles[2, i])\n            f.write(s)\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/wpdc_loss.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nfrom math import sqrt\nfrom utils.io import _numpy_to_cuda\nfrom utils.params import *\n\n_to_tensor = _numpy_to_cuda  # gpu\n\n\ndef _parse_param_batch(param):\n    \"\"\"Work for both numpy and tensor\"\"\"\n    N = param.shape[0]\n    p_ = param[:, :12].view(N, 3, -1)\n    p = p_[:, :, :3]\n    offset = p_[:, :, -1].view(N, 3, 1)\n    alpha_shp = param[:, 12:52].view(N, -1, 1)\n    alpha_exp = param[:, 52:].view(N, -1, 1)\n    return p, offset, alpha_shp, alpha_exp\n\n\nclass WPDCLoss(nn.Module):\n    \"\"\"Input and target are all 62-d param\"\"\"\n\n    def __init__(self, opt_style='resample', resample_num=132):\n        super(WPDCLoss, self).__init__()\n        self.opt_style = opt_style\n        self.param_mean = _to_tensor(param_mean)\n        self.param_std = _to_tensor(param_std)\n\n        self.u = _to_tensor(u)\n        self.w_shp = _to_tensor(w_shp)\n        self.w_exp = _to_tensor(w_exp)\n        self.w_norm = _to_tensor(w_norm)\n\n        self.w_shp_length = self.w_shp.shape[0] // 3\n        self.keypoints = _to_tensor(keypoints)\n        self.resample_num = resample_num\n\n    def reconstruct_and_parse(self, input, target):\n        # reconstruct\n        param = input * self.param_std + self.param_mean\n        param_gt = target * self.param_std + self.param_mean\n\n        # parse param\n        p, offset, alpha_shp, alpha_exp = _parse_param_batch(param)\n        pg, offsetg, alpha_shpg, alpha_expg = _parse_param_batch(param_gt)\n\n        return (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg)\n\n    def _calc_weights_resample(self, input_, target_):\n        # resample index\n        if self.resample_num <= 0:\n            keypoints_mix = self.keypoints\n        else:\n            index = torch.randperm(self.w_shp_length)[:self.resample_num].reshape(-1, 1)\n            keypoints_resample = torch.cat((3 * index, 3 * index + 1, 3 * index + 2), dim=1).view(-1).cuda()\n            keypoints_mix = torch.cat((self.keypoints, keypoints_resample))\n        w_shp_base = self.w_shp[keypoints_mix]\n        u_base = self.u[keypoints_mix]\n        w_exp_base = self.w_exp[keypoints_mix]\n\n        input = torch.tensor(input_.data.clone(), requires_grad=False)\n        target = torch.tensor(target_.data.clone(), requires_grad=False)\n\n        (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg) \\\n            = self.reconstruct_and_parse(input, target)\n\n        input = self.param_std * input + self.param_mean\n        target = self.param_std * target + self.param_mean\n\n        N = input.shape[0]\n\n        offset[:, -1] = offsetg[:, -1]\n\n        weights = torch.zeros_like(input, dtype=torch.float)\n        tmpv = (u_base + w_shp_base @ alpha_shpg + w_exp_base @ alpha_expg).view(N, -1, 3).permute(0, 2, 1)\n\n        tmpv_norm = torch.norm(tmpv, dim=2)\n        offset_norm = sqrt(w_shp_base.shape[0] // 3)\n\n        # for pose\n        param_diff_pose = torch.abs(input[:, :11] - target[:, :11])\n        for ind in range(11):\n            if ind in [0, 4, 8]:\n                weights[:, ind] = param_diff_pose[:, ind] * tmpv_norm[:, 0]\n            elif ind in [1, 5, 9]:\n                weights[:, ind] = param_diff_pose[:, ind] * tmpv_norm[:, 1]\n            elif ind in [2, 6, 10]:\n                weights[:, ind] = param_diff_pose[:, ind] * tmpv_norm[:, 2]\n            else:\n                weights[:, ind] = param_diff_pose[:, ind] * offset_norm\n\n        ## This is the optimizest version\n        # for shape_exp\n        magic_number = 0.00057339936  # scale\n        param_diff_shape_exp = torch.abs(input[:, 12:] - target[:, 12:])\n        # weights[:, 12:] = magic_number * param_diff_shape_exp * self.w_norm\n        w = torch.cat((w_shp_base, w_exp_base), dim=1)\n        w_norm = torch.norm(w, dim=0)\n        # print('here')\n        weights[:, 12:] = magic_number * param_diff_shape_exp * w_norm\n\n        eps = 1e-6\n        weights[:, :11] += eps\n        weights[:, 12:] += eps\n\n        # normalize the weights\n        maxes, _ = weights.max(dim=1)\n        maxes = maxes.view(-1, 1)\n        weights /= maxes\n\n        # zero the z\n        weights[:, 11] = 0\n\n        return weights\n\n    def forward(self, input, target, weights_scale=10):\n        if self.opt_style == 'resample':\n            weights = self._calc_weights_resample(input, target)\n            loss = weights * (input - target) ** 2\n            return loss.mean()\n        else:\n            raise Exception(f'Unknown opt style: {self.opt_style}')\n\n\nif __name__ == '__main__':\n    pass\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/3DDFA/utils/paf.py", "content": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport numpy as np\nfrom .ddfa import _parse_param\nfrom .params import u_filter, w_filter, w_exp_filter, std_size, param_mean, param_std\n\n\ndef reconstruct_paf_anchor(param, whitening=True):\n    if whitening:\n        param = param * param_std + param_mean\n    p, offset, alpha_shp, alpha_exp = _parse_param(param)\n    anchor = p @ (u_filter + w_filter @ alpha_shp + w_exp_filter @ alpha_exp).reshape(3, -1, order='F') + offset\n    anchor[1, :] = std_size + 1 - anchor[1, :]\n    return anchor[:2, :]\n\n\ndef gen_offsets(kernel_size):\n    offsets = np.zeros((2, kernel_size * kernel_size), dtype=np.int)\n    ind = 0\n    delta = (kernel_size - 1) // 2\n    for i in range(kernel_size):\n        y = i - delta\n        for j in range(kernel_size):\n            x = j - delta\n            offsets[0, ind] = x\n            offsets[1, ind] = y\n            ind += 1\n    return offsets\n\n\ndef gen_img_paf(img_crop, param, kernel_size=3):\n    \"\"\"Generate PAF image\n    img_crop: 120x120\n    kernel_size: kernel_size for convolution, should be even number like 3 or 5 or ...\n    \"\"\"\n    anchor = reconstruct_paf_anchor(param)\n    anchor = np.round(anchor).astype(np.int)\n    delta = (kernel_size - 1) // 2\n    anchor[anchor < delta] = delta\n    anchor[anchor >= std_size - delta - 1] = std_size - delta - 1\n\n    img_paf = np.zeros((64 * kernel_size, 64 * kernel_size, 3), dtype=np.uint8)\n    offsets = gen_offsets(kernel_size)\n    for i in range(kernel_size * kernel_size):\n        ox, oy = offsets[:, i]\n        index0 = anchor[0] + ox\n        index1 = anchor[1] + oy\n        p = img_crop[index1, index0].reshape(64, 64, 3).transpose(1, 0, 2)\n\n        img_paf[oy + delta::kernel_size, ox + delta::kernel_size] = p\n\n    return img_paf\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/FaceBoxes/utils/config.py", "content": "# coding: utf-8\n\ncfg = {\n    'name': 'FaceBoxes',\n    'min_sizes': [[32, 64, 128], [256], [512]],\n    'steps': [32, 64, 128],\n    'variance': [0.1, 0.2],\n    'clip': False\n}\n"}
{"type": "source_file", "path": "preprocess/3DDFA_V2-master/FaceBoxes/utils/build.py", "content": "# coding: utf-8\n\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\n\ndef find_in_path(name, path):\n    \"Find a file in a search path\"\n    # adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        # customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    Extension(\n        \"nms.cpu_nms\",\n        [\"nms/cpu_nms.pyx\"],\n        # extra_compile_args={'gcc': [\"-Wno-cpp\", \"-Wno-unused-function\"]},\n        extra_compile_args=[\"-Wno-cpp\", \"-Wno-unused-function\"],\n        include_dirs=[numpy_include]\n    )\n]\n\nsetup(\n    name='mot_utils',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={'build_ext': custom_build_ext},\n)\n"}
