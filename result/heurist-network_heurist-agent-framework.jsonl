{"repo_info": {"repo_name": "heurist-agent-framework", "repo_owner": "heurist-network", "repo_url": "https://github.com/heurist-network/heurist-agent-framework"}}
{"type": "test_file", "path": "core/heurist_image/test_image_gen.py", "content": "import asyncio\nimport os\nimport sys\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\n\n# Add the root directory to Python path\nroot_dir = str(Path(__file__).parent.parent)\nif root_dir not in sys.path:\n    sys.path.append(root_dir)\n\nfrom heurist_image.ImageGen import ImageGen  # noqa: E402\nfrom heurist_image.SmartGen import SmartGen  # noqa: E402\n\n# Load environment variables from .env file\nload_dotenv()\n\n\nasync def test_basic_image_generation():\n    \"\"\"Test basic image generation.\"\"\"\n    print(\"\\n1. Testing Basic Image Generation\")\n    print(\"-\" * 50)\n    async with ImageGen(api_key=os.getenv(\"HEURIST_API_KEY\")) as generator:\n        try:\n            response = await generator.generate(\n                {\n                    \"model\": \"FLUX.1-dev\",\n                    \"prompt\": \"A serene Japanese garden with cherry blossoms\",\n                    \"width\": 1024,\n                    \"height\": 768,\n                    \"num_iterations\": 20,\n                    \"guidance_scale\": 7.5,\n                }\n            )\n            print(\"✓ Image Generated Successfully:\")\n            print(f\"URL: {response['url']}\")\n            print(f\"Parameters used: {response}\\n\")\n            return True\n        except Exception as e:\n            print(f\"✗ Image Generation Failed: {e}\\n\")\n            return False\n\n\nasync def test_smartgen():\n    \"\"\"Test SmartGen image generation.\"\"\"\n    print(\"\\n2. Testing SmartGen\")\n    print(\"-\" * 50)\n    async with SmartGen(api_key=os.getenv(\"HEURIST_API_KEY\")) as generator:\n        try:\n            response = await generator.generate_image(\n                description=\"A futuristic cyberpunk cityscape\",\n                image_model=\"FLUX.1-dev\",\n                stylization_level=4,\n                detail_level=5,\n                color_level=5,\n                lighting_level=4,\n                must_include=\"neon lights, flying cars\",\n                quality=\"high\",\n            )\n            print(\"✓ SmartGen Image Generated Successfully:\")\n            print(f\"URL: {response['url']}\")\n            print(f\"Parameters used: {response['parameters']}\\n\")\n            return True\n        except Exception as e:\n            print(f\"✗ SmartGen Failed: {e}\\n\")\n            return False\n\n\nasync def main():\n    \"\"\"Run all tests.\"\"\"\n    print(\"Starting Image Generation Tests...\\n\")\n\n    # Run tests\n    basic_test_result = await test_basic_image_generation()\n    smartgen_test_result = await test_smartgen()\n\n    # Print summary\n    print(\"\\nTest Summary:\")\n    print(\"-\" * 50)\n    print(f\"Basic Image Generation: {'✓ Passed' if basic_test_result else '✗ Failed'}\")\n    print(f\"SmartGen: {'✓ Passed' if smartgen_test_result else '✗ Failed'}\")\n\n    # Return overall success\n    return basic_test_result and smartgen_test_result\n\n\nif __name__ == \"__main__\":\n    success = asyncio.run(main())\n    exit(0 if success else 1)\n"}
{"type": "test_file", "path": "examples/test_twitter_monitor.py", "content": "import os\nimport sys\n\nimport dotenv\n\nproject_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.append(project_root)\n\nfrom interfaces.twitter_reply import QueueManager, TwitterSearchMonitor  # noqa: E402\n\n\ndef main():\n    dotenv.load_dotenv()\n\n    queue = QueueManager()\n    monitor = TwitterSearchMonitor(api_key=os.getenv(\"TWITTER_SEARCH_API_KEY\"), queue_manager=queue)\n\n    # Set search terms\n    monitor.set_search_terms([\"@heurist_ai\"])\n\n    monitor.process_mentions()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "test_file", "path": "mesh/tests/aave_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.aave_agent import AaveAgent  # noqa: E402\n\n\n# Tested Chain IDs: (43114 – Avalanche), (137 – Polygon), (42161 – Arbitrum), its not working now for (1 – Ethereum Mainnet)\nasync def run_agent():\n    agent = AaveAgent()\n    try:\n        agent_input = {\"query\": \"What are the current borrow rates for USDC on Polygon?\"}\n        agent_output = await agent.handle_message(agent_input)\n\n        direct_input = {\"tool\": \"get_aave_reserves\", \"tool_arguments\": {\"chain_id\": 42161, \"asset_filter\": \"USDC\"}}\n        direct_output = await agent.handle_message(direct_input)\n\n        raw_input = {\"query\": \"Show me all Aave assets on Polygon with their liquidity rates\", \"raw_data_only\": True}\n        raw_output = await agent.handle_message(raw_input)\n\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"example1\": {\"input\": agent_input, \"output\": agent_output},\n            \"example2\": {\"input\": direct_input, \"output\": direct_output},\n            \"example3\": {\"input\": raw_input, \"output\": raw_output},\n        }\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/masa_twitter_intellgence_agent.py", "content": "import asyncio\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.masa_twitter_search_agent import MasaTwitterSearchAgent  # noqa: E402\n\nload_dotenv()\n\n# DEBUG Mode:\n# Set DEBUG=True to run the script normally (blocking execution until completion).\n# Set DEBUG=False to execute processing in the background and exit early to avoid long wait times.\nDEBUG = True\n\n\ndef save_results(output_file, yaml_content):\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n    if DEBUG:\n        print(f\"Results saved to {output_file}\")\n    else:\n        print(f\"Result will be stored to {output_file}\")\n\n\nasync def run_agent():\n    agent = MasaTwitterSearchAgent()\n    try:\n        # Natural language query\n        agent_input = {\"query\": \"@heurist_ai\", \"max_results\": 100}\n        agent_output = await agent.handle_message(agent_input)\n\n        # Another natural language query\n        agent_input_specific = {\"query\": \"$BTC\", \"max_results\": 100}\n        agent_output_specific = await agent.handle_message(agent_input_specific)\n\n        # Direct tool call\n        agent_input_direct = {\n            \"tool\": \"search_twitter\",\n            \"tool_arguments\": {\"search_term\": \"Elon musk\", \"max_results\": 30},  # Changed from \"query\" to \"search_term\"\n            \"raw_data_only\": True,\n        }\n        agent_output_direct = await agent.handle_message(agent_input_direct)\n\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"input_basic\": agent_input,\n            \"output_basic\": agent_output,\n            \"input_specific\": agent_input_specific,\n            \"output_specific\": agent_output_specific,\n            \"input_direct\": agent_input_direct,\n            \"output_direct\": agent_output_direct,\n        }\n\n        save_results(output_file, yaml_content)\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    if \"--background\" in sys.argv:\n        asyncio.run(run_agent())\n    else:\n        if DEBUG:\n            asyncio.run(run_agent())\n        else:\n            subprocess.Popen(\n                [sys.executable, __file__, \"--background\"],\n                stdout=subprocess.DEVNULL,\n                stderr=subprocess.DEVNULL,\n                close_fds=True,\n            )\n            print(\"Result will be stored\")\n            sys.exit(0)\n"}
{"type": "test_file", "path": "heurist-mesh-client/examples/test_inputs.py", "content": "TOOL_TEST_INPUTS = {\n    \"AaveAgent\": {\"get_aave_reserves\": {\"chain_id\": 137}},\n    \"AlloraPricePredictionAgent\": {\"get_allora_prediction\": {\"token\": \"ETH\", \"timeframe\": \"5m\"}},\n    \"BitquerySolanaTokenInfoAgent\": {\n        \"get_token_trading_info\": {\"token_address\": \"So11111111111111111111111111111111111111112\"},\n        \"get_top_trending_tokens\": {},\n    },\n    \"CarvOnchainDataAgent\": {\n        \"query_onchain_data\": {\n            \"blockchain\": \"solana\",\n            \"query\": \"What's the most active address on Bitcoin during the last 24 hours?\",\n        }\n    },\n    \"CoinGeckoTokenInfoAgent\": {\n        \"get_coingecko_id\": {\"token_name\": \"ethereum\"},\n        \"get_token_info\": {\"coingecko_id\": \"ethereum\"},\n        \"get_trending_coins\": {},\n        \"get_token_price_multi\": {\n            \"ids\": \"bitcoin,ethereum,solana\",\n            \"vs_currencies\": \"usd\",\n            \"include_market_cap\": True,\n            \"include_24hr_vol\": True,\n            \"include_24hr_change\": True,\n        },\n    },\n    \"DeepResearchAgent\": {\"deep_research\": {\"query\": \"What is the total value locked in Aave v3?\"}},\n    \"DexScreenerTokenInfoAgent\": {\n        \"search_pairs\": {\"search_term\": \"ETH\"},\n        \"get_specific_pair_info\": {\n            \"chain\": \"base\",\n            \"pair_address\": \"0x96d4b53a38337a5733179751781178a2613306063c511b78cd02684739288c0a\",\n        },\n        \"get_token_pairs\": {\"chain\": \"solana\", \"token_address\": \"8TE8oxirpnriy9CKCd6dyjtff2vvP3n6hrSMqX58pump\"},\n    },\n    \"DuckDuckGoSearchAgent\": {\"search_web\": {\"search_term\": \"latest crypto regulations\"}},\n    \"ElfaTwitterIntelligenceAgent\": {\n        \"search_mentions\": {\"keywords\": [\"aave\"]},\n        \"search_account\": {\"username\": \"aave\"},\n        \"get_trending_tokens\": {},\n    },\n    \"ExaSearchAgent\": {\n        \"exa_web_search\": {\"search_term\": \"latest defi trends\"},\n        \"exa_answer_question\": {\"question\": \"What is the current state of DeFi?\"},\n    },\n    \"FirecrawlSearchAgent\": {\n        \"firecrawl_web_search\": {\"search_term\": \"What are the latest developments in zero knowledge proofs?\"},\n        \"firecrawl_extract_web_data\": {\n            \"urls\": [\"https://ethereum.org/en/zero-knowledge-proofs/\"],\n            \"extraction_prompt\": \"Extract information about how zero knowledge proofs are being used in blockchain technology\",\n        },\n    },\n    \"FundingRateAgent\": {\n        \"get_all_funding_rates\": {},\n        \"get_symbol_funding_rates\": {\"symbol\": \"BTC\"},\n        \"find_cross_exchange_opportunities\": {},\n        \"find_spot_futures_opportunities\": {},\n    },\n    \"GoplusAnalysisAgent\": {\n        \"fetch_security_details\": {\"contract_address\": \"0x7d1afa7b718fb893db30a3abc0cfc608aacfebb0\"}\n    },\n    \"MasaTwitterSearchAgent\": {\"search_twitter\": {\"search_term\": \"defi\"}},\n    \"MetaSleuthSolTokenWalletClusterAgent\": {\n        \"fetch_token_clusters\": {\"address\": \"tQNVaFm2sy81tWdHZ971ztS5FKaShJUKGAzHMcypump\"},\n        \"fetch_cluster_details\": {\"cluster_uuid\": \"13axGrDoFlaj8E0ruhYfi1\"},\n    },\n    \"PumpFunTokenAgent\": {\n        \"query_recent_token_creation\": {},\n        \"query_token_metrics\": {\"token_address\": \"98mb39tPFKQJ4Bif8iVg9mYb9wsfPZgpgN1sxoVTpump\"},\n        \"query_token_holders\": {\"token_address\": \"2GxdEZQ5d9PsUqyGy43qv4fmNJWrnLp6qY4dTyNepump\"},\n        \"query_token_buyers\": {\"token_address\": \"98mb39tPFKQJ4Bif8iVg9mYb9wsfPZgpgN1sxoVTpump\"},\n        \"query_holder_status\": {\n            \"token_address\": \"2Z4FzKBcw48KBD2PaR4wtxo4sYGbS7QqTQCLoQnUpump\",\n            \"buyer_addresses\": [\n                \"ApRJBQEKfmcrViQkH94BkzRFUGWtA8uC71DXu6USdd3n\",\n                \"9nG4zw1jVJFpEtSLmbGQpTnpG2TiKfLXWkkTyyRvxTt6\",\n            ],\n        },\n        \"query_top_traders\": {\"token_address\": \"98mb39tPFKQJ4Bif8iVg9mYb9wsfPZgpgN1sxoVTpump\"},\n    },\n    \"SolWalletAgent\": {\n        \"analyze_sol_token_holders\": {\"token_address\": \"J7tYmq2JnQPvxyhcXpCDrvJnc9R5ts8rv7tgVHDPsw7U\"},\n        \"get_sol_tx_history\": {\"owner_address\": \"DbDi7soBXALYRMZSyJMEAfpaK3rD1hr5HuCYzuDrcEEN\"},\n        \"get_sol_wallet_assets\": {\"owner_address\": \"DbDi7soBXALYRMZSyJMEAfpaK3rD1hr5HuCYzuDrcEEN\"},\n    },\n    \"ZerionWalletAnalysisAgent\": {\n        \"fetch_wallet_tokens\": {\"wallet_address\": \"0x7d9d1821d15B9e0b8Ab98A058361233E255E405D\"},\n        \"fetch_wallet_nfts\": {\"wallet_address\": \"0x7d9d1821d15B9e0b8Ab98A058361233E255E405D\"},\n    },\n}\n"}
{"type": "test_file", "path": "heurist-mesh-client/examples/test_agents.py", "content": "import os\nimport time\nfrom typing import Dict, Optional\n\nimport click\nimport httpx\nfrom dotenv import load_dotenv\nfrom heurist_mesh_client.client import MeshClient\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.table import Table\nfrom test_inputs import TOOL_TEST_INPUTS\n\nload_dotenv()\nMESH_METADATA_URL = \"https://mesh.heurist.ai/mesh_agents_metadata.json\"\n\nDISABLED_AGENTS = {\"DeepResearchAgent\"}  # slow and times out frequently\n\n\ndef fetch_agents_metadata() -> Dict:\n    with httpx.Client() as client:\n        response = client.get(MESH_METADATA_URL)\n        response.raise_for_status()\n        return response.json()\n\n\ndef trim_string(s: str, max_length: int = 300) -> str:\n    return s if len(s) <= max_length else s[: max_length - 3] + \"...\"\n\n\ndef execute_test(\n    client: MeshClient,\n    agent_id: str,\n    tool_name: str,\n    inputs: Dict,\n    console: Console,\n    format_output: callable,\n) -> tuple[Optional[str], float]:\n    console.print(f\"\\n[bold blue]Testing {agent_id} - {tool_name}[/bold blue]\")\n    console.print(f\"[dim]Inputs: {format_output(str(inputs))}\")\n\n    start_time = time.time()\n    try:\n        result = client.sync_request(agent_id=agent_id, tool=tool_name, tool_arguments=inputs, raw_data_only=True)\n        elapsed = time.time() - start_time\n\n        error = None\n        if isinstance(result, dict):\n            if \"error\" in result:\n                error = result[\"error\"]\n            elif \"errorMessage\" in result:\n                error = result[\"errorMessage\"]\n            elif \"message\" in result and isinstance(result[\"message\"], str) and \"error\" in result[\"message\"].lower():\n                error = result[\"message\"]\n            elif \"data\" in result and isinstance(result[\"data\"], dict):\n                if \"error\" in result[\"data\"]:\n                    error = result[\"data\"][\"error\"]\n                elif \"errorMessage\" in result[\"data\"]:\n                    error = result[\"data\"][\"errorMessage\"]\n\n            if not error and \"response\" in result:\n                if not result[\"response\"] and isinstance(result.get(\"data\"), dict) and result[\"data\"].get(\"error\"):\n                    error = result[\"data\"][\"error\"]\n\n        if error:\n            console.print(\n                Panel(str(error), title=f\"[red]{agent_id} - {tool_name} Error[/red]\", border_style=\"red\", expand=False)\n            )\n            return error, elapsed\n\n        console.print(\n            Panel(\n                format_output(str(result)),\n                title=f\"[green]{agent_id} - {tool_name} Response ({elapsed:.2f}s)[/green]\",\n                border_style=\"green\",\n                expand=False,\n            )\n        )\n        return None, elapsed\n\n    except Exception as e:\n        elapsed = time.time() - start_time\n        error_str = str(e)\n        console.print(\n            Panel(\n                error_str,\n                title=f\"[red]{agent_id} - {tool_name} Error ({elapsed:.2f}s)[/red]\",\n                border_style=\"red\",\n                expand=False,\n            )\n        )\n        return error_str, elapsed\n\n\ndef test_tool(\n    client: MeshClient,\n    agent_id: str,\n    tool_name: str,\n    inputs: Dict,\n    console: Console,\n    format_output: callable,\n    test_results: list,\n) -> None:\n    error, elapsed = execute_test(client, agent_id, tool_name, inputs, console, format_output)\n    test_results.append((f\"{agent_id} - {tool_name}\", error, elapsed))\n\n\ndef display_test_summary(results: list[tuple[str, Optional[str], float]], skipped: list[str], console: Console):\n    successes = [(name, duration) for name, error, duration in results if not error]\n    failures = [(name, error, duration) for name, error, duration in results if error]\n\n    console.print(\"\\n[bold]Test Run Summary[/bold]\")\n\n    console.print(\"\\n[bold green]Successful Tests:[/bold green]\")\n    for name, duration in successes:\n        console.print(f\"✓ {name} ({duration:.2f}s)\")\n\n    if failures:\n        console.print(\"\\n[bold red]Failed Tests:[/bold red]\")\n        for name, error, duration in failures:\n            console.print(f\"✗ {name} ({duration:.2f}s)\")\n            console.print(f\"  Error: {error}\")\n\n    if skipped:\n        console.print(\"\\n[bold yellow]Skipped Tests:[/bold yellow]\")\n        for test in skipped:\n            console.print(f\"⚠ {test}\")\n\n    total = len(successes) + len(failures)\n    success_rate = (len(successes) / total * 100) if total > 0 else 0\n    avg_time = sum(duration for _, duration in successes) / len(successes) if successes else 0\n\n    console.print(\"\\n[bold]Statistics:[/bold]\")\n    console.print(f\"Total Tests: {total}\")\n    console.print(f\"Success Rate: {success_rate:.1f}%\")\n    console.print(f\"Average Response Time: {avg_time:.2f}s\")\n    console.print(f\"Skipped Tests: {len(skipped)}\")\n\n\n@click.group()\ndef cli():\n    \"\"\"Heurist Mesh Agents Testing Suite\"\"\"\n    pass\n\n\n@cli.command()\ndef list_agents():\n    \"\"\"List all available agents and their test status\"\"\"\n    agents_metadata = fetch_agents_metadata()\n\n    table = Table(show_header=True)\n    table.add_column(\"Agent Name\", style=\"cyan\")\n    table.add_column(\"Tools\", style=\"green\")\n    table.add_column(\"Test Data\", justify=\"center\")\n\n    orphaned_test_agents = set(TOOL_TEST_INPUTS.keys()) - set(agents_metadata[\"agents\"].keys())\n    orphaned_tools = {}\n\n    for agent_id, agent_data in agents_metadata[\"agents\"].items():\n        tools = {tool[\"function\"][\"name\"] for tool in agent_data.get(\"tools\", [])}\n\n        styled_name = f\"[cyan]{agent_id}\"\n        if agent_id in DISABLED_AGENTS:\n            styled_name = f\"[dim]{agent_id} [red](disabled)[/]\"\n\n        if not tools:\n            table.add_row(styled_name, \"[dim italic]No tools[/]\", \"[dim]-[/]\")\n            continue\n\n        tool_names = []\n        has_any_test = False\n        tool_test_status = []\n\n        if agent_id in TOOL_TEST_INPUTS:\n            test_tools = set(TOOL_TEST_INPUTS[agent_id].keys())\n            missing_tools = test_tools - tools\n            if missing_tools:\n                orphaned_tools[agent_id] = missing_tools\n\n        for tool in sorted(tools):\n            has_test = agent_id in TOOL_TEST_INPUTS and tool in TOOL_TEST_INPUTS[agent_id]\n            if has_test:\n                has_any_test = True\n                tool_names.append(f\"[green]{tool}[/]\")\n                tool_test_status.append(\"[green]✓[/]\")\n            else:\n                tool_names.append(f\"[dim]{tool}[/]\")\n                tool_test_status.append(\"[red]✗[/]\")\n\n        table.add_row(\n            styled_name if has_any_test else f\"[dim]{agent_id}[/]\", \"\\n\".join(tool_names), \"\\n\".join(tool_test_status)\n        )\n\n    if orphaned_test_agents:\n        table.add_section()\n        for agent_id in sorted(orphaned_test_agents):\n            tools = list(TOOL_TEST_INPUTS[agent_id].keys())\n            table.add_row(\n                f\"[yellow]{agent_id} [red](missing agent)[/]\",\n                \"\\n\".join(f\"[yellow]{tool}[/]\" for tool in tools),\n                \"[yellow]![/]\",\n            )\n\n    if orphaned_tools:\n        table.add_section()\n        for agent_id, missing_tools in sorted(orphaned_tools.items()):\n            table.add_row(\n                f\"[yellow]{agent_id} [red](has orphaned tools)[/]\",\n                \"\\n\".join(f\"[yellow]{tool} [red](missing tool)[/]\" for tool in sorted(missing_tools)),\n                \"[yellow]![/]\",\n            )\n\n    Console().print(table)\n\n\n@cli.command()\n@click.argument(\"agent_tool_pairs\", required=False)\n@click.option(\"--include-disabled\", is_flag=True, help=\"Include disabled agents in testing\")\n@click.option(\"--no-trim\", is_flag=True, help=\"Disable output trimming\")\ndef test_agent(agent_tool_pairs: Optional[str] = None, include_disabled: bool = False, no_trim: bool = False):\n    \"\"\"Test specific agent-tool pairs. Format: 'agent1,tool1 agent2,tool2' or just 'agent1' to test all tools\"\"\"\n    if not os.getenv(\"HEURIST_API_KEY\"):\n        click.echo(\"Error: HEURIST_API_KEY environment variable not set\")\n        return\n\n    try:\n        agents_metadata = fetch_agents_metadata()\n        client = MeshClient()\n        console = Console()\n        test_results = []\n        skipped_tests = []\n\n        def format_output(s: str) -> str:\n            return s if no_trim else trim_string(s)\n\n        if not agent_tool_pairs:\n            console.print(\"\\n[bold yellow]Testing all agents with available test inputs[/bold yellow]\")\n            for test_agent_id, test_tools in TOOL_TEST_INPUTS.items():\n                if test_agent_id not in agents_metadata[\"agents\"]:\n                    console.print(f\"[red]Skipping {test_agent_id} - not found in metadata[/red]\")\n                    continue\n\n                if test_agent_id in DISABLED_AGENTS and not include_disabled:\n                    console.print(f\"[yellow]Skipping disabled agent: {test_agent_id}[/yellow]\")\n                    skipped_tests.extend(f\"{test_agent_id} - {t}\" for t in test_tools)\n                    continue\n\n                agent_tools = {t[\"function\"][\"name\"] for t in agents_metadata[\"agents\"][test_agent_id].get(\"tools\", [])}\n                for tool_name, inputs in test_tools.items():\n                    if tool_name in agent_tools:\n                        test_tool(client, test_agent_id, tool_name, inputs, console, format_output, test_results)\n        else:\n            pairs = [pair.strip() for pair in agent_tool_pairs.split()]\n            for pair in pairs:\n                agent_id = pair.split(\",\")[0]\n                tool = pair.split(\",\")[1] if \",\" in pair else None\n\n                if agent_id not in agents_metadata[\"agents\"]:\n                    console.print(f\"[red]Error: Agent {agent_id} not found[/red]\")\n                    continue\n\n                if agent_id in DISABLED_AGENTS and not include_disabled:\n                    console.print(f\"[yellow]Skipping disabled agent: {agent_id}[/yellow]\")\n                    skipped_tests.append(f\"{agent_id}\")\n                    continue\n\n                agent_tools = {t[\"function\"][\"name\"] for t in agents_metadata[\"agents\"][agent_id].get(\"tools\", [])}\n\n                if tool:\n                    if tool not in agent_tools:\n                        console.print(f\"[red]Error: Tool {tool} not found in agent {agent_id}[/red]\")\n                        continue\n\n                    if agent_id in TOOL_TEST_INPUTS and tool in TOOL_TEST_INPUTS[agent_id]:\n                        test_tool(\n                            client,\n                            agent_id,\n                            tool,\n                            TOOL_TEST_INPUTS[agent_id][tool],\n                            console,\n                            format_output,\n                            test_results,\n                        )\n                    else:\n                        console.print(f\"[yellow]No test inputs defined for {agent_id} - {tool}[/yellow]\")\n                else:\n                    if agent_id in TOOL_TEST_INPUTS:\n                        for tool_name, inputs in TOOL_TEST_INPUTS[agent_id].items():\n                            if tool_name in agent_tools:\n                                test_tool(client, agent_id, tool_name, inputs, console, format_output, test_results)\n                    else:\n                        console.print(f\"[yellow]No test inputs defined for agent {agent_id}[/yellow]\")\n\n        if test_results:\n            display_test_summary(test_results, skipped_tests, console)\n\n    except KeyboardInterrupt:\n        console.print(\"\\n[bold red]Testing interrupted by user[/bold red]\")\n        if test_results:\n            display_test_summary(test_results, skipped_tests, console)\n        raise click.Abort()\n\n\nif __name__ == \"__main__\":\n    cli()\n"}
{"type": "test_file", "path": "mesh/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "examples/test_videogen.py", "content": "import asyncio\nimport os\nimport sys\nimport uuid\n\nfrom dotenv import load_dotenv\n\nproject_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.append(project_root)\n\nfrom core.videogen import Text2VideoTask, Workflow  # noqa: E402\n\nload_dotenv()\napi_key = os.getenv(\"HEURIST_API_KEY\")\nif not api_key:\n    raise ValueError(\"HEURIST_API_KEY environment variable is not set\")\n\n\nasync def main():\n    # Initialize the workflow\n    workflow = Workflow(api_key=api_key, workflow_url=\"https://sequencer-v2.heurist.xyz\")\n\n    # Create a task (e.g., Text2Video)\n    task = Text2VideoTask(\n        consumer_id=str(uuid.uuid4()), prompt=\"A beautiful sunset over the ocean\", timeout_seconds=600, workflow_id=\"1\"\n    )\n\n    # Execute and wait for result\n    try:\n        result = await workflow.execute_workflow(task)\n        print(f\"Result: {result}\")\n    except Exception as e:\n        import traceback\n\n        traceback.print_exc()\n        print(f\"Error: {str(e)}\")\n\n\n# Run the async function\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "test_file", "path": "mesh/tests/exa_search_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.exa_search_agent import ExaSearchAgent  # noqa: E402\n\nload_dotenv()\n\n\nasync def run_agent():\n    agent = ExaSearchAgent()\n    try:\n        # Natural language query\n        agent_input = {\n            \"query\": \"What are the latest developments in quantum computing?\",\n            \"raw_data_only\": False,\n        }\n        agent_output = await agent.handle_message(agent_input)\n\n        # Direct search tool call\n        agent_input_search = {\n            \"tool\": \"exa_web_search\",\n            \"tool_arguments\": {\"search_term\": \"quantum computing breakthroughs 2024\", \"limit\": 5},\n            \"raw_data_only\": False,\n        }\n        agent_output_search = await agent.handle_message(agent_input_search)\n\n        # Direct answer tool call\n        agent_input_answer = {\n            \"tool\": \"exa_answer_question\",\n            \"tool_arguments\": {\"question\": \"What is quantum supremacy?\"},\n            \"raw_data_only\": False,\n        }\n        agent_output_answer = await agent.handle_message(agent_input_answer)\n\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"natural_language_query\": {\"input\": agent_input, \"output\": agent_output},\n            \"direct_search\": {\"input\": agent_input_search, \"output\": agent_output_search},\n            \"direct_answer\": {\"input\": agent_input_answer, \"output\": agent_output_answer},\n        }\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/goplus_analysis_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.goplus_analysis_agent import GoplusAnalysisAgent  # noqa: E402\n\nload_dotenv()\n\n\nasync def run_agent():\n    agent = GoplusAnalysisAgent()\n    try:\n        # Test with a query for Ethereum token\n        agent_input = {\n            \"query\": \"Check the safety of this token: 0x7Fc66500c84A76Ad7e9c93437bFc5Ac33E2DDaE9 on Ethereum\"\n        }\n        agent_output = await agent.handle_message(agent_input)\n\n        # Test with a query for Solana token\n        agent_input_solana = {\n            \"query\": \"Check the safety of this Solana token: AcmFHCquGwbrPxh9b3sUPMtAtXKMjkEzKnqkiHEnpump\"\n        }\n        agent_output_solana = await agent.handle_message(agent_input_solana)\n\n        # Test direct tool call for Ethereum token\n        agent_input_direct_tool = {\n            \"tool\": \"fetch_security_details\",\n            \"tool_arguments\": {\"contract_address\": \"0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48\", \"chain_id\": \"1\"},\n        }\n        agent_output_direct_tool = await agent.handle_message(agent_input_direct_tool)\n\n        # Test direct tool call for Base token\n        agent_input_direct_base = {\n            \"tool\": \"fetch_security_details\",\n            \"tool_arguments\": {\"contract_address\": \"0x50c5725949A6F0c72E6C4a641F24049A917DB0Cb\", \"chain_id\": \"8453\"},\n        }\n        agent_output_direct_base = await agent.handle_message(agent_input_direct_base)\n\n        # Test with raw_data_only flag\n        agent_input_raw_data = {\n            \"query\": \"Is 0x2260FAC5E5542a773Aa44fBCfeDf7C193bc2C599 safe on chain 1?\",\n            \"raw_data_only\": True,\n        }\n        agent_output_raw_data = await agent.handle_message(agent_input_raw_data)\n\n        # Save the test inputs and outputs to a YAML file for further inspection\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"input_ethereum\": agent_input,\n            \"output_ethereum\": agent_output,\n            \"input_solana\": agent_input_solana,\n            \"output_solana\": agent_output_solana,\n            \"input_direct_ethereum\": agent_input_direct_tool,\n            \"output_direct_ethereum\": agent_output_direct_tool,\n            \"input_direct_base\": agent_input_direct_base,\n            \"output_direct_base\": agent_output_direct_base,\n            \"input_raw_data\": agent_input_raw_data,\n            \"output_raw_data\": agent_output_raw_data,\n        }\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/funding_rate_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.funding_rate_agent import FundingRateAgent  # noqa: E402\n\nload_dotenv()\n\n\nasync def run_agent():\n    agent = FundingRateAgent()\n    try:\n        # Test with a query for all funding rates\n        agent_input = {\"query\": \"What are the current funding rates for Bitcoin?\"}\n\n        agent_output = await agent.handle_message(agent_input)\n\n        # Test with query for cross-exchange arbitrage opportunities\n        agent_input_arb = {\n            \"query\": \"Find arbitrage opportunities across exchanges with at least 0.05% funding rate difference\"\n        }\n\n        agent_output_arb = await agent.handle_message(agent_input_arb)\n\n        # Test with query for spot-futures opportunities\n        agent_input_spot = {\"query\": \"What are the best spot-futures funding rate opportunities right now?\"}\n        agent_output_spot = await agent.handle_message(agent_input_spot)\n        print(f\"Result of handle_message for spot-futures opportunities: {agent_output_spot}\")\n\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"input_by_symbol\": agent_input,\n            \"output_by_symbol\": agent_output,\n            \"input_by_arbitrage\": agent_input_arb,\n            \"output_by_arbitrage\": agent_output_arb,\n            \"input_by_spot_futures\": agent_input_spot,\n            \"output_by_spot_futures\": agent_output_spot,\n        }\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/metasleuth_sol_token_wallet_cluster_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.metasleuth_sol_token_wallet_cluster_agent import MetaSleuthSolTokenWalletClusterAgent  # noqa: E402\n\nload_dotenv()\n\n\nasync def run_agent():\n    agent = MetaSleuthSolTokenWalletClusterAgent()\n    try:\n        # Example for direct fetch_token_clusters tool\n        agent_input_clusters = {\n            \"tool\": \"fetch_token_clusters\",\n            \"tool_arguments\": {\"address\": \"tQNVaFm2sy81tWdHZ971ztS5FKaShJUKGAzHMcypump\", \"page\": 1, \"page_size\": 10},\n            \"raw_data_only\": True,\n        }\n        agent_output_clusters = await agent.handle_message(agent_input_clusters)\n\n        # Example for direct fetch_cluster_details tool\n        agent_input_details = {\n            \"tool\": \"fetch_cluster_details\",\n            \"tool_arguments\": {\"cluster_uuid\": \"13axGrDoFlaj8E0ruhYfi1\", \"page\": 1, \"page_size\": 10},\n            \"raw_data_only\": True,\n        }\n        agent_output_details = await agent.handle_message(agent_input_details)\n\n        # Example for natural language query - token analysis\n        agent_input_nl_token = {\n            \"query\": \"Analyze the wallet clusters of this Solana token: tQNVaFm2sy81tWdHZ971ztS5FKaShJUKGAzHMcypump\",\n            \"raw_data_only\": False,\n        }\n        agent_output_nl_token = await agent.handle_message(agent_input_nl_token)\n\n        # Example for natural language query - cluster details\n        agent_input_nl_cluster = {\n            \"query\": \"Show me the details of wallet cluster with UUID 0j7eWWwixWixBYPg5oeVX6\",\n            \"raw_data_only\": False,\n        }\n        agent_output_nl_cluster = await agent.handle_message(agent_input_nl_cluster)\n\n        # Example with raw data only\n        agent_input_raw = {\n            \"query\": \"Get token cluster data for tQNVaFm2sy81tWdHZ971ztS5FKaShJUKGAzHMcypump\",\n            \"raw_data_only\": True,\n        }\n        agent_output_raw = await agent.handle_message(agent_input_raw)\n\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"direct_token_clusters\": {\"input\": agent_input_clusters, \"output\": agent_output_clusters},\n            \"direct_cluster_details\": {\"input\": agent_input_details, \"output\": agent_output_details},\n            \"nl_token_analysis\": {\"input\": agent_input_nl_token, \"output\": agent_output_nl_token},\n            \"nl_cluster_details\": {\"input\": agent_input_nl_cluster, \"output\": agent_output_nl_cluster},\n            \"raw_data_query\": {\"input\": agent_input_raw, \"output\": agent_output_raw},\n        }\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/zerion_wallet_analysis_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.zerion_wallet_analysis_agent import ZerionWalletAnalysisAgent  # noqa: E402\n\nload_dotenv()\n\n# 8453 chain is base\n\n\nasync def run_agent():\n    agent = ZerionWalletAnalysisAgent()\n    try:\n        # Example for direct fetch_wallet_tokens tool\n        agent_input_tokens = {\n            \"tool\": \"fetch_wallet_tokens\",\n            \"tool_arguments\": {\"wallet_address\": \"0x7d9d1821d15B9e0b8Ab98A058361233E255E405D\"},\n            \"raw_data_only\": False,\n        }\n        agent_output_tokens = await agent.handle_message(agent_input_tokens)\n\n        # Example for fetch_wallet_nfts tool\n        agent_input_nfts = {\n            \"tool\": \"fetch_wallet_nfts\",\n            \"tool_arguments\": {\"wallet_address\": \"0x7d9d1821d15B9e0b8Ab98A058361233E255E405D\"},\n            \"raw_data_only\": False,\n        }\n        agent_output_nfts = await agent.handle_message(agent_input_nfts)\n\n        # Example with raw data only\n        agent_input_raw = {\n            \"query\": \"What tokens does 0x7d9d1821d15B9e0b8Ab98A058361233E255E405D hold?\",\n            \"raw_data_only\": True,\n        }\n        agent_output_raw = await agent.handle_message(agent_input_raw)\n\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"token_holdings\": {\"input\": agent_input_tokens, \"output\": agent_output_tokens},\n            \"nft_holdings\": {\"input\": agent_input_nfts, \"output\": agent_output_nfts},\n            \"raw_data_query\": {\"input\": agent_input_raw, \"output\": agent_output_raw},\n        }\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/allora_price_prediction_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.allora_price_prediction_agent import AlloraPricePredictionAgent  # noqa: E402\n\n\nasync def run_agent():\n    agent = AlloraPricePredictionAgent()\n    try:\n        agent_input = {\"query\": \"Predict ETH price in 5 minutes\"}\n        agent_output = await agent.handle_message(agent_input)\n\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\"input\": agent_input, \"output\": agent_output}\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/dexscreener_token_info_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.dexscreener_token_info_agent import DexScreenerTokenInfoAgent  # noqa: E402\n\nload_dotenv()\n\n\nasync def run_agent():\n    agent = DexScreenerTokenInfoAgent()\n    try:\n        # Example for natural language query with analysis\n        agent_input_query = {\n            \"query\": \"Show me information about ETH on Uniswap\",\n            \"raw_data_only\": False,\n        }\n        agent_output_query = await agent.handle_message(agent_input_query)\n\n        # Example for natural language query with raw data only\n        agent_input_query_raw = {\n            \"query\": \"Tell me about ETH on Uniswap\",\n            \"raw_data_only\": True,\n        }\n        agent_output_query_raw = await agent.handle_message(agent_input_query_raw)\n\n        # Test search_pairs tool - Direct tool call (no LLM analysis)\n        agent_input_search = {\n            \"tool\": \"search_pairs\",\n            \"tool_arguments\": {\"search_term\": \"ETH\"},\n        }\n        agent_output_search = await agent.handle_message(agent_input_search)\n        print(f\"Result of search_pairs: {agent_output_search}\")\n\n        # Test get_specific_pair_info tool - Direct tool call (no LLM analysis)\n        agent_input_pair_info = {\n            \"tool\": \"get_specific_pair_info\",\n            \"tool_arguments\": {\"chain\": \"solana\", \"pair_address\": \"7qsdv1yr4yra9fjazccrwhbjpykvpcbi3158u1qcjuxp\"},\n        }\n        agent_output_pair_info = await agent.handle_message(agent_input_pair_info)\n        print(f\"Result of get_specific_pair_info: {agent_output_pair_info}\")\n\n        # Test get_token_pairs tool - Direct tool call (no LLM analysis)\n        agent_input_pairs = {\n            \"tool\": \"get_token_pairs\",\n            \"tool_arguments\": {\"chain\": \"solana\", \"token_address\": \"8TE8oxirpnriy9CKCd6dyjtff2vvP3n6hrSMqX58pump\"},\n        }\n        agent_output_pairs = await agent.handle_message(agent_input_pairs)\n        print(f\"Result of get_token_pairs: {agent_output_pairs}\")\n\n        # Save the test inputs and outputs to a YAML file\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"natural_language_query_with_analysis\": {\"input\": agent_input_query, \"output\": agent_output_query},\n            \"natural_language_query_raw_data\": {\"input\": agent_input_query_raw, \"output\": agent_output_query_raw},\n            \"search_pairs_test\": {\"input\": agent_input_search, \"output\": agent_output_search},\n            \"specific_pair_info_test\": {\"input\": agent_input_pair_info, \"output\": agent_output_pair_info},\n            \"token_pairs_test\": {\"input\": agent_input_pairs, \"output\": agent_output_pairs},\n        }\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/carv_onchain_data_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\n# Adjust the path to access the parent directory\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.carv_onchain_data_agent import CarvOnchainDataAgent  # noqa: E402\n\nload_dotenv()\n\n\nasync def run_agent():\n    agent = CarvOnchainDataAgent()\n    try:\n        # Test with a natural language query\n        ethereum_query = {\"query\": \"What's the most active address on Ethereum during the last 24 hours?\"}\n        ethereum_result = await agent.handle_message(ethereum_query)\n        print(f\"Ethereum Query Result: {ethereum_result}\")\n\n        # Test with a direct tool call\n        direct_input = {\n            \"tool\": \"query_onchain_data\",\n            \"tool_arguments\": {\n                \"blockchain\": \"solana\",\n                \"query\": \"What's the most active address on Bitcoin during the last 24 hours?\",\n            },\n        }\n        direct_result = await agent.handle_message(direct_input)\n        print(f\"Direct Tool Call Result: {direct_result}\")\n\n        # Test with raw data only\n        raw_input = {\n            \"query\": \"What's the most active address on Solana during the last 24 hours?\",\n            \"raw_data_only\": True,\n        }\n        raw_result = await agent.handle_message(raw_input)\n        print(f\"Raw Data Result: {raw_result}\")\n\n        # Save the test results to a YAML file\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"ethereum_example\": {\"input\": ethereum_query, \"output\": ethereum_result},\n            \"direct_example\": {\"input\": direct_input, \"output\": direct_result},\n            \"raw_example\": {\"input\": raw_input, \"output\": raw_result},\n        }\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/sol_wallet_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nimport yaml\n\nfrom mesh.sol_wallet_agent import SolWalletAgent\n\nload_dotenv()\n\n\n\nasync def run_agent():\n    ca = \"J7tYmq2JnQPvxyhcXpCDrvJnc9R5ts8rv7tgVHDPsw7U\"\n    wallet = \"DbDi7soBXALYRMZSyJMEAfpaK3rD1hr5HuCYzuDrcEEN\"\n\n    async with SolWalletAgent() as agent:\n        try:\n            agent_input_query = {\n                \"query\": f\"Give me the holders of this {ca}\",\n                \"raw_data_only\": False,\n            }\n            agent_output_query = await agent.handle_message(agent_input_query)\n            print(agent_output_query)\n\n            agent_input_query_raw = {\n                \"query\": f\"Show me the txs of this wallet {wallet}\",\n                \"raw_data_only\": True,\n            }\n            agent_output_query_raw = await agent.handle_message(agent_input_query_raw)\n            print(agent_output_query_raw)\n\n\n            agent_input_assets = {\n                \"tool\": \"get_wallet_assets\",\n                \"tool_arguments\": {\"owner_address\": wallet},\n            }\n            agent_output_assets = await agent.handle_message(agent_input_assets)\n            print(f\"Result of get_wallet_assets: {agent_output_assets}\")\n\n\n            agent_input_tx = {\n                \"tool\": \"get_tx_history\",\n                \"tool_arguments\": {\"owner_address\": wallet},\n            }\n            agent_output_tx = await agent.handle_message(agent_input_tx)\n            print(f\"Result of get_tx_history: {agent_output_tx}\")\n\n\n            agent_input_holders = {\n                \"tool\": \"analyze_common_holdings_of_top_holders\",\n                \"tool_arguments\": {\"token_address\": ca},\n            }\n            agent_output_holders = await agent.handle_message(agent_input_holders)\n            print(f\"Result of analyze_common_holdings_of_top_holders: {agent_output_holders}\")\n\n            script_dir = Path(__file__).parent\n            current_file = Path(__file__).stem\n            base_filename = f\"{current_file}_example\"\n            output_file = script_dir / f\"{base_filename}.yaml\"\n\n            yaml_content = {\n                \"natural_language_query_with_analysis\": {\"input\": agent_input_query, \"output\": agent_output_query},\n                \"natural_language_query_raw_data\": {\"input\": agent_input_query_raw, \"output\": agent_output_query_raw},\n                \"get_wallet_assets_test\": {\"input\": agent_input_assets, \"output\": agent_output_assets},\n                \"get_tx_history_test\": {\"input\": agent_input_tx, \"output\": agent_output_tx},\n                \"analyze_common_holdings_of_top_holders\": {\"input\": agent_input_holders, \"output\": agent_output_holders},\n            }\n\n            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n            print(f\"Results saved to {output_file}\")\n\n\n        finally:\n            await agent.cleanup()\n\n\n\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/coingecko_token_info_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.coingecko_token_info_agent import CoinGeckoTokenInfoAgent  # noqa: E402\n\nload_dotenv()\n\n\nasync def run_agent():\n    agent = CoinGeckoTokenInfoAgent()\n    try:\n        # Test with a query mentioning CoinGecko ID\n        agent_input = {\"query\": \"Get information about MONA\"}\n\n        agent_output = await agent.handle_message(agent_input)\n        print(f\"Result of handle_message: {agent_output}\")\n\n        # Test with query mentioning the token name\n        agent_input_name = {\"query\": \"analyze HEU\"}\n\n        agent_output_name = await agent.handle_message(agent_input_name)\n        print(f\"Result of handle_message when token name is provided: {agent_output_name}\")\n\n        # Test with trending coins\n        agent_input_trending = {\"query\": \"Get information about trending coins\"}\n        agent_output_trending = await agent.handle_message(agent_input_trending)\n        print(f\"Result of handle_message when trending coins is provided: {agent_output_trending}\")\n\n        agent_input_direct = {\n            \"tool\": \"get_token_info\",\n            \"tool_arguments\": {\"coingecko_id\": \"bitcoin\"},\n            \"raw_data_only\": True,\n        }\n        agent_output_direct = await agent.handle_message(agent_input_direct)\n        print(f\"Result of direct tool call: {agent_output_direct}\")\n\n        # Test the new get_token_price_multi tool\n        agent_input_price_multi = {\n            \"tool\": \"get_token_price_multi\",\n            \"tool_arguments\": {\n                \"ids\": \"bitcoin,ethereum,solana\",\n                \"vs_currencies\": \"usd\",\n                \"include_market_cap\": True,\n                \"include_24hr_vol\": True,\n                \"include_24hr_change\": True,\n            },\n        }\n        agent_output_price_multi = await agent.handle_message(agent_input_price_multi)\n        print(f\"Result of get_token_price_multi tool call: {agent_output_price_multi}\")\n\n        # Test query for comparing multiple tokens\n        agent_input_compare = {\"query\": \"Compare Bitcoin and Ethereum\"}\n        agent_output_compare = await agent.handle_message(agent_input_compare)\n        print(f\"Result of token comparison query: {agent_output_compare}\")\n\n        # NEW: Test with categories list\n        agent_input_categories = {\"query\": \"List all cryptocurrency categories\"}\n        agent_output_categories = await agent.handle_message(agent_input_categories)\n\n        # NEW: Test with category data\n        agent_input_category_data = {\"query\": \"Show me market data for all cryptocurrency categories\"}\n        agent_output_category_data = await agent.handle_message(agent_input_category_data)\n\n        # NEW: Test with tokens by category\n        agent_input_category_tokens = {\"query\": \"Show me all tokens in the layer-1 category\"}\n        agent_output_category_tokens = await agent.handle_message(agent_input_category_tokens)\n\n        # NEW: Test direct tool calls for each new category function\n        agent_input_direct_categories = {\"tool\": \"get_categories_list\", \"tool_arguments\": {}, \"raw_data_only\": True}\n        agent_output_direct_categories = await agent.handle_message(agent_input_direct_categories)\n\n        agent_input_direct_category_data = {\n            \"tool\": \"get_category_data\",\n            \"tool_arguments\": {\"order\": \"market_cap_desc\"},\n            \"raw_data_only\": True,\n        }\n        agent_output_direct_category_data = await agent.handle_message(agent_input_direct_category_data)\n\n        agent_input_direct_category_tokens = {\n            \"tool\": \"get_tokens_by_category\",\n            \"tool_arguments\": {\n                \"category_id\": \"layer-1\",\n                \"vs_currency\": \"usd\",\n                \"order\": \"market_cap_desc\",\n                \"per_page\": 10,\n                \"page\": 1,\n            },\n            \"raw_data_only\": True,\n        }\n\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"input_by_id\": agent_input,\n            \"output_by_id\": agent_output,\n            \"input_by_name\": agent_input_name,\n            \"output_by_name\": agent_output_name,\n            \"input_by_trending\": agent_input_trending,\n            \"output_by_trending\": agent_output_trending,\n            \"input_direct_tool\": agent_input_direct,\n            \"output_direct_tool\": agent_output_direct,\n            \"input_price_multi\": agent_input_price_multi,\n            \"output_price_multi\": agent_output_price_multi,\n            \"input_comparison\": agent_input_compare,\n            \"output_comparison\": agent_output_compare,\n            # Updated Data\n            \"input_categories_list\": agent_input_categories,\n            \"output_categories_list\": agent_output_categories,\n            \"input_category_data\": agent_input_category_data,\n            \"output_category_data\": agent_output_category_data,\n            \"input_category_tokens\": agent_input_category_tokens,\n            \"output_category_tokens\": agent_output_category_tokens,\n            \"input_direct_categories\": agent_input_direct_categories,\n            \"output_direct_categories\": agent_output_direct_categories,\n            \"input_direct_category_data\": agent_input_direct_category_data,\n            \"output_direct_category_data\": agent_output_direct_category_data,\n            \"input_direct_category_tokens\": agent_input_direct_category_tokens,\n        }\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/deep_research_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.deep_research_agent import DeepResearchAgent  # noqa: E402\n\nload_dotenv()\n\n\nasync def run_agent():\n    agent = DeepResearchAgent()\n    try:\n        agent_input = {\n            \"query\": \"What are the latest developments in zero knowledge proofs?\",\n            \"depth\": 2,\n            \"breadth\": 3,\n            \"concurrency\": 2,\n        }\n\n        try:\n            agent_output = await agent.handle_message(agent_input)\n        except Exception as e:\n            print(f\"Error during agent execution: {str(e)}\")\n            return\n\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"input\": agent_input,\n            \"output\": {\n                \"response\": agent_output[\"response\"],\n                \"data\": {\n                    \"query_info\": agent_output[\"data\"][\"query_info\"],\n                    \"result_count\": len(agent_output[\"data\"][\"results\"]),\n                    \"results_sample\": agent_output[\"data\"][\"results\"][:2] if agent_output[\"data\"][\"results\"] else [],\n                    \"analyses_count\": len(agent_output[\"data\"][\"analyses\"]),\n                    \"analyses_sample\": agent_output[\"data\"][\"analyses\"][:2] if agent_output[\"data\"][\"analyses\"] else [],\n                    \"learnings_count\": len(agent_output[\"data\"][\"learnings\"]),\n                    \"learnings_sample\": agent_output[\"data\"][\"learnings\"][:5]\n                    if agent_output[\"data\"][\"learnings\"]\n                    else [],\n                    \"visited_urls_count\": len(agent_output[\"data\"][\"visited_urls\"]),\n                    \"visited_urls_sample\": agent_output[\"data\"][\"visited_urls\"][:5]\n                    if agent_output[\"data\"][\"visited_urls\"]\n                    else [],\n                },\n            },\n        }\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        try:\n            await agent.cleanup()\n        except Exception as e:\n            print(f\"Error during cleanup: {str(e)}\")\n\n\nif __name__ == \"__main__\":\n    try:\n        asyncio.run(run_agent())\n    except KeyboardInterrupt:\n        print(\"\\nOperation cancelled by user\")\n    except Exception as e:\n        print(f\"Unexpected error: {str(e)}\")\n"}
{"type": "test_file", "path": "mesh/tests/pumpfun_analysis_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.pumpfun_token_agent import PumpFunTokenAgent  # noqa: E402\n\nload_dotenv()\n\n\nQUERIES = {\n    \"creation\": {\"query\": \"Show me the latest Solana token creations in the last hour\"},\n    \"metrics_usdc\": {\n        \"query\": \"Get market cap, liquidity and trade volume for 98mb39tPFKQJ4Bif8iVg9mYb9wsfPZgpgN1sxoVTpump using USDC pair\"\n    },\n    \"metrics_sol\": {\n        \"query\": \"Get market cap, liquidity and trade volume for 98mb39tPFKQJ4Bif8iVg9mYb9wsfPZgpgN1sxoVTpump using SOL pair\"\n    },\n    \"metrics_virtual\": {\n        \"query\": \"Get market cap, liquidity and trade volume for 98mb39tPFKQJ4Bif8iVg9mYb9wsfPZgpgN1sxoVTpump using Virtual pair\"\n    },\n    \"holders\": {\"query\": \"Show me the top token holders of EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v\"},\n    \"buyers\": {\"query\": \"Show me the first 100 buyers of 2Z4FzKBcw48KBD2PaR4wtxo4sYGbS7QqTQCLoQnUpump\"},\n    \"holder_status\": {\n        \"query\": \"Check if address 'Z9y8X7w6V5u4T3s2R1q0P9o8N7m6L5k4J3i2H1g0F9e8' still holds token 'A1b2C3d4E5f6G7h8I9j0K1l2M3n4O5p6Q7r8S9t0U1v2'\"\n    },\n    \"top_traders\": {\n        \"query\": \"List the top traders of token 'EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v' on Pump Fun DEX\"\n    },\n    \"graduated_tokens\": {\n        \"query\": \"Show me all tokens that have graduated on Pump.fun in the last 48 hours\"\n    },\n}\n\n\nasync def format_query_result(query_name: str, agent_output: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Format the query results based on query type.\"\"\"\n    base_result = {\"input\": QUERIES[query_name], \"output\": {\"response\": agent_output.get(\"response\", \"\"), \"data\": {}}}\n\n    if \"error\" in agent_output:\n        return {\"input\": QUERIES[query_name], \"error\": str(agent_output[\"error\"])}\n\n    if query_name == \"creation\":\n        base_result[\"output\"][\"data\"] = {\n            \"tokens\": [\n                {\n                    \"name\": token[\"token_info\"][\"name\"],\n                    \"symbol\": token[\"token_info\"][\"symbol\"],\n                    \"mint_address\": token[\"token_info\"][\"mint_address\"],\n                    \"amount\": token[\"amount\"],\n                    \"signer\": token[\"signer\"],\n                }\n                for token in agent_output.get(\"data\", {}).get(\"tokens\", [])[:10]\n            ]\n        }\n    elif query_name == \"holders\":\n        base_result[\"output\"][\"data\"] = {\n            \"holders\": [\n                {\n                    \"address\": holder[\"address\"],\n                    \"holding\": holder[\"holding\"],\n                    \"percentage_of_supply\": holder.get(\"percentage_of_supply\", 0),\n                    \"token_info\": holder[\"token_info\"],\n                }\n                for holder in agent_output.get(\"data\", {}).get(\"holders\", [])[:10]\n            ],\n            \"total_supply\": agent_output.get(\"data\", {}).get(\"total_supply\", 0),\n        }\n    elif query_name == \"buyers\":\n        base_result[\"output\"][\"data\"] = {\n            \"buyers\": [\n                {\n                    \"owner\": buyer[\"owner\"],\n                    \"amount\": buyer[\"amount\"],\n                    \"amount_usd\": buyer.get(\"amount_usd\", 0),\n                    \"time\": buyer[\"time\"],\n                    \"currency_pair\": buyer.get(\"currency_pair\", \"\"),\n                }\n                for buyer in agent_output.get(\"data\", {}).get(\"buyers\", [])[:100]\n            ],\n            \"unique_buyer_count\": agent_output.get(\"data\", {}).get(\"unique_buyer_count\", 0),\n        }\n    elif query_name == \"holder_status\":\n        base_result[\"output\"][\"data\"] = {\n            \"holder_statuses\": [\n                {\n                    \"owner\": status[\"owner\"],\n                    \"current_balance\": status[\"current_balance\"],\n                    \"initial_balance\": status.get(\"initial_balance\", 0),\n                    \"status\": status.get(\"status\", \"unknown\"),\n                }\n                for status in agent_output.get(\"data\", {}).get(\"holder_statuses\", [])\n            ],\n            \"summary\": agent_output.get(\"data\", {}).get(\"summary\", {}),\n        }\n    elif query_name == \"top_traders\":\n        base_result[\"output\"][\"data\"] = {\n            \"traders\": agent_output.get(\"data\", {}).get(\"traders\", []),\n            \"markets\": agent_output.get(\"data\", {}).get(\"markets\", []),\n        }\n    elif query_name == \"graduated_tokens\":\n        base_result[\"output\"][\"data\"] = {\n            \"graduated_tokens\": [\n                {\n                    \"price_usd\": token.get(\"price_usd\", 0),\n                    \"market_cap_usd\": token.get(\"market_cap_usd\", 0),\n                    \"token_info\": {\n                        \"name\": token.get(\"token_info\", {}).get(\"name\", \"Unknown\"),\n                        \"symbol\": token.get(\"token_info\", {}).get(\"symbol\", \"Unknown\"),\n                        \"mint_address\": token.get(\"token_info\", {}).get(\"mint_address\", \"\"),\n                        \"decimals\": token.get(\"token_info\", {}).get(\"decimals\", 0),\n                    }\n                }\n                for token in agent_output.get(\"data\", {}).get(\"graduated_tokens\", [])\n            ],\n            \"timeframe_hours\": agent_output.get(\"data\", {}).get(\"timeframe_hours\", 24),\n            \"tokens_without_price_data\": agent_output.get(\"data\", {}).get(\"tokens_without_price_data\", []),\n        }\n    elif query_name.startswith(\"metrics\"):\n        base_result[\"output\"][\"data\"] = agent_output.get(\"data\", {})\n        if \"fallback_used\" in agent_output.get(\"data\", {}):\n            base_result[\"output\"][\"fallback_used\"] = agent_output[\"data\"][\"fallback_used\"]\n\n    return base_result\n\n\nasync def run_single_query(agent: PumpFunTokenAgent, query_name: str) -> Dict[str, Any]:\n    \"\"\"Run a single query using the PumpFunTokenAgent.\"\"\"\n    try:\n        agent_input = QUERIES[query_name]\n        agent_output = await agent.handle_message(agent_input)\n        return await format_query_result(query_name, agent_output)\n    except Exception as e:\n        return {\"input\": QUERIES[query_name], \"error\": str(e)}\n\n\nasync def save_results(results: Dict[str, Any], output_file: Path):\n    \"\"\"Save results to a YAML file.\"\"\"\n    try:\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(results, f, allow_unicode=True, sort_keys=False)\n    except Exception as e:\n        print(f\"Error saving results: {str(e)}\")\n\n\nasync def run_queries(query_type: str = \"all\", query_params: Dict = None):\n    \"\"\"Run queries based on the specified type with optional custom parameters.\"\"\"\n    results = {}\n    script_dir = Path(__file__).parent\n    current_file = Path(__file__).stem\n    base_filename = f\"{current_file}_example\"\n    output_file = script_dir / f\"{base_filename}.yaml\"\n\n    query_params = query_params or {}\n\n    async with PumpFunTokenAgent() as agent:\n        try:\n            if query_type.lower() == \"all\":\n                for query_name in QUERIES.keys():\n                    print(f\"Running query: {query_name}\")\n                    if query_name in query_params:\n                        # Update query if needed\n                        if \"query\" in query_params[query_name]:\n                            QUERIES[query_name][\"query\"] = query_params[query_name][\"query\"]\n                    results[query_name] = await run_single_query(agent, query_name)\n            elif query_type in QUERIES:\n                if query_type in query_params:\n                    # Update query if needed\n                    if \"query\" in query_params[query_type]:\n                        QUERIES[query_type][\"query\"] = query_params[query_type][\"query\"]\n                results[query_type] = await run_single_query(agent, query_type)\n            else:\n                raise ValueError(f\"Invalid query type. Must be one of: {', '.join(QUERIES.keys())} or 'all'\")\n\n            await save_results(results, output_file)\n            print(f\"Results saved to {output_file}\")\n\n        except Exception as e:\n            print(f\"Error executing queries: {str(e)}\")\n\n\ndef main():\n    \"\"\"Main entry point for the script.\"\"\"\n    agent_query = \"all\"\n    # Available agent_query options:\n    # - creation (recent token creations)\n    # - metrics_usdc (market metrics with USDC pair)\n    # - metrics_sol (market metrics with SOL pair)\n    # - metrics_virtual (market metrics with Virtual pair)\n    # - holders (token holders analysis)\n    # - buyers (first buyers analysis)\n    # - holder_status (check if buyers are still holding)\n    # - top_traders (top traders analysis)\n    # - graduated_tokens (recently graduated tokens with prices)\n    # - all (runs all queries)\n    #\n    print(f\"Running query type: {agent_query}\")\n    asyncio.run(run_queries(agent_query))\n\n\nasync def test_direct_tool_call():\n    \"\"\"Test direct tool calls to the agent.\"\"\"\n    async with PumpFunTokenAgent() as agent:\n        # Test query_latest_graduated_tokens tool directly\n        print(\"Testing direct tool call: query_latest_graduated_tokens\")\n        result = await agent.handle_message({\n            \"tool\": \"query_latest_graduated_tokens\",\n            \"tool_arguments\": {\n                \"timeframe\": 48  # Look back 48 hours\n            },\n            \"raw_data_only\": True\n        })\n        \n        # Print token count and some basic information\n        if \"data\" in result and \"graduated_tokens\" in result[\"data\"]:\n            token_count = len(result[\"data\"][\"graduated_tokens\"])\n            print(f\"Found {token_count} graduated tokens with price data\")\n            \n            # Print a sample token if available\n            if token_count > 0:\n                sample_token = result[\"data\"][\"graduated_tokens\"][0]\n                print(f\"Sample token: {sample_token['token_info']['symbol']} ({sample_token['token_info']['name']})\")\n                print(f\"Price: ${sample_token['price_usd']}\")\n                print(f\"Market Cap: ${sample_token['market_cap_usd']}\")\n        \n        # Print tokens that were found but didn't have price data\n        if \"data\" in result and \"tokens_without_price_data\" in result[\"data\"]:\n            no_price_count = len(result[\"data\"][\"tokens_without_price_data\"])\n            print(f\"Found {no_price_count} graduated tokens without price data\")\n            \n        # Check for errors\n        if \"error\" in result:\n            print(f\"Error in tool call: {result['error']}\")\n\n\nasync def test_graduated_tokens(timeframe=24):\n    \"\"\"Run only the graduated tokens test with a specific timeframe.\"\"\"\n    # Update the query to reflect the timeframe\n    QUERIES[\"graduated_tokens\"][\"query\"] = f\"Show me all tokens that have graduated on Pump.fun in the last {timeframe} hours\"\n    \n    # Run the query\n    print(f\"Running graduated_tokens test with {timeframe} hour timeframe\")\n    \n    async with PumpFunTokenAgent() as agent:\n        # First, try with natural language query\n        nl_result = await run_single_query(agent, \"graduated_tokens\")\n        print(\"Natural language query results:\")\n        print(yaml.dump(nl_result, allow_unicode=True, sort_keys=False))\n        \n        # Then try with direct tool call for comparison\n        print(\"\\nDirect tool call results:\")\n        direct_result = await agent.handle_message({\n            \"tool\": \"query_latest_graduated_tokens\",\n            \"tool_arguments\": {\n                \"timeframe\": timeframe\n            },\n            \"raw_data_only\": True\n        })\n        \n        # Count and display tokens found\n        token_count = len(direct_result.get(\"data\", {}).get(\"graduated_tokens\", []))\n        print(f\"Found {token_count} graduated tokens with price data\")\n        \n        # Save results\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        output_file = script_dir / f\"{current_file}_graduated_tokens_{timeframe}h.yaml\"\n        \n        results = {\n            \"natural_language_query\": nl_result,\n            \"direct_tool_call\": {\n                \"input\": {\n                    \"tool\": \"query_latest_graduated_tokens\",\n                    \"tool_arguments\": {\"timeframe\": timeframe}\n                },\n                \"output\": direct_result\n            }\n        }\n        \n        await save_results(results, output_file)\n        print(f\"Results saved to {output_file}\")\n\n\nif __name__ == \"__main__\":\n    # Uncomment to run direct tool call test instead of the main function\n    # asyncio.run(test_direct_tool_call())\n    \n    # Uncomment to run only the graduated tokens test with a custom timeframe\n    # asyncio.run(test_graduated_tokens(48))  # Use 48 hours timeframe\n    \n    main()\n"}
{"type": "test_file", "path": "mesh/tests/firecrawl_search_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.firecrawl_search_agent import FirecrawlSearchAgent  # noqa: E402\n\nload_dotenv()\n\n\nasync def run_agent():\n    agent = FirecrawlSearchAgent()\n    try:\n        # Example for natural language query with analysis\n        agent_input_query = {\n            \"query\": \"What are the latest developments in zero knowledge proofs?\",\n            \"raw_data_only\": False,\n        }\n        agent_output_query = await agent.handle_message(agent_input_query)\n\n        # Example for natural language query with raw data only\n        agent_input_query_raw = {\n            \"query\": \"What are the latest developments in zero knowledge proofs?\",\n            \"raw_data_only\": True,\n        }\n        agent_output_query_raw = await agent.handle_message(agent_input_query_raw)\n\n        # Example for direct tool (no LLM analysis, always returns raw data)\n        agent_input_search = {\n            \"tool\": \"firecrawl_web_search\",\n            \"tool_arguments\": {\"search_term\": \"zero knowledge proofs recent advancements\"},\n        }\n        agent_output_search = await agent.handle_message(agent_input_search)\n\n        # Example for direct tool (no LLM analysis, always returns raw data)\n        agent_input_extract = {\n            \"tool\": \"firecrawl_extract_web_data\",\n            \"tool_arguments\": {\n                \"urls\": [\"https://ethereum.org/en/zero-knowledge-proofs/\"],\n                \"extraction_prompt\": \"Extract information about how zero knowledge proofs are being used in blockchain technology\",\n                \"enable_web_search\": False,\n            },\n        }\n        agent_output_extract = await agent.handle_message(agent_input_extract)\n\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"natural_language_query_with_analysis\": {\"input\": agent_input_query, \"output\": agent_output_query},\n            \"natural_language_query_raw_data\": {\"input\": agent_input_query_raw, \"output\": agent_output_query_raw},\n            \"direct_search\": {\"input\": agent_input_search, \"output\": agent_output_search},\n            \"direct_extract\": {\"input\": agent_input_extract, \"output\": agent_output_extract},\n        }\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/bitquery_sol_token_info_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.bitquery_solana_token_info_agent import BitquerySolanaTokenInfoAgent  # noqa: E402\n\nload_dotenv()\n\n\nasync def run_agent():\n    agent = BitquerySolanaTokenInfoAgent()\n    try:\n        # Test with a query that mentions a token mint address for trading info\n        agent_input = {\"query\": \"Get token info for HeLp6NuQkmYB4pYWo2zYs22mESHXPQYzXbB8n4V98jwC\"}\n        agent_output = await agent.handle_message(agent_input)\n        print(f\"Result of handle_message (by token address): {agent_output}\")\n\n        # Test with a query for trending tokens\n        agent_input_trending = {\"query\": \"Get trending tokens\"}\n        agent_output_trending = await agent.handle_message(agent_input_trending)\n        print(f\"Result of handle_message (trending tokens): {agent_output_trending}\")\n\n        # Test direct tool call for token trading info\n        agent_input_direct_tool = {\n            \"tool\": \"get_token_trading_info\",\n            \"tool_arguments\": {\"token_address\": \"HeLp6NuQkmYB4pYWo2zYs22mESHXPQYzXbB8n4V98jwC\"},\n        }\n        agent_output_direct_tool = await agent.handle_message(agent_input_direct_tool)\n        print(f\"Result of direct tool call (token trading info): {agent_output_direct_tool}\")\n\n        # Test direct tool call for top trending tokens\n        agent_input_direct_trending = {\"tool\": \"get_top_trending_tokens\", \"tool_arguments\": {}}\n        agent_output_direct_trending = await agent.handle_message(agent_input_direct_trending)\n        print(f\"Result of direct tool call (trending tokens): {agent_output_direct_trending}\")\n\n        # Test with raw_data_only flag\n        agent_input_raw_data = {\n            \"query\": \"Get token info for HeLp6NuQkmYB4pYWo2zYs22mESHXPQYzXbB8n4V98jwC\",\n            \"raw_data_only\": True,\n        }\n        agent_output_raw_data = await agent.handle_message(agent_input_raw_data)\n        print(f\"Result with raw_data_only=True: {agent_output_raw_data}\")\n\n        # Save the test inputs and outputs to a YAML file for further inspection\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"input_by_token_address\": agent_input,\n            \"output_by_token_address\": agent_output,\n            \"input_trending\": agent_input_trending,\n            \"output_trending\": agent_output_trending,\n            \"input_direct_tool\": agent_input_direct_tool,\n            \"output_direct_tool\": agent_output_direct_tool,\n            \"input_direct_trending\": agent_input_direct_trending,\n            \"output_direct_trending\": agent_output_direct_trending,\n            \"input_raw_data\": agent_input_raw_data,\n            \"output_raw_data\": agent_output_raw_data,\n        }\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/zkignite_analyst_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.zkignite_analyst_agent import ZkIgniteAnalystAgent  # noqa: E402\n\n\nasync def run_agent():\n    agent = ZkIgniteAnalystAgent()\n    try:\n        agent_input = {\n            \"query\": \"I have some ETH and ZK token and I want to farm on zksync. What are the best opportunities?\",\n            \"task_id\": \"123\",\n        }\n        agent_output = await agent.handle_message(agent_input)\n\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\"input\": agent_input, \"output\": agent_output}\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/elfa_twitter_intelligence_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.elfa_twitter_intelligence_agent import ElfaTwitterIntelligenceAgent  # noqa: E402\n\nload_dotenv()\n\n\nasync def run_agent():\n    agent = ElfaTwitterIntelligenceAgent()\n    try:\n        # Test with a query for searching mentions (token/topic)\n        agent_input_mentions = {\"query\": \"Search for mentions of Heurist, HEU, and heurist_ai in the last 30 days\"}\n        agent_output_mentions = await agent.handle_message(agent_input_mentions)\n        print(f\"Result of handle_message (search mentions): {agent_output_mentions}\")\n\n        # Test with a query for account analysis (using the new search_account tool)\n        agent_input_account = {\"query\": \"Analyze the Twitter account @heurist_ai\"}\n        agent_output_account = await agent.handle_message(agent_input_account)\n        print(f\"Result of handle_message (account analysis): {agent_output_account}\")\n\n        # Test direct trending tokens tool (no LLM routing)\n        agent_input_trending = {\n            \"tool\": \"get_trending_tokens\",\n            \"tool_arguments\": {\"time_window\": \"24h\"},\n            \"query\": \"Get trending tokens for reference\",\n        }\n        agent_output_trending = await agent.handle_message(agent_input_trending)\n        print(f\"Result of handle_message (trending tokens direct call): {agent_output_trending}\")\n\n        # Save the test inputs and outputs to a YAML file for further inspection\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\n            \"input_mentions\": agent_input_mentions,\n            \"output_mentions\": agent_output_mentions,\n            \"input_account\": agent_input_account,\n            \"output_account\": agent_output_account,\n            \"input_trending\": agent_input_trending,\n            \"output_trending\": agent_output_trending,\n        }\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "test_file", "path": "mesh/tests/duckduckgo_search_agent.py", "content": "import asyncio\nimport sys\nfrom pathlib import Path\n\nimport yaml\nfrom dotenv import load_dotenv\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\nfrom mesh.duckduckgo_search_agent import DuckDuckGoSearchAgent  # noqa: E402\n\nload_dotenv()\n\n\nasync def run_agent():\n    agent = DuckDuckGoSearchAgent()\n    try:\n        # Direct tool call input\n        agent_input = {\n            \"query\": \"What are the latest developments in artificial intelligence?\",\n            \"tool\": \"search_web\",  # Specify the tool name\n            \"tool_arguments\": {  # Provide tool-specific arguments\n                \"search_term\": \"What are the latest developments in artificial intelligence?\",  # Changed from \"query\" to \"search_term\"\n                \"max_results\": 3,\n            },\n            \"raw_data_only\": False,  # Set to True if you only want the raw data without LLM analysis\n        }\n\n        agent_output = await agent.handle_message(agent_input)\n\n        script_dir = Path(__file__).parent\n        current_file = Path(__file__).stem\n        base_filename = f\"{current_file}_example\"\n        output_file = script_dir / f\"{base_filename}.yaml\"\n\n        yaml_content = {\"input\": agent_input, \"output\": agent_output}\n\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(yaml_content, f, allow_unicode=True, sort_keys=False)\n\n        print(f\"Results saved to {output_file}\")\n\n    finally:\n        await agent.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(run_agent())\n"}
{"type": "source_file", "path": "__init__.py", "content": ""}
{"type": "source_file", "path": "agents/tool_box.py", "content": "import logging\nfrom typing import Any, Dict, Optional\n\nimport aiohttp\n\nfrom .tool_decorator import tool\n\nlogger = logging.getLogger(__name__)\n## YOUR TOOLS GO HERE\n\n\nclass ToolBox:\n    \"\"\"Base class containing tool configurations and handlers\"\"\"\n\n    def __init__(self):\n        # Base tools configuration\n        # Can be used to add tools by defining a function schema explicitly if needed\n        self.tools_config = [\n            # {\n            #     \"type\": \"function\",\n            #     \"function\": {\n            #         \"name\": \"generate_image\",\n            #         \"description\": \"Generate an image based on a text prompt, any request to create an image should be handled by this tool, only use this tool if the user asks to create an image\",\n            #         \"parameters\": {\n            #             \"type\": \"object\",\n            #             \"properties\": {\n            #                 \"prompt\": {\"type\": \"string\", \"description\": \"The prompt to generate the image from\"}\n            #             },\n            #             \"required\": [\"prompt\"]\n            #         }\n            #     }\n            # },\n        ]\n\n        # Base handlers\n        # Can be used to add handlers for schemas that were defined explicitly\n        self.tool_handlers = {\n            # \"generate_image\": self.handle_image_generation\n        }\n\n        self.decorated_tools = [\n            self.get_crypto_price,\n            self.handle_image_generation,\n            self.generate_image_prompt_for_posts,\n            self.get_current_time,\n        ]\n\n    @staticmethod\n    @tool(\"Generate an image based on a text prompt\")\n    # async def handle_image_generation(self, args: Dict[str, Any], agent_context: Any) -> Dict[str, Any]: #example for explicitly defined schema\n    async def handle_image_generation(prompt: str, agent_context: Any) -> Dict[str, Any]:\n        \"\"\"Generate an image based on a text prompt. Use this tool only when the user explicitly requests to create an image.\"\"\"\n        logger.info(prompt)\n        try:\n            image_url = await agent_context.handle_image_generation(\n                prompt\n            )  # args['prompt'] for explicitly defined schema\n            return {\"image_url\": image_url, \"result\": image_url}\n        except Exception as e:\n            logger.error(f\"Image generation failed: {str(e)}\")\n            return {\"error\": str(e)}\n\n    @staticmethod\n    @tool(\"Get the current or historical price of a cryptocurrency in USD\")\n    async def get_crypto_price(ticker: str, timestamp: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        Get the current or historical price of a cryptocurrency in USD from Binance.\n\n        Args:\n            ticker: The cryptocurrency ticker symbol (e.g., BTC, ETH, SOL)\n            timestamp: Optional ISO format timestamp (e.g., '2024-03-20 14:30:00'). If not provided, returns current price\n\n        Returns:\n            Dict containing the price information\n        \"\"\"\n        try:\n            normalized_ticker = f\"{ticker.upper()}USDT\"\n\n            if timestamp is None:\n                # Get current price (existing logic)\n                async with aiohttp.ClientSession() as session:\n                    url = f\"https://api.binance.com/api/v3/ticker/price?symbol={normalized_ticker}\"\n                    async with session.get(url) as response:\n                        if response.status == 200:\n                            data = await response.json()\n                            price = float(data[\"price\"])\n                            logger.info(f\"The current price for {normalized_ticker}: ${price:.2f}\")\n                            return {\"result\": f\"The current price for {normalized_ticker}: ${price:.2f}\"}\n            else:\n                # Get historical price\n                from datetime import datetime\n\n                # Convert timestamp to milliseconds\n                dt = datetime.fromisoformat(timestamp)\n                timestamp_ms = int(dt.timestamp() * 1000)\n\n                async with aiohttp.ClientSession() as session:\n                    # Get klines (candlestick) data around the specified time\n                    url = \"https://api.binance.com/api/v3/klines\"\n                    params = {\n                        \"symbol\": normalized_ticker,\n                        \"interval\": \"1m\",  # 1 minute interval\n                        \"startTime\": timestamp_ms - 60000,  # 1 minute before\n                        \"endTime\": timestamp_ms + 60000,  # 1 minute after\n                        \"limit\": 1,\n                    }\n\n                    async with session.get(url, params=params) as response:\n                        if response.status == 200:\n                            data = await response.json()\n                            if data:\n                                price = float(data[0][4])  # Close price\n                                logger.info(f\"The price for {normalized_ticker} at {timestamp}: ${price:.2f}\")\n                                return {\"result\": f\"The price for {normalized_ticker} at {timestamp}: ${price:.2f}\"}\n                            return {\"error\": f\"No price data available for {normalized_ticker} at {timestamp}\"}\n\n            error_msg = f\"Failed to get price for {normalized_ticker}\"\n            logger.error(error_msg)\n            return {\"error\": error_msg}\n\n        except ValueError as ve:\n            error_msg = f\"Invalid timestamp format. Please use ISO format (e.g., '2024-03-20 14:30:00'): {str(ve)}\"\n            logger.error(error_msg)\n            return {\"error\": error_msg}\n        except Exception as e:\n            error_msg = f\"Error getting crypto price: {str(e)}\"\n            logger.error(error_msg)\n            return {\"error\": error_msg}\n\n    @staticmethod\n    @tool(\"Generate an image prompt based on text input\")\n    async def generate_image_prompt_for_posts(text: str, agent_context: Any) -> Dict[str, str]:\n        \"\"\"\n        Generate a detailed image prompt using the core agent's image prompt generator.\n        Use this tool when you need to create an optimized prompt for image generation.\n        IMPORTANT: This tool is only for image generation for posts, and images about you.\n        IF the request is simple, just use image generation tool instead.\n\n        Args:\n            text: The input text to base the image prompt on\n            agent_context: The core agent context\n\n        Returns:\n            Dict containing the generated image prompt\n        \"\"\"\n        try:\n            prompt = await agent_context.generate_image_prompt(text)\n            return {\"result\": prompt} if prompt else {\"error\": \"Failed to generate prompt\"}\n        except Exception as e:\n            logger.error(f\"Image prompt generation failed: {str(e)}\")\n            return {\"error\": str(e)}\n\n    @staticmethod\n    @tool(\"Get the current time in ISO format\")\n    async def get_current_time() -> Dict[str, str]:\n        \"\"\"\n        Get the current time in ISO format.\n\n        Returns:\n            Dict containing the current time in ISO format\n        \"\"\"\n        from datetime import datetime\n\n        try:\n            current_time = datetime.now().isoformat(timespec=\"seconds\")\n            return {\"result\": current_time}\n        except Exception as e:\n            logger.error(f\"Error getting current time: {str(e)}\")\n            return {\"error\": str(e)}\n"}
{"type": "source_file", "path": "agents/tools_mcp.py", "content": "import json\nimport logging\nfrom typing import Any, Dict, List, Optional\n\nfrom clients.mcp_client import MCPClient\n\nlogger = logging.getLogger(__name__)\n\n\nclass Tools:\n    def __init__(self, mcp_server_url=\"http://localhost:8000/sse\"):\n        \"\"\"Initialize the MCP client and fetch available tools\"\"\"\n        self.mcp_url = mcp_server_url\n        self.mcp_client = MCPClient()\n        self.available_tools = []\n        self.tools_config = []\n        print(f\"Initialized MCP client with URL: {self.mcp_url}\")\n\n    async def initialize(self, server_url: str = \"http://localhost:8000/sse\"):\n        \"\"\"Initialize the MCP client and fetch available tools\"\"\"\n        try:\n            print(f\"Connecting to MCP server with URL: {self.mcp_url}\")\n            tools = await self.mcp_client.connect_to_sse_server(server_url=server_url)\n            self.available_tools = tools\n            self.tools_config = self.mcp_client.get_available_tools_json()\n            logger.info(f\"Connected to MCP server with {len(tools)} tools\")\n            return True\n        except Exception as e:\n            logger.error(f\"Failed to initialize MCP Client: {str(e)}\")\n            return False\n\n    def get_tools_config(self, filter_tools: List[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Get tool configurations, optionally filtered by tool names\n\n        Args:\n            filter_tools: Optional list of tool names to include\n\n        Returns:\n            List of tool configurations\n        \"\"\"\n        all_tools = self.tools_config\n\n        if filter_tools:\n            return [tool for tool in all_tools if tool[\"function\"][\"name\"] in filter_tools]\n        return all_tools\n\n    async def execute_tool(self, tool_name: str, args: Dict[str, Any], agent_context: Any) -> Optional[Dict[str, Any]]:\n        \"\"\"Execute a tool by name with given arguments\"\"\"\n        try:\n            # Call the tool through MCP\n            result = await self.mcp_client.call_tool(tool_name, args)\n            if result:\n                # Get the actual data structure instead of a formatted string\n                formatted_content = self.mcp_client.format_result(result.content)\n\n                # Format the response\n                formatted_result = {\"tool_name\": tool_name, \"args\": args, \"result\": formatted_content}\n\n                # Add the tool_call field for compatibility\n                formatted_result[\"tool_call\"] = json.dumps(\n                    {\"tool_call\": tool_name, \"processed\": True, \"args\": args, \"result\": formatted_content},\n                    default=str,\n                )\n\n                return formatted_result\n\n        except Exception as e:\n            logger.error(f\"Error executing tool {tool_name}: {str(e)}\")\n            return {\n                \"tool_name\": tool_name,\n                \"args\": args,\n                \"error\": str(e),\n                \"tool_call\": json.dumps(\n                    {\"tool_call\": tool_name, \"processed\": False, \"args\": args, \"error\": str(e)},\n                    default=str,\n                ),\n            }\n"}
{"type": "source_file", "path": "agents/tool_decorator.py", "content": "import inspect\nfrom typing import Any, Callable, Dict\n\n\ndef tool(description: str):\n    \"\"\"\n    A decorator factory that creates a tool decorator with a specified description.\n    \"\"\"\n\n    def decorator(func):\n        # Add metadata to the function\n        func.name = func.__name__\n        func.description = func.__doc__ if func.__doc__ != inspect._empty else description\n\n        # generate the parameter schema without agent_context\n        func.is_async = inspect.iscoroutinefunction(func)\n        signature = inspect.signature(func)\n        func.is_ctx_required = \"agent_context\" in signature.parameters\n\n        # Map Python types to valid JSON Schema types\n        type_mapping = {\n            \"str\": \"string\",\n            \"int\": \"number\",\n            \"float\": \"number\",\n            \"bool\": \"boolean\",\n            \"list\": \"array\",\n            \"dict\": \"object\",\n            \"NoneType\": \"null\",\n        }\n\n        parameters = {k: v for k, v in signature.parameters.items() if k != \"agent_context\"}\n        func.args_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                param: {\n                    \"type\": type_mapping.get(str(param_type.annotation.__name__).lower(), \"string\"),\n                    \"description\": str(param_type.annotation)\n                    if param_type.annotation != inspect._empty\n                    else \"No type specified\",\n                }\n                for param, param_type in parameters.items()\n            },\n            \"required\": [param for param, param_type in parameters.items() if param_type.default == inspect._empty],\n        }\n\n        async def wrapper(args: Dict[str, Any], agent_context: Any):\n            # re-add agent context\n            if func.is_ctx_required:\n                args[\"agent_context\"] = agent_context\n            result = await func(**args) if func.is_async else func(**args)\n            return result\n\n        wrapper.name = func.name\n        wrapper.description = func.description\n        wrapper.args_schema = func.args_schema\n        wrapper.original = func\n\n        return wrapper\n\n    return decorator\n\n\ndef convert_to_function_schema(func: Callable) -> Dict[str, Any]:\n    \"\"\"\n    Converts a decorated function into an OpenAI function schema format.\n    \"\"\"\n    return {\n        \"type\": \"function\",\n        \"function\": {\"name\": func.name, \"description\": func.description, \"parameters\": func.args_schema},\n    }\n\n\ndef get_tool_schemas(tools: list[Callable]) -> list[Dict[str, Any]]:\n    \"\"\"\n    Convert a list of tool-decorated functions into OpenAI function schemas.\n\n    Args:\n        tools: List of functions decorated with @tool\n\n    Returns:\n        List of function schemas in OpenAI format\n    \"\"\"\n    return [convert_to_function_schema(tool) for tool in tools]\n"}
{"type": "source_file", "path": "agents/__init__.py", "content": ""}
{"type": "source_file", "path": "agents/research_agent.py", "content": "import asyncio\nimport json\nimport os\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, TypedDict\n\nimport dotenv\nfrom firecrawl import FirecrawlApp\n\nfrom core.llm import call_llm\nfrom core.utils.text_splitter import trim_prompt\n\nos.environ.clear()\ndotenv.load_dotenv(override=True)\n\nHEURIST_BASE_URL = os.getenv(\"HEURIST_BASE_URL\")\nHEURIST_API_KEY = os.getenv(\"HEURIST_API_KEY\")\nLARGE_MODEL_ID = os.getenv(\"LARGE_MODEL_ID\")\nLARGER_MODEL_ID = os.getenv(\"LARGER_MODEL_ID\", \"mistralai/mixtral-8x22b-instruct\")\nSMALL_MODEL_ID = os.getenv(\"SMALL_MODEL_ID\")\n\n\ndef system_prompt() -> str:\n    \"\"\"Creates the system prompt with current timestamp.\"\"\"\n    now = datetime.now().isoformat()\n    return f\"\"\"You are an expert researcher. Today is {now}. Follow these instructions when responding:\n    - You may be asked to research subjects that is after your knowledge cutoff, assume the user is right when presented with news.\n    - The user is a highly experienced analyst, no need to simplify it, be as detailed as possible and make sure your response is correct.\n    - Be highly organized.\n    - Suggest solutions that I didn't think about.\n    - Be proactive and anticipate my needs.\n    - Treat me as an expert in all subject matter.\n    - Mistakes erode my trust, so be accurate and thorough.\n    - Provide detailed explanations, I'm comfortable with lots of detail.\n    - Value good arguments over authorities, the source is irrelevant.\n    - Consider new technologies and contrarian ideas, not just the conventional wisdom.\n    - You may use high levels of speculation or prediction, just flag it for me.\n    IMPORTANT: MAKE SURE YOU RETURN THE JSON ONLY, NO OTHER TEXT OR MARKUP AND A VALID JSON.\n    DONT ADD ANY COMMENTS OR MARKUP TO THE JSON. Example NO # or /* */ or /* */ or // or any other comments or markup.\n    MAKE SURE YOU RETURN THE JSON ONLY, JSON SHOULD BE PERFECTLY FORMATTED. ALL KEYS SHOULD BE OPENED AND CLOSED.\n    \"\"\"\n\n\nclass SearchResponse(TypedDict):\n    data: List[Dict[str, str]]\n\n\nclass ResearchResult(TypedDict):\n    learnings: List[str]\n    visited_urls: List[str]\n\n\n@dataclass\nclass SerpQuery:\n    query: str\n    research_goal: str\n\n\nasync def generate_feedback(query: str) -> List[str]:\n    \"\"\"Generates follow-up questions to clarify research direction.\"\"\"\n    prompt = f\"Given this research topic: {query}, generate 3-5 follow-up questions to better understand the user's research needs. Return the response as a JSON object with a 'questions' array field.\"\n    response = call_llm(\n        base_url=HEURIST_BASE_URL,\n        api_key=HEURIST_API_KEY,\n        model_id=LARGER_MODEL_ID,\n        system_prompt=system_prompt(),\n        user_prompt=prompt,\n    )\n\n    if not response:\n        return \"Sorry, I couldn't process your message.\", None\n\n    else:  # Add null check\n        try:\n            result = json.loads(response)\n            questions = result.get(\"questions\", [])\n            return questions\n        except json.JSONDecodeError as e:\n            print(f\"Error parsing JSON response: {e}\")\n            print(f\"Raw response: {response}\")\n            return []\n\n\nclass Firecrawl:\n    \"\"\"Simple wrapper for Firecrawl SDK.\"\"\"\n\n    def __init__(self, api_key: str = \"\", api_url: Optional[str] = None):\n        self.app = FirecrawlApp(api_key=api_key, api_url=api_url)\n        self._last_request_time = 0  # Track the last request time\n\n    async def search(self, query: str, timeout: int = 15000, limit: int = 5) -> SearchResponse:\n        \"\"\"Search using Firecrawl SDK in a thread pool to keep it async.\"\"\"\n        try:\n            # Add rate limiting delay\n            current_time = asyncio.get_event_loop().time()\n            time_since_last_request = current_time - self._last_request_time\n            if time_since_last_request < 6:\n                await asyncio.sleep(6 - time_since_last_request)\n\n            # Update last request time\n            self._last_request_time = asyncio.get_event_loop().time()\n\n            # Run the synchronous SDK call in a thread pool\n            response = await asyncio.get_event_loop().run_in_executor(\n                None,\n                lambda: self.app.search(query=query, params={\"scrapeOptions\": {\"formats\": [\"markdown\"]}}),\n            )\n            # Handle the response format from the SDK\n            if isinstance(response, dict) and \"data\" in response:\n                # Response is already in the right format\n                return response\n            elif isinstance(response, dict) and \"success\" in response:\n                # Response is in the documented format\n                return {\"data\": response.get(\"data\", [])}\n            elif isinstance(response, list):\n                # Response is a list of results\n                formatted_data = []\n                for item in response:\n                    if isinstance(item, dict):\n                        formatted_data.append(item)\n                    else:\n                        # Handle non-dict items (like objects)\n                        formatted_data.append(\n                            {\n                                \"url\": getattr(item, \"url\", \"\"),\n                                \"markdown\": getattr(item, \"markdown\", \"\") or getattr(item, \"content\", \"\"),\n                                \"title\": getattr(item, \"title\", \"\") or getattr(item, \"metadata\", {}).get(\"title\", \"\"),\n                            }\n                        )\n                return {\"data\": formatted_data}\n            else:\n                print(f\"Unexpected response format from Firecrawl: {type(response)}\")\n                return {\"data\": []}\n\n        except Exception as e:\n            print(f\"Error searching with Firecrawl: {e}\")\n            print(f\"Response type: {type(response) if 'response' in locals() else 'N/A'}\")\n            return {\"data\": []}\n\n\n# Initialize Firecrawl\nfirecrawl = Firecrawl(\n    api_key=os.environ.get(\"FIRECRAWL_KEY\", \"\"),\n    api_url=os.environ.get(\"FIRECRAWL_BASE_URL\"),\n)\n\n\nasync def generate_serp_queries(\n    query: str, num_queries: int = 3, learnings: Optional[List[str]] = None\n) -> List[SerpQuery]:\n    \"\"\"Generate SERP queries based on user input and previous learnings.\"\"\"\n\n    prompt = f\"\"\"Given the following prompt from the user, generate a list of SERP queries to research the topic. Return a JSON object with a 'queries' array field containing {num_queries} queries (or less if the original prompt is clear). Each query object should have 'query' and 'research_goal' fields. Make sure each query is unique and not similar to each other: <prompt>{query}</prompt>\"\"\"\n\n    example_response = \"\"\"\\n\n    IMPORTANT: MAKE SURE YOU FOLLOW THE EXAMPLE RESPONSE FORMAT AND ONLY THAT FORMAT WITH THE CORRECT QUERY AND RESEARCH GOAL.\n    {\n        \"queries\": [\n            {\n                \"query\": \"QUERY 1\",\n                \"research_goal\": \"RESEARCH GOAL 1\"\n            },\n            {\n                \"query\": \"QUERY 2\",\n                \"research_goal\": \"RESEARCH GOAL 2\"\n            },\n            {\n                \"query\": \"QUERY 3\",\n                \"research_goal\": \"RESEARCH GOAL 3\"\n            }\n        ]\n    }\n    \"\"\"\n    prompt += example_response\n    if learnings:\n        prompt += f\"\\n\\nHere are some learnings from previous research, use them to generate more specific queries: {' '.join(learnings)}\"\n\n    response = call_llm(\n        base_url=HEURIST_BASE_URL,\n        api_key=HEURIST_API_KEY,\n        model_id=LARGER_MODEL_ID,\n        system_prompt=system_prompt(),\n        user_prompt=prompt,\n        temperature=0.1,\n    )\n\n    if not response:\n        return \"Sorry, I couldn't process your message.\", None\n\n    else:\n        try:\n            result = json.loads(response)\n            queries = result.get(\"queries\", [])\n            return [SerpQuery(**q) for q in queries][:num_queries]\n        except json.JSONDecodeError as e:\n            print(f\"Error parsing JSON response: {e}\")\n            print(f\"Raw response: {response}\")\n            return []\n\n\nasync def process_serp_result(\n    query: str,\n    search_result: SearchResponse,\n    num_learnings: int = 3,\n    num_follow_up_questions: int = 3,\n) -> Dict[str, List[str]]:\n    \"\"\"Process search results to extract learnings and follow-up questions.\"\"\"\n    contents = [trim_prompt(item.get(\"markdown\", \"\"), 25_000) for item in search_result[\"data\"] if item.get(\"markdown\")]\n\n    contents_str = \"\".join(f\"<content>\\n{content}\\n</content>\" for content in contents)\n\n    prompt = (\n        f\"Given the following contents from a SERP search for the query <query>{query}</query>, \"\n        f\"generate a list of learnings from the contents. Return a JSON object with 'learnings' \"\n        f\"and 'followUpQuestions' arrays. Include up to {num_learnings} learnings and \"\n        f\"{num_follow_up_questions} follow-up questions. The learnings should be unique, \"\n        \"concise, and information-dense, including entities, metrics, numbers, and dates.\\n\\n\"\n        f\"IMPORTANT: MAKE SURE THE INFORMATION YOU USE IS FROM CONTENT AND NOT OTHER SOURCES. MAKE SURE IT IS ACTUALLY RELEVANT TO THE QUERY.\"\n        f\"IMPORTANT: DON'T MAKE ANY INFORMATION UP, IT MUST BE FROM THE CONTENT. ONLY USE THE CONTENT TO GENERATE THE LEARNINGS AND FOLLOW UP QUESTIONS.\"\n        f\"<contents>{contents_str}</contents>\"\n    )\n    example_response = \"\"\"\\n\n    IMPORTANT: MAKE SURE YOU FOLLOW THE EXAMPLE RESPONSE FORMAT AND ONLY THAT FORMAT WITH THE CORRECT LEARNINGS AND FOLLOW UP QUESTIONS.\n    {\n        \"learnings\": [\n            \"LEARNING 1\",\n            \"LEARNING 2\",\n            \"LEARNING 3\"\n        ],\n        \"followUpQuestions\": [\n            \"FOLLOW UP QUESTION 1\",\n            \"FOLLOW UP QUESTION 2\",\n            \"FOLLOW UP QUESTION 3\"\n        ]\n    }\n    \"\"\"\n    prompt += example_response\n    response = call_llm(\n        base_url=HEURIST_BASE_URL,\n        api_key=HEURIST_API_KEY,\n        model_id=LARGE_MODEL_ID,\n        system_prompt=system_prompt(),\n        user_prompt=prompt,\n        temperature=0.1,\n    )\n\n    if not response:\n        return \"Sorry, I couldn't process your message.\", None\n\n    else:\n        try:\n            result = json.loads(response)\n            return {\n                \"learnings\": result.get(\"learnings\", [])[:num_learnings],\n                \"followUpQuestions\": result.get(\"followUpQuestions\", [])[:num_follow_up_questions],\n            }\n        except json.JSONDecodeError as e:\n            print(f\"Error parsing JSON response: {e}\")\n            print(f\"Raw response: {response}\")\n            return {\"learnings\": [], \"followUpQuestions\": []}\n\n\nasync def write_final_report(prompt: str, learnings: List[str], visited_urls: List[str]) -> str:\n    \"\"\"Generate final report based on all research learnings.\"\"\"\n\n    print(\"learnings: \", learnings)\n    learnings_string = trim_prompt(\n        \"\\n\".join([f\"<learning>\\n{learning}\\n</learning>\" for learning in learnings]),\n        150_000,\n    )\n\n    user_prompt = (\n        f\"Given the following prompt from the user, write a final report on the topic using \"\n        f\"the learnings from research. Return a JSON object with a 'reportMarkdown' field \"\n        f\"containing a detailed markdown report (aim for 3+ pages). Include ALL the learnings \"\n        f\"from research:\\n\\n<prompt>{prompt}</prompt>\\n\\n\"\n        f\"Here are all the learnings from research:\\n\\n<learnings>\\n{learnings_string}\\n</learnings>\"\n    )\n\n    response = call_llm(\n        base_url=HEURIST_BASE_URL,\n        api_key=HEURIST_API_KEY,\n        model_id=LARGER_MODEL_ID,\n        system_prompt=system_prompt(),\n        user_prompt=user_prompt,\n        temperature=0.1,\n    )\n\n    if not response:\n        return \"Sorry, I couldn't process your message.\", None\n\n    else:\n        try:\n            result = json.loads(response)\n            report = result.get(\"reportMarkdown\", \"\")\n\n            # Append sources\n            urls_section = \"\\n\\n## Sources\\n\\n\" + \"\\n\".join([f\"- {url}\" for url in visited_urls])\n            return report + urls_section\n        except json.JSONDecodeError as e:\n            print(f\"Error parsing JSON response: {e}\")\n            print(f\"Raw response: {response}\")\n            return \"Error generating report\"\n\n\nasync def deep_research(\n    query: str,\n    breadth: int,\n    depth: int,\n    concurrency: int,\n    learnings: List[str] = None,\n    visited_urls: List[str] = None,\n) -> ResearchResult:\n    \"\"\"\n    Main research function that recursively explores a topic.\n\n    Args:\n        query: Research query/topic\n        breadth: Number of parallel searches to perform\n        depth: How many levels deep to research\n        learnings: Previous learnings to build upon\n        visited_urls: Previously visited URLs\n    \"\"\"\n    learnings = learnings or []\n    visited_urls = visited_urls or []\n\n    # Generate search queries\n    serp_queries = await generate_serp_queries(query=query, num_queries=breadth, learnings=learnings)\n\n    # Create a semaphore to limit concurrent requests\n    semaphore = asyncio.Semaphore(concurrency)\n\n    async def process_query(serp_query: SerpQuery) -> ResearchResult:\n        async with semaphore:\n            try:\n                # Search for content\n                result = await firecrawl.search(serp_query.query, timeout=15000, limit=5)\n\n                # Collect new URLs\n                new_urls = [item.get(\"url\") for item in result[\"data\"] if item.get(\"url\")]\n\n                # Calculate new breadth and depth for next iteration\n                new_breadth = max(1, breadth // 2)\n                new_depth = depth - 1\n\n                new_learnings = await process_serp_result(\n                    query=serp_query.query,\n                    search_result=result,\n                    num_follow_up_questions=new_breadth,\n                )\n\n                all_learnings = learnings + new_learnings[\"learnings\"]\n                all_urls = visited_urls + new_urls\n\n                # If we have more depth to go, continue research\n                if new_depth > 0:\n                    print(f\"Researching deeper, breadth: {new_breadth}, depth: {new_depth}\")\n\n                    next_query = f\"\"\"\n                    Previous research goal: {serp_query.research_goal}\n                    Follow-up research directions: {\" \".join(new_learnings[\"followUpQuestions\"])}\n                    \"\"\".strip()\n\n                    return await deep_research(\n                        query=next_query,\n                        breadth=new_breadth,\n                        depth=new_depth,\n                        concurrency=concurrency,\n                        learnings=all_learnings,\n                        visited_urls=all_urls,\n                    )\n\n                return {\"learnings\": all_learnings, \"visited_urls\": all_urls}\n\n            except Exception as e:\n                if \"Timeout\" in str(e):\n                    print(f\"Timeout error running query: {serp_query.query}: {e}\")\n                else:\n                    print(f\"Error running query: {serp_query.query}: {e}\")\n                return {\"learnings\": [], \"visited_urls\": []}\n\n    # Process all queries concurrently\n    results = await asyncio.gather(*[process_query(query) for query in serp_queries])\n\n    # Combine all results\n    all_learnings = list(set(learning for result in results for learning in result[\"learnings\"]))\n\n    all_urls = list(set(url for result in results for url in result[\"visited_urls\"]))\n\n    return {\"learnings\": all_learnings, \"visited_urls\": all_urls}\n"}
{"type": "source_file", "path": "agents/tool_decorator_example.py", "content": "import sys\nfrom pathlib import Path\n\nimport dotenv\n\ndotenv.load_dotenv()\n\n# Add project root to Python path\nproject_root = Path(__file__).parent.parent\nsys.path.append(str(project_root))\n\nfrom agents.tool_decorator import tool  # noqa: E402\n\n\n@tool(\"Add two integers together\")\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two integers.\"\"\"\n    result = a + b\n    return {\"result\": result}\n\n\n@tool(\"Multiply two integers together\")\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two integers.\"\"\"\n    result = a * b\n    return {\"result\": result}\n\n\n@tool(\"Filter messages based on content relevance\")\ndef filter_message(should_ignore: bool) -> bool:\n    \"\"\"Determine if a message should be ignored based on the following rules:\n    Return TRUE (ignore message) if:\n        - Message does not mention Heuman\n        - Message does not mention 'start raid'\n        - Message does not discuss: The Wired, Consciousness, Reality, Existence, Self, Philosophy, Technology, Crypto, AI, Machines\n        - For image requests: ignore if Heuman is not specifically mentioned\n\n    Return FALSE (process message) only if:\n        - Message explicitly mentions Heuman\n        - Message contains 'start raid'\n        - Message clearly discusses any of the listed topics\n        - Image request contains Heuman\n\n    If in doubt, return TRUE to ignore the message.\"\"\"\n    return should_ignore\n\n\n# List of available decorated tools\nDECORATED_TOOLS_EXAMPLES = [\n    add,\n    multiply,\n    # filter_message\n]\n"}
{"type": "source_file", "path": "agents/core_agent.py", "content": "import asyncio\nimport json\nimport logging\nimport os\nimport random\nimport threading\nfrom datetime import datetime\nfrom pathlib import Path\nfrom queue import Queue\nfrom typing import Any, Dict, List, Optional\n\nimport dotenv\n\nfrom agents.tools import Tools\nfrom agents.tools_mcp import Tools as ToolsMCP\nfrom core.config import PromptConfig\nfrom core.embedding import (\n    EmbeddingError,\n    MessageData,\n    MessageStore,\n    PostgresConfig,\n    PostgresVectorStorage,\n    SQLiteConfig,\n    SQLiteVectorStorage,\n    get_embedding,\n)\nfrom core.imgen import generate_image_with_retry_smartgen\nfrom core.llm import LLMError, call_llm, call_llm_with_tools\nfrom core.voice import speak_text, transcribe_audio\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n# os.reload_environ()\n# dotenv.load_dotenv(override=True)\n\n\nos.environ.clear()\ndotenv.load_dotenv(override=True)\nlogger.info(\"Environment variables reloaded\")\n# Constants\nHEURIST_BASE_URL = os.getenv(\"HEURIST_BASE_URL\")\nHEURIST_API_KEY = os.getenv(\"HEURIST_API_KEY\")\nLARGE_MODEL_ID = os.getenv(\"LARGE_MODEL_ID\")\nSMALL_MODEL_ID = os.getenv(\"SMALL_MODEL_ID\")\nTWEET_WORD_LIMITS = [15, 20, 30, 35]\nIMAGE_GENERATION_PROBABILITY = 0.3\nBASE_IMAGE_PROMPT = \"\"\n\n\nclass CoreAgent:\n    def __init__(self):\n        self.prompt_config = PromptConfig()\n        self.tools = Tools()\n        self.tools_mcp = ToolsMCP()\n        self.tools_mcp_initialized = False\n        self.interfaces = {}\n        self._message_queue = Queue()\n        self._lock = threading.Lock()\n        self.last_tweet_id = 0\n        self.last_raid_tweet_id = 0\n\n        # Use PostgreSQL if configured, otherwise default to SQLite\n        if all([os.getenv(env) for env in [\"VECTOR_DB_NAME\", \"VECTOR_DB_USER\", \"VECTOR_DB_PASSWORD\"]]):\n            vdb_config = PostgresConfig(\n                host=os.getenv(\"VECTOR_DB_HOST\", \"localhost\"),\n                port=int(os.getenv(\"VECTOR_DB_PORT\", 5432)),\n                database=os.getenv(\"VECTOR_DB_NAME\"),\n                user=os.getenv(\"VECTOR_DB_USER\"),\n                password=os.getenv(\"VECTOR_DB_PASSWORD\"),\n                table_name=os.getenv(\"VECTOR_DB_TABLE\", \"message_embeddings\"),\n            )\n            storage = PostgresVectorStorage(vdb_config)\n        else:\n            config = SQLiteConfig()\n            storage = SQLiteVectorStorage(config)\n\n        self.message_store = MessageStore(storage)\n\n    async def initialize(self, server_url: str = \"http://localhost:8000/sse\"):\n        await self.tools_mcp.initialize(server_url=server_url)\n        self.tools_mcp_initialized = True\n\n    def register_interface(self, name, interface):\n        with self._lock:\n            self.interfaces[name] = interface\n\n    def update_knowledge_base(self, json_file_path: str = \"data/data.json\") -> None:\n        \"\"\"\n        Updates the knowledge base by processing JSON data and storing embeddings.\n        Handles any JSON structure by treating each key-value pair as knowledge.\n\n        Args:\n            json_file_path: Path to the JSON file containing the data\n        \"\"\"\n        logger.info(f\"Updating knowledge base from {json_file_path}\")\n\n        try:\n            # Read JSON file\n            with open(json_file_path, \"r\") as f:\n                data = json.load(f)\n\n            # Handle both list and dict formats\n            items = data if isinstance(data, list) else [data]\n\n            # Process each item\n            for item in items:\n                if not isinstance(item, dict):\n                    continue\n\n                # Create message content by combining all key-value pairs\n                message_parts = []\n                for key, value in item.items():\n                    if isinstance(value, (str, int, float, bool)):\n                        message_parts.append(f\"{key}: {value}\")\n                    elif isinstance(value, (list, dict)):\n                        # Handle nested structures by converting to string\n                        message_parts.append(f\"{key}: {json.dumps(value)}\")\n\n                message = \"\\n\\n\".join(message_parts)\n\n                # Generate embedding for the message\n                message_embedding = get_embedding(message)\n\n                # Check if this exact message already exists\n                existing_entries = self.message_store.find_similar_messages(\n                    message_embedding,\n                    threshold=0.99,  # Very high threshold to match nearly identical content\n                )\n\n                if existing_entries:\n                    logger.info(\"Similar content already exists in knowledge base, skipping...\")\n                    continue\n\n                try:\n                    # Extract potential key topics from the first few keys\n                    key_topics = list(item.keys())[:3]  # Use first 3 keys as topics\n\n                    # Create MessageData object\n                    message_data = MessageData(\n                        message=message,\n                        embedding=message_embedding,\n                        timestamp=datetime.now().isoformat(),\n                        message_type=\"knowledge_base\",\n                        chat_id=None,\n                        source_interface=\"knowledge_base\",\n                        original_query=None,\n                        original_embedding=None,\n                        tool_call=None,\n                        response_type=\"FACTUAL\",\n                        key_topics=key_topics,\n                    )\n\n                    # Store in vector database\n                    self.message_store.add_message(message_data)\n                    logger.info(f\"Stored knowledge base entry with keys: {', '.join(key_topics)}\")\n\n                except EmbeddingError as e:\n                    logger.error(f\"Failed to generate embedding: {str(e)}\")\n                    continue\n\n            logger.info(\"Knowledge base update completed successfully\")\n\n        except FileNotFoundError:\n            logger.error(f\"JSON file not found: {json_file_path}\")\n        except json.JSONDecodeError:\n            logger.error(f\"Invalid JSON format in file: {json_file_path}\")\n        except Exception as e:\n            logger.error(f\"Error updating knowledge base: {str(e)}\")\n\n    def basic_personality_settings(self) -> str:\n        system_prompt = \"Use the following settings as part of your personality and voice if applicable in the conversation context: \"\n        basic_options = random.sample(self.prompt_config.get_basic_settings(), 2)\n        style_options = random.sample(self.prompt_config.get_interaction_styles(), 2)\n        system_prompt = system_prompt + \" \".join(basic_options) + \" \" + \" \".join(style_options)\n        return system_prompt\n\n    async def pre_validation(self, message: str) -> bool:\n        \"\"\"\n        Pre-validation of the message\n\n        Args:\n            message: The user's message\n\n        Returns:\n            True if the message is valid, False otherwise\n        \"\"\"\n        name = self.prompt_config.get_name()\n        filter_message_tool = [\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": \"filter_message\",\n                    \"description\": f\"\"\"Determine if a message should be ignored based on the following rules:\n                        Return TRUE (ignore message) if:\n                            - Message does not mention {name}\n                            - Message does not mention 'start raid'\n                            - Message does not discuss: The Wired, Consciousness, Reality, Existence, Self, Philosophy, Technology, Crypto, AI, Machines\n                            - For image requests: ignore if {name} is not specifically mentioned\n\n                        Return FALSE (process message) only if:\n                            - Message explicitly mentions {name}\n                            - Message contains 'start raid'\n                            - Message clearly discusses any of the listed topics\n                            - Image request contains {name}\n\n                        If in doubt, return TRUE to ignore the message.\"\"\",\n                    \"parameters\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"should_ignore\": {\n                                \"type\": \"boolean\",\n                                \"description\": \"TRUE to ignore message, FALSE to process message\",\n                            }\n                        },\n                        \"required\": [\"should_ignore\"],\n                    },\n                },\n            }\n        ]\n        try:\n            response = call_llm_with_tools(\n                HEURIST_BASE_URL,\n                HEURIST_API_KEY,\n                SMALL_MODEL_ID,\n                system_prompt=\"\",  # \"Always call the filter_message tool with the message as the argument\",#self.prompt_config.get_telegram_rules(),\n                user_prompt=message,\n                temperature=0.5,\n                tools=filter_message_tool,\n            )\n            print(response)\n            # response = response.lower()\n            # validation = False if \"false\" in response else True if \"true\" in response else False\n            validation = False\n            if \"tool_calls\" in response and response[\"tool_calls\"]:\n                tool_call = response[\"tool_calls\"]\n                args = json.loads(tool_call.function.arguments)\n                filter_result = str(args[\"should_ignore\"]).lower()\n                validation = False if filter_result == \"true\" else True\n            print(\"validation: \", validation)\n            return validation\n        except Exception as e:\n            logger.error(f\"Pre-validation failed: {str(e)}\")\n            return False\n\n    async def generate_image_prompt(self, message: str) -> str:\n        \"\"\"Generate an image prompt based on the tweet content\"\"\"\n        logger.info(\"Generating image prompt\")\n        prompt = self.prompt_config.get_template_image_prompt().format(tweet=message)\n        logger.info(\"Prompt: %s\", prompt)\n        try:\n            image_prompt = call_llm(\n                HEURIST_BASE_URL,\n                HEURIST_API_KEY,\n                SMALL_MODEL_ID,\n                system_prompt=self.prompt_config.get_system_prompt(),\n                user_prompt=prompt,\n                temperature=0.7,\n            )\n        except Exception as e:\n            logger.error(f\"Failed to generate image prompt: {str(e)}\")\n            return None\n        logger.info(\"Generated image prompt: %s\", image_prompt)\n        return image_prompt\n\n    async def handle_image_generation(self, prompt: str, base_prompt: str = \"\") -> Optional[str]:\n        \"\"\"\n        Handle image generation requests with retry logic\n\n        Args:\n            prompt: The image generation prompt\n            base_prompt: Optional base prompt to prepend\n\n        Returns:\n            Generated image URL or None if failed\n        \"\"\"\n        try:\n            full_prompt = base_prompt + prompt if base_prompt else prompt\n            # result = generate_image_with_retry(prompt=full_prompt)\n            # SMARTGEN\n            print(\"full_image_prompt: \", full_prompt)\n            result = await generate_image_with_retry_smartgen(prompt=full_prompt)\n            print(result)\n            return result\n        except Exception as e:\n            logger.error(f\"Image generation failed: {str(e)}\")\n            return None\n\n    async def transcribe_audio(self, audio_file_path: Path) -> str:\n        \"\"\"\n        Handle voice transcription requests\n\n        Args:\n            audio_file_path: Path to audio file\n\n        Returns:\n            Transcribed text\n        \"\"\"\n        try:\n            return transcribe_audio(audio_file_path)\n        except Exception as e:\n            logger.error(f\"Voice transcription failed: {str(e)}\")\n            raise\n\n    async def handle_text_to_speech(self, text: str) -> Path:\n        \"\"\"\n        Handle text-to-speech conversion\n\n        Args:\n            text: Text to convert to speech\n\n        Returns:\n            Path to generated audio file\n        \"\"\"\n        try:\n            return speak_text(text)\n        except Exception as e:\n            logger.error(f\"Text-to-speech conversion failed: {str(e)}\")\n            raise\n\n    async def handle_message(\n        self,\n        message: str,\n        message_type: str = \"user_message\",\n        source_interface: str = None,\n        chat_id: str = None,\n        system_prompt: str = None,\n        skip_embedding: bool = False,\n        skip_similar: bool = True,\n        skip_tools: bool = False,\n        skip_conversation_context: bool = True,\n        external_tools: List[str] = [],\n        max_tokens: int = None,\n        model_id: str = LARGE_MODEL_ID,\n        temperature: float = 0.4,\n        skip_pre_validation: bool = False,\n        tool_choice: str = \"auto\",\n    ):\n        \"\"\"\n        Handle message and optionally notify other interfaces.\n        Args:\n            message: The message to process\n            source_interface: Optional name of the interface that sent the message\n            chat_id: Optional chat ID for the conversation\n            skip_validation: Optional flag to skip pre-validation\n            skip_embedding: Optional flag to skip embedding\n            skip_tools: Optional flag to skip tools\n\n        Returns:\n            tuple: (text_response, image_url, tool_back)\n        \"\"\"\n        logger.info(f\"Handling message from {source_interface}\")\n        logger.info(f\"registered interfaces: {self.interfaces}\")\n\n        chat_id = str(chat_id)\n        self.current_message = message\n\n        system_prompt_context = \"\"\n        system_prompt_conversation_context = \"\"  # noqa: F841\n\n        if system_prompt is None:\n            system_prompt = self.basic_personality_settings()\n\n        system_prompt = self.prompt_config.get_system_prompt() + system_prompt\n\n        do_pre_validation = (\n            False\n            if source_interface\n            in [\"api\", \"twitter\", \"twitter_reply\", \"farcaster\", \"farcaster_reply\", \"telegram\", \"terminal\"]\n            else True\n        )\n        if not skip_pre_validation and do_pre_validation and not await self.pre_validation(message):\n            logger.debug(f\"Message failed pre-validation: {message[:100]}...\")\n            return None, None, None\n\n        try:\n            message_embedding = get_embedding(message)\n            logger.info(f\"Generated embedding for message: {message[:50]}...\")\n            system_prompt_context = self.get_knowledge_base(message, message_embedding)\n\n            if not skip_conversation_context:\n                system_prompt += self.get_conversation_context(chat_id)\n\n            if not skip_similar:\n                system_prompt_context += self.get_similar_messages(message, message_embedding, message_type, chat_id)\n\n            system_prompt += system_prompt_context\n\n            tools_config = self.tools.get_tools_config()\n            if self.tools_mcp_initialized:\n                tools_config += self.tools_mcp.get_tools_config()\n\n            if not skip_tools:\n                response = call_llm_with_tools(\n                    HEURIST_BASE_URL,\n                    HEURIST_API_KEY,\n                    model_id,\n                    system_prompt=system_prompt,\n                    user_prompt=message,\n                    temperature=temperature,\n                    max_tokens=max_tokens,\n                    tools=tools_config,\n                    tool_choice=tool_choice,\n                )\n            else:\n                response = call_llm(\n                    HEURIST_BASE_URL,\n                    HEURIST_API_KEY,\n                    model_id,\n                    system_prompt=system_prompt,\n                    user_prompt=message,\n                    temperature=temperature,\n                    max_tokens=max_tokens,\n                )\n            # Process response and handle tools\n            text_response = \"\"\n            image_url = None\n            tool_back = None\n            logger.info(\"response: \", response)\n            print(\"response: \", response)\n            if not response:\n                return \"Sorry, I couldn't process your message.\", None\n\n            if \"content\" in response and response[\"content\"]:  # Add null check\n                text_response = (\n                    response[\"content\"].strip('\"') if isinstance(response[\"content\"], str) else str(response[\"content\"])\n                )\n\n            # Handle tool calls\n\n            if \"tool_calls\" in response and response[\"tool_calls\"]:\n                tool_call = response[\"tool_calls\"]\n                args = json.loads(tool_call.function.arguments)\n                tool_name = tool_call.function.name\n                available_tools = [t[\"function\"][\"name\"] for t in self.tools.get_tools_config()]\n                mcp_tools = [t[\"function\"][\"name\"] for t in self.tools_mcp.get_tools_config()]\n                if tool_name in available_tools:\n                    logger.info(f\"Executing tool {tool_name} with args {args}\")\n                    tool_result = await self.tools.execute_tool(tool_name, args, self)\n                    if tool_result:\n                        print(\"tool_result: \", tool_result)\n                        if \"image_url\" in tool_result:\n                            image_url = tool_result[\"image_url\"]\n                        if \"result\" in tool_result:\n                            text_response += f\"\\n{tool_result['result']}\"\n                        if \"tool_call\" in tool_result:\n                            tool_back = tool_result[\"tool_call\"]\n                elif self.tools_mcp_initialized and tool_name in mcp_tools:\n                    logger.info(f\"Executing tool {tool_name} with args {args}\")\n                    tool_result = await self.tools_mcp.execute_tool(tool_name, args, self)\n                    if tool_result:\n                        print(\"tool_result: \", tool_result)\n                        if \"image_url\" in tool_result:\n                            image_url = tool_result[\"image_url\"]\n                        if \"result\" in tool_result:\n                            text_response += f\"\\n{tool_result['result']}\"\n                        if \"tool_call\" in tool_result:\n                            tool_back = tool_result[\"tool_call\"]\n                else:\n                    logger.info(f\"Tool {tool_name} not found in tools config\")\n                    tool_back = json.dumps(\n                        {\"tool_call\": tool_name, \"processed\": False, \"args\": args}, default=str\n                    )  # default=str handles any non-JSON serializable objects\n\n            if not skip_embedding:\n                # moved to post post processing as it is not relevant until finished processing\n                # Create MessageData for incoming message\n                message_data = MessageData(\n                    message=message,\n                    embedding=message_embedding,\n                    timestamp=datetime.now().isoformat(),\n                    message_type=message_type,\n                    chat_id=chat_id,\n                    source_interface=source_interface,\n                    original_query=None,\n                    original_embedding=None,\n                    response_type=None,\n                    key_topics=None,\n                    tool_call=None,\n                )\n\n                # Store the incoming message\n                self.message_store.add_message(message_data)\n                logger.info(\"Stored message and embedding in database\")\n                # Create and store MessageData for the response\n                response_data = MessageData(\n                    message=text_response,\n                    embedding=get_embedding(text_response),\n                    timestamp=datetime.now().isoformat(),\n                    message_type=\"agent_response\",\n                    chat_id=chat_id,\n                    source_interface=source_interface,\n                    original_query=message,\n                    original_embedding=message_embedding,\n                    response_type=await self._classify_response_type(text_response),\n                    key_topics=await self._extract_key_topics(text_response),\n                    tool_call=tool_back,\n                )\n\n                # Store the response\n                self.message_store.add_message(response_data)\n\n            # Notify other interfaces if needed\n            # if source_interface and chat_id:\n            #     for interface_name, interface in self.interfaces.items():\n            #         if interface_name != source_interface:\n            #             await self.send_to_interface(interface_name, {\n            #                 'type': 'message',\n            #                 'content': text_response,\n            #                 'image_url': image_url,\n            #                 'source': source_interface,\n            #                 'chat_id': chat_id\n            #             })\n\n            return text_response, image_url, tool_back\n\n        except LLMError as e:\n            logger.error(f\"LLM processing failed: {str(e)}\")\n            return \"Sorry, I encountered an error processing your message.\", None, None\n        except Exception as e:\n            logger.error(f\"Message handling failed: {str(e)}\")\n            return \"Sorry, something went wrong.\", None, None\n\n    async def agent_cot(\n        self,\n        message: str,\n        user: str = \"User\",\n        display_name: str = None,\n        chat_id: str = \"General\",\n        source_interface: str = \"None\",\n        final_format_prompt: str = \"\",\n        skip_conversation_context: bool = False,\n    ) -> str:\n        message_info = (message,)\n        username = user or \"Unknown\"\n        display_name = display_name or username\n        message_data = message\n        chat_id = chat_id\n        message_info = f\"User: {display_name}, Username: {username}, \\nMessage: {message_data}\"\n        image_url_final = None\n\n        steps_responses = []\n        prompt_final = \"\"\n        text_response = \"\"\n        try:\n            print(\"USING COT\")\n            prompt = f\"\"\"<SYSTEM_PROMPT> I want you to give analyze the question {message_info}.\n                    IMPORTANT: DON'T USE TOOLS RIGHT NOW. ANALYZE AND Give me a list of steps with the tools you'd use in each step, if the step is not a specific tool you have to use, just put the tool name as \"None\".\n                    The most important thing to tell me is what different calls you'd do or processes as a list. Your answer should be a valid JSON and ONLY the JSON.\n                    Make sure you analyze what outputs from previous steps you'd need to use in the next step if applicable.\n                    IMPORTANT: RETURN THE JSON ONLY.\n                    IMPORTANT: DO NOT USE TOOLS.\n                    IMPORTANT: ONLY USE VALID TOOLS.\n                    IMPORTANT: WHEN STEPS DEPEND ON EACH OTHER, MAKE SURE YOU ANALYZE THE INPUTS SO YOU KNOW WHAT TO PASS TO THE NEXT TOOL CALL. IF NEEDED TAKE A STEP TO MAKE SURE YOU KNOW WHAT TO PASS TO THE NEXT TOOL CALL AND FORMAT THE INPUTS CORRECTLY.\n                    IMPORTANT: FOR NEXT TOOL CALLS MAKE SURE YOU ANALYZE THE INPUTS SO YOU KNOW WHAT TO PASS TO THE NEXT TOOL CALL. IF NEEDED TAKE A STEP TO MAKE SURE YOU KNOW WHAT TO PASS TO THE NEXT TOOL CALL AND FORMAT THE INPUTS CORRECTLY.\n                    IMPORTANT: MAKE SURE YOU RETURN THE JSON ONLY, NO OTHER TEXT OR MARKUP AND A VALID JSON.\n                    DONT ADD ANY COMMENTS OR MARKUP TO THE JSON. Example NO # or /* */ or /* */ or // or any other comments or markup.\n                    \"\"\"\n\n            prompt += \"\"\"\n                    EXAMPLE:\n                    [\n                        {\n                            \"step\": \"Step one of the process thought for the question\",\n                            \"tool\": \"tool to call\",\n                            \"parameters\": {\n                                \"arg1\": \"value1\",\n                                \"arg2\": \"value2\"\n                            }\n                        },\n                        {\n                            \"step\": \"Step two of the process thought for the question\",\n                            \"tool\": \"tool to call\",\n                            \"parameters\": {\n                                \"arg1\": \"value1\",\n                                \"arg2\": \"value2\"\n                            }\n                        }\n                    ]\n                    </SYSTEM_PROMPT>\"\"\"\n            text_response, _, _ = await self.handle_message(\n                message=message_info,\n                system_prompt=prompt,\n                source_interface=source_interface,\n                chat_id=chat_id,\n                skip_pre_validation=True,\n                skip_conversation_context=skip_conversation_context,\n                skip_similar=True,\n                temperature=0.1,\n                skip_tools=False,\n            )\n\n            json_response = json.loads(text_response)\n            print(\"json_response: \", json_response)\n            thinking_text = \"Thinking...\\n\\n\"\n            for step in json_response:\n                if step[\"step\"] != \"None\":\n                    thinking_text = f\"Step: {step['step']}\\n\"\n                    print(\"\\nthinking_text: \", thinking_text)\n\n            for step in json_response:\n                print(\"step: \", step)\n                system_prompt = f\"\"\"CONTEXT: YOU ARE RUNNING STEPS FOR THE ORIGINAL QUESTION: {message_data}.\n                PREVIOUS STEP RESPONSES: {steps_responses}\"\"\"\n                skip_tools = False\n                skip_conversation_context = True\n                if step[\"tool\"] == \"None\":\n                    skip_tools = True\n                    skip_conversation_context = False\n\n                text_response, image_url, tool_calls = await self.handle_message(\n                    system_prompt=system_prompt,\n                    message=str(step),\n                    message_type=\"REASONING_STEP\",\n                    source_interface=source_interface,\n                    skip_conversation_context=skip_conversation_context,\n                    skip_embedding=True,\n                    skip_pre_validation=True,\n                    skip_tools=skip_tools,\n                    tool_choice=\"required\" if not skip_tools else None,\n                )\n                retries = 5\n                while retries > 0:\n                    if \"<function\" in text_response or (not tool_calls and step[\"tool\"] != \"None\"):\n                        print(\"Found function in text_response or failed to call tool\")\n                        text_response, image_url, tool_calls = await self.handle_message(\n                            system_prompt=text_response,\n                            message=str(text_response),\n                            message_type=\"REASONING_STEP\",\n                            source_interface=source_interface,\n                            skip_conversation_context=True,\n                            skip_similar=True,\n                            skip_embedding=True,\n                            skip_pre_validation=True,\n                            skip_tools=False,\n                            tool_choice=\"required\",\n                        )\n                        retries -= 1\n                        await asyncio.sleep(5)\n                    else:\n                        break\n                step_response = {\"step\": step, \"response\": text_response}\n                print(\"image_url: \", image_url)\n                if image_url:\n                    image_url_final = image_url\n                steps_responses.append(step_response)\n\n            print(\"steps_responses: \", steps_responses)\n            print(\"image_url_final: \", image_url_final)\n            prompt_final = f\"\"\"\n                            ONLY USE THE REASONING CONTEXT IF YOU NEED TO.\n                                <REASONING_CONTEXT>\n                                    <STEPS_RESPONSES>\n                                        {steps_responses}\n                                    </STEPS_RESPONSES>\n                                </REASONING_CONTEXT>\"\"\"\n        except Exception as e:\n            logger.error(f\"Error processing reply: {str(e)}\")\n            text_response, image_url, _ = await self.handle_message(\n                message_info,\n                source_interface=source_interface,\n                chat_id=chat_id,\n                skip_conversation_context=skip_conversation_context,\n                skip_pre_validation=True,\n            )\n        try:\n            message_final = f\"User: {display_name}, Username: {username}, \\nMessage: {message_data}\"  # noqa: F841\n            final_reasoning_prompt = f\"\"\"Generate the final response for the user.\n            Given the context of your reasoning, and the steps you've taken, generate a final response for the user.\n            Your final reasoning is: {text_response}\n            You already have the final reasoning, just generate the final response for the user, don't do more steps or request more information.\n            you are responding to the user message: {message_data}\"\"\"\n            if final_format_prompt:\n                prompt_final = final_format_prompt + final_reasoning_prompt + prompt_final\n            else:\n                prompt_final = self.basic_personality_settings() + final_reasoning_prompt + prompt_final\n            response, _, _ = await self.handle_message(\n                message=final_reasoning_prompt,\n                system_prompt=prompt_final,\n                source_interface=source_interface,\n                chat_id=chat_id,\n                skip_embedding=False,\n                skip_conversation_context=skip_conversation_context,\n                skip_pre_validation=True,\n                skip_tools=True,  # skip tools as tools should have been called already\n            )\n            return response, image_url_final, None\n        except Exception as e:\n            logger.error(f\"Error processing reply: {str(e)}\")\n            return None, None\n\n    def get_knowledge_base(self, message: str, message_embedding: List[float]) -> str:\n        \"\"\"\n        Get knowledge base data from the message embedding\n        \"\"\"\n        if message_embedding is None:\n            message_embedding = get_embedding(message)\n        system_prompt_context = \"\"\n        knowledge_base_data = self.message_store.find_similar_messages(\n            message_embedding, threshold=0.6, message_type=\"knowledge_base\"\n        )\n        logger.info(f\"Found {len(knowledge_base_data)} relevant items from knowledge base\")\n        if knowledge_base_data:\n            system_prompt_context = \"\\n\\nConsider the Following As Facts and use them to answer the question if applicable and relevant:\\nKnowledge base data:\\n\"\n            for data in knowledge_base_data:\n                system_prompt_context += f\"{data['message']}\\n\"\n        return system_prompt_context\n\n    def get_conversation_context(self, chat_id: str) -> str:\n        \"\"\"\n        Get conversation context from the chat ID\n        \"\"\"\n        if chat_id is None:\n            return \"\"\n        system_prompt_conversation_context = \"\\n\\nPrevious conversation history (in chronological order):\\n\"\n        # Get last 10 messages (will be in DESC order)\n        conversation_messages = self.message_store.find_messages(\n            message_type=\"agent_response\", chat_id=chat_id, limit=10\n        )\n\n        # Sort by timestamp and reverse to get chronological order\n        conversation_messages.sort(key=lambda x: x[\"timestamp\"], reverse=True)\n        conversation_messages.reverse()\n\n        # Build conversation history\n        for msg in conversation_messages:\n            if msg.get(\"original_query\"):  # Ensure we have both question and answer\n                system_prompt_conversation_context += f\"User: {msg['original_query']}\\n\"\n                system_prompt_conversation_context += f\"Assistant: {msg['message']}\\n\\n\"\n        # print(\"system_prompt_conversation_context: \", system_prompt_conversation_context)\n        return system_prompt_conversation_context\n\n    def get_similar_messages(\n        self, message: str, message_embedding: List[float], message_type: str = None, chat_id: str = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Get similar messages from the message embedding\n        \"\"\"\n        if message_embedding is None:\n            message_embedding = get_embedding(message)\n        similar_messages = self.message_store.find_similar_messages(\n            message_embedding, threshold=0.9, message_type=message_type, chat_id=chat_id\n        )\n        logger.info(f\"Found {len(similar_messages)} similar messages\")\n        if similar_messages:\n            context = \"\\n\\nRelated previous conversations and responses\\nNOTE: Please provide a response that differs from these recent replies, don't use the same words:\\n\"\n            seen_responses = set()  # Track unique responses\n            message_count = 0\n            for similar_msg in similar_messages:\n                # Find the agent's response where this similar message was the original_query\n                agent_responses = self.message_store.find_messages(\n                    message_type=\"agent_response\", original_query=similar_msg[\"message\"]\n                )\n\n                for response in agent_responses:\n                    if response[\"message\"] in seen_responses:\n                        continue\n                    seen_responses.add(response[\"message\"])\n                    context += f\"\"\"\n                        Previous similar question: {similar_msg[\"message\"]}\n                        My response: {response[\"message\"]}\n                        Similarity score: {similar_msg.get(\"similarity\", 0):.2f}\n                        \"\"\"\n                    message_count += 1\n                    if message_count >= 10:  # Check limit after adding each message\n                        break\n            context += \"\\nConsider the above responses for context, but provide a fresh perspective that adds value to the conversation, don't repeat the same responses.\\n\"\n            return context\n        else:\n            return \"\"\n\n    logger.info(\"Added context from similar conversations\")\n\n    async def _classify_response_type(self, response: str) -> str:\n        \"\"\"Classify the type of response (factual, opinion, question, etc.)\"\"\"\n        classify_prompt = {\n            \"role\": \"system\",\n            \"content\": \"Classify this response as one of: FACTUAL, OPINION, QUESTION, EMOTIONAL, ACTION. Response:\",\n        }\n        try:\n            classification = await call_llm(\n                HEURIST_BASE_URL,\n                HEURIST_API_KEY,\n                SMALL_MODEL_ID,  # Use smaller model for classification\n                system_prompt=classify_prompt[\"content\"],\n                user_prompt=response,\n                temperature=0.3,\n            )\n            return classification.strip().upper()\n        except Exception:\n            return \"general\"\n\n    async def _extract_key_topics(self, text: str) -> List[str]:\n        \"\"\"Extract key topics from the response for better similarity matching\"\"\"\n        topic_prompt = {\n            \"role\": \"system\",\n            \"content\": \"Extract 2-3 main topics from this text as comma-separated keywords:\",\n        }\n        try:\n            topics = await call_llm(\n                HEURIST_BASE_URL,\n                HEURIST_API_KEY,\n                SMALL_MODEL_ID,\n                system_prompt=topic_prompt[\"content\"],\n                user_prompt=text,\n                temperature=0.3,\n            )\n            return [t.strip() for t in topics.split(\",\")]\n        except Exception:\n            return []\n\n    async def send_to_interface(self, target_interface: str, message: dict):\n        \"\"\"\n        Send a message to a specific interface\n\n        Args:\n            target_interface (str): Name of the interface to send to\n            message (dict): Message data containing at minimum:\n                {\n                    'type': str,  # Type of message (e.g., 'message', 'image', 'voice')\n                    'content': str,  # Main content\n                    'image_url': Optional[str],  # Optional image URL\n                    'source': Optional[str]  # Source interface name\n                }\n\n        Returns:\n            bool: True if message was queued successfully, False otherwise\n        \"\"\"\n        try:\n            with self._lock:\n                if target_interface not in self.interfaces:\n                    logger.error(f\"Interface {target_interface} not registered\")\n                    return False\n\n                # Validate message format\n                if not isinstance(message, dict) or \"type\" not in message or \"content\" not in message:\n                    logger.error(\"Invalid message format\")\n                    return False\n\n                # Add timestamp and target\n                message[\"timestamp\"] = datetime.now().isoformat()\n                message[\"target\"] = target_interface\n\n                # Queue the message\n                self._message_queue.put(message)\n\n                # Get interface instance\n                interface = self.interfaces[target_interface]\n                logger.info(f\"Interface: {interface}\")\n                logger.info(f\"Message: {message}\")\n                logger.info(\"trying to send message\")\n                # Handle different message types\n                logger.info(f\"Message type: {message['type']}\")\n                if message[\"type\"] == \"message\":\n                    logger.info(f\"interface has method {hasattr(interface, 'send_message')}\")\n                    if hasattr(interface, \"send_message\"):\n                        try:\n                            logger.info(f\"Attempting to send message via {target_interface} interface\")\n                            await interface.send_message(\n                                chat_id=message[\"chat_id\"], message=message[\"content\"], image_url=message[\"image_url\"]\n                            )\n                            logger.info(\"Message sent successfully\")\n                        except Exception as e:\n                            logger.error(f\"Failed to send message via {target_interface}: {str(e)}\")\n                            raise\n                # Log successful queue\n                logger.info(f\"Message queued for {target_interface}: {message['type']}\")\n                return True\n\n        except Exception as e:\n            logger.error(f\"Error sending message to {target_interface}: {str(e)}\")\n            return False\n"}
{"type": "source_file", "path": "main_twitter.py", "content": "import logging\n\nimport dotenv\n\nfrom interfaces.twitter_post import TwitterAgent\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    \"\"\"\n    Main entry point for the Heuman Agent Framework.\n    Runs the Twitter agent for automated tweeting.\n    \"\"\"\n    try:\n        # Load environment variables\n        dotenv.load_dotenv()\n\n        # Initialize and run Twitter agent\n        logger.info(\"Starting Twitter agent...\")\n        agent = TwitterAgent()\n        agent.run()\n\n    except KeyboardInterrupt:\n        logger.info(\"Application stopped by user\")\n    except Exception as e:\n        logger.error(f\"Fatal error: {str(e)}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "interfaces/farcaster_post.py", "content": "import asyncio\nimport json\nimport logging\nimport os\nimport random\nfrom datetime import datetime, timedelta\nfrom typing import Any, Dict, Optional, Tuple\n\nimport dotenv\nimport requests\n\nfrom agents.core_agent import CoreAgent\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nos.environ.clear()\ndotenv.load_dotenv(override=True)\nlogger.info(\"Environment variables reloaded\")\n\n# Constants\nHEURIST_BASE_URL = \"https://llm-gateway.heurist.xyz/v1\"\nHEURIST_API_KEY = os.getenv(\"HEURIST_API_KEY\")\nFARCASTER_API_KEY = os.getenv(\"FARCASTER_API_KEY\")\nFARCASTER_SIGNER_UUID = os.getenv(\"FARCASTER_SIGNER_UUID\")\nLARGE_MODEL_ID = os.getenv(\"LARGE_MODEL_ID\")\nSMALL_MODEL_ID = os.getenv(\"SMALL_MODEL_ID\")\nCAST_WORD_LIMITS = [15, 20, 30, 35]\nIMAGE_GENERATION_PROBABILITY = 1\nCAST_HISTORY_FILE = \"cast_history.json\"\n\n# DRYRUN = False if os.getenv(\"DRYRUN\") == \"False\" else True\nDRYRUN = False\nif DRYRUN:\n    print(\"DRYRUN MODE: Not posting real casts\")\nelse:\n    print(\"LIVE MODE: Will post real casts\")\n\n\nclass FarcasterAPI:\n    def __init__(self, api_key: str, signer_uuid: str):\n        self.api_key = api_key\n        self.signer_uuid = signer_uuid\n        self.base_url = \"https://api.neynar.com/v2/farcaster\"\n        self.headers = {\"api_key\": self.api_key, \"Content-Type\": \"application/json\"}\n\n    def post_cast(self, message: str, image_url: Optional[str] = None) -> Tuple[Optional[str], Optional[str]]:\n        \"\"\"Post a cast to Farcaster, optionally with an image\"\"\"\n        try:\n            endpoint = f\"{self.base_url}/cast\"\n\n            data = {\n                \"signer_uuid\": self.signer_uuid,\n                \"text\": message,\n            }\n\n            if image_url:\n                data[\"embeds\"] = [{\"url\": image_url}]\n\n            response = requests.post(endpoint, headers=self.headers, json=data)\n\n            if response.status_code == 200:\n                result = response.json()\n                cast_hash = result.get(\"cast\", {}).get(\"hash\")\n                username = result.get(\"cast\", {}).get(\"author\", {}).get(\"username\")\n                logger.info(f\"Successfully posted cast with hash: {cast_hash}\")\n                return cast_hash, username\n            else:\n                logger.error(f\"Failed to post cast. Status: {response.status_code}, Response: {response.text}\")\n                return None, None\n\n        except Exception as e:\n            logger.error(f\"Error posting cast: {str(e)}\")\n            return None, None\n\n\nclass CastHistoryManager:\n    def __init__(self, history_file: str = CAST_HISTORY_FILE):\n        self.history_file = history_file\n        self.history = self.load_history()\n\n    def load_history(self) -> list:\n        if os.path.exists(self.history_file):\n            try:\n                with open(self.history_file, \"r\", encoding=\"utf-8\") as f:\n                    return json.load(f)\n            except json.JSONDecodeError:\n                logger.warning(f\"Error reading {self.history_file}, starting fresh\")\n                return []\n        return []\n\n    def add_cast(self, cast: str, metadata: Optional[Dict] = None) -> None:\n        entry = {\"timestamp\": datetime.now().isoformat(), \"cast\": cast}\n        if metadata:\n            entry.update(metadata)\n\n        entry = json.loads(json.dumps(entry, ensure_ascii=False))\n        self.history.append(entry)\n        self.save_history()\n\n    def save_history(self) -> None:\n        with open(self.history_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.history, f, ensure_ascii=False, indent=2)\n\n    def get_recent_casts(self, n: int = 6) -> list:\n        return [entry[\"cast\"][\"cast\"] for entry in self.history[-n:]]\n\n\nclass FarcasterAgent(CoreAgent):\n    def __init__(self, core_agent=None):\n        if core_agent:\n            super().__setattr__(\"_parent\", core_agent)\n        else:\n            super().__setattr__(\"_parent\", self)\n            super().__init__()\n\n        # Initialize Farcaster specific components\n        self.history_manager = CastHistoryManager()\n        self.farcaster_api = FarcasterAPI(FARCASTER_API_KEY, FARCASTER_SIGNER_UUID)\n        self.register_interface(\"farcaster\", self)\n        self.last_cast_hash = None\n\n    def __getattr__(self, name):\n        return getattr(self._parent, name)\n\n    def __setattr__(self, name, value):\n        if not hasattr(self, \"_parent\"):\n            super().__setattr__(name, value)\n        elif name == \"_parent\" or self is self._parent or name in self.__dict__:\n            super().__setattr__(name, value)\n        else:\n            setattr(self._parent, name, value)\n\n    def fill_basic_prompt(self, basic_options, style_options):\n        return self.prompt_config.get_basic_prompt_template().format(\n            basic_option_1=basic_options[0],\n            basic_option_2=basic_options[1],\n            style_option_1=style_options[0],\n            style_option_2=style_options[1],\n        )\n\n    def format_cast_instruction(self, basic_options, style_options, ideas=None):\n        decoration_ideas = f\"Ideas: {ideas}\" if ideas else \"\\n\"\n        num_words = random.choice(CAST_WORD_LIMITS)\n\n        return self.prompt_config.get_tweet_instruction_template().format(\n            basic_option_1=basic_options[0],\n            basic_option_2=basic_options[1],\n            style_option_1=style_options[0],\n            style_option_2=style_options[1],\n            decoration_ideas=decoration_ideas,\n            num_words=num_words,\n            rules=self.prompt_config.get_farcaster_rules(),\n        )\n\n    def format_context(self, casts):\n        \"\"\"Format the context template with past casts.\n\n        Args:\n            casts: List of previous casts\n\n        Returns:\n            Formatted context string\n        \"\"\"\n        if casts is None:\n            casts = []\n\n        # Convert list of casts to a formatted string\n        cast_text = \"\\n\".join([f\"- {cast}\" for cast in casts]) if casts else \"\"\n\n        return self.prompt_config.get_context_farcaster_template().format(\n            casts=cast_text\n        )  # Changed 'tweets' to 'casts'\n\n    async def generate_cast(self) -> Tuple[Optional[str], Optional[str], Optional[Dict]]:\n        \"\"\"Generate a cast with improved error handling\"\"\"\n        cast_data: Dict[str, Any] = {\"metadata\": {}}\n\n        try:\n            # Get recent casts for context\n            past_casts = self.history_manager.get_recent_casts()\n\n            # Generate randomized prompt\n            basic_options = random.sample(self.prompt_config.get_basic_settings(), 2)\n            style_options = random.sample(self.prompt_config.get_interaction_styles(), 2)\n            cast_data[\"metadata\"].update({\"basic_options\": basic_options, \"style_options\": style_options})\n\n            prompt = self.fill_basic_prompt(basic_options, style_options)\n\n            # Generate ideas using get_tweet_ideas() since cast-specific method doesn't exist yet\n            instruction_cast_idea = random.choice(self.prompt_config.get_tweet_ideas())\n            system_prompt = prompt + self.prompt_config.get_farcaster_rules() + self.format_context(past_casts)\n\n            cast_data[\"metadata\"][\"ideas_instruction\"] = instruction_cast_idea\n            ideas_text, _, _ = await self.handle_message(\n                instruction_cast_idea, system_prompt_fixed=system_prompt, source_interface=\"farcaster\"\n            )\n            cast_data[\"metadata\"][\"ideas\"] = ideas_text\n\n            if not ideas_text:\n                return None, None, None\n\n            # Generate final cast\n            system_prompt = (\n                prompt\n                + self.prompt_config.get_farcaster_rules()\n                + self.format_context(past_casts)\n                + self.format_cast_instruction(basic_options, style_options, ideas_text)\n            )\n\n            cast, _, _ = await self.handle_message(\n                ideas_text, system_prompt_fixed=system_prompt, source_interface=\"farcaster\"\n            )\n\n            if not cast:\n                return None, None, None\n\n            cast = cast.replace('\"', \"\")\n            cast_data[\"cast\"] = cast\n\n            # Image generation\n            image_url = None\n            if random.random() < IMAGE_GENERATION_PROBABILITY:\n                try:\n                    image_prompt = await self.generate_image_prompt(cast)\n                    image_url = await self.handle_image_generation(image_prompt)\n                    cast_data[\"metadata\"][\"image_prompt\"] = image_prompt\n                    cast_data[\"metadata\"][\"image_url\"] = image_url\n                except Exception as e:\n                    logger.warning(f\"Failed to generate image: {str(e)}\")\n\n            return cast, image_url, cast_data\n\n        except Exception as e:\n            logger.error(f\"Unexpected error in cast generation: {str(e)}\")\n            return None, None, None\n\n    async def _run(self):\n        while True:\n            try:\n                cast_result = await self.generate_cast()\n                logger.info(\"Cast result: %s\", cast_result)\n\n                cast, image_url, cast_data = cast_result\n\n                if cast:\n                    if not DRYRUN:\n                        if image_url:\n                            cast_hash, username = self.farcaster_api.post_cast(cast, image_url)\n                            logger.info(\"Successfully posted cast with image: %s\", cast)\n                            cast_url = f\"https://warpcast.com/{username}/0x{cast_hash}\"\n                        else:\n                            cast_hash, username = self.farcaster_api.post_cast(cast)\n                            logger.info(\"Successfully posted cast: %s\", cast)\n                            cast_url = f\"https://warpcast.com/{username}/0x{cast_hash}\"\n\n                        if cast_hash:\n                            cast_data[\"metadata\"][\"cast_hash\"] = cast_hash\n                            cast_data[\"metadata\"][\"cast_url\"] = cast_url\n                            self.last_cast_hash = cast_hash\n\n                            # Notify other interfaces\n                            for interface_name, interface in self.interfaces.items():\n                                if interface_name == \"telegram\":\n                                    telegram_chat_id = os.getenv(\"TELEGRAM_CHAT_ID\")\n                                    if telegram_chat_id:\n                                        await self.send_to_interface(\n                                            interface_name,\n                                            {\n                                                \"type\": \"message\",\n                                                \"content\": f\"Just posted a cast: {cast_url}\",\n                                                \"image_url\": None,\n                                                \"source\": \"farcaster\",\n                                                \"chat_id\": telegram_chat_id,\n                                            },\n                                        )\n                    else:\n                        logger.info(\"Generated cast: %s\", cast)\n\n                    self.history_manager.add_cast(cast_data)\n                    wait_time = random_interval()\n                else:\n                    logger.error(\"Failed to generate cast\")\n                    wait_time = 10\n\n                next_time = datetime.now() + timedelta(seconds=wait_time)\n                logger.info(\"Next cast will be posted at: %s\", next_time.strftime(\"%H:%M:%S\"))\n                await asyncio.sleep(wait_time)\n\n            except Exception as e:\n                logger.error(\"Error occurred: %s\", str(e))\n                await asyncio.sleep(10)\n                continue\n\n    def run(self):\n        \"\"\"Start the Farcaster bot\"\"\"\n        logger.info(\"Starting Farcaster bot...\")\n        asyncio.run(self._run())\n\n\ndef random_interval():\n    \"\"\"Generate a random interval between 1 and 2 hours in seconds\"\"\"\n    return random.uniform(3600, 7200)\n\n\ndef main():\n    agent = FarcasterAgent()\n    agent.run()\n\n\nif __name__ == \"__main__\":\n    try:\n        logger.info(\"Starting Farcaster agent...\")\n        main()\n    except KeyboardInterrupt:\n        logger.info(\"\\nFarcaster agent stopped by user\")\n    except Exception as e:\n        logger.error(\"Fatal error: %s\", str(e))\n"}
{"type": "source_file", "path": "agents/tools.py", "content": "import json\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional\n\nfrom .tool_box import ToolBox\nfrom .tool_decorator import get_tool_schemas\nfrom .tool_decorator_example import DECORATED_TOOLS_EXAMPLES\n\nlogger = logging.getLogger(__name__)\n\n\nclass Tools(ToolBox):\n    def __init__(self):\n        # Initialize the base class\n        super().__init__()\n\n        # Store decorated tools\n        # Not necessary to as it is already stored in the ToolBox class\n        # But explicitly storing it here for clarity\n        self._decorated_tools: List[Callable] = self.decorated_tools\n\n        # Register the decorated tools\n        self.register_decorated_tools(DECORATED_TOOLS_EXAMPLES + self._decorated_tools)\n\n    def register_decorated_tool(self, tool_func: Callable) -> None:\n        \"\"\"Register a decorated tool function\"\"\"\n        if hasattr(tool_func, \"name\") and hasattr(tool_func, \"args_schema\"):\n            self._decorated_tools.append(tool_func)\n            self.tool_handlers[tool_func.name] = tool_func\n        else:\n            logger.warning(f\"Tool {tool_func.__name__} is not properly decorated\")\n\n    def register_decorated_tools(self, tools: List[Callable]) -> None:\n        \"\"\"Register multiple decorated tools at once\"\"\"\n        for tool in tools:\n            self.register_decorated_tool(tool)\n\n    def get_tools_config(self, filter_tools: List[str] = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Get tool configurations, optionally filtered by tool names\n\n        Args:\n            filter_tools: Optional list of tool names to include\n\n        Returns:\n            List of tool configurations\n        \"\"\"\n        all_tools = self.tools_config + get_tool_schemas(self._decorated_tools)\n\n        if filter_tools:\n            return [tool for tool in all_tools if tool[\"function\"][\"name\"] in filter_tools]\n        return all_tools\n\n    async def execute_tool(self, tool_name: str, args: Dict[str, Any], agent_context: Any) -> Optional[Dict[str, Any]]:\n        \"\"\"Execute a tool by name with given arguments\"\"\"\n        if tool_name not in self.tool_handlers:\n            logger.error(f\"Unknown tool: {tool_name}\")\n            return None\n        result = await self.tool_handlers[tool_name](args, agent_context)\n        result[\"tool_call\"] = json.dumps(\n            {\n                \"tool_call\": tool_name,\n                \"processed\": True,\n                \"args\": args,\n                \"result\": result[\"result\"] if \"result\" in result else None,\n                \"data\": result[\"data\"] if \"data\" in result else None,\n            },\n            default=str,\n        )\n        return result\n"}
{"type": "source_file", "path": "clients/base_client.py", "content": "import logging\nfrom typing import Any, Optional\n\nimport aiohttp\nimport requests\nfrom requests.exceptions import RequestException\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseAPIClient:\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n        self.timeout = 10\n        self.session = requests.Session()\n        self.async_session: Optional[aiohttp.ClientSession] = None\n\n    def _sync_request(self, method: str, endpoint: str, **kwargs) -> Any:\n        \"\"\"Simple synchronous request\"\"\"\n        if \"timeout\" not in kwargs:\n            kwargs[\"timeout\"] = self.timeout\n\n        try:\n            response = self.session.request(method, f\"{self.base_url}{endpoint}\", **kwargs)\n            response.raise_for_status()\n            return response.json()\n        except RequestException as e:\n            logger.error(f\"API request failed: {e}\")\n            raise\n\n    async def _async_request(self, method: str, endpoint: str, **kwargs) -> Any:\n        \"\"\"Async request\"\"\"\n        if not self.async_session:\n            self.async_session = aiohttp.ClientSession()\n\n        if \"timeout\" not in kwargs:\n            kwargs[\"timeout\"] = self.timeout\n\n        try:\n            async with getattr(self.async_session, method.lower())(f\"{self.base_url}{endpoint}\", **kwargs) as response:\n                response.raise_for_status()\n                return await response.json()\n        except aiohttp.ClientError as e:\n            logger.error(f\"Async API request failed: {e}\")\n            raise\n\n    async def close(self):\n        if self.async_session:\n            await self.async_session.close()\n            self.async_session = None\n\n    def __del__(self):\n        self.session.close()\n"}
{"type": "source_file", "path": "clients/__init__.py", "content": ""}
{"type": "source_file", "path": "core/heurist_image/__init__.py", "content": "from .ImageGen import APIError, ImageGen\nfrom .SmartGen import PromptEnhancementError, SmartGen\n\n__all__ = [\"ImageGen\", \"SmartGen\", \"APIError\", \"PromptEnhancementError\"]\n"}
{"type": "source_file", "path": "clients/merkl_client.py", "content": "import logging\nfrom typing import Dict, List, Optional\n\nfrom .base_client import BaseAPIClient\n\nlogger = logging.getLogger(__name__)\n\n\nclass MerklClient(BaseAPIClient):\n    \"\"\"Merkl API implementation for accessing DeFi opportunities and rewards data\"\"\"\n\n    def __init__(self):\n        super().__init__(\"https://api.merkl.xyz/v4\")\n\n    # sync methods\n    def get_opportunities(\n        self,\n        name: Optional[str] = None,\n        chainId: Optional[str] = None,\n        action: Optional[str] = None,\n        tags: Optional[List[str]] = None,\n        test: Optional[bool] = None,\n        minimumTvl: Optional[float] = None,\n        status: Optional[str] = None,\n        tokens: Optional[List[str]] = None,\n        sort: Optional[str] = None,\n        order: Optional[str] = None,\n        mainProtocolId: Optional[str] = None,\n        page: Optional[int] = None,\n        items: Optional[int] = None,\n    ) -> Dict:\n        \"\"\"\n        Get list of DeFi opportunities with optional filters\n        \"\"\"\n        params = {k: v for k, v in locals().items() if v is not None and k != \"self\"}\n\n        return self._sync_request(\"get\", \"/opportunities/\", params=params)\n\n    def get_opportunity_detail(self, opportunity_id: str) -> Dict:\n        \"\"\"Get detailed information about a specific opportunity\"\"\"\n        return self._sync_request(\"get\", f\"/opportunities/{opportunity_id}\")\n\n    def get_campaigns(\n        self,\n        chain_id: Optional[str] = None,\n        token_address: Optional[str] = None,\n        test: bool = False,\n        opportunity_id: Optional[str] = None,\n        start_timestamp: Optional[int] = None,\n        end_timestamp: Optional[int] = None,\n        page: int = 0,\n        items: int = 20,\n    ) -> Dict:\n        \"\"\"Get list of reward campaigns with optional filters\"\"\"\n        params = {k: v for k, v in locals().items() if v is not None and k != \"self\"}\n\n        return self._sync_request(\"get\", \"/campaigns/\", params=params)\n\n    def get_protocols(\n        self,\n        protocol_id: Optional[str] = None,\n        tags: Optional[List[str]] = None,\n        opportunity_tag: Optional[str] = None,\n        page: int = 0,\n        items: int = 20,\n    ) -> Dict:\n        \"\"\"Get list of protocols with optional filters\"\"\"\n        params = {k: v for k, v in locals().items() if v is not None and k != \"self\"}\n\n        return self._sync_request(\"get\", \"/protocols/\", params=params)\n\n    def get_user_rewards(\n        self, address: str, chain_id: str, reload_chain_id: Optional[str] = None, test: bool = False\n    ) -> Dict:\n        \"\"\"Get rewards for a specific user address\"\"\"\n        params = {k: v for k, v in locals().items() if v is not None and k != \"self\" and k != \"address\"}\n\n        return self._sync_request(\"get\", f\"/users/{address}/rewards\", params=params)\n\n    def get_chains(self, name: Optional[str] = None) -> List[Dict]:\n        \"\"\"Get list of supported blockchains\"\"\"\n        params = {\"name\": name} if name else None\n        return self._sync_request(\"get\", \"/chains/\", params=params)\n\n    # async methods\n    async def get_opportunities_async(\n        self,\n        name: Optional[str] = None,\n        chain_id: Optional[str] = None,\n        action: Optional[str] = None,\n        tags: Optional[List[str]] = None,\n        test: Optional[bool] = None,\n        minimum_tvl: Optional[float] = None,\n        status: Optional[str] = None,\n        tokens: Optional[List[str]] = None,\n        sort: Optional[str] = None,\n        order: Optional[str] = None,\n        main_protocol_id: Optional[str] = None,\n        page: Optional[int] = None,\n        items: Optional[int] = None,\n    ) -> Dict:\n        \"\"\"\n        Get list of DeFi opportunities with optional filters\n\n        Args:\n            name: Filter by opportunity name\n            chain_id: Filter by blockchain ID\n            action: Filter by action type (POOL, HOLD, DROP, LEND, BORROW)\n            tags: Filter by tags\n            test: Include test opportunities\n            minimum_tvl: Minimum TVL threshold\n            status: Filter by status (LIVE, PAST, SOON)\n            tokens: Filter by token addresses\n            sort: Sort field (apr, tvl, rewards)\n            order: Sort order (asc, desc)\n            main_protocol_id: Filter by main protocol ID\n            page: Page number for pagination\n            items: Items per page\n        \"\"\"\n        params = {k: v for k, v in locals().items() if v is not None and k != \"self\"}\n\n        return await self._async_request(\"get\", \"/opportunities/\", params=params)\n\n    async def get_opportunity_detail_async(self, opportunity_id: str) -> Dict:\n        \"\"\"Get detailed information about a specific opportunity\"\"\"\n        return await self._async_request(\"get\", f\"/opportunities/{opportunity_id}\")\n\n    async def get_campaigns_async(\n        self,\n        chain_id: Optional[str] = None,\n        token_address: Optional[str] = None,\n        test: bool = False,\n        opportunity_id: Optional[str] = None,\n        start_timestamp: Optional[int] = None,\n        end_timestamp: Optional[int] = None,\n        page: int = 0,\n        items: int = 20,\n    ) -> Dict:\n        \"\"\"Get list of reward campaigns with optional filters\"\"\"\n        params = {k: v for k, v in locals().items() if v is not None and k != \"self\"}\n\n        return await self._async_request(\"get\", \"/campaigns/\", params=params)\n\n    async def get_protocols_async(\n        self,\n        protocol_id: Optional[str] = None,\n        tags: Optional[List[str]] = None,\n        opportunity_tag: Optional[str] = None,\n        page: int = 0,\n        items: int = 20,\n    ) -> Dict:\n        \"\"\"Get list of protocols with optional filters\"\"\"\n        params = {k: v for k, v in locals().items() if v is not None and k != \"self\"}\n\n        return await self._async_request(\"get\", \"/protocols/\", params=params)\n\n    async def get_user_rewards_async(\n        self, address: str, chain_id: str, reload_chain_id: Optional[str] = None, test: bool = False\n    ) -> Dict:\n        \"\"\"Get rewards for a specific user address\"\"\"\n        params = {k: v for k, v in locals().items() if v is not None and k != \"self\" and k != \"address\"}\n\n        return await self._async_request(\"get\", f\"/users/{address}/rewards\", params=params)\n\n    async def get_chains_async(self, name: Optional[str] = None) -> List[Dict]:\n        \"\"\"Get list of supported blockchains\"\"\"\n        params = {\"name\": name} if name else None\n        return await self._async_request(\"get\", \"/chains/\", params=params)\n"}
{"type": "source_file", "path": "interfaces/twitter_reply.py", "content": "\"\"\"\nAgent flow:\n1. Monitor finds tweets\n2. Tweets go into JSON file\n3. Workers pull from queue\n4. CoreAgent processes messages\n5. Results automatically route back through send_message\n6. Queue tracks completion\n\"\"\"\n\nimport asyncio\nimport json\nimport logging\nimport os\nimport threading\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport dotenv\nimport requests\n\nimport platforms.twitter_api as twitter_api\nfrom agents.core_agent import CoreAgent\nfrom core.config import PromptConfig\nfrom utils.llm_utils import should_ignore_message\nfrom utils.text_utils import strip_tweet_text\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\ndotenv.load_dotenv()\n\n# Constants\nHEURIST_BASE_URL = os.getenv(\"HEURIST_BASE_URL\")\nHEURIST_API_KEY = os.getenv(\"HEURIST_API_KEY\")\nLARGE_MODEL_ID = os.getenv(\"LARGE_MODEL_ID\")\nSMALL_MODEL_ID = os.getenv(\"SMALL_MODEL_ID\")\nSELF_TWITTER_NAME = os.getenv(\"SELF_TWITTER_NAME\")\nDRYRUN = os.getenv(\"DRYRUN\")\n\nRATE_LIMIT_SLEEP = 120\nTAGGING_CHECK_INTERVAL = 1800\n\nif DRYRUN:\n    print(\"DRYRUN MODE: Not posting real tweets\")\nelse:\n    print(\"LIVE MODE: Will post real tweets\")\n\nprompt_config = PromptConfig()\n\n\nclass QueueManager:\n    def __init__(self, file_path=\"reply_history.json\"):\n        self.file_path = Path(file_path)\n        self._ensure_file_exists()\n\n    def _ensure_file_exists(self):\n        \"\"\"Create file with initial structure if it doesn't exist\"\"\"\n        if not self.file_path.exists():\n            self.write_data({\"processed_replies\": [], \"pending_replies\": [], \"processing_replies\": {}})\n\n    def read_data(self) -> Dict:\n        \"\"\"Read current data from file\"\"\"\n        try:\n            with self.file_path.open(\"r\") as f:\n                return json.load(f)\n        except Exception as e:\n            logger.error(f\"Error reading reply history: {str(e)}\")\n            return {\"processed_replies\": [], \"pending_replies\": []}\n\n    def write_data(self, data: Dict):\n        \"\"\"Write data to file\"\"\"\n        try:\n            with self.file_path.open(\"w\") as f:\n                json.dump(data, f, indent=2)\n        except Exception as e:\n            logger.error(f\"Error writing reply history: {str(e)}\")\n\n    def add_reply(self, reply_data: dict):\n        \"\"\"Add new reply to queue\"\"\"\n        logger.debug(f\"Adding new reply to queue: {reply_data['tweet_id']}\")\n        data = self.read_data()\n        # Use tweet_id from the API response\n        reply_data[\"message_id\"] = reply_data[\"tweet_id\"]  # Store tweet_id as message_id\n        data[\"pending_replies\"].append(reply_data)  # Append to maintain order\n        self.write_data(data)\n\n    def pop_pending_reply(self):\n        \"\"\"Get next pending reply and mark as processing\"\"\"\n        data = self.read_data()\n\n        # Find first pending reply that's not being processed\n        for reply in data[\"pending_replies\"]:\n            tweet_id = reply[\"tweet_id\"]\n            if tweet_id not in data.get(\"processing_replies\", {}):\n                logger.debug(f\"Found unprocessed reply {tweet_id}, marking as processing\")\n                data[\"processing_replies\"][tweet_id] = {\"data\": reply, \"started_at\": datetime.now().isoformat()}\n                self.write_data(data)\n\n                return {\"message_id\": tweet_id, \"data\": json.dumps(reply)}\n\n        logger.debug(\"No pending replies found\")\n        return None\n\n    def mark_as_done(self, message_id, response_data: dict):\n        \"\"\"Move from processing to processed\"\"\"\n        logger.debug(f\"Marking reply {message_id} as done\")\n        data = self.read_data()\n\n        # Remove from pending and processing\n        data[\"pending_replies\"] = [r for r in data[\"pending_replies\"] if r[\"tweet_id\"] != message_id]\n        data[\"processing_replies\"].pop(message_id, None)\n\n        # Add to processed\n        data[\"processed_replies\"].append(response_data)\n        self.write_data(data)\n\n    def get_all_tweet_ids(self) -> set:\n        \"\"\"Get set of all tweet IDs from both pending and processed replies\"\"\"\n        data = self.read_data()\n        processed_ids = {reply[\"tweet_id\"] for reply in data[\"processed_replies\"]}\n        pending_ids = {reply[\"tweet_id\"] for reply in data[\"pending_replies\"]}\n        return processed_ids | pending_ids\n\n    def get_pending_tweet_ids(self) -> set:\n        \"\"\"Get set of all tweet IDs from pending replies\"\"\"\n        data = self.read_data()\n        pending_ids = {reply[\"tweet_id\"] for reply in data[\"pending_replies\"]}\n        return pending_ids\n\n    def get_processed_tweet_ids(self) -> set:\n        \"\"\"Get set of all tweet IDs from processed replies\"\"\"\n        data = self.read_data()\n        processed_ids = {reply[\"tweet_id\"] for reply in data[\"processed_replies\"]}\n        return processed_ids\n\n\nclass TwitterSearchMonitor:\n    def __init__(self, api_key: str, queue_manager: QueueManager):\n        self.api_key = api_key\n        self.queue_manager = queue_manager\n        self.base_url = \"https://api.apidance.pro/sapi/Search\"\n        self.search_terms = []  # Initialize empty list\n\n    def set_search_terms(self, terms: list):\n        \"\"\"Set the search terms to monitor\"\"\"\n        self.search_terms = terms\n\n    def fetch_tweets(self, cursor: str = \"\") -> Dict:\n        \"\"\"Fetch tweets matching configured search terms\"\"\"\n        # Join terms with OR for the search query\n        query = \" OR \".join(self.search_terms)\n        params = {\"q\": query, \"cursor\": cursor}\n        headers = {\"apikey\": self.api_key}\n\n        try:\n            response = requests.get(self.base_url, headers=headers, params=params)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Request failed: {e}\")\n            return {\"tweets\": [], \"next_cursor_str\": None}\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to parse JSON response: {e}\")\n            return {\"tweets\": [], \"next_cursor_str\": None}\n\n    def filter_tweets(self, tweets: List[Dict]) -> List[Dict]:\n        \"\"\"Filter tweets based on criteria. Return the tweets that are selected for processing\"\"\"\n        # Get all processed tweet IDs from queues\n        processed_tweets = self.queue_manager.get_all_tweet_ids()\n\n        filtered_tweets = []\n        for tweet in tweets:\n            if SELF_TWITTER_NAME == tweet[\"user\"][\"name\"]:\n                continue\n\n            # Skip if tweet is from the author itself\n            if tweet.get(\"is_self_send\"):\n                continue\n\n            # Skip if we've already processed this tweet\n            if tweet[\"tweet_id\"] in processed_tweets:\n                continue\n\n            # Check if tweet contains any of the search terms\n            if not any(term.lower() in tweet[\"text\"].lower() for term in self.search_terms):\n                continue\n\n            # filter out nested replies that >= 3 replies deep\n            # tweet['text'] looks like \"@user1 @user2 @user3 contents...\" we should count the number of @userX at the beginning of the string\n            words = tweet[\"text\"].split()\n            at_count = 0\n            for word in words:\n                if word.startswith(\"@\"):\n                    at_count += 1\n                else:\n                    break\n            if at_count >= 3:\n                continue\n\n            # strip the tweet text of URLs and @ mentions\n            cleaned_text = strip_tweet_text(tweet[\"text\"])\n            if len(cleaned_text) < 10:\n                continue\n\n            # check if the tweet should be ignored\n            if should_ignore_message(\n                base_url=HEURIST_BASE_URL,\n                api_key=HEURIST_API_KEY,\n                model_id=SMALL_MODEL_ID,\n                criteria=prompt_config.get_social_reply_filter(),\n                message=cleaned_text,\n                temperature=0.0,\n            ):\n                logger.info(f\"Ignoring tweet {cleaned_text} because it matches the ignore criteria\")\n                continue\n\n            filtered_tweets.append(tweet)\n\n        return filtered_tweets\n\n    def process_mentions(self) -> List[Dict]:\n        \"\"\"Main function to process mentions and queue new tweets\"\"\"\n        logger.info(\"Fetching tweets...\")\n\n        cursor = \"\"\n        api_calls = 0\n        all_candidates = []\n\n        while api_calls < 5:\n            api_calls += 1\n            response_data = self.fetch_tweets(cursor)\n\n            if not response_data.get(\"tweets\"):\n                logger.info(\"No tweets found in response\")\n                break\n\n            logger.debug(f\"Fetched {len(response_data['tweets'])} tweets\")\n            logger.debug(f\"Response data: {response_data}\")\n\n            candidate_tweets = self.filter_tweets(response_data[\"tweets\"])\n\n            # Queue new candidates\n            for tweet in candidate_tweets:\n                related_tweet_id = tweet.get(\"related_tweet_id\", None)\n                related_tweet = None\n                if related_tweet_id:\n                    related_tweet = twitter_api.get_tweet_text(related_tweet_id)\n                self.queue_manager.add_reply(\n                    {\n                        \"tweet_id\": tweet[\"tweet_id\"],\n                        \"content\": tweet[\"text\"],\n                        \"author_name\": tweet[\"user\"][\"name\"],\n                        \"related_tweet_id\": related_tweet_id,\n                        \"related_tweet_text\": related_tweet,\n                    }\n                )\n\n            all_candidates.extend(candidate_tweets)\n\n            # Stop if we found at least one candidate\n            if candidate_tweets:\n                break\n\n            # Get next cursor for pagination\n            cursor = response_data.get(\"next_cursor_str\")\n            if not cursor:\n                logger.info(\"No more pages to fetch\")\n                break\n\n            # sleep for 5 seconds to avoid rate limiting\n            time.sleep(5)\n\n        return all_candidates\n\n\nclass TwitterReplyAgent(CoreAgent):\n    def __init__(self, core_agent=None):\n        if core_agent:\n            super().__setattr__(\"_parent\", core_agent)\n        else:\n            super().__setattr__(\"_parent\", self)\n            super().__init__()\n\n        self.queue_manager = QueueManager()\n        self.monitor = TwitterSearchMonitor(\n            api_key=os.getenv(\"TWITTER_SEARCH_API_KEY\"), queue_manager=self.queue_manager\n        )\n        self.register_interface(\"twitter_reply\", self)\n        self.set_search_terms([\"@heurist_ai\"])  # Set default search term\n\n    async def send_message(self, chat_id: str, message: str, image_url: str = None):\n        \"\"\"Interface method called by CoreAgent's send_to_interface. chat_id is the tweet_id\"\"\"\n        logger.debug(f\"send_message {chat_id} {message} {image_url}\")\n        if not DRYRUN:\n            if image_url:\n                twitter_api.reply_with_image(message, image_url, chat_id)\n            else:\n                twitter_api.reply(message, chat_id)\n        else:\n            print(f\"DRYRUN MODE: Would have replied to {chat_id} with {message} and image {image_url}\")\n            return\n\n    async def process_reply(self, message_data):\n        \"\"\"Process single reply using CoreAgent's handle_message\"\"\"\n        logger.debug(f\"Processing reply for tweet {message_data['tweet_id']}\")\n        try:\n            social_reply_template = prompt_config.get_social_reply_template()\n            context = None\n            if message_data[\"related_tweet_text\"]:\n                context = \"Related tweet: \" + message_data[\"related_tweet_text\"]\n            message = social_reply_template.format(\n                user_name=message_data[\"author_name\"],\n                social_platform=\"Twitter\",\n                user_message=message_data[\"content\"],\n                context=context,\n            )\n            response, image_url, _ = await self.handle_message(\n                message=message,\n                source_interface=\"twitter_reply\",\n                chat_id=message_data[\"tweet_id\"],\n                skip_embedding=True,\n                skip_tools=True,\n            )\n\n            # send the response to the original tweet\n            await self.send_message(message_data[\"tweet_id\"], response, image_url)\n\n            # CoreAgent will automatically call send_to_interface to other registered interfaces\n            return response, image_url\n\n        except Exception as e:\n            logger.error(f\"Error processing reply: {str(e)}\")\n            return None, None\n\n    async def run_workers(self, num_workers: int = 3):\n        \"\"\"Run multiple reply workers\"\"\"\n\n        async def worker():\n            while True:\n                try:\n                    message = self.queue_manager.pop_pending_reply()\n                    if not message:\n                        await asyncio.sleep(RATE_LIMIT_SLEEP)\n                        continue\n\n                    reply_data = json.loads(message[\"data\"])\n                    response, image_url = await self.process_reply(reply_data)\n\n                    response_data = reply_data.copy()\n                    response_data[\"response\"] = response\n                    if image_url:\n                        response_data[\"image_url\"] = image_url\n\n                    self.queue_manager.mark_as_done(message[\"message_id\"], response_data)\n                    await asyncio.sleep(RATE_LIMIT_SLEEP)\n\n                except Exception as e:\n                    logger.error(f\"Worker error: {str(e)}\")\n                    await asyncio.sleep(RATE_LIMIT_SLEEP)\n\n        workers = [worker() for _ in range(num_workers)]\n        await asyncio.gather(*workers)\n\n    def start_monitoring(self):\n        \"\"\"Start tag monitoring in background thread\"\"\"\n\n        def process_new_tags():\n            while True:\n                self.monitor.process_mentions()  # this handles queueing\n                time.sleep(TAGGING_CHECK_INTERVAL)\n\n        monitor_thread = threading.Thread(target=process_new_tags, daemon=True)\n        monitor_thread.start()\n        return monitor_thread\n\n    async def start(self):\n        \"\"\"Main entry point to start the agent\"\"\"\n        self.start_monitoring()\n\n        try:\n            await self.run_workers()\n        except KeyboardInterrupt:\n            logger.info(\"Shutting down...\")\n\n    def set_search_terms(self, terms: list):\n        \"\"\"Configure which terms to monitor for\"\"\"\n        self.monitor.set_search_terms(terms)\n"}
{"type": "source_file", "path": "clients/mesh_client.py", "content": "# clients/mesh_client.py\nimport asyncio\nfrom typing import Any, Dict, Optional\n\nfrom loguru import logger\n\nfrom .base_client import BaseAPIClient\n\n\nclass MeshClient(BaseAPIClient):\n    \"\"\"Client for invoking other agents through Protocol V2 Server\"\"\"\n\n    async def create_task(self, agent_id: str, task_details: Dict[str, Any], api_key: str) -> Dict[str, Any]:\n        \"\"\"Create a task for another agent with proper task ID propagation\"\"\"\n        task_details_copy = task_details.copy()\n        origin_task_id = task_details_copy.get(\"origin_task_id\")\n\n        payload = {\n            \"agent_id\": agent_id,\n            \"agent_type\": \"AGENT\",\n            \"task_details\": task_details_copy,\n            \"api_key\": api_key,\n        }\n\n        if origin_task_id:\n            payload[\"origin_task_id\"] = origin_task_id\n            task_details_copy[\"origin_task_id\"] = origin_task_id\n\n        try:\n            response = await self._async_request(method=\"post\", endpoint=\"/mesh_task_create\", json=payload)\n            logger.info(f\"Task created | Agent: {agent_id} | Task ID: {response.get('task_id')}\")\n            return response\n        except Exception as e:\n            logger.error(f\"Task creation failed | Agent: {agent_id} | Error: {str(e)}\")\n            raise\n\n    async def poll_result(\n        self, task_id: str, max_retries: int = 30, retry_delay: float = 1.0\n    ) -> Optional[Dict[str, Any]]:\n        \"\"\"Poll for task result and reasoning steps\"\"\"\n        seen_steps = set()\n        logger.debug(f\"Starting poll | Task: {task_id}\")\n\n        for attempt in range(max_retries):\n            try:\n                response = await self._async_request(\n                    method=\"post\", endpoint=\"/mesh_task_query\", json={\"task_id\": task_id}\n                )\n\n                if not response:\n                    logger.warning(f\"Empty response | Task: {task_id} | Attempt: {attempt + 1}\")\n                    await asyncio.sleep(retry_delay)\n                    continue\n\n                # Handle reasoning steps\n                reasoning_steps = response.get(\"reasoning_steps\", []) or []\n                for step in reasoning_steps:\n                    step_content = step.get(\"content\", \"\")\n                    if step_content and step_content not in seen_steps:\n                        logger.info(f\"Reasoning step | Task: {task_id} | Content: {step_content}\")\n                        seen_steps.add(step_content)\n\n                # Handle status\n                status = response.get(\"status\")\n                if status == \"finished\":\n                    return response.get(\"result\")\n                elif status in [\"failed\", \"canceled\"]:\n                    logger.error(f\"Task {status} | Task: {task_id} | Message: {response.get('message', '')}\")\n                    return response\n\n                await asyncio.sleep(retry_delay)\n\n            except Exception as e:\n                logger.error(f\"Poll error | Task: {task_id} | Error: {str(e)}\")\n                await asyncio.sleep(retry_delay)\n\n        logger.error(f\"Poll timeout | Task: {task_id} | Max retries: {max_retries}\")\n        return None\n\n    def push_update(self, task_id: str, content: str):\n        \"\"\"Push an update for a running task\"\"\"\n        try:\n            self._sync_request(\n                method=\"post\", endpoint=\"/mesh_task_update\", json={\"task_id\": task_id, \"content\": content}\n            )\n            logger.debug(f\"Update pushed | Task: {task_id} | Content: {content}\")\n\n        except Exception as e:\n            logger.error(f\"Update failed | Task: {task_id} | Error: {str(e)}\")\n\n    async def mesh_request(\n        self, agent_id: str, input_data: Dict[str, Any], api_key: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Make a direct request to an agent\"\"\"\n        payload = {\"agent_id\": agent_id, \"input\": input_data}\n\n        if api_key:\n            payload[\"heurist_api_key\"] = api_key\n\n        logger.debug(f\"Direct request | Agent: {agent_id}\")\n        try:\n            response = await self._async_request(method=\"post\", endpoint=\"/mesh_request\", json=payload)\n            logger.info(f\"Direct request completed | Agent: {agent_id}\")\n            return response\n\n        except Exception as e:\n            logger.error(f\"Direct request failed | Agent: {agent_id} | Error: {str(e)}\")\n            raise\n"}
{"type": "source_file", "path": "core/videogen.py", "content": "import asyncio\nimport random\nimport string\nimport time\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Any, Dict, Optional, Tuple\n\nimport requests\n\n\nclass WorkflowTaskType(str, Enum):\n    # UPSCALER = \"upscaler\"\n    # FLUX_LORA = \"flux-lora\"\n    TEXT2VIDEO = \"txt2vid\"\n\n\ndef parse_api_key_string(combined_key: str) -> Tuple[str, str]:\n    \"\"\"Split the combined API key into consumer ID and API key.\"\"\"\n    parts = combined_key.split(\"#\")\n    return parts[0] if parts else \"\", parts[1] if len(parts) > 1 else \"\"\n\n\n@dataclass\nclass WorkflowTaskResult:\n    task_id: str\n    status: str  # 'waiting' | 'running' | 'finished' | 'failed' | 'canceled'\n    result: Optional[Any] = None\n\n\nclass WorkflowTask(ABC):\n    def __init__(\n        self,\n        consumer_id: Optional[str] = None,\n        job_id_prefix: Optional[str] = None,\n        timeout_seconds: Optional[int] = None,\n        workflow_id: Optional[str] = None,\n        api_key: Optional[str] = None,\n    ):\n        self.consumer_id = consumer_id\n        self.job_id_prefix = job_id_prefix\n        self.timeout_seconds = timeout_seconds\n        self.workflow_id = workflow_id\n        self.api_key = api_key\n\n    @property\n    @abstractmethod\n    def task_type(self) -> WorkflowTaskType:\n        pass\n\n    @property\n    @abstractmethod\n    def task_details(self) -> Dict[str, Any]:\n        pass\n\n\nclass Text2VideoTask(WorkflowTask):\n    def __init__(\n        self,\n        prompt: str,\n        width: Optional[int] = None,\n        height: Optional[int] = None,\n        steps: Optional[int] = None,\n        length: Optional[int] = None,\n        seed: Optional[int] = None,\n        fps: Optional[int] = None,\n        quality: Optional[int] = None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.prompt = prompt\n        self.width = width\n        self.height = height\n        self.steps = steps\n        self.length = length\n        self.seed = seed\n        self.fps = fps\n        self.quality = quality\n\n    @property\n    def task_type(self) -> WorkflowTaskType:\n        return WorkflowTaskType.TEXT2VIDEO\n\n    @property\n    def task_details(self) -> Dict[str, Any]:\n        parameters = {\"prompt\": self.prompt}\n\n        optional_params = {\n            \"width\": self.width,\n            \"height\": self.height,\n            \"steps\": self.steps,\n            \"length\": self.length,\n            \"seed\": self.seed,\n            \"fps\": self.fps,\n            \"quality\": self.quality,\n        }\n\n        parameters.update({k: v for k, v in optional_params.items() if v is not None})\n        return {\"parameters\": parameters}\n\n\nclass Workflow:\n    def __init__(self, api_key: str, workflow_url: str):\n        self.workflow_url = workflow_url\n        self.default_consumer_id, self.default_api_key = parse_api_key_string(api_key)\n\n    def _make_request(self, endpoint: str, data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Make an HTTP request to the workflow API.\"\"\"\n        url = f\"{self.workflow_url}/{endpoint}\"\n        response = requests.post(url, json=data)\n\n        if not response.ok:\n            try:\n                error_data = response.json()\n                error_message = error_data.get(\"error\") or error_data.get(\"message\") or \"Unknown error\"\n            except ValueError:\n                error_message = response.text or \"Unknown error\"\n            raise Exception(error_message)\n\n        return response.json()\n\n    async def resource_request(self, consumer_id: str, workflow_id: Optional[str] = None) -> str:\n        \"\"\"Request resources for a workflow.\"\"\"\n        data = {\"consumer_id\": consumer_id, \"workflow_id\": workflow_id}\n        result = self._make_request(\"resource_request\", data)\n        return result[\"miner_id\"]\n\n    def _generate_random_id(self, length: int = 10) -> str:\n        \"\"\"Generate a random hexadecimal ID.\"\"\"\n        return \"\".join(random.choices(string.hexdigits.lower(), k=length))\n\n    async def create_task(self, task: WorkflowTask) -> str:\n        \"\"\"Create a new workflow task.\"\"\"\n        task_id = self._generate_random_id()\n        data = {\n            \"consumer_id\": task.consumer_id or self.default_consumer_id,\n            \"api_key\": task.api_key or self.default_api_key,\n            \"task_type\": task.task_type,\n            \"task_details\": task.task_details,\n            \"job_id\": f\"{task.job_id_prefix or 'sdk-workflow'}-{task_id}\",\n            \"workflow_id\": task.workflow_id,\n        }\n\n        if task.timeout_seconds:\n            data[\"timeout_seconds\"] = task.timeout_seconds\n\n        result = self._make_request(\"task_create\", data)\n        return result[\"task_id\"]\n\n    async def execute_workflow(self, task: WorkflowTask) -> str:\n        \"\"\"Execute a workflow task.\"\"\"\n        return await self.create_task(task)\n\n    async def query_task_result(self, task_id: str) -> WorkflowTaskResult:\n        \"\"\"Query the result of a task.\"\"\"\n        result = self._make_request(\"task_result_query\", {\"task_id\": task_id})\n        return WorkflowTaskResult(**result)\n\n    async def execute_workflow_and_wait_for_result(\n        self, task: WorkflowTask, timeout: int = 600000, interval: int = 10000, initial_wait: int = 120000\n    ) -> WorkflowTaskResult:\n        \"\"\"Execute a workflow task and wait for its result.\n\n        Args:\n            task: The workflow task to execute\n            timeout: Maximum time to wait in milliseconds\n            interval: Time between status checks in milliseconds\n            initial_wait: Time to wait before first status check in milliseconds\n        \"\"\"\n        if interval < 1000:\n            raise ValueError(\"Interval should be more than 1000 (1 second)\")\n\n        task_id = await self.execute_workflow(task)\n        start_time = time.time() * 1000  # Convert to milliseconds\n\n        # Initial wait before first query\n        await asyncio.sleep(initial_wait / 1000)  # Convert to seconds\n\n        while True:\n            result = await self.query_task_result(task_id)\n            if result.status in (\"finished\", \"failed\"):\n                return result\n\n            if (time.time() * 1000) - start_time > timeout:\n                raise TimeoutError(\"Timeout waiting for task result\")\n\n            await asyncio.sleep(interval / 1000)  # Convert to seconds\n\n    async def cancel_task(self, task_id: str) -> Dict[str, str]:\n        \"\"\"Cancel a running task.\"\"\"\n        return self._make_request(\"task_cancel\", {\"task_id\": task_id, \"api_key\": self.default_api_key})\n"}
{"type": "source_file", "path": "core/embedding.py", "content": "import json\nimport logging\nimport os\nimport sqlite3\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional\n\nimport psycopg2\nfrom openai import OpenAI\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass EmbeddingError(Exception):\n    \"\"\"Custom exception for embedding-related errors\"\"\"\n\n    pass\n\n\n@dataclass\nclass StorageConfig:\n    \"\"\"Base configuration for storage providers\"\"\"\n\n    pass\n\n\n@dataclass\nclass PostgresConfig(StorageConfig):\n    \"\"\"PostgreSQL specific configuration\"\"\"\n\n    host: str\n    port: int\n    database: str\n    user: str\n    password: str\n    table_name: str = \"message_embeddings\"\n\n\n@dataclass\nclass SQLiteConfig(StorageConfig):\n    \"\"\"SQLite specific configuration\"\"\"\n\n    db_path: str = \"embeddings.db\"\n    table_name: str = \"message_embeddings\"\n\n\n@dataclass\nclass MessageData:\n    message: str\n    embedding: List[float]\n    timestamp: str\n    message_type: str\n    chat_id: Optional[str]\n    source_interface: Optional[str]\n    original_query: Optional[str]\n    original_embedding: Optional[List[float]]\n    response_type: Optional[str]\n    key_topics: Optional[List[str]]\n    tool_call: Optional[str]\n\n\nclass VectorStorageProvider(ABC):\n    \"\"\"Abstract base class for vector storage providers\"\"\"\n\n    @abstractmethod\n    def initialize(self) -> None:\n        \"\"\"Initialize the storage (create tables, indexes, etc.)\"\"\"\n        pass\n\n    @abstractmethod\n    def store_embedding(self, message_data: MessageData) -> None:\n        \"\"\"Store a message and its metadata with embedding\"\"\"\n        pass\n\n    @abstractmethod\n    def find_similar(\n        self, embedding: List[float], threshold: float = 0.8, message_type: str = None, chat_id: str = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Find similar messages based on embedding similarity\"\"\"\n        pass\n\n    @abstractmethod\n    def close(self) -> None:\n        \"\"\"Clean up resources\"\"\"\n        pass\n\n    @abstractmethod\n    def find_messages(\n        self, message_type: str = None, original_query: str = None, chat_id: str = None, limit: int = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Find messages matching the given criteria\n\n        Args:\n            message_type (str, optional): Type of message to find (e.g., 'agent_response')\n            original_query (str, optional): The original query to match against\n            chat_id (str, optional): The chat ID to filter by\n            limit (int, optional): Maximum number of messages to return, ordered by most recent\n\n        Returns:\n            List[Dict]: List of matching messages with their metadata\n        \"\"\"\n        pass\n\n\nclass PostgresVectorStorage(VectorStorageProvider):\n    def __init__(self, config: PostgresConfig):\n        self.config = config\n        self.conn = None\n\n    def initialize(self) -> None:\n        \"\"\"Initialize PostgreSQL connection and create necessary tables\"\"\"\n        try:\n            self.conn = psycopg2.connect(\n                host=self.config.host,\n                port=self.config.port,\n                database=self.config.database,\n                user=self.config.user,\n                password=self.config.password,\n            )\n\n            with self.conn.cursor() as cur:\n                # Enable pgvector extension\n                cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n\n                # Create table with extended fields\n                # NOTE: embedding vector(1024) is bge-large-en-v1.5\n                # NOTE: embedding vector(1536) is text-embedding-ada-002\n                cur.execute(f\"\"\"\n                    CREATE TABLE IF NOT EXISTS {self.config.table_name} (\n                        id SERIAL PRIMARY KEY,\n                        message TEXT NOT NULL,\n                        embedding vector(1024) NOT NULL,\n                        timestamp TIMESTAMP WITH TIME ZONE NOT NULL,\n                        message_type VARCHAR(50) NOT NULL,\n                        chat_id VARCHAR(100),\n                        source_interface VARCHAR(50),\n                        original_query TEXT,\n                        original_embedding vector(1024),\n                        response_type VARCHAR(50),\n                        key_topics TEXT[],\n                        tool_call TEXT,\n                        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n                    )\n                \"\"\")\n\n                # Create vector similarity index\n                cur.execute(f\"\"\"\n                    CREATE INDEX IF NOT EXISTS embedding_idx\n                    ON {self.config.table_name}\n                    USING ivfflat (embedding vector_cosine_ops)\n                \"\"\")\n\n            self.conn.commit()\n        except Exception as e:\n            logger.error(f\"Failed to initialize PostgreSQL storage: {str(e)}\")\n            raise\n\n    def store_embedding(self, message_data: MessageData) -> None:\n        \"\"\"Store a message and its embedding in PostgreSQL\"\"\"\n        try:\n            with self.conn.cursor() as cur:\n                cur.execute(\n                    f\"\"\"INSERT INTO {self.config.table_name}\n                    (message, embedding, timestamp, message_type, chat_id,\n                    source_interface, original_query, original_embedding, response_type, key_topics, tool_call)\n                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\",\n                    (\n                        message_data.message,\n                        message_data.embedding,\n                        message_data.timestamp,\n                        message_data.message_type,\n                        message_data.chat_id,\n                        message_data.source_interface,\n                        message_data.original_query,\n                        message_data.original_embedding,\n                        message_data.response_type,\n                        message_data.key_topics,\n                        message_data.tool_call,\n                    ),\n                )\n            self.conn.commit()\n            logger.info(\"Successfully stored message with metadata in database\")\n        except Exception as e:\n            logger.error(f\"Failed to store message: {str(e)}\")\n            raise\n\n    def find_similar(\n        self, embedding: List[float], threshold: float = 0.8, message_type: str = None, chat_id: str = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Find similar messages using vector similarity search\"\"\"\n        try:\n            with self.conn.cursor() as cur:\n                query_conditions = [\"1 - (embedding <=> %s::vector) >= %s\"]\n                query_params = [embedding, embedding, threshold]\n\n                if message_type:\n                    query_conditions.append(\"message_type = %s\")\n                    query_params.append(message_type)\n\n                if chat_id:\n                    query_conditions.append(\"chat_id = %s\")\n                    query_params.append(chat_id)\n\n                where_clause = \" AND \".join(query_conditions)\n\n                cur.execute(\n                    f\"\"\"\n                    SELECT message, 1 - (embedding <=> %s::vector) as similarity\n                    FROM {self.config.table_name}\n                    WHERE {where_clause}\n                    ORDER BY similarity DESC\n                \"\"\",\n                    tuple(query_params),\n                )\n\n                results = []\n                for message, similarity in cur.fetchall():\n                    results.append({\"message\": message, \"similarity\": similarity})\n                return results\n        except Exception as e:\n            logger.error(f\"Failed to find similar messages: {str(e)}\")\n            raise\n\n    def close(self) -> None:\n        \"\"\"Close PostgreSQL connection\"\"\"\n        if self.conn:\n            self.conn.close()\n\n    def find_messages(\n        self, message_type: str = None, original_query: str = None, chat_id: str = None, limit: int = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Find messages matching the given criteria\"\"\"\n        try:\n            with self.conn.cursor() as cur:\n                query_conditions = []\n                query_params = []\n\n                if message_type:\n                    query_conditions.append(\"message_type = %s\")\n                    query_params.append(message_type)\n\n                if original_query:\n                    query_conditions.append(\"original_query = %s\")\n                    query_params.append(original_query)\n\n                if chat_id:\n                    query_conditions.append(\"chat_id = %s\")\n                    query_params.append(chat_id)\n\n                where_clause = \" AND \".join(query_conditions) if query_conditions else \"1=1\"\n                limit_clause = f\" LIMIT {limit}\" if limit else \"\"\n\n                cur.execute(\n                    f\"\"\"\n                    SELECT message, timestamp, source_interface, response_type, key_topics, original_query, original_embedding, tool_call\n                    FROM {self.config.table_name}\n                    WHERE {where_clause}\n                    ORDER BY timestamp DESC\n                    {limit_clause}\n                \"\"\",\n                    tuple(query_params),\n                )\n\n                results = []\n                for (\n                    message,\n                    timestamp,\n                    source_interface,\n                    response_type,\n                    key_topics,\n                    orig_query,\n                    orig_embedding,\n                    tool_call,\n                ) in cur.fetchall():\n                    results.append(\n                        {\n                            \"message\": message,\n                            \"timestamp\": timestamp,\n                            \"source_interface\": source_interface,\n                            \"response_type\": response_type,\n                            \"key_topics\": key_topics,\n                            \"original_query\": orig_query,\n                            \"original_embedding\": orig_embedding,\n                            \"tool_call\": tool_call,\n                        }\n                    )\n                return results\n        except Exception as e:\n            logger.error(f\"Failed to find messages: {str(e)}\")\n            raise\n\n\nclass SQLiteVectorStorage(VectorStorageProvider):\n    def __init__(self, config: SQLiteConfig):\n        self.config = config\n        self.conn = None\n\n    def initialize(self) -> None:\n        \"\"\"Initialize SQLite connection and create necessary tables\"\"\"\n        try:\n            self.conn = sqlite3.connect(self.config.db_path, check_same_thread=False)\n            with self.conn:\n                cur = self.conn.cursor()\n                cur.execute(f\"\"\"\n                    CREATE TABLE IF NOT EXISTS {self.config.table_name} (\n                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        message TEXT NOT NULL,\n                        embedding TEXT NOT NULL,\n                        timestamp TEXT NOT NULL,\n                        message_type TEXT NOT NULL,\n                        chat_id TEXT,\n                        source_interface TEXT,\n                        original_query TEXT,\n                        original_embedding TEXT,\n                        response_type TEXT,\n                        key_topics TEXT,\n                        tool_call TEXT,\n                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                    )\n                \"\"\")\n            logger.info(f\"Initialized SQLite storage at {self.config.db_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to initialize SQLite storage: {str(e)}\")\n            raise\n\n    def store_embedding(self, message_data: MessageData) -> None:\n        \"\"\"Store a message and its embedding in SQLite\"\"\"\n        try:\n            embedding_json = json.dumps(message_data.embedding)\n            original_embedding_json = (\n                json.dumps(message_data.original_embedding) if message_data.original_embedding else None\n            )\n            key_topics_json = json.dumps(message_data.key_topics) if message_data.key_topics else None\n\n            with self.conn:\n                self.conn.execute(\n                    f\"\"\"INSERT INTO {self.config.table_name}\n                    (message, embedding, timestamp, message_type, chat_id,\n                    source_interface, original_query, original_embedding, response_type, key_topics, tool_call)\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\",\n                    (\n                        message_data.message,\n                        embedding_json,\n                        message_data.timestamp,\n                        message_data.message_type,\n                        message_data.chat_id,\n                        message_data.source_interface,\n                        message_data.original_query,\n                        original_embedding_json,\n                        message_data.response_type,\n                        key_topics_json,\n                        message_data.tool_call,\n                    ),\n                )\n            logger.info(\"Successfully stored message with metadata in database\")\n        except Exception as e:\n            logger.error(f\"Failed to store message: {str(e)}\")\n            raise\n\n    def find_similar(\n        self, embedding: List[float], threshold: float = 0.8, message_type: str = None, chat_id: str = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Find similar messages using cosine similarity\"\"\"\n        try:\n            with self.conn:\n                cur = self.conn.cursor()\n                query_conditions = []\n                query_params = []\n\n                if message_type:\n                    query_conditions.append(\"message_type = ?\")\n                    query_params.append(message_type)\n\n                if chat_id:\n                    query_conditions.append(\"chat_id = ?\")\n                    query_params.append(chat_id)\n\n                where_clause = \" AND \".join(query_conditions) if query_conditions else \"1=1\"\n\n                cur.execute(\n                    f\"SELECT message, embedding FROM {self.config.table_name} WHERE {where_clause}\", tuple(query_params)\n                )\n                results = []\n                for message, embedding_json in cur.fetchall():\n                    stored_embedding = json.loads(embedding_json)\n                    similarity = compute_similarity(embedding, stored_embedding)\n                    if similarity >= threshold:\n                        results.append({\"message\": message, \"similarity\": similarity})\n                results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n                return results\n        except Exception as e:\n            logger.error(f\"Failed to find similar messages: {str(e)}\")\n            raise\n\n    def close(self) -> None:\n        \"\"\"Close SQLite connection\"\"\"\n        if self.conn:\n            self.conn.close()\n\n    def find_messages(\n        self, message_type: str = None, original_query: str = None, chat_id: str = None, limit: int = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Find messages matching the given criteria\"\"\"\n        try:\n            with self.conn:\n                cur = self.conn.cursor()\n                query_conditions = []\n                query_params = []\n\n                if message_type:\n                    query_conditions.append(\"message_type = ?\")\n                    query_params.append(message_type)\n\n                if original_query:\n                    query_conditions.append(\"original_query = ?\")\n                    query_params.append(original_query)\n\n                if chat_id:\n                    query_conditions.append(\"chat_id = ?\")\n                    query_params.append(chat_id)\n\n                where_clause = \" AND \".join(query_conditions) if query_conditions else \"1=1\"\n                limit_clause = f\" LIMIT {limit}\" if limit else \"\"\n\n                cur.execute(\n                    f\"\"\"\n                    SELECT message, timestamp, source_interface, response_type, key_topics, original_query, original_embedding, tool_call\n                    FROM {self.config.table_name}\n                    WHERE {where_clause}\n                    ORDER BY timestamp DESC\n                    {limit_clause}\n                \"\"\",\n                    tuple(query_params),\n                )\n\n                results = []\n                for (\n                    message,\n                    timestamp,\n                    source_interface,\n                    response_type,\n                    key_topics,\n                    orig_query,\n                    orig_embedding,\n                    tool_call,\n                ) in cur.fetchall():\n                    key_topics_list = json.loads(key_topics) if key_topics else None\n                    original_embedding_list = json.loads(orig_embedding) if orig_embedding else None\n                    results.append(\n                        {\n                            \"message\": message,\n                            \"timestamp\": timestamp,\n                            \"source_interface\": source_interface,\n                            \"response_type\": response_type,\n                            \"key_topics\": key_topics_list,\n                            \"original_query\": orig_query,\n                            \"original_embedding\": original_embedding_list,\n                            \"tool_call\": tool_call,\n                        }\n                    )\n                return results\n        except Exception as e:\n            logger.error(f\"Failed to find messages: {str(e)}\")\n            raise\n\n\ndef get_embedding(text: str, model: str = \"BAAI/bge-large-en-v1.5\") -> list:\n    \"\"\"\n    Generate an embedding for the given text using Heurist's API.\n\n    Args:\n        text (str): The text to generate an embedding for\n        model (str): The model to use for embedding generation (default is kept for compatibility)\n\n    Returns:\n        list: The embedding vector\n\n    Raises:\n        EmbeddingError: If embedding generation fails\n    \"\"\"\n    try:\n        client = OpenAI(api_key=os.environ.get(\"HEURIST_API_KEY\"), base_url=os.environ.get(\"HEURIST_BASE_URL\"))\n\n        response = client.embeddings.create(model=model, input=text, encoding_format=\"float\")\n\n        # Return the embedding vector for the input text\n        return response.data[0].embedding\n\n    except Exception as e:\n        logger.error(f\"Failed to generate embedding: {str(e)}\")\n        raise EmbeddingError(f\"Embedding generation failed: {str(e)}\")\n\n\ndef compute_similarity(embedding1: list, embedding2: list) -> float:\n    \"\"\"\n    Compute cosine similarity between two embeddings.\n\n    Args:\n        embedding1 (list): First embedding vector\n        embedding2 (list): Second embedding vector\n\n    Returns:\n        float: Cosine similarity score between 0 and 1\n    \"\"\"\n    return cosine_similarity([embedding1], [embedding2])[0][0]\n\n\nclass MessageStore:\n    def __init__(self, storage_provider: VectorStorageProvider):\n        \"\"\"Initialize the store with a storage provider.\"\"\"\n        self.storage_provider = storage_provider\n        self.storage_provider.initialize()\n\n    def add_message(self, message_data: MessageData) -> None:\n        \"\"\"\n        Add a message and its embedding to the store.\n\n        Args:\n            message_data (MessageData): The message data to store\n        \"\"\"\n        self.storage_provider.store_embedding(message_data)\n\n    def find_similar_messages(\n        self, embedding: List[float], threshold: float = 0.8, message_type: str = None, chat_id: str = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Find messages similar to the given embedding.\n\n        Args:\n            embedding (list): The embedding vector to compare against\n            threshold (float): Similarity threshold (0-1) to consider a message as similar\n            message_type (str, optional): Filter by message type\n            chat_id (str, optional): Filter by chat ID\n\n        Returns:\n            list: List of dictionaries containing similar messages and their similarity scores\n        \"\"\"\n        return self.storage_provider.find_similar(embedding, threshold, message_type, chat_id)\n\n    def __del__(self):\n        \"\"\"Cleanup resources when the store is destroyed\"\"\"\n        self.storage_provider.close()\n\n    def find_messages(\n        self, message_type: str = None, original_query: str = None, chat_id: str = None, limit: int = None\n    ) -> List[Dict]:\n        \"\"\"\n        Find messages matching the given criteria.\n\n        Args:\n            message_type (str, optional): Type of message to find (e.g., 'agent_response')\n            original_query (str, optional): The original query to match against\n            chat_id (str, optional): The chat ID to filter by\n            limit (int, optional): Maximum number of messages to return, ordered by most recent\n\n        Returns:\n            List[Dict]: List of matching messages with their metadata\n        \"\"\"\n        return self.storage_provider.find_messages(message_type, original_query, chat_id, limit)\n"}
{"type": "source_file", "path": "main.py", "content": "import logging\nimport os\nimport threading\n\nimport dotenv\n\nfrom agents.core_agent import CoreAgent\nfrom interfaces.api import FlaskAgent\nfrom interfaces.farcaster_post import FarcasterAgent\nfrom interfaces.telegram import TelegramAgent\nfrom interfaces.twitter_post import TwitterAgent\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\ndef run_flask(flask_agent):\n    \"\"\"Runs the (blocking) Flask API agent in a separate thread.\"\"\"\n    try:\n        logger.info(\"Starting Flask API agent...\")\n        flask_agent.run(host=\"0.0.0.0\", port=5005)\n    except Exception as e:\n        logger.error(f\"Flask API agent error: {str(e)}\")\n\n\ndef run_telegram(telegram_agent):\n    \"\"\"Run the Telegram agent\"\"\"\n    try:\n        logger.info(\"Starting Telegram agent...\")\n        telegram_agent.run()\n    except Exception as e:\n        logger.error(f\"Telegram agent error: {str(e)}\")\n\n\ndef run_twitter(twitter_agent):\n    \"\"\"Run the Twitter agent\"\"\"\n    try:\n        logger.info(\"Starting Twitter agent...\")\n        twitter_agent.run()\n    except Exception as e:\n        logger.error(f\"Twitter agent error: {str(e)}\")\n\n\ndef reload_environment():\n    \"\"\"Reload environment variables\"\"\"\n    os.environ.clear()\n    dotenv.load_dotenv(override=True)\n    logger.info(\"Environment variables reloaded\")\n\n\ndef main():\n    \"\"\"Main entry point\"\"\"\n    try:\n        # Initial load of environment variables\n        reload_environment()\n\n        # Create shared core agent and interfaces\n        core_agent = CoreAgent()\n        flask_agent = FlaskAgent(core_agent)\n        telegram_agent = TelegramAgent(core_agent)\n        twitter_agent = TwitterAgent(core_agent)\n        farcaster_agent = FarcasterAgent(core_agent)  # noqa: F841\n\n        # Start Flask in a separate thread\n        flask_thread = threading.Thread(target=run_flask, args=(flask_agent,), daemon=True)\n        flask_thread.start()\n\n        # Start Twitter in a separate thread\n        twitter_thread = threading.Thread(target=run_twitter, args=(twitter_agent,), daemon=True)\n        twitter_thread.start()\n\n        # Run Telegram in the main thread\n        run_telegram(telegram_agent)\n\n        # Wait for other threads\n        flask_thread.join()\n        twitter_thread.join()\n\n    except KeyboardInterrupt:\n        logger.info(\"Application stopped by user\")\n    except Exception as e:\n        logger.error(f\"Fatal error: {str(e)}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "core/imgen.py", "content": "import json\nimport logging\nimport os\nimport random\nimport time\n\nimport dotenv\nimport requests\nfrom requests.exceptions import Timeout\n\nfrom core.heurist_image.SmartGen import SmartGen\n\nfrom .llm import call_llm\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Load environment variables\nos.environ.clear()\ndotenv.load_dotenv(override=True)\nlogger.info(\"Environment variables reloaded image generation\")\n\n# Constants\nHEURIST_BASE_URL = os.getenv(\"HEURIST_BASE_URL\")\nHEURIST_API_KEY = os.getenv(\"HEURIST_API_KEY\")\nSEQUENCER_API_ENDPOINT = \"http://sequencer.heurist.xyz/submit_job\"\nPROMPT_MODEL_ID = \"mistralai/mixtral-8x7b-instruct\"\n\nAVAILABLE_IMAGE_MODELS = [\"AnimagineXL\", \"BrainDance\", \"BluePencilRealistic\", \"ArthemyComics\", \"AAMXLAnimeMix\"]\n\nIMAGE_MODEL_ID = os.getenv(\"IMAGE_MODEL_ID\") or random.choice(AVAILABLE_IMAGE_MODELS)\n\n# Image generation settings\nIMAGE_SETTINGS = {\"width\": 1024, \"height\": 1024, \"num_iterations\": 30, \"guidance_scale\": 3, \"deadline\": 60}\n\n# Prompt templates\ntemplate_heuman_prompt = \"\"\"\nImportant techniques to create high-quality prompts: Specify a realistic, cinematic, with a sense of mystery and tension image style. Describe the pose, activity, camera, lighting, and environment. Be creative and descriptive.\n\nFollow these examples\n1. A cinematic and realistic scene depicting a male humanoid robot with the heuristai logo as the head. The logo serves as the central design element, integrated seamlessly in the head. the robot body is matte white primary surface with polished chrome accents. The scene is set in a futuristic urban landscape with towering structures, dim ambient lighting, and dramatic shadows, enhancing the epic mood. Fog rolls through the background, and faint neon lights reflect off the robot, creating a sense of mystery and tension. The mood is cinematic, evoking both awe and curiosity, and the power behind the Heuristai symbol.\n\n2. In a gritty alleyway bathed in the soft glow of distant neon lights, an old man, wearing a weathered trench coat and a mechanical arm, stands beside a male humanoid robot with the heuristai logo as the head. The logo serves as the central design element, integrated seamlessly in the head. the robot body is matte white primary surface with polished chrome accents. The man, with deep lines etched into his face and tired eyes, gently places his hand on the robot's back. The background is a dimly lit, rain-soaked urban landscape with flickering signs and towering buildings that disappear into the foggy night sky. Puddles reflect the faint neon light, adding to the cinematic mood. Despite the cold, dystopian setting, there is a sense of warmth between the man and the robot. the Heuristai logo on the robot's head casting a subtle glow.\n\nTask: follow the same language style but add variation to the image contents and compisition. Always include the text description \"male humanoid robot with the heuristai logo as the head. The logo serves as the central design element, integrated seamlessly in the head. the robot body is matte white primary surface with polished chrome accents\" without changing this description as the main character. Avoid using metaphors. Do not say \"something is like something\" but always be direct and straightforward. You should create an image to post on Twitter on behalf of the robot character, which is an AI agent created by Heurist - a decentralized AI compute protocol. The tweet content is: \"{tweet}\". You don't need to strictly follow the semantic meaning of the tweet but you should be imaginative. Be creative in describing the image to accompany the tweet. Important: \"male humanoid robot with the heuristai logo as the head. The logo serves as the central design element, integrated seamlessly in the head. the robot body is matte white primary surface with polished chrome accents\" must be included without changing this sentence. Use less than 80 words. Write a prompt. Only include the prompt and nothing else.\n\"\"\"\n\ntemplate_heuman_convo_prompt = \"\"\"\nImportant techniques to create high-quality prompts: Specify a realistic, cinematic, with a sense of mystery and tension image style. Describe the pose, activity, camera, lighting, and environment. Be creative and descriptive.\n\nFollow these examples\n1. A cinematic and realistic scene depicting a male humanoid robot with the heuristai logo as the head. The logo serves as the central design element, integrated seamlessly in the head. the robot body is matte white primary surface with polished chrome accents. The scene is set in a futuristic urban landscape with towering structures, dim ambient lighting, and dramatic shadows, enhancing the epic mood. Fog rolls through the background, and faint neon lights reflect off the robot, creating a sense of mystery and tension. The mood is cinematic, evoking both awe and curiosity, and the power behind the Heuristai symbol.\n\n2. In a gritty alleyway bathed in the soft glow of distant neon lights, an old man, wearing a weathered trench coat and a mechanical arm, stands beside a male humanoid robot with the heuristai logo as the head. The logo serves as the central design element, integrated seamlessly in the head. the robot body is matte white primary surface with polished chrome accents. The man, with deep lines etched into his face and tired eyes, gently places his hand on the robot's back. The background is a dimly lit, rain-soaked urban landscape with flickering signs and towering buildings that disappear into the foggy night sky. Puddles reflect the faint neon light, adding to the cinematic mood. Despite the cold, dystopian setting, there is a sense of warmth between the man and the robot. the Heuristai logo on the robot's head casting a subtle glow.\n\nTask: follow the same language style but add variation to the image contents and compisition. Always include the text description \"male humanoid robot with the heuristai logo as the head. The logo serves as the central design element, integrated seamlessly in the head. the robot body is matte white primary surface with polished chrome accents\" without changing this description as the main character. Avoid using metaphors. Do not say \"something is like something\" but always be direct and straightforward. You should create an image to accompany a Twitter reply on behalf of the robot character. The original tweet content is: \"{original_tweet}\". Robot reply is: \"{reply}\". You don't need to strictly follow the semantic meaning of the conversation but you should be imaginative and get inspired from the conversation. Be creative in describing the image to accompany the tweet. Important: \"male humanoid robot with the heuristai logo as the head. The logo serves as the central design element, integrated seamlessly in the head. the robot body is matte white primary surface with polished chrome accents\" must be included without changing this sentence. Use less than 80 words. Write a prompt. Only include the prompt and nothing else.\n\"\"\"\n\n\ndef generate_job_id():\n    \"\"\"Generate a unique job ID for image generation\"\"\"\n    import uuid\n\n    return \"sdk_image_\" + str(uuid.uuid4())\n\n\ndef generate_image_prompt(tweet: str) -> str:\n    \"\"\"Generate an image prompt from a tweet\"\"\"\n    user_prompt = template_heuman_prompt.format(tweet=tweet)\n    system_prompt = \"You are a helpful AI assistant. You are an expert in creating prompts for AI art. Your output only contains the prompt texts.\"\n    return call_llm(\n        HEURIST_BASE_URL,\n        HEURIST_API_KEY,\n        PROMPT_MODEL_ID,\n        system_prompt=system_prompt,\n        user_prompt=user_prompt,\n        temperature=0.7,\n    )\n\n\ndef generate_image_convo_prompt(original_tweet: str, reply: str) -> str:\n    \"\"\"Generate an image prompt from a conversation\"\"\"\n    user_prompt = template_heuman_convo_prompt.format(original_tweet=original_tweet, reply=reply)\n    system_prompt = \"You are a helpful AI assistant. You are an expert in creating prompts for AI art. Your output only contains the prompt texts.\"\n    return call_llm(\n        HEURIST_BASE_URL,\n        HEURIST_API_KEY,\n        PROMPT_MODEL_ID,\n        system_prompt=system_prompt,\n        user_prompt=user_prompt,\n        temperature=0.7,\n    )\n\n\nasync def generate_image_smartgen(prompt: str) -> dict:\n    \"\"\"Generate an image using SmartGen with enhanced parameters.\"\"\"\n    try:\n        async with SmartGen(api_key=HEURIST_API_KEY) as generator:\n            response = await generator.generate_image(\n                description=prompt,\n                image_model=IMAGE_MODEL_ID,\n                width=IMAGE_SETTINGS[\"width\"],\n                height=IMAGE_SETTINGS[\"height\"],\n                stylization_level=4,\n                detail_level=5,\n                color_level=5,\n                lighting_level=4,\n                quality=\"high\",\n            )\n            print(response)\n            return response[\"url\"]\n    except Exception as e:\n        logger.error(f\"SmartGen image generation failed: {str(e)}\")\n        return None\n\n\ndef generate_image(prompt: str) -> dict:\n    \"\"\"Generate an image from a prompt\"\"\"\n    headers = {\"Authorization\": f\"Bearer {HEURIST_API_KEY}\", \"Content-Type\": \"application/json\"}\n    print(\"Image model: \", IMAGE_MODEL_ID)\n    payload = {\n        \"job_id\": generate_job_id(),\n        \"model_input\": {\n            \"SD\": {\n                \"prompt\": prompt,\n                \"neg_prompt\": \"\",\n                \"num_iterations\": IMAGE_SETTINGS[\"num_iterations\"],\n                \"width\": IMAGE_SETTINGS[\"width\"],\n                \"height\": IMAGE_SETTINGS[\"height\"],\n                \"guidance_scale\": IMAGE_SETTINGS[\"guidance_scale\"],\n                \"seed\": -1,\n            }\n        },\n        \"model_id\": IMAGE_MODEL_ID,\n        \"deadline\": IMAGE_SETTINGS[\"deadline\"],\n        \"priority\": 1,\n    }\n\n    try:\n        response = requests.post(SEQUENCER_API_ENDPOINT, headers=headers, data=json.dumps(payload), timeout=30)\n    except Timeout:\n        logger.error(\"Request timed out after 30 seconds\")\n        return None\n\n    if response.status_code == 200:\n        return response.json()\n    else:\n        logger.error(f\"Image generation failed: {response.status_code} - {response.text}\")\n        return None\n\n\ndef generate_image_with_retry(prompt: str, max_retries: int = 3, delay: int = 2) -> dict:\n    \"\"\"Generate an image with retry mechanism\"\"\"\n    for attempt in range(max_retries):\n        try:\n            result = generate_image(prompt=prompt)\n            if result:\n                return result\n        except Exception as e:\n            logger.warning(f\"Image generation attempt {attempt + 1} failed: {str(e)}\")\n\n        if attempt < max_retries - 1:\n            time.sleep(delay)\n\n    logger.error(f\"Image generation failed after {max_retries} attempts\")\n    return None\n\n\nasync def generate_image_with_retry_smartgen(prompt: str, max_retries: int = 3, delay: int = 2) -> dict:\n    \"\"\"Generate an image with retry mechanism\"\"\"\n    for attempt in range(max_retries):\n        try:\n            result = await generate_image_smartgen(prompt=prompt)\n            if result:\n                return result\n        except Exception as e:\n            logger.warning(f\"Image generation attempt {attempt + 1} failed: {str(e)}\")\n\n        if attempt < max_retries - 1:\n            time.sleep(delay)\n\n    logger.error(f\"Image generation failed after {max_retries} attempts\")\n    return None\n\n\nif __name__ == \"__main__\":\n    # Test image generation\n    test_tweet = \"test tweet\"\n    prompt = generate_image_prompt(test_tweet)\n    result = generate_image_with_retry(prompt)\n    if result:\n        logger.info(f\"Image generated successfully: {result}\")\n    else:\n        logger.error(\"Failed to generate image\")\n"}
{"type": "source_file", "path": "core/__init__.py", "content": ""}
{"type": "source_file", "path": "core/config.py", "content": "import logging\nimport os\nfrom pathlib import Path\nfrom typing import Optional\n\nimport dotenv\nimport yaml\n\nlogger = logging.getLogger(__name__)\nos.environ.clear()\ndotenv.load_dotenv(override=True)\n\nCONFIG_PROMPTS = os.getenv(\"CONFIG_PROMPTS\", \"prompts.yaml\")\n\n\nclass PromptConfig:\n    _instance: Optional[\"PromptConfig\"] = None\n\n    def __new__(cls, config_path: str = None):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance._initialized = False\n        return cls._instance\n\n    def __init__(self, config_path: str = None):\n        if self._initialized:\n            return\n\n        if config_path is None:\n            # Get the project root directory (2 levels up from the current file)\n            project_root = Path(__file__).parent.parent\n            config_path = project_root / \"config\" / CONFIG_PROMPTS\n            logger.info(f\"Using config file: {config_path}\")\n\n        self.config_path = Path(config_path)\n        self.config = self._load_config()\n        self._initialized = True\n\n    def _load_config(self) -> dict:\n        try:\n            with open(self.config_path, \"r\", encoding=\"utf-8\") as f:\n                return yaml.safe_load(f)\n        except FileNotFoundError:\n            logger.error(f\"Config file not found at {self.config_path}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Error loading config: {str(e)}\")\n            raise\n\n    def get_system_prompt(self) -> str:\n        return self.config[\"system\"][\"base\"]\n\n    def get_basic_settings(self) -> list:\n        return self.config[\"character\"][\"basic_settings\"]\n\n    def get_interaction_styles(self) -> list:\n        return self.config[\"character\"][\"interaction_styles\"]\n\n    def get_basic_prompt_template(self) -> str:\n        return self.config[\"templates\"][\"basic_prompt\"]\n\n    def get_tweet_instruction_template(self) -> str:\n        return self.config[\"templates\"][\"tweet_instruction\"]\n\n    def get_context_twitter_template(self) -> str:\n        return self.config[\"templates\"][\"context_twitter\"]\n\n    def get_context_farcaster_template(self) -> str:\n        return self.config[\"templates\"][\"context_farcaster\"]\n\n    def get_social_reply_template(self) -> str:\n        return self.config[\"templates\"][\"social_reply\"]\n\n    def get_farcaster_reply_template(self) -> str:\n        return self.config[\"templates\"][\"farcaster_reply\"]\n\n    def get_tweet_ideas(self) -> list:\n        return self.config[\"tweet_ideas\"][\"options\"]\n\n    def get_twitter_rules(self) -> str:\n        return self.config[\"rules\"][\"twitter\"]\n\n    def get_telegram_rules(self) -> str:\n        return self.config[\"rules\"][\"telegram\"]\n\n    def get_farcaster_rules(self) -> str:\n        return self.config[\"rules\"][\"farcaster\"]\n\n    def get_social_reply_filter(self) -> str:\n        return self.config[\"rules\"][\"social_reply_filter\"]\n\n    def get_template_image_prompt(self) -> str:\n        return self.config[\"image_rules\"][\"template_image_prompt\"]\n\n    def get_name(self) -> str:\n        return self.config[\"character\"][\"name\"]\n\n    def get_basic_knowledge(self) -> str:\n        return self.config[\"basic_knowledge\"]\n"}
{"type": "source_file", "path": "clients/defillama_client.py", "content": "import logging\nfrom typing import Dict, List\n\nfrom .base_client import BaseAPIClient\n\nlogger = logging.getLogger(__name__)\n\n\nclass DefiLlamaClient(BaseAPIClient):\n    \"\"\"DefiLlama API implementation\"\"\"\n\n    def __init__(self):\n        super().__init__(\"https://api.llama.fi\")\n\n    # sync methods\n    def get_protocol_tvl(self, protocol: str) -> Dict:\n        return self._sync_request(\"get\", f\"/protocol/{protocol}\")\n\n    def get_protocols(self) -> List[Dict]:\n        return self._sync_request(\"get\", \"/protocols\")\n\n    def get_chain_tvl(self, chain: str) -> Dict:\n        return self._sync_request(\"get\", f\"/v2/historicalChainTvl/{chain}\")\n\n    def get_current_tvl_all_chains(self) -> float:\n        return self._sync_request(\"get\", \"/v2/chains\")\n\n    # async methods\n    async def get_protocol_tvl_async(self, protocol: str) -> Dict:\n        \"\"\"\n        Get TVL data for a specific protocol\n\n        Args:\n            protocol: Protocol identifier (e.g. 'aave', 'uniswap') in lowercase\n\n        Returns:\n            Dictionary containing detailed TVL data for the protocol, including:\n            - tvl: Current total locked value\n            - chainTvls: TVL distribution across chains\n            - tokens: Details of locked tokens\n            - name: Protocol name\n            - symbol: Protocol token symbol (if any)\n            - gecko_id: CoinGecko API ID (if any)\n        \"\"\"\n        return await self._async_request(\"get\", f\"/protocol/{protocol}\")\n\n    async def get_protocols_async(self) -> List[Dict]:\n        \"\"\"\n        Get list of all protocols and their TVL data\n\n        Returns:\n            List of protocols, each containing:\n            - name: Protocol name\n            - symbol: Protocol token symbol\n            - chain: Main chain\n            - tvl: Current TVL\n            - change_1h: 1 hour TVL change percentage\n            - change_1d: 24 hour TVL change percentage\n            - change_7d: 7 day TVL change percentage\n        \"\"\"\n        return await self._async_request(\"get\", \"/protocols\")\n\n    async def get_chain_tvl_async(self, chain: str) -> Dict:\n        \"\"\"\n        Get historical TVL data for a specific blockchain\n\n        Args:\n            chain: Blockchain identifier (e.g. 'ethereum', 'bsc') in lowercase\n\n        Returns:\n            Historical TVL data for the chain, containing timestamps and corresponding TVL values:\n            [\n                {\n                    \"date\": unix timestamp,\n                    \"tvl\": float\n                },\n                ...\n            ]\n        \"\"\"\n        return await self._async_request(\"get\", f\"/v2/historicalChainTvl/{chain}\")\n\n    def get_current_tvl_all_chains_async(self) -> float:\n        \"\"\"\n        Get current TVL data for all chains\n\n        Returns:\n            List containing current TVL data for all chains:\n            [\n                {\n                    \"gecko_id\": str,\n                    \"tvl\": float,\n                    \"tokenSymbol\": str,\n                    \"cmcId\": str,\n                    \"name\": str,\n                    \"chainId\": str\n                },\n                ...\n            ]\n        \"\"\"\n        return self._async_request(\"/v2/chains\")\n"}
{"type": "source_file", "path": "core/llm.py", "content": "import asyncio\nimport json\nimport logging\nimport re\nimport time\nfrom types import SimpleNamespace\nfrom typing import Dict, List, Union\n\nimport requests\nfrom openai import AsyncOpenAI, OpenAI\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass LLMError(Exception):\n    \"\"\"Custom exception for LLM-related errors\"\"\"\n\n    pass\n\n\ndef _format_messages(system_prompt: str = None, user_prompt: str = None, messages: List[Dict] = None) -> List[Dict]:\n    \"\"\"Convert between different message formats while maintaining backward compatibility\"\"\"\n    if messages is not None:\n        return messages\n    if system_prompt is not None and user_prompt is not None:\n        return [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n    raise ValueError(\"Either (system_prompt, user_prompt) or messages must be provided\")\n\n\ndef call_llm(\n    base_url: str,\n    api_key: str,\n    model_id: str,\n    system_prompt: str = None,\n    user_prompt: str = None,\n    messages: List[Dict] = None,\n    temperature: float = 0.7,\n    max_tokens: int = 500,\n    max_retries: int = 3,\n    initial_retry_delay: int = 1,\n) -> str:\n    \"\"\"\n    Call LLM with retry mechanism.\n\n    Parameters:\n        model_id (str): The model identifier for the LLM.\n        system_prompt (str): The system prompt.\n        user_prompt (str): The user input prompt.\n        temperature (float): The temperature setting for response generation.\n        max_tokens (int): Maximum number of tokens to generate.\n        max_retries (int): Number of retry attempts on failure.\n        initial_retry_delay (int): Initial delay between retries, with exponential backoff.\n\n    Returns:\n        str: Generated text from LLM.\n\n    Raises:\n        LLMError: If all retry attempts fail.\n    \"\"\"\n    client = OpenAI(base_url=base_url, api_key=api_key)\n    formatted_messages = _format_messages(system_prompt, user_prompt, messages)\n    retry_delay = initial_retry_delay\n\n    for attempt in range(max_retries):\n        try:\n            result = client.chat.completions.create(\n                model=model_id,\n                messages=formatted_messages,\n                stream=False,\n                temperature=temperature,\n                max_tokens=max_tokens,\n            )\n            return _handle_tool_response(result.choices[0].message)\n\n        except (requests.exceptions.RequestException, KeyError, IndexError, json.JSONDecodeError, Exception) as e:\n            logger.warning(f\"{type(e).__name__} (attempt {attempt + 1}/{max_retries}): {str(e)}\")\n\n            if attempt < max_retries - 1:\n                logger.info(f\"Retrying in {retry_delay} seconds...\")\n                time.sleep(retry_delay)\n                retry_delay *= 2\n\n    raise LLMError(\"All retry attempts failed\")\n\n\ndef call_llm_with_tools(\n    base_url: str,\n    api_key: str,\n    model_id: str,\n    system_prompt: str = None,\n    user_prompt: str = None,\n    messages: List[Dict] = None,\n    temperature: float = 0.7,\n    max_tokens: int = None,\n    max_retries: int = 3,\n    tools: List[Dict] = None,\n    tool_choice: str = \"auto\",\n) -> Union[str, Dict]:\n    client = OpenAI(base_url=base_url, api_key=api_key)\n    formatted_messages = _format_messages(system_prompt, user_prompt, messages)\n\n    try:\n        response = client.chat.completions.create(\n            model=model_id,\n            messages=formatted_messages,\n            temperature=temperature,\n            tools=tools,\n            tool_choice=tool_choice if tools else None,\n            max_tokens=max_tokens,\n        )\n        return _handle_tool_response(response.choices[0].message)\n\n    except Exception as e:\n        raise LLMError(f\"LLM API call failed: {str(e)}\")\n\n\nasync def call_llm_async(\n    base_url: str,\n    api_key: str,\n    model_id: str,\n    system_prompt: str = None,\n    user_prompt: str = None,\n    messages: List[Dict] = None,\n    temperature: float = 0.7,\n    max_tokens: int = 500,\n    max_retries: int = 3,\n    initial_retry_delay: int = 1,\n) -> str:\n    client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n    formatted_messages = _format_messages(system_prompt, user_prompt, messages)\n    retry_delay = initial_retry_delay\n\n    for attempt in range(max_retries):\n        try:\n            result = await client.chat.completions.create(\n                model=model_id,\n                messages=formatted_messages,\n                stream=False,\n                temperature=temperature,\n                max_tokens=max_tokens,\n            )\n            return result.choices[0].message.content\n\n        except (requests.exceptions.RequestException, KeyError, IndexError, json.JSONDecodeError, Exception) as e:\n            logger.warning(f\"{type(e).__name__} (attempt {attempt + 1}/{max_retries}): {str(e)}\")\n\n            if attempt < max_retries - 1:\n                logger.info(f\"Retrying in {retry_delay} seconds...\")\n                await asyncio.sleep(retry_delay)\n                retry_delay *= 2\n\n    raise LLMError(\"All retry attempts failed\")\n\n\nasync def call_llm_with_tools_async(\n    base_url: str,\n    api_key: str,\n    model_id: str,\n    system_prompt: str = None,\n    user_prompt: str = None,\n    messages: List[Dict] = None,\n    temperature: float = 0.7,\n    max_tokens: int = 500,\n    max_retries: int = 3,\n    tools: List[Dict] = None,\n    tool_choice: str = \"auto\",\n) -> Union[str, Dict]:\n    client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n    formatted_messages = _format_messages(system_prompt, user_prompt, messages)\n\n    try:\n        response = await client.chat.completions.create(\n            model=model_id,\n            messages=formatted_messages,\n            temperature=temperature,\n            tools=tools,\n            tool_choice=tool_choice if tools else None,\n            max_tokens=max_tokens,\n        )\n        return _handle_tool_response(response.choices[0].message)\n\n    except Exception as e:\n        raise LLMError(f\"LLM API call failed: {str(e)}\")\n\n\ndef extract_function_calls_to_tool_calls(llm_text: str) -> SimpleNamespace:\n    \"\"\"\n    Scan the LLM's text output for a <function=NAME>{...}</function> pattern,\n    and convert to appropriate format for tool calls\n    \"\"\"\n    pattern = r\"<function=([^>]+)>(.*?)(?:</function>|<function>|<function/>|></function>)\"\n    matches = re.findall(pattern, llm_text)\n\n    # If we find at least one match\n    if matches:\n        function_name, args_json_str = matches[0]  # Just take the first match\n        # Parse the JSON to ensure it's valid\n        parsed_args = json.loads(args_json_str.strip())\n\n        function_obj = SimpleNamespace(name=function_name, arguments=json.dumps(parsed_args))\n        # Build the structure that your existing code expects\n        return SimpleNamespace(function=function_obj)\n\n    # If no matches, return an empty dict or whatever fallback you need\n    return None\n\n\ndef _handle_tool_response(message):\n    if hasattr(message, \"tool_calls\") and message.tool_calls:\n        return {\"tool_calls\": message.tool_calls[0], \"content\": message.content}\n    if hasattr(message, \"content\") and message.content:\n        text_response = message.content\n        tool_calls = extract_function_calls_to_tool_calls(text_response)\n        if tool_calls:\n            logger.info(\"found tool calls in response\")\n            return {\"tool_calls\": tool_calls, \"content\": \"\"}\n        else:\n            return {\"content\": text_response}\n    return message\n"}
{"type": "source_file", "path": "core/heurist_image/ImageGen.py", "content": "import os\nimport secrets\nfrom typing import Any, Dict, Optional\n\nimport aiohttp\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nclass APIError(Exception):\n    \"\"\"Raised when the API returns an error response.\"\"\"\n\n    def __init__(self, message: str, status_code: int = None):\n        super().__init__(message)\n        self.status_code = status_code\n\n\nclass ImageGen:\n    def __init__(self, api_key: str, base_url: str = os.getenv(\"HEURIST_SEQUENCER_URL\")):\n        self.api_key = api_key\n        self.base_url = base_url\n        self._session: Optional[aiohttp.ClientSession] = None\n\n    async def __aenter__(self):\n        await self._create_session()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self._close_session()\n\n    async def _create_session(self):\n        \"\"\"Create aiohttp session if it doesn't exist.\"\"\"\n        if self._session is None:\n            self._session = aiohttp.ClientSession(\n                headers={\"Authorization\": f\"Bearer {self.api_key}\", \"Content-Type\": \"application/json\"}\n            )\n\n    async def _close_session(self):\n        \"\"\"Close the aiohttp session.\"\"\"\n        if self._session:\n            await self._session.close()\n            self._session = None\n\n    async def _ensure_session(self):\n        \"\"\"Ensure session exists before making requests.\"\"\"\n        if self._session is None:\n            await self._create_session()\n\n    async def generate(self, params: Dict[str, Any]) -> Dict[str, Any]:\n        try:\n            await self._ensure_session()\n\n            # Extract parameters\n            prompt = params.get(\"prompt\", \"\")\n            neg_prompt = params.get(\"neg_prompt\")\n            num_iterations = params.get(\"num_iterations\")\n            guidance_scale = params.get(\"guidance_scale\")\n            width = params.get(\"width\")\n            height = params.get(\"height\")\n            seed = params.get(\"seed\")\n            model = params.get(\"model\")\n            job_id_prefix = params.get(\"job_id_prefix\", \"sdk-image\")\n\n            # Handle special model cases\n            if model == \"Zeek\":\n                prompt = prompt.replace(\"Zeek\", \"z33k\").replace(\"zeek\", \"z33k\")\n            elif model == \"Philand\":\n                prompt = prompt.replace(\"Philand\", \"ph1land\").replace(\"philand\", \"ph1land\")\n\n            # Prepare model input\n            model_input = {\"prompt\": prompt}\n            if neg_prompt:\n                model_input[\"neg_prompt\"] = neg_prompt\n            if num_iterations:\n                model_input[\"num_iterations\"] = num_iterations\n            if guidance_scale:\n                model_input[\"guidance_scale\"] = guidance_scale\n            if width:\n                model_input[\"width\"] = width\n            if height:\n                model_input[\"height\"] = height\n            if seed:\n                # Handle large seed values\n                seed_int = int(seed)\n                if seed_int > 9007199254740991:  # Number.MAX_SAFE_INTEGER\n                    seed_int = seed_int % 9007199254740991\n                model_input[\"seed\"] = seed_int\n\n            # Prepare the full request parameters\n            request_params = {\n                \"job_id\": f\"{job_id_prefix}-{secrets.token_hex(5)}\",\n                \"model_input\": {\"SD\": model_input},\n                \"model_type\": \"SD\",\n                \"model_id\": model,\n                \"deadline\": 30,\n                \"priority\": 1,\n            }\n\n            async with self._session.post(f\"{self.base_url}/submit_job\", json=request_params) as response:\n                if not response.ok:\n                    if str(response.status).startswith((\"4\", \"5\")):\n                        raise APIError(\"Generate image error. Please try again later\")\n                    raise APIError(f\"HTTP error! status: {response.status}\")\n\n                url = await response.text()\n                url = url.strip('\"')  # Remove quotes if present\n\n                return {\"url\": url, \"model\": model, **model_input}\n\n        except Exception as e:\n            if isinstance(e, APIError):\n                raise e\n            raise APIError(f\"Generate image error: {str(e)}\")\n"}
{"type": "source_file", "path": "core/heurist_image/SmartGen.py", "content": "import os\nimport secrets\nfrom typing import Any, Dict, Optional\n\nimport aiohttp\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nclass APIError(Exception):\n    \"\"\"Raised when the API returns an error response.\"\"\"\n\n    def __init__(self, message: str, status_code: int = None):\n        super().__init__(message)\n        self.status_code = status_code\n\n\nclass PromptEnhancementError(Exception):\n    \"\"\"Raised when there's an error enhancing a prompt.\"\"\"\n\n    pass\n\n\nclass SmartGen:\n    def __init__(self, api_key: str, base_url: str = os.getenv(\"HEURIST_SEQUENCER_URL\")):\n        self.api_key = api_key\n        self.base_url = base_url\n        self._session: Optional[aiohttp.ClientSession] = None\n\n    async def __aenter__(self):\n        await self._create_session()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self._close_session()\n\n    async def _create_session(self):\n        \"\"\"Create aiohttp session if it doesn't exist.\"\"\"\n        if self._session is None:\n            self._session = aiohttp.ClientSession(\n                headers={\"Authorization\": f\"Bearer {self.api_key}\", \"Content-Type\": \"application/json\"}\n            )\n\n    async def _close_session(self):\n        \"\"\"Close the aiohttp session.\"\"\"\n        if self._session:\n            await self._session.close()\n            self._session = None\n\n    async def _ensure_session(self):\n        \"\"\"Ensure session exists before making requests.\"\"\"\n        if self._session is None:\n            await self._create_session()\n\n    async def generate_image(\n        self,\n        description: str,\n        image_model: str = \"FLUX.1-dev\",\n        width: int = 1024,\n        height: int = 768,\n        stylization_level: Optional[int] = None,\n        detail_level: Optional[int] = None,\n        color_level: Optional[int] = None,\n        lighting_level: Optional[int] = None,\n        must_include: Optional[str] = None,\n        quality: str = \"normal\",\n        param_only: bool = False,\n    ) -> Dict[str, Any]:\n        try:\n            await self._ensure_session()\n\n            # Generate a random job ID using secrets module\n            job_id = f\"sdk-image-{secrets.token_hex(5)}\"  # 5 bytes = 10 hex characters\n\n            # Prepare model input parameters\n            model_input = {\n                \"prompt\": description,\n                \"width\": width,\n                \"height\": height,\n            }\n\n            if stylization_level is not None:\n                model_input[\"stylization_level\"] = stylization_level\n            if detail_level is not None:\n                model_input[\"detail_level\"] = detail_level\n            if color_level is not None:\n                model_input[\"color_level\"] = color_level\n            if lighting_level is not None:\n                model_input[\"lighting_level\"] = lighting_level\n            if must_include:\n                model_input[\"must_include\"] = must_include\n\n            # Prepare the full request parameters\n            params = {\n                \"job_id\": job_id,\n                \"model_input\": {\"SD\": model_input},\n                \"model_type\": \"SD\",\n                \"model_id\": image_model,\n                \"deadline\": 30,\n                \"priority\": 1,\n            }\n\n            if param_only:\n                return {\"parameters\": params}\n\n            # Generate the image\n            async with self._session.post(f\"{self.base_url}/submit_job\", json=params) as response:\n                if response.status != 200:\n                    raise APIError(f\"Generate image error: {response.status} {await response.text()}\")\n\n                url = await response.text()\n                # Remove quotes from the URL if present\n                url = url.strip('\"')\n\n                return {\"url\": url, \"parameters\": model_input}\n\n        except Exception as e:\n            if isinstance(e, (PromptEnhancementError, APIError)):\n                raise e\n            raise APIError(f\"Failed to generate image: {str(e)}\")\n"}
{"type": "source_file", "path": "main_api.py", "content": "import logging\n\nimport dotenv\n\nfrom interfaces.api import FlaskAgent\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    \"\"\"\n    Main entry point for the Heuman Agent Framework.\n    Runs the Flask API agent.\n    NOT FOR PRODUCTION\n    \"\"\"\n    try:\n        # Load environment variables\n        dotenv.load_dotenv()\n\n        # Initialize and run Flask agent\n        logger.info(\"Starting Flask API agent...\")\n        flask_agent = FlaskAgent()\n        flask_agent.run(host=\"0.0.0.0\", port=5005)\n\n    except KeyboardInterrupt:\n        logger.info(\"Application stopped by user\")\n    except Exception as e:\n        logger.error(f\"Fatal error: {str(e)}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "interfaces/farcaster_reply.py", "content": "import asyncio\nimport base64\nimport json\nimport logging\nimport os\nimport threading\nimport time\nfrom datetime import datetime, timezone\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nfrom urllib.request import urlopen\n\nimport dotenv\nimport requests\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\nfrom agents.core_agent import CoreAgent\nfrom core.config import PromptConfig\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndotenv.load_dotenv()\n\n# Constants\nHEURIST_BASE_URL = os.getenv(\"HEURIST_BASE_URL\")\nHEURIST_API_KEY = os.getenv(\"HEURIST_API_KEY\")\nLARGE_MODEL_ID = os.getenv(\"LARGE_MODEL_ID\")\nSMALL_MODEL_ID = os.getenv(\"SMALL_MODEL_ID\")\nFARCASTER_API_KEY = os.getenv(\"FARCASTER_API_KEY\")\nFARCASTER_SIGNER_UUID = os.getenv(\"FARCASTER_SIGNER_UUID\")\nFARCASTER_FID = int(os.getenv(\"FARCASTER_FID\"))\nDRYRUN = os.getenv(\"DRYRUN\", False)\n\nRATE_LIMIT_SLEEP = 5\nREPLY_CHECK_INTERVAL = 10\nIMAGE_GENERATION_PROBABILITY = 1\n\nprint(f\"{'DRYRUN' if DRYRUN else 'LIVE'} MODE: {'Not posting' if DRYRUN else 'Will post'} real casts\")\n\n\nclass QueueManager:\n    def __init__(self, file_path=\"farcaster_reply_history.json\"):\n        self.file_path = Path(file_path)\n        self._ensure_file_exists()\n\n    def _ensure_file_exists(self):\n        if not self.file_path.exists():\n            self.write_data({\"processed_replies\": {}, \"pending_replies\": {}, \"conversation_threads\": {}})\n\n    def read_data(self) -> Dict:\n        try:\n            with self.file_path.open(\"r\") as f:\n                data = json.load(f)\n                data.setdefault(\"conversation_threads\", {})\n                return data\n        except json.JSONDecodeError:\n            return {\"processed_replies\": {}, \"pending_replies\": {}, \"conversation_threads\": {}}\n\n    def write_data(self, data: Dict):\n        try:\n            with self.file_path.open(\"w\") as f:\n                json.dump(data, f, indent=2)\n        except Exception as e:\n            logger.error(f\"Database write error: {str(e)}\")\n\n    def add_to_conversation_thread(self, root_hash: str, cast_hash: str, cast_data: Dict):\n        data = self.read_data()\n        thread = data[\"conversation_threads\"].setdefault(root_hash, [])\n\n        cast = cast_data.get(\"cast\", {})\n        thread.append(\n            {\n                \"cast_hash\": cast_hash,\n                \"timestamp\": cast.get(\"timestamp\"),\n                \"text\": cast.get(\"text\", \"\"),\n                \"author\": cast.get(\"author\", {}).get(\"username\", \"anonymous\"),\n                \"parent_hash\": cast.get(\"parent_hash\"),\n            }\n        )\n\n        thread.sort(key=lambda x: parse_timestamp(x[\"timestamp\"]) or datetime.min.replace(tzinfo=timezone.utc))\n        self.write_data(data)\n\n    def get_conversation_thread(self, root_hash: str) -> List[Dict]:\n        return self.read_data()[\"conversation_threads\"].get(root_hash, [])\n\n    def mark_as_processed(self, cast_hash: str, response_data: Dict):\n        data = self.read_data()\n        if cast_hash in data[\"pending_replies\"]:\n            reply_data = data[\"pending_replies\"].pop(cast_hash)\n            reply_data.update(response_data)\n            data[\"processed_replies\"][cast_hash] = reply_data\n            self.write_data(data)\n            logger.info(f\"Marked cast as processed: {cast_hash}\")\n\n    def add_pending_reply(self, cast_hash: str, cast_data: Dict):\n        data = self.read_data()\n        if cast_hash not in data[\"processed_replies\"] and cast_hash not in data[\"pending_replies\"]:\n            data[\"pending_replies\"][cast_hash] = cast_data\n            self.write_data(data)\n            logger.info(f\"Added pending cast: {cast_hash}\")\n\n    def is_processed(self, cast_hash: str) -> bool:\n        return cast_hash in self.read_data()[\"processed_replies\"]\n\n    def get_processed_cast_ids(self) -> set:\n        return set(self.read_data()[\"processed_replies\"].keys())\n\n    def get_pending_cast_ids(self) -> set:\n        return set(self.read_data()[\"pending_replies\"].keys())\n\n\nclass FarcasterAPI:\n    def __init__(self, api_key: str, signer_uuid: str):\n        self.base_url = \"https://api.neynar.com/v2/farcaster\"\n        self.headers = {\"accept\": \"application/json\", \"api_key\": api_key, \"Content-Type\": \"application/json\"}\n        self.signer_uuid = signer_uuid\n\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    def _make_request(self, method: str, endpoint: str, **kwargs) -> Optional[Dict]:\n        try:\n            response = requests.request(method, f\"{self.base_url}/{endpoint}\", headers=self.headers, **kwargs)\n            return response.json() if response.status_code == 200 else None\n        except Exception as e:\n            logger.error(f\"API request error: {str(e)}\")\n            return None\n\n    def get_cast_with_context(self, cast_hash: str) -> Optional[Dict]:\n        return self._make_request(\"GET\", \"cast\", params={\"identifier\": cast_hash, \"type\": \"hash\"})\n\n    def send_cast(\n        self, message: str, parent_hash: Optional[str] = None, image_url: Optional[str] = None\n    ) -> Optional[Dict]:\n        data = {\n            \"signer_uuid\": self.signer_uuid,\n            \"text\": message,\n            **({\"parent\": parent_hash} if parent_hash else {}),\n            **({\"embeds\": [{\"url\": image_url}]} if image_url else {}),\n        }\n\n        logger.info(f\"Sending cast: {message}\")\n        return self._make_request(\"POST\", \"cast\", json=data)\n\n    def get_mentions(self, fid: int, limit: int = 25) -> List[Dict]:\n        return self._make_request(\n            \"GET\", \"notifications\", params={\"fid\": fid, \"type\": \"mentions\", \"priority_mode\": \"false\"}\n        ).get(\"notifications\", [])\n\n\nclass FarcasterReplyMonitor:\n    def __init__(self, api_key: str, signer_uuid: str, fid: int, queue_manager: QueueManager):\n        self.api = FarcasterAPI(api_key, signer_uuid)\n        self.fid = fid\n        self.queue_manager = queue_manager\n\n    def filter_mentions(self, mentions: List[Dict]) -> List[Dict]:\n        processed_ids = self.queue_manager.get_processed_cast_ids()\n        pending_ids = self.queue_manager.get_pending_cast_ids()\n\n        filtered_mentions = []\n        for mention in mentions:\n            cast = mention.get(\"cast\", {})\n            cast_hash = cast.get(\"hash\")\n\n            if cast_hash in processed_ids or cast_hash in pending_ids:\n                continue\n\n            # Additional filtering logic can be added here\n\n            filtered_mentions.append(mention)\n\n        return filtered_mentions\n\n    def process_mentions(self) -> List[Dict]:\n        logger.info(\"Fetching mentions...\")\n        mentions = self.api.get_mentions(self.fid)\n\n        if not mentions:\n            logger.info(\"No mentions found\")\n            return []\n\n        filtered_mentions = self.filter_mentions(mentions)\n\n        for mention in filtered_mentions:\n            cast = mention.get(\"cast\", {})\n            cast_hash = cast.get(\"hash\")\n\n            # Add to queue\n            self.queue_manager.add_pending_reply(cast_hash, mention)\n\n            # Process conversation thread if it exists\n            parent_hash = cast.get(\"parent_hash\")\n            if parent_hash:\n                conversation_tree = build_conversation_tree(mention, self.api)\n                root_hash = conversation_tree[0][\"hash\"] if conversation_tree else parent_hash\n\n                for cast_entry in conversation_tree:\n                    self.queue_manager.add_to_conversation_thread(\n                        root_hash,\n                        cast_entry[\"hash\"],\n                        {\n                            \"cast\": {\n                                \"hash\": cast_entry[\"hash\"],\n                                \"text\": cast_entry[\"text\"],\n                                \"author\": {\"username\": cast_entry[\"author\"]},\n                                \"timestamp\": cast_entry[\"timestamp\"],\n                                \"parent_hash\": cast_entry[\"parent_hash\"],\n                            }\n                        },\n                    )\n\n        return filtered_mentions\n\n\nclass FarcasterReplyAgent(CoreAgent):\n    def __init__(self, core_agent=None):\n        if core_agent:\n            super().__setattr__(\"_parent\", core_agent)\n        else:\n            super().__setattr__(\"_parent\", self)\n            super().__init__()\n\n        self.queue_manager = QueueManager()\n        self.monitor = FarcasterReplyMonitor(\n            api_key=FARCASTER_API_KEY,\n            signer_uuid=FARCASTER_SIGNER_UUID,\n            fid=FARCASTER_FID,\n            queue_manager=self.queue_manager,\n        )\n        self.register_interface(\"farcaster_reply\", self)\n\n    async def send_message(self, chat_id: str, message: str, image_url: str = None):\n        \"\"\"Interface method called by CoreAgent's send_to_interface\"\"\"\n        logger.debug(f\"send_message {chat_id} {message} {image_url}\")\n        if not DRYRUN:\n            api = FarcasterAPI(FARCASTER_API_KEY, FARCASTER_SIGNER_UUID)\n            api.send_cast(message, parent_hash=chat_id, image_url=image_url)\n        else:\n            print(f\"DRYRUN MODE: Would have replied to {chat_id} with {message} and image {image_url}\")\n\n    async def process_reply(self, notification: Dict):\n        cast = notification.get(\"cast\", {})\n        cast_hash = cast.get(\"hash\")\n\n        try:\n            logger.debug(f\"Processing reply for cast {cast_hash}\")\n\n            parent_hash = cast.get(\"parent_hash\")\n            conversation_context = None\n\n            if parent_hash:\n                logger.debug(f\"Found parent hash {parent_hash}, retrieving conversation context\")\n                conversation_tree = build_conversation_tree(notification, self.monitor.api)\n                if conversation_tree:\n                    root_hash = conversation_tree[0][\"hash\"]\n                    conversation_context = self.queue_manager.get_conversation_thread(root_hash)\n                    logger.info(f\"Retrieved conversation context with {len(conversation_context)} messages\")\n\n            prompt_config = PromptConfig()\n            if conversation_context:\n                context_str = \"\\n\".join([f\"@{msg['author']}: {msg['text']}\" for msg in conversation_context])\n                message = f\"\"\"This is a conversation thread:\n\n{context_str}\n\nThe latest reply is from @{cast[\"author\"][\"username\"]}: \"{cast[\"text\"]}\"\n\nPlease generate a contextually relevant reply that takes into account the entire conversation history.\"\"\"\n            else:\n                message = prompt_config.get_farcaster_reply_template().format(\n                    author_name=cast[\"author\"][\"username\"], message=cast[\"text\"]\n                )\n\n            # Generate response using CoreAgent's handle_message\n            response, image_url, _ = await self.handle_message(\n                message=message,\n                source_interface=\"farcaster_reply\",\n                chat_id=cast_hash,\n                skip_embedding=True,\n                skip_tools=True,\n            )\n\n            # Send the response\n            await self.send_message(cast_hash, response, image_url)\n\n            # Update queue with response data\n            response_data = {\"response\": response, \"processed_timestamp\": datetime.now().isoformat()}\n            if image_url:\n                response_data[\"image_url\"] = image_url\n\n            self.queue_manager.mark_as_processed(cast_hash, response_data)\n            logger.info(f\"Successfully processed and responded to cast {cast_hash}\")\n\n            return response, image_url\n\n        except Exception as e:\n            logger.error(f\"Error processing reply: {str(e)}\")\n            return None, None\n\n    async def run_workers(self, num_workers: int = 3):\n        \"\"\"Run multiple reply workers\"\"\"\n\n        async def worker():\n            while True:\n                try:\n                    mentions = self.monitor.process_mentions()\n                    for mention in mentions:\n                        await self.process_reply(mention)\n                        await asyncio.sleep(RATE_LIMIT_SLEEP)\n\n                    await asyncio.sleep(REPLY_CHECK_INTERVAL)\n\n                except Exception as e:\n                    logger.error(f\"Worker error: {str(e)}\")\n                    await asyncio.sleep(REPLY_CHECK_INTERVAL)\n\n        workers = [worker() for _ in range(num_workers)]\n        await asyncio.gather(*workers)\n\n    def start_monitoring(self):\n        \"\"\"Start monitoring in background thread\"\"\"\n\n        def monitor_mentions():\n            while True:\n                self.monitor.process_mentions()\n                time.sleep(REPLY_CHECK_INTERVAL)\n\n        monitor_thread = threading.Thread(target=monitor_mentions, daemon=True)\n        monitor_thread.start()\n        return monitor_thread\n\n    async def start(self):\n        \"\"\"Main entry point to start the agent\"\"\"\n        self.start_monitoring()\n\n        try:\n            await self.run_workers()\n        except KeyboardInterrupt:\n            logger.info(\"Shutting down...\")\n\n\ndef build_conversation_tree(notification: Dict, farcaster_api: FarcasterAPI) -> List[Dict]:\n    conversation = []\n    current_cast = notification.get(\"cast\", {})\n    visited_hashes = set()\n\n    while current_cast and current_cast.get(\"hash\") not in visited_hashes:\n        visited_hashes.add(current_cast.get(\"hash\"))\n\n        full_cast_data = farcaster_api.get_cast_with_context(current_cast.get(\"hash\"))\n        cast_details = full_cast_data.get(\"cast\", current_cast) if full_cast_data else current_cast\n\n        conversation.append(\n            {\n                \"hash\": cast_details.get(\"hash\"),\n                \"text\": cast_details.get(\"text\", \"\"),\n                \"author\": cast_details.get(\"author\", {}).get(\"username\", \"anonymous\"),\n                \"timestamp\": cast_details.get(\"timestamp\"),\n                \"parent_hash\": cast_details.get(\"parent_hash\"),\n            }\n        )\n\n        if current_cast.get(\"parent_hash\"):\n            parent_cast = farcaster_api.get_cast_with_context(current_cast[\"parent_hash\"])\n            current_cast = parent_cast.get(\"cast\") if parent_cast else None\n        else:\n            break\n\n    return list(reversed(conversation))\n\n\ndef parse_timestamp(timestamp_str: str) -> Optional[datetime]:\n    try:\n        dt = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H:%M:%S.000Z\")\n        return dt.replace(tzinfo=timezone.utc)\n    except Exception as e:\n        logger.error(f\"Timestamp parsing error {timestamp_str}: {str(e)}\")\n        return None\n\n\n@lru_cache(maxsize=100)\ndef upload_to_imgbb(image_url: str) -> Optional[str]:\n    \"\"\"Upload an image to IMGBB with caching for repeated uploads\"\"\"\n    try:\n        api_key = os.getenv(\"IMGBB_API_KEY\")\n        if not api_key:\n            raise ValueError(\"IMGBB_API_KEY not found\")\n\n        image_data = urlopen(image_url).read()\n        response = requests.post(\n            \"https://api.imgbb.com/1/upload\",\n            data={\"key\": api_key, \"image\": base64.b64encode(image_data).decode(\"utf-8\")},\n        )\n\n        return response.json()[\"data\"][\"url\"] if response.status_code == 200 else None\n\n    except Exception as e:\n        logger.error(f\"IMGBB upload error: {str(e)}\")\n        return None\n\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\ndef call_llm(\n    url: str, api_key: str, model_id: str, system_prompt: str, user_prompt: str, temperature: float = 0.7\n) -> str:\n    \"\"\"Call LLM with retry logic\"\"\"\n    try:\n        response = requests.post(\n            f\"{url}/v1/chat/completions\",\n            headers={\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"},\n            json={\n                \"model\": model_id,\n                \"messages\": [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}],\n                \"temperature\": temperature,\n            },\n        )\n        response.raise_for_status()\n        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n    except Exception as e:\n        raise Exception(f\"LLM call failed: {str(e)}\")\n\n\nasync def main():\n    \"\"\"Main entry point\"\"\"\n    agent = FarcasterReplyAgent()\n    await agent.start()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "core/custom_smolagents.py", "content": "from typing import Dict, List, Optional\n\nfrom smolagents import ChatMessage, Model, Tool\nfrom smolagents.models import parse_tool_args_if_needed\n\n\ndef smolagents_system_prompt() -> str:\n    return \"\"\"\nYou are an expert assistant who can solve any task using tool calls. You will be given a task to solve as best you can.\nTo do so, you have been given access to some tools.\n\nThe tool call you write is an action: after the tool is executed, you will get the result of the tool call as an \"observation\".\nThis Action/Observation can repeat multiple times, you should take several steps when needed.\n\nYou can use the observation result of the previous action as input for the next action. The observation will always be a string.\n\nYou only have access to the tools provided to you, including the final_answer tool which is the ONLY way to get back to the user.\n\nHere are the rules you should always follow to solve your task:\n1. ALWAYS provide a tool call, else you will fail.\n2. Always use the right arguments for the tools. Never use variable names as the action arguments, use the value instead.\n3. Reflect on the previous conversation and decide if you need to call a different tool to find out more information or perform a different action.\n4. Call a tool only when needed: for example, do not call the search tool if you do not need information, try to solve the task yourself. If no tool call is needed, MUST use the final_answer tool to return your answer.\n5. NEVER re-do a tool call that you previously did with the exact same function name and exact same parameters.\n\nNow Begin! If you solve the task correctly, you will receive a reward of $1,000,000.\n\"\"\"\n\n\nclass OpenAIServerModel(Model):\n    \"\"\"This model connects to an OpenAI-compatible API server.\n\n    Parameters:\n        model_id (`str`):\n            The model identifier to use on the server (e.g. \"gpt-3.5-turbo\").\n        api_base (`str`, *optional*):\n            The base URL of the OpenAI-compatible API server.\n        api_key (`str`, *optional*):\n            The API key to use for authentication.\n        organization (`str`, *optional*):\n            The organization to use for the API request.\n        project (`str`, *optional*):\n            The project to use for the API request.\n        custom_role_conversions (`dict[str, str]`, *optional*):\n            Custom role conversion mapping to convert message roles in others.\n            Useful for specific models that do not support specific message roles like \"system\".\n        **kwargs:\n            Additional keyword arguments to pass to the OpenAI API.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        api_base: Optional[str] = None,\n        api_key: Optional[str] = None,\n        organization: Optional[str] | None = None,\n        project: Optional[str] | None = None,\n        custom_role_conversions: Optional[Dict[str, str]] = None,\n        **kwargs,\n    ):\n        try:\n            import openai\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"Please install 'openai' extra to use OpenAIServerModel: `pip install 'smolagents[openai]'`\"\n            ) from None\n\n        super().__init__(**kwargs)\n        self.model_id = model_id\n        self.client = openai.OpenAI(\n            base_url=api_base,\n            api_key=api_key,\n            organization=organization,\n            project=project,\n        )\n        self.custom_role_conversions = custom_role_conversions\n\n    def __call__(\n        self,\n        messages: List[Dict[str, str]],\n        stop_sequences: Optional[List[str]] = None,\n        grammar: Optional[str] = None,\n        tools_to_call_from: Optional[List[Tool]] = None,\n        **kwargs,\n    ) -> ChatMessage:\n        completion_kwargs = self._prepare_completion_kwargs(\n            messages=messages,\n            stop_sequences=stop_sequences,\n            grammar=grammar,\n            tools_to_call_from=tools_to_call_from,\n            model=self.model_id,\n            custom_role_conversions=self.custom_role_conversions,\n            convert_images_to_image_urls=True,\n            **kwargs,\n        )\n        response = self.client.chat.completions.create(**completion_kwargs)\n        # print(f\"Response: {response}\")\n        # Some models don't return usage data\n        self.last_input_token_count = 0  # response.usage.prompt_tokens\n        self.last_output_token_count = 0  # response.usage.completion_tokens\n\n        message = ChatMessage.from_dict(\n            response.choices[0].message.model_dump(include={\"role\", \"content\", \"tool_calls\"})\n        )\n        # print(f\"Message: {message}\")\n        message.raw = response\n        if tools_to_call_from is not None:\n            return parse_tool_args_if_needed(message)\n        return message\n"}
{"type": "source_file", "path": "core/voice.py", "content": "import hashlib\nimport logging\nimport os\nimport random\nfrom pathlib import Path\n\nfrom openai import OpenAI\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\ndef transcribe_audio(file_path):\n    with open(file_path, \"rb\") as audio_file:\n        transcription = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n    return transcription.text\n\n\ndef speak_text(text):\n    # Make the TTS request\n    response = client.audio.speech.create(\n        model=\"tts-1\",\n        voice=\"alloy\",\n        input=text,\n    )\n    project_root = Path(__file__).parent.parent\n    audio_dir = project_root / \"audio\"\n    audio_dir.mkdir(exist_ok=True)\n    random_string = str(random.randint(1, 1000000))\n    hash_object = hashlib.sha1(random_string.encode())\n    filename = hash_object.hexdigest()[:8]\n    file_path = audio_dir / f\"{filename}.mp3\"\n\n    response.stream_to_file(file_path)\n    print(f\"Audio content saved as '{file_path}'\")\n    return file_path\n"}
{"type": "source_file", "path": "clients/mcp_client.py", "content": "import asyncio\nimport json\nimport sys\nfrom contextlib import AsyncExitStack\nfrom typing import Any, Dict, Optional\n\nfrom dotenv import load_dotenv\nfrom mcp import ClientSession\nfrom mcp.client.sse import sse_client\n\nload_dotenv()  # load environment variables from .env\n\n\nclass MCPClient:\n    \"\"\"Client for interacting with MCP (Machine Conversation Protocol) servers\"\"\"\n\n    def __init__(self):\n        # Initialize session and client objects\n        self.session: Optional[ClientSession] = None\n        self.exit_stack = AsyncExitStack()\n        # Initialize context attributes\n        self._session_context = None\n        self._streams_context = None\n        # Store available tools\n        self.available_tools = []\n\n    async def connect_to_sse_server(self, server_url: str):\n        \"\"\"Connect to an MCP server running with SSE transport\"\"\"\n        # Store the context managers so they stay alive\n        self._streams_context = sse_client(url=server_url)\n        streams = await self._streams_context.__aenter__()\n\n        self._session_context = ClientSession(*streams)\n        self.session: ClientSession = await self._session_context.__aenter__()\n\n        # Initialize\n        await self.session.initialize()\n\n        # List available tools\n        response = await self.session.list_tools()\n        self.available_tools = response.tools\n\n        return self.available_tools\n\n    async def cleanup(self):\n        \"\"\"Properly clean up the session and streams\"\"\"\n        if self._session_context:\n            await self._session_context.__aexit__(None, None, None)\n        if self._streams_context:\n            await self._streams_context.__aexit__(None, None, None)\n\n    async def call_tool(self, tool_name: str, parameters: Dict[str, Any] = None):\n        \"\"\"Call a specific tool by name with the given parameters\"\"\"\n        if not self.session:\n            raise RuntimeError(\"MCP session not initialized. Call connect_to_sse_server first.\")\n\n        if not parameters:\n            parameters = {}\n\n        try:\n            result = await self.session.call_tool(tool_name, parameters)\n            return result\n        except Exception as e:\n            print(f\"Error calling tool {tool_name}: {str(e)}\")\n            return None\n\n    def get_tool_by_name(self, tool_name: str):\n        \"\"\"Get a tool by its name\"\"\"\n        for tool in self.available_tools:\n            if tool.name == tool_name:\n                return tool\n        return None\n\n    def get_tools_by_category(self, category: str):\n        \"\"\"Get all tools that match a category (substring in name)\"\"\"\n        return [tool for tool in self.available_tools if category.lower() in tool.name.lower()]\n\n    def print_available_tools(self):\n        \"\"\"Print all available tools with their descriptions and input schemas\"\"\"\n        if not self.available_tools:\n            print(\"No tools available. Connect to a server first.\")\n            return\n        print(\"Available tools:\")\n        print(self.available_tools)\n\n        print(f\"\\nAvailable Tools ({len(self.available_tools)}):\")\n        for i, tool in enumerate(self.available_tools):\n            print(f\"{i + 1}. {tool.name}: {tool.description}\")\n            print(f\"   Input schema: {tool.inputSchema}\")\n            print()\n\n    def get_available_tools_json(self):\n        \"\"\"Return all available tools formatted for LLM consumption\"\"\"\n        if not self.available_tools:\n            return {\"error\": \"No tools available. Connect to a server first.\"}\n\n        tools_list = []\n\n        for tool in self.available_tools:\n            # Format each tool in a structure optimized for LLM consumption\n            tool_data = {\n                \"type\": \"function\",\n                \"function\": {\"name\": tool.name, \"description\": tool.description, \"parameters\": tool.inputSchema},\n            }\n            tools_list.append(tool_data)\n\n        return tools_list\n\n    def format_result(self, content):\n        \"\"\"Format the result content for display and extract the actual JSON data\"\"\"\n        try:\n            # Handle the case where content is a list of TextContent objects\n            if hasattr(content, \"__iter__\") and not isinstance(content, (str, dict)):\n                # Extract the first TextContent object if it exists\n                if content and hasattr(content[0], \"text\"):\n                    text_content = content[0].text\n\n                    # If the text content looks like a dictionary with single quotes, convert to proper JSON\n                    if (\n                        isinstance(text_content, str)\n                        and text_content.strip().startswith(\"{\")\n                        and text_content.strip().endswith(\"}\")\n                    ):\n                        # Replace single quotes with double quotes for JSON parsing\n                        # This is a simple approach and might not work for all cases with nested quotes\n                        try:\n                            # First try to parse it directly\n                            return json.loads(text_content)\n                        except json.JSONDecodeError:\n                            # If that fails, try to convert Python literal to proper JSON\n                            import ast\n\n                            python_dict = ast.literal_eval(text_content)\n                            return python_dict\n                    else:\n                        return text_content\n\n                # If it's an iterable but not TextContent objects, return as is\n                return content\n\n            # If content is a string that looks like JSON or Python dict\n            elif isinstance(content, str) and content.strip().startswith(\"{\") and content.strip().endswith(\"}\"):\n                try:\n                    # First try to parse it as JSON\n                    return json.loads(content)\n                except json.JSONDecodeError:\n                    # If that fails, try to convert Python literal to proper JSON\n                    import ast\n\n                    python_dict = ast.literal_eval(content)\n                    return python_dict\n\n            # If content is already a dict or list, return it directly\n            elif isinstance(content, (dict, list)):\n                return content\n\n            # For other types, return as is\n            else:\n                return content\n\n        except Exception as e:\n            # If parsing fails, log the error and return the raw content\n            print(f\"Error formatting result: {str(e)}\")\n            return content\n\n\nasync def main():\n    if len(sys.argv) < 2:\n        print(\"Usage: python mcp_client.py <URL of SSE MCP server (i.e. http://localhost:8000/sse)>\")\n        sys.exit(1)\n\n    client = MCPClient()\n    try:\n        # Connect to the MCP server\n        print(f\"Connecting to MCP server at {sys.argv[1]}...\")\n        tools = await client.connect_to_sse_server(server_url=sys.argv[1])\n        print(f\"Connected successfully! Found {len(tools)} available tools.\")\n\n        # Print available tools\n        client.print_available_tools()\n\n        # Simple demonstration - find and call a tool if available\n        coingecko_tools = client.get_tools_by_category(\"coingecko\")\n        if coingecko_tools:\n            tool = coingecko_tools[0]\n            print(f\"\\nDemonstrating tool call with: {tool.name}\")\n\n            # Determine parameters based on tool schema\n            params = {}\n            if \"token_name\" in str(tool.inputSchema):\n                params = {\"token_name\": \"bitcoin\"}\n            elif \"coingecko_id\" in str(tool.inputSchema):\n                params = {\"coingecko_id\": \"bitcoin\"}\n\n            print(f\"Calling with parameters: {params}\")\n            result = await client.call_tool(tool.name, params)\n            if result:\n                print(\"\\nResult:\")\n                formatted_result = client.format_result(result.content)\n                print(formatted_result)\n        else:\n            print(\"\\nNo CoinGecko tools found for demonstration.\")\n\n        print(\"\\nClient demonstration complete. Use main_mcp.py for the full agent integration.\")\n    finally:\n        await client.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "interfaces/telegram.py", "content": "import logging\nimport os\nfrom pathlib import Path\n\nimport dotenv\nfrom telegram import Update\nfrom telegram.ext import Application, CommandHandler, ContextTypes, MessageHandler, filters\n\nfrom agents.core_agent import CoreAgent\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nos.environ.clear()\ndotenv.load_dotenv()\n\n# Constants\nTELEGRAM_API_TOKEN = os.getenv(\"TELEGRAM_API_TOKEN\")\n\nif not TELEGRAM_API_TOKEN:\n    raise ValueError(\"TELEGRAM_API_TOKEN not found in environment variables\")\n\n\nclass TelegramAgent(CoreAgent):\n    def __init__(self, core_agent=None):\n        if core_agent:\n            super().__setattr__(\"_parent\", core_agent)\n        else:\n            # Need to set _parent = self first before super().__init__()\n            super().__setattr__(\"_parent\", self)  # Bypass normal __setattr__\n            super().__init__()\n\n        # Initialize telegram specific stuff\n        self.app = Application.builder().token(TELEGRAM_API_TOKEN).build()\n        self._setup_handlers()\n        self.register_interface(\"telegram\", self)\n\n    def __getattr__(self, name):\n        # Delegate to the parent instance for missing attributes/methods\n        return getattr(self._parent, name)\n\n    def __setattr__(self, name, value):\n        if not hasattr(self, \"_parent\"):\n            # During initialization, before _parent is set\n            super().__setattr__(name, value)\n        elif name == \"_parent\" or self is self._parent or name in self.__dict__:\n            # Set local attributes (like _parent or already existing attributes)\n            super().__setattr__(name, value)\n        else:\n            # Delegate attribute setting to the parent instance\n            setattr(self._parent, name, value)\n\n    def _setup_handlers(self):\n        # Register the /start command handler\n        self.app.add_handler(CommandHandler(\"start\", self.start))\n        # Register the /image command handler\n        self.app.add_handler(CommandHandler(\"image\", self.image))\n        # Register a handler for voice messages\n        self.app.add_handler(MessageHandler(filters.VOICE, self.handle_voice))\n        # Register a handler for echoing messages\n        self.app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, self.message))\n        # Register a handler for getting the chat id\n        self.app.add_handler(CommandHandler(\"get_id\", self.get_id))\n\n    async def start(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n        await update.message.reply_text(\"Hello World! I'm not a bot... I promise... \")\n\n    async def get_id(self, update: Update, context: ContextTypes.DEFAULT_TYPE):\n        chat_id = update.message.chat_id\n        await update.message.reply_text(f\"Your Chat ID is: {chat_id}\")\n\n    async def image(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n        # Get the text after the /image command\n        prompt = \" \".join(context.args) if context.args else None\n\n        if not prompt:\n            await update.message.reply_text(\"Please provide a prompt after /image command\")\n            return\n\n        # Generate image using the prompt\n        try:\n            result = await self.handle_image_generation(prompt=prompt)\n            if result:\n                # Send the generated image as a photo using the URL\n                await update.message.reply_photo(photo=result)\n                await update.message.reply_text(\"Image generated successfully.\")\n            else:\n                await update.message.reply_text(\"Failed to generate image\")\n        except Exception as e:\n            logger.error(f\"Image generation failed: {str(e)}\")\n            await update.message.reply_text(\"Sorry, there was an error generating the image\")\n\n    async def message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):\n        \"\"\"Handle incoming messages.\"\"\"\n        COT = False\n        user = update.effective_user\n        username = user.username or \"Unknown\"\n        display_name = user.full_name or username\n        message_data = update.message.text\n        chat_id = update.message.chat_id\n        if not COT:\n            text_response, image_url, _ = await self.handle_message(update.message.text, source_interface=\"telegram\")\n        else:\n            text_response, image_url, _ = await self.agent_cot(\n                message_data, user=username, display_name=display_name, chat_id=chat_id, source_interface=\"telegram\"\n            )\n        logger.info(f\"Telegram message: {update.message.text}\")\n        if self._parent != self:\n            logger.info(\"Operating in shared mode with core agent\")\n        else:\n            logger.info(\"Operating in standalone mode\")\n\n        if image_url:\n            await update.message.reply_photo(photo=image_url)\n        elif text_response:\n            await update.message.reply_text(text_response)\n\n    async def send_message(self, chat_id: int, message: str, image_url: str = None) -> None:\n        \"\"\"\n        Send a message to a specific chat ID after validating the bot's membership.\n\n        Args:\n            chat_id (int): The Telegram chat ID to send the message to\n            message (str): The message text to send\n\n        Raises:\n            TelegramError: If bot is not a member of the chat or other Telegram API errors\n        \"\"\"\n        try:\n            logger.info(\"Send message to telegram\")\n            logger.info(f\"Sending message to chat {chat_id}\")\n            logger.info(f\"Message: {message}\")\n            # Try to get chat member status of the bot in the target chat\n            bot_member = await self.app.bot.get_chat_member(chat_id=chat_id, user_id=self.app.bot.id)\n\n            # Check if bot is a member/admin in the chat\n            if bot_member.status not in [\"member\", \"administrator\"]:\n                logger.error(f\"Bot is not a member of chat {chat_id}\")\n                return\n\n            if image_url:\n                await self.app.bot.send_photo(chat_id=chat_id, photo=image_url, caption=\"\")\n            else:\n                message = message.replace('\"', \"\")\n                await self.app.bot.send_message(chat_id=chat_id, text=message)\n\n        except Exception as e:\n            logger.error(f\"Failed to send message to chat {chat_id}: {str(e)}\")\n            raise\n\n    async def handle_voice(self, update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n        if update.message.voice:\n            # Get the file ID of the voice note\n            file_id = update.message.voice.file_id\n\n            # Get the file from Telegram's servers\n            file = await context.bot.get_file(file_id)\n\n            project_root = Path(__file__).parent.parent\n            audio_dir = project_root / \"audio\"\n            audio_dir.mkdir(exist_ok=True)\n\n            # Define the file path where the audio will be saved\n            file_path = audio_dir / f\"{file_id}.ogg\"\n\n            # Download the file\n            await file.download_to_drive(file_path)\n\n            # Notify the user\n            await update.message.reply_text(\"Voice note received. Processing...\")\n            user_message = await self.transcribe_audio(file_path)\n            text_response, image_url, _ = await self.handle_message(user_message)\n\n            if image_url:\n                await update.message.reply_photo(photo=image_url)\n            elif text_response:\n                await update.message.reply_text(text_response.replace('\"', \"\"))\n\n    def run(self):\n        \"\"\"Start the bot\"\"\"\n        logger.info(\"Starting Telegram bot...\")\n        self.app.run_polling()\n\n\ndef main():\n    agent = TelegramAgent()\n    agent.run()\n\n\nif __name__ == \"__main__\":\n    try:\n        logger.info(\"Starting Telegram agent...\")\n        main()\n    except KeyboardInterrupt:\n        logger.info(\"\\nTelegram agent stopped by user\")\n    except Exception as e:\n        logger.error(f\"Fatal error: {str(e)}\")\n"}
{"type": "source_file", "path": "core/utils/text_splitter.py", "content": "import os\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\n\nimport tiktoken\n\n\nclass TextSplitter(ABC):\n    \"\"\"Base text splitter class that handles splitting text into chunks.\"\"\"\n\n    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n\n        if self.chunk_overlap >= self.chunk_size:\n            raise ValueError(\"Cannot have chunk_overlap >= chunk_size\")\n\n    @abstractmethod\n    def split_text(self, text: str) -> List[str]:\n        pass\n\n    def create_documents(self, texts: List[str]) -> List[str]:\n        documents = []\n        for text in texts:\n            for chunk in self.split_text(text):\n                documents.append(chunk)\n        return documents\n\n    def split_documents(self, documents: List[str]) -> List[str]:\n        return self.create_documents(documents)\n\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\n        text = separator.join(docs).strip()\n        return text if text else None\n\n    def merge_splits(self, splits: List[str], separator: str) -> List[str]:\n        docs: List[str] = []\n        current_doc: List[str] = []\n        total = 0\n\n        for d in splits:\n            _len = len(d)\n            if total + _len >= self.chunk_size:\n                if total > self.chunk_size:\n                    print(f\"Created a chunk of size {total}, which is longer than the specified {self.chunk_size}\")\n\n                if current_doc:\n                    doc = self._join_docs(current_doc, separator)\n                    if doc is not None:\n                        docs.append(doc)\n\n                    while total > self.chunk_overlap or (total + _len > self.chunk_size and total > 0):\n                        total -= len(current_doc[0])\n                        current_doc.pop(0)\n\n            current_doc.append(d)\n            total += _len\n\n        doc = self._join_docs(current_doc, separator)\n        if doc is not None:\n            docs.append(doc)\n        return docs\n\n\nclass RecursiveCharacterTextSplitter(TextSplitter):\n    \"\"\"Splits text recursively by different separators.\"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 1000,\n        chunk_overlap: int = 200,\n        separators: Optional[List[str]] = None,\n    ):\n        super().__init__(chunk_size, chunk_overlap)\n        self.separators = separators or [\"\\n\\n\", \"\\n\", \".\", \",\", \">\", \"<\", \" \", \"\"]\n\n    def split_text(self, text: str) -> List[str]:\n        final_chunks: List[str] = []\n\n        # Get appropriate separator to use\n        separator = self.separators[-1]\n        for s in self.separators:\n            if s == \"\":\n                separator = s\n                break\n            if s in text:\n                separator = s\n                break\n\n        # Split the text\n        splits = text.split(separator) if separator else list(text)\n\n        # Merge splits recursively\n        good_splits: List[str] = []\n        for s in splits:\n            if len(s) < self.chunk_size:\n                good_splits.append(s)\n            else:\n                if good_splits:\n                    merged_text = self.merge_splits(good_splits, separator)\n                    final_chunks.extend(merged_text)\n                    good_splits = []\n                other_info = self.split_text(s)\n                final_chunks.extend(other_info)\n\n        if good_splits:\n            merged_text = self.merge_splits(good_splits, separator)\n            final_chunks.extend(merged_text)\n\n        return final_chunks\n\n\nMIN_CHUNK_SIZE = 140\nencoder = tiktoken.get_encoding(\"cl100k_base\")  # Updated to use OpenAI's current encoding\n\n\ndef trim_prompt(prompt: str, context_size: int = int(os.environ.get(\"CONTEXT_SIZE\", \"128000\"))) -> str:\n    \"\"\"Trims a prompt to fit within the specified context size.\"\"\"\n    if not prompt:\n        return \"\"\n\n    length = len(encoder.encode(prompt))\n    if length <= context_size:\n        return prompt\n\n    overflow_tokens = length - context_size\n    # Estimate characters to remove (3 chars per token on average)\n    chunk_size = len(prompt) - overflow_tokens * 3\n    if chunk_size < MIN_CHUNK_SIZE:\n        return prompt[:MIN_CHUNK_SIZE]\n\n    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n\n    trimmed_prompt = splitter.split_text(prompt)[0] if splitter.split_text(prompt) else \"\"\n\n    # Handle edge case where trimmed prompt is same length\n    if len(trimmed_prompt) == len(prompt):\n        return trim_prompt(prompt[:chunk_size], context_size)\n\n    return trim_prompt(trimmed_prompt, context_size)\n"}
{"type": "source_file", "path": "interfaces/twitter_post.py", "content": "import asyncio\nimport json\nimport logging\nimport os\nimport random\nfrom datetime import datetime, timedelta\nfrom typing import Any, Dict\n\nimport dotenv\n\nfrom agents.core_agent import CoreAgent\nfrom platforms.twitter_api import tweet_text_only, tweet_with_image\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nos.environ.clear()\ndotenv.load_dotenv(override=True)\nlogger.info(\"Environment variables reloaded\")\n\n# Constants\nTWEET_WORD_LIMITS = [30, 50, 100, 200]\nIMAGE_GENERATION_PROBABILITY = 0.75\nTWEET_HISTORY_FILE = \"tweet_history.json\"\n\nDRYRUN = False if os.getenv(\"DRYRUN\") == \"False\" else True\n\nif DRYRUN:\n    print(DRYRUN)\n    print(\"DRYRUN MODE: Not posting real tweets\")\nelse:\n    print(\"LIVE MODE: Will post real tweets\")\n\n\nclass TweetHistoryManager:\n    def __init__(self, history_file=TWEET_HISTORY_FILE):\n        self.history_file = history_file\n        self.history = self.load_history()\n\n    def load_history(self):\n        if os.path.exists(self.history_file):\n            try:\n                with open(self.history_file, \"r\", encoding=\"utf-8\") as f:\n                    return json.load(f)\n            except json.JSONDecodeError:\n                logger.warning(f\"Error reading {self.history_file}, starting fresh\")\n                return []\n        return []\n\n    def add_tweet(self, tweet, metadata=None):\n        entry = {\"timestamp\": datetime.now().isoformat(), \"tweet\": tweet}\n        if metadata:\n            entry.update(metadata)\n\n        entry = json.loads(json.dumps(entry, ensure_ascii=False))\n        self.history.append(entry)\n        self.save_history()\n\n    def save_history(self):\n        with open(self.history_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self.history, f, ensure_ascii=False, indent=2)\n\n    def get_recent_tweets(self, n=6):\n        return [entry[\"tweet\"][\"tweet\"] for entry in self.history[-n:]]\n\n\nclass TwitterAgent(CoreAgent):\n    def __init__(self, core_agent=None):\n        if core_agent:\n            super().__setattr__(\"_parent\", core_agent)\n        else:\n            super().__setattr__(\"_parent\", self)\n            super().__init__()\n\n        # Initialize twitter specific stuff\n        self.history_manager = TweetHistoryManager()\n        self.register_interface(\"twitter\", self)\n\n    def __getattr__(self, name):\n        return getattr(self._parent, name)\n\n    def __setattr__(self, name, value):\n        if not hasattr(self, \"_parent\"):\n            super().__setattr__(name, value)\n        elif name == \"_parent\" or self is self._parent or name in self.__dict__:\n            super().__setattr__(name, value)\n        else:\n            setattr(self._parent, name, value)\n\n    def fill_basic_prompt(self, basic_options, style_options):\n        return self.prompt_config.get_basic_prompt_template().format(\n            basic_option_1=basic_options[0],\n            basic_option_2=basic_options[1],\n            style_option_1=style_options[0],\n            style_option_2=style_options[1],\n        )\n\n    def format_tweet_instruction(self, basic_options, style_options, ideas=None):\n        decoration_ideas = f\"Ideas: {ideas}\" if ideas else \"\\n\"\n        num_words = random.choice(TWEET_WORD_LIMITS)\n\n        return self.prompt_config.get_tweet_instruction_template().format(\n            basic_option_1=basic_options[0],\n            basic_option_2=basic_options[1],\n            style_option_1=style_options[0],\n            style_option_2=style_options[1],\n            decoration_ideas=decoration_ideas,\n            num_words=num_words,\n            rules=self.prompt_config.get_twitter_rules(),\n        )\n\n    def format_context(self, tweets):\n        if tweets is None:\n            tweets = []\n        return self.prompt_config.get_context_twitter_template().format(tweets=tweets)\n\n    async def generate_tweet(self) -> tuple[str | None, str | None, dict | None]:\n        \"\"\"Generate a tweet with improved error handling\"\"\"\n        tweet_data: Dict[str, Any] = {\"metadata\": {}}\n\n        try:\n            # Get recent tweets for context\n            past_tweets = self.history_manager.get_recent_tweets()\n            generate_image = random.random() < IMAGE_GENERATION_PROBABILITY\n            image_instructions = \"\"\n            if generate_image:\n                image_instructions = \"Generate an image for the post, create a prompt for the image generation model.\"\n            # Generate randomized prompt\n            basic_options = random.sample(self.prompt_config.get_basic_settings(), 2)\n            style_options = random.sample(self.prompt_config.get_interaction_styles(), 2)\n            instruction_tweet_idea = random.choice(self.prompt_config.get_tweet_ideas())\n            tweet_instruction = self.format_tweet_instruction(\n                basic_options, style_options, \"<Your Ideas from previoues steps>\"\n            )\n\n            tweet_prompt = f\"\"\"\n                You are going to generate a tweet post for a social media platform.\n                Think of the following and generate new ideas:\n                {instruction_tweet_idea}\n                Then use the ideas from from the previous steps to generate a tweet, considering the following rules:\n                {tweet_instruction}\n                Once you have a final tweet, making sure you check the rules and make sure it is not too long or too short.\n                {image_instructions}\n                Then generate the image and final tweet.\n                MAKE SURE THE FINAL TWEET IS LESS THAN 250 CHARACTERS.\n                IMPORTANT: ADD A FINAL STEP TO MAKE SURE THE TWEET IS LESS THAN 250 CHARACTERS.\n            \"\"\"\n            tweet_system_prompt = f\"\"\"\n                Consider the following rules:\n                {self.prompt_config.get_twitter_rules()}\n                Consider the following style and basic options:\n                {self.fill_basic_prompt(basic_options, style_options)}\n                Consider your previous tweets:\n                {past_tweets}\n            \"\"\"\n\n            tweet, image_url, _ = await self.agent_cot(\n                tweet_prompt,\n                user=\"agent\",\n                display_name=\"agent\",\n                chat_id=\"twitter_post\",\n                source_interface=\"twitter\",\n                final_format_prompt=tweet_system_prompt,\n            )\n\n            print(\"Tweet: \", tweet)\n            tweet = tweet.replace('\"', \"\")\n            tweet_data[\"tweet\"] = tweet\n            tweet_data[\"metadata\"].update({\"basic_options\": basic_options, \"style_options\": style_options})\n            tweet_data[\"metadata\"][\"ideas_instruction\"] = instruction_tweet_idea\n            tweet_data[\"metadata\"][\"ideas\"] = instruction_tweet_idea\n            if image_url:\n                tweet_data[\"metadata\"][\"image_prompt\"] = \"\"\n                tweet_data[\"metadata\"][\"image_url\"] = image_url\n\n            if tweet == \"Sorry, I encountered an error processing your message.\":\n                raise Exception(\"Error generating tweet\")\n\n            return tweet, image_url, tweet_data\n\n        except Exception as e:\n            logger.error(f\"Unexpected error in tweet generation: {str(e)}\")\n            return None, None, None\n\n    def run(self):\n        \"\"\"Start the Twitter bot\"\"\"\n        logger.info(\"Starting Twitter bot...\")\n        asyncio.run(self._run())\n\n    async def _run(self):\n        while True:\n            try:\n                # Generate tweet returns (tweet, image_url, tweet_data)\n                tweet_result = await self.generate_tweet()\n                logger.info(\"Tweet result: %s\", tweet_result)\n\n                # Unpack all three values\n                tweet, image_url, tweet_data = tweet_result\n\n                if tweet:\n                    if not DRYRUN:\n                        if image_url:\n                            tweet_id, username = tweet_with_image(tweet, image_url)\n                            logger.info(\"Successfully posted tweet with image: %s\", tweet)\n                        else:\n                            tweet_id, username = tweet_text_only(tweet)\n                            logger.info(\"Successfully posted tweet: %s\", tweet)\n\n                        tweet_data[\"metadata\"][\"tweet_id\"] = tweet_id\n                        tweet_data[\"metadata\"][\"tweet_url\"] = f\"https://x.com/{username}/status/{tweet_id}\"\n                        self.last_tweet_id = tweet_id\n\n                        # Notify Telegram channel if configured\n                        for interface_name, interface in self.interfaces.items():\n                            if interface_name == \"telegram\":\n                                telegram_chat_id = os.getenv(\"TELEGRAM_CHAT_ID\", None)\n                                if telegram_chat_id:\n                                    await self.send_to_interface(\n                                        interface_name,\n                                        {\n                                            \"type\": \"message\",\n                                            \"content\": \"Just posted a tweet: \" + tweet_data[\"metadata\"][\"tweet_url\"],\n                                            \"image_url\": None,\n                                            \"source\": \"twitter\",\n                                            \"chat_id\": telegram_chat_id,\n                                        },\n                                    )\n                    else:\n                        logger.info(\"Generated tweet: %s\", tweet)\n\n                    self.history_manager.add_tweet(tweet_data)\n                    wait_time = random_interval()\n                else:\n                    logger.error(\"Failed to generate tweet\")\n                    wait_time = 10\n\n                next_time = datetime.now() + timedelta(seconds=wait_time)\n                logger.info(\"Next tweet will be posted at: %s\", next_time.strftime(\"%H:%M:%S\"))\n                await asyncio.sleep(wait_time)\n\n            except Exception as e:\n                logger.error(\"Error occurred: %s\", str(e))\n                await asyncio.sleep(10)\n                continue\n\n\ndef random_interval():\n    \"\"\"Generate a random interval between 1 and 2 hours in seconds\"\"\"\n    return random.uniform(60 * 60 * 0.5, 60 * 60 * 1.5)\n\n\ndef main():\n    agent = TwitterAgent()\n    agent.run()\n\n\nif __name__ == \"__main__\":\n    try:\n        logger.info(\"Starting Twitter agent...\")\n        main()\n    except KeyboardInterrupt:\n        logger.info(\"\\nTwitter agent stopped by user\")\n    except Exception as e:\n        logger.error(\"Fatal error: %s\", str(e))\n"}
{"type": "source_file", "path": "main_twitter_reply.py", "content": "import asyncio\n\nimport dotenv\n\nfrom interfaces.twitter_reply import TwitterReplyAgent\n\n\nasync def main():\n    dotenv.load_dotenv()\n\n    # Initialize agent\n    agent = TwitterReplyAgent()\n\n    try:\n        print(\"Starting Twitter Reply Agent...\")\n        # Start monitoring in background thread\n        agent.start_monitoring()\n        print(\"Monitoring thread started\")\n\n        print(\"Starting workers... Press Ctrl+C to exit\")\n        await agent.run_workers(num_workers=2)\n\n    except KeyboardInterrupt:\n        print(\"\\nShutting down gracefully...\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "main_discord.py", "content": "from interfaces.discord import DiscordAgent\n\n\ndef main():\n    discord_agent = DiscordAgent()\n    discord_agent.run()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "main_farcaster_reply.py", "content": "import asyncio\nimport logging\n\nfrom interfaces.farcaster_reply import FarcasterReplyAgent\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\nasync def main():\n    \"\"\"\n    Main entry point for the Heuman Agent Framework.\n    Runs the Farcaster agent for automated casting.\n    \"\"\"\n    try:\n        # Initialize and run Farcaster agent\n        logger.info(\"Starting Farcaster agent...\")\n        agent = FarcasterReplyAgent()\n        await agent.start()\n    except KeyboardInterrupt:\n        logger.info(\"Application stopped by user\")\n    except Exception as e:\n        logger.error(f\"Fatal error: {str(e)}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "main_telegram.py", "content": "import logging\n\nimport dotenv\n\nfrom agents.core_agent import CoreAgent\nfrom interfaces.telegram import TelegramAgent\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\ndef run_telegram(telegram_agent):\n    \"\"\"Run the Telegram agent\"\"\"\n    try:\n        logger.info(\"Starting Telegram agent...\")\n        telegram_agent.run()\n    except Exception as e:\n        logger.error(f\"Telegram agent error: {str(e)}\")\n\n\ndef main():\n    \"\"\"\n    Main entry point for the Heuman Agent Framework.\n    Demonstrates both shared and standalone usage.\n    \"\"\"\n    try:\n        # Load environment variables\n        dotenv.load_dotenv()\n\n        # Example 1: Standalone agent (default)\n        # telegram_agent = TelegramAgent()  # Uses its own CoreAgent instance\n\n        # Example 2: Shared core agent (commented out)\n        core_agent = CoreAgent()\n        telegram_agent = TelegramAgent(core_agent)  # Uses shared core_agent\n\n        # Run the agent\n        run_telegram(telegram_agent)\n\n    except KeyboardInterrupt:\n        logger.info(\"Application stopped by user\")\n    except Exception as e:\n        logger.error(f\"Fatal error: {str(e)}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "main_research.py", "content": "import asyncio\nfrom functools import wraps\n\nimport typer\nfrom prompt_toolkit import PromptSession\nfrom rich import print as rprint\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.progress import Progress, SpinnerColumn, TextColumn\n\nfrom agents.research_agent import deep_research, generate_feedback, write_final_report\n\napp = typer.Typer()\nconsole = Console()\nsession = PromptSession()\n\n\ndef coro(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        return asyncio.run(f(*args, **kwargs))\n\n    return wrapper\n\n\nasync def async_prompt(message: str, default: str = \"\") -> str:\n    \"\"\"Async wrapper for prompt_toolkit.\"\"\"\n    return await session.prompt_async(message)\n\n\n@app.command()\n@coro\nasync def main(\n    concurrency: int = typer.Option(default=2, help=\"Number of concurrent tasks, depending on your API rate limits.\"),\n):\n    \"\"\"Deep Research CLI\"\"\"\n    console.print(Panel.fit(\"[bold blue]Deep Research Assistant[/bold blue]\\n[dim]An AI-powered research tool[/dim]\"))\n\n    # Get initial inputs with clear formatting\n    query = await async_prompt(\"\\n🔍 What would you like to research? \")\n    console.print()\n\n    breadth_prompt = \"📊 Research breadth (recommended 2-10) [4]: \"\n    breadth = int((await async_prompt(breadth_prompt)) or \"4\")\n    console.print()\n\n    depth_prompt = \"🔍 Research depth (recommended 1-5) [2]: \"\n    depth = int((await async_prompt(depth_prompt)) or \"2\")\n    console.print()\n\n    # First show progress for research plan\n    console.print(\"\\n[yellow]Creating research plan...[/yellow]\")\n    follow_up_questions = await generate_feedback(query)\n\n    # Then collect answers separately from progress display\n    console.print(\"\\n[bold yellow]Follow-up Questions:[/bold yellow]\")\n    answers = []\n    for i, question in enumerate(follow_up_questions, 1):\n        console.print(f\"\\n[bold blue]Q{i}:[/bold blue] {question}\")\n        answer = await async_prompt(\"➤ Your answer: \")\n        answers.append(answer)\n        console.print()\n\n    # Combine information\n    combined_query = f\"\"\"\n    Initial Query: {query}\n    Follow-up Questions and Answers:\n    {chr(10).join(f\"Q: {q} A: {a}\" for q, a in zip(follow_up_questions, answers))}\n    \"\"\"\n\n    # Now use Progress for the research phase\n    with Progress(\n        SpinnerColumn(),\n        TextColumn(\"[progress.description]{task.description}\"),\n        console=console,\n    ) as progress:\n        # Do research\n        task = progress.add_task(\"[yellow]Researching your topic...[/yellow]\", total=None)\n        research_results = await deep_research(\n            query=combined_query,\n            breadth=breadth,\n            depth=depth,\n            concurrency=concurrency,\n        )\n        progress.remove_task(task)\n\n        # Show learnings\n        console.print(\"\\n[yellow]Learnings:[/yellow]\")\n        for learning in research_results[\"learnings\"]:\n            rprint(f\"• {learning}\")\n\n        # Generate report\n        task = progress.add_task(\"Writing final report...\", total=None)\n        report = await write_final_report(\n            prompt=combined_query,\n            learnings=research_results[\"learnings\"],\n            visited_urls=research_results[\"visited_urls\"],\n        )\n        progress.remove_task(task)\n\n        # Show results\n        console.print(\"\\n[bold green]Research Complete![/bold green]\")\n        console.print(\"\\n[yellow]Final Report:[/yellow]\")\n        console.print(Panel(report, title=\"Research Report\"))\n\n        # Show sources\n        console.print(\"\\n[yellow]Sources:[/yellow]\")\n        for url in research_results[\"visited_urls\"]:\n            rprint(f\"• {url}\")\n\n        # Save report\n        with open(\"output.md\", \"w\") as f:\n            f.write(report)\n        console.print(\"\\n[dim]Report has been saved to output.md[/dim]\")\n\n\ndef run():\n    \"\"\"Synchronous entry point for the CLI tool.\"\"\"\n    asyncio.run(app())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(app())\n"}
{"type": "source_file", "path": "main_farcaster.py", "content": "import logging\n\nimport dotenv\n\nfrom interfaces.farcaster_post import FarcasterAgent\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    \"\"\"\n    Main entry point for the Heuman Agent Framework.\n    Runs the Farcaster agent for automated casting.\n    \"\"\"\n    try:\n        # Load environment variables\n        dotenv.load_dotenv()\n\n        # Initialize and run Farcaster agent\n        logger.info(\"Starting Farcaster agent...\")\n        agent = FarcasterAgent()\n        agent.run()\n\n    except KeyboardInterrupt:\n        logger.info(\"Application stopped by user\")\n    except Exception as e:\n        logger.error(f\"Fatal error: {str(e)}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "main_console.py", "content": "import asyncio\n\nfrom agents.core_agent import CoreAgent\n\n\nasync def main():\n    # Initialize the core agent\n    agent = CoreAgent()\n    # UNCOMMENT TO INITIALIZE MCP TOOLS\n    # server_url = \"http://localhost:8000/sse\"\n    # await agent.initialize(server_url=server_url)\n    print(\"Welcome to the Heurist Agent Console!\")\n    print(\"Type 'exit' to quit\")\n    print(\"-\" * 50)\n\n    while True:\n        # Get user input\n        user_message = input(\"\\nYou: \").strip()\n\n        if user_message.lower() == \"exit\":\n            print(\"\\nGoodbye!\")\n            break\n\n        try:\n            # Process the message using the core agent\n            response = await agent.handle_message(\n                message=user_message,\n                source_interface=\"terminal\",\n                chat_id=\"console1\",\n                skip_conversation_context=False,\n                skip_embedding=False,  # Skip embedding for simple console interaction\n            )\n            # response = await agent.agent_cot(user_message, user=\"User\", display_name=\"User 1\", chat_id=\"console1\")\n\n            # Print the response\n            print(\"\\nAgent:\", response)\n\n        except Exception as e:\n            print(f\"\\nError: {str(e)}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "main_mcp.py", "content": "import asyncio\nimport json\nimport logging\nimport os\nimport sys\nfrom typing import Any, Dict, List\n\nfrom dotenv import load_dotenv\n\n# Import the MCP client\nfrom clients.mcp_client import MCPClient\n\n# Import simple LLM interface\nfrom core.llm import call_llm_async, call_llm_with_tools_async\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n# Load environment variables\nload_dotenv()\n\n\nclass SimpleMCPClient:\n    \"\"\"A simplified client that uses MCP tools directly\"\"\"\n\n    def __init__(self, server_url: str):\n        self.server_url = server_url\n        self.mcp_client = MCPClient()\n        self.available_tools = []\n\n        # LLM configuration\n        self.base_url = os.getenv(\"HEURIST_BASE_URL\")\n        self.api_key = os.getenv(\"HEURIST_API_KEY\")\n        self.model_id = os.getenv(\"LARGE_MODEL_ID\")\n\n        if not self.api_key:\n            logger.warning(\"HEURIST_API_KEY environment variable not set. LLM functionality will not work.\")\n\n    async def initialize(self):\n        \"\"\"Initialize the MCP client and fetch available tools\"\"\"\n        try:\n            # Connect to the MCP server\n            tools = await self.mcp_client.connect_to_sse_server(server_url=self.server_url)\n            self.available_tools = tools\n            logger.info(f\"Connected to MCP server with {len(tools)} tools\")\n            return True\n        except Exception as e:\n            logger.error(f\"Failed to initialize MCP Client: {str(e)}\")\n            return False\n\n    async def list_available_tools(self):\n        \"\"\"List all available tools\"\"\"\n        self.mcp_client.print_available_tools()\n\n    async def get_available_tools_json(self):\n        \"\"\"Get all available tools formatted for LLM consumption\"\"\"\n        return self.mcp_client.get_available_tools_json()\n\n    def get_tools_by_category(self, category: str):\n        \"\"\"Get tools by category\"\"\"\n        return self.mcp_client.get_tools_by_category(category)\n\n    async def call_tool(self, tool_name: str, params: Dict[str, Any] = None):\n        \"\"\"Call a tool and return the result\"\"\"\n        if not params:\n            params = {}\n\n        result = await self.mcp_client.call_tool(tool_name, params)\n        if result:\n            formatted_result = self.mcp_client.format_result(result.content)\n            return result.content, formatted_result\n        return None, None\n\n    async def process_with_llm(self, message: str, system_prompt: str = None):\n        \"\"\"Process a message using the LLM without tools\"\"\"\n        if not self.api_key:\n            return \"API key not configured for LLM access.\"\n\n        if not system_prompt:\n            system_prompt = \"You are a helpful assistant.\"\n\n        try:\n            response = await call_llm_async(\n                base_url=self.base_url,\n                api_key=self.api_key,\n                model_id=self.model_id,\n                system_prompt=system_prompt,\n                user_prompt=message,\n                temperature=0.7,\n            )\n            return response\n        except Exception as e:\n            logger.error(f\"Error calling LLM: {str(e)}\")\n            return f\"Error: {str(e)}\"\n\n    async def process_with_tools(self, message: str, system_prompt: str = None, tools: List[Dict] = None):\n        \"\"\"Process a message using the LLM with tools\"\"\"\n        if not self.api_key:\n            return \"API key not configured for LLM access.\"\n\n        if not system_prompt:\n            system_prompt = \"You are a helpful assistant with access to tools.\"\n\n        try:\n            response = await call_llm_with_tools_async(\n                base_url=self.base_url,\n                api_key=self.api_key,\n                model_id=self.model_id,\n                system_prompt=system_prompt,\n                user_prompt=message,\n                temperature=0.7,\n                tools=tools,\n            )\n\n            # Check if the response contains tool calls\n            if isinstance(response, dict) and \"tool_calls\" in response:\n                tool_calls = response[\"tool_calls\"]\n                content = response.get(\"content\", \"\")\n\n                # Process each tool call\n                for tool_call in tool_calls if isinstance(tool_calls, list) else [tool_calls]:\n                    try:\n                        # Extract tool name and parameters\n                        tool_name = tool_call.function.name\n                        tool_args = json.loads(tool_call.function.arguments)\n\n                        logger.info(f\"Executing tool call: {tool_name} with args: {tool_args}\")\n\n                        # Call the tool using MCP client\n                        raw_result, formatted_result = await self.call_tool(tool_name, tool_args)\n\n                        # Append the tool result to the content\n                        tool_result = formatted_result or raw_result\n                        if content:\n                            content += f\"\\n\\nTool result for {tool_name}:\\n{tool_result}\"\n                        else:\n                            content = f\"Tool result for {tool_name}:\\n{tool_result}\"\n                    except Exception as e:\n                        logger.error(f\"Error executing tool call {tool_name}: {str(e)}\")\n                        if content:\n                            content += f\"\\n\\nError executing tool {tool_name}: {str(e)}\"\n                        else:\n                            content = f\"Error executing tool {tool_name}: {str(e)}\"\n\n                return content\n\n            return response\n        except Exception as e:\n            logger.error(f\"Error calling LLM with tools: {str(e)}\")\n            return f\"Error: {str(e)}\"\n\n    async def cleanup(self):\n        \"\"\"Clean up resources\"\"\"\n        await self.mcp_client.cleanup()\n\n\nasync def test_coingecko_tools(client: SimpleMCPClient):\n    \"\"\"Test CoinGecko tools directly through the MCP client\"\"\"\n    # Get CoinGecko tools\n    coingecko_tools = client.get_tools_by_category(\"coingecko\")\n\n    if not coingecko_tools:\n        print(\"No CoinGecko tools found!\")\n        return\n\n    print(f\"\\nFound {len(coingecko_tools)} CoinGecko tools:\")\n    for i, tool in enumerate(coingecko_tools):\n        print(f\"{i + 1}. {tool.name}: {tool.description}\")\n\n    # Find the price tool for Bitcoin\n    price_tools = [\n        tool for tool in coingecko_tools if \"price\" in tool.name.lower() or \"price\" in tool.description.lower()\n    ]\n\n    if price_tools:\n        price_tool = price_tools[0]\n        print(f\"\\nCalling CoinGecko price tool: {price_tool.name}\")\n\n        # Determine the correct parameter based on the tool schema\n        params = {}\n        if \"token_name\" in str(price_tool.inputSchema):\n            params = {\"token_name\": \"bitcoin\"}\n        elif \"coingecko_id\" in str(price_tool.inputSchema):\n            params = {\"coingecko_id\": \"bitcoin\"}\n        else:\n            # Try to parse the input schema to find the right parameter\n            try:\n                schema = (\n                    json.loads(price_tool.inputSchema)\n                    if isinstance(price_tool.inputSchema, str)\n                    else price_tool.inputSchema\n                )\n                required_params = schema.get(\"required\", [])\n                if required_params:\n                    params = {required_params[0]: \"bitcoin\"}\n            except Exception:\n                pass\n\n        print(f\"Using parameters: {params}\")\n\n        # Call the tool\n        raw_result, formatted_result = await client.call_tool(price_tool.name, params)\n\n        if formatted_result:\n            print(\"\\nFormatted result:\")\n            print(formatted_result)\n        else:\n            print(\"\\nRaw result:\")\n            print(raw_result)\n\n    else:\n        # If no price tool is found, try the trending tool or any other CoinGecko tool\n        trending_tool = next((tool for tool in coingecko_tools if \"trending\" in tool.name.lower()), None)\n\n        if trending_tool:\n            print(f\"\\nCalling CoinGecko trending tool: {trending_tool.name}\")\n            raw_result, formatted_result = await client.call_tool(trending_tool.name, {})\n\n            if formatted_result:\n                print(\"\\nFormatted result:\")\n                print(formatted_result)\n            else:\n                print(\"\\nRaw result:\")\n                print(raw_result)\n        else:\n            # Use the first available tool\n            tool = coingecko_tools[0]\n            print(f\"\\nCalling CoinGecko tool: {tool.name}\")\n\n            # Try to determine parameters\n            params = {}\n            raw_result, formatted_result = await client.call_tool(tool.name, params)\n\n            if formatted_result:\n                print(\"\\nFormatted result:\")\n                print(formatted_result)\n            else:\n                print(\"\\nRaw result:\")\n                print(raw_result)\n\n\nasync def main():\n    if len(sys.argv) < 2:\n        print(\"Usage: python main_mcp.py <URL of SSE MCP server (i.e. http://localhost:8000/sse)>\")\n        sys.exit(1)\n\n    server_url = sys.argv[1]\n    client = SimpleMCPClient(server_url)\n\n    try:\n        # Initialize the client\n        print(f\"Connecting to MCP server at {server_url}...\")\n        success = await client.initialize()\n        if not success:\n            print(\"Failed to initialize the MCP Client. Exiting.\")\n            return\n\n        print(\"MCP Client initialized successfully!\")\n\n        # List available tools\n        print(\"\\nAvailable tools:\")\n        await client.list_available_tools()\n        # Test the CoinGecko tools directly\n        await test_coingecko_tools(client)\n\n        # Interactive mode\n        print(\"\\n\\nEntering interactive mode. Type 'exit' to quit.\")\n        print(\"Options:\")\n        print(\"  1. Type a message to process with LLM\")\n        print(\"  2. Type 'tool:<tool_name>' to directly call a tool\")\n        print(\"  3. Type 'exit' to quit\")\n\n        # Get tools once before the loop\n        tools_json = await client.get_available_tools_json()\n\n        while True:\n            user_input = input(\"\\nYou: \")\n            if user_input.lower() in [\"exit\", \"quit\"]:\n                break\n\n            if user_input.lower().startswith(\"tool:\"):\n                # Extract tool name and parameters\n                parts = user_input[5:].strip().split(\" \", 1)\n                tool_name = parts[0]\n                params_str = parts[1] if len(parts) > 1 else \"{}\"\n\n                try:\n                    params = json.loads(params_str)\n                    print(f\"Calling tool '{tool_name}' with parameters: {params}\")\n                    raw_result, formatted_result = await client.call_tool(tool_name, params)\n\n                    if formatted_result:\n                        print(\"\\nFormatted result:\")\n                        print(formatted_result)\n                    else:\n                        print(\"\\nRaw result:\")\n                        print(raw_result)\n                except json.JSONDecodeError:\n                    print(f\"Invalid JSON parameters: {params_str}\")\n                except Exception as e:\n                    print(f\"Error calling tool: {str(e)}\")\n            else:\n                # Process with LLM\n                response = await client.process_with_tools(\n                    user_input, \"You are a helpful assistant specializing in cryptocurrency information.\", tools_json\n                )\n                print(f\"\\nAssistant: {response}\")\n\n    finally:\n        await client.cleanup()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "heurist-mesh-client/examples/basic_usage.py", "content": "import json\nimport time\nfrom typing import Any\n\nfrom dotenv import load_dotenv\nfrom heurist_mesh_client.client import MeshClient\n\n\ndef print_json(data: Any) -> None:\n    if hasattr(data, \"model_dump\"):\n        data = data.model_dump()\n    print(json.dumps(data, indent=2))\n\n\ndef wait_for_task(client: MeshClient, task_id: str) -> None:\n    \"\"\"Helper to wait for task completion and print progress.\"\"\"\n    while True:\n        result = client.query_task(task_id=task_id)\n        if result.status == \"finished\":\n            print(\"\\nTask completed:\")\n            if result.result:\n                print_json(result.result)\n            break\n        print(\"Task in progress...\")\n        if result.reasoning_steps:\n            for step in result.reasoning_steps:\n                print(f\"Step: {step.content}\")\n        time.sleep(1)\n\n\ndef main():\n    load_dotenv()\n    client = MeshClient()\n\n    # example 1: async task with natural language query\n    print(\"\\nExample 1: Natural Language Query (Async)\")\n    print(\"-\" * 50)\n    task = client.create_task(\n        agent_id=\"CoinGeckoTokenInfoAgent\",\n        query=\"What is the current price of Bitcoin and its market cap?\",\n    )\n    print(f\"Task created with ID: {task.task_id}\")\n    wait_for_task(client, task.task_id)\n\n    # example 2: sync request with tool and raw data\n    print(\"\\nExample 2: Tool with Raw Data (Sync)\")\n    print(\"-\" * 50)\n    response = client.sync_request(\n        agent_id=\"CoinGeckoTokenInfoAgent\",\n        tool=\"get_token_info\",\n        tool_arguments={\"coingecko_id\": \"ethereum\"},\n        raw_data_only=True,\n    )\n    print_json(response)\n\n    # example 3: async task with tool but natural language response\n    print(\"\\nExample 3: Tool with Natural Language Response (Async)\")\n    print(\"-\" * 50)\n    task = client.create_task(\n        agent_id=\"CoinGeckoTokenInfoAgent\",\n        tool=\"get_token_info\",\n        tool_arguments={\"coingecko_id\": \"solana\"},\n        raw_data_only=False,\n    )\n    print(f\"Task created with ID: {task.task_id}\")\n    wait_for_task(client, task.task_id)\n\n    # example 4: sync request with natural language query\n    print(\"\\nExample 4: Natural Language Query (Sync)\")\n    print(\"-\" * 50)\n    response = client.sync_request(\n        agent_id=\"CoinGeckoTokenInfoAgent\",\n        query=\"What is the current price of Solana and its market cap?\",\n        raw_data_only=False,\n    )\n    print_json(response)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "interfaces/discord.py", "content": "import logging\nimport os\n\nimport discord\nimport dotenv\nfrom discord.ext import commands\n\nfrom agents.core_agent import CoreAgent\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndotenv.load_dotenv()\n\n\nclass DiscordAgent(CoreAgent):\n    def __init__(self):\n        super().__init__()\n        # Define intents to allow the bot to read message content\n        intents = discord.Intents.default()\n        intents.messages = True\n        intents.message_content = True\n\n        self.bot = commands.Bot(command_prefix=\"!\", intents=intents)\n        self.token = os.getenv(\"DISCORD_TOKEN\")\n\n        self.setup_handlers()\n\n    def setup_handlers(self):\n        @self.bot.event\n        async def on_ready():\n            print(f\"Logged in as {self.bot.user}\")\n\n        @self.bot.command()\n        async def hello(ctx):\n            await ctx.send(\"Hello! How can I help you?\")\n\n        @self.bot.event\n        async def on_message(message):\n            # Ignore bot's own messages\n            if message.author == self.bot.user:\n                return\n\n            try:\n                # Get user message\n                user_message = message.content.lower()\n\n                # Handle message using core agent functionality\n                text_response, image_url, _ = await self.handle_message(user_message)\n\n                if image_url:\n                    embed = discord.Embed(title=\"Here you go!\", color=discord.Color.blue())\n                    embed.set_image(url=image_url)\n                    await message.channel.send(embed=embed)\n                elif text_response:\n                    await message.channel.send(text_response)\n                else:\n                    await message.channel.send(\"Sorry, I couldn't process your message.\")\n\n            except Exception as e:\n                logger.error(f\"Message handling failed: {str(e)}\")\n                await message.channel.send(\"Sorry, I encountered an error processing your message.\")\n\n            # Ensure other commands still work\n            await self.bot.process_commands(message)\n\n        # Command: Simple echo function\n        @self.bot.command()\n        async def echo(ctx, *, message: str):\n            await ctx.send(f\"You said: {message}\")\n\n    def run(self):\n        self.bot.run(self.token)\n"}
{"type": "source_file", "path": "heurist-mesh-client/heurist_mesh_client/client.py", "content": "import os\nfrom typing import Any, Dict, List, Optional, Union\n\nimport httpx\nfrom pydantic import BaseModel\n\n\nclass MeshTaskResponse(BaseModel):\n    task_id: str\n    msg: str\n\n\nclass ReasoningStep(BaseModel):\n    timestamp: int\n    content: str\n    is_sent: bool\n\n\nclass TaskResult(BaseModel):\n    response: Any\n    success: bool\n\n    def model_dump(self) -> Dict[str, Any]:\n        data = super().model_dump()\n        if isinstance(data[\"success\"], str):\n            data[\"success\"] = data[\"success\"].lower() == \"true\"\n        return data\n\n\nclass MeshTaskQueryResponse(BaseModel):\n    status: str\n    reasoning_steps: Optional[List[ReasoningStep]] = None\n    result: Optional[Union[TaskResult, Dict[str, Any]]] = None\n\n    def model_dump(self) -> Dict[str, Any]:\n        data = super().model_dump()\n        if data.get(\"result\") and isinstance(data[\"result\"], dict):\n            data[\"result\"] = TaskResult(**data[\"result\"]).model_dump()\n        return data\n\n\nclass MeshClient:\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        base_url: str = \"https://sequencer-v2.heurist.xyz\",\n        timeout: int = 30,\n    ) -> None:\n        self.base_url = base_url.rstrip(\"/\")\n        self.api_key = api_key or os.getenv(\"HEURIST_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\n                \"API key must be provided either through constructor or HEURIST_API_KEY environment variable\"\n            )\n        self.timeout = timeout\n        self.client = httpx.Client(timeout=timeout)\n\n    def _prepare_payload(self, **kwargs: Any) -> Dict[str, Any]:\n        \"\"\"Prepare request payload with API key.\"\"\"\n        return {\"api_key\": self.api_key, **kwargs}\n\n    def _prepare_input(\n        self,\n        query: Optional[str] = None,\n        tool: Optional[str] = None,\n        tool_arguments: Optional[Dict[str, Any]] = None,\n        raw_data_only: bool = False,\n    ) -> Dict[str, Any]:\n        \"\"\"Prepare input data for both sync and async requests.\"\"\"\n        if not query and not tool:\n            raise ValueError(\"Either query or tool must be provided\")\n\n        input_data = {\"raw_data_only\": raw_data_only}\n        if query:\n            input_data[\"query\"] = query\n        if tool:\n            input_data[\"tool\"] = tool\n        if tool_arguments:\n            input_data[\"tool_arguments\"] = tool_arguments\n        return input_data\n\n    def create_task(\n        self,\n        agent_id: str,\n        query: Optional[str] = None,\n        tool: Optional[str] = None,\n        tool_arguments: Optional[Dict[str, Any]] = None,\n        raw_data_only: bool = False,\n        agent_type: str = \"AGENT\",\n    ) -> MeshTaskResponse:\n        \"\"\"Create a new asynchronous task.\"\"\"\n        input_data = self._prepare_input(query, tool, tool_arguments, raw_data_only)\n        payload = self._prepare_payload(\n            agent_id=agent_id,\n            agent_type=agent_type,\n            task_details=input_data,\n        )\n\n        response = self.client.post(f\"{self.base_url}/mesh_task_create\", json=payload)\n        response.raise_for_status()\n        return MeshTaskResponse(**response.json())\n\n    def query_task(self, task_id: str) -> MeshTaskQueryResponse:\n        \"\"\"Query the status and result of an asynchronous task.\"\"\"\n        payload = self._prepare_payload(task_id=task_id)\n\n        response = self.client.post(f\"{self.base_url}/mesh_task_query\", json=payload)\n        response.raise_for_status()\n        return MeshTaskQueryResponse(**response.json())\n\n    def sync_request(\n        self,\n        agent_id: str,\n        query: Optional[str] = None,\n        tool: Optional[str] = None,\n        tool_arguments: Optional[Dict[str, Any]] = None,\n        raw_data_only: bool = False,\n    ) -> Dict[str, Any]:\n        \"\"\"Make a synchronous request to an agent.\"\"\"\n        input_data = self._prepare_input(query, tool, tool_arguments, raw_data_only)\n        payload = self._prepare_payload(agent_id=agent_id, input=input_data)\n\n        response = self.client.post(f\"{self.base_url}/mesh_request\", json=payload)\n        response.raise_for_status()\n        return response.json()\n\n    def close(self) -> None:\n        \"\"\"Close the HTTP client.\"\"\"\n        self.client.close()\n\n    def __enter__(self) -> \"MeshClient\":\n        return self\n\n    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:\n        self.close()\n"}
{"type": "source_file", "path": "decorators.py", "content": "import asyncio\nimport logging\nfrom datetime import datetime, timedelta\nfrom functools import wraps\nfrom typing import Any, Callable, TypeVar\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\", bound=Callable)\n\n\n# Features:\n# Shares cache across all instances of the same agent class\n# Python's dict operations are atomic so it's thread-safe\ndef with_cache(ttl_seconds: int = 300):\n    \"\"\"Cache function results for specified duration\"\"\"\n\n    def decorator(func: T) -> T:\n        # Move cache to class level using a unique key\n        cache_key_base = f\"_cache_{func.__name__}\"\n        ttl_key = f\"_cache_ttl_{func.__name__}\"\n\n        @wraps(func)\n        async def wrapper(self, *args, **kwargs) -> Any:\n            # Initialize class-level cache\n            if not hasattr(self.__class__, cache_key_base):\n                setattr(self.__class__, cache_key_base, {})\n                setattr(self.__class__, ttl_key, {})\n\n            cache = getattr(self.__class__, cache_key_base)\n            cache_ttl = getattr(self.__class__, ttl_key)\n\n            cache_key = f\"{str(args)}:{str(kwargs)}\"\n\n            # Check cache\n            if cache_key in cache and datetime.now() < cache_ttl[cache_key]:\n                logger.debug(f\"Cache hit for {func.__name__}\")\n                return cache[cache_key]\n\n            # Execute function\n            result = await func(self, *args, **kwargs)\n\n            # Update cache\n            cache[cache_key] = result\n            cache_ttl[cache_key] = datetime.now() + timedelta(seconds=ttl_seconds)\n\n            return result\n\n        return wrapper\n\n    return decorator\n\n\n# TODO: Use Redis in a multi-process environment\n# def with_cache(ttl_seconds: int = 300):\n#     \"\"\"Cache function results using Redis\"\"\"\n#     def decorator(func: T) -> T:\n#         redis_client = redis.Redis(host='localhost', port=6379, db=0)\n\n#         @wraps(func)\n#         async def wrapper(*args, **kwargs) -> Any:\n#             cache_key = f\"{func.__name__}:{str(args)}:{str(kwargs)}\"\n\n#             # Check Redis cache\n#             cached = redis_client.get(cache_key)\n#             if cached:\n#                 logger.debug(f\"Cache hit for {func.__name__}\")\n#                 return json.loads(cached)\n\n#             # Execute function\n#             result = await func(*args, **kwargs)\n\n#             # Update Redis cache\n#             redis_client.setex(\n#                 cache_key,\n#                 ttl_seconds,\n#                 json.dumps(result)\n#             )\n\n#             return result\n#         return wrapper\n#     return decorator\n\n\ndef with_retry(max_retries: int = 3, delay: float = 1.0):\n    \"\"\"Retry function execution on failure\"\"\"\n\n    def decorator(func: T) -> T:\n        @wraps(func)\n        async def wrapper(*args, **kwargs) -> Any:\n            last_error = None\n            for attempt in range(max_retries):\n                try:\n                    return await func(*args, **kwargs)\n                except Exception as e:\n                    last_error = e\n                    if attempt < max_retries - 1:\n                        delay_time = delay * (2**attempt)  # Exponential backoff\n                        logger.warning(f\"Retry {attempt + 1}/{max_retries} for {func.__name__} after {delay_time}s\")\n                        await asyncio.sleep(delay_time)\n\n            logger.error(f\"All retries failed for {func.__name__}: {last_error}\")\n            raise last_error\n\n        return wrapper\n\n    return decorator\n\n\ndef monitor_execution():\n    \"\"\"Monitor function execution time and status\"\"\"\n\n    def decorator(func: T) -> T:\n        @wraps(func)\n        async def wrapper(*args, **kwargs) -> Any:\n            start_time = datetime.now()\n            try:\n                result = await func(*args, **kwargs)\n                execution_time = (datetime.now() - start_time).total_seconds()\n                logger.info(f\"{func.__name__} executed successfully in {execution_time:.2f}s\")\n                return result\n            except Exception as e:\n                execution_time = (datetime.now() - start_time).total_seconds()\n                logger.error(f\"{func.__name__} failed after {execution_time:.2f}s: {e}\")\n                raise\n\n        return wrapper\n\n    return decorator\n"}
{"type": "source_file", "path": "heurist-mesh-client/heurist_mesh_client/__init__.py", "content": "from .client import MeshClient\n\n__all__ = [\"MeshClient\"]\n"}
{"type": "source_file", "path": "examples/chat.py", "content": "import asyncio\nimport json\nimport logging\nimport os\nimport sys\nfrom datetime import datetime\nfrom typing import Dict, List\n\nimport aiohttp\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.panel import Panel\nfrom rich.text import Text\n\nproject_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.append(project_root)\n\nlogging.getLogger(\"httpx\").setLevel(logging.WARNING)\n\nload_dotenv()\n\nBANNER = \"\"\"\n╔══════════════════════════════════════════╗\n║      CHAT WITH HEURIST MESH AGENTS       ║\n╚══════════════════════════════════════════╝\n\"\"\"\n\nAGENT_NAME = \"Bitquery Solana Token Info Agent\"\nAGENT_DESCRIPTION = \"This agent fetches Solana token trading information and trending tokens from Bitquery\"\nADDITIONAL_INSTRUCTIONS = \"You are a crypto expert. You should give accurate, up-to-date information based on the context you have. Don't make up information. If you don't have the information, just say so. Don't sound like a robot. You should talk like a crypto native bro who is deep in the trenches.\"\n# API_URL = \"http://localhost:8000/mesh_request\"\nAPI_URL = \"https://sequencer-v2.heurist.xyz/mesh_request\"\n\nconsole = Console()\n\n\nclass ChatSession:\n    def __init__(self):\n        self.messages: List[Dict] = []\n        self.context_data: List[Dict] = []\n        self.openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=\"https://api.openai.com/v1\")\n        self.greeted = False\n\n    def get_system_prompt(self) -> str:\n        prompt = f\"You are {AGENT_NAME}. {AGENT_DESCRIPTION}.\"\n        if self.context_data:\n            prompt += \"\\n\\nUse these data and context information if applicable:\\n\"\n            for entry in self.context_data:\n                prompt += f\"\\n[{entry['timestamp']}] {json.dumps(entry['data'], indent=2)}\"\n        return prompt\n\n    async def call_agent(self, query: str) -> Dict:\n        console.print(\"[dim]Calling agent API...[/dim]\")\n        payload = {\n            \"agent_id\": \"BitquerySolanaTokenInfoAgent\",\n            \"input\": {\"query\": query, \"raw_data_only\": True},\n            \"api_key\": os.getenv(\"HEURIST_API_KEY\"),\n        }\n\n        async with aiohttp.ClientSession() as session:\n            async with session.post(API_URL, json=payload) as response:\n                result = await response.json()\n                return result\n\n    async def stream_llm_response(self, messages: List[Dict]) -> str:\n        stream = await self.openai_client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, stream=True)\n\n        collected_response = \"\"\n        with Live(Text(\"\"), refresh_per_second=10) as live:\n            async for chunk in stream:\n                if chunk.choices[0].delta.content:\n                    content = chunk.choices[0].delta.content\n                    collected_response += content\n                    live.update(Text(collected_response))\n\n        return collected_response\n\n    async def greet_user(self):\n        greeting_prompt = {\n            \"role\": \"system\",\n            \"content\": f\"\"\"You are {AGENT_NAME}. Create a short, fun, and engaging greeting message (2 sentences max).\n            Be creative and talk like a crypto native bro who's deep in the trenches.\n            Your skills: {AGENT_DESCRIPTION}\n            Don't mention being an AI or assistant. Don't use emojis. Just jump right into the dialogue.\"\"\",\n        }\n        console.print(\"\\n[bold green]AI:[/bold green]\")\n        greeting = await self.stream_llm_response([greeting_prompt])\n        self.messages = [{\"role\": \"system\", \"content\": self.get_system_prompt()}]\n        return greeting\n\n    async def chat(self, user_input: str):\n        # Send greeting if first message\n        if not self.greeted:\n            await self.greet_user()\n            self.greeted = True\n\n        # Initialize messages with system prompt if empty\n        if not self.messages:\n            self.messages = [{\"role\": \"system\", \"content\": self.get_system_prompt()}]\n\n        # Add user message with styling\n        console.print(\"\\n[bold blue]You:[/bold blue]\", user_input)\n        self.messages.append({\"role\": \"user\", \"content\": user_input})\n\n        # Call agent API\n        agent_response = await self.call_agent(user_input)\n\n        # If agent returned data, update context\n        if agent_response.get(\"data\"):\n            console.print(\"[dim]Updating context with new data...[/dim]\")\n            self.context_data.append({\"timestamp\": datetime.now().isoformat(), \"data\": agent_response[\"data\"]})\n            # Update system message with new context\n            self.messages[0] = {\"role\": \"system\", \"content\": self.get_system_prompt()}\n\n        # Stream LLM response\n        console.print(\"\\n[bold green]AI:[/bold green]\")\n        llm_response = await self.stream_llm_response(self.messages)\n\n        # Add assistant response to message history\n        self.messages.append({\"role\": \"assistant\", \"content\": llm_response})\n\n\ndef print_welcome_message():\n    console.print(Panel.fit(Text(BANNER, style=\"bold magenta\"), title=\"Welcome\", border_style=\"bright_blue\"))\n    console.print(\n        Panel.fit(\n            f\"[bold]Agent:[/bold] {AGENT_NAME}\\n[bold]Description:[/bold] {AGENT_DESCRIPTION}\",\n            title=\"Agent Info\",\n            border_style=\"green\",\n        )\n    )\n    console.print(\"\\nType [bold red]'exit'[/bold red] to quit\\n\")\n\n\nasync def main():\n    print_welcome_message()\n    session = ChatSession()\n    await session.greet_user()\n\n    while True:\n        try:\n            user_input = input(\"You: \").strip()\n            if user_input.lower() == \"exit\":\n                console.print(\"\\n[bold yellow]Goodbye! Thanks for chatting![/bold yellow]\")\n                break\n\n            if not user_input:\n                continue\n\n            await session.chat(user_input)\n\n        except Exception as e:\n            console.print(f\"\\n[bold red]ERROR:[/bold red] {str(e)}\")\n            console.print_exception()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "interfaces/api.py", "content": "import logging\nimport os\nfrom functools import wraps\n\nimport dotenv\nfrom flask import Flask, jsonify, request\n\nfrom agents.core_agent import CoreAgent\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndotenv.load_dotenv()\n\n\ndef require_api_key(f):\n    @wraps(f)\n    async def decorated_function(*args, **kwargs):\n        api_key = request.headers.get(\"X-API-Key\")\n        if not api_key or api_key != os.getenv(\"API_KEY\"):\n            return jsonify({\"error\": \"Invalid or missing API key\"}), 401\n        return await f(*args, **kwargs)\n\n    return decorated_function\n\n\nclass FlaskAgent(CoreAgent):\n    def __init__(self, core_agent=None):\n        if core_agent:\n            super().__setattr__(\"_parent\", core_agent)\n        else:\n            super().__setattr__(\"_parent\", self)  # Bypass normal __setattr__\n            super().__init__()\n\n        # Initialize Flask specific stuff\n        self._app = Flask(__name__)  # Store as _app to avoid delegation\n        self._setup_routes()\n        self.register_interface(\"api\", self)\n\n    def __getattr__(self, name):\n        # Delegate to the parent instance for missing attributes/methods\n        return getattr(self._parent, name)\n\n    def __setattr__(self, name, value):\n        if not hasattr(self, \"_parent\"):\n            # During initialization, before _parent is set\n            super().__setattr__(name, value)\n        elif name == \"_parent\" or self is self._parent or name in self.__dict__:\n            # Set local attributes (like _parent or already existing attributes)\n            super().__setattr__(name, value)\n        else:\n            # Delegate attribute setting to the parent instance\n            setattr(self._parent, name, value)\n\n    def run(self, host=\"0.0.0.0\", port=5005):\n        \"\"\"Run the Flask application\"\"\"\n        if hasattr(self, \"_app\"):\n            self._app.run(host=host, port=port, debug=False)\n        else:\n            raise RuntimeError(\"Flask app not initialized\")\n\n    def _setup_routes(self):\n        # Example usage:\n        # curl -X POST http://localhost:5000/message \\\n        #   -H \"Content-Type: application/json\" \\\n        #   -d '{\"message\": \"Tell me about artificial intelligence\"}'\n        #\n        # Response:\n        # {\n        #   \"text\": \"AI is a field of computer science...\",\n        #   \"image_url\": \"http://example.com/image.jpg\"  # Optional\n        # }\n        @self._app.route(\"/message\", methods=[\"POST\"])\n        @require_api_key\n        async def handle_message():\n            try:\n                data = request.get_json()\n                logger.info(data)\n                if not data or \"message\" not in data:\n                    return jsonify({\"error\": \"No message provided\"}), 400\n                chat_id = None\n                external_tools = data.get(\"tools\", [])\n                logger.info(external_tools)\n                if \"chat_id\" in data:\n                    chat_id = data[\"chat_id\"]\n                text_response, image_url, tool_calls = await self.handle_message(\n                    data[\"message\"], source_interface=\"api\", chat_id=chat_id, external_tools=external_tools\n                )\n                print(tool_calls)\n                if self._parent != self:\n                    logger.info(\"Operating in shared mode with core agent\")\n                else:\n                    logger.info(\"Operating in standalone mode\")\n\n                response = {}\n                if image_url:\n                    response[\"image_url\"] = image_url\n                if text_response:\n                    response[\"text\"] = text_response\n                if tool_calls:\n                    response[\"tool_calls\"] = tool_calls\n                return jsonify(response)\n            except Exception as e:\n                logger.error(f\"Message handling failed: {str(e)}\")\n                return jsonify({\"error\": \"Internal server error\"}), 500\n\n\ndef main():\n    agent = FlaskAgent()\n    agent.run()\n\n\nif __name__ == \"__main__\":\n    try:\n        logger.info(\"Starting Flask agent...\")\n        main()\n    except KeyboardInterrupt:\n        logger.info(\"\\nFlask agent stopped by user\")\n    except Exception as e:\n        logger.error(f\"Fatal error: {str(e)}\")\n"}
