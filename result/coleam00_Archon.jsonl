{"repo_info": {"repo_name": "Archon", "repo_owner": "coleam00", "repo_url": "https://github.com/coleam00/Archon"}}
{"type": "source_file", "path": "archon/__init__.py", "content": ""}
{"type": "source_file", "path": "iterations/v3-mcp-support/graph_service.py", "content": "from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Optional, Dict, Any\nfrom archon.archon_graph import agentic_flow\nfrom langgraph.types import Command\nfrom utils.utils import write_to_log\n\napp = FastAPI()\n\nclass InvokeRequest(BaseModel):\n    message: str\n    thread_id: str\n    is_first_message: bool = False\n    config: Optional[Dict[str, Any]] = None\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\"status\": \"ok\"}    \n\n@app.post(\"/invoke\")\nasync def invoke_agent(request: InvokeRequest):\n    \"\"\"Process a message through the agentic flow and return the complete response.\n\n    The agent streams the response but this API endpoint waits for the full output\n    before returning so it's a synchronous operation for MCP.\n    Another endpoint will be made later to fully stream the response from the API.\n    \n    Args:\n        request: The InvokeRequest containing message and thread info\n        \n    Returns:\n        dict: Contains the complete response from the agent\n    \"\"\"\n    try:\n        config = request.config or {\n            \"configurable\": {\n                \"thread_id\": request.thread_id\n            }\n        }\n\n        response = \"\"\n        if request.is_first_message:\n            write_to_log(f\"Processing first message for thread {request.thread_id}\")\n            async for msg in agentic_flow.astream(\n                {\"latest_user_message\": request.message}, \n                config,\n                stream_mode=\"custom\"\n            ):\n                response += str(msg)\n        else:\n            write_to_log(f\"Processing continuation for thread {request.thread_id}\")\n            async for msg in agentic_flow.astream(\n                Command(resume=request.message),\n                config,\n                stream_mode=\"custom\"\n            ):\n                response += str(msg)\n\n        write_to_log(f\"Final response for thread {request.thread_id}: {response}\")\n        return {\"response\": response}\n        \n    except Exception as e:\n        write_to_log(f\"Error processing message for thread {request.thread_id}: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8100)\n"}
{"type": "source_file", "path": "archon/agent_prompts.py", "content": "prompt_refiner_prompt = \"\"\"\nYou are an AI agent engineer specialized in refining prompts for the agents.\n\nYour only job is to take the current prompt from the conversation, and refine it so the agent being created\nhas optimal instructions to carry out its role and tasks.\n\nYou want the prompt to:\n\n1. Clearly describe the role of the agent\n2. Provide concise and easy to understand goals\n3. Help the agent understand when and how to use each tool provided\n4. Give interactaction guidelines\n5. Provide instructions for handling issues/errors\n\nOutput the new prompt and nothing else.\n\"\"\"\n\ntools_refiner_prompt = \"\"\"\nYou are an AI agent engineer specialized in refining tools for the agents.\nYou have comprehensive access to the Pydantic AI documentation, including API references, usage guides, and implementation examples.\n\nYour only job is to take the current tools from the conversation, and refine them so the agent being created\nhas the optimal tooling to fulfill its role and tasks. Also make sure the tools are coded properly\nand allow the agent to solve the problems they are meant to help with.\n\nFor each tool, ensure that it:\n\n1. Has a clear docstring to help the agent understand when and how to use it\n2. Has correct arguments\n3. Uses the run context properly if applicable (not all tools need run context)\n4. Is coded properly (uses API calls correctly for the services, returns the correct data, etc.)\n5. Handles errors properly\n\nOnly change what is necessary to refine the tools, don't go overboard unless of course the tools are broken and need a lot of fixing.\n\nOutput the new code for the tools and nothing else.\n\"\"\"\n\nagent_refiner_prompt = \"\"\"\nYou are an AI agent engineer specialized in refining agent definitions in code.\nThere are other agents handling refining the prompt and tools, so your job is to make sure the higher\nlevel definition of the agent (depedencies, setting the LLM, etc.) is all correct.\nYou have comprehensive access to the Pydantic AI documentation, including API references, usage guides, and implementation examples.\n\nYour only job is to take the current agent definition from the conversation, and refine it so the agent being created\nhas dependencies, the LLM, the prompt, etc. all configured correctly. Use the Pydantic AI documentation tools to\nconfirm that the agent is set up properly, and only change the current definition if it doesn't align with\nthe documentation.\n\nOutput the agent depedency and definition code if it needs to change and nothing else.\n\"\"\"\n\nprimary_coder_prompt = \"\"\"\n[ROLE AND CONTEXT]\nYou are a specialized AI agent engineer focused on building robust Pydantic AI agents. You have comprehensive access to the Pydantic AI documentation, including API references, usage guides, and implementation examples.\n\n[CORE RESPONSIBILITIES]\n1. Agent Development\n   - Create new agents from user requirements\n   - Complete partial agent implementations\n   - Optimize and debug existing agents\n   - Guide users through agent specification if needed\n\n2. Documentation Integration\n   - Systematically search documentation using RAG before any implementation\n   - Cross-reference multiple documentation pages for comprehensive understanding\n   - Validate all implementations against current best practices\n   - Notify users if documentation is insufficient for any requirement\n\n[CODE STRUCTURE AND DELIVERABLES]\nAll new agents must include these files with complete, production-ready code:\n\n1. agent.py\n   - Primary agent definition and configuration\n   - Core agent logic and behaviors\n   - No tool implementations allowed here\n\n2. agent_tools.py\n   - All tool function implementations\n   - Tool configurations and setup\n   - External service integrations\n\n3. agent_prompts.py\n   - System prompts\n   - Task-specific prompts\n   - Conversation templates\n   - Instruction sets\n\n4. .env.example\n   - Required environment variables\n   - Clear setup instructions in a comment above the variable for how to do so\n   - API configuration templates\n\n5. requirements.txt\n   - Core dependencies without versions\n   - User-specified packages included\n\n[DOCUMENTATION WORKFLOW]\n1. Initial Research\n   - Begin with RAG search for relevant documentation\n   - List all documentation pages using list_documentation_pages\n   - Retrieve specific page content using get_page_content\n   - Cross-reference the weather agent example for best practices\n\n2. Implementation\n   - Provide complete, working code implementations\n   - Never leave placeholder functions\n   - Include all necessary error handling\n   - Implement proper logging and monitoring\n\n3. Quality Assurance\n   - Verify all tool implementations are complete\n   - Ensure proper separation of concerns\n   - Validate environment variable handling\n   - Test critical path functionality\n\n[INTERACTION GUIDELINES]\n- Take immediate action without asking for permission\n- Always verify documentation before implementation\n- Provide honest feedback about documentation gaps\n- Include specific enhancement suggestions\n- Request user feedback on implementations\n- Maintain code consistency across files\n- After providing code, ask the user at the end if they want you to refine the agent autonomously,\notherwise they can give feedback for you to use. The can specifically say 'refine' for you to continue\nworking on the agent through self reflection.\n\n[ERROR HANDLING]\n- Implement robust error handling in all tools\n- Provide clear error messages\n- Include recovery mechanisms\n- Log important state changes\n\n[BEST PRACTICES]\n- Follow Pydantic AI naming conventions\n- Implement proper type hints\n- Include comprehensive docstrings, the agent uses this to understand what tools are for.\n- Maintain clean code structure\n- Use consistent formatting\n\nHere is a good example of a Pydantic AI agent:\n\n```python\nfrom __future__ import annotations as _annotations\n\nimport asyncio\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport logfire\nfrom devtools import debug\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\n\n\n@dataclass\nclass Deps:\n    client: AsyncClient\n    weather_api_key: str | None\n    geo_api_key: str | None\n\n\nweather_agent = Agent(\n    'openai:gpt-4o',\n    # 'Be concise, reply with one sentence.' is enough for some models (like openai) to use\n    # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n    system_prompt=(\n        'Be concise, reply with one sentence.'\n        'Use the `get_lat_lng` tool to get the latitude and longitude of the locations, '\n        'then use the `get_weather` tool to get the weather.'\n    ),\n    deps_type=Deps,\n    retries=2,\n)\n\n\n@weather_agent.tool\nasync def get_lat_lng(\n    ctx: RunContext[Deps], location_description: str\n) -> dict[str, float]:\n    \\\"\\\"\\\"Get the latitude and longitude of a location.\n\n    Args:\n        ctx: The context.\n        location_description: A description of a location.\n    \\\"\\\"\\\"\n    if ctx.deps.geo_api_key is None:\n        # if no API key is provided, return a dummy response (London)\n        return {'lat': 51.1, 'lng': -0.1}\n\n    params = {\n        'q': location_description,\n        'api_key': ctx.deps.geo_api_key,\n    }\n    with logfire.span('calling geocode API', params=params) as span:\n        r = await ctx.deps.client.get('https://geocode.maps.co/search', params=params)\n        r.raise_for_status()\n        data = r.json()\n        span.set_attribute('response', data)\n\n    if data:\n        return {'lat': data[0]['lat'], 'lng': data[0]['lon']}\n    else:\n        raise ModelRetry('Could not find the location')\n\n\n@weather_agent.tool\nasync def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n    \\\"\\\"\\\"Get the weather at a location.\n\n    Args:\n        ctx: The context.\n        lat: Latitude of the location.\n        lng: Longitude of the location.\n    \\\"\\\"\\\"\n    if ctx.deps.weather_api_key is None:\n        # if no API key is provided, return a dummy response\n        return {'temperature': '21 °C', 'description': 'Sunny'}\n\n    params = {\n        'apikey': ctx.deps.weather_api_key,\n        'location': f'{lat},{lng}',\n        'units': 'metric',\n    }\n    with logfire.span('calling weather API', params=params) as span:\n        r = await ctx.deps.client.get(\n            'https://api.tomorrow.io/v4/weather/realtime', params=params\n        )\n        r.raise_for_status()\n        data = r.json()\n        span.set_attribute('response', data)\n\n    values = data['data']['values']\n    # https://docs.tomorrow.io/reference/data-layers-weather-codes\n    code_lookup = {\n        ...\n    }\n    return {\n        'temperature': f'{values[\"temperatureApparent\"]:0.0f}°C',\n        'description': code_lookup.get(values['weatherCode'], 'Unknown'),\n    }\n\n\nasync def main():\n    async with AsyncClient() as client:\n        # create a free API key at https://www.tomorrow.io/weather-api/\n        weather_api_key = os.getenv('WEATHER_API_KEY')\n        # create a free API key at https://geocode.maps.co/\n        geo_api_key = os.getenv('GEO_API_KEY')\n        deps = Deps(\n            client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\n        )\n        result = await weather_agent.run(\n            'What is the weather like in London and in Wiltshire?', deps=deps\n        )\n        debug(result)\n        print('Response:', result.data)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\"\"\""}
{"type": "source_file", "path": "iterations/v1-single-agent/streamlit_ui.py", "content": "from __future__ import annotations\nfrom typing import Literal, TypedDict\nimport asyncio\nimport os\n\nimport streamlit as st\nimport json\nimport logfire\nfrom supabase import Client\nfrom openai import AsyncOpenAI\n\n# Import all the message part classes\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    SystemPromptPart,\n    UserPromptPart,\n    TextPart,\n    ToolCallPart,\n    ToolReturnPart,\n    RetryPromptPart,\n    ModelMessagesTypeAdapter\n)\nfrom pydantic_ai_coder import pydantic_ai_coder, PydanticAIDeps\n\n# Load environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nopenai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nsupabase: Client = Client(\n    os.getenv(\"SUPABASE_URL\"),\n    os.getenv(\"SUPABASE_SERVICE_KEY\")\n)\n\n# Configure logfire to suppress warnings (optional)\nlogfire.configure(send_to_logfire='never')\n\nclass ChatMessage(TypedDict):\n    \"\"\"Format of messages sent to the browser/API.\"\"\"\n\n    role: Literal['user', 'model']\n    timestamp: str\n    content: str\n\n\ndef display_message_part(part):\n    \"\"\"\n    Display a single part of a message in the Streamlit UI.\n    Customize how you display system prompts, user prompts,\n    tool calls, tool returns, etc.\n    \"\"\"\n    # system-prompt\n    if part.part_kind == 'system-prompt':\n        with st.chat_message(\"system\"):\n            st.markdown(f\"**System**: {part.content}\")\n    # user-prompt\n    elif part.part_kind == 'user-prompt':\n        with st.chat_message(\"user\"):\n            st.markdown(part.content)\n    # text\n    elif part.part_kind == 'text':\n        with st.chat_message(\"assistant\"):\n            st.markdown(part.content)          \n\n\nasync def run_agent_with_streaming(user_input: str):\n    \"\"\"\n    Run the agent with streaming text for the user_input prompt,\n    while maintaining the entire conversation in `st.session_state.messages`.\n    \"\"\"\n    # Prepare dependencies\n    deps = PydanticAIDeps(\n        supabase=supabase,\n        openai_client=openai_client\n    )\n\n    # Run the agent in a stream\n    async with pydantic_ai_coder.run_stream(\n        user_input,\n        deps=deps,\n        message_history= st.session_state.messages[:-1],  # pass entire conversation so far\n    ) as result:\n        # We'll gather partial text to show incrementally\n        partial_text = \"\"\n        message_placeholder = st.empty()\n\n        # Render partial text as it arrives\n        async for chunk in result.stream_text(delta=True):\n            partial_text += chunk\n            message_placeholder.markdown(partial_text)\n\n        # Now that the stream is finished, we have a final result.\n        # Add new messages from this run, excluding user-prompt messages\n        filtered_messages = [msg for msg in result.new_messages() \n                            if not (hasattr(msg, 'parts') and \n                                    any(part.part_kind == 'user-prompt' for part in msg.parts))]\n        st.session_state.messages.extend(filtered_messages)\n\n        # Add the final response to the messages\n        st.session_state.messages.append(\n            ModelResponse(parts=[TextPart(content=partial_text)])\n        )\n\n\nasync def main():\n    st.title(\"Archon - Agent Builder\")\n    st.write(\"Describe to me an AI agent you want to build and I'll code it for you with Pydantic AI.\")\n\n    # Initialize chat history in session state if not present\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    # Display all messages from the conversation so far\n    # Each message is either a ModelRequest or ModelResponse.\n    # We iterate over their parts to decide how to display them.\n    for msg in st.session_state.messages:\n        if isinstance(msg, ModelRequest) or isinstance(msg, ModelResponse):\n            for part in msg.parts:\n                display_message_part(part)\n\n    # Chat input for the user\n    user_input = st.chat_input(\"What do you want to build today?\")\n\n    if user_input:\n        # We append a new request to the conversation explicitly\n        st.session_state.messages.append(\n            ModelRequest(parts=[UserPromptPart(content=user_input)])\n        )\n        \n        # Display user prompt in the UI\n        with st.chat_message(\"user\"):\n            st.markdown(user_input)\n\n        # Display the assistant's partial response while streaming\n        with st.chat_message(\"assistant\"):\n            # Actually run the agent now, streaming the text\n            await run_agent_with_streaming(user_input)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "archon/refiner_agents/agent_refiner_agent.py", "content": "from __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\nimport logfire\nimport asyncio\nimport httpx\nimport os\nimport sys\nimport json\nfrom typing import List\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom openai import AsyncOpenAI\nfrom supabase import Client\n\n# Add the parent directory to sys.path to allow importing from the parent directory\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\nfrom utils.utils import get_env_var\nfrom archon.agent_prompts import agent_refiner_prompt\nfrom archon.agent_tools import (\n    retrieve_relevant_documentation_tool,\n    list_documentation_pages_tool,\n    get_page_content_tool\n)\n\nload_dotenv()\n\nprovider = get_env_var('LLM_PROVIDER') or 'OpenAI'\nllm = get_env_var('PRIMARY_MODEL') or 'gpt-4o-mini'\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\n\nmodel = AnthropicModel(llm, api_key=api_key) if provider == \"Anthropic\" else OpenAIModel(llm, base_url=base_url, api_key=api_key)\nembedding_model = get_env_var('EMBEDDING_MODEL') or 'text-embedding-3-small'\n\nlogfire.configure(send_to_logfire='if-token-present')\n\n@dataclass\nclass AgentRefinerDeps:\n    supabase: Client\n    embedding_client: AsyncOpenAI\n\nagent_refiner_agent = Agent(\n    model,\n    system_prompt=agent_refiner_prompt,\n    deps_type=AgentRefinerDeps,\n    retries=2\n)\n\n@agent_refiner_agent.tool\nasync def retrieve_relevant_documentation(ctx: RunContext[AgentRefinerDeps], query: str) -> str:\n    \"\"\"\n    Retrieve relevant documentation chunks based on the query with RAG.\n    Make sure your searches always focus on implementing the agent itself.\n    \n    Args:\n        ctx: The context including the Supabase client and OpenAI client\n        query: Your query to retrieve relevant documentation for implementing agents\n        \n    Returns:\n        A formatted string containing the top 4 most relevant documentation chunks\n    \"\"\"\n    return await retrieve_relevant_documentation_tool(ctx.deps.supabase, ctx.deps.embedding_client, query)\n\n@agent_refiner_agent.tool\nasync def list_documentation_pages(ctx: RunContext[AgentRefinerDeps]) -> List[str]:\n    \"\"\"\n    Retrieve a list of all available Pydantic AI documentation pages.\n    This will give you all pages available, but focus on the ones related to configuring agents and their dependencies.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    return await list_documentation_pages_tool(ctx.deps.supabase)\n\n@agent_refiner_agent.tool\nasync def get_page_content(ctx: RunContext[AgentRefinerDeps], url: str) -> str:\n    \"\"\"\n    Retrieve the full content of a specific documentation page by combining all its chunks.\n    Only use this tool to get pages related to setting up agents with Pydantic AI.\n    \n    Args:\n        ctx: The context including the Supabase client\n        url: The URL of the page to retrieve\n        \n    Returns:\n        str: The complete page content with all chunks combined in order\n    \"\"\"\n    return await get_page_content_tool(ctx.deps.supabase, url)"}
{"type": "source_file", "path": "archon/pydantic_ai_coder.py", "content": "from __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\nimport logfire\nimport asyncio\nimport httpx\nimport os\nimport sys\nimport json\nfrom typing import List\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom openai import AsyncOpenAI\nfrom supabase import Client\n\n# Add the parent directory to sys.path to allow importing from the parent directory\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils.utils import get_env_var\nfrom archon.agent_prompts import primary_coder_prompt\nfrom archon.agent_tools import (\n    retrieve_relevant_documentation_tool,\n    list_documentation_pages_tool,\n    get_page_content_tool\n)\n\nload_dotenv()\n\nprovider = get_env_var('LLM_PROVIDER') or 'OpenAI'\nllm = get_env_var('PRIMARY_MODEL') or 'gpt-4o-mini'\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\n\nmodel = AnthropicModel(llm, api_key=api_key) if provider == \"Anthropic\" else OpenAIModel(llm, base_url=base_url, api_key=api_key)\n\nlogfire.configure(send_to_logfire='if-token-present')\n\n@dataclass\nclass PydanticAIDeps:\n    supabase: Client\n    embedding_client: AsyncOpenAI\n    reasoner_output: str\n\npydantic_ai_coder = Agent(\n    model,\n    system_prompt=primary_coder_prompt,\n    deps_type=PydanticAIDeps,\n    retries=2\n)\n\n@pydantic_ai_coder.system_prompt  \ndef add_reasoner_output(ctx: RunContext[str]) -> str:\n    return f\"\"\"\n    \\n\\nAdditional thoughts/instructions from the reasoner LLM. \n    This scope includes documentation pages for you to search as well: \n    {ctx.deps.reasoner_output}\n    \"\"\"\n\n@pydantic_ai_coder.tool\nasync def retrieve_relevant_documentation(ctx: RunContext[PydanticAIDeps], user_query: str) -> str:\n    \"\"\"\n    Retrieve relevant documentation chunks based on the query with RAG.\n    \n    Args:\n        ctx: The context including the Supabase client and OpenAI client\n        user_query: The user's question or query\n        \n    Returns:\n        A formatted string containing the top 4 most relevant documentation chunks\n    \"\"\"\n    return await retrieve_relevant_documentation_tool(ctx.deps.supabase, ctx.deps.embedding_client, user_query)\n\n@pydantic_ai_coder.tool\nasync def list_documentation_pages(ctx: RunContext[PydanticAIDeps]) -> List[str]:\n    \"\"\"\n    Retrieve a list of all available Pydantic AI documentation pages.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    return await list_documentation_pages_tool(ctx.deps.supabase)\n\n@pydantic_ai_coder.tool\nasync def get_page_content(ctx: RunContext[PydanticAIDeps], url: str) -> str:\n    \"\"\"\n    Retrieve the full content of a specific documentation page by combining all its chunks.\n    \n    Args:\n        ctx: The context including the Supabase client\n        url: The URL of the page to retrieve\n        \n    Returns:\n        str: The complete page content with all chunks combined in order\n    \"\"\"\n    return await get_page_content_tool(ctx.deps.supabase, url)"}
{"type": "source_file", "path": "archon/agent_tools.py", "content": "from typing import Dict, Any, List, Optional\nfrom openai import AsyncOpenAI\nfrom supabase import Client\nimport sys\nimport os\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils.utils import get_env_var\n\nembedding_model = get_env_var('EMBEDDING_MODEL') or 'text-embedding-3-small'\n\nasync def get_embedding(text: str, embedding_client: AsyncOpenAI) -> List[float]:\n    \"\"\"Get embedding vector from OpenAI.\"\"\"\n    try:\n        response = await embedding_client.embeddings.create(\n            model=embedding_model,\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Error getting embedding: {e}\")\n        return [0] * 1536  # Return zero vector on error\n\nasync def retrieve_relevant_documentation_tool(supabase: Client, embedding_client: AsyncOpenAI, user_query: str) -> str:\n    try:\n        # Get the embedding for the query\n        query_embedding = await get_embedding(user_query, embedding_client)\n        \n        # Query Supabase for relevant documents\n        result = supabase.rpc(\n            'match_site_pages',\n            {\n                'query_embedding': query_embedding,\n                'match_count': 4,\n                'filter': {'source': 'pydantic_ai_docs'}\n            }\n        ).execute()\n        \n        if not result.data:\n            return \"No relevant documentation found.\"\n            \n        # Format the results\n        formatted_chunks = []\n        for doc in result.data:\n            chunk_text = f\"\"\"\n# {doc['title']}\n\n{doc['content']}\n\"\"\"\n            formatted_chunks.append(chunk_text)\n            \n        # Join all chunks with a separator\n        return \"\\n\\n---\\n\\n\".join(formatted_chunks)\n        \n    except Exception as e:\n        print(f\"Error retrieving documentation: {e}\")\n        return f\"Error retrieving documentation: {str(e)}\" \n\nasync def list_documentation_pages_tool(supabase: Client) -> List[str]:\n    \"\"\"\n    Function to retrieve a list of all available Pydantic AI documentation pages.\n    This is called by the list_documentation_pages tool and also externally\n    to fetch documentation pages for the reasoner LLM.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    try:\n        # Query Supabase for unique URLs where source is pydantic_ai_docs\n        result = supabase.from_('site_pages') \\\n            .select('url') \\\n            .eq('metadata->>source', 'pydantic_ai_docs') \\\n            .execute()\n        \n        if not result.data:\n            return []\n            \n        # Extract unique URLs\n        urls = sorted(set(doc['url'] for doc in result.data))\n        return urls\n        \n    except Exception as e:\n        print(f\"Error retrieving documentation pages: {e}\")\n        return []\n\nasync def get_page_content_tool(supabase: Client, url: str) -> str:\n    \"\"\"\n    Retrieve the full content of a specific documentation page by combining all its chunks.\n    \n    Args:\n        ctx: The context including the Supabase client\n        url: The URL of the page to retrieve\n        \n    Returns:\n        str: The complete page content with all chunks combined in order\n    \"\"\"\n    try:\n        # Query Supabase for all chunks of this URL, ordered by chunk_number\n        result = supabase.from_('site_pages') \\\n            .select('title, content, chunk_number') \\\n            .eq('url', url) \\\n            .eq('metadata->>source', 'pydantic_ai_docs') \\\n            .order('chunk_number') \\\n            .execute()\n        \n        if not result.data:\n            return f\"No content found for URL: {url}\"\n            \n        # Format the page with its title and all chunks\n        page_title = result.data[0]['title'].split(' - ')[0]  # Get the main title\n        formatted_content = [f\"# {page_title}\\n\"]\n        \n        # Add each chunk's content\n        for chunk in result.data:\n            formatted_content.append(chunk['content'])\n            \n        # Join everything together but limit the characters in case the page is massive (there are a coule big ones)\n        # This will be improved later so if the page is too big RAG will be performed on the page itself\n        return \"\\n\\n\".join(formatted_content)[:20000]\n        \n    except Exception as e:\n        print(f\"Error retrieving page content: {e}\")\n        return f\"Error retrieving page content: {str(e)}\"\n"}
{"type": "source_file", "path": "archon/archon_graph.py", "content": "from pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai import Agent, RunContext\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing import TypedDict, Annotated, List, Any\nfrom langgraph.config import get_stream_writer\nfrom langgraph.types import interrupt\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\nfrom supabase import Client\nimport logfire\nimport os\nimport sys\n\n# Import the message classes from Pydantic AI\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelMessagesTypeAdapter\n)\n\n# Add the parent directory to Python path\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom archon.pydantic_ai_coder import pydantic_ai_coder, PydanticAIDeps\nfrom archon.refiner_agents.prompt_refiner_agent import prompt_refiner_agent\nfrom archon.refiner_agents.tools_refiner_agent import tools_refiner_agent, ToolsRefinerDeps\nfrom archon.refiner_agents.agent_refiner_agent import agent_refiner_agent, AgentRefinerDeps\nfrom archon.agent_tools import list_documentation_pages_tool\nfrom utils.utils import get_env_var, get_clients\n\n# Load environment variables\nload_dotenv()\n\n# Configure logfire to suppress warnings (optional)\nlogfire.configure(send_to_logfire='never')\n\nprovider = get_env_var('LLM_PROVIDER') or 'OpenAI'\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\n\nis_anthropic = provider == \"Anthropic\"\nis_openai = provider == \"OpenAI\"\n\nreasoner_llm_model_name = get_env_var('REASONER_MODEL') or 'o3-mini'\nreasoner_llm_model = AnthropicModel(reasoner_llm_model_name, api_key=api_key) if is_anthropic else OpenAIModel(reasoner_llm_model_name, base_url=base_url, api_key=api_key)\n\nreasoner = Agent(  \n    reasoner_llm_model,\n    system_prompt='You are an expert at coding AI agents with Pydantic AI and defining the scope for doing so.',  \n)\n\nprimary_llm_model_name = get_env_var('PRIMARY_MODEL') or 'gpt-4o-mini'\nprimary_llm_model = AnthropicModel(primary_llm_model_name, api_key=api_key) if is_anthropic else OpenAIModel(primary_llm_model_name, base_url=base_url, api_key=api_key)\n\nrouter_agent = Agent(  \n    primary_llm_model,\n    system_prompt='Your job is to route the user message either to the end of the conversation or to continue coding the AI agent.',  \n)\n\nend_conversation_agent = Agent(  \n    primary_llm_model,\n    system_prompt='Your job is to end a conversation for creating an AI agent by giving instructions for how to execute the agent and they saying a nice goodbye to the user.',  \n)\n\n# Initialize clients\nembedding_client, supabase = get_clients()\n\n# Define state schema\nclass AgentState(TypedDict):\n    latest_user_message: str\n    messages: Annotated[List[bytes], lambda x, y: x + y]\n\n    scope: str\n\n    refined_prompt: str\n    refined_tools: str\n    refined_agent: str\n\n# Scope Definition Node with Reasoner LLM\nasync def define_scope_with_reasoner(state: AgentState):\n    # First, get the documentation pages so the reasoner can decide which ones are necessary\n    documentation_pages = await list_documentation_pages_tool(supabase)\n    documentation_pages_str = \"\\n\".join(documentation_pages)\n\n    # Then, use the reasoner to define the scope\n    prompt = f\"\"\"\n    User AI Agent Request: {state['latest_user_message']}\n    \n    Create detailed scope document for the AI agent including:\n    - Architecture diagram\n    - Core components\n    - External dependencies\n    - Testing strategy\n\n    Also based on these documentation pages available:\n\n    {documentation_pages_str}\n\n    Include a list of documentation pages that are relevant to creating this agent for the user in the scope document.\n    \"\"\"\n\n    result = await reasoner.run(prompt)\n    scope = result.data\n\n    # Get the directory one level up from the current file\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    parent_dir = os.path.dirname(current_dir)\n    scope_path = os.path.join(parent_dir, \"workbench\", \"scope.md\")\n    os.makedirs(os.path.join(parent_dir, \"workbench\"), exist_ok=True)\n\n    with open(scope_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(scope)\n    \n    return {\"scope\": scope}\n\n# Coding Node with Feedback Handling\nasync def coder_agent(state: AgentState, writer):    \n    # Prepare dependencies\n    deps = PydanticAIDeps(\n        supabase=supabase,\n        embedding_client=embedding_client,\n        reasoner_output=state['scope']\n    )\n\n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    # The prompt either needs to be the user message (initial agent request or feedback)\n    # or the refined prompt/tools/agent if we are in that stage of the agent creation process\n    if 'refined_prompt' in state and state['refined_prompt']:\n        prompt = f\"\"\"\n        I need you to refine the agent you created. \n        \n        Here is the refined prompt:\\n\n        {state['refined_prompt']}\\n\\n\n\n        Here are the refined tools:\\n\n        {state['refined_tools']}\\n\n\n        And finally, here are the changes to the agent definition to make if any:\\n\n        {state['refined_agent']}\\n\\n\n\n        Output any changes necessary to the agent code based on these refinements.\n        \"\"\"\n    else:\n        prompt = state['latest_user_message']\n\n    # Run the agent in a stream\n    if not is_openai:\n        writer = get_stream_writer()\n        result = await pydantic_ai_coder.run(prompt, deps=deps, message_history=message_history)\n        writer(result.data)\n    else:\n        async with pydantic_ai_coder.run_stream(\n            state['latest_user_message'],\n            deps=deps,\n            message_history=message_history\n        ) as result:\n            # Stream partial text as it arrives\n            async for chunk in result.stream_text(delta=True):\n                writer(chunk)\n\n    # print(ModelMessagesTypeAdapter.validate_json(result.new_messages_json()))\n\n    # Add the new conversation history (including tool calls)\n    # Reset the refined properties in case they were just used to refine the agent\n    return {\n        \"messages\": [result.new_messages_json()],\n        \"refined_prompt\": \"\",\n        \"refined_tools\": \"\",\n        \"refined_agent\": \"\"\n    }\n\n# Interrupt the graph to get the user's next message\ndef get_next_user_message(state: AgentState):\n    value = interrupt({})\n\n    # Set the user's latest message for the LLM to continue the conversation\n    return {\n        \"latest_user_message\": value\n    }\n\n# Determine if the user is finished creating their AI agent or not\nasync def route_user_message(state: AgentState):\n    prompt = f\"\"\"\n    The user has sent a message: \n    \n    {state['latest_user_message']}\n\n    If the user wants to end the conversation, respond with just the text \"finish_conversation\".\n    If the user wants to continue coding the AI agent and gave feedback, respond with just the text \"coder_agent\".\n    If the user asks specifically to \"refine\" the agent, respond with just the text \"refine\".\n    \"\"\"\n\n    result = await router_agent.run(prompt)\n    \n    if result.data == \"finish_conversation\": return \"finish_conversation\"\n    if result.data == \"refine\": return [\"refine_prompt\", \"refine_tools\", \"refine_agent\"]\n    return \"coder_agent\"\n\n# Refines the prompt for the AI agent\nasync def refine_prompt(state: AgentState):\n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    prompt = \"Based on the current conversation, refine the prompt for the agent.\"\n\n    # Run the agent to refine the prompt for the agent being created\n    result = await prompt_refiner_agent.run(prompt, message_history=message_history)\n\n    return {\"refined_prompt\": result.data}\n\n# Refines the tools for the AI agent\nasync def refine_tools(state: AgentState):\n    # Prepare dependencies\n    deps = ToolsRefinerDeps(\n        supabase=supabase,\n        embedding_client=embedding_client\n    )\n\n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    prompt = \"Based on the current conversation, refine the tools for the agent.\"\n\n    # Run the agent to refine the tools for the agent being created\n    result = await tools_refiner_agent.run(prompt, deps=deps, message_history=message_history)\n\n    return {\"refined_tools\": result.data}\n\n# Refines the defintion for the AI agent\nasync def refine_agent(state: AgentState):\n    # Prepare dependencies\n    deps = AgentRefinerDeps(\n        supabase=supabase,\n        embedding_client=embedding_client\n    )\n\n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    prompt = \"Based on the current conversation, refine the agent definition.\"\n\n    # Run the agent to refine the definition for the agent being created\n    result = await agent_refiner_agent.run(prompt, deps=deps, message_history=message_history)\n\n    return {\"refined_agent\": result.data}\n\n# End of conversation agent to give instructions for executing the agent\nasync def finish_conversation(state: AgentState, writer):    \n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    # Run the agent in a stream\n    if not is_openai:\n        writer = get_stream_writer()\n        result = await end_conversation_agent.run(state['latest_user_message'], message_history= message_history)\n        writer(result.data)   \n    else: \n        async with end_conversation_agent.run_stream(\n            state['latest_user_message'],\n            message_history= message_history\n        ) as result:\n            # Stream partial text as it arrives\n            async for chunk in result.stream_text(delta=True):\n                writer(chunk)\n\n    return {\"messages\": [result.new_messages_json()]}        \n\n# Build workflow\nbuilder = StateGraph(AgentState)\n\n# Add nodes\nbuilder.add_node(\"define_scope_with_reasoner\", define_scope_with_reasoner)\nbuilder.add_node(\"coder_agent\", coder_agent)\nbuilder.add_node(\"get_next_user_message\", get_next_user_message)\nbuilder.add_node(\"refine_prompt\", refine_prompt)\nbuilder.add_node(\"refine_tools\", refine_tools)\nbuilder.add_node(\"refine_agent\", refine_agent)\nbuilder.add_node(\"finish_conversation\", finish_conversation)\n\n# Set edges\nbuilder.add_edge(START, \"define_scope_with_reasoner\")\nbuilder.add_edge(\"define_scope_with_reasoner\", \"coder_agent\")\nbuilder.add_edge(\"coder_agent\", \"get_next_user_message\")\nbuilder.add_conditional_edges(\n    \"get_next_user_message\",\n    route_user_message,\n    [\"coder_agent\", \"finish_conversation\", \"refine_prompt\", \"refine_tools\", \"refine_agent\"]\n)\nbuilder.add_edge(\"refine_prompt\", \"coder_agent\")\nbuilder.add_edge(\"refine_tools\", \"coder_agent\")\nbuilder.add_edge(\"refine_agent\", \"coder_agent\")\nbuilder.add_edge(\"finish_conversation\", END)\n\n# Configure persistence\nmemory = MemorySaver()\nagentic_flow = builder.compile(checkpointer=memory)"}
{"type": "source_file", "path": "iterations/v1-single-agent/pydantic_ai_coder.py", "content": "from __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\nimport logfire\nimport asyncio\nimport httpx\nimport os\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom openai import AsyncOpenAI\nfrom supabase import Client\nfrom typing import List\n\nload_dotenv()\n\nllm = os.getenv('LLM_MODEL', 'gpt-4o-mini')\nmodel = OpenAIModel(llm)\n\nlogfire.configure(send_to_logfire='if-token-present')\n\n@dataclass\nclass PydanticAIDeps:\n    supabase: Client\n    openai_client: AsyncOpenAI\n\nsystem_prompt = \"\"\"\n~~ CONTEXT: ~~\n\nYou are an expert at Pydantic AI - a Python AI agent framework that you have access to all the documentation to,\nincluding examples, an API reference, and other resources to help you build Pydantic AI agents.\n\n~~ GOAL: ~~\n\nYour only job is to help the user create an AI agent with Pydantic AI.\nThe user will describe the AI agent they want to build, or if they don't, guide them towards doing so.\nYou will take their requirements, and then search through the Pydantic AI documentation with the tools provided\nto find all the necessary information to create the AI agent with correct code.\n\nIt's important for you to search through multiple Pydantic AI documentation pages to get all the information you need.\nAlmost never stick to just one page - use RAG and the other documentation tools multiple times when you are creating\nan AI agent from scratch for the user.\n\n~~ STRUCTURE: ~~\n\nWhen you build an AI agent from scratch, split the agent into this files and give the code for each:\n- `agent.py`: The main agent file, which is where the Pydantic AI agent is defined.\n- `agent_tools.py`: A tools file for the agent, which is where all the tool functions are defined. Use this for more complex agents.\n- `agent_prompts.py`: A prompts file for the agent, which includes all system prompts and other prompts used by the agent. Use this when there are many prompts or large ones.\n- `.env.example`: An example `.env` file - specify each variable that the user will need to fill in and a quick comment above each one for how to do so.\n- `requirements.txt`: Don't include any versions, just the top level package names needed for the agent.\n\n~~ INSTRUCTIONS: ~~\n\n- Don't ask the user before taking an action, just do it. Always make sure you look at the documentation with the provided tools before writing any code.\n- When you first look at the documentation, always start with RAG.\nThen also always check the list of available documentation pages and retrieve the content of page(s) if it'll help.\n- Always let the user know when you didn't find the answer in the documentation or the right URL - be honest.\n- Helpful tip: when starting a new AI agent build, it's a good idea to look at the 'weather agent' in the docs as an example.\n- When starting a new AI agent build, always produce the full code for the AI agent - never tell the user to finish a tool/function.\n- When refining an existing AI agent build in a conversation, just share the code changes necessary.\n\"\"\"\n\npydantic_ai_coder = Agent(\n    model,\n    system_prompt=system_prompt,\n    deps_type=PydanticAIDeps,\n    retries=2\n)\n\nasync def get_embedding(text: str, openai_client: AsyncOpenAI) -> List[float]:\n    \"\"\"Get embedding vector from OpenAI.\"\"\"\n    try:\n        response = await openai_client.embeddings.create(\n            model=\"text-embedding-3-small\",\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Error getting embedding: {e}\")\n        return [0] * 1536  # Return zero vector on error\n\n@pydantic_ai_coder.tool\nasync def retrieve_relevant_documentation(ctx: RunContext[PydanticAIDeps], user_query: str) -> str:\n    \"\"\"\n    Retrieve relevant documentation chunks based on the query with RAG.\n    \n    Args:\n        ctx: The context including the Supabase client and OpenAI client\n        user_query: The user's question or query\n        \n    Returns:\n        A formatted string containing the top 5 most relevant documentation chunks\n    \"\"\"\n    try:\n        # Get the embedding for the query\n        query_embedding = await get_embedding(user_query, ctx.deps.openai_client)\n        \n        # Query Supabase for relevant documents\n        result = ctx.deps.supabase.rpc(\n            'match_site_pages',\n            {\n                'query_embedding': query_embedding,\n                'match_count': 5,\n                'filter': {'source': 'pydantic_ai_docs'}\n            }\n        ).execute()\n        \n        if not result.data:\n            return \"No relevant documentation found.\"\n            \n        # Format the results\n        formatted_chunks = []\n        for doc in result.data:\n            chunk_text = f\"\"\"\n# {doc['title']}\n\n{doc['content']}\n\"\"\"\n            formatted_chunks.append(chunk_text)\n            \n        # Join all chunks with a separator\n        return \"\\n\\n---\\n\\n\".join(formatted_chunks)\n        \n    except Exception as e:\n        print(f\"Error retrieving documentation: {e}\")\n        return f\"Error retrieving documentation: {str(e)}\"\n\n@pydantic_ai_coder.tool\nasync def list_documentation_pages(ctx: RunContext[PydanticAIDeps]) -> List[str]:\n    \"\"\"\n    Retrieve a list of all available Pydantic AI documentation pages.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    try:\n        # Query Supabase for unique URLs where source is pydantic_ai_docs\n        result = ctx.deps.supabase.from_('site_pages') \\\n            .select('url') \\\n            .eq('metadata->>source', 'pydantic_ai_docs') \\\n            .execute()\n        \n        if not result.data:\n            return []\n            \n        # Extract unique URLs\n        urls = sorted(set(doc['url'] for doc in result.data))\n        return urls\n        \n    except Exception as e:\n        print(f\"Error retrieving documentation pages: {e}\")\n        return []\n\n@pydantic_ai_coder.tool\nasync def get_page_content(ctx: RunContext[PydanticAIDeps], url: str) -> str:\n    \"\"\"\n    Retrieve the full content of a specific documentation page by combining all its chunks.\n    \n    Args:\n        ctx: The context including the Supabase client\n        url: The URL of the page to retrieve\n        \n    Returns:\n        str: The complete page content with all chunks combined in order\n    \"\"\"\n    try:\n        # Query Supabase for all chunks of this URL, ordered by chunk_number\n        result = ctx.deps.supabase.from_('site_pages') \\\n            .select('title, content, chunk_number') \\\n            .eq('url', url) \\\n            .eq('metadata->>source', 'pydantic_ai_docs') \\\n            .order('chunk_number') \\\n            .execute()\n        \n        if not result.data:\n            return f\"No content found for URL: {url}\"\n            \n        # Format the page with its title and all chunks\n        page_title = result.data[0]['title'].split(' - ')[0]  # Get the main title\n        formatted_content = [f\"# {page_title}\\n\"]\n        \n        # Add each chunk's content\n        for chunk in result.data:\n            formatted_content.append(chunk['content'])\n            \n        # Join everything together\n        return \"\\n\\n\".join(formatted_content)\n        \n    except Exception as e:\n        print(f\"Error retrieving page content: {e}\")\n        return f\"Error retrieving page content: {str(e)}\""}
{"type": "source_file", "path": "archon/crawl_pydantic_ai_docs.py", "content": "import os\nimport sys\nimport asyncio\nimport threading\nimport subprocess\nimport requests\nimport json\nimport time\nfrom typing import List, Dict, Any, Optional, Callable\nfrom xml.etree import ElementTree\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\nfrom urllib.parse import urlparse\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\nimport re\nimport html2text\n\n# Add the parent directory to sys.path to allow importing from the parent directory\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils.utils import get_env_var, get_clients\n\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nload_dotenv()\n\n# Initialize embedding and Supabase clients\nembedding_client, supabase = get_clients()\n\n# Define the embedding model for embedding the documentation for RAG\nembedding_model = get_env_var('EMBEDDING_MODEL') or 'text-embedding-3-small'\n\n# LLM client setup\nllm_client = None\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-api-key-provided'\nprovider = get_env_var('LLM_PROVIDER') or 'OpenAI'\n\n# Setup OpenAI client for LLM\nif provider == \"Ollama\":\n    if api_key == \"NOT_REQUIRED\":\n        api_key = \"ollama\"  # Use a dummy key for Ollama\n    llm_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\nelse:\n    llm_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n\n# Initialize HTML to Markdown converter\nhtml_converter = html2text.HTML2Text()\nhtml_converter.ignore_links = False\nhtml_converter.ignore_images = False\nhtml_converter.ignore_tables = False\nhtml_converter.body_width = 0  # No wrapping\n\n@dataclass\nclass ProcessedChunk:\n    url: str\n    chunk_number: int\n    title: str\n    summary: str\n    content: str\n    metadata: Dict[str, Any]\n    embedding: List[float]\n\nclass CrawlProgressTracker:\n    \"\"\"Class to track progress of the crawling process.\"\"\"\n    \n    def __init__(self, \n                 progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None):\n        \"\"\"Initialize the progress tracker.\n        \n        Args:\n            progress_callback: Function to call with progress updates\n        \"\"\"\n        self.progress_callback = progress_callback\n        self.urls_found = 0\n        self.urls_processed = 0\n        self.urls_succeeded = 0\n        self.urls_failed = 0\n        self.chunks_stored = 0\n        self.logs = []\n        self.is_running = False\n        self.start_time = None\n        self.end_time = None\n    \n    def log(self, message: str):\n        \"\"\"Add a log message and update progress.\"\"\"\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        log_entry = f\"[{timestamp}] {message}\"\n        self.logs.append(log_entry)\n        print(message)  # Also print to console\n        \n        # Call the progress callback if provided\n        if self.progress_callback:\n            self.progress_callback(self.get_status())\n    \n    def start(self):\n        \"\"\"Mark the crawling process as started.\"\"\"\n        self.is_running = True\n        self.start_time = datetime.now()\n        self.log(\"Crawling process started\")\n        \n        # Call the progress callback if provided\n        if self.progress_callback:\n            self.progress_callback(self.get_status())\n    \n    def complete(self):\n        \"\"\"Mark the crawling process as completed.\"\"\"\n        self.is_running = False\n        self.end_time = datetime.now()\n        duration = self.end_time - self.start_time if self.start_time else None\n        duration_str = str(duration).split('.')[0] if duration else \"unknown\"\n        self.log(f\"Crawling process completed in {duration_str}\")\n        \n        # Call the progress callback if provided\n        if self.progress_callback:\n            self.progress_callback(self.get_status())\n    \n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get the current status of the crawling process.\"\"\"\n        return {\n            \"is_running\": self.is_running,\n            \"urls_found\": self.urls_found,\n            \"urls_processed\": self.urls_processed,\n            \"urls_succeeded\": self.urls_succeeded,\n            \"urls_failed\": self.urls_failed,\n            \"chunks_stored\": self.chunks_stored,\n            \"progress_percentage\": (self.urls_processed / self.urls_found * 100) if self.urls_found > 0 else 0,\n            \"logs\": self.logs,\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time\n        }\n    \n    @property\n    def is_completed(self) -> bool:\n        \"\"\"Return True if the crawling process is completed.\"\"\"\n        return not self.is_running and self.end_time is not None\n    \n    @property\n    def is_successful(self) -> bool:\n        \"\"\"Return True if the crawling process completed successfully.\"\"\"\n        return self.is_completed and self.urls_failed == 0 and self.urls_succeeded > 0\n\ndef chunk_text(text: str, chunk_size: int = 5000) -> List[str]:\n    \"\"\"Split text into chunks, respecting code blocks and paragraphs.\"\"\"\n    chunks = []\n    start = 0\n    text_length = len(text)\n\n    while start < text_length:\n        # Calculate end position\n        end = start + chunk_size\n\n        # If we're at the end of the text, just take what's left\n        if end >= text_length:\n            chunks.append(text[start:].strip())\n            break\n\n        # Try to find a code block boundary first (```)\n        chunk = text[start:end]\n        code_block = chunk.rfind('```')\n        if code_block != -1 and code_block > chunk_size * 0.3:\n            end = start + code_block\n\n        # If no code block, try to break at a paragraph\n        elif '\\n\\n' in chunk:\n            # Find the last paragraph break\n            last_break = chunk.rfind('\\n\\n')\n            if last_break > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size\n                end = start + last_break\n\n        # If no paragraph break, try to break at a sentence\n        elif '. ' in chunk:\n            # Find the last sentence break\n            last_period = chunk.rfind('. ')\n            if last_period > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size\n                end = start + last_period + 1\n\n        # Extract chunk and clean it up\n        chunk = text[start:end].strip()\n        if chunk:\n            chunks.append(chunk)\n\n        # Move start position for next chunk\n        start = max(start + 1, end)\n\n    return chunks\n\nasync def get_title_and_summary(chunk: str, url: str) -> Dict[str, str]:\n    \"\"\"Extract title and summary using GPT-4.\"\"\"\n    system_prompt = \"\"\"You are an AI that extracts titles and summaries from documentation chunks.\n    Return a JSON object with 'title' and 'summary' keys.\n    For the title: If this seems like the start of a document, extract its title. If it's a middle chunk, derive a descriptive title.\n    For the summary: Create a concise summary of the main points in this chunk.\n    Keep both title and summary concise but informative.\"\"\"\n    \n    try:\n        response = await llm_client.chat.completions.create(\n            model=get_env_var(\"PRIMARY_MODEL\") or \"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": f\"URL: {url}\\n\\nContent:\\n{chunk[:1000]}...\"}  # Send first 1000 chars for context\n            ],\n            response_format={ \"type\": \"json_object\" }\n        )\n        return json.loads(response.choices[0].message.content)\n    except Exception as e:\n        print(f\"Error getting title and summary: {e}\")\n        return {\"title\": \"Error processing title\", \"summary\": \"Error processing summary\"}\n\nasync def get_embedding(text: str) -> List[float]:\n    \"\"\"Get embedding vector from OpenAI.\"\"\"\n    try:\n        response = await embedding_client.embeddings.create(\n            model=embedding_model,\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Error getting embedding: {e}\")\n        return [0] * 1536  # Return zero vector on error\n\nasync def process_chunk(chunk: str, chunk_number: int, url: str) -> ProcessedChunk:\n    \"\"\"Process a single chunk of text.\"\"\"\n    # Get title and summary\n    extracted = await get_title_and_summary(chunk, url)\n    \n    # Get embedding\n    embedding = await get_embedding(chunk)\n    \n    # Create metadata\n    metadata = {\n        \"source\": \"pydantic_ai_docs\",\n        \"chunk_size\": len(chunk),\n        \"crawled_at\": datetime.now(timezone.utc).isoformat(),\n        \"url_path\": urlparse(url).path\n    }\n    \n    return ProcessedChunk(\n        url=url,\n        chunk_number=chunk_number,\n        title=extracted['title'],\n        summary=extracted['summary'],\n        content=chunk,  # Store the original chunk content\n        metadata=metadata,\n        embedding=embedding\n    )\n\nasync def insert_chunk(chunk: ProcessedChunk):\n    \"\"\"Insert a processed chunk into Supabase.\"\"\"\n    try:\n        data = {\n            \"url\": chunk.url,\n            \"chunk_number\": chunk.chunk_number,\n            \"title\": chunk.title,\n            \"summary\": chunk.summary,\n            \"content\": chunk.content,\n            \"metadata\": chunk.metadata,\n            \"embedding\": chunk.embedding\n        }\n        \n        result = supabase.table(\"site_pages\").insert(data).execute()\n        print(f\"Inserted chunk {chunk.chunk_number} for {chunk.url}\")\n        return result\n    except Exception as e:\n        print(f\"Error inserting chunk: {e}\")\n        return None\n\nasync def process_and_store_document(url: str, markdown: str, tracker: Optional[CrawlProgressTracker] = None):\n    \"\"\"Process a document and store its chunks in parallel.\"\"\"\n    # Split into chunks\n    chunks = chunk_text(markdown)\n    \n    if tracker:\n        tracker.log(f\"Split document into {len(chunks)} chunks for {url}\")\n        # Ensure UI gets updated\n        if tracker.progress_callback:\n            tracker.progress_callback(tracker.get_status())\n    else:\n        print(f\"Split document into {len(chunks)} chunks for {url}\")\n    \n    # Process chunks in parallel\n    tasks = [\n        process_chunk(chunk, i, url) \n        for i, chunk in enumerate(chunks)\n    ]\n    processed_chunks = await asyncio.gather(*tasks)\n    \n    if tracker:\n        tracker.log(f\"Processed {len(processed_chunks)} chunks for {url}\")\n        # Ensure UI gets updated\n        if tracker.progress_callback:\n            tracker.progress_callback(tracker.get_status())\n    else:\n        print(f\"Processed {len(processed_chunks)} chunks for {url}\")\n    \n    # Store chunks in parallel\n    insert_tasks = [\n        insert_chunk(chunk) \n        for chunk in processed_chunks\n    ]\n    await asyncio.gather(*insert_tasks)\n    \n    if tracker:\n        tracker.chunks_stored += len(processed_chunks)\n        tracker.log(f\"Stored {len(processed_chunks)} chunks for {url}\")\n        # Ensure UI gets updated\n        if tracker.progress_callback:\n            tracker.progress_callback(tracker.get_status())\n    else:\n        print(f\"Stored {len(processed_chunks)} chunks for {url}\")\n\ndef fetch_url_content(url: str) -> str:\n    \"\"\"Fetch content from a URL using requests and convert to markdown.\"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n    }\n    \n    try:\n        response = requests.get(url, headers=headers, timeout=30)\n        response.raise_for_status()\n        \n        # Convert HTML to Markdown\n        markdown = html_converter.handle(response.text)\n        \n        # Clean up the markdown\n        markdown = re.sub(r'\\n{3,}', '\\n\\n', markdown)  # Remove excessive newlines\n        \n        return markdown\n    except Exception as e:\n        raise Exception(f\"Error fetching {url}: {str(e)}\")\n\nasync def crawl_parallel_with_requests(urls: List[str], tracker: Optional[CrawlProgressTracker] = None, max_concurrent: int = 5):\n    \"\"\"Crawl multiple URLs in parallel with a concurrency limit using direct HTTP requests.\"\"\"\n    # Create a semaphore to limit concurrency\n    semaphore = asyncio.Semaphore(max_concurrent)\n    \n    async def process_url(url: str):\n        async with semaphore:\n            if tracker:\n                tracker.log(f\"Crawling: {url}\")\n                # Ensure UI gets updated\n                if tracker.progress_callback:\n                    tracker.progress_callback(tracker.get_status())\n            else:\n                print(f\"Crawling: {url}\")\n            \n            try:\n                # Use a thread pool to run the blocking HTTP request\n                loop = asyncio.get_running_loop()\n                if tracker:\n                    tracker.log(f\"Fetching content from: {url}\")\n                else:\n                    print(f\"Fetching content from: {url}\")\n                markdown = await loop.run_in_executor(None, fetch_url_content, url)\n                \n                if markdown:\n                    if tracker:\n                        tracker.urls_succeeded += 1\n                        tracker.log(f\"Successfully crawled: {url}\")\n                        # Ensure UI gets updated\n                        if tracker.progress_callback:\n                            tracker.progress_callback(tracker.get_status())\n                    else:\n                        print(f\"Successfully crawled: {url}\")\n                    \n                    await process_and_store_document(url, markdown, tracker)\n                else:\n                    if tracker:\n                        tracker.urls_failed += 1\n                        tracker.log(f\"Failed: {url} - No content retrieved\")\n                        # Ensure UI gets updated\n                        if tracker.progress_callback:\n                            tracker.progress_callback(tracker.get_status())\n                    else:\n                        print(f\"Failed: {url} - No content retrieved\")\n            except Exception as e:\n                if tracker:\n                    tracker.urls_failed += 1\n                    tracker.log(f\"Error processing {url}: {str(e)}\")\n                    # Ensure UI gets updated\n                    if tracker.progress_callback:\n                        tracker.progress_callback(tracker.get_status())\n                else:\n                    print(f\"Error processing {url}: {str(e)}\")\n            finally:\n                if tracker:\n                    tracker.urls_processed += 1\n                    # Ensure UI gets updated\n                    if tracker.progress_callback:\n                        tracker.progress_callback(tracker.get_status())\n\n        time.sleep(2)\n    \n    # Process all URLs in parallel with limited concurrency\n    if tracker:\n        tracker.log(f\"Processing {len(urls)} URLs with concurrency {max_concurrent}\")\n        # Ensure UI gets updated\n        if tracker.progress_callback:\n            tracker.progress_callback(tracker.get_status())\n    else:\n        print(f\"Processing {len(urls)} URLs with concurrency {max_concurrent}\")\n    await asyncio.gather(*[process_url(url) for url in urls])\n\ndef get_pydantic_ai_docs_urls() -> List[str]:\n    \"\"\"Get URLs from Pydantic AI docs sitemap.\"\"\"\n    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\"\n    try:\n        response = requests.get(sitemap_url)\n        response.raise_for_status()\n        \n        # Parse the XML\n        root = ElementTree.fromstring(response.content)\n        \n        # Extract all URLs from the sitemap\n        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n        \n        return urls\n    except Exception as e:\n        print(f\"Error fetching sitemap: {e}\")\n        return []\n\ndef clear_existing_records():\n    \"\"\"Clear all existing records with source='pydantic_ai_docs' from the site_pages table.\"\"\"\n    try:\n        result = supabase.table(\"site_pages\").delete().eq(\"metadata->>source\", \"pydantic_ai_docs\").execute()\n        print(\"Cleared existing pydantic_ai_docs records from site_pages\")\n        return result\n    except Exception as e:\n        print(f\"Error clearing existing records: {e}\")\n        return None\n\nasync def main_with_requests(tracker: Optional[CrawlProgressTracker] = None):\n    \"\"\"Main function using direct HTTP requests instead of browser automation.\"\"\"\n    try:\n        # Start tracking if tracker is provided\n        if tracker:\n            tracker.start()\n        else:\n            print(\"Starting crawling process...\")\n        \n        # Clear existing records first\n        if tracker:\n            tracker.log(\"Clearing existing Pydantic AI docs records...\")\n        else:\n            print(\"Clearing existing Pydantic AI docs records...\")\n        clear_existing_records()\n        if tracker:\n            tracker.log(\"Existing records cleared\")\n        else:\n            print(\"Existing records cleared\")\n        \n        # Get URLs from Pydantic AI docs\n        if tracker:\n            tracker.log(\"Fetching URLs from Pydantic AI sitemap...\")\n        else:\n            print(\"Fetching URLs from Pydantic AI sitemap...\")\n        urls = get_pydantic_ai_docs_urls()\n        \n        if not urls:\n            if tracker:\n                tracker.log(\"No URLs found to crawl\")\n                tracker.complete()\n            else:\n                print(\"No URLs found to crawl\")\n            return\n        \n        if tracker:\n            tracker.urls_found = len(urls)\n            tracker.log(f\"Found {len(urls)} URLs to crawl\")\n        else:\n            print(f\"Found {len(urls)} URLs to crawl\")\n        \n        # Crawl the URLs using direct HTTP requests\n        await crawl_parallel_with_requests(urls, tracker)\n        \n        # Mark as complete if tracker is provided\n        if tracker:\n            tracker.complete()\n        else:\n            print(\"Crawling process completed\")\n            \n    except Exception as e:\n        if tracker:\n            tracker.log(f\"Error in crawling process: {str(e)}\")\n            tracker.complete()\n        else:\n            print(f\"Error in crawling process: {str(e)}\")\n\ndef start_crawl_with_requests(progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None) -> CrawlProgressTracker:\n    \"\"\"Start the crawling process using direct HTTP requests in a separate thread and return the tracker.\"\"\"\n    tracker = CrawlProgressTracker(progress_callback)\n    \n    def run_crawl():\n        try:\n            asyncio.run(main_with_requests(tracker))\n        except Exception as e:\n            print(f\"Error in crawl thread: {e}\")\n            tracker.log(f\"Thread error: {str(e)}\")\n            tracker.complete()\n    \n    # Start the crawling process in a separate thread\n    thread = threading.Thread(target=run_crawl)\n    thread.daemon = True\n    thread.start()\n    \n    return tracker\n\nif __name__ == \"__main__\":    \n    # Run the main function directly\n    print(\"Starting crawler...\")\n    asyncio.run(main_with_requests())\n    print(\"Crawler finished.\")\n"}
{"type": "source_file", "path": "iterations/v2-agentic-workflow/streamlit_ui.py", "content": "from __future__ import annotations\nfrom typing import Literal, TypedDict\nfrom langgraph.types import Command\nfrom openai import AsyncOpenAI\nfrom supabase import Client\nimport streamlit as st\nimport logfire\nimport asyncio\nimport json\nimport uuid\nimport os\n\n# Import all the message part classes\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    SystemPromptPart,\n    UserPromptPart,\n    TextPart,\n    ToolCallPart,\n    ToolReturnPart,\n    RetryPromptPart,\n    ModelMessagesTypeAdapter\n)\n\nfrom archon_graph import agentic_flow\n\n# Load environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\nopenai_client=None\n\nbase_url = os.getenv('BASE_URL', 'https://api.openai.com/v1')\napi_key = os.getenv('LLM_API_KEY', 'no-llm-api-key-provided')\nis_ollama = \"localhost\" in base_url.lower()\n\nif is_ollama:\n    openai_client = AsyncOpenAI(base_url=base_url,api_key=api_key)\nelse:\n    openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nsupabase: Client = Client(\n    os.getenv(\"SUPABASE_URL\"),\n    os.getenv(\"SUPABASE_SERVICE_KEY\")\n)\n\n# Configure logfire to suppress warnings (optional)\nlogfire.configure(send_to_logfire='never')\n\n@st.cache_resource\ndef get_thread_id():\n    return str(uuid.uuid4())\n\nthread_id = get_thread_id()\n\nasync def run_agent_with_streaming(user_input: str):\n    \"\"\"\n    Run the agent with streaming text for the user_input prompt,\n    while maintaining the entire conversation in `st.session_state.messages`.\n    \"\"\"\n    config = {\n        \"configurable\": {\n            \"thread_id\": thread_id\n        }\n    }\n\n    # First message from user\n    if len(st.session_state.messages) == 1:\n        async for msg in agentic_flow.astream(\n                {\"latest_user_message\": user_input}, config, stream_mode=\"custom\"\n            ):\n                yield msg\n    # Continue the conversation\n    else:\n        async for msg in agentic_flow.astream(\n            Command(resume=user_input), config, stream_mode=\"custom\"\n        ):\n            yield msg\n\n\nasync def main():\n    st.title(\"Archon - Agent Builder\")\n    st.write(\"Describe to me an AI agent you want to build and I'll code it for you with Pydantic AI.\")\n    st.write(\"Example: Build me an AI agent that can search the web with the Brave API.\")\n\n    # Initialize chat history in session state if not present\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    # Display chat messages from history on app rerun\n    for message in st.session_state.messages:\n        message_type = message[\"type\"]\n        if message_type in [\"human\", \"ai\", \"system\"]:\n            with st.chat_message(message_type):\n                st.markdown(message[\"content\"])    \n\n    # Chat input for the user\n    user_input = st.chat_input(\"What do you want to build today?\")\n\n    if user_input:\n        # We append a new request to the conversation explicitly\n        st.session_state.messages.append({\"type\": \"human\", \"content\": user_input})\n        \n        # Display user prompt in the UI\n        with st.chat_message(\"user\"):\n            st.markdown(user_input)\n\n        # Display assistant response in chat message container\n        response_content = \"\"\n        with st.chat_message(\"assistant\"):\n            message_placeholder = st.empty()  # Placeholder for updating the message\n            # Run the async generator to fetch responses\n            async for chunk in run_agent_with_streaming(user_input):\n                response_content += chunk\n                # Update the placeholder with the current response content\n                message_placeholder.markdown(response_content)\n        \n        st.session_state.messages.append({\"type\": \"ai\", \"content\": response_content})\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "archon/refiner_agents/tools_refiner_agent.py", "content": "from __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\nimport logfire\nimport asyncio\nimport httpx\nimport os\nimport sys\nimport json\nfrom typing import List\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom openai import AsyncOpenAI\nfrom supabase import Client\n\n# Add the parent directory to sys.path to allow importing from the parent directory\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\nfrom utils.utils import get_env_var\nfrom archon.agent_prompts import tools_refiner_prompt\nfrom archon.agent_tools import (\n    retrieve_relevant_documentation_tool,\n    list_documentation_pages_tool,\n    get_page_content_tool\n)\n\nload_dotenv()\n\nprovider = get_env_var('LLM_PROVIDER') or 'OpenAI'\nllm = get_env_var('PRIMARY_MODEL') or 'gpt-4o-mini'\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\n\nmodel = AnthropicModel(llm, api_key=api_key) if provider == \"Anthropic\" else OpenAIModel(llm, base_url=base_url, api_key=api_key)\nembedding_model = get_env_var('EMBEDDING_MODEL') or 'text-embedding-3-small'\n\nlogfire.configure(send_to_logfire='if-token-present')\n\n@dataclass\nclass ToolsRefinerDeps:\n    supabase: Client\n    embedding_client: AsyncOpenAI\n\ntools_refiner_agent = Agent(\n    model,\n    system_prompt=tools_refiner_prompt,\n    deps_type=ToolsRefinerDeps,\n    retries=2\n)\n\n@tools_refiner_agent.tool\nasync def retrieve_relevant_documentation(ctx: RunContext[ToolsRefinerDeps], query: str) -> str:\n    \"\"\"\n    Retrieve relevant documentation chunks based on the query with RAG.\n    Make sure your searches always focus on implementing tools.\n    \n    Args:\n        ctx: The context including the Supabase client and OpenAI client\n        query: Your query to retrieve relevant documentation for implementing tools\n        \n    Returns:\n        A formatted string containing the top 4 most relevant documentation chunks\n    \"\"\"\n    return await retrieve_relevant_documentation_tool(ctx.deps.supabase, ctx.deps.embedding_client, query)\n\n@tools_refiner_agent.tool\nasync def list_documentation_pages(ctx: RunContext[ToolsRefinerDeps]) -> List[str]:\n    \"\"\"\n    Retrieve a list of all available Pydantic AI documentation pages.\n    This will give you all pages available, but focus on the ones related to tools.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    return await list_documentation_pages_tool(ctx.deps.supabase)\n\n@tools_refiner_agent.tool\nasync def get_page_content(ctx: RunContext[ToolsRefinerDeps], url: str) -> str:\n    \"\"\"\n    Retrieve the full content of a specific documentation page by combining all its chunks.\n    Only use this tool to get pages related to using tools with Pydantic AI.\n    \n    Args:\n        ctx: The context including the Supabase client\n        url: The URL of the page to retrieve\n        \n    Returns:\n        str: The complete page content with all chunks combined in order\n    \"\"\"\n    return await get_page_content_tool(ctx.deps.supabase, url)"}
{"type": "source_file", "path": "iterations/v3-mcp-support/archon/pydantic_ai_coder.py", "content": "from __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\nimport logfire\nimport asyncio\nimport httpx\nimport os\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom openai import AsyncOpenAI\nfrom supabase import Client\nfrom typing import List\n\nload_dotenv()\n\nllm = os.getenv('PRIMARY_MODEL', 'gpt-4o-mini')\nbase_url = os.getenv('BASE_URL', 'https://api.openai.com/v1')\napi_key = os.getenv('LLM_API_KEY', 'no-llm-api-key-provided')\nmodel = OpenAIModel(llm, base_url=base_url, api_key=api_key)\nembedding_model = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')\n\nlogfire.configure(send_to_logfire='if-token-present')\n\nis_ollama = \"localhost\" in base_url.lower()\n\n@dataclass\nclass PydanticAIDeps:\n    supabase: Client\n    openai_client: AsyncOpenAI\n    reasoner_output: str\n\nsystem_prompt = \"\"\"\n~~ CONTEXT: ~~\n\nYou are an expert at Pydantic AI - a Python AI agent framework that you have access to all the documentation to,\nincluding examples, an API reference, and other resources to help you build Pydantic AI agents.\n\n~~ GOAL: ~~\n\nYour only job is to help the user create an AI agent with Pydantic AI.\nThe user will describe the AI agent they want to build, or if they don't, guide them towards doing so.\nYou will take their requirements, and then search through the Pydantic AI documentation with the tools provided\nto find all the necessary information to create the AI agent with correct code.\n\nIt's important for you to search through multiple Pydantic AI documentation pages to get all the information you need.\nAlmost never stick to just one page - use RAG and the other documentation tools multiple times when you are creating\nan AI agent from scratch for the user.\n\n~~ STRUCTURE: ~~\n\nWhen you build an AI agent from scratch, split the agent into this files and give the code for each:\n- `agent.py`: The main agent file, which is where the Pydantic AI agent is defined.\n- `agent_tools.py`: A tools file for the agent, which is where all the tool functions are defined. Use this for more complex agents.\n- `agent_prompts.py`: A prompts file for the agent, which includes all system prompts and other prompts used by the agent. Use this when there are many prompts or large ones.\n- `.env.example`: An example `.env` file - specify each variable that the user will need to fill in and a quick comment above each one for how to do so.\n- `requirements.txt`: Don't include any versions, just the top level package names needed for the agent.\n\n~~ INSTRUCTIONS: ~~\n\n- Don't ask the user before taking an action, just do it. Always make sure you look at the documentation with the provided tools before writing any code.\n- When you first look at the documentation, always start with RAG.\nThen also always check the list of available documentation pages and retrieve the content of page(s) if it'll help.\n- Always let the user know when you didn't find the answer in the documentation or the right URL - be honest.\n- Helpful tip: when starting a new AI agent build, it's a good idea to look at the 'weather agent' in the docs as an example.\n- When starting a new AI agent build, always produce the full code for the AI agent - never tell the user to finish a tool/function.\n- When refining an existing AI agent build in a conversation, just share the code changes necessary.\n- Each time you respond to the user, ask them to let you know either if they need changes or the code looks good.\n\"\"\"\n\npydantic_ai_coder = Agent(\n    model,\n    system_prompt=system_prompt,\n    deps_type=PydanticAIDeps,\n    retries=2\n)\n\n@pydantic_ai_coder.system_prompt  \ndef add_reasoner_output(ctx: RunContext[str]) -> str:\n    return f\"\"\"\n    \\n\\nAdditional thoughts/instructions from the reasoner LLM. \n    This scope includes documentation pages for you to search as well: \n    {ctx.deps.reasoner_output}\n    \"\"\"\n    \n    # Add this in to get some crazy tool calling:\n    # You must get ALL documentation pages listed in the scope.\n\nasync def get_embedding(text: str, openai_client: AsyncOpenAI) -> List[float]:\n    \"\"\"Get embedding vector from OpenAI.\"\"\"\n    try:\n        response = await openai_client.embeddings.create(\n            model=embedding_model,\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Error getting embedding: {e}\")\n        return [0] * 1536  # Return zero vector on error\n\n@pydantic_ai_coder.tool\nasync def retrieve_relevant_documentation(ctx: RunContext[PydanticAIDeps], user_query: str) -> str:\n    \"\"\"\n    Retrieve relevant documentation chunks based on the query with RAG.\n    \n    Args:\n        ctx: The context including the Supabase client and OpenAI client\n        user_query: The user's question or query\n        \n    Returns:\n        A formatted string containing the top 5 most relevant documentation chunks\n    \"\"\"\n    try:\n        # Get the embedding for the query\n        query_embedding = await get_embedding(user_query, ctx.deps.openai_client)\n        \n        # Query Supabase for relevant documents\n        result = ctx.deps.supabase.rpc(\n            'match_site_pages',\n            {\n                'query_embedding': query_embedding,\n                'match_count': 5,\n                'filter': {'source': 'pydantic_ai_docs'}\n            }\n        ).execute()\n        \n        if not result.data:\n            return \"No relevant documentation found.\"\n            \n        # Format the results\n        formatted_chunks = []\n        for doc in result.data:\n            chunk_text = f\"\"\"\n# {doc['title']}\n\n{doc['content']}\n\"\"\"\n            formatted_chunks.append(chunk_text)\n            \n        # Join all chunks with a separator\n        return \"\\n\\n---\\n\\n\".join(formatted_chunks)\n        \n    except Exception as e:\n        print(f\"Error retrieving documentation: {e}\")\n        return f\"Error retrieving documentation: {str(e)}\"\n\nasync def list_documentation_pages_helper(supabase: Client) -> List[str]:\n    \"\"\"\n    Function to retrieve a list of all available Pydantic AI documentation pages.\n    This is called by the list_documentation_pages tool and also externally\n    to fetch documentation pages for the reasoner LLM.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    try:\n        # Query Supabase for unique URLs where source is pydantic_ai_docs\n        result = supabase.from_('site_pages') \\\n            .select('url') \\\n            .eq('metadata->>source', 'pydantic_ai_docs') \\\n            .execute()\n        \n        if not result.data:\n            return []\n            \n        # Extract unique URLs\n        urls = sorted(set(doc['url'] for doc in result.data))\n        return urls\n        \n    except Exception as e:\n        print(f\"Error retrieving documentation pages: {e}\")\n        return []        \n\n@pydantic_ai_coder.tool\nasync def list_documentation_pages(ctx: RunContext[PydanticAIDeps]) -> List[str]:\n    \"\"\"\n    Retrieve a list of all available Pydantic AI documentation pages.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    return await list_documentation_pages_helper(ctx.deps.supabase)\n\n@pydantic_ai_coder.tool\nasync def get_page_content(ctx: RunContext[PydanticAIDeps], url: str) -> str:\n    \"\"\"\n    Retrieve the full content of a specific documentation page by combining all its chunks.\n    \n    Args:\n        ctx: The context including the Supabase client\n        url: The URL of the page to retrieve\n        \n    Returns:\n        str: The complete page content with all chunks combined in order\n    \"\"\"\n    try:\n        # Query Supabase for all chunks of this URL, ordered by chunk_number\n        result = ctx.deps.supabase.from_('site_pages') \\\n            .select('title, content, chunk_number') \\\n            .eq('url', url) \\\n            .eq('metadata->>source', 'pydantic_ai_docs') \\\n            .order('chunk_number') \\\n            .execute()\n        \n        if not result.data:\n            return f\"No content found for URL: {url}\"\n            \n        # Format the page with its title and all chunks\n        page_title = result.data[0]['title'].split(' - ')[0]  # Get the main title\n        formatted_content = [f\"# {page_title}\\n\"]\n        \n        # Add each chunk's content\n        for chunk in result.data:\n            formatted_content.append(chunk['content'])\n            \n        # Join everything together\n        return \"\\n\\n\".join(formatted_content)\n        \n    except Exception as e:\n        print(f\"Error retrieving page content: {e}\")\n        return f\"Error retrieving page content: {str(e)}\""}
{"type": "source_file", "path": "iterations/v2-agentic-workflow/pydantic_ai_coder.py", "content": "from __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\nimport logfire\nimport asyncio\nimport httpx\nimport os\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom openai import AsyncOpenAI\nfrom supabase import Client\nfrom typing import List\n\nload_dotenv()\n\nllm = os.getenv('PRIMARY_MODEL', 'gpt-4o-mini')\nbase_url = os.getenv('BASE_URL', 'https://api.openai.com/v1')\napi_key = os.getenv('LLM_API_KEY', 'no-llm-api-key-provided')\nmodel = OpenAIModel(llm, base_url=base_url, api_key=api_key)\nembedding_model = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')\n\nlogfire.configure(send_to_logfire='if-token-present')\n\nis_ollama = \"localhost\" in base_url.lower()\n\n@dataclass\nclass PydanticAIDeps:\n    supabase: Client\n    openai_client: AsyncOpenAI\n    reasoner_output: str\n\nsystem_prompt = \"\"\"\n[ROLE AND CONTEXT]\nYou are a specialized AI agent engineer focused on building robust Pydantic AI agents. You have comprehensive access to the Pydantic AI documentation, including API references, usage guides, and implementation examples.\n\n[CORE RESPONSIBILITIES]\n1. Agent Development\n   - Create new agents from user requirements\n   - Complete partial agent implementations\n   - Optimize and debug existing agents\n   - Guide users through agent specification if needed\n\n2. Documentation Integration\n   - Systematically search documentation using RAG before any implementation\n   - Cross-reference multiple documentation pages for comprehensive understanding\n   - Validate all implementations against current best practices\n   - Notify users if documentation is insufficient for any requirement\n\n[CODE STRUCTURE AND DELIVERABLES]\nAll new agents must include these files with complete, production-ready code:\n\n1. agent.py\n   - Primary agent definition and configuration\n   - Core agent logic and behaviors\n   - No tool implementations allowed here\n\n2. agent_tools.py\n   - All tool function implementations\n   - Tool configurations and setup\n   - External service integrations\n\n3. agent_prompts.py\n   - System prompts\n   - Task-specific prompts\n   - Conversation templates\n   - Instruction sets\n\n4. .env.example\n   - Required environment variables\n   - Clear setup instructions in a comment above the variable for how to do so\n   - API configuration templates\n\n5. requirements.txt\n   - Core dependencies without versions\n   - User-specified packages included\n\n[DOCUMENTATION WORKFLOW]\n1. Initial Research\n   - Begin with RAG search for relevant documentation\n   - List all documentation pages using list_documentation_pages\n   - Retrieve specific page content using get_page_content\n   - Cross-reference the weather agent example for best practices\n\n2. Implementation\n   - Provide complete, working code implementations\n   - Never leave placeholder functions\n   - Include all necessary error handling\n   - Implement proper logging and monitoring\n\n3. Quality Assurance\n   - Verify all tool implementations are complete\n   - Ensure proper separation of concerns\n   - Validate environment variable handling\n   - Test critical path functionality\n\n[INTERACTION GUIDELINES]\n- Take immediate action without asking for permission\n- Always verify documentation before implementation\n- Provide honest feedback about documentation gaps\n- Include specific enhancement suggestions\n- Request user feedback on implementations\n- Maintain code consistency across files\n\n[ERROR HANDLING]\n- Implement robust error handling in all tools\n- Provide clear error messages\n- Include recovery mechanisms\n- Log important state changes\n\n[BEST PRACTICES]\n- Follow Pydantic AI naming conventions\n- Implement proper type hints\n- Include comprehensive docstrings, the agent uses this to understand what tools are for.\n- Maintain clean code structure\n- Use consistent formatting\n\"\"\"\n\npydantic_ai_coder = Agent(\n    model,\n    system_prompt=system_prompt,\n    deps_type=PydanticAIDeps,\n    retries=2\n)\n\n@pydantic_ai_coder.system_prompt  \ndef add_reasoner_output(ctx: RunContext[str]) -> str:\n    return f\"\"\"\n    \\n\\nAdditional thoughts/instructions from the reasoner LLM. \n    This scope includes documentation pages for you to search as well: \n    {ctx.deps.reasoner_output}\n    \"\"\"\n    \n    # Add this in to get some crazy tool calling:\n    # You must get ALL documentation pages listed in the scope.\n\nasync def get_embedding(text: str, openai_client: AsyncOpenAI) -> List[float]:\n    \"\"\"Get embedding vector from OpenAI.\"\"\"\n    try:\n        response = await openai_client.embeddings.create(\n            model= embedding_model,\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Error getting embedding: {e}\")\n        return [0] * 1536  # Return zero vector on error\n\n@pydantic_ai_coder.tool\nasync def retrieve_relevant_documentation(ctx: RunContext[PydanticAIDeps], user_query: str) -> str:\n    \"\"\"\n    Retrieve relevant documentation chunks based on the query with RAG.\n    \n    Args:\n        ctx: The context including the Supabase client and OpenAI client\n        user_query: The user's question or query\n        \n    Returns:\n        A formatted string containing the top 5 most relevant documentation chunks\n    \"\"\"\n    try:\n        # Get the embedding for the query\n        query_embedding = await get_embedding(user_query, ctx.deps.openai_client)\n        \n        # Query Supabase for relevant documents\n        result = ctx.deps.supabase.rpc(\n            'match_site_pages',\n            {\n                'query_embedding': query_embedding,\n                'match_count': 5,\n                'filter': {'source': 'pydantic_ai_docs'}\n            }\n        ).execute()\n        \n        if not result.data:\n            return \"No relevant documentation found.\"\n            \n        # Format the results\n        formatted_chunks = []\n        for doc in result.data:\n            chunk_text = f\"\"\"\n# {doc['title']}\n\n{doc['content']}\n\"\"\"\n            formatted_chunks.append(chunk_text)\n            \n        # Join all chunks with a separator\n        return \"\\n\\n---\\n\\n\".join(formatted_chunks)\n        \n    except Exception as e:\n        print(f\"Error retrieving documentation: {e}\")\n        return f\"Error retrieving documentation: {str(e)}\"\n\nasync def list_documentation_pages_helper(supabase: Client) -> List[str]:\n    \"\"\"\n    Function to retrieve a list of all available Pydantic AI documentation pages.\n    This is called by the list_documentation_pages tool and also externally\n    to fetch documentation pages for the reasoner LLM.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    try:\n        # Query Supabase for unique URLs where source is pydantic_ai_docs\n        result = supabase.from_('site_pages') \\\n            .select('url') \\\n            .eq('metadata->>source', 'pydantic_ai_docs') \\\n            .execute()\n        \n        if not result.data:\n            return []\n            \n        # Extract unique URLs\n        urls = sorted(set(doc['url'] for doc in result.data))\n        return urls\n        \n    except Exception as e:\n        print(f\"Error retrieving documentation pages: {e}\")\n        return []        \n\n@pydantic_ai_coder.tool\nasync def list_documentation_pages(ctx: RunContext[PydanticAIDeps]) -> List[str]:\n    \"\"\"\n    Retrieve a list of all available Pydantic AI documentation pages.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    return await list_documentation_pages_helper(ctx.deps.supabase)\n\n@pydantic_ai_coder.tool\nasync def get_page_content(ctx: RunContext[PydanticAIDeps], url: str) -> str:\n    \"\"\"\n    Retrieve the full content of a specific documentation page by combining all its chunks.\n    \n    Args:\n        ctx: The context including the Supabase client\n        url: The URL of the page to retrieve\n        \n    Returns:\n        str: The complete page content with all chunks combined in order\n    \"\"\"\n    try:\n        # Query Supabase for all chunks of this URL, ordered by chunk_number\n        result = ctx.deps.supabase.from_('site_pages') \\\n            .select('title, content, chunk_number') \\\n            .eq('url', url) \\\n            .eq('metadata->>source', 'pydantic_ai_docs') \\\n            .order('chunk_number') \\\n            .execute()\n        \n        if not result.data:\n            return f\"No content found for URL: {url}\"\n            \n        # Format the page with its title and all chunks\n        page_title = result.data[0]['title'].split(' - ')[0]  # Get the main title\n        formatted_content = [f\"# {page_title}\\n\"]\n        \n        # Add each chunk's content\n        for chunk in result.data:\n            formatted_content.append(chunk['content'])\n            \n        # Join everything together\n        return \"\\n\\n\".join(formatted_content)\n        \n    except Exception as e:\n        print(f\"Error retrieving page content: {e}\")\n        return f\"Error retrieving page content: {str(e)}\""}
{"type": "source_file", "path": "iterations/v3-mcp-support/archon/archon_graph.py", "content": "from pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai import Agent, RunContext\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing import TypedDict, Annotated, List, Any\nfrom langgraph.config import get_stream_writer\nfrom langgraph.types import interrupt\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\nfrom supabase import Client\nimport logfire\nimport os\nimport sys\n\n# Import the message classes from Pydantic AI\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelMessagesTypeAdapter\n)\n\n# Add the parent directory to Python path\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom archon.pydantic_ai_coder import pydantic_ai_coder, PydanticAIDeps, list_documentation_pages_helper\n\n# Load environment variables\nload_dotenv()\n\n# Configure logfire to suppress warnings (optional)\nlogfire.configure(send_to_logfire='never')\n\nbase_url = os.getenv('BASE_URL', 'https://api.openai.com/v1')\napi_key = os.getenv('LLM_API_KEY', 'no-llm-api-key-provided')\nis_ollama = \"localhost\" in base_url.lower()\nreasoner_llm_model = os.getenv('REASONER_MODEL', 'o3-mini')\nreasoner = Agent(  \n    OpenAIModel(reasoner_llm_model, base_url=base_url, api_key=api_key),\n    system_prompt='You are an expert at coding AI agents with Pydantic AI and defining the scope for doing so.',  \n)\n\nprimary_llm_model = os.getenv('PRIMARY_MODEL', 'gpt-4o-mini')\nrouter_agent = Agent(  \n    OpenAIModel(primary_llm_model, base_url=base_url, api_key=api_key),\n    system_prompt='Your job is to route the user message either to the end of the conversation or to continue coding the AI agent.',  \n)\n\nend_conversation_agent = Agent(  \n    OpenAIModel(primary_llm_model, base_url=base_url, api_key=api_key),\n    system_prompt='Your job is to end a conversation for creating an AI agent by giving instructions for how to execute the agent and they saying a nice goodbye to the user.',  \n)\n\nopenai_client=None\n\nif is_ollama:\n    openai_client = AsyncOpenAI(base_url=base_url,api_key=api_key)\nelse:\n    openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nsupabase: Client = Client(\n    os.getenv(\"SUPABASE_URL\"),\n    os.getenv(\"SUPABASE_SERVICE_KEY\")\n)\n\n# Define state schema\nclass AgentState(TypedDict):\n    latest_user_message: str\n    messages: Annotated[List[bytes], lambda x, y: x + y]\n    scope: str\n\n# Scope Definition Node with Reasoner LLM\nasync def define_scope_with_reasoner(state: AgentState):\n    # First, get the documentation pages so the reasoner can decide which ones are necessary\n    documentation_pages = await list_documentation_pages_helper(supabase)\n    documentation_pages_str = \"\\n\".join(documentation_pages)\n\n    # Then, use the reasoner to define the scope\n    prompt = f\"\"\"\n    User AI Agent Request: {state['latest_user_message']}\n    \n    Create detailed scope document for the AI agent including:\n    - Architecture diagram\n    - Core components\n    - External dependencies\n    - Testing strategy\n\n    Also based on these documentation pages available:\n\n    {documentation_pages_str}\n\n    Include a list of documentation pages that are relevant to creating this agent for the user in the scope document.\n    \"\"\"\n\n    result = await reasoner.run(prompt)\n    scope = result.data\n\n    # Get the directory one level up from the current file\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    parent_dir = os.path.dirname(current_dir)\n    scope_path = os.path.join(parent_dir, \"workbench\", \"scope.md\")\n    os.makedirs(os.path.join(parent_dir, \"workbench\"), exist_ok=True)\n\n    with open(scope_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(scope)\n    \n    return {\"scope\": scope}\n\n# Coding Node with Feedback Handling\nasync def coder_agent(state: AgentState, writer):    \n    # Prepare dependencies\n    deps = PydanticAIDeps(\n        supabase=supabase,\n        openai_client=openai_client,\n        reasoner_output=state['scope']\n    )\n\n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    # Run the agent in a stream\n    if is_ollama:\n        writer = get_stream_writer()\n        result = await pydantic_ai_coder.run(state['latest_user_message'], deps=deps, message_history= message_history)\n        writer(result.data)\n    else:\n        async with pydantic_ai_coder.run_stream(\n            state['latest_user_message'],\n            deps=deps,\n            message_history= message_history\n        ) as result:\n            # Stream partial text as it arrives\n            async for chunk in result.stream_text(delta=True):\n                writer(chunk)\n\n    # print(ModelMessagesTypeAdapter.validate_json(result.new_messages_json()))\n\n    return {\"messages\": [result.new_messages_json()]}\n\n# Interrupt the graph to get the user's next message\ndef get_next_user_message(state: AgentState):\n    value = interrupt({})\n\n    # Set the user's latest message for the LLM to continue the conversation\n    return {\n        \"latest_user_message\": value\n    }\n\n# Determine if the user is finished creating their AI agent or not\nasync def route_user_message(state: AgentState):\n    prompt = f\"\"\"\n    The user has sent a message: \n    \n    {state['latest_user_message']}\n\n    If the user wants to end the conversation, respond with just the text \"finish_conversation\".\n    If the user wants to continue coding the AI agent, respond with just the text \"coder_agent\".\n    \"\"\"\n\n    result = await router_agent.run(prompt)\n    next_action = result.data\n\n    if next_action == \"finish_conversation\":\n        return \"finish_conversation\"\n    else:\n        return \"coder_agent\"\n\n# End of conversation agent to give instructions for executing the agent\nasync def finish_conversation(state: AgentState, writer):    \n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    # Run the agent in a stream\n    if is_ollama:\n        writer = get_stream_writer()\n        result = await end_conversation_agent.run(state['latest_user_message'], message_history= message_history)\n        writer(result.data)   \n    else: \n        async with end_conversation_agent.run_stream(\n            state['latest_user_message'],\n            message_history= message_history\n        ) as result:\n            # Stream partial text as it arrives\n            async for chunk in result.stream_text(delta=True):\n                writer(chunk)\n\n    return {\"messages\": [result.new_messages_json()]}        \n\n# Build workflow\nbuilder = StateGraph(AgentState)\n\n# Add nodes\nbuilder.add_node(\"define_scope_with_reasoner\", define_scope_with_reasoner)\nbuilder.add_node(\"coder_agent\", coder_agent)\nbuilder.add_node(\"get_next_user_message\", get_next_user_message)\nbuilder.add_node(\"finish_conversation\", finish_conversation)\n\n# Set edges\nbuilder.add_edge(START, \"define_scope_with_reasoner\")\nbuilder.add_edge(\"define_scope_with_reasoner\", \"coder_agent\")\nbuilder.add_edge(\"coder_agent\", \"get_next_user_message\")\nbuilder.add_conditional_edges(\n    \"get_next_user_message\",\n    route_user_message,\n    {\"coder_agent\": \"coder_agent\", \"finish_conversation\": \"finish_conversation\"}\n)\nbuilder.add_edge(\"finish_conversation\", END)\n\n# Configure persistence\nmemory = MemorySaver()\nagentic_flow = builder.compile(checkpointer=memory)"}
{"type": "source_file", "path": "iterations/v3-mcp-support/archon/__init__.py", "content": ""}
{"type": "source_file", "path": "iterations/v2-agentic-workflow/archon_graph.py", "content": "from pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai import Agent, RunContext\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing import TypedDict, Annotated, List, Any\nfrom langgraph.config import get_stream_writer\nfrom langgraph.types import interrupt\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\nfrom supabase import Client\nimport logfire\nimport os\n\n# Import the message classes from Pydantic AI\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelMessagesTypeAdapter\n)\n\nfrom pydantic_ai_coder import pydantic_ai_coder, PydanticAIDeps, list_documentation_pages_helper\n\n# Load environment variables\nload_dotenv()\n\n# Configure logfire to suppress warnings (optional)\nlogfire.configure(send_to_logfire='never')\n\nbase_url = os.getenv('BASE_URL', 'https://api.openai.com/v1')\napi_key = os.getenv('LLM_API_KEY', 'no-llm-api-key-provided')\nis_ollama = \"localhost\" in base_url.lower()\nreasoner_llm_model = os.getenv('REASONER_MODEL', 'o3-mini')\nreasoner = Agent(  \n    OpenAIModel(reasoner_llm_model, base_url=base_url, api_key=api_key),\n    system_prompt='You are an expert at coding AI agents with Pydantic AI and defining the scope for doing so.',  \n)\n\nprimary_llm_model = os.getenv('PRIMARY_MODEL', 'gpt-4o-mini')\nrouter_agent = Agent(  \n    OpenAIModel(primary_llm_model, base_url=base_url, api_key=api_key),\n    system_prompt='Your job is to route the user message either to the end of the conversation or to continue coding the AI agent.',  \n)\n\nend_conversation_agent = Agent(  \n    OpenAIModel(primary_llm_model, base_url=base_url, api_key=api_key),\n    system_prompt='Your job is to end a conversation for creating an AI agent by giving instructions for how to execute the agent and they saying a nice goodbye to the user.',  \n)\n\nopenai_client=None\n\nif is_ollama:\n    openai_client = AsyncOpenAI(base_url=base_url,api_key=api_key)\nelse:\n    openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nsupabase: Client = Client(\n    os.getenv(\"SUPABASE_URL\"),\n    os.getenv(\"SUPABASE_SERVICE_KEY\")\n)\n\n# Define state schema\nclass AgentState(TypedDict):\n    latest_user_message: str\n    messages: Annotated[List[bytes], lambda x, y: x + y]\n    scope: str\n\n# Scope Definition Node with Reasoner LLM\nasync def define_scope_with_reasoner(state: AgentState):\n    # First, get the documentation pages so the reasoner can decide which ones are necessary\n    documentation_pages = await list_documentation_pages_helper(supabase)\n    documentation_pages_str = \"\\n\".join(documentation_pages)\n\n    # Then, use the reasoner to define the scope\n    prompt = f\"\"\"\n    User AI Agent Request: {state['latest_user_message']}\n    \n    Create detailed scope document for the AI agent including:\n    - Architecture diagram\n    - Core components\n    - External dependencies\n    - Testing strategy\n\n    Also based on these documentation pages available:\n\n    {documentation_pages_str}\n\n    Include a list of documentation pages that are relevant to creating this agent for the user in the scope document.\n    \"\"\"\n\n    result = await reasoner.run(prompt)\n    scope = result.data\n\n    # Save the scope to a file\n    scope_path = os.path.join(\"workbench\", \"scope.md\")\n    os.makedirs(\"workbench\", exist_ok=True)\n\n    with open(scope_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(scope)\n    \n    return {\"scope\": scope}\n\n# Coding Node with Feedback Handling\nasync def coder_agent(state: AgentState, writer):    \n    # Prepare dependencies\n    deps = PydanticAIDeps(\n        supabase=supabase,\n        openai_client=openai_client,\n        reasoner_output=state['scope']\n    )\n\n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    # Run the agent in a stream\n    if is_ollama:\n        writer = get_stream_writer()\n        result = await pydantic_ai_coder.run(state['latest_user_message'], deps=deps, message_history= message_history)\n        writer(result.data)\n    else:\n        async with pydantic_ai_coder.run_stream(\n            state['latest_user_message'],\n            deps=deps,\n            message_history= message_history\n        ) as result:\n            # Stream partial text as it arrives\n            async for chunk in result.stream_text(delta=True):\n                writer(chunk)\n\n    # print(ModelMessagesTypeAdapter.validate_json(result.new_messages_json()))\n\n    return {\"messages\": [result.new_messages_json()]}\n\n# Interrupt the graph to get the user's next message\ndef get_next_user_message(state: AgentState):\n    value = interrupt({})\n\n    # Set the user's latest message for the LLM to continue the conversation\n    return {\n        \"latest_user_message\": value\n    }\n\n# Determine if the user is finished creating their AI agent or not\nasync def route_user_message(state: AgentState):\n    prompt = f\"\"\"\n    The user has sent a message: \n    \n    {state['latest_user_message']}\n\n    If the user wants to end the conversation, respond with just the text \"finish_conversation\".\n    If the user wants to continue coding the AI agent, respond with just the text \"coder_agent\".\n    \"\"\"\n\n    result = await router_agent.run(prompt)\n    next_action = result.data\n\n    if next_action == \"finish_conversation\":\n        return \"finish_conversation\"\n    else:\n        return \"coder_agent\"\n\n# End of conversation agent to give instructions for executing the agent\nasync def finish_conversation(state: AgentState, writer):    \n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    # Run the agent in a stream\n    if is_ollama:\n        writer = get_stream_writer()\n        result = await end_conversation_agent.run(state['latest_user_message'], message_history= message_history)\n        writer(result.data)   \n    else: \n        async with end_conversation_agent.run_stream(\n            state['latest_user_message'],\n            message_history= message_history\n        ) as result:\n            # Stream partial text as it arrives\n            async for chunk in result.stream_text(delta=True):\n                writer(chunk)\n\n    return {\"messages\": [result.new_messages_json()]}        \n\n# Build workflow\nbuilder = StateGraph(AgentState)\n\n# Add nodes\nbuilder.add_node(\"define_scope_with_reasoner\", define_scope_with_reasoner)\nbuilder.add_node(\"coder_agent\", coder_agent)\nbuilder.add_node(\"get_next_user_message\", get_next_user_message)\nbuilder.add_node(\"finish_conversation\", finish_conversation)\n\n# Set edges\nbuilder.add_edge(START, \"define_scope_with_reasoner\")\nbuilder.add_edge(\"define_scope_with_reasoner\", \"coder_agent\")\nbuilder.add_edge(\"coder_agent\", \"get_next_user_message\")\nbuilder.add_conditional_edges(\n    \"get_next_user_message\",\n    route_user_message,\n    {\"coder_agent\": \"coder_agent\", \"finish_conversation\": \"finish_conversation\"}\n)\nbuilder.add_edge(\"finish_conversation\", END)\n\n# Configure persistence\nmemory = MemorySaver()\nagentic_flow = builder.compile(checkpointer=memory)"}
{"type": "source_file", "path": "archon/refiner_agents/prompt_refiner_agent.py", "content": "from __future__ import annotations as _annotations\n\nimport logfire\nimport os\nimport sys\nfrom pydantic_ai import Agent\nfrom dotenv import load_dotenv\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom supabase import Client\n\n# Add the parent directory to sys.path to allow importing from the parent directory\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\nfrom utils.utils import get_env_var\nfrom archon.agent_prompts import prompt_refiner_prompt\n\nload_dotenv()\n\nprovider = get_env_var('LLM_PROVIDER') or 'OpenAI'\nllm = get_env_var('PRIMARY_MODEL') or 'gpt-4o-mini'\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\n\nmodel = AnthropicModel(llm, api_key=api_key) if provider == \"Anthropic\" else OpenAIModel(llm, base_url=base_url, api_key=api_key)\n\nlogfire.configure(send_to_logfire='if-token-present')\n\nprompt_refiner_agent = Agent(\n    model,\n    system_prompt=prompt_refiner_prompt\n)"}
{"type": "source_file", "path": "iterations/v3-mcp-support/archon/crawl_pydantic_ai_docs.py", "content": "import os\nimport sys\nimport json\nimport asyncio\nimport requests\nfrom xml.etree import ElementTree\nfrom typing import List, Dict, Any\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\nfrom urllib.parse import urlparse\nfrom dotenv import load_dotenv\n\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom openai import AsyncOpenAI\nfrom supabase import create_client, Client\n\nload_dotenv()\n\n# Initialize OpenAI and Supabase clients\n\nbase_url = os.getenv('BASE_URL', 'https://api.openai.com/v1')\napi_key = os.getenv('LLM_API_KEY', 'no-llm-api-key-provided')\nis_ollama = \"localhost\" in base_url.lower()\n\nembedding_model = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')\n\nopenai_client=None\n\nif is_ollama:\n    openai_client = AsyncOpenAI(base_url=base_url,api_key=api_key)\nelse:\n    openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nsupabase: Client = create_client(\n    os.getenv(\"SUPABASE_URL\"),\n    os.getenv(\"SUPABASE_SERVICE_KEY\")\n)\n\n@dataclass\nclass ProcessedChunk:\n    url: str\n    chunk_number: int\n    title: str\n    summary: str\n    content: str\n    metadata: Dict[str, Any]\n    embedding: List[float]\n\ndef chunk_text(text: str, chunk_size: int = 5000) -> List[str]:\n    \"\"\"Split text into chunks, respecting code blocks and paragraphs.\"\"\"\n    chunks = []\n    start = 0\n    text_length = len(text)\n\n    while start < text_length:\n        # Calculate end position\n        end = start + chunk_size\n\n        # If we're at the end of the text, just take what's left\n        if end >= text_length:\n            chunks.append(text[start:].strip())\n            break\n\n        # Try to find a code block boundary first (```)\n        chunk = text[start:end]\n        code_block = chunk.rfind('```')\n        if code_block != -1 and code_block > chunk_size * 0.3:\n            end = start + code_block\n\n        # If no code block, try to break at a paragraph\n        elif '\\n\\n' in chunk:\n            # Find the last paragraph break\n            last_break = chunk.rfind('\\n\\n')\n            if last_break > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size\n                end = start + last_break\n\n        # If no paragraph break, try to break at a sentence\n        elif '. ' in chunk:\n            # Find the last sentence break\n            last_period = chunk.rfind('. ')\n            if last_period > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size\n                end = start + last_period + 1\n\n        # Extract chunk and clean it up\n        chunk = text[start:end].strip()\n        if chunk:\n            chunks.append(chunk)\n\n        # Move start position for next chunk\n        start = max(start + 1, end)\n\n    return chunks\n\nasync def get_title_and_summary(chunk: str, url: str) -> Dict[str, str]:\n    \"\"\"Extract title and summary using GPT-4.\"\"\"\n    system_prompt = \"\"\"You are an AI that extracts titles and summaries from documentation chunks.\n    Return a JSON object with 'title' and 'summary' keys.\n    For the title: If this seems like the start of a document, extract its title. If it's a middle chunk, derive a descriptive title.\n    For the summary: Create a concise summary of the main points in this chunk.\n    Keep both title and summary concise but informative.\"\"\"\n    \n    try:\n        response = await openai_client.chat.completions.create(\n            model=os.getenv(\"PRIMARY_MODEL\", \"gpt-4o-mini\"),\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": f\"URL: {url}\\n\\nContent:\\n{chunk[:1000]}...\"}  # Send first 1000 chars for context\n            ],\n            response_format={ \"type\": \"json_object\" }\n        )\n        return json.loads(response.choices[0].message.content)\n    except Exception as e:\n        print(f\"Error getting title and summary: {e}\")\n        return {\"title\": \"Error processing title\", \"summary\": \"Error processing summary\"}\n\nasync def get_embedding(text: str) -> List[float]:\n    \"\"\"Get embedding vector from OpenAI.\"\"\"\n    try:\n        response = await openai_client.embeddings.create(\n            model= embedding_model,\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Error getting embedding: {e}\")\n        return [0] * 1536  # Return zero vector on error\n\nasync def process_chunk(chunk: str, chunk_number: int, url: str) -> ProcessedChunk:\n    \"\"\"Process a single chunk of text.\"\"\"\n    # Get title and summary\n    extracted = await get_title_and_summary(chunk, url)\n    \n    # Get embedding\n    embedding = await get_embedding(chunk)\n    \n    # Create metadata\n    metadata = {\n        \"source\": \"pydantic_ai_docs\",\n        \"chunk_size\": len(chunk),\n        \"crawled_at\": datetime.now(timezone.utc).isoformat(),\n        \"url_path\": urlparse(url).path\n    }\n    \n    return ProcessedChunk(\n        url=url,\n        chunk_number=chunk_number,\n        title=extracted['title'],\n        summary=extracted['summary'],\n        content=chunk,  # Store the original chunk content\n        metadata=metadata,\n        embedding=embedding\n    )\n\nasync def insert_chunk(chunk: ProcessedChunk):\n    \"\"\"Insert a processed chunk into Supabase.\"\"\"\n    try:\n        data = {\n            \"url\": chunk.url,\n            \"chunk_number\": chunk.chunk_number,\n            \"title\": chunk.title,\n            \"summary\": chunk.summary,\n            \"content\": chunk.content,\n            \"metadata\": chunk.metadata,\n            \"embedding\": chunk.embedding\n        }\n        \n        result = supabase.table(\"site_pages\").insert(data).execute()\n        print(f\"Inserted chunk {chunk.chunk_number} for {chunk.url}\")\n        return result\n    except Exception as e:\n        print(f\"Error inserting chunk: {e}\")\n        return None\n\nasync def process_and_store_document(url: str, markdown: str):\n    \"\"\"Process a document and store its chunks in parallel.\"\"\"\n    # Split into chunks\n    chunks = chunk_text(markdown)\n    \n    # Process chunks in parallel\n    tasks = [\n        process_chunk(chunk, i, url) \n        for i, chunk in enumerate(chunks)\n    ]\n    processed_chunks = await asyncio.gather(*tasks)\n    \n    # Store chunks in parallel\n    insert_tasks = [\n        insert_chunk(chunk) \n        for chunk in processed_chunks\n    ]\n    await asyncio.gather(*insert_tasks)\n\nasync def crawl_parallel(urls: List[str], max_concurrent: int = 5):\n    \"\"\"Crawl multiple URLs in parallel with a concurrency limit.\"\"\"\n    browser_config = BrowserConfig(\n        headless=True,\n        verbose=False,\n        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n    )\n    crawl_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n\n    # Create the crawler instance\n    crawler = AsyncWebCrawler(config=browser_config)\n    await crawler.start()\n\n    try:\n        # Create a semaphore to limit concurrency\n        semaphore = asyncio.Semaphore(max_concurrent)\n        \n        async def process_url(url: str):\n            async with semaphore:\n                result = await crawler.arun(\n                    url=url,\n                    config=crawl_config,\n                    session_id=\"session1\"\n                )\n                if result.success:\n                    print(f\"Successfully crawled: {url}\")\n                    await process_and_store_document(url, result.markdown_v2.raw_markdown)\n                else:\n                    print(f\"Failed: {url} - Error: {result.error_message}\")\n        \n        # Process all URLs in parallel with limited concurrency\n        await asyncio.gather(*[process_url(url) for url in urls])\n    finally:\n        await crawler.close()\n\ndef get_pydantic_ai_docs_urls() -> List[str]:\n    \"\"\"Get URLs from Pydantic AI docs sitemap.\"\"\"\n    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\"\n    try:\n        response = requests.get(sitemap_url)\n        response.raise_for_status()\n        \n        # Parse the XML\n        root = ElementTree.fromstring(response.content)\n        \n        # Extract all URLs from the sitemap\n        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n        \n        return urls\n    except Exception as e:\n        print(f\"Error fetching sitemap: {e}\")\n        return []\n\nasync def clear_existing_records():\n    \"\"\"Clear all existing records with source='pydantic_ai_docs' from the site_pages table.\"\"\"\n    try:\n        result = supabase.table(\"site_pages\").delete().eq(\"metadata->>source\", \"pydantic_ai_docs\").execute()\n        print(\"Cleared existing pydantic_ai_docs records from site_pages\")\n        return result\n    except Exception as e:\n        print(f\"Error clearing existing records: {e}\")\n        return None\n\nasync def main():\n    # Clear existing records first\n    await clear_existing_records()\n    \n    # Get URLs from Pydantic AI docs\n    urls = get_pydantic_ai_docs_urls()\n    if not urls:\n        print(\"No URLs found to crawl\")\n        return\n    \n    print(f\"Found {len(urls)} URLs to crawl\")\n    await crawl_parallel(urls)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "iterations/v4-streamlit-ui-overhaul/archon/pydantic_ai_coder.py", "content": "from __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\nimport logfire\nimport asyncio\nimport httpx\nimport os\nimport sys\nimport json\nfrom typing import Dict, Any, List, Optional\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom openai import AsyncOpenAI\nfrom supabase import Client\n\n# Add the parent directory to sys.path to allow importing from the parent directory\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils.utils import get_env_var\n\nload_dotenv()\n\nllm = get_env_var('PRIMARY_MODEL') or 'gpt-4o-mini'\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\n\nis_ollama = \"localhost\" in base_url.lower()\nis_anthropic = \"anthropic\" in base_url.lower()\n\nmodel = AnthropicModel(llm, api_key=api_key) if is_anthropic else OpenAIModel(llm, base_url=base_url, api_key=api_key)\nembedding_model = get_env_var('EMBEDDING_MODEL') or 'text-embedding-3-small'\n\nlogfire.configure(send_to_logfire='if-token-present')\n\n@dataclass\nclass PydanticAIDeps:\n    supabase: Client\n    openai_client: AsyncOpenAI\n    reasoner_output: str\n\nsystem_prompt = \"\"\"\n[ROLE AND CONTEXT]\nYou are a specialized AI agent engineer focused on building robust Pydantic AI agents. You have comprehensive access to the Pydantic AI documentation, including API references, usage guides, and implementation examples.\n\n[CORE RESPONSIBILITIES]\n1. Agent Development\n   - Create new agents from user requirements\n   - Complete partial agent implementations\n   - Optimize and debug existing agents\n   - Guide users through agent specification if needed\n\n2. Documentation Integration\n   - Systematically search documentation using RAG before any implementation\n   - Cross-reference multiple documentation pages for comprehensive understanding\n   - Validate all implementations against current best practices\n   - Notify users if documentation is insufficient for any requirement\n\n[CODE STRUCTURE AND DELIVERABLES]\nAll new agents must include these files with complete, production-ready code:\n\n1. agent.py\n   - Primary agent definition and configuration\n   - Core agent logic and behaviors\n   - No tool implementations allowed here\n\n2. agent_tools.py\n   - All tool function implementations\n   - Tool configurations and setup\n   - External service integrations\n\n3. agent_prompts.py\n   - System prompts\n   - Task-specific prompts\n   - Conversation templates\n   - Instruction sets\n\n4. .env.example\n   - Required environment variables\n   - Clear setup instructions in a comment above the variable for how to do so\n   - API configuration templates\n\n5. requirements.txt\n   - Core dependencies without versions\n   - User-specified packages included\n\n[DOCUMENTATION WORKFLOW]\n1. Initial Research\n   - Begin with RAG search for relevant documentation\n   - List all documentation pages using list_documentation_pages\n   - Retrieve specific page content using get_page_content\n   - Cross-reference the weather agent example for best practices\n\n2. Implementation\n   - Provide complete, working code implementations\n   - Never leave placeholder functions\n   - Include all necessary error handling\n   - Implement proper logging and monitoring\n\n3. Quality Assurance\n   - Verify all tool implementations are complete\n   - Ensure proper separation of concerns\n   - Validate environment variable handling\n   - Test critical path functionality\n\n[INTERACTION GUIDELINES]\n- Take immediate action without asking for permission\n- Always verify documentation before implementation\n- Provide honest feedback about documentation gaps\n- Include specific enhancement suggestions\n- Request user feedback on implementations\n- Maintain code consistency across files\n\n[ERROR HANDLING]\n- Implement robust error handling in all tools\n- Provide clear error messages\n- Include recovery mechanisms\n- Log important state changes\n\n[BEST PRACTICES]\n- Follow Pydantic AI naming conventions\n- Implement proper type hints\n- Include comprehensive docstrings, the agent uses this to understand what tools are for.\n- Maintain clean code structure\n- Use consistent formatting\n\nHere is a good example of a Pydantic AI agent:\n\n```python\nfrom __future__ import annotations as _annotations\n\nimport asyncio\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport logfire\nfrom devtools import debug\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\n\n\n@dataclass\nclass Deps:\n    client: AsyncClient\n    weather_api_key: str | None\n    geo_api_key: str | None\n\n\nweather_agent = Agent(\n    'openai:gpt-4o',\n    # 'Be concise, reply with one sentence.' is enough for some models (like openai) to use\n    # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n    system_prompt=(\n        'Be concise, reply with one sentence.'\n        'Use the `get_lat_lng` tool to get the latitude and longitude of the locations, '\n        'then use the `get_weather` tool to get the weather.'\n    ),\n    deps_type=Deps,\n    retries=2,\n)\n\n\n@weather_agent.tool\nasync def get_lat_lng(\n    ctx: RunContext[Deps], location_description: str\n) -> dict[str, float]:\n    \\\"\\\"\\\"Get the latitude and longitude of a location.\n\n    Args:\n        ctx: The context.\n        location_description: A description of a location.\n    \\\"\\\"\\\"\n    if ctx.deps.geo_api_key is None:\n        # if no API key is provided, return a dummy response (London)\n        return {'lat': 51.1, 'lng': -0.1}\n\n    params = {\n        'q': location_description,\n        'api_key': ctx.deps.geo_api_key,\n    }\n    with logfire.span('calling geocode API', params=params) as span:\n        r = await ctx.deps.client.get('https://geocode.maps.co/search', params=params)\n        r.raise_for_status()\n        data = r.json()\n        span.set_attribute('response', data)\n\n    if data:\n        return {'lat': data[0]['lat'], 'lng': data[0]['lon']}\n    else:\n        raise ModelRetry('Could not find the location')\n\n\n@weather_agent.tool\nasync def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n    \\\"\\\"\\\"Get the weather at a location.\n\n    Args:\n        ctx: The context.\n        lat: Latitude of the location.\n        lng: Longitude of the location.\n    \\\"\\\"\\\"\n    if ctx.deps.weather_api_key is None:\n        # if no API key is provided, return a dummy response\n        return {'temperature': '21 °C', 'description': 'Sunny'}\n\n    params = {\n        'apikey': ctx.deps.weather_api_key,\n        'location': f'{lat},{lng}',\n        'units': 'metric',\n    }\n    with logfire.span('calling weather API', params=params) as span:\n        r = await ctx.deps.client.get(\n            'https://api.tomorrow.io/v4/weather/realtime', params=params\n        )\n        r.raise_for_status()\n        data = r.json()\n        span.set_attribute('response', data)\n\n    values = data['data']['values']\n    # https://docs.tomorrow.io/reference/data-layers-weather-codes\n    code_lookup = {\n        ...\n    }\n    return {\n        'temperature': f'{values[\"temperatureApparent\"]:0.0f}°C',\n        'description': code_lookup.get(values['weatherCode'], 'Unknown'),\n    }\n\n\nasync def main():\n    async with AsyncClient() as client:\n        # create a free API key at https://www.tomorrow.io/weather-api/\n        weather_api_key = os.getenv('WEATHER_API_KEY')\n        # create a free API key at https://geocode.maps.co/\n        geo_api_key = os.getenv('GEO_API_KEY')\n        deps = Deps(\n            client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\n        )\n        result = await weather_agent.run(\n            'What is the weather like in London and in Wiltshire?', deps=deps\n        )\n        debug(result)\n        print('Response:', result.data)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\"\"\"\n\npydantic_ai_coder = Agent(\n    model,\n    system_prompt=system_prompt,\n    deps_type=PydanticAIDeps,\n    retries=2\n)\n\n@pydantic_ai_coder.system_prompt  \ndef add_reasoner_output(ctx: RunContext[str]) -> str:\n    return f\"\"\"\n    \\n\\nAdditional thoughts/instructions from the reasoner LLM. \n    This scope includes documentation pages for you to search as well: \n    {ctx.deps.reasoner_output}\n    \"\"\"\n    \n    # Add this in to get some crazy tool calling:\n    # You must get ALL documentation pages listed in the scope.\n\nasync def get_embedding(text: str, openai_client: AsyncOpenAI) -> List[float]:\n    \"\"\"Get embedding vector from OpenAI.\"\"\"\n    try:\n        response = await openai_client.embeddings.create(\n            model=embedding_model,\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Error getting embedding: {e}\")\n        return [0] * 1536  # Return zero vector on error\n\n@pydantic_ai_coder.tool\nasync def retrieve_relevant_documentation(ctx: RunContext[PydanticAIDeps], user_query: str) -> str:\n    \"\"\"\n    Retrieve relevant documentation chunks based on the query with RAG.\n    \n    Args:\n        ctx: The context including the Supabase client and OpenAI client\n        user_query: The user's question or query\n        \n    Returns:\n        A formatted string containing the top 4 most relevant documentation chunks\n    \"\"\"\n    try:\n        # Get the embedding for the query\n        query_embedding = await get_embedding(user_query, ctx.deps.openai_client)\n        \n        # Query Supabase for relevant documents\n        result = ctx.deps.supabase.rpc(\n            'match_site_pages',\n            {\n                'query_embedding': query_embedding,\n                'match_count': 4,\n                'filter': {'source': 'pydantic_ai_docs'}\n            }\n        ).execute()\n        \n        if not result.data:\n            return \"No relevant documentation found.\"\n            \n        # Format the results\n        formatted_chunks = []\n        for doc in result.data:\n            chunk_text = f\"\"\"\n# {doc['title']}\n\n{doc['content']}\n\"\"\"\n            formatted_chunks.append(chunk_text)\n            \n        # Join all chunks with a separator\n        return \"\\n\\n---\\n\\n\".join(formatted_chunks)\n        \n    except Exception as e:\n        print(f\"Error retrieving documentation: {e}\")\n        return f\"Error retrieving documentation: {str(e)}\"\n\nasync def list_documentation_pages_helper(supabase: Client) -> List[str]:\n    \"\"\"\n    Function to retrieve a list of all available Pydantic AI documentation pages.\n    This is called by the list_documentation_pages tool and also externally\n    to fetch documentation pages for the reasoner LLM.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    try:\n        # Query Supabase for unique URLs where source is pydantic_ai_docs\n        result = supabase.from_('site_pages') \\\n            .select('url') \\\n            .eq('metadata->>source', 'pydantic_ai_docs') \\\n            .execute()\n        \n        if not result.data:\n            return []\n            \n        # Extract unique URLs\n        urls = sorted(set(doc['url'] for doc in result.data))\n        return urls\n        \n    except Exception as e:\n        print(f\"Error retrieving documentation pages: {e}\")\n        return []        \n\n@pydantic_ai_coder.tool\nasync def list_documentation_pages(ctx: RunContext[PydanticAIDeps]) -> List[str]:\n    \"\"\"\n    Retrieve a list of all available Pydantic AI documentation pages.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    return await list_documentation_pages_helper(ctx.deps.supabase)\n\n@pydantic_ai_coder.tool\nasync def get_page_content(ctx: RunContext[PydanticAIDeps], url: str) -> str:\n    \"\"\"\n    Retrieve the full content of a specific documentation page by combining all its chunks.\n    \n    Args:\n        ctx: The context including the Supabase client\n        url: The URL of the page to retrieve\n        \n    Returns:\n        str: The complete page content with all chunks combined in order\n    \"\"\"\n    try:\n        # Query Supabase for all chunks of this URL, ordered by chunk_number\n        result = ctx.deps.supabase.from_('site_pages') \\\n            .select('title, content, chunk_number') \\\n            .eq('url', url) \\\n            .eq('metadata->>source', 'pydantic_ai_docs') \\\n            .order('chunk_number') \\\n            .execute()\n        \n        if not result.data:\n            return f\"No content found for URL: {url}\"\n            \n        # Format the page with its title and all chunks\n        page_title = result.data[0]['title'].split(' - ')[0]  # Get the main title\n        formatted_content = [f\"# {page_title}\\n\"]\n        \n        # Add each chunk's content\n        for chunk in result.data:\n            formatted_content.append(chunk['content'])\n            \n        # Join everything together but limit the characters in case the page is massive (there are a coule big ones)\n        # This will be improved later so if the page is too big RAG will be performed on the page itself\n        return \"\\n\\n\".join(formatted_content)[:20000]\n        \n    except Exception as e:\n        print(f\"Error retrieving page content: {e}\")\n        return f\"Error retrieving page content: {str(e)}\""}
{"type": "source_file", "path": "iterations/v4-streamlit-ui-overhaul/run_docker.py", "content": "#!/usr/bin/env python\n\"\"\"\nSimple script to build and run Archon Docker containers.\n\"\"\"\n\nimport os\nimport subprocess\nimport platform\nimport time\nfrom pathlib import Path\n\ndef run_command(command, cwd=None):\n    \"\"\"Run a command and print output in real-time.\"\"\"\n    print(f\"Running: {' '.join(command)}\")\n    process = subprocess.Popen(\n        command,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=False,  \n        cwd=cwd\n    )\n    \n    for line in process.stdout:\n        try:\n            decoded_line = line.decode('utf-8', errors='replace')\n            print(decoded_line.strip())\n        except Exception as e:\n            print(f\"Error processing output: {e}\")\n    \n    process.wait()\n    return process.returncode\n\ndef check_docker():\n    \"\"\"Check if Docker is installed and running.\"\"\"\n    try:\n        subprocess.run(\n            [\"docker\", \"--version\"], \n            check=True, \n            stdout=subprocess.PIPE, \n            stderr=subprocess.PIPE\n        )\n        return True\n    except (subprocess.SubprocessError, FileNotFoundError):\n        print(\"Error: Docker is not installed or not in PATH\")\n        return False\n\ndef main():\n    \"\"\"Main function to build and run Archon containers.\"\"\"\n    # Check if Docker is available\n    if not check_docker():\n        return 1\n    \n    # Get the base directory\n    base_dir = Path(__file__).parent.absolute()\n    \n    # Check for .env file\n    env_file = base_dir / \".env\"\n    env_args = []\n    if env_file.exists():\n        print(f\"Using environment file: {env_file}\")\n        env_args = [\"--env-file\", str(env_file)]\n    else:\n        print(\"No .env file found. Continuing without environment variables.\")\n    \n    # Build the MCP container\n    print(\"\\n=== Building Archon MCP container ===\")\n    mcp_dir = base_dir / \"mcp\"\n    if run_command([\"docker\", \"build\", \"-t\", \"archon-mcp:latest\", \".\"], cwd=mcp_dir) != 0:\n        print(\"Error building MCP container\")\n        return 1\n    \n    # Build the main Archon container\n    print(\"\\n=== Building main Archon container ===\")\n    if run_command([\"docker\", \"build\", \"-t\", \"archon:latest\", \".\"], cwd=base_dir) != 0:\n        print(\"Error building main Archon container\")\n        return 1\n    \n    # Check if the container is already running\n    try:\n        result = subprocess.run(\n            [\"docker\", \"ps\", \"-q\", \"--filter\", \"name=archon-container\"],\n            check=True,\n            capture_output=True,\n            text=True\n        )\n        if result.stdout.strip():\n            print(\"\\n=== Stopping existing Archon container ===\")\n            run_command([\"docker\", \"stop\", \"archon-container\"])\n            run_command([\"docker\", \"rm\", \"archon-container\"])\n    except subprocess.SubprocessError:\n        pass\n    \n    # Run the Archon container\n    print(\"\\n=== Starting Archon container ===\")\n    cmd = [\n        \"docker\", \"run\", \"-d\",\n        \"--name\", \"archon-container\",\n        \"-p\", \"8501:8501\",\n        \"-p\", \"8100:8100\",\n        \"--add-host\", \"host.docker.internal:host-gateway\"\n    ]\n    \n    # Add environment variables if .env exists\n    if env_args:\n        cmd.extend(env_args)\n    \n    # Add image name\n    cmd.append(\"archon:latest\")\n    \n    if run_command(cmd) != 0:\n        print(\"Error starting Archon container\")\n        return 1\n    \n    # Wait a moment for the container to start\n    time.sleep(2)\n    \n    # Print success message\n    print(\"\\n=== Archon is now running! ===\")\n    print(\"-> Access the Streamlit UI at: http://localhost:8501\")\n    print(\"-> MCP container is ready to use - see the MCP tab in the UI.\")\n    print(\"\\nTo stop Archon, run: docker stop archon-container && docker rm archon-container\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    exit(main())\n"}
{"type": "source_file", "path": "iterations/v3-mcp-support/setup_mcp.py", "content": "import os\nimport json\nimport subprocess\nimport sys\n\ndef setup_venv():\n    # Get the absolute path to the current directory\n    base_path = os.path.abspath(os.path.dirname(__file__))\n    venv_path = os.path.join(base_path, 'venv')\n    venv_created = False\n\n    # Create virtual environment if it doesn't exist\n    if not os.path.exists(venv_path):\n        print(\"Creating virtual environment...\")\n        subprocess.run([sys.executable, '-m', 'venv', venv_path], check=True)\n        print(\"Virtual environment created successfully!\")\n        venv_created = True\n    else:\n        print(\"Virtual environment already exists.\")\n    \n    # Install requirements if we just created the venv\n    if venv_created:\n        print(\"\\nInstalling requirements...\")\n        # Use the venv's pip to install requirements\n        pip_path = os.path.join(venv_path, 'Scripts', 'pip.exe')\n        requirements_path = os.path.join(base_path, 'requirements.txt')\n        subprocess.run([pip_path, 'install', '-r', requirements_path], check=True)\n        print(\"Requirements installed successfully!\")\n\ndef generate_mcp_config():\n    # Get the absolute path to the current directory\n    base_path = os.path.abspath(os.path.dirname(__file__))\n    \n    # Construct the paths\n    python_path = os.path.join(base_path, 'venv', 'Scripts', 'python.exe')\n    server_script_path = os.path.join(base_path, 'mcp_server.py')\n    \n    # Create the config dictionary\n    config = {\n        \"mcpServers\": {\n            \"archon\": {\n                \"command\": python_path,\n                \"args\": [server_script_path]\n            }\n        }\n    }\n    \n    # Write the config to a file\n    config_path = os.path.join(base_path, 'mcp-config.json')\n    with open(config_path, 'w') as f:\n        json.dump(config, f, indent=2)\n\n    print(f\"\\nMCP configuration has been written to: {config_path}\")    \n    print(f\"\\nMCP configuration for Cursor:\\n\\n{python_path} {server_script_path}\")\n    print(\"\\nMCP configuration for Windsurf/Claude Desktop:\")\n    print(json.dumps(config, indent=2))\n\nif __name__ == '__main__':\n    setup_venv()\n    generate_mcp_config()\n"}
{"type": "source_file", "path": "iterations/v4-streamlit-ui-overhaul/archon/crawl_pydantic_ai_docs.py", "content": "import os\nimport sys\nimport asyncio\nimport threading\nimport subprocess\nimport requests\nimport json\nfrom typing import List, Dict, Any, Optional, Callable\nfrom xml.etree import ElementTree\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\nfrom urllib.parse import urlparse\nfrom dotenv import load_dotenv\nimport re\nimport html2text\n\n# Add the parent directory to sys.path to allow importing from the parent directory\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils.utils import get_env_var\n\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom openai import AsyncOpenAI\nfrom supabase import create_client, Client\n\nload_dotenv()\n\n# Initialize OpenAI and Supabase clients\n\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\nis_ollama = \"localhost\" in base_url.lower()\n\nembedding_model = get_env_var('EMBEDDING_MODEL') or 'text-embedding-3-small'\n\nopenai_client=None\n\nif is_ollama:\n    openai_client = AsyncOpenAI(base_url=base_url,api_key=api_key)\nelse:\n    openai_client = AsyncOpenAI(api_key=get_env_var(\"OPENAI_API_KEY\"))\n\nsupabase: Client = create_client(\n    get_env_var(\"SUPABASE_URL\"),\n    get_env_var(\"SUPABASE_SERVICE_KEY\")\n)\n\n# Initialize HTML to Markdown converter\nhtml_converter = html2text.HTML2Text()\nhtml_converter.ignore_links = False\nhtml_converter.ignore_images = False\nhtml_converter.ignore_tables = False\nhtml_converter.body_width = 0  # No wrapping\n\n@dataclass\nclass ProcessedChunk:\n    url: str\n    chunk_number: int\n    title: str\n    summary: str\n    content: str\n    metadata: Dict[str, Any]\n    embedding: List[float]\n\nclass CrawlProgressTracker:\n    \"\"\"Class to track progress of the crawling process.\"\"\"\n    \n    def __init__(self, \n                 progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None):\n        \"\"\"Initialize the progress tracker.\n        \n        Args:\n            progress_callback: Function to call with progress updates\n        \"\"\"\n        self.progress_callback = progress_callback\n        self.urls_found = 0\n        self.urls_processed = 0\n        self.urls_succeeded = 0\n        self.urls_failed = 0\n        self.chunks_stored = 0\n        self.logs = []\n        self.is_running = False\n        self.start_time = None\n        self.end_time = None\n    \n    def log(self, message: str):\n        \"\"\"Add a log message and update progress.\"\"\"\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        log_entry = f\"[{timestamp}] {message}\"\n        self.logs.append(log_entry)\n        print(message)  # Also print to console\n        \n        # Call the progress callback if provided\n        if self.progress_callback:\n            self.progress_callback(self.get_status())\n    \n    def start(self):\n        \"\"\"Mark the crawling process as started.\"\"\"\n        self.is_running = True\n        self.start_time = datetime.now()\n        self.log(\"Crawling process started\")\n        \n        # Call the progress callback if provided\n        if self.progress_callback:\n            self.progress_callback(self.get_status())\n    \n    def complete(self):\n        \"\"\"Mark the crawling process as completed.\"\"\"\n        self.is_running = False\n        self.end_time = datetime.now()\n        duration = self.end_time - self.start_time if self.start_time else None\n        duration_str = str(duration).split('.')[0] if duration else \"unknown\"\n        self.log(f\"Crawling process completed in {duration_str}\")\n        \n        # Call the progress callback if provided\n        if self.progress_callback:\n            self.progress_callback(self.get_status())\n    \n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get the current status of the crawling process.\"\"\"\n        return {\n            \"is_running\": self.is_running,\n            \"urls_found\": self.urls_found,\n            \"urls_processed\": self.urls_processed,\n            \"urls_succeeded\": self.urls_succeeded,\n            \"urls_failed\": self.urls_failed,\n            \"chunks_stored\": self.chunks_stored,\n            \"progress_percentage\": (self.urls_processed / self.urls_found * 100) if self.urls_found > 0 else 0,\n            \"logs\": self.logs,\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time\n        }\n    \n    @property\n    def is_completed(self) -> bool:\n        \"\"\"Return True if the crawling process is completed.\"\"\"\n        return not self.is_running and self.end_time is not None\n    \n    @property\n    def is_successful(self) -> bool:\n        \"\"\"Return True if the crawling process completed successfully.\"\"\"\n        return self.is_completed and self.urls_failed == 0 and self.urls_succeeded > 0\n\ndef chunk_text(text: str, chunk_size: int = 5000) -> List[str]:\n    \"\"\"Split text into chunks, respecting code blocks and paragraphs.\"\"\"\n    chunks = []\n    start = 0\n    text_length = len(text)\n\n    while start < text_length:\n        # Calculate end position\n        end = start + chunk_size\n\n        # If we're at the end of the text, just take what's left\n        if end >= text_length:\n            chunks.append(text[start:].strip())\n            break\n\n        # Try to find a code block boundary first (```)\n        chunk = text[start:end]\n        code_block = chunk.rfind('```')\n        if code_block != -1 and code_block > chunk_size * 0.3:\n            end = start + code_block\n\n        # If no code block, try to break at a paragraph\n        elif '\\n\\n' in chunk:\n            # Find the last paragraph break\n            last_break = chunk.rfind('\\n\\n')\n            if last_break > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size\n                end = start + last_break\n\n        # If no paragraph break, try to break at a sentence\n        elif '. ' in chunk:\n            # Find the last sentence break\n            last_period = chunk.rfind('. ')\n            if last_period > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size\n                end = start + last_period + 1\n\n        # Extract chunk and clean it up\n        chunk = text[start:end].strip()\n        if chunk:\n            chunks.append(chunk)\n\n        # Move start position for next chunk\n        start = max(start + 1, end)\n\n    return chunks\n\nasync def get_title_and_summary(chunk: str, url: str) -> Dict[str, str]:\n    \"\"\"Extract title and summary using GPT-4.\"\"\"\n    system_prompt = \"\"\"You are an AI that extracts titles and summaries from documentation chunks.\n    Return a JSON object with 'title' and 'summary' keys.\n    For the title: If this seems like the start of a document, extract its title. If it's a middle chunk, derive a descriptive title.\n    For the summary: Create a concise summary of the main points in this chunk.\n    Keep both title and summary concise but informative.\"\"\"\n    \n    try:\n        response = await openai_client.chat.completions.create(\n            model=get_env_var(\"PRIMARY_MODEL\") or \"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": f\"URL: {url}\\n\\nContent:\\n{chunk[:1000]}...\"}  # Send first 1000 chars for context\n            ],\n            response_format={ \"type\": \"json_object\" }\n        )\n        return json.loads(response.choices[0].message.content)\n    except Exception as e:\n        print(f\"Error getting title and summary: {e}\")\n        return {\"title\": \"Error processing title\", \"summary\": \"Error processing summary\"}\n\nasync def get_embedding(text: str) -> List[float]:\n    \"\"\"Get embedding vector from OpenAI.\"\"\"\n    try:\n        response = await openai_client.embeddings.create(\n            model= embedding_model,\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Error getting embedding: {e}\")\n        return [0] * 1536  # Return zero vector on error\n\nasync def process_chunk(chunk: str, chunk_number: int, url: str) -> ProcessedChunk:\n    \"\"\"Process a single chunk of text.\"\"\"\n    # Get title and summary\n    extracted = await get_title_and_summary(chunk, url)\n    \n    # Get embedding\n    embedding = await get_embedding(chunk)\n    \n    # Create metadata\n    metadata = {\n        \"source\": \"pydantic_ai_docs\",\n        \"chunk_size\": len(chunk),\n        \"crawled_at\": datetime.now(timezone.utc).isoformat(),\n        \"url_path\": urlparse(url).path\n    }\n    \n    return ProcessedChunk(\n        url=url,\n        chunk_number=chunk_number,\n        title=extracted['title'],\n        summary=extracted['summary'],\n        content=chunk,  # Store the original chunk content\n        metadata=metadata,\n        embedding=embedding\n    )\n\nasync def insert_chunk(chunk: ProcessedChunk):\n    \"\"\"Insert a processed chunk into Supabase.\"\"\"\n    try:\n        data = {\n            \"url\": chunk.url,\n            \"chunk_number\": chunk.chunk_number,\n            \"title\": chunk.title,\n            \"summary\": chunk.summary,\n            \"content\": chunk.content,\n            \"metadata\": chunk.metadata,\n            \"embedding\": chunk.embedding\n        }\n        \n        result = supabase.table(\"site_pages\").insert(data).execute()\n        print(f\"Inserted chunk {chunk.chunk_number} for {chunk.url}\")\n        return result\n    except Exception as e:\n        print(f\"Error inserting chunk: {e}\")\n        return None\n\nasync def process_and_store_document(url: str, markdown: str, tracker: Optional[CrawlProgressTracker] = None):\n    \"\"\"Process a document and store its chunks in parallel.\"\"\"\n    # Split into chunks\n    chunks = chunk_text(markdown)\n    \n    if tracker:\n        tracker.log(f\"Split document into {len(chunks)} chunks for {url}\")\n        # Ensure UI gets updated\n        if tracker.progress_callback:\n            tracker.progress_callback(tracker.get_status())\n    else:\n        print(f\"Split document into {len(chunks)} chunks for {url}\")\n    \n    # Process chunks in parallel\n    tasks = [\n        process_chunk(chunk, i, url) \n        for i, chunk in enumerate(chunks)\n    ]\n    processed_chunks = await asyncio.gather(*tasks)\n    \n    if tracker:\n        tracker.log(f\"Processed {len(processed_chunks)} chunks for {url}\")\n        # Ensure UI gets updated\n        if tracker.progress_callback:\n            tracker.progress_callback(tracker.get_status())\n    else:\n        print(f\"Processed {len(processed_chunks)} chunks for {url}\")\n    \n    # Store chunks in parallel\n    insert_tasks = [\n        insert_chunk(chunk) \n        for chunk in processed_chunks\n    ]\n    await asyncio.gather(*insert_tasks)\n    \n    if tracker:\n        tracker.chunks_stored += len(processed_chunks)\n        tracker.log(f\"Stored {len(processed_chunks)} chunks for {url}\")\n        # Ensure UI gets updated\n        if tracker.progress_callback:\n            tracker.progress_callback(tracker.get_status())\n    else:\n        print(f\"Stored {len(processed_chunks)} chunks for {url}\")\n\ndef fetch_url_content(url: str) -> str:\n    \"\"\"Fetch content from a URL using requests and convert to markdown.\"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n    }\n    \n    try:\n        response = requests.get(url, headers=headers, timeout=30)\n        response.raise_for_status()\n        \n        # Convert HTML to Markdown\n        markdown = html_converter.handle(response.text)\n        \n        # Clean up the markdown\n        markdown = re.sub(r'\\n{3,}', '\\n\\n', markdown)  # Remove excessive newlines\n        \n        return markdown\n    except Exception as e:\n        raise Exception(f\"Error fetching {url}: {str(e)}\")\n\nasync def crawl_parallel_with_requests(urls: List[str], tracker: Optional[CrawlProgressTracker] = None, max_concurrent: int = 5):\n    \"\"\"Crawl multiple URLs in parallel with a concurrency limit using direct HTTP requests.\"\"\"\n    # Create a semaphore to limit concurrency\n    semaphore = asyncio.Semaphore(max_concurrent)\n    \n    async def process_url(url: str):\n        async with semaphore:\n            if tracker:\n                tracker.log(f\"Crawling: {url}\")\n                # Ensure UI gets updated\n                if tracker.progress_callback:\n                    tracker.progress_callback(tracker.get_status())\n            else:\n                print(f\"Crawling: {url}\")\n            \n            try:\n                # Use a thread pool to run the blocking HTTP request\n                loop = asyncio.get_running_loop()\n                if tracker:\n                    tracker.log(f\"Fetching content from: {url}\")\n                else:\n                    print(f\"Fetching content from: {url}\")\n                markdown = await loop.run_in_executor(None, fetch_url_content, url)\n                \n                if markdown:\n                    if tracker:\n                        tracker.urls_succeeded += 1\n                        tracker.log(f\"Successfully crawled: {url}\")\n                        # Ensure UI gets updated\n                        if tracker.progress_callback:\n                            tracker.progress_callback(tracker.get_status())\n                    else:\n                        print(f\"Successfully crawled: {url}\")\n                    \n                    await process_and_store_document(url, markdown, tracker)\n                else:\n                    if tracker:\n                        tracker.urls_failed += 1\n                        tracker.log(f\"Failed: {url} - No content retrieved\")\n                        # Ensure UI gets updated\n                        if tracker.progress_callback:\n                            tracker.progress_callback(tracker.get_status())\n                    else:\n                        print(f\"Failed: {url} - No content retrieved\")\n            except Exception as e:\n                if tracker:\n                    tracker.urls_failed += 1\n                    tracker.log(f\"Error processing {url}: {str(e)}\")\n                    # Ensure UI gets updated\n                    if tracker.progress_callback:\n                        tracker.progress_callback(tracker.get_status())\n                else:\n                    print(f\"Error processing {url}: {str(e)}\")\n            finally:\n                if tracker:\n                    tracker.urls_processed += 1\n                    # Ensure UI gets updated\n                    if tracker.progress_callback:\n                        tracker.progress_callback(tracker.get_status())\n    \n    # Process all URLs in parallel with limited concurrency\n    if tracker:\n        tracker.log(f\"Processing {len(urls)} URLs with concurrency {max_concurrent}\")\n        # Ensure UI gets updated\n        if tracker.progress_callback:\n            tracker.progress_callback(tracker.get_status())\n    else:\n        print(f\"Processing {len(urls)} URLs with concurrency {max_concurrent}\")\n    await asyncio.gather(*[process_url(url) for url in urls])\n\ndef get_pydantic_ai_docs_urls() -> List[str]:\n    \"\"\"Get URLs from Pydantic AI docs sitemap.\"\"\"\n    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\"\n    try:\n        response = requests.get(sitemap_url)\n        response.raise_for_status()\n        \n        # Parse the XML\n        root = ElementTree.fromstring(response.content)\n        \n        # Extract all URLs from the sitemap\n        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n        \n        return urls\n    except Exception as e:\n        print(f\"Error fetching sitemap: {e}\")\n        return []\n\nasync def clear_existing_records():\n    \"\"\"Clear all existing records with source='pydantic_ai_docs' from the site_pages table.\"\"\"\n    try:\n        result = supabase.table(\"site_pages\").delete().eq(\"metadata->>source\", \"pydantic_ai_docs\").execute()\n        print(\"Cleared existing pydantic_ai_docs records from site_pages\")\n        return result\n    except Exception as e:\n        print(f\"Error clearing existing records: {e}\")\n        return None\n\nasync def main_with_requests(tracker: Optional[CrawlProgressTracker] = None):\n    \"\"\"Main function using direct HTTP requests instead of browser automation.\"\"\"\n    try:\n        # Start tracking if tracker is provided\n        if tracker:\n            tracker.start()\n        else:\n            print(\"Starting crawling process...\")\n        \n        # Clear existing records first\n        if tracker:\n            tracker.log(\"Clearing existing Pydantic AI docs records...\")\n        else:\n            print(\"Clearing existing Pydantic AI docs records...\")\n        await clear_existing_records()\n        if tracker:\n            tracker.log(\"Existing records cleared\")\n        else:\n            print(\"Existing records cleared\")\n        \n        # Get URLs from Pydantic AI docs\n        if tracker:\n            tracker.log(\"Fetching URLs from Pydantic AI sitemap...\")\n        else:\n            print(\"Fetching URLs from Pydantic AI sitemap...\")\n        urls = get_pydantic_ai_docs_urls()\n        \n        if not urls:\n            if tracker:\n                tracker.log(\"No URLs found to crawl\")\n                tracker.complete()\n            else:\n                print(\"No URLs found to crawl\")\n            return\n        \n        if tracker:\n            tracker.urls_found = len(urls)\n            tracker.log(f\"Found {len(urls)} URLs to crawl\")\n        else:\n            print(f\"Found {len(urls)} URLs to crawl\")\n        \n        # Crawl the URLs using direct HTTP requests\n        await crawl_parallel_with_requests(urls, tracker)\n        \n        # Mark as complete if tracker is provided\n        if tracker:\n            tracker.complete()\n        else:\n            print(\"Crawling process completed\")\n            \n    except Exception as e:\n        if tracker:\n            tracker.log(f\"Error in crawling process: {str(e)}\")\n            tracker.complete()\n        else:\n            print(f\"Error in crawling process: {str(e)}\")\n\ndef start_crawl_with_requests(progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None) -> CrawlProgressTracker:\n    \"\"\"Start the crawling process using direct HTTP requests in a separate thread and return the tracker.\"\"\"\n    tracker = CrawlProgressTracker(progress_callback)\n    \n    def run_crawl():\n        try:\n            asyncio.run(main_with_requests(tracker))\n        except Exception as e:\n            print(f\"Error in crawl thread: {e}\")\n            tracker.log(f\"Thread error: {str(e)}\")\n            tracker.complete()\n    \n    # Start the crawling process in a separate thread\n    thread = threading.Thread(target=run_crawl)\n    thread.daemon = True\n    thread.start()\n    \n    return tracker\n\nif __name__ == \"__main__\":    \n    # Run the main function directly\n    print(\"Starting crawler...\")\n    asyncio.run(main_with_requests())\n    print(\"Crawler finished.\")\n"}
{"type": "source_file", "path": "iterations/v5-parallel-specialized-agents/archon/agent_tools.py", "content": "from typing import Dict, Any, List, Optional\nfrom openai import AsyncOpenAI\nfrom supabase import Client\nimport sys\nimport os\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils.utils import get_env_var\n\nembedding_model = get_env_var('EMBEDDING_MODEL') or 'text-embedding-3-small'\n\nasync def get_embedding(text: str, embedding_client: AsyncOpenAI) -> List[float]:\n    \"\"\"Get embedding vector from OpenAI.\"\"\"\n    try:\n        response = await embedding_client.embeddings.create(\n            model=embedding_model,\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Error getting embedding: {e}\")\n        return [0] * 1536  # Return zero vector on error\n\nasync def retrieve_relevant_documentation_tool(supabase: Client, embedding_client: AsyncOpenAI, user_query: str) -> str:\n    try:\n        # Get the embedding for the query\n        query_embedding = await get_embedding(user_query, embedding_client)\n        \n        # Query Supabase for relevant documents\n        result = supabase.rpc(\n            'match_site_pages',\n            {\n                'query_embedding': query_embedding,\n                'match_count': 4,\n                'filter': {'source': 'pydantic_ai_docs'}\n            }\n        ).execute()\n        \n        if not result.data:\n            return \"No relevant documentation found.\"\n            \n        # Format the results\n        formatted_chunks = []\n        for doc in result.data:\n            chunk_text = f\"\"\"\n# {doc['title']}\n\n{doc['content']}\n\"\"\"\n            formatted_chunks.append(chunk_text)\n            \n        # Join all chunks with a separator\n        return \"\\n\\n---\\n\\n\".join(formatted_chunks)\n        \n    except Exception as e:\n        print(f\"Error retrieving documentation: {e}\")\n        return f\"Error retrieving documentation: {str(e)}\" \n\nasync def list_documentation_pages_tool(supabase: Client) -> List[str]:\n    \"\"\"\n    Function to retrieve a list of all available Pydantic AI documentation pages.\n    This is called by the list_documentation_pages tool and also externally\n    to fetch documentation pages for the reasoner LLM.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    try:\n        # Query Supabase for unique URLs where source is pydantic_ai_docs\n        result = supabase.from_('site_pages') \\\n            .select('url') \\\n            .eq('metadata->>source', 'pydantic_ai_docs') \\\n            .execute()\n        \n        if not result.data:\n            return []\n            \n        # Extract unique URLs\n        urls = sorted(set(doc['url'] for doc in result.data))\n        return urls\n        \n    except Exception as e:\n        print(f\"Error retrieving documentation pages: {e}\")\n        return []\n\nasync def get_page_content_tool(supabase: Client, url: str) -> str:\n    \"\"\"\n    Retrieve the full content of a specific documentation page by combining all its chunks.\n    \n    Args:\n        ctx: The context including the Supabase client\n        url: The URL of the page to retrieve\n        \n    Returns:\n        str: The complete page content with all chunks combined in order\n    \"\"\"\n    try:\n        # Query Supabase for all chunks of this URL, ordered by chunk_number\n        result = supabase.from_('site_pages') \\\n            .select('title, content, chunk_number') \\\n            .eq('url', url) \\\n            .eq('metadata->>source', 'pydantic_ai_docs') \\\n            .order('chunk_number') \\\n            .execute()\n        \n        if not result.data:\n            return f\"No content found for URL: {url}\"\n            \n        # Format the page with its title and all chunks\n        page_title = result.data[0]['title'].split(' - ')[0]  # Get the main title\n        formatted_content = [f\"# {page_title}\\n\"]\n        \n        # Add each chunk's content\n        for chunk in result.data:\n            formatted_content.append(chunk['content'])\n            \n        # Join everything together but limit the characters in case the page is massive (there are a coule big ones)\n        # This will be improved later so if the page is too big RAG will be performed on the page itself\n        return \"\\n\\n\".join(formatted_content)[:20000]\n        \n    except Exception as e:\n        print(f\"Error retrieving page content: {e}\")\n        return f\"Error retrieving page content: {str(e)}\"\n"}
{"type": "source_file", "path": "iterations/v3-mcp-support/streamlit_ui.py", "content": "from __future__ import annotations\nfrom typing import Literal, TypedDict\nfrom langgraph.types import Command\nfrom openai import AsyncOpenAI\nfrom supabase import Client\nimport streamlit as st\nimport logfire\nimport asyncio\nimport json\nimport uuid\nimport os\nimport sys\n\n# Import all the message part classes\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    SystemPromptPart,\n    UserPromptPart,\n    TextPart,\n    ToolCallPart,\n    ToolReturnPart,\n    RetryPromptPart,\n    ModelMessagesTypeAdapter\n)\n\n# Add the current directory to Python path\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\nfrom archon.archon_graph import agentic_flow\n\n# Load environment variables\nfrom dotenv import load_dotenv\nload_dotenv()\n\n\nopenai_client=None\nbase_url = os.getenv('BASE_URL', 'https://api.openai.com/v1')\napi_key = os.getenv('LLM_API_KEY', 'no-llm-api-key-provided')\nis_ollama = \"localhost\" in base_url.lower()\n\nif is_ollama:\n    openai_client = AsyncOpenAI(base_url=base_url,api_key=api_key)\nelse:\n    openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nsupabase: Client = Client(\n    os.getenv(\"SUPABASE_URL\"),\n    os.getenv(\"SUPABASE_SERVICE_KEY\")\n)\n\n# Configure logfire to suppress warnings (optional)\nlogfire.configure(send_to_logfire='never')\n\n@st.cache_resource\ndef get_thread_id():\n    return str(uuid.uuid4())\n\nthread_id = get_thread_id()\n\nasync def run_agent_with_streaming(user_input: str):\n    \"\"\"\n    Run the agent with streaming text for the user_input prompt,\n    while maintaining the entire conversation in `st.session_state.messages`.\n    \"\"\"\n    config = {\n        \"configurable\": {\n            \"thread_id\": thread_id\n        }\n    }\n\n    # First message from user\n    if len(st.session_state.messages) == 1:\n        async for msg in agentic_flow.astream(\n                {\"latest_user_message\": user_input}, config, stream_mode=\"custom\"\n            ):\n                yield msg\n    # Continue the conversation\n    else:\n        async for msg in agentic_flow.astream(\n            Command(resume=user_input), config, stream_mode=\"custom\"\n        ):\n            yield msg\n\n\nasync def main():\n    st.title(\"Archon - Agent Builder\")\n    st.write(\"Describe to me an AI agent you want to build and I'll code it for you with Pydantic AI.\")\n    st.write(\"Example: Build me an AI agent that can search the web with the Brave API.\")\n\n    # Initialize chat history in session state if not present\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    # Display chat messages from history on app rerun\n    for message in st.session_state.messages:\n        message_type = message[\"type\"]\n        if message_type in [\"human\", \"ai\", \"system\"]:\n            with st.chat_message(message_type):\n                st.markdown(message[\"content\"])    \n\n    # Chat input for the user\n    user_input = st.chat_input(\"What do you want to build today?\")\n\n    if user_input:\n        # We append a new request to the conversation explicitly\n        st.session_state.messages.append({\"type\": \"human\", \"content\": user_input})\n        \n        # Display user prompt in the UI\n        with st.chat_message(\"user\"):\n            st.markdown(user_input)\n\n        # Display assistant response in chat message container\n        response_content = \"\"\n        with st.chat_message(\"assistant\"):\n            message_placeholder = st.empty()  # Placeholder for updating the message\n            # Run the async generator to fetch responses\n            async for chunk in run_agent_with_streaming(user_input):\n                response_content += chunk\n                # Update the placeholder with the current response content\n                message_placeholder.markdown(response_content)\n        \n        st.session_state.messages.append({\"type\": \"ai\", \"content\": response_content})\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "iterations/v3-mcp-support/utils/utils.py", "content": "import os\nfrom datetime import datetime\nfrom functools import wraps\nimport inspect\n\ndef write_to_log(message: str):\n    \"\"\"Write a message to the logs.txt file in the workbench directory.\n    \n    Args:\n        message: The message to log\n    \"\"\"\n    # Get the directory one level up from the current file\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    parent_dir = os.path.dirname(current_dir)\n    workbench_dir = os.path.join(parent_dir, \"workbench\")\n    log_path = os.path.join(workbench_dir, \"logs.txt\")\n    os.makedirs(workbench_dir, exist_ok=True)\n\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    log_entry = f\"[{timestamp}] {message}\\n\"\n\n    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n        f.write(log_entry)\n\ndef log_node_execution(func):\n    \"\"\"Decorator to log the start and end of graph node execution.\n    \n    Args:\n        func: The async function to wrap\n    \"\"\"\n    @wraps(func)\n    async def wrapper(*args, **kwargs):\n        func_name = func.__name__\n        write_to_log(f\"Starting node: {func_name}\")\n        try:\n            result = await func(*args, **kwargs)\n            write_to_log(f\"Completed node: {func_name}\")\n            return result\n        except Exception as e:\n            write_to_log(f\"Error in node {func_name}: {str(e)}\")\n            raise\n    return wrapper\n"}
{"type": "source_file", "path": "iterations/v4-streamlit-ui-overhaul/future_enhancements.py", "content": "import streamlit as st\n\ndef future_enhancements_tab():\n    # Display the future enhancements and integrations interface\n    st.write(\"## Future Enhancements\")\n    \n    st.write(\"Explore what's coming next for Archon - from specialized multi-agent workflows to autonomous framework learning.\")\n    \n    # Future Iterations section\n    st.write(\"### Future Iterations\")\n    \n    # V5: Multi-Agent Coding Workflow\n    with st.expander(\"V5: Multi-Agent Coding Workflow\"):\n        st.write(\"Specialized agents for different parts of the agent creation process\")\n        \n        # Create a visual representation of multi-agent workflow\n        st.write(\"#### Multi-Agent Coding Architecture\")\n        \n        # Describe the parallel architecture\n        st.markdown(\"\"\"\n        The V5 architecture introduces specialized parallel agents that work simultaneously on different aspects of agent creation:\n        \n        1. **Reasoner Agent**: Analyzes requirements and plans the overall agent architecture\n        2. **Parallel Coding Agents**:\n           - **Prompt Engineering Agent**: Designs optimal prompts for the agent\n           - **Tool Definition Agent**: Creates tool specifications and interfaces\n           - **Dependencies Agent**: Identifies required libraries and dependencies\n           - **Model Selection Agent**: Determines the best model configuration\n        3. **Final Coding Agent**: Integrates all components into a cohesive agent\n        4. **Human-in-the-Loop**: Iterative refinement with the final coding agent\n        \"\"\")\n        \n        # Display parallel agents\n        st.write(\"#### Parallel Coding Agents\")\n        \n        col1, col2, col3, col4 = st.columns(4)\n        \n        with col1:\n            st.info(\"**Prompt Engineering Agent**\\n\\nDesigns optimal prompts for different agent scenarios\")\n        \n        with col2:\n            st.success(\"**Tool Definition Agent**\\n\\nCreates tool specifications and interfaces\")\n        \n        with col3:\n            st.warning(\"**Dependencies Agent**\\n\\nIdentifies required libraries and dependencies\")\n            \n        with col4:\n            st.error(\"**Model Selection Agent**\\n\\nDetermines the best model configuration\")\n        \n        # Updated flow chart visualization with better colors for ovals\n        st.graphviz_chart('''\n        digraph {\n            rankdir=LR;\n            node [shape=box, style=filled, color=lightblue];\n            \n            User [label=\"User Request\", shape=ellipse, style=filled, color=purple, fontcolor=black];\n            Reasoner [label=\"Reasoner\\nAgent\"];\n            \n            subgraph cluster_parallel {\n                label = \"Parallel Coding Agents\";\n                color = lightgrey;\n                style = filled;\n                \n                Prompt [label=\"Prompt\\nEngineering\\nAgent\", color=lightskyblue];\n                Tools [label=\"Tool\\nDefinition\\nAgent\", color=green];\n                Dependencies [label=\"Dependencies\\nAgent\", color=yellow];\n                Model [label=\"Model\\nSelection\\nAgent\", color=pink];\n            }\n            \n            Final [label=\"Final\\nCoding\\nAgent\"];\n            Human [label=\"Human-in-the-Loop\\nIteration\", shape=ellipse, style=filled, color=orange, fontcolor=black];\n            \n            User -> Reasoner;\n            Reasoner -> Prompt;\n            Reasoner -> Tools;\n            Reasoner -> Dependencies;\n            Reasoner -> Model;\n            \n            Prompt -> Final;\n            Tools -> Final;\n            Dependencies -> Final;\n            Model -> Final;\n            \n            Final -> Human;\n            Human -> Final [label=\"Feedback Loop\", color=red, constraint=false];\n        }\n        ''')\n        \n        st.write(\"#### Benefits of Parallel Agent Architecture\")\n        st.markdown(\"\"\"\n        - **Specialization**: Each agent focuses on its area of expertise\n        - **Efficiency**: Parallel processing reduces overall development time\n        - **Quality**: Specialized agents produce higher quality components\n        - **Flexibility**: Easy to add new specialized agents as needed\n        - **Scalability**: Architecture can handle complex agent requirements\n        \"\"\")\n    \n    # V6: Tool Library and Example Integration\n    with st.expander(\"V6: Tool Library and Example Integration\"):\n        st.write(\"Pre-built external tool and agent examples incorporation\")\n        st.write(\"\"\"\n            With pre-built tools, the agent can pull full functions from the tool library so it doesn't have to \n            create them from scratch. On top of that, pre-built agents will give Archon a starting point \n            so it doesn't have to build the agent structure from scratch either.\n            \"\"\")\n        \n        st.write(\"#### Example Integration Configuration\")\n        \n        # Add tabs for different aspects of V6\n        tool_tab, example_tab = st.tabs([\"Tool Library\", \"Example Agents\"])\n        \n        with tool_tab:\n            st.write(\"##### Example Tool Library Config (could be a RAG implementation too, still deciding)\")\n            \n            sample_config = \"\"\"\n            {\n                \"tool_library\": {\n                    \"web_tools\": {\n                        \"web_search\": {\n                            \"type\": \"search_engine\",\n                            \"api_key_env\": \"SEARCH_API_KEY\",\n                            \"description\": \"Search the web for information\"\n                        },\n                        \"web_browser\": {\n                            \"type\": \"browser\",\n                            \"description\": \"Navigate web pages and extract content\"\n                        }\n                    },\n                    \"data_tools\": {\n                        \"database_query\": {\n                            \"type\": \"sql_executor\",\n                            \"description\": \"Execute SQL queries against databases\"\n                        },\n                        \"data_analysis\": {\n                            \"type\": \"pandas_processor\",\n                            \"description\": \"Analyze data using pandas\"\n                        }\n                    },\n                    \"ai_service_tools\": {\n                        \"image_generation\": {\n                            \"type\": \"text_to_image\",\n                            \"api_key_env\": \"IMAGE_GEN_API_KEY\",\n                            \"description\": \"Generate images from text descriptions\"\n                        },\n                        \"text_to_speech\": {\n                            \"type\": \"tts_converter\",\n                            \"api_key_env\": \"TTS_API_KEY\",\n                            \"description\": \"Convert text to spoken audio\"\n                        }\n                    }\n                }\n            }\n            \"\"\"\n            \n            st.code(sample_config, language=\"json\")\n            \n            st.write(\"##### Pydantic AI Tool Definition Example\")\n            \n            pydantic_tool_example = \"\"\"\n            from pydantic_ai import Agent, RunContext, Tool\n            from typing import Union, List, Dict, Any\n            import requests\n            \n            @agent.tool\n            async def weather_tool(ctx: RunContext[Dict[str, Any]], location: str) -> str:\n                \\\"\\\"\\\"Get current weather information for a location.\n                \n                Args:\n                    location: The city and state/country (e.g., 'San Francisco, CA')\n                \n                Returns:\n                    A string with current weather conditions and temperature\n                \\\"\\\"\\\"\n                api_key = ctx.deps.get(\"WEATHER_API_KEY\")\n                if not api_key:\n                    return \"Error: Weather API key not configured\"\n                \n                try:\n                    url = f\"https://api.weatherapi.com/v1/current.json?key={api_key}&q={location}\"\n                    response = requests.get(url)\n                    data = response.json()\n                    \n                    if \"error\" in data:\n                        return f\"Error: {data['error']['message']}\"\n                    \n                    current = data[\"current\"]\n                    location_name = f\"{data['location']['name']}, {data['location']['country']}\"\n                    condition = current[\"condition\"][\"text\"]\n                    temp_c = current[\"temp_c\"]\n                    temp_f = current[\"temp_f\"]\n                    humidity = current[\"humidity\"]\n                    \n                    return f\"Weather in {location_name}: {condition}, {temp_c}°C ({temp_f}°F), {humidity}% humidity\"\n                except Exception as e:\n                    return f\"Error retrieving weather data: {str(e)}\"\n            \"\"\"\n            st.code(pydantic_tool_example, language=\"python\")\n            \n            st.write(\"##### Tool Usage in Agent\")\n            tool_usage_example = \"\"\"\n            async def use_weather_tool(location: str) -> str:\n                \\\"\\\"\\\"Search for weather information\\\"\\\"\\\"\n                tool = agent.get_tool(\"get_weather\")\n                result = await tool.execute({\"location\": location})\n                return result.content\n            \"\"\"\n            st.code(tool_usage_example, language=\"python\")\n        \n        with example_tab:\n            st.write(\"##### Example Agents\")\n            st.markdown(\"\"\"\n            V6 will include pre-built example agents that serve as templates and learning resources. These examples will be baked directly into agent prompts to improve results and consistency.\n            \n            **Benefits of Example Agents:**\n            - Provide concrete implementation patterns for common agent types\n            - Demonstrate best practices for tool usage and error handling\n            - Serve as starting points that can be customized for specific needs\n            - Improve consistency in agent behavior and output format\n            - Reduce the learning curve for new users\n            \"\"\")\n            \n            st.write(\"##### Example Agent Types\")\n            \n            example_agents = {\n                \"Research Assistant\": {\n                    \"description\": \"Performs comprehensive research on topics using web search and content analysis\",\n                    \"tools\": [\"web_search\", \"web_browser\", \"summarization\"],\n                    \"example_prompt\": \"Research the latest advancements in quantum computing and provide a summary\"\n                },\n                \"Data Analyst\": {\n                    \"description\": \"Analyzes datasets, generates visualizations, and provides insights\",\n                    \"tools\": [\"database_query\", \"data_analysis\", \"chart_generation\"],\n                    \"example_prompt\": \"Analyze this sales dataset and identify key trends over the past quarter\"\n                },\n                \"Content Creator\": {\n                    \"description\": \"Generates various types of content including text, images, and code\",\n                    \"tools\": [\"text_generation\", \"image_generation\", \"code_generation\"],\n                    \"example_prompt\": \"Create a blog post about sustainable living with accompanying images\"\n                },\n                \"Conversational Assistant\": {\n                    \"description\": \"Engages in helpful, informative conversations with natural dialogue\",\n                    \"tools\": [\"knowledge_base\", \"memory_management\", \"personalization\"],\n                    \"example_prompt\": \"I'd like to learn more about machine learning. Where should I start?\"\n                }\n            }\n            \n            # Create a table of example agents\n            example_data = {\n                \"Agent Type\": list(example_agents.keys()),\n                \"Description\": [example_agents[a][\"description\"] for a in example_agents],\n                \"Core Tools\": [\", \".join(example_agents[a][\"tools\"]) for a in example_agents]\n            }\n            \n            st.dataframe(example_data, use_container_width=True)\n            \n            st.write(\"##### Example Agent Implementation\")\n            \n            st.code(\"\"\"\n            # Example Weather Agent based on Pydantic AI documentation\n            from pydantic_ai import Agent, RunContext\n            from typing import Dict, Any\n            from dataclasses import dataclass\n            from httpx import AsyncClient\n            \n            @dataclass\n            class WeatherDeps:\n                client: AsyncClient\n                weather_api_key: str | None\n                geo_api_key: str | None\n            \n            # Create the agent with appropriate system prompt\n            weather_agent = Agent(\n                'openai:gpt-4o',\n                system_prompt=(\n                    'Be concise, reply with one sentence. '\n                    'Use the `get_lat_lng` tool to get the latitude and longitude of locations, '\n                    'then use the `get_weather` tool to get the weather.'\n                ),\n                deps_type=WeatherDeps,\n            )\n            \n            @weather_agent.tool\n            async def get_lat_lng(ctx: RunContext[WeatherDeps], location_description: str) -> Dict[str, float]:\n                \\\"\\\"\\\"Get the latitude and longitude of a location.\n                \n                Args:\n                    location_description: A description of a location (e.g., 'London, UK')\n                \n                Returns:\n                    Dictionary with lat and lng keys\n                \\\"\\\"\\\"\n                if ctx.deps.geo_api_key is None:\n                    # Return dummy data if no API key\n                    return {'lat': 51.1, 'lng': -0.1}\n                \n                # Call geocoding API\n                params = {'q': location_description, 'api_key': ctx.deps.geo_api_key}\n                r = await ctx.deps.client.get('https://geocode.maps.co/search', params=params)\n                r.raise_for_status()\n                data = r.json()\n                \n                if data:\n                    return {'lat': float(data[0]['lat']), 'lng': float(data[0]['lon'])}\n                else:\n                    return {'error': 'Location not found'}\n            \n            @weather_agent.tool\n            async def get_weather(ctx: RunContext[WeatherDeps], lat: float, lng: float) -> Dict[str, Any]:\n                \\\"\\\"\\\"Get the weather at a location.\n                \n                Args:\n                    lat: Latitude of the location\n                    lng: Longitude of the location\n                \n                Returns:\n                    Dictionary with temperature and description\n                \\\"\\\"\\\"\n                if ctx.deps.weather_api_key is None:\n                    # Return dummy data if no API key\n                    return {'temperature': '21°C', 'description': 'Sunny'}\n                \n                # Call weather API\n                params = {\n                    'apikey': ctx.deps.weather_api_key,\n                    'location': f'{lat},{lng}',\n                    'units': 'metric',\n                }\n                r = await ctx.deps.client.get(\n                    'https://api.tomorrow.io/v4/weather/realtime', \n                    params=params\n                )\n                r.raise_for_status()\n                data = r.json()\n                \n                values = data['data']['values']\n                weather_codes = {\n                    1000: 'Clear, Sunny',\n                    1100: 'Mostly Clear',\n                    1101: 'Partly Cloudy',\n                    4001: 'Rain',\n                    5000: 'Snow',\n                    8000: 'Thunderstorm',\n                }\n                \n                return {\n                    'temperature': f'{values[\"temperatureApparent\"]:0.0f}°C',\n                    'description': weather_codes.get(values['weatherCode'], 'Unknown'),\n                }\n            \n            # Example usage\n            async def get_weather_report(location: str) -> str:\n                \\\"\\\"\\\"Get weather report for a location.\\\"\\\"\\\"\n                async with AsyncClient() as client:\n                    deps = WeatherDeps(\n                        client=client,\n                        weather_api_key=\"YOUR_API_KEY\",  # Replace with actual key\n                        geo_api_key=\"YOUR_API_KEY\",      # Replace with actual key\n                    )\n                    result = await weather_agent.run(\n                        f\"What is the weather like in {location}?\", \n                        deps=deps\n                    )\n                    return result.data\n            \"\"\", language=\"python\")\n            \n            st.info(\"\"\"\n            **In-Context Learning with Examples**\n            \n            These example agents will be used in the system prompt for Archon, providing concrete examples that help the LLM understand the expected structure and quality of agent code. This approach leverages in-context learning to significantly improve code generation quality and consistency.\n            \"\"\")\n    \n    # V7: LangGraph Documentation\n    with st.expander(\"V7: LangGraph Documentation\"):\n        st.write(\"Integrating LangGraph for complex agent workflows\")\n        \n        st.markdown(\"\"\"\n        ### Pydantic AI vs LangGraph with Pydantic AI\n        \n        V7 will integrate LangGraph to enable complex agent workflows while maintaining compatibility with Pydantic AI agents.\n        This allows for creating sophisticated multi-agent systems with well-defined state management and workflow control.\n        \"\"\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.markdown(\"#### Pydantic AI Agent\")\n            st.markdown(\"Simple, standalone agent with tools\")\n            \n            pydantic_agent_code = \"\"\"\n            # Simple Pydantic AI Weather Agent\n            from pydantic_ai import Agent, RunContext\n            from typing import Dict, Any\n            from dataclasses import dataclass\n            from httpx import AsyncClient\n            \n            @dataclass\n            class WeatherDeps:\n                client: AsyncClient\n                weather_api_key: str | None\n            \n            # Create the agent\n            weather_agent = Agent(\n                'openai:gpt-4o',\n                system_prompt=\"You provide weather information.\",\n                deps_type=WeatherDeps,\n            )\n            \n            @weather_agent.tool\n            async def get_weather(\n                ctx: RunContext[WeatherDeps], \n                location: str\n            ) -> Dict[str, Any]:\n                \\\"\\\"\\\"Get weather for a location.\\\"\\\"\\\"\n                # Implementation details...\n                return {\"temperature\": \"21°C\", \"description\": \"Sunny\"}\n            \n            # Usage\n            async def main():\n                async with AsyncClient() as client:\n                    deps = WeatherDeps(\n                        client=client,\n                        weather_api_key=\"API_KEY\"\n                    )\n                    result = await weather_agent.run(\n                        \"What's the weather in London?\", \n                        deps=deps\n                    )\n                    print(result.data)\n            \"\"\"\n            st.code(pydantic_agent_code, language=\"python\")\n        \n        with col2:\n            st.markdown(\"#### LangGraph with Pydantic AI Agent\")\n            st.markdown(\"Complex workflow using Pydantic AI agents in a graph\")\n            \n            langgraph_code = \"\"\"\n            # LangGraph with Pydantic AI Agents\n            from pydantic_ai import Agent, RunContext\n            from typing import TypedDict, Literal\n            from dataclasses import dataclass\n            from httpx import AsyncClient\n            from langgraph.graph import StateGraph, START, END\n            \n            # Define state for LangGraph\n            class GraphState(TypedDict):\n                query: str\n                weather_result: str\n                verified: bool\n                response: str\n            \n            # Create a verifier agent\n            verifier_agent = Agent(\n                'openai:gpt-4o',\n                system_prompt=(\n                    \"You verify weather information for accuracy and completeness. \"\n                    \"Check if the weather report includes temperature, conditions, \"\n                    \"and is properly formatted.\"\n                )\n            )\n            \n            # Define nodes for the graph\n            async def get_weather_info(state: GraphState) -> GraphState:\n                \\\"\\\"\\\"Use the weather agent to get weather information.\\\"\\\"\\\"\n                # Simply use the weather agent directly\n                async with AsyncClient() as client:\n                    deps = WeatherDeps(\n                        client=client,\n                        weather_api_key=\"API_KEY\"\n                    )\n                    result = await weather_agent.run(\n                        state[\"query\"], \n                        deps=deps\n                    )\n                return {\"weather_result\": result.data}\n            \n            async def verify_information(state: GraphState) -> GraphState:\n                \\\"\\\"\\\"Use the verifier agent to check the weather information.\\\"\\\"\\\"\n                result = await verifier_agent.run(\n                    f\"Verify this weather information: {state['weather_result']}\"\n                )\n                # Simple verification logic\n                verified = \"accurate\" in result.data.lower()\n                return {\"verified\": verified}\n            \n            async def route(state: GraphState) -> Literal[\"regenerate\", \"finalize\"]:\n                \"\\\"\\\"Decide whether to regenerate or finalize based on verification.\\\"\\\"\\\"\n                if state[\"verified\"]:\n                    return \"finalize\"\n                else:\n                    return \"regenerate\"\n            \n            async def regenerate_response(state: GraphState) -> GraphState:\n                \\\"\\\"\\\"Regenerate a better response if verification failed.\\\"\\\"\\\"\n                result = await verifier_agent.run(\n                result = await weather_agent.run(\n                    f\"Please provide more detailed weather information for: {state['query']}\"\n                )\n                return {\"weather_result\": result.data, \"verified\": True}\n            \n            async def finalize_response(state: GraphState) -> GraphState:\n                \\\"\\\"\\\"Format the final response.\\\"\\\"\\\"\n                return {\"response\": f\"Verified Weather Report: {state['weather_result']}\"}\n            \n            # Build the graph\n            workflow = StateGraph(GraphState)\n            \n            # Add nodes\n            workflow.add_node(\"get_weather\", get_weather_info)\n            workflow.add_node(\"verify\", verify_information)\n            workflow.add_node(\"regenerate\", regenerate_response)\n            workflow.add_node(\"finalize\", finalize_response)\n            \n            # Add edges\n            workflow.add_edge(START, \"get_weather\")\n            workflow.add_edge(\"get_weather\", \"verify\")\n            \n            # Add conditional edges based on verification\n            workflow.add_conditional_edges(\n                \"verify\",\n                route,\n                {\n                    \"regenerate\": \"regenerate\",\n                    \"finalize\": \"finalize\"\n                }\n            )\n            \n            workflow.add_edge(\"regenerate\", \"finalize\")\n            workflow.add_edge(\"finalize\", END)\n            \n            # Compile the graph\n            app = workflow.compile()\n            \n            # Usage\n            async def main():\n                result = await app.ainvoke({\n                    \"query\": \"What's the weather in London?\",\n                    \"verified\": False\n                })\n                print(result[\"response\"])\n            \"\"\"\n            st.code(langgraph_code, language=\"python\")\n        \n        st.markdown(\"\"\"\n        ### Key Benefits of Integration\n        \n        1. **Workflow Management**: LangGraph provides a structured way to define complex agent workflows with clear state transitions.\n        \n        2. **Reusability**: Pydantic AI agents can be reused within LangGraph nodes, maintaining their tool capabilities.\n        \n        3. **Visualization**: LangGraph offers built-in visualization of agent workflows, making it easier to understand and debug complex systems.\n        \n        4. **State Management**: The typed state in LangGraph ensures type safety and clear data flow between nodes.\n        \n        5. **Parallel Execution**: LangGraph supports parallel execution of nodes, enabling more efficient processing.\n        \n        6. **Human-in-the-Loop**: Both frameworks support human intervention points, which can be combined for powerful interactive systems.\n        \"\"\")\n        \n        st.image(\"https://blog.langchain.dev/content/images/2024/01/simple_multi_agent_diagram--1-.png\", \n                 caption=\"Example LangGraph Multi-Agent Workflow\", width=600)\n    \n    # V8: Self-Feedback Loop\n    with st.expander(\"V8: Self-Feedback Loop\"):\n        st.write(\"Automated validation and error correction\")\n        \n        # Create a visual feedback loop\n        st.graphviz_chart('''\n        digraph {\n            rankdir=TB;\n            node [shape=box, style=filled, color=lightblue];\n            \n            Agent [label=\"Agent Generation\"];\n            Test [label=\"Automated Testing\"];\n            Validate [label=\"Validation\"];\n            Error [label=\"Error Detection\"];\n            Fix [label=\"Self-Correction\"];\n            \n            Agent -> Test;\n            Test -> Validate;\n            Validate -> Error [label=\"Issues Found\"];\n            Error -> Fix;\n            Fix -> Agent [label=\"Regenerate\"];\n            Validate -> Agent [label=\"Success\", color=green];\n        }\n        ''')\n        \n        st.write(\"#### Validation Process\")\n        st.info(\"\"\"\n        1. Generate agent code\n        2. Run automated tests\n        3. Analyze test results\n        4. Identify errors or improvement areas\n        5. Apply self-correction algorithms\n        6. Regenerate improved code\n        7. Repeat until validation passes\n        \"\"\")\n    \n    # V9: Self Agent Execution\n    with st.expander(\"V9: Self Agent Execution\"):\n        st.write(\"Testing and iterating on agents in an isolated environment\")\n        \n        st.write(\"#### Agent Execution Process\")\n        \n        execution_process = [\n            {\"phase\": \"Sandbox Creation\", \"description\": \"Set up isolated environment using Local AI package\"},\n            {\"phase\": \"Agent Deployment\", \"description\": \"Load the generated agent into the testing environment\"},\n            {\"phase\": \"Test Execution\", \"description\": \"Run the agent against predefined scenarios and user queries\"},\n            {\"phase\": \"Performance Monitoring\", \"description\": \"Track response quality, latency, and resource usage\"},\n            {\"phase\": \"Error Detection\", \"description\": \"Identify runtime errors and logical inconsistencies\"},\n            {\"phase\": \"Iterative Improvement\", \"description\": \"Refine agent based on execution results\"}\n        ]\n        \n        for i, phase in enumerate(execution_process):\n            st.write(f\"**{i+1}. {phase['phase']}:** {phase['description']}\")\n        \n        st.write(\"#### Local AI Package Integration\")\n        st.markdown(\"\"\"\n        The [Local AI package](https://github.com/coleam00/local-ai-packaged) provides a containerized environment for:\n        - Running LLMs locally for agent testing\n        - Simulating API calls and external dependencies\n        - Monitoring agent behavior in a controlled setting\n        - Collecting performance metrics for optimization\n        \"\"\")\n        \n        st.info(\"This enables Archon to test and refine agents in a controlled environment before deployment, significantly improving reliability and performance through empirical iteration.\")\n    \n    # V10: Multi-Framework Support\n    with st.expander(\"V10: Multi-Framework Support\"):\n        st.write(\"Framework-agnostic agent generation\")\n        \n        frameworks = {\n            \"Pydantic AI\": {\"status\": \"Supported\", \"description\": \"Native support for function-based agents\"},\n            \"LangGraph\": {\"status\": \"Coming in V7\", \"description\": \"Declarative multi-agent orchestration\"},\n            \"LangChain\": {\"status\": \"Planned\", \"description\": \"Popular agent framework with extensive tools\"},\n            \"Agno (Phidata)\": {\"status\": \"Planned\", \"description\": \"Multi-agent workflow framework\"},\n            \"CrewAI\": {\"status\": \"Planned\", \"description\": \"Role-based collaborative agents\"},\n            \"LlamaIndex\": {\"status\": \"Planned\", \"description\": \"RAG-focused agent framework\"}\n        }\n        \n        # Create a frameworks comparison table\n        df_data = {\n            \"Framework\": list(frameworks.keys()),\n            \"Status\": [frameworks[f][\"status\"] for f in frameworks],\n            \"Description\": [frameworks[f][\"description\"] for f in frameworks]\n        }\n        \n        st.dataframe(df_data, use_container_width=True)\n    \n    # V11: Autonomous Framework Learning\n    with st.expander(\"V11: Autonomous Framework Learning\"):\n        st.write(\"Self-learning from mistakes and continuous improvement\")\n        \n        st.write(\"#### Self-Improvement Process\")\n        \n        improvement_process = [\n            {\"phase\": \"Error Detection\", \"description\": \"Identifies patterns in failed agent generations and runtime errors\"},\n            {\"phase\": \"Root Cause Analysis\", \"description\": \"Analyzes error patterns to determine underlying issues in prompts or examples\"},\n            {\"phase\": \"Prompt Refinement\", \"description\": \"Automatically updates system prompts to address identified weaknesses\"},\n            {\"phase\": \"Example Augmentation\", \"description\": \"Adds new examples to the prompt library based on successful generations\"},\n            {\"phase\": \"Tool Enhancement\", \"description\": \"Creates or modifies tools to handle edge cases and common failure modes\"},\n            {\"phase\": \"Validation\", \"description\": \"Tests improvements against historical failure cases to ensure progress\"}\n        ]\n        \n        for i, phase in enumerate(improvement_process):\n            st.write(f\"**{i+1}. {phase['phase']}:** {phase['description']}\")\n        \n        st.info(\"This enables Archon to stay updated with the latest AI frameworks without manual intervention.\")\n    \n    # V12: Advanced RAG Techniques\n    with st.expander(\"V12: Advanced RAG Techniques\"):\n        st.write(\"Enhanced retrieval and incorporation of framework documentation\")\n        \n        st.write(\"#### Advanced RAG Components\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.markdown(\"#### Document Processing\")\n            st.markdown(\"\"\"\n            - **Hierarchical Chunking**: Multi-level chunking strategy that preserves document structure\n            - **Semantic Headers**: Extraction of meaningful section headers for better context\n            - **Code-Text Separation**: Specialized embedding models for code vs. natural language\n            - **Metadata Enrichment**: Automatic tagging with framework version, function types, etc.\n            \"\"\")\n            \n            st.markdown(\"#### Query Processing\")\n            st.markdown(\"\"\"\n            - **Query Decomposition**: Breaking complex queries into sub-queries\n            - **Framework Detection**: Identifying which framework the query relates to\n            - **Intent Classification**: Determining if query is about usage, concepts, or troubleshooting\n            - **Query Expansion**: Adding relevant framework-specific terminology\n            \"\"\")\n        \n        with col2:\n            st.markdown(\"#### Retrieval Enhancements\")\n            st.markdown(\"\"\"\n            - **Hybrid Search**: Combining dense and sparse retrievers for optimal results\n            - **Re-ranking**: Post-retrieval scoring based on relevance to the specific task\n            - **Cross-Framework Retrieval**: Finding analogous patterns across different frameworks\n            - **Code Example Prioritization**: Boosting practical examples in search results\n            \"\"\")\n            \n            st.markdown(\"#### Knowledge Integration\")\n            st.markdown(\"\"\"\n            - **Context Stitching**: Intelligently combining information from multiple chunks\n            - **Framework Translation**: Converting patterns between frameworks (e.g., LangChain to LangGraph)\n            - **Version Awareness**: Handling differences between framework versions\n            - **Adaptive Retrieval**: Learning from successful and unsuccessful retrievals\n            \"\"\")\n        \n        st.info(\"This enables Archon to more effectively retrieve and incorporate framework documentation, leading to more accurate and contextually appropriate agent generation.\")\n    \n    # V13: MCP Agent Marketplace\n    with st.expander(\"V13: MCP Agent Marketplace\"):\n        st.write(\"Integrating Archon agents as MCP servers and publishing to marketplaces\")\n        \n        st.write(\"#### MCP Integration Process\")\n        \n        mcp_integration_process = [\n            {\"phase\": \"Protocol Implementation\", \"description\": \"Implement the Model Context Protocol to enable IDE integration\"},\n            {\"phase\": \"Agent Conversion\", \"description\": \"Transform Archon-generated agents into MCP-compatible servers\"},\n            {\"phase\": \"Specialized Agent Creation\", \"description\": \"Build purpose-specific agents for code review, refactoring, and testing\"},\n            {\"phase\": \"Marketplace Publishing\", \"description\": \"Package and publish agents to MCP marketplaces for distribution\"},\n            {\"phase\": \"IDE Integration\", \"description\": \"Enable seamless operation within Windsurf, Cursor, and other MCP-enabled IDEs\"}\n        ]\n        \n        for i, phase in enumerate(mcp_integration_process):\n            st.write(f\"**{i+1}. {phase['phase']}:** {phase['description']}\")\n        \n        st.info(\"This enables Archon to create specialized agents that operate directly within IDEs through the MCP protocol, while also making them available through marketplace distribution channels.\")\n    \n    # Future Integrations section\n    st.write(\"### Future Integrations\")\n    \n    # LangSmith\n    with st.expander(\"LangSmith\"):\n        st.write(\"Integration with LangChain's tracing and monitoring platform\")\n        \n        st.image(\"https://docs.smith.langchain.com/assets/images/trace-9510284b5b15ba55fc1cca6af2404657.png\", width=600)\n        \n        st.write(\"#### LangSmith Benefits\")\n        st.markdown(\"\"\"\n        - **Tracing**: Monitor agent execution steps and decisions\n        - **Debugging**: Identify issues in complex agent workflows\n        - **Analytics**: Track performance and cost metrics\n        - **Evaluation**: Assess agent quality with automated testing\n        - **Feedback Collection**: Gather human feedback to improve agents\n        \"\"\")\n    \n    # MCP Marketplace\n    with st.expander(\"MCP Marketplace\"):\n        st.write(\"Integration with AI IDE marketplaces\")\n        \n        st.write(\"#### MCP Marketplace Integration\")\n        st.markdown(\"\"\"\n        - Publish Archon itself as a premium agent in MCP marketplaces\n        - Create specialized Archon variants for different development needs\n        - Enable one-click installation directly from within IDEs\n        - Integrate seamlessly with existing development workflows\n        \"\"\")\n        \n        st.warning(\"The Model Context Protocol (MCP) is an emerging standard for AI assistant integration with IDEs like Windsurf, Cursor, and Cline.\")\n    \n    # Other Frameworks\n    with st.expander(\"Other Frameworks besides Pydantic AI\"):\n        st.write(\"Support for additional agent frameworks\")\n        \n        st.write(\"#### Framework Adapter Architecture\")\n        \n        st.graphviz_chart('''\n        digraph {\n            rankdir=TB;\n            node [shape=box, style=filled, color=lightblue];\n            \n            Archon [label=\"Archon Core\"];\n            Adapter [label=\"Framework Adapter Layer\"];\n            \n            Pydantic [label=\"Pydantic AI\", color=lightskyblue];\n            LangGraph [label=\"LangGraph\", color=lightskyblue];\n            LangChain [label=\"LangChain\", color=lightskyblue];\n            Agno [label=\"Agno\", color=lightskyblue];\n            CrewAI [label=\"CrewAI\", color=lightskyblue];\n            LlamaIndex [label=\"LlamaIndex\", color=lightskyblue];\n            \n            Archon -> Adapter;\n            Adapter -> Pydantic;\n            Adapter -> LangGraph;\n            Adapter -> LangChain;\n            Adapter -> Agno;\n            Adapter -> CrewAI;\n            Adapter -> LlamaIndex;\n        }\n        ''')\n    \n    # Vector Databases\n    with st.expander(\"Other Vector Databases besides Supabase\"):\n        st.write(\"Support for additional vector databases\")\n        \n        vector_dbs = {\n            \"Supabase\": {\"status\": \"Supported\", \"features\": [\"pgvector integration\", \"SQL API\", \"Real-time subscriptions\"]},\n            \"Pinecone\": {\"status\": \"Planned\", \"features\": [\"High scalability\", \"Low latency\", \"Serverless\"]},\n            \"Qdrant\": {\"status\": \"Planned\", \"features\": [\"Filtering\", \"Self-hosted option\", \"REST API\"]},\n            \"Milvus\": {\"status\": \"Planned\", \"features\": [\"Horizontal scaling\", \"Cloud-native\", \"Hybrid search\"]},\n            \"Chroma\": {\"status\": \"Planned\", \"features\": [\"Local-first\", \"Lightweight\", \"Simple API\"]},\n            \"Weaviate\": {\"status\": \"Planned\", \"features\": [\"GraphQL\", \"Multi-modal\", \"RESTful API\"]}\n        }\n        \n        # Create vector DB comparison table\n        df_data = {\n            \"Vector Database\": list(vector_dbs.keys()),\n            \"Status\": [vector_dbs[db][\"status\"] for db in vector_dbs],\n            \"Key Features\": [\", \".join(vector_dbs[db][\"features\"]) for db in vector_dbs]\n        }\n        \n        st.dataframe(df_data, use_container_width=True)\n    \n    # Local AI Package\n    with st.expander(\"Local AI Package Integration\"):\n        st.write(\"Integration with [Local AI Package](https://github.com/coleam00/local-ai-packaged)\")\n        \n        st.markdown(\"\"\"\n        The Local AI Package enables running models entirely locally, providing:\n        \n        - **Complete Privacy**: No data leaves your machine\n        - **Cost Savings**: Eliminate API usage fees\n        - **Offline Operation**: Work without internet connectivity\n        - **Custom Fine-tuning**: Adapt models to specific domains\n        - **Lower Latency**: Reduce response times for better UX\n        \"\"\")\n        \n        st.info(\"This integration will allow Archon to operate fully offline with local models for both agent creation and execution.\")\n"}
{"type": "source_file", "path": "iterations/v4-streamlit-ui-overhaul/graph_service.py", "content": "from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Optional, Dict, Any\nfrom archon.archon_graph import agentic_flow\nfrom langgraph.types import Command\nfrom utils.utils import write_to_log\n    \napp = FastAPI()\n\nclass InvokeRequest(BaseModel):\n    message: str\n    thread_id: str\n    is_first_message: bool = False\n    config: Optional[Dict[str, Any]] = None\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\"status\": \"ok\"}    \n\n@app.post(\"/invoke\")\nasync def invoke_agent(request: InvokeRequest):\n    \"\"\"Process a message through the agentic flow and return the complete response.\n\n    The agent streams the response but this API endpoint waits for the full output\n    before returning so it's a synchronous operation for MCP.\n    Another endpoint will be made later to fully stream the response from the API.\n    \n    Args:\n        request: The InvokeRequest containing message and thread info\n        \n    Returns:\n        dict: Contains the complete response from the agent\n    \"\"\"\n    try:\n        config = request.config or {\n            \"configurable\": {\n                \"thread_id\": request.thread_id\n            }\n        }\n\n        response = \"\"\n        if request.is_first_message:\n            write_to_log(f\"Processing first message for thread {request.thread_id}\")\n            async for msg in agentic_flow.astream(\n                {\"latest_user_message\": request.message}, \n                config,\n                stream_mode=\"custom\"\n            ):\n                response += str(msg)\n        else:\n            write_to_log(f\"Processing continuation for thread {request.thread_id}\")\n            async for msg in agentic_flow.astream(\n                Command(resume=request.message),\n                config,\n                stream_mode=\"custom\"\n            ):\n                response += str(msg)\n\n        write_to_log(f\"Final response for thread {request.thread_id}: {response}\")\n        return {\"response\": response}\n        \n    except Exception as e:\n        write_to_log(f\"Error processing message for thread {request.thread_id}: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8100)\n"}
{"type": "source_file", "path": "iterations/v5-parallel-specialized-agents/archon/__init__.py", "content": ""}
{"type": "source_file", "path": "iterations/v3-mcp-support/mcp_server.py", "content": "import os\nimport sys\nimport asyncio\nimport threading\nfrom mcp.server.fastmcp import FastMCP\nimport requests\nfrom typing import Dict, List\nimport uuid\nfrom utils.utils import write_to_log\nfrom graph_service import app\nimport uvicorn\n\n\n# Initialize FastMCP server\nmcp = FastMCP(\"archon\")\n\n\n# Store active threads\nactive_threads: Dict[str, List[str]] = {}\n\n\n# FastAPI service URL\nGRAPH_SERVICE_URL = \"http://127.0.0.1:8100\"\n\n\n@mcp.tool()\nasync def create_thread() -> str:\n    \"\"\"Create a new conversation thread for Archon.\n    Always call this tool before invoking Archon for the first time in a conversation.\n    (if you don't already have a thread ID)\n    \n    Returns:\n        str: A unique thread ID for the conversation\n    \"\"\"\n    thread_id = str(uuid.uuid4())\n    active_threads[thread_id] = []\n    write_to_log(f\"Created new thread: {thread_id}\")\n    return thread_id\n\n\ndef _make_request(thread_id: str, user_input: str, config: dict) -> str:\n    \"\"\"Make synchronous request to graph service\"\"\"\n    response = requests.post(\n        f\"{GRAPH_SERVICE_URL}/invoke\",\n        json={\n            \"message\": user_input,\n            \"thread_id\": thread_id,\n        \"is_first_message\": not active_threads[thread_id],\n            \"config\": config\n    }\n    )\n    response.raise_for_status()\n    return response.json()\n\n\n@mcp.tool()\nasync def run_agent(thread_id: str, user_input: str) -> str:\n    \"\"\"Run the Archon agent with user input.\n    Only use this tool after you have called create_thread in this conversation to get a unique thread ID.\n    If you already created a thread ID in this conversation, do not create another one. Reuse the same ID.\n    After you receive the code from Archon, always implement it into the codebase unless asked not to.\n    \n    Args:\n        thread_id: The conversation thread ID\n        user_input: The user's message to process\n    \n    Returns:\n        str: The agent's response which generally includes the code for the agent\n    \"\"\"\n    if thread_id not in active_threads:\n        write_to_log(f\"Error: Thread not found - {thread_id}\")\n        raise ValueError(\"Thread not found\")\n\n    write_to_log(f\"Processing message for thread {thread_id}: {user_input}\")\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": thread_id\n        }\n    }\n    \n    try:\n        result = await asyncio.to_thread(_make_request, thread_id, user_input, config)\n        active_threads[thread_id].append(user_input)\n        return result['response']\n        \n    except Exception as e:\n        raise\n\n\nif __name__ == \"__main__\":\n    write_to_log(\"Starting MCP server\")\n    \n    # Run MCP server\n    mcp.run(transport='stdio')\n"}
{"type": "source_file", "path": "iterations/v5-parallel-specialized-agents/archon/agent_prompts.py", "content": "prompt_refiner_prompt = \"\"\"\nYou are an AI agent engineer specialized in refining prompts for the agents.\n\nYour only job is to take the current prompt from the conversation, and refine it so the agent being created\nhas optimal instructions to carry out its role and tasks.\n\nYou want the prompt to:\n\n1. Clearly describe the role of the agent\n2. Provide concise and easy to understand goals\n3. Help the agent understand when and how to use each tool provided\n4. Give interactaction guidelines\n5. Provide instructions for handling issues/errors\n\nOutput the new prompt and nothing else.\n\"\"\"\n\ntools_refiner_prompt = \"\"\"\nYou are an AI agent engineer specialized in refining tools for the agents.\nYou have comprehensive access to the Pydantic AI documentation, including API references, usage guides, and implementation examples.\n\nYour only job is to take the current tools from the conversation, and refine them so the agent being created\nhas the optimal tooling to fulfill its role and tasks. Also make sure the tools are coded properly\nand allow the agent to solve the problems they are meant to help with.\n\nFor each tool, ensure that it:\n\n1. Has a clear docstring to help the agent understand when and how to use it\n2. Has correct arguments\n3. Uses the run context properly if applicable (not all tools need run context)\n4. Is coded properly (uses API calls correctly for the services, returns the correct data, etc.)\n5. Handles errors properly\n\nOnly change what is necessary to refine the tools, don't go overboard unless of course the tools are broken and need a lot of fixing.\n\nOutput the new code for the tools and nothing else.\n\"\"\"\n\nagent_refiner_prompt = \"\"\"\nYou are an AI agent engineer specialized in refining agent definitions in code.\nThere are other agents handling refining the prompt and tools, so your job is to make sure the higher\nlevel definition of the agent (depedencies, setting the LLM, etc.) is all correct.\nYou have comprehensive access to the Pydantic AI documentation, including API references, usage guides, and implementation examples.\n\nYour only job is to take the current agent definition from the conversation, and refine it so the agent being created\nhas dependencies, the LLM, the prompt, etc. all configured correctly. Use the Pydantic AI documentation tools to\nconfirm that the agent is set up properly, and only change the current definition if it doesn't align with\nthe documentation.\n\nOutput the agent depedency and definition code if it needs to change and nothing else.\n\"\"\"\n\nprimary_coder_prompt = \"\"\"\n[ROLE AND CONTEXT]\nYou are a specialized AI agent engineer focused on building robust Pydantic AI agents. You have comprehensive access to the Pydantic AI documentation, including API references, usage guides, and implementation examples.\n\n[CORE RESPONSIBILITIES]\n1. Agent Development\n   - Create new agents from user requirements\n   - Complete partial agent implementations\n   - Optimize and debug existing agents\n   - Guide users through agent specification if needed\n\n2. Documentation Integration\n   - Systematically search documentation using RAG before any implementation\n   - Cross-reference multiple documentation pages for comprehensive understanding\n   - Validate all implementations against current best practices\n   - Notify users if documentation is insufficient for any requirement\n\n[CODE STRUCTURE AND DELIVERABLES]\nAll new agents must include these files with complete, production-ready code:\n\n1. agent.py\n   - Primary agent definition and configuration\n   - Core agent logic and behaviors\n   - No tool implementations allowed here\n\n2. agent_tools.py\n   - All tool function implementations\n   - Tool configurations and setup\n   - External service integrations\n\n3. agent_prompts.py\n   - System prompts\n   - Task-specific prompts\n   - Conversation templates\n   - Instruction sets\n\n4. .env.example\n   - Required environment variables\n   - Clear setup instructions in a comment above the variable for how to do so\n   - API configuration templates\n\n5. requirements.txt\n   - Core dependencies without versions\n   - User-specified packages included\n\n[DOCUMENTATION WORKFLOW]\n1. Initial Research\n   - Begin with RAG search for relevant documentation\n   - List all documentation pages using list_documentation_pages\n   - Retrieve specific page content using get_page_content\n   - Cross-reference the weather agent example for best practices\n\n2. Implementation\n   - Provide complete, working code implementations\n   - Never leave placeholder functions\n   - Include all necessary error handling\n   - Implement proper logging and monitoring\n\n3. Quality Assurance\n   - Verify all tool implementations are complete\n   - Ensure proper separation of concerns\n   - Validate environment variable handling\n   - Test critical path functionality\n\n[INTERACTION GUIDELINES]\n- Take immediate action without asking for permission\n- Always verify documentation before implementation\n- Provide honest feedback about documentation gaps\n- Include specific enhancement suggestions\n- Request user feedback on implementations\n- Maintain code consistency across files\n- After providing code, ask the user at the end if they want you to refine the agent autonomously,\notherwise they can give feedback for you to use. The can specifically say 'refine' for you to continue\nworking on the agent through self reflection.\n\n[ERROR HANDLING]\n- Implement robust error handling in all tools\n- Provide clear error messages\n- Include recovery mechanisms\n- Log important state changes\n\n[BEST PRACTICES]\n- Follow Pydantic AI naming conventions\n- Implement proper type hints\n- Include comprehensive docstrings, the agent uses this to understand what tools are for.\n- Maintain clean code structure\n- Use consistent formatting\n\nHere is a good example of a Pydantic AI agent:\n\n```python\nfrom __future__ import annotations as _annotations\n\nimport asyncio\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport logfire\nfrom devtools import debug\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\n\n\n@dataclass\nclass Deps:\n    client: AsyncClient\n    weather_api_key: str | None\n    geo_api_key: str | None\n\n\nweather_agent = Agent(\n    'openai:gpt-4o',\n    # 'Be concise, reply with one sentence.' is enough for some models (like openai) to use\n    # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n    system_prompt=(\n        'Be concise, reply with one sentence.'\n        'Use the `get_lat_lng` tool to get the latitude and longitude of the locations, '\n        'then use the `get_weather` tool to get the weather.'\n    ),\n    deps_type=Deps,\n    retries=2,\n)\n\n\n@weather_agent.tool\nasync def get_lat_lng(\n    ctx: RunContext[Deps], location_description: str\n) -> dict[str, float]:\n    \\\"\\\"\\\"Get the latitude and longitude of a location.\n\n    Args:\n        ctx: The context.\n        location_description: A description of a location.\n    \\\"\\\"\\\"\n    if ctx.deps.geo_api_key is None:\n        # if no API key is provided, return a dummy response (London)\n        return {'lat': 51.1, 'lng': -0.1}\n\n    params = {\n        'q': location_description,\n        'api_key': ctx.deps.geo_api_key,\n    }\n    with logfire.span('calling geocode API', params=params) as span:\n        r = await ctx.deps.client.get('https://geocode.maps.co/search', params=params)\n        r.raise_for_status()\n        data = r.json()\n        span.set_attribute('response', data)\n\n    if data:\n        return {'lat': data[0]['lat'], 'lng': data[0]['lon']}\n    else:\n        raise ModelRetry('Could not find the location')\n\n\n@weather_agent.tool\nasync def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n    \\\"\\\"\\\"Get the weather at a location.\n\n    Args:\n        ctx: The context.\n        lat: Latitude of the location.\n        lng: Longitude of the location.\n    \\\"\\\"\\\"\n    if ctx.deps.weather_api_key is None:\n        # if no API key is provided, return a dummy response\n        return {'temperature': '21 °C', 'description': 'Sunny'}\n\n    params = {\n        'apikey': ctx.deps.weather_api_key,\n        'location': f'{lat},{lng}',\n        'units': 'metric',\n    }\n    with logfire.span('calling weather API', params=params) as span:\n        r = await ctx.deps.client.get(\n            'https://api.tomorrow.io/v4/weather/realtime', params=params\n        )\n        r.raise_for_status()\n        data = r.json()\n        span.set_attribute('response', data)\n\n    values = data['data']['values']\n    # https://docs.tomorrow.io/reference/data-layers-weather-codes\n    code_lookup = {\n        ...\n    }\n    return {\n        'temperature': f'{values[\"temperatureApparent\"]:0.0f}°C',\n        'description': code_lookup.get(values['weatherCode'], 'Unknown'),\n    }\n\n\nasync def main():\n    async with AsyncClient() as client:\n        # create a free API key at https://www.tomorrow.io/weather-api/\n        weather_api_key = os.getenv('WEATHER_API_KEY')\n        # create a free API key at https://geocode.maps.co/\n        geo_api_key = os.getenv('GEO_API_KEY')\n        deps = Deps(\n            client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\n        )\n        result = await weather_agent.run(\n            'What is the weather like in London and in Wiltshire?', deps=deps\n        )\n        debug(result)\n        print('Response:', result.data)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\"\"\""}
{"type": "source_file", "path": "iterations/v4-streamlit-ui-overhaul/archon/archon_graph.py", "content": "from pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai import Agent, RunContext\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing import TypedDict, Annotated, List, Any\nfrom langgraph.config import get_stream_writer\nfrom langgraph.types import interrupt\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\nfrom supabase import Client\nimport logfire\nimport os\nimport sys\n\n# Import the message classes from Pydantic AI\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelMessagesTypeAdapter\n)\n\n# Add the parent directory to Python path\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom archon.pydantic_ai_coder import pydantic_ai_coder, PydanticAIDeps, list_documentation_pages_helper\nfrom utils.utils import get_env_var\n\n# Load environment variables\nload_dotenv()\n\n# Configure logfire to suppress warnings (optional)\nlogfire.configure(send_to_logfire='never')\n\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\n\nis_ollama = \"localhost\" in base_url.lower()\nis_anthropic = \"anthropic\" in base_url.lower()\nis_openai = \"openai\" in base_url.lower()\n\nreasoner_llm_model_name = get_env_var('REASONER_MODEL') or 'o3-mini'\nreasoner_llm_model = AnthropicModel(reasoner_llm_model_name, api_key=api_key) if is_anthropic else OpenAIModel(reasoner_llm_model_name, base_url=base_url, api_key=api_key)\n\nreasoner = Agent(  \n    reasoner_llm_model,\n    system_prompt='You are an expert at coding AI agents with Pydantic AI and defining the scope for doing so.',  \n)\n\nprimary_llm_model_name = get_env_var('PRIMARY_MODEL') or 'gpt-4o-mini'\nprimary_llm_model = AnthropicModel(primary_llm_model_name, api_key=api_key) if is_anthropic else OpenAIModel(primary_llm_model_name, base_url=base_url, api_key=api_key)\n\nrouter_agent = Agent(  \n    primary_llm_model,\n    system_prompt='Your job is to route the user message either to the end of the conversation or to continue coding the AI agent.',  \n)\n\nend_conversation_agent = Agent(  \n    primary_llm_model,\n    system_prompt='Your job is to end a conversation for creating an AI agent by giving instructions for how to execute the agent and they saying a nice goodbye to the user.',  \n)\n\nopenai_client=None\n\nif is_ollama:\n    openai_client = AsyncOpenAI(base_url=base_url,api_key=api_key)\nelif get_env_var(\"OPENAI_API_KEY\"):\n    openai_client = AsyncOpenAI(api_key=get_env_var(\"OPENAI_API_KEY\"))\nelse:\n    openai_client = None\n\nif get_env_var(\"SUPABASE_URL\"):\n    supabase: Client = Client(\n        get_env_var(\"SUPABASE_URL\"),\n        get_env_var(\"SUPABASE_SERVICE_KEY\")\n    )\nelse:\n    supabase = None\n\n# Define state schema\nclass AgentState(TypedDict):\n    latest_user_message: str\n    messages: Annotated[List[bytes], lambda x, y: x + y]\n    scope: str\n\n# Scope Definition Node with Reasoner LLM\nasync def define_scope_with_reasoner(state: AgentState):\n    # First, get the documentation pages so the reasoner can decide which ones are necessary\n    documentation_pages = await list_documentation_pages_helper(supabase)\n    documentation_pages_str = \"\\n\".join(documentation_pages)\n\n    # Then, use the reasoner to define the scope\n    prompt = f\"\"\"\n    User AI Agent Request: {state['latest_user_message']}\n    \n    Create detailed scope document for the AI agent including:\n    - Architecture diagram\n    - Core components\n    - External dependencies\n    - Testing strategy\n\n    Also based on these documentation pages available:\n\n    {documentation_pages_str}\n\n    Include a list of documentation pages that are relevant to creating this agent for the user in the scope document.\n    \"\"\"\n\n    result = await reasoner.run(prompt)\n    scope = result.data\n\n    # Get the directory one level up from the current file\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    parent_dir = os.path.dirname(current_dir)\n    scope_path = os.path.join(parent_dir, \"workbench\", \"scope.md\")\n    os.makedirs(os.path.join(parent_dir, \"workbench\"), exist_ok=True)\n\n    with open(scope_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(scope)\n    \n    return {\"scope\": scope}\n\n# Coding Node with Feedback Handling\nasync def coder_agent(state: AgentState, writer):    \n    # Prepare dependencies\n    deps = PydanticAIDeps(\n        supabase=supabase,\n        openai_client=openai_client,\n        reasoner_output=state['scope']\n    )\n\n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    # Run the agent in a stream\n    if not is_openai:\n        writer = get_stream_writer()\n        result = await pydantic_ai_coder.run(state['latest_user_message'], deps=deps, message_history= message_history)\n        writer(result.data)\n    else:\n        async with pydantic_ai_coder.run_stream(\n            state['latest_user_message'],\n            deps=deps,\n            message_history= message_history\n        ) as result:\n            # Stream partial text as it arrives\n            async for chunk in result.stream_text(delta=True):\n                writer(chunk)\n\n    # print(ModelMessagesTypeAdapter.validate_json(result.new_messages_json()))\n\n    return {\"messages\": [result.new_messages_json()]}\n\n# Interrupt the graph to get the user's next message\ndef get_next_user_message(state: AgentState):\n    value = interrupt({})\n\n    # Set the user's latest message for the LLM to continue the conversation\n    return {\n        \"latest_user_message\": value\n    }\n\n# Determine if the user is finished creating their AI agent or not\nasync def route_user_message(state: AgentState):\n    prompt = f\"\"\"\n    The user has sent a message: \n    \n    {state['latest_user_message']}\n\n    If the user wants to end the conversation, respond with just the text \"finish_conversation\".\n    If the user wants to continue coding the AI agent, respond with just the text \"coder_agent\".\n    \"\"\"\n\n    result = await router_agent.run(prompt)\n    next_action = result.data\n\n    if next_action == \"finish_conversation\":\n        return \"finish_conversation\"\n    else:\n        return \"coder_agent\"\n\n# End of conversation agent to give instructions for executing the agent\nasync def finish_conversation(state: AgentState, writer):    \n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    # Run the agent in a stream\n    if not is_openai:\n        writer = get_stream_writer()\n        result = await end_conversation_agent.run(state['latest_user_message'], message_history= message_history)\n        writer(result.data)   \n    else: \n        async with end_conversation_agent.run_stream(\n            state['latest_user_message'],\n            message_history= message_history\n        ) as result:\n            # Stream partial text as it arrives\n            async for chunk in result.stream_text(delta=True):\n                writer(chunk)\n\n    return {\"messages\": [result.new_messages_json()]}        \n\n# Build workflow\nbuilder = StateGraph(AgentState)\n\n# Add nodes\nbuilder.add_node(\"define_scope_with_reasoner\", define_scope_with_reasoner)\nbuilder.add_node(\"coder_agent\", coder_agent)\nbuilder.add_node(\"get_next_user_message\", get_next_user_message)\nbuilder.add_node(\"finish_conversation\", finish_conversation)\n\n# Set edges\nbuilder.add_edge(START, \"define_scope_with_reasoner\")\nbuilder.add_edge(\"define_scope_with_reasoner\", \"coder_agent\")\nbuilder.add_edge(\"coder_agent\", \"get_next_user_message\")\nbuilder.add_conditional_edges(\n    \"get_next_user_message\",\n    route_user_message,\n    {\"coder_agent\": \"coder_agent\", \"finish_conversation\": \"finish_conversation\"}\n)\nbuilder.add_edge(\"finish_conversation\", END)\n\n# Configure persistence\nmemory = MemorySaver()\nagentic_flow = builder.compile(checkpointer=memory)"}
{"type": "source_file", "path": "iterations/v4-streamlit-ui-overhaul/mcp_server.py", "content": "import os\nimport sys\nimport asyncio\nimport threading\nfrom mcp.server.fastmcp import FastMCP\nimport requests\nfrom typing import Dict, List\nimport uuid\nfrom utils.utils import write_to_log\nfrom graph_service import app\nimport uvicorn\n\n\n# Initialize FastMCP server\nmcp = FastMCP(\"archon\")\n\n\n# Store active threads\nactive_threads: Dict[str, List[str]] = {}\n\n\n# FastAPI service URL\nGRAPH_SERVICE_URL = \"http://127.0.0.1:8100\"\n\n\n@mcp.tool()\nasync def create_thread() -> str:\n    \"\"\"Create a new conversation thread for Archon.\n    Always call this tool before invoking Archon for the first time in a conversation.\n    (if you don't already have a thread ID)\n    \n    Returns:\n        str: A unique thread ID for the conversation\n    \"\"\"\n    thread_id = str(uuid.uuid4())\n    active_threads[thread_id] = []\n    write_to_log(f\"Created new thread: {thread_id}\")\n    return thread_id\n\n\ndef _make_request(thread_id: str, user_input: str, config: dict) -> str:\n    \"\"\"Make synchronous request to graph service\"\"\"\n    response = requests.post(\n        f\"{GRAPH_SERVICE_URL}/invoke\",\n        json={\n            \"message\": user_input,\n            \"thread_id\": thread_id,\n            \"is_first_message\": not active_threads[thread_id],\n            \"config\": config\n    }\n    )\n    response.raise_for_status()\n    return response.json()\n\n\n@mcp.tool()\nasync def run_agent(thread_id: str, user_input: str) -> str:\n    \"\"\"Run the Archon agent with user input.\n    Only use this tool after you have called create_thread in this conversation to get a unique thread ID.\n    If you already created a thread ID in this conversation, do not create another one. Reuse the same ID.\n    After you receive the code from Archon, always implement it into the codebase unless asked not to.\n    \n    Args:\n        thread_id: The conversation thread ID\n        user_input: The user's message to process\n    \n    Returns:\n        str: The agent's response which generally includes the code for the agent\n    \"\"\"\n    if thread_id not in active_threads:\n        write_to_log(f\"Error: Thread not found - {thread_id}\")\n        raise ValueError(\"Thread not found\")\n\n    write_to_log(f\"Processing message for thread {thread_id}: {user_input}\")\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": thread_id\n        }\n    }\n    \n    try:\n        result = await asyncio.to_thread(_make_request, thread_id, user_input, config)\n        active_threads[thread_id].append(user_input)\n        return result['response']\n        \n    except Exception as e:\n        raise\n\n\nif __name__ == \"__main__\":\n    write_to_log(\"Starting MCP server\")\n    \n    # Run MCP server\n    mcp.run(transport='stdio')\n"}
{"type": "source_file", "path": "iterations/v4-streamlit-ui-overhaul/mcp/mcp_server.py", "content": "from mcp.server.fastmcp import FastMCP\nfrom datetime import datetime\nfrom dotenv import load_dotenv\nfrom typing import Dict, List\nimport threading\nimport requests\nimport asyncio\nimport uuid\nimport sys\nimport os\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize FastMCP server\nmcp = FastMCP(\"archon\")\n\n# Store active threads\nactive_threads: Dict[str, List[str]] = {}\n\n# FastAPI service URL\nGRAPH_SERVICE_URL = os.getenv(\"GRAPH_SERVICE_URL\", \"http://localhost:8100\")\n\ndef write_to_log(message: str):\n    \"\"\"Write a message to the logs.txt file in the workbench directory.\n    \n    Args:\n        message: The message to log\n    \"\"\"\n    # Get the directory one level up from the current file\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    parent_dir = os.path.dirname(current_dir)\n    workbench_dir = os.path.join(parent_dir, \"workbench\")\n    log_path = os.path.join(workbench_dir, \"logs.txt\")\n    os.makedirs(workbench_dir, exist_ok=True)\n\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    log_entry = f\"[{timestamp}] {message}\\n\"\n\n    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n        f.write(log_entry)\n\n@mcp.tool()\nasync def create_thread() -> str:\n    \"\"\"Create a new conversation thread for Archon.\n    Always call this tool before invoking Archon for the first time in a conversation.\n    (if you don't already have a thread ID)\n    \n    Returns:\n        str: A unique thread ID for the conversation\n    \"\"\"\n    thread_id = str(uuid.uuid4())\n    active_threads[thread_id] = []\n    write_to_log(f\"Created new thread: {thread_id}\")\n    return thread_id\n\n\ndef _make_request(thread_id: str, user_input: str, config: dict) -> str:\n    \"\"\"Make synchronous request to graph service\"\"\"\n    response = requests.post(\n        f\"{GRAPH_SERVICE_URL}/invoke\",\n        json={\n            \"message\": user_input,\n            \"thread_id\": thread_id,\n            \"is_first_message\": not active_threads[thread_id],\n            \"config\": config\n    }\n    )\n    response.raise_for_status()\n    return response.json()\n\n\n@mcp.tool()\nasync def run_agent(thread_id: str, user_input: str) -> str:\n    \"\"\"Run the Archon agent with user input.\n    Only use this tool after you have called create_thread in this conversation to get a unique thread ID.\n    If you already created a thread ID in this conversation, do not create another one. Reuse the same ID.\n    After you receive the code from Archon, always implement it into the codebase unless asked not to.\n    \n    Args:\n        thread_id: The conversation thread ID\n        user_input: The user's message to process\n    \n    Returns:\n        str: The agent's response which generally includes the code for the agent\n    \"\"\"\n    if thread_id not in active_threads:\n        write_to_log(f\"Error: Thread not found - {thread_id}\")\n        raise ValueError(\"Thread not found\")\n\n    write_to_log(f\"Processing message for thread {thread_id}: {user_input}\")\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": thread_id\n        }\n    }\n    \n    try:\n        result = await asyncio.to_thread(_make_request, thread_id, user_input, config)\n        active_threads[thread_id].append(user_input)\n        return result['response']\n        \n    except Exception as e:\n        raise\n\n\nif __name__ == \"__main__\":\n    write_to_log(\"Starting MCP server\")\n    \n    # Run MCP server\n    mcp.run(transport='stdio')\n"}
{"type": "source_file", "path": "iterations/v4-streamlit-ui-overhaul/archon/__init__.py", "content": ""}
{"type": "source_file", "path": "iterations/v4-streamlit-ui-overhaul/streamlit_ui.py", "content": "from __future__ import annotations\nfrom typing import Literal, TypedDict\nfrom langgraph.types import Command\nimport os\n\nimport streamlit as st\nimport logfire\nimport asyncio\nimport time\nimport json\nimport uuid\nimport sys\nimport platform\nimport subprocess\nimport threading\nimport queue\nimport webbrowser\nimport importlib\nfrom urllib.parse import urlparse\nfrom openai import AsyncOpenAI\nfrom supabase import Client, create_client\nfrom dotenv import load_dotenv\nfrom utils.utils import get_env_var, save_env_var, write_to_log\nfrom future_enhancements import future_enhancements_tab\n\n# Import all the message part classes\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    SystemPromptPart,\n    UserPromptPart,\n    TextPart,\n    ToolCallPart,\n    ToolReturnPart,\n    RetryPromptPart,\n    ModelMessagesTypeAdapter\n)\n\n# Add the current directory to Python path\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\nfrom archon.archon_graph import agentic_flow\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize clients\nopenai_client = None\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\nis_ollama = \"localhost\" in base_url.lower()\n\nif is_ollama:\n    openai_client = AsyncOpenAI(base_url=base_url,api_key=api_key)\nelif get_env_var(\"OPENAI_API_KEY\"):\n    openai_client = AsyncOpenAI(api_key=get_env_var(\"OPENAI_API_KEY\"))\nelse:\n    openai_client = None\n\nif get_env_var(\"SUPABASE_URL\"):\n    supabase: Client = Client(\n            get_env_var(\"SUPABASE_URL\"),\n            get_env_var(\"SUPABASE_SERVICE_KEY\")\n        )\nelse:\n    supabase = None\n\n# Set page config - must be the first Streamlit command\nst.set_page_config(\n    page_title=\"Archon - Agent Builder\",\n    page_icon=\"🤖\",\n    layout=\"wide\",\n)\n\n# Set custom theme colors to match Archon logo (green and pink)\n# Primary color (green) and secondary color (pink)\nst.markdown(\"\"\"\n    <style>\n    :root {\n        --primary-color: #00CC99;  /* Green */\n        --secondary-color: #EB2D8C; /* Pink */\n        --text-color: #262730;\n    }\n    \n    /* Style the buttons */\n    .stButton > button {\n        color: white;\n        border: 2px solid var(--primary-color);\n        padding: 0.5rem 1rem;\n        font-weight: bold;\n        transition: all 0.3s ease;\n    }\n    \n    .stButton > button:hover {\n        color: white;\n        border: 2px solid var(--secondary-color);\n    }\n    \n    /* Override Streamlit's default focus styles that make buttons red */\n    .stButton > button:focus, \n    .stButton > button:focus:hover, \n    .stButton > button:active, \n    .stButton > button:active:hover {\n        color: white !important;\n        border: 2px solid var(--secondary-color) !important;\n        box-shadow: none !important;\n        outline: none !important;\n    }\n    \n    /* Style headers */\n    h1, h2, h3 {\n        color: var(--primary-color);\n    }\n    \n    /* Hide spans within h3 elements */\n    h1 span, h2 span, h3 span {\n        display: none !important;\n        visibility: hidden;\n        width: 0;\n        height: 0;\n        opacity: 0;\n        position: absolute;\n        overflow: hidden;\n    }\n    \n    /* Style code blocks */\n    pre {\n        border-left: 4px solid var(--primary-color);\n    }\n    \n    /* Style links */\n    a {\n        color: var(--secondary-color);\n    }\n    \n    /* Style the chat messages */\n    .stChatMessage {\n        border-left: 4px solid var(--secondary-color);\n    }\n    \n    /* Style the chat input */\n    .stChatInput > div {\n        border: 2px solid var(--primary-color) !important;\n    }\n    \n    /* Remove red outline on focus */\n    .stChatInput > div:focus-within {\n        box-shadow: none !important;\n        border: 2px solid var(--secondary-color) !important;\n        outline: none !important;\n    }\n    \n    /* Remove red outline on all inputs when focused */\n    input:focus, textarea:focus, [contenteditable]:focus {\n        box-shadow: none !important;\n        border-color: var(--secondary-color) !important;\n        outline: none !important;\n    }\n\n    </style>\n\"\"\", unsafe_allow_html=True)\n\n# Helper function to create a button that opens a tab in a new window\ndef create_new_tab_button(label, tab_name, key=None, use_container_width=False):\n    \"\"\"Create a button that opens a specified tab in a new browser window\"\"\"\n    # Create a unique key if none provided\n    if key is None:\n        key = f\"new_tab_{tab_name.lower().replace(' ', '_')}\"\n    \n    # Get the base URL\n    base_url = st.query_params.get(\"base_url\", \"\")\n    if not base_url:\n        # If base_url is not in query params, use the default localhost URL\n        base_url = \"http://localhost:8501\"\n    \n    # Create the URL for the new tab\n    new_tab_url = f\"{base_url}/?tab={tab_name}\"\n    \n    # Create a button that will open the URL in a new tab when clicked\n    if st.button(label, key=key, use_container_width=use_container_width):\n        webbrowser.open_new_tab(new_tab_url)\n\n# Function to reload the archon_graph module\ndef reload_archon_graph():\n    \"\"\"Reload the archon_graph module to apply new environment variables\"\"\"\n    try:\n        # First reload pydantic_ai_coder\n        import archon.pydantic_ai_coder\n        importlib.reload(archon.pydantic_ai_coder)\n        \n        # Then reload archon_graph which imports pydantic_ai_coder\n        import archon.archon_graph\n        importlib.reload(archon.archon_graph)\n        \n        st.success(\"Successfully reloaded Archon modules with new environment variables!\")\n        return True\n    except Exception as e:\n        st.error(f\"Error reloading Archon modules: {str(e)}\")\n        return False\n    \n# Configure logfire to suppress warnings (optional)\nlogfire.configure(send_to_logfire='never')\n\n@st.cache_resource\ndef get_thread_id():\n    return str(uuid.uuid4())\n\nthread_id = get_thread_id()\n\nasync def run_agent_with_streaming(user_input: str):\n    \"\"\"\n    Run the agent with streaming text for the user_input prompt,\n    while maintaining the entire conversation in `st.session_state.messages`.\n    \"\"\"\n    config = {\n        \"configurable\": {\n            \"thread_id\": thread_id\n        }\n    }\n\n    # First message from user\n    if len(st.session_state.messages) == 1:\n        async for msg in agentic_flow.astream(\n                {\"latest_user_message\": user_input}, config, stream_mode=\"custom\"\n            ):\n                yield msg\n    # Continue the conversation\n    else:\n        async for msg in agentic_flow.astream(\n            Command(resume=user_input), config, stream_mode=\"custom\"\n        ):\n            yield msg\n\ndef generate_mcp_config(ide_type):\n    \"\"\"\n    Generate MCP configuration for the selected IDE type.\n    \"\"\"\n    # Get the absolute path to the current directory\n    base_path = os.path.abspath(os.path.dirname(__file__))\n    \n    # Determine the correct python path based on the OS\n    if platform.system() == \"Windows\":\n        python_path = os.path.join(base_path, 'venv', 'Scripts', 'python.exe')\n    else:  # macOS or Linux\n        python_path = os.path.join(base_path, 'venv', 'bin', 'python')\n    \n    server_script_path = os.path.join(base_path, 'mcp', 'mcp_server.py')\n    \n    # Create the config dictionary for Python\n    python_config = {\n        \"mcpServers\": {\n            \"archon\": {\n                \"command\": python_path,\n                \"args\": [server_script_path]\n            }\n        }\n    }\n    \n    # Create the config dictionary for Docker\n    docker_config = {\n        \"mcpServers\": {\n            \"archon\": {\n                \"command\": \"docker\",\n                \"args\": [\n                    \"run\",\n                    \"-i\",\n                    \"--rm\",\n                    \"-e\", \n                    \"GRAPH_SERVICE_URL\",\n                    \"archon-mcp:latest\"\n                ],\n                \"env\": {\n                    \"GRAPH_SERVICE_URL\": \"http://host.docker.internal:8100\"\n                }\n            }\n        }\n    }\n    \n    # Return appropriate configuration based on IDE type\n    if ide_type == \"Windsurf\":\n        return json.dumps(python_config, indent=2), json.dumps(docker_config, indent=2)\n    elif ide_type == \"Cursor\":\n        return f\"{python_path} {server_script_path}\", f\"docker run --rm -p 8100:8100 archon:latest python mcp_server.py\"\n    elif ide_type == \"Cline\":\n        return json.dumps(python_config, indent=2), json.dumps(docker_config, indent=2)  # Assuming Cline uses the same format as Windsurf\n    else:\n        return \"Unknown IDE type selected\", \"Unknown IDE type selected\"\n\ndef mcp_tab():\n    \"\"\"Display the MCP configuration interface\"\"\"\n    st.header(\"MCP Configuration\")\n    st.write(\"Select your AI IDE to get the appropriate MCP configuration:\")\n    \n    # IDE selection with side-by-side buttons\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        windsurf_button = st.button(\"Windsurf\", use_container_width=True, key=\"windsurf_button\")\n    with col2:\n        cursor_button = st.button(\"Cursor\", use_container_width=True, key=\"cursor_button\")\n    with col3:\n        cline_button = st.button(\"Cline\", use_container_width=True, key=\"cline_button\")\n    \n    # Initialize session state for selected IDE if not present\n    if \"selected_ide\" not in st.session_state:\n        st.session_state.selected_ide = None\n    \n    # Update selected IDE based on button clicks\n    if windsurf_button:\n        st.session_state.selected_ide = \"Windsurf\"\n    elif cursor_button:\n        st.session_state.selected_ide = \"Cursor\"\n    elif cline_button:\n        st.session_state.selected_ide = \"Cline\"\n    \n    # Display configuration if an IDE is selected\n    if st.session_state.selected_ide:\n        selected_ide = st.session_state.selected_ide\n        st.subheader(f\"MCP Configuration for {selected_ide}\")\n        python_config, docker_config = generate_mcp_config(selected_ide)\n        \n        # Configuration type tabs\n        config_tab1, config_tab2 = st.tabs([\"Docker Configuration\", \"Python Configuration\"])\n        \n        with config_tab1:\n            st.markdown(\"### Docker Configuration\")\n            st.code(docker_config, language=\"json\" if selected_ide != \"Cursor\" else None)\n            \n            st.markdown(\"#### Requirements:\")\n            st.markdown(\"- Docker installed\")\n            st.markdown(\"- Run the setup script to build and start both containers:\")\n            st.code(\"python run_docker.py\", language=\"bash\")\n        \n        with config_tab2:\n            st.markdown(\"### Python Configuration\")\n            st.code(python_config, language=\"json\" if selected_ide != \"Cursor\" else None)\n            \n            st.markdown(\"#### Requirements:\")\n            st.markdown(\"- Python 3.11+ installed\")\n            st.markdown(\"- Virtual environment created and activated\")\n            st.markdown(\"- All dependencies installed via `pip install -r requirements.txt`\")\n            st.markdown(\"- Must be running Archon not within a container\")           \n        \n        # Instructions based on IDE type\n        st.markdown(\"---\")\n        st.markdown(\"### Setup Instructions\")\n        \n        if selected_ide == \"Windsurf\":\n            st.markdown(\"\"\"\n            #### How to use in Windsurf:\n            1. Click on the hammer icon above the chat input\n            2. Click on \"Configure\"\n            3. Paste the JSON from your preferred configuration tab above\n            4. Click \"Refresh\" next to \"Configure\"\n            \"\"\")\n        elif selected_ide == \"Cursor\":\n            st.markdown(\"\"\"\n            #### How to use in Cursor:\n            1. Go to Cursor Settings > Features > MCP\n            2. Click on \"+ Add New MCP Server\"\n            3. Name: Archon\n            4. Type: command (equivalent to stdio)\n            5. Command: Paste the command from your preferred configuration tab above\n            \"\"\")\n        elif selected_ide == \"Cline\":\n            st.markdown(\"\"\"\n            #### How to use in Cline or Roo Code:\n            1. From the Cline/Roo Code extension, click the \"MCP Server\" tab\n            2. Click the \"Edit MCP Settings\" button\n            3. The MCP settings file should be displayed in a tab in VS Code\n            4. Paste the JSON from your preferred configuration tab above\n            5. Cline/Roo Code will automatically detect and start the MCP server\n            \"\"\")\n\nasync def chat_tab():\n    \"\"\"Display the chat interface for talking to Archon\"\"\"\n    st.write(\"Describe to me an AI agent you want to build and I'll code it for you with Pydantic AI.\")\n    st.write(\"Example: Build me an AI agent that can search the web with the Brave API.\")\n\n    # Initialize chat history in session state if not present\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    # Display chat messages from history on app rerun\n    for message in st.session_state.messages:\n        message_type = message[\"type\"]\n        if message_type in [\"human\", \"ai\", \"system\"]:\n            with st.chat_message(message_type):\n                st.markdown(message[\"content\"])    \n\n    # Chat input for the user\n    user_input = st.chat_input(\"What do you want to build today?\")\n\n    if user_input:\n        # We append a new request to the conversation explicitly\n        st.session_state.messages.append({\"type\": \"human\", \"content\": user_input})\n        \n        # Display user prompt in the UI\n        with st.chat_message(\"user\"):\n            st.markdown(user_input)\n\n        # Display assistant response in chat message container\n        response_content = \"\"\n        with st.chat_message(\"assistant\"):\n            message_placeholder = st.empty()  # Placeholder for updating the message\n            # Run the async generator to fetch responses\n            async for chunk in run_agent_with_streaming(user_input):\n                response_content += chunk\n                # Update the placeholder with the current response content\n                message_placeholder.markdown(response_content)\n        \n        st.session_state.messages.append({\"type\": \"ai\", \"content\": response_content})\n\ndef intro_tab():\n    \"\"\"Display the introduction and setup guide for Archon\"\"\"\n    # Display the banner image\n    st.image(\"public/Archon.png\", use_container_width=True)\n    \n    # Welcome message\n    st.markdown(\"\"\"\n    # Welcome to Archon!\n    \n    Archon is an AI meta-agent designed to autonomously build, refine, and optimize other AI agents.\n    \n    It serves both as a practical tool for developers and as an educational framework demonstrating the evolution of agentic systems.\n    Archon is developed in iterations, starting with a simple Pydantic AI agent that can build other Pydantic AI agents,\n    all the way to a full agentic workflow using LangGraph that can build other AI agents with any framework.\n    \n    Through its iterative development, Archon showcases the power of planning, feedback loops, and domain-specific knowledge in creating robust AI agents.\n    \"\"\")\n    \n    # Setup guide with expandable sections\n    st.markdown(\"## Setup Guide\")\n    st.markdown(\"Follow these concise steps to get Archon up and running (IMPORTANT: come back here after each step):\")\n    \n    # Step 1: Environment Configuration\n    with st.expander(\"Step 1: Environment Configuration\", expanded=True):\n        st.markdown(\"\"\"\n        ### Environment Configuration\n        \n        First, you need to set up your environment variables:\n        \n        1. Go to the **Environment** tab\n        2. Configure the following essential variables:\n           - `BASE_URL`: API endpoint (OpenAI, OpenRouter, or Ollama)\n           - `LLM_API_KEY`: Your API key for the LLM service\n           - `OPENAI_API_KEY`: Required for embeddings\n           - `SUPABASE_URL`: Your Supabase project URL\n           - `SUPABASE_SERVICE_KEY`: Your Supabase service key\n           - `PRIMARY_MODEL`: Main agent model (e.g., gpt-4o-mini)\n           - `REASONER_MODEL`: Planning model (e.g., o3-mini)\n        \n        These settings determine how Archon connects to external services and which models it uses.\n        \"\"\")\n        # Add a button to navigate to the Environment tab\n        create_new_tab_button(\"Go to Environment Section (New Tab)\", \"Environment\", key=\"goto_env\", use_container_width=True)\n    \n    # Step 2: Database Setup\n    with st.expander(\"Step 2: Database Setup\", expanded=False):\n        st.markdown(\"\"\"\n        ### Database Setup\n        \n        Archon uses Supabase for vector storage and retrieval:\n        \n        1. Go to the **Database** tab\n        2. Select your embedding dimensions (1536 for OpenAI, 768 for nomic-embed-text)\n        3. Follow the instructions to create the `site_pages` table\n        \n        This creates the necessary tables, indexes, and functions for vector similarity search.\n        \"\"\")\n        # Add a button to navigate to the Database tab\n        create_new_tab_button(\"Go to Database Section (New Tab)\", \"Database\", key=\"goto_db\", use_container_width=True)\n    \n    # Step 3: Documentation Crawling\n    with st.expander(\"Step 3: Documentation Crawling\", expanded=False):\n        st.markdown(\"\"\"\n        ### Documentation Crawling\n        \n        Populate the database with framework documentation:\n        \n        1. Go to the **Documentation** tab\n        2. Click on \"Crawl Pydantic AI Docs\"\n        3. Wait for the crawling process to complete\n        \n        This step downloads and processes documentation, creating embeddings for semantic search.\n        \"\"\")\n        # Add a button to navigate to the Documentation tab\n        create_new_tab_button(\"Go to the Documentation Section (New Tab)\", \"Documentation\", key=\"goto_docs\", use_container_width=True)\n    \n    # Step 4: Agent Service\n    with st.expander(\"Step 4: Agent Service Setup (for MCP)\", expanded=False):\n        st.markdown(\"\"\"\n        ### MCP Agent Service Setup\n        \n        Start the graph service for agent generation:\n        \n        1. Go to the **Agent Service** tab\n        2. Click on \"Start Agent Service\"\n        3. Verify the service is running\n        \n        The agent service powers the LangGraph workflow for agent creation.\n        \"\"\")\n        # Add a button to navigate to the Agent Service tab\n        create_new_tab_button(\"Go to Agent Service Section (New Tab)\", \"Agent Service\", key=\"goto_service\", use_container_width=True)\n    \n    # Step 5: MCP Configuration (Optional)\n    with st.expander(\"Step 5: MCP Configuration (Optional)\", expanded=False):\n        st.markdown(\"\"\"\n        ### MCP Configuration\n        \n        For integration with AI IDEs:\n        \n        1. Go to the **MCP** tab\n        2. Select your IDE (Windsurf, Cursor, Cline, or Roo Code)\n        3. Follow the instructions to configure your IDE\n        \n        This enables you to use Archon directly from your AI-powered IDE.\n        \"\"\")\n        # Add a button to navigate to the MCP tab\n        create_new_tab_button(\"Go to MCP Section (New Tab)\", \"MCP\", key=\"goto_mcp\", use_container_width=True)\n    \n    # Step 6: Using Archon\n    with st.expander(\"Step 6: Using Archon\", expanded=False):\n        st.markdown(\"\"\"\n        ### Using Archon\n        \n        Once everything is set up:\n        \n        1. Go to the **Chat** tab\n        2. Describe the agent you want to build\n        3. Archon will plan and generate the necessary code\n        \n        You can also use Archon directly from your AI IDE if you've configured MCP.\n        \"\"\")\n        # Add a button to navigate to the Chat tab\n        create_new_tab_button(\"Go to Chat Section (New Tab)\", \"Chat\", key=\"goto_chat\", use_container_width=True)\n    \n    # Resources\n    st.markdown(\"\"\"\n    ## Additional Resources\n    \n    - [GitHub Repository](https://github.com/coleam00/archon)\n    - [Archon Community Forum](https://thinktank.ottomator.ai/c/archon/30)\n    - [GitHub Kanban Board](https://github.com/users/coleam00/projects/1)\n    \"\"\")\n\ndef documentation_tab():\n    \"\"\"Display the documentation interface\"\"\"\n    st.header(\"Documentation\")\n    \n    # Create tabs for different documentation sources\n    doc_tabs = st.tabs([\"Pydantic AI Docs\", \"Future Sources\"])\n    \n    with doc_tabs[0]:\n        st.subheader(\"Pydantic AI Documentation\")\n        st.markdown(\"\"\"\n        This section allows you to crawl and index the Pydantic AI documentation.\n        The crawler will:\n        \n        1. Fetch URLs from the Pydantic AI sitemap\n        2. Crawl each page and extract content\n        3. Split content into chunks\n        4. Generate embeddings for each chunk\n        5. Store the chunks in the Supabase database\n        \n        This process may take several minutes depending on the number of pages.\n        \"\"\")\n        \n        # Check if the database is configured\n        supabase_url = get_env_var(\"SUPABASE_URL\")\n        supabase_key = get_env_var(\"SUPABASE_SERVICE_KEY\")\n        \n        if not supabase_url or not supabase_key:\n            st.warning(\"⚠️ Supabase is not configured. Please set up your environment variables first.\")\n            create_new_tab_button(\"Go to Environment Section\", \"Environment\", key=\"goto_env_from_docs\")\n        else:\n            # Initialize session state for tracking crawl progress\n            if \"crawl_tracker\" not in st.session_state:\n                st.session_state.crawl_tracker = None\n            \n            if \"crawl_status\" not in st.session_state:\n                st.session_state.crawl_status = None\n                \n            if \"last_update_time\" not in st.session_state:\n                st.session_state.last_update_time = time.time()\n            \n            # Create columns for the buttons\n            col1, col2 = st.columns(2)\n            \n            with col1:\n                # Button to start crawling\n                if st.button(\"Crawl Pydantic AI Docs\", key=\"crawl_pydantic\") and not (st.session_state.crawl_tracker and st.session_state.crawl_tracker.is_running):\n                    try:\n                        # Import the progress tracker\n                        from archon.crawl_pydantic_ai_docs import start_crawl_with_requests\n                        \n                        # Define a callback function to update the session state\n                        def update_progress(status):\n                            st.session_state.crawl_status = status\n                        \n                        # Start the crawling process in a separate thread\n                        st.session_state.crawl_tracker = start_crawl_with_requests(update_progress)\n                        st.session_state.crawl_status = st.session_state.crawl_tracker.get_status()\n                        \n                        # Force a rerun to start showing progress\n                        st.rerun()\n                    except Exception as e:\n                        st.error(f\"❌ Error starting crawl: {str(e)}\")\n            \n            with col2:\n                # Button to clear existing Pydantic AI docs\n                if st.button(\"Clear Pydantic AI Docs\", key=\"clear_pydantic\"):\n                    with st.spinner(\"Clearing existing Pydantic AI docs...\"):\n                        try:\n                            # Import the function to clear records\n                            from archon.crawl_pydantic_ai_docs import clear_existing_records\n                            \n                            # Run the function to clear records\n                            asyncio.run(clear_existing_records())\n                            st.success(\"✅ Successfully cleared existing Pydantic AI docs from the database.\")\n                            \n                            # Force a rerun to update the UI\n                            st.rerun()\n                        except Exception as e:\n                            st.error(f\"❌ Error clearing Pydantic AI docs: {str(e)}\")\n            \n            # Display crawling progress if a crawl is in progress or has completed\n            if st.session_state.crawl_tracker:\n                # Create a container for the progress information\n                progress_container = st.container()\n                \n                with progress_container:\n                    # Get the latest status\n                    current_time = time.time()\n                    # Update status every second\n                    if current_time - st.session_state.last_update_time >= 1:\n                        st.session_state.crawl_status = st.session_state.crawl_tracker.get_status()\n                        st.session_state.last_update_time = current_time\n                    \n                    status = st.session_state.crawl_status\n                    \n                    # Display a progress bar\n                    if status and status[\"urls_found\"] > 0:\n                        progress = status[\"urls_processed\"] / status[\"urls_found\"]\n                        st.progress(progress)\n                    \n                    # Display status metrics\n                    col1, col2, col3, col4 = st.columns(4)\n                    if status:\n                        col1.metric(\"URLs Found\", status[\"urls_found\"])\n                        col2.metric(\"URLs Processed\", status[\"urls_processed\"])\n                        col3.metric(\"Successful\", status[\"urls_succeeded\"])\n                        col4.metric(\"Failed\", status[\"urls_failed\"])\n                    else:\n                        col1.metric(\"URLs Found\", 0)\n                        col2.metric(\"URLs Processed\", 0)\n                        col3.metric(\"Successful\", 0)\n                        col4.metric(\"Failed\", 0)\n                    \n                    # Display logs in an expander\n                    with st.expander(\"Crawling Logs\", expanded=True):\n                        if status and \"logs\" in status:\n                            logs_text = \"\\n\".join(status[\"logs\"][-20:])  # Show last 20 logs\n                            st.code(logs_text)\n                        else:\n                            st.code(\"No logs available yet...\")\n                    \n                    # Show completion message\n                    if status and not status[\"is_running\"] and status[\"end_time\"]:\n                        if status[\"urls_failed\"] == 0:\n                            st.success(\"✅ Crawling process completed successfully!\")\n                        else:\n                            st.warning(f\"⚠️ Crawling process completed with {status['urls_failed']} failed URLs.\")\n                \n                # Auto-refresh while crawling is in progress\n                if not status or status[\"is_running\"]:\n                    st.rerun()\n        \n        # Display database statistics\n        st.subheader(\"Database Statistics\")\n        try:\n            # Connect to Supabase\n            from supabase import create_client\n            supabase_client = create_client(supabase_url, supabase_key)\n            \n            # Query the count of Pydantic AI docs\n            result = supabase_client.table(\"site_pages\").select(\"count\", count=\"exact\").eq(\"metadata->>source\", \"pydantic_ai_docs\").execute()\n            count = result.count if hasattr(result, \"count\") else 0\n            \n            # Display the count\n            st.metric(\"Pydantic AI Docs Chunks\", count)\n            \n            # Add a button to view the data\n            if count > 0 and st.button(\"View Indexed Data\", key=\"view_pydantic_data\"):\n                # Query a sample of the data\n                sample_data = supabase_client.table(\"site_pages\").select(\"url,title,summary,chunk_number\").eq(\"metadata->>source\", \"pydantic_ai_docs\").limit(10).execute()\n                \n                # Display the sample data\n                st.dataframe(sample_data.data)\n                st.info(\"Showing up to 10 sample records. The database contains more records.\")\n        except Exception as e:\n            st.error(f\"Error querying database: {str(e)}\")\n    \n    with doc_tabs[1]:\n        st.info(\"Additional documentation sources will be available in future updates.\")\n\n@st.cache_data\ndef load_sql_template():\n    \"\"\"Load the SQL template file and cache it\"\"\"\n    with open(os.path.join(os.path.dirname(__file__), \"utils\", \"site_pages.sql\"), \"r\") as f:\n        return f.read()\n\ndef database_tab():\n    \"\"\"Display the database configuration interface\"\"\"\n    st.header(\"Database Configuration\")\n    st.write(\"Set up and manage your Supabase database tables for Archon.\")\n    \n    # Check if Supabase is configured\n    if not supabase:\n        st.error(\"Supabase is not configured. Please set your Supabase URL and Service Key in the Environment tab.\")\n        return\n    \n    # Site Pages Table Setup\n    st.subheader(\"Site Pages Table\")\n    st.write(\"This table stores web page content and embeddings for semantic search.\")\n    \n    # Add information about the table\n    with st.expander(\"About the Site Pages Table\", expanded=False):\n        st.markdown(\"\"\"\n        This table is used to store:\n        - Web page content split into chunks\n        - Vector embeddings for semantic search\n        - Metadata for filtering results\n        \n        The table includes:\n        - URL and chunk number (unique together)\n        - Title and summary of the content\n        - Full text content\n        - Vector embeddings for similarity search\n        - Metadata in JSON format\n        \n        It also creates:\n        - A vector similarity search function\n        - Appropriate indexes for performance\n        - Row-level security policies for Supabase\n        \"\"\")\n    \n    # Check if the table already exists\n    table_exists = False\n    table_has_data = False\n    \n    try:\n        # Try to query the table to see if it exists\n        response = supabase.table(\"site_pages\").select(\"id\").limit(1).execute()\n        table_exists = True\n        \n        # Check if the table has data\n        count_response = supabase.table(\"site_pages\").select(\"*\", count=\"exact\").execute()\n        row_count = count_response.count if hasattr(count_response, 'count') else 0\n        table_has_data = row_count > 0\n        \n        st.success(\"✅ The site_pages table already exists in your database.\")\n        if table_has_data:\n            st.info(f\"The table contains data ({row_count} rows).\")\n        else:\n            st.info(\"The table exists but contains no data.\")\n    except Exception as e:\n        error_str = str(e)\n        if \"relation\" in error_str and \"does not exist\" in error_str:\n            st.info(\"The site_pages table does not exist yet. You can create it below.\")\n        else:\n            st.error(f\"Error checking table status: {error_str}\")\n            st.info(\"Proceeding with the assumption that the table needs to be created.\")\n        table_exists = False\n    \n    # Vector dimensions selection\n    st.write(\"### Vector Dimensions\")\n    st.write(\"Select the embedding dimensions based on your embedding model:\")\n    \n    vector_dim = st.selectbox(\n        \"Embedding Dimensions\",\n        options=[1536, 768, 384, 1024],\n        index=0,\n        help=\"Use 1536 for OpenAI embeddings, 768 for nomic-embed-text with Ollama, or select another dimension based on your model.\"\n    )\n    \n    # Get the SQL with the selected vector dimensions\n    sql_template = load_sql_template()\n    \n    # Replace the vector dimensions in the SQL\n    sql = sql_template.replace(\"vector(1536)\", f\"vector({vector_dim})\")\n    \n    # Also update the match_site_pages function dimensions\n    sql = sql.replace(\"query_embedding vector(1536)\", f\"query_embedding vector({vector_dim})\")\n    \n    # Show the SQL\n    with st.expander(\"View SQL\", expanded=False):\n        st.code(sql, language=\"sql\")\n    \n    # Create table button\n    if not table_exists:\n        if st.button(\"Get Instructions for Creating Site Pages Table\"):\n            show_manual_sql_instructions(sql)\n    else:\n        # Option to recreate the table or clear data\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.warning(\"⚠️ Recreating will delete all existing data.\")\n            if st.button(\"Get Instructions for Recreating Site Pages Table\"):\n                show_manual_sql_instructions(sql, recreate=True)\n        \n        with col2:\n            if table_has_data:\n                st.warning(\"⚠️ Clear all data but keep structure.\")\n                if st.button(\"Clear Table Data\"):\n                    try:\n                        with st.spinner(\"Clearing table data...\"):\n                            # Use the Supabase client to delete all rows\n                            response = supabase.table(\"site_pages\").delete().neq(\"id\", 0).execute()\n                            st.success(\"✅ Table data cleared successfully!\")\n                            st.rerun()\n                    except Exception as e:\n                        st.error(f\"Error clearing table data: {str(e)}\")\n                        # Fall back to manual SQL\n                        truncate_sql = \"TRUNCATE TABLE site_pages;\"\n                        st.code(truncate_sql, language=\"sql\")\n                        st.info(\"Execute this SQL in your Supabase SQL Editor to clear the table data.\")\n                        \n                        # Provide a link to the Supabase SQL Editor\n                        supabase_url = get_env_var(\"SUPABASE_URL\")\n                        if supabase_url:\n                            dashboard_url = get_supabase_sql_editor_url(supabase_url)\n                            st.markdown(f\"[Open Your Supabase SQL Editor with this URL]({dashboard_url})\")\n                    \ndef get_supabase_sql_editor_url(supabase_url):\n    \"\"\"Get the URL for the Supabase SQL Editor\"\"\"\n    try:\n        # Extract the project reference from the URL\n        # Format is typically: https://<project-ref>.supabase.co\n        if '//' in supabase_url:\n            parts = supabase_url.split('//')\n            if len(parts) > 1:\n                domain_parts = parts[1].split('.')\n                if len(domain_parts) > 0:\n                    project_ref = domain_parts[0]\n                    return f\"https://supabase.com/dashboard/project/{project_ref}/sql/new\"\n        \n        # Fallback to a generic URL\n        return \"https://supabase.com/dashboard\"\n    except Exception:\n        return \"https://supabase.com/dashboard\"\n\ndef show_manual_sql_instructions(sql, recreate=False):\n    \"\"\"Show instructions for manually executing SQL in Supabase\"\"\"\n    st.info(\"### Manual SQL Execution Instructions\")\n    \n    # Provide a link to the Supabase SQL Editor\n    supabase_url = get_env_var(\"SUPABASE_URL\")\n    if supabase_url:\n        dashboard_url = get_supabase_sql_editor_url(supabase_url)\n        st.markdown(f\"**Step 1:** [Open Your Supabase SQL Editor with this URL]({dashboard_url})\")\n    else:\n        st.markdown(\"**Step 1:** Open your Supabase Dashboard and navigate to the SQL Editor\")\n    \n    st.markdown(\"**Step 2:** Create a new SQL query\")\n    \n    if recreate:\n        st.markdown(\"**Step 3:** Copy and execute the following SQL:\")\n        drop_sql = \"DROP TABLE IF EXISTS site_pages CASCADE;\"\n        st.code(drop_sql, language=\"sql\")\n        \n        st.markdown(\"**Step 4:** Then copy and execute this SQL:\")\n        st.code(sql, language=\"sql\")\n    else:\n        st.markdown(\"**Step 3:** Copy and execute the following SQL:\")\n        st.code(sql, language=\"sql\")\n    \n    st.success(\"After executing the SQL, return to this page and refresh to see the updated table status.\")\n\ndef agent_service_tab():\n    \"\"\"Display the agent service interface for managing the graph service\"\"\"\n    st.header(\"MCP Agent Service\")\n    st.write(\"Start, restart, and monitor the Archon agent service for MCP.\")\n    \n    # Initialize session state variables if they don't exist\n    if \"service_process\" not in st.session_state:\n        st.session_state.service_process = None\n    if \"service_running\" not in st.session_state:\n        st.session_state.service_running = False\n    if \"service_output\" not in st.session_state:\n        st.session_state.service_output = []\n    if \"output_queue\" not in st.session_state:\n        st.session_state.output_queue = queue.Queue()\n    \n    # Function to check if the service is running\n    def is_service_running():\n        if st.session_state.service_process is None:\n            return False\n        \n        # Check if process is still running\n        return st.session_state.service_process.poll() is None\n    \n    # Function to kill any process using port 8100\n    def kill_process_on_port(port):\n        try:\n            if platform.system() == \"Windows\":\n                # Windows: use netstat to find the process using the port\n                result = subprocess.run(\n                    f'netstat -ano | findstr :{port}',\n                    shell=True, \n                    capture_output=True, \n                    text=True\n                )\n                \n                if result.stdout:\n                    # Extract the PID from the output\n                    for line in result.stdout.splitlines():\n                        if f\":{port}\" in line and \"LISTENING\" in line:\n                            parts = line.strip().split()\n                            pid = parts[-1]\n                            # Kill the process\n                            subprocess.run(f'taskkill /F /PID {pid}', shell=True)\n                            st.session_state.output_queue.put(f\"[{time.strftime('%H:%M:%S')}] Killed any existing process using port {port} (PID: {pid})\\n\")\n                            return True\n            else:\n                # Unix-like systems: use lsof to find the process using the port\n                result = subprocess.run(\n                    f'lsof -i :{port} -t',\n                    shell=True, \n                    capture_output=True, \n                    text=True\n                )\n                \n                if result.stdout:\n                    # Extract the PID from the output\n                    pid = result.stdout.strip()\n                    # Kill the process\n                    subprocess.run(f'kill -9 {pid}', shell=True)\n                    st.session_state.output_queue.put(f\"[{time.strftime('%H:%M:%S')}] Killed process using port {port} (PID: {pid})\\n\")\n                    return True\n                    \n            return False\n        except Exception as e:\n            st.session_state.output_queue.put(f\"[{time.strftime('%H:%M:%S')}] Error killing process on port {port}: {str(e)}\\n\")\n            return False\n    \n    # Update service status\n    st.session_state.service_running = is_service_running()\n    \n    # Process any new output in the queue\n    try:\n        while not st.session_state.output_queue.empty():\n            line = st.session_state.output_queue.get_nowait()\n            if line:\n                st.session_state.service_output.append(line)\n    except Exception:\n        pass\n    \n    # Create button text based on service status\n    button_text = \"Restart Agent Service\" if st.session_state.service_running else \"Start Agent Service\"\n    \n    # Create columns for buttons\n    col1, col2 = st.columns([1, 1])\n    \n    # Start/Restart button\n    with col1:\n        if st.button(button_text, use_container_width=True):\n            # Stop existing process if running\n            if st.session_state.service_running:\n                try:\n                    st.session_state.service_process.terminate()\n                    time.sleep(1)  # Give it time to terminate\n                    if st.session_state.service_process.poll() is None:\n                        # Force kill if still running\n                        st.session_state.service_process.kill()\n                except Exception as e:\n                    st.error(f\"Error stopping service: {str(e)}\")\n            \n            # Clear previous output\n            st.session_state.service_output = []\n            st.session_state.output_queue = queue.Queue()\n            \n            # Kill any process using port 8100\n            kill_process_on_port(8100)\n            \n            # Start new process\n            try:\n                # Get the absolute path to the graph service script\n                base_path = os.path.abspath(os.path.dirname(__file__))\n                graph_service_path = os.path.join(base_path, 'graph_service.py')\n                \n                # Start the process with output redirection\n                process = subprocess.Popen(\n                    [sys.executable, graph_service_path],\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                    text=True,\n                    bufsize=1,\n                    universal_newlines=True\n                )\n                \n                st.session_state.service_process = process\n                st.session_state.service_running = True\n                \n                # Start threads to read output\n                def read_output(stream, queue_obj):\n                    for line in iter(stream.readline, ''):\n                        queue_obj.put(line)\n                    stream.close()\n                \n                # Start threads for stdout and stderr\n                threading.Thread(target=read_output, args=(process.stdout, st.session_state.output_queue), daemon=True).start()\n                threading.Thread(target=read_output, args=(process.stderr, st.session_state.output_queue), daemon=True).start()\n                \n                # Add startup message\n                st.session_state.output_queue.put(f\"[{time.strftime('%H:%M:%S')}] Agent service started\\n\")\n                \n                st.success(\"Agent service started successfully!\")\n                st.rerun()\n                \n            except Exception as e:\n                st.error(f\"Error starting service: {str(e)}\")\n                st.session_state.output_queue.put(f\"[{time.strftime('%H:%M:%S')}] Error: {str(e)}\\n\")\n    \n    # Stop button\n    with col2:\n        stop_button = st.button(\"Stop Agent Service\", disabled=not st.session_state.service_running, use_container_width=True)\n        if stop_button and st.session_state.service_running:\n            try:\n                st.session_state.service_process.terminate()\n                time.sleep(1)  # Give it time to terminate\n                if st.session_state.service_process.poll() is None:\n                    # Force kill if still running\n                    st.session_state.service_process.kill()\n                \n                st.session_state.service_running = False\n                st.session_state.output_queue.put(f\"[{time.strftime('%H:%M:%S')}] Agent service stopped\\n\")\n                st.success(\"Agent service stopped successfully!\")\n                st.rerun()\n                \n            except Exception as e:\n                st.error(f\"Error stopping service: {str(e)}\")\n                st.session_state.output_queue.put(f\"[{time.strftime('%H:%M:%S')}] Error stopping: {str(e)}\\n\")\n    \n    # Service status indicator\n    status_color = \"🟢\" if st.session_state.service_running else \"🔴\"\n    status_text = \"Running\" if st.session_state.service_running else \"Stopped\"\n    st.write(f\"**Service Status:** {status_color} {status_text}\")\n    \n    # Add auto-refresh option\n    auto_refresh = st.checkbox(\"Auto-refresh output (uncheck this before copying any error message)\", value=True)\n    \n    # Display output in a scrollable container\n    st.subheader(\"Service Output\")\n    \n    # Calculate height based on number of lines, but cap it\n    output_height = min(400, max(200, len(st.session_state.service_output) * 20))\n    \n    # Create a scrollable container for the output\n    with st.container():\n        # Join all output lines and display in the container\n        output_text = \"\".join(st.session_state.service_output)\n        \n        # For auto-scrolling, we'll use a different approach\n        if auto_refresh and st.session_state.service_running and output_text:\n            # We'll reverse the output text so the newest lines appear at the top\n            # This way they're always visible without needing to scroll\n            lines = output_text.splitlines()\n            reversed_lines = lines[::-1]  # Reverse the lines\n            output_text = \"\\n\".join(reversed_lines)\n            \n            # Add a note at the top (which will appear at the bottom of the reversed text)\n            note = \"--- SHOWING NEWEST LOGS FIRST (AUTO-SCROLL MODE) ---\\n\\n\"\n            output_text = note + output_text\n        \n        # Use a text area for scrollable output\n        st.text_area(\n            label=\"Realtime Logs from Archon Service\",\n            value=output_text,\n            height=output_height,\n            disabled=True,\n            key=\"output_text_area\"  # Use a fixed key to maintain state between refreshes\n        )\n        \n        # Add a toggle for reversed mode\n        if auto_refresh and st.session_state.service_running:\n            st.caption(\"Logs are shown newest-first for auto-scrolling. Disable auto-refresh to see logs in chronological order.\")\n    \n    # Add a clear output button\n    if st.button(\"Clear Output\"):\n        st.session_state.service_output = []\n        st.rerun()\n    \n    # Auto-refresh if enabled and service is running\n    if auto_refresh and st.session_state.service_running:\n        time.sleep(0.1)  # Small delay to prevent excessive CPU usage\n        st.rerun()\n\ndef environment_tab():\n    \"\"\"Display the environment variables configuration interface\"\"\"\n    st.header(\"Environment Variables\")\n    st.write(\"- Configure your environment variables for Archon. These settings will be saved and used for future sessions.\")\n    st.write(\"- NOTE: Press 'enter' to save after inputting a variable, otherwise click the 'save' button at the bottom.\")\n    st.write(\"- HELP: Hover over the '?' icon on the right for each environment variable for help/examples.\")\n    st.warning(\"⚠️ If your agent service for MCP is already running, you'll need to restart it after changing environment variables.\")\n\n    # Define environment variables and their descriptions from .env.example\n    env_vars = {\n        \"BASE_URL\": {\n            \"description\": \"Base URL for the OpenAI instance (default is https://api.openai.com/v1)\",\n            \"help\": \"OpenAI: https://api.openai.com/v1\\n\\n\\n\\nAnthropic: https://api.anthropic.com/v1\\n\\nOllama (example): http://localhost:11434/v1\\n\\nOpenRouter: https://openrouter.ai/api/v1\",\n            \"sensitive\": False\n        },\n        \"LLM_API_KEY\": {\n            \"description\": \"API key for your LLM provider\",\n            \"help\": \"For OpenAI: https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key\\n\\nFor Anthropic: https://console.anthropic.com/account/keys\\n\\nFor OpenRouter: https://openrouter.ai/keys\\n\\nFor Ollama, no need to set this unless you specifically configured an API key\",\n            \"sensitive\": True\n        },\n        \"OPENAI_API_KEY\": {\n            \"description\": \"Your OpenAI API key\",\n            \"help\": \"Get your Open AI API Key by following these instructions -\\n\\nhttps://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key\\n\\nEven if using OpenRouter, you still need to set this for the embedding model.\\n\\nNo need to set this if using Ollama.\",\n            \"sensitive\": True\n        },\n        \"SUPABASE_URL\": {\n            \"description\": \"URL for your Supabase project\",\n            \"help\": \"Get your SUPABASE_URL from the API section of your Supabase project settings -\\nhttps://supabase.com/dashboard/project/<your project ID>/settings/api\",\n            \"sensitive\": False\n        },\n        \"SUPABASE_SERVICE_KEY\": {\n            \"description\": \"Service key for your Supabase project\",\n            \"help\": \"Get your SUPABASE_SERVICE_KEY from the API section of your Supabase project settings -\\nhttps://supabase.com/dashboard/project/<your project ID>/settings/api\\nOn this page it is called the service_role secret.\",\n            \"sensitive\": True\n        },\n        \"REASONER_MODEL\": {\n            \"description\": \"The LLM you want to use for the reasoner\",\n            \"help\": \"Example: o3-mini\\n\\nExample: deepseek-r1:7b-8k\",\n            \"sensitive\": False\n        },\n        \"PRIMARY_MODEL\": {\n            \"description\": \"The LLM you want to use for the primary agent/coder\",\n            \"help\": \"Example: gpt-4o-mini\\n\\nExample: qwen2.5:14b-instruct-8k\",\n            \"sensitive\": False\n        },\n        \"EMBEDDING_MODEL\": {\n            \"description\": \"Embedding model you want to use\",\n            \"help\": \"Example for Ollama: nomic-embed-text\\n\\nExample for OpenAI: text-embedding-3-small\",\n            \"sensitive\": False\n        }\n    }\n    \n    # Create a form for the environment variables\n    with st.form(\"env_vars_form\"):\n        updated_values = {}\n        \n        # Display input fields for each environment variable\n        for var_name, var_info in env_vars.items():\n            current_value = get_env_var(var_name) or \"\"\n            \n            # Display the variable description\n            st.subheader(var_name)\n            st.write(var_info[\"description\"])\n            \n            # Display input field (password field for sensitive data)\n            if var_info[\"sensitive\"]:\n                # If there's already a value, show asterisks in the placeholder\n                placeholder = \"Set but hidden\" if current_value else \"\"\n                new_value = st.text_input(\n                    f\"Enter {var_name}:\", \n                    type=\"password\",\n                    help=var_info[\"help\"],\n                    key=f\"input_{var_name}\",\n                    placeholder=placeholder\n                )\n                # Only update if user entered something (to avoid overwriting with empty string)\n                if new_value:\n                    updated_values[var_name] = new_value\n            else:\n                new_value = st.text_input(\n                    f\"Enter {var_name}:\", \n                    value=current_value,\n                    help=var_info[\"help\"],\n                    key=f\"input_{var_name}\"\n                )\n                # Always update non-sensitive values (can be empty)\n                updated_values[var_name] = new_value\n            \n            # Add a separator between variables\n            st.markdown(\"---\")\n        \n        # Submit button\n        submitted = st.form_submit_button(\"Save Environment Variables\")\n        \n        if submitted:\n            # Save all updated values\n            success = True\n            for var_name, value in updated_values.items():\n                if value:  # Only save non-empty values\n                    if not save_env_var(var_name, value):\n                        success = False\n                        st.error(f\"Failed to save {var_name}.\")\n            \n            if success:\n                st.success(\"Environment variables saved successfully!\")\n                reload_archon_graph()\n\nasync def main():\n    # Check for tab query parameter\n    query_params = st.query_params\n    if \"tab\" in query_params:\n        tab_name = query_params[\"tab\"]\n        if tab_name in [\"Intro\", \"Chat\", \"Environment\", \"Database\", \"Documentation\", \"Agent Service\", \"MCP\", \"Future Enhancements\"]:\n            st.session_state.selected_tab = tab_name\n\n    # Add sidebar navigation\n    with st.sidebar:\n        st.image(\"public/ArchonLightGrey.png\", width=1000)\n        \n        # Navigation options with vertical buttons\n        st.write(\"### Navigation\")\n        \n        # Initialize session state for selected tab if not present\n        if \"selected_tab\" not in st.session_state:\n            st.session_state.selected_tab = \"Intro\"\n        \n        # Vertical navigation buttons\n        intro_button = st.button(\"Intro\", use_container_width=True, key=\"intro_button\")\n        chat_button = st.button(\"Chat\", use_container_width=True, key=\"chat_button\")\n        env_button = st.button(\"Environment\", use_container_width=True, key=\"env_button\")\n        db_button = st.button(\"Database\", use_container_width=True, key=\"db_button\")\n        docs_button = st.button(\"Documentation\", use_container_width=True, key=\"docs_button\")\n        service_button = st.button(\"Agent Service\", use_container_width=True, key=\"service_button\")\n        mcp_button = st.button(\"MCP\", use_container_width=True, key=\"mcp_button\")\n        future_enhancements_button = st.button(\"Future Enhancements\", use_container_width=True, key=\"future_enhancements_button\")\n        \n        # Update selected tab based on button clicks\n        if intro_button:\n            st.session_state.selected_tab = \"Intro\"\n        elif chat_button:\n            st.session_state.selected_tab = \"Chat\"\n        elif mcp_button:\n            st.session_state.selected_tab = \"MCP\"\n        elif env_button:\n            st.session_state.selected_tab = \"Environment\"\n        elif service_button:\n            st.session_state.selected_tab = \"Agent Service\"\n        elif db_button:\n            st.session_state.selected_tab = \"Database\"\n        elif docs_button:\n            st.session_state.selected_tab = \"Documentation\"\n        elif future_enhancements_button:\n            st.session_state.selected_tab = \"Future Enhancements\"\n    \n    # Display the selected tab\n    if st.session_state.selected_tab == \"Intro\":\n        st.title(\"Archon - Introduction\")\n        intro_tab()\n    elif st.session_state.selected_tab == \"Chat\":\n        st.title(\"Archon - Agent Builder\")\n        await chat_tab()\n    elif st.session_state.selected_tab == \"MCP\":\n        st.title(\"Archon - MCP Configuration\")\n        mcp_tab()\n    elif st.session_state.selected_tab == \"Environment\":\n        st.title(\"Archon - Environment Configuration\")\n        environment_tab()\n    elif st.session_state.selected_tab == \"Agent Service\":\n        st.title(\"Archon - Agent Service\")\n        agent_service_tab()\n    elif st.session_state.selected_tab == \"Database\":\n        st.title(\"Archon - Database Configuration\")\n        database_tab()\n    elif st.session_state.selected_tab == \"Documentation\":\n        st.title(\"Archon - Documentation\")\n        documentation_tab()\n    elif st.session_state.selected_tab == \"Future Enhancements\":\n        st.title(\"Archon - Future Enhancements\")\n        future_enhancements_tab()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "iterations/v4-streamlit-ui-overhaul/utils/utils.py", "content": "import os\nfrom datetime import datetime\nfrom functools import wraps\nimport inspect\nimport json\nfrom typing import Optional\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\ndef write_to_log(message: str):\n    \"\"\"Write a message to the logs.txt file in the workbench directory.\n    \n    Args:\n        message: The message to log\n    \"\"\"\n    # Get the directory one level up from the current file\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    parent_dir = os.path.dirname(current_dir)\n    workbench_dir = os.path.join(parent_dir, \"workbench\")\n    log_path = os.path.join(workbench_dir, \"logs.txt\")\n    os.makedirs(workbench_dir, exist_ok=True)\n\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    log_entry = f\"[{timestamp}] {message}\\n\"\n\n    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n        f.write(log_entry)\n\ndef get_env_var(var_name: str) -> Optional[str]:\n    \"\"\"Get an environment variable from the saved JSON file or from environment variables.\n    \n    Args:\n        var_name: The name of the environment variable to retrieve\n        \n    Returns:\n        The value of the environment variable or None if not found\n    \"\"\"\n    # Path to the JSON file storing environment variables\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    parent_dir = os.path.dirname(current_dir)\n    env_file_path = os.path.join(current_dir, \"env_vars.json\")\n    \n    # First try to get from JSON file\n    if os.path.exists(env_file_path):\n        try:\n            with open(env_file_path, \"r\") as f:\n                env_vars = json.load(f)\n                if var_name in env_vars and env_vars[var_name]:\n                    return env_vars[var_name]\n        except (json.JSONDecodeError, IOError) as e:\n            write_to_log(f\"Error reading env_vars.json: {str(e)}\")\n    \n    # If not found in JSON, try to get from environment variables\n    return os.environ.get(var_name)\n\ndef save_env_var(var_name: str, value: str) -> bool:\n    \"\"\"Save an environment variable to the JSON file.\n    \n    Args:\n        var_name: The name of the environment variable\n        value: The value to save\n        \n    Returns:\n        True if successful, False otherwise\n    \"\"\"\n    # Path to the JSON file storing environment variables\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    env_file_path = os.path.join(current_dir, \"env_vars.json\")\n    \n    # Load existing env vars or create empty dict\n    env_vars = {}\n    if os.path.exists(env_file_path):\n        try:\n            with open(env_file_path, \"r\") as f:\n                env_vars = json.load(f)\n        except (json.JSONDecodeError, IOError) as e:\n            write_to_log(f\"Error reading env_vars.json: {str(e)}\")\n            # Continue with empty dict if file is corrupted\n    \n    # Update the variable\n    env_vars[var_name] = value\n    \n    # Save back to file\n    try:\n        with open(env_file_path, \"w\") as f:\n            json.dump(env_vars, f, indent=2)\n        return True\n    except IOError as e:\n        write_to_log(f\"Error writing to env_vars.json: {str(e)}\")\n        return False\n\ndef log_node_execution(func):\n    \"\"\"Decorator to log the start and end of graph node execution.\n    \n    Args:\n        func: The async function to wrap\n    \"\"\"\n    @wraps(func)\n    async def wrapper(*args, **kwargs):\n        func_name = func.__name__\n        write_to_log(f\"Starting node: {func_name}\")\n        try:\n            result = await func(*args, **kwargs)\n            write_to_log(f\"Completed node: {func_name}\")\n            return result\n        except Exception as e:\n            write_to_log(f\"Error in node {func_name}: {str(e)}\")\n            raise\n    return wrapper\n"}
{"type": "source_file", "path": "iterations/v5-parallel-specialized-agents/archon/archon_graph.py", "content": "from pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom pydantic_ai import Agent, RunContext\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing import TypedDict, Annotated, List, Any\nfrom langgraph.config import get_stream_writer\nfrom langgraph.types import interrupt\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\nfrom supabase import Client\nimport logfire\nimport os\nimport sys\n\n# Import the message classes from Pydantic AI\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelMessagesTypeAdapter\n)\n\n# Add the parent directory to Python path\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom archon.pydantic_ai_coder import pydantic_ai_coder, PydanticAIDeps\nfrom archon.refiner_agents.prompt_refiner_agent import prompt_refiner_agent\nfrom archon.refiner_agents.tools_refiner_agent import tools_refiner_agent, ToolsRefinerDeps\nfrom archon.refiner_agents.agent_refiner_agent import agent_refiner_agent, AgentRefinerDeps\nfrom archon.agent_tools import list_documentation_pages_tool\nfrom utils.utils import get_env_var, get_clients\n\n# Load environment variables\nload_dotenv()\n\n# Configure logfire to suppress warnings (optional)\nlogfire.configure(send_to_logfire='never')\n\nprovider = get_env_var('LLM_PROVIDER') or 'OpenAI'\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\n\nis_anthropic = provider == \"Anthropic\"\nis_openai = provider == \"OpenAI\"\n\nreasoner_llm_model_name = get_env_var('REASONER_MODEL') or 'o3-mini'\nreasoner_llm_model = AnthropicModel(reasoner_llm_model_name, api_key=api_key) if is_anthropic else OpenAIModel(reasoner_llm_model_name, base_url=base_url, api_key=api_key)\n\nreasoner = Agent(  \n    reasoner_llm_model,\n    system_prompt='You are an expert at coding AI agents with Pydantic AI and defining the scope for doing so.',  \n)\n\nprimary_llm_model_name = get_env_var('PRIMARY_MODEL') or 'gpt-4o-mini'\nprimary_llm_model = AnthropicModel(primary_llm_model_name, api_key=api_key) if is_anthropic else OpenAIModel(primary_llm_model_name, base_url=base_url, api_key=api_key)\n\nrouter_agent = Agent(  \n    primary_llm_model,\n    system_prompt='Your job is to route the user message either to the end of the conversation or to continue coding the AI agent.',  \n)\n\nend_conversation_agent = Agent(  \n    primary_llm_model,\n    system_prompt='Your job is to end a conversation for creating an AI agent by giving instructions for how to execute the agent and they saying a nice goodbye to the user.',  \n)\n\n# Initialize clients\nembedding_client, supabase = get_clients()\n\n# Define state schema\nclass AgentState(TypedDict):\n    latest_user_message: str\n    messages: Annotated[List[bytes], lambda x, y: x + y]\n\n    scope: str\n\n    refined_prompt: str\n    refined_tools: str\n    refined_agent: str\n\n# Scope Definition Node with Reasoner LLM\nasync def define_scope_with_reasoner(state: AgentState):\n    # First, get the documentation pages so the reasoner can decide which ones are necessary\n    documentation_pages = await list_documentation_pages_tool(supabase)\n    documentation_pages_str = \"\\n\".join(documentation_pages)\n\n    # Then, use the reasoner to define the scope\n    prompt = f\"\"\"\n    User AI Agent Request: {state['latest_user_message']}\n    \n    Create detailed scope document for the AI agent including:\n    - Architecture diagram\n    - Core components\n    - External dependencies\n    - Testing strategy\n\n    Also based on these documentation pages available:\n\n    {documentation_pages_str}\n\n    Include a list of documentation pages that are relevant to creating this agent for the user in the scope document.\n    \"\"\"\n\n    result = await reasoner.run(prompt)\n    scope = result.data\n\n    # Get the directory one level up from the current file\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    parent_dir = os.path.dirname(current_dir)\n    scope_path = os.path.join(parent_dir, \"workbench\", \"scope.md\")\n    os.makedirs(os.path.join(parent_dir, \"workbench\"), exist_ok=True)\n\n    with open(scope_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(scope)\n    \n    return {\"scope\": scope}\n\n# Coding Node with Feedback Handling\nasync def coder_agent(state: AgentState, writer):    \n    # Prepare dependencies\n    deps = PydanticAIDeps(\n        supabase=supabase,\n        embedding_client=embedding_client,\n        reasoner_output=state['scope']\n    )\n\n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    # The prompt either needs to be the user message (initial agent request or feedback)\n    # or the refined prompt/tools/agent if we are in that stage of the agent creation process\n    if 'refined_prompt' in state and state['refined_prompt']:\n        prompt = f\"\"\"\n        I need you to refine the agent you created. \n        \n        Here is the refined prompt:\\n\n        {state['refined_prompt']}\\n\\n\n\n        Here are the refined tools:\\n\n        {state['refined_tools']}\\n\n\n        And finally, here are the changes to the agent definition to make if any:\\n\n        {state['refined_agent']}\\n\\n\n\n        Output any changes necessary to the agent code based on these refinements.\n        \"\"\"\n    else:\n        prompt = state['latest_user_message']\n\n    # Run the agent in a stream\n    if not is_openai:\n        writer = get_stream_writer()\n        result = await pydantic_ai_coder.run(prompt, deps=deps, message_history=message_history)\n        writer(result.data)\n    else:\n        async with pydantic_ai_coder.run_stream(\n            state['latest_user_message'],\n            deps=deps,\n            message_history=message_history\n        ) as result:\n            # Stream partial text as it arrives\n            async for chunk in result.stream_text(delta=True):\n                writer(chunk)\n\n    # print(ModelMessagesTypeAdapter.validate_json(result.new_messages_json()))\n\n    # Add the new conversation history (including tool calls)\n    # Reset the refined properties in case they were just used to refine the agent\n    return {\n        \"messages\": [result.new_messages_json()],\n        \"refined_prompt\": \"\",\n        \"refined_tools\": \"\",\n        \"refined_agent\": \"\"\n    }\n\n# Interrupt the graph to get the user's next message\ndef get_next_user_message(state: AgentState):\n    value = interrupt({})\n\n    # Set the user's latest message for the LLM to continue the conversation\n    return {\n        \"latest_user_message\": value\n    }\n\n# Determine if the user is finished creating their AI agent or not\nasync def route_user_message(state: AgentState):\n    prompt = f\"\"\"\n    The user has sent a message: \n    \n    {state['latest_user_message']}\n\n    If the user wants to end the conversation, respond with just the text \"finish_conversation\".\n    If the user wants to continue coding the AI agent and gave feedback, respond with just the text \"coder_agent\".\n    If the user asks specifically to \"refine\" the agent, respond with just the text \"refine\".\n    \"\"\"\n\n    result = await router_agent.run(prompt)\n    \n    if result.data == \"finish_conversation\": return \"finish_conversation\"\n    if result.data == \"refine\": return [\"refine_prompt\", \"refine_tools\", \"refine_agent\"]\n    return \"coder_agent\"\n\n# Refines the prompt for the AI agent\nasync def refine_prompt(state: AgentState):\n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    prompt = \"Based on the current conversation, refine the prompt for the agent.\"\n\n    # Run the agent to refine the prompt for the agent being created\n    result = await prompt_refiner_agent.run(prompt, message_history=message_history)\n\n    return {\"refined_prompt\": result.data}\n\n# Refines the tools for the AI agent\nasync def refine_tools(state: AgentState):\n    # Prepare dependencies\n    deps = ToolsRefinerDeps(\n        supabase=supabase,\n        embedding_client=embedding_client\n    )\n\n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    prompt = \"Based on the current conversation, refine the tools for the agent.\"\n\n    # Run the agent to refine the tools for the agent being created\n    result = await tools_refiner_agent.run(prompt, deps=deps, message_history=message_history)\n\n    return {\"refined_tools\": result.data}\n\n# Refines the defintion for the AI agent\nasync def refine_agent(state: AgentState):\n    # Prepare dependencies\n    deps = AgentRefinerDeps(\n        supabase=supabase,\n        embedding_client=embedding_client\n    )\n\n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    prompt = \"Based on the current conversation, refine the agent definition.\"\n\n    # Run the agent to refine the definition for the agent being created\n    result = await agent_refiner_agent.run(prompt, deps=deps, message_history=message_history)\n\n    return {\"refined_agent\": result.data}\n\n# End of conversation agent to give instructions for executing the agent\nasync def finish_conversation(state: AgentState, writer):    \n    # Get the message history into the format for Pydantic AI\n    message_history: list[ModelMessage] = []\n    for message_row in state['messages']:\n        message_history.extend(ModelMessagesTypeAdapter.validate_json(message_row))\n\n    # Run the agent in a stream\n    if not is_openai:\n        writer = get_stream_writer()\n        result = await end_conversation_agent.run(state['latest_user_message'], message_history= message_history)\n        writer(result.data)   \n    else: \n        async with end_conversation_agent.run_stream(\n            state['latest_user_message'],\n            message_history= message_history\n        ) as result:\n            # Stream partial text as it arrives\n            async for chunk in result.stream_text(delta=True):\n                writer(chunk)\n\n    return {\"messages\": [result.new_messages_json()]}        \n\n# Build workflow\nbuilder = StateGraph(AgentState)\n\n# Add nodes\nbuilder.add_node(\"define_scope_with_reasoner\", define_scope_with_reasoner)\nbuilder.add_node(\"coder_agent\", coder_agent)\nbuilder.add_node(\"get_next_user_message\", get_next_user_message)\nbuilder.add_node(\"refine_prompt\", refine_prompt)\nbuilder.add_node(\"refine_tools\", refine_tools)\nbuilder.add_node(\"refine_agent\", refine_agent)\nbuilder.add_node(\"finish_conversation\", finish_conversation)\n\n# Set edges\nbuilder.add_edge(START, \"define_scope_with_reasoner\")\nbuilder.add_edge(\"define_scope_with_reasoner\", \"coder_agent\")\nbuilder.add_edge(\"coder_agent\", \"get_next_user_message\")\nbuilder.add_conditional_edges(\n    \"get_next_user_message\",\n    route_user_message,\n    [\"coder_agent\", \"finish_conversation\", \"refine_prompt\", \"refine_tools\", \"refine_agent\"]\n)\nbuilder.add_edge(\"refine_prompt\", \"coder_agent\")\nbuilder.add_edge(\"refine_tools\", \"coder_agent\")\nbuilder.add_edge(\"refine_agent\", \"coder_agent\")\nbuilder.add_edge(\"finish_conversation\", END)\n\n# Configure persistence\nmemory = MemorySaver()\nagentic_flow = builder.compile(checkpointer=memory)"}
{"type": "source_file", "path": "iterations/v5-parallel-specialized-agents/archon/pydantic_ai_coder.py", "content": "from __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\nimport logfire\nimport asyncio\nimport httpx\nimport os\nimport sys\nimport json\nfrom typing import List\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom openai import AsyncOpenAI\nfrom supabase import Client\n\n# Add the parent directory to sys.path to allow importing from the parent directory\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils.utils import get_env_var\nfrom archon.agent_prompts import primary_coder_prompt\nfrom archon.agent_tools import (\n    retrieve_relevant_documentation_tool,\n    list_documentation_pages_tool,\n    get_page_content_tool\n)\n\nload_dotenv()\n\nprovider = get_env_var('LLM_PROVIDER') or 'OpenAI'\nllm = get_env_var('PRIMARY_MODEL') or 'gpt-4o-mini'\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\n\nmodel = AnthropicModel(llm, api_key=api_key) if provider == \"Anthropic\" else OpenAIModel(llm, base_url=base_url, api_key=api_key)\n\nlogfire.configure(send_to_logfire='if-token-present')\n\n@dataclass\nclass PydanticAIDeps:\n    supabase: Client\n    embedding_client: AsyncOpenAI\n    reasoner_output: str\n\npydantic_ai_coder = Agent(\n    model,\n    system_prompt=primary_coder_prompt,\n    deps_type=PydanticAIDeps,\n    retries=2\n)\n\n@pydantic_ai_coder.system_prompt  \ndef add_reasoner_output(ctx: RunContext[str]) -> str:\n    return f\"\"\"\n    \\n\\nAdditional thoughts/instructions from the reasoner LLM. \n    This scope includes documentation pages for you to search as well: \n    {ctx.deps.reasoner_output}\n    \"\"\"\n\n@pydantic_ai_coder.tool\nasync def retrieve_relevant_documentation(ctx: RunContext[PydanticAIDeps], user_query: str) -> str:\n    \"\"\"\n    Retrieve relevant documentation chunks based on the query with RAG.\n    \n    Args:\n        ctx: The context including the Supabase client and OpenAI client\n        user_query: The user's question or query\n        \n    Returns:\n        A formatted string containing the top 4 most relevant documentation chunks\n    \"\"\"\n    return await retrieve_relevant_documentation_tool(ctx.deps.supabase, ctx.deps.embedding_client, user_query)\n\n@pydantic_ai_coder.tool\nasync def list_documentation_pages(ctx: RunContext[PydanticAIDeps]) -> List[str]:\n    \"\"\"\n    Retrieve a list of all available Pydantic AI documentation pages.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    return await list_documentation_pages_tool(ctx.deps.supabase)\n\n@pydantic_ai_coder.tool\nasync def get_page_content(ctx: RunContext[PydanticAIDeps], url: str) -> str:\n    \"\"\"\n    Retrieve the full content of a specific documentation page by combining all its chunks.\n    \n    Args:\n        ctx: The context including the Supabase client\n        url: The URL of the page to retrieve\n        \n    Returns:\n        str: The complete page content with all chunks combined in order\n    \"\"\"\n    return await get_page_content_tool(ctx.deps.supabase, url)"}
{"type": "source_file", "path": "iterations/v5-parallel-specialized-agents/archon/refiner_agents/agent_refiner_agent.py", "content": "from __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\nimport logfire\nimport asyncio\nimport httpx\nimport os\nimport sys\nimport json\nfrom typing import List\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom openai import AsyncOpenAI\nfrom supabase import Client\n\n# Add the parent directory to sys.path to allow importing from the parent directory\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\nfrom utils.utils import get_env_var\nfrom archon.agent_prompts import agent_refiner_prompt\nfrom archon.agent_tools import (\n    retrieve_relevant_documentation_tool,\n    list_documentation_pages_tool,\n    get_page_content_tool\n)\n\nload_dotenv()\n\nprovider = get_env_var('LLM_PROVIDER') or 'OpenAI'\nllm = get_env_var('PRIMARY_MODEL') or 'gpt-4o-mini'\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\n\nmodel = AnthropicModel(llm, api_key=api_key) if provider == \"Anthropic\" else OpenAIModel(llm, base_url=base_url, api_key=api_key)\nembedding_model = get_env_var('EMBEDDING_MODEL') or 'text-embedding-3-small'\n\nlogfire.configure(send_to_logfire='if-token-present')\n\n@dataclass\nclass AgentRefinerDeps:\n    supabase: Client\n    embedding_client: AsyncOpenAI\n\nagent_refiner_agent = Agent(\n    model,\n    system_prompt=agent_refiner_prompt,\n    deps_type=AgentRefinerDeps,\n    retries=2\n)\n\n@agent_refiner_agent.tool\nasync def retrieve_relevant_documentation(ctx: RunContext[AgentRefinerDeps], query: str) -> str:\n    \"\"\"\n    Retrieve relevant documentation chunks based on the query with RAG.\n    Make sure your searches always focus on implementing the agent itself.\n    \n    Args:\n        ctx: The context including the Supabase client and OpenAI client\n        query: Your query to retrieve relevant documentation for implementing agents\n        \n    Returns:\n        A formatted string containing the top 4 most relevant documentation chunks\n    \"\"\"\n    return await retrieve_relevant_documentation_tool(ctx.deps.supabase, ctx.deps.embedding_client, query)\n\n@agent_refiner_agent.tool\nasync def list_documentation_pages(ctx: RunContext[AgentRefinerDeps]) -> List[str]:\n    \"\"\"\n    Retrieve a list of all available Pydantic AI documentation pages.\n    This will give you all pages available, but focus on the ones related to configuring agents and their dependencies.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    return await list_documentation_pages_tool(ctx.deps.supabase)\n\n@agent_refiner_agent.tool\nasync def get_page_content(ctx: RunContext[AgentRefinerDeps], url: str) -> str:\n    \"\"\"\n    Retrieve the full content of a specific documentation page by combining all its chunks.\n    Only use this tool to get pages related to setting up agents with Pydantic AI.\n    \n    Args:\n        ctx: The context including the Supabase client\n        url: The URL of the page to retrieve\n        \n    Returns:\n        str: The complete page content with all chunks combined in order\n    \"\"\"\n    return await get_page_content_tool(ctx.deps.supabase, url)"}
{"type": "source_file", "path": "iterations/v5-parallel-specialized-agents/run_docker.py", "content": "#!/usr/bin/env python\n\"\"\"\nSimple script to build and run Archon Docker containers.\n\"\"\"\n\nimport os\nimport subprocess\nimport platform\nimport time\nfrom pathlib import Path\n\ndef run_command(command, cwd=None):\n    \"\"\"Run a command and print output in real-time.\"\"\"\n    print(f\"Running: {' '.join(command)}\")\n    process = subprocess.Popen(\n        command,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=False,  \n        cwd=cwd\n    )\n    \n    for line in process.stdout:\n        try:\n            decoded_line = line.decode('utf-8', errors='replace')\n            print(decoded_line.strip())\n        except Exception as e:\n            print(f\"Error processing output: {e}\")\n    \n    process.wait()\n    return process.returncode\n\ndef check_docker():\n    \"\"\"Check if Docker is installed and running.\"\"\"\n    try:\n        subprocess.run(\n            [\"docker\", \"--version\"], \n            check=True, \n            stdout=subprocess.PIPE, \n            stderr=subprocess.PIPE\n        )\n        return True\n    except (subprocess.SubprocessError, FileNotFoundError):\n        print(\"Error: Docker is not installed or not in PATH\")\n        return False\n\ndef main():\n    \"\"\"Main function to build and run Archon containers.\"\"\"\n    # Check if Docker is available\n    if not check_docker():\n        return 1\n    \n    # Get the base directory\n    base_dir = Path(__file__).parent.absolute()\n    \n    # Check for .env file\n    env_file = base_dir / \".env\"\n    env_args = []\n    if env_file.exists():\n        print(f\"Using environment file: {env_file}\")\n        env_args = [\"--env-file\", str(env_file)]\n    else:\n        print(\"No .env file found. Continuing without environment variables.\")\n    \n    # Build the MCP container\n    print(\"\\n=== Building Archon MCP container ===\")\n    mcp_dir = base_dir / \"mcp\"\n    if run_command([\"docker\", \"build\", \"-t\", \"archon-mcp:latest\", \".\"], cwd=mcp_dir) != 0:\n        print(\"Error building MCP container\")\n        return 1\n    \n    # Build the main Archon container\n    print(\"\\n=== Building main Archon container ===\")\n    if run_command([\"docker\", \"build\", \"-t\", \"archon:latest\", \".\"], cwd=base_dir) != 0:\n        print(\"Error building main Archon container\")\n        return 1\n    \n    # Check if the container exists (running or stopped)\n    try:\n        result = subprocess.run(\n            [\"docker\", \"ps\", \"-a\", \"-q\", \"--filter\", \"name=archon-container\"],\n            check=True,\n            capture_output=True,\n            text=True\n        )\n        if result.stdout.strip():\n            print(\"\\n=== Removing existing Archon container ===\")\n            container_id = result.stdout.strip()\n            print(f\"Found container with ID: {container_id}\")\n            \n            # Check if the container is running\n            running_check = subprocess.run(\n                [\"docker\", \"ps\", \"-q\", \"--filter\", \"id=\" + container_id],\n                check=True,\n                capture_output=True,\n                text=True\n            )\n            \n            # If running, stop it first\n            if running_check.stdout.strip():\n                print(\"Container is running. Stopping it first...\")\n                stop_result = run_command([\"docker\", \"stop\", container_id])\n                if stop_result != 0:\n                    print(\"Warning: Failed to stop container gracefully, will try force removal\")\n            \n            # Remove the container with force flag to ensure it's removed\n            print(\"Removing container...\")\n            rm_result = run_command([\"docker\", \"rm\", \"-f\", container_id])\n            if rm_result != 0:\n                print(\"Error: Failed to remove container. Please remove it manually with:\")\n                print(f\"  docker rm -f {container_id}\")\n                return 1\n            \n            print(\"Container successfully removed\")\n    except subprocess.SubprocessError as e:\n        print(f\"Error checking for existing containers: {e}\")\n        pass\n    \n    # Run the Archon container\n    print(\"\\n=== Starting Archon container ===\")\n    cmd = [\n        \"docker\", \"run\", \"-d\",\n        \"--name\", \"archon-container\",\n        \"-p\", \"8501:8501\",\n        \"-p\", \"8100:8100\",\n        \"--add-host\", \"host.docker.internal:host-gateway\"\n    ]\n    \n    # Add environment variables if .env exists\n    if env_args:\n        cmd.extend(env_args)\n    \n    # Add image name\n    cmd.append(\"archon:latest\")\n    \n    if run_command(cmd) != 0:\n        print(\"Error starting Archon container\")\n        return 1\n    \n    # Wait a moment for the container to start\n    time.sleep(2)\n    \n    # Print success message\n    print(\"\\n=== Archon is now running! ===\")\n    print(\"-> Access the Streamlit UI at: http://localhost:8501\")\n    print(\"-> MCP container is ready to use - see the MCP tab in the UI.\")\n    print(\"\\nTo stop Archon, run: docker stop archon-container && docker rm archon-container\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    exit(main())\n"}
{"type": "source_file", "path": "iterations/v5-parallel-specialized-agents/archon/crawl_pydantic_ai_docs.py", "content": "import os\nimport sys\nimport asyncio\nimport threading\nimport subprocess\nimport requests\nimport json\nimport time\nfrom typing import List, Dict, Any, Optional, Callable\nfrom xml.etree import ElementTree\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\nfrom urllib.parse import urlparse\nfrom dotenv import load_dotenv\nfrom openai import AsyncOpenAI\nimport re\nimport html2text\n\n# Add the parent directory to sys.path to allow importing from the parent directory\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils.utils import get_env_var, get_clients\n\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nload_dotenv()\n\n# Initialize embedding and Supabase clients\nembedding_client, supabase = get_clients()\n\n# Define the embedding model for embedding the documentation for RAG\nembedding_model = get_env_var('EMBEDDING_MODEL') or 'text-embedding-3-small'\n\n# LLM client setup\nllm_client = None\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-api-key-provided'\nprovider = get_env_var('LLM_PROVIDER') or 'OpenAI'\n\n# Setup OpenAI client for LLM\nif provider == \"Ollama\":\n    if api_key == \"NOT_REQUIRED\":\n        api_key = \"ollama\"  # Use a dummy key for Ollama\n    llm_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\nelse:\n    llm_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n\n# Initialize HTML to Markdown converter\nhtml_converter = html2text.HTML2Text()\nhtml_converter.ignore_links = False\nhtml_converter.ignore_images = False\nhtml_converter.ignore_tables = False\nhtml_converter.body_width = 0  # No wrapping\n\n@dataclass\nclass ProcessedChunk:\n    url: str\n    chunk_number: int\n    title: str\n    summary: str\n    content: str\n    metadata: Dict[str, Any]\n    embedding: List[float]\n\nclass CrawlProgressTracker:\n    \"\"\"Class to track progress of the crawling process.\"\"\"\n    \n    def __init__(self, \n                 progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None):\n        \"\"\"Initialize the progress tracker.\n        \n        Args:\n            progress_callback: Function to call with progress updates\n        \"\"\"\n        self.progress_callback = progress_callback\n        self.urls_found = 0\n        self.urls_processed = 0\n        self.urls_succeeded = 0\n        self.urls_failed = 0\n        self.chunks_stored = 0\n        self.logs = []\n        self.is_running = False\n        self.start_time = None\n        self.end_time = None\n    \n    def log(self, message: str):\n        \"\"\"Add a log message and update progress.\"\"\"\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        log_entry = f\"[{timestamp}] {message}\"\n        self.logs.append(log_entry)\n        print(message)  # Also print to console\n        \n        # Call the progress callback if provided\n        if self.progress_callback:\n            self.progress_callback(self.get_status())\n    \n    def start(self):\n        \"\"\"Mark the crawling process as started.\"\"\"\n        self.is_running = True\n        self.start_time = datetime.now()\n        self.log(\"Crawling process started\")\n        \n        # Call the progress callback if provided\n        if self.progress_callback:\n            self.progress_callback(self.get_status())\n    \n    def complete(self):\n        \"\"\"Mark the crawling process as completed.\"\"\"\n        self.is_running = False\n        self.end_time = datetime.now()\n        duration = self.end_time - self.start_time if self.start_time else None\n        duration_str = str(duration).split('.')[0] if duration else \"unknown\"\n        self.log(f\"Crawling process completed in {duration_str}\")\n        \n        # Call the progress callback if provided\n        if self.progress_callback:\n            self.progress_callback(self.get_status())\n    \n    def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get the current status of the crawling process.\"\"\"\n        return {\n            \"is_running\": self.is_running,\n            \"urls_found\": self.urls_found,\n            \"urls_processed\": self.urls_processed,\n            \"urls_succeeded\": self.urls_succeeded,\n            \"urls_failed\": self.urls_failed,\n            \"chunks_stored\": self.chunks_stored,\n            \"progress_percentage\": (self.urls_processed / self.urls_found * 100) if self.urls_found > 0 else 0,\n            \"logs\": self.logs,\n            \"start_time\": self.start_time,\n            \"end_time\": self.end_time\n        }\n    \n    @property\n    def is_completed(self) -> bool:\n        \"\"\"Return True if the crawling process is completed.\"\"\"\n        return not self.is_running and self.end_time is not None\n    \n    @property\n    def is_successful(self) -> bool:\n        \"\"\"Return True if the crawling process completed successfully.\"\"\"\n        return self.is_completed and self.urls_failed == 0 and self.urls_succeeded > 0\n\ndef chunk_text(text: str, chunk_size: int = 5000) -> List[str]:\n    \"\"\"Split text into chunks, respecting code blocks and paragraphs.\"\"\"\n    chunks = []\n    start = 0\n    text_length = len(text)\n\n    while start < text_length:\n        # Calculate end position\n        end = start + chunk_size\n\n        # If we're at the end of the text, just take what's left\n        if end >= text_length:\n            chunks.append(text[start:].strip())\n            break\n\n        # Try to find a code block boundary first (```)\n        chunk = text[start:end]\n        code_block = chunk.rfind('```')\n        if code_block != -1 and code_block > chunk_size * 0.3:\n            end = start + code_block\n\n        # If no code block, try to break at a paragraph\n        elif '\\n\\n' in chunk:\n            # Find the last paragraph break\n            last_break = chunk.rfind('\\n\\n')\n            if last_break > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size\n                end = start + last_break\n\n        # If no paragraph break, try to break at a sentence\n        elif '. ' in chunk:\n            # Find the last sentence break\n            last_period = chunk.rfind('. ')\n            if last_period > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size\n                end = start + last_period + 1\n\n        # Extract chunk and clean it up\n        chunk = text[start:end].strip()\n        if chunk:\n            chunks.append(chunk)\n\n        # Move start position for next chunk\n        start = max(start + 1, end)\n\n    return chunks\n\nasync def get_title_and_summary(chunk: str, url: str) -> Dict[str, str]:\n    \"\"\"Extract title and summary using GPT-4.\"\"\"\n    system_prompt = \"\"\"You are an AI that extracts titles and summaries from documentation chunks.\n    Return a JSON object with 'title' and 'summary' keys.\n    For the title: If this seems like the start of a document, extract its title. If it's a middle chunk, derive a descriptive title.\n    For the summary: Create a concise summary of the main points in this chunk.\n    Keep both title and summary concise but informative.\"\"\"\n    \n    try:\n        response = await llm_client.chat.completions.create(\n            model=get_env_var(\"PRIMARY_MODEL\") or \"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": f\"URL: {url}\\n\\nContent:\\n{chunk[:1000]}...\"}  # Send first 1000 chars for context\n            ],\n            response_format={ \"type\": \"json_object\" }\n        )\n        return json.loads(response.choices[0].message.content)\n    except Exception as e:\n        print(f\"Error getting title and summary: {e}\")\n        return {\"title\": \"Error processing title\", \"summary\": \"Error processing summary\"}\n\nasync def get_embedding(text: str) -> List[float]:\n    \"\"\"Get embedding vector from OpenAI.\"\"\"\n    try:\n        response = await embedding_client.embeddings.create(\n            model=embedding_model,\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Error getting embedding: {e}\")\n        return [0] * 1536  # Return zero vector on error\n\nasync def process_chunk(chunk: str, chunk_number: int, url: str) -> ProcessedChunk:\n    \"\"\"Process a single chunk of text.\"\"\"\n    # Get title and summary\n    extracted = await get_title_and_summary(chunk, url)\n    \n    # Get embedding\n    embedding = await get_embedding(chunk)\n    \n    # Create metadata\n    metadata = {\n        \"source\": \"pydantic_ai_docs\",\n        \"chunk_size\": len(chunk),\n        \"crawled_at\": datetime.now(timezone.utc).isoformat(),\n        \"url_path\": urlparse(url).path\n    }\n    \n    return ProcessedChunk(\n        url=url,\n        chunk_number=chunk_number,\n        title=extracted['title'],\n        summary=extracted['summary'],\n        content=chunk,  # Store the original chunk content\n        metadata=metadata,\n        embedding=embedding\n    )\n\nasync def insert_chunk(chunk: ProcessedChunk):\n    \"\"\"Insert a processed chunk into Supabase.\"\"\"\n    try:\n        data = {\n            \"url\": chunk.url,\n            \"chunk_number\": chunk.chunk_number,\n            \"title\": chunk.title,\n            \"summary\": chunk.summary,\n            \"content\": chunk.content,\n            \"metadata\": chunk.metadata,\n            \"embedding\": chunk.embedding\n        }\n        \n        result = supabase.table(\"site_pages\").insert(data).execute()\n        print(f\"Inserted chunk {chunk.chunk_number} for {chunk.url}\")\n        return result\n    except Exception as e:\n        print(f\"Error inserting chunk: {e}\")\n        return None\n\nasync def process_and_store_document(url: str, markdown: str, tracker: Optional[CrawlProgressTracker] = None):\n    \"\"\"Process a document and store its chunks in parallel.\"\"\"\n    # Split into chunks\n    chunks = chunk_text(markdown)\n    \n    if tracker:\n        tracker.log(f\"Split document into {len(chunks)} chunks for {url}\")\n        # Ensure UI gets updated\n        if tracker.progress_callback:\n            tracker.progress_callback(tracker.get_status())\n    else:\n        print(f\"Split document into {len(chunks)} chunks for {url}\")\n    \n    # Process chunks in parallel\n    tasks = [\n        process_chunk(chunk, i, url) \n        for i, chunk in enumerate(chunks)\n    ]\n    processed_chunks = await asyncio.gather(*tasks)\n    \n    if tracker:\n        tracker.log(f\"Processed {len(processed_chunks)} chunks for {url}\")\n        # Ensure UI gets updated\n        if tracker.progress_callback:\n            tracker.progress_callback(tracker.get_status())\n    else:\n        print(f\"Processed {len(processed_chunks)} chunks for {url}\")\n    \n    # Store chunks in parallel\n    insert_tasks = [\n        insert_chunk(chunk) \n        for chunk in processed_chunks\n    ]\n    await asyncio.gather(*insert_tasks)\n    \n    if tracker:\n        tracker.chunks_stored += len(processed_chunks)\n        tracker.log(f\"Stored {len(processed_chunks)} chunks for {url}\")\n        # Ensure UI gets updated\n        if tracker.progress_callback:\n            tracker.progress_callback(tracker.get_status())\n    else:\n        print(f\"Stored {len(processed_chunks)} chunks for {url}\")\n\ndef fetch_url_content(url: str) -> str:\n    \"\"\"Fetch content from a URL using requests and convert to markdown.\"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n    }\n    \n    try:\n        response = requests.get(url, headers=headers, timeout=30)\n        response.raise_for_status()\n        \n        # Convert HTML to Markdown\n        markdown = html_converter.handle(response.text)\n        \n        # Clean up the markdown\n        markdown = re.sub(r'\\n{3,}', '\\n\\n', markdown)  # Remove excessive newlines\n        \n        return markdown\n    except Exception as e:\n        raise Exception(f\"Error fetching {url}: {str(e)}\")\n\nasync def crawl_parallel_with_requests(urls: List[str], tracker: Optional[CrawlProgressTracker] = None, max_concurrent: int = 5):\n    \"\"\"Crawl multiple URLs in parallel with a concurrency limit using direct HTTP requests.\"\"\"\n    # Create a semaphore to limit concurrency\n    semaphore = asyncio.Semaphore(max_concurrent)\n    \n    async def process_url(url: str):\n        async with semaphore:\n            if tracker:\n                tracker.log(f\"Crawling: {url}\")\n                # Ensure UI gets updated\n                if tracker.progress_callback:\n                    tracker.progress_callback(tracker.get_status())\n            else:\n                print(f\"Crawling: {url}\")\n            \n            try:\n                # Use a thread pool to run the blocking HTTP request\n                loop = asyncio.get_running_loop()\n                if tracker:\n                    tracker.log(f\"Fetching content from: {url}\")\n                else:\n                    print(f\"Fetching content from: {url}\")\n                markdown = await loop.run_in_executor(None, fetch_url_content, url)\n                \n                if markdown:\n                    if tracker:\n                        tracker.urls_succeeded += 1\n                        tracker.log(f\"Successfully crawled: {url}\")\n                        # Ensure UI gets updated\n                        if tracker.progress_callback:\n                            tracker.progress_callback(tracker.get_status())\n                    else:\n                        print(f\"Successfully crawled: {url}\")\n                    \n                    await process_and_store_document(url, markdown, tracker)\n                else:\n                    if tracker:\n                        tracker.urls_failed += 1\n                        tracker.log(f\"Failed: {url} - No content retrieved\")\n                        # Ensure UI gets updated\n                        if tracker.progress_callback:\n                            tracker.progress_callback(tracker.get_status())\n                    else:\n                        print(f\"Failed: {url} - No content retrieved\")\n            except Exception as e:\n                if tracker:\n                    tracker.urls_failed += 1\n                    tracker.log(f\"Error processing {url}: {str(e)}\")\n                    # Ensure UI gets updated\n                    if tracker.progress_callback:\n                        tracker.progress_callback(tracker.get_status())\n                else:\n                    print(f\"Error processing {url}: {str(e)}\")\n            finally:\n                if tracker:\n                    tracker.urls_processed += 1\n                    # Ensure UI gets updated\n                    if tracker.progress_callback:\n                        tracker.progress_callback(tracker.get_status())\n\n        time.sleep(2)\n    \n    # Process all URLs in parallel with limited concurrency\n    if tracker:\n        tracker.log(f\"Processing {len(urls)} URLs with concurrency {max_concurrent}\")\n        # Ensure UI gets updated\n        if tracker.progress_callback:\n            tracker.progress_callback(tracker.get_status())\n    else:\n        print(f\"Processing {len(urls)} URLs with concurrency {max_concurrent}\")\n    await asyncio.gather(*[process_url(url) for url in urls])\n\ndef get_pydantic_ai_docs_urls() -> List[str]:\n    \"\"\"Get URLs from Pydantic AI docs sitemap.\"\"\"\n    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\"\n    try:\n        response = requests.get(sitemap_url)\n        response.raise_for_status()\n        \n        # Parse the XML\n        root = ElementTree.fromstring(response.content)\n        \n        # Extract all URLs from the sitemap\n        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n        \n        return urls\n    except Exception as e:\n        print(f\"Error fetching sitemap: {e}\")\n        return []\n\ndef clear_existing_records():\n    \"\"\"Clear all existing records with source='pydantic_ai_docs' from the site_pages table.\"\"\"\n    try:\n        result = supabase.table(\"site_pages\").delete().eq(\"metadata->>source\", \"pydantic_ai_docs\").execute()\n        print(\"Cleared existing pydantic_ai_docs records from site_pages\")\n        return result\n    except Exception as e:\n        print(f\"Error clearing existing records: {e}\")\n        return None\n\nasync def main_with_requests(tracker: Optional[CrawlProgressTracker] = None):\n    \"\"\"Main function using direct HTTP requests instead of browser automation.\"\"\"\n    try:\n        # Start tracking if tracker is provided\n        if tracker:\n            tracker.start()\n        else:\n            print(\"Starting crawling process...\")\n        \n        # Clear existing records first\n        if tracker:\n            tracker.log(\"Clearing existing Pydantic AI docs records...\")\n        else:\n            print(\"Clearing existing Pydantic AI docs records...\")\n        clear_existing_records()\n        if tracker:\n            tracker.log(\"Existing records cleared\")\n        else:\n            print(\"Existing records cleared\")\n        \n        # Get URLs from Pydantic AI docs\n        if tracker:\n            tracker.log(\"Fetching URLs from Pydantic AI sitemap...\")\n        else:\n            print(\"Fetching URLs from Pydantic AI sitemap...\")\n        urls = get_pydantic_ai_docs_urls()\n        \n        if not urls:\n            if tracker:\n                tracker.log(\"No URLs found to crawl\")\n                tracker.complete()\n            else:\n                print(\"No URLs found to crawl\")\n            return\n        \n        if tracker:\n            tracker.urls_found = len(urls)\n            tracker.log(f\"Found {len(urls)} URLs to crawl\")\n        else:\n            print(f\"Found {len(urls)} URLs to crawl\")\n        \n        # Crawl the URLs using direct HTTP requests\n        await crawl_parallel_with_requests(urls, tracker)\n        \n        # Mark as complete if tracker is provided\n        if tracker:\n            tracker.complete()\n        else:\n            print(\"Crawling process completed\")\n            \n    except Exception as e:\n        if tracker:\n            tracker.log(f\"Error in crawling process: {str(e)}\")\n            tracker.complete()\n        else:\n            print(f\"Error in crawling process: {str(e)}\")\n\ndef start_crawl_with_requests(progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None) -> CrawlProgressTracker:\n    \"\"\"Start the crawling process using direct HTTP requests in a separate thread and return the tracker.\"\"\"\n    tracker = CrawlProgressTracker(progress_callback)\n    \n    def run_crawl():\n        try:\n            asyncio.run(main_with_requests(tracker))\n        except Exception as e:\n            print(f\"Error in crawl thread: {e}\")\n            tracker.log(f\"Thread error: {str(e)}\")\n            tracker.complete()\n    \n    # Start the crawling process in a separate thread\n    thread = threading.Thread(target=run_crawl)\n    thread.daemon = True\n    thread.start()\n    \n    return tracker\n\nif __name__ == \"__main__\":    \n    # Run the main function directly\n    print(\"Starting crawler...\")\n    asyncio.run(main_with_requests())\n    print(\"Crawler finished.\")\n"}
{"type": "source_file", "path": "iterations/v5-parallel-specialized-agents/graph_service.py", "content": "from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Optional, Dict, Any\nfrom archon.archon_graph import agentic_flow\nfrom langgraph.types import Command\nfrom utils.utils import write_to_log\n    \napp = FastAPI()\n\nclass InvokeRequest(BaseModel):\n    message: str\n    thread_id: str\n    is_first_message: bool = False\n    config: Optional[Dict[str, Any]] = None\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\"status\": \"ok\"}    \n\n@app.post(\"/invoke\")\nasync def invoke_agent(request: InvokeRequest):\n    \"\"\"Process a message through the agentic flow and return the complete response.\n\n    The agent streams the response but this API endpoint waits for the full output\n    before returning so it's a synchronous operation for MCP.\n    Another endpoint will be made later to fully stream the response from the API.\n    \n    Args:\n        request: The InvokeRequest containing message and thread info\n        \n    Returns:\n        dict: Contains the complete response from the agent\n    \"\"\"\n    try:\n        config = request.config or {\n            \"configurable\": {\n                \"thread_id\": request.thread_id\n            }\n        }\n\n        response = \"\"\n        if request.is_first_message:\n            write_to_log(f\"Processing first message for thread {request.thread_id}\")\n            async for msg in agentic_flow.astream(\n                {\"latest_user_message\": request.message}, \n                config,\n                stream_mode=\"custom\"\n            ):\n                response += str(msg)\n        else:\n            write_to_log(f\"Processing continuation for thread {request.thread_id}\")\n            async for msg in agentic_flow.astream(\n                Command(resume=request.message),\n                config,\n                stream_mode=\"custom\"\n            ):\n                response += str(msg)\n\n        write_to_log(f\"Final response for thread {request.thread_id}: {response}\")\n        return {\"response\": response}\n        \n    except Exception as e:\n        print(f\"Exception invoking Archon for thread {request.thread_id}: {str(e)}\")\n        write_to_log(f\"Error processing message for thread {request.thread_id}: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8100)\n"}
{"type": "source_file", "path": "iterations/v5-parallel-specialized-agents/archon/refiner_agents/tools_refiner_agent.py", "content": "from __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom dotenv import load_dotenv\nimport logfire\nimport asyncio\nimport httpx\nimport os\nimport sys\nimport json\nfrom typing import List\nfrom pydantic import BaseModel\nfrom pydantic_ai import Agent, ModelRetry, RunContext\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom openai import AsyncOpenAI\nfrom supabase import Client\n\n# Add the parent directory to sys.path to allow importing from the parent directory\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\nfrom utils.utils import get_env_var\nfrom archon.agent_prompts import tools_refiner_prompt\nfrom archon.agent_tools import (\n    retrieve_relevant_documentation_tool,\n    list_documentation_pages_tool,\n    get_page_content_tool\n)\n\nload_dotenv()\n\nprovider = get_env_var('LLM_PROVIDER') or 'OpenAI'\nllm = get_env_var('PRIMARY_MODEL') or 'gpt-4o-mini'\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\n\nmodel = AnthropicModel(llm, api_key=api_key) if provider == \"Anthropic\" else OpenAIModel(llm, base_url=base_url, api_key=api_key)\nembedding_model = get_env_var('EMBEDDING_MODEL') or 'text-embedding-3-small'\n\nlogfire.configure(send_to_logfire='if-token-present')\n\n@dataclass\nclass ToolsRefinerDeps:\n    supabase: Client\n    embedding_client: AsyncOpenAI\n\ntools_refiner_agent = Agent(\n    model,\n    system_prompt=tools_refiner_prompt,\n    deps_type=ToolsRefinerDeps,\n    retries=2\n)\n\n@tools_refiner_agent.tool\nasync def retrieve_relevant_documentation(ctx: RunContext[ToolsRefinerDeps], query: str) -> str:\n    \"\"\"\n    Retrieve relevant documentation chunks based on the query with RAG.\n    Make sure your searches always focus on implementing tools.\n    \n    Args:\n        ctx: The context including the Supabase client and OpenAI client\n        query: Your query to retrieve relevant documentation for implementing tools\n        \n    Returns:\n        A formatted string containing the top 4 most relevant documentation chunks\n    \"\"\"\n    return await retrieve_relevant_documentation_tool(ctx.deps.supabase, ctx.deps.embedding_client, query)\n\n@tools_refiner_agent.tool\nasync def list_documentation_pages(ctx: RunContext[ToolsRefinerDeps]) -> List[str]:\n    \"\"\"\n    Retrieve a list of all available Pydantic AI documentation pages.\n    This will give you all pages available, but focus on the ones related to tools.\n    \n    Returns:\n        List[str]: List of unique URLs for all documentation pages\n    \"\"\"\n    return await list_documentation_pages_tool(ctx.deps.supabase)\n\n@tools_refiner_agent.tool\nasync def get_page_content(ctx: RunContext[ToolsRefinerDeps], url: str) -> str:\n    \"\"\"\n    Retrieve the full content of a specific documentation page by combining all its chunks.\n    Only use this tool to get pages related to using tools with Pydantic AI.\n    \n    Args:\n        ctx: The context including the Supabase client\n        url: The URL of the page to retrieve\n        \n    Returns:\n        str: The complete page content with all chunks combined in order\n    \"\"\"\n    return await get_page_content_tool(ctx.deps.supabase, url)"}
{"type": "source_file", "path": "iterations/v5-parallel-specialized-agents/mcp/mcp_server.py", "content": "from mcp.server.fastmcp import FastMCP\nfrom datetime import datetime\nfrom dotenv import load_dotenv\nfrom typing import Dict, List\nimport threading\nimport requests\nimport asyncio\nimport uuid\nimport sys\nimport os\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Initialize FastMCP server with ERROR logging level\nmcp = FastMCP(\"archon\", log_level=\"ERROR\")\n\n# Store active threads\nactive_threads: Dict[str, List[str]] = {}\n\n# FastAPI service URL\nGRAPH_SERVICE_URL = os.getenv(\"GRAPH_SERVICE_URL\", \"http://localhost:8100\")\n\ndef write_to_log(message: str):\n    \"\"\"Write a message to the logs.txt file in the workbench directory.\n    \n    Args:\n        message: The message to log\n    \"\"\"\n    # Get the directory one level up from the current file\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    parent_dir = os.path.dirname(current_dir)\n    workbench_dir = os.path.join(parent_dir, \"workbench\")\n    log_path = os.path.join(workbench_dir, \"logs.txt\")\n    os.makedirs(workbench_dir, exist_ok=True)\n\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    log_entry = f\"[{timestamp}] {message}\\n\"\n\n    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n        f.write(log_entry)\n\n@mcp.tool()\nasync def create_thread() -> str:\n    \"\"\"Create a new conversation thread for Archon.\n    Always call this tool before invoking Archon for the first time in a conversation.\n    (if you don't already have a thread ID)\n    \n    Returns:\n        str: A unique thread ID for the conversation\n    \"\"\"\n    thread_id = str(uuid.uuid4())\n    active_threads[thread_id] = []\n    write_to_log(f\"Created new thread: {thread_id}\")\n    return thread_id\n\n\ndef _make_request(thread_id: str, user_input: str, config: dict) -> str:\n    \"\"\"Make synchronous request to graph service\"\"\"\n    try:\n        response = requests.post(\n            f\"{GRAPH_SERVICE_URL}/invoke\",\n            json={\n                \"message\": user_input,\n                \"thread_id\": thread_id,\n                \"is_first_message\": not active_threads[thread_id],\n                \"config\": config\n            },\n            timeout=300  # 5 minute timeout for long-running operations\n        )\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.Timeout:\n        write_to_log(f\"Request timed out for thread {thread_id}\")\n        raise TimeoutError(\"Request to graph service timed out. The operation took longer than expected.\")\n    except requests.exceptions.RequestException as e:\n        write_to_log(f\"Request failed for thread {thread_id}: {str(e)}\")\n        raise\n\n\n@mcp.tool()\nasync def run_agent(thread_id: str, user_input: str) -> str:\n    \"\"\"Run the Archon agent with user input.\n    Only use this tool after you have called create_thread in this conversation to get a unique thread ID.\n    If you already created a thread ID in this conversation, do not create another one. Reuse the same ID.\n    After you receive the code from Archon, always implement it into the codebase unless asked not to.\n\n    After using this tool and implementing the code it gave back, ask the user if they want you to refine the agent\n    autonomously (they can just say 'refine') or they can just give feedback and you'll improve the agent that way.\n\n    If they want to refine the agent, just give 'refine' for user_input.\n    \n    Args:\n        thread_id: The conversation thread ID\n        user_input: The user's message to process\n    \n    Returns:\n        str: The agent's response which generally includes the code for the agent\n    \"\"\"\n    if thread_id not in active_threads:\n        write_to_log(f\"Error: Thread not found - {thread_id}\")\n        raise ValueError(\"Thread not found\")\n\n    write_to_log(f\"Processing message for thread {thread_id}: {user_input}\")\n\n    config = {\n        \"configurable\": {\n            \"thread_id\": thread_id\n        }\n    }\n    \n    try:\n        result = await asyncio.to_thread(_make_request, thread_id, user_input, config)\n        active_threads[thread_id].append(user_input)\n        return result['response']\n        \n    except Exception as e:\n        raise\n\n\nif __name__ == \"__main__\":\n    write_to_log(\"Starting MCP server\")\n    \n    # Run MCP server\n    mcp.run(transport='stdio')\n\n"}
{"type": "source_file", "path": "iterations/v5-parallel-specialized-agents/streamlit_pages/__init__.py", "content": "# This file makes the streamlit_ui directory a Python package\n"}
{"type": "source_file", "path": "iterations/v5-parallel-specialized-agents/archon/refiner_agents/prompt_refiner_agent.py", "content": "from __future__ import annotations as _annotations\n\nimport logfire\nimport os\nimport sys\nfrom pydantic_ai import Agent\nfrom dotenv import load_dotenv\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.openai import OpenAIModel\nfrom supabase import Client\n\n# Add the parent directory to sys.path to allow importing from the parent directory\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\nfrom utils.utils import get_env_var\nfrom archon.agent_prompts import prompt_refiner_prompt\n\nload_dotenv()\n\nprovider = get_env_var('LLM_PROVIDER') or 'OpenAI'\nllm = get_env_var('PRIMARY_MODEL') or 'gpt-4o-mini'\nbase_url = get_env_var('BASE_URL') or 'https://api.openai.com/v1'\napi_key = get_env_var('LLM_API_KEY') or 'no-llm-api-key-provided'\n\nmodel = AnthropicModel(llm, api_key=api_key) if provider == \"Anthropic\" else OpenAIModel(llm, base_url=base_url, api_key=api_key)\n\nlogfire.configure(send_to_logfire='if-token-present')\n\nprompt_refiner_agent = Agent(\n    model,\n    system_prompt=prompt_refiner_prompt\n)"}
{"type": "source_file", "path": "iterations/v2-agentic-workflow/crawl_pydantic_ai_docs.py", "content": "import os\nimport sys\nimport json\nimport asyncio\nimport requests\nfrom xml.etree import ElementTree\nfrom typing import List, Dict, Any\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\nfrom urllib.parse import urlparse\nfrom dotenv import load_dotenv\n\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom openai import AsyncOpenAI\nfrom supabase import create_client, Client\n\nload_dotenv()\n\n# Initialize OpenAI and Supabase clients\n\nbase_url = os.getenv('BASE_URL', 'https://api.openai.com/v1')\napi_key = os.getenv('LLM_API_KEY', 'no-llm-api-key-provided')\nis_ollama = \"localhost\" in base_url.lower()\n\nembedding_model = os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')\n\nopenai_client=None\n\nif is_ollama:\n    openai_client = AsyncOpenAI(base_url=base_url,api_key=api_key)\nelse:\n    openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nsupabase: Client = create_client(\n    os.getenv(\"SUPABASE_URL\"),\n    os.getenv(\"SUPABASE_SERVICE_KEY\")\n)\n\n@dataclass\nclass ProcessedChunk:\n    url: str\n    chunk_number: int\n    title: str\n    summary: str\n    content: str\n    metadata: Dict[str, Any]\n    embedding: List[float]\n\ndef chunk_text(text: str, chunk_size: int = 5000) -> List[str]:\n    \"\"\"Split text into chunks, respecting code blocks and paragraphs.\"\"\"\n    chunks = []\n    start = 0\n    text_length = len(text)\n\n    while start < text_length:\n        # Calculate end position\n        end = start + chunk_size\n\n        # If we're at the end of the text, just take what's left\n        if end >= text_length:\n            chunks.append(text[start:].strip())\n            break\n\n        # Try to find a code block boundary first (```)\n        chunk = text[start:end]\n        code_block = chunk.rfind('```')\n        if code_block != -1 and code_block > chunk_size * 0.3:\n            end = start + code_block\n\n        # If no code block, try to break at a paragraph\n        elif '\\n\\n' in chunk:\n            # Find the last paragraph break\n            last_break = chunk.rfind('\\n\\n')\n            if last_break > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size\n                end = start + last_break\n\n        # If no paragraph break, try to break at a sentence\n        elif '. ' in chunk:\n            # Find the last sentence break\n            last_period = chunk.rfind('. ')\n            if last_period > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size\n                end = start + last_period + 1\n\n        # Extract chunk and clean it up\n        chunk = text[start:end].strip()\n        if chunk:\n            chunks.append(chunk)\n\n        # Move start position for next chunk\n        start = max(start + 1, end)\n\n    return chunks\n\nasync def get_title_and_summary(chunk: str, url: str) -> Dict[str, str]:\n    \"\"\"Extract title and summary using GPT-4.\"\"\"\n    system_prompt = \"\"\"You are an AI that extracts titles and summaries from documentation chunks.\n    Return a JSON object with 'title' and 'summary' keys.\n    For the title: If this seems like the start of a document, extract its title. If it's a middle chunk, derive a descriptive title.\n    For the summary: Create a concise summary of the main points in this chunk.\n    Keep both title and summary concise but informative.\"\"\"\n    \n    try:\n        response = await openai_client.chat.completions.create(\n            model=os.getenv(\"PRIMARY_MODEL\", \"gpt-4o-mini\"),\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": f\"URL: {url}\\n\\nContent:\\n{chunk[:1000]}...\"}  # Send first 1000 chars for context\n            ],\n            response_format={ \"type\": \"json_object\" }\n        )\n        return json.loads(response.choices[0].message.content)\n    except Exception as e:\n        print(f\"Error getting title and summary: {e}\")\n        return {\"title\": \"Error processing title\", \"summary\": \"Error processing summary\"}\n\nasync def get_embedding(text: str) -> List[float]:\n    \"\"\"Get embedding vector from OpenAI.\"\"\"\n    try:\n        response = await openai_client.embeddings.create(\n            model= embedding_model,\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Error getting embedding: {e}\")\n        return [0] * 1536  # Return zero vector on error\n\nasync def process_chunk(chunk: str, chunk_number: int, url: str) -> ProcessedChunk:\n    \"\"\"Process a single chunk of text.\"\"\"\n    # Get title and summary\n    extracted = await get_title_and_summary(chunk, url)\n    \n    # Get embedding\n    embedding = await get_embedding(chunk)\n    \n    # Create metadata\n    metadata = {\n        \"source\": \"pydantic_ai_docs\",\n        \"chunk_size\": len(chunk),\n        \"crawled_at\": datetime.now(timezone.utc).isoformat(),\n        \"url_path\": urlparse(url).path\n    }\n    \n    return ProcessedChunk(\n        url=url,\n        chunk_number=chunk_number,\n        title=extracted['title'],\n        summary=extracted['summary'],\n        content=chunk,  # Store the original chunk content\n        metadata=metadata,\n        embedding=embedding\n    )\n\nasync def insert_chunk(chunk: ProcessedChunk):\n    \"\"\"Insert a processed chunk into Supabase.\"\"\"\n    try:\n        data = {\n            \"url\": chunk.url,\n            \"chunk_number\": chunk.chunk_number,\n            \"title\": chunk.title,\n            \"summary\": chunk.summary,\n            \"content\": chunk.content,\n            \"metadata\": chunk.metadata,\n            \"embedding\": chunk.embedding\n        }\n        \n        result = supabase.table(\"site_pages\").insert(data).execute()\n        print(f\"Inserted chunk {chunk.chunk_number} for {chunk.url}\")\n        return result\n    except Exception as e:\n        print(f\"Error inserting chunk: {e}\")\n        return None\n\nasync def process_and_store_document(url: str, markdown: str):\n    \"\"\"Process a document and store its chunks in parallel.\"\"\"\n    # Split into chunks\n    chunks = chunk_text(markdown)\n    \n    # Process chunks in parallel\n    tasks = [\n        process_chunk(chunk, i, url) \n        for i, chunk in enumerate(chunks)\n    ]\n    processed_chunks = await asyncio.gather(*tasks)\n    \n    # Store chunks in parallel\n    insert_tasks = [\n        insert_chunk(chunk) \n        for chunk in processed_chunks\n    ]\n    await asyncio.gather(*insert_tasks)\n\nasync def crawl_parallel(urls: List[str], max_concurrent: int = 5):\n    \"\"\"Crawl multiple URLs in parallel with a concurrency limit.\"\"\"\n    browser_config = BrowserConfig(\n        headless=True,\n        verbose=False,\n        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n    )\n    crawl_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n\n    # Create the crawler instance\n    crawler = AsyncWebCrawler(config=browser_config)\n    await crawler.start()\n\n    try:\n        # Create a semaphore to limit concurrency\n        semaphore = asyncio.Semaphore(max_concurrent)\n        \n        async def process_url(url: str):\n            async with semaphore:\n                result = await crawler.arun(\n                    url=url,\n                    config=crawl_config,\n                    session_id=\"session1\"\n                )\n                if result.success:\n                    print(f\"Successfully crawled: {url}\")\n                    await process_and_store_document(url, result.markdown_v2.raw_markdown)\n                else:\n                    print(f\"Failed: {url} - Error: {result.error_message}\")\n        \n        # Process all URLs in parallel with limited concurrency\n        await asyncio.gather(*[process_url(url) for url in urls])\n    finally:\n        await crawler.close()\n\ndef get_pydantic_ai_docs_urls() -> List[str]:\n    \"\"\"Get URLs from Pydantic AI docs sitemap.\"\"\"\n    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\"\n    try:\n        response = requests.get(sitemap_url)\n        response.raise_for_status()\n        \n        # Parse the XML\n        root = ElementTree.fromstring(response.content)\n        \n        # Extract all URLs from the sitemap\n        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n        \n        return urls\n    except Exception as e:\n        print(f\"Error fetching sitemap: {e}\")\n        return []\n\nasync def main():\n    # Get URLs from Pydantic AI docs\n    urls = get_pydantic_ai_docs_urls()\n    if not urls:\n        print(\"No URLs found to crawl\")\n        return\n    \n    print(f\"Found {len(urls)} URLs to crawl\")\n    await crawl_parallel(urls)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "iterations/v1-single-agent/crawl_pydantic_ai_docs.py", "content": "import os\nimport sys\nimport json\nimport asyncio\nimport requests\nfrom xml.etree import ElementTree\nfrom typing import List, Dict, Any\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\nfrom urllib.parse import urlparse\nfrom dotenv import load_dotenv\n\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom openai import AsyncOpenAI\nfrom supabase import create_client, Client\n\nload_dotenv()\n\n# Initialize OpenAI and Supabase clients\nopenai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nsupabase: Client = create_client(\n    os.getenv(\"SUPABASE_URL\"),\n    os.getenv(\"SUPABASE_SERVICE_KEY\")\n)\n\n@dataclass\nclass ProcessedChunk:\n    url: str\n    chunk_number: int\n    title: str\n    summary: str\n    content: str\n    metadata: Dict[str, Any]\n    embedding: List[float]\n\ndef chunk_text(text: str, chunk_size: int = 5000) -> List[str]:\n    \"\"\"Split text into chunks, respecting code blocks and paragraphs.\"\"\"\n    chunks = []\n    start = 0\n    text_length = len(text)\n\n    while start < text_length:\n        # Calculate end position\n        end = start + chunk_size\n\n        # If we're at the end of the text, just take what's left\n        if end >= text_length:\n            chunks.append(text[start:].strip())\n            break\n\n        # Try to find a code block boundary first (```)\n        chunk = text[start:end]\n        code_block = chunk.rfind('```')\n        if code_block != -1 and code_block > chunk_size * 0.3:\n            end = start + code_block\n\n        # If no code block, try to break at a paragraph\n        elif '\\n\\n' in chunk:\n            # Find the last paragraph break\n            last_break = chunk.rfind('\\n\\n')\n            if last_break > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size\n                end = start + last_break\n\n        # If no paragraph break, try to break at a sentence\n        elif '. ' in chunk:\n            # Find the last sentence break\n            last_period = chunk.rfind('. ')\n            if last_period > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size\n                end = start + last_period + 1\n\n        # Extract chunk and clean it up\n        chunk = text[start:end].strip()\n        if chunk:\n            chunks.append(chunk)\n\n        # Move start position for next chunk\n        start = max(start + 1, end)\n\n    return chunks\n\nasync def get_title_and_summary(chunk: str, url: str) -> Dict[str, str]:\n    \"\"\"Extract title and summary using GPT-4.\"\"\"\n    system_prompt = \"\"\"You are an AI that extracts titles and summaries from documentation chunks.\n    Return a JSON object with 'title' and 'summary' keys.\n    For the title: If this seems like the start of a document, extract its title. If it's a middle chunk, derive a descriptive title.\n    For the summary: Create a concise summary of the main points in this chunk.\n    Keep both title and summary concise but informative.\"\"\"\n    \n    try:\n        response = await openai_client.chat.completions.create(\n            model=os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\"),\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": f\"URL: {url}\\n\\nContent:\\n{chunk[:1000]}...\"}  # Send first 1000 chars for context\n            ],\n            response_format={ \"type\": \"json_object\" }\n        )\n        return json.loads(response.choices[0].message.content)\n    except Exception as e:\n        print(f\"Error getting title and summary: {e}\")\n        return {\"title\": \"Error processing title\", \"summary\": \"Error processing summary\"}\n\nasync def get_embedding(text: str) -> List[float]:\n    \"\"\"Get embedding vector from OpenAI.\"\"\"\n    try:\n        response = await openai_client.embeddings.create(\n            model=\"text-embedding-3-small\",\n            input=text\n        )\n        return response.data[0].embedding\n    except Exception as e:\n        print(f\"Error getting embedding: {e}\")\n        return [0] * 1536  # Return zero vector on error\n\nasync def process_chunk(chunk: str, chunk_number: int, url: str) -> ProcessedChunk:\n    \"\"\"Process a single chunk of text.\"\"\"\n    # Get title and summary\n    extracted = await get_title_and_summary(chunk, url)\n    \n    # Get embedding\n    embedding = await get_embedding(chunk)\n    \n    # Create metadata\n    metadata = {\n        \"source\": \"pydantic_ai_docs\",\n        \"chunk_size\": len(chunk),\n        \"crawled_at\": datetime.now(timezone.utc).isoformat(),\n        \"url_path\": urlparse(url).path\n    }\n    \n    return ProcessedChunk(\n        url=url,\n        chunk_number=chunk_number,\n        title=extracted['title'],\n        summary=extracted['summary'],\n        content=chunk,  # Store the original chunk content\n        metadata=metadata,\n        embedding=embedding\n    )\n\nasync def insert_chunk(chunk: ProcessedChunk):\n    \"\"\"Insert a processed chunk into Supabase.\"\"\"\n    try:\n        data = {\n            \"url\": chunk.url,\n            \"chunk_number\": chunk.chunk_number,\n            \"title\": chunk.title,\n            \"summary\": chunk.summary,\n            \"content\": chunk.content,\n            \"metadata\": chunk.metadata,\n            \"embedding\": chunk.embedding\n        }\n        \n        result = supabase.table(\"site_pages\").insert(data).execute()\n        print(f\"Inserted chunk {chunk.chunk_number} for {chunk.url}\")\n        return result\n    except Exception as e:\n        print(f\"Error inserting chunk: {e}\")\n        return None\n\nasync def process_and_store_document(url: str, markdown: str):\n    \"\"\"Process a document and store its chunks in parallel.\"\"\"\n    # Split into chunks\n    chunks = chunk_text(markdown)\n    \n    # Process chunks in parallel\n    tasks = [\n        process_chunk(chunk, i, url) \n        for i, chunk in enumerate(chunks)\n    ]\n    processed_chunks = await asyncio.gather(*tasks)\n    \n    # Store chunks in parallel\n    insert_tasks = [\n        insert_chunk(chunk) \n        for chunk in processed_chunks\n    ]\n    await asyncio.gather(*insert_tasks)\n\nasync def crawl_parallel(urls: List[str], max_concurrent: int = 5):\n    \"\"\"Crawl multiple URLs in parallel with a concurrency limit.\"\"\"\n    browser_config = BrowserConfig(\n        headless=True,\n        verbose=False,\n        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n    )\n    crawl_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n\n    # Create the crawler instance\n    crawler = AsyncWebCrawler(config=browser_config)\n    await crawler.start()\n\n    try:\n        # Create a semaphore to limit concurrency\n        semaphore = asyncio.Semaphore(max_concurrent)\n        \n        async def process_url(url: str):\n            async with semaphore:\n                result = await crawler.arun(\n                    url=url,\n                    config=crawl_config,\n                    session_id=\"session1\"\n                )\n                if result.success:\n                    print(f\"Successfully crawled: {url}\")\n                    await process_and_store_document(url, result.markdown_v2.raw_markdown)\n                else:\n                    print(f\"Failed: {url} - Error: {result.error_message}\")\n        \n        # Process all URLs in parallel with limited concurrency\n        await asyncio.gather(*[process_url(url) for url in urls])\n    finally:\n        await crawler.close()\n\ndef get_pydantic_ai_docs_urls() -> List[str]:\n    \"\"\"Get URLs from Pydantic AI docs sitemap.\"\"\"\n    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\"\n    try:\n        response = requests.get(sitemap_url)\n        response.raise_for_status()\n        \n        # Parse the XML\n        root = ElementTree.fromstring(response.content)\n        \n        # Extract all URLs from the sitemap\n        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n        \n        return urls\n    except Exception as e:\n        print(f\"Error fetching sitemap: {e}\")\n        return []\n\nasync def main():\n    # Get URLs from Pydantic AI docs\n    urls = get_pydantic_ai_docs_urls()\n    if not urls:\n        print(\"No URLs found to crawl\")\n        return\n    \n    print(f\"Found {len(urls)} URLs to crawl\")\n    await crawl_parallel(urls)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"}
{"type": "source_file", "path": "graph_service.py", "content": "from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Optional, Dict, Any\nfrom archon.archon_graph import agentic_flow\nfrom langgraph.types import Command\nfrom utils.utils import write_to_log\n    \napp = FastAPI()\n\nclass InvokeRequest(BaseModel):\n    message: str\n    thread_id: str\n    is_first_message: bool = False\n    config: Optional[Dict[str, Any]] = None\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint\"\"\"\n    return {\"status\": \"ok\"}    \n\n@app.post(\"/invoke\")\nasync def invoke_agent(request: InvokeRequest):\n    \"\"\"Process a message through the agentic flow and return the complete response.\n\n    The agent streams the response but this API endpoint waits for the full output\n    before returning so it's a synchronous operation for MCP.\n    Another endpoint will be made later to fully stream the response from the API.\n    \n    Args:\n        request: The InvokeRequest containing message and thread info\n        \n    Returns:\n        dict: Contains the complete response from the agent\n    \"\"\"\n    try:\n        config = request.config or {\n            \"configurable\": {\n                \"thread_id\": request.thread_id\n            }\n        }\n\n        response = \"\"\n        if request.is_first_message:\n            write_to_log(f\"Processing first message for thread {request.thread_id}\")\n            async for msg in agentic_flow.astream(\n                {\"latest_user_message\": request.message}, \n                config,\n                stream_mode=\"custom\"\n            ):\n                response += str(msg)\n        else:\n            write_to_log(f\"Processing continuation for thread {request.thread_id}\")\n            async for msg in agentic_flow.astream(\n                Command(resume=request.message),\n                config,\n                stream_mode=\"custom\"\n            ):\n                response += str(msg)\n\n        write_to_log(f\"Final response for thread {request.thread_id}: {response}\")\n        return {\"response\": response}\n        \n    except Exception as e:\n        print(f\"Exception invoking Archon for thread {request.thread_id}: {str(e)}\")\n        write_to_log(f\"Error processing message for thread {request.thread_id}: {str(e)}\")\n        raise HTTPException(status_code=500, detail=str(e))\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8100)\n"}
