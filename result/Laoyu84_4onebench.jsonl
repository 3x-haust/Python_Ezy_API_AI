{"repo_info": {"repo_name": "4onebench", "repo_owner": "Laoyu84", "repo_url": "https://github.com/Laoyu84/4onebench"}}
{"type": "source_file", "path": "app/app.py", "content": "import os\nimport streamlit as st\nimport pandas as pd\nfrom controller.eval_controller import EvalController\nfrom controller.schema_controller import SchemaController\nfrom controller.few_shots_controller import FewShotsController\nfrom controller.api_router_controller import APIRouterController\nfrom llms.general import verify_api_key\nfrom util import constant\nfrom util.logger import logger\n\n# functions\ndef load_css(css_file):\n    \"\"\"Load and inject CSS from a file.\"\"\"\n    with open(css_file) as f:\n        st.html(f'<style>{f.read()}</style>')\n\n# Error message\n@st.dialog(\"提示\")\ndef show_msg(msg):\n    if isinstance(msg, Exception):\n        # 提示error messages\n        st.write(msg.__repr__())\n    else:\n        st.write(msg)\n\n## Loading CSS\ncss_path = os.path.join(os.path.dirname(__file__), constant.css_dir, 'styles.css')\nload_css(css_path)\n\n## loading session data\nschema_controller = SchemaController()\nif \"schemas\" not in st.session_state:\n    st.session_state.schemas = schema_controller.load_all_schemas()\n\napi_route_controller = APIRouterController()\nif \"routes\" not in st.session_state:\n    st.session_state.routes =  api_route_controller.load_api_routes()\n\nif \"eval_controller\" not in st.session_state:\n    st.session_state.eval_controller = EvalController()\n\nfew_shot_controller = FewShotsController()\nif \"few_shots\" not in st.session_state:\n        st.session_state.few_shots = few_shot_controller.load_few_shots()\n\n\n## UI\nwith st.sidebar:\n    st.session_state.model = st.selectbox(\n        \"生成模型：\", \n        ('glm-4-plus','glm-4-0520', 'glm-4-flash','glm-4-air','gpt-4o','gpt-4o-mini','qwen-max', 'qwen-plus', 'doubao-pro-32k', 'deepseek-chat')\n    )\n    if st.session_state.model:\n        os.environ['MODEL'] = st.session_state.model\n\n    API_KEY = st.text_input(\"模型API_Key:\", placeholder=\"API_KEY\")\n    if st.session_state.model in constant.DOUBAO_FAMILY:\n        END_POINT = st.text_input(\"模型接入点\", placeholder=\"接入点名称\")\n        \n    num_of_record = st.number_input(\"数据条数:\", value=1, max_value=len(st.session_state.few_shots), min_value=1)\n    st.write(f\"我们使用：\\n1. **{constant.OPENAI_VERIFIER}** 验证 OpenAI 系列模型； \\n2. **{constant.GLM_VERIFIER}** 验证 GLM 系列模型； \\n3. **{constant.QWEN_VERIFIER}** 验证 QWEN 系列模型； \\n4. **{constant.DOUBAO_VERIFIER}** 验证 DOUBAO 系列模型。\\n5. **{constant.DEEPSEEK_VERIFIER}** 验证 DEEPSEEK 系列模型。\")\n\n# Setup tabs    \ntab_eval,tab_questionnir = st.tabs([\"评估\", \"问题列表\"])\nwith tab_eval:\n    if st.button(\"开始测评!\", type=\"primary\"):\n        try:\n            #setup API_KEY and END_POINT for Doubao\n            if len(API_KEY) == 0 or ('END_POINT' in locals() and len(END_POINT) == 0):\n                msg = \"\"\n                if len(API_KEY) == 0: msg += \"请提供API Key。\"\n                if 'END_POINT' in locals() and len(END_POINT) == 0: msg += \"请提供模型接入点\"\n                show_msg(msg)\n            else:\n                os.environ['API_KEY'] = API_KEY\n                if 'END_POINT' in locals() and len(END_POINT) != 0:\n                    os.environ['END_POINT'] = END_POINT\n                #try a hello message to verify api key\n                verify_api_key()\n        \n                evals = []\n                #Proceeding \n                with st.status(f\"需要处理{len(st.session_state.few_shots)}条数据...\", expanded=False) as status:\n                    for i, fs in enumerate(st.session_state.few_shots):\n                        if i + 1 > num_of_record: break\n                        status.update(label = f\"处理{i+1}/{num_of_record}条数据...\", state = \"running\", expanded = False)\n                        e = st.session_state.eval_controller.scoring_by_few_shot(fs, st.session_state.schemas, st.session_state.routes, st.session_state.few_shots)\n                        evals.append(e)\n                    status.update(label=\"数据处理完成\", state=\"complete\", expanded=False)\n                    logger.info(\"问题处理完毕。\")\n                if evals and len(evals) > 0:\n                    st.title(f\"{st.session_state['model']}测评结果:\")\n                    total, corrected, percent = st.session_state.eval_controller.metrics(evals)\n                    \n                    metric_total = \"测评总数:\"\n                    metric_corrected = \"正确条数:\"\n                    metric_percent = \"正确率:\"\n                    col1, col2, col3 = st.columns(3)\n                    col1.metric(label=metric_total, value=total, delta=None)\n                    col2.metric(label=metric_corrected, value=corrected, delta=None)\n                    col3.metric(label=metric_percent, value=percent, delta=None)\n\n                    df = pd.DataFrame(evals)\n                    new_column_order = ['id', 'result', 'question', 's_answer', 'r_answer', 'reason']\n                    df_sorted = df[new_column_order]\n                    st.dataframe(df_sorted, column_config={\n                        \"id\": \"id\",\n                        \"result\": \"正确\",\n                        \"question\": \"问题\",\n                        \"s_answer\": \"正确答案\",\n                        \"r_answer\": \"大模型答案\",\n                        \"reason\": \"判定\" \n                    }, hide_index=True)\n        except Exception as ex:\n            show_msg(ex) \n                \nwith tab_questionnir:\n    for i in range(0, len(st.session_state.few_shots), constant.COL_PERROW):\n        cols = st.columns(constant.COL_PERROW)\n        for j in range(constant.COL_PERROW):\n            if i + j < len(st.session_state.few_shots):\n                with cols[j]:\n                    st.html(f\"<div class='query-container'><div class='query-id'>ID: {st.session_state.few_shots[i+j].id}</div><div class='query-question'>{st.session_state.few_shots[i+j].question}</div></div>\")\n    \n    \n    \n                \n"}
{"type": "source_file", "path": "app/controller/plan_verifier.py", "content": "\nimport os\nfrom tenacity import retry, stop_after_attempt,wait_fixed\nfrom models.query import Query\nfrom llms.general import complete_verify\nfrom util import constant\nfrom util.logger import logger\n\nPLAN_SYS = \"你善于验证答案，用Json输出验证结果\"\n\nPLAN_USER = \"\"\"\n请基于Knowledge Graph,'我的问题'，验证'我的计划'是否符合'标准答案',并给出你的Reason。\n\n## CONSTRAIN\n-请严格遵循Knowledge Graph所规划的关系\n-请使用中文\n\n## 我的问题\n{question}\n\n## Knowledge Graph\n{schema}\n{routes}\n\n## Example\n1.\n- Question:\"金都药业的子公司的一级行业是什么?\"\n- Standard_Answer:\nSOS;\na.金都药业的子公司行业一级\n[\n    1.公司简称:金都药业->上市公司基本信息;\n    2.关联上市公司全称:上市公司基本信息.公司名称->子公司关联信息;\n    3.公司名称:子公司关联信息.公司名称->公司工商照面信息;\n    4.行业一级:公司工商照面信息.行业一级;\n];\nEOS;\n- My_Answer:\nSOS;\na.金都药业的子公司及行业信息\n[\n    1.公司简称:金都药业->上市公司基本信息;\n    2.公司名称:上市公司基本信息.公司名称->子公司关联信息;\n    3.公司名称:子公司关联信息.公司名称->公司工商照面信息;\n    4.行业一级:公司工商照面信息.行业一级;\n];\nEOS;\n- Evaluation:\n{{\"pass\":false,\"reason\": \"在我的计划中，步骤2使用了公司名称:上市公司基本信息.公司名称->子公司关联信息，而标准答案中使用的是关联上市公司全称:上市公司基本信息.公司名称->子公司关联信息。两者在查询子公司关联信息时使用的属性不同，因此不符合标准答案。\"}}\n\n2.\n- Question: 审理(2019)川0129民初1361号案件的法院名称是哪个法院，地址在什么地方\n- Standard_Answer:\nSOS;\na.北京市密云区人民法院的区县区划代码\n[\n    1.法院名称:北京市密云区人民法院->法院基础信息;\n    2.地址:法院基础信息.法院地址->通用地址省市区信息;\n    3.省份:通用地址省市区信息.省份,城市:通用地址省市区信息.城市,区县:通用地址省市区信息.区县->通用地址编码;\n    4.区县区划代码:通用地址编码.区县区划代码;\n];\nEOS;\n- My Answer:\nSOS;\na.北京市密云区人民法院的区县区划代码\n[\n    1.法院名称:北京市密云区人民法院->法院地址代字信息;\n    2.地址:法院地址代字信息.法院地址->通用地址省市区信息;\n    3.区县:通用地址省市区信息.区县->通用地址编码;\n    4.区县区划代码:通用地址编码.区县区划代码;\n];\nEOS;\n- Evaluation:\n{{\"pass\":false,\"reason\": \"1.在我的计划中，使用了法院地址代字信息来获取法院地址，而根据Knowledge Graph，应该使用法院基础信息来获取法院地址。2.查询通用地址编码应同时使用省份,城市,区县三个条件。\"}}\n\n3.\n- Question: 天通股份的参保人数有多少人？\n- Standard_Answer:\nSOS;\na.天通股份的参保人数\n[\n    1.公司简称:天通股份->上市公司基本信息;\n    2.公司名称:上市公司基本信息.公司名称->公司工商照面信息;\n    3.参保人数:公司工商照面信息.参保人数;\n];\nEOS;\n- My Answer:\nSOS;\na.天通股份的工商照面信息\n[\n  1.公司名称:天通股份->公司工商照面信息;\n  2.参保人数:公司工商照面信息.参保人数;\n];\nEOS;\n- Evaluation:\n{{\"pass\":false,\"reason\": \"根据Knowledge Graph，我的计划中直接使用公司名称查询公司工商照面信息的步骤不符合标准答案的流程。标准答案要求先通过公司简称查询上市公司基本信息，然后再通过上市公司基本信息中的公司名称查询公司工商照面信息。我的计划缺少了这一步骤，因此不符合标准答案。\"}}\n\n## 标准答案\n{standardanswer}\n\n## 我的计划\n{myanswer}\n\n## 结果\n{{'pass':True/False,'reason': '...'}}\n\"\"\"\n@retry(stop=stop_after_attempt(constant.MAX_RETRY), wait=wait_fixed(1))\ndef verify(q: Query, schemas, routes, myanswer):\n    schema = \"\\n\".join(str(s) for s in schemas)\n    usr_msg = PLAN_USER.format(question = q.question,\n                            schema = schema,\n                            routes = str(routes),\n                            standardanswer = q.answer,\n                            myanswer = myanswer                        \n                            )\n    return complete_verify(PLAN_SYS, usr_msg)\n    \n\n"}
{"type": "source_file", "path": "app/controller/plan_generator.py", "content": "import os\nfrom tenacity import retry, stop_after_attempt,wait_fixed\nfrom controller.few_shots_controller import FewShotsController\nfrom llms.general import complete\nfrom models.query import Query\nfrom util import constant\nfrom util.logger import logger\n\n\nENRICH_SYS = \"你说话非常精炼。\"\n\nENRICH_USER = \"\"\"\n请基于Knowledge Graph，改写'我的查询'\n\n## CONSTRAIN\n-不要丢失我的查询中任何表名、字段名和值\n-不要做过多解释\n-不要生成SQL\n-严格遵循Knowledge Graph中的给出的规则，不要编造路径\n-请在'我的查询'开始增加SOS表示开始\n-请在'我的查询'结尾增加EOS表示结束\n\n## Knowledge Graph\n{schema}\n{routes}\n\n## Examples\n{examples}\n\n## 我的查询\nQ: {query}\n\"\"\"\n@retry(stop=stop_after_attempt(constant.MAX_RETRY), wait=wait_fixed(1))\ndef enrich_question(q: Query, schemas, routes, few_shots)->str:\n    schema = \"\\n\".join(str(s) for s in schemas)\n\n    #选择合适的few-shot examples\n    few_shots_controller = FewShotsController()\n    selected_fs, related = few_shots_controller.get_question_few_shots(q,few_shots, bench_mark=0.2)\n    logger.debug(f\"问题{q.id}使用的{len(selected_fs)}个Few-Shots案例包括:\")\n    for i, fs in enumerate(selected_fs):\n        logger.debug(f\"问题{fs.id}.{fs.question}，相关度:{related[i]}\")\n    \n    examples = \"\".join(f.get_enriched_examples() for f in selected_fs)\n    usr_msg = ENRICH_USER.format(\n                            query = q.question,\n                            schema = schema,\n                            routes = str(routes),\n                            examples = examples                          \n                            )\n    return complete(ENRICH_SYS,usr_msg)\n    \nPLAN_SYS = \"你说话非常精炼。\"\n\nPLAN_USER = \"\"\"\n请基于Knowledge Graph和Tools，生成'我的查询'的查询路径\n\n## CONSTRAIN\n-不要丢失我的查询中任何表名、字段名和值\n-不要做过多解释\n-不要生成SQL\n-严格遵循Knowledge Graph中的给出的规则，不要编造路径\n-请在'我的查询'开始增加SOS表示开始\n-请在'我的查询'结尾增加EOS表示结束\n\n## Knowledge Graph\n{schema}\n{routes}\n\n## Tools\n1. 总计; 例如，涉案次数:法律文书信息.总计()\n2. 合计; 例如: 涉案总额:法律文书信息.合计(涉案金额)\n3. 排序;\n4. 过滤;\n5. 存在;\n6. IF...ELSE...;\n## Examples\n{examples}\n\n## 查询路径\nQ: {query}\n\"\"\"\n@retry(stop=stop_after_attempt(constant.MAX_RETRY), wait=wait_fixed(1))\ndef build(q: Query, schemas, routes, few_shots):\n    schema = \"\\n\".join(str(s) for s in schemas)\n\n    few_shots_controller = FewShotsController()\n    selected_fs, related = few_shots_controller.get_enriched_few_shots(q,few_shots, bench_mark=0.6)\n    logger.debug(\"该问题使用的Few-Shots案例包括:\")\n    for i, fs in enumerate(selected_fs):\n        logger.debug(f\"问题{fs.id}.{fs.question}，相关度:{related[i]}\")\n    \n    examples = \"\".join(f.get_instructions_examples() for f in selected_fs)\n    usr_msg = PLAN_USER.format(\n                            query = q.enriched,\n                            schema = schema,\n                            routes = str(routes),\n                            examples = examples                          \n                            )\n    return complete(PLAN_SYS, usr_msg)\n   \n"}
{"type": "source_file", "path": "app/controller/few_shots_controller.py", "content": "import os\nimport yaml\nimport json\nfrom pathlib import Path\nfrom scipy import spatial\n\nfrom models.query import Query\nfrom util import constant\n\nclass FewShotsController:\n    def __init__(self):\n        self.config_dir = Path(__file__).parent.parent / 'config'\n\n    def get_question_few_shots(self, \n                    q: Query, \n                    few_shots,\n                    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n                    top_n: int = 10,\n                    bench_mark = 0.40):\n        fs_and_relatednesses = [\n            (s, relatedness_fn(q.question_embeddings, s.question_embeddings))\n            for i, s in enumerate(few_shots) if str(s.id) != str(q.id) and relatedness_fn(q.question_embeddings, s.question_embeddings) >= bench_mark\n            #only return those with high related records.\n        ]\n\n        if not fs_and_relatednesses:\n            return [], []\n        #sort by relatednesses\n        fs_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n        selected_fs, relatednesses = zip(*fs_and_relatednesses)\n        return selected_fs[0:top_n], relatednesses[0:top_n]\n\n    def get_enriched_few_shots(self, \n                    q:Query, \n                    few_shots,\n                    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n                    top_n: int = 10,\n                    bench_mark = 0.40):\n        fs_and_relatednesses = [\n            (s, relatedness_fn(q.enriched_embeddings, s.enriched_embeddings))\n            for i, s in enumerate(few_shots) if str(s.id) != str(q.id) and relatedness_fn(q.enriched_embeddings, s.enriched_embeddings) >= bench_mark\n            #only return those with high related records.\n        ]\n        \n        if not fs_and_relatednesses:\n            return [], []\n        #sort by relatednesses\n        fs_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n        selected_fs, relatednesses = zip(*fs_and_relatednesses)\n        return selected_fs[0:top_n], relatednesses[0:top_n]\n    \n    def load_few_shots(self):\n        few_shots_yaml_path = self.config_dir / constant.few_shot_dir \n        few_shots_folders = os.listdir(few_shots_yaml_path)\n        yaml_files = [file for file in few_shots_folders if file.endswith('.yaml')]\n\n        few_shots = []\n        for yaml_file in yaml_files:\n            # loading examples as a list of Query from few_shots folder \n            file_path = os.path.join(few_shots_yaml_path, yaml_file)\n            with open(file_path, 'r', encoding='utf-8') as file:\n                data = yaml.safe_load(file)\n                few_shots.append(Query(data['question'],\n                                       id=data['id'], \n                                       enriched=data['enriched'], \n                                       answer=data['answer']))\n        \n        for s in few_shots:\n            #loading embeddings from the embedding folder\n            item = self.load_few_shots_embeddings(s.id)\n            if item:\n                s.question_embeddings = item['question_embeddings']\n                s.enriched_embeddings = item['enriched_embeddings']\n        few_shots = sorted(few_shots, key=lambda x: x.id)\n        return few_shots\n    \n    def load_few_shots_embeddings(self, id):\n        # loading embeddings\n        few_shot_embeddings_path = self.config_dir / constant.embeddings_dir / f\"{id}.json\"\n        try:\n            with open(few_shot_embeddings_path, 'r') as file:\n                item = json.load(file)\n                return item \n        except Exception as e:\n            return []\n    \n    def write_few_shots(self, q: Query):\n        few_shot_embeddings_path = self.config_dir / constant.few_shot_dir / f\"{q.id}.yaml\"\n        with open(few_shot_embeddings_path, 'w', encoding='utf-8') as file:\n            yaml.dump(q.to_yaml_dict(), file, allow_unicode=True, default_flow_style=False)\n            file.write('\\n')\n    \n\n    def write_few_shots_embeddings(self, q: Query):\n        few_shot_embeddings_path = self.config_dir / constant.embeddings_dir / f\"{q.id}.json\"\n        with open(few_shot_embeddings_path, 'w') as file:\n            json.dump(q.to_embedding_dict(), file, ensure_ascii=False)\n            file.write('\\n')\n\n    \"\"\"\n    def convert_few_shots_to_query(self, id)->Query:\n        few_shots_path = self.config_dir / 'few_shots' / f\"{id}.yaml\"\n        with open(few_shots_path, 'r', encoding='utf-8') as file:\n            data = yaml.safe_load(file)\n            #load embeddings\n            embeddings = self.load_few_shots_embeddings(id)\n            q = Query(data['question'],\n                      id=id,\n                      enriched=data['enriched'],\n                      answer=data['answer'],\n                      question_embeddings=embeddings['question_embeddings'],\n                      enriched_embeddings=embeddings['enriched_embeddings'])\n            return q\n\n    \"\"\"\n    \n"}
{"type": "source_file", "path": "app/controller/api_router_controller.py", "content": "import yaml\nfrom pathlib import Path\nfrom models.api_router import APIRoutes\n\nclass APIRouterController:\n    def __init__(self):\n        self.config_dir = Path(__file__).parent.parent / 'config'\n        self.routes = []\n\n    def load_api_routes(self):\n        api_yaml_path = self.config_dir / 'api.yaml'\n        with open(api_yaml_path, 'r', encoding='utf-8') as file:\n            data = yaml.safe_load(file)\n        return APIRoutes(data['routes'])"}
{"type": "source_file", "path": "app/controller/eval_controller.py", "content": "\nimport json\nimport traceback\nimport random\nimport string\nimport time\nfrom pathlib import Path\n\nfrom controller.plan_generator import build,enrich_question\nfrom controller.plan_verifier import verify\nfrom controller.schema_controller import SchemaController\nfrom controller.api_router_controller import APIRouterController\nfrom controller.few_shots_controller import FewShotsController\nfrom models.query import Query\nfrom util.logger import logger\nfrom util import constant\n\nclass EvalController:\n    def __init__(self):\n        self.config_dir = Path(__file__).parent.parent / constant.eval_dir\n    \n    def scoring_by_few_shot(self, fs, schemas, routes, few_shots):\n        try: \n            logger.info(f\"处理问题{fs.id}...\")\n            run_q = Query(fs.question,id=fs.id,question_embeddings=fs.question_embeddings, enriched_embeddings=fs.enriched_embeddings)\n            #run_q.set_question_embeddings()\n            logger.debug(\"生成Enriched question:\")\n            run_q.enriched = enrich_question(run_q, schemas,routes,few_shots)\n            #run_q.set_enriched_embeddings()\n            logger.debug(run_q.enriched)\n            logger.debug(\"生成Routine:\")\n            run_q.answer = build(run_q,schemas,routes,few_shots)\n            logger.debug(run_q.answer)\n            result = verify(fs,schemas,routes,run_q.answer)\n            logger.debug(result)\n            return {\"id\": fs.id, \n                            \"question\": fs.question,\n                            \"s_enriched\": fs.enriched,\n                            \"s_answer\": fs.answer,\n                            \"r_enriched\": run_q.enriched,\n                            \"r_answer\": run_q.answer,\n                            \"result\": result[\"pass\"],\n                            \"reason\": result[\"reason\"] }     \n        except Exception as ex:\n            logger.error(f\"处理问题 {fs.id} 时发生未预期的错误: {ex}\")\n            logger.info(f\"错误详情: {traceback.format_exc()}\")\n            \n\n    def scoring(self, ids):\n        schema_controller = SchemaController()\n        schemas = schema_controller.load_all_schemas()\n        \n        api_route_controller = APIRouterController()\n        routes = api_route_controller.load_api_routes()\n\n        few_shots_controller = FewShotsController()\n        few_shots = few_shots_controller.load_few_shots()\n\n        evals = []\n        for fs in few_shots:\n            if fs.id in ids:\n                try: \n                    e = self.scoring_by_few_shot(fs, schemas, routes, few_shots)\n                    evals.append(e)\n                except Exception as e:\n                    logger.error(f\"处理问题 {fs.id} 时发生未预期的错误: {e}\")\n                    logger.info(f\"错误详情: {traceback.format_exc()}\")\n                    continue\n        if len(evals) > 0:\n            self.write_evals(evals)\n    \n    \n    def load_evals(self, model):\n        try: \n            eval_filename = model + \"-eval.jsonl\"\n            eval_path = self.config_dir / eval_filename\n            evals = []\n            with open(eval_path, 'r') as file:\n                for line in file:\n                    if line.strip():  # Make sure to skip any empty lines\n                        item = json.loads(line)\n                        #e = Eval(item['id'], item['question'], item['s_enriched'], item['s_answer'],\n                        #    item['r_enriched'],item['r_answer'],item['result'], item['reason'])\n                        evals.append(item)\n            evals = sorted(evals, key=lambda x: x['id'])\n            return evals\n        except FileNotFoundError:\n            logger.info(f\"没有找到文件\")\n\n    def metrics(self, evals):\n        corrected = sum(1 for e in evals if e[\"result\"] is True)\n        return len(evals), corrected, str(round(corrected/len(evals)*100,2)) + \"%\"\n    \n    def write_evals(self, evals):\n        eval_filename = ''.join(random.choices(string.ascii_lowercase + string.digits, k=10)) + '.json'\n        eval_path = self.config_dir / eval_filename\n        \n        with open(eval_path, 'w') as file:\n            for e in evals:\n                json.dump(e, file, ensure_ascii=False)\n                file.write('\\n')\n\n    def try_api_key(self):\n        logger.info(\"testing!\")\n"}
{"type": "source_file", "path": "app/models/api_router.py", "content": "class APIRoute:\n    def __init__(self, route):\n        self.route = route\n\n    def __str__(self):\n        return f\" - {self.route}\"\n\nclass APIRoutes:\n    def __init__(self, routes):\n        self.routes = [APIRoute(route) for route in routes]\n\n    def __str__(self):\n        return \"\\n\".join(str(route) for route in self.routes)"}
{"type": "source_file", "path": "app/controller/schema_controller.py", "content": "import os\nimport yaml\nfrom pathlib import Path\nfrom models.tables import TableSchema\n\nclass SchemaController:\n    def __init__(self):\n        self.config_dir = Path(__file__).parent.parent / 'config' / 'tables'\n\n    def read_yaml_schema(self, file_path):\n        with open(file_path, 'r', encoding='utf-8') as file:\n            data = yaml.safe_load(file)\n        return TableSchema(data['table'], data['columns'])\n\n    def load_all_schemas(self):\n        schemas = []\n        for yaml_file in self.config_dir.glob('*.yaml'):\n            schemas.append(self.read_yaml_schema(yaml_file))\n        return schemas"}
{"type": "source_file", "path": "app/llms/deepseek.py", "content": "import os\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom util import constant\n\nload_dotenv()\n\ndef launch_client():\n    API_KEY = os.getenv('API_KEY')\n    if API_KEY is None:\n        API_KEY = os.getenv('DEEPSEEK_API_KEY')\n    os.environ[\"OPENAI_API_KEY\"] = API_KEY\n    client = OpenAI(api_key=API_KEY, base_url=\"https://api.deepseek.com\")\n    return client\n\ndef format_messages(sys_msg, user_msg, historicals=None):\n    messages = []\n    if sys_msg != \"\":\n        messages.append({\"role\": \"system\", \"content\": sys_msg})\n    \n    if historicals:\n        for s in historicals:\n            messages.append({\"role\": s[\"role\"], \"content\": s[\"content\"]})\n    \n    if user_msg != \"\":\n        messages.append( {\"role\": \"user\", \"content\": user_msg})   \n    #print(f\"messages are sent to llms: \\n\\n {messages}\")\n    return messages\n\ndef completion(\n        msgs,\n        model = constant.DEEPSEEKCHAT,\n        stream = False,\n        temperature = 0.05\n    ):\n    client = launch_client()\n    full_response = \"\"\n    completion = client.chat.completions.create(\n        model = model,\n        messages = msgs,\n        temperature = temperature\n    )\n\n    if stream:\n        for chunk in completion:\n            full_response += chunk.choices[0].delta.content\n    else: full_response += completion.choices[0].message.content\n    return full_response\n"}
{"type": "source_file", "path": "app/llms/doubao.py", "content": "import os\nfrom dotenv import load_dotenv\nfrom volcenginesdkarkruntime import Ark\nfrom util import constant\nload_dotenv()\n\ndef launch_client():\n    API_KEY = os.getenv('API_KEY')\n    if API_KEY is None:\n        API_KEY = os.getenv('ARK_API_KEY')\n    client = Ark(api_key=API_KEY)\n    return client\n\ndef format_messages(sys_msg, user_msg, historicals=None):\n    messages = []\n    if sys_msg != \"\":\n        messages.append({\"role\": \"system\", \"content\": sys_msg})\n    \n    if historicals:\n        for s in historicals:\n            messages.append({\"role\": s[\"role\"], \"content\": s[\"content\"]})\n    \n    if user_msg != \"\":\n        messages.append( {\"role\": \"user\", \"content\": user_msg})   \n    #print(f\"messages are sent to llms: \\n\\n {messages}\")\n    return messages\n\ndef completion(\n        msgs,\n        model = constant.DOUBAOPRO32K,\n        stream = False,\n        temperature = 0.05\n    ):\n    client = launch_client()\n    full_response = \"\"\n    if model == constant.DOUBAOPRO32K:\n        completion = client.chat.completions.create(\n            model = os.getenv(\"END_POINT\"),\n            messages = msgs,\n            temperature = temperature\n        )\n\n    if stream:\n        for chunk in completion:\n            full_response += chunk.choices[0].delta.content\n    else: full_response += completion.choices[0].message.content\n    return full_response\n    "}
{"type": "source_file", "path": "app/llms/general.py", "content": "import os\nfrom llms import zhipu,openai,qwen,doubao,deepseek\nfrom util import constant\nfrom util.tools import extract_json\nfrom util.logger import logger\n\ndef verify_api_key():\n    complete(\"\",\"Hello!\")\n\n\ndef complete_verify(sys_msg, usr_msg):\n    model = os.getenv('MODEL')\n    # use the strongest model to verify routines\n    if model in constant.GLM_FAMILY:\n        messages = zhipu.format_messages(sys_msg,usr_msg)\n        result = extract_json(zhipu.completion(messages, temperature=0.1, model=constant.GLM_VERIFIER))\n    elif model in constant.OPENAI_FAMILY:\n        messages = openai.format_messages(sys_msg,usr_msg)\n        result = extract_json(openai.completion(messages, temperature=0, model=constant.OPENAI_VERIFIER))\n    elif model in constant.QWEN_FAMILY:\n        messages = qwen.format_messages(sys_msg,usr_msg)\n        data_json = qwen.completion(messages, temperature=0, model=constant.QWEN_VERIFIER)\n        result = extract_json(data_json)\n    elif model in constant.DOUBAO_FAMILY:\n        messages = doubao.format_messages(sys_msg,usr_msg)\n        data_json = doubao.completion(messages, temperature=0, model=constant.DOUBAO_VERIFIER)\n        result = extract_json(data_json)\n    elif model in constant.DEEPSEEK_FAMILY:\n        messages = deepseek.format_messages(sys_msg,usr_msg)\n        data_json = deepseek.completion(messages, temperature=1.0, model=constant.DEEPSEEK_VERIFIER)\n        result = extract_json(data_json) \n    return result\n\n\ndef complete(sys_msg, usr_msg):\n    model = os.getenv('MODEL')\n    if model in constant.GLM_FAMILY:\n        messages = zhipu.format_messages(sys_msg,usr_msg)\n        result = zhipu.completion(messages, \n                                  model=model,\n                                  temperature=0.1)\n    elif model in constant.OPENAI_FAMILY:\n        messages = openai.format_messages(sys_msg,usr_msg)\n        result = openai.completion(messages, \n                                   model= model, \n                                   temperature=0)\n    elif model in constant.QWEN_FAMILY:\n        messages = qwen.format_messages(sys_msg,usr_msg)\n        result = qwen.completion(messages, \n                                model= model, \n                                temperature=0)\n    elif model in constant.DOUBAO_FAMILY:\n        messages = doubao.format_messages(sys_msg,usr_msg)\n        result = doubao.completion(messages, \n                                model= model, \n                                temperature=0)\n    elif model in constant.DEEPSEEK_FAMILY:\n        messages = deepseek.format_messages(sys_msg,usr_msg)\n        result = deepseek.completion(messages, \n                                model= model, \n                                temperature=1.0)\n    return result\n    "}
{"type": "source_file", "path": "app/llms/qwen.py", "content": "import os\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom util import constant\n\nload_dotenv()\n\ndef launch_client():\n    API_KEY = os.getenv('API_KEY')\n    if API_KEY is None:\n        API_KEY = os.getenv('DASHSCOPE_API_KEY')\n    client = OpenAI(\n        api_key=API_KEY, \n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",   \n    )\n    return client\n\ndef format_messages(sys_msg, user_msg, historicals=None):\n    messages = []\n    if sys_msg != \"\":\n        messages.append({\"role\": \"system\", \"content\": sys_msg})\n    \n    if historicals:\n        for s in historicals:\n            messages.append({\"role\": s[\"role\"], \"content\": s[\"content\"]})\n    \n    if user_msg != \"\":\n        messages.append( {\"role\": \"user\", \"content\": user_msg})   \n    #print(f\"messages are sent to llms: \\n\\n {messages}\")\n    return messages\n\ndef completion(\n        msgs,\n        model = constant.QWENMAX,\n        stream = False,\n        temperature = 0.05\n    ):\n    client = launch_client()\n    full_response = \"\"\n    completion = client.chat.completions.create(\n        model = model,\n        messages = msgs,\n        temperature = temperature\n    )\n\n    if stream:\n        for chunk in completion:\n            full_response += chunk.choices[0].delta.content\n    else: full_response += completion.choices[0].message.content\n    return full_response\n"}
{"type": "source_file", "path": "app/llms/openai.py", "content": "import os\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom util import constant\n\nload_dotenv()\n\ndef launch_client():\n    API_KEY = os.getenv('API_KEY')\n    if API_KEY is None:\n        API_KEY = os.getenv('OPENAI_API_KEY')\n    os.environ[\"OPENAI_API_KEY\"] = API_KEY\n    client = OpenAI()\n    return client\n\ndef format_messages(sys_msg, user_msg, historicals=None):\n    messages = []\n    if sys_msg != \"\":\n        messages.append({\"role\": \"system\", \"content\": sys_msg})\n    \n    if historicals:\n        for s in historicals:\n            messages.append({\"role\": s[\"role\"], \"content\": s[\"content\"]})\n    \n    if user_msg != \"\":\n        messages.append( {\"role\": \"user\", \"content\": user_msg})   \n    #print(f\"messages are sent to llms: \\n\\n {messages}\")\n    return messages\n\ndef completion(\n        msgs,\n        model = constant.GPT4OMINI,\n        stream = False,\n        temperature = 0.05\n    ):\n    client = launch_client()\n    full_response = \"\"\n    completion = client.chat.completions.create(\n        model = model,\n        messages = msgs,\n        temperature = temperature\n    )\n\n    if stream:\n        for chunk in completion:\n            full_response += chunk.choices[0].delta.content\n    else: full_response += completion.choices[0].message.content\n    return full_response\n"}
{"type": "source_file", "path": "app/llms/zhipu.py", "content": "import os\nfrom dotenv import load_dotenv\nfrom util import constant\nfrom zhipuai import ZhipuAI\nfrom util.logger import logger\n\nload_dotenv()\n\ndef launch_client():\n    API_KEY = os.getenv('API_KEY')\n    if API_KEY is None:\n        API_KEY = os.getenv('GLM_API_KEY')\n    client = ZhipuAI(api_key= API_KEY)\n    return client\n\ndef format_messages(sys_msg, user_msg, historicals=None):\n    messages = []\n    if sys_msg != \"\":\n        messages.append({\"role\": \"system\", \"content\": sys_msg})\n    \n    if historicals:\n        for s in historicals:\n            messages.append({\"role\": s[\"role\"], \"content\": s[\"content\"]})\n    \n    if user_msg != \"\":\n        messages.append( {\"role\": \"user\", \"content\": user_msg})   \n    #print(f\"messages are sent to llms: \\n\\n {messages}\")\n    return messages\n\ndef completion(\n        msgs,\n        model = constant.GLM4PLUS,\n        stream = False,\n        temperature = 0.05\n    ):\n\n    \n    client = launch_client()\n    full_response = \"\"\n    logger.debug(msgs)\n    response = client.chat.completions.create(\n        model=model,  # 填写需要调用的模型名称\n        messages=msgs,\n        stream=stream,\n        max_tokens = 8192,\n        temperature=temperature,\n        tools=[{\"type\": \"web_search\", \"web_search\": {\"enable\": False}}]\n    )\n    if stream:\n        for chunk in response:\n            full_response += chunk.choices[0].delta\n    else:\n        full_response += response.choices[0].message.content\n    \n    return full_response\n\ndef get_embeddings(s):\n    client = launch_client()\n    response = client.embeddings.create(\n        model=constant.EMBEDDING, #填写需要调用的模型名称\n        input=s,\n    )\n    return response.data[0].embedding\n\n"}
{"type": "source_file", "path": "app/util/tools.py", "content": "import json\n\ndef extract_json(content): \n    if content.find('[') != -1 and content.find(']') != -1:\n        return extract_json_array(content)\n    else: \n        return extract_json_item(content)\n\ndef extract_json_item(content):\n    #先验证是否可以被解析\n    # Find the index where the list starts and ends\n    start_index = content.find('{')\n    end_index = content.rfind('}') + 1\n    \n    # Extract the JSON-like string between these indices\n    json_string = content[start_index:end_index]\n    print(json_string)\n    # Convert the JSON string to a Python list\n    json_data = json.loads(json_string)\n    \n    return json_data\n\ndef extract_json_array(content):\n    # Find the index where the list starts and ends\n    start_index = content.find('[')\n    end_index = content.rfind(']') + 1\n    \n    # Extract the JSON-like string between these indices\n    json_string = content[start_index:end_index]\n    \n    # Convert the JSON string to a Python list\n    json_data = json.loads(json_string)\n    \n    return json_data\n\n"}
{"type": "source_file", "path": "app/util/constant.py", "content": "## GLM Models\nGLM4PLUS = \"glm-4-plus\"\nGLM40520 = \"glm-4-0520\"\nGLM4 = \"glm-4\"\nGLM4AIR = \"glm-4-air\"\nGLM4FLASH = \"glm-4-flash\"\nEMBEDDING = \"embedding-2\"\nGLM_VERIFIER =GLM40520\nGLM_FAMILY = [GLM4PLUS,GLM40520,GLM4,GLM4AIR,GLM4FLASH]\n\n## OpenAI Models\nGPT4O = \"gpt-4o\"\nGPT4OMINI = \"gpt-4o-mini\"\nOPENAI_VERIFIER = GPT4O\nOPENAI_FAMILY= [GPT4O,GPT4OMINI]\n## QWEN\nQWENMAX = \"qwen-max\"\nQWENPLUS = \"qwen-plus\"\nQWEN_VERIFIER = QWENMAX\nQWEN_FAMILY = [QWENMAX,QWENPLUS]\n\n#DOUBAO\nDOUBAOPRO32K = \"doubao-pro-32k\"\nDOUBAO_VERIFIER = DOUBAOPRO32K\nDOUBAO_FAMILY = [DOUBAOPRO32K]\n\n#DEEPSEEK\nDEEPSEEKCHAT = \"deepseek-chat\"\nDEEPSEEK_VERIFIER = DEEPSEEKCHAT\nDEEPSEEK_FAMILY = [DEEPSEEKCHAT]\n\nDEFAULT_MODEL = GLM4PLUS\n\nMAX_RETRY = 3\nlog_path = \"logs/\"\nconfig_path = \"config/\"\nfew_shot_dir = \"few_shots\"\nembeddings_dir = \"embeddings\"\neval_dir = \"eval\"\ncss_dir = \"css\"\n\n# UI\nCOL_PERROW = 4\n\n\n\n"}
{"type": "source_file", "path": "app/models/query.py", "content": "\nfrom llms.zhipu import get_embeddings\nclass Query: \n    def __init__(self, question, id=0, enriched=\"\", answer=\"\", question_embeddings=[], enriched_embeddings=[]):\n        \n        self.id = id\n        self.question = question\n        self.enriched = enriched\n        self.answer = answer\n        if len(question_embeddings):\n            self.question_embeddings = question_embeddings\n        else:\n            self.question_embeddings = []\n        \n        if len(enriched_embeddings) > 0:\n            self.enriched_embeddings = enriched_embeddings\n        else:\n            self.enriched_embeddings = []\n    \n    def set_question_embeddings(self):\n        self.question_embeddings = get_embeddings(self.question)\n\n    def set_enriched_embeddings(self):\n        if len(self.enriched) > 0:\n            self.enriched_embeddings = get_embeddings(self.enriched)\n\n    def get_enriched_examples(self):\n        return f\"Q:{self.question}\\n\\nA:{self.enriched}\"\n    \n    def get_instructions_examples(self):\n        return f\"Q:{self.enriched}\\n\\nA:{self.answer}\"\n\n    def to_yaml_dict(self):\n        return { \"question\": self.question, \"enriched\": self.enriched, \"id\": self.id,\"answer\": self.answer}\n    \n    def to_embedding_dict(self):\n        return {\"id\": self.id, \"question\": self.question,\"enriched\": self.enriched,\"question_embeddings\": self.question_embeddings, \"enriched_embeddings\": self.enriched_embeddings}\n    "}
{"type": "source_file", "path": "app/models/tables.py", "content": "import yaml\n\nclass Column:\n    def __init__(self, name, desc=None):\n        self.name = name\n        self.desc = desc\n    def __str__(self):\n        if self.desc:\n            return f\"'{self.name}'{self.desc}\"\n        return f\"'{self.name}'\"\n\nclass TableSchema:\n    def __init__(self, table_name, columns):\n        self.table_name = table_name\n        self.columns = [Column(col['name'], col.get('desc')) for col in columns]\n\n    def __str__(self):\n        column_names = [col.name for col in self.columns]\n        base_string = f\"{self.table_name}包含属性:{', '.join(repr(name) for name in column_names)};\"\n        \n        additional_info = [str(col) for col in self.columns if col.desc]\n        \n        if additional_info:\n            base_string += '\\n-' + ';\\n-'.join(additional_info) + ';'\n\n        return base_string"}
{"type": "source_file", "path": "app/models/eval.py", "content": "class Eval:\n    def __init__(self, id, question, s_enriched, s_answer, r_enriched, r_answer, result, reason): \n        self.id = id\n        self.question = question\n        self.s_enriched = s_enriched\n        self.s_answer = s_answer\n        self.r_enriched = r_enriched\n        self.r_answer = r_answer\n        self.result = bool(result)\n        self.reason = reason\n    \n    def to_dict(self):\n        return { \"id\": self.id,\n                \"question\": self.question, \n                \"s_enriched\": self.s_enriched, \n                \"s_answer\": self.s_answer,\n                \"r_enriched\": self.r_enriched, \n                \"r_answer\": self.r_answer,\n                \"result\": self.result,\n                \"reason\": self.reason\n                }\n"}
{"type": "source_file", "path": "app/util/logger.py", "content": "# logger.py\nimport os\nfrom util import constant\nfrom datetime import datetime\nimport logging\n\n\nlog_file_prefix = 'log_'\n# 获取当前日期，格式化为%Y%m%d\ncurrent_date = datetime.now().strftime('%Y%m%d')\n# 日志文件名后缀\nlog_file_suffix = '.log'\n# 完整的日志文件路径\nlog_file_path = os.path.join(constant.log_path, log_file_prefix + current_date + log_file_suffix)\n\n# 检查日志目录是否存在，如果不存在则创建\nif not os.path.exists(constant.log_path):\n    os.makedirs(constant.log_path)\n\n# 创建一个logger\nlogger = logging.getLogger('my_logger')\nlogger.setLevel(logging.INFO)\n\n# 创建一个handler，用于写入日志文件\nfh = logging.FileHandler(log_file_path)\n#fh.setLevel(logging.INFO)\nfh.setLevel(logging.INFO)\n\n# 创建一个handler，用于将日志输出到控制台\nch = logging.StreamHandler()\n#ch.setLevel(logging.INFO)\nch.setLevel(logging.INFO)\n\n# 定义handler的输出格式\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nfh.setFormatter(formatter)\nch.setFormatter(formatter)\n\n# 给logger添加handler\nlogger.addHandler(fh)\nlogger.addHandler(ch)\n"}
