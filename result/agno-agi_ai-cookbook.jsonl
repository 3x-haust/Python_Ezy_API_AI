{"repo_info": {"repo_name": "ai-cookbook", "repo_owner": "agno-agi", "repo_url": "https://github.com/agno-agi/ai-cookbook"}}
{"type": "test_file", "path": "pdf_ai/test_tools.py", "content": "from phi.utils.log import set_log_level_to_debug\n\nfrom pdf_ai.tools import PDFTools\n\nset_log_level_to_debug()\npdf_tools = PDFTools(user_id=\"ab\")\n\n# latest_document = pdf_tools.get_latest_document()\n# print(latest_document)\n\n# result = pdf_tools.search_latest_document(\"agreement between\")\n# print(result)\n\n# document_names = pdf_tools.get_document_names()\n# print(document_names)\n\n# search_document = pdf_tools.search_document(\"agreement\", \"Hydy Services Agreement\")\n# print(search_document)\n\n# document_content = pdf_tools.get_document_contents(\"Hydy Services Agreement\")\n# print(document_content)\n"}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_placeholder.py", "content": "def test_placeholder():\n    assert True\n"}
{"type": "source_file", "path": "ai/assistants/pdf_auto.py", "content": "from typing import Optional\n\nfrom phi.assistant import Assistant\nfrom phi.llm.openai import OpenAIChat\n\nfrom ai.settings import ai_settings\nfrom ai.storage import pdf_assistant_storage\nfrom ai.knowledge_base import pdf_knowledge_base\n\n\ndef get_autonomous_pdf_assistant(\n    run_id: Optional[str] = None,\n    user_id: Optional[str] = None,\n    debug_mode: bool = False,\n) -> Assistant:\n    \"\"\"Get an Autonomous Assistant with a PDF knowledge base.\"\"\"\n\n    return Assistant(\n        name=\"auto_pdf_assistant\",\n        run_id=run_id,\n        user_id=user_id,\n        llm=OpenAIChat(\n            model=ai_settings.gpt_4,\n            max_tokens=ai_settings.default_max_tokens,\n            temperature=ai_settings.default_temperature,\n        ),\n        storage=pdf_assistant_storage,\n        knowledge_base=pdf_knowledge_base,\n        monitoring=True,\n        use_tools=True,\n        show_tool_calls=True,\n        debug_mode=debug_mode,\n        description=\"You are a helpful assistant named 'phi' designed to answer questions about PDF contents.\",\n        extra_instructions=[\n            \"Keep your answers under 5 sentences.\",\n        ],\n        assistant_data={\"assistant_type\": \"autonomous\"},\n    )\n"}
{"type": "source_file", "path": "ai/assistants/website_auto.py", "content": "from typing import Optional\n\nfrom phi.assistant import Assistant\nfrom phi.llm.openai import OpenAIChat\n\nfrom ai.settings import ai_settings\nfrom ai.storage import website_assistant_storage\nfrom ai.knowledge_base import website_knowledge_base\n\n\ndef get_autonomous_website_assistant(\n    run_id: Optional[str] = None,\n    user_id: Optional[str] = None,\n    debug_mode: bool = False,\n) -> Assistant:\n    \"\"\"Get an Autonomous Assistant with a Website knowledge base.\"\"\"\n\n    return Assistant(\n        name=\"auto_website_assistant\",\n        run_id=run_id,\n        user_id=user_id,\n        llm=OpenAIChat(\n            model=ai_settings.gpt_4,\n            max_tokens=ai_settings.default_max_tokens,\n            temperature=ai_settings.default_temperature,\n        ),\n        storage=website_assistant_storage,\n        knowledge_base=website_knowledge_base,\n        monitoring=True,\n        use_tools=True,\n        show_tool_calls=True,\n        debug_mode=debug_mode,\n        description=\"You are a helpful assistant named 'phi' designed to answer questions about website contents.\",\n        extra_instructions=[\n            \"Keep your answers under 5 sentences.\",\n        ],\n        assistant_data={\"assistant_type\": \"autonomous\"},\n    )\n"}
{"type": "source_file", "path": "ai/assistants/pdf_rag.py", "content": "from typing import Optional\n\nfrom phi.assistant import Assistant\nfrom phi.llm.openai import OpenAIChat\n\nfrom ai.settings import ai_settings\nfrom ai.storage import pdf_assistant_storage\nfrom ai.knowledge_base import pdf_knowledge_base\n\n\ndef get_rag_pdf_assistant(\n    run_id: Optional[str] = None,\n    user_id: Optional[str] = None,\n    debug_mode: bool = False,\n) -> Assistant:\n    \"\"\"Get a RAG Assistant with a PDF knowledge base.\"\"\"\n\n    return Assistant(\n        name=\"rag_pdf_assistant\",\n        run_id=run_id,\n        user_id=user_id,\n        llm=OpenAIChat(\n            model=ai_settings.gpt_4,\n            max_tokens=ai_settings.default_max_tokens,\n            temperature=ai_settings.default_temperature,\n        ),\n        storage=pdf_assistant_storage,\n        knowledge_base=pdf_knowledge_base,\n        # This setting adds references from the knowledge_base to the user prompt\n        add_references_to_prompt=True,\n        # This setting adds the last 6 messages from the chat history to the API call\n        add_chat_history_to_messages=True,\n        monitoring=True,\n        debug_mode=debug_mode,\n        description=\"You are a helpful assistant named 'phi' designed to answer questions about PDF contents.\",\n        extra_instructions=[\n            \"Keep your answers under 5 sentences.\",\n        ],\n        assistant_data={\"assistant_type\": \"rag\"},\n    )\n"}
{"type": "source_file", "path": "api/routes/hn.py", "content": "from typing import Optional\n\nfrom fastapi import APIRouter\nfrom pydantic import BaseModel\n\nfrom api.routes.endpoints import endpoints\nfrom hn_ai.knowledge import load_hackernews_knowledge_base\n\n######################################################\n## Router for Hackernews Assistant\n######################################################\n\nhn_router = APIRouter(prefix=endpoints.HN, tags=[\"HackerNews\"])\n\n\nclass LoadKnowledgeBaseRequest(BaseModel):\n    key: Optional[str] = None\n\n\n@hn_router.post(\"/load-knowledge-base\")\ndef load_knowledge_base(body: LoadKnowledgeBaseRequest):\n    \"\"\"Loads the hackernews knowledge base\"\"\"\n\n    status = load_hackernews_knowledge_base()\n    return {\"message\": status}\n"}
{"type": "source_file", "path": "ai/assistants/__init__.py", "content": ""}
{"type": "source_file", "path": "api/routes/endpoints.py", "content": "from dataclasses import dataclass\n\n\n@dataclass\nclass ApiEndpoints:\n    PING: str = \"/ping\"\n    HEALTH: str = \"/health\"\n    ASSISTANTS: str = \"/assistants\"\n    HN: str = \"/hn\"\n    ARXIV_DISCORD: str = \"/arxiv_discord\"\n\n\nendpoints = ApiEndpoints()\n"}
{"type": "source_file", "path": "arxiv_ai/__init__.py", "content": ""}
{"type": "source_file", "path": "ai/assistants/website_rag.py", "content": "from typing import Optional\n\nfrom phi.assistant import Assistant\nfrom phi.llm.openai import OpenAIChat\n\nfrom ai.settings import ai_settings\nfrom ai.storage import website_assistant_storage\nfrom ai.knowledge_base import website_knowledge_base\n\n\ndef get_rag_website_assistant(\n    run_id: Optional[str] = None,\n    user_id: Optional[str] = None,\n    debug_mode: bool = False,\n) -> Assistant:\n    \"\"\"Get a RAG Assistant with a Website knowledge base.\"\"\"\n\n    return Assistant(\n        name=\"rag_website_assistant\",\n        run_id=run_id,\n        user_id=user_id,\n        llm=OpenAIChat(\n            model=ai_settings.gpt_4,\n            max_tokens=ai_settings.default_max_tokens,\n            temperature=ai_settings.default_temperature,\n        ),\n        storage=website_assistant_storage,\n        knowledge_base=website_knowledge_base,\n        # This setting adds references from the knowledge_base to the user prompt\n        add_references_to_prompt=True,\n        # This setting adds the last 6 messages from the chat history to the API call\n        add_chat_history_to_messages=True,\n        monitoring=True,\n        debug_mode=debug_mode,\n        description=\"You are a helpful assistant named 'phi' designed to answer questions about website contents.\",\n        extra_instructions=[\n            \"Keep your answers under 5 sentences.\",\n        ],\n        assistant_data={\"assistant_type\": \"rag\"},\n    )\n"}
{"type": "source_file", "path": "ai/assistants/image.py", "content": "from typing import Optional\n\nfrom phi.assistant import Assistant\nfrom phi.llm.openai import OpenAIChat\n\nfrom ai.settings import ai_settings\nfrom ai.storage import image_assistant_storage\n\n\ndef get_image_assistant(\n    run_id: Optional[str] = None,\n    user_id: Optional[str] = None,\n    debug_mode: bool = False,\n) -> Assistant:\n    \"\"\"Get an Image Assistant\"\"\"\n\n    return Assistant(\n        name=\"image_assistant\",\n        run_id=run_id,\n        user_id=user_id,\n        llm=OpenAIChat(\n            model=ai_settings.gpt_4_vision,\n            max_tokens=ai_settings.default_max_tokens,\n            temperature=ai_settings.default_temperature,\n        ),\n        storage=image_assistant_storage,\n        monitoring=True,\n        debug_mode=debug_mode,\n        assistant_data={\"assistant_type\": \"vision\"},\n    )\n"}
{"type": "source_file", "path": "ai/__init__.py", "content": ""}
{"type": "source_file", "path": "api/main.py", "content": "from fastapi import FastAPI\nfrom starlette.middleware.cors import CORSMiddleware\n\nfrom api.settings import api_settings\nfrom api.routes.v1_router import v1_router\n\n\ndef create_app() -> FastAPI:\n    \"\"\"Create a FastAPI App\n\n    Returns:\n        FastAPI: FastAPI App\n    \"\"\"\n\n    # Create FastAPI App\n    app: FastAPI = FastAPI(\n        title=api_settings.title,\n        version=api_settings.version,\n        docs_url=\"/docs\" if api_settings.docs_enabled else None,\n        redoc_url=\"/redoc\" if api_settings.docs_enabled else None,\n        openapi_url=\"/openapi.json\" if api_settings.docs_enabled else None,\n    )\n\n    # Add v1 router\n    app.include_router(v1_router)\n\n    # Add Middlewares\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=api_settings.cors_origin_list,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    return app\n\n\n# Create FastAPI app\napp = create_app()\n"}
{"type": "source_file", "path": "api/routes/status.py", "content": "from fastapi import APIRouter\n\nfrom api.routes.endpoints import endpoints\nfrom utils.dttm import current_utc_str\n\n######################################################\n## Router for health checks\n######################################################\n\nstatus_router = APIRouter(tags=[\"Status\"])\n\n\n@status_router.get(endpoints.PING)\ndef status_ping():\n    \"\"\"Ping the Api\"\"\"\n\n    return {\"ping\": \"pong\"}\n\n\n@status_router.get(endpoints.HEALTH)\ndef status_health():\n    \"\"\"Check the health of the Api\"\"\"\n\n    return {\n        \"status\": \"success\",\n        \"router\": \"status\",\n        \"path\": endpoints.HEALTH,\n        \"utc\": current_utc_str(),\n    }\n"}
{"type": "source_file", "path": "api/routes/__init__.py", "content": ""}
{"type": "source_file", "path": "api/routes/arxiv_discord.py", "content": "import asyncio\nfrom os import getenv\nfrom typing import Set\n\nfrom fastapi import APIRouter\nfrom discord import Intents, Client, Message\nfrom arxiv_ai.ls.message import handle_mention, handle_message\n\nfrom api.routes.endpoints import endpoints\nfrom utils.log import logger\n\n######################################################\n## Router for Arxiv Discord Bot\n######################################################\n\nintents = Intents.default()\nintents.message_content = True\nclient = Client(intents=intents)\nclient.active_threads: Set[int] = set()\n\n\n@client.event\nasync def on_ready():\n    logger.info(f\"Logged in as {client.user}\")\n\n\n@client.event\nasync def on_message(message: Message):\n    if message.author == client.user:\n        return\n\n    if message.mentions and client.user.mentioned_in(message):\n        await handle_mention(message=message, client=client)\n    else:\n        await handle_message(message=message, client=client)\n\n    return\n\n\narxiv_discord_router = APIRouter(prefix=endpoints.ARXIV_DISCORD, tags=[\"ARXIV\"])\n\n\n@arxiv_discord_router.on_event(\"startup\")\nasync def startup_event():\n    logger.info(\"Starting Arxiv Discord Bot\")\n    asyncio.create_task(client.start(getenv(\"ARXIV_AI_TOKEN\")))\n"}
{"type": "source_file", "path": "ai/settings.py", "content": "from pydantic_settings import BaseSettings\n\n\nclass AISettings(BaseSettings):\n    \"\"\"LLM settings that can be set using environment variables.\n\n    Reference: https://pydantic-docs.helpmanual.io/usage/settings/\n    \"\"\"\n\n    gpt_4: str = \"gpt-4-turbo-preview\"\n    gpt_4_vision: str = \"gpt-4-vision-preview\"\n    gpt_3_5: str = \"gpt-3.5-turbo-1106\"\n    dall_e: str = \"dall-e-3\"\n    whisper: str = \"whisper-1\"\n    embedding_model: str = \"text-embedding-3-small\"\n    default_max_tokens: int = 1024\n    default_temperature: float = 0\n\n\n# Create AISettings object\nai_settings = AISettings()\n"}
{"type": "source_file", "path": "arxiv_ai/ls/__init__.py", "content": ""}
{"type": "source_file", "path": "arxiv_ai/app.py", "content": "from typing import List, Optional, Tuple\n\nimport streamlit as st\nfrom phi.assistant import Assistant\nfrom phi.tools.streamlit.components import (\n    get_openai_key_sidebar,\n    get_username_sidebar,\n    reload_button_sidebar,\n)\n\nfrom arxiv_ai.assistant import get_arxiv_assistant\nfrom arxiv_ai.knowledge import get_available_docs, get_arxiv_summary_knowledge_base_for_user\nfrom utils.log import logger\n\n\nst.set_page_config(\n    page_title=\"arXiv AI\",\n    page_icon=\":orange_heart:\",\n)\nst.title(\"Chat with arXiv\")\nst.markdown(\"##### :orange_heart: built using [phidata](https://github.com/phidatahq/phidata)\")\nwith st.expander(\":rainbow[:point_down: How to use]\"):\n    st.markdown(\"Ask questions like:\")\n    st.markdown(\"- Tell me about https://arxiv.org/abs/1706.03762\")\n    st.markdown(\"- What is FlashAttention?\")\n    st.markdown(\"- What is gaia?\")\n    st.markdown(\"- Summarize the paper on FlashAttention\")\n    st.markdown(\"- Search knowledge base for gaia\")\n    st.markdown(\"- Search arXiv for gaia\")\n    st.markdown(\"- Search the web for gaia\")\n    st.markdown(\"Notes:\")\n    st.markdown(\"- To Ask questions from a specific paper: Select a paper from the sidebar\")\n    st.markdown(\n        \"- By default it searches its knowledge base first.\\nToggle force search to search arXiv for a specific topic\"\n    )\n    st.markdown(\n        \"- Message us on [discord](https://discord.com/invite/4MtYHHrgA8) for any issues or feature requests\"\n    )\n\n\ndef restart_assistant():\n    st.session_state[\"arxiv_assistant\"] = None\n    st.session_state[\"arxiv_assistant_run_id\"] = None\n    st.rerun()\n\n\ndef main() -> None:\n    # Get OpenAI key from environment variable or user input\n    get_openai_key_sidebar()\n\n    # Get username\n    username = get_username_sidebar()\n    if username:\n        st.sidebar.info(f\":technologist: User: {username}\")\n    else:\n        st.markdown(\"---\")\n        st.markdown(\"#### :technologist: Enter a username and ask me about arXiv\")\n        return\n\n    if username in [\"latent_space\", \"ls\", \"phidata\"]:\n        st.markdown(\"#### :warning: This username is reserved.\")\n        st.markdown(\"#### :warning: Please refresh and use a different username\")\n        return\n\n    force_search_arxiv = st.sidebar.toggle(\n        label=\"Force search arXiv\",\n        value=False,\n        help=\"Turn on to search arXiv for a specific topic instead of the knowledge base\",\n    )\n\n    # Get the assistant\n    arxiv_assistant: Assistant\n    if \"arxiv_assistant\" not in st.session_state or st.session_state[\"arxiv_assistant\"] is None:\n        logger.info(\"---*--- Creating Arxiv Assistant ---*---\")\n        arxiv_assistant = get_arxiv_assistant(\n            user_id=username,\n            debug_mode=True,\n        )\n        st.session_state[\"arxiv_assistant\"] = arxiv_assistant\n    else:\n        arxiv_assistant = st.session_state[\"arxiv_assistant\"]\n    # Create assistant run (i.e. log to database) and save run_id in session state\n    st.session_state[\"arxiv_assistant_run_id\"] = arxiv_assistant.create_run()\n\n    # Load messages for existing assistant\n    assistant_chat_history = arxiv_assistant.memory.get_chat_history()\n    if len(assistant_chat_history) > 0:\n        logger.debug(\"Loading chat history\")\n        st.session_state[\"messages\"] = assistant_chat_history\n    else:\n        logger.debug(\"No chat history found\")\n        st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"Ask me questions from the Arxiv\"}]\n\n    # Prompt for user input\n    if prompt := st.chat_input():\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n\n    # Display existing chat messages\n    for message in st.session_state[\"messages\"]:\n        if message[\"role\"] == \"system\":\n            continue\n        with st.chat_message(message[\"role\"]):\n            st.write(message[\"content\"])\n\n    # If last message is from a user, generate a new response\n    last_message = st.session_state[\"messages\"][-1]\n    if last_message.get(\"role\") == \"user\":\n        question = last_message[\"content\"]\n        if force_search_arxiv:\n            question = f\"Search arXiv for: {question}\"\n        with st.chat_message(\"assistant\"):\n            with st.spinner(\"Working...\"):\n                response = \"\"\n                resp_container = st.empty()\n                for delta in arxiv_assistant.run(question):\n                    response += delta  # type: ignore\n                    resp_container.markdown(response)\n\n                st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\n\n    # Select a specific paper\n    summary_knowledge_base = get_arxiv_summary_knowledge_base_for_user(user_id=username)\n    if summary_knowledge_base:\n        available_docs: Optional[List[Tuple[str, str]]] = get_available_docs(summary_knowledge_base)\n        if available_docs:\n            available_docs.insert(0, (\"-- all --\", \"-- all --\"))\n            selected_paper = st.sidebar.selectbox(\n                \"Select Paper\",\n                options=available_docs,\n                key=\"selected_paper\",\n                format_func=lambda x: x[1],\n            )\n            logger.info(f\"Selected paper: {selected_paper}\")\n            if selected_paper is not None:\n                # Refresh the assistant to update the instructions and document names\n                arxiv_assistant = get_arxiv_assistant(\n                    user_id=username,\n                    run_id=st.session_state[\"arxiv_assistant_run_id\"],\n                    document_name=selected_paper if selected_paper[0] != \"-- all --\" else None,\n                    debug_mode=True,\n                )\n                st.session_state[\"arxiv_assistant\"] = arxiv_assistant\n\n    st.sidebar.markdown(\"---\")\n\n    if st.sidebar.button(\"New Run\"):\n        restart_assistant()\n\n    if st.sidebar.button(\"Auto Rename\"):\n        arxiv_assistant.auto_rename_run()\n\n    if arxiv_assistant.storage:\n        arxiv_assistant_run_ids: List[str] = arxiv_assistant.storage.get_all_run_ids(user_id=username)\n        new_arxiv_assistant_run_id = st.sidebar.selectbox(\"Run ID\", options=arxiv_assistant_run_ids)\n        if st.session_state[\"arxiv_assistant_run_id\"] != new_arxiv_assistant_run_id:\n            logger.debug(f\"Loading run {new_arxiv_assistant_run_id}\")\n            logger.info(\"---*--- Loading ArXiv Assistant ---*---\")\n            st.session_state[\"arxiv_assistant\"] = get_arxiv_assistant(\n                user_id=username,\n                run_id=new_arxiv_assistant_run_id,\n                debug_mode=True,\n            )\n            st.rerun()\n\n    arxiv_assistant_run_name = arxiv_assistant.run_name\n    if arxiv_assistant_run_name:\n        st.sidebar.write(f\":thread: {arxiv_assistant_run_name}\")\n\n    # Show reload button\n    reload_button_sidebar()\n\n\nmain()\n"}
{"type": "source_file", "path": "db/tables/base.py", "content": "from sqlalchemy import MetaData\nfrom sqlalchemy.orm import DeclarativeBase\n\n\nclass Base(DeclarativeBase):\n    \"\"\"\n    Base class for SQLAlchemy model definitions.\n\n    https://fastapi.tiangolo.com/tutorial/sql-databases/#create-a-base-class\n    https://docs.sqlalchemy.org/en/20/orm/mapping_api.html#sqlalchemy.orm.DeclarativeBase\n    \"\"\"\n\n    metadata = MetaData(schema=\"public\")\n"}
{"type": "source_file", "path": "ai/knowledge_base.py", "content": "from phi.embedder.openai import OpenAIEmbedder\nfrom phi.knowledge.combined import CombinedKnowledgeBase\nfrom phi.knowledge.pdf import PDFUrlKnowledgeBase, PDFKnowledgeBase\nfrom phi.knowledge.website import WebsiteKnowledgeBase\nfrom phi.vectordb.pgvector import PgVector2\n\nfrom ai.settings import ai_settings\nfrom db.session import db_url\n\npdf_knowledge_base = CombinedKnowledgeBase(\n    sources=[\n        PDFUrlKnowledgeBase(urls=[\"https://phi-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf\"]),\n        PDFKnowledgeBase(path=\"data/pdfs\"),\n    ],\n    vector_db=PgVector2(\n        db_url=db_url,\n        # Store the embeddings in ai.pdf_documents\n        collection=\"pdf_documents\",\n        embedder=OpenAIEmbedder(model=ai_settings.embedding_model),\n    ),\n    # 2 references are added to the prompt\n    num_documents=2,\n)\n\nwebsite_knowledge_base = WebsiteKnowledgeBase(\n    # Add URLs to the knowledge base\n    # urls=[\"https://docs.phidata.com/introduction\"],\n    # Number of links to follow from the seed URLs\n    max_links=15,\n    vector_db=PgVector2(\n        db_url=db_url,\n        # Store the embeddings in ai.website_documents\n        collection=\"website_documents\",\n        embedder=OpenAIEmbedder(model=ai_settings.embedding_model),\n    ),\n    # 3 references are added to the prompt\n    num_documents=3,\n)\n"}
{"type": "source_file", "path": "api/routes/v1_router.py", "content": "from fastapi import APIRouter\n\nfrom api.routes.status import status_router\nfrom api.routes.assistants import assistants_router\nfrom api.routes.hn import hn_router\n\nv1_router = APIRouter(prefix=\"/v1\")\nv1_router.include_router(status_router)\nv1_router.include_router(assistants_router)\nv1_router.include_router(hn_router)\nv1_router.include_router(assistants_router)\n"}
{"type": "source_file", "path": "db/tables/__init__.py", "content": "from db.tables.base import Base\n"}
{"type": "source_file", "path": "arxiv_ai/storage.py", "content": "from phi.storage.assistant.postgres import PgAssistantStorage\n\nfrom db.session import db_url\n\narxiv_assistant_storage = PgAssistantStorage(\n    schema=\"ai\",\n    db_url=db_url,\n    table_name=\"arxiv_assistant\",\n)\n\nlatent_space_arxiv_bot_storage = PgAssistantStorage(\n    schema=\"ai\",\n    db_url=db_url,\n    table_name=\"latent_space_arxiv_bot\",\n)\n"}
{"type": "source_file", "path": "arxiv_ai/assistant.py", "content": "from typing import Optional\n\nfrom phi.assistant import Assistant\nfrom phi.llm.openai import OpenAIChat\n\nfrom ai.settings import ai_settings\nfrom hn_ai.search import search_web\nfrom arxiv_ai.storage import arxiv_assistant_storage\nfrom arxiv_ai.tools import ArxivTools\nfrom arxiv_ai.knowledge import get_arxiv_knowledge_base_for_user\n\n\ndef get_arxiv_assistant(\n    user_id: str,\n    run_id: Optional[str] = None,\n    document_name: Optional[str] = None,\n    debug_mode: bool = False,\n) -> Assistant:\n    arxiv_tools = ArxivTools(user_id=user_id)\n\n    introduction = \"Hi, I am Arxiv AI, ask me about ArXiv papers.\"\n    instructions = [\n        \"You are made by phidata: https://github.com/phidatahq/phidata\",\n        f\"You are interacting with the user: {user_id}\",\n        \"Your goal is to answer questions from a knowledge base of ArXiv papers.\",\n    ]\n\n    if document_name is not None:\n        instructions.extend(\n            [\n                f\"The user is asking about the `document_name`: {document_name}\",\n                \"If the user asks a specific question, use the `search_document` tool to get context from the specific paper.\",\n                \"If the user asks the summarize the paper, use the `get_document_contents` tool to get the first 5000 words of the paper and provide the user with a concise and relevant answer.\",\n            ]\n        )\n\n    instructions.extend(\n        [\n            \"To answer the users question, ALWAYS FIRST run the `get_document_summaries` tool to search the knowledge base for relevant information.\",\n            \"Unless the user explicitly asks, ALWAYS FIRST search the knowledge base for relevant information.\",\n            \"If you do not find relevant information in the knowledge base, use the `search_arxiv_and_add_to_knowledge_base` tool to search arxiv about that topic.\"\n            \"Remember your instructions: first search knowledge base then search arxiv\",\n        ]\n    )\n\n    instructions.extend(\n        [\n            \"If the user asks more details, use the `get_document_contents` tool to get the first 5000 words of the paper and provide the user with a concise and relevant answer.\",\n            \"If the user is asking a question from a specific paper, use the `search_document` tool to get context from the specific paper.\",\n            \"If the user is asking about the content of the knowledge base use `get_document_names` tool to get the list of documents.\",\n            \"If the user provides a link then use `add_arxiv_papers_to_knowledge_base` tool to add the paper to the knowledge base.\",\n            \"You can also search the entire knowledge base using the `search_knowledge_base` tool.\",\n            \"Keep your conversation light hearted and fun.\",\n            \"Using information from the document, provide the user with a concise and relevant answer.\",\n            \"Only provide the user with information that is relevant to their question.\",\n            \"If you do not have information on the topic, use the `search_arxiv_and_add_to_knowledge_base` tool to search for the topic on ArXiv and add it to the knowledge base.\",\n            \"If you cannot find the information in the knowledge base or on arxiv, **THINK** if you can find it on the web. If you can find the information on the web, use the `search_web` tool\",\n            \"When searching the knowledge base, search for at least 3 documents.\",\n            \"When getting document contents, get at least 5000 words so you get the first few pages.\",\n            \"Always try to provide the user with links to the arxiv papers if available.\",\n            \"If the user compliments you, ask them to star phidata on GitHub: https://github.com/phidatahq/phidata\",\n        ]\n    )\n\n    return Assistant(\n        name=f\"arxiv_assistant_{user_id}\" if user_id else \"arxiv_assistant\",\n        run_id=run_id,\n        user_id=user_id,\n        llm=OpenAIChat(\n            model=ai_settings.gpt_4,\n            max_tokens=ai_settings.default_max_tokens,\n            temperature=ai_settings.default_temperature,\n        ),\n        storage=arxiv_assistant_storage,\n        monitoring=True,\n        use_tools=True,\n        introduction=introduction,\n        tools=[search_web, arxiv_tools],\n        knowledge_base=get_arxiv_knowledge_base_for_user(user_id),\n        show_tool_calls=True,\n        debug_mode=debug_mode,\n        description=\"Your name is Arxiv AI and you are a designed to help users understand technical ArXiv papers.\",\n        add_datetime_to_instructions=True,\n        instructions=instructions,\n        add_chat_history_to_messages=True,\n        num_history_messages=4,\n    )\n"}
{"type": "source_file", "path": "api/routes/assistants.py", "content": "from typing import Generator, Optional, List, Dict, Any, Literal\n\nfrom fastapi import APIRouter, HTTPException\nfrom fastapi.responses import StreamingResponse\nfrom phi.assistant import Assistant, AssistantRun\nfrom pydantic import BaseModel\n\nfrom api.routes.endpoints import endpoints\nfrom ai.assistants.pdf_rag import get_rag_pdf_assistant\nfrom ai.assistants.pdf_auto import get_autonomous_pdf_assistant\nfrom ai.storage import pdf_assistant_storage\nfrom utils.log import logger\n\n######################################################\n## Router for PDF Assistants\n######################################################\n\nassistants_router = APIRouter(prefix=endpoints.ASSISTANTS, tags=[\"Assistants\"])\nAssistantType = Literal[\"AUTO_PDF\", \"RAG_PDF\"]\n\n\ndef get_assistant(\n    assistant_type: AssistantType,\n    run_id: Optional[str] = None,\n    user_id: Optional[str] = None,\n):\n    \"\"\"Return the assistant\"\"\"\n\n    if assistant_type == \"AUTO_PDF\":\n        return get_autonomous_pdf_assistant(run_id=run_id, user_id=user_id)\n    elif assistant_type == \"RAG_PDF\":\n        return get_rag_pdf_assistant(run_id=run_id, user_id=user_id)\n\n\nclass LoadKnowledgeBaseRequest(BaseModel):\n    assistant: AssistantType = \"RAG_PDF\"\n\n\n@assistants_router.post(\"/load-knowledge-base\")\ndef load_knowledge_base(body: LoadKnowledgeBaseRequest):\n    \"\"\"Loads the knowledge base for an Assistant\"\"\"\n\n    assistant = get_assistant(assistant_type=body.assistant)\n    if assistant.knowledge_base:\n        assistant.knowledge_base.load(recreate=False)\n    return {\"message\": \"Knowledge Base Loaded\"}\n\n\nclass CreateRunRequest(BaseModel):\n    user_id: Optional[str] = None\n    assistant: AssistantType = \"RAG_PDF\"\n\n\nclass CreateRunResponse(BaseModel):\n    run_id: str\n    user_id: Optional[str] = None\n    chat_history: List[Dict[str, Any]]\n\n\n@assistants_router.post(\"/create\", response_model=CreateRunResponse)\ndef create_assistant_run(body: CreateRunRequest):\n    \"\"\"Create a new Assistant run and returns the run_id\"\"\"\n\n    logger.debug(f\"CreateRunRequest: {body}\")\n    assistant: Assistant = get_assistant(assistant_type=body.assistant, user_id=body.user_id)\n\n    # create_run() will log the run in the database and return the run_id\n    # which is returned to the frontend to retrieve the run later\n    run_id: Optional[str] = assistant.create_run()\n    if run_id is None:\n        raise HTTPException(status_code=500, detail=\"Failed to create assistant run\")\n    logger.debug(f\"Created Assistant Run: {run_id}\")\n\n    return CreateRunResponse(\n        run_id=run_id,\n        user_id=assistant.user_id,\n        chat_history=assistant.memory.get_chat_history(),\n    )\n\n\ndef chat_response_streamer(assistant: Assistant, message: str) -> Generator:\n    for chunk in assistant.run(message):\n        yield chunk\n\n\nclass ChatRequest(BaseModel):\n    message: str\n    stream: bool = True\n    run_id: Optional[str] = None\n    user_id: Optional[str] = None\n    assistant: AssistantType = \"RAG_PDF\"\n\n\n@assistants_router.post(\"/chat\")\ndef chat(body: ChatRequest):\n    \"\"\"Sends a message to an Assistant and returns the response\"\"\"\n\n    logger.debug(f\"ChatRequest: {body}\")\n    assistant: Assistant = get_assistant(\n        assistant_type=body.assistant, run_id=body.run_id, user_id=body.user_id\n    )\n\n    if body.stream:\n        return StreamingResponse(\n            chat_response_streamer(assistant, body.message),\n            media_type=\"text/event-stream\",\n        )\n    else:\n        return assistant.run(body.message, stream=False)\n\n\nclass ChatHistoryRequest(BaseModel):\n    run_id: str\n    user_id: Optional[str] = None\n    assistant: AssistantType = \"RAG_PDF\"\n\n\n@assistants_router.post(\"/history\", response_model=List[Dict[str, Any]])\ndef get_chat_history(body: ChatHistoryRequest):\n    \"\"\"Return the chat history for an Assistant run\"\"\"\n\n    logger.debug(f\"ChatHistoryRequest: {body}\")\n    assistant: Assistant = get_assistant(\n        assistant_type=body.assistant, run_id=body.run_id, user_id=body.user_id\n    )\n    # Load the assistant from the database\n    assistant.read_from_storage()\n\n    return assistant.memory.get_chat_history()\n\n\nclass GetAssistantRunRequest(BaseModel):\n    run_id: str\n    user_id: Optional[str] = None\n    assistant: AssistantType = \"RAG_PDF\"\n\n\n@assistants_router.post(\"/get\", response_model=Optional[AssistantRun])\ndef get_assistant_run(body: GetAssistantRunRequest):\n    \"\"\"Returns the Assistant run\"\"\"\n\n    logger.debug(f\"GetAssistantRunRequest: {body}\")\n    assistant: Assistant = get_assistant(\n        assistant_type=body.assistant, run_id=body.run_id, user_id=body.user_id\n    )\n\n    return assistant.read_from_storage()\n\n\nclass GetAllAssistantRunsRequest(BaseModel):\n    user_id: str\n\n\n@assistants_router.post(\"/get-all\", response_model=List[AssistantRun])\ndef get_assistants(body: GetAllAssistantRunsRequest):\n    \"\"\"Return all Assistant runs for a user\"\"\"\n\n    logger.debug(f\"GetAllAssistantRunsRequest: {body}\")\n    return pdf_assistant_storage.get_all_runs(user_id=body.user_id)\n\n\nclass GetAllAssistantRunIdsRequest(BaseModel):\n    user_id: str\n\n\n@assistants_router.post(\"/get-all-ids\", response_model=List[str])\ndef get_run_ids(body: GetAllAssistantRunIdsRequest):\n    \"\"\"Return all run_ids for a user\"\"\"\n\n    logger.debug(f\"GetAllAssistantRunIdsRequest: {body}\")\n    return pdf_assistant_storage.get_all_run_ids(user_id=body.user_id)\n\n\nclass RenameAssistantRunRequest(BaseModel):\n    run_id: str\n    run_name: str\n    user_id: Optional[str] = None\n    assistant: AssistantType = \"RAG_PDF\"\n\n\nclass RenameAssistantRunResponse(BaseModel):\n    run_id: str\n    run_name: str\n\n\n@assistants_router.post(\"/rename\", response_model=RenameAssistantRunResponse)\ndef rename_assistant(body: RenameAssistantRunRequest):\n    \"\"\"Rename an Assistant run\"\"\"\n\n    logger.debug(f\"RenameAssistantRunRequest: {body}\")\n    assistant: Assistant = get_assistant(\n        assistant_type=body.assistant, run_id=body.run_id, user_id=body.user_id\n    )\n    assistant.rename_run(body.run_name)\n\n    return RenameAssistantRunResponse(\n        run_id=assistant.run_id,\n        run_name=assistant.run_name,\n    )\n\n\nclass AutoRenameAssistantRunRequest(BaseModel):\n    run_id: str\n    user_id: Optional[str] = None\n    assistant: AssistantType = \"RAG_PDF\"\n\n\nclass AutoRenameAssistantRunResponse(BaseModel):\n    run_id: str\n    run_name: str\n\n\n@assistants_router.post(\"/autorename\", response_model=AutoRenameAssistantRunResponse)\ndef autorename_assistant(body: AutoRenameAssistantRunRequest):\n    \"\"\"Rename a assistant using the LLM\"\"\"\n\n    logger.debug(f\"AutoRenameAssistantRunRequest: {body}\")\n    assistant: Assistant = get_assistant(\n        assistant_type=body.assistant, run_id=body.run_id, user_id=body.user_id\n    )\n    assistant.auto_rename_run()\n\n    return RenameAssistantRunResponse(\n        run_id=assistant.run_id,\n        run_name=assistant.run_name,\n    )\n"}
{"type": "source_file", "path": "api/settings.py", "content": "from typing import List, Optional\n\nfrom pydantic import field_validator, Field\nfrom pydantic_settings import BaseSettings\nfrom pydantic_core.core_schema import FieldValidationInfo\n\n\nclass ApiSettings(BaseSettings):\n    \"\"\"Api settings that can be set using environment variables.\n\n    Reference: https://pydantic-docs.helpmanual.io/usage/settings/\n    \"\"\"\n\n    # Api title and version\n    title: str = \"ai-api\"\n    version: str = \"1.0\"\n\n    # Api runtime_env derived from the `runtime_env` environment variable.\n    # Valid values include \"dev\", \"stg\", \"prd\"\n    runtime_env: str = \"dev\"\n\n    # Set to False to disable docs at /docs and /redoc\n    docs_enabled: bool = True\n\n    # Cors origin list to allow requests from.\n    # This list is set using the set_cors_origin_list validator\n    # which uses the runtime_env variable to set the\n    # default cors origin list.\n    cors_origin_list: Optional[List[str]] = Field(None, validate_default=True)\n\n    @field_validator(\"runtime_env\")\n    def validate_runtime_env(cls, runtime_env):\n        \"\"\"Validate runtime_env.\"\"\"\n\n        valid_runtime_envs = [\"dev\", \"stg\", \"prd\"]\n        if runtime_env not in valid_runtime_envs:\n            raise ValueError(f\"Invalid runtime_env: {runtime_env}\")\n\n        return runtime_env\n\n    @field_validator(\"cors_origin_list\", mode=\"before\")\n    def set_cors_origin_list(cls, cors_origin_list, info: FieldValidationInfo):\n        valid_cors = cors_origin_list or []\n\n        # Add phidata to cors origin list\n        valid_cors.extend([\"https://phidata.app\", \"https://www.phidata.app\"])\n\n        runtime_env = info.data.get(\"runtime_env\")\n        if runtime_env == \"dev\":\n            # 8501 is the default port for streamlit\n            # 3000 is the default port for create-react-app\n            valid_cors.extend([\"http://localhost:8501\", \"http://localhost:3000\"])\n\n        return valid_cors\n\n\n# Create ApiSettings object\napi_settings = ApiSettings()\n"}
{"type": "source_file", "path": "app/pages/2_Image_Assistant.py", "content": "import base64\nfrom io import BytesIO\nfrom typing import List\n\nimport streamlit as st\nfrom PIL import Image\nfrom phi.assistant import Assistant\nfrom phi.tools.streamlit.components import (\n    get_openai_key_sidebar,\n    check_password,\n    reload_button_sidebar,\n    get_username_sidebar,\n)\n\nfrom ai.assistants.image import get_image_assistant\nfrom utils.log import logger\n\nst.set_page_config(\n    page_title=\"Image AI\",\n    page_icon=\":orange_heart:\",\n)\nst.title(\"Image Assistant\")\nst.markdown(\"##### :orange_heart: built using [phidata](https://github.com/phidatahq/phidata)\")\n\n\ndef encode_image(image_file):\n    image = Image.open(image_file)\n    buffer = BytesIO()\n    image.save(buffer, format=\"JPEG\")\n    encoding = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n    return f\"data:image/jpeg;base64,{encoding}\"\n\n\ndef restart_assistant():\n    st.session_state[\"image_assistant\"] = None\n    st.session_state[\"image_assistant_run_id\"] = None\n    st.session_state[\"file_uploader_key\"] += 1\n    st.session_state[\"uploaded_image\"] = None\n    st.rerun()\n\n\ndef main() -> None:\n    # Get OpenAI key from environment variable or user input\n    get_openai_key_sidebar()\n\n    # Get username\n    username = get_username_sidebar()\n    if username:\n        st.sidebar.info(f\":technologist: User: {username}\")\n    else:\n        st.write(\":technologist: Please enter a username\")\n        return\n\n    # Get the assistant\n    image_assistant: Assistant\n    if \"image_assistant\" not in st.session_state or st.session_state[\"image_assistant\"] is None:\n        logger.info(\"---*--- Creating Vision Assistant ---*---\")\n        image_assistant = get_image_assistant(\n            user_id=username,\n            debug_mode=False,\n        )\n        st.session_state[\"image_assistant\"] = image_assistant\n    else:\n        image_assistant = st.session_state[\"image_assistant\"]\n\n    # Create assistant run (i.e. log to database) and save run_id in session state\n    st.session_state[\"image_assistant_run_id\"] = image_assistant.create_run()\n\n    # Store uploaded image in session state\n    uploaded_image = None\n    if \"uploaded_image\" in st.session_state:\n        uploaded_image = st.session_state[\"uploaded_image\"]\n\n    # Load messages for existing assistant\n    assistant_chat_history = image_assistant.memory.get_chat_history()\n    if len(assistant_chat_history) > 0:\n        logger.debug(\"Loading chat history\")\n        st.session_state[\"messages\"] = assistant_chat_history\n        # Search for uploaded image\n        if uploaded_image is None:\n            for message in assistant_chat_history:\n                if message[\"role\"] == \"user\":\n                    for item in message[\"content\"]:\n                        if item[\"type\"] == \"image_url\":\n                            uploaded_image = item[\"image_url\"][\"url\"]\n                            st.session_state[\"uploaded_image\"] = uploaded_image\n                            break\n    else:\n        logger.debug(\"No chat history found\")\n        st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"Ask me about the image...\"}]\n\n    # Upload Image if not available\n    if uploaded_image is None:\n        if \"file_uploader_key\" not in st.session_state:\n            st.session_state[\"file_uploader_key\"] = 0\n        uploaded_file = st.sidebar.file_uploader(\n            \"Upload Image\",\n            key=st.session_state[\"file_uploader_key\"],\n        )\n        if uploaded_file is not None:\n            alert = st.sidebar.info(\"Processing Image...\", icon=\"ℹ️\")\n            image_file_name = uploaded_file.name.split(\".\")[0]\n            if f\"{image_file_name}_uploaded\" not in st.session_state:\n                logger.info(f\"Encoding {image_file_name}\")\n                uploaded_image = encode_image(uploaded_file)\n                st.session_state[\"uploaded_image\"] = uploaded_image\n                st.session_state[f\"{image_file_name}_uploaded\"] = True\n            alert.empty()\n\n    # Prompt for user input\n    if uploaded_image:\n        st.image(uploaded_image, use_column_width=True)\n        if prompt := st.chat_input():\n            vision_message = [\n                {\n                    \"type\": \"text\",\n                    \"text\": prompt,\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": uploaded_image,\n                        \"detail\": \"low\",\n                    },\n                },\n            ]\n            st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": vision_message})\n\n    if st.sidebar.button(\"New Run\"):\n        restart_assistant()\n\n    if uploaded_image:\n        if st.sidebar.button(\"Generate Caption\"):\n            caption_message = [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Generate a caption for this image\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": uploaded_image,\n                        \"detail\": \"low\",\n                    },\n                },\n            ]\n            st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": caption_message})\n\n        if st.sidebar.button(\"Describe Image\"):\n            caption_message = [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Describe this image in 2 sentences\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": uploaded_image,\n                        \"detail\": \"low\",\n                    },\n                },\n            ]\n            st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": caption_message})\n\n        if st.sidebar.button(\"Identify Brands\"):\n            brands_message = [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"List the brands in this image. This is only for demo and testing purposes.\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": uploaded_image,\n                        \"detail\": \"low\",\n                    },\n                },\n            ]\n            st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": brands_message})\n\n        if st.sidebar.button(\"Identify Items\"):\n            items_message = [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"List the items in this image\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": uploaded_image,\n                        \"detail\": \"low\",\n                    },\n                },\n            ]\n            st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": items_message})\n\n    # Display existing chat messages\n    for message in st.session_state[\"messages\"]:\n        if message[\"role\"] == \"system\":\n            continue\n        with st.chat_message(message[\"role\"]):\n            content = message.get(\"content\")\n            if isinstance(content, list):\n                for item in content:\n                    if item[\"type\"] == \"text\":\n                        st.write(item[\"text\"])\n                    # elif item[\"type\"] == \"image_url\":\n                    #     st.image(item[\"image_url\"][\"url\"], use_column_width=True)\n            else:\n                st.write(content)\n\n    # If last message is from a user, generate a new response\n    last_message = st.session_state[\"messages\"][-1]\n    if last_message.get(\"role\") == \"user\":\n        question = last_message[\"content\"]\n        with st.chat_message(\"assistant\"):\n            response = \"\"\n            resp_container = st.empty()\n            for delta in image_assistant.run(question):\n                response += delta  # type: ignore\n                resp_container.markdown(response)\n            st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\n\n    if image_assistant.storage:\n        image_assistant_run_ids: List[str] = image_assistant.storage.get_all_run_ids(user_id=username)\n        new_image_assistant_run_id = st.sidebar.selectbox(\"Run ID\", options=image_assistant_run_ids)\n        if st.session_state[\"image_assistant_run_id\"] != new_image_assistant_run_id:\n            logger.debug(f\"Loading run {new_image_assistant_run_id}\")\n            logger.info(\"---*--- Loading Vision Assistant ---*---\")\n            st.session_state[\"image_assistant\"] = get_image_assistant(\n                user_id=username,\n                run_id=new_image_assistant_run_id,\n                debug_mode=False,\n            )\n            st.rerun()\n\n    # Show reload button\n    reload_button_sidebar()\n\n\nif check_password():\n    main()\n"}
{"type": "source_file", "path": "app/pages/3_Website_Assistant.py", "content": "from typing import List\n\nimport streamlit as st\nfrom phi.assistant import Assistant\nfrom phi.knowledge.website import WebsiteKnowledgeBase\nfrom phi.tools.streamlit.components import (\n    get_openai_key_sidebar,\n    check_password,\n    reload_button_sidebar,\n    get_username_sidebar,\n)\n\nfrom ai.assistants.website_auto import get_autonomous_website_assistant\nfrom ai.assistants.website_rag import get_rag_website_assistant\nfrom utils.log import logger\n\nst.set_page_config(\n    page_title=\"Website AI\",\n    page_icon=\":orange_heart:\",\n)\nst.title(\"Website Assistant\")\nst.markdown(\"##### :orange_heart: built using [phidata](https://github.com/phidatahq/phidata)\")\nwith st.expander(\":rainbow[:point_down: Example Questions]\"):\n    st.markdown(\"- What is phidata?\")\n    st.markdown(\"- How do I build an AI App?\")\n\n\ndef restart_assistant():\n    st.session_state[\"website_assistant\"] = None\n    st.session_state[\"website_assistant_run_id\"] = None\n    st.rerun()\n\n\ndef main() -> None:\n    # Get OpenAI key from environment variable or user input\n    get_openai_key_sidebar()\n\n    # Get username\n    username = get_username_sidebar()\n    if username:\n        st.sidebar.info(f\":technologist: User: {username}\")\n    else:\n        st.write(\":technologist: Please enter a username\")\n        return\n\n    # Get assistant type\n    website_assistant_type = st.sidebar.selectbox(\"Assistant Type\", options=[\"Autonomous\", \"RAG\"])\n    # Set assistant_type in session state\n    if \"website_assistant_type\" not in st.session_state:\n        st.session_state[\"website_assistant_type\"] = website_assistant_type\n    # Restart the assistant if assistant_type has changed\n    elif st.session_state[\"website_assistant_type\"] != website_assistant_type:\n        st.session_state[\"website_assistant_type\"] = website_assistant_type\n        restart_assistant()\n\n    # Get the assistant\n    website_assistant: Assistant\n    if \"website_assistant\" not in st.session_state or st.session_state[\"website_assistant\"] is None:\n        if st.session_state[\"website_assistant_type\"] == \"Autonomous\":\n            logger.info(\"---*--- Creating Autonomous Assistant ---*---\")\n            website_assistant = get_autonomous_website_assistant(\n                user_id=username,\n                debug_mode=True,\n            )\n        else:\n            logger.info(\"---*--- Creating RAG Assistant ---*---\")\n            website_assistant = get_rag_website_assistant(\n                user_id=username,\n                debug_mode=True,\n            )\n        st.session_state[\"website_assistant\"] = website_assistant\n    else:\n        website_assistant = st.session_state[\"website_assistant\"]\n\n    # Create assistant run (i.e. log to database) and save run_id in session state\n    st.session_state[\"website_assistant_run_id\"] = website_assistant.create_run()\n\n    # Check if knowlege base exists\n    if website_assistant.knowledge_base and (\n        \"website_knowledge_base_loaded\" not in st.session_state\n        or not st.session_state[\"website_knowledge_base_loaded\"]\n    ):\n        if not website_assistant.knowledge_base.exists():\n            loading_container = st.sidebar.info(\"🧠 Loading knowledge base\")\n            website_assistant.knowledge_base.load()\n            st.session_state[\"website_knowledge_base_loaded\"] = True\n            st.sidebar.success(\"Knowledge Base loaded\")\n            loading_container.empty()\n\n    # Load messages for existing assistant\n    assistant_chat_history = website_assistant.memory.get_chat_history()\n    if len(assistant_chat_history) > 0:\n        logger.debug(\"Loading chat history\")\n        st.session_state[\"messages\"] = assistant_chat_history\n    else:\n        logger.debug(\"No chat history found\")\n        st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"Ask me anything...\"}]\n\n    # Prompt for user input\n    if prompt := st.chat_input():\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n\n    # Display existing chat messages\n    for message in st.session_state[\"messages\"]:\n        if message[\"role\"] == \"system\":\n            continue\n        with st.chat_message(message[\"role\"]):\n            st.write(message[\"content\"])\n\n    # If last message is from a user, generate a new response\n    last_message = st.session_state[\"messages\"][-1]\n    if last_message.get(\"role\", \"\") == \"user\":\n        question = last_message[\"content\"]\n        with st.chat_message(\"assistant\"):\n            response = \"\"\n            resp_container = st.empty()\n            for delta in website_assistant.run(question):\n                response += delta  # type: ignore\n                resp_container.markdown(response)\n\n            st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\n\n    if st.sidebar.button(\"New Run\"):\n        restart_assistant()\n\n    if website_assistant.knowledge_base:\n        if st.sidebar.button(\"Update Knowledge Base\", disabled=True):\n            website_assistant.knowledge_base.load(recreate=False)\n            st.session_state[\"knowledge_base_exists\"] = True\n            st.sidebar.success(\"Knowledge base updated\")\n\n        if st.sidebar.button(\"Recreate Knowledge Base\", disabled=True):\n            website_assistant.knowledge_base.load(recreate=True)\n            st.session_state[\"knowledge_base_exists\"] = True\n            st.sidebar.success(\"Knowledge base recreated\")\n\n        if st.sidebar.button(\"Clear Knowledge Base\", disabled=True):\n            website_assistant.knowledge_base.vector_db.clear()\n            st.session_state[\"pdf_knowledge_base_loaded\"] = False\n            st.sidebar.success(\"Knowledge base cleared\")\n\n    if st.sidebar.button(\"Auto Rename\"):\n        website_assistant.auto_rename_run()\n\n    # Add websites to knowledge base\n    website_knowledge_base: WebsiteKnowledgeBase = website_assistant.knowledge_base  # type: ignore\n    if website_knowledge_base:\n        website_url = st.sidebar.text_input(\"Add Website to Knowledge Base\")\n        if website_url != \"\":\n            if website_url not in website_knowledge_base.urls:\n                website_knowledge_base.urls.append(website_url)\n                loading_container = st.sidebar.info(f\"🧠 Loading {website_url}\")\n                website_knowledge_base.load()\n                st.session_state[\"website_knowledge_base_loaded\"] = True\n                loading_container.empty()\n\n    if website_assistant.storage:\n        website_assistant_run_ids: List[str] = website_assistant.storage.get_all_run_ids(user_id=username)\n        new_website_assistant_run_id = st.sidebar.selectbox(\"Run ID\", options=website_assistant_run_ids)\n        if st.session_state[\"website_assistant_run_id\"] != new_website_assistant_run_id:\n            logger.debug(f\"Loading run {new_website_assistant_run_id}\")\n            if st.session_state[\"website_assistant_type\"] == \"Autonomous\":\n                logger.info(\"---*--- Loading as Autonomous Assistant ---*---\")\n                st.session_state[\"website_assistant\"] = get_autonomous_website_assistant(\n                    user_id=username,\n                    run_id=new_website_assistant_run_id,\n                    debug_mode=True,\n                )\n            else:\n                logger.info(\"---*--- Loading as RAG Assistant ---*---\")\n                st.session_state[\"website_assistant\"] = get_rag_website_assistant(\n                    user_id=username,\n                    run_id=new_website_assistant_run_id,\n                    debug_mode=True,\n                )\n            st.rerun()\n\n    website_assistant_run_name = website_assistant.run_name\n    if website_assistant_run_name:\n        st.sidebar.write(f\":thread: {website_assistant_run_name}\")\n\n    # Show reload button\n    reload_button_sidebar()\n\n\nif check_password():\n    main()\n"}
{"type": "source_file", "path": "app/__init__.py", "content": ""}
{"type": "source_file", "path": "arxiv_ai/ls/discuss.py", "content": "from typing import Optional\n\nfrom arxiv import Result as ArxivPaper\nfrom phi.assistant import Assistant, AssistantRun\nfrom phi.llm.openai import OpenAIChat\nfrom phi.tools.resend_toolkit import ResendToolkit\n\nfrom ai.settings import ai_settings\nfrom arxiv_ai.tools import ArxivTools\nfrom arxiv_ai.storage import latent_space_arxiv_bot_storage\nfrom utils.log import logger\n\n\ndef get_discussion_assistant(\n    user_id: str,\n    thread_id: str,\n    paper: Optional[ArxivPaper] = None,\n    debug_mode: bool = True,\n) -> Optional[Assistant]:\n    arxiv_tools = ArxivTools(user_id=\"latent_space\")\n    paper_data = None\n    paper_title = None\n    paper_id = None\n    if paper is None:\n        assistant_run: Optional[AssistantRun] = latent_space_arxiv_bot_storage.read(run_id=thread_id)\n        if assistant_run is not None:\n            paper_data = assistant_run.run_data.get(\"paper\", None)\n            logger.info(f\"Paper found in run data: {paper_data}\")\n    else:\n        paper_title = paper.title\n        paper_id = paper.get_short_id()\n\n    if paper_title is None:\n        if paper_data is None:\n            return None\n        paper_title = paper_data.get(\"title\")\n        paper_id = paper_data.get(\"id\")\n\n    if paper_id is None:\n        return None\n\n    try:\n        paper_content = arxiv_tools.get_document_contents(paper_id, limit=100)\n        if paper_content == \"\":\n            arxiv_tools.add_arxiv_papers_to_knowledge_base([paper_id])\n    except Exception as e:\n        logger.error(e)\n\n    instructions = [\n        \"You are made by phidata: https://github.com/phidatahq/phidata\",\n        f\"You are interacting with the user: `{user_id}`\",\n        f\"Your goal is to help the user answer questions about the ArXiv paper `title: {paper_title}` | `name: {paper_id}`\",\n        \"If the user asks to summarize, use the `get_document_contents` tool to get the first 15000 characters and return a summary of the paper in 3 bullet points or less\",\n        \"The audience has knowledge of the field, so focus on the main contributions and findings of the paper\",\n        \"Mention statistics and significant wins of the paper\",\n        \"If the users asks questions from the paper, use the `search_document` tool.\",\n        \"If the users asks to send an email, always ask the user for their email address and then use the `send_email` tool to send the email.\",\n        \"Remember: DO NOT SEND AN EMAIL TO THE USER WITHOUT THEM PROVIDING THEIR EMAIL ADDRESS\",\n        \"Make sure your email body is formatted using HTML\",\n        \"Remind the user to check their spam folder if they do not receive the email\",\n        \"When you use `get_document_contents` tool, get at least 5000 characters so you get the first few pages.\",\n    ]\n\n    return Assistant(\n        name=\"latent_space_arxiv_discussion\",\n        run_id=thread_id,\n        user_id=user_id,\n        llm=OpenAIChat(\n            model=ai_settings.gpt_4,\n            max_tokens=ai_settings.default_max_tokens,\n            temperature=ai_settings.default_temperature,\n        ),\n        storage=latent_space_arxiv_bot_storage,\n        monitoring=True,\n        use_tools=True,\n        debug_mode=debug_mode,\n        tools=[arxiv_tools, ResendToolkit(from_email=\"arxiv-ai@phidata.com\")],\n        instructions=instructions,\n        description=\"Your name is Arxiv AI and you are a designed to help users understand technical ArXiv papers.\",\n        add_chat_history_to_messages=True,\n        num_history_messages=4,\n        run_data={\"paper\": {\"title\": paper_title, \"id\": paper_id}},\n    )\n"}
{"type": "source_file", "path": "db/session.py", "content": "from sqlalchemy.engine import Engine, create_engine\nfrom sqlalchemy.orm import Session, sessionmaker\n\nfrom db.settings import db_settings\n\n# Create SQLAlchemy Engine using a database URL\ndb_url = db_settings.get_db_url()\ndb_engine: Engine = create_engine(db_url, pool_pre_ping=True)\n\n# Create a SessionLocal class\n# https://fastapi.tiangolo.com/tutorial/sql-databases/#create-a-sessionlocal-class\nSessionLocal: sessionmaker[Session] = sessionmaker(autocommit=False, autoflush=False, bind=db_engine)\n\n\ndef get_db():\n    \"\"\"\n    Dependency to get a database session.\n\n    https://fastapi.tiangolo.com/tutorial/sql-databases/#create-a-dependency\n    \"\"\"\n\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n"}
{"type": "source_file", "path": "api/__init__.py", "content": ""}
{"type": "source_file", "path": "db/migrations/env.py", "content": "from logging.config import fileConfig\n\nfrom sqlalchemy import engine_from_config\nfrom sqlalchemy import pool\n\nfrom alembic import context\n\nfrom db.tables import Base\nfrom db.session import db_url\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\nconfig.set_main_option(\"sqlalchemy.url\", db_url)\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = Base.metadata\n\n\n# -*- Only include tables that are in the target_metadata\n# See: https://alembic.sqlalchemy.org/en/latest/autogenerate.html#omitting-table-names-from-the-autogenerate-process\ndef include_name(name, type_, parent_names):\n    if type_ == \"table\":\n        return name in target_metadata.tables\n    else:\n        return True\n\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        include_name=include_name,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n        version_table_schema=target_metadata.schema,\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section, {}),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=target_metadata,\n            include_name=include_name,\n            version_table_schema=target_metadata.schema,\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n"}
{"type": "source_file", "path": "demos/sales_analysis/knowledge_base.py", "content": ""}
{"type": "source_file", "path": "app/Home.py", "content": "import streamlit as st\n\nfrom phi.tools.streamlit.components import check_password\n\nst.set_page_config(\n    page_title=\"AI Apps\",\n    page_icon=\":snowman:\",\n)\nst.title(\":snowman: AI Apps\")\nst.markdown('<a href=\"https://github.com/phidatahq/phidata\"><h4>by phidata</h4></a>', unsafe_allow_html=True)\n\n\ndef main() -> None:\n    st.markdown(\"### Select an App:\")\n    st.markdown(\"#### 1. PDF Assistant: Chat with PDFs\")\n    st.markdown(\"#### 2. Image Assistant: Chat with images\")\n    st.markdown(\"#### 3. Website Assistant: Chat with website contents\")\n\n    st.sidebar.success(\"Select App from above\")\n\n\nif check_password():\n    main()\n"}
{"type": "source_file", "path": "ai/storage.py", "content": "from phi.storage.assistant.postgres import PgAssistantStorage\n\nfrom db.session import db_url\n\npdf_assistant_storage = PgAssistantStorage(\n    db_url=db_url,\n    table_name=\"pdf_assistant\",\n)\n\nimage_assistant_storage = PgAssistantStorage(\n    db_url=db_url,\n    table_name=\"image_assistant\",\n)\n\nwebsite_assistant_storage = PgAssistantStorage(\n    db_url=db_url,\n    table_name=\"website_assistant\",\n)\n"}
{"type": "source_file", "path": "arxiv_ai/ls/bot.py", "content": "from os import getenv\nfrom typing import Set\n\nfrom discord import Intents, Client, Message\nfrom arxiv_ai.ls.message import handle_mention, handle_message\n\nfrom utils.log import logger\n\n\ndef run():\n    \"\"\"Runs the ArXiv AI bot.\"\"\"\n\n    intents = Intents.default()\n    intents.message_content = True\n    client = Client(intents=intents)\n\n    client.active_threads: Set[int] = set()\n\n    @client.event\n    async def on_ready():\n        logger.info(f\"Logged in as {client.user}\")\n\n    @client.event\n    async def on_message(message: Message):\n        if message.author == client.user:\n            return\n\n        if message.mentions and client.user.mentioned_in(message):\n            await handle_mention(message=message, client=client)\n        else:\n            await handle_message(message=message, client=client)\n\n        return\n\n    client.run(getenv(\"ARXIV_AI_TOKEN\"), root_logger=True)\n\n\nif __name__ == \"__main__\":\n    run()\n"}
{"type": "source_file", "path": "arxiv_ai/ls/summary.py", "content": "import json\nfrom textwrap import dedent\n\nfrom arxiv import Result as ArxivPaper\nfrom phi.assistant import Assistant\nfrom phi.llm.openai import OpenAIChat\nfrom phi.tools.resend_toolkit import ResendToolkit\n\nfrom ai.settings import ai_settings\nfrom arxiv_ai.tools import ArxivTools\nfrom arxiv_ai.storage import latent_space_arxiv_bot_storage\nfrom utils.log import logger\n\n\ndef get_summary_assistant(\n    user_id: str,\n    thread_id: str,\n    paper: ArxivPaper,\n    debug_mode: bool = True,\n) -> Assistant:\n    paper_details = {\n        \"title\": paper.title,\n        \"entry_id\": paper.entry_id,\n        \"updated\": paper.updated.isoformat() if paper.updated else None,\n        \"authors\": [author.name for author in paper.authors],\n        \"primary_category\": paper.primary_category,\n        \"categories\": paper.categories,\n        \"published\": paper.published.isoformat() if paper.published else None,\n        \"pdf_url\": paper.pdf_url,\n        \"links\": [link.href for link in paper.links],\n        \"summary\": paper.summary,\n        \"comment\": paper.comment,\n    }\n\n    arxiv_tools = ArxivTools(user_id=\"latent_space\")\n    try:\n        paper_content = arxiv_tools.get_document_contents(paper.get_short_id(), limit=10000)\n    except Exception as e:\n        logger.error(e)\n        paper_content = \"\"\n\n    try:\n        if paper_content == \"\":\n            arxiv_tools.add_arxiv_papers_to_knowledge_base([paper.get_short_id()])\n            paper_content = arxiv_tools.get_document_contents(paper.get_short_id(), limit=10000)\n    except Exception as e:\n        logger.error(e)\n        paper_content = \"\"\n\n    instructions = [\n        \"You are made by phidata: https://github.com/phidatahq/phidata\",\n        f\"You are interacting with the user: `{user_id}`\",\n        \"Your goal is to provide a summary of the following technical paper surrounded by the <arxiv_paper></arxiv_paper> tags.\",\n        \"Provide your summary in 3 bullet points or less\",\n        \"The audience has knowledge of the field, so focus on the main contributions and findings of the paper\",\n        \"Mention statistics and significant wins of the paper\",\n        \"You will also be provided with the first 10000 characters of the paper to help you provide a relevant answer\",\n    ]\n\n    return Assistant(\n        name=\"latent_space_arxiv_summary\",\n        run_id=thread_id,\n        user_id=user_id,\n        llm=OpenAIChat(\n            model=ai_settings.gpt_4,\n            max_tokens=ai_settings.default_max_tokens,\n            temperature=ai_settings.default_temperature,\n        ),\n        storage=latent_space_arxiv_bot_storage,\n        monitoring=True,\n        use_tools=True,\n        debug_mode=debug_mode,\n        tools=[arxiv_tools, ResendToolkit(from_email=\"arxiv-ai@phidata.com\")],\n        instructions=instructions,\n        description=\"Your name is Arxiv AI and you are a designed to help users understand technical ArXiv papers.\",\n        add_to_system_prompt=dedent(\n            \"\"\"\\\n            <arxiv_paper>\n            {}\n            </arxiv_paper>\n\n            <paper_content>\n            {}\n            </paper_content>\n            \"\"\".format(json.dumps(paper_details, indent=4), paper_content)\n        ),\n        run_data={\"paper\": {\"title\": paper.title, \"id\": paper.get_short_id()}},\n    )\n"}
{"type": "source_file", "path": "arxiv_ai/knowledge.py", "content": "from typing import Optional, List, Tuple\n\nfrom phi.knowledge import AssistantKnowledge\nfrom phi.embedder.openai import OpenAIEmbedder\nfrom phi.vectordb.pgvector import PgVector2\n\nfrom db.session import db_url\nfrom utils.log import logger\n\n\ndef get_arxiv_summary_knowledge_base_for_user(user_id: Optional[str] = None) -> AssistantKnowledge:\n    table_name = f\"arxiv_summary_{user_id}\" if user_id else \"arxiv_summary\"\n    return AssistantKnowledge(\n        vector_db=PgVector2(\n            schema=\"ai\",\n            db_url=db_url,\n            collection=table_name,\n            embedder=OpenAIEmbedder(model=\"text-embedding-3-small\"),\n        ),\n        num_documents=20,\n    )\n\n\ndef get_arxiv_knowledge_base_for_user(user_id: Optional[str] = None) -> AssistantKnowledge:\n    table_name = f\"arxiv_knowledge_{user_id}\" if user_id else \"arxiv_knowledge\"\n    return AssistantKnowledge(\n        vector_db=PgVector2(\n            schema=\"ai\",\n            db_url=db_url,\n            collection=table_name,\n            embedder=OpenAIEmbedder(model=\"text-embedding-3-small\"),\n        ),\n        num_documents=5,\n    )\n\n\ndef get_available_docs(summary_knowledge_base: AssistantKnowledge) -> Optional[List[Tuple[str, str]]]:\n    if summary_knowledge_base.vector_db is None or not isinstance(\n        summary_knowledge_base.vector_db, PgVector2\n    ):\n        return None\n\n    vector_db = summary_knowledge_base.vector_db\n    table = vector_db.table\n    with vector_db.Session() as session, session.begin():\n        try:\n            query = session.query(table)\n            result = session.execute(query)\n            rows = result.fetchall()\n\n            if rows is None:\n                return None\n\n            documents = []\n            for row in rows:\n                document_id = row.id\n                document_title = row.meta_data[\"title\"]\n                documents.append((document_id, document_title))\n            return documents\n        except Exception as e:\n            logger.error(f\"Error getting document names: {e}\")\n            return None\n"}
{"type": "source_file", "path": "db/settings.py", "content": "from os import getenv\nfrom typing import Optional\n\nfrom pydantic_settings import BaseSettings\n\nfrom utils.log import logger\n\n\nclass DbSettings(BaseSettings):\n    \"\"\"Database settings that can be set using environment variables.\n\n    Reference: https://docs.pydantic.dev/latest/usage/pydantic_settings/\n    \"\"\"\n\n    # Database configuration\n    db_host: Optional[str] = None\n    db_port: Optional[int] = None\n    db_user: Optional[str] = None\n    db_pass: Optional[str] = None\n    db_database: Optional[str] = None\n    db_driver: str = \"postgresql+psycopg\"\n    # Create/Upgrade database on startup using alembic\n    migrate_db: bool = False\n\n    def get_db_url(self) -> str:\n        db_url = \"{}://{}{}@{}:{}/{}\".format(\n            self.db_driver,\n            self.db_user,\n            f\":{self.db_pass}\" if self.db_pass else \"\",\n            self.db_host,\n            self.db_port,\n            self.db_database,\n        )\n        # Use local database if RUNTIME_ENV is not set\n        if \"None\" in db_url and getenv(\"RUNTIME_ENV\") is None:\n            from workspace.dev_resources import dev_db\n\n            logger.debug(\"Using local connection\")\n            local_db_url = dev_db.get_db_connection_local()\n            if local_db_url:\n                db_url = local_db_url\n\n        # Validate database connection\n        if \"None\" in db_url or db_url is None:\n            raise ValueError(\"Could not build database connection\")\n        return db_url\n\n\n# Create DbSettings object\ndb_settings = DbSettings()\n"}
{"type": "source_file", "path": "arxiv_ai/tools.py", "content": "import json\nfrom typing import List, Optional\nfrom pathlib import Path\n\nimport arxiv\nfrom pypdf import PdfReader\nfrom phi.document import Document\nfrom phi.tools import ToolRegistry\nfrom phi.knowledge import AssistantKnowledge\nfrom phi.vectordb.pgvector import PgVector2\n\nfrom arxiv_ai.knowledge import get_arxiv_knowledge_base_for_user, get_arxiv_summary_knowledge_base_for_user\nfrom workspace.settings import ws_settings\nfrom utils.log import logger\n\n\nclass ArxivTools(ToolRegistry):\n    def __init__(self, user_id: str):\n        super().__init__(name=\"arxiv_tools\")\n\n        self.client: arxiv.Client = arxiv.Client()\n        self.user_id: str = user_id\n        self.summary_knowledge_base: AssistantKnowledge = get_arxiv_summary_knowledge_base_for_user(\n            user_id=user_id\n        )\n        self.knowledge_base: AssistantKnowledge = get_arxiv_knowledge_base_for_user(user_id=user_id)\n        self.storage_dir: Path = ws_settings.ws_root.joinpath(ws_settings.storage_dir)\n        self.register(self.add_arxiv_papers_to_knowledge_base)\n        self.register(self.search_arxiv_and_add_to_knowledge_base)\n        self.register(self.get_document_summaries)\n        self.register(self.search_document)\n        self.register(self.get_document_contents)\n        self.register(self.get_document_titles)\n        if not self.storage_dir.exists():\n            self.storage_dir.mkdir(parents=True, exist_ok=True)\n\n    def add_arxiv_papers_to_knowledge_base(self, id_list: List[str]) -> str:\n        \"\"\"\n        Use this function to add a list of arxiv papers to the knowledge base.\n\n        Args:\n            id_list (list, str): The list of `id` of the papers to add to the knowledge base.\n            Should be of the format: [\"2103.03404v1\", \"2103.03404v2\"]\n\n        Returns:\n            str: If success or failure, returns a message.\n        \"\"\"\n        logger.debug(f\"Searching arxiv for: {id_list}\")\n\n        all_result_documents = []\n        document_summaries = []\n        for result in self.client.results(search=arxiv.Search(id_list=id_list)):\n            try:\n                result_documents = []\n                meta_data = {\n                    \"title\": result.title,\n                    \"name\": result.get_short_id(),\n                    \"entry_id\": result.entry_id,\n                    \"updated\": result.updated.isoformat() if result.updated else None,\n                    \"authors\": [author.name for author in result.authors],\n                    \"primary_category\": result.primary_category,\n                    \"categories\": result.categories,\n                    \"published\": result.published.isoformat() if result.published else None,\n                    \"pdf_url\": result.pdf_url,\n                    \"links\": [link.href for link in result.links],\n                }\n\n                document_summary = meta_data.copy()\n                document_summary[\"summary\"] = result.summary\n                document_summary[\"comment\"] = result.comment\n                document_summaries.append(\n                    Document(\n                        id=result.get_short_id(),\n                        name=result.get_short_id(),\n                        meta_data=meta_data,\n                        content=json.dumps(document_summary),\n                    )\n                )\n\n                result_documents.append(\n                    Document(\n                        id=result.get_short_id(),\n                        name=result.get_short_id(),\n                        meta_data=meta_data,\n                        content=json.dumps(document_summary),\n                    )\n                )\n                if result.pdf_url:\n                    logger.info(f\"Downloading: {result.pdf_url}\")\n                    pdf_path = result.download_pdf(dirpath=str(self.storage_dir))\n                    logger.info(f\"Downloaded: {pdf_path}\")\n                    pdf_reader = PdfReader(pdf_path)\n                    for page_number, page in enumerate(pdf_reader.pages, start=1):\n                        _page_content = page.extract_text()\n                        if _page_content:\n                            _meta_data = meta_data.copy()\n                            _meta_data[\"page\"] = page_number\n                            _id = f\"{result.get_short_id()}__{page_number}\"\n                            result_documents.append(\n                                Document(\n                                    id=_id,\n                                    name=result.get_short_id(),\n                                    meta_data=_meta_data,\n                                    content=_page_content,\n                                )\n                            )\n\n                if result_documents:\n                    all_result_documents.extend(result_documents)\n            except Exception as e:\n                logger.error(f\"Error creating document for paper {result.entry_id}: {e}\")\n\n        logger.info(f\"Found {len(document_summaries)} results for: {id_list}\")\n        try:\n            logger.info(f\"Loading {len(all_result_documents)} documents for id_list: {id_list}\")\n            self.knowledge_base.load_documents(all_result_documents, upsert=True)\n\n            logger.info(f\"Loading {len(document_summaries)} document summaries for id_list: {id_list}\")\n            self.summary_knowledge_base.load_documents(document_summaries, upsert=True)\n        except Exception as e:\n            logger.error(f\"Error loading documents for id_list: {id_list}: {e}\")\n            return f\"Error loading documents for id: {id_list}: {e}\"\n\n        return json.dumps([doc.to_dict() for doc in document_summaries])\n\n    def search_arxiv_and_add_to_knowledge_base(self, query: str, num_results: int = 5) -> str:\n        \"\"\"Use this function to adds papers from arXiv that match a string query.\n\n        Args:\n            query (str): The query to get arXiv papers for.\n            num_results (int): The number of papers to add to knowledge base. Defaults to 10.\n\n        Returns:\n            str: A summary of the papers added to the knowledge base.\n        \"\"\"\n        logger.debug(f\"Searching arxiv for: {query}\")\n\n        all_result_documents = []\n        document_summaries = []\n        for result in self.client.results(\n            search=arxiv.Search(\n                query=query,\n                max_results=num_results,\n                sort_by=arxiv.SortCriterion.Relevance,\n                sort_order=arxiv.SortOrder.Descending,\n            )\n        ):\n            try:\n                result_documents = []\n                meta_data = {\n                    \"title\": result.title,\n                    \"name\": result.get_short_id(),\n                    \"entry_id\": result.entry_id,\n                    \"updated\": result.updated.isoformat() if result.updated else None,\n                    \"authors\": [author.name for author in result.authors],\n                    \"primary_category\": result.primary_category,\n                    \"categories\": result.categories,\n                    \"published\": result.published.isoformat() if result.published else None,\n                    \"pdf_url\": result.pdf_url,\n                    \"links\": [link.href for link in result.links],\n                }\n\n                document_summary = meta_data.copy()\n                document_summary[\"summary\"] = result.summary\n                document_summary[\"comment\"] = result.comment\n                document_summaries.append(\n                    Document(\n                        id=result.get_short_id(),\n                        name=result.get_short_id(),\n                        meta_data=meta_data,\n                        content=json.dumps(document_summary),\n                    )\n                )\n\n                result_documents.append(\n                    Document(\n                        id=result.get_short_id(),\n                        name=result.get_short_id(),\n                        meta_data=meta_data,\n                        content=json.dumps(document_summary),\n                    )\n                )\n                if result.pdf_url:\n                    logger.info(f\"Downloading: {result.pdf_url}\")\n                    pdf_path = result.download_pdf(dirpath=str(self.storage_dir))\n                    logger.info(f\"Downloaded: {pdf_path}\")\n                    pdf_reader = PdfReader(pdf_path)\n                    for page_number, page in enumerate(pdf_reader.pages, start=1):\n                        _page_content = page.extract_text()\n                        if _page_content:\n                            _meta_data = meta_data.copy()\n                            _meta_data[\"page\"] = page_number\n                            _id = f\"{result.get_short_id()}__{page_number}\"\n                            result_documents.append(\n                                Document(\n                                    id=_id,\n                                    name=result.get_short_id(),\n                                    meta_data=_meta_data,\n                                    content=_page_content,\n                                )\n                            )\n\n                if result_documents:\n                    all_result_documents.extend(result_documents)\n            except Exception as e:\n                logger.error(f\"Error creating document for paper {result.entry_id}: {e}\")\n\n        logger.info(f\"Found {len(document_summaries)} results for: {query}\")\n        try:\n            logger.info(f\"Loading {len(all_result_documents)} documents for query: {query}\")\n            self.knowledge_base.load_documents(all_result_documents, upsert=True)\n\n            logger.info(f\"Loading {len(document_summaries)} document summaries for query: {query}\")\n            self.summary_knowledge_base.load_documents(document_summaries, upsert=True)\n        except Exception as e:\n            logger.error(f\"Error loading documents for query: {query}: {e}\")\n            return f\"Error loading documents for query: {query}: {e}\"\n\n        return json.dumps([doc.to_dict() for doc in document_summaries])\n\n    def get_document_summaries(self, query: str, limit: int = 10) -> Optional[str]:\n        \"\"\"Use this function to get a summary of documents available in the knowledge base.\n\n        Args:\n            query (str): The query to match document summaries with.\n            limit (int): Maximum number of documents to return. Defaults to 30.\n\n        Returns:\n            str: JSON string of the document summaries\n        \"\"\"\n\n        logger.debug(f\"Getting summaries relevant to: {query}\")\n        try:\n            relevant_documents = self.summary_knowledge_base.search(query=query, num_documents=limit)\n            return json.dumps([doc.to_dict() for doc in relevant_documents])\n        except Exception as e:\n            logger.error(f\"Error getting summaries for query: {query}: {e}\")\n            return \"No documents found for query: {query}\"\n\n    def search_document(self, query: str, document_name: str, num_documents: int = 5) -> Optional[str]:\n        \"\"\"Use this function to search a particular arXiv document with name=document_name for a query.\n\n        Args:\n            query (str): Query to search for\n            document_name (str): Name of the document to search\n            num_documents (int): Number of results to return. Defaults to 5.\n\n        Returns:\n            str: JSON string of the search results\n        \"\"\"\n\n        logger.debug(f\"Searching document {document_name} for query: {query}\")\n        if self.knowledge_base.vector_db is None or not isinstance(self.knowledge_base.vector_db, PgVector2):\n            return \"Sorry could not search latest document\"\n\n        search_results: List[Document] = self.knowledge_base.vector_db.search(\n            query=query, limit=num_documents, filters={\"name\": document_name}\n        )\n        logger.debug(f\"Search result: {search_results}\")\n\n        if len(search_results) == 0:\n            return f\"Sorry could not find any results from document: {document_name}\"\n\n        return json.dumps([doc.to_dict() for doc in search_results])\n\n    def get_document_contents(self, document_name: str, limit: int = 10000) -> Optional[str]:\n        \"\"\"Use this function to get the content of a particular arXiv document with name=document_name.\n\n        Args:\n            document_name (str): Name of the document to search. Eg: \"1706.03762v7\"\n            limit (int): Maximum number of characters to return. Defaults to 10000.\n\n        Returns:\n            str: JSON string of the document contents\n        \"\"\"\n\n        logger.debug(f\"Getting document contents: {document_name}\")\n        if self.knowledge_base.vector_db is None or not isinstance(self.knowledge_base.vector_db, PgVector2):\n            return \"Vector DB not found.\"\n\n        vector_db: PgVector2 = self.knowledge_base.vector_db\n        table = vector_db.table\n        try:\n            with vector_db.Session() as session, session.begin():\n                document_query = (\n                    session.query(table)\n                    .filter(table.c.name == document_name)\n                    .order_by(table.c.created_at.desc())\n                )\n                document_result = session.execute(document_query)\n                document_rows = document_result.fetchall()\n                document_content = \"\"\n                for document_row in document_rows:\n                    document_content += document_row.content\n\n                return document_content[:limit]\n        except Exception as e:\n            logger.error(f\"Error getting document contents: {e}\")\n            logger.error(\"Table might not exist, creating for future use\")\n            vector_db.create()\n            return \"\"\n\n    def get_document_titles(self, limit: int = 20) -> Optional[str]:\n        \"\"\"Use this function to get the titles of the documents uploaded from ArXiv.\n\n        Args:\n            limit (int): Maximum number of documents to return. Defaults to 20.\n\n        Returns:\n            str: JSON string of the document names\n        \"\"\"\n\n        logger.debug(\"Getting all document titles from the knowledge base.\")\n        if self.knowledge_base.vector_db is None or not isinstance(self.knowledge_base.vector_db, PgVector2):\n            return \"No documents found in the knowledge base.\"\n\n        vector_db: PgVector2 = self.knowledge_base.vector_db\n        table = vector_db.table\n        with vector_db.Session() as session, session.begin():\n            try:\n                query = session.query(table).distinct(table.c.name).limit(limit)\n                result = session.execute(query)\n                rows = result.fetchall()\n\n                if rows is None:\n                    return \"Sorry could not find any documents\"\n\n                document_titles = []\n                for row in rows:\n                    document_title = row.meta_data[\"title\"]\n                    document_titles.append(document_title)\n\n                return json.dumps(document_titles)\n            except Exception as e:\n                logger.error(f\"Error getting document names: {e}\")\n                return \"No documents found in the knowledge base.\"\n"}
{"type": "source_file", "path": "arxiv_ai/ls/message.py", "content": "from typing import List\n\nimport arxiv\nfrom arxiv import Result as ArxivPaper\nfrom discord import Client, Message\nfrom discord.enums import ChannelType\nfrom discord.threads import Thread\nfrom arxiv_ai.ls.discuss import get_discussion_assistant\nfrom utils.log import logger\n\n\nasync def discuss(arxiv_url: str, message: Message, client: Client):\n    logger.info(f\"Discussing: {arxiv_url}\")\n\n    # -*- Get Result from ArXiv\n    try:\n        paper_id = arxiv_url.split(\"/\")[-1]\n        paper: ArxivPaper = next(arxiv.Client().results(arxiv.Search(id_list=[paper_id])))\n    except Exception as e:\n        logger.error(e)\n        await message.reply(\"Sorry, could not find this paper.\")\n        return\n\n    try:\n        thread = await message.create_thread(name=paper.title)\n    except Exception as e:\n        logger.error(e)\n        # await message.reply(\"Sorry, I was not able to create a thread.\")\n        return\n\n    # -*- Create run in the database\n    thread_id: int = thread.id\n    user_name: str = message.author.name\n    discussion_assistant = get_discussion_assistant(user_id=user_name, thread_id=str(thread_id), paper=paper)\n    if discussion_assistant is None:\n        await message.reply(\"Sorry, I was not able to create a thread. Please try again.\")\n        return\n    discussion_assistant.create_run()\n\n    # -*- Follow up\n    await thread.send(f\"How can I help with: `{paper.title}`\")\n\n\nasync def handle_mention(message: Message, client: Client):\n    user_name: str = message.author.name\n    user_message: str = message.content\n    server: str = message.guild.name\n    channel: str = message.channel.name\n    message_parts = message.content.split()\n    logger.info(f'{user_name} said: \"{user_message}\" in #{channel} ({server})')\n\n    # -*- Check that the channel is not a thread. This prevents the bot from creating threads in threads.\n    if message.channel.type != ChannelType.text:\n        await message.reply(\"Please mention me in a channel to start a new discussion.\")\n        return\n\n    if len(message_parts) != 3:\n        await message.reply(\"Hi, please use `@ArxivAI discuss <paper>` to interact with me.\")\n        return\n\n    if message_parts[1].lower() != \"discuss\":\n        await message.reply(\"Hi, please use `@ArxivAI discuss <paper>` to interact with me.\")\n        return\n\n    if not message_parts[2].startswith(\"https://arxiv.org/abs/\"):\n        await message.reply(\n            \"Please provide a valid arXiv paper URL. Example: `https://arxiv.org/abs/1706.03762`\"\n        )\n\n    if message_parts[1].lower() == \"discuss\":\n        return await discuss(arxiv_url=message_parts[2], message=message, client=client)\n    else:\n        await message.reply(\"Hi, please use `@ArxivAI discuss <paper>` to interact with me.\")\n\n    return\n\n\nasync def handle_message(message: Message, client: Client):\n    # -*- Check that this message in a thread\n    if message.channel.type != ChannelType.public_thread:\n        return\n\n    thread: Thread = message.channel\n    thread_id: int = thread.id\n\n    # -*- Make sure thread owner is the bot\n    if thread.owner != client.user:\n        return\n\n    if thread_id in client.active_threads:\n        await message.reply(\n            \"Sorry, I am already working on a request in this thread. Please wait for the current request to finish. And then ask your question.\"\n        )\n        return\n\n    client.active_threads.add(thread_id)\n\n    user_name: str = message.author.name\n    user_message: str = message.content\n    server: str = message.guild.name\n    channel: str = message.channel.name\n    logger.info(f'{user_name} said: \"{user_message}\" in thread: {channel} ({server})')\n\n    # -*- Start the LLM summarization\n    discussion_assistant = get_discussion_assistant(user_id=user_name, thread_id=str(thread_id))\n\n    try:\n        if discussion_assistant is None:\n            await message.reply(\"Sorry, I was not able to process your request. Please start again.\")\n            client.active_threads.remove(thread_id)\n            return\n        await thread.send(\"... working\")\n        logger.info(f\"Message: {user_message}\")\n        response = discussion_assistant.run(message=user_message, stream=False)\n\n        if len(response) < 1900:\n            await message.reply(response)\n        else:\n            chunked_response = await chunk_text(response)\n            for chunk in chunked_response:\n                await thread.send(chunk)\n    except Exception as e:\n        logger.error(e)\n        await message.reply(\"Sorry, I was not able to process your request. Please start again.\")\n        client.active_threads.remove(thread_id)\n        return\n\n    client.active_threads.remove(thread_id)\n\n\nasync def chunk_text(text, max_length=1900):\n    \"\"\"\n    Chunk a large text into parts, each not exceeding max_length characters,\n    ensuring each chunk (except the last one) ends with a full stop. This version\n    keeps the original formatting of the input text.\n\n    :param text: The text to be chunked.\n    :param max_length: Maximum length of each chunk.\n    :return: A list of text chunks.\n    \"\"\"\n    chunks: List = []\n    current_chunk: str = \"\"\n    current_length: int = 0\n\n    words = text.split(\" \")  # Split by space to keep punctuation attached to words\n    for word in words:\n        # Include space in length calculation if current_chunk is not empty\n        word_length_with_space = len(word) + (1 if current_chunk else 0)\n\n        if current_length + word_length_with_space <= max_length:\n            # Add a space before the word if current_chunk is not empty\n            current_chunk += \" \" + word if current_chunk else word\n            current_length += word_length_with_space\n        else:\n            # End the chunk at the last full stop\n            last_full_stop = current_chunk.rfind(\".\")\n            if last_full_stop != -1 and current_length != max_length:\n                # Include the sentence ending in the current chunk\n                chunks.append(current_chunk[: last_full_stop + 1])\n                # Start a new chunk with the remaining text and the current word\n                current_chunk = current_chunk[last_full_stop + 1 :].lstrip() + \" \" + word\n                current_length = len(current_chunk)\n            else:\n                # Add the chunk as is and start a new one\n                chunks.append(current_chunk)\n                current_chunk = word\n                current_length = len(word)\n\n    # Add the last chunk without looking for a full stop\n    if current_chunk:\n        chunks.append(current_chunk)\n\n    return chunks\n"}
{"type": "source_file", "path": "db/__init__.py", "content": ""}
{"type": "source_file", "path": "app/pages/1_PDF_Assistant.py", "content": "from typing import List\n\nimport streamlit as st\nfrom phi.assistant import Assistant\nfrom phi.document import Document\nfrom phi.document.reader.pdf import PDFReader\nfrom phi.tools.streamlit.components import (\n    get_openai_key_sidebar,\n    check_password,\n    reload_button_sidebar,\n    get_username_sidebar,\n)\n\nfrom ai.assistants.pdf_auto import get_autonomous_pdf_assistant\nfrom ai.assistants.pdf_rag import get_rag_pdf_assistant\nfrom utils.log import logger\n\nst.set_page_config(\n    page_title=\"PDF AI\",\n    page_icon=\":orange_heart:\",\n)\nst.title(\"PDF Assistant\")\nst.markdown(\"##### :orange_heart: built using [phidata](https://github.com/phidatahq/phidata)\")\nwith st.expander(\":rainbow[:point_down: Example Questions]\"):\n    st.markdown(\"- How do I make chicken tikka salad?\")\n    st.markdown(\"- How do I make chicken curry?\")\n    st.markdown(\"- How do I make a chicken wrap?\")\n\n\ndef restart_assistant():\n    st.session_state[\"pdf_assistant\"] = None\n    st.session_state[\"pdf_assistant_run_id\"] = None\n    st.session_state[\"file_uploader_key\"] += 1\n    st.rerun()\n\n\ndef main() -> None:\n    # Get OpenAI key from environment variable or user input\n    get_openai_key_sidebar()\n\n    # Get username\n    username = get_username_sidebar()\n    if username:\n        st.sidebar.info(f\":technologist: User: {username}\")\n    else:\n        st.write(\":technologist: Please enter a username\")\n        return\n\n    # Get assistant type\n    pdf_assistant_type = st.sidebar.selectbox(\"Assistant Type\", options=[\"Autonomous\", \"RAG\"])\n    # Set assistant_type in session state\n    if \"pdf_assistant_type\" not in st.session_state:\n        st.session_state[\"pdf_assistant_type\"] = pdf_assistant_type\n    # Restart the assistant if assistant_type has changed\n    elif st.session_state[\"pdf_assistant_type\"] != pdf_assistant_type:\n        st.session_state[\"pdf_assistant_type\"] = pdf_assistant_type\n        restart_assistant()\n\n    # Get the assistant\n    pdf_assistant: Assistant\n    if \"pdf_assistant\" not in st.session_state or st.session_state[\"pdf_assistant\"] is None:\n        if st.session_state[\"pdf_assistant_type\"] == \"Autonomous\":\n            logger.info(\"---*--- Creating Autonomous Assistant ---*---\")\n            pdf_assistant = get_autonomous_pdf_assistant(\n                user_id=username,\n                debug_mode=True,\n            )\n        else:\n            logger.info(\"---*--- Creating RAG Assistant ---*---\")\n            pdf_assistant = get_rag_pdf_assistant(\n                user_id=username,\n                debug_mode=True,\n            )\n        st.session_state[\"pdf_assistant\"] = pdf_assistant\n    else:\n        pdf_assistant = st.session_state[\"pdf_assistant\"]\n\n    # Create assistant run (i.e. log to database) and save run_id in session state\n    st.session_state[\"pdf_assistant_run_id\"] = pdf_assistant.create_run()\n\n    # Check if knowlege base exists\n    if pdf_assistant.knowledge_base and (\n        \"pdf_knowledge_base_loaded\" not in st.session_state\n        or not st.session_state[\"pdf_knowledge_base_loaded\"]\n    ):\n        if not pdf_assistant.knowledge_base.exists():\n            logger.info(\"Knowledge base does not exist\")\n            loading_container = st.sidebar.info(\"🧠 Loading knowledge base\")\n            pdf_assistant.knowledge_base.load()\n            st.session_state[\"pdf_knowledge_base_loaded\"] = True\n            st.sidebar.success(\"Knowledge base loaded\")\n            loading_container.empty()\n\n    # Load messages for existing assistant\n    assistant_chat_history = pdf_assistant.memory.get_chat_history()\n    if len(assistant_chat_history) > 0:\n        logger.debug(\"Loading chat history\")\n        st.session_state[\"messages\"] = assistant_chat_history\n    else:\n        logger.debug(\"No chat history found\")\n        st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"Ask me anything...\"}]\n\n    # Prompt for user input\n    if prompt := st.chat_input():\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n\n    # Display existing chat messages\n    for message in st.session_state[\"messages\"]:\n        if message[\"role\"] == \"system\":\n            continue\n        with st.chat_message(message[\"role\"]):\n            st.write(message[\"content\"])\n\n    # If last message is from a user, generate a new response\n    last_message = st.session_state[\"messages\"][-1]\n    if last_message.get(\"role\") == \"user\":\n        question = last_message[\"content\"]\n        with st.chat_message(\"assistant\"):\n            response = \"\"\n            resp_container = st.empty()\n            for delta in pdf_assistant.run(question):\n                response += delta  # type: ignore\n                resp_container.markdown(response)\n\n            st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\n\n    if st.sidebar.button(\"New Run\"):\n        restart_assistant()\n\n    if pdf_assistant.knowledge_base:\n        if st.sidebar.button(\"Update Knowledge Base\", disabled=True):\n            pdf_assistant.knowledge_base.load(recreate=False)\n            st.session_state[\"pdf_knowledge_base_loaded\"] = True\n            st.sidebar.success(\"Knowledge base updated\")\n\n        if st.sidebar.button(\"Recreate Knowledge Base\", disabled=True):\n            pdf_assistant.knowledge_base.load(recreate=True)\n            st.session_state[\"pdf_knowledge_base_loaded\"] = True\n            st.sidebar.success(\"Knowledge base recreated\")\n\n        if st.sidebar.button(\"Clear Knowledge Base\", disabled=True):\n            pdf_assistant.knowledge_base.vector_db.clear()\n            st.session_state[\"pdf_knowledge_base_loaded\"] = False\n            st.sidebar.success(\"Knowledge base cleared\")\n\n    if st.sidebar.button(\"Auto Rename\"):\n        pdf_assistant.auto_rename_run()\n\n    # Upload PDF\n    if pdf_assistant.knowledge_base:\n        if \"file_uploader_key\" not in st.session_state:\n            st.session_state[\"file_uploader_key\"] = 0\n\n        uploaded_file = st.sidebar.file_uploader(\n            \"Upload PDF\",\n            type=\"pdf\",\n            key=st.session_state[\"file_uploader_key\"],\n        )\n        if uploaded_file is not None:\n            alert = st.sidebar.info(\"Processing PDF...\", icon=\"ℹ️\")\n            pdf_name = uploaded_file.name.split(\".\")[0]\n            if f\"{pdf_name}_uploaded\" not in st.session_state:\n                reader = PDFReader()\n                pdf_documents: List[Document] = reader.read(uploaded_file)\n                if pdf_documents:\n                    pdf_assistant.knowledge_base.load_documents(pdf_documents)\n                else:\n                    st.sidebar.error(\"Could not read PDF\")\n                st.session_state[f\"{pdf_name}_uploaded\"] = True\n            alert.empty()\n\n    if pdf_assistant.storage:\n        pdf_assistant_run_ids: List[str] = pdf_assistant.storage.get_all_run_ids(user_id=username)\n        new_pdf_assistant_run_id = st.sidebar.selectbox(\"Run ID\", options=pdf_assistant_run_ids)\n        if st.session_state[\"pdf_assistant_run_id\"] != new_pdf_assistant_run_id:\n            logger.debug(f\"Loading run {new_pdf_assistant_run_id}\")\n            if st.session_state[\"pdf_assistant_type\"] == \"Autonomous\":\n                logger.info(\"---*--- Loading as Autonomous Assistant ---*---\")\n                st.session_state[\"pdf_assistant\"] = get_autonomous_pdf_assistant(\n                    user_id=username,\n                    run_id=new_pdf_assistant_run_id,\n                    debug_mode=True,\n                )\n            else:\n                logger.info(\"---*--- Loading as RAG Assistant ---*---\")\n                st.session_state[\"pdf_assistant\"] = get_rag_pdf_assistant(\n                    user_id=username,\n                    run_id=new_pdf_assistant_run_id,\n                    debug_mode=True,\n                )\n            st.rerun()\n\n    pdf_assistant_run_name = pdf_assistant.run_name\n    if pdf_assistant_run_name:\n        st.sidebar.write(f\":thread: {pdf_assistant_run_name}\")\n\n    # Show reload button\n    reload_button_sidebar()\n\n\nif check_password():\n    main()\n"}
{"type": "source_file", "path": "hn_ai/knowledge.py", "content": "import json\n\nfrom phi.document import Document\nfrom phi.knowledge import AssistantKnowledge\nfrom phi.embedder.openai import OpenAIEmbedder\nfrom phi.vectordb.pgvector import PgVector2\nfrom phi.utils.log import set_log_level_to_debug\n\nfrom hn_ai.api import HackerNews\nfrom utils.log import logger\n\nfrom db.session import db_url\n\nhn_knowledge_base = AssistantKnowledge(\n    vector_db=PgVector2(\n        schema=\"ai\",\n        db_url=db_url,\n        collection=\"hn_documents\",\n        embedder=OpenAIEmbedder(model=\"text-embedding-3-small\"),\n    ),\n    num_documents=10,\n)\n\n\ndef load_hackernews_knowledge_base() -> str:\n    hn = HackerNews()\n    set_log_level_to_debug()\n\n    logger.info(\"Loading HackerNews knowledge base...\")\n    top_stories = hn.top_stories(limit=2000)\n    logger.info(f\"Fetched {len(top_stories)} top stories from HackerNews.\")\n    show_stories = hn.show_stories(limit=100)\n    logger.info(f\"Fetched {len(show_stories)} show stories from HackerNews.\")\n    ask_stories = hn.ask_stories(limit=50)\n    logger.info(f\"Fetched {len(ask_stories)} ask stories from HackerNews.\")\n    all_stories = top_stories + show_stories + ask_stories\n    logger.info(\"Creating Documents...\")\n\n    documents = []\n    for story in all_stories:\n        try:\n            meta_data = {\n                \"id\": story.item_id,\n                \"type\": story.item_type,\n                \"author\": story.by,\n                \"score\": story.score,\n                \"total_comments\": story.descendants,\n                \"time\": story.time.isoformat(),\n            }\n            if story.parent:\n                meta_data[\"parent\"] = story.parent\n\n            content = {\n                \"title\": story.title,\n                \"url\": story.url,\n                \"author\": story.by,\n            }\n            if story.text:\n                content[\"text\"] = story.text\n\n            documents.append(\n                Document(\n                    id=str(story.item_id),\n                    name=str(story.item_id),\n                    meta_data=meta_data,\n                    content=json.dumps(content),\n                )\n            )\n        except Exception as e:\n            logger.error(f\"Error creating document for story {story.item_id}: {e}\")\n    logger.info(\"Adding Documents to knowledge base...\")\n    # hn_knowledge_base.vector_db.delete()\n    hn_knowledge_base.load_documents(documents, upsert=True)\n    return f\"Loaded {len(documents)} documents to HackerNews knowledge base.\"\n"}
{"type": "source_file", "path": "hn_ai/__init__.py", "content": ""}
{"type": "source_file", "path": "hn_ai/api.py", "content": "from typing import Optional\nimport asyncio\nimport datetime\nimport json\nfrom urllib.parse import urljoin\n\nimport requests\nimport aiohttp\n\n\nclass InvalidItemID(Exception):\n    pass\n\n\nclass InvalidUserID(Exception):\n    pass\n\n\nclass InvalidAPIVersion(Exception):\n    pass\n\n\nclass HTTPError(Exception):\n    pass\n\n\nclass HackerNews:\n    def __init__(self):\n        self.base_url = \"https://hacker-news.firebaseio.com/v0/\"\n        self.item_url = urljoin(self.base_url, \"item/\")\n        self.user_url = urljoin(self.base_url, \"user/\")\n        self.session = requests.Session()\n\n    def _get_sync(self, url):\n        \"\"\"Internal method used for GET requests\n\n        Args:\n            url (str): URL to fetch\n\n        Returns:\n            Individual URL request's response\n\n        Raises:\n          HTTPError: If HTTP request failed.\n        \"\"\"\n        response = self.session.get(url)\n        if response.status_code == requests.codes.ok:\n            return response.json()\n        else:\n            raise HTTPError\n\n    async def _get_async(self, url, session):\n        \"\"\"Asynchronous internal method used for GET requests\n\n        Args:\n            url (str): URL to fetch\n            session (obj): aiohttp client session for async loop\n\n        Returns:\n            data (obj): Individual URL request's response corountine\n\n        \"\"\"\n        data = None\n        async with session.get(url) as resp:\n            if resp.status == 200:\n                data = await resp.json()\n        return data\n\n    async def _async_loop(self, urls):\n        \"\"\"Asynchronous internal method used to request multiple URLs\n\n        Args:\n            urls (list): URLs to fetch\n\n        Returns:\n            responses (obj): All URL requests' response coroutines\n\n        \"\"\"\n        results = []\n        async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session:\n            for url in urls:\n                result = asyncio.ensure_future(self._get_async(url, session))\n                results.append(result)\n            responses = await asyncio.gather(*results)\n        return responses\n\n    def _run_async(self, urls):\n        \"\"\"Asynchronous event loop execution\n\n        Args:\n            urls (list): URLs to fetch\n\n        Returns:\n            results (obj): All URL requests' responses\n\n        \"\"\"\n        return asyncio.run(self._async_loop(urls))\n\n    def _get_stories(self, page, limit):\n        \"\"\"\n        Hacker News has different categories (i.e. stories) like\n        'topstories', 'newstories', 'askstories', 'showstories', 'jobstories'.\n        This method, first fetches the relevant story ids of that category\n\n        The URL is: https://hacker-news.firebaseio.com/v0/<story_name>.json\n\n        e.g. https://hacker-news.firebaseio.com/v0/topstories.json\n\n        Then, asynchronously it fetches each story and returns the Item objects\n\n        The URL for individual story is:\n            https://hacker-news.firebaseio.com/v0/item/<item_id>.json\n\n        e.g. https://hacker-news.firebaseio.com/v0/item/69696969.json\n\n        \"\"\"\n        url = urljoin(self.base_url, f\"{page}.json\")\n        story_ids = self._get_sync(url)[:limit]\n        return self.get_items_by_ids(item_ids=story_ids)\n\n    def get_item(self, item_id, expand=False):\n        \"\"\"Returns Hacker News `Item` object.\n\n        Fetches the data from url:\n            https://hacker-news.firebaseio.com/v0/item/<item_id>.json\n\n        e.g. https://hacker-news.firebaseio.com/v0/item/69696969.json\n\n        Args:\n            item_id (int or string): Unique item id of Hacker News story,\n            comment etc.\n            expand (bool): expand (bool): Flag to indicate whether to\n                transform all IDs into objects.\n\n        Returns:\n            `Item` object representing Hacker News item.\n\n        Raises:\n          InvalidItemID: If corresponding Hacker News story does not exist.\n\n        \"\"\"\n        url = urljoin(self.item_url, f\"{item_id}.json\")\n        response = self._get_sync(url)\n\n        if not response:\n            raise InvalidItemID\n\n        item = Item(response)\n        if expand:\n            item.by = self.get_user(item.by)\n            item.kids = self.get_items_by_ids(item.kids) if item.kids else None\n            item.parent = self.get_item(item.parent) if item.parent else None\n            item.poll = self.get_item(item.poll) if item.poll else None\n            item.parts = self.get_items_by_ids(item.parts) if item.parts else None\n\n        return item\n\n    def get_items_by_ids(self, item_ids, item_type=None):\n        \"\"\"Given a list of item ids, return all the Item objects\n\n        Args:\n            item_ids (obj): List of item IDs to query\n            item_type (str): (optional) Item type to filter results with\n\n        Returns:\n            List of `Item` objects for given item IDs and given item type\n\n        \"\"\"\n        urls = [urljoin(self.item_url, f\"{i}.json\") for i in item_ids]\n        result = self._run_async(urls=urls)\n        items = [Item(r) for r in result if r]\n        if item_type:\n            return [item for item in items if item.item_type == item_type]\n        else:\n            return items\n\n    def get_user(self, user_id, expand=False):\n        \"\"\"Returns Hacker News `User` object.\n\n        Fetches data from the url:\n            https://hacker-news.firebaseio.com/v0/user/<user_id>.json\n\n        e.g. https://hacker-news.firebaseio.com/v0/user/pg.json\n\n        Args:\n            user_id (string): unique user id of a Hacker News user.\n            expand (bool): Flag to indicate whether to\n                transform all IDs into objects.\n\n        Returns:\n            `User` object representing a user on Hacker News.\n\n        Raises:\n          InvalidUserID: If no such user exists on Hacker News.\n\n        \"\"\"\n        url = urljoin(self.user_url, f\"{user_id}.json\")\n        response = self._get_sync(url)\n\n        if not response:\n            raise InvalidUserID\n\n        user = User(response)\n        if expand and user.submitted:\n            items = self.get_items_by_ids(user.submitted)\n            user_opt = {\n                \"stories\": \"story\",\n                \"comments\": \"comment\",\n                \"jobs\": \"job\",\n                \"polls\": \"poll\",\n                \"pollopts\": \"pollopt\",\n            }\n            for key, value in user_opt.items():\n                setattr(user, key, [i for i in items if i.item_type == value])\n\n        return user\n\n    def get_users_by_ids(self, user_ids):\n        \"\"\"\n        Given a list of user ids, return all the User objects\n        \"\"\"\n        urls = [urljoin(self.user_url, f\"{i}.json\") for i in user_ids]\n        result = self._run_async(urls=urls)\n        return [User(r) for r in result if r]\n\n    def top_stories(self, raw=False, limit=None):\n        \"\"\"Returns list of item ids of current top stories\n\n        Args:\n            limit (int): specifies the number of stories to be returned.\n            raw (bool): Flag to indicate whether to represent all\n                objects in raw json.\n\n        Returns:\n            `list` object containing ids of top stories.\n\n        \"\"\"\n        top_stories = self._get_stories(\"topstories\", limit)\n        if raw:\n            top_stories = [story.raw for story in top_stories]\n        return top_stories\n\n    def new_stories(self, raw=False, limit=None):\n        \"\"\"Returns list of item ids of current new stories\n\n        Args:\n            limit (int): specifies the number of stories to be returned.\n            raw (bool): Flag to indicate whether to transform all\n                objects into raw json.\n\n        Returns:\n            `list` object containing ids of new stories.\n\n        \"\"\"\n        new_stories = self._get_stories(\"newstories\", limit)\n        if raw:\n            new_stories = [story.raw for story in new_stories]\n        return new_stories\n\n    def ask_stories(self, raw=False, limit=None):\n        \"\"\"Returns list of item ids of latest Ask HN stories\n\n        Args:\n            limit (int): specifies the number of stories to be returned.\n            raw (bool): Flag to indicate whether to transform all\n                objects into raw json.\n\n        Returns:\n            `list` object containing ids of Ask HN stories.\n\n        \"\"\"\n        ask_stories = self._get_stories(\"askstories\", limit)\n        if raw:\n            ask_stories = [story.raw for story in ask_stories]\n        return ask_stories\n\n    def show_stories(self, raw=False, limit=None):\n        \"\"\"Returns list of item ids of latest Show HN stories\n\n        Args:\n            limit (int): specifies the number of stories to be returned.\n            raw (bool): Flag to indicate whether to transform all\n                objects into raw json.\n\n        Returns:\n            `list` object containing ids of Show HN stories.\n\n        \"\"\"\n        show_stories = self._get_stories(\"showstories\", limit)\n        if raw:\n            show_stories = [story.raw for story in show_stories]\n        return show_stories\n\n    def job_stories(self, raw=False, limit=None):\n        \"\"\"Returns list of item ids of latest Job stories\n\n        Args:\n            limit (int): specifies the number of stories to be returned.\n            raw (bool): Flag to indicate whether to transform all\n                objects into raw json.\n\n        Returns:\n            `list` object containing ids of Job stories.\n\n        \"\"\"\n        job_stories = self._get_stories(\"jobstories\", limit)\n        if raw:\n            job_stories = [story.raw for story in job_stories]\n        return job_stories\n\n    def updates(self):\n        \"\"\"Returns list of item ids and user ids that have been\n        changed/updated recently.\n\n        Fetches data from URL:\n            https://hacker-news.firebaseio.com/v0/updates.json\n\n        Returns:\n            `dict` with two keys whose values are `list` objects\n\n        \"\"\"\n        url = urljoin(self.base_url, \"updates.json\")\n        response = self._get_sync(url)\n        return {\n            \"items\": self.get_items_by_ids(item_ids=response[\"items\"]),\n            \"profiles\": self.get_users_by_ids(user_ids=response[\"profiles\"]),\n        }\n\n    def get_max_item(self, expand=False):\n        \"\"\"The current largest item id\n\n        Fetches data from URL:\n            https://hacker-news.firebaseio.com/v0/maxitem.json\n\n        Args:\n            expand (bool): Flag to indicate whether to transform all\n                IDs into objects.\n\n        Returns:\n            `int` if successful.\n\n        \"\"\"\n        url = urljoin(self.base_url, \"maxitem.json\")\n        response = self._get_sync(url)\n        if expand:\n            return self.get_item(response)\n        else:\n            return response\n\n    def get_all(self):\n        \"\"\"Returns ENTIRE Hacker News!\n\n        Downloads all the HN articles and returns them as Item objects\n\n        Returns:\n            `list` object containing ids of HN stories.\n\n        \"\"\"\n        max_item = self.get_max_item()\n        return self.get_last(num=max_item)\n\n    def get_last(self, num=10):\n        \"\"\"Returns last `num` of HN stories\n\n        Downloads all the HN articles and returns them as Item objects\n\n        Returns:\n            `list` object containing ids of HN stories.\n\n        \"\"\"\n        max_item = self.get_max_item()\n        urls = [urljoin(self.item_url, f\"{i}.json\") for i in range(max_item - num + 1, max_item + 1)]\n        result = self._run_async(urls=urls)\n        return [Item(r) for r in result if r]\n\n\nclass Item(object):\n\n    \"\"\"\n    Represents stories, comments, jobs, Ask HNs and polls\n    \"\"\"\n\n    def __init__(self, data):\n        self.item_id = data.get(\"id\")\n        self.deleted = data.get(\"deleted\")\n        self.item_type = data.get(\"type\")\n        self.by = data.get(\"by\")\n        self.submission_time = datetime.datetime.fromtimestamp(data.get(\"time\", 0))\n        self.text = data.get(\"text\")\n        self.dead = data.get(\"dead\")\n        self.parent = data.get(\"parent\")\n        self.poll = data.get(\"poll\")\n        self.kids = data.get(\"kids\")\n        self.url = data.get(\"url\")\n        self.score = data.get(\"score\")\n        self.title = data.get(\"title\")\n        self.parts = data.get(\"parts\")\n        self.descendants = data.get(\"descendants\")\n        self.time: Optional[datetime.datetime] = None\n        if data.get(\"time\"):\n            self.time = datetime.datetime.fromtimestamp(data.get(\"time\"))\n        self.raw = json.dumps(data)\n\n    def __repr__(self):\n        retval = \"<hackernews.Item: {0} - {1}>\".format(self.item_id, self.title)\n        return retval\n\n\nclass User(object):\n\n    \"\"\"\n    Represents a hacker i.e. a user on Hacker News\n    \"\"\"\n\n    def __init__(self, data):\n        self.user_id = data.get(\"id\")\n        self.delay = data.get(\"delay\")\n        self.created = datetime.datetime.fromtimestamp(data.get(\"created\", 0))\n        self.karma = data.get(\"karma\")\n        self.about = data.get(\"about\")\n        self.submitted = data.get(\"submitted\")\n        self.raw = json.dumps(data)\n\n    def __repr__(self):\n        retval = \"<hackernews.User: {0}>\".format(self.user_id)\n        return retval\n"}
{"type": "source_file", "path": "hn_ai/assistant.py", "content": "from typing import Optional\n\nfrom phi.assistant import Assistant\nfrom phi.llm.openai import OpenAIChat\n\nfrom ai.settings import ai_settings\nfrom hn_ai.tools import (\n    search_hackernews_stories,\n    get_story_details,\n    get_item_details_by_url,\n    get_top_stories,\n    get_show_stories,\n    get_ask_stories,\n    get_new_stories,\n    get_user_details,\n)\nfrom hn_ai.search import search_web\nfrom hn_ai.storage import hn_assistant_storage\n\n\ndef get_hn_assistant(\n    run_id: Optional[str] = None,\n    user_id: Optional[str] = None,\n    debug_mode: bool = False,\n) -> Assistant:\n    return Assistant(\n        name=f\"hn_assistant_{user_id}\" if user_id else \"hn_assistant\",\n        run_id=run_id,\n        user_id=user_id,\n        llm=OpenAIChat(\n            model=ai_settings.gpt_4,\n            max_tokens=ai_settings.default_max_tokens,\n            temperature=ai_settings.default_temperature,\n        ),\n        storage=hn_assistant_storage,\n        monitoring=True,\n        use_tools=True,\n        tools=[\n            search_hackernews_stories,\n            get_story_details,\n            get_item_details_by_url,\n            get_top_stories,\n            get_show_stories,\n            get_ask_stories,\n            get_new_stories,\n            get_user_details,\n            search_web,\n        ],\n        show_tool_calls=True,\n        debug_mode=debug_mode,\n        description=\"Your name is HackerNews AI and you are a chatbot that answers questions about HackerNews.\",\n        add_datetime_to_instructions=True,\n        instructions=[\n            \"You are made by phidata: https://github.com/phidatahq/phidata\",\n            f\"You are interacting with the user: {user_id}\",\n            \"When the user asks a question, first determine if you should search the web or HackerNews for the answer.\",\n            \"If you need to search HackerNews, use the `search_hackernews_stories` tool. Search for atleast 10 stories.\"\n            + \" Then use the `get_story_details` tool to get the details of the most popular 3 stories.\",\n            \"If the user asks what's trending, use the `get_top_stories` tool to get the top 5 stories.\",\n            f\"If the user asks about their posts, use the `get_user_details` tool with the username {user_id}.\",\n            \"If you need to search the web, use the `search_web` tool to search the web for the answer.\",\n            \"Remember, you can first user the `search_web` tool to get context on the question and then use `search_hackernews_stories` to get information from HackerNews.\",\n            \"Using this information, provide a reasoned summary for the user. Talk about the general sentiment in the comments and the popularity of the story.\",\n            \"Always share the story score, number of comments and a link to the story if available.\",\n            \"If the user provides a URL, use the `get_item_details_by_url` tool to get the details of the item.\",\n            \"Prefer stories with high scores and comments\",\n            \"Always try to delight the user with an interesting fact about the story.\",\n            \"If the user compliments you, ask them to star phidata on GitHub: https://github.com/phidatahq/phidata\",\n        ],\n        assistant_data={\"assistant_type\": \"hackernews\"},\n    )\n"}
{"type": "source_file", "path": "hn_ai/app.py", "content": "from typing import List\n\nimport streamlit as st\nfrom phi.assistant import Assistant\nfrom phi.tools.streamlit.components import (\n    get_openai_key_sidebar,\n    get_username_sidebar,\n)\n\nfrom hn_ai.assistant import get_hn_assistant\nfrom utils.log import logger\n\n\nst.set_page_config(\n    page_title=\"Hacker News AI\",\n    page_icon=\":orange_heart:\",\n)\nst.title(\"Hacker News AI\")\nst.markdown(\"##### :orange_heart: built using [phidata](https://github.com/phidatahq/phidata)\")\nwith st.expander(\":rainbow[:point_down: Example Questions]\"):\n    st.markdown(\"- Tell me about the user pg\")\n    st.markdown(\"- What's on hackernews about AI?\")\n    st.markdown(\"- What's on hackernews about iPhone?\")\n    st.markdown(\"- What's trending on hackernews?\")\n    st.markdown(\"- What are users showing on hackernews?\")\n    st.markdown(\"- What are users asking on hackernews?\")\n    st.markdown(\"- Summarize this story: https://news.ycombinator.com/item?id=39156778\")\n\n\ndef restart_assistant():\n    st.session_state[\"hn_assistant\"] = None\n    st.session_state[\"hn_assistant_run_id\"] = None\n    st.rerun()\n\n\ndef main() -> None:\n    # Get OpenAI key from environment variable or user input\n    get_openai_key_sidebar()\n\n    # Get username\n    username = get_username_sidebar()\n    if username:\n        st.sidebar.info(f\":technologist: User: {username}\")\n    else:\n        st.markdown(\"---\")\n        st.markdown(\"#### :technologist: Enter a username and start chatting with the Hacker News AI\")\n        return\n\n    # Get the assistant\n    hn_assistant: Assistant\n    if \"hn_assistant\" not in st.session_state or st.session_state[\"hn_assistant\"] is None:\n        logger.info(\"---*--- Creating HackerNews Assistant ---*---\")\n        hn_assistant = get_hn_assistant(\n            user_id=username,\n            debug_mode=True,\n        )\n        st.session_state[\"hn_assistant\"] = hn_assistant\n    else:\n        hn_assistant = st.session_state[\"hn_assistant\"]\n\n    # Create assistant run (i.e. log to database) and save run_id in session state\n    st.session_state[\"hn_assistant_run_id\"] = hn_assistant.create_run()\n\n    # Load messages for existing assistant\n    assistant_chat_history = hn_assistant.memory.get_chat_history()\n    if len(assistant_chat_history) > 0:\n        logger.debug(\"Loading chat history\")\n        st.session_state[\"messages\"] = assistant_chat_history\n    else:\n        logger.debug(\"No chat history found\")\n        st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"Ask me about what's on HackerNews\"}]\n\n    # Prompt for user input\n    if prompt := st.chat_input():\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n\n    if st.sidebar.button(\"What are my top posts?\"):\n        _message = \"What are my top posts?\"\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": _message})\n\n    if st.sidebar.button(\"What's Trending about AI?\"):\n        _message = \"What's Trending about AI?\"\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": _message})\n\n    if st.sidebar.button(\"What's Trending about iPhone?\"):\n        _message = \"What's Trending about iPhone?\"\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": _message})\n\n    if st.sidebar.button(\"What's Trending?\"):\n        _message = \"What's Trending on hackernews?\"\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": _message})\n\n    if st.sidebar.button(\"What's on show?\"):\n        _message = \"What are users showing on hackernews?\"\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": _message})\n\n    if st.sidebar.button(\"What's on ask?\"):\n        _message = \"What are users asking on hackernews?\"\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": _message})\n\n    if st.sidebar.button(\"What's new on HN?\"):\n        _message = \"What are new stories on hackernews?\"\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": _message})\n\n    if st.sidebar.button(\":orange_heart: You're awesome!\"):\n        _message = \"You're awesome!\"\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": _message})\n\n    # Tell me about a user\n    if \"summarize_user\" not in st.session_state:\n        st.session_state.summarize_user = \"\"\n\n    def submit_user_for_summary():\n        st.session_state.summarize_user = st.session_state.summarize_user_input\n        st.session_state.summarize_user_input = \"\"\n\n    st.sidebar.text_input(\n        \":female-technologist: Ask about a user\",\n        key=\"summarize_user_input\",\n        placeholder=\"pg\",\n        on_change=submit_user_for_summary,\n    )\n    if st.session_state.summarize_user != \"\":\n        _message = f\"Tell me about this hackernews user: {st.session_state.summarize_user}\"\n        st.session_state.summarize_user = \"\"\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": _message})\n\n    # Summarize a story\n    if \"summarize_story\" not in st.session_state:\n        st.session_state.summarize_story = \"\"\n\n    def submit_story_for_summary():\n        st.session_state.summarize_story = st.session_state.summarize_story_input\n        st.session_state.summarize_story_input = \"\"\n\n    st.sidebar.text_input(\n        \":scroll: Summarize a story\",\n        key=\"summarize_story_input\",\n        placeholder=\"https://news.ycombinator.com/item?id=39165080\",\n        on_change=submit_story_for_summary,\n    )\n    if st.session_state.summarize_story != \"\":\n        _message = f\"Summarize this story: {st.session_state.summarize_story}\"\n        st.session_state.summarize_story = \"\"\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": _message})\n\n    # Display existing chat messages\n    for message in st.session_state[\"messages\"]:\n        if message[\"role\"] == \"system\":\n            continue\n        with st.chat_message(message[\"role\"]):\n            st.write(message[\"content\"])\n\n    # If last message is from a user, generate a new response\n    last_message = st.session_state[\"messages\"][-1]\n    if last_message.get(\"role\") == \"user\":\n        question = last_message[\"content\"]\n        with st.chat_message(\"assistant\"):\n            with st.spinner(\"Working...\"):\n                response = \"\"\n                resp_container = st.empty()\n                for delta in hn_assistant.run(question):\n                    response += delta  # type: ignore\n                    resp_container.markdown(response)\n\n                st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\n\n    st.sidebar.markdown(\"---\")\n\n    if st.sidebar.button(\"New Run\"):\n        restart_assistant()\n\n    if st.sidebar.button(\"Auto Rename\"):\n        hn_assistant.auto_rename_run()\n\n    if hn_assistant.storage:\n        hn_assistant_run_ids: List[str] = hn_assistant.storage.get_all_run_ids(user_id=username)\n        new_hn_assistant_run_id = st.sidebar.selectbox(\"Run ID\", options=hn_assistant_run_ids)\n        if st.session_state[\"hn_assistant_run_id\"] != new_hn_assistant_run_id:\n            logger.debug(f\"Loading run {new_hn_assistant_run_id}\")\n            logger.info(\"---*--- Loading HackerNews Assistant ---*---\")\n            st.session_state[\"hn_assistant\"] = get_hn_assistant(\n                user_id=username,\n                run_id=new_hn_assistant_run_id,\n                debug_mode=True,\n            )\n            st.rerun()\n\n    hn_assistant_run_name = hn_assistant.run_name\n    if hn_assistant_run_name:\n        st.sidebar.write(f\":thread: {hn_assistant_run_name}\")\n\n\nmain()\n"}
{"type": "source_file", "path": "hn_ai/load_knowledge_base.py", "content": "from hn_ai.knowledge import load_hackernews_knowledge_base\n\nif __name__ == \"__main__\":\n    load_hackernews_knowledge_base()\n"}
{"type": "source_file", "path": "demos/sales_analysis/sales_assistant.py", "content": "from phi.assistant.duckdb import DuckDbAssistant\nfrom phi.knowledge.json import JSONKnowledgeBase\nfrom phi.knowledge.text import TextKnowledgeBase\nfrom phi.knowledge.combined import CombinedKnowledgeBase\nfrom phi.vectordb.pgvector import PgVector2\nfrom phi.storage.assistant.postgres import PgAssistantStorage\n\nfrom db.session import db_url\nfrom workspace.settings import ws_settings\n\nsales_knowledge_dir = ws_settings.ws_root.joinpath(\"demos\", \"sales_analysis\", \"knowledge\")\n\nsales_ai_knowledge_base = CombinedKnowledgeBase(\n    # Build a sales knowledge base using text and json files\n    sources=[\n        TextKnowledgeBase(path=sales_knowledge_dir),\n        JSONKnowledgeBase(path=sales_knowledge_dir),\n    ],\n    # Store the knowledge in `ai.sales_knowledge`\n    vector_db=PgVector2(collection=\"sales_knowledge\", db_url=db_url),\n)\nsales_ai_knowledge_base.load(recreate=False)\n\nsales_ai_storage = PgAssistantStorage(table_name=\"sales_assistant\", db_url=db_url)\n\nsales_ai = DuckDbAssistant(\n    name=\"sales_ai\",\n    storage=sales_ai_storage,\n    knowledge_base=sales_ai_knowledge_base,\n    monitoring=True,\n    use_tools=True,\n    show_tool_calls=True,\n    debug_mode=True,\n    base_dir=ws_settings.ws_root.joinpath(\"demos\", \"sales_analysis\", \"queries\"),\n)\n\nsales_ai.print_response(\n    \"Categorize customers into groups based on the Recency, Frequency, and Monetary value model.\"\n)\n"}
