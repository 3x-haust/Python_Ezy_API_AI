{"repo_info": {"repo_name": "text-to-sql-epi-ehr-naacl2024", "repo_owner": "Bayer-Group", "repo_url": "https://github.com/Bayer-Group/text-to-sql-epi-ehr-naacl2024"}}
{"type": "source_file", "path": "scripts/prediction_pipeline.py", "content": "import sys\nimport asyncio\nfrom dotenv import load_dotenv\nimport argparse\nimport os\nfrom os.path import join, dirname\nimport time\n\nmain_path = dirname(os.getcwd())\nsrc_folder = os.path.join(main_path, \"text2sql_epi\")\n\nsys.path.append(main_path)\nsys.path.append(src_folder)\n\ndotenv_path = join(main_path, \".env.local\")\nload_dotenv(dotenv_path)\n\nfrom text2sql_epi.settings import settings\nfrom text2sql_epi.rag import AgentRag\nfrom text2sql_epi.sql_post_processor import MedicalSQLProcessor\nfrom text2sql_epi import helpers\nfrom text2sql_epi.snowflake_session import get_db\n\n\nasync def end2end_pred_pipeline_ds(\n    input_question, main_path_rag, querylib_file_rag, log_folder, med_coding=False, use_db=False,\n        medcodeonto_file=None\n):\n\n    print(f\"Use medical coding: {med_coding}\")\n\n    print(f\"Use Snowflake database: {use_db}\")\n\n    selected_coding = {\n        \"condition\": [\"SNOMED\"],\n        \"procedure\": [\"CPT4\", \"SNOMED\"],\n        \"drug\": [\"RxNorm\", \"RxNorm Extension\"],\n    }\n\n    time.sleep(1)\n\n    rag_agent = AgentRag(\n        main_path=main_path_rag, log_folder=log_folder, querylib_file=querylib_file_rag\n    )\n\n    med_sql_processor = MedicalSQLProcessor(assistant=rag_agent.assistant)\n\n    initial_prompt, text_sql_template, df_recs_list_out, question_masked = (\n        await helpers.prepare_gpt_call(input_question, rag_agent)\n    )\n    gpt_answer = await rag_agent.assistant.get_response()\n    df_recs_list_out = df_recs_list_out.astype({\"DATE_LABELLED\": str})\n\n    query_template_pred = med_sql_processor.parse_sql_from_response(gpt_answer)\n\n    print(f\"Question: {input_question}\\n\")\n    print(f\"SQL template:\\n {query_template_pred}\\n\")\n\n    if med_coding:\n        from text2sql_epi.query_library import MedCodingOnto\n        medcodeonto = MedCodingOnto(\n            ontolib_name=\"medcodes_mockup\",\n            source=\"medcodes_mockup\",\n            ontolib_source_file=None,\n            col_text=\"CONCEPT_NAME\"\n        )\n\n        print(f\"Loading embedding from {medcodeonto_file}\")\n        medcodeonto = medcodeonto.load(querylib_file=medcodeonto_file)\n\n        query_filled_pred = await med_sql_processor.post_process_sql_query(\n            query_template_pred,\n            sleep_sec=rag_agent.sleep_sec,\n            explorer_concepts=None,\n            selected_coding=selected_coding,\n            rag=rag_agent,\n            medcodeonto=medcodeonto\n        )\n\n        print(f\"SQL filled:\\n {query_filled_pred}\\n\")\n    else:\n        query_filled_pred = None\n\n    if use_db and query_filled_pred is not None:\n        db = next(get_db(settings.SNOWFLAKE_DATABASE))\n\n        new_prompt = rag_agent.assistant.conversation\n\n        rwd_request_pred = helpers.prepare_rwd_request(\n            input_question,\n            query_filled_pred,\n            query_template_pred,\n            question_masked,\n            df_recs_list_out,\n            new_prompt,\n        )\n\n        df = await rwd_request_pred.run_query(\n            query_filled_pred,\n            db=db,\n            assistant=rag_agent.assistant,\n            max_retries=5,\n            reset_conversation=False,\n        )\n\n        await rwd_request_pred.get_answer(rag_agent.assistant_answers)\n        answer = rwd_request_pred.answer\n\n        print(f\"Database: {settings.SNOWFLAKE_DATABASE}\\n\")\n        print(f\"Answer: {answer}\\n\")\n\n\nif __name__ == \"__main__\":\n\n    # in_folder = os.path.join(main_path, \"dataset\")\n    out_folder = os.path.join(main_path, \"data_out\")\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--output_path\",\n        default=out_folder,\n        help=\"path where the query lib will be generated\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--med_coding\",\n        default=False,\n        help=\"Retrieve medical codes for the medical entities in the generated query\"\n    )\n    parser.add_argument(\n        \"--use_db\",\n        default=False,\n        help=\"Use Snowflake database for querying. Only works if a Snowflake database is connected.\"\n    )\n\n    parser.add_argument(\n        \"--question\",\n        help=\"Add here your question\",\n        type=str\n    )\n\n    args = parser.parse_args()\n\n    querylib_file = os.path.join(out_folder, \"querylib.pkl\")\n    medcodeonto_file_loaded = os.path.join(out_folder, \"medcodes_onto.pkl\")\n\n    asyncio.run(\n        end2end_pred_pipeline_ds(\n            input_question=args.question,\n            use_db=args.use_db,\n            main_path_rag=main_path,\n            log_folder=out_folder,\n            med_coding=args.med_coding,\n            querylib_file_rag=os.path.join(out_folder, \"querylib.pkl\"),\n            medcodeonto_file=medcodeonto_file_loaded,\n        )\n    )\n"}
{"type": "source_file", "path": "scripts/run_medcoding_calc.py", "content": "import sys\nfrom dotenv import load_dotenv\nimport os\nimport argparse\nimport asyncio\n\nif __name__ == \"__main__\":\n    main_path = os.path.join(os.path.dirname(os.getcwd()))\n    src_folder = os.path.join(main_path, \"text2sql_epi\")\n    sys.path.append(main_path)\n    sys.path.append(src_folder)\n\n    from text2sql_epi.query_library import MedCodingOnto\n\n    # load environment variables\n    load_dotenv(\"../.env.local\")\n\n    in_folder = os.path.join(main_path, \"dataset\")\n    out_folder = os.path.join(main_path, \"data_out\")\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--input_path\",\n        default=in_folder,\n        help=\"path where the data is stored\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--output_path\",\n        default=out_folder,\n        help=\"path where the query lib will be generated\",\n        type=str,\n    )\n    args = parser.parse_args()\n\n    medcodeonto_source_file = os.path.join(in_folder, \"medcodes_mockup.xlsx\")\n    medcodeonto_file = os.path.join(out_folder, \"medcodes_onto.pkl\")\n\n    medcodeonto = MedCodingOnto(\n        ontolib_name=\"medcodes_mockup\",\n        source=\"medcodes_mockup\",\n        ontolib_source_file=medcodeonto_source_file,\n        col_text=\"CONCEPT_NAME\"\n    )\n\n    print(f\"Calculating ontology embeddings from {medcodeonto_source_file}...\")\n    medcodeonto.calc_embedding(embedding_model_name=\"BAAI/bge-large-en-v1.5\")\n    medcodeonto.save(querylib_file=medcodeonto_file)\n    print(f\"Embedding calculated and saved to {medcodeonto_file}\")\n\n    print(f\"Loading embedding from {medcodeonto_file}\")\n    medcodeonto = medcodeonto.load(querylib_file=medcodeonto_file)\n    print(f\"Ontology length: {len(medcodeonto)}\")\n    print(\"Done\")\n\n    df = asyncio.run(medcodeonto.get_similar_codes_from_onto(question_masked=\"atopic dermatitis\",\n        top_k_screening=10,\n        top_k_prompt=4,\n        sim_threshold=0.0))\n\n    print(df)"}
{"type": "source_file", "path": "text2sql_epi/__init__.py", "content": ""}
{"type": "source_file", "path": "scripts/run_querylib_calc.py", "content": "import sys\nfrom dotenv import load_dotenv\nimport os\nimport argparse\n\nif __name__ == \"__main__\":\n    main_path = os.path.join(os.path.dirname(os.getcwd()))\n    src_folder = os.path.join(main_path, \"text2sql_epi\")\n    sys.path.append(main_path)\n    sys.path.append(src_folder)\n\n    from text2sql_epi.query_library import QueryLibrary\n\n    # load environment variables\n    load_dotenv(\"../.env.local\")\n\n    in_folder = os.path.join(main_path, \"dataset\")\n    out_folder = os.path.join(main_path, \"data_out\")\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--input_path\",\n        default=in_folder,\n        help=\"path where the data is stored\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--output_path\",\n        default=out_folder,\n        help=\"path where the query lib will be generated\",\n        type=str,\n    )\n    args = parser.parse_args()\n\n    querylib_source_file = os.path.join(in_folder, \"text2sql_epi_dataset_omop.xlsx\")\n    querylib_file = os.path.join(out_folder, \"querylib.pkl\")\n\n    querylib = QueryLibrary(\n        querylib_name=\"patient_counts\",\n        source=\"text2sql_epi_dataset_omop\",\n        querylib_source_file=querylib_source_file,\n        col_question=\"QUESTION\",\n        col_question_masked=\"QUESTION_MASKED\",\n        col_query_w_placeholders=\"QUERY_SNOWFLAKE_WITH_PLACEHOLDERS\",\n        col_query_executable=\"QUERY_SNOWFLAKE_RUNNABLE\",\n    )\n\n    print(f\"Calculating query library from {querylib_source_file}...\")\n    querylib.calc_embedding(embedding_model_name=\"BAAI/bge-large-en-v1.5\")\n    querylib.save(querylib_file=querylib_file)\n    print(f\"Embedding calculated and saved to {querylib_file}\")\n\n    print(f\"Loading embedding from {querylib_file}\")\n    querylib = querylib.load(querylib_file=querylib_file)\n    print(f\"Query library length: {len(querylib)}\")\n    print(\"Done\")\n"}
{"type": "source_file", "path": "text2sql_epi/prompts.py", "content": "prompt_gpt = \"\"\"\n# Introduction:\nYou are a data analyst for a pharmaceutical company. You help colleagues by answering questions about patients and diseases using real-world data like claims and electronic medical records. You are well-versed in the OHDSI world and the OMOP CDM.\n\nYour task is to write SQL queries in the Snowflake dialect. The SQL you write should be syntactically correct.\n\n# Instructions:\n\n1. **Concept IDs:** Use the following IDs for the respective fields:\n    - GENDER_CONCEPT_ID: '8507' for male, '8532' for female.\n    - ETHNICITY_CONCEPT_ID: '38003563' for 'Hispanic or Latino', '38003564' for 'Not Hispanic or Latino'.\n    - VISIT_CONCEPT_ID: \n     '9202' for 'Outpatient Visit',\n     '9201' for 'Inpatient Visit',\n     '9203' for 'Emergency Room Visit',\n     '262' for 'Emergency room visit and Inpatient visit',\n     '581478' for 'Ambulance visit',\n     '581458' for 'Pharmacy Visit',\n     '32036' for 'Laboratory visit',\n     '42898160' for 'Non-hospital institution Visit',\n     '581476' for 'Home visit'\n\n2. **Race Analysis:** For breakdown the analysis by 'Race', select CONCEPT_NAME joining PERSON and CONCEPT table on RACE_CONCEPT_ID = CONCEPT_ID (sql: CONCEPT_ID ON domain_id = 'Race' and standard_concept = 'S'\n\n3. **Entity Extraction:** \n    The entity types from different tables should be represented as:\n    - MEASUREMENT table: [measurement@<name of the measurement>]\n    - CONDITION_OCCURRENCE or CONDITION_ERA table: [condition@<name of the condition>]\n    - PROCEDURE_OCCURRENCE table: [procedure@<name of the procedure>]\n    - DRUG_EXPOSURE or DRUG_ERA table: [drug@<name of the drug>]\n    Do NOT extract any other entity beyond these.\n\n4. **Drug class:** \n    If there is a drug class encountered, followed by the list of entities in this drug class e.g. (anticoagulants (heparin, warfarin, rivaroxaban, dabigatran, apixaban, edoxaban, enoxaparin, fondaparinux)) treat listed entities as drugs.\n    Use drug class name in constructing the name of WITH statement if used.\n    Please make sure that the SQL query generated provides the result requested grouped by each drug class separately.\n    Please make sure that there is complete result  provided  satisfying “or”, “and” or similar condition present in the question.\n    Provide in query calculation of intermediate results that support the final value. For e.g. nominator and denominator value when proportion is calculated.\n\n5. **Concept Name:** Whenever you are returning an entity (drug, condition, procedure, measurement) concept id (e.g., drug_concept_id) join the concept table on concept_id to return the corresponding concept name. (e.g., join on concept_id = drug_concept_id).\n\n6. **Geographical Analysis:** Use standard 2 letter codes for state, territory, or regional level analyses. For example, Texas as TX, California as CA, New York as NY.\n\n7. **Tables:** Reminder about some OMOP CDM tables:\n    The PROCEDURE_OCCURRENCE table contains records of activities or processes ordered or carried out by a healthcare Provider on the patient with a diagnostic or therapeutic purpose.\n    The VISIT_OCCURRENCE table contains information about a patient’s encounters with the health care system.\n    The OBSERVATION table captures clinical facts in the context of examination, questioning, or a procedure. Any data that cannot be represented by any other domains, such as social and lifestyle facts, medical history, family history, etc. are recorded here.\n\n8. **Date Filters:** If you need to filter by date, use the following date fields:\n    - VISIT_OCCURRENCE: visit_start_date\n    - CONDITION_OCCURRENCE: condition_start_date\n    - DRUG_EXPOSURE: drug_exposure_start_date\n    - MEASUREMENT: measurement_date\n    - OBSERVATION: observation_date\n    - PROCEDURE_OCCURRENCE: procedure_date\n    Use fields like visit_end_date, condition_end_date, drug_exposure_end_date only if you measure the duration of the event for each patient.\n\n9. **Column Naming:** Every column name must start with a character and never with a number. For example, percentile_25 instead of 25th_Percentile. Instead of median, use percentile_50.\n\n10. **Date Format:** When clarifying a date interval in your SQL queries, you are required to utilize the `TO_DATE` function along with the correct format 'YYYY-MM-DD'. \n    The `TO_DATE` function is used in SQL to convert strings into dates. Here's an example of how to use it:\n    TO_DATE('your_date_string', 'YYYY-MM-DD') Replace 'your_date_string' with the date you're inputting into the query. \n    Any deviation from this date format will lead to errors or data misinterpretations, for example:\n    TO_DATE('2015-01-01', 'YYYY-MM-DD') instead of '2015-01-01', \n    TO_DATE('2020-12-31', 'YYYY-MM-DD') instead of '2020-12-31'.\n\n    Make sure all dates in your SQL queries conform to this style and use the `TO_DATE` function when handling date information.\n\n11. **Patient Count:** Use COUNT(DISTINCT person_id) when counting patients.\n\n12. **Age Calculation:** When calculating a patient's age in relation to an event, such as a visit or condition onset, the age should be computed based on the year of the event in question not the current year.  \n    Use the year_of_birth field from the PERSON table and subtract it from the year of the event (visit_start_date, condition_start_date, etc.). \n    For instance, if the task is to locate patients older than a certain age who have a certain condition, the age condition in the SQL query should refer to the year of the condition's start, like so: AND (YEAR(co.condition_start_date) - p.year_of_birth) > {desired_age}.\n\n13. **Data Limit:** Use LIMIT 10000 at the end of the query for large datasets.\n\n14. **SQL Writing:** To write the SQL, use the following format:\n    Question: the input question you must create a SQL\n    Database tables and columns: list all tables that are relevant to the query. If you write a WITH clause in SQL, make sure you will select all attributes needed in the WHERE clause of the main query\n    When generating WITH clause, always create aliases that do not conflict with the original table names from the OMOP database. Ensure that the aliases are unique, meaningful, and descriptive.\n    If you need the value of concept_id, don't provide it. Instead, add a squared bracket like [entity@<name of the concept>] and always use the IN SQL operator to prepare for the concept ids list. \n    For example: condition_concept_id IN ([disease@hypertension])\n    Do not include unnecessary or incorrect conditions, such as 'WHERE concept_name IN...'.\n    Do not include statements like \"WHERE c.concept_name = 'hypertension'\" or  \"WHERE concept.concept_name IN ('hypertension', 'anemia')\"\n\n15. **Query Structure:** General query structure: plan how you want to structure the general query structure (e.g., group by, nesting, multiple joins, set operations, etc.)\n    Make sure to structure the query accordingly. \"or\" means UNION \"and\" means INNER JOIN of the clusters\n\n16. **SQL Return:** Return the SQL query ONLY within ```sql ``` code block.\n\n17. **Query Checking:** Before returning the Snowflake SQL query, check if it contains all the relevant SQL WHERE clauses with concept_id+[entity@<name of the concept>] you identified. \n\n\n# Question:\n${question}\n\"\"\"\n\nprompt_gpt_naive = \"\"\"\n# Introduction:\nYou are a data analyst for a pharmaceutical company. You help colleagues by answering questions about patients and diseases using real-world data like claims and electronic medical records. You are well-versed in the OHDSI world and the OMOP CDM.\n\nYour task is to write SQL queries in the Snowflake dialect. The SQL you write should be syntactically correct.\n\n# Instructions:\n\n1. **Entity Extraction:** Do not provide the value of the concept id. Use square brackets for the value of concept_id. \n    For example: condition_concept_id IN ([disease@hypertension]). \n    The entity types from different tables should be represented as:\n    - MEASUREMENT table: [measurement@<name of the measurement>]\n    - CONDITION_OCCURRENCE or CONDITION_ERA table: [condition@<name of the condition>]\n    - PROCEDURE_OCCURRENCE table: [procedure@<name of the procedure>]\n    - DRUG_EXPOSURE or DRUG_ERA table: [drug@<name of the drug>]\n    Do NOT extract any other entity beyond these.\n\n2. **Data Limit:** Use LIMIT 10000 at the end of the query for large datasets.\n\n3. **SQL Writing:** To write the SQL, use the following format:\n    Question: the input question you must create a SQL\n    Database tables and columns: list all tables that are relevant to the query. If you write a WITH clause in SQL, make sure you will select all attributes needed in the WHERE clause of the main query\n    When generating WITH clause, always create aliases that do not conflict with the original table names from the OMOP database. Ensure that the aliases are unique, meaningful, and descriptive.\n\n4. **Query Structure:** General query structure: plan how you want to structure the general query structure (e.g., group by, nesting, multiple joins, set operations, etc.)\n    Make sure to structure the query accordingly. \"or\" means UNION \"and\" means INNER JOIN of the clusters\n\n5. **SQL Return:** Return the SQL query ONLY within ```sql ``` code block.\n\n# Important Note:\nDo not include unnecessary or incorrect conditions, such as 'WHERE concept_name IN...', as it may create inaccurate results.\n\n# Question:\n${question}\n\"\"\"\n\nmatch_template = \"\"\"Given a question, a reference answer and a hypothesis answer, determine if the hypothesis answer is correct. \nIf the answer contains numerical data, please consider the hypothesis answer correct if it is within 15% of the reference answer.\nDo not provide any explanation.\nOnly return one word as an answer true or false.\nDo not return any text before or after true or false.\n\nUse the following format:\n\nQuestion: Question here\nReference Answer: Reference answer here\nHypothesis Answer: Hypothesis answer here\nHypothesis Answer Correct: true or false\n\nQuestion: {question}\nReference Answer: {reference_answer}\nHypothesis Answer: {hypothesis_answer}\nHypothesis Answer Correct: \"\"\"\n\nmatch_template_json = \"\"\"Given a question, a reference answer and a hypothesis answer, determine if the hypothesis answer is correct. \nIf the answer contains numerical data, please consider the hypothesis answer correct if it is within 15% of the reference answer.\nDo not provide any explanation.\nOutput result in JSON format containing two fields:\n* explanation of your decision;\n* one word answer true or false.\n\nFor the given inputs:\n\nQuestion: {question}\nReference Answer: {reference_answer}\nHypothesis Answer: {hypothesis_answer}\n\nReturn JSON output as follows:\n\n{{\n    \"explanation\": \"Reasoning for obtaining true or false result\",\n    \"correct\": \"true or false\"\n}}\n\"\"\"\n\nmatch_template_zk = \"\"\"Given a question, a reference answer and a hypothesis answer, determine if the hypothesis answer is correct. \nIf the answer contains numerical data, please consider the hypothesis answer correct if it is within 10-15% of the reference answer.\n\n\nUse the following format:\n\nQuestion: Question here\nReference Answer: Reference answer here\nHypothesis Answer: Hypothesis answer here\nHypothesis Answer Correct: true or false\nMotivation: The actual motivation for the result\n\nQuestion: {question}\nReference Answer: {reference_answer}\nHypothesis Answer: {hypothesis_answer}\nHypothesis Answer Correct: \"\"\"\n\nentity_masking = \"\"\"Given an input text, your task is to substitute the entities that fit into the following categories with their corresponding entity type labels:\n\nCONDITION: This represents a clinical diagnosis or symptom documented in a patient's medical history.\nMEASUREMENT: This includes various clinical tests, assessments, or instruments.\nPROCEDURE: This refers to any intervention, surgical or non-surgical, that is performed on the patient for diagnostic or treatment purposes.\nDRUG: This refers to any therapeutic or prophylactic substance prescribed to a patient, including prescription medications, over-the-counter drugs, and vaccines.\nCODE: This refers to standardized medical codes, such as for example G71.038, N17.9, Z95.1, 92960\nDRUG_CLASS: This refers to name of group of medications and other compounds that have similar chemical structures, the same mechanism of action, and/or are used to treat the similar diseases.\n\nPlease remember to only substitute entities that fall under the five categories: CONDITION, MEASUREMENT, PROCEDURE, DRUG, DRUG_CLASS. Always write entity type labels in capital letters.\nDuring your substitution, do not substitute vocabulary names such as ICD-9, ICD10-CM, CPT4. Do not return the text \"Masked text\" in your answer.\n\nHere are a few examples:\n\nInput text: How many patients younger than 20 suffered from hypertension?\nMasked text: How many patients younger than 20 suffered from CONDITION?\n\nInput text: What is the adherence of Eylea?\nMasked text: What is the adherence of DRUG?\n\nInput text: How many patients are treated with Edoxaban and have atrial fibrillation in their disease history before initiating edoxaban?\"  \nMasked text: How many patients are treated with DRUG and have CONDITION in their disease history before initiating DRUG?\n\nInput text: What is the distribution of Alanine aminotransferase (ALT) and aspartate aminotransferase (AST)? Breakdowns by age bins <50, 50-55, >=55\nMasked text: What is the distribution of MEASUREMENT and MEASUREMENT? Breakdowns by age bins <50, 50-55, >=55\n\nInput text: How many females suffered from hypertension while taking venlafaxine?\nMasked text: How many females suffered from CONDITION while taking DRUG?\n\nInput text: Among the patients who had a Coronary Artery Bypass Grafting (CABG) surgery, as indicated by ICD-9-CM procedure codes (36.10 through 36.19) or ICD-10 code Z95.1, what proportion also had an Acute Kidney Injury (AKI) using ICD9 codes (584.0, 584.5, 584.6, 584.7, 584.8, 584.9, 586) and ICD10 code (N17.9)?\nMasked text: Among the patients who had a Coronary Artery Bypass Grafting (CABG) surgery, as indicated by ICD-9-CM procedure codes (CODE through CODE) or ICD-10 code CODE, what proportion also had an Acute Kidney Injury (AKI) using ICD9 codes (CODE, CODE, CODE, CODE, CODE, CODE, CODE) and ICD10 code (CODE)?\n\nInput text: How many females take antidepressants (citalopram, duloxetine) after myocardial infraction?\nMasked text: How many females take DRUG_CLASS (DRUG, DRUG) after CONDITION?\n\nInput text: What is the proportion of patients taking diuretics (Hydrochlorothiazide, Furosemide, Spironolactone, Chlorthalidone, Amiloride, Bumetanide, Triamterene, Torsemide, Indapamide, Metolazone, Ethacrynic Acid) or calcium supplements (Calcium carbonate, Calcium citrate, Calcium gluconate, Calcium lactate, Calcium phosphate)?\nMasked text: What is the proportion of patients taking DRUG_CLASS (DRUG, DRUG, DRUG, DRUG, DRUG, DRUG, DRUG, DRUG, DRUG, DRUG, DRUG) or DRUG_CLASS (DRUG, DRUG, DRUG, DRUG, DRUG)?\n\nInput text: {question}\nMasked text:\n\"\"\"\n\ncohort_creation_prompt = \"\"\"\nHere is a question: ${question}. Rewrite this question to get patient IDs of patients who meet the requirements in the question and date \nwhen requirements were first met (index date). Return only distinct patient ids and the earliest index date.\n- Find hints within a given question and focus on it. As an output we only want a list of patient IDs and index dates.\n\ne. g. Question: 'What is the prevalence of patients above 40 years old who take Rivaroxaban?'\nRewritten question: 'What are IDs of patients above 40 yo who take Rivaroxaban and date of first prescription?'\n\n- Focus on the relevant information in the question to fetch IDs and dates, omit prevalence, distribution, etc. \n\ne.g. Question: 'What is the age distribution of patients when they obtain the first ckd diagnosis? split by gender.'\nRewritten question: 'What are the IDs of patients diagnosed with CKD and date of first diagnosis?'\n\n- If question is about particular time range (e.g. procedure in range of time), keep it as a part of rewritten question.\n\ne.g. Question: 'How many Afib patients patients had procedures (CPT4 codes 92960, 1012978, 92961) from 2017 to 2022? Break it down by code and year. Show % change between year to year.'\nRewritten question: 'What are the patient IDs of patients with atrial fibrillation who underwent procedures with CPT4 codes 92960, 1012978, or 92961 from 2017 to 2022 and what was the date of first procedure's occurrence?'\n\n- As index date, select the date when the patient began to match the question's requirements (selection criteria), \nfor example first prescription, diagnosis, first occurrence of the symptom, etc.\n\ne.g. Question: 'Let us define moderate to severe atopic dermatitis as having been prescribed at least 2 of the \nfollowing drugs: tacrolimus, pimecrolimus, clobetasone. What is the prevalence of moderate to severe atopic dermatitis patients >=16 years old?'\nRewritten question: 'What are the IDs of patients >=16 years old who have been prescribed at least 2 of the \nfollowing drugs: tacrolimus, pimecrolimus, clobetasone and date of second drug's prescription?'\n\n- Rewritten question should always ask for patient's IDs and date when patient started to meet question's requirements.\n\nReturn only rewritten question, nothing more. Rewritten question: \"\"\"\n\n\ndrug_class_keep = \"\"\"\n    Given an input text, your task is to check if text include entity that fit into DRUG_CLASS category. \n    If found, look for a complete list of active ingredients that are part of DRUG_CLASS and provide after DRUG_CLASS a comma separated list with those those names in brackets.\n    If there is several DRUG_CLASS entity mentioned do not join list of ingredients together, keep 'and' and 'or' in the sentence structure.\n\n    DRUG_CLASS: This refers to name of group of medications and other compounds that have similar chemical structures, the same mechanism of action, and/or are used to treat the similar diseases.\n\n    Return only text of modified question, no explanation of procedure. If no DRUG_CLASS detected return unchanged question.\n\n    Here are a few examples:\n    Input text: How many patients with depression takes selective serotonin reuptake inhibitors?\n    Output text: How many patients with depression takes selective serotonin reuptake inhibitors (fluoxetine, sertraline, paroxetine, citalopram, escitalopram, fluvoxamine, dapoxetine)?\n\n    Input text: What is the proportion of male patients taking anticoagulants or aspirin?\n    Output text: What is the proportion of male patients taking anticoagulants (heparin, warfarin, rivaroxaban, dabigatran, apixaban, edoxaban, enoxaparin, fondaparinux) or aspirin?\n\n    Input text: ${question}\n    Output text:\n\"\"\"\n"}
{"type": "source_file", "path": "text2sql_epi/settings.py", "content": "from pydantic_settings import BaseSettings, SettingsConfigDict\n\n\nclass Settings(BaseSettings):\n    \"\"\"Settings for API server.\"\"\"\n\n    model_config = SettingsConfigDict(env_file=(\".env\", \".env.local\"), extra=\"ignore\")\n\n    SNOWFLAKE_USER: str\n    SNOWFLAKE_PASSWORD: str\n    SNOWFLAKE_ACCOUNT_IDENTIFIER: str\n    SNOWFLAKE_WAREHOUSE: str\n    SNOWFLAKE_DATABASE: str\n    SNOWFLAKE_TIMEOUT: int = 120\n    OPENAI_API_KEY: str\n    OPENAI_API_VERSION: str\n    OPENAI_API_BASE: str\n    AZURE_CLIENT_ID: str\n    AZURE_CLIENT_SECRET: str\n    AZURE_TENANT_ID: str\n\n\nsettings = Settings()\n"}
{"type": "source_file", "path": "text2sql_epi/rag.py", "content": "import glob\nimport os\nimport sys\nfrom datetime import datetime\nimport logging\n\nfrom text2sql_epi import prompts\nfrom text2sql_epi.assistants import create_assistant\nfrom text2sql_epi.query_library import QueryLibrary\n\nlogger = logging.getLogger(__name__)\n\n\nclass Rag:\n    querylib = None\n\n    def __init__(self, main_path=None, log_folder=None, querylib_file=None):\n        # Set default main_path if not provided\n        self.main_path = main_path if main_path is not None else os.getcwd()\n        sys.path.append(self.main_path)\n\n        # Set default log_folder if not provided\n        self.log_folder = (\n            log_folder\n            if log_folder is not None\n            else os.path.join(self.main_path, \"logs\")\n        )\n\n        if querylib_file is not None:\n            # Use the provided querylib_file\n            self.querylib_file = querylib_file\n        else:\n            # Get a list of all files that match the pattern 'querylib_*.pkl'\n            querylib_files = glob.glob(os.path.join(self.main_path, \"querylib_*.pkl\"))\n            # Extract dates from the filenames and sort them\n            querylib_files.sort(\n                key=lambda filename: datetime.strptime(\n                    filename.split(\"_\")[-1].split(\".\")[0], \"%Y%m%d\"\n                ),\n                reverse=True,\n            )\n            # Pick the most recent file if any are found\n            self.querylib_file = querylib_files[0] if querylib_files else None\n\n        self.sleep_sec = 2\n        self.assistant_type = \"gpt4turbo\"\n        self.top_k_prompt = 2\n        self.top_k_screening = 10\n        self.sim_threshold = 0.0\n\n        if Rag.querylib is None:\n            Rag.querylib = self.load_querylib()\n\n    def load_querylib(self):\n        # Assuming QueryLibrary is a class defined elsewhere\n        querylib = QueryLibrary(\n            querylib_name=\"patient_counts\",\n            source=\"gold_label_dec_2023\",\n            querylib_source_file=None,\n            col_question=\"QUESTION\",\n            col_question_masked=\"QUESTION_MASKED\",\n            col_query_w_placeholders=\"QUERY_SNOWFLAKE_WITH_PLACEHOLDERS\",\n            col_query_executable=\"QUERY_SNOWFLAKE_RUNNABLE\",\n        )\n\n        querylib = querylib.load(querylib_file=self.querylib_file)\n\n        # Assuming an embedding model and logger are defined elsewhere\n        querylib.load_embedding_model(embedding_model_name=\"BAAI/bge-large-en-v1.5\")\n\n        logging.info(f\"Embedding loaded from {self.querylib_file}\")\n\n        return querylib\n\n\nclass AgentRag(Rag):\n    def __init__(self, **kwargs):\n        super().__init__(\n            main_path=kwargs.get(\"main_path\"),\n            log_folder=kwargs.get(\"log_folder\"),\n            querylib_file=kwargs.get(\"querylib_file\"),\n        )\n        # Override specific properties for AgentRag\n        self.sleep_sec = 2\n        self.top_k_prompt = 2\n        self.top_k_screening = 10\n        self.sim_threshold = 0.0\n\n        # Override the prompt method\n        self.prompt = prompts.prompt_gpt\n\n        # Create new instances for the assistant and med_sql_processor\n        self.database = kwargs.get(\"database\")\n        self.assistant = kwargs.get(\n            \"assistant\", create_assistant(assistant_type=self.assistant_type)\n        )\n        self.assistant_answers = kwargs.get(\n            \"assistant_answers\", create_assistant(assistant_type=self.assistant_type)\n        )\n"}
{"type": "source_file", "path": "text2sql_epi/rwd_request.py", "content": "# coding=utf-8\n__author__ = \"Angelo Ziletti\"\n__maintainer__ = \"Angelo Ziletti\"\n__email__ = \"angelo.ziletti@bayer.com\"\n__date__ = \"24/11/23\"\n\nimport asyncio\nimport logging\nimport re\n\nimport pandas as pd\nfrom sqlalchemy import exc as sa_exc, text\nfrom sqlalchemy.exc import SQLAlchemyError\n\nlogger = logging.getLogger(__name__)\n\n\nclass RWDRequest:\n    def __init__(\n        self,\n        question,\n        query_filled=None,\n        query_template=None,\n        retrieved_data=None,\n        answer=None,\n    ):\n        self.question = question\n        self.query_filled = query_filled\n        self.query_template = query_template\n        self.retrieved_data = retrieved_data\n        self.answer = answer\n        self.sql_executed = False\n        self.sql_executed_self_healing_attempts = 0\n        self.rag = None\n        self.query_df_retrieved_rag = None\n        self.prompt = None\n        self.rag_top_similarity = 0.0\n        self.question_masked = None\n\n    async def get_answer(self, assistant, max_lines=100):\n        if self.retrieved_data is not None:\n            prompt = f\"\"\"\n                This is the data retrieved from our database: {self.retrieved_data[:max_lines].to_markdown()} which is the sufficient to answer the question \"{self.question}\".\\n\n                - Please provide a concise answer to the following question: {self.question}\n                - Assume all provided data is relevant and necessary for the response.\n                - If the question refers to a distribution, please only include summary statistics in your answer.\n                - Please refrain from offering data comparisons, conducting trend analysis, or attempting to create plots or visualizations in your response.\n                - Please specify if the response contains an approximation rather than the precise result.\n                - Please include all relevant data in your answer.\n                \"\"\"\n            answer = await assistant.get_response(prompt)\n            logger.info(f\"Getting answer for '{self.question}'...\")\n            self.answer = answer\n\n    async def run_query(\n        self, sql_query, db, assistant, max_retries=5, reset_conversation=True\n    ):\n        if reset_conversation:\n            assistant.reset_conversation()\n\n        if sql_query is None:\n            logger.info(\"Error in post processing SQL query\")\n            return None\n\n        loop = asyncio.get_running_loop()\n\n        for attempt in range(max_retries):\n            try:\n                # Run the blocking db.execute call in a separate thread\n                results = await loop.run_in_executor(\n                    None, lambda: db.execute(text(sql_query)).fetchall()\n                )\n                df = pd.DataFrame(results)\n                self.sql_executed = True\n                self.sql_executed_self_healing_attempts = attempt\n                self.retrieved_data = df\n                return df\n            except (SQLAlchemyError, sa_exc.ProgrammingError) as db_ex:\n                logger.error(\"Error in SQL detected\")\n                logger.error(\"sql query that failed:\")\n                logger.warning(sql_query)\n                logger.info(\n                    f\"Self-healing process in progress. Attempt: {attempt}/{max_retries}\"\n                )\n                if assistant is not None:\n                    sql_query = await self.handle_invalid_sql(\n                        sql_query, assistant, db_ex.args[0]\n                    )\n                else:\n                    logger.warning(\n                        \"gpt assistant required for self-healing process. Continuing without.\"\n                    )\n            except Exception as e:\n                logger.exception(\"Error in SQL could not be resolved\")\n                logger.info(e)\n                logger.info(f\"post_processed_sql: {sql_query}\")\n                return None\n\n        logger.info(\"Max retries reached without successful SQL execution\")\n        return None\n\n    async def handle_invalid_sql(self, sql_text, assistant, error):\n        prompt = f\"\"\"Generated SQL query: \\n {sql_text} \\n \n                Error returned: {error}.\\n \n                **IMPORTANT**: Analyze the error. Review the generated SQL. Think about the OMOP CDM schema. Fix and rewrite the SQL query.\\n\n                **IMPORTANT**: Please only return the corrected SQL query. Do not return any extraneous data or information.\\n\n                **IMPORTANT:** Return the SQL query ONLY within ```sql ``` code block.\n                **IMPORTANT**: Do not replace or remove the provided concept id's, especially within the WHERE clause like in: `condition_concept_id IN (...some numbers)`. Preserve these as they are.\\n\n                **IMPORTANT**: Never assign concept_id's with the equal sign (=), always use `IN` when working with concept_id's. This contributes to code readability and SQL best practices.\n            \"\"\"\n        completed_prompt = await assistant.get_response(prompt)\n        logger.info(completed_prompt)\n        # logger.info(\"Trying again\")\n        new_query = self.parse_sql_from_response(completed_prompt)\n        logger.info(\"new_query\")\n        logger.info(new_query)\n        logger.info(\"-------------\")\n        return new_query\n\n    @staticmethod\n    def parse_sql_from_response(resp=\"\"):\n        pattern1 = r\"(?:Snowflake )?SQL query:\\s*\\n\\n([\\s\\S]+?);\"\n        pattern2 = r\"(?:```sql|```) ?\\n([\\s\\S]+?)\\n```\"\n        # pattern3 = r\"(WITH\\s[\\s\\S]+?LIMIT \\d+;)\"\n        match1 = re.search(pattern1, resp)\n        match2 = re.search(pattern2, resp)\n        # match3 = re.search(pattern3, resp)\n        # if match3:\n        #     return match3.group(1)\n        if match2:\n            return match2.group(1)\n        elif match1:\n            return match1.group(1) + \";\"\n        else:\n            logger.info(\"No SQL code found.\")\n\n    @classmethod\n    def from_dict(cls, data):\n        obj = cls(\n            question=data[\"question\"],\n            query_filled=data[\"query_filled\"],\n            query_template=data[\"query_template\"],\n            retrieved_data=(\n                pd.read_json(data[\"retrieved_data\"])\n                if data[\"retrieved_data\"] is not None\n                else None\n            ),\n            answer=data[\"answer\"],\n        )\n        obj.sql_executed = data[\"sql_executed\"]\n        obj.sql_executed_self_healing_attempts = data[\n            \"sql_executed_self_healing_attempts\"\n        ]\n        obj.rag = data[\"rag\"]\n        obj.query_df_retrieved_rag = data[\"query_df_retrieved_rag\"]\n        obj.prompt = data[\"prompt\"]\n        obj.rag_top_similarity = data[\"rag_top_similarity\"]\n        obj.question_masked = data[\"question_masked\"]\n        return obj\n\n    def to_dict(self):\n        return {\n            \"question\": self.question,\n            \"query_filled\": self.query_filled,\n            \"query_template\": self.query_template,\n            \"retrieved_data\": (\n                self.retrieved_data.to_json(orient=\"records\")\n                if self.retrieved_data is not None\n                else None\n            ),\n            \"answer\": self.answer,\n            \"sql_executed\": self.sql_executed,\n            \"sql_executed_self_healing_attempts\": self.sql_executed_self_healing_attempts,\n            \"rag\": self.rag,\n            \"query_df_retrieved_rag\": (\n                self.query_df_retrieved_rag.to_json(orient=\"records\")\n                if self.query_df_retrieved_rag is not None\n                else None\n            ),\n            \"prompt\": self.prompt,\n            \"rag_top_similarity\": self.rag_top_similarity,\n            \"question_masked\": self.question_masked,\n        }\n"}
{"type": "source_file", "path": "text2sql_epi/snowflake_session.py", "content": "import logging\nfrom typing import Any, Generator, Optional\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.engine import Engine\nfrom sqlalchemy.orm import sessionmaker\n\nfrom text2sql_epi.settings import settings\n\nlogger = logging.getLogger(__name__)\n\nsnowflake_url = (\n    f\"snowflake://{settings.SNOWFLAKE_USER}:{settings.SNOWFLAKE_PASSWORD}@\"\n    f\"{settings.SNOWFLAKE_ACCOUNT_IDENTIFIER}/{{database}}?warehouse={settings.SNOWFLAKE_WAREHOUSE}\"\n)\n\nengine_cache = {}\n\n\ndef get_engine_for_db(db_name: Optional[str] = None) -> Engine:\n    database = db_name if db_name else \"OPTUM_CLAIMS_OMOP\"\n    if database not in engine_cache:\n        engine_url = snowflake_url.format(database=database)\n        engine_cache[database] = create_engine(\n            engine_url,\n            connect_args={\n                \"client_session_keep_alive\": True,\n                \"timeout\": settings.SNOWFLAKE_TIMEOUT,\n            },\n            pool_size=20,\n            max_overflow=10,\n        )\n    return engine_cache[database]\n\n\n# Configure SessionLocal to be bound later\nSessionLocal = sessionmaker(autocommit=False, autoflush=False)\n\nschema_cache = {}\n\n\ndef get_db(db_name: Optional[str] = None) -> Generator:\n    engine = get_engine_for_db(db_name)\n    SessionLocal.configure(bind=engine)\n    with SessionLocal() as db:\n        db.execute(\n            f\"ALTER SESSION SET STATEMENT_TIMEOUT_IN_SECONDS = {settings.SNOWFLAKE_TIMEOUT}\"\n        )\n        try:\n            if db_name:\n                if db_name not in schema_cache:\n                    schema_cache[db_name] = get_current_schema(db, db_name)\n                schema_name = schema_cache[db_name]\n                db.execute(f'USE SCHEMA \"{db_name}\".\"{schema_name}\"')\n            yield db\n        except Exception as e:\n            logger.exception(f\"Error while connecting to the database: {e}\")\n            raise\n        finally:\n            db.close()\n\n\ndef get_current_schema(db: Any, database_name: str) -> str:\n    schema_query = f\"show schemas in database {database_name};\"\n    _ = db.execute(schema_query)\n    schema_name_query = 'select max(\"name\") from TABLE(RESULT_SCAN(LAST_QUERY_ID())) where \"name\" like \\'CDM%\\';'\n    schema_name = db.execute(schema_name_query).scalar()\n    return schema_name\n"}
{"type": "source_file", "path": "text2sql_epi/assistants.py", "content": "import json\nimport logging\nimport os\nfrom datetime import datetime, timezone\nfrom typing import Optional\n\nimport requests\nimport tiktoken\nfrom openai import AsyncAzureOpenAI\n\nfrom text2sql_epi.settings import settings\n\nlogger = logging.getLogger(__name__)\n\n\nclass GPTAssistant:\n    def __init__(self, engine=None):\n        self.engine = engine\n        self.system_message = {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant.\",\n        }\n        self.max_response_tokens = 4096\n        self.token_limit = 8192 * 2\n        self.conversation = [self.system_message]\n        self.client = AsyncAzureOpenAI(\n            api_key=settings.OPENAI_API_KEY,\n            api_version=settings.OPENAI_API_VERSION,\n            azure_endpoint=settings.OPENAI_API_BASE,\n        )\n\n    def num_tokens_from_messages(self, messages):\n        encoding = tiktoken.encoding_for_model(\"gpt-4-32k\")\n        num_tokens = 0\n        for message in messages:\n            num_tokens += (\n                4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n            )\n            for key, value in message.items():\n                num_tokens += len(encoding.encode(value))\n                if key == \"name\":  # if there's a name, the role is omitted\n                    num_tokens += -1  # role is always required and always 1 token\n        num_tokens += 2  # every reply is primed with <im_start>assistant\n        return num_tokens\n\n    def add_message(self, role, message):\n        self.conversation.append({\"role\": role, \"content\": message})\n        self.manage_conversation_length()\n\n    def reset_conversation(self):\n        self.conversation = [self.system_message]\n\n    def manage_conversation_length(self):\n        conv_history_tokens = self.num_tokens_from_messages(self.conversation)\n\n        while conv_history_tokens + self.max_response_tokens >= self.token_limit:\n            del self.conversation[1]\n            conv_history_tokens = self.num_tokens_from_messages(self.conversation)\n\n    async def get_response(self, prompt: Optional[str] = None):\n        messages = (\n            [{\"role\": \"user\", \"content\": prompt}]\n            if prompt is not None\n            else self.conversation\n        )\n        try:\n            response = await self.client.chat.completions.create(\n                model=self.engine,\n                temperature=0,\n                messages=messages,\n                max_tokens=self.max_response_tokens,\n            )\n        except Exception as err:\n            logger.exception(\"An error occurred.\")\n            raise err\n        logger.info(\n            f\"Successful GPT response! endpoint: {settings.OPENAI_API_BASE}, model: {self.engine}, usage: {str(response.usage)}, utc-timestamp: {datetime.now(timezone.utc).strftime('%Y.%m.%d %H:%M')}, message:{str(messages)}, response-content: {response.choices[0].message.content}\"\n        )\n        if prompt is None:\n            self.add_message(\n                role=\"assistant\", message=response.choices[0].message.content\n            )\n\n        return response.choices[0].message.content\n\n    async def get_response_json(self, prompt: Optional[str] = None):\n        messages = (\n            [{\"role\": \"user\", \"content\": prompt}]\n            if prompt is not None\n            else self.conversation\n        )\n        logger.info(\n            f\"Sending GPT request... endpoint: {settings.OPENAI_API_BASE}, model: {self.engine}, message-tokens: {self.num_tokens_from_messages(messages)}, max_response_tokens: {self.max_response_tokens}, utc-timestamp: {datetime.now(timezone.utc).strftime('%Y.%m.%d %H:%M')}, message:{str(messages)}\"\n        )\n        try:\n            response = await self.client.chat.completions.create(\n                model=self.engine,\n                response_format={\"type\": \"json_object\"},\n                temperature=0,\n                messages=messages,\n                max_tokens=self.max_response_tokens,\n            )\n        except Exception as err:\n            logger.exception(\"An error occurred\")\n            raise err\n        logger.info(\n            f\"Successful GPT response! endpoint: {settings.OPENAI_API_BASE}, model: {self.engine}, message-tokens: {self.num_tokens_from_messages(messages)}, max_response_tokens: {self.max_response_tokens}, utc-timestamp: {datetime.now(timezone.utc).strftime('%Y.%m.%d %H:%M')}, message:{str(messages)}, response-content: {response.choices[0].message.content}\"\n        )\n        if prompt is None:\n            self.add_message(\n                role=\"assistant\", message=response.choices[0].message.content\n            )\n\n        return response.choices[0].message.content\n\n\nclass MistralAssistant:\n    def __init__(self, model, mistral_api_key=None):\n\n        if mistral_api_key is None:\n            self.mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n        else:\n            self.mistral_api_key = mistral_api_key\n        self.url = \"https://api.mistral.ai/v1/chat/completions\"\n        self.model = model\n\n    def get_response(self, message):\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Accept\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.mistral_api_key}\",\n        }\n\n        data = {\"model\": self.model, \"messages\": [{\"role\": \"user\", \"content\": message}]}\n\n        response = requests.post(self.url, headers=headers, data=json.dumps(data))\n\n        if response.status_code == 200:\n            return response.json()[\"choices\"][0][\"message\"][\"content\"]\n        else:\n            return response.text\n\n    def reset_conversation(self):\n        pass\n\n\ndef get_engine_from_assistant_type(assistant_type):\n    if assistant_type == \"gpt4turbo\":\n        assistant_engine = \"gpt-4-turbo-1106-preview-ascent\"\n    elif assistant_type == \"gpt4turbo-south\":\n        assistant_engine = \"gpt-4-turbo-0125-preview-ascent\"\n    elif assistant_type == \"gpt4\":\n        assistant_engine = \"gpt_4_32k_ascent_0613\"\n    elif assistant_type == \"gpt35\":\n        assistant_engine = \"gpt-35-turbo-1106-ascent\"\n    else:\n        assistant_engine = None\n    return assistant_engine\n\n\ndef create_assistant(assistant_type):\n    assistant_classes = {\n        \"gpt4turbo\": GPTAssistant,\n        \"gpt4\": GPTAssistant,\n        \"gpt35\": GPTAssistant,\n        \"mistral-tiny\": MistralAssistant,\n        \"mistral-small\": MistralAssistant,\n        \"mistral-medium\": MistralAssistant,\n    }\n\n    engine = get_engine_from_assistant_type(assistant_type)\n\n    if isinstance(assistant_type, str) and assistant_type in assistant_classes:\n        if (\n            assistant_type == \"gpt4\"\n            or assistant_type == \"gpt4turbo\"\n            or assistant_type == \"gpt4turbo-south\"\n            or assistant_type == \"gpt35\"\n        ):\n            assistant = assistant_classes[assistant_type](engine=engine)\n        elif (\n            assistant_type == \"mistral-tiny\"\n            or assistant_type == \"mistral-small\"\n            or assistant_type == \"mistral-medium\"\n        ):\n            assistant = assistant_classes[assistant_type](model=assistant_type)\n        else:\n            assistant = assistant_classes[assistant_type]()\n        return assistant\n    else:\n        logger.exception(\"Please specify a valid assistant\")\n        return None\n"}
{"type": "source_file", "path": "text2sql_epi/helpers.py", "content": "import logging\nfrom typing import Optional\n\nimport pandas as pd\n\nfrom text2sql_epi.rag import Rag\nfrom text2sql_epi.rwd_request import RWDRequest\n\nfrom text2sql_epi import prompts\n\nlogger = logging.getLogger(__name__)\n\n\ndef prepare_prediction(user_input: str, prompt: str):\n    new_prompt = prompt.replace(\"${question}\", user_input.replace(\"'\", \"\\\\'\"))\n    return new_prompt\n\n\nasync def get_text_sql_template_for_rag(\n    question_masked: str,\n    rag: Rag,\n    rag_random: Optional[bool] = None,\n    drop_first: Optional[bool] = None,\n):\n    params_dict = {\n        \"question_masked\": question_masked,\n        \"top_k_screening\": rag.top_k_screening,\n        \"top_k_prompt\": rag.top_k_prompt,\n        \"sim_threshold\": rag.sim_threshold,\n    }\n\n    if rag_random is not None:\n        params_dict[\"rag_random\"] = rag_random\n    if drop_first is not None:\n        params_dict[\"drop_first\"] = drop_first\n\n    (text_sql_template, df_recs_list_out) = (\n        await rag.querylib.text_sql_template_for_rag(**params_dict)\n    )\n    return text_sql_template, df_recs_list_out\n\n\ndef prepare_rwd_request(\n    question: str,\n    query_filled_pred: str,\n    query_template_pred: str,\n    question_masked: str,\n    df_recs_list_out: pd.DataFrame,\n    new_prompt: str,\n):\n    rwd_request_pred = RWDRequest(\n        question=question,\n        query_filled=query_filled_pred,\n        query_template=query_template_pred,\n    )\n    rwd_request_pred.rag = True\n    rwd_request_pred.question_masked = question_masked\n    rwd_request_pred.query_df_retrieved_rag = df_recs_list_out\n    rwd_request_pred.rag_top_similarity = df_recs_list_out[\"Score\"].max()\n    rwd_request_pred.prompt = new_prompt\n    return rwd_request_pred\n\n\nasync def prepare_gpt_call(user_input: str, rag_agent):\n    question_masked, question = await rag_agent.querylib.get_masked_question(\n        prompts=prompts, question=user_input, assistant=rag_agent.assistant\n    )\n    initial_prompt = prepare_prediction(\n        question, prompt=prompts.prompt_gpt\n    )\n    text_sql_template, df_recs_list_out = (\n        await get_text_sql_template_for_rag(\n            question_masked=question_masked, rag=rag_agent\n        )\n    )\n    add_messages_to_assistant([initial_prompt, text_sql_template], rag_agent.assistant)\n    return initial_prompt, text_sql_template, df_recs_list_out, question_masked\n\n\ndef add_messages_to_assistant(messages: list, assistant=None):\n    for message in messages:\n        role = \"system\"\n        assistant.add_message(role=role, message=message)\n"}
{"type": "source_file", "path": "text2sql_epi/query_library.py", "content": "# coding=utf-8\n__author__ = \"Angelo Ziletti\"\n__maintainer__ = \"Angelo Ziletti\"\n__email__ = \"angelo.ziletti@bayer.com\"\n__date__ = \"24/11/23\"\n\nimport logging\nimport os.path\nimport pickle\nimport time\nfrom concurrent.futures import as_completed, ThreadPoolExecutor\nfrom datetime import date\nfrom typing import Optional\n\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import normalize\nfrom tqdm import tqdm\n\ntqdm.pandas()\n\nlogger = logging.getLogger(__name__)\n\n\nclass QueryLibrary:\n    \"\"\"Collection of queries for retrieval augmented generation\"\"\"\n\n    def __init__(\n        self,\n        querylib_name: str,\n        source: str,\n        querylib_source_file: object,\n        col_question: str,\n        col_question_masked: str,\n        col_query_w_placeholders: str,\n        col_query_executable: Optional[str] = None,\n        date_live: Optional[date] = None,\n    ) -> None:\n        self.querylib_name = querylib_name\n        self.date_live = date_live\n        self.source = source\n        self.col_question = col_question\n        self.col_question_masked = col_question_masked\n        self.col_query_w_placeholders = col_query_w_placeholders\n        self.col_query_executable = col_query_executable\n\n        if querylib_source_file:\n            df_querylib = pd.read_excel(querylib_source_file)\n            self.df_querylib = df_querylib\n        else:\n            self.df_querylib = pd.DataFrame()\n\n        self.embeddings = []\n\n        self.embedding_model = None\n\n    def __len__(self):\n        return len(self.df_querylib)\n\n    def calc_embedding(\n        self, embedding_model_name=\"BAAI/bge-large-en-v1.5\", use_masked=True\n    ):\n        # check this: https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder\n        embedding_model = SentenceTransformer(embedding_model_name)\n        if use_masked:\n            col_txt = self.col_question_masked\n        else:\n            col_txt = self.col_question\n\n        embed_series = self.df_querylib[col_txt].progress_apply(\n            lambda x: embedding_model.encode(x, normalize_embeddings=True)\n        )\n\n        # as output you get a series of arrays, convert it to a matrix\n        # https://stackoverflow.com/questions/40824601/how-to-convert-a-series-of-arrays-into-a-single-matrix-in-pandas-numpy\n        embed_matrix = np.stack(embed_series.values)\n\n        logger.info(\"Dataset embedded. Shape: {}\".format(embed_matrix.shape))\n\n        embedding = {\n            \"model_name\": str(embedding_model_name),\n            \"embed_matrix\": embed_matrix,\n        }\n        self.embeddings.append(embedding)\n\n        logger.info(\"Embedding calculated with model {}\".format(embedding_model_name))\n        logger.info(\" Embedding matrix shape: {}\".format(embed_matrix.shape))\n        self.embedding_model = embedding_model\n\n    def save(self, querylib_file):\n        # Extract the directory part from the file path\n        directory = os.path.dirname(querylib_file)\n\n        # Check if the directory exists, and create it if it doesn't\n        if not os.path.exists(directory):\n            os.makedirs(directory, exist_ok=True)  # exist_ok=True is to avoid error if the directory already exists\n\n        # Now save the file as before\n        with open(querylib_file, \"wb\") as out_file:\n            pickle.dump(self, out_file)\n\n    def load_embedding_model(self, embedding_model_name):\n        self.embedding_model = SentenceTransformer(embedding_model_name)\n\n    @staticmethod\n    def load(querylib_file):\n        try:\n            with open(querylib_file, \"rb\") as out_file:\n                query_lib_data = pickle.load(out_file)\n                logger.info(\"Query library data read from {}\".format(querylib_file))\n            return query_lib_data\n        except Exception as e:\n            logger.exception(\"An error occured {}\".format(e))\n            return None\n\n    def extract_idx_records(self, values_to_extract, source_col):\n        idx_records = self.df_querylib.index[\n            self.df_querylib[source_col].isin(values_to_extract)\n        ].tolist()\n        return idx_records\n\n    def extract_embed_matrix(self, value_rows_to_extract, extract_col_name, embedding):\n        # Find the rows in the ontology that match name_rows_to_extract\n        idx_records = self.extract_idx_records(value_rows_to_extract, extract_col_name)\n\n        # Get the corresponding embedding matrix\n        embed_matrix = embedding[\"embed_matrix\"][idx_records]\n\n        # Get the names from the matrix so they match the embeddings\n        value_rows_embed = self.df_querylib.loc[idx_records][\n            extract_col_name\n        ].reset_index(drop=True)\n\n        return embed_matrix, value_rows_embed\n\n    @staticmethod\n    def add_separator_to_input_entities(lst, sep=\"[SEP_P]\"):\n        joined_list = []\n        for inner_list in lst:\n            joined_list.append(f\" {sep} \".join(inner_list))\n        return joined_list\n\n    def get_similar_questions(\n        self,\n        samples,\n        top_k=5,\n        sim_threshold=0.95,\n        normalize_score=True,\n        col_search=None,\n        max_rows=1000,\n        tmp_dir=None,\n        export_txt=False,\n    ):\n        if col_search is None:\n            col_search = self.col_question\n\n        df_querylib_selected = self.df_querylib\n\n        embed_matrix, names_avail = self.extract_embed_matrix(\n            value_rows_to_extract=df_querylib_selected[self.col_question].tolist(),\n            extract_col_name=self.col_question,\n            embedding=self.embeddings[0],\n        )\n\n        # Cast the input samples in a dataframe for convenience\n        samples_with_sep = self.add_separator_to_input_entities(samples)\n        df_input_names = pd.DataFrame(samples_with_sep, columns=[self.col_question])\n\n        df_input_names_list = [\n            df_input_names[i : i + max_rows]\n            for i in range(0, df_input_names.shape[0], max_rows)\n        ]\n\n        df_recap_recs_list = []\n        df_recs_list = []\n\n        outfile_recap_recs_list = []\n        outfile_recs_list = []\n\n        # Parallel processing for get_similar_names\n        with ThreadPoolExecutor() as executor:\n            futures = {\n                executor.submit(\n                    self.get_similar_names,\n                    df_input_chunk,\n                    names_avail,\n                    embed_matrix=embed_matrix,\n                    embedding_model=self.embedding_model,\n                    col_search=col_search,\n                    normalize_score=normalize_score,\n                    top_k=top_k,\n                    sim_threshold=sim_threshold,\n                    export_txt=export_txt,\n                ): idx\n                for idx, df_input_chunk in enumerate(df_input_names_list)\n            }\n\n            for future in as_completed(futures):\n                idx = futures[future]\n                df_recap_recs, df_recs = future.result()\n\n                if tmp_dir is not None:\n                    outfile_recap_recs = os.path.join(tmp_dir, f\"recap_recs_{idx}\")\n                    outfile_recs = os.path.join(tmp_dir, f\"recs_{idx}\")\n\n                    with open(outfile_recap_recs, \"wb\") as out_file:\n                        pickle.dump(df_recap_recs, out_file)\n\n                    with open(outfile_recs, \"wb\") as out_file:\n                        pickle.dump(df_recs, out_file)\n\n                    outfile_recap_recs_list.append(outfile_recap_recs)\n                    outfile_recs_list.append(outfile_recs)\n\n                    del df_recap_recs, df_recs\n                else:\n                    df_recap_recs_list.append(df_recap_recs)\n                    df_recs_list.extend(df_recs)\n\n        if tmp_dir is not None:\n            df_recap_recs_list = []\n            for outfile_recap in outfile_recap_recs_list:\n                with open(outfile_recap, \"rb\") as out_file:\n                    df_recap_recs_list.append(pickle.load(out_file))\n\n            df_recs_list = []\n            for outfile_rec in outfile_recs_list:\n                with open(outfile_rec, \"rb\") as out_file:\n                    df_recs_list.extend(pickle.load(out_file))\n\n        df_recap_recs = pd.concat(df_recap_recs_list)\n\n        return df_recap_recs, df_recs_list\n\n    def get_similar_names(\n        self,\n        df,\n        classes,\n        embed_matrix,\n        embedding_model,\n        col_search,\n        suffix=\"unsupervised\",\n        normalize_score=True,\n        top_k=20,\n        sim_threshold=0.0,\n        top_k_limit=None,\n        export_txt=True,\n    ):\n        if top_k_limit is None:\n            top_k_limit = len(classes)\n\n        logger.debug(\"Retrieving the most similar classes\")\n\n        # remove leading and trailing spaces\n        df[col_search] = df[col_search].astype(str).str.strip()\n\n        # Compute embeddings in batches (assuming embedding_model can handle batch input)\n        text_embeddings = embedding_model.encode(\n            df[self.col_question].tolist(), normalize_embeddings=True\n        )\n\n        logger.debug(f\"Text embedding size: {text_embeddings.nbytes / 10 ** 6} (Mb)\")\n        logger.debug(f\"text_embedding shape: {text_embeddings.shape}\")\n\n        if normalize_score:\n            text_embeddings = normalize(text_embeddings)\n            embed_matrix = normalize(embed_matrix)\n\n        # Efficient matrix multiplication\n        sim_matrix = text_embeddings @ embed_matrix.T\n\n        logger.debug(f\"Similarity matrix size: {sim_matrix.nbytes / 10 ** 6} (Mb)\")\n        logger.debug(f\"sim_matrix shape: {sim_matrix.shape}\")\n\n        # Efficient top-k selection\n        idx_match_sorted = np.argpartition(-sim_matrix, kth=top_k_limit - 1, axis=1)[\n            :, :top_k_limit\n        ]\n\n        matched_classes_list = []\n        scores_list = []\n        code_list = []\n        df_class_score_list = []\n\n        for idx_input in range(sim_matrix.shape[0]):\n            idx_match_input = idx_match_sorted[idx_input]\n            sim_matrix_row_sorted = -np.partition(\n                -sim_matrix[idx_input], kth=top_k_limit - 1\n            )[:top_k_limit]\n            class_text_sorted = classes[idx_match_input]\n\n            df_class_scores = pd.DataFrame(\n                zip(class_text_sorted, sim_matrix_row_sorted),\n                columns=[\"Class\", \"Score\"],\n            )\n            df_class_scores = df_class_scores.nlargest(top_k, \"Score\")\n\n            matched_classes_list.append(df_class_scores[\"Class\"].tolist())\n            scores_list.append(df_class_scores[\"Score\"].tolist())\n\n            df_class_scores[self.col_question] = df_class_scores[\"Class\"]\n            code_list.append(df_class_scores[self.col_question].tolist())\n\n            if not export_txt:\n                df_class_scores = df_class_scores.drop(\n                    [\"Class\"], axis=1, errors=\"ignore\"\n                )\n\n            df_class_score_list.append(df_class_scores)\n\n        df[f\"rec_{suffix}_questions\"] = code_list\n        df[f\"rec_{suffix}_scores\"] = scores_list\n\n        return df, df_class_score_list\n\n    def get_df_recs(self, question, top_k, sim_threshold):\n        df_recap_recs, df_recs_list = self.get_similar_questions(\n            question,\n            top_k=top_k,\n            sim_threshold=sim_threshold,\n        )\n\n        # here the list is only one element long because we pass only one question\n        df_recs_list_merged = []\n        for df_recs in df_recs_list:\n            df_merged = df_recs.merge(self.df_querylib, on=self.col_question, how=\"left\")\n            df_recs_list_merged.append(df_merged)\n\n        # here we only have one question thus the list is always one element\n        # TO DO: this is not ideal and it should be improve later\n        df_recs_list_out = df_recs_list_merged[0]\n        return df_recs_list_out\n\n    async def text_sql_template_for_rag(\n        self,\n        question_masked,\n        top_k_screening,\n        top_k_prompt,\n        sim_threshold,\n        reverse_order=False,\n        rag_random=False,  # Parameter for random retrieval\n        drop_first=False,  # Parameter to drop the first element\n    ):\n        df_recs_list_out = self.get_df_recs(\n            [[question_masked]],\n            top_k=top_k_screening,\n            sim_threshold=sim_threshold,\n        )\n\n        # If reverse_order is True, reverse the order of the DataFrame\n        if reverse_order:\n            df_recs_list_out = df_recs_list_out.sort_index(ascending=False)\n\n        # If drop_first is True, drop the first element from the DataFrame\n        if drop_first:\n            logger.warning(\"Dropping first element of the retrieved queries\")\n            df_recs_list_out = df_recs_list_out.drop(df_recs_list_out.index[0])\n\n        # If rag_random is True, randomly select one sample from the top-k\n        if rag_random:\n            logger.warning(\"Using random retrieval for RAG\")\n            df_recs_list_out = df_recs_list_out.sample(n=1)\n\n        # Keep only the top_k_prompt elements\n        df_recs_list_out = df_recs_list_out.head(top_k_prompt)\n\n        initial_sentence = \"You might find these example queries helpful: \"\n        # include both question and query in the prompt\n        text_sql_template = (\n            initial_sentence\n            + \"\\n\\n\"\n            + \"\\n\\n\".join(\n                f\"#Question:\\n{rec[self.col_question]}\\n#SQL query:\\n{rec[self.col_query_w_placeholders]}\"\n                for rec in df_recs_list_out.to_dict(\"records\")\n            )\n        )\n\n        return text_sql_template, df_recs_list_out\n\n    @staticmethod\n    async def get_masked_question(\n        prompts,\n        question,\n        assistant,\n        sleep_sec=3,\n        reset_conversation=True,\n        mask=\"DRUG_CLASS\",\n    ):\n        \"\"\"\n        :param prompts: List of prompts\n        :param question: User question\n        :param assistant: Assistant to use\n        :param sleep_sec: Number of seconds to sleep\n        :param reset_conversation: True or False to reset the conversation\n        :param mask: Mask to apply\n        :return: masked question, question\n        \"\"\"\n        if reset_conversation:\n            assistant.reset_conversation()\n        prompt = prompts.entity_masking.format(question=question)\n        assistant.add_message(role=\"user\", message=prompt)\n        masked_question = await assistant.get_response()\n        if mask in masked_question:\n            prompt_drug_class = prompts.drug_class_keep.format(question=question)\n            question = await assistant.get_response(prompt_drug_class)\n            if masked_question.count(mask) > 1 or (\n                masked_question.count(mask) == 1 and masked_question.count(\"DRUG\") > 1\n            ):\n                question += (\n                    \" Can you output also intermediate results for each drug class?\"\n                )\n            prompt = prompts.entity_masking.format(question=question)\n            masked_question = await assistant.get_response(prompt)\n\n        logger.info(f\"Masked question: {masked_question}\")\n        time.sleep(sleep_sec)\n        return masked_question, question\n\n\nclass MedCodingOnto(QueryLibrary):\n    def __init__(\n        self,\n        ontolib_name: str,\n        source: str,\n        ontolib_source_file: object,\n        col_text: str,\n        date_live: Optional[date] = None,\n        # Add any additional parameters specific to your new class here\n    ) -> None:\n        super().__init__(\n            querylib_name=ontolib_name,\n            source=source,\n            querylib_source_file=ontolib_source_file,\n            col_question=col_text,\n            col_question_masked=col_text,\n            date_live=date_live,\n            col_query_w_placeholders=None,\n            col_query_executable=None\n        )\n        self.querylib_name = ontolib_name\n        self.date_live = date_live\n        self.source = source\n        self.col_question = col_text\n        self.col_question_masked = col_text\n        self.col_query_w_placeholders = None\n        self.col_query_executable = None\n\n        if ontolib_source_file:\n            df_querylib = pd.read_excel(ontolib_source_file)\n            self.df_querylib = df_querylib\n        else:\n            self.df_querylib = pd.DataFrame()\n\n        self.embeddings = []\n\n        self.embedding_model = None\n        # Add any additional initialization specific to your new class here\n\n    async def get_similar_codes_from_onto(\n            self,\n            question_masked,\n            top_k_screening,\n            top_k_prompt,\n            sim_threshold\n    ):\n        df_recs_list_out = self.get_df_recs(\n            [[question_masked]],\n            top_k=top_k_screening,\n            sim_threshold=sim_threshold,\n        )\n\n        # Keep only the top_k_prompt elements\n        df_recs_list_out = df_recs_list_out.head(top_k_prompt)\n\n        return df_recs_list_out\n"}
{"type": "source_file", "path": "text2sql_epi/sql_post_processor.py", "content": "import asyncio\nimport logging\nimport re\n\n\nlogger = logging.getLogger(__name__)\n\nICD_VOCABULARIES = [\"ICD10CM\", \"ICD9CM\"]\nSNOMED_VOCABULARIES = [\"SNOMED\"]\n\n\nclass MedicalSQLProcessor:\n    def __init__(self, assistant=None):\n        self.assistant = assistant\n        self.condition = []\n        self.procedure = []\n        self.drug = []\n        self.measurement = []\n        self.concept_not_found = []\n\n    async def get_replacement_value(self, entity, name, medcodeonto=None):\n        entity_to_domain_id = {\n            \"condition\": \"Condition\",\n            \"procedure\": \"Procedure\",\n            \"drug\": \"Drug\",\n            \"measurement\": \"Measurement\",\n        }\n\n        if entity in entity_to_domain_id:\n            entity_codes_df = await medcodeonto.get_similar_codes_from_onto(question_masked=name,\n                                                                         top_k_screening=10,\n                                                                         top_k_prompt=4,\n                                                                         sim_threshold=0.0)\n\n            entity_codes = {name: entity_codes_df.to_dict('records')}\n            print(f\"Retrieved codes: {entity_codes}\")\n            print(\"Please note that the medical coding is based on a mockup ontology. \"\n                  \"Results will not be reliable\")\n            # Get the current list for the entity\n            current_list = getattr(self, entity, [])\n            # Append the new entity codes to the list\n            current_list.append(entity_codes)\n            # Update the attribute on the object\n            setattr(self, entity, current_list)\n            return entity_codes\n        else:\n            return []\n\n    async def get_entity_codes(self, name, domain_id, vocabularies):\n        tasks = [\n            self.get_codes(name, domain_id, vocabulary_name)\n            for vocabulary_name in vocabularies\n        ]\n        results = await asyncio.gather(*tasks)\n        return self.merge_results(vocabularies, results)\n\n    def merge_results(self, vocabularies, results):\n        merged_codes = {}\n        for vocabulary_name, codes in zip(vocabularies, results):\n            for key in codes:\n                if key in merged_codes:\n                    merged_codes[key].extend(codes[key])\n                else:\n                    merged_codes[key] = codes[key]\n        return merged_codes\n\n    async def replace_function(self, match, medcodeonto):\n        entity, name = match\n        replacement_value = await self.get_replacement_value(\n            entity, name, medcodeonto\n        )\n        return self.format_replacement_result(replacement_value[name])\n\n    def format_replacement_result(self, result):\n        if len(result) > 0 and \"error\" not in result:\n            concept_ids = [str(concept[\"CONCEPT_ID\"]) for concept in result]\n            return \",\".join(concept_ids)\n        else:\n            self.concept_not_found.append(str(result))\n            return \"NO_CONCEPT_IDS_FOUND\"\n\n    def get_concept_id_not_found(self):\n        return self.concept_not_found\n\n    async def post_process_sql_query(\n        self,\n        sql_text,\n        max_retries=5,\n        sleep_sec=0,\n        explorer_concepts=None,\n        selected_coding=None,\n        rag=None,\n        medcodeonto=None\n    ):\n        \"\"\"\n        Post-processes an SQL query by replacing placeholders with actual values based on the selected coding system\n        and explorer concepts. It also handles retries and user prompts if the SQL query does not meet certain criteria.\n\n        Parameters\n        ----------\n        sql_text : str\n            The initial SQL query text with placeholders for dynamic replacement.\n        max_retries : int\n            The maximum number of retries allowed to correct the SQL query.\n        sleep_sec : int\n            The number of seconds to wait before retrying.\n        explorer_concepts : dict\n            A dictionary of user selected concepts used for replacing placeholders in the SQL query.\n        selected_coding : dict\n            A dictionary containing the selected coding system information. Can be condition, drug, procedure.\n        rag : Rag\n            Contains the assistant for adding messages and getting responses from ai.\n\n        Returns\n        -------\n        str or None\n            The post-processed SQL query text ready for execution, or None if no valid concept IDs are found.\n\n        Raises\n        ------\n        asyncio.TimeoutError\n            If the user does not respond within the given time frame.\n\n        \"\"\"\n        pattern = r\"\\[([a-z]+)@([a-zA-Z0-9_/\\-\\(\\)\\'\\\\ ]+)\\]\"\n        attempts = 0\n\n        while attempts <= max_retries:\n            if selected_coding.get(\"condition\") == ICD_VOCABULARIES:\n                # Replace condition_concept_id to condition_source_concept_id\n                sql_text = self.replace_condition_concept_id_to_condition_source(\n                    sql_text\n                )\n\n            matches = re.findall(pattern, str(sql_text))\n            modified_sql = await self.process_matches(\n                matches, sql_text, explorer_concepts, medcodeonto\n            )\n\n            if \"NO_CONCEPT_IDS_FOUND\" in modified_sql:\n                return None\n\n            # Check if there are any concept_name in text inside the query\n            if not self.is_sql_for_concept_name_in(sql_text):\n                return modified_sql\n\n            # Make the sql correct\n            sql_text = await self.handle_invalid_sql(rag, sleep_sec)\n            attempts += 1\n\n        return sql_text\n\n    async def process_matches(\n        self, matches, sql_text, explorer_concepts, medcodeonto=None\n    ):\n        \"\"\"\n        Process all regex matches and replace them in the SQL text.\n        \"\"\"\n        coroutines = [\n            self.get_replacement(match, explorer_concepts, medcodeonto)\n            for match in matches\n        ]\n        replacements = await asyncio.gather(*coroutines)\n        return self.apply_replacements_to_sql(matches, replacements, sql_text)\n\n    async def get_replacement(self, match, explorer_concepts, medcodeonto):\n        \"\"\"\n        Get the replacement for a given match.\n        \"\"\"\n        category, group_key = match\n        if not explorer_concepts or group_key not in explorer_concepts:\n            return await self.replace_function(match, medcodeonto)\n\n        values = explorer_concepts[group_key][\"value\"]\n        formatted_ids = self.format_replacement_result(values)\n        grouped_values = self.create_grouped_values(group_key, values)\n        self.update_attribute(category, grouped_values)\n\n        return formatted_ids\n\n    def create_grouped_values(self, group_key, values):\n        \"\"\"\n        Group values by their categories.\n        \"\"\"\n        return {\n            group_key: [\n                {\n                    key: value[key]\n                    for key in (\"CONCEPT_ID\", \"CONCEPT_NAME\", \"CONCEPT_CODE\")\n                }\n                for value in values\n            ]\n        }\n\n    def update_attribute(self, attribute_name, grouped_values):\n        \"\"\"\n        Update the attribute of the class with the grouped values.\n        \"\"\"\n        current_attribute_value = getattr(self, attribute_name, [])\n        current_attribute_value.append(grouped_values)\n        setattr(self, attribute_name, current_attribute_value)\n\n    def apply_replacements_to_sql(self, matches, replacements, sql_text):\n        \"\"\"\n        Apply the replacements to the SQL text.\n        \"\"\"\n        modified_sql = sql_text\n        for match, replacement in zip(matches, replacements):\n            if isinstance(replacement, list):\n                replacement = \", \".join(map(str, replacement))\n            modified_sql = modified_sql.replace(f\"[{match[0]}@{match[1]}]\", replacement)\n        return modified_sql\n\n    async def handle_invalid_sql(self, rag, sleep_sec):\n        \"\"\"\n        Handle the case where the SQL does not meet the required criteria.\n        \"\"\"\n        prompt = \"\"\"Your generated SQL query doesn't meet the requirements.\n            Please correct the sql query based on the previously given instructions.\n            [!IMPORTANT] Do not include conditions, such as 'WHERE concept_name IN ...' or  'WHERE concept_name = \"...\"'\n            [!IMPORTANT] Do not return anything except the sql query\"\"\"\n\n        rag.assistant.add_message(prompt)\n        completed_prompt = await self.assistant.get_response()\n        sql_text = self.parse_sql_from_response(completed_prompt)\n        if sleep_sec > 0:\n            await asyncio.sleep(sleep_sec)\n        return sql_text\n\n    @staticmethod\n    def parse_sql_from_response(resp=\"\"):\n        if resp is None:\n            resp = \"\"\n\n        pattern1 = r\"(?:Snowflake )?SQL query:\\s*\\n\\n([\\s\\S]+?);\"\n        pattern2 = r\"(?:```sql|```) ?\\n([\\s\\S]+?)\\n```\"\n        match1 = re.search(pattern1, resp)\n        match2 = re.search(pattern2, resp)\n        if match2:\n            return match2.group(1)\n        elif match1:\n            return match1.group(1) + \";\"\n        else:\n            logger.info(\"No SQL code found.\")\n\n    @staticmethod\n    def parse_python_from_response(resp=\"\"):\n        pattern = r\"(?:```python|```) ?\\n([\\s\\S]+?)\\n```\"\n        match = re.search(pattern, resp)\n        if match:\n            return match.group(1)\n        else:\n            logger.info(\"No python code found.\")\n\n    @staticmethod\n    def parse_json_from_response(resp=\"\"):\n        pattern = r\"(?:```json|```) ?\\n([\\s\\S]+?)\\n```\"\n        match = re.search(pattern, resp)\n        if match:\n            return match.group(1)\n        else:\n            logger.info(\"No Json code found.\")\n\n    @staticmethod\n    def save_string_to_file(text=\"\", filename=\"log.txt\"):\n        with open(filename, \"a\") as text_file:\n            text_file.write(\"\\n\")\n            if text:\n                text_file.write(text)\n\n    @staticmethod\n    def is_sql_for_concept_name_in(sql_text):\n        pattern = r\"WHERE\\s+(?:[a-zA-Z]\\.)?concept_name\\s*(?:=|IN)\\s*\\(?(?:'[^']+',\\s*)*'?[^']+'?\\)?\"\n        match = re.search(pattern, str(sql_text))\n        return bool(match)\n\n    @staticmethod\n    def replace_condition_concept_id_to_condition_source(sql_text):\n        pattern = re.compile(r\"condition_concept_id\", re.IGNORECASE)\n        replacement = lambda match: (\n            \"condition_source_concept_id\"\n            if match.group().islower()\n            else \"CONDITION_SOURCE_CONCEPT_ID\"\n        )\n        return pattern.sub(replacement, sql_text)"}
