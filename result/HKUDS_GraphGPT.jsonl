{"repo_info": {"repo_name": "GraphGPT", "repo_owner": "HKUDS", "repo_url": "https://github.com/HKUDS/GraphGPT"}}
{"type": "test_file", "path": "playground/test_embedding/test_sentence_similarity.py", "content": "import json\nimport os\n\nimport numpy as np\nimport openai\nimport requests\nfrom scipy.spatial.distance import cosine\n\n\ndef get_embedding_from_api(word, model=\"vicuna-7b-v1.1\"):\n    if \"ada\" in model:\n        resp = openai.Embedding.create(\n            model=model,\n            input=word,\n        )\n        embedding = np.array(resp[\"data\"][0][\"embedding\"])\n        return embedding\n\n    url = \"http://localhost:8000/v1/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = json.dumps({\"model\": model, \"input\": word})\n\n    response = requests.post(url, headers=headers, data=data)\n    if response.status_code == 200:\n        embedding = np.array(response.json()[\"data\"][0][\"embedding\"])\n        return embedding\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n\n\ndef cosine_similarity(vec1, vec2):\n    return 1 - cosine(vec1, vec2)\n\n\ndef print_cosine_similarity(embeddings, texts):\n    for i in range(len(texts)):\n        for j in range(i + 1, len(texts)):\n            sim = cosine_similarity(embeddings[texts[i]], embeddings[texts[j]])\n            print(f\"Cosine similarity between '{texts[i]}' and '{texts[j]}': {sim:.2f}\")\n\n\ntexts = [\n    \"The quick brown fox\",\n    \"The quick brown dog\",\n    \"The fast brown fox\",\n    \"A completely different sentence\",\n]\n\nembeddings = {}\nfor text in texts:\n    embeddings[text] = get_embedding_from_api(text)\n\nprint(\"Vicuna-7B:\")\nprint_cosine_similarity(embeddings, texts)\n\nfor text in texts:\n    embeddings[text] = get_embedding_from_api(text, model=\"text-similarity-ada-001\")\n\nprint(\"text-similarity-ada-001:\")\nprint_cosine_similarity(embeddings, texts)\n\nfor text in texts:\n    embeddings[text] = get_embedding_from_api(text, model=\"text-embedding-ada-002\")\n\nprint(\"text-embedding-ada-002:\")\nprint_cosine_similarity(embeddings, texts)\n"}
{"type": "test_file", "path": "graphgpt/serve/test_message.py", "content": "\"\"\"Send a test message.\"\"\"\nimport argparse\nimport json\n\nimport requests\n\nfrom fastchat.model.model_adapter import get_conversation_template\n\n\ndef main():\n    model_name = args.model_name\n\n    if args.worker_address:\n        worker_addr = args.worker_address\n    else:\n        controller_addr = args.controller_address\n        ret = requests.post(controller_addr + \"/refresh_all_workers\")\n        ret = requests.post(controller_addr + \"/list_models\")\n        models = ret.json()[\"models\"]\n        models.sort()\n        print(f\"Models: {models}\")\n\n        ret = requests.post(\n            controller_addr + \"/get_worker_address\", json={\"model\": model_name}\n        )\n        worker_addr = ret.json()[\"address\"]\n        print(f\"worker_addr: {worker_addr}\")\n\n    if worker_addr == \"\":\n        print(f\"No available workers for {model_name}\")\n        return\n\n    conv = get_conversation_template(model_name)\n    conv.append_message(conv.roles[0], args.message)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\n    headers = {\"User-Agent\": \"FastChat Client\"}\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"temperature\": args.temperature,\n        \"max_new_tokens\": args.max_new_tokens,\n        \"stop\": conv.stop_str,\n        \"stop_token_ids\": conv.stop_token_ids,\n        \"echo\": False,\n    }\n    response = requests.post(\n        worker_addr + \"/worker_generate_stream\",\n        headers=headers,\n        json=gen_params,\n        stream=True,\n    )\n\n    print(f\"{conv.roles[0]}: {args.message}\")\n    print(f\"{conv.roles[1]}: \", end=\"\")\n    prev = 0\n    for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n        if chunk:\n            data = json.loads(chunk.decode())\n            output = data[\"text\"].strip()\n            print(output[prev:], end=\"\", flush=True)\n            prev = len(output)\n    print(\"\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    parser.add_argument(\"--worker-address\", type=str)\n    parser.add_argument(\"--model-name\", type=str, required=True)\n    parser.add_argument(\"--temperature\", type=float, default=0.0)\n    parser.add_argument(\"--max-new-tokens\", type=int, default=32)\n    parser.add_argument(\n        \"--message\", type=str, default=\"Tell me a story with more than 1000 words.\"\n    )\n    args = parser.parse_args()\n\n    main()\n"}
{"type": "test_file", "path": "playground/test_embedding/test_semantic_search.py", "content": "import json\nimport os\n\nimport numpy as np\nimport openai\nimport pandas as pd\nimport requests\nfrom scipy.spatial.distance import cosine\n\n\ndef cosine_similarity(vec1, vec2):\n    try:\n        return 1 - cosine(vec1, vec2)\n    except:\n        print(vec1.shape, vec2.shape)\n\n\ndef get_embedding_from_api(word, model=\"vicuna-7b-v1.1\"):\n    if \"ada\" in model:\n        resp = openai.Embedding.create(\n            model=model,\n            input=word,\n        )\n        embedding = np.array(resp[\"data\"][0][\"embedding\"])\n        return embedding\n\n    url = \"http://localhost:8000/v1/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = json.dumps({\"model\": model, \"input\": word})\n\n    response = requests.post(url, headers=headers, data=data)\n    if response.status_code == 200:\n        embedding = np.array(response.json()[\"data\"][0][\"embedding\"])\n        return embedding\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n\n\ndef create_embedding_data_frame(data_path, model, max_tokens=500):\n    df = pd.read_csv(data_path, index_col=0)\n    df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n    df = df.dropna()\n    df[\"combined\"] = (\n        \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n    )\n    top_n = 1000\n    df = df.sort_values(\"Time\").tail(top_n * 2)\n    df.drop(\"Time\", axis=1, inplace=True)\n\n    df[\"n_tokens\"] = df.combined.apply(lambda x: len(x))\n    df = df[df.n_tokens <= max_tokens].tail(top_n)\n    df[\"embedding\"] = df.combined.apply(lambda x: get_embedding_from_api(x, model))\n    return df\n\n\ndef search_reviews(df, product_description, n=3, pprint=False, model=\"vicuna-7b-v1.1\"):\n    product_embedding = get_embedding_from_api(product_description, model=model)\n    df[\"similarity\"] = df.embedding.apply(\n        lambda x: cosine_similarity(x, product_embedding)\n    )\n\n    results = (\n        df.sort_values(\"similarity\", ascending=False)\n        .head(n)\n        .combined.str.replace(\"Title: \", \"\")\n        .str.replace(\"; Content:\", \": \")\n    )\n    if pprint:\n        for r in results:\n            print(r[:200])\n            print()\n    return results\n\n\ndef print_model_search(input_path, model):\n    print(f\"Model: {model}\")\n    df = create_embedding_data_frame(input_path, model)\n    print(\"search: delicious beans\")\n    results = search_reviews(df, \"delicious beans\", n=5, model=model)\n    print(results)\n    print(\"search: whole wheat pasta\")\n    results = search_reviews(df, \"whole wheat pasta\", n=5, model=model)\n    print(results)\n    print(\"search: bad delivery\")\n    results = search_reviews(df, \"bad delivery\", n=5, model=model)\n    print(results)\n\n\ninput_datapath = \"amazon_fine_food_review.csv\"\nif not os.path.exists(input_datapath):\n    raise Exception(\n        f\"Please download data from: https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\"\n    )\n\n\nprint_model_search(input_datapath, \"vicuna-7b-v1.1\")\nprint_model_search(input_datapath, \"text-similarity-ada-001\")\nprint_model_search(input_datapath, \"text-embedding-ada-002\")\n"}
{"type": "test_file", "path": "graphgpt/serve/test_throughput.py", "content": "\"\"\"Benchmarking script to test the throughput of serving workers.\"\"\"\nimport argparse\nimport json\n\nimport requests\nimport threading\nimport time\n\nfrom fastchat.conversation import default_conversation\n\n\ndef main():\n    if args.worker_address:\n        worker_addr = args.worker_address\n    else:\n        controller_addr = args.controller_address\n        ret = requests.post(controller_addr + \"/refresh_all_workers\")\n        ret = requests.post(controller_addr + \"/list_models\")\n        models = ret.json()[\"models\"]\n        models.sort()\n        print(f\"Models: {models}\")\n\n        ret = requests.post(\n            controller_addr + \"/get_worker_address\", json={\"model\": args.model_name}\n        )\n        worker_addr = ret.json()[\"address\"]\n        print(f\"worker_addr: {worker_addr}\")\n\n    if worker_addr == \"\":\n        return\n\n    conv = default_conversation.copy()\n    conv.append_message(conv.roles[0], \"Tell me a story with more than 1000 words\")\n    prompt_template = conv.get_prompt()\n    prompts = [prompt_template for _ in range(args.n_thread)]\n\n    headers = {\"User-Agent\": \"fastchat Client\"}\n    ploads = [\n        {\n            \"model\": args.model_name,\n            \"prompt\": prompts[i],\n            \"max_new_tokens\": args.max_new_tokens,\n            \"temperature\": 0.0,\n            # \"stop\": conv.sep,\n        }\n        for i in range(len(prompts))\n    ]\n\n    def send_request(results, i):\n        if args.test_dispatch:\n            ret = requests.post(\n                controller_addr + \"/get_worker_address\", json={\"model\": args.model_name}\n            )\n            thread_worker_addr = ret.json()[\"address\"]\n        else:\n            thread_worker_addr = worker_addr\n        print(f\"thread {i} goes to {thread_worker_addr}\")\n        response = requests.post(\n            thread_worker_addr + \"/worker_generate_stream\",\n            headers=headers,\n            json=ploads[i],\n            stream=False,\n        )\n        k = list(\n            response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\")\n        )\n        # print(k)\n        response_new_words = json.loads(k[-2].decode(\"utf-8\"))[\"text\"]\n        error_code = json.loads(k[-2].decode(\"utf-8\"))[\"error_code\"]\n        # print(f\"=== Thread {i} ===, words: {1}, error code: {error_code}\")\n        results[i] = len(response_new_words.split(\" \")) - len(prompts[i].split(\" \"))\n\n    # use N threads to prompt the backend\n    tik = time.time()\n    threads = []\n    results = [None] * args.n_thread\n    for i in range(args.n_thread):\n        t = threading.Thread(target=send_request, args=(results, i))\n        t.start()\n        # time.sleep(0.5)\n        threads.append(t)\n\n    for t in threads:\n        t.join()\n\n    print(f\"Time (POST): {time.time() - tik} s\")\n    # n_words = 0\n    # for i, response in enumerate(results):\n    #     # print(prompt[i].replace(conv.sep, \"\\n\"), end=\"\")\n    #     # make sure the streaming finishes at EOS or stopping criteria\n    #     k = list(response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"))\n    #     response_new_words = json.loads(k[-2].decode(\"utf-8\"))[\"text\"]\n    #     # print(response_new_words)\n    #     n_words += len(response_new_words.split(\" \")) - len(prompts[i].split(\" \"))\n    n_words = sum(results)\n    time_seconds = time.time() - tik\n    print(\n        f\"Time (Completion): {time_seconds}, n threads: {args.n_thread}, \"\n        f\"throughput: {n_words / time_seconds} words/s.\"\n    )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    parser.add_argument(\"--worker-address\", type=str)\n    parser.add_argument(\"--model-name\", type=str, default=\"vicuna\")\n    parser.add_argument(\"--max-new-tokens\", type=int, default=2048)\n    parser.add_argument(\"--n-thread\", type=int, default=8)\n    parser.add_argument(\"--test-dispatch\", action=\"store_true\")\n    args = parser.parse_args()\n\n    main()\n"}
{"type": "test_file", "path": "tests/test_openai_langchain.py", "content": "# export OPENAI_API_BASE=http://localhost:8000/v1\n# export OPENAI_API_KEY=EMPTY\n\nfrom langchain import OpenAI, LLMChain, PromptTemplate\nfrom langchain.memory import ConversationBufferWindowMemory\nfrom langchain.embeddings import OpenAIEmbeddings\nimport numpy as np\n\ntemplate = \"\"\"{history}\nHuman: {human_input}\nAssistant:\"\"\"\n\ndef test_embedding():\n    embeddings = OpenAIEmbeddings()\n    texts = [\"Why does the chicken cross the road\", \"To be honest\", \"Long time ago\"]\n    query_result = embeddings.embed_query(texts[0])\n    doc_result = embeddings.embed_documents(texts)\n    assert np.allclose(query_result, doc_result[0], atol=1e-3)\n\ndef test_chain():\n\n    prompt = PromptTemplate(\n        input_variables=[\"history\", \"human_input\"],\n        template=template\n    )\n    chain = LLMChain(\n        llm=OpenAI(model=\"text-embedding-ada-002\", temperature=1), \n        prompt=prompt, \n        verbose=True, \n        memory=ConversationBufferWindowMemory(k=2),\n    )\n    output = chain.predict(human_input=\"ls ~\")\n    print(output)\n\nif __name__ == \"__main__\":\n    test_embedding()\n    test_chain()\n\n"}
{"type": "test_file", "path": "playground/test_embedding/test_classification.py", "content": "import json\nimport os\n\nimport numpy as np\nimport openai\nimport pandas as pd\nimport requests\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\n\n\nnp.set_printoptions(threshold=10000)\n\n\ndef get_embedding_from_api(word, model=\"vicuna-7b-v1.1\"):\n    if \"ada\" in model:\n        resp = openai.Embedding.create(\n            model=model,\n            input=word,\n        )\n        embedding = np.array(resp[\"data\"][0][\"embedding\"])\n        return embedding\n\n    url = \"http://localhost:8000/v1/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = json.dumps({\"model\": model, \"input\": word})\n\n    response = requests.post(url, headers=headers, data=data)\n    if response.status_code == 200:\n        embedding = np.array(response.json()[\"data\"][0][\"embedding\"])\n        return embedding\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n\n\ndef create_embedding_data_frame(data_path, model, max_tokens=500):\n    df = pd.read_csv(data_path, index_col=0)\n    df = df[[\"Time\", \"ProductId\", \"UserId\", \"Score\", \"Summary\", \"Text\"]]\n    df = df.dropna()\n    df[\"combined\"] = (\n        \"Title: \" + df.Summary.str.strip() + \"; Content: \" + df.Text.str.strip()\n    )\n    top_n = 1000\n    df = df.sort_values(\"Time\").tail(top_n * 2)\n    df.drop(\"Time\", axis=1, inplace=True)\n\n    df[\"n_tokens\"] = df.combined.apply(lambda x: len(x))\n    df = df[df.n_tokens <= max_tokens].tail(top_n)\n    df[\"embedding\"] = df.combined.apply(lambda x: get_embedding_from_api(x, model))\n    return df\n\n\ndef train_random_forest(df):\n    X_train, X_test, y_train, y_test = train_test_split(\n        list(df.embedding.values), df.Score, test_size=0.2, random_state=42\n    )\n\n    clf = RandomForestClassifier(n_estimators=100)\n    clf.fit(X_train, y_train)\n    preds = clf.predict(X_test)\n\n    report = classification_report(y_test, preds)\n    accuracy = accuracy_score(y_test, preds)\n    return clf, accuracy, report\n\n\ninput_datapath = \"amazon_fine_food_review.csv\"\nif not os.path.exists(input_datapath):\n    raise Exception(\n        f\"Please download data from: https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\"\n    )\n\ndf = create_embedding_data_frame(input_datapath, \"vicuna-7b-v1.1\")\nclf, accuracy, report = train_random_forest(df)\nprint(f\"Vicuna-7b-v1.1 accuracy:{accuracy}\")\ndf = create_embedding_data_frame(input_datapath, \"text-similarity-ada-001\")\nclf, accuracy, report = train_random_forest(df)\nprint(f\"text-similarity-ada-001 accuracy:{accuracy}\")\ndf = create_embedding_data_frame(input_datapath, \"text-embedding-ada-002\")\nclf, accuracy, report = train_random_forest(df)\nprint(f\"text-embedding-ada-002 accuracy:{accuracy}\")\n"}
{"type": "test_file", "path": "tests/test_openai_sdk.py", "content": "import openai\n\nopenai.api_key = \"EMPTY\"  # Not support yet\nopenai.api_base = \"http://localhost:8000/v1\"\n\nmodel = \"vicuna-7b-v1.1\"\n\n\ndef test_list_models():\n    model_list = openai.Model.list()\n    print(model_list[\"data\"][0][\"id\"])\n\n\ndef test_completion():\n    prompt = \"Once upon a time\"\n    completion = openai.Completion.create(model=model, prompt=prompt, max_tokens=64)\n    print(prompt + completion.choices[0].text)\n\n\ndef test_embedding():\n    embedding = openai.Embedding.create(model=model, input=\"Hello world!\")\n    print(len(embedding[\"data\"][0][\"embedding\"]))\n\n\ndef test_chat_completion():\n    completion = openai.ChatCompletion.create(\n        model=model, messages=[{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n    )\n    print(completion.choices[0].message.content)\n\n\ndef test_chat_completion_stream():\n    messages = [{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n    res = openai.ChatCompletion.create(model=model, messages=messages, stream=True)\n    for chunk in res:\n        content = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n        print(content, end=\"\", flush=True)\n    print()\n\n\nif __name__ == \"__main__\":\n    test_list_models()\n    test_completion()\n    test_embedding()\n    test_chat_completion()\n    test_chat_completion_stream()\n"}
{"type": "test_file", "path": "playground/test_openai_api/anthropic_api.py", "content": "import os\n\nfrom fastchat.model import get_conversation_template\n\n\ndef claude():\n    import anthropic\n    c = anthropic.Client(os.environ[\"ANTHROPIC_API_KEY\"])\n\n    model = \"claude-v1\"\n    conv = get_conversation_template(model)\n    conv.append_message(conv.roles[0], \"Hello!\")\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\n    response = c.completion_stream(\n        prompt=prompt,\n        stop_sequences=[anthropic.HUMAN_PROMPT],\n        max_tokens_to_sample=256,\n        model=model,\n        stream=True,\n    )\n    for data in response:\n        print(data[\"completion\"])\n\n\nclaude()\n"}
{"type": "test_file", "path": "playground/test_openai_api/openai_api.py", "content": "import os\n\nfrom fastchat.model import get_conversation_template\n\ndef chatgpt():\n    import openai\n    model = \"gpt-3.5-turbo\"\n    conv = get_conversation_template(model)\n    conv.append_message(conv.roles[0], \"Hello!\")\n    conv.append_message(conv.roles[1], None)\n\n    messages = conv.to_openai_api_messages()\n    print(messages)\n\n    res = openai.ChatCompletion.create(model=model, messages=messages)\n    msg = res[\"choices\"][0][\"message\"][\"content\"]\n    print(msg)\n\n    res = openai.ChatCompletion.create(model=model, messages=messages, stream=True)\n    msg = \"\"\n    for chunk in res:\n        msg += chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n    print(msg)\n\n\nchatgpt()\n"}
{"type": "source_file", "path": "graphgpt/model/graph_layers/__init__.py", "content": "from graphgpt.model.graph_layers.mpnn import MPNN\nfrom graphgpt.model.graph_layers.clip_graph import CLIP, GNN\nfrom graphgpt.model.graph_layers.graph_transformer import graph_transformer"}
{"type": "source_file", "path": "graphgpt/model/graph_layers/simple_tokenizer.py", "content": "import gzip\nimport html\nimport os\nfrom functools import lru_cache\n\nimport ftfy\nimport regex as re\n\n\n@lru_cache()\ndef default_bpe():\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"bpe_simple_vocab_16e6.txt.gz\")\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n\n\ndef whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\n\n\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe()):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n        merges = merges[1:49152-256-2+1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v+'</w>' for v in vocab]\n        for merge in merges:\n            vocab.append(''.join(merge))\n        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token+'</w>'\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = ' '.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = ''.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n        return text\n"}
{"type": "source_file", "path": "graphgpt/model/graph_layers/clip_graph.py", "content": "from collections import OrderedDict\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom typing import Any, Union, List\nfrom graphgpt.model.graph_layers.simple_tokenizer import SimpleTokenizer as _Tokenizer\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_scatter import scatter_add\nfrom torch_geometric.utils import add_remaining_self_loops\nfrom torch.nn import Parameter\nfrom torch import nn, optim\nfrom graphgpt.model.graph_layers.graph_transformer import graph_transformer\nfrom transformers.configuration_utils import PretrainedConfig\n\n_tokenizer = _Tokenizer()\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n        ]))\n        self.ln_2 = LayerNorm(d_model)\n        self.attn_mask = attn_mask\n\n    def attention(self, x: torch.Tensor):\n        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n\n    def forward(self, x: torch.Tensor):\n        x = x + self.attention(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n\n    def forward(self, x: torch.Tensor):\n        return self.resblocks(x)\n\n\nclass GNN(MessagePassing):\n    def __init__(self, args, **kwargs):\n        super(GNN, self).__init__(aggr='add', **kwargs)\n        self.config = PretrainedConfig()\n        self.vars = nn.ParameterList()\n\n        w = nn.Parameter(torch.ones([args.gnn_hid, args.gnn_input]))\n        torch.nn.init.xavier_uniform_(w)\n        self.vars.append(w)\n        self.vars.append(nn.Parameter(torch.zeros(args.gnn_hid)))\n\n        w = nn.Parameter(torch.ones([args.gnn_output, args.gnn_hid]))\n        torch.nn.init.xavier_uniform_(w)\n        self.vars.append(w)\n        self.vars.append(nn.Parameter(torch.zeros(args.gnn_output)))\n\n    @staticmethod\n    def norm(edge_index, num_nodes, improved=False, dtype=None):\n        edge_weight = torch.ones((edge_index.size(1),), dtype=dtype,\n                                 device=edge_index.device)\n\n        fill_value = 1.0 if not improved else 2.0\n        edge_index, edge_weight = add_remaining_self_loops(\n            edge_index, edge_weight, fill_value, num_nodes)\n\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n\n        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n\n    def forward(self, g, vars=None):\n        device = self.parameters()[0].device\n        g = g.to(device)\n        \n        edge_index = g.edge_index\n        x = g.graph_node\n        if vars is None:\n            vars = self.vars\n        improved = False\n\n        w, b = vars[0], vars[1]\n        edge_index, norm = self.norm(edge_index, x.size(self.node_dim), improved, x.dtype)\n        x = self.propagate(edge_index, x=x, norm=norm)\n        w = w.to(x.device)\n        b = b.to(x.device)\n        x = F.linear(x, w, b)\n        x = F.leaky_relu(x)\n\n        w, b = vars[2], vars[3]\n        edge_index, norm = self.norm(edge_index, x.size(self.node_dim), improved, x.dtype)\n        x = self.propagate(edge_index, x=x, norm=norm)\n        w = w.to(x.device)\n        b = b.to(x.device)\n        x = F.linear(x, w, b)\n\n        return x\n\n    def parameters(self):\n        return self.vars\n\n\n\ndef Mv2SameDevice(var_list):\n    for vid in range(1, len(var_list)):\n        var_list[vid] = var_list[vid].to(var_list[0].device)\n    return var_list\n\n\nclass CLIP(nn.Module):\n    def __init__(self,\n                 args\n                 ):\n        super().__init__()\n\n        self.context_length = args.context_length\n        self.args = args\n        self.edge_coef = args.edge_coef\n\n        if args.gnn_type == 'gcn':\n            self.gnn = GNN(args)\n        elif args.gnn_type == 'gt': \n            self.gnn = graph_transformer(args)\n        self.transformer = Transformer(\n            width=args.transformer_width,\n            layers=args.transformer_layers,\n            heads=args.transformer_heads,\n            attn_mask=self.build_attention_mask()\n        )\n\n        self.vocab_size = args.vocab_size\n        self.token_embedding = nn.Embedding(args.vocab_size,\n                                            args.transformer_width)  # the embedding for all possible tokens\n        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, args.transformer_width))\n        self.ln_final = LayerNorm(args.transformer_width)\n\n        self.text_projection = nn.Parameter(torch.empty(args.transformer_width, args.embed_dim))\n        # self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n        if args.gnn_type == 'gcn':\n            self.dtype = self.gnn.vars[0].dtype\n        elif args.gnn_type == 'gt': \n            self.dtype = self.gnn.W_pos.dtype\n\n        self.optim = optim.Adam([{'params': self.token_embedding.weight},\n                                 {'params': self.positional_embedding},\n                                 {'params': self.transformer.parameters()},\n                                 {'params': self.text_projection},\n                                 {'params': self.gnn.parameters()}\n                                 ], lr=args.lr)\n\n        self.initialize_parameters()\n\n    def initialize_parameters(self):\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n        attn_std = self.transformer.width ** -0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n        if self.text_projection is not None:\n            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n\n    def build_attention_mask(self):\n        # lazily create causal attention mask, with full attention between the vision tokens\n        # pytorch uses additive attention mask; fill with -inf\n        mask = torch.empty(self.context_length, self.context_length)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)  # zero out the lower diagonal\n        return mask\n\n    def encode_image(self, idx_train, g):\n        embs = self.gnn(g)\n        idx_train = idx_train.to(embs.device)\n        idx_train = idx_train\n        train_embs = embs[idx_train]\n        return train_embs\n\n    def encode_text(self, text):\n        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.type(self.dtype)\n        x = x.permute(1, 0,\n                      2)  # NLD -> LND, batch_size * context_length *emb_dim -> context_length * batch_size  *emb_dim\n        x = self.transformer(x)\n        x = x.permute(1, 0,\n                      2)  # LND -> NLD, context_length * batch_size *emb_dim -> batch_size * context_length *emb_dim\n        x = self.ln_final(x).type(self.dtype)\n        # x.shape = [batch_size, n_ctx, transformer.width]\n        # take features from the eot （end of token） embedding (eot_token is the highest number in each sequence)\n        # so there is node need to shorten the context length\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)]  #\n        x = x @ self.text_projection\n        return x\n\n    def forward(self, g, s_n, t_n, s_n_text, t_n_text, training=True):\n\n        s_image_features = self.encode_image(s_n, g)\n\n        s_text_features = self.encode_text(s_n_text)\n\n        t_text_features = self.encode_text(t_n_text)\n        t_text_features = t_text_features.reshape(s_image_features.shape[0], self.args.neigh_num, self.args.gnn_output)\n        t_text_features = torch.mean(t_text_features, dim=1, keepdim=False)\n        # normalized features\n        s_image_features = s_image_features / s_image_features.norm(dim=-1, keepdim=True)\n        s_text_features = s_text_features / s_text_features.norm(dim=-1, keepdim=True)\n        t_text_features = t_text_features / t_text_features.norm(dim=-1, keepdim=True)\n\n        # cosine similarity as logits\n\n        labels = torch.arange(s_image_features.shape[0]).cuda()\n\n        # logit_scale = self.logit_scale.exp()  # the temporature hyperparameter\n        # logit_scale, s_image_features, s_text_features = Mv2SameDevice([logit_scale, s_image_features, s_text_features])\n        # logits = logit_scale * s_image_features @ s_text_features.t()\n        # loss_i = F.cross_entropy(logits, labels)\n        # loss_t = F.cross_entropy(logits.T, labels)\n        # node_loss = (loss_i + loss_t) / 2\n\n        # logit_scale, s_image_features, t_text_features = Mv2SameDevice([logit_scale, s_image_features, t_text_features])\n        # logits = logit_scale * s_image_features @ t_text_features.t()\n        # loss_i = F.cross_entropy(logits, labels)\n        # loss_t = F.cross_entropy(logits.T, labels)\n        # gt_loss = (loss_i + loss_t)/2\n\n        # logit_scale, s_text_features, t_text_features = Mv2SameDevice([logit_scale, s_text_features, t_text_features])\n        # logits = logit_scale * s_text_features @ t_text_features.t()\n        # loss_i = F.cross_entropy(logits, labels)\n        # loss_t = F.cross_entropy(logits.T, labels)\n        # tt_loss = (loss_i + loss_t)/2\n\n        \n\n        # shape = [global_batch_size, global_batch_size]\n        # return all_loss\n        return s_image_features, s_text_features, t_text_features, labels\n\n\ndef tokenize(texts: Union[str, List[str]], context_length: int = 128, truncate: bool = True) -> torch.LongTensor:\n\n    \"\"\"\n    Returns the tokenized representation of given input string(s)\n\n    Parameters\n    ----------\n    texts : Union[str, List[str]]\n        An input string or a list of input strings to tokenize\n\n    context_length : int\n        The context length to use; all CLIP models use 77 as the context length\n\n    truncate: bool\n        Whether to truncate the text in case its encoding is longer than the context length\n\n    Returns\n    -------\n    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        if len(tokens) > context_length:\n            if truncate:\n                tokens = tokens[:context_length]\n                tokens[-1] = eot_token\n            else:\n                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n        result[i, :len(tokens)] = torch.tensor(tokens)\n\n    return result\n\n"}
{"type": "source_file", "path": "graphgpt/protocol/openai_api_protocol.py", "content": "from typing import Literal, Optional, List, Dict, Any, Union\n\nimport time\n\nimport shortuuid\nfrom pydantic import BaseModel, Field\n\n\nclass ErrorResponse(BaseModel):\n    object: str = \"error\"\n    message: str\n    code: int\n\n\nclass ModelPermission(BaseModel):\n    id: str = Field(default_factory=lambda: f\"modelperm-{shortuuid.random()}\")\n    object: str = \"model_permission\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    allow_create_engine: bool = False\n    allow_sampling: bool = True\n    allow_logprobs: bool = True\n    allow_search_indices: bool = True\n    allow_view: bool = True\n    allow_fine_tuning: bool = False\n    organization: str = \"*\"\n    group: Optional[str] = None\n    is_blocking: str = False\n\n\nclass ModelCard(BaseModel):\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"fastchat\"\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: List[ModelPermission] = []\n\n\nclass ModelList(BaseModel):\n    object: str = \"list\"\n    data: List[ModelCard] = []\n\n\nclass UsageInfo(BaseModel):\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    completion_tokens: Optional[int] = 0\n\n\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: Union[str, List[Dict[str, str]]]\n    temperature: Optional[float] = 0.7\n    top_p: Optional[float] = 1.0\n    n: Optional[int] = 1\n    max_tokens: Optional[int] = None\n    stop: Optional[Union[str, List[str]]] = None\n    stream: Optional[bool] = False\n    presence_penalty: Optional[float] = 0.0\n    frequency_penalty: Optional[float] = 0.0\n    user: Optional[str] = None\n\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\n\nclass ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: ChatMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]]\n\n\nclass ChatCompletionResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{shortuuid.random()}\")\n    object: str = \"chat.completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseChoice]\n    usage: UsageInfo\n\n\nclass DeltaMessage(BaseModel):\n    role: Optional[str] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]]\n\n\nclass ChatCompletionStreamResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{shortuuid.random()}\")\n    object: str = \"chat.completion.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseStreamChoice]\n\nclass TokenCheckRequest(BaseModel):\n    model: str\n    prompt: str\n    max_tokens: int\n\nclass TokenCheckResponse(BaseModel):\n    fits: bool\n    tokenCount: int\n    contextLength: int\n\nclass EmbeddingsRequest(BaseModel):\n    model: Optional[str] = None\n    engine: Optional[str] = None\n    input: Union[str, List[Any]]\n    user: Optional[str] = None\n\n\nclass EmbeddingsResponse(BaseModel):\n    object: str = \"list\"\n    data: List[Dict[str, Any]]\n    model: str\n    usage: UsageInfo\n\n\nclass CompletionRequest(BaseModel):\n    model: str\n    prompt: Union[str, List[Any]]\n    suffix: Optional[str] = None\n    temperature: Optional[float] = 0.7\n    n: Optional[int] = 1\n    max_tokens: Optional[int] = 16\n    stop: Optional[Union[str, List[str]]] = None\n    stream: Optional[bool] = False\n    top_p: Optional[float] = 1.0\n    logprobs: Optional[int] = None\n    echo: Optional[bool] = False\n    presence_penalty: Optional[float] = 0.0\n    frequency_penalty: Optional[float] = 0.0\n    user: Optional[str] = None\n\n\nclass CompletionResponseChoice(BaseModel):\n    index: int\n    text: str\n    logprobs: Optional[int] = None\n    finish_reason: Optional[Literal[\"stop\", \"length\"]]\n\n\nclass CompletionResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{shortuuid.random()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[CompletionResponseChoice]\n    usage: UsageInfo\n\n\nclass CompletionResponseStreamChoice(BaseModel):\n    index: int\n    text: str\n    logprobs: Optional[float] = None\n    finish_reason: Optional[Literal[\"stop\", \"length\"]] = None\n\n\nclass CompletionStreamResponse(BaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{shortuuid.random()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[CompletionResponseStreamChoice]\n"}
{"type": "source_file", "path": "graphgpt/model/graph_layers/mpnn.py", "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.utils import remove_self_loops, add_self_loops, degree\nimport json\nimport copy\nfrom transformers import AutoTokenizer \nimport transformers\nfrom transformers.configuration_utils import PretrainedConfig\nimport os\n\ndef gcn_conv(h, edge_index):\n    # print(edge_index)\n    N, node_feas = h.shape\n    edge_index, _ = remove_self_loops(edge_index)\n    edge_index, _ = add_self_loops(edge_index, num_nodes=N)\n    \n    src, dst = edge_index\n    deg = degree(dst, num_nodes=N)\n\n    deg_src = deg[src].pow(-0.5) \n    deg_src.masked_fill_(deg_src == float('inf'), 0)\n    deg_dst = deg[dst].pow(-0.5)\n    deg_dst.masked_fill_(deg_dst == float('inf'), 0)\n    edge_weight = deg_src * deg_dst\n\n    a = torch.sparse_coo_tensor(edge_index, edge_weight, torch.Size([N, N])).t()\n    rows, cols = edge_index\n    edge_msg = h[rows, :] * torch.unsqueeze(edge_weight, dim=-1)\n    col_embeds = h[cols, :]\n    tem = torch.zeros([N, node_feas]).to(edge_msg.device)\n    rows = rows.to(edge_msg.device)\n    h_prime = tem.index_add_(0, rows, edge_msg) # nd\n    # h = h.float() \n    # h_prime = a @ h \n    # h_prime = h_prime.bfloat16()\n    return h_prime\n\n# Implementation of MPNN, which can become MLP or GCN depending on whether using message passing\nclass MPNN(nn.Module): \n    def __init__(self, in_channels, hidden_channels, out_channels, **kwargs):\n        super(MPNN, self).__init__()\n        self.config = PretrainedConfig()\n        self.dropout = kwargs.get('dropout')# args.dropout\n        self.num_layers = kwargs.get('num_layers')# args.num_layers\n        self.ff_bias = True  # Use bias for FF layers in default\n\n        self.bns = nn.BatchNorm1d(hidden_channels, affine=False, track_running_stats=False)\n        self.activation = F.relu\n        self.if_param = kwargs.get('if_param')\n\n        if self.if_param: \n            self.fcs = nn.ModuleList([])\n            self.fcs.append(nn.Linear(in_channels, hidden_channels, bias=self.ff_bias))\n            for _ in range(self.num_layers - 2): self.fcs.append(nn.Linear(hidden_channels, hidden_channels, bias=self.ff_bias)) #1s\n            self.fcs.append(nn.Linear(hidden_channels, out_channels, bias=self.ff_bias)) #1\n            self.reset_parameters()\n    \n\n    def reset_parameters(self):\n        for mlp in self.fcs: \n            nn.init.xavier_uniform_(mlp.weight, gain=1.414)\n            nn.init.zeros_(mlp.bias)\n\n    def forward(self, g, use_conv=True):\n        \n        x = g.graph_node\n        edge_index = g.edge_index\n        try:\n            device = self.parameters().__next__().device\n        except: \n            device = x.device\n        x = x.to(device)\n        edge_index = edge_index.to(device)\n        for i in range(self.num_layers - 1):\n            if self.if_param: x = x @ self.fcs[i].weight.t() \n            if use_conv: x = gcn_conv(x, edge_index)  # Optionally replace 'gcn_conv' with other conv functions in conv.py\n            if self.ff_bias and self.if_param: x = x + self.fcs[i].bias\n            try: \n                x = self.activation(self.bns(x))\n            except: \n                x = self.activation((x))\n            x = F.dropout(x, p=self.dropout, training=self.training)\n\n        if self.if_param: x = x @ self.fcs[-1].weight.t() \n        if use_conv: x = gcn_conv(x, edge_index)\n        if self.ff_bias and self.if_param: x = x + self.fcs[-1].bias\n        return x\n"}
{"type": "source_file", "path": "graphgpt/model/graph_layers/graph_transformer.py", "content": "import torch as t\nfrom torch import nn\nimport torch.nn.functional as F\nimport math\nfrom transformers.configuration_utils import PretrainedConfig\n\ninit = nn.init.xavier_uniform_\nuniformInit = nn.init.uniform\n\ndef PositionalEncoding(q_len, d_model, normalize=True):\n    pe = t.zeros(q_len, d_model)\n    position = t.arange(0, q_len).unsqueeze(1)\n    div_term = t.exp(t.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n    pe[:, 0::2] = t.sin(position * div_term)\n    pe[:, 1::2] = t.cos(position * div_term)\n    if normalize:\n        pe = pe - pe.mean()\n        pe = pe / (pe.std() * 10)\n    return pe\n\n\ndef pos_encoding(pe, learn_pe, nvar, d_model):\n    # Positional encoding\n    if pe == None:\n        W_pos = t.empty((nvar, d_model)) # pe = None and learn_pe = False can be used to measure impact of pe\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n        learn_pe = False\n    elif pe == 'zero':\n        W_pos = t.empty((nvar, 1))\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n    elif pe == 'zeros':\n        W_pos = t.empty((nvar, d_model))\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n    elif pe == 'normal' or pe == 'gauss':\n        W_pos = t.zeros((nvar, 1))\n        t.nn.init.normal_(W_pos, mean=0.0, std=0.1)\n    elif pe == 'uniform':\n        W_pos = t.zeros((nvar, 1))\n        nn.init.uniform_(W_pos, a=0.0, b=0.1)\n    elif pe == 'sincos': W_pos = PositionalEncoding(nvar, d_model, normalize=True)\n    else: raise ValueError(f\"{pe} is not a valid pe (positional encoder. Available types: 'gauss'=='normal', \\\n        'zeros', 'zero', uniform', 'sincos', None.)\")\n    return nn.Parameter(W_pos, requires_grad=learn_pe)\n\n\nclass graph_transformer(nn.Module):\n    def __init__(self, args):\n        super(graph_transformer, self).__init__()\n        self.config = PretrainedConfig()\n        self.gtLayers = nn.Sequential(*[GTLayer(args) for i in range(args.gt_layers)])\n\n        self.W_pos = pos_encoding('zeros', True, 1, args.att_d_model)\n                \n        self.W_P = nn.Linear(args.gnn_input, args.att_d_model)\n        self.dropout = nn.Dropout(0.1)\n        self.inverW_P = nn.Linear(args.att_d_model, args.gnn_output)\n        self.args = args\n\n    def forward(self, g):\n        # Adj: sp adj\n        # x: bs * n * d_model * num_patch\n        \n        # print(edge_index)\n        device = self.parameters().__next__().device\n        g = g.to(device)\n        \n        x = g.graph_node\n        \n        # x, W_P_weight, W_P_bias= Mv2Samedevice([x, self.W_P.weight, self.W_P.bias])\n        # self.W_P.weight = nn.Parameter(W_P_weight.to(x.dtype))\n        # self.W_P.bias = nn.Parameter(W_P_bias.to(x.dtype))\n        # print(self.W_P.dtype, x.dtype)\n        z = self.W_P(x)\n        if self.args.if_pos: \n            embeds = self.dropout(z + self.W_pos) \n        else: \n            embeds = self.dropout(z) \n        for gt in self.gtLayers:\n            embeds = gt(g, embeds) # bs * num_patch * n * d_model\n        # embeds, inverW_P_weight, inverW_P_bias = Mv2Samedevice([embeds, self.inverW_P.weight, self.inverW_P.bias])\n        # self.inverW_P.weight = nn.Parameter(inverW_P_weight.to(embeds.dtype))\n        # self.inverW_P.bias = nn.Parameter(inverW_P_bias.to(embeds.dtype))\n        ret = self.inverW_P(embeds)\n        return ret\ndef Mv2Samedevice(vars): \n    return [var.to(vars[0].device) for var in vars]\n\nclass GTLayer(nn.Module):\n    def __init__(self, args):\n        super(GTLayer, self).__init__()\n        self.qTrans = nn.Parameter(init(t.empty(args.att_d_model, args.att_d_model)))\n        self.kTrans = nn.Parameter(init(t.empty(args.att_d_model, args.att_d_model)))\n        self.vTrans = nn.Parameter(init(t.empty(args.att_d_model, args.att_d_model)))\n        if args.att_norm: \n            self.norm = nn.LayerNorm(args.att_d_model, eps=1e-6)\n        self.args = args\n        \n        \n    \n    def forward(self, g, embeds):\n        # Adj: adj\n        # x: n * d_model\n        rows, cols = g.edge_index\n        nvar, _ = embeds.shape\n        # print(rows)\n        # print(cols)\n\n        rowEmbeds = embeds[rows, :]\n        colEmbeds = embeds[cols, :]\n        evar, _ = rowEmbeds.shape\n\n        # rowEmbeds, qTrans, kTrans, vTrans = Mv2Samedevice([rowEmbeds, self.qTrans, self.kTrans, self.vTrans])\n        # self.qTrans = nn.Parameter(qTrans.to(rowEmbeds.dtype))\n        # self.kTrans = nn.Parameter(kTrans.to(rowEmbeds.dtype))\n        # self.vTrans = nn.Parameter(vTrans.to(rowEmbeds.dtype))\n        qEmbeds = (rowEmbeds @ self.qTrans).view([evar, self.args.head, self.args.att_d_model // self.args.head])\n        kEmbeds = (colEmbeds @ self.kTrans).view([evar, self.args.head, self.args.att_d_model // self.args.head])\n        vEmbeds = (colEmbeds @ self.vTrans).view([evar, self.args.head, self.args.att_d_model // self.args.head])\n        \n        att = t.einsum('ehd, ehd -> eh', qEmbeds, kEmbeds)\n        att = t.clamp(att, -10.0, 10.0)\n        expAtt = t.exp(att)\n        \n        tem = t.zeros([nvar, self.args.head]).to(expAtt.device, dtype=expAtt.dtype)\n        # print(tem.device, expAtt.device, rows.device)\n        rows = rows.to(expAtt.device)\n        attNorm = (tem.index_add_(0, rows, expAtt))[rows, :]\n        att = expAtt / (attNorm + 1e-8) # bleh\n        \n        resEmbeds = t.einsum('eh, ehd -> ehd', att, vEmbeds).view([evar, self.args.att_d_model])\n        tem = t.zeros([nvar, self.args.att_d_model]).to(resEmbeds.device, dtype=resEmbeds.dtype)\n        rows = rows.to(resEmbeds.device)\n        tem = tem.to(resEmbeds.dtype)\n        resEmbeds = tem.index_add_(0, rows, resEmbeds) # nd\n        resEmbeds = resEmbeds + embeds\n        if self.args.att_norm: \n            # resEmbeds, norm_weight, norm_bias = Mv2Samedevice([resEmbeds, self.norm.weight, self.norm.bias])\n            # self.norm.weight = nn.Parameter(norm_weight.to(resEmbeds.dtype))\n            # self.norm.bias = nn.Parameter(norm_bias.to(resEmbeds.dtype))\n            resEmbeds = self.norm(resEmbeds)\n\n        return resEmbeds"}
{"type": "source_file", "path": "graphgpt/serve/__init__.py", "content": ""}
{"type": "source_file", "path": "graphgpt/model/make_delta.py", "content": "\"\"\"\nMake the delta weights by subtracting base weights.\n\nUsage:\npython3 -m fastchat.model.make_delta --base ~/model_weights/llama-13b --target ~/model_weights/vicuna-13b --delta ~/model_weights/vicuna-13b-delta --hub-repo-id lmsys/vicuna-13b-delta-v1.1\n\"\"\"\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef make_delta(base_model_path, target_model_path, delta_path):\n    print(f\"Loading the base model from {base_model_path}\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n\n    print(f\"Loading the target model from {target_model_path}\")\n    target = AutoModelForCausalLM.from_pretrained(\n        target_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    target_tokenizer = AutoTokenizer.from_pretrained(target_model_path, use_fast=False)\n\n    print(\"Calculating the delta\")\n    for name, param in tqdm(target.state_dict().items(), desc=\"Calculating delta\"):\n        assert name in base.state_dict()\n        param.data -= base.state_dict()[name]\n\n    print(f\"Saving the delta to {delta_path}\")\n    if args.hub_repo_id:\n        kwargs = {\"push_to_hub\": True, \"repo_id\": args.hub_repo_id}\n    else:\n        kwargs = {}\n    target.save_pretrained(delta_path, **kwargs)\n    target_tokenizer.save_pretrained(delta_path, **kwargs)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n    parser.add_argument(\"--hub-repo-id\", type=str)\n    args = parser.parse_args()\n\n    make_delta(args.base_model_path, args.target_model_path, args.delta_path)\n"}
{"type": "source_file", "path": "graphgpt/serve/gradio_css.py", "content": "code_highlight_css = \"\"\"\n#chatbot .hll { background-color: #ffffcc }\n#chatbot .c { color: #408080; font-style: italic }\n#chatbot .err { border: 1px solid #FF0000 }\n#chatbot .k { color: #008000; font-weight: bold }\n#chatbot .o { color: #666666 }\n#chatbot .ch { color: #408080; font-style: italic }\n#chatbot .cm { color: #408080; font-style: italic }\n#chatbot .cp { color: #BC7A00 }\n#chatbot .cpf { color: #408080; font-style: italic }\n#chatbot .c1 { color: #408080; font-style: italic }\n#chatbot .cs { color: #408080; font-style: italic }\n#chatbot .gd { color: #A00000 }\n#chatbot .ge { font-style: italic }\n#chatbot .gr { color: #FF0000 }\n#chatbot .gh { color: #000080; font-weight: bold }\n#chatbot .gi { color: #00A000 }\n#chatbot .go { color: #888888 }\n#chatbot .gp { color: #000080; font-weight: bold }\n#chatbot .gs { font-weight: bold }\n#chatbot .gu { color: #800080; font-weight: bold }\n#chatbot .gt { color: #0044DD }\n#chatbot .kc { color: #008000; font-weight: bold }\n#chatbot .kd { color: #008000; font-weight: bold }\n#chatbot .kn { color: #008000; font-weight: bold }\n#chatbot .kp { color: #008000 }\n#chatbot .kr { color: #008000; font-weight: bold }\n#chatbot .kt { color: #B00040 }\n#chatbot .m { color: #666666 }\n#chatbot .s { color: #BA2121 }\n#chatbot .na { color: #7D9029 }\n#chatbot .nb { color: #008000 }\n#chatbot .nc { color: #0000FF; font-weight: bold }\n#chatbot .no { color: #880000 }\n#chatbot .nd { color: #AA22FF }\n#chatbot .ni { color: #999999; font-weight: bold }\n#chatbot .ne { color: #D2413A; font-weight: bold }\n#chatbot .nf { color: #0000FF }\n#chatbot .nl { color: #A0A000 }\n#chatbot .nn { color: #0000FF; font-weight: bold }\n#chatbot .nt { color: #008000; font-weight: bold }\n#chatbot .nv { color: #19177C }\n#chatbot .ow { color: #AA22FF; font-weight: bold }\n#chatbot .w { color: #bbbbbb }\n#chatbot .mb { color: #666666 }\n#chatbot .mf { color: #666666 }\n#chatbot .mh { color: #666666 }\n#chatbot .mi { color: #666666 }\n#chatbot .mo { color: #666666 }\n#chatbot .sa { color: #BA2121 }\n#chatbot .sb { color: #BA2121 }\n#chatbot .sc { color: #BA2121 }\n#chatbot .dl { color: #BA2121 }\n#chatbot .sd { color: #BA2121; font-style: italic }\n#chatbot .s2 { color: #BA2121 }\n#chatbot .se { color: #BB6622; font-weight: bold }\n#chatbot .sh { color: #BA2121 }\n#chatbot .si { color: #BB6688; font-weight: bold }\n#chatbot .sx { color: #008000 }\n#chatbot .sr { color: #BB6688 }\n#chatbot .s1 { color: #BA2121 }\n#chatbot .ss { color: #19177C }\n#chatbot .bp { color: #008000 }\n#chatbot .fm { color: #0000FF }\n#chatbot .vc { color: #19177C }\n#chatbot .vg { color: #19177C }\n#chatbot .vi { color: #19177C }\n#chatbot .vm { color: #19177C }\n#chatbot .il { color: #666666 }\n\"\"\"\n# .highlight  { background: #f8f8f8; }\n\ntable_css = \"\"\"\ntable {\n    line-height: 0em\n}\n\"\"\"\n"}
{"type": "source_file", "path": "graphgpt/serve/controller_graph.py", "content": "\"\"\"\nA controller manages distributed workers.\nIt sends worker addresses to clients.\n\"\"\"\nimport argparse\nimport asyncio\nimport dataclasses\nfrom enum import Enum, auto\nimport json\nimport logging\nimport time\nfrom typing import List, Union\nimport threading\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse\nimport numpy as np\nimport requests\nimport uvicorn\n\nfrom graphgpt.constants import CONTROLLER_HEART_BEAT_EXPIRATION\nfrom graphgpt.utils import build_logger, server_error_msg\n\n\nlogger = build_logger(\"controller\", \"controller.log\")\n\n\nclass DispatchMethod(Enum):\n    LOTTERY = auto()\n    SHORTEST_QUEUE = auto()\n\n    @classmethod\n    def from_str(cls, name):\n        if name == \"lottery\":\n            return cls.LOTTERY\n        elif name == \"shortest_queue\":\n            return cls.SHORTEST_QUEUE\n        else:\n            raise ValueError(f\"Invalid dispatch method\")\n\n\n@dataclasses.dataclass\nclass WorkerInfo:\n    model_names: List[str]\n    speed: int\n    queue_length: int\n    check_heart_beat: bool\n    last_heart_beat: str\n\n\ndef heart_beat_controller(controller):\n    while True:\n        time.sleep(CONTROLLER_HEART_BEAT_EXPIRATION)\n        controller.remove_stable_workers_by_expiration()\n\n\nclass Controller:\n    def __init__(self, dispatch_method: str):\n        # Dict[str -> WorkerInfo]\n        self.worker_info = {}\n        self.dispatch_method = DispatchMethod.from_str(dispatch_method)\n\n        self.heart_beat_thread = threading.Thread(\n            target=heart_beat_controller, args=(self,))\n        self.heart_beat_thread.start()\n\n        logger.info(\"Init controller\")\n\n    def register_worker(self, worker_name: str, check_heart_beat: bool,\n                        worker_status: dict):\n        if worker_name not in self.worker_info:\n            logger.info(f\"Register a new worker: {worker_name}\")\n        else:\n            logger.info(f\"Register an existing worker: {worker_name}\")\n\n        if not worker_status:\n            worker_status = self.get_worker_status(worker_name)\n        if not worker_status:\n            return False\n\n        self.worker_info[worker_name] = WorkerInfo(\n            worker_status[\"model_names\"], worker_status[\"speed\"], worker_status[\"queue_length\"],\n            check_heart_beat, time.time())\n\n        logger.info(f\"Register done: {worker_name}, {worker_status}\")\n        return True\n\n    def get_worker_status(self, worker_name: str):\n        try:\n            r = requests.post(worker_name + \"/worker_get_status\", timeout=5)\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Get status fails: {worker_name}, {e}\")\n            return None\n\n        if r.status_code != 200:\n            logger.error(f\"Get status fails: {worker_name}, {r}\")\n            return None\n\n        return r.json()\n\n    def remove_worker(self, worker_name: str):\n        del self.worker_info[worker_name]\n\n    def refresh_all_workers(self):\n        old_info = dict(self.worker_info)\n        self.worker_info = {}\n\n        for w_name, w_info in old_info.items():\n            if not self.register_worker(w_name, w_info.check_heart_beat, None):\n                logger.info(f\"Remove stale worker: {w_name}\")\n\n    def list_models(self):\n        model_names = set()\n\n        for w_name, w_info in self.worker_info.items():\n            model_names.update(w_info.model_names)\n\n        return list(model_names)\n\n    def get_worker_address(self, model_name: str):\n        if self.dispatch_method == DispatchMethod.LOTTERY:\n            worker_names = []\n            worker_speeds = []\n            for w_name, w_info in self.worker_info.items():\n                if model_name in w_info.model_names:\n                    worker_names.append(w_name)\n                    worker_speeds.append(w_info.speed)\n            worker_speeds = np.array(worker_speeds, dtype=np.float32)\n            norm = np.sum(worker_speeds)\n            if norm < 1e-4:\n                return \"\"\n            worker_speeds = worker_speeds / norm\n            if True:  # Directly return address\n                pt = np.random.choice(np.arange(len(worker_names)),\n                    p=worker_speeds)\n                worker_name = worker_names[pt]\n                return worker_name\n\n            # Check status before returning\n            while True:\n                pt = np.random.choice(np.arange(len(worker_names)),\n                    p=worker_speeds)\n                worker_name = worker_names[pt]\n\n                if self.get_worker_status(worker_name):\n                    break\n                else:\n                    self.remove_worker(worker_name)\n                    worker_speeds[pt] = 0\n                    norm = np.sum(worker_speeds)\n                    if norm < 1e-4:\n                        return \"\"\n                    worker_speeds = worker_speeds / norm\n                    continue\n            return worker_name\n        elif self.dispatch_method == DispatchMethod.SHORTEST_QUEUE:\n            worker_names = []\n            worker_qlen = []\n            for w_name, w_info in self.worker_info.items():\n                if model_name in w_info.model_names:\n                    worker_names.append(w_name)\n                    worker_qlen.append(w_info.queue_length / w_info.speed)\n            if len(worker_names) == 0:\n                return \"\"\n            min_index = np.argmin(worker_qlen)\n            w_name = worker_names[min_index]\n            self.worker_info[w_name].queue_length += 1\n            logger.info(f\"names: {worker_names}, queue_lens: {worker_qlen}, ret: {w_name}\")\n            return w_name\n        else:\n            raise ValueError(f\"Invalid dispatch method: {self.dispatch_method}\")\n\n    def receive_heart_beat(self, worker_name: str, queue_length: int):\n        if worker_name not in self.worker_info:\n            logger.info(f\"Receive unknown heart beat. {worker_name}\")\n            return False\n\n        self.worker_info[worker_name].queue_length = queue_length\n        self.worker_info[worker_name].last_heart_beat = time.time()\n        logger.info(f\"Receive heart beat. {worker_name}\")\n        return True\n\n    def remove_stable_workers_by_expiration(self):\n        expire = time.time() - CONTROLLER_HEART_BEAT_EXPIRATION\n        to_delete = []\n        for worker_name, w_info in self.worker_info.items():\n            if w_info.check_heart_beat and w_info.last_heart_beat < expire:\n                to_delete.append(worker_name)\n\n        for worker_name in to_delete:\n            self.remove_worker(worker_name)\n\n    def worker_api_generate_stream(self, params):\n        worker_addr = self.get_worker_address(params[\"model\"])\n        if not worker_addr:\n            logger.info(f\"no worker: {params['model']}\")\n            ret = {\n                \"text\": server_error_msg,\n                \"error_code\": 2,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n\n        try:\n            response = requests.post(worker_addr + \"/worker_generate_stream\",\n                json=params, stream=True, timeout=5)\n            for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n                if chunk:\n                    yield chunk + b\"\\0\"\n        except requests.exceptions.RequestException as e:\n            logger.info(f\"worker timeout: {worker_addr}\")\n            ret = {\n                \"text\": server_error_msg,\n                \"error_code\": 3,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n\n\n    # Let the controller act as a worker to achieve hierarchical\n    # management. This can be used to connect isolated sub networks.\n    def worker_api_get_status(self):\n        model_names = set()\n        speed = 0\n        queue_length = 0\n\n        for w_name in self.worker_info:\n            worker_status = self.get_worker_status(w_name)\n            if worker_status is not None:\n                model_names.update(worker_status[\"model_names\"])\n                speed += worker_status[\"speed\"]\n                queue_length += worker_status[\"queue_length\"]\n\n        return {\n            \"model_names\": list(model_names),\n            \"speed\": speed,\n            \"queue_length\": queue_length,\n        }\n\n\napp = FastAPI()\n\n\n@app.post(\"/register_worker\")\nasync def register_worker(request: Request):\n    data = await request.json()\n    controller.register_worker(\n        data[\"worker_name\"], data[\"check_heart_beat\"],\n        data.get(\"worker_status\", None))\n\n\n@app.post(\"/refresh_all_workers\")\nasync def refresh_all_workers():\n    models = controller.refresh_all_workers()\n\n\n@app.post(\"/list_models\")\nasync def list_models():\n    models = controller.list_models()\n    return {\"models\": models}\n\n\n@app.post(\"/get_worker_address\")\nasync def get_worker_address(request: Request):\n    data = await request.json()\n    addr = controller.get_worker_address(data[\"model\"])\n    return {\"address\": addr}\n\n\n@app.post(\"/receive_heart_beat\")\nasync def receive_heart_beat(request: Request):\n    data = await request.json()\n    exist = controller.receive_heart_beat(\n        data[\"worker_name\"], data[\"queue_length\"])\n    return {\"exist\": exist}\n\n\n@app.post(\"/worker_generate_stream\")\nasync def worker_api_generate_stream(request: Request):\n    params = await request.json()\n    generator = controller.worker_api_generate_stream(params)\n    return StreamingResponse(generator)\n\n\n@app.post(\"/worker_get_status\")\nasync def worker_api_get_status(request: Request):\n    return controller.worker_api_get_status()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21001)\n    parser.add_argument(\"--dispatch-method\", type=str, choices=[\n        \"lottery\", \"shortest_queue\"], default=\"shortest_queue\")\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    controller = Controller(args.dispatch_method)\n    uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")"}
{"type": "source_file", "path": "graphgpt/serve/cli.py", "content": "\"\"\"\nChat with a model with command line interface.\n\nUsage:\npython3 -m fastchat.serve.cli --model lmsys/fastchat-t5-3b-v1.0\npython3 -m fastchat.serve.cli --model ~/model_weights/vicuna-7b\n\"\"\"\nimport argparse\nimport os\nimport re\nimport sys\n\nfrom prompt_toolkit import PromptSession\nfrom prompt_toolkit.auto_suggest import AutoSuggestFromHistory\nfrom prompt_toolkit.completion import WordCompleter\nfrom prompt_toolkit.history import InMemoryHistory\nfrom rich.console import Console\nfrom rich.markdown import Markdown\nfrom rich.live import Live\n\nfrom fastchat.model.model_adapter import add_model_args\nfrom fastchat.serve.inference import chat_loop, ChatIO\n\n\nclass SimpleChatIO(ChatIO):\n    def prompt_for_input(self, role) -> str:\n        return input(f\"{role}: \")\n\n    def prompt_for_output(self, role: str):\n        print(f\"{role}: \", end=\"\", flush=True)\n\n    def stream_output(self, output_stream):\n        pre = 0\n        for outputs in output_stream:\n            output_text = outputs[\"text\"]\n            output_text = output_text.strip().split(\" \")\n            now = len(output_text) - 1\n            if now > pre:\n                print(\" \".join(output_text[pre:now]), end=\" \", flush=True)\n                pre = now\n        print(\" \".join(output_text[pre:]), flush=True)\n        return \" \".join(output_text)\n\n\nclass RichChatIO(ChatIO):\n    def __init__(self):\n        self._prompt_session = PromptSession(history=InMemoryHistory())\n        self._completer = WordCompleter(\n            words=[\"!exit\", \"!reset\"], pattern=re.compile(\"$\")\n        )\n        self._console = Console()\n\n    def prompt_for_input(self, role) -> str:\n        self._console.print(f\"[bold]{role}:\")\n        # TODO(suquark): multiline input has some issues. fix it later.\n        prompt_input = self._prompt_session.prompt(\n            completer=self._completer,\n            multiline=False,\n            auto_suggest=AutoSuggestFromHistory(),\n            key_bindings=None,\n        )\n        self._console.print()\n        return prompt_input\n\n    def prompt_for_output(self, role: str):\n        self._console.print(f\"[bold]{role}:\")\n\n    def stream_output(self, output_stream):\n        \"\"\"Stream output from a role.\"\"\"\n        # TODO(suquark): the console flickers when there is a code block\n        #  above it. We need to cut off \"live\" when a code block is done.\n\n        # Create a Live context for updating the console output\n        with Live(console=self._console, refresh_per_second=4) as live:\n            # Read lines from the stream\n            for outputs in output_stream:\n                if not outputs:\n                    continue\n                text = outputs[\"text\"]\n                # Render the accumulated text as Markdown\n                # NOTE: this is a workaround for the rendering \"unstandard markdown\"\n                #  in rich. The chatbots output treat \"\\n\" as a new line for\n                #  better compatibility with real-world text. However, rendering\n                #  in markdown would break the format. It is because standard markdown\n                #  treat a single \"\\n\" in normal text as a space.\n                #  Our workaround is adding two spaces at the end of each line.\n                #  This is not a perfect solution, as it would\n                #  introduce trailing spaces (only) in code block, but it works well\n                #  especially for console output, because in general the console does not\n                #  care about trailing spaces.\n                lines = []\n                for line in text.splitlines():\n                    lines.append(line)\n                    if line.startswith(\"```\"):\n                        # Code block marker - do not add trailing spaces, as it would\n                        #  break the syntax highlighting\n                        lines.append(\"\\n\")\n                    else:\n                        lines.append(\"  \\n\")\n                markdown = Markdown(\"\".join(lines))\n                # Update the Live console output\n                live.update(markdown)\n        self._console.print()\n        return text\n\n\nclass ProgrammaticChatIO(ChatIO):\n    def prompt_for_input(self, role) -> str:\n        print(f\"[!OP:{role}]: \", end=\"\", flush=True)\n        contents = \"\"\n        # `end_sequence` is a randomly-generated, 16-digit number\n        #  that signals the end of a message. It is unlikely to occur in\n        #  message content.\n        end_sequence = \"9745805894023423\"\n        while True:\n            if len(contents) >= 16:\n                last_chars = contents[-16:]\n                if last_chars == end_sequence:\n                    break\n            try:\n                char = sys.stdin.read(1)\n                contents = contents + char\n            except EOFError:\n                continue\n        return contents[:-16]\n\n    def prompt_for_output(self, role: str):\n        print(f\"[!OP:{role}]: \", end=\"\", flush=True)\n\n    def stream_output(self, output_stream):\n        pre = 0\n        for outputs in output_stream:\n            output_text = outputs[\"text\"]\n            output_text = output_text.strip().split(\" \")\n            now = len(output_text) - 1\n            if now > pre:\n                print(\" \".join(output_text[pre:now]), end=\" \", flush=True)\n                pre = now\n        print(\" \".join(output_text[pre:]), flush=True)\n        return \" \".join(output_text)\n\n\ndef main(args):\n    if args.gpus:\n        if len(args.gpus.split(\",\")) < args.num_gpus:\n            raise ValueError(\n                f\"Larger --num-gpus ({args.num_gpus}) than --gpus {args.gpus}!\"\n            )\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n\n    if args.style == \"simple\":\n        chatio = SimpleChatIO()\n    elif args.style == \"rich\":\n        chatio = RichChatIO()\n    elif args.style == \"programmatic\":\n        chatio = ProgrammaticChatIO()\n    else:\n        raise ValueError(f\"Invalid style for console: {args.style}\")\n    try:\n        chat_loop(\n            args.model_path,\n            args.device,\n            args.num_gpus,\n            args.max_gpu_memory,\n            args.load_8bit,\n            args.cpu_offloading,\n            args.conv_template,\n            args.temperature,\n            args.repetition_penalty,\n            args.max_new_tokens,\n            chatio,\n            args.debug,\n        )\n    except KeyboardInterrupt:\n        print(\"exit...\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    add_model_args(parser)\n    parser.add_argument(\n        \"--conv-template\", type=str, default=None, help=\"Conversation prompt template.\"\n    )\n    parser.add_argument(\"--temperature\", type=float, default=0.7)\n    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0)\n    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\n    parser.add_argument(\n        \"--style\",\n        type=str,\n        default=\"simple\",\n        choices=[\"simple\", \"rich\", \"programmatic\"],\n        help=\"Display style.\",\n    )\n    parser.add_argument(\n        \"--debug\",\n        action=\"store_true\",\n        help=\"Print useful debug information (e.g., prompts)\",\n    )\n    args = parser.parse_args()\n    main(args)"}
{"type": "source_file", "path": "graphgpt/serve/gradio_block_arena_anony.py", "content": "\"\"\"\nChatbot Arena (battle) tab.\nUsers chat with two anonymous models.\n\"\"\"\n\nimport json\nimport time\n\nimport gradio as gr\nimport numpy as np\n\nfrom fastchat.constants import (\n    MODERATION_MSG,\n    CONVERSATION_LIMIT_MSG,\n    INPUT_CHAR_LEN_LIMIT,\n    CONVERSATION_LEN_LIMIT,\n)\nfrom fastchat.model.model_adapter import get_conversation_template\nfrom fastchat.serve.gradio_patch import Chatbot as grChatbot\nfrom fastchat.serve.gradio_web_server import (\n    State,\n    http_bot,\n    get_conv_log_filename,\n    no_change_btn,\n    enable_btn,\n    disable_btn,\n    learn_more_md,\n)\nfrom fastchat.utils import (\n    build_logger,\n    violates_moderation,\n)\n\nlogger = build_logger(\"gradio_web_server_multi\", \"gradio_web_server_multi.log\")\n\nnum_models = 2\nenable_moderation = False\nanony_names = [\"\", \"\"]\nmodels = []\n\n\ndef set_global_vars_anony(enable_moderation_):\n    global enable_moderation\n    enable_moderation = enable_moderation_\n\n\ndef load_demo_side_by_side_anony(models_, url_params):\n    global models\n    models = models_\n\n    states = (None,) * num_models\n    selector_updates = (\n        gr.Markdown.update(visible=True),\n        gr.Markdown.update(visible=True),\n    )\n\n    return (\n        states\n        + selector_updates\n        + (gr.Chatbot.update(visible=True),) * num_models\n        + (\n            gr.Textbox.update(visible=True),\n            gr.Box.update(visible=True),\n            gr.Row.update(visible=True),\n            gr.Row.update(visible=True),\n            gr.Accordion.update(visible=True),\n        )\n    )\n\n\ndef vote_last_response(states, vote_type, model_selectors, request: gr.Request):\n    with open(get_conv_log_filename(), \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"models\": [x for x in model_selectors],\n            \"states\": [x.dict() for x in states],\n            \"ip\": request.client.host,\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n\n    if \":\" not in model_selectors[0]:\n        for i in range(15):\n            names = (\n                \"### Model A: \" + states[0].model_name,\n                \"### Model B: \" + states[1].model_name,\n            )\n            yield names + (\"\",) + (disable_btn,) * 4\n            time.sleep(0.2)\n    else:\n        names = (\n            \"### Model A: \" + states[0].model_name,\n            \"### Model B: \" + states[1].model_name,\n        )\n        yield names + (\"\",) + (disable_btn,) * 4\n\n\ndef leftvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"leftvote (anony). ip: {request.client.host}\")\n    for x in vote_last_response(\n        [state0, state1], \"leftvote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef rightvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"rightvote (anony). ip: {request.client.host}\")\n    for x in vote_last_response(\n        [state0, state1], \"rightvote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef tievote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"tievote (anony). ip: {request.client.host}\")\n    for x in vote_last_response(\n        [state0, state1], \"tievote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef bothbad_vote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"bothbad_vote (anony). ip: {request.client.host}\")\n    for x in vote_last_response(\n        [state0, state1], \"bothbad_vote\", [model_selector0, model_selector1], request\n    ):\n        yield x\n\n\ndef regenerate(state0, state1, request: gr.Request):\n    logger.info(f\"regenerate (anony). ip: {request.client.host}\")\n    states = [state0, state1]\n    for i in range(num_models):\n        states[i].conv.update_last_message(None)\n    return states + [x.to_gradio_chatbot() for x in states] + [\"\"] + [disable_btn] * 6\n\n\ndef clear_history(request: gr.Request):\n    logger.info(f\"clear_history (anony). ip: {request.client.host}\")\n    return (\n        [None] * num_models\n        + [None] * num_models\n        + anony_names\n        + [\"\"]\n        + [disable_btn] * 6\n    )\n\n\ndef share_click(state0, state1, model_selector0, model_selector1, request: gr.Request):\n    logger.info(f\"share (anony). ip: {request.client.host}\")\n    if state0 is not None and state1 is not None:\n        vote_last_response(\n            [state0, state1], \"share\", [model_selector0, model_selector1], request\n        )\n\n\nDEFAULT_WEIGHTS = {\n    \"gpt-4\": 1.5,\n    \"gpt-3.5-turbo\": 1.5,\n    \"claude-v1\": 1.5,\n    \"claude-instant-v1\": 1.5,\n    \"bard\": 1.5,\n    \"vicuna-13b\": 1.5,\n    \"koala-13b\": 1.5,\n    \"vicuna-7b\": 1.2,\n    \"mpt-7b-chat\": 1.2,\n    \"oasst-pythia-12b\": 1.2,\n    \"RWKV-4-Raven-14B\": 1.2,\n    \"fastchat-t5-3b\": 1,\n    \"alpaca-13b\": 1,\n    \"chatglm-6b\": 1,\n    \"stablelm-tuned-alpha-7b\": 0.5,\n    \"dolly-v2-12b\": 0.5,\n    \"llama-13b\": 0.1,\n}\n\n\ndef add_text(\n    state0, state1, model_selector0, model_selector1, text, request: gr.Request\n):\n    logger.info(f\"add_text (anony). ip: {request.client.host}. len: {len(text)}\")\n    states = [state0, state1]\n    model_selectors = [model_selector0, model_selector1]\n\n    if states[0] is None:\n        assert states[1] is None\n        weights = [DEFAULT_WEIGHTS.get(m, 1.0) for m in models]\n        if len(models) > 1:\n            weights = weights / np.sum(weights)\n            model_left, model_right = np.random.choice(\n                models, size=(2,), p=weights, replace=False\n            )\n        else:\n            model_left = model_right = models[0]\n\n        states = [\n            State(model_left),\n            State(model_right),\n        ]\n\n    if len(text) <= 0:\n        for i in range(num_models):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [\"\"]\n            + [\n                no_change_btn,\n            ]\n            * 6\n        )\n\n    if enable_moderation:\n        flagged = violates_moderation(text)\n        if flagged:\n            logger.info(\n                f\"violate moderation (anony). ip: {request.client.host}. text: {text}\"\n            )\n            for i in range(num_models):\n                states[i].skip_next = True\n            return (\n                states\n                + [x.to_gradio_chatbot() for x in states]\n                + [MODERATION_MSG]\n                + [\n                    no_change_btn,\n                ]\n                * 6\n            )\n\n    conv = states[0].conv\n    if (len(conv.messages) - conv.offset) // 2 >= CONVERSATION_LEN_LIMIT:\n        logger.info(\n            f\"hit conversation length limit. ip: {request.client.host}. text: {text}\"\n        )\n        for i in range(num_models):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [CONVERSATION_LIMIT_MSG]\n            + [\n                no_change_btn,\n            ]\n            * 6\n        )\n\n    text = text[:INPUT_CHAR_LEN_LIMIT]  # Hard cut-off\n    for i in range(num_models):\n        states[i].conv.append_message(states[i].conv.roles[0], text)\n        states[i].conv.append_message(states[i].conv.roles[1], None)\n        states[i].skip_next = False\n\n    return (\n        states\n        + [x.to_gradio_chatbot() for x in states]\n        + [\"\"]\n        + [\n            disable_btn,\n        ]\n        * 6\n    )\n\n\ndef http_bot_all(\n    state0,\n    state1,\n    temperature,\n    top_p,\n    max_new_tokens,\n    request: gr.Request,\n):\n    logger.info(f\"http_bot_all (anony). ip: {request.client.host}\")\n\n    if state0.skip_next:\n        # This generate call is skipped due to invalid inputs\n        yield (\n            state0,\n            state1,\n            state0.to_gradio_chatbot(),\n            state1.to_gradio_chatbot(),\n        ) + (no_change_btn,) * 6\n        return\n\n    states = [state0, state1]\n    gen = []\n    for i in range(num_models):\n        gen.append(\n            http_bot(\n                states[i],\n                temperature,\n                top_p,\n                max_new_tokens,\n                request,\n            )\n        )\n\n    chatbots = [None] * num_models\n    while True:\n        stop = True\n        for i in range(num_models):\n            try:\n                ret = next(gen[i])\n                states[i], chatbots[i] = ret[0], ret[1]\n                stop = False\n            except StopIteration:\n                pass\n        yield states + chatbots + [disable_btn] * 6\n        if stop:\n            break\n\n    for i in range(10):\n        if i % 2 == 0:\n            yield states + chatbots + [disable_btn] * 4 + [enable_btn] * 2\n        else:\n            yield states + chatbots + [enable_btn] * 6\n        time.sleep(0.2)\n\n\ndef build_side_by_side_ui_anony(models):\n    notice_markdown = \"\"\"\n# ⚔️  Chatbot Arena ⚔️ \n### Rules\n- Chat with two anonymous models side-by-side and vote for which one is better!\n- You can do multiple rounds of conversations before voting.\n- The names of the models will be revealed after your vote. Conversations with identity keywords (e.g., ChatGPT, Bard, Vicuna) or any votes after the names are revealed will not count towards the leaderboard.\n- Click \"Clear history\" to start a new round.\n- [[Blog](https://lmsys.org/blog/2023-05-03-arena/)] [[GitHub]](https://github.com/lm-sys/FastChat) [[Twitter]](https://twitter.com/lmsysorg) [[Discord]](https://discord.gg/KjdtsE9V)\n\n### Terms of use\nBy using this service, users are required to agree to the following terms: The service is a research preview intended for non-commercial use only. It only provides limited safety measures and may generate offensive content. It must not be used for any illegal, harmful, violent, racist, or sexual purposes. **The service collects user dialogue data and reserves the right to distribute it under a Creative Commons Attribution (CC-BY) license.** The demo works better on desktop devices with a wide screen.\n\n### Battle\nPlease scroll down and start chatting. You can view a leaderboard of participating models in the fourth tab above labeled 'Leaderboard' or by clicking [here](?leaderboard). The models include both closed-source models (e.g., ChatGPT) and open-source models (e.g., Vicuna).\n\"\"\"\n\n    states = [gr.State() for _ in range(num_models)]\n    model_selectors = [None] * num_models\n    chatbots = [None] * num_models\n\n    gr.Markdown(notice_markdown, elem_id=\"notice_markdown\")\n\n    with gr.Box(elem_id=\"share-region-anony\"):\n        with gr.Row():\n            for i in range(num_models):\n                with gr.Column():\n                    model_selectors[i] = gr.Markdown(anony_names[i])\n\n        with gr.Row():\n            for i in range(num_models):\n                label = \"Model A\" if i == 0 else \"Model B\"\n                with gr.Column():\n                    chatbots[i] = grChatbot(\n                        label=label, elem_id=f\"chatbot\", visible=False\n                    ).style(height=550)\n\n        with gr.Box() as button_row:\n            with gr.Row():\n                leftvote_btn = gr.Button(value=\"👈  A is better\", interactive=False)\n                rightvote_btn = gr.Button(value=\"👉  B is better\", interactive=False)\n                tie_btn = gr.Button(value=\"🤝  Tie\", interactive=False)\n                bothbad_btn = gr.Button(value=\"👎  Both are bad\", interactive=False)\n\n    with gr.Row():\n        with gr.Column(scale=20):\n            textbox = gr.Textbox(\n                show_label=False,\n                placeholder=\"Enter text and press ENTER\",\n                visible=False,\n            ).style(container=False)\n        with gr.Column(scale=1, min_width=50):\n            send_btn = gr.Button(value=\"Send\", visible=False)\n\n    with gr.Row() as button_row2:\n        regenerate_btn = gr.Button(value=\"🔄  Regenerate\", interactive=False)\n        clear_btn = gr.Button(value=\"🗑️  Clear history\", interactive=False)\n        share_btn = gr.Button(value=\"📷  Share\")\n\n    with gr.Accordion(\"Parameters\", open=False, visible=True) as parameter_row:\n        temperature = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.7,\n            step=0.1,\n            interactive=True,\n            label=\"Temperature\",\n        )\n        top_p = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=1.0,\n            step=0.1,\n            interactive=True,\n            label=\"Top P\",\n        )\n        max_output_tokens = gr.Slider(\n            minimum=16,\n            maximum=1024,\n            value=512,\n            step=64,\n            interactive=True,\n            label=\"Max output tokens\",\n        )\n\n    gr.Markdown(learn_more_md)\n\n    # Register listeners\n    btn_list = [\n        leftvote_btn,\n        rightvote_btn,\n        tie_btn,\n        bothbad_btn,\n        regenerate_btn,\n        clear_btn,\n    ]\n    leftvote_btn.click(\n        leftvote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    rightvote_btn.click(\n        rightvote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    tie_btn.click(\n        tievote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    bothbad_btn.click(\n        bothbad_vote_last_response,\n        states + model_selectors,\n        model_selectors + [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    regenerate_btn.click(\n        regenerate, states, states + chatbots + [textbox] + btn_list\n    ).then(\n        http_bot_all,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    )\n    clear_btn.click(\n        clear_history, None, states + chatbots + model_selectors + [textbox] + btn_list\n    )\n\n    share_js = \"\"\"\nfunction (a, b, c, d) {\n    const captureElement = document.querySelector('#share-region-anony');\n    html2canvas(captureElement)\n        .then(canvas => {\n            canvas.style.display = 'none'\n            document.body.appendChild(canvas)\n            return canvas\n        })\n        .then(canvas => {\n            const image = canvas.toDataURL('image/png')\n            const a = document.createElement('a')\n            a.setAttribute('download', 'chatbot-arena.png')\n            a.setAttribute('href', image)\n            a.click()\n            canvas.remove()\n        });\n    return [a, b, c, d];\n}\n\"\"\"\n    share_btn.click(share_click, states + model_selectors, [], _js=share_js)\n\n    textbox.submit(\n        add_text,\n        states + model_selectors + [textbox],\n        states + chatbots + [textbox] + btn_list,\n    ).then(\n        http_bot_all,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    )\n    send_btn.click(\n        add_text,\n        states + model_selectors + [textbox],\n        states + chatbots + [textbox] + btn_list,\n    ).then(\n        http_bot_all,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    )\n\n    return (\n        states,\n        model_selectors,\n        chatbots,\n        textbox,\n        send_btn,\n        button_row,\n        button_row2,\n        parameter_row,\n    )\n"}
{"type": "source_file", "path": "graphgpt/serve/bard_worker.py", "content": "\"\"\"\nAdapted from https://github.com/acheong08/Bard.\n\"\"\"\nimport argparse\nimport json\nimport random\nimport re\nimport string\n\nfrom fastapi import FastAPI\nimport httpx\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional, Union\nimport uvicorn\n\n\nclass ConversationState(BaseModel):\n    conversation_id: str = \"\"\n    response_id: str = \"\"\n    choice_id: str = \"\"\n    req_id: int = 0\n\n\nclass Message(BaseModel):\n    content: str\n    state: ConversationState = Field(default_factory=ConversationState)\n\n\nclass Response(BaseModel):\n    content: str\n    factualityQueries: Optional[List]\n    textQuery: Optional[Union[str, List]]\n    choices: List[dict]\n    state: ConversationState\n\n\nclass Chatbot:\n    \"\"\"\n    A class to interact with Google Bard.\n    Parameters\n        session_id: str\n            The __Secure-1PSID cookie.\n    \"\"\"\n\n    def __init__(self, session_id):\n        headers = {\n            \"Host\": \"bard.google.com\",\n            \"X-Same-Domain\": \"1\",\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n            \"Content-Type\": \"application/x-www-form-urlencoded;charset=UTF-8\",\n            \"Origin\": \"https://bard.google.com\",\n            \"Referer\": \"https://bard.google.com/\",\n        }\n        self.session = httpx.AsyncClient()\n        self.session.headers = headers\n        self.session.cookies.set(\"__Secure-1PSID\", session_id)\n        self.SNlM0e = None\n\n    async def _get_snlm0e(self):\n        resp = await self.session.get(url=\"https://bard.google.com/\", timeout=10)\n        # Find \"SNlM0e\":\"<ID>\"\n        if resp.status_code != 200:\n            raise Exception(\"Could not get Google Bard\")\n        SNlM0e = re.search(r\"SNlM0e\\\":\\\"(.*?)\\\"\", resp.text).group(1)\n        return SNlM0e\n\n    async def ask(self, message: Message) -> Response:\n        \"\"\"\n        Send a message to Google Bard and return the response.\n        :param message: The message to send to Google Bard.\n        :return: A dict containing the response from Google Bard.\n        \"\"\"\n        if message.state.conversation_id == \"\":\n            message.state.req_id = int(\"\".join(random.choices(string.digits, k=4)))\n        # url params\n        params = {\n            # \"bl\": \"boq_assistant-bard-web-server_20230315.04_p2\",\n            # This is a newer API version\n            \"bl\": \"boq_assistant-bard-web-server_20230507.20_p2\",\n            \"_reqid\": str(message.state.req_id),\n            \"rt\": \"c\",\n        }\n\n        # message arr -> data[\"f.req\"]. Message is double json stringified\n        message_struct = [\n            [message.content],\n            None,\n            [\n                message.state.conversation_id,\n                message.state.response_id,\n                message.state.choice_id,\n            ],\n        ]\n        data = {\n            \"f.req\": json.dumps([None, json.dumps(message_struct)]),\n            \"at\": self.SNlM0e,\n        }\n\n        # do the request!\n        resp = await self.session.post(\n            \"https://bard.google.com/_/BardChatUi/data/assistant.lamda.BardFrontendService/StreamGenerate\",\n            params=params,\n            data=data,\n            timeout=60,\n        )\n\n        chat_data = json.loads(resp.content.splitlines()[3])[0][2]\n        if not chat_data:\n            return Response(\n                content=f\"Google Bard encountered an error: {resp.content}.\",\n                factualityQueries=[],\n                textQuery=\"\",\n                choices=[],\n                state=message.state,\n            )\n        json_chat_data = json.loads(chat_data)\n        conversation = ConversationState(\n            conversation_id=json_chat_data[1][0],\n            response_id=json_chat_data[1][1],\n            choice_id=json_chat_data[4][0][0],\n            req_id=message.state.req_id + 100000,\n        )\n        return Response(\n            content=json_chat_data[0][0],\n            factualityQueries=json_chat_data[3],\n            textQuery=json_chat_data[2][0] if json_chat_data[2] is not None else \"\",\n            choices=[{\"id\": i[0], \"content\": i[1]} for i in json_chat_data[4]],\n            state=conversation,\n        )\n\n\napp = FastAPI()\nchatbot = None\n\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    global chatbot\n    cookie = json.load(open(\"bard_cookie.json\"))\n    chatbot = Chatbot(cookie[\"__Secure-1PSID\"])\n    chatbot.SNlM0e = await chatbot._get_snlm0e()\n\n\n@app.post(\"/chat\", response_model=Response)\nasync def chat(message: Message):\n    response = await chatbot.ask(message)\n    return response\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\"Google Bard worker\")\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=18900)\n    parser.add_argument(\"--reload\", action=\"store_true\")\n    args = parser.parse_args()\n    uvicorn.run(\n        \"bard_worker:app\", host=args.host, port=args.port, log_level=\"info\",\n        reload=args.reload\n    )\n"}
{"type": "source_file", "path": "graphgpt/serve/gradio_block_arena_named.py", "content": "\"\"\"\nChatbot Arena (side-by-side) tab.\nUsers chat with two chosen models.\n\"\"\"\n\nimport json\nimport time\n\nimport gradio as gr\nimport numpy as np\n\nfrom fastchat.constants import (\n    MODERATION_MSG,\n    CONVERSATION_LIMIT_MSG,\n    INPUT_CHAR_LEN_LIMIT,\n    CONVERSATION_LEN_LIMIT,\n)\nfrom fastchat.model.model_adapter import get_conversation_template\nfrom fastchat.serve.gradio_patch import Chatbot as grChatbot\nfrom fastchat.serve.gradio_web_server import (\n    State,\n    http_bot,\n    get_conv_log_filename,\n    get_model_description_md,\n    no_change_btn,\n    enable_btn,\n    disable_btn,\n    learn_more_md,\n)\nfrom fastchat.utils import (\n    build_logger,\n    violates_moderation,\n)\n\n\nlogger = build_logger(\"gradio_web_server_multi\", \"gradio_web_server_multi.log\")\n\nnum_models = 2\nenable_moderation = False\n\n\ndef set_global_vars_named(enable_moderation_):\n    global enable_moderation\n    enable_moderation = enable_moderation_\n\n\ndef load_demo_side_by_side_named(models, url_params):\n    states = (None,) * num_models\n\n    model_left = models[0] if len(models) > 0 else \"\"\n    if len(models) > 1:\n        weights = ([8, 4, 2, 1] + [1] * 32)[: len(models) - 1]\n        weights = weights / np.sum(weights)\n        model_right = np.random.choice(models[1:], p=weights)\n    else:\n        model_right = model_left\n\n    selector_updates = (\n        gr.Dropdown.update(model_left, visible=True),\n        gr.Dropdown.update(model_right, visible=True),\n    )\n\n    return (\n        states\n        + selector_updates\n        + (gr.Chatbot.update(visible=True),) * num_models\n        + (\n            gr.Textbox.update(visible=True),\n            gr.Box.update(visible=True),\n            gr.Row.update(visible=True),\n            gr.Row.update(visible=True),\n            gr.Accordion.update(visible=True),\n        )\n    )\n\n\ndef vote_last_response(states, vote_type, model_selectors, request: gr.Request):\n    with open(get_conv_log_filename(), \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"models\": [x for x in model_selectors],\n            \"states\": [x.dict() for x in states],\n            \"ip\": request.client.host,\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n\n\ndef leftvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"leftvote (named). ip: {request.client.host}\")\n    vote_last_response(\n        [state0, state1], \"leftvote\", [model_selector0, model_selector1], request\n    )\n    return (\"\",) + (disable_btn,) * 4\n\n\ndef rightvote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"rightvote (named). ip: {request.client.host}\")\n    vote_last_response(\n        [state0, state1], \"rightvote\", [model_selector0, model_selector1], request\n    )\n    return (\"\",) + (disable_btn,) * 4\n\n\ndef tievote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"tievote (named). ip: {request.client.host}\")\n    vote_last_response(\n        [state0, state1], \"tievote\", [model_selector0, model_selector1], request\n    )\n    return (\"\",) + (disable_btn,) * 4\n\n\ndef bothbad_vote_last_response(\n    state0, state1, model_selector0, model_selector1, request: gr.Request\n):\n    logger.info(f\"bothbad_vote (named). ip: {request.client.host}\")\n    vote_last_response(\n        [state0, state1], \"bothbad_vote\", [model_selector0, model_selector1], request\n    )\n    return (\"\",) + (disable_btn,) * 4\n\n\ndef regenerate(state0, state1, request: gr.Request):\n    logger.info(f\"regenerate (named). ip: {request.client.host}\")\n    states = [state0, state1]\n    for i in range(num_models):\n        states[i].conv.update_last_message(None)\n    return states + [x.to_gradio_chatbot() for x in states] + [\"\"] + [disable_btn] * 6\n\n\ndef clear_history(request: gr.Request):\n    logger.info(f\"clear_history (named). ip: {request.client.host}\")\n    return [None] * num_models + [None] * num_models + [\"\"] + [disable_btn] * 6\n\n\ndef share_click(state0, state1, model_selector0, model_selector1, request: gr.Request):\n    logger.info(f\"share (named). ip: {request.client.host}\")\n    if state0 is not None and state1 is not None:\n        vote_last_response(\n            [state0, state1], \"share\", [model_selector0, model_selector1], request\n        )\n\n\ndef add_text(\n    state0, state1, model_selector0, model_selector1, text, request: gr.Request\n):\n    logger.info(f\"add_text (named). ip: {request.client.host}. len: {len(text)}\")\n    states = [state0, state1]\n    model_selectors = [model_selector0, model_selector1]\n\n    for i in range(num_models):\n        if states[i] is None:\n            states[i] = State(model_selectors[i])\n\n    if len(text) <= 0:\n        for i in range(num_models):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [\"\"]\n            + [\n                no_change_btn,\n            ]\n            * 6\n        )\n\n    if enable_moderation:\n        flagged = violates_moderation(text)\n        if flagged:\n            logger.info(\n                f\"violate moderation (named). ip: {request.client.host}. text: {text}\"\n            )\n            for i in range(num_models):\n                states[i].skip_next = True\n            return (\n                states\n                + [x.to_gradio_chatbot() for x in states]\n                + [MODERATION_MSG]\n                + [\n                    no_change_btn,\n                ]\n                * 6\n            )\n\n    conv = states[0].conv\n    if (len(conv.messages) - conv.offset) // 2 >= CONVERSATION_LEN_LIMIT:\n        logger.info(\n            f\"hit conversation length limit. ip: {request.client.host}. text: {text}\"\n        )\n        for i in range(num_models):\n            states[i].skip_next = True\n        return (\n            states\n            + [x.to_gradio_chatbot() for x in states]\n            + [CONVERSATION_LIMIT_MSG]\n            + [\n                no_change_btn,\n            ]\n            * 6\n        )\n\n    text = text[:INPUT_CHAR_LEN_LIMIT]  # Hard cut-off\n    for i in range(num_models):\n        states[i].conv.append_message(states[i].conv.roles[0], text)\n        states[i].conv.append_message(states[i].conv.roles[1], None)\n        states[i].skip_next = False\n\n    return (\n        states\n        + [x.to_gradio_chatbot() for x in states]\n        + [\"\"]\n        + [\n            disable_btn,\n        ]\n        * 6\n    )\n\n\ndef http_bot_all(\n    state0,\n    state1,\n    temperature,\n    top_p,\n    max_new_tokens,\n    request: gr.Request,\n):\n    logger.info(f\"http_bot_all (named). ip: {request.client.host}\")\n\n    if state0.skip_next:\n        # This generate call is skipped due to invalid inputs\n        yield (\n            state0,\n            state1,\n            state0.to_gradio_chatbot(),\n            state1.to_gradio_chatbot(),\n        ) + (no_change_btn,) * 6\n        return\n\n    states = [state0, state1]\n    gen = []\n    for i in range(num_models):\n        gen.append(\n            http_bot(\n                states[i],\n                temperature,\n                top_p,\n                max_new_tokens,\n                request,\n            )\n        )\n\n    chatbots = [None] * num_models\n    while True:\n        stop = True\n        for i in range(num_models):\n            try:\n                ret = next(gen[i])\n                states[i], chatbots[i] = ret[0], ret[1]\n                stop = False\n            except StopIteration:\n                pass\n        yield states + chatbots + [disable_btn] * 6\n        if stop:\n            break\n\n    for i in range(10):\n        if i % 2 == 0:\n            yield states + chatbots + [disable_btn] * 4 + [enable_btn] * 2\n        else:\n            yield states + chatbots + [enable_btn] * 6\n        time.sleep(0.2)\n\n\ndef build_side_by_side_ui_named(models):\n    notice_markdown = \"\"\"\n# ⚔️  Chatbot Arena ⚔️ \n### Rules\n- Chat with two models side-by-side and vote for which one is better!\n- You pick the models you want to chat with.\n- You can do multiple rounds of conversations before voting.\n- Click \"Clear history\" to start a new round.\n- [[Blog](https://lmsys.org/blog/2023-05-03-arena/)] [[GitHub]](https://github.com/lm-sys/FastChat) [[Twitter]](https://twitter.com/lmsysorg) [[Discord]](https://discord.gg/KjdtsE9V)\n\n### Terms of use\nBy using this service, users are required to agree to the following terms: The service is a research preview intended for non-commercial use only. It only provides limited safety measures and may generate offensive content. It must not be used for any illegal, harmful, violent, racist, or sexual purposes. **The service collects user dialogue data and reserves the right to distribute it under a Creative Commons Attribution (CC-BY) license.** The demo works better on desktop devices with a wide screen.\n\n### Choose two models to chat with (view [leaderboard](?leaderboard))\n\"\"\"\n\n    states = [gr.State() for _ in range(num_models)]\n    model_selectors = [None] * num_models\n    chatbots = [None] * num_models\n\n    model_description_md = get_model_description_md(models)\n    notice = gr.Markdown(\n        notice_markdown + model_description_md, elem_id=\"notice_markdown\"\n    )\n\n    with gr.Box(elem_id=\"share-region-named\"):\n        with gr.Row():\n            for i in range(num_models):\n                with gr.Column():\n                    model_selectors[i] = gr.Dropdown(\n                        choices=models,\n                        value=models[i] if len(models) > i else \"\",\n                        interactive=True,\n                        show_label=False,\n                    ).style(container=False)\n\n        with gr.Row():\n            for i in range(num_models):\n                label = \"Model A\" if i == 0 else \"Model B\"\n                with gr.Column():\n                    chatbots[i] = grChatbot(\n                        label=label, elem_id=f\"chatbot\", visible=False\n                    ).style(height=550)\n\n        with gr.Box() as button_row:\n            with gr.Row():\n                leftvote_btn = gr.Button(value=\"👈  A is better\", interactive=False)\n                rightvote_btn = gr.Button(value=\"👉  B is better\", interactive=False)\n                tie_btn = gr.Button(value=\"🤝  Tie\", interactive=False)\n                bothbad_btn = gr.Button(value=\"👎  Both are bad\", interactive=False)\n\n    with gr.Row():\n        with gr.Column(scale=20):\n            textbox = gr.Textbox(\n                show_label=False,\n                placeholder=\"Enter text and press ENTER\",\n                visible=False,\n            ).style(container=False)\n        with gr.Column(scale=1, min_width=50):\n            send_btn = gr.Button(value=\"Send\", visible=False)\n\n    with gr.Row() as button_row2:\n        regenerate_btn = gr.Button(value=\"🔄  Regenerate\", interactive=False)\n        clear_btn = gr.Button(value=\"🗑️  Clear history\", interactive=False)\n        share_btn = gr.Button(value=\"📷  Share\")\n\n    with gr.Accordion(\"Parameters\", open=False, visible=True) as parameter_row:\n        temperature = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.7,\n            step=0.1,\n            interactive=True,\n            label=\"Temperature\",\n        )\n        top_p = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=1.0,\n            step=0.1,\n            interactive=True,\n            label=\"Top P\",\n        )\n        max_output_tokens = gr.Slider(\n            minimum=16,\n            maximum=1024,\n            value=512,\n            step=64,\n            interactive=True,\n            label=\"Max output tokens\",\n        )\n\n    gr.Markdown(learn_more_md)\n\n    # Register listeners\n    btn_list = [\n        leftvote_btn,\n        rightvote_btn,\n        tie_btn,\n        bothbad_btn,\n        regenerate_btn,\n        clear_btn,\n    ]\n    leftvote_btn.click(\n        leftvote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    rightvote_btn.click(\n        rightvote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    tie_btn.click(\n        tievote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    bothbad_btn.click(\n        bothbad_vote_last_response,\n        states + model_selectors,\n        [textbox, leftvote_btn, rightvote_btn, tie_btn, bothbad_btn],\n    )\n    regenerate_btn.click(\n        regenerate, states, states + chatbots + [textbox] + btn_list\n    ).then(\n        http_bot_all,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    )\n    clear_btn.click(clear_history, None, states + chatbots + [textbox] + btn_list)\n\n    share_js = \"\"\"\nfunction (a, b, c, d) {\n    const captureElement = document.querySelector('#share-region-named');\n    html2canvas(captureElement)\n        .then(canvas => {\n            canvas.style.display = 'none'\n            document.body.appendChild(canvas)\n            return canvas\n        })\n        .then(canvas => {\n            const image = canvas.toDataURL('image/png')\n            const a = document.createElement('a')\n            a.setAttribute('download', 'chatbot-arena.png')\n            a.setAttribute('href', image)\n            a.click()\n            canvas.remove()\n        });\n    return [a, b, c, d];\n}\n\"\"\"\n    share_btn.click(share_click, states + model_selectors, [], _js=share_js)\n\n    for i in range(num_models):\n        model_selectors[i].change(\n            clear_history, None, states + chatbots + [textbox] + btn_list\n        )\n\n    textbox.submit(\n        add_text,\n        states + model_selectors + [textbox],\n        states + chatbots + [textbox] + btn_list,\n    ).then(\n        http_bot_all,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    )\n    send_btn.click(\n        add_text,\n        states + model_selectors + [textbox],\n        states + chatbots + [textbox] + btn_list,\n    ).then(\n        http_bot_all,\n        states + [temperature, top_p, max_output_tokens],\n        states + chatbots + btn_list,\n    )\n\n    return (\n        states,\n        model_selectors,\n        chatbots,\n        textbox,\n        send_btn,\n        button_row,\n        button_row2,\n        parameter_row,\n    )\n"}
{"type": "source_file", "path": "graphgpt/serve/api_provider.py", "content": "\"\"\"Call API providers.\"\"\"\n\nimport os\nimport random\nimport time\n\nfrom fastchat.utils import build_logger\n\n\nlogger = build_logger(\"gradio_web_server\", \"gradio_web_server.log\")\n\n\ndef openai_api_stream_iter(model_name, messages, temperature, top_p, max_new_tokens):\n    import openai\n\n    # Make requests\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": messages,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    res = openai.ChatCompletion.create(\n        model=model_name, messages=messages, temperature=temperature, stream=True\n    )\n    text = \"\"\n    for chunk in res:\n        text += chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n        data = {\n            \"text\": text,\n            \"error_code\": 0,\n        }\n        yield data\n\n\ndef anthropic_api_stream_iter(model_name, prompt, temperature, top_p, max_new_tokens):\n    import anthropic\n\n    c = anthropic.Client(os.environ[\"ANTHROPIC_API_KEY\"])\n\n    # Make requests\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    res = c.completion_stream(\n        prompt=prompt,\n        stop_sequences=[anthropic.HUMAN_PROMPT],\n        max_tokens_to_sample=max_new_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        model=model_name,\n        stream=True,\n    )\n    for chunk in res:\n        data = {\n            \"text\": chunk[\"completion\"],\n            \"error_code\": 0,\n        }\n        yield data\n\n\ndef bard_api_stream_iter(state):\n    # TODO: we will use the official PaLM 2 API sooner or later,\n    # and we will update this function accordingly. So here we just hard code the\n    # Bard worker address. It is going to be deprecated anyway.\n    conv = state.conv\n\n    # Make requests\n    gen_params = {\n        \"model\": \"bard\",\n        \"prompt\": state.messages,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    response = requests.post(\n        \"http://localhost:18900/chat\",\n        json={\n            \"content\": conv.messages[-2][-1],\n            \"state\": state.bard_session_state,\n        },\n        stream=False,\n        timeout=WORKER_API_TIMEOUT,\n    )\n    resp_json = response.json()\n    state.bard_session_state = resp_json[\"state\"]\n    content = resp_json[\"content\"]\n    # The Bard Web API does not support streaming yet. Here we have to simulate\n    # the streaming behavior by adding some time.sleep().\n    pos = 0\n    while pos < len(content):\n        # This is a fancy way to simulate token generation latency combined\n        # with a Poisson process.\n        pos += random.randint(1, 5)\n        time.sleep(random.expovariate(50))\n        data = {\n            \"text\": content[:pos],\n            \"error_code\": 0,\n        }\n        yield data\n\n\ndef init_palm_chat(model_name):\n    import vertexai  # pip3 install google-cloud-aiplatform\n    from vertexai.preview.language_models import ChatModel\n\n    project_id = os.environ[\"GCP_PROJECT_ID\"]\n    location = \"us-central1\"\n    vertexai.init(project=project_id, location=location)\n\n    chat_model = ChatModel.from_pretrained(model_name)\n    chat = chat_model.start_chat(examples=[])\n    return chat\n\n\ndef palm_api_stream_iter(chat, message, temperature, top_p, max_new_tokens):\n    parameters = {\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_output_tokens\": max_new_tokens,\n    }\n    gen_params = {\n        \"model\": \"bard\",\n        \"prompt\": message,\n    }\n    gen_params.update(parameters)\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    response = chat.send_message(message, **parameters)\n    content = response.text\n\n    pos = 0\n    while pos < len(content):\n        # This is a fancy way to simulate token generation latency combined\n        # with a Poisson process.\n        pos += random.randint(10, 20)\n        time.sleep(random.expovariate(50))\n        data = {\n            \"text\": content[:pos],\n            \"error_code\": 0,\n        }\n        yield data\n"}
{"type": "source_file", "path": "graphgpt/model/utils.py", "content": "import torch\n\nfrom transformers import AutoConfig, StoppingCriteria\n\n\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, keywords, tokenizer, input_ids):\n        self.keywords = keywords\n        self.keyword_ids = [tokenizer(keyword).input_ids for keyword in keywords]\n        self.keyword_ids = [keyword_id[0] for keyword_id in self.keyword_ids if type(keyword_id) is list and len(keyword_id) == 1]\n        self.tokenizer = tokenizer\n        self.start_len = None\n        self.input_ids = input_ids\n\n    def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        if self.start_len is None:\n            self.start_len = self.input_ids.shape[1]\n        else:\n            for keyword_id in self.keyword_ids:\n                if output_ids[0, -1] == keyword_id:\n                    return True\n            outputs = self.tokenizer.batch_decode(output_ids[:, self.start_len:], skip_special_tokens=True)[0]\n            for keyword in self.keywords:\n                if keyword in outputs:\n                    return True\n        return False"}
{"type": "source_file", "path": "graphgpt/model/monkey_patch_non_inplace.py", "content": "\"\"\"\nMonkey patch the llama implementation in the huggingface/transformers library.\nAvoid bugs in mps backend by not using in-place operations.\n\"\"\"\nimport math\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nimport transformers\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2].clone()\n    x2 = x[..., x.shape[-1] // 2 :].clone()\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]\n    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])\n    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    key_states = (\n        self.k_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    value_states = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n\n    kv_seq_len = key_states.shape[-2]\n    if past_key_value is not None:\n        kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, position_ids\n    )\n    # [bsz, nh, t, hd]\n\n    if past_key_value is not None:\n        # reuse k, v, self_attention\n        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n    past_key_value = (key_states, value_states) if use_cache else None\n\n    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(\n        self.head_dim\n    )\n\n    if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n        raise ValueError(\n            f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n            f\" {attn_weights.size()}\"\n        )\n\n    if attention_mask is not None:\n        if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n            )\n        attn_weights = attn_weights + attention_mask\n        attn_weights = torch.max(\n            attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min)\n        )\n\n    # upcast attention to fp32\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(\n        query_states.dtype\n    )\n    attn_output = torch.matmul(attn_weights, value_states)\n\n    if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n        raise ValueError(\n            f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n            f\" {attn_output.size()}\"\n        )\n\n    attn_output = attn_output.transpose(1, 2)\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n    attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n\n\ndef replace_llama_attn_with_non_inplace_operations():\n    \"\"\"Avoid bugs in mps backend by not using in-place operations.\"\"\"\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward\n"}
{"type": "source_file", "path": "graphgpt/serve/model_worker.py", "content": "\"\"\"\nA model worker executes the model.\n\"\"\"\nimport argparse\nimport asyncio\nimport dataclasses\nimport logging\nimport json\nimport os\nimport time\nfrom typing import List, Union\nimport threading\nimport uuid\n\nfrom fastapi import FastAPI, Request, BackgroundTasks\nfrom fastapi.responses import StreamingResponse, JSONResponse\nimport requests\n\ntry:\n    from transformers import (\n        AutoTokenizer,\n        AutoModelForCausalLM,\n        LlamaTokenizer,\n        AutoModel,\n    )\nexcept ImportError:\n    from transformers import (\n        AutoTokenizer,\n        AutoModelForCausalLM,\n        LLaMATokenizer,\n        AutoModel,\n    )\nimport torch\nimport torch.nn.functional as F\nimport uvicorn\n\nfrom fastchat.constants import WORKER_HEART_BEAT_INTERVAL, ErrorCode, SERVER_ERROR_MSG\nfrom fastchat.model.model_adapter import load_model, add_model_args\nfrom fastchat.model.chatglm_model import chatglm_generate_stream\nfrom fastchat.serve.inference import generate_stream\nfrom fastchat.utils import build_logger, pretty_print_semaphore\n\nGB = 1 << 30\n\nworker_id = str(uuid.uuid4())[:6]\nlogger = build_logger(\"model_worker\", f\"model_worker_{worker_id}.log\")\nglobal_counter = 0\n\nmodel_semaphore = None\n\n\ndef heart_beat_worker(controller):\n    while True:\n        time.sleep(WORKER_HEART_BEAT_INTERVAL)\n        controller.send_heart_beat()\n\n\nclass ModelWorker:\n    def __init__(\n        self,\n        controller_addr,\n        worker_addr,\n        worker_id,\n        no_register,\n        model_path,\n        model_name,\n        device,\n        num_gpus,\n        max_gpu_memory,\n        load_8bit=False,\n        cpu_offloading=False,\n    ):\n        self.controller_addr = controller_addr\n        self.worker_addr = worker_addr\n        self.worker_id = worker_id\n        if model_path.endswith(\"/\"):\n            model_path = model_path[:-1]\n        self.model_name = model_name or model_path.split(\"/\")[-1]\n        self.device = device\n\n        logger.info(f\"Loading the model {self.model_name} on worker {worker_id} ...\")\n        self.model, self.tokenizer = load_model(\n            model_path, device, num_gpus, max_gpu_memory, load_8bit, cpu_offloading\n        )\n        if self.tokenizer.pad_token == None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        if hasattr(self.model.config, \"max_sequence_length\"):\n            self.context_len = self.model.config.max_sequence_length\n        elif hasattr(self.model.config, \"max_position_embeddings\"):\n            self.context_len = self.model.config.max_position_embeddings\n        else:\n            self.context_len = 2048\n\n        # generate_stream\n        is_chatglm = \"chatglm\" in str(type(self.model)).lower()\n        if is_chatglm:\n            self.generate_stream_func = chatglm_generate_stream\n        else:\n            self.generate_stream_func = generate_stream\n\n        if not no_register:\n            self.register_to_controller()\n            self.heart_beat_thread = threading.Thread(\n                target=heart_beat_worker, args=(self,)\n            )\n            self.heart_beat_thread.start()\n\n    def register_to_controller(self):\n        logger.info(\"Register to controller\")\n\n        url = self.controller_addr + \"/register_worker\"\n        data = {\n            \"worker_name\": self.worker_addr,\n            \"check_heart_beat\": True,\n            \"worker_status\": self.get_status(),\n        }\n        r = requests.post(url, json=data)\n        assert r.status_code == 200\n\n    def send_heart_beat(self):\n        logger.info(\n            f\"Send heart beat. Models: {[self.model_name]}. \"\n            f\"Semaphore: {pretty_print_semaphore(model_semaphore)}. \"\n            f\"global_counter: {global_counter}\"\n        )\n\n        url = self.controller_addr + \"/receive_heart_beat\"\n\n        while True:\n            try:\n                ret = requests.post(\n                    url,\n                    json={\n                        \"worker_name\": self.worker_addr,\n                        \"queue_length\": self.get_queue_length(),\n                    },\n                    timeout=5,\n                )\n                exist = ret.json()[\"exist\"]\n                break\n            except requests.exceptions.RequestException as e:\n                logger.error(f\"heart beat error: {e}\")\n            time.sleep(5)\n\n        if not exist:\n            self.register_to_controller()\n\n    def get_queue_length(self):\n        if (\n            model_semaphore is None\n            or model_semaphore._value is None\n            or model_semaphore._waiters is None\n        ):\n            return 0\n        else:\n            return (\n                args.limit_model_concurrency\n                - model_semaphore._value\n                + len(model_semaphore._waiters)\n            )\n\n    def get_status(self):\n        return {\n            \"model_names\": [self.model_name],\n            \"speed\": 1,\n            \"queue_length\": self.get_queue_length(),\n        }\n\n    def count_token(self, params):\n        prompt = params[\"prompt\"]\n        input_ids = self.tokenizer(prompt).input_ids\n        input_echo_len = len(input_ids)\n\n        ret = {\n            \"count\": input_echo_len,\n            \"error_code\": 0,\n        }\n        return ret\n\n    def generate_stream_gate(self, params):\n        try:\n            for output in self.generate_stream_func(\n                self.model,\n                self.tokenizer,\n                params,\n                self.device,\n                self.context_len,\n                args.stream_interval,\n            ):\n                ret = {\n                    \"text\": output[\"text\"],\n                    \"error_code\": 0,\n                }\n                if \"usage\" in output:\n                    ret[\"usage\"] = output[\"usage\"]\n                if \"finish_reason\" in output:\n                    ret[\"finish_reason\"] = output[\"finish_reason\"]\n                if \"logprobs\" in output:\n                    ret[\"logprobs\"] = output[\"logprobs\"]\n                yield json.dumps(ret).encode() + b\"\\0\"\n        except torch.cuda.OutOfMemoryError as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.CUDA_OUT_OF_MEMORY,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n        except (ValueError, RuntimeError) as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.INTERNAL_ERROR,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n\n    def generate_gate(self, params):\n        try:\n            ret = {\"text\": \"\", \"error_code\": 0}\n            for output in self.generate_stream_func(\n                self.model,\n                self.tokenizer,\n                params,\n                self.device,\n                self.context_len,\n                args.stream_interval,\n            ):\n                ret[\"text\"] = output[\"text\"]\n            if \"usage\" in output:\n                ret[\"usage\"] = output[\"usage\"]\n            if \"finish_reason\" in output:\n                ret[\"finish_reason\"] = output[\"finish_reason\"]\n            if \"logprobs\" in output:\n                ret[\"logprobs\"] = output[\"logprobs\"]\n        except torch.cuda.OutOfMemoryError as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.CUDA_OUT_OF_MEMORY,\n            }\n        except (ValueError, RuntimeError) as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.INTERNAL_ERROR,\n            }\n        return ret\n\n    @torch.inference_mode()\n    def get_embeddings(self, params):\n        try:\n            tokenizer = self.tokenizer\n            is_llama = \"llama\" in str(type(self.model)) # vicuna support batch inference\n            is_chatglm = \"chatglm\" in str(type(self.model))\n            is_t5 = \"t5\" in str(type(self.model))\n            if is_llama:\n                encoding = tokenizer.batch_encode_plus(\n                    params[\"input\"], padding=True, return_tensors=\"pt\"\n                )\n                input_ids = encoding[\"input_ids\"].to(self.device)\n                attention_mask = encoding[\"attention_mask\"].to(self.device)\n                model_output = self.model(\n                    input_ids, attention_mask, output_hidden_states=True\n                )\n                data = model_output.hidden_states[-1]\n                mask = attention_mask.unsqueeze(-1).expand(data.size()).float()\n                masked_embeddings = data * mask\n                sum_embeddings = torch.sum(masked_embeddings, dim=1)\n                seq_length = torch.sum(mask, dim=1)\n                embedding = sum_embeddings / seq_length\n                normalized_embeddings = F.normalize(embedding, p=2, dim=1)\n                ret = {\n                    \"embedding\": normalized_embeddings.tolist(),\n                    \"token_num\": torch.sum(attention_mask).item(),\n                }\n            else:\n                embedding = []\n                token_num = 0\n                for text in params[\"input\"]:\n                    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(\n                        self.device\n                    )\n                    if is_t5:\n                        model_output = self.model(input_ids, decoder_input_ids=input_ids)\n                    else:\n                        model_output = self.model(input_ids, output_hidden_states=True)\n                    if is_chatglm:\n                        data = (model_output.hidden_states[-1].transpose(0, 1))[0]\n                    elif is_t5:\n                        data = model_output.encoder_last_hidden_state[0]\n                    else:\n                        data = model_output.hidden_states[-1][0]\n                    data = F.normalize(torch.mean(data, dim=0), p=2, dim=0)\n                    embedding.append(data.tolist())\n                    token_num += len(input_ids[0])\n                ret = {\n                    \"embedding\": embedding,\n                    \"token_num\": token_num,\n                }\n        except torch.cuda.OutOfMemoryError as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.CUDA_OUT_OF_MEMORY,\n            }\n        except (ValueError, RuntimeError) as e:\n            ret = {\n                \"text\": f\"{SERVER_ERROR_MSG}\\n\\n({e})\",\n                \"error_code\": ErrorCode.INTERNAL_ERROR,\n            }\n        return ret\n\n\napp = FastAPI()\n\n\ndef release_model_semaphore():\n    model_semaphore.release()\n\n\ndef acquire_model_semaphore():\n    global model_semaphore, global_counter\n    global_counter += 1\n    if model_semaphore is None:\n        model_semaphore = asyncio.Semaphore(args.limit_model_concurrency)\n    return model_semaphore.acquire()\n\n\ndef create_background_tasks():\n    background_tasks = BackgroundTasks()\n    background_tasks.add_task(release_model_semaphore)\n    return background_tasks\n\n\n@app.post(\"/worker_generate_stream\")\nasync def api_generate_stream(request: Request):\n    params = await request.json()\n    await acquire_model_semaphore()\n    generator = worker.generate_stream_gate(params)\n    background_tasks = create_background_tasks()\n    return StreamingResponse(generator, background=background_tasks)\n\n\n@app.post(\"/worker_generate\")\nasync def api_generate(request: Request):\n    params = await request.json()\n    await acquire_model_semaphore()\n    output = worker.generate_gate(params)\n    release_model_semaphore()\n    return JSONResponse(output)\n\n\n@app.post(\"/worker_generate_completion_stream\")\nasync def api_generate_completion_stream(request: Request):\n    params = await request.json()\n    await acquire_model_semaphore()\n    generator = worker.generate_stream_gate(params)\n    background_tasks = create_background_tasks()\n    return StreamingResponse(generator, background=background_tasks)\n\n\n@app.post(\"/worker_generate_completion\")\nasync def api_generate_completion(request: Request):\n    params = await request.json()\n    await acquire_model_semaphore()\n    completion = worker.generate_gate(params)\n    background_tasks = create_background_tasks()\n    return JSONResponse(content=completion, background=background_tasks)\n\n\n@app.post(\"/worker_get_embeddings\")\nasync def api_get_embeddings(request: Request):\n    params = await request.json()\n    await acquire_model_semaphore()\n    embedding = worker.get_embeddings(params)\n    background_tasks = create_background_tasks()\n    return JSONResponse(content=embedding, background=background_tasks)\n\n\n@app.post(\"/worker_get_status\")\nasync def api_get_status(request: Request):\n    return worker.get_status()\n\n\n@app.post(\"/count_token\")\nasync def count_token(request: Request):\n    params = await request.json()\n    return worker.count_token(params)\n\n\n@app.post(\"/model_details\")\nasync def model_details(request: Request):\n    return {\"context_length\": worker.context_len}\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21002)\n    parser.add_argument(\"--worker-address\", type=str, default=\"http://localhost:21002\")\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    add_model_args(parser)\n    parser.add_argument(\"--model-name\", type=str, help=\"Optional display name\")\n    parser.add_argument(\"--limit-model-concurrency\", type=int, default=5)\n    parser.add_argument(\"--stream-interval\", type=int, default=2)\n    parser.add_argument(\"--no-register\", action=\"store_true\")\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    if args.gpus:\n        if len(args.gpus.split(\",\")) < args.num_gpus:\n            raise ValueError(\n                f\"Larger --num-gpus ({args.num_gpus}) than --gpus {args.gpus}!\"\n            )\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n\n    worker = ModelWorker(\n        args.controller_address,\n        args.worker_address,\n        worker_id,\n        args.no_register,\n        args.model_path,\n        args.model_name,\n        args.device,\n        args.num_gpus,\n        args.max_gpu_memory,\n        args.load_8bit,\n        args.cpu_offloading,\n    )\n    uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n"}
{"type": "source_file", "path": "graphgpt/serve/cacheflow_worker.py", "content": "\"\"\"\nA model worker executes the model based on Cacheflow.\n\nInstall Cacheflow first. Then, assuming controller is live:\n1. ray start --head\n2. python3 -m fastchat.serve.cacheflow_worker --model-path path_to_vicuna\n\nlaunch Gradio:\n3. python3 -m fastchat.serve.gradio_web_server --concurrency-count 10000\n\"\"\"\nimport argparse\nimport asyncio\nimport json\nimport threading\nimport time\nimport uuid\nfrom typing import List, Dict\n\nimport requests\nimport torch\nimport uvicorn\nfrom fastapi import FastAPI, Request, BackgroundTasks\nfrom fastapi.responses import StreamingResponse\nfrom transformers import AutoTokenizer\n\nfrom cacheflow.master.server import Server, initialize_ray_cluster\nfrom cacheflow.sampling_params import SamplingParams\nfrom cacheflow.sequence import Sequence, SequenceGroup\nfrom cacheflow.utils import Counter, get_gpu_memory, get_cpu_memory\nfrom fastchat.constants import WORKER_HEART_BEAT_INTERVAL\nfrom fastchat.utils import build_logger, pretty_print_semaphore\n\nGB = 1 << 30\nTIMEOUT_TO_PREVENT_DEADLOCK = 1  # seconds\n\nworker_id = str(uuid.uuid4())[:6]\nlogger = build_logger(\"model_worker\", f\"model_worker_{worker_id}.log\")\nglobal_counter = 0\nseed = torch.cuda.current_device()\n\n\ndef heart_beat_worker(controller):\n    while True:\n        time.sleep(WORKER_HEART_BEAT_INTERVAL)\n        controller.send_heart_beat()\n\n\nclass CacheFlowWorker:\n    def __init__(\n        self,\n        controller_addr,\n        worker_addr,\n        worker_id,\n        no_register,\n        model_path,\n        model_name,\n        block_size,\n        seed,\n        swap_space,\n        max_num_batched_tokens,\n        distributed_init_method,\n        all_stage_devices,\n    ):\n        self.controller_addr = controller_addr\n        self.worker_addr = worker_addr\n        self.worker_id = worker_id\n        if model_path.endswith(\"/\"):\n            model_path = model_path[:-1]\n        self.model_name = model_name or model_path.split(\"/\")[-1]\n\n        logger.info(f\"Loading the model {self.model_name} on worker {worker_id} ...\")\n        self.block_size = block_size\n\n        # FIXME(Hao): we need to pass the tokenizer into cacheflow because we need\n        # to detect the stopping criteria \"###\".\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n        self.seq_group_counter = Counter()\n        self.seq_counter = Counter()\n        # FIXME(Hao): hard code context len\n        self.context_len = 2048\n        # pipeline_parallel_size = 1,\n        # tensor_parallel_size = 1,\n        # dtype = torch.float16\n        remote_server_class = Server\n        self.server = remote_server_class(\n            model=self.model_name,\n            model_path=model_path,\n            pipeline_parallel_size=1,\n            tensor_parallel_size=1,\n            block_size=block_size,\n            dtype=torch.float16,\n            seed=seed,\n            swap_space=swap_space,\n            max_num_batched_tokens=max_num_batched_tokens,\n            num_nodes=1,\n            num_devices_per_node=4,\n            distributed_init_method=distributed_init_method,\n            all_stage_devices=all_stage_devices,\n            gpu_memory=get_gpu_memory(),\n            cpu_memory=get_cpu_memory(),\n        )\n        self.running_seq_groups: Dict[int, SequenceGroup] = {}\n        self.sequence_group_events: Dict[int, asyncio.Event] = {}\n        self.is_server_running = False\n\n        if not no_register:\n            time.sleep(30)  # wait for model loading\n            self.register_to_controller()\n            self.heart_beat_thread = threading.Thread(\n                target=heart_beat_worker, args=(self,)\n            )\n            self.heart_beat_thread.start()\n\n    def register_to_controller(self):\n        logger.info(\"Register to controller\")\n\n        url = self.controller_addr + \"/register_worker\"\n        data = {\n            \"worker_name\": self.worker_addr,\n            \"check_heart_beat\": True,\n            \"worker_status\": self.get_status(),\n        }\n        r = requests.post(url, json=data)\n        assert r.status_code == 200\n\n    def send_heart_beat(self):\n        logger.info(\n            f\"Send heart beat. Models: {[self.model_name]}. \"\n            f\"Semaphore: {pretty_print_semaphore(model_semaphore)}. \"\n            f\"global_counter: {global_counter}\"\n        )\n\n        url = self.controller_addr + \"/receive_heart_beat\"\n\n        while True:\n            try:\n                ret = requests.post(\n                    url,\n                    json={\n                        \"worker_name\": self.worker_addr,\n                        \"queue_length\": self.get_queue_length(),\n                    },\n                    timeout=5,\n                )\n                exist = ret.json()[\"exist\"]\n                break\n            except requests.exceptions.RequestException as e:\n                logger.error(f\"heart beat error: {e}\")\n            time.sleep(5)\n\n        if not exist:\n            self.register_to_controller()\n\n    def get_queue_length(self):\n        if (\n            model_semaphore is None\n            or model_semaphore._value is None\n            or model_semaphore._waiters is None\n        ):\n            return 0\n        else:\n            return (\n                args.limit_model_concurrency\n                - model_semaphore._value\n                + len(model_semaphore._waiters)\n            )\n\n    def get_status(self):\n        return {\n            \"model_names\": [self.model_name],\n            \"speed\": 1,\n            \"queue_length\": self.get_queue_length(),\n        }\n\n    async def server_step(self):\n        self.is_server_running = True\n        updated_seq_groups = self.server.step()\n        self.is_server_running = False\n        # Notify the waiting coroutines that there new outputs ready.\n        for seq_group in updated_seq_groups:\n            group_id = seq_group.group_id\n            self.running_seq_groups[group_id] = seq_group\n            self.sequence_group_events[group_id].set()\n\n    async def generate_stream(self, params):\n        tokenizer = self.tokenizer\n        context = params[\"prompt\"]\n        temperature = float(params.get(\"temperature\", 1.0))\n        top_p = float(params.get(\"top_p\", 1.0))\n        max_new_tokens = min(int(params.get(\"max_new_tokens\", 256)), 1024)\n        stop_str = params.get(\"stop\", None)\n        echo = params.get(\"echo\", True)\n        stop_token_ids = params.get(\"stop_token_ids\", None) or []\n        stop_token_ids.append(tokenizer.eos_token_id)\n\n        input_ids = tokenizer(context).input_ids\n        max_src_len = self.context_len - max_new_tokens - 8\n        input_ids = input_ids[-max_src_len:]\n\n        # make sampling params in cacheflow\n        top_p = max(top_p, 1e-5)\n        if temperature <= 1e-5:\n            top_p = 1.0\n        sampling_params = SamplingParams(\n            n=1,\n            temperature=temperature,\n            top_p=top_p,\n            use_beam_search=False,\n            stop_token_ids=stop_token_ids,\n            max_num_steps=max_new_tokens,\n            num_logprobs=0,\n            context_window_size=None,\n        )\n\n        if stop_str is not None:\n            sampling_params.stop_str = stop_str\n        # we might sample multiple sequences, but in chatbot, this is one\n        seqs: List[Sequence] = []\n        for _ in range(sampling_params.n):\n            seq_id = next(self.seq_counter)\n            seq = Sequence(seq_id, input_ids, block_size=self.block_size)\n            seqs.append(seq)\n\n        arrival_time = time.time()\n        group_id = next(self.seq_group_counter)\n        # logger.info(f\"Group {group_id} arrives at {time.time()}\")\n        seq_group = SequenceGroup(group_id, seqs, arrival_time)\n        group_event = asyncio.Event()\n        self.running_seq_groups[group_id] = seq_group\n        self.sequence_group_events[group_id] = group_event\n        self.server.add_sequence_groups([(seq_group, sampling_params)])\n        while True:\n            if not self.is_server_running:\n                await self.server_step()\n            try:\n                await asyncio.wait_for(\n                    group_event.wait(), timeout=TIMEOUT_TO_PREVENT_DEADLOCK\n                )\n            except:\n                pass\n            group_event.clear()\n            seq_group = self.running_seq_groups[group_id]\n            all_outputs = []\n            for seq in seq_group.seqs:\n                token_ids = seq.get_token_ids()\n                if not echo:\n                    token_ids = token_ids[len(input_ids) :]\n                output = self.tokenizer.decode(token_ids, skip_special_tokens=True)\n                if stop_str is not None:\n                    if output.endswith(stop_str):\n                        output = output[: -len(stop_str)]\n                all_outputs.append(output)\n            assert len(seq_group.seqs) == 1\n            ret = {\n                \"text\": all_outputs[0],\n                \"error_code\": 0,\n            }\n            yield (json.dumps(ret) + \"\\0\").encode(\"utf-8\")\n            if seq_group.is_finished():\n                del self.running_seq_groups[group_id]\n                del self.sequence_group_events[group_id]\n                break\n\n\napp = FastAPI()\nmodel_semaphore = None\n\n\ndef release_model_semaphore():\n    model_semaphore.release()\n\n\n@app.post(\"/worker_generate_stream\")\nasync def generate_stream(request: Request):\n    global model_semaphore, global_counter\n    global_counter += 1\n    params = await request.json()\n\n    if model_semaphore is None:\n        model_semaphore = asyncio.Semaphore(args.limit_model_concurrency)\n    await model_semaphore.acquire()\n    background_tasks = BackgroundTasks()\n    background_tasks.add_task(release_model_semaphore)\n    # return StreamingResponse(generator, background=background_tasks)\n    return StreamingResponse(\n        worker.generate_stream(params), background=background_tasks\n    )\n\n\n@app.post(\"/worker_get_status\")\nasync def get_status(request: Request):\n    return worker.get_status()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21002)\n    parser.add_argument(\"--worker-address\", type=str, default=\"http://localhost:21002\")\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    parser.add_argument(\n        \"--model-path\", type=str, default=\"/home/haozhang/weights/hf-llama-7b\"\n    )\n    parser.add_argument(\"--model-name\", type=str)\n    parser.add_argument(\"--limit-model-concurrency\", type=int, default=1024)\n    parser.add_argument(\"--stream-interval\", type=int, default=2)\n    parser.add_argument(\"--no-register\", action=\"store_true\")\n    # cacheflow specific params\n    parser.add_argument(\n        \"--block-size\", type=int, default=8, choices=[8, 16], help=\"token block size\"\n    )\n    parser.add_argument(\n        \"--swap-space\", type=int, default=20, help=\"CPU swap space size (GiB) per GPU\"\n    )\n    parser.add_argument(\n        \"--max-num-batched-tokens\",\n        type=int,\n        default=2560,\n        help=\"maximum number of batched tokens\",\n    )\n    args = parser.parse_args()\n\n    (\n        num_nodes,\n        num_devices_per_node,\n        distributed_init_method,\n        all_stage_devices,\n    ) = initialize_ray_cluster(pipeline_parallel_size=1, tensor_parallel_size=1)\n\n    worker = CacheFlowWorker(\n        args.controller_address,\n        args.worker_address,\n        worker_id,\n        args.no_register,\n        args.model_path,\n        args.model_name,\n        args.block_size,\n        seed,\n        args.swap_space,\n        args.max_num_batched_tokens,\n        distributed_init_method,\n        all_stage_devices,\n    )\n    uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n"}
{"type": "source_file", "path": "graphgpt/serve/controller.py", "content": "\"\"\"\nA controller manages distributed workers.\nIt sends worker addresses to clients.\n\"\"\"\nimport argparse\nimport asyncio\nimport dataclasses\nfrom enum import Enum, auto\nimport json\nimport logging\nimport time\nfrom typing import List, Union\nimport threading\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse\nimport numpy as np\nimport requests\nimport uvicorn\n\nfrom fastchat.constants import (CONTROLLER_HEART_BEAT_EXPIRATION, ErrorCode,\n    SERVER_ERROR_MSG)\nfrom fastchat.utils import build_logger\n\n\nlogger = build_logger(\"controller\", \"controller.log\")\n\n\nclass DispatchMethod(Enum):\n    LOTTERY = auto()\n    SHORTEST_QUEUE = auto()\n\n    @classmethod\n    def from_str(cls, name):\n        if name == \"lottery\":\n            return cls.LOTTERY\n        elif name == \"shortest_queue\":\n            return cls.SHORTEST_QUEUE\n        else:\n            raise ValueError(f\"Invalid dispatch method\")\n\n\n@dataclasses.dataclass\nclass WorkerInfo:\n    model_names: List[str]\n    speed: int\n    queue_length: int\n    check_heart_beat: bool\n    last_heart_beat: str\n\n\ndef heart_beat_controller(controller):\n    while True:\n        time.sleep(CONTROLLER_HEART_BEAT_EXPIRATION)\n        controller.remove_stable_workers_by_expiration()\n\n\nclass Controller:\n    def __init__(self, dispatch_method: str):\n        # Dict[str -> WorkerInfo]\n        self.worker_info = {}\n        self.dispatch_method = DispatchMethod.from_str(dispatch_method)\n\n        self.heart_beat_thread = threading.Thread(\n            target=heart_beat_controller, args=(self,)\n        )\n        self.heart_beat_thread.start()\n\n        logger.info(\"Init controller\")\n\n    def register_worker(\n        self, worker_name: str, check_heart_beat: bool, worker_status: dict\n    ):\n        if worker_name not in self.worker_info:\n            logger.info(f\"Register a new worker: {worker_name}\")\n        else:\n            logger.info(f\"Register an existing worker: {worker_name}\")\n\n        if not worker_status:\n            worker_status = self.get_worker_status(worker_name)\n        if not worker_status:\n            return False\n\n        self.worker_info[worker_name] = WorkerInfo(\n            worker_status[\"model_names\"],\n            worker_status[\"speed\"],\n            worker_status[\"queue_length\"],\n            check_heart_beat,\n            time.time(),\n        )\n\n        logger.info(f\"Register done: {worker_name}, {worker_status}\")\n        return True\n\n    def get_worker_status(self, worker_name: str):\n        try:\n            r = requests.post(worker_name + \"/worker_get_status\", timeout=5)\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"Get status fails: {worker_name}, {e}\")\n            return None\n\n        if r.status_code != 200:\n            logger.error(f\"Get status fails: {worker_name}, {r}\")\n            return None\n\n        return r.json()\n\n    def remove_worker(self, worker_name: str):\n        del self.worker_info[worker_name]\n\n    def refresh_all_workers(self):\n        old_info = dict(self.worker_info)\n        self.worker_info = {}\n\n        for w_name, w_info in old_info.items():\n            if not self.register_worker(w_name, w_info.check_heart_beat, None):\n                logger.info(f\"Remove stale worker: {w_name}\")\n\n    def list_models(self):\n        model_names = set()\n\n        for w_name, w_info in self.worker_info.items():\n            model_names.update(w_info.model_names)\n\n        return list(model_names)\n\n    def get_worker_address(self, model_name: str):\n        if self.dispatch_method == DispatchMethod.LOTTERY:\n            worker_names = []\n            worker_speeds = []\n            for w_name, w_info in self.worker_info.items():\n                if model_name in w_info.model_names:\n                    worker_names.append(w_name)\n                    worker_speeds.append(w_info.speed)\n            worker_speeds = np.array(worker_speeds, dtype=np.float32)\n            norm = np.sum(worker_speeds)\n            if norm < 1e-4:\n                return \"\"\n            worker_speeds = worker_speeds / norm\n            if True:  # Directly return address\n                pt = np.random.choice(np.arange(len(worker_names)), p=worker_speeds)\n                worker_name = worker_names[pt]\n                return worker_name\n\n            # Check status before returning\n            while True:\n                pt = np.random.choice(np.arange(len(worker_names)), p=worker_speeds)\n                worker_name = worker_names[pt]\n\n                if self.get_worker_status(worker_name):\n                    break\n                else:\n                    self.remove_worker(worker_name)\n                    worker_speeds[pt] = 0\n                    norm = np.sum(worker_speeds)\n                    if norm < 1e-4:\n                        return \"\"\n                    worker_speeds = worker_speeds / norm\n                    continue\n            return worker_name\n        elif self.dispatch_method == DispatchMethod.SHORTEST_QUEUE:\n            worker_names = []\n            worker_qlen = []\n            for w_name, w_info in self.worker_info.items():\n                if model_name in w_info.model_names:\n                    worker_names.append(w_name)\n                    worker_qlen.append(w_info.queue_length / w_info.speed)\n            if len(worker_names) == 0:\n                return \"\"\n            min_index = np.argmin(worker_qlen)\n            w_name = worker_names[min_index]\n            self.worker_info[w_name].queue_length += 1\n            logger.info(\n                f\"names: {worker_names}, queue_lens: {worker_qlen}, ret: {w_name}\"\n            )\n            return w_name\n        else:\n            raise ValueError(f\"Invalid dispatch method: {self.dispatch_method}\")\n\n    def receive_heart_beat(self, worker_name: str, queue_length: int):\n        if worker_name not in self.worker_info:\n            logger.info(f\"Receive unknown heart beat. {worker_name}\")\n            return False\n\n        self.worker_info[worker_name].queue_length = queue_length\n        self.worker_info[worker_name].last_heart_beat = time.time()\n        logger.info(f\"Receive heart beat. {worker_name}\")\n        return True\n\n    def remove_stable_workers_by_expiration(self):\n        expire = time.time() - CONTROLLER_HEART_BEAT_EXPIRATION\n        to_delete = []\n        for worker_name, w_info in self.worker_info.items():\n            if w_info.check_heart_beat and w_info.last_heart_beat < expire:\n                to_delete.append(worker_name)\n\n        for worker_name in to_delete:\n            self.remove_worker(worker_name)\n\n    def handle_no_worker(params):\n        logger.info(f\"no worker: {params['model']}\")\n        ret = {\n            \"text\": SERVER_ERROR_MSG,\n            \"error_code\": ErrorCode.CONTROLLER_NO_WORKER,\n        }\n        return json.dumps(ret).encode() + b\"\\0\"\n\n    def handle_worker_timeout(worker_address):\n        logger.info(f\"worker timeout: {worker_address}\")\n        ret = {\n            \"text\": SERVER_ERROR_MSG,\n            \"error_code\": ErrorCode.CONTROLLER_WORKER_TIMEOUT,\n        }\n        return json.dumps(ret).encode() + b\"\\0\"\n\n    def worker_api_generate_stream(self, params):\n        worker_addr = self.get_worker_address(params[\"model\"])\n        if not worker_addr:\n            yield self.handle_no_worker(params)\n\n        try:\n            response = requests.post(\n                worker_addr + \"/worker_generate_stream\",\n                json=params,\n                stream=True,\n                timeout=15,\n            )\n            for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n                if chunk:\n                    yield chunk + b\"\\0\"\n        except requests.exceptions.RequestException as e:\n            yield self.handle_worker_timeout(worker_addr)\n\n    def worker_api_generate_completion(self, params):\n        worker_addr = self.get_worker_address(params[\"model\"])\n        if not worker_addr:\n            return self.handle_no_worker(params)\n\n        try:\n            response = requests.post(\n                worker_addr + \"/worker_generate_completion\",\n                json=params,\n                timeout=15,\n            )\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            return self.handle_worker_timeout(worker_addr)\n\n    def worker_api_embeddings(self, params):\n        worker_addr = self.get_worker_address(params[\"model\"])\n        if not worker_addr:\n            return self.handle_no_worker(params)\n\n        try:\n            response = requests.post(\n                worker_addr + \"/worker_get_embeddings\",\n                json=params,\n                timeout=15,\n            )\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            return self.handle_worker_timeout(worker_addr)\n\n    # Let the controller act as a worker to achieve hierarchical\n    # management. This can be used to connect isolated sub networks.\n    def worker_api_get_status(self):\n        model_names = set()\n        speed = 0\n        queue_length = 0\n\n        for w_name in self.worker_info:\n            worker_status = self.get_worker_status(w_name)\n            if worker_status is not None:\n                model_names.update(worker_status[\"model_names\"])\n                speed += worker_status[\"speed\"]\n                queue_length += worker_status[\"queue_length\"]\n\n        return {\n            \"model_names\": list(model_names),\n            \"speed\": speed,\n            \"queue_length\": queue_length,\n        }\n\n\napp = FastAPI()\n\n\n@app.post(\"/register_worker\")\nasync def register_worker(request: Request):\n    data = await request.json()\n    controller.register_worker(\n        data[\"worker_name\"], data[\"check_heart_beat\"], data.get(\"worker_status\", None)\n    )\n\n\n@app.post(\"/refresh_all_workers\")\nasync def refresh_all_workers():\n    models = controller.refresh_all_workers()\n\n\n@app.post(\"/list_models\")\nasync def list_models():\n    models = controller.list_models()\n    return {\"models\": models}\n\n\n@app.post(\"/get_worker_address\")\nasync def get_worker_address(request: Request):\n    data = await request.json()\n    addr = controller.get_worker_address(data[\"model\"])\n    return {\"address\": addr}\n\n\n@app.post(\"/receive_heart_beat\")\nasync def receive_heart_beat(request: Request):\n    data = await request.json()\n    exist = controller.receive_heart_beat(data[\"worker_name\"], data[\"queue_length\"])\n    return {\"exist\": exist}\n\n\n@app.post(\"/worker_generate_stream\")\nasync def worker_api_generate_stream(request: Request):\n    params = await request.json()\n    generator = controller.worker_api_generate_stream(params)\n    return StreamingResponse(generator)\n\n\n@app.post(\"/worker_generate_completion\")\nasync def worker_api_generate_completion(request: Request):\n    params = await request.json()\n    output = controller.worker_api_generate_completion(params)\n    return output\n\n\n@app.post(\"/worker_get_embeddings\")\nasync def worker_api_embeddings(request: Request):\n    params = await request.json()\n    output = controller.worker_api_embeddings(params)\n    return output\n\n\n@app.post(\"/worker_get_status\")\nasync def worker_api_get_status(request: Request):\n    return controller.worker_api_get_status()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21001)\n    parser.add_argument(\n        \"--dispatch-method\",\n        type=str,\n        choices=[\"lottery\", \"shortest_queue\"],\n        default=\"shortest_queue\",\n    )\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    controller = Controller(args.dispatch_method)\n    uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n"}
{"type": "source_file", "path": "graphgpt/model/model_registry.py", "content": "\"\"\"Additional information of the models.\"\"\"\nfrom collections import namedtuple\nfrom typing import List\n\n\nModelInfo = namedtuple(\"ModelInfo\", [\"simple_name\", \"link\", \"description\"])\n\n\nmodel_info = {}\n\n\ndef register_model_info(\n    full_names: List[str], simple_name: str, link: str, description: str\n):\n    info = ModelInfo(simple_name, link, description)\n\n    for full_name in full_names:\n        model_info[full_name] = info\n\n\ndef get_model_info(name: str) -> ModelInfo:\n    return model_info[name]\n\n\nregister_model_info(\n    [\"gpt-4\"], \"ChatGPT-4\", \"https://openai.com/research/gpt-4\", \"ChatGPT-4 by OpenAI\"\n)\nregister_model_info(\n    [\"gpt-3.5-turbo\"],\n    \"ChatGPT-3.5\",\n    \"https://openai.com/blog/chatgpt\",\n    \"ChatGPT-3.5 by OpenAI\",\n)\nregister_model_info(\n    [\"claude-v1\"],\n    \"Claude\",\n    \"https://www.anthropic.com/index/introducing-claude\",\n    \"Claude by Anthropic\",\n)\nregister_model_info(\n    [\"claude-instant-v1\"],\n    \"Claude Instant\",\n    \"https://www.anthropic.com/index/introducing-claude\",\n    \"Claude Instant by Anthropic\",\n)\nregister_model_info(\n    [\"palm-2\"],\n    \"PaLM 2 Chat\",\n    \"https://cloud.google.com/vertex-ai/docs/release-notes#May_10_2023\",\n    \"PaLM 2 for Chat (chat-bison@001) by Google\",\n)\nregister_model_info(\n    [\"vicuna-13b\", \"vicuna-7b\"],\n    \"Vicuna\",\n    \"https://lmsys.org/blog/2023-03-30-vicuna/\",\n    \"a chat assistant fine-tuned from LLaMA on user-shared conversations by LMSYS\",\n)\nregister_model_info(\n    [\"koala-13b\"],\n    \"Koala\",\n    \"https://bair.berkeley.edu/blog/2023/04/03/koala\",\n    \"a dialogue model for academic research by BAIR\",\n)\nregister_model_info(\n    [\"oasst-pythia-12b\"],\n    \"OpenAssistant (oasst)\",\n    \"https://open-assistant.io\",\n    \"an Open Assistant for everyone by LAION\",\n)\nregister_model_info(\n    [\"RWKV-4-Raven-14B\"],\n    \"RWKV-4-Raven\",\n    \"https://huggingface.co/BlinkDL/rwkv-4-raven\",\n    \"an RNN with transformer-level LLM performance\",\n)\nregister_model_info(\n    [\"alpaca-13b\"],\n    \"Alpaca\",\n    \"https://crfm.stanford.edu/2023/03/13/alpaca.html\",\n    \"a model fine-tuned from LLaMA on instruction-following demonstrations by Stanford\",\n)\nregister_model_info(\n    [\"chatglm-6b\"],\n    \"ChatGLM\",\n    \"https://chatglm.cn/blog\",\n    \"an open bilingual dialogue language model by Tsinghua University\",\n)\nregister_model_info(\n    [\"llama-13b\"],\n    \"LLaMA\",\n    \"https://arxiv.org/abs/2302.13971\",\n    \"open and efficient foundation language models by Meta\",\n)\nregister_model_info(\n    [\"dolly-v2-12b\"],\n    \"Dolly\",\n    \"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\",\n    \"an instruction-tuned open large language model by Databricks\",\n)\nregister_model_info(\n    [\"stablelm-tuned-alpha-7b\"],\n    \"StableLM\",\n    \"https://github.com/stability-AI/stableLM\",\n    \"Stability AI language models\",\n)\nregister_model_info(\n    [\"fastchat-t5-3b\"],\n    \"FastChat-T5\",\n    \"https://huggingface.co/lmsys/fastchat-t5-3b-v1.0\",\n    \"a chat assistant fine-tuned from FLAN-T5 by LMSYS\",\n)\nregister_model_info(\n    [\"phoenix-inst-chat-7b\"],\n    \"Phoenix-7B\",\n    \"https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b\",\n    \"a multilingual chat assistant fine-tuned from Bloomz to democratize ChatGPT across languages by CUHK(SZ)\",\n)\nregister_model_info(\n    [\"mpt-7b-chat\"],\n    \"MPT-Chat\",\n    \"https://www.mosaicml.com/blog/mpt-7b\",\n    \"a chatbot fine-tuned from MPT-7B by MosaicML\",\n)\nregister_model_info(\n    [\"billa-7b-sft\"],\n    \"BiLLa-7B-SFT\",\n    \"https://huggingface.co/Neutralzz/BiLLa-7B-SFT\",\n    \"an instruction-tuned bilingual LLaMA with enhanced reasoning ability by an independent researcher\",\n)\nregister_model_info(\n    [\"h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2\"],\n    \"h2oGPT-GM-7b\",\n    \"https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2\",\n    \"an instruction-tuned OpenLLaMA with enhanced conversational ability by H2O.ai\",\n)\nregister_model_info(\n    [\"baize-v2-7b\", \"baize-v2-13b\"],\n    \"Baize v2\",\n    \"https://github.com/project-baize/baize-chatbot#v2\",\n    \"A chatbot fine-tuned from LLaMA with ChatGPT self-chat data and Self-Disillation with Feedback (SDF) by UCSD and SYSU.\",\n)\n"}
{"type": "source_file", "path": "graphgpt/serve/gradio_web_server_multi.py", "content": "\"\"\"\nThe gradio demo server with multiple tabs.\nIt supports chatting with a single model or chatting with two models side-by-side.\n\"\"\"\n\nimport argparse\nimport pickle\n\nimport gradio as gr\n\nfrom fastchat.serve.gradio_block_arena_anony import (\n    build_side_by_side_ui_anony,\n    load_demo_side_by_side_anony,\n    set_global_vars_anony,\n)\nfrom fastchat.serve.gradio_block_arena_named import (\n    build_side_by_side_ui_named,\n    load_demo_side_by_side_named,\n    set_global_vars_named,\n)\nfrom fastchat.serve.gradio_patch import Chatbot as grChatbot\nfrom fastchat.serve.gradio_web_server import (\n    set_global_vars,\n    block_css,\n    build_single_model_ui,\n    get_model_list,\n    load_demo_single,\n)\nfrom fastchat.serve.monitor.monitor import build_leaderboard_tab\nfrom fastchat.utils import build_logger, get_window_url_params_js\n\n\nlogger = build_logger(\"gradio_web_server_multi\", \"gradio_web_server_multi.log\")\n\n\ndef load_demo(url_params, request: gr.Request):\n    logger.info(f\"load_demo. ip: {request.client.host}. params: {url_params}\")\n    selected = 0\n    if \"arena\" in url_params:\n        selected = 1\n    elif \"compare\" in url_params:\n        selected = 2\n    elif \"leaderboard\" in url_params:\n        selected = 3\n    single_updates = load_demo_single(models, url_params)\n\n    models_anony = models\n    if args.anony_only_for_proprietary_model:\n        # Only enable these models in anony battles.\n        if args.add_chatgpt:\n            models_anony = [\"gpt-4\", \"gpt-3.5-turbo\"] + models_anony\n        if args.add_claude:\n            models_anony = [\"claude-v1\", \"claude-instant-v1\"] + models_anony\n        if args.add_bard:\n            models_anony = [\"bard\"] + models_anony\n\n    side_by_side_anony_updates = load_demo_side_by_side_anony(models_anony, url_params)\n    side_by_side_named_updates = load_demo_side_by_side_named(models, url_params)\n    return (\n        (gr.Tabs.update(selected=selected),)\n        + single_updates\n        + side_by_side_anony_updates\n        + side_by_side_named_updates\n    )\n\n\ndef build_demo(models, elo_results_file):\n    with gr.Blocks(\n        title=\"Chat with Open Large Language Models\",\n        theme=gr.themes.Base(),\n        css=block_css,\n    ) as demo:\n        with gr.Tabs() as tabs:\n            with gr.Tab(\"Single Model\", id=0):\n                (\n                    a_state,\n                    a_model_selector,\n                    a_chatbot,\n                    a_textbox,\n                    a_send_btn,\n                    a_button_row,\n                    a_parameter_row,\n                ) = build_single_model_ui(models)\n                a_list = [\n                    a_state,\n                    a_model_selector,\n                    a_chatbot,\n                    a_textbox,\n                    a_send_btn,\n                    a_button_row,\n                    a_parameter_row,\n                ]\n\n            with gr.Tab(\"Chatbot Arena (battle)\", id=1):\n                (\n                    b_states,\n                    b_model_selectors,\n                    b_chatbots,\n                    b_textbox,\n                    b_send_btn,\n                    b_button_row,\n                    b_button_row2,\n                    b_parameter_row,\n                ) = build_side_by_side_ui_anony(models)\n                b_list = (\n                    b_states\n                    + b_model_selectors\n                    + b_chatbots\n                    + [\n                        b_textbox,\n                        b_send_btn,\n                        b_button_row,\n                        b_button_row2,\n                        b_parameter_row,\n                    ]\n                )\n\n            with gr.Tab(\"Chatbot Arena (side-by-side)\", id=2):\n                (\n                    c_states,\n                    c_model_selectors,\n                    c_chatbots,\n                    c_textbox,\n                    c_send_btn,\n                    c_button_row,\n                    c_button_row2,\n                    c_parameter_row,\n                ) = build_side_by_side_ui_named(models)\n                c_list = (\n                    c_states\n                    + c_model_selectors\n                    + c_chatbots\n                    + [\n                        c_textbox,\n                        c_send_btn,\n                        c_button_row,\n                        c_button_row2,\n                        c_parameter_row,\n                    ]\n                )\n\n            if elo_results_file:\n                with gr.Tab(\"Leaderboard\", id=3):\n                    build_leaderboard_tab(elo_results_file)\n\n        url_params = gr.JSON(visible=False)\n\n        if args.model_list_mode == \"once\":\n            demo.load(\n                load_demo,\n                [url_params],\n                [tabs] + a_list + b_list + c_list,\n                _js=get_window_url_params_js,\n            )\n        else:\n            raise ValueError(f\"Unknown model list mode: {args.model_list_mode}\")\n\n    return demo\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int)\n    parser.add_argument(\"--controller-url\", type=str, default=\"http://localhost:21001\")\n    parser.add_argument(\"--concurrency-count\", type=int, default=10)\n    parser.add_argument(\n        \"--model-list-mode\",\n        type=str,\n        default=\"once\",\n        choices=[\"once\"],\n    )\n    parser.add_argument(\"--share\", action=\"store_true\")\n    parser.add_argument(\n        \"--moderate\", action=\"store_true\", help=\"Enable content moderation\"\n    )\n    parser.add_argument(\n        \"--add-chatgpt\",\n        action=\"store_true\",\n        help=\"Add OpenAI ChatGPT models (gpt-3.5-turbo, gpt-4)\",\n    )\n    parser.add_argument(\n        \"--add-claude\",\n        action=\"store_true\",\n        help=\"Add Anthropic's Claude models (claude-v1, claude-instant-v1)\",\n    )\n    parser.add_argument(\n        \"--add-bard\",\n        action=\"store_true\",\n        help=\"Add Google's Bard model (PaLM 2 for Chat: chat-bison@001)\",\n    )\n    parser.add_argument(\n        \"--anony-only-for-proprietary-model\",\n        action=\"store_true\",\n        help=\"Only add ChatGPT, Claude, Bard under anony battle tab\",\n    )\n    parser.add_argument(\"--elo-results-file\", type=str)\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    set_global_vars(args.controller_url, args.moderate)\n    set_global_vars_named(args.moderate)\n    set_global_vars_anony(args.moderate)\n    models = get_model_list(args.controller_url)\n\n    if not args.anony_only_for_proprietary_model:\n        if args.add_chatgpt:\n            models = [\"gpt-3.5-turbo\", \"gpt-4\"] + models\n        if args.add_claude:\n            models = [\"claude-v1\", \"claude-instant-v1\"] + models\n        if args.add_bard:\n            models = [\"bard\"] + models\n\n    demo = build_demo(models, args.elo_results_file)\n    demo.queue(\n        concurrency_count=args.concurrency_count, status_update_rate=10, api_open=False\n    ).launch(\n        server_name=args.host, server_port=args.port, share=args.share, max_threads=200\n    )\n"}
{"type": "source_file", "path": "graphgpt/model/model_adapter.py", "content": "\"\"\"Model adapter registration.\"\"\"\n\nimport math\nimport sys\nfrom typing import List, Optional\nimport warnings\n\nif sys.version_info >= (3, 9):\n    from functools import cache\nelse:\n    from functools import lru_cache as cache\n\nimport psutil\nimport torch\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    LlamaTokenizer,\n    LlamaForCausalLM,\n    T5Tokenizer,\n)\n\nfrom fastchat.conversation import Conversation, get_conv_template\nfrom fastchat.model.compression import load_compress_model\nfrom fastchat.model.monkey_patch_non_inplace import (\n    replace_llama_attn_with_non_inplace_operations,\n)\nfrom fastchat.utils import get_gpu_memory\n\n\nclass BaseAdapter:\n    \"\"\"The base and the default model adapter.\"\"\"\n\n    def match(self, model_path: str):\n        return True\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path, low_cpu_mem_usage=True, **from_pretrained_kwargs\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"one_shot\")\n\n\n# A global registry for all model adapters\nmodel_adapters: List[BaseAdapter] = []\n\n\ndef register_model_adapter(cls):\n    \"\"\"Register a model adapter.\"\"\"\n    model_adapters.append(cls())\n\n\n@cache\ndef get_model_adapter(model_path: str) -> BaseAdapter:\n    \"\"\"Get a model adapter for a model_path.\"\"\"\n    for adapter in model_adapters:\n        if adapter.match(model_path):\n            return adapter\n    raise ValueError(f\"No valid model adapter for {model_path}\")\n\n\ndef raise_warning_for_incompatible_cpu_offloading_configuration(\n    device: str, load_8bit: bool, cpu_offloading: bool\n):\n    if cpu_offloading:\n        if not load_8bit:\n            warnings.warn(\n                \"The cpu-offloading feature can only be used while also using 8-bit-quantization.\\n\"\n                \"Use '--load-8bit' to enable 8-bit-quantization\\n\"\n                \"Continuing without cpu-offloading enabled\\n\"\n            )\n            return False\n        if not \"linux\" in sys.platform:\n            warnings.warn(\n                \"CPU-offloading is only supported on linux-systems due to the limited compatability with the bitsandbytes-package\\n\"\n                \"Continuing without cpu-offloading enabled\\n\"\n            )\n            return False\n        if device != \"cuda\":\n            warnings.warn(\n                \"CPU-offloading is only enabled when using CUDA-devices\\n\"\n                \"Continuing without cpu-offloading enabled\\n\"\n            )\n            return False\n    return cpu_offloading\n\n\ndef load_model(\n    model_path: str,\n    device: str,\n    num_gpus: int,\n    max_gpu_memory: Optional[str] = None,\n    load_8bit: bool = False,\n    cpu_offloading: bool = False,\n    debug: bool = False,\n):\n    \"\"\"Load a model from Hugging Face.\"\"\"\n\n    # Handle device mapping\n    cpu_offloading = raise_warning_for_incompatible_cpu_offloading_configuration(\n        device, load_8bit, cpu_offloading\n    )\n    if device == \"cpu\":\n        kwargs = {\"torch_dtype\": torch.float32}\n    elif device == \"cuda\":\n        kwargs = {\"torch_dtype\": torch.float16}\n        if num_gpus != 1:\n            kwargs[\"device_map\"] = \"auto\"\n            if max_gpu_memory is None:\n                kwargs[\n                    \"device_map\"\n                ] = \"sequential\"  # This is important for not the same VRAM sizes\n                available_gpu_memory = get_gpu_memory(num_gpus)\n                kwargs[\"max_memory\"] = {\n                    i: str(int(available_gpu_memory[i] * 0.85)) + \"GiB\"\n                    for i in range(num_gpus)\n                }\n            else:\n                kwargs[\"max_memory\"] = {i: max_gpu_memory for i in range(num_gpus)}\n    elif device == \"mps\":\n        kwargs = {\"torch_dtype\": torch.float16}\n        # Avoid bugs in mps backend by not using in-place operations.\n        replace_llama_attn_with_non_inplace_operations()\n    else:\n        raise ValueError(f\"Invalid device: {device}\")\n\n    if cpu_offloading:\n        # raises an error on incompatible platforms\n        from transformers import BitsAndBytesConfig\n\n        if \"max_memory\" in kwargs:\n            kwargs[\"max_memory\"][\"cpu\"] = (\n                str(math.floor(psutil.virtual_memory().available / 2**20)) + \"Mib\"\n            )\n        kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n            load_in_8bit_fp32_cpu_offload=cpu_offloading\n        )\n        kwargs[\"load_in_8bit\"] = load_8bit\n    elif load_8bit:\n        if num_gpus != 1:\n            warnings.warn(\n                \"8-bit quantization is not supported for multi-gpu inference.\"\n            )\n        else:\n            return load_compress_model(\n                model_path=model_path, device=device, torch_dtype=kwargs[\"torch_dtype\"]\n            )\n\n    # Load model\n    adapter = get_model_adapter(model_path)\n    model, tokenizer = adapter.load_model(model_path, kwargs)\n\n    if (device == \"cuda\" and num_gpus == 1 and not cpu_offloading) or device == \"mps\":\n        model.to(device)\n\n    if debug:\n        print(model)\n\n    return model, tokenizer\n\n\ndef get_conversation_template(model_path: str) -> Conversation:\n    adapter = get_model_adapter(model_path)\n    return adapter.get_default_conv_template(model_path)\n\n\ndef add_model_args(parser):\n    parser.add_argument(\n        \"--model-path\",\n        type=str,\n        default=\"lmsys/fastchat-t5-3b-v1.0\",\n        help=\"The path to the weights. This can be a local folder or a Hugging Face repo ID.\",\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        choices=[\"cpu\", \"cuda\", \"mps\"],\n        default=\"cuda\",\n        help=\"The device type\",\n    )\n    parser.add_argument(\n        \"--gpus\",\n        type=str,\n        default=None,\n        help=\"A single GPU like 1 or multiple GPUs like 0,2\",\n    )\n    parser.add_argument(\"--num-gpus\", type=int, default=1)\n    parser.add_argument(\n        \"--max-gpu-memory\",\n        type=str,\n        help=\"The maximum memory per gpu. Use a string like '13Gib'\",\n    )\n    parser.add_argument(\n        \"--load-8bit\", action=\"store_true\", help=\"Use 8-bit quantization\"\n    )\n    parser.add_argument(\n        \"--cpu-offloading\",\n        action=\"store_true\",\n        help=\"Only when using 8-bit quantization: Offload excess weights to the CPU that don't fit on the GPU\",\n    )\n\n\ndef remove_parent_directory_name(model_path):\n    \"\"\"Remove parent directory name.\"\"\"\n    if model_path[-1] == \"/\":\n        model_path = model_path[:-1]\n    return model_path.split(\"/\")[-1]\n\n\nclass VicunaAdapter(BaseAdapter):\n    \"Model adapater for vicuna-v1.1\"\n\n    def match(self, model_path: str):\n        return \"vicuna\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        self.raise_warning_for_old_weights(model)\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        if \"v0\" in remove_parent_directory_name(model_path):\n            return get_conv_template(\"one_shot\")\n        return get_conv_template(\"vicuna_v1.1\")\n\n    def raise_warning_for_old_weights(self, model):\n        if isinstance(model, LlamaForCausalLM) and model.model.vocab_size > 32000:\n            warnings.warn(\n                \"\\nYou are probably using the old Vicuna-v0 model, \"\n                \"which will generate unexpected results with the \"\n                \"current fastchat.\\nYou can try one of the following methods:\\n\"\n                \"1. Upgrade your weights to the new Vicuna-v1.1: https://github.com/lm-sys/FastChat#vicuna-weights.\\n\"\n                \"2. Use the old conversation template by `python3 -m fastchat.serve.cli --model-path /path/to/vicuna-v0 --conv-template conv_one_shot`\\n\"\n                \"3. Downgrade fschat to fschat==0.1.10 (Not recommonded).\\n\"\n            )\n\n\nclass T5Adapter(BaseAdapter):\n    \"\"\"The model adapter for lmsys/fastchat-t5-3b-v1.0\"\"\"\n\n    def match(self, model_path: str):\n        return \"t5\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        tokenizer = T5Tokenizer.from_pretrained(model_path, use_fast=False)\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            model_path, low_cpu_mem_usage=True, **from_pretrained_kwargs\n        )\n        return model, tokenizer\n\n\nclass KoalaAdapter(BaseAdapter):\n    \"\"\"The model adapter for koala\"\"\"\n\n    def match(self, model_path: str):\n        return \"koala\" in model_path\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"koala_v1\")\n\n\nclass AlpacaAdapter(BaseAdapter):\n    \"\"\"The model adapter for alpaca.\"\"\"\n\n    def match(self, model_path: str):\n        return \"alpaca\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"alpaca\")\n\n\nclass ChatGLMAdapter(BaseAdapter):\n    \"\"\"The model adapter for THUDM/chatglm-6b\"\"\"\n\n    def match(self, model_path: str):\n        return \"chatglm\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n        model = AutoModel.from_pretrained(\n            model_path, trust_remote_code=True, **from_pretrained_kwargs\n        )\n        return model, tokenizer\n\n\nclass DollyV2Adapter(BaseAdapter):\n    \"\"\"The model adapter for databricks/dolly-v2-12b\"\"\"\n\n    def match(self, model_path: str):\n        return \"dolly-v2\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        # 50277 means \"### End\"\n        tokenizer.eos_token_id = 50277\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"dolly_v2\")\n\n\nclass OasstPythiaAdapter(BaseAdapter):\n    \"\"\"The model adapter for OpenAssistant/oasst-sft-1-pythia-12b\"\"\"\n\n    def match(self, model_path: str):\n        return \"oasst\" in model_path and \"pythia\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"oasst_pythia\")\n\n\nclass StableLMAdapter(BaseAdapter):\n    \"\"\"The model adapter for StabilityAI/stablelm-tuned-alpha-7b\"\"\"\n\n    def match(self, model_path: str):\n        return \"stablelm\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"stablelm\")\n\n\nclass MPTAdapter(BaseAdapter):\n    \"\"\"The model adapter for mosaicml/mpt-7b-chat\"\"\"\n\n    def match(self, model_path: str):\n        return \"mpt\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            trust_remote_code=True,\n            max_seq_len=8192,\n            **from_pretrained_kwargs,\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path, trust_remote_code=True, use_fast=True\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"mpt\")\n\n\nclass BaizeAdapter(BaseAdapter):\n    \"\"\"The model adapter for project-baize/baize-lora-7B\"\"\"\n\n    def match(self, model_path: str):\n        return \"baize\" in model_path\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"baize\")\n\n\nclass RwkvAdapter(BaseAdapter):\n    \"\"\"The model adapter for BlinkDL/RWKV-4-Raven\"\"\"\n\n    def match(self, model_path: str):\n        return \"RWKV-4\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        from fastchat.model.rwkv_model import RwkvModel\n\n        model = RwkvModel(model_path)\n        tokenizer = AutoTokenizer.from_pretrained(\n            \"EleutherAI/pythia-160m\", use_fast=True\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"rwkv\")\n\n\nclass OpenBuddyAdapter(BaseAdapter):\n    \"\"\"The model adapter for OpenBuddy/openbuddy-7b-v1.1-bf16-enc\"\"\"\n\n    def match(self, model_path: str):\n        return \"openbuddy\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        if \"-bf16\" in model_path:\n            from_pretrained_kwargs[\"torch_dtype\"] = torch.bfloat16\n            warnings.warn(\n                \"## This is a bf16(bfloat16) variant of OpenBuddy. Please make sure your GPU supports bf16.\"\n            )\n        model = LlamaForCausalLM.from_pretrained(\n            model_path, low_cpu_mem_usage=True, **from_pretrained_kwargs\n        )\n        tokenizer = LlamaTokenizer.from_pretrained(model_path)\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"openbuddy\")\n\n\nclass PhoenixAdapter(BaseAdapter):\n    \"\"\"The model adapter for FreedomIntelligence/phoenix-inst-chat-7b\"\"\"\n\n    def match(self, model_path: str):\n        return \"phoenix\" in model_path\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"phoenix\")\n\n\nclass ChatGPTAdapter(BaseAdapter):\n    \"\"\"The model adapter for ChatGPT.\"\"\"\n\n    def match(self, model_path: str):\n        return model_path == \"gpt-3.5-turbo\" or model_path == \"gpt-4\"\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"chatgpt\")\n\n\nclass ClaudeAdapter(BaseAdapter):\n    \"\"\"The model adapter for Claude.\"\"\"\n\n    def match(self, model_path: str):\n        return model_path in [\"claude-v1\", \"claude-instant-v1\"]\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"claude\")\n\n\nclass BardAdapter(BaseAdapter):\n    \"\"\"The model adapter for Bard.\"\"\"\n\n    def match(self, model_path: str):\n        return model_path == \"bard\"\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        raise NotImplementedError()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"bard\")\n\n\nclass BiLLaAdapter(BaseAdapter):\n    \"\"\"The model adapter for BiLLa.\"\"\"\n\n    def match(self, model_path: str):\n        return \"billa\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"billa\")\n\n\nclass RedPajamaINCITEAdapter(BaseAdapter):\n    \"\"\"The model adapter for RedPajama INCITE.\"\"\"\n\n    def match(self, model_path: str):\n        return \"redpajama-incite\" in model_path.lower()\n\n    def load_model(self, model_path: str, from_pretrained_kwargs: dict):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)  # no use_fast=False\n        model = AutoModelForCausalLM.from_pretrained(\n            model_path,\n            low_cpu_mem_usage=True,\n            **from_pretrained_kwargs,\n        )\n        return model, tokenizer\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"redpajama-incite\")\n\n\nclass H2OGPTAdapter(BaseAdapter):\n    \"\"\"The model adapter for h2oGPT.\"\"\"\n\n    def match(self, model_path: str):\n        return \"h2ogpt\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"h2ogpt\")\n\n\n# Note: the registration order matters.\n# The one registered earlier has a higher matching priority.\nregister_model_adapter(VicunaAdapter)\nregister_model_adapter(T5Adapter)\nregister_model_adapter(KoalaAdapter)\nregister_model_adapter(AlpacaAdapter)\nregister_model_adapter(ChatGLMAdapter)\nregister_model_adapter(DollyV2Adapter)\nregister_model_adapter(OasstPythiaAdapter)\nregister_model_adapter(StableLMAdapter)\nregister_model_adapter(BaizeAdapter)\nregister_model_adapter(RwkvAdapter)\nregister_model_adapter(OpenBuddyAdapter)\nregister_model_adapter(PhoenixAdapter)\nregister_model_adapter(BardAdapter)\nregister_model_adapter(ChatGPTAdapter)\nregister_model_adapter(ClaudeAdapter)\nregister_model_adapter(MPTAdapter)\nregister_model_adapter(BiLLaAdapter)\nregister_model_adapter(RedPajamaINCITEAdapter)\nregister_model_adapter(H2OGPTAdapter)\n\n# After all adapters, try the default base adapter.\nregister_model_adapter(BaseAdapter)\n"}
{"type": "source_file", "path": "graphgpt/serve/huggingface_api.py", "content": "\"\"\"\nUse FastChat with Hugging Face generation APIs.\n\nUsage:\npython3 -m fastchat.serve.huggingface_api --model lmsys/fastchat-t5-3b-v1.0\npython3 -m fastchat.serve.huggingface_api --model ~/model_weights/vicuna-7b/\n\"\"\"\nimport argparse\nimport json\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nfrom fastchat.model import load_model, get_conversation_template, add_model_args\n\n\n@torch.inference_mode()\ndef main(args):\n    model, tokenizer = load_model(\n        args.model_path,\n        args.device,\n        args.num_gpus,\n        args.max_gpu_memory,\n        args.load_8bit,\n        args.cpu_offloading,\n        debug=args.debug,\n    )\n\n    msg = args.message\n\n    conv = get_conversation_template(args.model_path)\n    conv.append_message(conv.roles[0], msg)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n\n    input_ids = tokenizer([prompt]).input_ids\n    \n    if \"t5\" in args.model_path and args.repetition_penalty == 1.0:\n        args.repetition_penalty = 1.2\n    output_ids = model.generate(\n        torch.as_tensor(input_ids).cuda(),\n        do_sample=True,\n        temperature=args.temperature,\n        repetition_penalty=args.repetition_penalty,\n        max_new_tokens=args.max_new_tokens,\n    )\n    if model.config.is_encoder_decoder:\n        output_ids = output_ids[0]\n    else:\n        output_ids = output_ids[0][len(input_ids[0]) :]\n    outputs = tokenizer.decode(\n        output_ids, skip_special_tokens=True, spaces_between_special_tokens=False\n    )\n\n    print(f\"{conv.roles[0]}: {msg}\")\n    print(f\"{conv.roles[1]}: {outputs}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    add_model_args(parser)\n    parser.add_argument(\"--temperature\", type=float, default=0.7)\n    parser.add_argument(\"--repetition_penalty\", type=float, default=1.0)\n    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    parser.add_argument(\"--message\", type=str, default=\"Hello! Who are you?\")\n    args = parser.parse_args()\n\n    main(args)\n"}
{"type": "source_file", "path": "graphgpt/serve/gradio_patch.py", "content": "\"\"\"\nAdopted from https://github.com/gradio-app/gradio/blob/main/gradio/components.py\nFix a markdown render problem.\n\"\"\"\nfrom __future__ import annotations\n\nfrom gradio.components import *\nfrom markdown2 import Markdown\nimport nh3\n\n\nclass _Keywords(Enum):\n    NO_VALUE = \"NO_VALUE\"  # Used as a sentinel to determine if nothing is provided as a argument for `value` in `Component.update()`\n    FINISHED_ITERATING = \"FINISHED_ITERATING\"  # Used to skip processing of a component's value (needed for generators + state)\n\n\n@document(\"style\")\nclass Chatbot(Changeable, Selectable, IOComponent, JSONSerializable):\n    \"\"\"\n    Displays a chatbot output showing both user submitted messages and responses. Supports a subset of Markdown including bold, italics, code, and images.\n    Preprocessing: this component does *not* accept input.\n    Postprocessing: expects function to return a {List[Tuple[str | None | Tuple, str | None | Tuple]]}, a list of tuples with user message and response messages. Messages should be strings, tuples, or Nones. If the message is a string, it can include Markdown. If it is a tuple, it should consist of (string filepath to image/video/audio, [optional string alt text]). Messages that are `None` are not displayed.\n\n    Demos: chatbot_simple, chatbot_multimodal\n    \"\"\"\n\n    def __init__(\n        self,\n        value: List[Tuple[str | None, str | None]] | Callable | None = None,\n        color_map: Dict[str, str] | None = None,  # Parameter moved to Chatbot.style()\n        *,\n        label: str | None = None,\n        every: float | None = None,\n        show_label: bool = True,\n        visible: bool = True,\n        elem_id: str | None = None,\n        elem_classes: List[str] | str | None = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Parameters:\n            value: Default value to show in chatbot. If callable, the function will be called whenever the app loads to set the initial value of the component.\n            label: component name in interface.\n            every: If `value` is a callable, run the function 'every' number of seconds while the client connection is open. Has no effect otherwise. Queue must be enabled. The event can be accessed (e.g. to cancel it) via this component's .load_event attribute.\n            show_label: if True, will display label.\n            visible: If False, component will be hidden.\n            elem_id: An optional string that is assigned as the id of this component in the HTML DOM. Can be used for targeting CSS styles.\n            elem_classes: An optional list of strings that are assigned as the classes of this component in the HTML DOM. Can be used for targeting CSS styles.\n        \"\"\"\n        if color_map is not None:\n            warnings.warn(\n                \"The 'color_map' parameter has been deprecated.\",\n            )\n        # self.md = utils.get_markdown_parser()\n        self.md = Markdown(extras=[\"fenced-code-blocks\", \"tables\", \"break-on-newline\"])\n        self.select: EventListenerMethod\n        \"\"\"\n        Event listener for when the user selects message from Chatbot.\n        Uses event data gradio.SelectData to carry `value` referring to text of selected message, and `index` tuple to refer to [message, participant] index.\n        See EventData documentation on how to use this event data.\n        \"\"\"\n\n        IOComponent.__init__(\n            self,\n            label=label,\n            every=every,\n            show_label=show_label,\n            visible=visible,\n            elem_id=elem_id,\n            elem_classes=elem_classes,\n            value=value,\n            **kwargs,\n        )\n\n    def get_config(self):\n        return {\n            \"value\": self.value,\n            \"selectable\": self.selectable,\n            **IOComponent.get_config(self),\n        }\n\n    @staticmethod\n    def update(\n        value: Any | Literal[_Keywords.NO_VALUE] | None = _Keywords.NO_VALUE,\n        label: str | None = None,\n        show_label: bool | None = None,\n        visible: bool | None = None,\n    ):\n        updated_config = {\n            \"label\": label,\n            \"show_label\": show_label,\n            \"visible\": visible,\n            \"value\": value,\n            \"__type__\": \"update\",\n        }\n        return updated_config\n\n    def _process_chat_messages(\n        self, chat_message: str | Tuple | List | Dict | None\n    ) -> str | Dict | None:\n        if chat_message is None:\n            return None\n        elif isinstance(chat_message, (tuple, list)):\n            mime_type = processing_utils.get_mimetype(chat_message[0])\n            return {\n                \"name\": chat_message[0],\n                \"mime_type\": mime_type,\n                \"alt_text\": chat_message[1] if len(chat_message) > 1 else None,\n                \"data\": None,  # These last two fields are filled in by the frontend\n                \"is_file\": True,\n            }\n        elif isinstance(\n            chat_message, dict\n        ):  # This happens for previously processed messages\n            return chat_message\n        elif isinstance(chat_message, str):\n            # return self.md.render(chat_message)\n            return str(self.md.convert(chat_message))\n        else:\n            raise ValueError(f\"Invalid message for Chatbot component: {chat_message}\")\n\n    def postprocess(\n        self,\n        y: List[\n            Tuple[str | Tuple | List | Dict | None, str | Tuple | List | Dict | None]\n        ],\n    ) -> List[Tuple[str | Dict | None, str | Dict | None]]:\n        \"\"\"\n        Parameters:\n            y: List of tuples representing the message and response pairs. Each message and response should be a string, which may be in Markdown format.  It can also be a tuple whose first element is a string filepath or URL to an image/video/audio, and second (optional) element is the alt text, in which case the media file is displayed. It can also be None, in which case that message is not displayed.\n        Returns:\n            List of tuples representing the message and response. Each message and response will be a string of HTML, or a dictionary with media information.\n        \"\"\"\n        if y is None:\n            return []\n        processed_messages = []\n        for message_pair in y:\n            assert isinstance(\n                message_pair, (tuple, list)\n            ), f\"Expected a list of lists or list of tuples. Received: {message_pair}\"\n            assert (\n                len(message_pair) == 2\n            ), f\"Expected a list of lists of length 2 or list of tuples of length 2. Received: {message_pair}\"\n            processed_messages.append(\n                (\n                    # self._process_chat_messages(message_pair[0]),\n                    '<pre style=\"font-family: var(--font)\">'\n                    + nh3.clean(message_pair[0])\n                    + \"</pre>\",\n                    self._process_chat_messages(message_pair[1]),\n                )\n            )\n        return processed_messages\n\n    def style(self, height: int | None = None, **kwargs):\n        \"\"\"\n        This method can be used to change the appearance of the Chatbot component.\n        \"\"\"\n        if height is not None:\n            self._style[\"height\"] = height\n        if kwargs.get(\"color_map\") is not None:\n            warnings.warn(\"The 'color_map' parameter has been deprecated.\")\n\n        Component.style(\n            self,\n            **kwargs,\n        )\n        return self\n"}
{"type": "source_file", "path": "graphgpt/serve/gradio_web_server.py", "content": "\"\"\"\nThe gradio demo server for chatting with a single model.\n\"\"\"\n\nimport argparse\nfrom collections import defaultdict\nimport datetime\nimport json\nimport os\nimport random\nimport time\nimport uuid\n\nimport gradio as gr\nimport requests\n\nfrom fastchat.conversation import SeparatorStyle\nfrom fastchat.constants import (\n    LOGDIR,\n    WORKER_API_TIMEOUT,\n    ErrorCode,\n    MODERATION_MSG,\n    CONVERSATION_LIMIT_MSG,\n    SERVER_ERROR_MSG,\n    INPUT_CHAR_LEN_LIMIT,\n    CONVERSATION_LEN_LIMIT,\n)\nfrom fastchat.model.model_adapter import get_conversation_template\nfrom fastchat.model.model_registry import model_info\nfrom fastchat.serve.api_provider import (\n    anthropic_api_stream_iter,\n    bard_api_stream_iter,\n    openai_api_stream_iter,\n    palm_api_stream_iter,\n    init_palm_chat,\n)\nfrom fastchat.serve.gradio_patch import Chatbot as grChatbot\nfrom fastchat.serve.gradio_css import code_highlight_css\nfrom fastchat.utils import (\n    build_logger,\n    violates_moderation,\n    get_window_url_params_js,\n)\n\n\nlogger = build_logger(\"gradio_web_server\", \"gradio_web_server.log\")\n\nheaders = {\"User-Agent\": \"fastchat Client\"}\n\nno_change_btn = gr.Button.update()\nenable_btn = gr.Button.update(interactive=True)\ndisable_btn = gr.Button.update(interactive=False)\n\ncontroller_url = None\nenable_moderation = False\n\nlearn_more_md = \"\"\"\n### License\nThe service is a research preview intended for non-commercial use only, subject to the model [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.\n\"\"\"\n\n\nclass State:\n    def __init__(self, model_name):\n        self.conv = get_conversation_template(model_name)\n        self.conv_id = uuid.uuid4().hex\n        self.skip_next = False\n        self.model_name = model_name\n\n        if model_name == \"bard\":\n            self.bard_session_state = {\n                \"conversation_id\": \"\",\n                \"response_id\": \"\",\n                \"choice_id\": \"\",\n                \"req_id\": 0,\n            }\n            # According to release note, \"chat-bison@001\" is PaLM 2 for chat.\n            # https://cloud.google.com/vertex-ai/docs/release-notes#May_10_2023\n            self.palm_chat = init_palm_chat(\"chat-bison@001\")\n\n    def to_gradio_chatbot(self):\n        return self.conv.to_gradio_chatbot()\n\n    def dict(self):\n        base = self.conv.dict()\n        base.update(\n            {\n                \"conv_id\": self.conv_id,\n                \"model_name\": self.model_name,\n            }\n        )\n        return base\n\n\ndef set_global_vars(controller_url_, enable_moderation_):\n    global controller_url, enable_moderation\n    controller_url = controller_url_\n    enable_moderation = enable_moderation_\n\n\ndef get_conv_log_filename():\n    t = datetime.datetime.now()\n    name = os.path.join(LOGDIR, f\"{t.year}-{t.month:02d}-{t.day:02d}-conv.json\")\n    return name\n\n\ndef get_model_list(controller_url):\n    ret = requests.post(controller_url + \"/refresh_all_workers\")\n    assert ret.status_code == 200\n    ret = requests.post(controller_url + \"/list_models\")\n    models = ret.json()[\"models\"]\n    priority = {k: f\"___{i:02d}\" for i, k in enumerate(model_info)}\n    models.sort(key=lambda x: priority.get(x, x))\n    logger.info(f\"Models: {models}\")\n    return models\n\n\ndef load_demo_refresh_model_list(url_params):\n    models = get_model_list(controller_url)\n    selected_model = models[0] if len(models) > 0 else \"\"\n    if \"model\" in url_params:\n        model = url_params[\"model\"]\n        if model in models:\n            selected_model = model\n\n    dropdown_update = gr.Dropdown.update(\n        choices=models, value=selected_model, visible=True\n    )\n\n    state = None\n    return (\n        state,\n        dropdown_update,\n        gr.Chatbot.update(visible=True),\n        gr.Textbox.update(visible=True),\n        gr.Button.update(visible=True),\n        gr.Row.update(visible=True),\n        gr.Accordion.update(visible=True),\n    )\n\n\ndef load_demo_reload_model(url_params, request: gr.Request):\n    logger.info(\n        f\"load_demo_reload_model. ip: {request.client.host}. params: {url_params}\"\n    )\n    return load_demo_refresh_model_list(url_params)\n\n\ndef load_demo_single(models, url_params):\n    dropdown_update = gr.Dropdown.update(visible=True)\n    if \"model\" in url_params:\n        model = url_params[\"model\"]\n        if model in models:\n            dropdown_update = gr.Dropdown.update(value=model, visible=True)\n\n    state = None\n    return (\n        state,\n        dropdown_update,\n        gr.Chatbot.update(visible=True),\n        gr.Textbox.update(visible=True),\n        gr.Button.update(visible=True),\n        gr.Row.update(visible=True),\n        gr.Accordion.update(visible=True),\n    )\n\n\ndef load_demo(url_params, request: gr.Request):\n    logger.info(f\"load_demo. ip: {request.client.host}. params: {url_params}\")\n    return load_demo_single(models, url_params)\n\n\ndef vote_last_response(state, vote_type, model_selector, request: gr.Request):\n    with open(get_conv_log_filename(), \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"model\": model_selector,\n            \"state\": state.dict(),\n            \"ip\": request.client.host,\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n\n\ndef upvote_last_response(state, model_selector, request: gr.Request):\n    logger.info(f\"upvote. ip: {request.client.host}\")\n    vote_last_response(state, \"upvote\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef downvote_last_response(state, model_selector, request: gr.Request):\n    logger.info(f\"downvote. ip: {request.client.host}\")\n    vote_last_response(state, \"downvote\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef flag_last_response(state, model_selector, request: gr.Request):\n    logger.info(f\"flag. ip: {request.client.host}\")\n    vote_last_response(state, \"flag\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef regenerate(state, request: gr.Request):\n    logger.info(f\"regenerate. ip: {request.client.host}\")\n    state.conv.update_last_message(None)\n    return (state, state.to_gradio_chatbot(), \"\") + (disable_btn,) * 5\n\n\ndef clear_history(request: gr.Request):\n    logger.info(f\"clear_history. ip: {request.client.host}\")\n    state = None\n    return (state, [], \"\") + (disable_btn,) * 5\n\n\ndef add_text(state, model_selector, text, request: gr.Request):\n    logger.info(f\"add_text. ip: {request.client.host}. len: {len(text)}\")\n\n    if state is None:\n        state = State(model_selector)\n\n    if len(text) <= 0:\n        state.skip_next = True\n        return (state, state.to_gradio_chatbot(), \"\") + (no_change_btn,) * 5\n\n    if enable_moderation:\n        flagged = violates_moderation(text)\n        if flagged:\n            logger.info(f\"violate moderation. ip: {request.client.host}. text: {text}\")\n            state.skip_next = True\n            return (state, state.to_gradio_chatbot(), MODERATION_MSG) + (\n                no_change_btn,\n            ) * 5\n\n    conv = state.conv\n    if (len(conv.messages) - conv.offset) // 2 >= CONVERSATION_LEN_LIMIT:\n        logger.info(\n            f\"hit conversation length limit. ip: {request.client.host}. text: {text}\"\n        )\n        state.skip_next = True\n        return (state, state.to_gradio_chatbot(), CONVERSATION_LIMIT_MSG) + (\n            no_change_btn,\n        ) * 5\n\n    text = text[:INPUT_CHAR_LEN_LIMIT]  # Hard cut-off\n    conv.append_message(conv.roles[0], text)\n    conv.append_message(conv.roles[1], None)\n    return (state, state.to_gradio_chatbot(), \"\") + (disable_btn,) * 5\n\n\ndef post_process_code(code):\n    sep = \"\\n```\"\n    if sep in code:\n        blocks = code.split(sep)\n        if len(blocks) % 2 == 1:\n            for i in range(1, len(blocks), 2):\n                blocks[i] = blocks[i].replace(\"\\\\_\", \"_\")\n        code = sep.join(blocks)\n    return code\n\n\ndef model_worker_stream_iter(\n    conv,\n    model_name,\n    worker_addr,\n    prompt,\n    temperature,\n    repetition_penalty,\n    top_p,\n    max_new_tokens,\n):\n    # Make requests\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"temperature\": temperature,\n        \"repetition_penalty\": repetition_penalty,\n        \"top_p\": top_p,\n        \"max_new_tokens\": max_new_tokens,\n        \"stop\": conv.stop_str,\n        \"stop_token_ids\": conv.stop_token_ids,\n        \"echo\": False,\n    }\n    logger.info(f\"==== request ====\\n{gen_params}\")\n\n    # Stream output\n    response = requests.post(\n        worker_addr + \"/worker_generate_stream\",\n        headers=headers,\n        json=gen_params,\n        stream=True,\n        timeout=WORKER_API_TIMEOUT,\n    )\n    for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n        if chunk:\n            data = json.loads(chunk.decode())\n            yield data\n\n\ndef http_bot(state, temperature, top_p, max_new_tokens, request: gr.Request):\n    logger.info(f\"http_bot. ip: {request.client.host}\")\n    start_tstamp = time.time()\n    temperature = float(temperature)\n    top_p = float(top_p)\n    max_new_tokens = int(max_new_tokens)\n\n    if state.skip_next:\n        # This generate call is skipped due to invalid inputs\n        state.skip_next = False\n        yield (state, state.to_gradio_chatbot()) + (no_change_btn,) * 5\n        return\n\n    conv, model_name = state.conv, state.model_name\n    if model_name == \"gpt-3.5-turbo\" or model_name == \"gpt-4\":\n        prompt = conv.to_openai_api_messages()\n        stream_iter = openai_api_stream_iter(\n            model_name, prompt, temperature, top_p, max_new_tokens\n        )\n    elif model_name in [\"claude-v1\", \"claude-instant-v1\"]:\n        prompt = conv.get_prompt()\n        stream_iter = anthropic_api_stream_iter(\n            model_name, prompt, temperature, top_p, max_new_tokens\n        )\n    elif model_name == \"bard\":\n        # stream_iter = bard_api_stream_iter(state)\n        stream_iter = palm_api_stream_iter(\n            state.palm_chat, conv.messages[-2][1], temperature, top_p, max_new_tokens\n        )\n    else:\n        # Query worker address\n        ret = requests.post(\n            controller_url + \"/get_worker_address\", json={\"model\": model_name}\n        )\n        worker_addr = ret.json()[\"address\"]\n        logger.info(f\"model_name: {model_name}, worker_addr: {worker_addr}\")\n\n        # No available worker\n        if worker_addr == \"\":\n            conv.update_last_message(SERVER_ERROR_MSG)\n            yield (\n                state,\n                state.to_gradio_chatbot(),\n                disable_btn,\n                disable_btn,\n                disable_btn,\n                enable_btn,\n                enable_btn,\n            )\n            return\n\n        # Construct prompt\n        if \"chatglm\" in model_name:\n            prompt = list(list(x) for x in conv.messages[conv.offset :])\n        else:\n            prompt = conv.get_prompt()\n\n        # Construct repetition_penalty\n        if \"t5\" in model_name:\n            repetition_penalty = 1.2\n        else:\n            repetition_penalty = 1.0\n        stream_iter = model_worker_stream_iter(\n            conv,\n            model_name,\n            worker_addr,\n            prompt,\n            temperature,\n            repetition_penalty,\n            top_p,\n            max_new_tokens,\n        )\n\n    conv.update_last_message(\"▌\")\n    yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5\n\n    try:\n        for data in stream_iter:\n            if data[\"error_code\"] == 0:\n                output = data[\"text\"].strip()\n                if \"vicuna\" in model_name:\n                    output = post_process_code(output)\n                conv.update_last_message(output + \"▌\")\n                yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5\n            else:\n                output = data[\"text\"] + f\"\\n\\n(error_code: {data['error_code']})\"\n                conv.update_last_message(output)\n                yield (state, state.to_gradio_chatbot()) + (\n                    disable_btn,\n                    disable_btn,\n                    disable_btn,\n                    enable_btn,\n                    enable_btn,\n                )\n                return\n            time.sleep(0.02)\n    except requests.exceptions.RequestException as e:\n        conv.update_last_message(\n            f\"{SERVER_ERROR_MSG}\\n\\n\"\n            f\"(error_code: {ErrorCode.GRADIO_REQUEST_ERROR}, {e})\"\n        )\n        yield (state, state.to_gradio_chatbot()) + (\n            disable_btn,\n            disable_btn,\n            disable_btn,\n            enable_btn,\n            enable_btn,\n        )\n        return\n    except Exception as e:\n        conv.update_last_message(\n            f\"{SERVER_ERROR_MSG}\\n\\n\"\n            f\"(error_code: {ErrorCode.GRADIO_STREAM_UNKNOWN_ERROR}, {e})\"\n        )\n        yield (state, state.to_gradio_chatbot()) + (\n            disable_btn,\n            disable_btn,\n            disable_btn,\n            enable_btn,\n            enable_btn,\n        )\n        return\n\n    # Delete \"▌\"\n    conv.update_last_message(conv.messages[-1][-1][:-1])\n    yield (state, state.to_gradio_chatbot()) + (enable_btn,) * 5\n\n    finish_tstamp = time.time()\n    logger.info(f\"{output}\")\n\n    with open(get_conv_log_filename(), \"a\") as fout:\n        data = {\n            \"tstamp\": round(finish_tstamp, 4),\n            \"type\": \"chat\",\n            \"model\": model_name,\n            \"gen_params\": {\n                \"temperature\": temperature,\n                \"top_p\": top_p,\n                \"max_new_tokens\": max_new_tokens,\n            },\n            \"start\": round(start_tstamp, 4),\n            \"finish\": round(finish_tstamp, 4),\n            \"state\": state.dict(),\n            \"ip\": request.client.host,\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n\n\nblock_css = (\n    code_highlight_css\n    + \"\"\"\npre {\n    white-space: pre-wrap;       /* Since CSS 2.1 */\n    white-space: -moz-pre-wrap;  /* Mozilla, since 1999 */\n    white-space: -pre-wrap;      /* Opera 4-6 */\n    white-space: -o-pre-wrap;    /* Opera 7 */\n    word-wrap: break-word;       /* Internet Explorer 5.5+ */\n}\n#notice_markdown th {\n    display: none;\n}\n\"\"\"\n)\n\n\ndef get_model_description_md(models):\n    model_description_md = \"\"\"\n| | | |\n| ---- | ---- | ---- |\n\"\"\"\n    ct = 0\n    visited = set()\n    for i, name in enumerate(models):\n        if name in model_info:\n            minfo = model_info[name]\n            if minfo.simple_name in visited:\n                continue\n            visited.add(minfo.simple_name)\n            one_model_md = f\"[{minfo.simple_name}]({minfo.link}): {minfo.description}\"\n        else:\n            visited.add(name)\n            one_model_md = (\n                f\"[{name}](): Add the description at fastchat/model/model_registry.py\"\n            )\n\n        if ct % 3 == 0:\n            model_description_md += \"|\"\n        model_description_md += f\" {one_model_md} |\"\n        if ct % 3 == 2:\n            model_description_md += \"\\n\"\n        ct += 1\n    return model_description_md\n\n\ndef build_single_model_ui(models):\n    notice_markdown = \"\"\"\n# 🏔️ Chat with Open Large Language Models\n- Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality. [[Blog post]](https://lmsys.org/blog/2023-03-30-vicuna/)\n- Koala: A Dialogue Model for Academic Research. [[Blog post]](https://bair.berkeley.edu/blog/2023/04/03/koala/)\n- [[GitHub]](https://github.com/lm-sys/FastChat) [[Twitter]](https://twitter.com/lmsysorg) [[Discord]](https://discord.gg/KjdtsE9V)\n\n### Terms of use\nBy using this service, users are required to agree to the following terms: The service is a research preview intended for non-commercial use only. It only provides limited safety measures and may generate offensive content. It must not be used for any illegal, harmful, violent, racist, or sexual purposes. **The service collects user dialogue data and reserves the right to distribute it under a Creative Commons Attribution (CC-BY) license.**\n\n### Choose a model to chat with\n\"\"\"\n\n    state = gr.State()\n    model_description_md = get_model_description_md(models)\n    gr.Markdown(notice_markdown + model_description_md, elem_id=\"notice_markdown\")\n\n    with gr.Row(elem_id=\"model_selector_row\"):\n        model_selector = gr.Dropdown(\n            choices=models,\n            value=models[0] if len(models) > 0 else \"\",\n            interactive=True,\n            show_label=False,\n        ).style(container=False)\n\n    chatbot = grChatbot(\n        elem_id=\"chatbot\", label=\"Scroll down and start chatting\", visible=False\n    ).style(height=550)\n    with gr.Row():\n        with gr.Column(scale=20):\n            textbox = gr.Textbox(\n                show_label=False,\n                placeholder=\"Enter text and press ENTER\",\n                visible=False,\n            ).style(container=False)\n        with gr.Column(scale=1, min_width=50):\n            send_btn = gr.Button(value=\"Send\", visible=False)\n\n    with gr.Row(visible=False) as button_row:\n        upvote_btn = gr.Button(value=\"👍  Upvote\", interactive=False)\n        downvote_btn = gr.Button(value=\"👎  Downvote\", interactive=False)\n        flag_btn = gr.Button(value=\"⚠️  Flag\", interactive=False)\n        regenerate_btn = gr.Button(value=\"🔄  Regenerate\", interactive=False)\n        clear_btn = gr.Button(value=\"🗑️  Clear history\", interactive=False)\n\n    with gr.Accordion(\"Parameters\", open=False, visible=False) as parameter_row:\n        temperature = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=0.7,\n            step=0.1,\n            interactive=True,\n            label=\"Temperature\",\n        )\n        top_p = gr.Slider(\n            minimum=0.0,\n            maximum=1.0,\n            value=1.0,\n            step=0.1,\n            interactive=True,\n            label=\"Top P\",\n        )\n        max_output_tokens = gr.Slider(\n            minimum=16,\n            maximum=1024,\n            value=512,\n            step=64,\n            interactive=True,\n            label=\"Max output tokens\",\n        )\n\n    gr.Markdown(learn_more_md)\n\n    # Register listeners\n    btn_list = [upvote_btn, downvote_btn, flag_btn, regenerate_btn, clear_btn]\n    upvote_btn.click(\n        upvote_last_response,\n        [state, model_selector],\n        [textbox, upvote_btn, downvote_btn, flag_btn],\n    )\n    downvote_btn.click(\n        downvote_last_response,\n        [state, model_selector],\n        [textbox, upvote_btn, downvote_btn, flag_btn],\n    )\n    flag_btn.click(\n        flag_last_response,\n        [state, model_selector],\n        [textbox, upvote_btn, downvote_btn, flag_btn],\n    )\n    regenerate_btn.click(regenerate, state, [state, chatbot, textbox] + btn_list).then(\n        http_bot,\n        [state, temperature, top_p, max_output_tokens],\n        [state, chatbot] + btn_list,\n    )\n    clear_btn.click(clear_history, None, [state, chatbot, textbox] + btn_list)\n\n    model_selector.change(clear_history, None, [state, chatbot, textbox] + btn_list)\n\n    textbox.submit(\n        add_text, [state, model_selector, textbox], [state, chatbot, textbox] + btn_list\n    ).then(\n        http_bot,\n        [state, temperature, top_p, max_output_tokens],\n        [state, chatbot] + btn_list,\n    )\n    send_btn.click(\n        add_text, [state, model_selector, textbox], [state, chatbot, textbox] + btn_list\n    ).then(\n        http_bot,\n        [state, temperature, top_p, max_output_tokens],\n        [state, chatbot] + btn_list,\n    )\n\n    return state, model_selector, chatbot, textbox, send_btn, button_row, parameter_row\n\n\ndef build_demo(models):\n    with gr.Blocks(\n        title=\"Chat with Open Large Language Models\",\n        theme=gr.themes.Base(),\n        css=block_css,\n    ) as demo:\n        url_params = gr.JSON(visible=False)\n\n        (\n            state,\n            model_selector,\n            chatbot,\n            textbox,\n            send_btn,\n            button_row,\n            parameter_row,\n        ) = build_single_model_ui(models)\n\n        if args.model_list_mode == \"once\":\n            demo.load(\n                load_demo,\n                [url_params],\n                [\n                    state,\n                    model_selector,\n                    chatbot,\n                    textbox,\n                    send_btn,\n                    button_row,\n                    parameter_row,\n                ],\n                _js=get_window_url_params_js,\n            )\n        elif args.model_list_mode == \"reload\":\n            demo.load(\n                load_demo_reload_model,\n                [url_params],\n                [\n                    state,\n                    model_selector,\n                    chatbot,\n                    textbox,\n                    send_btn,\n                    button_row,\n                    parameter_row,\n                ],\n                _js=get_window_url_params_js,\n            )\n        else:\n            raise ValueError(f\"Unknown model list mode: {args.model_list_mode}\")\n\n    return demo\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int)\n    parser.add_argument(\"--controller-url\", type=str, default=\"http://localhost:21001\")\n    parser.add_argument(\"--concurrency-count\", type=int, default=10)\n    parser.add_argument(\n        \"--model-list-mode\",\n        type=str,\n        default=\"once\",\n        choices=[\"once\", \"reload\"],\n        help=\"Whether to load the model list once or reload the model list every time.\",\n    )\n    parser.add_argument(\"--share\", action=\"store_true\")\n    parser.add_argument(\n        \"--moderate\", action=\"store_true\", help=\"Enable content moderation\"\n    )\n    parser.add_argument(\n        \"--add-chatgpt\",\n        action=\"store_true\",\n        help=\"Add OpenAI's ChatGPT models (gpt-3.5-turbo, gpt-4)\",\n    )\n    parser.add_argument(\n        \"--add-claude\",\n        action=\"store_true\",\n        help=\"Add Anthropic's Claude models (claude-v1, claude-instant-v1)\",\n    )\n    parser.add_argument(\n        \"--add-bard\",\n        action=\"store_true\",\n        help=\"Add Google's Bard model (PaLM 2 for Chat: chat-bison@001)\",\n    )\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    set_global_vars(args.controller_url, args.moderate)\n    models = get_model_list(args.controller_url)\n\n    if args.add_chatgpt:\n        models = [\"gpt-3.5-turbo\", \"gpt-4\"] + models\n    if args.add_claude:\n        models = [\"claude-v1\", \"claude-instant-v1\"] + models\n    if args.add_bard:\n        models = [\"bard\"] + models\n\n    demo = build_demo(models)\n    demo.queue(\n        concurrency_count=args.concurrency_count, status_update_rate=10, api_open=False\n    ).launch(\n        server_name=args.host, server_port=args.port, share=args.share, max_threads=200\n    )\n"}
{"type": "source_file", "path": "graphgpt/serve/inference.py", "content": "\"\"\"Inference for FastChat models.\"\"\"\nimport abc\nimport gc\nimport math\nfrom typing import Iterable, Optional\nimport sys\nimport warnings\n\nimport psutil\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    LlamaTokenizer,\n    LlamaForCausalLM,\n    AutoModel,\n    AutoModelForSeq2SeqLM,\n    T5Tokenizer,\n    AutoConfig,\n)\nfrom transformers.generation.logits_process import (\n    LogitsProcessorList,\n    RepetitionPenaltyLogitsProcessor,\n    TemperatureLogitsWarper,\n    TopKLogitsWarper,\n    TopPLogitsWarper,\n)\n\nfrom fastchat.conversation import get_conv_template, SeparatorStyle\nfrom fastchat.model.model_adapter import load_model, get_conversation_template\nfrom fastchat.model.chatglm_model import chatglm_generate_stream\n\n\ndef prepare_logits_processor(\n    temperature: float, repetition_penalty: float, top_p: float, top_k: int\n) -> LogitsProcessorList:\n    processor_list = LogitsProcessorList()\n    # TemperatureLogitsWarper doesn't accept 0.0, 1.0 makes it a no-op so we skip two cases.\n    if temperature >= 1e-5 and temperature != 1.0:\n        processor_list.append(TemperatureLogitsWarper(temperature))\n    if repetition_penalty > 1.0:\n        processor_list.append(RepetitionPenaltyLogitsProcessor(repetition_penalty))\n    if 1e-8 <= top_p < 1.0:\n        processor_list.append(TopPLogitsWarper(top_p))\n    if top_k > 0:\n        processor_list.append(TopKLogitsWarper(top_k))\n    return processor_list\n\n\ndef partial_stop(output, stop_str):\n    for i in range(0, min(len(output), len(stop_str))):\n        if stop_str.startswith(output[-i:]):\n            return True\n    return False\n\n\n@torch.inference_mode()\ndef generate_stream(\n    model, tokenizer, params, device, context_len=2048, stream_interval=2\n):\n    prompt = params[\"prompt\"]\n    len_prompt = len(prompt)\n    temperature = float(params.get(\"temperature\", 1.0))\n    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n    top_p = float(params.get(\"top_p\", 1.0))\n    top_k = int(params.get(\"top_k\", -1))  # -1 means disable\n    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n    stop_str = params.get(\"stop\", None)\n    echo = bool(params.get(\"echo\", True))\n    stop_token_ids = params.get(\"stop_token_ids\", None) or []\n    stop_token_ids.append(tokenizer.eos_token_id)\n\n    logits_processor = prepare_logits_processor(\n        temperature, repetition_penalty, top_p, top_k\n    )\n\n    input_ids = tokenizer(prompt).input_ids\n    input_echo_len = len(input_ids)\n    output_ids = list(input_ids)\n\n    if model.config.is_encoder_decoder:\n        max_src_len = context_len\n    else:\n        max_src_len = context_len - max_new_tokens - 8\n\n    input_ids = input_ids[-max_src_len:]\n\n    if model.config.is_encoder_decoder:\n        encoder_output = model.encoder(\n            input_ids=torch.as_tensor([input_ids], device=device)\n        )[0]\n        start_ids = torch.as_tensor(\n            [[model.generation_config.decoder_start_token_id]],\n            dtype=torch.int64,\n            device=device,\n        )\n\n    past_key_values = out = None\n    for i in range(max_new_tokens):\n        if i == 0:\n            if model.config.is_encoder_decoder:\n                out = model.decoder(\n                    input_ids=start_ids,\n                    encoder_hidden_states=encoder_output,\n                    use_cache=True,\n                )\n                logits = model.lm_head(out[0])\n            else:\n                out = model(torch.as_tensor([input_ids], device=device), use_cache=True)\n                logits = out.logits\n            past_key_values = out.past_key_values\n        else:\n            if model.config.is_encoder_decoder:\n                out = model.decoder(\n                    input_ids=torch.as_tensor([[token]], device=device),\n                    encoder_hidden_states=encoder_output,\n                    use_cache=True,\n                    past_key_values=past_key_values,\n                )\n\n                logits = model.lm_head(out[0])\n            else:\n                out = model(\n                    input_ids=torch.as_tensor([[token]], device=device),\n                    use_cache=True,\n                    past_key_values=past_key_values,\n                )\n                logits = out.logits\n            past_key_values = out.past_key_values\n\n        if logits_processor:\n            if repetition_penalty > 1.0:\n                tmp_output_ids = torch.as_tensor([output_ids], device=logits.device)\n            else:\n                tmp_output_ids = None\n            last_token_logits = logits_processor(tmp_output_ids, logits[:, -1, :])[0]\n        else:\n            last_token_logits = logits[0, -1, :]\n\n        if device == \"mps\":\n            # Switch to CPU by avoiding some bugs in mps backend.\n            last_token_logits = last_token_logits.float().to(\"cpu\")\n\n        if temperature < 1e-5 or top_p < 1e-8:  # greedy\n            token = int(torch.argmax(last_token_logits))\n        else:\n            probs = torch.softmax(last_token_logits, dim=-1)\n            token = int(torch.multinomial(probs, num_samples=1))\n\n        output_ids.append(token)\n\n        if token in stop_token_ids:\n            stopped = True\n        else:\n            stopped = False\n\n        if i % stream_interval == 0 or i == max_new_tokens - 1 or stopped:\n            if echo:\n                tmp_output_ids = output_ids\n                rfind_start = len_prompt\n            else:\n                tmp_output_ids = output_ids[input_echo_len:]\n                rfind_start = 0\n\n            output = tokenizer.decode(\n                tmp_output_ids,\n                skip_special_tokens=True,\n                spaces_between_special_tokens=False,\n            )\n\n            partially_stopped = False\n            if stop_str:\n                if isinstance(stop_str, str):\n                    pos = output.rfind(stop_str, rfind_start)\n                    if pos != -1:\n                        output = output[:pos]\n                        stopped = True\n                    else:\n                        partially_stopped = partial_stop(output, stop_str)\n                elif isinstance(stop_str, Iterable):\n                    for each_stop in stop_str:\n                        pos = output.rfind(each_stop, rfind_start)\n                        if pos != -1:\n                            output = output[:pos]\n                            stopped = True\n                            break\n                        else:\n                            partially_stopped = partial_stop(output, each_stop)\n                            if partially_stopped:\n                                break\n                else:\n                    raise ValueError(\"Invalid stop field type.\")\n\n            # prevent yielding partial stop sequence\n            if not partially_stopped:\n                yield {\n                    \"text\": output,\n                    \"usage\": {\n                        \"prompt_tokens\": input_echo_len,\n                        \"completion_tokens\": i,\n                        \"total_tokens\": input_echo_len + i,\n                    },\n                    \"finish_reason\": None,\n                }\n\n        if stopped:\n            break\n\n    # finish stream event, which contains finish reason\n    if i == max_new_tokens - 1:\n        finish_reason = \"length\"\n    elif stopped:\n        finish_reason = \"stop\"\n    else:\n        finish_reason = None\n\n    yield {\n        \"text\": output,\n        \"usage\": {\n            \"prompt_tokens\": input_echo_len,\n            \"completion_tokens\": i,\n            \"total_tokens\": input_echo_len + i,\n        },\n        \"finish_reason\": finish_reason,\n    }\n\n    # clean\n    del past_key_values, out\n    gc.collect()\n    torch.cuda.empty_cache()\n\n\nclass ChatIO(abc.ABC):\n    @abc.abstractmethod\n    def prompt_for_input(self, role: str) -> str:\n        \"\"\"Prompt for input from a role.\"\"\"\n\n    @abc.abstractmethod\n    def prompt_for_output(self, role: str):\n        \"\"\"Prompt for output from a role.\"\"\"\n\n    @abc.abstractmethod\n    def stream_output(self, output_stream):\n        \"\"\"Stream output.\"\"\"\n\n\ndef chat_loop(\n    model_path: str,\n    device: str,\n    num_gpus: int,\n    max_gpu_memory: str,\n    load_8bit: bool,\n    cpu_offloading: bool,\n    conv_template: Optional[str],\n    temperature: float,\n    repetition_penalty: float,\n    max_new_tokens: int,\n    chatio: ChatIO,\n    debug: bool,\n):\n    # Model\n    model, tokenizer = load_model(\n        model_path, device, num_gpus, max_gpu_memory, load_8bit, cpu_offloading, debug\n    )\n    is_chatglm = \"chatglm\" in str(type(model)).lower()\n    is_fastchat_t5 = \"t5\" in str(type(model)).lower()\n\n    # Hardcode T5 repetition penalty to be 1.2\n    if is_fastchat_t5 and repetition_penalty == 1.0:\n        repetition_penalty = 1.2\n\n    # Chat\n    if conv_template:\n        conv = get_conv_template(conv_template)\n    else:\n        conv = get_conversation_template(model_path)\n\n    while True:\n        try:\n            inp = chatio.prompt_for_input(conv.roles[0])\n        except EOFError:\n            inp = \"\"\n        if not inp:\n            print(\"exit...\")\n            break\n\n        conv.append_message(conv.roles[0], inp)\n        conv.append_message(conv.roles[1], None)\n\n        if is_chatglm:\n            generate_stream_func = chatglm_generate_stream\n            prompt = conv.messages[conv.offset :]\n        else:\n            generate_stream_func = generate_stream\n            prompt = conv.get_prompt()\n\n        gen_params = {\n            \"model\": model_path,\n            \"prompt\": prompt,\n            \"temperature\": temperature,\n            \"repetition_penalty\": repetition_penalty,\n            \"max_new_tokens\": max_new_tokens,\n            \"stop\": conv.stop_str,\n            \"stop_token_ids\": conv.stop_token_ids,\n            \"echo\": False,\n        }\n\n        chatio.prompt_for_output(conv.roles[1])\n        output_stream = generate_stream_func(model, tokenizer, gen_params, device)\n        outputs = chatio.stream_output(output_stream)\n        conv.update_last_message(outputs.strip())\n\n        if debug:\n            print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\n"}
{"type": "source_file", "path": "graphgpt/serve/gradio_web_server_graph.py", "content": "import argparse\nimport datetime\nimport json\nimport os\nimport time\n\nimport gradio as gr\nimport requests\n\nfrom graphgpt.conversation import (default_conversation, conv_templates,\n                                   SeparatorStyle)\nfrom graphgpt.constants import LOGDIR\nfrom graphgpt.utils import (build_logger, server_error_msg,\n    violates_moderation, moderation_msg)\nimport hashlib\n\n\nlogger = build_logger(\"gradio_web_server\", \"gradio_web_server.log\")\n\nheaders = {\"User-Agent\": \"GraphGPT Client\"}\n\nno_change_btn = gr.Button.update()\nenable_btn = gr.Button.update(interactive=True)\ndisable_btn = gr.Button.update(interactive=False)\n\npriority = {\n    \"vicuna-13b\": \"aaaaaaa\",\n    \"koala-13b\": \"aaaaaab\",\n}\n\n\ndef get_conv_log_filename():\n    t = datetime.datetime.now()\n    name = os.path.join(LOGDIR, f\"{t.year}-{t.month:02d}-{t.day:02d}-conv.json\")\n    return name\n\n\ndef get_model_list():\n    ret = requests.post(args.controller_url + \"/refresh_all_workers\")\n    assert ret.status_code == 200\n    ret = requests.post(args.controller_url + \"/list_models\")\n    models = ret.json()[\"models\"]\n    models.sort(key=lambda x: priority.get(x, x))\n    logger.info(f\"Models: {models}\")\n    return models\n\n\nget_window_url_params = \"\"\"\nfunction() {\n    const params = new URLSearchParams(window.location.search);\n    url_params = Object.fromEntries(params);\n    console.log(url_params);\n    return url_params;\n    }\n\"\"\"\n\n\ndef load_demo(url_params, request: gr.Request):\n    logger.info(f\"load_demo. ip: {request.client.host}. params: {url_params}\")\n\n    dropdown_update = gr.Dropdown.update(visible=True)\n    if \"model\" in url_params:\n        model = url_params[\"model\"]\n        if model in models:\n            dropdown_update = gr.Dropdown.update(\n                value=model, visible=True)\n\n    state = default_conversation.copy()\n    return state, dropdown_update\n\n\ndef load_demo_refresh_model_list(request: gr.Request):\n    logger.info(f\"load_demo. ip: {request.client.host}\")\n    models = get_model_list()\n    state = default_conversation.copy()\n    dropdown_update = gr.Dropdown.update(\n        choices=models,\n        value=models[0] if len(models) > 0 else \"\"\n    )\n    return state, dropdown_update\n\n\ndef vote_last_response(state, vote_type, model_selector, request: gr.Request):\n    with open(get_conv_log_filename(), \"a\") as fout:\n        data = {\n            \"tstamp\": round(time.time(), 4),\n            \"type\": vote_type,\n            \"model\": model_selector,\n            \"state\": state.dict(),\n            \"ip\": request.client.host,\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n\n\ndef upvote_last_response(state, model_selector, request: gr.Request):\n    logger.info(f\"upvote. ip: {request.client.host}\")\n    vote_last_response(state, \"upvote\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef downvote_last_response(state, model_selector, request: gr.Request):\n    logger.info(f\"downvote. ip: {request.client.host}\")\n    vote_last_response(state, \"downvote\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef flag_last_response(state, model_selector, request: gr.Request):\n    logger.info(f\"flag. ip: {request.client.host}\")\n    vote_last_response(state, \"flag\", model_selector, request)\n    return (\"\",) + (disable_btn,) * 3\n\n\ndef regenerate(state, image_process_mode, request: gr.Request):\n    logger.info(f\"regenerate. ip: {request.client.host}\")\n    state.messages[-1][-1] = None\n    prev_human_msg = state.messages[-2]\n    if type(prev_human_msg[1]) in (tuple, list):\n        prev_human_msg[1] = (*prev_human_msg[1][:2], image_process_mode)\n    state.skip_next = False\n    return (state, state.to_gradio_chatbot(), \"\", None) + (disable_btn,) * 5\n\n\ndef clear_history(request: gr.Request):\n    logger.info(f\"clear_history. ip: {request.client.host}\")\n    state = default_conversation.copy()\n    return (state, state.to_gradio_chatbot(), \"\", None) + (disable_btn,) * 5\n\n\ndef add_text(state, text, image, image_process_mode, request: gr.Request):\n    logger.info(f\"add_text. ip: {request.client.host}. len: {len(text)}\")\n    if len(text) <= 0 and image is None:\n        state.skip_next = True\n        return (state, state.to_gradio_chatbot(), \"\", None) + (no_change_btn,) * 5\n    if args.moderate:\n        flagged = violates_moderation(text)\n        if flagged:\n            state.skip_next = True\n            return (state, state.to_gradio_chatbot(), moderation_msg, None) + (\n                no_change_btn,) * 5\n\n    text = text[:1536]  # Hard cut-off\n    if image is not None:\n        text = text[:1200]  # Hard cut-off for images\n        if '<image>' not in text:\n            # text = '<Image><image></Image>' + text\n            text = text + '\\n<image>'\n        text = (text, image, image_process_mode)\n        if len(state.get_images(return_pil=True)) > 0:\n            state = default_conversation.copy()\n    state.append_message(state.roles[0], text)\n    state.append_message(state.roles[1], None)\n    state.skip_next = False\n    return (state, state.to_gradio_chatbot(), \"\", None) + (disable_btn,) * 5\n\n\ndef http_bot(state, model_selector, temperature, top_p, max_new_tokens, request: gr.Request):\n    logger.info(f\"http_bot. ip: {request.client.host}\")\n    start_tstamp = time.time()\n    model_name = model_selector\n\n    if state.skip_next:\n        # This generate call is skipped due to invalid inputs\n        yield (state, state.to_gradio_chatbot()) + (no_change_btn,) * 5\n        return\n\n    if len(state.messages) == state.offset + 2:\n        # First round of conversation\n        if \"llava\" in model_name.lower():\n            if 'llama-2' in model_name.lower():\n                template_name = \"llava_llama_2\"\n            elif \"v1\" in model_name.lower():\n                if 'mmtag' in model_name.lower():\n                    template_name = \"v1_mmtag\"\n                elif 'plain' in model_name.lower() and 'finetune' not in model_name.lower():\n                    template_name = \"v1_mmtag\"\n                else:\n                    template_name = \"llava_v1\"\n            elif \"mpt\" in model_name.lower():\n                template_name = \"mpt\"\n            else:\n                if 'mmtag' in model_name.lower():\n                    template_name = \"v0_mmtag\"\n                elif 'plain' in model_name.lower() and 'finetune' not in model_name.lower():\n                    template_name = \"v0_mmtag\"\n                else:\n                    template_name = \"llava_v0\"\n        elif \"mpt\" in model_name:\n            template_name = \"mpt_text\"\n        elif \"llama-2\" in model_name:\n            template_name = \"llama_2\"\n        else:\n            template_name = \"vicuna_v1\"\n        new_state = conv_templates[template_name].copy()\n        new_state.append_message(new_state.roles[0], state.messages[-2][1])\n        new_state.append_message(new_state.roles[1], None)\n        state = new_state\n\n    # Query worker address\n    controller_url = args.controller_url\n    ret = requests.post(controller_url + \"/get_worker_address\",\n            json={\"model\": model_name})\n    worker_addr = ret.json()[\"address\"]\n    logger.info(f\"model_name: {model_name}, worker_addr: {worker_addr}\")\n\n    # No available worker\n    if worker_addr == \"\":\n        state.messages[-1][-1] = server_error_msg\n        yield (state, state.to_gradio_chatbot(), disable_btn, disable_btn, disable_btn, enable_btn, enable_btn)\n        return\n\n    # Construct prompt\n    prompt = state.get_prompt()\n\n    all_images = state.get_images(return_pil=True)\n    all_image_hash = [hashlib.md5(image.tobytes()).hexdigest() for image in all_images]\n    for image, hash in zip(all_images, all_image_hash):\n        t = datetime.datetime.now()\n        filename = os.path.join(LOGDIR, \"serve_images\", f\"{t.year}-{t.month:02d}-{t.day:02d}\", f\"{hash}.jpg\")\n        if not os.path.isfile(filename):\n            os.makedirs(os.path.dirname(filename), exist_ok=True)\n            image.save(filename)\n\n    # Make requests\n    pload = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"temperature\": float(temperature),\n        \"top_p\": float(top_p),\n        \"max_new_tokens\": min(int(max_new_tokens), 1536),\n        \"stop\": state.sep if state.sep_style in [SeparatorStyle.SINGLE, SeparatorStyle.MPT] else state.sep2,\n        \"images\": f'List of {len(state.get_images())} images: {all_image_hash}',\n    }\n    logger.info(f\"==== request ====\\n{pload}\")\n\n    pload['images'] = state.get_images()\n\n    state.messages[-1][-1] = \"▌\"\n    yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5\n\n    try:\n        # Stream output\n        response = requests.post(worker_addr + \"/worker_generate_stream\",\n            headers=headers, json=pload, stream=True, timeout=10)\n        for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n            if chunk:\n                data = json.loads(chunk.decode())\n                if data[\"error_code\"] == 0:\n                    output = data[\"text\"][len(prompt):].strip()\n                    state.messages[-1][-1] = output + \"▌\"\n                    yield (state, state.to_gradio_chatbot()) + (disable_btn,) * 5\n                else:\n                    output = data[\"text\"] + f\" (error_code: {data['error_code']})\"\n                    state.messages[-1][-1] = output\n                    yield (state, state.to_gradio_chatbot()) + (disable_btn, disable_btn, disable_btn, enable_btn, enable_btn)\n                    return\n                time.sleep(0.03)\n    except requests.exceptions.RequestException as e:\n        state.messages[-1][-1] = server_error_msg\n        yield (state, state.to_gradio_chatbot()) + (disable_btn, disable_btn, disable_btn, enable_btn, enable_btn)\n        return\n\n    state.messages[-1][-1] = state.messages[-1][-1][:-1]\n    yield (state, state.to_gradio_chatbot()) + (enable_btn,) * 5\n\n    finish_tstamp = time.time()\n    logger.info(f\"{output}\")\n\n    with open(get_conv_log_filename(), \"a\") as fout:\n        data = {\n            \"tstamp\": round(finish_tstamp, 4),\n            \"type\": \"chat\",\n            \"model\": model_name,\n            \"start\": round(start_tstamp, 4),\n            \"finish\": round(start_tstamp, 4),\n            \"state\": state.dict(),\n            \"images\": all_image_hash,\n            \"ip\": request.client.host,\n        }\n        fout.write(json.dumps(data) + \"\\n\")\n\ntitle_markdown = (\"\"\"\n# 🌋 LLaVA: Large Language and Vision Assistant\n[[Project Page](https://llava-vl.github.io)] [[Code](https://github.com/haotian-liu/LLaVA)] [[Model](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md)] | 📚 [[LLaVA](https://arxiv.org/abs/2304.08485)] [[LLaVA-v1.5](https://arxiv.org/abs/2310.03744)]\n\"\"\")\n\ntos_markdown = (\"\"\"\n### Terms of use\nBy using this service, users are required to agree to the following terms:\nThe service is a research preview intended for non-commercial use only. It only provides limited safety measures and may generate offensive content. It must not be used for any illegal, harmful, violent, racist, or sexual purposes. The service may collect user dialogue data for future research.\nPlease click the \"Flag\" button if you get any inappropriate answer! We will collect those to keep improving our moderator.\nFor an optimal experience, please use desktop computers for this demo, as mobile devices may compromise its quality.\n\"\"\")\n\n\nlearn_more_markdown = (\"\"\"\n### License\nThe service is a research preview intended for non-commercial use only, subject to the model [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.\n\"\"\")\n\nblock_css = \"\"\"\n\n#buttons button {\n    min-width: min(120px,100%);\n}\n\n\"\"\"\n\ndef build_demo(embed_mode):\n    textbox = gr.Textbox(show_label=False, placeholder=\"Enter text and press ENTER\", container=False)\n    with gr.Blocks(title=\"LLaVA\", theme=gr.themes.Default(), css=block_css) as demo:\n        state = gr.State()\n\n        if not embed_mode:\n            gr.Markdown(title_markdown)\n\n        with gr.Row():\n            with gr.Column(scale=3):\n                with gr.Row(elem_id=\"model_selector_row\"):\n                    model_selector = gr.Dropdown(\n                        choices=models,\n                        value=models[0] if len(models) > 0 else \"\",\n                        interactive=True,\n                        show_label=False,\n                        container=False)\n\n                imagebox = gr.Image(type=\"pil\")\n                image_process_mode = gr.Radio(\n                    [\"Crop\", \"Resize\", \"Pad\", \"Default\"],\n                    value=\"Default\",\n                    label=\"Preprocess for non-square image\", visible=False)\n\n                cur_dir = os.path.dirname(os.path.abspath(__file__))\n                gr.Examples(examples=[\n                    [f\"{cur_dir}/examples/extreme_ironing.jpg\", \"What is unusual about this image?\"],\n                    [f\"{cur_dir}/examples/waterview.jpg\", \"What are the things I should be cautious about when I visit here?\"],\n                ], inputs=[imagebox, textbox])\n\n                with gr.Accordion(\"Parameters\", open=False) as parameter_row:\n                    temperature = gr.Slider(minimum=0.0, maximum=1.0, value=0.2, step=0.1, interactive=True, label=\"Temperature\",)\n                    top_p = gr.Slider(minimum=0.0, maximum=1.0, value=0.7, step=0.1, interactive=True, label=\"Top P\",)\n                    max_output_tokens = gr.Slider(minimum=0, maximum=1024, value=512, step=64, interactive=True, label=\"Max output tokens\",)\n\n            with gr.Column(scale=8):\n                chatbot = gr.Chatbot(elem_id=\"chatbot\", label=\"LLaVA Chatbot\", height=550)\n                with gr.Row():\n                    with gr.Column(scale=8):\n                        textbox.render()\n                    with gr.Column(scale=1, min_width=50):\n                        submit_btn = gr.Button(value=\"Send\", variant=\"primary\")\n                with gr.Row(elem_id=\"buttons\") as button_row:\n                    upvote_btn = gr.Button(value=\"👍  Upvote\", interactive=False)\n                    downvote_btn = gr.Button(value=\"👎  Downvote\", interactive=False)\n                    flag_btn = gr.Button(value=\"⚠️  Flag\", interactive=False)\n                    #stop_btn = gr.Button(value=\"⏹️  Stop Generation\", interactive=False)\n                    regenerate_btn = gr.Button(value=\"🔄  Regenerate\", interactive=False)\n                    clear_btn = gr.Button(value=\"🗑️  Clear\", interactive=False)\n\n        if not embed_mode:\n            gr.Markdown(tos_markdown)\n            gr.Markdown(learn_more_markdown)\n        url_params = gr.JSON(visible=False)\n\n        # Register listeners\n        btn_list = [upvote_btn, downvote_btn, flag_btn, regenerate_btn, clear_btn]\n        upvote_btn.click(upvote_last_response,\n            [state, model_selector], [textbox, upvote_btn, downvote_btn, flag_btn])\n        downvote_btn.click(downvote_last_response,\n            [state, model_selector], [textbox, upvote_btn, downvote_btn, flag_btn])\n        flag_btn.click(flag_last_response,\n            [state, model_selector], [textbox, upvote_btn, downvote_btn, flag_btn])\n        regenerate_btn.click(regenerate, [state, image_process_mode],\n            [state, chatbot, textbox, imagebox] + btn_list).then(\n            http_bot, [state, model_selector, temperature, top_p, max_output_tokens],\n            [state, chatbot] + btn_list)\n        clear_btn.click(clear_history, None, [state, chatbot, textbox, imagebox] + btn_list)\n\n        textbox.submit(add_text, [state, textbox, imagebox, image_process_mode], [state, chatbot, textbox, imagebox] + btn_list\n            ).then(http_bot, [state, model_selector, temperature, top_p, max_output_tokens],\n                   [state, chatbot] + btn_list)\n        submit_btn.click(add_text, [state, textbox, imagebox, image_process_mode], [state, chatbot, textbox, imagebox] + btn_list\n            ).then(http_bot, [state, model_selector, temperature, top_p, max_output_tokens],\n                   [state, chatbot] + btn_list)\n\n        if args.model_list_mode == \"once\":\n            demo.load(load_demo, [url_params], [state, model_selector],\n                _js=get_window_url_params)\n        elif args.model_list_mode == \"reload\":\n            demo.load(load_demo_refresh_model_list, None, [state, model_selector])\n        else:\n            raise ValueError(f\"Unknown model list mode: {args.model_list_mode}\")\n\n    return demo\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int)\n    parser.add_argument(\"--controller-url\", type=str, default=\"http://localhost:21001\")\n    parser.add_argument(\"--concurrency-count\", type=int, default=10)\n    parser.add_argument(\"--model-list-mode\", type=str, default=\"once\",\n        choices=[\"once\", \"reload\"])\n    parser.add_argument(\"--share\", action=\"store_true\")\n    parser.add_argument(\"--moderate\", action=\"store_true\")\n    parser.add_argument(\"--embed\", action=\"store_true\")\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    models = get_model_list()\n\n    logger.info(args)\n    demo = build_demo(args.embed)\n    demo.queue(\n        concurrency_count=args.concurrency_count,\n        api_open=False\n    ).launch(\n        server_name=args.host,\n        server_port=args.port,\n        share=args.share\n    )"}
{"type": "source_file", "path": "graphgpt/__init__.py", "content": "__version__ = \"0.2.11\"\n"}
{"type": "source_file", "path": "graphgpt/build_instruct_arxiv_ds.py", "content": "import json\nimport random\nimport re\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch as th\nfrom torch_geometric.utils import subgraph\nfrom torch_geometric.data import NeighborSampler, Data\nfrom torch_geometric.loader import NeighborLoader\nfrom torch_geometric.utils import to_undirected, is_undirected\nfrom torch_geometric.utils import from_scipy_sparse_matrix\nimport logging\nimport copy\n\ndef get_logger(fname,): \n    # 1. 获取logger对象,这是日志记录的入口\n    logger = logging.getLogger('process logging')\n\n    # 2. 设置日志级别 \n    logger.setLevel(logging.INFO)\n\n    # 3. 创建日志文件handler\n    log_file = f'./log_dir/{fname}.log'\n    file_handler = logging.FileHandler(log_file)\n\n    # 4. 创建日志格式\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter) \n\n    # 5. 将handler添加到logger\n    logger.addHandler(file_handler)\n\n    console_handler = logging.StreamHandler() \n    console_handler.setFormatter(formatter)\n    logger.addHandler(console_handler)\n\n    # 6. 记录日志  \n    # logger.info('App started') \n    return logger\nlogger =  get_logger('process_train_arxiv')\n# 设置全局种子\nth.manual_seed(0)\nrandom.seed(123)  # 设置随机数种子，这里使用了整数123作为种子\ninstruct_list = {}\nbatch_id_list = [\n     [0, 27560], \n     [27560, 42005], \n     [42005, 56450],\n     [56450, 81915], \n     [81915, 97408],\n     [97408, 112900], \n     [112900, 137294], \n     [137294, 153319],\n     [153319, 169343]\n     ]\nfor batch_id in batch_id_list:\n    s_idx, e_idx = batch_id\n    with open(f'./tjb_cot_result/arxiv_cot_pred_{s_idx}_{e_idx}.json') as f:\n        instruct_item = json.load(f)\n        assert len(instruct_item) == (e_idx - s_idx)\n        ins_key = list(range(s_idx, e_idx))\n        instruct_list.update(zip(ins_key, instruct_item))\nprint(instruct_list[0]['instruction'])\nprint(instruct_list[0]['input'])\nprint(instruct_list[0]['output'])\n\ntra_df = pd.read_csv('./res_df/tra_df.csv')\nval_df = pd.read_csv('./res_df/val_df.csv')\ntst_df = pd.read_csv('./res_df/tst_df.csv')\nres_df = pd.concat([tra_df, val_df, tst_df])\n\nprint(res_df[res_df['node_idx'] == 0])\n\n'''\ninstruct dataset: \n[{'id': 'dsname_train_nodeidx', 'graph': [edge_row, edge_col], 'conversations': [{'from': 'human', 'value': 'human prompting.\\n<graph>'}, {'from': 'gpt', 'value': 'gpt response'}]}, {...}]\n\ngraph_token: <graph>\n'''\ndsname ='arxiv'\nsplit_type = 'test'\n\ninstruct_ds = []\n\n\n\ngraph_data = th.load('./tjb_cot_result/graph_data.pt')['arxiv']\nprint(graph_data.test_mask)\nindices = th.nonzero(graph_data.test_mask).reshape(-1)\nselect_idx = indices.tolist()\n# select_idx = [0, 1]\nprint(indices)\n\nprint(graph_data.edge_index)\ns = graph_data.edge_index.to_scipy() # 转换为稀疏张量\n\nedge_index, edge_attr = from_scipy_sparse_matrix(s) # 转换为COO格式\n# edge_index = th.stack([row, col], dim=0)\nprint(f'is undirected: {is_undirected(edge_index)}')\npyg_data = Data(edge_index = edge_index, edge_attr = edge_attr, num_nodes = graph_data.num_nodes)\n# Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243], train_mask=[169343], val_mask=[169343], test_mask=[169343], edge_index=[169343, 169343, nnz=2315598])\n\ndef cal_acc(output_item, input_item, nidx, topk): \n    title_pattern = r'Title: (.*?) \\n'\n    pattern = r\"cs\\.[A-Z]{2}\"\n    title = re.search(title_pattern, input_item).group(1)\n    # print(title)\n    \n    matches = list(set(re.findall(pattern, output_item))) # pred\n    sorted_matches = sorted(matches, key=lambda x: output_item.index(x))\n    # result_list[nidx] = matches\n    # print(res_df[res_df['node_idx'] == nidx])\n    assert res_df[res_df['node_idx'] == nidx]['title'].values[0] == title, '{} ! = {}'.format(res_df[res_df['node_idx'] == nidx]['title'].values[0], title)\n    true_item = 'cs.' + res_df[res_df['node_idx'] == nidx]['label'].values[0].upper()\n    # print(true_item)\n    # print(sorted_matches)\n    return true_item in sorted_matches[:topk]\n\nfor nidx in tqdm(select_idx): \n    center_node = nidx \n    num_hops = 2\n    num_neighbors = 10\n\n    # 邻居采样    \n    sampler = NeighborLoader(pyg_data, input_nodes=th.Tensor([center_node]).long(),\n                            num_neighbors=[num_neighbors] * num_hops, \n                            batch_size=1)\n\n    # 获取子图    \n    sampled_data = next(iter(sampler))\n    # for sampled_data in sampler:\n\n    try:\n        if cal_acc(instruct_list[nidx]['output'], instruct_list[nidx]['instruction'], nidx, topk=2) is False: \n            temp_dict = {}\n            temp_dict['id'] = f'{dsname}_{split_type}_{nidx}'\n            temp_dict['graph'] = {'node_idx':nidx, 'edge_index': sampled_data.edge_index.tolist(), 'node_list': sampled_data.n_id.tolist()}\n            conv_list = []\n            conv_temp = {}\n            conv_temp['from'] = 'human'\n            conv_temp['value'] = 'Given a citation graph: \\n<graph>\\nwhere the 0th node is the target paper, with the following information: \\n' + instruct_list[nidx]['instruction'] + instruct_list[nidx]['input']\n            conv_list.append(copy.deepcopy(conv_temp))\n\n            conv_temp['from'] = 'gpt'\n            conv_temp['value'] = instruct_list[nidx]['output']\n            conv_list.append(copy.deepcopy(conv_temp))\n\n            temp_dict['conversations'] = conv_list\n\n            instruct_ds.append(temp_dict)\n    except Exception as e:\n        logger.info(e)\n        \n\n\nlogger.info(f'total item: {len(instruct_ds)}')\nwith open(f'./instruct_ds/{dsname}_{split_type}_instruct_new.json', 'w') as f:\n    json.dump(instruct_ds, f)\n\n"}
{"type": "source_file", "path": "graphgpt/serve/model_worker_graph.py", "content": "\"\"\"\nA model worker executes the model.\n\"\"\"\nimport argparse\nimport asyncio\nimport json\nimport time\nimport threading\nimport uuid\n\nfrom fastapi import FastAPI, Request, BackgroundTasks\nfrom fastapi.responses import StreamingResponse\nimport requests\nimport torch\nimport uvicorn\nfrom functools import partial\n\nfrom graphgpt.constants import WORKER_HEART_BEAT_INTERVAL\nfrom graphgpt.utils import (build_logger, server_error_msg,\n    pretty_print_semaphore)\nfrom graphgpt.model.builder import load_pretrained_model\nfrom graphgpt.mm_utils import process_images, load_image_from_base64, tokenizer_image_token, KeywordsStoppingCriteria\nfrom graphgpt.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom transformers import TextIteratorStreamer\nfrom threading import Thread\n\n\nGB = 1 << 30\n\nworker_id = str(uuid.uuid4())[:6]\nlogger = build_logger(\"model_worker\", f\"model_worker_{worker_id}.log\")\nglobal_counter = 0\n\nmodel_semaphore = None\n\n\ndef heart_beat_worker(controller):\n\n    while True:\n        time.sleep(WORKER_HEART_BEAT_INTERVAL)\n        controller.send_heart_beat()\n\n\nclass ModelWorker:\n    def __init__(self, controller_addr, worker_addr,\n                 worker_id, no_register,\n                 model_path, model_base, model_name,\n                 load_8bit, load_4bit, device):\n        self.controller_addr = controller_addr\n        self.worker_addr = worker_addr\n        self.worker_id = worker_id\n        if model_path.endswith(\"/\"):\n            model_path = model_path[:-1]\n        if model_name is None:\n            model_paths = model_path.split(\"/\")\n            if model_paths[-1].startswith('checkpoint-'):\n                self.model_name = model_paths[-2] + \"_\" + model_paths[-1]\n            else:\n                self.model_name = model_paths[-1]\n        else:\n            self.model_name = model_name\n\n        self.device = device\n        logger.info(f\"Loading the model {self.model_name} on worker {worker_id} ...\")\n        self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\n            model_path, model_base, self.model_name, load_8bit, load_4bit, device=self.device)\n        self.is_multimodal = 'llava' in self.model_name.lower()\n\n        if not no_register:\n            self.register_to_controller()\n            self.heart_beat_thread = threading.Thread(\n                target=heart_beat_worker, args=(self,))\n            self.heart_beat_thread.start()\n\n    def register_to_controller(self):\n        logger.info(\"Register to controller\")\n\n        url = self.controller_addr + \"/register_worker\"\n        data = {\n            \"worker_name\": self.worker_addr,\n            \"check_heart_beat\": True,\n            \"worker_status\": self.get_status()\n        }\n        r = requests.post(url, json=data)\n        assert r.status_code == 200\n\n    def send_heart_beat(self):\n        logger.info(f\"Send heart beat. Models: {[self.model_name]}. \"\n                    f\"Semaphore: {pretty_print_semaphore(model_semaphore)}. \"\n                    f\"global_counter: {global_counter}\")\n\n        url = self.controller_addr + \"/receive_heart_beat\"\n\n        while True:\n            try:\n                ret = requests.post(url, json={\n                    \"worker_name\": self.worker_addr,\n                    \"queue_length\": self.get_queue_length()}, timeout=5)\n                exist = ret.json()[\"exist\"]\n                break\n            except requests.exceptions.RequestException as e:\n                logger.error(f\"heart beat error: {e}\")\n            time.sleep(5)\n\n        if not exist:\n            self.register_to_controller()\n\n    def get_queue_length(self):\n        if model_semaphore is None:\n            return 0\n        else:\n            return args.limit_model_concurrency - model_semaphore._value + (len(\n                model_semaphore._waiters) if model_semaphore._waiters is not None else 0)\n\n    def get_status(self):\n        return {\n            \"model_names\": [self.model_name],\n            \"speed\": 1,\n            \"queue_length\": self.get_queue_length(),\n        }\n\n    @torch.inference_mode()\n    def generate_stream(self, params):\n        tokenizer, model, image_processor = self.tokenizer, self.model, self.image_processor\n\n        prompt = params[\"prompt\"]\n        ori_prompt = prompt\n        images = params.get(\"images\", None)\n        num_image_tokens = 0\n        if images is not None and len(images) > 0 and self.is_multimodal:\n            if len(images) > 0:\n                if len(images) != prompt.count(DEFAULT_IMAGE_TOKEN):\n                    raise ValueError(\"Number of images does not match number of <image> tokens in prompt\")\n\n                images = [load_image_from_base64(image) for image in images]\n                images = process_images(images, image_processor, model.config)\n\n                if type(images) is list:\n                    images = [image.to(self.model.device, dtype=torch.float16) for image in images]\n                else:\n                    images = images.to(self.model.device, dtype=torch.float16)\n\n                replace_token = DEFAULT_IMAGE_TOKEN\n                if getattr(self.model.config, 'mm_use_im_start_end', False):\n                    replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n                prompt = prompt.replace(DEFAULT_IMAGE_TOKEN, replace_token)\n\n                num_image_tokens = prompt.count(replace_token) * model.get_vision_tower().num_patches\n            else:\n                images = None\n            image_args = {\"images\": images}\n        else:\n            images = None\n            image_args = {}\n\n        temperature = float(params.get(\"temperature\", 1.0))\n        top_p = float(params.get(\"top_p\", 1.0))\n        max_context_length = getattr(model.config, 'max_position_embeddings', 2048)\n        max_new_tokens = min(int(params.get(\"max_new_tokens\", 256)), 1024)\n        stop_str = params.get(\"stop\", None)\n        do_sample = True if temperature > 0.001 else False\n\n        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(self.device)\n        keywords = [stop_str]\n        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=15)\n\n        max_new_tokens = min(max_new_tokens, max_context_length - input_ids.shape[-1] - num_image_tokens)\n\n        if max_new_tokens < 1:\n            yield json.dumps({\"text\": ori_prompt + \"Exceeds max token length. Please start a new conversation, thanks.\", \"error_code\": 0}).encode() + b\"\\0\"\n            return\n\n        thread = Thread(target=model.generate, kwargs=dict(\n            inputs=input_ids,\n            do_sample=do_sample,\n            temperature=temperature,\n            top_p=top_p,\n            max_new_tokens=max_new_tokens,\n            streamer=streamer,\n            stopping_criteria=[stopping_criteria],\n            use_cache=True,\n            **image_args\n        ))\n        thread.start()\n\n        generated_text = ori_prompt\n        for new_text in streamer:\n            generated_text += new_text\n            if generated_text.endswith(stop_str):\n                generated_text = generated_text[:-len(stop_str)]\n            yield json.dumps({\"text\": generated_text, \"error_code\": 0}).encode() + b\"\\0\"\n\n    def generate_stream_gate(self, params):\n        try:\n            for x in self.generate_stream(params):\n                yield x\n        except ValueError as e:\n            print(\"Caught ValueError:\", e)\n            ret = {\n                \"text\": server_error_msg,\n                \"error_code\": 1,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n        except torch.cuda.CudaError as e:\n            print(\"Caught torch.cuda.CudaError:\", e)\n            ret = {\n                \"text\": server_error_msg,\n                \"error_code\": 1,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n        except Exception as e:\n            print(\"Caught Unknown Error\", e)\n            ret = {\n                \"text\": server_error_msg,\n                \"error_code\": 1,\n            }\n            yield json.dumps(ret).encode() + b\"\\0\"\n\n\napp = FastAPI()\n\n\ndef release_model_semaphore(fn=None):\n    model_semaphore.release()\n    if fn is not None:\n        fn()\n\n\n@app.post(\"/worker_generate_stream\")\nasync def generate_stream(request: Request):\n    global model_semaphore, global_counter\n    global_counter += 1\n    params = await request.json()\n\n    if model_semaphore is None:\n        model_semaphore = asyncio.Semaphore(args.limit_model_concurrency)\n    await model_semaphore.acquire()\n    worker.send_heart_beat()\n    generator = worker.generate_stream_gate(params)\n    background_tasks = BackgroundTasks()\n    background_tasks.add_task(partial(release_model_semaphore, fn=worker.send_heart_beat))\n    return StreamingResponse(generator, background=background_tasks)\n\n\n@app.post(\"/worker_get_status\")\nasync def get_status(request: Request):\n    return worker.get_status()\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=21002)\n    parser.add_argument(\"--worker-address\", type=str,\n        default=\"http://localhost:21002\")\n    parser.add_argument(\"--controller-address\", type=str,\n        default=\"http://localhost:21001\")\n    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--model-base\", type=str, default=None)\n    parser.add_argument(\"--model-name\", type=str)\n    parser.add_argument(\"--device\", type=str, default=\"cuda\")\n    parser.add_argument(\"--multi-modal\", action=\"store_true\", help=\"Multimodal mode is automatically detected with model name, please make sure `llava` is included in the model path.\")\n    parser.add_argument(\"--limit-model-concurrency\", type=int, default=5)\n    parser.add_argument(\"--stream-interval\", type=int, default=1)\n    parser.add_argument(\"--no-register\", action=\"store_true\")\n    parser.add_argument(\"--load-8bit\", action=\"store_true\")\n    parser.add_argument(\"--load-4bit\", action=\"store_true\")\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    if args.multi_modal:\n        logger.warning(\"Multimodal mode is automatically detected with model name, please make sure `llava` is included in the model path.\")\n\n    worker = ModelWorker(args.controller_address,\n                         args.worker_address,\n                         worker_id,\n                         args.no_register,\n                         args.model_path,\n                         args.model_base,\n                         args.model_name,\n                         args.load_8bit,\n                         args.load_4bit,\n                         args.device)\n    uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")"}
{"type": "source_file", "path": "graphgpt/model/apply_delta.py", "content": "\"\"\"\nApply the delta weights on top of a base model.\n\nUsage:\npython3 -m fastchat.model.apply_delta --base ~/model_weights/llama-7b --target ~/model_weights/vicuna-7b --delta lmsys/vicuna-7b-delta-v1.1\n\"\"\"\nimport argparse\nimport gc\nimport glob\nimport json\nimport os\nimport shutil\nimport tempfile\n\nfrom huggingface_hub import snapshot_download\nimport torch\nfrom torch import nn\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n\n\nGB = 1 << 30\n\n\ndef split_files(model_path, tmp_path, split_size):\n    if not os.path.exists(model_path):\n        model_path = snapshot_download(repo_id=model_path)\n    if not os.path.exists(tmp_path):\n        os.makedirs(tmp_path)\n\n    file_pattern = os.path.join(model_path, \"pytorch_model-*.bin\")\n    files = glob.glob(file_pattern)\n\n    part = 0\n    try:\n        for file_path in tqdm(files):\n            state_dict = torch.load(file_path)\n            new_state_dict = {}\n\n            current_size = 0\n            for name, param in state_dict.items():\n                param_size = param.numel() * param.element_size()\n\n                if current_size + param_size > split_size:\n                    new_file_name = f\"pytorch_model-{part}.bin\"\n                    new_file_path = os.path.join(tmp_path, new_file_name)\n                    torch.save(new_state_dict, new_file_path)\n                    current_size = 0\n                    new_state_dict = None\n                    gc.collect()\n                    new_state_dict = {}\n                    part += 1\n\n                new_state_dict[name] = param\n                current_size += param_size\n\n            new_file_name = f\"pytorch_model-{part}.bin\"\n            new_file_path = os.path.join(tmp_path, new_file_name)\n            torch.save(new_state_dict, new_file_path)\n            new_state_dict = None\n            gc.collect()\n            new_state_dict = {}\n            part += 1\n    except Exception as e:\n        print(f\"An error occurred during split_files: {e}\")\n        shutil.rmtree(tmp_path)\n        raise\n\n\ndef apply_delta_low_cpu_mem(base_model_path, target_model_path, delta_path):\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path, use_fast=False)\n    delta_config = AutoConfig.from_pretrained(delta_path)\n\n    if os.path.exists(target_model_path):\n        shutil.rmtree(target_model_path)\n    os.makedirs(target_model_path)\n\n    split_size = 4 * GB\n\n    with tempfile.TemporaryDirectory() as tmp_base_path, tempfile.TemporaryDirectory() as tmp_delta_path:\n        print(f\"Split files for the base model to {tmp_base_path}\")\n        split_files(base_model_path, tmp_base_path, split_size)\n        print(f\"Split files for the delta weights to {tmp_delta_path}\")\n        split_files(delta_path, tmp_delta_path, split_size)\n\n        base_pattern = os.path.join(tmp_base_path, \"pytorch_model-*.bin\")\n        base_files = glob.glob(base_pattern)\n        delta_pattern = os.path.join(tmp_delta_path, \"pytorch_model-*.bin\")\n        delta_files = glob.glob(delta_pattern)\n        delta_state_dict = torch.load(delta_files[0])\n\n        print(\"Applying the delta\")\n        weight_map = {}\n        total_size = 0\n\n        for i, base_file in tqdm(enumerate(base_files)):\n            state_dict = torch.load(base_file)\n            file_name = f\"pytorch_model-{i}.bin\"\n            for name, param in state_dict.items():\n                if name not in delta_state_dict:\n                    for delta_file in delta_files:\n                        delta_state_dict = torch.load(delta_file)\n                        gc.collect()\n                        if name in delta_state_dict:\n                            break\n\n                state_dict[name] += delta_state_dict[name]\n                weight_map[name] = file_name\n                total_size += param.numel() * param.element_size()\n                gc.collect()\n            torch.save(state_dict, os.path.join(target_model_path, file_name))\n\n        with open(\n            os.path.join(target_model_path, \"pytorch_model.bin.index.json\"), \"w\"\n        ) as f:\n            json.dump(\n                {\"weight_map\": weight_map, \"metadata\": {\"total_size\": total_size}}, f\n            )\n\n    print(f\"Saving the target model to {target_model_path}\")\n    delta_tokenizer.save_pretrained(target_model_path)\n    delta_config.save_pretrained(target_model_path)\n\n\ndef apply_delta(base_model_path, target_model_path, delta_path):\n    print(f\"Loading the delta weights from {delta_path}\")\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path, use_fast=False)\n    delta = AutoModelForCausalLM.from_pretrained(\n        delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n\n    print(f\"Loading the base model from {base_model_path}\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n\n    print(\"Applying the delta\")\n    for name, param in tqdm(base.state_dict().items(), desc=\"Applying delta\"):\n        assert name in delta.state_dict()\n        param.data += delta.state_dict()[name]\n\n    print(f\"Saving the target model to {target_model_path}\")\n    base.save_pretrained(target_model_path)\n    delta_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n    parser.add_argument(\n        \"--low-cpu-mem\",\n        action=\"store_true\",\n        help=\"Lower the cpu memory usage. This will split large files and use \"\n        \"disk as swap to reduce the memory usage below 10GB.\",\n    )\n    args = parser.parse_args()\n\n    if args.low_cpu_mem:\n        apply_delta_low_cpu_mem(\n            args.base_model_path, args.target_model_path, args.delta_path\n        )\n    else:\n        apply_delta(args.base_model_path, args.target_model_path, args.delta_path)\n"}
{"type": "source_file", "path": "graphgpt/eval/run_graphgpt.py", "content": "import argparse\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport os\nfrom graphgpt.conversation import conv_templates, SeparatorStyle\nfrom graphgpt.utils import disable_torch_init\nfrom transformers import CLIPVisionModel, CLIPImageProcessor, StoppingCriteria\nfrom graphgpt.model import *\nfrom graphgpt.model.utils import KeywordsStoppingCriteria\nfrom torch_geometric.data import Data\nimport json\nimport copy\n\nimport os\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom tqdm import tqdm\nimport json\nimport os.path as osp\n\nimport ray\n\n# os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n\nDEFAULT_GRAPH_TOKEN = \"<graph>\"\nDEFAULT_GRAPH_PATCH_TOKEN = \"<g_patch>\"\nDEFAULT_G_START_TOKEN = \"<g_start>\"\nDEFAULT_G_END_TOKEN = \"<g_end>\"\n\n\ndef load_graph(instruct_item, graph_data_path): \n    graph_data_all = torch.load(graph_data_path)\n    graph_dict = instruct_item['graph']\n    graph_edge_index = torch.Tensor(copy.deepcopy(graph_dict['edge_index'])).long()\n    graph_node_list = copy.deepcopy(graph_dict['node_list'])\n    target_node = copy.deepcopy(graph_dict['node_idx'])\n    graph_type = copy.deepcopy(instruct_item['id']).split('_')[0]\n    graph_node_rep = graph_data_all[graph_type].x[graph_node_list] ## \n    \n    cur_token_len = len(graph_node_rep)   # FIXME: 14 is hardcoded patch size\n\n    graph_ret = Data(graph_node = graph_node_rep, edge_index=graph_edge_index, target_node = torch.tensor([target_node]))\n\n    return {\n        'graph_data': graph_ret, \n        'graph_token_len': cur_token_len\n    }\n\n\ndef load_prompting_file(file_path): \n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return data\n\n# def prepare_query(instruct_item): \n\n\ndef run_eval(args, num_gpus):\n    # split question file into num_gpus files\n    prompt_file = load_prompting_file(args.prompting_file)\n    prompt_file = prompt_file[args.start_id:args.end_id]\n    chunk_size = len(prompt_file) // num_gpus\n    ans_handles = []\n    split_list = list(range(args.start_id, args.end_id, chunk_size))\n    idx_list = list(range(0, len(prompt_file), chunk_size))\n    if len(split_list) == num_gpus: \n        split_list.append(args.end_id)\n        idx_list.append(len(prompt_file))\n    elif len(split_list) == num_gpus + 1: \n        split_list[-1] = args.end_id\n        idx_list[-1] = len(prompt_file)\n    else: \n        raise ValueError('error in the number of list')\n\n    if osp.exists(args.output_res_path) is False: \n        os.mkdir(args.output_res_path)\n    \n    for idx in range(len(idx_list) - 1):\n        start_idx = idx_list[idx]\n        end_idx = idx_list[idx + 1]\n        \n        start_split = split_list[idx]\n        end_split = split_list[idx + 1]\n        ans_handles.append(\n            eval_model.remote(\n                args, prompt_file[start_idx:end_idx], start_split, end_split\n            )\n        )\n\n    ans_jsons = []\n    for ans_handle in ans_handles:\n        ans_jsons.extend(ray.get(ans_handle))\n\n    # with open(args.output_res_path, \"w\") as ans_file:\n    #     for line in ans_jsons:\n    #         ans_file.write(json.dumps(line) + \"\\n\")\n\n\n@ray.remote(num_gpus=1)\n@torch.inference_mode()\ndef eval_model(args, prompt_file, start_idx, end_idx):\n    # load prompting file\n    # prompt_file = load_prompting_file(args.prompting_file)\n\n\n    # Model\n    disable_torch_init()\n    # model_name = os.path.expanduser(args.model_name)\n    print('start loading')\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    print('finish loading')\n\n    print('start loading')\n    model = GraphLlamaForCausalLM.from_pretrained(args.model_name, torch_dtype=torch.float16, use_cache=True, low_cpu_mem_usage=True).cuda()\n    print('finish loading')\n\n    use_graph_start_end = getattr(model.config, \"use_graph_start_end\", False)\n    tokenizer.add_tokens([DEFAULT_GRAPH_PATCH_TOKEN], special_tokens=True)\n    if use_graph_start_end:\n        tokenizer.add_tokens([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN], special_tokens=True)\n\n    graph_tower = model.get_model().graph_tower\n    \n    # TODO: add graph tower\n    # if graph_tower.device.type == 'meta':\n    #     print('meta')\n    clip_graph, args_graph= load_model_pretrained(CLIP, './clip_gt_arxiv_pub')\n    graph_tower = graph_transformer(args_graph)\n    graph_tower = transfer_param_tograph(clip_graph, graph_tower)\n    \n    model.get_model().graph_tower = graph_tower.cuda()\n    # else:\n    #     print('other')\n    # print(next(graph_tower.parameters()).dtype)\n    graph_tower.to(device='cuda', dtype=torch.float16)\n    graph_config = graph_tower.config\n    graph_config.graph_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_GRAPH_PATCH_TOKEN])[0]\n    graph_config.use_graph_start_end = use_graph_start_end\n    if use_graph_start_end:\n        graph_config.graph_start_token, graph_config.graph_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN])\n    # TODO: add graph token len\n\n    res_data = []\n    print(f'total: {len(prompt_file)}')\n    for idx, instruct_item in tqdm(enumerate(prompt_file)):\n        # instruct_item = prompt_file[0]\n        # if idx >= 3: \n        #     break\n        graph_dict = load_graph(instruct_item, args.graph_data_path)\n        graph_token_len = graph_dict['graph_token_len']\n        graph_data = graph_dict['graph_data']\n\n        qs = instruct_item[\"conversations\"][0][\"value\"]\n        # if use_graph_start_end:\n        #     qs = qs + '\\n' + DEFAULT_G_START_TOKEN + DEFAULT_GRAPH_PATCH_TOKEN * graph_token_len + DEFAULT_G_END_TOKEN\n        # else:\n        #     qs = qs + '\\n' + DEFAULT_GRAPH_PATCH_TOKEN * graph_token_len\n\n        replace_token = DEFAULT_GRAPH_PATCH_TOKEN * graph_token_len\n        replace_token = DEFAULT_G_START_TOKEN + replace_token + DEFAULT_G_END_TOKEN\n        qs = qs.replace(DEFAULT_GRAPH_TOKEN, replace_token)\n\n        # if \"v1\" in args.model_name.lower():\n        #     conv_mode = \"graphchat_v1\"\n        # else: \n        #     raise ValueError('Don\\'t support this model')\n        conv_mode = \"graphchat_v1\"\n\n        if args.conv_mode is not None and conv_mode != args.conv_mode:\n            print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\n        else:\n            args.conv_mode = conv_mode\n\n        conv = conv_templates[args.conv_mode].copy()\n        conv.append_message(conv.roles[0], qs)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n        inputs = tokenizer([prompt])\n\n        \n\n        input_ids = torch.as_tensor(inputs.input_ids).cuda()\n\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n        keywords = [stop_str]\n        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n        graph_data.graph_node = graph_data.graph_node.to(torch.float16)\n        # graph_data.edge_index = graph_data.edge_index.to(torch.float16)\n\n        with torch.inference_mode():\n            output_ids = model.generate(\n                input_ids,\n                graph_data=graph_data.cuda(),\n                do_sample=True,\n                temperature=0.2,\n                max_new_tokens=1024,\n                stopping_criteria=[stopping_criteria])\n\n        input_token_len = input_ids.shape[1]\n        n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\n        if n_diff_input_output > 0:\n            print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')\n        outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\n        outputs = outputs.strip()\n        if outputs.endswith(stop_str):\n            outputs = outputs[:-len(stop_str)]\n        outputs = outputs.strip()\n        # print(outputs)\n\n        res_data.append({\"id\": instruct_item[\"id\"], \"node_idx\": instruct_item[\"graph\"][\"node_idx\"], \"res\": outputs}.copy())\n        with open(osp.join(args.output_res_path, 'arxiv_test_res_{}_{}.json'.format(start_idx, end_idx)), \"w\") as fout:\n            json.dump(res_data, fout, indent=4)\n    return res_data\n    # with open(args.output_res_path, \"w\") as fout:\n    #     json.dump(res_data, fout, indent=4)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\n    # parser.add_argument(\"--image-file\", type=str, required=True)\n    # parser.add_argument(\"--query\", type=str, required=True)\n    parser.add_argument(\"--prompting_file\", type=str, default=None)\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\n    parser.add_argument(\"--graph_data_path\", type=str, default=None)\n\n    parser.add_argument(\"--output_res_path\", type=str, default=None)\n    parser.add_argument(\"--num_gpus\", type=int, default=4)\n\n    parser.add_argument(\"--start_id\", type=int, default=0)\n    parser.add_argument(\"--end_id\", type=int, default=20567)\n\n    args = parser.parse_args()\n\n    # eval_model(args)\n\n    ray.init()\n    run_eval(args, args.num_gpus)\n\n\n# protobuf             4.22.3"}
{"type": "source_file", "path": "graphgpt/model/GraphLlama.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, \\\n                         LlamaConfig, LlamaModel, LlamaForCausalLM, \\\n                         CLIPVisionModel, CLIPImageProcessor\n\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n\nfrom graphgpt.model.graph_layers import MPNN, GNN, CLIP, graph_transformer\nfrom torch_geometric.data import Data\nimport json\nimport os.path as osp\nimport glob\n\nDEFAULT_GRAPH_TOKEN = \"<graph>\"\nDEFAULT_GRAPH_PATCH_TOKEN = \"<g_patch>\"\nDEFAULT_G_START_TOKEN = \"<g_start>\"\nDEFAULT_G_END_TOKEN = \"<g_end>\"\n\n\n\n\nclass GraphLlamaConfig(LlamaConfig):\n    model_type = \"GraphLlama\"\n\nclass GraphPretrainConfig:\n    def __init__(self, dictionary):\n        for key, value in dictionary.items():\n            setattr(self, key, value)\n\ndef load_model_pretrained(model_name, pretrain_model_path): \n    # load conig json\n    \n    assert osp.exists(osp.join(pretrain_model_path, 'config.json')), 'config.json missing'\n    with open(osp.join(pretrain_model_path, 'config.json'), 'r') as f:\n        config_dict = json.load(f)\n    args = GraphPretrainConfig(config_dict)\n    model = model_name(args)\n    pkl_files = glob.glob(osp.join(pretrain_model_path, '*.pkl'))\n    state_dict = torch.load(pkl_files[0])\n    # print(state_dict.keys())\n    if 'logit_scale' in state_dict.keys(): \n        state_dict.pop('logit_scale')\n    print('loading graph pre train model')\n    model.load_state_dict(state_dict)\n\n\n    return model, args\ndef transfer_param_tograph(clip_graph, gnn):\n    \n    print(clip_graph)\n    gnn_state_dict = clip_graph.gnn.state_dict()\n    gnn.load_state_dict(gnn_state_dict)\n    return gnn\n\n\nclass GraphLlamaModel(LlamaModel):\n    config_class = GraphLlamaConfig\n\n    def __init__(self, config: LlamaConfig):\n        super(GraphLlamaModel, self).__init__(config)\n\n        if hasattr(config, \"graph_tower\"):\n            # HACK: for FSDP\n            # self.vision_tower = [CLIPVisionModel.from_pretrained(config.graph_tower)]\n            # self.arxiv_projector = nn.Linear(config.graph_hidden_size, config.hidden_size)\n            if config.graph_tower == 'MPNN': \n                self.graph_tower = MPNN(in_channels = config.graph_hidden_size, hidden_channels = config.graph_hidden_size * 2, out_channels = config.graph_hidden_size, dropout = 0.1, num_layers = 2, if_param = False)\n            elif config.graph_tower == \"clip_gcn_arxiv\": \n\n                clip_graph, args= load_model_pretrained(CLIP, config.pretrain_graph_model_path)\n                self.graph_tower = GNN(args)\n                self.graph_tower = transfer_param_tograph(clip_graph, self.graph_tower)\n            elif config.graph_tower == \"clip_gt\":\n                clip_graph, args= load_model_pretrained(CLIP, config.pretrain_graph_model_path) \n                self.graph_tower = graph_transformer(args)\n                self.graph_tower = transfer_param_tograph(clip_graph, self.graph_tower)\n            elif config.graph_tower == \"clip_gt_arxiv\": \n                clip_graph, args= load_model_pretrained(CLIP, config.pretrain_graph_model_path) \n                self.graph_tower = graph_transformer(args)\n                self.graph_tower = transfer_param_tograph(clip_graph, self.graph_tower)\n            elif config.graph_tower == \"clip_gt_arxiv_pub\": \n                clip_graph, args= load_model_pretrained(CLIP, config.pretrain_graph_model_path) \n                self.graph_tower = graph_transformer(args)\n                self.graph_tower = transfer_param_tograph(clip_graph, self.graph_tower)\n\n            \n\n            # self.vision_tower = CLIPVisionModel.from_pretrained(config.mm_vision_tower)\n\n        if hasattr(config, \"use_graph_proj\"):\n            self.graph_projector = nn.Linear(config.graph_hidden_size, config.hidden_size)\n\n    def get_graph_tower(self):\n        graph_tower = getattr(self, 'graph_tower', None)\n        if type(graph_tower) is list:\n            graph_tower = graph_tower[0]\n        return graph_tower\n\n    def initialize_graph_modules(self, graph_tower, graph_select_layer,\n                                  pretrain_graph_mlp_adapter=None, fsdp=None): # TODO: modify this function\n        self.config.graph_tower = graph_tower\n\n\n        if not hasattr(self, 'graph_tower'):\n            if self.config.graph_tower == 'MPNN': \n                graph_tower = MPNN(in_channels = self.config.graph_hidden_size, hidden_channels = self.config.graph_hidden_size * 2, out_channels = self.config.graph_hidden_size, dropout = 0.1, num_layers = 2, if_param = False)\n            elif self.config.graph_tower == \"clip_gcn_arxiv\": \n\n                clip_graph, args= load_model_pretrained(CLIP, self.config.pretrain_graph_model_path)\n                graph_tower = GNN(args)\n                graph_tower = transfer_param_tograph(clip_graph, graph_tower)\n            elif self.config.graph_tower == \"clip_gt\":\n                clip_graph, args= load_model_pretrained(CLIP, self.config.pretrain_graph_model_path) \n                graph_tower = graph_transformer(args)\n                graph_tower = transfer_param_tograph(clip_graph, graph_tower)\n            # graph_tower = MPNN(in_channels = self.config.graph_hidden_size, hidden_channels = self.config.graph_hidden_size * 2, out_channels = self.config.graph_hidden_size, dropout = 0.1, num_layers = 2)\n            elif self.config.graph_tower == \"clip_gt_arxiv\":\n                clip_graph, args= load_model_pretrained(CLIP, self.config.pretrain_graph_model_path) \n                graph_tower = graph_transformer(args)\n                graph_tower = transfer_param_tograph(clip_graph, graph_tower)\n            elif self.config.graph_tower == \"clip_gt_arxiv_pub\":\n                clip_graph, args= load_model_pretrained(CLIP, self.config.pretrain_graph_model_path) \n                graph_tower = graph_transformer(args)\n                graph_tower = transfer_param_tograph(clip_graph, graph_tower)\n        else:\n            graph_tower = self.graph_tower\n        graph_tower.requires_grad_(False)\n\n        if fsdp is not None and len(fsdp) > 0:\n            self.graph_tower = [graph_tower]\n        else:\n            self.graph_tower = graph_tower\n\n        \n\n        self.config.use_graph_proj = True\n        self.config.graph_select_layer = graph_select_layer\n\n        if not hasattr(self, 'graph_projector'):\n            self.graph_projector = nn.Linear(self.config.graph_hidden_size, self.config.hidden_size)\n\n        if pretrain_graph_mlp_adapter is not None:\n            graph_projector_weights = torch.load(pretrain_graph_mlp_adapter, map_location='cpu')\n            self.graph_projector.load_state_dict({k.split('.')[-1]: v for k, v in graph_projector_weights.items()})\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        # graph_node_reps: Optional[torch.FloatTensor] = None,\n        # edge_index_reps: Optional[torch.FloatTensor] = None,\n        graph_data: Optional[Data] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n\n        # HACK: replace back original embeddings for LLaVA pretraining\n        orig_embeds_params = getattr(self, 'orig_embeds_params', None)\n        # if orig_embeds_params is not None:\n        #     orig_embeds_params = orig_embeds_params[0]\n        #     with torch.no_grad():\n        #         self.get_input_embeddings().weight.data[:-2] = orig_embeds_params[:-2].data\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        graph_tower = self.get_graph_tower()\n        if graph_tower is not None and (input_ids.shape[1] != 1 or self.training) and graph_data is not None:\n            # TODO: this is a modified multimodal LLM -- Haotian Liu\n            with torch.no_grad():\n                if type(graph_data) is list:\n                    # variable length images\n                    graph_node_features = []\n                    if type(graph_data[0]) is Data:\n                        for g in graph_data:\n                            # print(g)\n                            node_forward_out = graph_tower(g)\n                            graph_node_features.append(node_forward_out)\n                    elif type(graph_data[0]) is dict:\n                        for g_dict in graph_data:\n                            node_forward_out_1 = graph_tower(g_dict['graph_1'])\n                            node_forward_out_2 = graph_tower(g_dict['graph_2'])\n                            graph_node_features.append(node_forward_out_1)\n                            graph_node_features.append(node_forward_out_2)\n                else:\n                    raise ValueError(f'graph_node_reps is expected to be a list but got {type(graph_data)}')\n            if type(graph_data) is list:\n                # if type(graph_node_features[0]) is not dict:\n                graph_node_features = [self.graph_projector(node_feature) for node_feature in graph_node_features]\n                # else: \n                #     graph_node_features = [{'graph_1': self.graph_projector(node_feature['graph_1']), 'graph_2': self.graph_projector(node_feature['graph_2'])} for node_feature in graph_node_features]\n            else:\n                raise ValueError(f'graph_node_reps is expected to be a list but got {type(graph_data)}')\n            dummy_graph_features = torch.zeros(256, 128, device=inputs_embeds.device, dtype=inputs_embeds.dtype)\n            dummy_graph_features = self.graph_projector(dummy_graph_features)\n\n            new_input_embeds = []\n            cur_graph_idx = 0\n            for cur_input_ids, cur_input_embeds in zip(input_ids, inputs_embeds):\n                if (cur_input_ids == graph_tower.config.graph_patch_token).sum() == 0:\n                    # multimodal LLM, but the current sample is not multimodal\n                    cur_input_embeds = cur_input_embeds + (0. * dummy_graph_features).sum()\n                    new_input_embeds.append(cur_input_embeds)\n                    cur_graph_idx += 1\n                    continue\n                if graph_tower.config.use_graph_start_end:\n                    cur_graph_features = graph_node_features[cur_graph_idx]\n                    num_patches = cur_graph_features.shape[0]\n                    if (cur_input_ids == graph_tower.config.graph_start_token).sum() != (cur_input_ids == graph_tower.config.graph_end_token).sum():\n                        raise ValueError(\"The number of graph start tokens and graph end tokens should be the same.\")\n                    graph_start_tokens = torch.where(cur_input_ids == graph_tower.config.graph_start_token)[0]\n                    # print(graph_start_tokens)\n                    for graph_start_token_pos in graph_start_tokens:\n                        cur_graph_features = graph_node_features[cur_graph_idx].to(device=cur_input_embeds.device)\n                        num_patches = cur_graph_features.shape[0]\n                        if cur_input_ids[graph_start_token_pos + num_patches + 1] != graph_tower.config.graph_end_token:\n                            raise ValueError(\"The graph end token should follow the graph start token.\")\n                        if orig_embeds_params is not None:\n                            cur_new_input_embeds = torch.cat((cur_input_embeds[:graph_start_token_pos].detach(), cur_input_embeds[graph_start_token_pos:graph_start_token_pos+1], cur_graph_features, cur_input_embeds[graph_start_token_pos + num_patches + 1:graph_start_token_pos + num_patches + 2], cur_input_embeds[graph_start_token_pos + num_patches + 2:].detach()), dim=0)\n                        else:\n                            cur_new_input_embeds = torch.cat((cur_input_embeds[:graph_start_token_pos+1], cur_graph_features, cur_input_embeds[graph_start_token_pos + num_patches + 1:]), dim=0)\n                        cur_graph_idx += 1\n                    new_input_embeds.append(cur_new_input_embeds)\n                else:\n                    cur_graph_features = graph_node_features[cur_graph_idx]\n                    num_patches = cur_graph_features.shape[0]\n                    if (cur_input_ids == graph_tower.config.graph_patch_token).sum() != num_patches:\n                        raise ValueError(\"The number of graph patch tokens should be the same as the number of graph patches.\")\n                    masked_indices = torch.where(cur_input_ids == graph_tower.config.graph_patch_token)[0]\n                    mask_index_start = masked_indices[0]\n                    if (masked_indices != torch.arange(mask_index_start, mask_index_start+num_patches, device=masked_indices.device, dtype=masked_indices.dtype)).any():\n                        raise ValueError(\"The graph patch tokens should be consecutive.\")\n                    if orig_embeds_params is not None:\n                        cur_new_input_embeds = torch.cat((cur_input_embeds[:mask_index_start].detach(), cur_graph_features, cur_input_embeds[mask_index_start+num_patches:].detach()), dim=0)\n                    else:\n                        cur_new_input_embeds = torch.cat((cur_input_embeds[:mask_index_start], cur_graph_features, cur_input_embeds[mask_index_start+num_patches:]), dim=0)\n                    new_input_embeds.append(cur_new_input_embeds)\n                    cur_graph_idx += 1\n\n            # print(cur_graph_idx)\n            # print(len(graph_node_features))\n            assert cur_graph_idx == len(graph_node_features)\n            inputs_embeds = torch.stack(new_input_embeds, dim=0)\n\n        return super(GraphLlamaModel, self).forward(\n            input_ids=None, attention_mask=attention_mask, past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds, use_cache=use_cache,\n            output_attentions=output_attentions, output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\n\n\nclass GraphLlamaForCausalLM(LlamaForCausalLM):\n    config_class = GraphLlamaConfig\n\n    def __init__(self, config):\n        super(LlamaForCausalLM, self).__init__(config)\n        self.model = GraphLlamaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def get_graph_tower(self):\n        return self.get_model().get_graph_tower()\n\n    def get_vision_tower(self):\n        model = self.get_model()\n        graph_tower = model.graph_tower\n        if type(graph_tower) is list:\n            graph_tower = graph_tower[0]\n        return graph_tower\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        # graph_node_reps: Optional[torch.FloatTensor] = None,\n        # edge_index_reps: Optional[torch.FloatTensor] = None,\n        graph_data: Optional[Data] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            # graph_node_reps=graph_node_reps, \n            # edge_index_reps=edge_index_reps\n            graph_data = graph_data\n        )\n\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model/pipeline parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n                \"graph_data\": [kwargs.get(\"graph_data\", None)],\n                # \"edge_index_reps\": kwargs.get(\"edge_index_reps\", None),\n            }\n        )\n        return model_inputs\n\n    def initialize_graph_tokenizer(self, use_graph_start_end, tokenizer, device,\n                                    tune_graph_mlp_adapter=False, pretrain_graph_mlp_adapter=None):\n        vision_config = self.get_graph_tower().config\n        vision_config.use_graph_start_end = use_graph_start_end\n        tokenizer.add_tokens([DEFAULT_GRAPH_PATCH_TOKEN], special_tokens=True)\n        self.resize_token_embeddings(len(tokenizer))\n\n        if use_graph_start_end:\n            num_new_tokens = tokenizer.add_tokens([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n            vision_config.graph_start_token, vision_config.graph_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN])\n\n            if num_new_tokens > 0:\n                input_embeddings = self.get_input_embeddings().weight.data\n                output_embeddings = self.get_output_embeddings().weight.data\n\n                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n                    dim=0, keepdim=True)\n                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n                    dim=0, keepdim=True)\n\n                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\n            if tune_graph_mlp_adapter:\n                self.get_model().orig_embeds_params = [self.get_input_embeddings().weight.data.clone().to(device=device)]\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = True\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n\n            if pretrain_graph_mlp_adapter:\n                mm_projector_weights = torch.load(pretrain_graph_mlp_adapter, map_location='cpu')\n                embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\n                assert num_new_tokens == 2\n                if input_embeddings.shape == embed_tokens_weight.shape:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n                elif embed_tokens_weight.shape[0] == num_new_tokens:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n                else:\n                    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n\n        vision_config.graph_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_GRAPH_PATCH_TOKEN])[0]\n\nAutoConfig.register(\"GraphLlama\", GraphLlamaConfig)\nAutoModelForCausalLM.register(GraphLlamaConfig, GraphLlamaForCausalLM)\n"}
{"type": "source_file", "path": "graphgpt/eval/run_vicuna.py", "content": "import argparse\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport os\nfrom graphgpt.conversation import conv_templates, SeparatorStyle\nfrom graphgpt.utils import disable_torch_init\nfrom transformers import CLIPVisionModel, CLIPImageProcessor, StoppingCriteria\nfrom graphgpt.model import *\nfrom graphgpt.model.utils import KeywordsStoppingCriteria\nfrom torch_geometric.data import Data\nimport json\nimport copy\nimport re\n\nimport os\nimport os.path as osp\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom tqdm import tqdm\nimport json\n\nimport ray\n\n# os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n\nDEFAULT_GRAPH_TOKEN = \"<graph>\"\nDEFAULT_GRAPH_PATCH_TOKEN = \"<g_patch>\"\nDEFAULT_G_START_TOKEN = \"<g_start>\"\nDEFAULT_G_END_TOKEN = \"<g_end>\"\n\n\ndef load_graph(instruct_item, graph_data_path): \n    graph_data_all = torch.load(graph_data_path)\n    graph_dict = instruct_item['graph']\n    graph_edge_index = torch.Tensor(copy.deepcopy(graph_dict['edge_index'])).long()\n    graph_node_list = copy.deepcopy(graph_dict['node_list'])\n    target_node = copy.deepcopy(graph_dict['node_idx'])\n    graph_type = copy.deepcopy(instruct_item['id']).split('_')[0]\n    graph_node_rep = graph_data_all[graph_type].x[graph_node_list] ## \n    \n    cur_token_len = len(graph_node_rep)   # FIXME: 14 is hardcoded patch size\n\n    graph_ret = Data(graph_node = graph_node_rep, edge_index=graph_edge_index, target_node = torch.tensor([target_node]))\n\n    return {\n        'graph_data': graph_ret, \n        'graph_token_len': cur_token_len\n    }\n\n\ndef load_prompting_file(file_path): \n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return data\n\n# def prepare_query(instruct_item): \n\n\ndef run_eval(args, num_gpus):\n    # split question file into num_gpus files\n    prompt_file = load_prompting_file(args.prompting_file)\n    prompt_file = prompt_file[args.start_id:args.end_id]\n    chunk_size = len(prompt_file) // num_gpus\n    ans_handles = []\n    split_list = list(range(args.start_id, args.end_id, chunk_size))\n    idx_list = list(range(0, len(prompt_file), chunk_size))\n    if len(split_list) == num_gpus: \n        split_list.append(args.end_id)\n        idx_list.append(len(prompt_file))\n    elif len(split_list) == num_gpus + 1: \n        split_list[-1] = args.end_id\n        idx_list[-1] = len(prompt_file)\n    else: \n        raise ValueError('error in the number of list')\n    \n    if osp.exists(args.output_res_path) is False: \n        os.mkdir(args.output_res_path)\n\n    for idx in range(len(idx_list) - 1):\n        start_idx = idx_list[idx]\n        end_idx = idx_list[idx + 1]\n        \n        start_split = split_list[idx]\n        end_split = split_list[idx + 1]\n        ans_handles.append(\n            eval_model.remote(\n                args, prompt_file[start_idx:end_idx], start_split, end_split\n            )\n        )\n\n    ans_jsons = []\n\n    for ans_handle in ans_handles:\n        ans_jsons.extend(ray.get(ans_handle))\n\n    # with open(args.output_res_path, \"w\") as ans_file:\n    #     for line in ans_jsons:\n    #         ans_file.write(json.dumps(line) + \"\\n\")\n\n\n@ray.remote(num_gpus=1)\n@torch.inference_mode()\ndef eval_model(args, prompt_file, start_id, end_id):\n    # load prompting file\n    # prompt_file = load_prompting_file(args.prompting_file)\n\n\n    # Model\n\n    disable_torch_init()\n    # model_name = os.path.expanduser(args.model_name)\n    print('start loading')\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    print('finish loading')\n\n    print('start loading')\n    model = AutoModelForCausalLM.from_pretrained(args.model_name, torch_dtype=torch.float16, use_cache=True, low_cpu_mem_usage=True).cuda()\n    print('finish loading')\n\n    \n    print(f'total: {len(prompt_file)}')\n    res_file = osp.join(args.output_res_path, f'arxiv_test_res_{start_id}_{end_id}.json')\n    if osp.exists(res_file): \n        with open(res_file, 'r') as f:\n            res_data = json.load(f)\n        ready_len = len(res_data)\n        if ready_len == (end_id - start_id): \n            return res_data\n    else: \n        res_data = []\n        ready_len = 0\n        print('*'*10, 'create res file', '*'*10)\n        with open(res_file, 'w') as f:\n            json.dump(res_data, f)\n            \n        \n\n    for idx, instruct_item in tqdm(enumerate(prompt_file[ready_len:])):\n        # instruct_item = prompt_file[0]\n        # if idx >= 3: \n        #     break\n\n        qs = instruct_item[\"conversations\"][0][\"value\"]\n        \n        pattern = r'<graph>'\n\n        qs = re.sub(pattern, '', qs)\n\n        conv_mode = \"vicuna_v1_1\"\n\n        if args.conv_mode is not None and conv_mode != args.conv_mode:\n            print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\n        else:\n            args.conv_mode = conv_mode\n\n        conv = conv_templates[args.conv_mode].copy()\n        conv.append_message(conv.roles[0], qs)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n        input_ids = tokenizer([prompt]).input_ids\n\n        output_ids = model.generate(\n            torch.as_tensor(input_ids).cuda(),\n            do_sample=True,\n            temperature=0.7,\n            max_new_tokens=1024,\n        )\n        output_ids = output_ids[0][len(input_ids[0]) :]\n        outputs = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n\n        res_data.append({\"id\": instruct_item[\"id\"], \"node_idx\": instruct_item[\"graph\"][\"node_idx\"], \"res\": outputs}.copy())\n        with open(res_file, 'w') as f:\n            json.dump(res_data, f)\n    return res_data\n    # with open(args.output_res_path, \"w\") as fout:\n    #     json.dump(res_data, fout, indent=4)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\n    # parser.add_argument(\"--image-file\", type=str, required=True)\n    # parser.add_argument(\"--query\", type=str, required=True)\n    parser.add_argument(\"--prompting_file\", type=str, default=None)\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\n    parser.add_argument(\"--graph_data_path\", type=str, default=None)\n\n    parser.add_argument(\"--output_res_path\", type=str, default=None)\n    parser.add_argument(\"--num_gpus\", type=int, default=2)\n\n    parser.add_argument(\"--start_id\", type=int, default=0)\n    parser.add_argument(\"--end_id\", type=int, default=20567)\n\n    args = parser.parse_args()\n\n    # eval_model(args)\n\n    ray.init()\n    run_eval(args, args.num_gpus)\n\n\n# protobuf             4.22.3"}
{"type": "source_file", "path": "graphgpt/eval/run_graphgpt_LP.py", "content": "import argparse\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport os\nfrom graphgpt.conversation import conv_templates, SeparatorStyle\nfrom graphgpt.utils import disable_torch_init\nfrom transformers import CLIPVisionModel, CLIPImageProcessor, StoppingCriteria\nfrom graphgpt.model import *\nfrom graphgpt.model.utils import KeywordsStoppingCriteria\nfrom torch_geometric.data import Data\nimport json\nimport copy\nimport random\n\nimport os\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom tqdm import tqdm\nimport json\nimport os.path as osp\n\nimport ray\n\n# os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n\nDEFAULT_GRAPH_TOKEN = \"<graph>\"\nDEFAULT_GRAPH_PATCH_TOKEN = \"<g_patch>\"\nDEFAULT_G_START_TOKEN = \"<g_start>\"\nDEFAULT_G_END_TOKEN = \"<g_end>\"\n\n\ndef load_graph(instruct_item, graph_data_path): \n    graph_data_all = torch.load(graph_data_path)\n    graph_dict = instruct_item['graph']\n    graph_edge_index = torch.Tensor(copy.deepcopy(graph_dict['edge_index'])).long()\n    graph_node_list = copy.deepcopy(graph_dict['node_list'])\n    target_node = copy.deepcopy(graph_dict['node_idx'])\n    graph_type = copy.deepcopy(instruct_item['id']).split('_')[0]\n    graph_node_rep = graph_data_all[graph_type].x[graph_node_list] ## \n    \n    cur_token_len = len(graph_node_rep)   # FIXME: 14 is hardcoded patch size\n\n    graph_ret = Data(graph_node = graph_node_rep, edge_index=graph_edge_index, target_node = torch.tensor([target_node]))\n\n    return {\n        'graph_data': graph_ret, \n        'graph_token_len': cur_token_len\n    }\n\ndef load_graph_LP(instruct_item, graph_data_path): \n    graph_data_all = torch.load(graph_data_path)\n    graph_dict = instruct_item['graph']\n    graph_edge_index_1 = torch.Tensor(copy.deepcopy(graph_dict['edge_index_1'])).long()\n    graph_node_list_1 = copy.deepcopy(graph_dict['node_list_1'])\n    target_node_1 = copy.deepcopy(graph_dict['node_idx_1'])\n    graph_type = copy.deepcopy(instruct_item['id']).split('_')[0]\n    graph_node_rep_1 = graph_data_all[graph_type].x[graph_node_list_1] ## \n    \n    cur_token_len_1 = len(graph_node_rep_1)   # FIXME: 14 is hardcoded patch size\n\n    graph_edge_index_2 = torch.Tensor(copy.deepcopy(graph_dict['edge_index_2'])).long()\n    graph_node_list_2 = copy.deepcopy(graph_dict['node_list_2'])\n    target_node_2 = copy.deepcopy(graph_dict['node_idx_2'])\n    graph_node_rep_2 = graph_data_all[graph_type].x[graph_node_list_2] ## \n    \n    cur_token_len_2 = len(graph_node_rep_2)   # FIXME: 14 is hardcoded patch \n\n    graph_ret = {\n        'graph_1': Data(graph_node = graph_node_rep_1, edge_index=graph_edge_index_1, target_node = torch.tensor([target_node_1])), \n        'graph_2': Data(graph_node = graph_node_rep_2, edge_index=graph_edge_index_2, target_node = torch.tensor([target_node_2]))\n        }\n\n    return {\n        'graph_data': graph_ret, \n        'graph_token_len_1': cur_token_len_1, \n        'graph_token_len_2': cur_token_len_2 \n    }\n\n\ndef load_prompting_file(file_path): \n    with open(file_path, 'r') as f:\n        data = json.load(f)\n    return data\n\n# def prepare_query(instruct_item): \n\n\ndef run_eval(args, num_gpus):\n    # split question file into num_gpus files\n    prompt_file = load_prompting_file(args.prompting_file)\n    if args.is_shuffle: \n        print('shuffle the prompt file!')\n        random.seed(0) # 设置随机种子\n        random.shuffle(prompt_file)\n    else: \n        print('Not shuffle the prompt file!')\n\n    prompt_file = prompt_file[args.start_id:args.end_id]\n    chunk_size = len(prompt_file) // num_gpus\n    ans_handles = []\n    split_list = list(range(args.start_id, args.end_id, chunk_size))\n    idx_list = list(range(0, len(prompt_file), chunk_size))\n    if len(split_list) == num_gpus: \n        split_list.append(args.end_id)\n        idx_list.append(len(prompt_file))\n    elif len(split_list) == num_gpus + 1: \n        split_list[-1] = args.end_id\n        idx_list[-1] = len(prompt_file)\n    else: \n        raise ValueError('error in the number of list')\n\n    if osp.exists(args.output_res_path) is False: \n        os.mkdir(args.output_res_path)\n    \n    for idx in range(len(idx_list) - 1):\n        start_idx = idx_list[idx]\n        end_idx = idx_list[idx + 1]\n        \n        start_split = split_list[idx]\n        end_split = split_list[idx + 1]\n        ans_handles.append(\n            eval_model.remote(\n                args, prompt_file[start_idx:end_idx], start_split, end_split\n            )\n        )\n\n    ans_jsons = []\n    for ans_handle in ans_handles:\n        ans_jsons.extend(ray.get(ans_handle))\n\n    # with open(args.output_res_path, \"w\") as ans_file:\n    #     for line in ans_jsons:\n    #         ans_file.write(json.dumps(line) + \"\\n\")\n\n\n@ray.remote(num_gpus=1)\n@torch.inference_mode()\ndef eval_model(args, prompt_file, start_idx, end_idx):\n    # load prompting file\n    # prompt_file = load_prompting_file(args.prompting_file)\n\n\n    # Model\n    disable_torch_init()\n    # model_name = os.path.expanduser(args.model_name)\n    print('start loading')\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    print('finish loading')\n\n    print('start loading')\n    model = GraphLlamaForCausalLM.from_pretrained(args.model_name, torch_dtype=torch.float16, use_cache=True, low_cpu_mem_usage=True).cuda()\n    print('finish loading')\n\n    use_graph_start_end = getattr(model.config, \"use_graph_start_end\", False)\n    tokenizer.add_tokens([DEFAULT_GRAPH_PATCH_TOKEN], special_tokens=True)\n    if use_graph_start_end:\n        tokenizer.add_tokens([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN], special_tokens=True)\n\n    graph_tower = model.get_model().graph_tower\n    \n    # TODO: add graph tower\n    # if graph_tower.device.type == 'meta':\n    #     print('meta')\n    clip_graph, args_graph= load_model_pretrained(CLIP, './clip_gt_arxiv_pub')\n    graph_tower = graph_transformer(args_graph)\n    graph_tower = transfer_param_tograph(clip_graph, graph_tower)\n    \n    model.get_model().graph_tower = graph_tower.cuda()\n    # else:\n    #     print('other')\n    # print(next(graph_tower.parameters()).dtype)\n    graph_tower.to(device='cuda', dtype=torch.float16)\n    graph_config = graph_tower.config\n    graph_config.graph_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_GRAPH_PATCH_TOKEN])[0]\n    graph_config.use_graph_start_end = use_graph_start_end\n    if use_graph_start_end:\n        graph_config.graph_start_token, graph_config.graph_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN])\n    # TODO: add graph token len\n\n    res_data = []\n    print(f'total: {len(prompt_file)}')\n    for idx, instruct_item in tqdm(enumerate(prompt_file)):\n        # instruct_item = prompt_file[0]\n        # if idx >= 3: \n        #     break\n        task_type = instruct_item['id'].split('_')[-1]\n        if task_type != 'LP':\n            graph_dict = load_graph(instruct_item, args.graph_data_path)\n            graph_token_len = graph_dict['graph_token_len']\n            graph_data = graph_dict['graph_data']\n\n            qs = instruct_item[\"conversations\"][0][\"value\"]\n            # if use_graph_start_end:\n            #     qs = qs + '\\n' + DEFAULT_G_START_TOKEN + DEFAULT_GRAPH_PATCH_TOKEN * graph_token_len + DEFAULT_G_END_TOKEN\n            # else:\n            #     qs = qs + '\\n' + DEFAULT_GRAPH_PATCH_TOKEN * graph_token_len\n\n            replace_token = DEFAULT_GRAPH_PATCH_TOKEN * graph_token_len\n            replace_token = DEFAULT_G_START_TOKEN + replace_token + DEFAULT_G_END_TOKEN\n            qs = qs.replace(DEFAULT_GRAPH_TOKEN, replace_token)\n        else: \n            graph_dict = load_graph_LP(instruct_item, args.graph_data_path)\n            graph_token_len_1 = graph_dict['graph_token_len_1']\n            graph_token_len_2 = graph_dict['graph_token_len_2']\n            graph_data = graph_dict['graph_data']\n\n            qs = instruct_item[\"conversations\"][0][\"value\"]\n            # if use_graph_start_end:\n            #     qs = qs + '\\n' + DEFAULT_G_START_TOKEN + DEFAULT_GRAPH_PATCH_TOKEN * graph_token_len + DEFAULT_G_END_TOKEN\n            # else:\n            #     qs = qs + '\\n' + DEFAULT_GRAPH_PATCH_TOKEN * graph_token_len\n\n            # replace_token = DEFAULT_GRAPH_PATCH_TOKEN * graph_token_len\n            # replace_token = DEFAULT_G_START_TOKEN + replace_token + DEFAULT_G_END_TOKEN\n            # qs = qs.replace(DEFAULT_GRAPH_TOKEN, replace_token)\n\n            replace_token_1 = DEFAULT_GRAPH_PATCH_TOKEN * graph_token_len_1\n            replace_token_2 = DEFAULT_GRAPH_PATCH_TOKEN * graph_token_len_2\n\n            replace_token_1 = DEFAULT_G_START_TOKEN + replace_token_1 + DEFAULT_G_END_TOKEN\n            replace_token_2 = DEFAULT_G_START_TOKEN + replace_token_2 + DEFAULT_G_END_TOKEN\n\n            if DEFAULT_GRAPH_TOKEN in qs:\n                first_index = qs.find(DEFAULT_GRAPH_TOKEN)\n                qs = qs[:first_index] + replace_token_1 + qs[first_index+len(DEFAULT_GRAPH_TOKEN):]\n\n                second_index = qs.find(DEFAULT_GRAPH_TOKEN)\n                qs = qs[:second_index] + replace_token_2 + qs[second_index+len(DEFAULT_GRAPH_TOKEN):]\n\n        # if \"v1\" in args.model_name.lower():\n        #     conv_mode = \"graphchat_v1\"\n        # else: \n        #     raise ValueError('Don\\'t support this model')\n        conv_mode = \"graphchat_v1\"\n\n        if args.conv_mode is not None and conv_mode != args.conv_mode:\n            print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\n        else:\n            args.conv_mode = conv_mode\n\n        conv = conv_templates[args.conv_mode].copy()\n        conv.append_message(conv.roles[0], qs)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n        inputs = tokenizer([prompt])\n\n        \n\n        input_ids = torch.as_tensor(inputs.input_ids).cuda()\n\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n        keywords = [stop_str]\n        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n        if task_type != 'LP':\n            graph_data.graph_node = graph_data.graph_node.to(torch.float16)\n            graph_data = graph_data.cuda()\n        else: \n            graph_data['graph_1'].graph_node = graph_data['graph_1'].graph_node.to(torch.float16)\n            graph_data['graph_2'].graph_node = graph_data['graph_2'].graph_node.to(torch.float16)\n\n            graph_data['graph_1'] = graph_data['graph_1'].cuda()\n            graph_data['graph_2'] = graph_data['graph_2'].cuda()\n\n        # graph_data.edge_index = graph_data.edge_index.to(torch.float16)\n\n        with torch.inference_mode():\n            output_ids = model.generate(\n                input_ids,\n                graph_data=graph_data,\n                do_sample=True,\n                temperature=0.2,\n                max_new_tokens=1024,\n                stopping_criteria=[stopping_criteria])\n\n        input_token_len = input_ids.shape[1]\n        n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\n        if n_diff_input_output > 0:\n            print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')\n        outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\n        outputs = outputs.strip()\n        if outputs.endswith(stop_str):\n            outputs = outputs[:-len(stop_str)]\n        outputs = outputs.strip()\n        # print(outputs)\n\n        res_data.append({\"id\": instruct_item[\"id\"], \"node_idx_1\": instruct_item[\"graph\"][\"node_idx_1\"], \"node_idx_2\": instruct_item[\"graph\"][\"node_idx_2\"], 'truth': instruct_item[\"conversations\"][1][\"value\"], \"res\": outputs}.copy())\n        with open(osp.join(args.output_res_path, 'arxiv_test_res_{}_{}.json'.format(start_idx, end_idx)), \"w\") as fout:\n            json.dump(res_data, fout, indent=4)\n    return res_data\n    # with open(args.output_res_path, \"w\") as fout:\n    #     json.dump(res_data, fout, indent=4)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\n    # parser.add_argument(\"--image-file\", type=str, required=True)\n    # parser.add_argument(\"--query\", type=str, required=True)\n    parser.add_argument(\"--prompting_file\", type=str, default=None)\n    parser.add_argument(\"--conv-mode\", type=str, default=None)\n    parser.add_argument(\"--graph_data_path\", type=str, default=None)\n\n    parser.add_argument(\"--output_res_path\", type=str, default=None)\n    parser.add_argument(\"--num_gpus\", type=int, default=4)\n\n    parser.add_argument(\"--start_id\", type=int, default=0)\n    parser.add_argument(\"--end_id\", type=int, default=20567)\n    parser.add_argument(\"--is_shuffle\", type=bool, default=False)\n\n    args = parser.parse_args()\n\n    # eval_model(args)\n\n    ray.init()\n    run_eval(args, args.num_gpus)\n\n\n# protobuf             4.22.3"}
{"type": "source_file", "path": "graphgpt/constants.py", "content": "from enum import IntEnum\nimport os\n\n# For the gradio web server\nSERVER_ERROR_MSG = (\n    \"**NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.**\"\n)\nMODERATION_MSG = \"YOUR INPUT VIOLATES OUR CONTENT MODERATION GUIDELINES. PLEASE FIX YOUR INPUT AND TRY AGAIN.\"\nCONVERSATION_LIMIT_MSG = \"YOU HAVE REACHED THE CONVERSATION LENGTH LIMIT. PLEASE CLEAR HISTORY AND START A NEW CONVERSATION.\"\nINPUT_CHAR_LEN_LIMIT = 2560\nCONVERSATION_LEN_LIMIT = 50\nLOGDIR = \".\"\n\n# For the controller and workers(could be overwritten through ENV variables.)\nCONTROLLER_HEART_BEAT_EXPIRATION = int(\n    os.getenv(\"FASTCHAT_CONTROLLER_HEART_BEAT_EXPIRATION\", 90)\n)\nWORKER_HEART_BEAT_INTERVAL = int(os.getenv(\"FASTCHAT_WORKER_HEART_BEAT_INTERVAL\", 30))\nWORKER_API_TIMEOUT = int(os.getenv(\"FASTCHAT_WORKER_API_TIMEOUT\", 100))\nWORKER_API_EMBEDDING_BATCH_SIZE = int(os.getenv(\"WORKER_API_EMBEDDING_BATCH_SIZE\", 4))\n\n\nclass ErrorCode(IntEnum):\n    \"\"\"\n    https://platform.openai.com/docs/guides/error-codes/api-errors\n    \"\"\"\n\n    VALIDATION_TYPE_ERROR = 40001\n\n    INVALID_AUTH_KEY = 40101\n    INCORRECT_AUTH_KEY = 40102\n    NO_PERMISSION = 40103\n\n    INVALID_MODEL = 40301\n    PARAM_OUT_OF_RANGE = 40302\n    CONTEXT_OVERFLOW = 40303\n\n    RATE_LIMIT = 42901\n    QUOTA_EXCEEDED = 42902\n    ENGINE_OVERLOADED = 42903\n\n    INTERNAL_ERROR = 50001\n    CUDA_OUT_OF_MEMORY = 50002\n    GRADIO_REQUEST_ERROR = 50003\n    GRADIO_STREAM_UNKNOWN_ERROR = 50004\n    CONTROLLER_NO_WORKER = 50005\n    CONTROLLER_WORKER_TIMEOUT = 50006\n\nDEFAULT_GRAPH_TOKEN = \"<graph>\"\nDEFAULT_GRAPH_PATCH_TOKEN = \"<g_patch>\"\nDEFAULT_G_START_TOKEN = \"<g_start>\"\nDEFAULT_G_END_TOKEN = \"<g_end>\""}
{"type": "source_file", "path": "graphgpt/model/convert_fp16.py", "content": "\"\"\"\nUsage:\npython3 -m fastchat.model.convert_fp16 --in in-folder --out out-folder\n\"\"\"\nimport argparse\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n\ndef convert_fp16(in_checkpoint, out_checkpoint):\n    tokenizer = AutoTokenizer.from_pretrained(in_checkpoint, use_fast=False)\n    model = AutoModelForCausalLM.from_pretrained(\n        in_checkpoint, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    model.save_pretrained(out_checkpoint)\n    tokenizer.save_pretrained(out_checkpoint)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--in-checkpoint\", type=str, help=\"Path to the model\")\n    parser.add_argument(\"--out-checkpoint\", type=str, help=\"Path to the output model\")\n    args = parser.parse_args()\n\n    convert_fp16(args.in_checkpoint, args.out_checkpoint)\n"}
{"type": "source_file", "path": "graphgpt/model/builder.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nimport os\nimport shutil\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\nimport torch\nfrom graphgpt.model import *\nfrom graphgpt.constants import DEFAULT_GRAPH_PATCH_TOKEN, DEFAULT_G_START_TOKEN, DEFAULT_G_END_TOKEN\n\n\ndef load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\"):\n    kwargs = {\"device_map\": device_map}\n\n    if load_8bit:\n        kwargs['load_in_8bit'] = True\n    elif load_4bit:\n        kwargs['load_in_4bit'] = True\n        kwargs['quantization_config'] = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type='nf4'\n        )\n    else:\n        kwargs['torch_dtype'] = torch.float16\n\n    if 'graphchat' in model_name.lower():\n        # Load LLaVA model\n        if 'lora' in model_name.lower() and model_base is not None:\n            lora_cfg_pretrained = AutoConfig.from_pretrained(model_path)\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            print('Loading LLaVA from base model...')\n            model = GraphLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\n            token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\n            if model.lm_head.weight.shape[0] != token_num:\n                model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n                model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n\n            print('Loading additional LLaVA weights...')\n            if os.path.exists(os.path.join(model_path, 'non_lora_trainables.bin')):\n                non_lora_trainables = torch.load(os.path.join(model_path, 'non_lora_trainables.bin'), map_location='cpu')\n            else:\n                # this is probably from HF Hub\n                from huggingface_hub import hf_hub_download\n                def load_from_hf(repo_id, filename, subfolder=None):\n                    cache_file = hf_hub_download(\n                        repo_id=repo_id,\n                        filename=filename,\n                        subfolder=subfolder)\n                    return torch.load(cache_file, map_location='cpu')\n                non_lora_trainables = load_from_hf(model_path, 'non_lora_trainables.bin')\n            non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\n            if any(k.startswith('model.model.') for k in non_lora_trainables):\n                non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}\n            model.load_state_dict(non_lora_trainables, strict=False)\n\n            from peft import PeftModel\n            print('Loading LoRA weights...')\n            model = PeftModel.from_pretrained(model, model_path)\n            print('Merging LoRA weights...')\n            model = model.merge_and_unload()\n            print('Model is loaded...')\n        elif model_base is not None:\n            # this may be mm projector only\n            print('Loading LLaVA from base model...')\n            if 'mpt' in model_name.lower():\n                if not os.path.isfile(os.path.join(model_path, 'configuration_mpt.py')):\n                    shutil.copyfile(os.path.join(model_base, 'configuration_mpt.py'), os.path.join(model_path, 'configuration_mpt.py'))\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n                model = LlavaMPTForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\n            else:\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\n\n            mm_projector_weights = torch.load(os.path.join(model_path, 'mm_projector.bin'), map_location='cpu')\n            mm_projector_weights = {k: v.to(torch.float16) for k, v in mm_projector_weights.items()}\n            model.load_state_dict(mm_projector_weights, strict=False)\n        else:\n            if 'mpt' in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n                model = LlavaMPTForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n            else:\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n    else:\n        # Load language model\n        if model_base is not None:\n            # PEFT model\n            from peft import PeftModel\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            model = AutoModelForCausalLM.from_pretrained(model_base, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\n            print(f\"Loading LoRA weights from {model_path}\")\n            model = PeftModel.from_pretrained(model, model_path)\n            print(f\"Merging weights\")\n            model = model.merge_and_unload()\n            print('Convert to FP16...')\n            model.to(torch.float16)\n        else:\n            use_fast = False\n            if 'mpt' in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\n            else:\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n\n    image_processor = None\n\n    if 'llava' in model_name.lower():\n        mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n        mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\n        if mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n        if mm_use_im_start_end:\n            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n        model.resize_token_embeddings(len(tokenizer))\n\n        vision_tower = model.get_vision_tower()\n        if not vision_tower.is_loaded:\n            vision_tower.load_model()\n        vision_tower.to(device='cuda', dtype=torch.float16)\n        image_processor = vision_tower.image_processor\n\n    if hasattr(model.config, \"max_sequence_length\"):\n        context_len = model.config.max_sequence_length\n    else:\n        context_len = 2048\n\n    return tokenizer, model, image_processor, context_len"}
{"type": "source_file", "path": "graphgpt/model/apply_lora.py", "content": "\"\"\"\nApply the LoRA weights on top of a base model.\n\nUsage:\npython3 -m fastchat.model.apply_lora --base ~/model_weights/llama-7b --target ~/model_weights/baize-7b --lora project-baize/baize-lora-7B\n\nDependency:\npip3 install git+https://github.com/huggingface/peft.git@2822398fbe896f25d4dac5e468624dc5fd65a51b\n\"\"\"\nimport argparse\n\nimport torch\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef apply_lora(base_model_path, target_model_path, lora_path):\n    print(f\"Loading the base model from {base_model_path}\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    base_tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=False)\n\n    print(f\"Loading the LoRA adapter from {lora_path}\")\n\n    lora_model = PeftModel.from_pretrained(\n        base,\n        lora_path,\n        torch_dtype=torch.float16,\n    )\n\n    print(\"Applying the LoRA\")\n    model = lora_model.merge_and_unload()\n\n    print(f\"Saving the target model to {target_model_path}\")\n    model.save_pretrained(target_model_path)\n    base_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--lora-path\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    apply_lora(args.base_model_path, args.target_model_path, args.lora_path)\n"}
{"type": "source_file", "path": "graphgpt/serve/openai_api_server.py", "content": "\"\"\"A server that provides OpenAI-compatible RESTful APIs. It supports:\n\n- Chat Completions. (Reference: https://platform.openai.com/docs/api-reference/chat)\n- Completions. (Reference: https://platform.openai.com/docs/api-reference/completions)\n- Embeddings. (Reference: https://platform.openai.com/docs/api-reference/embeddings)\n\nUsage:\npython3 -m fastchat.serve.openai_api_server\n\"\"\"\nimport asyncio\n\nimport argparse\nimport asyncio\nimport json\nimport logging\n\nimport os\nfrom typing import Generator, Optional, Union, Dict, List, Any\n\nimport fastapi\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import StreamingResponse, JSONResponse\nimport httpx\nfrom pydantic import BaseSettings\nimport shortuuid\nimport tiktoken\nimport uvicorn\n\nfrom fastchat.constants import WORKER_API_TIMEOUT, WORKER_API_EMBEDDING_BATCH_SIZE, ErrorCode\nfrom fastchat.model.model_adapter import get_conversation_template\nfrom fastapi.exceptions import RequestValidationError\nfrom fastchat.protocol.openai_api_protocol import (\n    ChatCompletionRequest,\n    ChatCompletionResponse,\n    ChatCompletionResponseStreamChoice,\n    ChatCompletionStreamResponse,\n    ChatMessage,\n    ChatCompletionResponseChoice,\n    CompletionRequest,\n    CompletionResponse,\n    CompletionResponseChoice,\n    DeltaMessage,\n    CompletionResponseStreamChoice,\n    CompletionStreamResponse,\n    EmbeddingsRequest,\n    EmbeddingsResponse,\n    ErrorResponse,\n    ModelCard,\n    ModelList,\n    ModelPermission,\n    TokenCheckRequest,\n    TokenCheckResponse,\n    UsageInfo,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass AppSettings(BaseSettings):\n    # The address of the model controller.\n    controller_address: str = \"http://localhost:21001\"\n\n\napp_settings = AppSettings()\n\napp = fastapi.FastAPI()\nheaders = {\"User-Agent\": \"FastChat API Server\"}\n\n\ndef create_error_response(code: int, message: str) -> JSONResponse:\n    return JSONResponse(\n        ErrorResponse(message=message, code=code).dict(), status_code=400\n    )\n\n\n@app.exception_handler(RequestValidationError)\nasync def validation_exception_handler(request, exc):\n    return create_error_response(ErrorCode.VALIDATION_TYPE_ERROR, str(exc))\n\n\nasync def check_model(request) -> Optional[JSONResponse]:\n    controller_address = app_settings.controller_address\n    ret = None\n    async with httpx.AsyncClient() as client:\n        try:\n            _worker_addr = await _get_worker_address(request.model, client)\n        except:\n            models_ret = await client.post(controller_address + \"/list_models\")\n            models = models_ret.json()[\"models\"]\n            ret = create_error_response(\n                ErrorCode.INVALID_MODEL,\n                f\"Only {'&&'.join(models)} allowed now, your model {request.model}\",\n            )\n    return ret\n\n\nasync def check_length(request, prompt, max_tokens):\n    async with httpx.AsyncClient() as client:\n        worker_addr = await _get_worker_address(request.model, client)\n\n        response = await client.post(\n            worker_addr + \"/model_details\",\n            headers=headers,\n            json={},\n            timeout=WORKER_API_TIMEOUT,\n        )\n        context_len = response.json()[\"context_length\"]\n\n        response = await client.post(\n            worker_addr + \"/count_token\",\n            headers=headers,\n            json={\"prompt\": prompt},\n            timeout=WORKER_API_TIMEOUT,\n        )\n        token_num = response.json()[\"count\"]\n\n    if token_num + max_tokens > context_len:\n        return create_error_response(\n            ErrorCode.CONTEXT_OVERFLOW,\n            f\"This model's maximum context length is {context_len} tokens. \"\n            f\"However, you requested {max_tokens + token_num} tokens \"\n            f\"({token_num} in the messages, \"\n            f\"{max_tokens} in the completion). \"\n            f\"Please reduce the length of the messages or completion.\",\n        )\n    else:\n        return None\n\n\ndef check_requests(request) -> Optional[JSONResponse]:\n    # Check all params\n    if request.max_tokens is not None and request.max_tokens <= 0:\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.max_tokens} is less than the minimum of 1 - 'max_tokens'\",\n        )\n    if request.n is not None and request.n <= 0:\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.n} is less than the minimum of 1 - 'n'\",\n        )\n    if request.temperature is not None and request.temperature < 0:\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.temperature} is less than the minimum of 0 - 'temperature'\",\n        )\n    if request.temperature is not None and request.temperature > 2:\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.temperature} is greater than the maximum of 2 - 'temperature'\",\n        )\n    if request.top_p is not None and request.top_p < 0:\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.top_p} is less than the minimum of 0 - 'top_p'\",\n        )\n    if request.top_p is not None and request.top_p > 1:\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.top_p} is greater than the maximum of 1 - 'temperature'\",\n        )\n    if request.stop is not None and (\n        not isinstance(request.stop, str) and not isinstance(request.stop, list)\n    ):\n        return create_error_response(\n            ErrorCode.PARAM_OUT_OF_RANGE,\n            f\"{request.stop} is not valid under any of the given schemas - 'stop'\",\n        )\n\n    return None\n\n\ndef process_input(model_name, input):\n    if isinstance(input, str):\n        input = [input]\n    elif isinstance(input, list):\n        if isinstance(input[0], int):\n            decoding = tiktoken.model.encoding_for_model(model_name)\n            input = [decoding.decode(input)]\n        elif isinstance(input[0], list):\n            decoding = tiktoken.model.encoding_for_model(model_name)\n            input = [decoding.decode(text) for text in input]\n\n    return input\n\n\ndef get_gen_params(\n    model_name: str,\n    messages: Union[str, List[Dict[str, str]]],\n    *,\n    temperature: float,\n    top_p: float,\n    max_tokens: Optional[int],\n    echo: Optional[bool],\n    stream: Optional[bool],\n    stop: Optional[Union[str, List[str]]],\n) -> Dict[str, Any]:\n    conv = get_conversation_template(model_name)\n\n    if isinstance(messages, str):\n        prompt = messages\n    else:\n        for message in messages:\n            msg_role = message[\"role\"]\n            if msg_role == \"system\":\n                conv.system = message[\"content\"]\n            elif msg_role == \"user\":\n                conv.append_message(conv.roles[0], message[\"content\"])\n            elif msg_role == \"assistant\":\n                conv.append_message(conv.roles[1], message[\"content\"])\n            else:\n                raise ValueError(f\"Unknown role: {msg_role}\")\n\n        # Add a blank message for the assistant.\n        conv.append_message(conv.roles[1], None)\n\n        is_chatglm = \"chatglm\" in model_name.lower()\n        if is_chatglm:\n            prompt = conv.messages[conv.offset :]\n        else:\n            prompt = conv.get_prompt()\n\n    if max_tokens is None:\n        max_tokens = 512\n\n    gen_params = {\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"temperature\": temperature,\n        \"top_p\": top_p,\n        \"max_new_tokens\": max_tokens,\n        \"echo\": echo,\n        \"stream\": stream,\n    }\n\n    if stop is None:\n        gen_params.update(\n            {\"stop\": conv.stop_str, \"stop_token_ids\": conv.stop_token_ids}\n        )\n    else:\n        gen_params.update({\"stop\": stop})\n\n    logger.debug(f\"==== request ====\\n{gen_params}\")\n    return gen_params\n\n\nasync def _get_worker_address(model_name: str, client: httpx.AsyncClient) -> str:\n    \"\"\"\n    Get worker address based on the requested model\n\n    :param model_name: The worker's model name\n    :param client: The httpx client to use\n    :return: Worker address from the controller\n    :raises: :class:`ValueError`: No available worker for requested model\n    \"\"\"\n    controller_address = app_settings.controller_address\n\n    ret = await client.post(\n        controller_address + \"/get_worker_address\", json={\"model\": model_name}\n    )\n    worker_addr = ret.json()[\"address\"]\n    # No available worker\n    if worker_addr == \"\":\n        raise ValueError(f\"No available worker for {model_name}\")\n\n    logger.debug(f\"model_name: {model_name}, worker_addr: {worker_addr}\")\n    return worker_addr\n\n\n@app.get(\"/v1/models\")\nasync def show_available_models():\n    controller_address = app_settings.controller_address\n    async with httpx.AsyncClient() as client:\n        ret = await client.post(controller_address + \"/refresh_all_workers\")\n        ret = await client.post(controller_address + \"/list_models\")\n    models = ret.json()[\"models\"]\n    models.sort()\n    # TODO: return real model permission details\n    model_cards = []\n    for m in models:\n        model_cards.append(ModelCard(id=m, root=m, permission=[ModelPermission()]))\n    return ModelList(data=model_cards)\n\n\n# TODO: Have check_length and count_tokens share code.\n@app.post(\"/v1/token_check\")\nasync def count_tokens(request: TokenCheckRequest):\n    \"\"\"\n    Checks the token count against your message\n    This is not part of the OpenAI API spec.\n    \"\"\"\n    async with httpx.AsyncClient() as client:\n        worker_addr = await _get_worker_address(request.model, client)\n\n        response = await client.post(\n            worker_addr + \"/model_details\",\n            headers=headers,\n            json={},\n            timeout=WORKER_API_TIMEOUT,\n        )\n        context_len = response.json()[\"context_length\"]\n\n        response = await client.post(\n            worker_addr + \"/count_token\",\n            headers=headers,\n            json={\"prompt\": request.prompt},\n            timeout=WORKER_API_TIMEOUT,\n        )\n        token_num = response.json()[\"count\"]\n\n    can_fit = True\n    if token_num + request.max_tokens > context_len:\n        can_fit = False\n\n    return TokenCheckResponse(fits=can_fit, contextLength=context_len, tokenCount=token_num)\n\n\n@app.post(\"/v1/chat/completions\")\nasync def create_chat_completion(request: ChatCompletionRequest):\n    \"\"\"Creates a completion for the chat message\"\"\"\n    error_check_ret = await check_model(request)\n    if error_check_ret is not None:\n        return error_check_ret\n    error_check_ret = check_requests(request)\n    if error_check_ret is not None:\n        return error_check_ret\n\n    gen_params = get_gen_params(\n        request.model,\n        request.messages,\n        temperature=request.temperature,\n        top_p=request.top_p,\n        max_tokens=request.max_tokens,\n        echo=False,\n        stream=request.stream,\n        stop=request.stop,\n    )\n    error_check_ret = await check_length(\n        request, gen_params[\"prompt\"], gen_params[\"max_new_tokens\"]\n    )\n    if error_check_ret is not None:\n        return error_check_ret\n\n    if request.stream:\n        generator = chat_completion_stream_generator(\n            request.model, gen_params, request.n\n        )\n        return StreamingResponse(generator, media_type=\"text/event-stream\")\n\n    choices = []\n    # TODO: batch the requests. maybe not necessary if using CacheFlow worker\n    chat_completions = []\n    for i in range(request.n):\n        content = asyncio.create_task(chat_completion(request.model, gen_params))\n        chat_completions.append(content)\n    try:\n        all_tasks = await asyncio.gather(*chat_completions)\n    except Exception as e:\n        return create_error_response(ErrorCode.INTERNAL_ERROR, str(e))\n    usage = UsageInfo()\n    for i, content in enumerate(all_tasks):\n        if content[\"error_code\"] != 0:\n            return create_error_response(content[\"error_code\"], content[\"text\"])\n        choices.append(\n            ChatCompletionResponseChoice(\n                index=i,\n                message=ChatMessage(role=\"assistant\", content=content[\"text\"]),\n                finish_reason=content.get(\"finish_reason\", \"stop\"),\n            )\n        )\n        task_usage = UsageInfo.parse_obj(content[\"usage\"])\n        for usage_key, usage_value in task_usage.dict().items():\n            setattr(usage, usage_key, getattr(usage, usage_key) + usage_value)\n\n    return ChatCompletionResponse(model=request.model, choices=choices, usage=usage)\n\n\nasync def chat_completion_stream_generator(\n    model_name: str, gen_params: Dict[str, Any], n: int\n) -> Generator[str, Any, None]:\n    \"\"\"\n    Event stream format:\n    https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format\n    \"\"\"\n    id = f\"chatcmpl-{shortuuid.random()}\"\n    finish_stream_events = []\n    for i in range(n):\n        # First chunk with role\n        choice_data = ChatCompletionResponseStreamChoice(\n            index=i,\n            delta=DeltaMessage(role=\"assistant\"),\n            finish_reason=None,\n        )\n        chunk = ChatCompletionStreamResponse(\n            id=id, choices=[choice_data], model=model_name\n        )\n        yield f\"data: {chunk.json(exclude_unset=True, ensure_ascii=False)}\\n\\n\"\n\n        previous_text = \"\"\n        async for content in chat_completion_stream(model_name, gen_params):\n            if content[\"error_code\"] != 0:\n                yield f\"data: {json.dumps(content, ensure_ascii=False)}\\n\\n\"\n                yield \"data: [DONE]\\n\\n\"\n                return\n            decoded_unicode = content[\"text\"].replace(\"\\ufffd\", \"\")\n            delta_text = decoded_unicode[len(previous_text) :]\n            previous_text = decoded_unicode\n\n            if len(delta_text) == 0:\n                delta_text = None\n            choice_data = ChatCompletionResponseStreamChoice(\n                index=i,\n                delta=DeltaMessage(content=delta_text),\n                finish_reason=content.get(\"finish_reason\", None),\n            )\n            chunk = ChatCompletionStreamResponse(\n                id=id, choices=[choice_data], model=model_name\n            )\n            if delta_text is None:\n                if content.get(\"finish_reason\", None) is not None:\n                    finish_stream_events.append(chunk)\n                continue\n            yield f\"data: {chunk.json(exclude_unset=True, ensure_ascii=False)}\\n\\n\"\n    # There is not \"content\" field in the last delta message, so exclude_none to exclude field \"content\".\n    for finish_chunk in finish_stream_events:\n        yield f\"data: {finish_chunk.json(exclude_none=True, ensure_ascii=False)}\\n\\n\"\n    yield \"data: [DONE]\\n\\n\"\n\n\nasync def chat_completion_stream(model_name: str, gen_params: Dict[str, Any]):\n    controller_url = app_settings.controller_address\n    async with httpx.AsyncClient() as client:\n        worker_addr = await _get_worker_address(model_name, client)\n        delimiter = b\"\\0\"\n        async with client.stream(\n            \"POST\",\n            worker_addr + \"/worker_generate_stream\",\n            headers=headers,\n            json=gen_params,\n            timeout=WORKER_API_TIMEOUT,\n        ) as response:\n            # content = await response.aread()\n            async for raw_chunk in response.aiter_raw():\n                for chunk in raw_chunk.split(delimiter):\n                    if not chunk:\n                        continue\n                    data = json.loads(chunk.decode())\n                    yield data\n\n\nasync def chat_completion(\n    model_name: str, gen_params: Dict[str, Any]\n) -> Optional[Dict[str, Any]]:\n    async with httpx.AsyncClient() as client:\n        worker_addr = await _get_worker_address(model_name, client)\n\n        output = None\n        delimiter = b\"\\0\"\n\n        async with client.stream(\n            \"POST\",\n            worker_addr + \"/worker_generate_stream\",\n            headers=headers,\n            json=gen_params,\n            timeout=WORKER_API_TIMEOUT,\n        ) as response:\n            content = await response.aread()\n\n        for chunk in content.split(delimiter):\n            if not chunk:\n                continue\n            data = json.loads(chunk.decode())\n            output = data\n\n        return output\n\n\n@app.post(\"/v1/completions\")\nasync def create_completion(request: CompletionRequest):\n    error_check_ret = await check_model(request)\n    if error_check_ret is not None:\n        return error_check_ret\n    error_check_ret = check_requests(request)\n    if error_check_ret is not None:\n        return error_check_ret\n\n    request.prompt = process_input(request.model, request.prompt)\n\n    for text in request.prompt:\n        error_check_ret = await check_length(request, text, request.max_tokens)\n        if error_check_ret is not None:\n            return error_check_ret\n\n    if request.stream:\n        generator = generate_completion_stream_generator(request, request.n)\n        return StreamingResponse(generator, media_type=\"text/event-stream\")\n    else:\n        text_completions = []\n        for text in request.prompt:\n            payload = get_gen_params(\n                request.model,\n                text,\n                temperature=request.temperature,\n                top_p=request.top_p,\n                max_tokens=request.max_tokens,\n                echo=request.echo,\n                stream=request.stream,\n                stop=request.stop,\n            )\n            for i in range(request.n):\n                content = asyncio.create_task(generate_completion(payload))\n                text_completions.append(content)\n\n        try:\n            all_tasks = await asyncio.gather(*text_completions)\n        except Exception as e:\n            return create_error_response(ErrorCode.INTERNAL_ERROR, str(e))\n\n        choices = []\n        usage = UsageInfo()\n        for i, content in enumerate(all_tasks):\n            if content[\"error_code\"] != 0:\n                return create_error_response(content[\"error_code\"], content[\"text\"])\n            choices.append(\n                CompletionResponseChoice(\n                    index=i,\n                    text=content[\"text\"],\n                    logprobs=content.get(\"logprobs\", None),\n                    finish_reason=content.get(\"finish_reason\", \"stop\"),\n                )\n            )\n            task_usage = UsageInfo.parse_obj(content[\"usage\"])\n            for usage_key, usage_value in task_usage.dict().items():\n                setattr(usage, usage_key, getattr(usage, usage_key) + usage_value)\n\n        return CompletionResponse(\n            model=request.model, choices=choices, usage=UsageInfo.parse_obj(usage)\n        )\n\n\nasync def generate_completion_stream_generator(request: CompletionRequest, n: int):\n    model_name = request.model\n    id = f\"cmpl-{shortuuid.random()}\"\n    finish_stream_events = []\n    for text in request.prompt:\n        for i in range(n):\n            previous_text = \"\"\n            payload = get_gen_params(\n                request.model,\n                text,\n                temperature=request.temperature,\n                top_p=request.top_p,\n                max_tokens=request.max_tokens,\n                echo=request.echo,\n                stream=request.stream,\n                stop=request.stop,\n            )\n            async for content in generate_completion_stream(payload):\n                if content[\"error_code\"] != 0:\n                    yield f\"data: {json.dumps(content, ensure_ascii=False)}\\n\\n\"\n                    yield \"data: [DONE]\\n\\n\"\n                    return\n                decoded_unicode = content[\"text\"].replace(\"\\ufffd\", \"\")\n                delta_text = decoded_unicode[len(previous_text) :]\n                previous_text = decoded_unicode\n                # todo: index is not apparent\n                choice_data = CompletionResponseStreamChoice(\n                    index=i,\n                    text=delta_text,\n                    logprobs=content.get(\"logprobs\", None),\n                    finish_reason=content.get(\"finish_reason\", None),\n                )\n                chunk = CompletionStreamResponse(\n                    id=id,\n                    object=\"text_completion\",\n                    choices=[choice_data],\n                    model=model_name,\n                )\n                if len(delta_text) == 0:\n                    if content.get(\"finish_reason\", None) is not None:\n                        finish_stream_events.append(chunk)\n                    continue\n                yield f\"data: {chunk.json(exclude_unset=True, ensure_ascii=False)}\\n\\n\"\n    # There is not \"content\" field in the last delta message, so exclude_none to exclude field \"content\".\n    for finish_chunk in finish_stream_events:\n        yield f\"data: {finish_chunk.json(exclude_unset=True, ensure_ascii=False)}\\n\\n\"\n    yield \"data: [DONE]\\n\\n\"\n\n\nasync def generate_completion_stream(payload: Dict[str, Any]):\n    controller_address = app_settings.controller_address\n    async with httpx.AsyncClient() as client:\n        worker_addr = await _get_worker_address(payload[\"model\"], client)\n\n        delimiter = b\"\\0\"\n        async with client.stream(\n            \"POST\",\n            worker_addr + \"/worker_generate_completion_stream\",\n            headers=headers,\n            json=payload,\n            timeout=WORKER_API_TIMEOUT,\n        ) as response:\n            # content = await response.aread()\n            async for raw_chunk in response.aiter_raw():\n                for chunk in raw_chunk.split(delimiter):\n                    if not chunk:\n                        continue\n                    data = json.loads(chunk.decode())\n                    yield data\n\n\nasync def generate_completion(payload: Dict[str, Any]):\n    controller_address = app_settings.controller_address\n    async with httpx.AsyncClient() as client:\n        worker_addr = await _get_worker_address(payload[\"model\"], client)\n\n        response = await client.post(\n            worker_addr + \"/worker_generate_completion\",\n            headers=headers,\n            json=payload,\n            timeout=WORKER_API_TIMEOUT,\n        )\n        completion = response.json()\n        return completion\n\n\n@app.post(\"/v1/embeddings\")\n@app.post(\"/v1/engines/{model_name}/embeddings\")\nasync def create_embeddings(request: EmbeddingsRequest, model_name: str = None):\n    \"\"\"Creates embeddings for the text\"\"\"\n    if request.model is None:\n        request.model = model_name\n    error_check_ret = await check_model(request)\n    if error_check_ret is not None:\n        return error_check_ret\n\n    request.input = process_input(request.model, request.input)\n\n    data = []\n    token_num = 0\n    batch_size = WORKER_API_EMBEDDING_BATCH_SIZE\n    batches = [\n        request.input[i : min(i + batch_size, len(request.input))]\n        for i in range(0, len(request.input), batch_size)\n    ]\n    for num_batch, batch in enumerate(batches):\n        payload = {\n            \"model\": request.model,\n            \"input\": batch,\n        }\n        embedding = await get_embedding(payload)\n        data += [\n            {\n                \"object\": \"embedding\",\n                \"embedding\": emb,\n                \"index\": num_batch * batch_size + i,\n            }\n            for i, emb in enumerate(embedding[\"embedding\"])\n        ]\n        token_num += embedding[\"token_num\"]\n    return EmbeddingsResponse(\n        data=data,\n        model=request.model,\n        usage=UsageInfo(\n            prompt_tokens=token_num,\n            total_tokens=token_num,\n            completion_tokens=None,\n        ),\n    ).dict(exclude_none=True)\n\n\nasync def get_embedding(payload: Dict[str, Any]):\n    controller_address = app_settings.controller_address\n    model_name = payload[\"model\"]\n    async with httpx.AsyncClient() as client:\n        worker_addr = await _get_worker_address(model_name, client)\n\n        response = await client.post(\n            worker_addr + \"/worker_get_embeddings\",\n            headers=headers,\n            json=payload,\n            timeout=WORKER_API_TIMEOUT,\n        )\n        embedding = response.json()\n        return embedding\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"FastChat ChatGPT-Compatible RESTful API server.\"\n    )\n    parser.add_argument(\"--host\", type=str, default=\"localhost\", help=\"host name\")\n    parser.add_argument(\"--port\", type=int, default=8000, help=\"port number\")\n    parser.add_argument(\n        \"--controller-address\", type=str, default=\"http://localhost:21001\"\n    )\n    parser.add_argument(\n        \"--allow-credentials\", action=\"store_true\", help=\"allow credentials\"\n    )\n    parser.add_argument(\n        \"--allowed-origins\", type=json.loads, default=[\"*\"], help=\"allowed origins\"\n    )\n    parser.add_argument(\n        \"--allowed-methods\", type=json.loads, default=[\"*\"], help=\"allowed methods\"\n    )\n    parser.add_argument(\n        \"--allowed-headers\", type=json.loads, default=[\"*\"], help=\"allowed headers\"\n    )\n    args = parser.parse_args()\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n    app_settings.controller_address = args.controller_address\n\n    logger.info(f\"args: {args}\")\n\n    uvicorn.run(app, host=args.host, port=args.port, log_level=\"info\")\n"}
{"type": "source_file", "path": "graphgpt/model/__init__.py", "content": "from graphgpt.model.model_adapter import (\n    load_model,\n    get_conversation_template,\n    add_model_args,\n)\n\nfrom graphgpt.model.GraphLlama import GraphLlamaForCausalLM, load_model_pretrained, transfer_param_tograph\nfrom graphgpt.model.graph_layers.clip_graph import GNN, graph_transformer, CLIP\n"}
{"type": "source_file", "path": "graphgpt/conversation.py", "content": "import dataclasses\nfrom enum import auto, Enum\nfrom typing import List, Tuple\n\n\nclass SeparatorStyle(Enum):\n    \"\"\"Different separator style.\"\"\"\n    SINGLE = auto()\n    TWO = auto()\n    MPT = auto()\n\n\n@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that keeps all conversation history.\"\"\"\n    system: str\n    roles: List[str]\n    messages: List[List[str]]\n    offset: int\n    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n    sep: str = \"###\"\n    sep2: str = None\n    version: str = \"Unknown\"\n\n    skip_next: bool = False\n\n    def get_prompt(self):\n        if self.sep_style == SeparatorStyle.SINGLE:\n            ret = self.system + self.sep\n            for role, message in self.messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.TWO:\n            seps = [self.sep, self.sep2]\n            ret = self.system + seps[0]\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n            return ret\n        if self.sep_style == SeparatorStyle.MPT:\n            ret = self.system + self.sep\n            for role, message in self.messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n            return ret\n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\n    def append_message(self, role, message):\n        self.messages.append([role, message])\n\n    def get_images(self, return_pil=False):\n        images = []\n        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    import base64\n                    from io import BytesIO\n                    from PIL import Image\n                    msg, image, image_process_mode = msg\n                    if image_process_mode == \"Pad\":\n                        def expand2square(pil_img, background_color=(122, 116, 104)):\n                            width, height = pil_img.size\n                            if width == height:\n                                return pil_img\n                            elif width > height:\n                                result = Image.new(pil_img.mode, (width, width), background_color)\n                                result.paste(pil_img, (0, (width - height) // 2))\n                                return result\n                            else:\n                                result = Image.new(pil_img.mode, (height, height), background_color)\n                                result.paste(pil_img, ((height - width) // 2, 0))\n                                return result\n                        image = expand2square(image)\n                    elif image_process_mode == \"Crop\":\n                        pass\n                    elif image_process_mode == \"Resize\":\n                        image = image.resize((224, 224))\n                    else:\n                        raise ValueError(f\"Invalid image_process_mode: {image_process_mode}\")\n                    max_hw, min_hw = max(image.size), min(image.size)\n                    aspect_ratio = max_hw / min_hw\n                    max_len, min_len = 800, 400\n                    shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n                    longest_edge = int(shortest_edge * aspect_ratio)\n                    W, H = image.size\n                    if H > W:\n                        H, W = longest_edge, shortest_edge\n                    else:\n                        H, W = shortest_edge, longest_edge\n                    image = image.resize((W, H))\n                    if return_pil:\n                        images.append(image)\n                    else:\n                        buffered = BytesIO()\n                        image.save(buffered, format=\"JPEG\")\n                        img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n                        images.append(img_b64_str)\n        return images\n\n    def to_gradio_chatbot(self):\n        ret = []\n        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    import base64\n                    from io import BytesIO\n                    msg, image, image_process_mode = msg\n                    max_hw, min_hw = max(image.size), min(image.size)\n                    aspect_ratio = max_hw / min_hw\n                    max_len, min_len = 800, 400\n                    shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n                    longest_edge = int(shortest_edge * aspect_ratio)\n                    W, H = image.size\n                    if H > W:\n                        H, W = longest_edge, shortest_edge\n                    else:\n                        H, W = shortest_edge, longest_edge\n                    image = image.resize((W, H))\n                    # image = image.resize((224, 224))\n                    buffered = BytesIO()\n                    image.save(buffered, format=\"JPEG\")\n                    img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n                    img_str = f'<img src=\"data:image/png;base64,{img_b64_str}\" alt=\"user upload image\" />'\n                    msg = msg.replace('<image>', img_str)\n                ret.append([msg, None])\n            else:\n                ret[-1][-1] = msg\n        return ret\n\n    def copy(self):\n        return Conversation(\n            system=self.system,\n            roles=self.roles,\n            messages=[[x, y] for x, y in self.messages],\n            offset=self.offset,\n            sep_style=self.sep_style,\n            sep=self.sep,\n            sep2=self.sep2)\n\n    def dict(self):\n        if len(self.get_images()) > 0:\n            return {\n                \"system\": self.system,\n                \"roles\": self.roles,\n                \"messages\": [[x, y[0] if type(y) is tuple else y] for x, y in self.messages],\n                \"offset\": self.offset,\n                \"sep\": self.sep,\n                \"sep2\": self.sep2,\n            }\n        return {\n            \"system\": self.system,\n            \"roles\": self.roles,\n            \"messages\": self.messages,\n            \"offset\": self.offset,\n            \"sep\": self.sep,\n            \"sep2\": self.sep2,\n        }\n\n\nconv_v1 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n        (\"Human\", \"Give three tips for staying healthy.\"),\n        (\"Assistant\",\n            \"Sure, here are three tips for staying healthy:\\n\"\n            \"1. Exercise regularly: Regular physical activity can help improve your overall health and wellbeing. \"\n            \"It can also help reduce your risk of chronic conditions such as obesity, diabetes, heart disease, \"\n            \"and certain cancers. Aim for at least 150 minutes of moderate-intensity aerobic exercise or \"\n            \"75 minutes of vigorous-intensity aerobic exercise per week, along with muscle-strengthening \"\n            \"activities at least two days per week.\\n\"\n            \"2. Eat a balanced diet: Eating a balanced diet that is rich in fruits, \"\n            \"vegetables, whole grains, lean proteins, and healthy fats can help support \"\n            \"your overall health. Try to limit your intake of processed and high-sugar foods, \"\n            \"and aim to drink plenty of water throughout the day.\\n\"\n            \"3. Get enough sleep: Getting enough quality sleep is essential for your physical \"\n            \"and mental health. Adults should aim for seven to nine hours of sleep per night. \"\n            \"Establish a regular sleep schedule and try to create a relaxing bedtime routine to \"\n            \"help improve the quality of your sleep.\")\n    ),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_v1_2 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n        (\"Human\", \"What are the key differences between renewable and non-renewable energy sources?\"),\n        (\"Assistant\",\n            \"Renewable energy sources are those that can be replenished naturally in a relatively \"\n            \"short amount of time, such as solar, wind, hydro, geothermal, and biomass. \"\n            \"Non-renewable energy sources, on the other hand, are finite and will eventually be \"\n            \"depleted, such as coal, oil, and natural gas. Here are some key differences between \"\n            \"renewable and non-renewable energy sources:\\n\"\n            \"1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable \"\n            \"energy sources are finite and will eventually run out.\\n\"\n            \"2. Environmental impact: Renewable energy sources have a much lower environmental impact \"\n            \"than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, \"\n            \"and other negative effects.\\n\"\n            \"3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically \"\n            \"have lower operational costs than non-renewable sources.\\n\"\n            \"4. Reliability: Renewable energy sources are often more reliable and can be used in more remote \"\n            \"locations than non-renewable sources.\\n\"\n            \"5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different \"\n            \"situations and needs, while non-renewable sources are more rigid and inflexible.\\n\"\n            \"6. Sustainability: Renewable energy sources are more sustainable over the long term, while \"\n            \"non-renewable sources are not, and their depletion can lead to economic and social instability.\\n\")\n    ),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_vicuna_v1_1 = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_mpt = Conversation(\n    system=\"\"\"<|im_start|>system\n- You are a helpful language and vision assistant.\n- You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\n- You should follow the instructions carefully and explain your answers in detail.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_mpt_text = Conversation(\n    system=\"\"\"<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_bair_v1 = Conversation(\n    system=\"BEGINNING OF CONVERSATION:\",\n    roles=(\"USER\", \"GPT\"),\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nsimple_conv = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n        (\"Human\", \"Hi!\"),\n        (\"Assistant\", \"Hi there! How can I help you today?\")\n    ),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nsimple_conv_multimodal = Conversation(\n    system=\"You are LLaVA, a large language and vision assistant trained by UW Madison WAIV Lab.\"\n           \"You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n           \"Follow the instructions carefully and explain your answers in detail.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n        (\"Human\", \"Hi!\"),\n        (\"Assistant\", \"Hi there!  How can I help you today?\\n\")\n    ),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nsimple_conv_mpt_multimodal = Conversation(\n    system=\"\"\"<|im_start|>system\n- You are LLaVA, a large language and vision assistant trained by UW Madison WAIV Lab.\n- You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\n- You should follow the instructions carefully and explain your answers in detail.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nsimple_conv_legacy = Conversation(\n    system=\"You are LLaVA, a large language model trained by UW Madison WAIV Lab.\"\n           \"You are designed to assist human with a variety of tasks using natural language.\"\n           \"Follow the instructions carefully.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n        (\"Human\", \"Hi!\\n\\n### Response:\"),\n        (\"Assistant\", \"Hi there!  How can I help you today?\\n\")\n    ),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_llava_v1 = Conversation(\n    system=\"You are LLaVA, a large language and vision assistant trained by UW Madison WAIV Lab.\"\n           \"You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n           \"Follow the instructions carefully and explain your answers in detail.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_graphchat_v1 = Conversation(\n    system=\"You are GraphGPT, a large language and graph-structral assistant trained by HKUDS Lab.\"\n           \"You are able to understand the graph structures that the user provides, and assist the user with a variety of tasks using natural language.\"\n           \"Follow the instructions carefully and explain your answers in detail.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\ndefault_conversation = conv_v1_2\nconv_templates = {\n    \"default\": conv_v1_2,\n    \"simple\": simple_conv,\n    \"simple_legacy\": simple_conv_legacy,\n    \"multimodal\": simple_conv_multimodal,\n    \"mpt_multimodal\": simple_conv_mpt_multimodal,\n    \"llava_v1\": conv_llava_v1, \n    \"graphchat_v1\": conv_graphchat_v1, \n\n\n    # fastchat\n    \"v1\": conv_v1_2,\n    \"bair_v1\": conv_bair_v1,\n    \"vicuna_v1_1\": conv_vicuna_v1_1,\n    \"mpt\": conv_mpt,\n    \"mpt_text\": conv_mpt_text,\n}\n\n\nif __name__ == \"__main__\":\n    print(default_conversation.get_prompt())\n"}
{"type": "source_file", "path": "graphgpt/model/compression.py", "content": "import dataclasses\nimport gc\nimport glob\nimport os\n\nfrom accelerate import init_empty_weights\nfrom accelerate.utils import set_module_tensor_to_device\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n\n\n@dataclasses.dataclass\nclass CompressionConfig:\n    \"\"\"Group-wise quantization.\"\"\"\n\n    num_bits: int\n    group_size: int\n    group_dim: int\n    symmetric: bool\n    enabled: bool = True\n\n\ndefault_compression_config = CompressionConfig(\n    num_bits=8, group_size=256, group_dim=1, symmetric=True, enabled=True\n)\n\n\nclass CLinear(nn.Module):\n    \"\"\"Compressed Linear Layer.\"\"\"\n\n    def __init__(self, weight=None, bias=None, device=None):\n        super().__init__()\n        if weight is None:\n            self.weight = None\n        elif isinstance(weight, Tensor):\n            self.weight = compress(weight.data.to(device), default_compression_config)\n        else:\n            self.weight = weight\n        self.bias = bias\n\n    def forward(self, input: Tensor) -> Tensor:\n        weight = decompress(self.weight, default_compression_config)\n        return F.linear(input.to(weight.dtype), weight, self.bias)\n\n\ndef compress_module(module, target_device):\n    for attr_str in dir(module):\n        target_attr = getattr(module, attr_str)\n        if type(target_attr) == torch.nn.Linear:\n            setattr(\n                module,\n                attr_str,\n                CLinear(target_attr.weight, target_attr.bias, target_device),\n            )\n    for name, child in module.named_children():\n        compress_module(child, target_device)\n\n\ndef get_compressed_list(module, prefix=\"\"):\n    compressed_list = []\n    for attr_str in dir(module):\n        target_attr = getattr(module, attr_str)\n        if type(target_attr) == torch.nn.Linear:\n            full_name = (\n                f\"{prefix}.{attr_str}.weight\" if prefix else f\"{attr_str}.weight\"\n            )\n            compressed_list.append(full_name)\n    for name, child in module.named_children():\n        child_prefix = f\"{prefix}.{name}\" if prefix else name\n        for each in get_compressed_list(child, child_prefix):\n            compressed_list.append(each)\n    return compressed_list\n\n\ndef apply_compressed_weight(module, compressed_state_dict, target_device, prefix=\"\"):\n    for attr_str in dir(module):\n        target_attr = getattr(module, attr_str)\n        if type(target_attr) == torch.nn.Linear:\n            full_name = (\n                f\"{prefix}.{attr_str}.weight\" if prefix else f\"{attr_str}.weight\"\n            )\n            setattr(\n                module,\n                attr_str,\n                CLinear(\n                    compressed_state_dict[full_name], target_attr.bias, target_device\n                ),\n            )\n    for name, child in module.named_children():\n        child_prefix = f\"{prefix}.{name}\" if prefix else name\n        apply_compressed_weight(\n            child, compressed_state_dict, target_device, child_prefix\n        )\n\n\ndef load_compress_model(model_path, device, torch_dtype):\n    # partially load model\n    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n    base_pattern = os.path.join(model_path, \"pytorch_model-*.bin\")\n    files = glob.glob(base_pattern)\n\n    with init_empty_weights():\n        config = AutoConfig.from_pretrained(\n            model_path, low_cpu_mem_usage=True, torch_dtype=torch_dtype\n        )\n        model = AutoModelForCausalLM.from_config(config)\n        linear_weights = get_compressed_list(model)\n\n    compressed_state_dict = {}\n\n    for filename in tqdm(files):\n        tmp_state_dict = torch.load(filename)\n        for name in tmp_state_dict:\n            if name in linear_weights:\n                tensor = tmp_state_dict[name].to(device).data.to(torch_dtype)\n                compressed_state_dict[name] = compress(\n                    tensor, default_compression_config\n                )\n            else:\n                compressed_state_dict[name] = tmp_state_dict[name].to(device)\n            tmp_state_dict[name] = None\n            tensor = None\n            gc.collect()\n            torch.cuda.empty_cache()\n\n    for name in model.state_dict():\n        if name not in linear_weights:\n            set_module_tensor_to_device(\n                model, name, device, value=compressed_state_dict[name]\n            )\n    apply_compressed_weight(model, compressed_state_dict, device)\n\n    model.to(device)\n\n    return model, tokenizer\n\n\ndef compress(tensor, config):\n    \"\"\"Simulate group-wise quantization.\"\"\"\n    if not config.enabled:\n        return tensor\n\n    group_size, num_bits, group_dim, symmetric = (\n        config.group_size,\n        config.num_bits,\n        config.group_dim,\n        config.symmetric,\n    )\n    assert num_bits <= 8\n\n    original_shape = tensor.shape\n    num_groups = (original_shape[group_dim] + group_size - 1) // group_size\n    new_shape = (\n        original_shape[:group_dim]\n        + (num_groups, group_size)\n        + original_shape[group_dim + 1 :]\n    )\n\n    # Pad\n    pad_len = (group_size - original_shape[group_dim] % group_size) % group_size\n    if pad_len != 0:\n        pad_shape = (\n            original_shape[:group_dim] + (pad_len,) + original_shape[group_dim + 1 :]\n        )\n        tensor = torch.cat(\n            [tensor, torch.zeros(pad_shape, dtype=tensor.dtype, device=tensor.device)],\n            dim=group_dim,\n        )\n    data = tensor.view(new_shape)\n\n    # Quantize\n    if symmetric:\n        B = 2 ** (num_bits - 1) - 1\n        scale = B / torch.max(data.abs(), dim=group_dim + 1, keepdim=True)[0]\n        data = data * scale\n        data = data.clamp_(-B, B).round_().to(torch.int8)\n        return data, scale, original_shape\n    else:\n        B = 2**num_bits - 1\n        mn = torch.min(data, dim=group_dim + 1, keepdim=True)[0]\n        mx = torch.max(data, dim=group_dim + 1, keepdim=True)[0]\n\n        scale = B / (mx - mn)\n        data = data - mn\n        data.mul_(scale)\n\n        data = data.clamp_(0, B).round_().to(torch.uint8)\n        return data, mn, scale, original_shape\n\n\ndef decompress(packed_data, config):\n    \"\"\"Simulate group-wise dequantization.\"\"\"\n    if not config.enabled:\n        return packed_data\n\n    group_size, num_bits, group_dim, symmetric = (\n        config.group_size,\n        config.num_bits,\n        config.group_dim,\n        config.symmetric,\n    )\n\n    # Dequantize\n    if symmetric:\n        data, scale, original_shape = packed_data\n        data = data / scale\n    else:\n        data, mn, scale, original_shape = packed_data\n        data = data / scale\n        data.add_(mn)\n\n    # Unpad\n    pad_len = (group_size - original_shape[group_dim] % group_size) % group_size\n    if pad_len:\n        padded_original_shape = (\n            original_shape[:group_dim]\n            + (original_shape[group_dim] + pad_len,)\n            + original_shape[group_dim + 1 :]\n        )\n        data = data.reshape(padded_original_shape)\n        indices = [slice(0, x) for x in original_shape]\n        return data[indices].contiguous()\n    else:\n        return data.view(original_shape)\n"}
{"type": "source_file", "path": "graphgpt/model/GraphLlama_pl.py", "content": "import os\nimport random\nfrom typing import Any, Optional, Dict, List\nimport logging\nimport torch\nfrom lightning.pytorch import LightningModule\nfrom transformers import get_linear_schedule_with_warmup, CLIPTextModel, CLIPTokenizer, PreTrainedTokenizer, get_cosine_schedule_with_warmup\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom graphgpt.model.GraphLlama import GraphLlamaForCausalLM\nimport transformers\n\ndef find_all_linear_names(model):\n    cls = torch.nn.Linear\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nclass GraphGPT_pl(LightningModule): \n    def __init__(self,\n        training_args, model_args, data_args, tokenizer, \n        **kwargs,\n    ):\n        super().__init__()\n        self.training_args = training_args\n        self.model_args = model_args\n        self.data_args = data_args\n        compute_dtype = (torch.float16 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n\n        bnb_model_from_pretrained_args = {}\n\n    ## load 4 8 bit \n        if training_args.bits in [4, 8]:\n            from transformers import BitsAndBytesConfig\n            from peft import prepare_model_for_int8_training\n            bnb_model_from_pretrained_args.update(dict(\n                device_map={\"\": training_args.device},\n                load_in_4bit=training_args.bits == 4,\n                load_in_8bit=training_args.bits == 8,\n                quantization_config=BitsAndBytesConfig(\n                    load_in_4bit=training_args.bits == 4,\n                    load_in_8bit=training_args.bits == 8,\n                    llm_int8_threshold=6.0,\n                    llm_int8_has_fp16_weight=False,\n                    bnb_4bit_compute_dtype=compute_dtype,\n                    bnb_4bit_use_double_quant=training_args.double_quant,\n                    bnb_4bit_quant_type=training_args.quant_type # {'fp4', 'nf4'}\n                )\n            ))\n\n        if model_args.graph_tower is not None:\n            self.model = GraphLlamaForCausalLM.from_pretrained(\n                    model_args.model_name_or_path,\n                    cache_dir=training_args.cache_dir,\n                    **bnb_model_from_pretrained_args\n                ) ## TODO: add real Graph Llama model \n        else:\n            self.model = transformers.LlamaForCausalLM.from_pretrained(\n                model_args.model_name_or_path,\n                cache_dir=training_args.cache_dir,\n                **bnb_model_from_pretrained_args\n            )\n        self.model.config.pretrain_graph_model_path = self.model.config.pretrain_graph_model_path + model_args.graph_tower\n        self.model.config.use_cache = False\n        if model_args.freeze_backbone:\n            self.model.model.requires_grad_(False)\n\n        if training_args.bits in [4, 8]:\n            self.model.config.torch_dtype=(torch.float32 if training_args.fp16 else (torch.bfloat16 if training_args.bf16 else torch.float32))\n            self.model = prepare_model_for_int8_training(self.model, use_gradient_checkpointing=training_args.gradient_checkpointing)\n\n        if training_args.gradient_checkpointing and model_args.graph_tower is None:\n            if hasattr(self.model, \"enable_input_require_grads\"):\n                self.model.enable_input_require_grads()\n            else:\n                def make_inputs_require_grad(module, input, output):\n                    output.requires_grad_(True)\n                self.model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n        if training_args.lora_enable:\n            from peft import LoraConfig, get_peft_model\n            lora_config = LoraConfig(\n                r=training_args.lora_r,\n                lora_alpha=training_args.lora_alpha,\n                target_modules=find_all_linear_names(model),\n                lora_dropout=training_args.lora_dropout,\n                bias=training_args.lora_bias,\n                task_type=\"CAUSAL_LM\",\n            )\n            if training_args.bits == 16:\n                if training_args.bf16:\n                    model.to(torch.bfloat16)\n                if training_args.fp16:\n                    model.to(torch.float16)\n            logging.warning(\"Adding LoRA adapters...\")\n            model = get_peft_model(model, lora_config)\n        \n        if model_args.graph_tower is not None:\n            model_graph_dict = self.model.get_model().initialize_graph_modules(\n                graph_tower=model_args.graph_tower,\n                graph_select_layer=model_args.graph_select_layer,\n                pretrain_graph_mlp_adapter=model_args.pretrain_graph_mlp_adapter,\n                fsdp=None\n            )\n            self.model.get_graph_tower().to(dtype=compute_dtype)\n            # graph_config = model_graph_dict['graph_config']\n\n            # data_args.graph_token_len = model_graph_dict['graph_token_len']\n            # data_args.graph_processor = model_graph_dict['graph_processor']\n            data_args.is_graph = True\n\n            self.model.config.tune_graph_mlp_adapter = training_args.tune_graph_mlp_adapter = model_args.tune_graph_mlp_adapter\n            if model_args.tune_graph_mlp_adapter:\n                self.model.requires_grad_(False)\n                for p in self.model.get_model().graph_projector.parameters():\n                    p.requires_grad = True\n\n            self.model.config.freeze_graph_mlp_adapter = training_args.freeze_graph_mlp_adapter\n            if training_args.freeze_graph_mlp_adapter:\n                for p in self.model.get_model().graph_projector.parameters():\n                    p.requires_grad = False\n\n            if training_args.bits in [4, 8]:\n                self.model.get_model().graph_projector.to(dtype=compute_dtype, device=training_args.device)\n\n            self.model.config.use_graph_start_end = data_args.use_graph_start_end = model_args.use_graph_start_end\n            # graph_config.use_graph_start_end = training_args.use_graph_start_end = model_args.use_graph_start_end\n            training_args.use_graph_start_end = model_args.use_graph_start_end\n            self.model.config.sep_graph_conv_front = data_args.sep_graph_conv_front\n            self.model.initialize_graph_tokenizer(use_graph_start_end=model_args.use_graph_start_end, tokenizer=tokenizer, device='cuda',\n                                            tune_graph_mlp_adapter=model_args.tune_graph_mlp_adapter, pretrain_graph_mlp_adapter=model_args.pretrain_graph_mlp_adapter)\n\n            params_no_grad = [n for n, p in self.model.named_parameters() if not p.requires_grad]\n            if training_args.bits in [4, 8]:\n                from peft.tuners.lora import LoraLayer\n                for name, module in self.model.named_modules():\n                    if isinstance(module, LoraLayer):\n                        if training_args.bf16:\n                            module = module.to(torch.bfloat16)\n                    if 'norm' in name:\n                        module = module.to(torch.float32)\n                    if 'lm_head' in name or 'embed_tokens' in name:\n                        if hasattr(module, 'weight'):\n                            if training_args.bf16 and module.weight.dtype == torch.float32:\n                                module = module.to(torch.bfloat16)\n\n            print('************************** parameters: #', sum(p.numel() for p in self.model.parameters() if p.requires_grad))\n            tuned_params = []\n            for name, param in self.model.named_parameters():\n                if param.requires_grad:\n                    tuned_params.append(name)\n            print(tuned_params)\n        \n    def training_step(self, batch, batch_idx):\n        bs = len(batch[\"input_ids\"])\n        loss_dict = self.model(**batch)\n        loss = loss_dict['loss']\n        \n        log_dict = {f'train_loss': loss.item()}\n        self.log_dict(log_dict, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True, batch_size=bs)\n        return loss\n\n    def configure_optimizers(self):\n        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n        # no_decay = [\"bias\", \"LayerNorm.weight\"]\n        # if IS_STAGE2:\n        \n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in self.model.named_parameters()], \"lr_scale\": [1e-5, 1e-4]\n            }\n        ]\n        \n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.training_args.learning_rate)\n\n        # scheduler = get_linear_schedule_with_warmup(\n        #     optimizer,\n        #     num_warmup_steps=self.training_args.warmup_steps,\n        #     num_training_steps=self.trainer.estimated_stepping_batches,\n        # )\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=self.training_args.warmup_steps,\n            num_training_steps=self.trainer.estimated_stepping_batches,\n        )\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n        return [optimizer], [scheduler]"}
{"type": "source_file", "path": "graphgpt/serve/monitor/monitor.py", "content": "# sudo apt install pkg-config libicu-dev\n# pip install pytz gradio gdown plotly polyglot pyicu pycld2 tabulate\n\nimport argparse\nimport pickle\nimport os\nimport threading\nimport time\n\nimport gradio as gr\n\nfrom fastchat.serve.monitor.basic_stats import report_basic_stats, get_log_files\nfrom fastchat.serve.monitor.clean_battle_data import clean_battle_data\nfrom fastchat.serve.monitor.elo_analysis import report_elo_analysis_results\nfrom fastchat.utils import build_logger, get_window_url_params_js\n\n\nnotebook_url = \"https://colab.research.google.com/drive/17L9uCiAivzWfzOxo2Tb9RMauT7vS6nVU?usp=sharing\"\n\n\nlogger = build_logger(\"monitor\", \"monitor.log\")\n\n\nbasic_component_values = [None] * 6\nleader_component_values = [None] * 5\n\n\ndef make_leaderboard_md(elo_results):\n    leaderboard_md = f\"\"\"\n# Leaderboard\n[[Blog](https://lmsys.org/blog/2023-05-03-arena/)] [[GitHub]](https://github.com/lm-sys/FastChat) [[Twitter]](https://twitter.com/lmsysorg) [[Discord]](https://discord.gg/KjdtsE9V)\n\nWe use the Elo rating system to calculate the relative performance of the models. You can view the voting data, basic analyses, and calculation procedure in this [notebook]({notebook_url}). We will periodically release new leaderboards. If you want to see more models, please help us [add them](https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model).\nLast updated: {elo_results[\"last_updated_datetime\"]}\n{elo_results[\"leaderboard_table\"]}\n\"\"\"\n    return leaderboard_md\n\n\ndef update_elo_components(max_num_files, elo_results_file):\n    log_files = get_log_files(max_num_files)\n\n    # Leaderboard\n    if elo_results_file is None:\n        battles = clean_battle_data(log_files)\n        elo_results = report_elo_analysis_results(battles)\n\n        leader_component_values[0] = make_leaderboard_md(elo_results)\n        leader_component_values[1] = elo_results[\"win_fraction_heatmap\"]\n        leader_component_values[2] = elo_results[\"battle_count_heatmap\"]\n        leader_component_values[3] = elo_results[\"average_win_rate_bar\"]\n        leader_component_values[4] = elo_results[\"bootstrap_elo_rating\"]\n\n    # Basic stats\n    basic_stats = report_basic_stats(log_files)\n    md0 = f\"Last updated: {basic_stats['last_updated_datetime']}\"\n\n    md1 = \"### Action Histogram\\n\"\n    md1 += basic_stats[\"action_hist_md\"] + \"\\n\"\n\n    md2 = \"### Anony. Vote Histogram\\n\"\n    md2 += basic_stats[\"anony_vote_hist_md\"] + \"\\n\"\n\n    md3 = \"### Model Call Histogram\\n\"\n    md3 += basic_stats[\"model_hist_md\"] + \"\\n\"\n\n    md4 = \"### Model Call (Last 24 Hours)\\n\"\n    md4 += basic_stats[\"num_chats_last_24_hours\"] + \"\\n\"\n\n    basic_component_values[0] = md0\n    basic_component_values[1] = basic_stats[\"chat_dates_bar\"]\n    basic_component_values[2] = md1\n    basic_component_values[3] = md2\n    basic_component_values[4] = md3\n    basic_component_values[5] = md4\n\n\ndef update_worker(max_num_files, interval, elo_results_file):\n    while True:\n        tic = time.time()\n        update_elo_components(max_num_files, elo_results_file)\n        durtaion = time.time() - tic\n        print(f\"update duration: {durtaion:.2f} s\")\n        time.sleep(max(interval - durtaion, 0))\n\n\ndef load_demo(url_params, request: gr.Request):\n    logger.info(f\"load_demo. ip: {request.client.host}. params: {url_params}\")\n    return basic_component_values + leader_component_values\n\n\ndef build_basic_stats_tab():\n    empty = \"Loading ...\"\n    basic_component_values[:] = [empty, None, empty, empty, empty, empty]\n\n    md0 = gr.Markdown(empty)\n    gr.Markdown(\n        \"#### Figure 1: Number of model calls and votes\"\n    )\n    plot_1 = gr.Plot(show_label=False)\n    with gr.Row():\n        with gr.Column():\n            md1 = gr.Markdown(empty)\n        with gr.Column():\n            md2 = gr.Markdown(empty)\n    with gr.Row():\n        with gr.Column():\n            md3 = gr.Markdown(empty)\n        with gr.Column():\n            md4 = gr.Markdown(empty)\n    return [md0, plot_1, md1, md2, md3, md4]\n\n\ndef build_leaderboard_tab(elo_results_file):\n    if elo_results_file is not None:\n        with open(elo_results_file, \"rb\") as fin:\n            elo_results = pickle.load(fin)\n\n        md = make_leaderboard_md(elo_results)\n        p1 = elo_results[\"win_fraction_heatmap\"]\n        p2 = elo_results[\"battle_count_heatmap\"]\n        p3 = elo_results[\"average_win_rate_bar\"]\n        p4 = elo_results[\"bootstrap_elo_rating\"]\n    else:\n        md = \"Loading ...\"\n        p1 = p2 = p3 = p4 = None\n\n    leader_component_values[:] = [md, p1, p2, p3, p4]\n\n    md_1 = gr.Markdown(md)\n    gr.Markdown(\n        f\"\"\"## More Statistics\\n\nWe added some additional figures to show more statistics. The code for generating them is also included in this [notebook]({notebook_url}).\nPlease note that you may see different orders from different ranking methods. This is expected for models that perform similarly, as demonstrated by the confidence interval in the bootstrap figure. Going forward, we prefer the classical Elo calculation because of its scalability and interpretability. You can find more discussions in this blog [post](https://lmsys.org/blog/2023-05-03-arena/).\n\"\"\"\n    )\n\n    with gr.Row():\n        with gr.Column():\n            gr.Markdown(\n                \"#### Figure 1: Fraction of Model A Wins for All Non-tied A vs. B Battles\"\n            )\n            plot_1 = gr.Plot(p1, show_label=False)\n        with gr.Column():\n            gr.Markdown(\n                \"#### Figure 2: Battle Count for Each Combination of Models (without Ties)\"\n            )\n            plot_2 = gr.Plot(p2, show_label=False)\n    with gr.Row():\n        with gr.Column():\n            gr.Markdown(\n                \"#### Figure 3: Average Win Rate Against All Other Models (Assuming Uniform Sampling and No Ties)\"\n            )\n            plot_3 = gr.Plot(p3, show_label=False)\n        with gr.Column():\n            gr.Markdown(\n                \"#### Figure 4: Bootstrap of Elo Estimates (1000 Rounds of Random Sampling)\"\n            )\n            plot_4 = gr.Plot(p4, show_label=False)\n    return [md_1, plot_1, plot_2, plot_3, plot_4]\n\n\ndef build_demo(elo_results_file):\n    with gr.Blocks(\n        title=\"Monitor\",\n        theme=gr.themes.Base(),\n    ) as demo:\n        with gr.Tabs() as tabs:\n            with gr.Tab(\"Leaderboard\", id=0):\n                leader_components = build_leaderboard_tab(elo_results_file)\n\n            with gr.Tab(\"Basic Stats\", id=1):\n                basic_components = build_basic_stats_tab()\n\n        url_params = gr.JSON(visible=False)\n        demo.load(\n            load_demo,\n            [url_params],\n            basic_components + leader_components,\n            _js=get_window_url_params_js,\n        )\n\n    return demo\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", type=int)\n    parser.add_argument(\"--share\", action=\"store_true\")\n    parser.add_argument(\"--concurrency-count\", type=int, default=10)\n    parser.add_argument(\"--update-interval\", type=int, default=300)\n    parser.add_argument(\"--max-num-files\", type=int)\n    parser.add_argument(\"--elo-results-file\", type=str)\n    args = parser.parse_args()\n    logger.info(f\"args: {args}\")\n\n    update_thread = threading.Thread(\n        target=update_worker,\n        args=(args.max_num_files, args.update_interval, args.elo_results_file),\n    )\n    update_thread.start()\n\n    demo = build_demo(args.elo_results_file)\n    demo.queue(\n        concurrency_count=args.concurrency_count, status_update_rate=10, api_open=False\n    ).launch(\n        server_name=args.host, server_port=args.port, share=args.share, max_threads=200\n    )\n"}
{"type": "source_file", "path": "graphgpt/serve/monitor/basic_stats.py", "content": "import argparse\nimport code\nimport datetime\nimport json\nimport os\nfrom pytz import timezone\nimport time\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom tqdm import tqdm\n\n\ndef get_log_files(max_num_files=None):\n    dates = []\n    for month in [4, 5]:\n        for day in range(1, 32):\n            dates.append(f\"2023-{month:02d}-{day:02d}\")\n\n    num_servers = 12\n    filenames = []\n    for d in dates:\n        for i in range(num_servers):\n            name = os.path.expanduser(f\"~/fastchat_logs/server{i}/{d}-conv.json\")\n            if os.path.exists(name):\n                filenames.append(name)\n    max_num_files = max_num_files or len(filenames)\n    filenames = filenames[-max_num_files:]\n    return filenames\n\n\ndef load_log_files(log_files):\n    data = []\n    for filename in tqdm(log_files, desc=\"read files\"):\n        for retry in range(5):\n            try:\n                lines = open(filename).readlines()\n                break\n            except FileNotFoundError:\n                time.sleep(2)\n\n        for l in lines:\n            row = json.loads(l)\n\n            data.append(\n                dict(\n                    type=row[\"type\"],\n                    tstamp=row[\"tstamp\"],\n                    model=row.get(\"model\", \"\"),\n                    models=row.get(\"models\", [\"\", \"\"]),\n                )\n            )\n\n    return data\n\n\ndef get_anony_vote_df(df):\n    anony_vote_df = df[\n        df[\"type\"].isin([\"leftvote\", \"rightvote\", \"tievote\", \"bothbad_vote\"])\n    ]\n    anony_vote_df = anony_vote_df[\n        anony_vote_df[\"models\"].apply(lambda x: x[0] == \"\")\n    ]\n    return anony_vote_df\n\n\ndef merge_counts(series, on, names):\n    ret = pd.merge(series[0], series[1], on=on)\n    for i in range(2, len(series)):\n        ret = pd.merge(ret, series[i], on=on)\n    ret = ret.reset_index()\n    old_names = list(ret.columns)[-len(series) :]\n    rename = {old_name: new_name for old_name, new_name in zip(old_names, names)}\n    ret = ret.rename(columns=rename)\n    return ret\n\n\ndef report_basic_stats(log_files):\n    df_all = load_log_files(log_files)\n    df_all = pd.DataFrame(df_all)\n    now_t = df_all[\"tstamp\"].max()\n    df_1_hour = df_all[df_all[\"tstamp\"] > (now_t - 3600)]\n    df_1_day = df_all[df_all[\"tstamp\"] > (now_t - 3600 * 24)]\n    anony_vote_df_all = get_anony_vote_df(df_all)\n\n    # Chat trends\n    chat_dates = [\n        datetime.datetime.fromtimestamp(\n            x, tz=timezone(\"US/Pacific\")\n        ).strftime(\"%Y-%m-%d\")\n        for x in df_all[df_all[\"type\"] == \"chat\"][\"tstamp\"]\n    ]\n    chat_dates_counts = pd.value_counts(chat_dates)\n    vote_dates = [\n        datetime.datetime.fromtimestamp(\n            x, tz=timezone(\"US/Pacific\")\n        ).strftime(\"%Y-%m-%d\")\n        for x in anony_vote_df_all[\"tstamp\"]\n    ]\n    vote_dates_counts = pd.value_counts(vote_dates)\n    chat_dates_bar = go.Figure(data=[\n        go.Bar(name=\"Anony. Vote\", x=vote_dates_counts.index, y=vote_dates_counts,\n               text=[f\"{val:.0f}\" for val in vote_dates_counts], textposition=\"auto\"),\n        go.Bar(name=\"Chat\", x=chat_dates_counts.index, y=chat_dates_counts,\n               text=[f\"{val:.0f}\" for val in chat_dates_counts], textposition=\"auto\"),\n    ])\n    chat_dates_bar.update_layout(\n        barmode=\"stack\",\n        xaxis_title=\"Dates\",\n        yaxis_title=\"Count\",\n        height=300,\n        width=1200,\n    )\n\n    # Model call counts\n    model_hist_all = df_all[df_all[\"type\"] == \"chat\"][\"model\"].value_counts()\n    model_hist_1_day = df_1_day[df_1_day[\"type\"] == \"chat\"][\"model\"].value_counts()\n    model_hist_1_hour = df_1_hour[df_1_hour[\"type\"] == \"chat\"][\"model\"].value_counts()\n    model_hist = merge_counts(\n        [model_hist_all, model_hist_1_day, model_hist_1_hour],\n        on=\"model\",\n        names=[\"All\", \"Last Day\", \"Last Hour\"],\n    )\n    model_hist_md = model_hist.to_markdown(index=False, tablefmt=\"github\")\n\n    # Action counts\n    action_hist_all = df_all[\"type\"].value_counts()\n    action_hist_1_day = df_1_day[\"type\"].value_counts()\n    action_hist_1_hour = df_1_hour[\"type\"].value_counts()\n    action_hist = merge_counts(\n        [action_hist_all, action_hist_1_day, action_hist_1_hour],\n        on=\"type\",\n        names=[\"All\", \"Last Day\", \"Last Hour\"],\n    )\n    action_hist_md = action_hist.to_markdown(index=False, tablefmt=\"github\")\n\n    # Anony vote counts\n    anony_vote_hist_all = anony_vote_df_all[\"type\"].value_counts()\n    anony_vote_df_1_day = get_anony_vote_df(df_1_day)\n    anony_vote_hist_1_day = anony_vote_df_1_day[\"type\"].value_counts()\n    anony_vote_df_1_hour = get_anony_vote_df(df_1_hour)\n    anony_vote_hist_1_hour = anony_vote_df_1_hour[\"type\"].value_counts()\n    anony_vote_hist = merge_counts(\n        [anony_vote_hist_all, anony_vote_hist_1_day, anony_vote_hist_1_hour],\n        on=\"type\",\n        names=[\"All\", \"Last Day\", \"Last Hour\"],\n    )\n    anony_vote_hist_md = anony_vote_hist.to_markdown(index=False, tablefmt=\"github\")\n\n    # Last 24 hours\n    chat_1_day = df_1_day[df_1_day[\"type\"] == \"chat\"]\n    num_chats_last_24_hours = []\n    base = df_1_day[\"tstamp\"].min()\n    for i in range(24, 0, -1):\n        left = base + (i - 1) * 3600\n        right = base + i * 3600\n        num = ((chat_1_day[\"tstamp\"] >= left) & (chat_1_day[\"tstamp\"] < right)).sum()\n        num_chats_last_24_hours.append(num)\n    times = [\n        datetime.datetime.fromtimestamp(\n            base + i * 3600, tz=timezone(\"US/Pacific\")\n        ).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n        for i in range(24, 0, -1)\n    ]\n    last_24_hours_df = pd.DataFrame({\"time\": times, \"value\": num_chats_last_24_hours})\n    last_24_hours_md = last_24_hours_df.to_markdown(index=False, tablefmt=\"github\")\n\n    # Last update datetime\n    last_updated_tstamp = now_t\n    last_updated_datetime = datetime.datetime.fromtimestamp(\n        last_updated_tstamp, tz=timezone(\"US/Pacific\")\n    ).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n\n    # code.interact(local=locals())\n\n    return {\n        \"chat_dates_bar\": chat_dates_bar,\n        \"model_hist_md\": model_hist_md,\n        \"action_hist_md\": action_hist_md,\n        \"anony_vote_hist_md\": anony_vote_hist_md,\n        \"num_chats_last_24_hours\": last_24_hours_md,\n        \"last_updated_datetime\": last_updated_datetime,\n    }\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--max-num-files\", type=int)\n    args = parser.parse_args()\n\n    log_files = get_log_files(args.max_num_files)\n    basic_stats = report_basic_stats(log_files)\n\n    print(basic_stats[\"action_hist_md\"] + \"\\n\")\n    print(basic_stats[\"model_hist_md\"] + \"\\n\")\n    print(basic_stats[\"anony_vote_hist_md\"] + \"\\n\")\n    print(basic_stats[\"num_chats_last_24_hours\"] + \"\\n\")\n"}
{"type": "source_file", "path": "graphgpt/serve/monitor/hf_space_leaderboard_app.py", "content": "\"\"\"A gradio app that renders a static leaderboard. This is used for Hugging Face Space.\"\"\"\nimport argparse\nimport pickle\n\nimport gradio as gr\n\n\nnotebook_url = \"https://colab.research.google.com/drive/17L9uCiAivzWfzOxo2Tb9RMauT7vS6nVU?usp=sharing\"\n\n\ndef make_leaderboard_md(elo_results):\n    leaderboard_md = f\"\"\"\n# Leaderboard\n[[Blog](https://lmsys.org/blog/2023-05-03-arena/)] [[Vote](https://arena.lmsys.org/)] [[Github]](https://github.com/lm-sys/FastChat) [[Twitter]](https://twitter.com/lmsysorg) [[Discord]](https://discord.gg/KjdtsE9V)\n\nWe use the Elo rating system to calculate the relative performance of the models. You can view the voting data, basic analyses, and calculation procedure in this [notebook]({notebook_url}). We will periodically release new leaderboards. If you want to see more models, please help us [add them](https://github.com/lm-sys/FastChat/blob/main/docs/arena.md#how-to-add-a-new-model).\nLast updated: {elo_results[\"last_updated_datetime\"]}\n{elo_results[\"leaderboard_table\"]}\n\"\"\"\n    return leaderboard_md\n\n\ndef build_leaderboard_tab(elo_results_file):\n    if elo_results_file is not None:\n        with open(elo_results_file, \"rb\") as fin:\n            elo_results = pickle.load(fin)\n\n        md = make_leaderboard_md(elo_results)\n        p1 = elo_results[\"win_fraction_heatmap\"]\n        p2 = elo_results[\"battle_count_heatmap\"]\n        p3 = elo_results[\"average_win_rate_bar\"]\n        p4 = elo_results[\"bootstrap_elo_rating\"]\n    else:\n        md = \"Loading ...\"\n        p1 = p2 = p3 = p4 = None\n\n    md_1 = gr.Markdown(md)\n    gr.Markdown(\n        f\"\"\"## More Statistics\\n\nWe added some additional figures to show more statistics. The code for generating them is also included in this [notebook]({notebook_url}).\nPlease note that you may see different orders from different ranking methods. This is expected for models that perform similarly, as demonstrated by the confidence interval in the bootstrap figure. Going forward, we prefer the classical Elo calculation because of its scalability and interpretability. You can find more discussions in this blog [post](https://lmsys.org/blog/2023-05-03-arena/).\n\"\"\"\n    )\n\n    with gr.Row():\n        with gr.Column():\n            gr.Markdown(\n                \"#### Figure 1: Fraction of Model A Wins for All Non-tied A vs. B Battles\"\n            )\n            plot_1 = gr.Plot(p1, show_label=False)\n        with gr.Column():\n            gr.Markdown(\n                \"#### Figure 2: Battle Count for Each Combination of Models (without Ties)\"\n            )\n            plot_2 = gr.Plot(p2, show_label=False)\n    with gr.Row():\n        with gr.Column():\n            gr.Markdown(\n                \"#### Figure 3: Average Win Rate Against All Other Models (Assuming Uniform Sampling and No Ties)\"\n            )\n            plot_3 = gr.Plot(p3, show_label=False)\n        with gr.Column():\n            gr.Markdown(\n                \"#### Figure 4: Bootstrap of Elo Estimates (1000 Rounds of Random Sampling)\"\n            )\n            plot_4 = gr.Plot(p4, show_label=False)\n    return [md_1, plot_1, plot_2, plot_3, plot_4]\n\n\ndef build_demo(elo_results_file):\n    with gr.Blocks(\n        title=\"Chatbot Arena Leaderboard\",\n        theme=gr.themes.Base(),\n    ) as demo:\n        leader_components = build_leaderboard_tab(elo_results_file)\n\n    return demo\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--share\", action=\"store_true\")\n    args = parser.parse_args()\n\n    demo = build_demo(\"elo_results_20230508.pkl\")\n    demo.launch(share=args.share)\n"}
{"type": "source_file", "path": "graphgpt/serve/monitor/clean_battle_data.py", "content": "import argparse\nimport datetime\nimport json\nfrom pytz import timezone\nimport os\nimport time\n\nfrom tqdm import tqdm\n\nfrom fastchat.serve.monitor.basic_stats import get_log_files\nfrom fastchat.utils import detect_language\n\n\nVOTES = [\"tievote\", \"leftvote\", \"rightvote\", \"bothbad_vote\"]\nIDENTITY_WORDS = [\n    \"vicuna\",\n    \"lmsys\",\n    \"koala\",\n    \"uc berkeley\",\n    \"open assistant\",\n    \"laion\",\n    \"chatglm\",\n    \"chatgpt\",\n    \"openai\",\n    \"anthropic\",\n    \"claude\",\n    \"bard\",\n    \"palm\",\n    \"Lamda\",\n    \"google\",\n    \"**NETWORK ERROR DUE TO HIGH TRAFFIC. PLEASE REGENERATE OR REFRESH THIS PAGE.**\",\n]\n\n\ndef get_log_files(max_num_files=None):\n    dates = []\n    for month in [4]:\n        for day in range(24, 32):\n            dates.append(f\"2023-{month:02d}-{day:02d}\")\n    for month in [5]:\n        for day in range(1, 24):\n            dates.append(f\"2023-{month:02d}-{day:02d}\")\n    cutoff_date = dates[-1].replace(\"-\", \"\")\n\n    num_servers = 12\n    filenames = []\n    for d in dates:\n        for i in range(num_servers):\n            name = os.path.expanduser(f\"~/fastchat_logs/server{i}/{d}-conv.json\")\n            if os.path.exists(name):\n                filenames.append(name)\n    max_num_files = max_num_files or len(filenames)\n    filenames = filenames[-max_num_files:]\n    return filenames, cutoff_date\n\n\ndef remove_html(raw):\n    if raw.startswith(\"<h3>\"):\n        return raw[raw.find(\": \") + 2 : -len(\"</h3>\\n\")]\n    return raw\n\n\ndef clean_battle_data(log_files):\n    data = []\n    for filename in tqdm(log_files, desc=\"read files\"):\n        for retry in range(5):\n            try:\n                lines = open(filename).readlines()\n                break\n            except FileNotFoundError:\n                time.sleep(2)\n\n        for l in lines:\n            row = json.loads(l)\n            if row[\"type\"] in VOTES:\n                data.append(row)\n\n    convert_type = {\n        \"leftvote\": \"model_a\",\n        \"rightvote\": \"model_b\",\n        \"tievote\": \"tie\",\n        \"bothbad_vote\": \"tie (bothbad)\",\n    }\n\n    all_models = set()\n    ct_annoy = 0\n    ct_invalid = 0\n    ct_leaked_identity = 0\n    battles = []\n    for row in data:\n        # Resolve model names\n        models_public = [remove_html(row[\"models\"][0]), remove_html(row[\"models\"][1])]\n        if \"model_name\" in row[\"states\"][0]:\n            models_hidden = [\n                row[\"states\"][0][\"model_name\"],\n                row[\"states\"][1][\"model_name\"],\n            ]\n            if models_hidden[0] is None:\n                models_hidden = models_public\n        else:\n            models_hidden = models_public\n\n        if (models_public[0] == \"\" and models_public[1] != \"\") or (\n            models_public[1] == \"\" and models_public[0] != \"\"\n        ):\n            ct_invalid += 1\n            continue\n\n        if models_public[0] == \"\" or models_public[0] == \"Model A\":\n            anony = True\n            models = models_hidden\n            ct_annoy += 1\n        else:\n            anony = False\n            models = models_public\n            if not models_public == models_hidden:\n                ct_invalid += 1\n                continue\n\n        # Detect langauge\n        state = row[\"states\"][0]\n        if state[\"offset\"] >= len(state[\"messages\"]):\n            ct_invalid += 1\n            continue\n        lang_code = detect_language(state[\"messages\"][state[\"offset\"]][1])\n        rounds = (len(state[\"messages\"]) - state[\"offset\"]) // 2\n\n        # Drop conversations if the model names are leaked\n        leaked_identity = False\n        messages = \"\"\n        for i in range(2):\n            state = row[\"states\"][i]\n            for role, msg in state[\"messages\"][state[\"offset\"] :]:\n                if msg:\n                    messages += msg.lower()\n        for word in IDENTITY_WORDS:\n            if word in messages:\n                leaked_identity = True\n                break\n\n        if leaked_identity:\n            ct_leaked_identity += 1\n            continue\n\n        # Replace bard with palm\n        models = [m.replace(\"bard\", \"palm-2\") for m in models]\n\n        # Keep the result\n        battles.append(\n            dict(\n                model_a=models[0],\n                model_b=models[1],\n                win=convert_type[row[\"type\"]],\n                anony=anony,\n                rounds=rounds,\n                language=lang_code,\n                tstamp=row[\"tstamp\"],\n            )\n        )\n\n        all_models.update(models_hidden)\n    battles.sort(key=lambda x: x[\"tstamp\"])\n    last_updated_tstamp = battles[-1][\"tstamp\"]\n\n    last_updated_datetime = datetime.datetime.fromtimestamp(\n        last_updated_tstamp, tz=timezone(\"US/Pacific\")\n    ).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n\n    print(\n        f\"#votes: {len(data)}, #invalid votes: {ct_invalid}, \"\n        f\"#leaked_identity: {ct_leaked_identity}\"\n    )\n    print(f\"#battles: {len(battles)}, #annoy: {ct_annoy}\")\n    print(f\"#models: {len(all_models)}, {all_models}\")\n    print(f\"last-updated: {last_updated_datetime}\")\n\n    return battles\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--max-num-files\", type=int)\n    args = parser.parse_args()\n\n    log_files, cutoff_date = get_log_files(args.max_num_files)\n    battles = clean_battle_data(log_files)\n\n    print(\"Samples:\")\n    for i in range(4):\n        print(battles[i])\n\n    output = f\"clean_battle_{cutoff_date}.json\"\n    with open(output, \"w\") as fout:\n        json.dump(battles, fout, indent=2)\n    print(f\"Write cleaned data to {output}\")\n"}
{"type": "source_file", "path": "graphgpt/serve/monitor/elo_analysis.py", "content": "import argparse\nfrom collections import defaultdict\nimport datetime\nimport json\nimport math\nimport pickle\nfrom pytz import timezone\n\nimport gdown\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom tqdm import tqdm\n\nfrom fastchat.model.model_registry import get_model_info\nfrom fastchat.serve.monitor.basic_stats import get_log_files\nfrom fastchat.serve.monitor.clean_battle_data import clean_battle_data\n\n\npd.options.display.float_format = \"{:.2f}\".format\n\n\ndef compute_elo(battles, K=4, SCALE=400, BASE=10, INIT_RATING=1000):\n    rating = defaultdict(lambda: INIT_RATING)\n\n    for rd, model_a, model_b, win in battles[\n        [\"model_a\", \"model_b\", \"win\"]\n    ].itertuples():\n        ra = rating[model_a]\n        rb = rating[model_b]\n        ea = 1 / (1 + BASE ** ((rb - ra) / SCALE))\n        eb = 1 / (1 + BASE ** ((ra - rb) / SCALE))\n        if win == \"model_a\":\n            sa = 1\n        elif win == \"model_b\":\n            sa = 0\n        elif win == \"tie\" or win == \"tie (bothbad)\":\n            sa = 0.5\n        else:\n            raise Exception(f\"unexpected vote {win}\")\n        rating[model_a] += K * (sa - ea)\n        rating[model_b] += K * (1 - sa - eb)\n\n    return dict(rating)\n\n\ndef get_bootstrap_result(battles, func_compute_elo, num_round=1000):\n    rows = []\n    for i in tqdm(range(num_round), desc=\"bootstrap\"):\n        tmp_battles = battles.sample(frac=1.0, replace=True)\n        # tmp_battles = tmp_battles.sort_values(ascending=True, by=[\"tstamp\"])\n        rows.append(func_compute_elo(tmp_battles))\n    df = pd.DataFrame(rows)\n    return df[df.median().sort_values(ascending=False).index]\n\n\ndef get_elo_from_bootstrap(bootstrap_df):\n    return dict(bootstrap_df.quantile(0.5))\n\n\ndef compute_pairwise_win_fraction(battles, model_order):\n    # Times each model wins as Model A\n    a_win_ptbl = pd.pivot_table(\n        battles[battles[\"win\"] == \"model_a\"],\n        index=\"model_a\",\n        columns=\"model_b\",\n        aggfunc=\"size\",\n        fill_value=0,\n    )\n\n    # Table counting times each model wins as Model B\n    b_win_ptbl = pd.pivot_table(\n        battles[battles[\"win\"] == \"model_b\"],\n        index=\"model_a\",\n        columns=\"model_b\",\n        aggfunc=\"size\",\n        fill_value=0,\n    )\n\n    # Table counting number of A-B pairs\n    num_battles_ptbl = pd.pivot_table(\n        battles, index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0\n    )\n\n    # Computing the proportion of wins for each model as A and as B\n    # against all other models\n    row_beats_col_freq = (a_win_ptbl + b_win_ptbl.T) / (\n        num_battles_ptbl + num_battles_ptbl.T\n    )\n\n    if model_order is None:\n        prop_wins = row_beats_col_freq.mean(axis=1).sort_values(ascending=False)\n        model_order = list(prop_wins.keys())\n\n    # Arrange ordering according to proprition of wins\n    row_beats_col = row_beats_col_freq.loc[model_order, model_order]\n    return row_beats_col\n\n\ndef visualize_leaderboard_table(rating):\n    models = list(rating.keys())\n    models.sort(key=lambda k: -rating[k])\n\n    emoji_dict = {\n        1: \"🥇\",\n        2: \"🥈\",\n        3: \"🥉\",\n    }\n\n    md = \"\"\n    md += \"| Rank | Model | Elo Rating | Description |\\n\"\n    md += \"| --- | --- | --- | --- |\\n\"\n    for i, model in enumerate(models):\n        rank = i + 1\n        minfo = get_model_info(model)\n        emoji = emoji_dict.get(rank, \"\")\n        md += f\"| {rank} | {emoji} [{model}]({minfo.link}) | {rating[model]:.0f} | {minfo.description} |\\n\"\n\n    return md\n\n\ndef visualize_pairwise_win_fraction(battles, model_order):\n    row_beats_col = compute_pairwise_win_fraction(battles, model_order)\n    fig = px.imshow(\n        row_beats_col,\n        color_continuous_scale=\"RdBu\",\n        text_auto=\".2f\",\n        height=600,\n        width=600,\n    )\n    fig.update_layout(\n        xaxis_title=\"Model B\",\n        yaxis_title=\"Model A\",\n        xaxis_side=\"top\",\n        title_y=0.07,\n        title_x=0.5,\n    )\n    fig.update_traces(\n        hovertemplate=\"Model A: %{y}<br>Model B: %{x}<br>Fraction of A Wins: %{z}<extra></extra>\"\n    )\n\n    return fig\n\n\ndef visualize_battle_count(battles, model_order):\n    ptbl = pd.pivot_table(\n        battles, index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0\n    )\n    battle_counts = ptbl + ptbl.T\n    fig = px.imshow(\n        battle_counts.loc[model_order, model_order],\n        text_auto=True,\n        height=600,\n        width=600,\n    )\n    fig.update_layout(\n        xaxis_title=\"Model B\",\n        yaxis_title=\"Model A\",\n        xaxis_side=\"top\",\n        title_y=0.07,\n        title_x=0.5,\n    )\n    fig.update_traces(\n        hovertemplate=\"Model A: %{y}<br>Model B: %{x}<br>Count: %{z}<extra></extra>\"\n    )\n    return fig\n\n\ndef visualize_average_win_rate(battles):\n    row_beats_col_freq = compute_pairwise_win_fraction(battles, None)\n    fig = px.bar(\n        row_beats_col_freq.mean(axis=1).sort_values(ascending=False),\n        text_auto=\".2f\",\n        height=400,\n        width=600,\n    )\n    fig.update_layout(\n        yaxis_title=\"Average Win Rate\", xaxis_title=\"Model\", showlegend=False\n    )\n    return fig\n\n\ndef visualize_bootstrap_elo_rating(df):\n    bars = (\n        pd.DataFrame(\n            dict(\n                lower=df.quantile(0.025),\n                rating=df.quantile(0.5),\n                upper=df.quantile(0.975),\n            )\n        )\n        .reset_index(names=\"model\")\n        .sort_values(\"rating\", ascending=False)\n    )\n    bars[\"error_y\"] = bars[\"upper\"] - bars[\"rating\"]\n    bars[\"error_y_minus\"] = bars[\"rating\"] - bars[\"lower\"]\n    bars[\"rating_rounded\"] = np.round(bars[\"rating\"], 2)\n    fig = px.scatter(\n        bars,\n        x=\"model\",\n        y=\"rating\",\n        error_y=\"error_y\",\n        error_y_minus=\"error_y_minus\",\n        text=\"rating_rounded\",\n        height=400,\n        width=600,\n    )\n    fig.update_layout(xaxis_title=\"Model\", yaxis_title=\"Rating\")\n    return fig\n\n\ndef report_elo_analysis_results(battles_json):\n    battles = pd.DataFrame(battles_json)\n    battles = battles.sort_values(ascending=True, by=[\"tstamp\"])\n    # Only use anonymous votes\n    battles = battles[battles[\"anony\"]].reset_index(drop=True)\n    battles_no_ties = battles[~battles[\"win\"].str.contains(\"tie\")]\n\n    # Online update\n    elo_rating_online = compute_elo(battles)\n\n    # Bootstrap\n    bootstrap_df = get_bootstrap_result(battles, compute_elo)\n    elo_rating_median = get_elo_from_bootstrap(bootstrap_df)\n    elo_rating_median = {k: int(v + 0.5) for k, v in elo_rating_median.items()}\n    model_order = list(elo_rating_online.keys())\n    model_order.sort(key=lambda k: -elo_rating_online[k])\n\n    # Plots\n    leaderboard_table = visualize_leaderboard_table(elo_rating_online)\n    win_fraction_heatmap = visualize_pairwise_win_fraction(battles_no_ties, model_order)\n    battle_count_heatmap = visualize_battle_count(battles_no_ties, model_order)\n    average_win_rate_bar = visualize_average_win_rate(battles_no_ties)\n    bootstrap_elo_rating = visualize_bootstrap_elo_rating(bootstrap_df)\n\n    last_updated_tstamp = battles[\"tstamp\"].max()\n    last_updated_datetime = datetime.datetime.fromtimestamp(\n        last_updated_tstamp, tz=timezone(\"US/Pacific\")\n    ).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n\n    return {\n        \"elo_rating_online\": elo_rating_online,\n        \"elo_rating_median\": elo_rating_median,\n        \"leaderboard_table\": leaderboard_table,\n        \"win_fraction_heatmap\": win_fraction_heatmap,\n        \"battle_count_heatmap\": battle_count_heatmap,\n        \"average_win_rate_bar\": average_win_rate_bar,\n        \"bootstrap_elo_rating\": bootstrap_elo_rating,\n        \"last_updated_datetime\": last_updated_datetime,\n    }\n\n\ndef pretty_print_elo_rating(rating):\n    model_order = list(rating.keys())\n    model_order.sort(key=lambda k: -rating[k])\n    for i, model in enumerate(model_order):\n        print(f\"{i+1:2d}, {model:25s}, {rating[model]:.0f}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--clean-battle-file\", type=str)\n    parser.add_argument(\"--max-num-files\", type=int)\n    args = parser.parse_args()\n\n    if args.clean_battle_file:\n        # Read data from a cleaned battle files\n        battles = pd.read_json(args.clean_battle_file)\n    else:\n        # Read data from all log files\n        log_files = get_log_files(args.max_num_files)\n        battles = clean_battle_data(log_files)\n\n    results = report_elo_analysis_results(battles)\n\n    print(\"# Online\")\n    pretty_print_elo_rating(results[\"elo_rating_online\"])\n    print(\"# Median\")\n    pretty_print_elo_rating(results[\"elo_rating_median\"])\n    print(f\"last update : {results['last_updated_datetime']}\")\n\n    with open(\"elo_results.pkl\", \"wb\") as fout:\n        pickle.dump(results, fout)\n"}
