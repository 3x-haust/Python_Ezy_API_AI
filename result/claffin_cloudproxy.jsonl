{"repo_info": {"repo_name": "cloudproxy", "repo_owner": "claffin", "repo_url": "https://github.com/claffin/cloudproxy"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_providers_digitalocean_config.py", "content": "import os\n\nfrom cloudproxy.providers.config import set_auth\nfrom cloudproxy.providers import settings\n\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n\n\ndef test_set_auth():\n    with open(os.path.join(__location__, 'test_user_data.sh')) as file:\n        filedata = file.read()\n    settings.config[\"no_auth\"] = False\n    settings.config[\"only_host_ip\"] = False  # Ensure we use generic UFW rules\n    assert set_auth(\"testingusername\", \"testinguserpassword\") == filedata\n    "}
{"type": "test_file", "path": "tests/test_api_providers.py", "content": "import pytest\nfrom fastapi.testclient import TestClient\nfrom unittest.mock import patch, MagicMock\nfrom cloudproxy.main import app\nfrom cloudproxy.providers import settings\n\nclient = TestClient(app)\n\n\n@pytest.fixture\ndef mock_providers_config(monkeypatch):\n    \"\"\"Fixture for mock provider configuration.\"\"\"\n    mock_config = {\n        \"providers\": {\n            \"digitalocean\": {\n                \"enabled\": True,\n                \"ips\": [\"192.168.1.1\", \"192.168.1.2\", \"192.168.2.1\", \"192.168.2.2\"],  # Combined IPs from all instances\n                \"instances\": {\n                    \"default\": {\n                        \"enabled\": True,\n                        \"secrets\": {\"access_token\": \"default-do-token\"},\n                        \"size\": \"s-1vcpu-1gb\",\n                        \"region\": \"lon1\",\n                        \"min_scaling\": 2,\n                        \"max_scaling\": 5,\n                        \"display_name\": \"London DO\",\n                        \"ips\": [\"192.168.1.1\", \"192.168.1.2\"],\n                        \"scaling\": {\"min_scaling\": 2, \"max_scaling\": 5}\n                    },\n                    \"nyc\": {\n                        \"enabled\": True,\n                        \"secrets\": {\"access_token\": \"nyc-do-token\"},\n                        \"size\": \"s-1vcpu-1gb\",\n                        \"region\": \"nyc1\",\n                        \"min_scaling\": 1,\n                        \"max_scaling\": 3,\n                        \"display_name\": \"New York DO\",\n                        \"ips\": [\"192.168.2.1\", \"192.168.2.2\"],\n                        \"scaling\": {\"min_scaling\": 1, \"max_scaling\": 3}\n                    }\n                }\n            },\n            \"aws\": {\n                \"enabled\": True,\n                \"ips\": [\"10.0.1.1\", \"10.0.1.2\", \"10.0.2.1\", \"10.0.2.2\"],  # Combined IPs from all instances\n                \"instances\": {\n                    \"default\": {\n                        \"enabled\": True,\n                        \"secrets\": {\n                            \"access_key_id\": \"default-aws-key\",\n                            \"secret_access_key\": \"default-aws-secret\"\n                        },\n                        \"size\": \"t2.micro\",\n                        \"region\": \"us-east-1\",\n                        \"min_scaling\": 2,\n                        \"max_scaling\": 4,\n                        \"spot\": False,\n                        \"display_name\": \"US East AWS\",\n                        \"ips\": [\"10.0.1.1\", \"10.0.1.2\"],\n                        \"scaling\": {\"min_scaling\": 2, \"max_scaling\": 4}\n                    },\n                    \"eu\": {\n                        \"enabled\": True,\n                        \"secrets\": {\n                            \"access_key_id\": \"eu-aws-key\",\n                            \"secret_access_key\": \"eu-aws-secret\"\n                        },\n                        \"size\": \"t2.micro\",\n                        \"region\": \"eu-west-1\",\n                        \"min_scaling\": 1,\n                        \"max_scaling\": 2,\n                        \"spot\": True,\n                        \"display_name\": \"EU West AWS\",\n                        \"ips\": [\"10.0.2.1\", \"10.0.2.2\"],\n                        \"scaling\": {\"min_scaling\": 1, \"max_scaling\": 2}\n                    }\n                }\n            },\n            \"hetzner\": {\n                \"enabled\": True,\n                \"ips\": [\"172.16.1.1\", \"172.16.1.2\"],  # Combined IPs from all instances\n                \"instances\": {\n                    \"default\": {\n                        \"enabled\": True,\n                        \"secrets\": {\"access_token\": \"default-hetzner-token\"},\n                        \"size\": \"cx11\",\n                        \"location\": \"nbg1\",\n                        \"min_scaling\": 2,\n                        \"max_scaling\": 3,\n                        \"display_name\": \"Germany Hetzner\",\n                        \"ips\": [\"172.16.1.1\", \"172.16.1.2\"],\n                        \"scaling\": {\"min_scaling\": 2, \"max_scaling\": 3}\n                    }\n                }\n            },\n            \"gcp\": {\n                \"enabled\": False,\n                \"zone\": \"us-east1-b\",\n                \"image_project\": \"debian-cloud\",\n                \"image_family\": \"debian-11\",\n                \"ips\": [],  # No IPs since it's disabled\n                \"scaling\": {\"min_scaling\": 0, \"max_scaling\": 0},\n                \"size\": \"e2-small\",\n                \"instances\": {\n                    \"default\": {\n                        \"enabled\": False,\n                        \"secrets\": {\"json_key\": \"mock-gcp-key\"},\n                        \"zone\": \"us-east1-b\",\n                        \"image_project\": \"debian-cloud\",\n                        \"image_family\": \"debian-11\",\n                        \"size\": \"e2-small\",\n                        \"min_scaling\": 0,\n                        \"max_scaling\": 0,\n                        \"display_name\": \"US East GCP\",\n                        \"ips\": [],\n                        \"scaling\": {\"min_scaling\": 0, \"max_scaling\": 0}\n                    }\n                }\n            }\n        }\n    }\n    \n    monkeypatch.setattr(settings, \"config\", mock_config)\n    return mock_config[\"providers\"]\n\n\n@pytest.fixture\ndef mock_provider_ips():\n    \"\"\"Fixture for mock provider IPs.\"\"\"\n    # Original provider IPs\n    original_ips = {\n        provider: {\n            instance: settings.config[\"providers\"][provider][\"instances\"][instance].get(\"ips\", []).copy()\n            for instance in settings.config[\"providers\"][provider][\"instances\"]\n        } \n        for provider in settings.config[\"providers\"]\n        if provider in settings.config[\"providers\"]\n    }\n    \n    # Set mock IPs for each provider instance\n    mock_ips = {\n        \"digitalocean\": {\n            \"default\": [\"192.168.1.1\", \"192.168.1.2\"],\n            \"nyc\": [\"192.168.2.1\", \"192.168.2.2\"]\n        },\n        \"aws\": {\n            \"default\": [\"10.0.1.1\", \"10.0.1.2\"],\n            \"eu\": [\"10.0.2.1\", \"10.0.2.2\"]\n        },\n        \"hetzner\": {\n            \"default\": [\"172.16.1.1\", \"172.16.1.2\"]\n        }\n    }\n    \n    # Update config with mock IPs\n    for provider, instances in mock_ips.items():\n        for instance, ips in instances.items():\n            settings.config[\"providers\"][provider][\"instances\"][instance][\"ips\"] = ips\n    \n    yield mock_ips\n    \n    # Restore original IPs\n    for provider, instances in original_ips.items():\n        for instance, ips in instances.items():\n            if provider in settings.config[\"providers\"] and instance in settings.config[\"providers\"][provider][\"instances\"]:\n                settings.config[\"providers\"][provider][\"instances\"][instance][\"ips\"] = ips\n\n\ndef test_get_providers_list(mock_providers_config, mock_provider_ips):\n    \"\"\"Test retrieving the list of all providers with their instances.\"\"\"\n    response = client.get(\"/providers\")\n    assert response.status_code == 200\n    \n    data = response.json()\n    assert \"metadata\" in data\n    assert \"providers\" in data\n    \n    providers = data[\"providers\"]\n    \n    # Check DigitalOcean provider\n    assert \"digitalocean\" in providers\n    digitalocean = providers[\"digitalocean\"]\n    assert digitalocean[\"enabled\"] is True\n    assert len(digitalocean[\"ips\"]) == 4  # Combined IPs from all instances\n    assert \"instances\" in digitalocean\n    \n    # Check DigitalOcean instances\n    do_instances = digitalocean[\"instances\"]\n    assert \"default\" in do_instances\n    assert \"nyc\" in do_instances\n    assert do_instances[\"default\"][\"display_name\"] == \"London DO\"\n    assert do_instances[\"nyc\"][\"display_name\"] == \"New York DO\"\n    \n    # Check AWS provider\n    assert \"aws\" in providers\n    aws = providers[\"aws\"]\n    assert aws[\"enabled\"] is True\n    assert len(aws[\"ips\"]) == 4  # Combined IPs from all instances\n    \n    # Check AWS instances\n    aws_instances = aws[\"instances\"]\n    assert \"default\" in aws_instances\n    assert \"eu\" in aws_instances\n    assert aws_instances[\"default\"][\"display_name\"] == \"US East AWS\"\n    assert aws_instances[\"eu\"][\"display_name\"] == \"EU West AWS\"\n    assert aws_instances[\"eu\"][\"spot\"] is True\n\n\ndef test_get_provider_details(mock_providers_config, mock_provider_ips):\n    \"\"\"Test retrieving details for a specific provider with all its instances.\"\"\"\n    response = client.get(\"/providers/digitalocean\")\n    assert response.status_code == 200\n    \n    data = response.json()\n    assert \"metadata\" in data\n    assert \"message\" in data\n    assert \"provider\" in data\n    assert \"instances\" in data\n    \n    # Check provider details\n    provider = data[\"provider\"]\n    assert provider[\"enabled\"] is True\n    assert len(provider[\"ips\"]) == 4  # Combined IPs from all instances\n    \n    # Check instances details\n    instances = data[\"instances\"]\n    assert \"default\" in instances\n    assert \"nyc\" in instances\n    \n    default_instance = instances[\"default\"]\n    assert default_instance[\"enabled\"] is True\n    assert default_instance[\"region\"] == \"lon1\"\n    assert default_instance[\"size\"] == \"s-1vcpu-1gb\"\n    assert default_instance[\"min_scaling\"] == 2\n    assert default_instance[\"max_scaling\"] == 5\n    assert default_instance[\"display_name\"] == \"London DO\"\n    assert len(default_instance[\"ips\"]) == 2\n    \n    nyc_instance = instances[\"nyc\"]\n    assert nyc_instance[\"enabled\"] is True\n    assert nyc_instance[\"region\"] == \"nyc1\"\n    assert nyc_instance[\"min_scaling\"] == 1\n    assert nyc_instance[\"max_scaling\"] == 3\n    assert nyc_instance[\"display_name\"] == \"New York DO\"\n    assert len(nyc_instance[\"ips\"]) == 2\n\n\ndef test_get_provider_instance_details(mock_providers_config, mock_provider_ips):\n    \"\"\"Test retrieving details for a specific instance of a provider.\"\"\"\n    response = client.get(\"/providers/aws/eu\")\n    assert response.status_code == 200\n    \n    data = response.json()\n    assert \"metadata\" in data\n    assert \"message\" in data\n    assert \"provider\" in data\n    assert \"instance\" in data\n    assert \"config\" in data\n    \n    # Check response structure\n    assert data[\"provider\"] == \"aws\"\n    assert data[\"instance\"] == \"eu\"\n    \n    # Check instance config\n    config = data[\"config\"]\n    assert config[\"enabled\"] is True\n    assert config[\"region\"] == \"eu-west-1\"\n    assert config[\"size\"] == \"t2.micro\"\n    assert config[\"spot\"] is True\n    assert \"scaling\" in config\n    assert config[\"scaling\"][\"min_scaling\"] == 1\n    assert config[\"scaling\"][\"max_scaling\"] == 2\n    assert config[\"display_name\"] == \"EU West AWS\"\n    assert len(config[\"ips\"]) == 2\n    assert config[\"ips\"] == mock_provider_ips[\"aws\"][\"eu\"]\n\n\ndef test_update_provider_instance_scaling(mock_providers_config):\n    \"\"\"Test updating scaling configuration for a provider instance.\"\"\"\n    # Data to send\n    update_data = {\n        \"min_scaling\": 3,\n        \"max_scaling\": 5\n    }\n    \n    response = client.patch(\"/providers/aws/eu\", json=update_data)\n    assert response.status_code == 200\n    \n    data = response.json()\n    assert \"message\" in data\n    assert \"updated successfully\" in data[\"message\"]\n    assert \"aws\" in data[\"message\"]\n    assert \"eu\" in data[\"message\"]\n    \n    # Verify config was updated\n    config = data[\"config\"]\n    assert config[\"scaling\"][\"min_scaling\"] == 3\n    assert config[\"scaling\"][\"max_scaling\"] == 5\n    \n    # Verify actual settings were updated\n    assert settings.config[\"providers\"][\"aws\"][\"instances\"][\"eu\"][\"scaling\"][\"min_scaling\"] == 3\n    assert settings.config[\"providers\"][\"aws\"][\"instances\"][\"eu\"][\"scaling\"][\"max_scaling\"] == 5\n\n\ndef test_update_provider_instance_scaling_validation_error(mock_providers_config):\n    \"\"\"Test validation error when max_scaling < min_scaling.\"\"\"\n    # Invalid data to send\n    update_data = {\n        \"min_scaling\": 5,\n        \"max_scaling\": 3  # Less than min_scaling\n    }\n    \n    response = client.patch(\"/providers/aws/eu\", json=update_data)\n    assert response.status_code == 422  # Unprocessable Entity\n    \n    data = response.json()\n    assert \"detail\" in data\n    \n    # Original values should remain unchanged\n    assert settings.config[\"providers\"][\"aws\"][\"instances\"][\"eu\"][\"min_scaling\"] == 1\n    assert settings.config[\"providers\"][\"aws\"][\"instances\"][\"eu\"][\"max_scaling\"] == 2\n\n\ndef test_get_nonexistent_provider():\n    \"\"\"Test retrieving a provider that doesn't exist.\"\"\"\n    response = client.get(\"/providers/nonexistent\")\n    assert response.status_code == 404\n    \n    data = response.json()\n    assert \"detail\" in data\n    assert \"nonexistent\" in data[\"detail\"]\n\n\ndef test_get_nonexistent_provider_instance():\n    \"\"\"Test retrieving an instance that doesn't exist for a provider.\"\"\"\n    response = client.get(\"/providers/aws/nonexistent\")\n    assert response.status_code == 404\n    \n    data = response.json()\n    assert \"detail\" in data\n    assert \"nonexistent\" in data[\"detail\"]\n\n\n@patch('cloudproxy.main.get_provider_model')\ndef test_provider_models_with_instances(mock_get_provider_model, mock_providers_config):\n    \"\"\"Test that provider models include instances data.\"\"\"\n    # Create a real BaseProvider object instead of a MagicMock\n    from cloudproxy.main import BaseProvider, ProviderScaling, ProviderInstance\n    \n    # Create a proper provider model for each provider\n    def get_mock_provider(provider_name, provider_config):\n        provider = BaseProvider(\n            enabled=True,\n            ips=[],\n            region=\"us-east-1\",\n            size=\"t2.micro\",\n            image=\"\",\n            scaling=ProviderScaling(min_scaling=2, max_scaling=5),\n            instances={}\n        )\n        # Add test instance to the provider\n        provider.instances[\"default\"] = ProviderInstance(\n            enabled=True,\n            ips=[],\n            scaling=ProviderScaling(min_scaling=2, max_scaling=5),\n            size=\"t2.micro\",\n            region=\"us-east-1\",\n            display_name=\"Test Instance\"\n        )\n        return provider\n    \n    # Configure the mock to return our proper provider object\n    mock_get_provider_model.side_effect = get_mock_provider\n    \n    # Make a request to trigger provider model creation\n    response = client.get(\"/providers\")\n    \n    # Verify response\n    assert response.status_code == 200\n    assert \"providers\" in response.json() "}
{"type": "test_file", "path": "tests/test_providers_aws_main.py", "content": "import pytest\nfrom unittest.mock import patch, Mock\nimport datetime\nfrom datetime import timezone\nimport time\n\nfrom cloudproxy.providers.aws.main import (\n    aws_deployment,\n    aws_check_alive,\n    aws_check_delete,\n    aws_check_stop,\n    aws_start\n)\nfrom cloudproxy.providers.settings import delete_queue, restart_queue, config\n\n# Setup fixtures\n@pytest.fixture\ndef setup_instances():\n    \"\"\"Setup test instances and restore original state after test\"\"\"\n    # Save original values\n    original_delete_queue = delete_queue.copy()\n    original_restart_queue = restart_queue.copy()\n    \n    # Calculate a launch time that's just a few seconds ago to avoid age limit recycling\n    just_now = datetime.datetime.now(timezone.utc)\n    \n    # Create test instances\n    running_instance = {\n        \"Instances\": [\n            {\n                \"InstanceId\": \"i-12345\",\n                \"PublicIpAddress\": \"1.2.3.4\",\n                \"State\": {\n                    \"Name\": \"running\"\n                },\n                \"LaunchTime\": just_now  # Use a recent launch time\n            }\n        ]\n    }\n    \n    stopped_instance = {\n        \"Instances\": [\n            {\n                \"InstanceId\": \"i-67890\",\n                \"PublicIpAddress\": \"5.6.7.8\",\n                \"State\": {\n                    \"Name\": \"stopped\"\n                },\n                \"LaunchTime\": just_now  # Use a recent launch time\n            }\n        ]\n    }\n    \n    instances = [running_instance, stopped_instance]\n    \n    yield instances\n    \n    # Restore original state\n    delete_queue.clear()\n    delete_queue.update(original_delete_queue)\n    restart_queue.clear()\n    restart_queue.update(original_restart_queue)\n\n@pytest.fixture\ndef test_instance_config():\n    \"\"\"Create a test instance configuration\"\"\"\n    return {\n        \"enabled\": True,\n        \"ips\": [],\n        \"scaling\": {\"min_scaling\": 2, \"max_scaling\": 5},\n        \"size\": \"t3.micro\",\n        \"region\": \"us-west-2\",\n        \"ami\": \"ami-test\",\n        \"display_name\": \"Test Instance\",\n        \"secrets\": {\n            \"access_key_id\": \"test-access-key\",\n            \"secret_access_key\": \"test-secret-key\"\n        },\n        \"spot\": False\n    }\n\n@patch('cloudproxy.providers.aws.main.list_instances')\n@patch('cloudproxy.providers.aws.main.create_proxy')\n@patch('cloudproxy.providers.aws.main.delete_proxy')\ndef test_aws_deployment_scale_up(mock_delete_proxy, mock_create_proxy, mock_list_instances, setup_instances):\n    \"\"\"Test scaling up AWS instances\"\"\"\n    # Setup - Only 2 instances, need to scale up to 4\n    mock_list_instances.return_value = setup_instances\n    mock_create_proxy.return_value = True\n    \n    # Execute\n    min_scaling = 4\n    result = aws_deployment(min_scaling)\n    \n    # Verify\n    assert mock_create_proxy.call_count == 2  # Should create 2 new instances\n    assert mock_delete_proxy.call_count == 0  # Should not delete any\n    assert result == 2  # Returns number of instances after deployment\n\n@patch('cloudproxy.providers.aws.main.list_instances')\n@patch('cloudproxy.providers.aws.main.create_proxy')\n@patch('cloudproxy.providers.aws.main.delete_proxy')\ndef test_aws_deployment_with_instance_config(mock_delete_proxy, mock_create_proxy, mock_list_instances, setup_instances, test_instance_config):\n    \"\"\"Test scaling up AWS instances with specific instance configuration\"\"\"\n    # Setup - Only 2 instances, need to scale up to 4\n    mock_list_instances.return_value = setup_instances\n    mock_create_proxy.return_value = True\n    \n    # Execute\n    min_scaling = 4\n    result = aws_deployment(min_scaling, test_instance_config)\n    \n    # Verify\n    assert mock_create_proxy.call_count == 2  # Should create 2 new instances\n    \n    # Check that create_proxy was called with the test_instance_config\n    for call in mock_create_proxy.call_args_list:\n        assert call[0][0] == test_instance_config\n    \n    # Check that list_instances was called with the test_instance_config\n    mock_list_instances.assert_called_with(test_instance_config)\n    \n    assert mock_delete_proxy.call_count == 0  # Should not delete any\n    assert result == 2  # Returns number of instances after deployment\n\n@patch('cloudproxy.providers.aws.main.list_instances')\n@patch('cloudproxy.providers.aws.main.create_proxy')\n@patch('cloudproxy.providers.aws.main.delete_proxy')\ndef test_aws_deployment_scale_down(mock_delete_proxy, mock_create_proxy, mock_list_instances, setup_instances):\n    \"\"\"Test scaling down AWS instances\"\"\"\n    # Setup - 2 instances, need to scale down to 1\n    mock_list_instances.return_value = setup_instances\n    mock_delete_proxy.return_value = True\n    \n    # Execute\n    min_scaling = 1\n    result = aws_deployment(min_scaling)\n    \n    # Verify\n    assert mock_delete_proxy.call_count == 1  # Should delete 1 instance\n    assert mock_create_proxy.call_count == 0  # Should not create any\n    assert result == 2  # Returns number of instances after deployment\n\n@patch('cloudproxy.providers.aws.main.list_instances')\n@patch('cloudproxy.providers.aws.main.create_proxy')\n@patch('cloudproxy.providers.aws.main.delete_proxy')\ndef test_aws_deployment_scale_down_with_instance_config(mock_delete_proxy, mock_create_proxy, mock_list_instances, setup_instances, test_instance_config):\n    \"\"\"Test scaling down AWS instances with specific instance configuration\"\"\"\n    # Setup - 2 instances, need to scale down to 1\n    mock_list_instances.return_value = setup_instances\n    mock_delete_proxy.return_value = True\n    \n    # Execute\n    min_scaling = 1\n    result = aws_deployment(min_scaling, test_instance_config)\n    \n    # Verify\n    assert mock_delete_proxy.call_count == 1  # Should delete 1 instance\n    \n    # Check that delete_proxy was called with the correct instance ID and config\n    mock_delete_proxy.assert_called_once_with(\n        setup_instances[0][\"Instances\"][0][\"InstanceId\"], \n        test_instance_config\n    )\n    \n    assert mock_create_proxy.call_count == 0  # Should not create any\n    assert result == 2  # Returns number of instances after deployment\n\n@patch('cloudproxy.providers.aws.main.list_instances')\n@patch('cloudproxy.providers.aws.main.create_proxy')\n@patch('cloudproxy.providers.aws.main.delete_proxy')\ndef test_aws_deployment_no_change(mock_delete_proxy, mock_create_proxy, mock_list_instances, setup_instances):\n    \"\"\"Test when no scaling change is needed\"\"\"\n    # Setup - 2 instances, need to keep 2\n    mock_list_instances.return_value = setup_instances\n    \n    # Execute\n    min_scaling = 2\n    result = aws_deployment(min_scaling)\n    \n    # Verify\n    assert mock_delete_proxy.call_count == 0  # Should not delete any\n    assert mock_create_proxy.call_count == 0  # Should not create any\n    assert result == 2  # Returns number of instances\n\n@patch('cloudproxy.providers.aws.main.list_instances')\n@patch('cloudproxy.providers.aws.main.create_proxy')\n@patch('cloudproxy.providers.aws.main.delete_proxy')\ndef test_aws_deployment_no_change_with_instance_config(mock_delete_proxy, mock_create_proxy, mock_list_instances, setup_instances, test_instance_config):\n    \"\"\"Test when no scaling change is needed with specific instance configuration\"\"\"\n    # Setup - 2 instances, need to keep 2\n    mock_list_instances.return_value = setup_instances\n    \n    # Execute with instance config\n    min_scaling = 2\n    result = aws_deployment(min_scaling, test_instance_config)\n    \n    # Verify\n    assert mock_delete_proxy.call_count == 0  # Should not delete any\n    assert mock_create_proxy.call_count == 0  # Should not create any\n    \n    # Check that list_instances was called with the test_instance_config\n    mock_list_instances.assert_called_with(test_instance_config)\n    \n    assert result == 2  # Returns number of instances\n\ndef test_aws_check_alive_running_directly():\n    \"\"\"Test the aws_check_alive function directly with real function execution\"\"\"\n    # This is a direct test that actually calls the real function,\n    # but intercepts calls to dependencies with mocks\n    with patch('cloudproxy.providers.aws.main.list_instances') as mock_list_instances:\n        with patch('cloudproxy.providers.aws.main.check_alive') as mock_check_alive:\n            with patch('cloudproxy.providers.aws.main.delete_proxy') as mock_delete_proxy:\n                with patch('cloudproxy.providers.aws.main.start_proxy') as mock_start_proxy:\n                    # Setup mocks\n                    recent_time = datetime.datetime.now(timezone.utc)\n                    mock_list_instances.return_value = [{\n                        \"Instances\": [{\n                            \"InstanceId\": \"i-12345\",\n                            \"PublicIpAddress\": \"1.2.3.4\",\n                            \"State\": {\"Name\": \"running\"},\n                            \"LaunchTime\": recent_time\n                        }]\n                    }]\n                    mock_check_alive.return_value = True\n                    \n                    # Execute\n                    result = aws_check_alive()\n                    \n                    # Verify\n                    assert \"1.2.3.4\" in result\n                    assert mock_delete_proxy.call_count == 0  # Shouldn't delete recent instances\n                    assert mock_start_proxy.call_count == 0  # Shouldn't start running instances\n\n@patch('cloudproxy.providers.aws.main.check_alive')\n@patch('cloudproxy.providers.aws.main.list_instances')\n@patch('cloudproxy.providers.aws.main.delete_proxy')\n@patch('cloudproxy.providers.aws.main.start_proxy')\ndef test_aws_check_alive_with_instance_config(mock_start_proxy, mock_delete_proxy, mock_list_instances, \n                                             mock_check_alive, setup_instances, test_instance_config):\n    \"\"\"Test checking alive for instances with specific instance configuration\"\"\"\n    # Setup\n    mock_list_instances.return_value = setup_instances\n    mock_check_alive.return_value = True\n    \n    # Execute with instance config\n    result = aws_check_alive(test_instance_config)\n    \n    # Verify\n    mock_list_instances.assert_called_once_with(test_instance_config)\n    assert \"1.2.3.4\" in result  # IP from running instance should be in result\n    \n    # For stopped instance, start_proxy should be called\n    mock_start_proxy.assert_called_once_with(\n        setup_instances[1][\"Instances\"][0][\"InstanceId\"], \n        test_instance_config\n    )\n\n@patch('cloudproxy.providers.aws.main.check_alive')\n@patch('cloudproxy.providers.aws.main.list_instances')\n@patch('cloudproxy.providers.aws.main.delete_proxy')\n@patch('cloudproxy.providers.aws.main.start_proxy')\ndef test_aws_check_alive_stopped(mock_start_proxy, mock_delete_proxy, mock_list_instances, mock_check_alive, setup_instances):\n    \"\"\"Test checking alive for stopped instances\"\"\"\n    # Setup\n    mock_list_instances.return_value = [setup_instances[1]]  # Just the stopped instance\n    mock_start_proxy.return_value = True\n    mock_check_alive.return_value = False  # Instance not reachable yet since we just started it\n    \n    # Execute\n    result = aws_check_alive()\n    \n    # Verify\n    mock_start_proxy.call_count == 1  # Should start the stopped instance\n    assert mock_delete_proxy.call_count == 0  # Should not delete any\n    assert len(result) == 0  # No IPs ready yet as instance was just started\n\ndef test_aws_check_alive_age_limit_exceeded_directly():\n    \"\"\"Test the aws_check_alive function directly with simulated old instance\"\"\"\n    # Save original age limit value\n    original_age_limit = config[\"age_limit\"]\n    \n    try:\n        # Set age limit to a small value to make instances expire quickly\n        config[\"age_limit\"] = 60  # 60 seconds\n        \n        # Create a mock instance with a launch time far in the past\n        with patch('cloudproxy.providers.aws.main.list_instances') as mock_list_instances:\n            with patch('cloudproxy.providers.aws.main.check_alive') as mock_check_alive:\n                with patch('cloudproxy.providers.aws.main.delete_proxy') as mock_delete_proxy:\n                    with patch('cloudproxy.providers.aws.main.start_proxy') as mock_start_proxy:\n                        # Setup mocks with an old instance (from year 2000)\n                        very_old_time = datetime.datetime(2000, 1, 1, tzinfo=timezone.utc)\n                        mock_list_instances.return_value = [{\n                            \"Instances\": [{\n                                \"InstanceId\": \"i-12345\", \n                                \"PublicIpAddress\": \"1.2.3.4\",\n                                \"State\": {\"Name\": \"running\"},\n                                \"LaunchTime\": very_old_time\n                            }]\n                        }]\n                        mock_check_alive.return_value = True\n                        mock_delete_proxy.return_value = True\n                        \n                        # Execute\n                        result = aws_check_alive()\n                        \n                        # Verify\n                        assert mock_delete_proxy.call_count == 1  # Should delete the expired instance\n                        assert len(result) == 0  # No IPs in result as the instance was deleted\n    finally:\n        # Restore original age limit\n        config[\"age_limit\"] = original_age_limit\n\n@patch('cloudproxy.providers.aws.main.list_instances')\n@patch('cloudproxy.providers.aws.main.delete_proxy')\ndef test_aws_check_delete(mock_delete_proxy, mock_list_instances, setup_instances):\n    \"\"\"Test checking for instances to delete\"\"\"\n    # Setup\n    mock_list_instances.return_value = setup_instances\n    mock_delete_proxy.return_value = True\n    delete_queue.add(\"1.2.3.4\")  # Add IP to delete queue\n    \n    # Execute\n    aws_check_delete()\n    \n    # Verify\n    assert mock_delete_proxy.call_count == 1  # Should delete the instance in delete queue\n    assert \"1.2.3.4\" not in delete_queue  # IP should be removed from queue\n\n@patch('cloudproxy.providers.aws.main.list_instances')\n@patch('cloudproxy.providers.aws.main.delete_proxy')\ndef test_aws_check_delete_with_instance_config(mock_delete_proxy, mock_list_instances, setup_instances, test_instance_config):\n    \"\"\"Test checking for instances to delete with specific instance configuration\"\"\"\n    # Setup\n    mock_list_instances.return_value = setup_instances\n    mock_delete_proxy.return_value = True\n    delete_queue.add(\"1.2.3.4\")  # Add IP to delete queue\n    \n    # Execute with instance config\n    aws_check_delete(test_instance_config)\n    \n    # Verify\n    mock_list_instances.assert_called_once_with(test_instance_config)\n    \n    # Should delete the instance with the instance config\n    mock_delete_proxy.assert_called_once_with(\n        setup_instances[0][\"Instances\"][0][\"InstanceId\"], \n        test_instance_config\n    )\n    \n    assert \"1.2.3.4\" not in delete_queue  # IP should be removed from queue\n\n@patch('cloudproxy.providers.aws.main.list_instances')\n@patch('cloudproxy.providers.aws.main.stop_proxy')\ndef test_aws_check_stop(mock_stop_proxy, mock_list_instances, setup_instances):\n    \"\"\"Test checking for instances to stop\"\"\"\n    # Setup\n    mock_list_instances.return_value = [setup_instances[0]]  # Just the running instance\n    mock_stop_proxy.return_value = True\n    restart_queue.add(\"1.2.3.4\")  # Add IP to restart queue\n    \n    # Execute\n    aws_check_stop()\n    \n    # Verify\n    assert mock_stop_proxy.call_count == 1  # Should stop the instance in restart queue\n\n@patch('cloudproxy.providers.aws.main.list_instances')\n@patch('cloudproxy.providers.aws.main.stop_proxy')\ndef test_aws_check_stop_with_instance_config(mock_stop_proxy, mock_list_instances, setup_instances, test_instance_config):\n    \"\"\"Test checking for instances to stop with specific instance configuration\"\"\"\n    # Setup\n    mock_list_instances.return_value = [setup_instances[0]]  # Just the running instance\n    mock_stop_proxy.return_value = True\n    restart_queue.add(\"1.2.3.4\")  # Add IP to restart queue\n    \n    # Execute with instance config\n    aws_check_stop(test_instance_config)\n    \n    # Verify\n    mock_list_instances.assert_called_once_with(test_instance_config)\n    \n    # Should stop the instance with the instance config\n    mock_stop_proxy.assert_called_once_with(\n        setup_instances[0][\"Instances\"][0][\"InstanceId\"], \n        test_instance_config\n    )\n    \n    assert \"1.2.3.4\" not in restart_queue  # IP should be removed from queue\n\n@patch('cloudproxy.providers.aws.main.aws_check_delete')\n@patch('cloudproxy.providers.aws.main.aws_check_stop')\n@patch('cloudproxy.providers.aws.main.aws_check_alive')\n@patch('cloudproxy.providers.aws.main.aws_deployment')\ndef test_aws_start(mock_aws_deployment, mock_aws_check_alive, mock_aws_check_stop, mock_aws_check_delete):\n    \"\"\"Test the main aws_start function\"\"\"\n    # Setup\n    mock_aws_check_alive.return_value = [\"1.2.3.4\", \"5.6.7.8\"]\n    original_min_scaling = config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"scaling\"][\"min_scaling\"]\n    \n    try:\n        # Set test scaling\n        config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"scaling\"][\"min_scaling\"] = 3\n        \n        # Execute\n        result = aws_start()\n        \n        # Verify\n        mock_aws_check_delete.assert_called_once()\n        mock_aws_check_stop.assert_called_once()\n        # aws_check_alive is only called once in aws_start\n        mock_aws_check_alive.assert_called_once()\n        \n        # Check deployment was called with the correct min_scaling from the instance config and the instance config itself\n        default_config = config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n        mock_aws_deployment.assert_called_once_with(3, default_config)\n        \n        assert result == [\"1.2.3.4\", \"5.6.7.8\"]  # Should return IPs from check_alive\n    finally:\n        # Restore setting\n        config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"scaling\"][\"min_scaling\"] = original_min_scaling\n\n@patch('cloudproxy.providers.aws.main.aws_check_delete')\n@patch('cloudproxy.providers.aws.main.aws_check_stop')\n@patch('cloudproxy.providers.aws.main.aws_check_alive')\n@patch('cloudproxy.providers.aws.main.aws_deployment')\ndef test_aws_start_with_instance_config(mock_aws_deployment, mock_aws_check_alive, \n                                        mock_aws_check_stop, mock_aws_check_delete, test_instance_config):\n    \"\"\"Test the main aws_start function with specific instance configuration\"\"\"\n    # Setup\n    mock_aws_check_alive.return_value = [\"1.2.3.4\", \"5.6.7.8\"]\n    \n    # Execute with instance config\n    result = aws_start(test_instance_config)\n    \n    # Verify all methods were called with the instance config\n    mock_aws_check_delete.assert_called_once_with(test_instance_config)\n    mock_aws_check_stop.assert_called_once_with(test_instance_config)\n    mock_aws_check_alive.assert_called_once_with(test_instance_config)\n    \n    # Check deployment was called with the correct min_scaling from the instance config\n    mock_aws_deployment.assert_called_once_with(\n        test_instance_config[\"scaling\"][\"min_scaling\"],\n        test_instance_config\n    )\n    \n    assert result == [\"1.2.3.4\", \"5.6.7.8\"]  # Should return IPs from check_alive "}
{"type": "test_file", "path": "tests/test_main_models.py", "content": "import pytest\nfrom unittest.mock import patch\nfrom datetime import datetime\nfrom pydantic import ValidationError\n\nfrom cloudproxy.main import (\n    Metadata, \n    ProxyAddress,\n    ProxyList,\n    ProxyResponse,\n    ErrorResponse,\n    ProviderScaling,\n    BaseProvider,\n    DigitalOceanProvider,\n    AWSProvider,\n    GCPProvider,\n    HetznerProvider,\n    ProviderUpdateRequest,\n    ProviderInstance,\n)\nfrom cloudproxy.providers import settings\n\n# Tests for Metadata model\ndef test_metadata_default_values():\n    \"\"\"Test that Metadata model has proper default values\"\"\"\n    metadata = Metadata()\n    \n    # Check that request_id is a valid UUID string\n    assert isinstance(metadata.request_id, str)\n    assert len(metadata.request_id) > 0\n    \n    # Check that timestamp is a datetime\n    assert isinstance(metadata.timestamp, datetime)\n    \n    # Each instance should have a different request_id\n    another_metadata = Metadata()\n    assert metadata.request_id != another_metadata.request_id\n\n# Tests for ProxyAddress model\ndef test_proxy_address_defaults():\n    \"\"\"Test ProxyAddress model with default values\"\"\"\n    proxy = ProxyAddress(ip=\"192.168.1.1\")\n    \n    assert str(proxy.ip) == \"192.168.1.1\"\n    assert proxy.port == 8899\n    assert proxy.auth_enabled is True\n\ndef test_proxy_address_custom_values():\n    \"\"\"Test ProxyAddress model with custom values\"\"\"\n    proxy = ProxyAddress(\n        ip=\"192.168.1.1\", \n        port=8080, \n        auth_enabled=False,\n        provider=\"aws\",\n        instance=\"production\",\n        display_name=\"AWS Production\"\n    )\n    \n    assert str(proxy.ip) == \"192.168.1.1\"\n    assert proxy.port == 8080\n    assert proxy.auth_enabled is False\n    assert proxy.provider == \"aws\"\n    assert proxy.instance == \"production\"\n    assert proxy.display_name == \"AWS Production\"\n\n# Test the model integration with config values\ndef test_create_proxy_address():\n    \"\"\"Test the create_proxy_address helper function\"\"\"\n    from cloudproxy.main import create_proxy_address\n    \n    # Save original no_auth setting\n    original_no_auth = settings.config[\"no_auth\"]\n    \n    try:\n        # Test with auth enabled\n        settings.config[\"no_auth\"] = False\n        proxy = create_proxy_address(\"192.168.1.1\")\n        assert str(proxy.ip) == \"192.168.1.1\"\n        assert proxy.auth_enabled is True\n        \n        # Test with auth disabled\n        settings.config[\"no_auth\"] = True\n        proxy = create_proxy_address(\"192.168.1.1\")\n        assert str(proxy.ip) == \"192.168.1.1\"\n        assert proxy.auth_enabled is False\n    finally:\n        # Restore original setting\n        settings.config[\"no_auth\"] = original_no_auth\n\n# Tests for ProxyList model\ndef test_proxy_list():\n    \"\"\"Test ProxyList model\"\"\"\n    proxy1 = ProxyAddress(ip=\"192.168.1.1\")\n    proxy2 = ProxyAddress(ip=\"192.168.1.2\")\n    \n    proxy_list = ProxyList(total=2, proxies=[proxy1, proxy2])\n    \n    assert proxy_list.total == 2\n    assert len(proxy_list.proxies) == 2\n    assert isinstance(proxy_list.metadata, Metadata)\n    assert str(proxy_list.proxies[0].ip) == \"192.168.1.1\"\n    assert str(proxy_list.proxies[1].ip) == \"192.168.1.2\"\n\n# Tests for ProxyResponse model\ndef test_proxy_response():\n    \"\"\"Test ProxyResponse model\"\"\"\n    proxy = ProxyAddress(ip=\"192.168.1.1\")\n    \n    response = ProxyResponse(message=\"Test message\", proxy=proxy)\n    \n    assert response.message == \"Test message\"\n    assert str(response.proxy.ip) == \"192.168.1.1\"\n    assert isinstance(response.metadata, Metadata)\n\n# Tests for ErrorResponse model\ndef test_error_response():\n    \"\"\"Test ErrorResponse model\"\"\"\n    error = ErrorResponse(error=\"Test error\", detail=\"Error details\")\n    \n    assert error.error == \"Test error\"\n    assert error.detail == \"Error details\"\n    assert isinstance(error.metadata, Metadata)\n\n# Tests for ProviderScaling model\ndef test_provider_scaling():\n    \"\"\"Test ProviderScaling model\"\"\"\n    scaling = ProviderScaling(min_scaling=1, max_scaling=5)\n    \n    assert scaling.min_scaling == 1\n    assert scaling.max_scaling == 5\n\ndef test_provider_scaling_validation():\n    \"\"\"Test ProviderScaling model validation\"\"\"\n    # Invalid scaling value (negative)\n    with pytest.raises(ValidationError):\n        ProviderScaling(min_scaling=-1, max_scaling=5)\n    \n    # Invalid scaling value (negative)\n    with pytest.raises(ValidationError):\n        ProviderScaling(min_scaling=1, max_scaling=-5)\n\n# Tests for ProviderInstance model\ndef test_provider_instance():\n    \"\"\"Test ProviderInstance model\"\"\"\n    instance = ProviderInstance(\n        enabled=True,\n        ips=[\"192.168.1.1\", \"192.168.1.2\"],\n        scaling={\"min_scaling\": 2, \"max_scaling\": 5},\n        size=\"t2.micro\",\n        display_name=\"Production Instance\",\n        region=\"us-west-2\"\n    )\n    \n    assert instance.enabled is True\n    assert instance.ips == [\"192.168.1.1\", \"192.168.1.2\"]\n    assert instance.scaling.min_scaling == 2\n    assert instance.scaling.max_scaling == 5\n    assert instance.size == \"t2.micro\"\n    assert instance.display_name == \"Production Instance\"\n    assert instance.region == \"us-west-2\"\n\ndef test_provider_instance_optional_fields():\n    \"\"\"Test ProviderInstance model with optional fields\"\"\"\n    instance = ProviderInstance(\n        enabled=True,\n        ips=[],\n        scaling={\"min_scaling\": 1, \"max_scaling\": 3},\n        size=\"cx11\",\n        display_name=\"Hetzner Test\",\n        location=\"nbg1\",  # Hetzner-specific\n        zone=\"us-central1-a\",  # GCP-specific\n    )\n    \n    assert instance.enabled is True\n    assert instance.ips == []\n    assert instance.scaling.min_scaling == 1\n    assert instance.scaling.max_scaling == 3\n    assert instance.size == \"cx11\"\n    assert instance.display_name == \"Hetzner Test\"\n    assert instance.location == \"nbg1\"\n    assert instance.zone == \"us-central1-a\"\n\n# Tests for provider models\ndef test_base_provider():\n    \"\"\"Test BaseProvider model\"\"\"\n    # Create provider instance\n    default_instance = ProviderInstance(\n        enabled=True,\n        ips=[\"192.168.1.1\", \"192.168.1.2\"],\n        scaling={\"min_scaling\": 1, \"max_scaling\": 3},\n        size=\"small\",\n        display_name=\"Default Instance\",\n        region=\"europe\"\n    )\n    \n    # Create another instance\n    production_instance = ProviderInstance(\n        enabled=True,\n        ips=[\"192.168.1.3\"],\n        scaling={\"min_scaling\": 2, \"max_scaling\": 5},\n        size=\"large\",\n        display_name=\"Production Instance\",\n        region=\"us-west\"\n    )\n    \n    # Create provider with instances\n    provider = BaseProvider(\n        instances={\n            \"default\": default_instance,\n            \"production\": production_instance\n        }\n    )\n    \n    assert \"default\" in provider.instances\n    assert \"production\" in provider.instances\n    assert provider.instances[\"default\"].enabled is True\n    assert provider.instances[\"default\"].ips == [\"192.168.1.1\", \"192.168.1.2\"]\n    assert provider.instances[\"default\"].scaling.min_scaling == 1\n    assert provider.instances[\"default\"].scaling.max_scaling == 3\n    assert provider.instances[\"default\"].size == \"small\"\n    assert provider.instances[\"default\"].display_name == \"Default Instance\"\n    assert provider.instances[\"default\"].region == \"europe\"\n    assert provider.instances[\"production\"].ips == [\"192.168.1.3\"]\n    assert provider.instances[\"production\"].display_name == \"Production Instance\"\n\ndef test_digitalocean_provider():\n    \"\"\"Test DigitalOceanProvider model\"\"\"\n    provider = DigitalOceanProvider(\n        enabled=True,\n        ips=[\"192.168.1.1\"],\n        scaling={\"min_scaling\": 1, \"max_scaling\": 3},\n        size=\"s-1vcpu-1gb\",\n        region=\"nyc1\"\n    )\n    \n    assert provider.enabled is True\n    assert provider.ips == [\"192.168.1.1\"]\n    assert provider.scaling.min_scaling == 1\n    assert provider.scaling.max_scaling == 3\n    assert provider.size == \"s-1vcpu-1gb\"\n    assert provider.region == \"nyc1\"\n\ndef test_aws_provider():\n    \"\"\"Test AWSProvider model\"\"\"\n    provider = AWSProvider(\n        enabled=True,\n        ips=[\"192.168.1.1\"],\n        scaling={\"min_scaling\": 1, \"max_scaling\": 3},\n        size=\"t2.micro\",\n        region=\"us-east-1\",\n        ami=\"ami-12345\",\n        spot=True\n    )\n    \n    assert provider.enabled is True\n    assert provider.ips == [\"192.168.1.1\"]\n    assert provider.scaling.min_scaling == 1\n    assert provider.scaling.max_scaling == 3\n    assert provider.size == \"t2.micro\"\n    assert provider.region == \"us-east-1\"\n    assert provider.ami == \"ami-12345\"\n    assert provider.spot is True\n\ndef test_gcp_provider():\n    \"\"\"Test GCPProvider model\"\"\"\n    provider = GCPProvider(\n        enabled=True,\n        ips=[\"192.168.1.1\"],\n        scaling={\"min_scaling\": 1, \"max_scaling\": 3},\n        size=\"e2-micro\",\n        zone=\"us-central1-a\",\n        image_project=\"debian-cloud\",\n        image_family=\"debian-10\"\n    )\n    \n    assert provider.enabled is True\n    assert provider.ips == [\"192.168.1.1\"]\n    assert provider.scaling.min_scaling == 1\n    assert provider.scaling.max_scaling == 3\n    assert provider.size == \"e2-micro\"\n    assert provider.zone == \"us-central1-a\"\n    assert provider.image_project == \"debian-cloud\"\n    assert provider.image_family == \"debian-10\"\n\ndef test_hetzner_provider():\n    \"\"\"Test HetznerProvider model\"\"\"\n    provider = HetznerProvider(\n        enabled=True,\n        ips=[\"192.168.1.1\"],\n        scaling={\"min_scaling\": 1, \"max_scaling\": 3},\n        size=\"cx11\",\n        location=\"nbg1\"\n    )\n    \n    assert provider.enabled is True\n    assert provider.ips == [\"192.168.1.1\"]\n    assert provider.scaling.min_scaling == 1\n    assert provider.scaling.max_scaling == 3\n    assert provider.size == \"cx11\"\n    assert provider.location == \"nbg1\"\n\n# Tests for ProviderUpdateRequest model\ndef test_provider_update_request():\n    \"\"\"Test ProviderUpdateRequest model with valid values\"\"\"\n    update = ProviderUpdateRequest(min_scaling=1, max_scaling=5)\n    \n    assert update.min_scaling == 1\n    assert update.max_scaling == 5\n\ndef test_provider_update_request_validation():\n    \"\"\"Test ProviderUpdateRequest model validation for max_scaling\"\"\"\n    # Valid: max_scaling equals min_scaling\n    update = ProviderUpdateRequest(min_scaling=5, max_scaling=5)\n    assert update.min_scaling == 5\n    assert update.max_scaling == 5\n    \n    # Invalid: max_scaling less than min_scaling\n    with pytest.raises(ValidationError):\n        ProviderUpdateRequest(min_scaling=5, max_scaling=2) "}
{"type": "test_file", "path": "tests/test_main_additional.py", "content": "from fastapi.testclient import TestClient\nimport pytest\nfrom unittest.mock import patch\nimport os\n\nfrom cloudproxy.main import app, custom_openapi, main, get_ip_list\nfrom cloudproxy.providers.settings import delete_queue, restart_queue, config\n\n# Create test client\nclient = TestClient(app)\n\ndef test_random_with_proxies():\n    \"\"\"Test the /random endpoint when proxies are available\"\"\"\n    # Add a mock proxy to the IP list temporarily\n    original_ip_list = config[\"providers\"][\"digitalocean\"][\"ips\"].copy()\n    config[\"providers\"][\"digitalocean\"][\"ips\"] = [\"192.168.1.10\"]\n    \n    try:\n        response = client.get(\"/random\")\n        assert response.status_code == 200\n        data = response.json()\n        assert \"metadata\" in data\n        assert \"message\" in data\n        assert \"proxy\" in data\n        assert data[\"message\"] == \"Random proxy retrieved successfully\"\n        assert data[\"proxy\"][\"ip\"] == \"192.168.1.10\"\n    finally:\n        # Restore original IP list\n        config[\"providers\"][\"digitalocean\"][\"ips\"] = original_ip_list\n\ndef test_custom_openapi():\n    \"\"\"Test custom OpenAPI schema generation\"\"\"\n    # First call should create and return schema\n    schema = custom_openapi()\n    assert \"openapi\" in schema\n    assert \"info\" in schema\n    assert schema[\"info\"][\"title\"] == \"CloudProxy API\"\n    \n    # Second call should return the cached schema\n    cached_schema = custom_openapi()\n    assert cached_schema is schema\n\ndef test_custom_swagger_ui_html():\n    \"\"\"Test Swagger UI HTML endpoint\"\"\"\n    response = client.get(\"/docs\")\n    assert response.status_code == 200\n    # Check that the response contains HTML with Swagger UI elements\n    assert b\"swagger-ui\" in response.content\n    assert b\"openapi.json\" in response.content\n\ndef test_openapi_json():\n    \"\"\"Test OpenAPI JSON endpoint\"\"\"\n    response = client.get(\"/openapi.json\")\n    assert response.status_code == 200\n    data = response.json()\n    assert \"openapi\" in data\n    assert \"paths\" in data\n    assert \"info\" in data\n    assert data[\"info\"][\"title\"] == \"CloudProxy API\"\n\ndef test_static_files_error_handling():\n    \"\"\"Test error handling for static files\"\"\"\n    # This tests that our fix for the GitHub workflow issue works properly\n    # We should get a 404 instead of a server error\n    response = client.get(\"/ui/nonexistent-file\")\n    assert response.status_code in (404, 307)  # Either 404 or redirect to index\n\ndef test_auth_endpoint():\n    \"\"\"Test authentication settings endpoint\"\"\"\n    # Set test values\n    original_username = config[\"auth\"][\"username\"]\n    original_password = config[\"auth\"][\"password\"]\n    original_no_auth = config[\"no_auth\"]\n    \n    try:\n        config[\"auth\"][\"username\"] = \"test_user\"\n        config[\"auth\"][\"password\"] = \"test_pass\"\n        config[\"no_auth\"] = False\n        \n        response = client.get(\"/auth\")\n        assert response.status_code == 200\n        data = response.json()\n        assert data[\"username\"] == \"test_user\"\n        assert data[\"password\"] == \"test_pass\"\n        assert data[\"auth_enabled\"] is True\n    finally:\n        # Restore original values\n        config[\"auth\"][\"username\"] = original_username\n        config[\"auth\"][\"password\"] = original_password\n        config[\"no_auth\"] = original_no_auth\n\ndef test_get_ip_list():\n    \"\"\"Test the get_ip_list function with various provider configurations\"\"\"\n    # Setup test data\n    original_do_ips = config[\"providers\"][\"digitalocean\"][\"ips\"].copy()\n    original_aws_ips = config[\"providers\"][\"aws\"][\"ips\"].copy()\n    original_gcp_ips = config[\"providers\"][\"gcp\"][\"ips\"].copy()\n    original_hetzner_ips = config[\"providers\"][\"hetzner\"][\"ips\"].copy()\n    original_delete_queue = delete_queue.copy()\n    original_restart_queue = restart_queue.copy()\n    \n    try:\n        # Set test values\n        config[\"providers\"][\"digitalocean\"][\"ips\"] = [\"1.1.1.1\", \"2.2.2.2\"]\n        config[\"providers\"][\"aws\"][\"ips\"] = [\"3.3.3.3\"]\n        config[\"providers\"][\"gcp\"][\"ips\"] = [\"4.4.4.4\"]\n        config[\"providers\"][\"hetzner\"][\"ips\"] = [\"5.5.5.5\"]\n        config[\"no_auth\"] = False\n        \n        # Empty queues\n        delete_queue.clear()\n        restart_queue.clear()\n        \n        # Test with no IPs in queues\n        result = get_ip_list()\n        assert len(result) == 5\n        ip_values = [str(proxy.ip) for proxy in result]\n        assert \"1.1.1.1\" in ip_values\n        assert \"3.3.3.3\" in ip_values\n        assert \"5.5.5.5\" in ip_values\n        \n        # Test with some IPs in delete queue\n        delete_queue.add(\"1.1.1.1\")\n        restart_queue.add(\"4.4.4.4\")\n        result = get_ip_list()\n        assert len(result) == 3\n        ip_values = [str(proxy.ip) for proxy in result]\n        assert \"1.1.1.1\" not in ip_values  # Should be filtered out\n        assert \"4.4.4.4\" not in ip_values  # Should be filtered out\n        \n    finally:\n        # Restore original data\n        config[\"providers\"][\"digitalocean\"][\"ips\"] = original_do_ips\n        config[\"providers\"][\"aws\"][\"ips\"] = original_aws_ips\n        config[\"providers\"][\"gcp\"][\"ips\"] = original_gcp_ips\n        config[\"providers\"][\"hetzner\"][\"ips\"] = original_hetzner_ips\n        delete_queue.clear()\n        delete_queue.update(original_delete_queue)\n        restart_queue.clear()\n        restart_queue.update(original_restart_queue)\n\ndef test_error_responses():\n    \"\"\"Test that API returns proper error responses\"\"\"\n    # Test invalid IP address\n    response = client.delete(\"/destroy?ip_address=invalid-ip\")\n    assert response.status_code == 422\n    \n    # Test provider not found\n    response = client.get(\"/providers/nonexistent\")\n    assert response.status_code == 404\n    assert \"Provider 'nonexistent' not found\" in response.json()[\"detail\"]\n    \n    # Test invalid provider update\n    response = client.patch(\"/providers/digitalocean\", json={\n        \"min_scaling\": 10,\n        \"max_scaling\": 5  # Invalid: max should be >= min\n    })\n    assert response.status_code == 422\n\n@patch(\"uvicorn.run\")\ndef test_main_function(mock_run):\n    \"\"\"Test the main function that starts the server\"\"\"\n    # Call the main function\n    main()\n    \n    # Verify that uvicorn.run was called with the correct parameters\n    mock_run.assert_called_once()\n    args, kwargs = mock_run.call_args\n    assert kwargs[\"host\"] == \"0.0.0.0\"\n    assert kwargs[\"port\"] == 8000\n    assert kwargs[\"log_level\"] == \"info\" "}
{"type": "test_file", "path": "tests/test_main.py", "content": "from fastapi.testclient import TestClient\nimport os\n\nfrom cloudproxy.main import app\nfrom cloudproxy.providers.settings import delete_queue, config\n\n# Configure test environment\nos.environ[\"DIGITALOCEAN_ENABLED\"] = \"false\"\nos.environ[\"PROXY_USERNAME\"] = \"test_user\"\nos.environ[\"PROXY_PASSWORD\"] = \"test_pass\"\nos.environ[\"ONLY_HOST_IP\"] = \"False\"\n\n# Create test client\n# Note: The HTTPX deprecation warning is internal to the library and doesn't affect functionality\nclient = TestClient(app)\n\n# Set up test environment\nconfig[\"providers\"][\"digitalocean\"][\"enabled\"] = False\nconfig[\"providers\"][\"digitalocean\"][\"ips\"] = []\nconfig[\"providers\"][\"digitalocean\"][\"scaling\"][\"min_scaling\"] = 2\nconfig[\"providers\"][\"digitalocean\"][\"scaling\"][\"max_scaling\"] = 2\nconfig[\"providers\"][\"digitalocean\"][\"size\"] = \"s-1vcpu-1gb\"\nconfig[\"providers\"][\"digitalocean\"][\"region\"] = \"lon1\"\n\n# Update auth config with test values\nconfig[\"auth\"][\"username\"] = os.environ[\"PROXY_USERNAME\"]\nconfig[\"auth\"][\"password\"] = os.environ[\"PROXY_PASSWORD\"]\nconfig[\"no_auth\"] = False\n\n\ndef test_read_root():\n    response = client.get(\"/\")\n    assert response.status_code == 200\n    data = response.json()\n    assert \"metadata\" in data\n    assert \"total\" in data\n    assert \"proxies\" in data\n    assert isinstance(data[\"proxies\"], list)\n    assert data[\"total\"] == 0\n\n\ndef test_random():\n    response = client.get(\"/random\")\n    assert response.status_code == 404\n    assert response.json() == {\"detail\": \"No proxies available\"}\n\n\ndef test_remove_proxy_list():\n    response = client.get(\"/destroy\")\n    assert response.status_code == 200\n    data = response.json()\n    assert \"metadata\" in data\n    assert \"total\" in data\n    assert \"proxies\" in data\n    assert isinstance(data[\"proxies\"], list)\n    assert data[\"total\"] == len(data[\"proxies\"])\n\n\ndef test_remove_proxy():\n    response = client.delete(\"/destroy?ip_address=192.168.1.1\")\n    assert response.status_code == 200\n    data = response.json()\n    assert \"metadata\" in data\n    assert \"message\" in data\n    assert \"proxy\" in data\n    assert data[\"message\"] == \"Proxy scheduled for deletion\"\n    assert data[\"proxy\"][\"ip\"] == \"192.168.1.1\"\n\n\ndef test_restart_proxy_list():\n    response = client.get(\"/restart\")\n    assert response.status_code == 200\n    data = response.json()\n    assert \"metadata\" in data\n    assert \"total\" in data\n    assert \"proxies\" in data\n    assert isinstance(data[\"proxies\"], list)\n    assert data[\"total\"] == len(data[\"proxies\"])\n\n\ndef test_restart_proxy():\n    response = client.delete(\"/restart?ip_address=192.168.1.1\")\n    assert response.status_code == 200\n    data = response.json()\n    assert \"metadata\" in data\n    assert \"message\" in data\n    assert \"proxy\" in data\n    assert data[\"message\"] == \"Proxy scheduled for restart\"\n    assert data[\"proxy\"][\"ip\"] == \"192.168.1.1\"\n\n\ndef test_providers_digitalocean():\n    response = client.get(\"/providers/digitalocean\")\n    assert response.status_code == 200\n    data = response.json()\n    print(\"DEBUG PROVIDER RESPONSE:\", data[\"provider\"])\n    assert \"metadata\" in data\n    assert \"message\" in data\n    assert \"provider\" in data\n    assert data[\"provider\"] == {\n        \"enabled\": False,\n        \"ips\": [],\n        \"scaling\": {\n            \"min_scaling\": 2,\n            \"max_scaling\": 2\n        },\n        \"size\": \"s-1vcpu-1gb\",\n        \"region\": \"lon1\"\n    }\n\n\ndef test_providers_404():\n    response = client.get(\"/providers/notaprovider\")\n    assert response.status_code == 404\n\n\ndef test_configure():\n    response = client.patch(\"/providers/digitalocean\", json={\n        \"min_scaling\": 4,\n        \"max_scaling\": 4\n    })\n    assert response.status_code == 200\n    data = response.json()\n    assert \"metadata\" in data\n    assert \"message\" in data\n    assert \"provider\" in data\n    assert data[\"provider\"] == {\n        \"enabled\": False,\n        \"ips\": [],\n        \"scaling\": {\n            \"min_scaling\": 4,\n            \"max_scaling\": 4\n        },\n        \"size\": \"s-1vcpu-1gb\",\n        \"region\": \"lon1\"\n    }\n"}
{"type": "test_file", "path": "tests/test_providers_config.py", "content": "import pytest\nfrom unittest.mock import patch, Mock, MagicMock, mock_open\n\nfrom cloudproxy.providers import settings\nfrom cloudproxy.providers.config import set_auth\n\n\n# Create a mock user data script that represents the content of user_data.sh\nMOCK_USER_DATA = \"\"\"#!/bin/bash\n# Install Tinyproxy\nsudo apt-get update\nsudo apt-get install -y tinyproxy ufw\n\n# Configure Tinyproxy\nsudo mv /etc/tinyproxy/tinyproxy.conf /etc/tinyproxy/tinyproxy.conf.bak\nsudo bash -c \"cat > /etc/tinyproxy/tinyproxy.conf\" << 'EOL'\nUser nobody\nGroup nogroup\nPort 8899\nTimeout 600\nDefaultErrorFile \"/usr/share/tinyproxy/default.html\"\nStatHost \"127.0.0.1\"\nStatFile \"/usr/share/tinyproxy/stats.html\"\nLogFile \"/var/log/tinyproxy/tinyproxy.log\"\nLogLevel Info\nPidFile \"/run/tinyproxy/tinyproxy.pid\"\nMaxClients 100\nAllow 127.0.0.1\n\nBasicAuth PROXY_USERNAME PROXY_PASSWORD\n\nConnectPort 443\nConnectPort 563\nEOL\n\n# Configure firewall\nsudo ufw allow 22/tcp\nsudo ufw allow 8899/tcp\nsudo ufw enable\n\"\"\"\n\n\n@pytest.fixture\ndef setup_config_test():\n    \"\"\"Save original settings and restore them after test\"\"\"\n    # Save original settings\n    original_no_auth = settings.config.get(\"no_auth\", False)\n    original_only_host_ip = settings.config.get(\"only_host_ip\", False)\n    \n    # Run the test\n    yield\n    \n    # Restore original settings\n    settings.config[\"no_auth\"] = original_no_auth\n    settings.config[\"only_host_ip\"] = original_only_host_ip\n\n\ndef test_set_auth_with_auth(setup_config_test):\n    \"\"\"Test set_auth with authentication enabled\"\"\"\n    # Set config values\n    settings.config[\"no_auth\"] = False\n    settings.config[\"only_host_ip\"] = False\n    \n    # Mock the open function to return our mock user_data content\n    with patch(\"builtins.open\", mock_open(read_data=MOCK_USER_DATA)):\n        result = set_auth(\"testuser\", \"testpass\")\n    \n    # Verify username and password were replaced\n    assert \"BasicAuth testuser testpass\" in result\n    assert \"PROXY_USERNAME\" not in result\n    assert \"PROXY_PASSWORD\" not in result\n\n\ndef test_set_auth_without_auth(setup_config_test):\n    \"\"\"Test set_auth with authentication disabled\"\"\"\n    # Set config values\n    settings.config[\"no_auth\"] = True\n    settings.config[\"only_host_ip\"] = False\n    \n    # Mock the open function to return our mock user_data content\n    with patch(\"builtins.open\", mock_open(read_data=MOCK_USER_DATA)):\n        result = set_auth(\"testuser\", \"testpass\")\n    \n    # Verify BasicAuth line was removed\n    assert \"\\nBasicAuth PROXY_USERNAME PROXY_PASSWORD\\n\" not in result\n    # The replacement seems to leave an extra newline, so we get three newlines\n    assert \"Allow 127.0.0.1\\n\\n\\nConnectPort\" in result\n\n\ndef test_set_auth_with_host_ip(setup_config_test):\n    \"\"\"Test set_auth with host IP enabled\"\"\"\n    # Set config values\n    settings.config[\"no_auth\"] = False\n    settings.config[\"only_host_ip\"] = True\n    \n    # Mock the requests.get call to return a specific IP\n    mock_response = MagicMock()\n    mock_response.text = \"192.168.1.1\"\n    \n    with patch(\"cloudproxy.providers.config.requests.get\", return_value=mock_response):\n        # Mock the open function to return our mock user_data content\n        with patch(\"builtins.open\", mock_open(read_data=MOCK_USER_DATA)):\n            result = set_auth(\"testuser\", \"testpass\")\n    \n    # Verify IP address was included in UFW rules and Allow rule\n    assert \"sudo ufw allow from 192.168.1.1 to any port 22 proto tcp\" in result\n    assert \"sudo ufw allow from 192.168.1.1 to any port 8899 proto tcp\" in result\n    assert \"Allow 127.0.0.1\\nAllow 192.168.1.1\" in result\n    assert \"BasicAuth testuser testpass\" in result\n\n\ndef test_set_auth_with_both_options(setup_config_test):\n    \"\"\"Test set_auth with both no_auth and only_host_ip enabled\"\"\"\n    # Set config values\n    settings.config[\"no_auth\"] = True\n    settings.config[\"only_host_ip\"] = True\n    \n    # Mock the requests.get call to return a specific IP\n    mock_response = MagicMock()\n    mock_response.text = \"192.168.1.1\"\n    \n    with patch(\"cloudproxy.providers.config.requests.get\", return_value=mock_response):\n        # Mock the open function to return our mock user_data content\n        with patch(\"builtins.open\", mock_open(read_data=MOCK_USER_DATA)):\n            result = set_auth(\"testuser\", \"testpass\")\n    \n    # Verify both modifications were applied\n    assert \"\\nBasicAuth PROXY_USERNAME PROXY_PASSWORD\\n\" not in result\n    assert \"sudo ufw allow from 192.168.1.1 to any port 22 proto tcp\" in result\n    assert \"Allow 127.0.0.1\\nAllow 192.168.1.1\" in result "}
{"type": "test_file", "path": "tests/test_providers_aws_functions.py", "content": "import pytest\nfrom unittest.mock import patch, Mock, MagicMock\nimport botocore\nfrom botocore.exceptions import ClientError\nimport os\nimport sys\n\nfrom cloudproxy.providers import settings\nfrom cloudproxy.providers.aws.functions import (\n    create_proxy,\n    delete_proxy,\n    stop_proxy,\n    start_proxy,\n    list_instances,\n    get_clients,\n    get_tags\n)\n\n# Setup fixtures\n@pytest.fixture\ndef mock_vpc_response():\n    return {'Vpcs': [{'IsDefault': True, 'VpcId': 'vpc-12345'}]}\n\n@pytest.fixture\ndef mock_sg_response():\n    return {'SecurityGroups': [{'GroupId': 'sg-12345'}]}\n\n@pytest.fixture\ndef mock_instance():\n    instance = Mock()\n    instance.id = \"i-12345\"\n    instance.public_ip_address = \"1.2.3.4\"\n    return instance\n\n@pytest.fixture\ndef mock_instances_response():\n    return {\n        'Reservations': [\n            {\n                'Instances': [\n                    {\n                        'InstanceId': 'i-12345',\n                        'PublicIpAddress': '1.2.3.4',\n                        'State': {'Name': 'running'}\n                    }\n                ]\n            },\n            {\n                'Instances': [\n                    {\n                        'InstanceId': 'i-67890',\n                        'PublicIpAddress': '5.6.7.8',\n                        'State': {'Name': 'running'}\n                    }\n                ]\n            }\n        ]\n    }\n\n@pytest.fixture\ndef test_instance_config():\n    \"\"\"Create a test instance configuration\"\"\"\n    return {\n        \"enabled\": True,\n        \"ips\": [\"10.0.0.1\"],\n        \"scaling\": {\"min_scaling\": 2, \"max_scaling\": 5},\n        \"size\": \"t3.micro\",\n        \"region\": \"us-west-2\",\n        \"ami\": \"ami-test\",\n        \"display_name\": \"Test Instance\",\n        \"secrets\": {\n            \"access_key_id\": \"test-access-key\",\n            \"secret_access_key\": \"test-secret-key\"\n        },\n        \"spot\": False\n    }\n\n# Test get_clients function with instance configuration\n@patch('cloudproxy.providers.aws.functions.boto3.resource')\n@patch('cloudproxy.providers.aws.functions.boto3.client')\ndef test_get_clients_with_instance_config(mock_boto3_client, mock_boto3_resource, test_instance_config):\n    \"\"\"Test getting AWS clients with instance-specific configuration\"\"\"\n    # Setup\n    mock_boto3_resource.return_value = \"mock-resource\"\n    mock_boto3_client.return_value = \"mock-client\"\n    \n    # Execute\n    ec2_resource, ec2_client = get_clients(test_instance_config)\n    \n    # Verify\n    assert ec2_resource == \"mock-resource\"\n    assert ec2_client == \"mock-client\"\n    \n    # Check that boto3.resource was called with correct params\n    mock_boto3_resource.assert_called_once_with(\n        \"ec2\", \n        region_name=\"us-west-2\",\n        aws_access_key_id=\"test-access-key\",\n        aws_secret_access_key=\"test-secret-key\"\n    )\n    \n    # Check that boto3.client was called with correct params  \n    mock_boto3_client.assert_called_once_with(\n        \"ec2\", \n        region_name=\"us-west-2\",\n        aws_access_key_id=\"test-access-key\",\n        aws_secret_access_key=\"test-secret-key\"\n    )\n\n# Test get_tags function with instance configuration\ndef test_get_tags_with_instance_config(test_instance_config):\n    \"\"\"Test getting AWS tags with instance-specific configuration\"\"\"\n    # Setup - store original config to restore later\n    original_instances = settings.config[\"providers\"][\"aws\"][\"instances\"].copy()\n    \n    try:\n        # Add test instance to config for proper name lookup\n        settings.config[\"providers\"][\"aws\"][\"instances\"][\"test-instance\"] = test_instance_config\n        \n        # Execute\n        tags, tag_specification = get_tags(test_instance_config)\n        \n        # Verify\n        assert any(tag[\"Key\"] == \"cloudproxy\" for tag in tags)\n        assert any(tag[\"Key\"] == \"cloudproxy-instance\" and tag[\"Value\"] == \"test-instance\" for tag in tags)\n        assert any(tag[\"Key\"] == \"Name\" and tag[\"Value\"] == \"CloudProxy-Test Instance\" for tag in tags)\n        assert tag_specification[0][\"ResourceType\"] == \"instance\"\n        assert tag_specification[0][\"Tags\"] == tags\n    finally:\n        # Restore original config\n        settings.config[\"providers\"][\"aws\"][\"instances\"] = original_instances\n\n@patch('cloudproxy.providers.aws.functions.get_clients')\ndef test_create_proxy_on_demand(mock_get_clients, mock_vpc_response, mock_sg_response, mock_instance):\n    \"\"\"Test creating an on-demand EC2 instance\"\"\"\n    # Setup mocks\n    mock_ec2 = MagicMock()\n    mock_ec2_client = MagicMock()\n    mock_get_clients.return_value = (mock_ec2, mock_ec2_client)\n    \n    vpc_mock = Mock()\n    vpc_mock.id = \"vpc-12345\"\n    mock_ec2.vpcs.filter.return_value = [vpc_mock]\n\n    mock_ec2_client.describe_vpcs.return_value = mock_vpc_response\n    mock_ec2_client.describe_security_groups.return_value = mock_sg_response\n\n    # Configure the ec2 mock\n    mock_ec2.create_instances.return_value = [mock_instance]\n\n    # Save original spot setting\n    original_spot = settings.config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"spot\"]\n    try:\n        # Set to False for on-demand instance\n        settings.config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"spot\"] = False\n\n        # Execute\n        result = create_proxy()\n\n        # Verify\n        mock_get_clients.assert_called_once()\n        mock_ec2.create_instances.assert_called_once()\n        assert \"InstanceMarketOptions\" not in mock_ec2.create_instances.call_args[1]\n        assert result == [mock_instance]\n    finally:\n        # Restore setting\n        settings.config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"spot\"] = original_spot\n\n@patch('cloudproxy.providers.aws.functions.get_clients')\ndef test_create_proxy_with_instance_config(mock_get_clients, mock_vpc_response, mock_sg_response, mock_instance, test_instance_config):\n    \"\"\"Test creating an EC2 instance with specific instance configuration\"\"\"\n    # Setup mocks\n    mock_ec2 = MagicMock()\n    mock_ec2_client = MagicMock()\n    mock_get_clients.return_value = (mock_ec2, mock_ec2_client)\n    \n    vpc_mock = Mock()\n    vpc_mock.id = \"vpc-12345\"\n    mock_ec2.vpcs.filter.return_value = [vpc_mock]\n\n    mock_ec2_client.describe_vpcs.return_value = mock_vpc_response\n    mock_ec2_client.describe_security_groups.return_value = mock_sg_response\n\n    # Configure the ec2 mock\n    mock_ec2.create_instances.return_value = [mock_instance]\n\n    # Execute\n    result = create_proxy(test_instance_config)\n\n    # Verify\n    mock_get_clients.assert_called_once_with(test_instance_config)\n    mock_ec2.create_instances.assert_called_once()\n    assert result == [mock_instance]\n\n@patch('cloudproxy.providers.aws.functions.get_clients')\ndef test_create_proxy_spot_persistent(mock_get_clients, mock_vpc_response, mock_sg_response, mock_instance):\n    \"\"\"Test creating a spot EC2 instance with persistent option\"\"\"\n    # Setup mocks\n    mock_ec2 = MagicMock()\n    mock_ec2_client = MagicMock()\n    mock_get_clients.return_value = (mock_ec2, mock_ec2_client)\n    \n    vpc_mock = Mock()\n    vpc_mock.id = \"vpc-12345\"\n    mock_ec2.vpcs.filter.return_value = [vpc_mock]\n\n    mock_ec2_client.describe_vpcs.return_value = mock_vpc_response\n    mock_ec2_client.describe_security_groups.return_value = mock_sg_response\n\n    # Configure the ec2 mock\n    mock_ec2.create_instances.return_value = [mock_instance]\n\n    # Save original spot setting\n    original_spot = settings.config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"spot\"]\n    try:\n        # Set to persistent for spot instance\n        settings.config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"spot\"] = 'persistent'\n\n        # Execute\n        result = create_proxy()\n\n        # Verify\n        mock_get_clients.assert_called_once()\n        mock_ec2.create_instances.assert_called_once()\n        assert \"InstanceMarketOptions\" in mock_ec2.create_instances.call_args[1]\n        market_options = mock_ec2.create_instances.call_args[1][\"InstanceMarketOptions\"]\n        assert market_options[\"MarketType\"] == \"spot\"\n        assert market_options[\"SpotOptions\"][\"SpotInstanceType\"] == \"persistent\"\n        assert market_options[\"SpotOptions\"][\"InstanceInterruptionBehavior\"] == \"stop\"\n    finally:\n        # Restore setting\n        settings.config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"spot\"] = original_spot\n\n@patch('cloudproxy.providers.aws.functions.get_clients')\ndef test_create_proxy_security_group_exists(mock_get_clients, mock_vpc_response, mock_sg_response, mock_instance):\n    \"\"\"Test creating an EC2 instance when security group already exists\"\"\"\n    # Setup mocks\n    mock_ec2 = MagicMock()\n    mock_ec2_client = MagicMock()\n    mock_get_clients.return_value = (mock_ec2, mock_ec2_client)\n    \n    vpc_mock = Mock()\n    vpc_mock.id = \"vpc-12345\"\n    mock_ec2.vpcs.filter.return_value = [vpc_mock]\n\n    mock_ec2_client.describe_vpcs.return_value = mock_vpc_response\n    mock_ec2_client.describe_security_groups.return_value = mock_sg_response\n\n    # Configure sg creation to raise ClientError\n    error_response = {'Error': {'Code': 'InvalidGroup.Duplicate'}}\n    mock_ec2.create_security_group.side_effect = botocore.exceptions.ClientError(\n        error_response, 'CreateSecurityGroup'\n    )\n\n    # Configure the ec2 mock\n    mock_ec2.create_instances.return_value = [mock_instance]\n\n    # Execute\n    result = create_proxy()\n\n    # Verify\n    mock_get_clients.assert_called_once()\n    mock_ec2.create_instances.assert_called_once()\n    assert result == [mock_instance]\n\n@patch('cloudproxy.providers.aws.functions.get_clients')\ndef test_delete_proxy(mock_get_clients):\n    \"\"\"Test deleting an EC2 instance\"\"\"\n    # Setup mocks\n    mock_ec2 = MagicMock()\n    mock_ec2_client = MagicMock()\n    mock_get_clients.return_value = (mock_ec2, mock_ec2_client)\n    \n    # Create a mock for the instances collection\n    instances_collection = MagicMock()\n    terminated_instances = [{'InstanceId': 'i-12345', 'CurrentState': {'Name': 'shutting-down'}}]\n    \n    # Mock the instances collection filter method\n    instances = MagicMock()\n    instances.terminate.return_value = terminated_instances\n    instances_collection.filter.return_value = instances\n    mock_ec2.instances = instances_collection\n    \n    # Mock spot instance describe and cancel calls\n    mock_ec2_client.describe_spot_instance_requests.return_value = {\"SpotInstanceRequests\": []}\n    \n    # Execute\n    instance_id = \"i-12345\"\n    result = delete_proxy(instance_id)\n    \n    # Verify\n    mock_get_clients.assert_called_once()\n    instances_collection.filter.assert_called_once_with(InstanceIds=[instance_id])\n    instances.terminate.assert_called_once()\n    assert result == terminated_instances\n\n@patch('cloudproxy.providers.aws.functions.get_clients')\ndef test_delete_proxy_with_instance_config(mock_get_clients, test_instance_config):\n    \"\"\"Test deleting an EC2 instance with specific instance configuration\"\"\"\n    # Setup mocks\n    mock_ec2 = MagicMock()\n    mock_ec2_client = MagicMock()\n    mock_get_clients.return_value = (mock_ec2, mock_ec2_client)\n    \n    # Create a mock for the instances collection\n    instances_collection = MagicMock()\n    terminated_instances = [{'InstanceId': 'i-12345', 'CurrentState': {'Name': 'shutting-down'}}]\n    \n    # Mock the instances collection filter method\n    instances = MagicMock()\n    instances.terminate.return_value = terminated_instances\n    instances_collection.filter.return_value = instances\n    mock_ec2.instances = instances_collection\n    \n    # Mock spot instance describe and cancel calls\n    mock_ec2_client.describe_spot_instance_requests.return_value = {\"SpotInstanceRequests\": []}\n    \n    # Execute\n    instance_id = \"i-12345\"\n    result = delete_proxy(instance_id, test_instance_config)\n    \n    # Verify\n    mock_get_clients.assert_called_once_with(test_instance_config)\n    instances_collection.filter.assert_called_once_with(InstanceIds=[instance_id])\n    instances.terminate.assert_called_once()\n    assert result == terminated_instances\n\n@patch('cloudproxy.providers.aws.functions.get_clients')\ndef test_stop_proxy(mock_get_clients):\n    \"\"\"Test stopping an EC2 instance\"\"\"\n    # Setup mocks\n    mock_ec2 = MagicMock()\n    mock_ec2_client = MagicMock()\n    mock_get_clients.return_value = (mock_ec2, mock_ec2_client)\n    \n    # Create a mock for the instances collection\n    instances_collection = MagicMock()\n    stopped_instances = [{'InstanceId': 'i-12345', 'CurrentState': {'Name': 'stopping'}}]\n    \n    # Mock the instances collection filter method\n    instances = MagicMock()\n    instances.stop.return_value = stopped_instances\n    instances_collection.filter.return_value = instances\n    mock_ec2.instances = instances_collection\n    \n    # Execute\n    instance_id = \"i-12345\"\n    result = stop_proxy(instance_id)\n    \n    # Verify\n    mock_get_clients.assert_called_once()\n    instances_collection.filter.assert_called_once_with(InstanceIds=[instance_id])\n    instances.stop.assert_called_once()\n    assert result == stopped_instances\n\n@patch('cloudproxy.providers.aws.functions.get_clients')\ndef test_stop_proxy_with_instance_config(mock_get_clients, test_instance_config):\n    \"\"\"Test stopping an EC2 instance with a specific instance configuration\"\"\"\n    # Setup mocks\n    mock_ec2 = MagicMock()\n    mock_ec2_client = MagicMock()\n    mock_get_clients.return_value = (mock_ec2, mock_ec2_client)\n    \n    # Create a mock for the instances collection\n    instances_collection = MagicMock()\n    stopped_instances = [{'InstanceId': 'i-12345', 'CurrentState': {'Name': 'stopping'}}]\n    \n    # Mock the instances collection filter method\n    instances = MagicMock()\n    instances.stop.return_value = stopped_instances\n    instances_collection.filter.return_value = instances\n    mock_ec2.instances = instances_collection\n    \n    # Execute\n    instance_id = \"i-12345\"\n    result = stop_proxy(instance_id, test_instance_config)\n    \n    # Verify\n    mock_get_clients.assert_called_once_with(test_instance_config)\n    instances_collection.filter.assert_called_once_with(InstanceIds=[instance_id])\n    instances.stop.assert_called_once()\n    assert result == stopped_instances\n\n@patch('cloudproxy.providers.aws.functions.get_clients')\ndef test_start_proxy(mock_get_clients):\n    \"\"\"Test starting an EC2 instance\"\"\"\n    # Setup mocks\n    mock_ec2 = MagicMock()\n    mock_ec2_client = MagicMock()\n    mock_get_clients.return_value = (mock_ec2, mock_ec2_client)\n    \n    # Create a mock for the instances collection\n    instances_collection = MagicMock()\n    started_instances = [{'InstanceId': 'i-12345', 'CurrentState': {'Name': 'pending'}}]\n    \n    # Mock the instances collection filter method\n    instances = MagicMock()\n    instances.start.return_value = started_instances\n    instances_collection.filter.return_value = instances\n    mock_ec2.instances = instances_collection\n    \n    # Execute\n    instance_id = \"i-12345\"\n    result = start_proxy(instance_id)\n    \n    # Verify\n    mock_get_clients.assert_called_once()\n    instances_collection.filter.assert_called_once_with(InstanceIds=[instance_id])\n    instances.start.assert_called_once()\n    assert result == started_instances\n\n@patch('cloudproxy.providers.aws.functions.get_clients')\ndef test_start_proxy_with_instance_config(mock_get_clients, test_instance_config):\n    \"\"\"Test starting an EC2 instance with specific instance configuration\"\"\"\n    # Setup mocks\n    mock_ec2 = MagicMock()\n    mock_ec2_client = MagicMock()\n    mock_get_clients.return_value = (mock_ec2, mock_ec2_client)\n    \n    # Create a mock for the instances collection\n    instances_collection = MagicMock()\n    started_instances = [{'InstanceId': 'i-12345', 'CurrentState': {'Name': 'pending'}}]\n    \n    # Mock the instances collection filter method\n    instances = MagicMock()\n    instances.start.return_value = started_instances\n    instances_collection.filter.return_value = instances\n    mock_ec2.instances = instances_collection\n    \n    # Execute\n    instance_id = \"i-12345\"\n    result = start_proxy(instance_id, test_instance_config)\n    \n    # Verify\n    mock_get_clients.assert_called_once_with(test_instance_config)\n    instances_collection.filter.assert_called_once_with(InstanceIds=[instance_id])\n    instances.start.assert_called_once()\n    assert result == started_instances\n\n@patch('cloudproxy.providers.aws.functions.get_clients')\ndef test_list_instances(mock_get_clients, mock_instances_response):\n    \"\"\"Test listing EC2 instances\"\"\"\n    # Setup mocks\n    mock_ec2 = MagicMock()\n    mock_ec2_client = MagicMock()\n    mock_get_clients.return_value = (mock_ec2, mock_ec2_client)\n    \n    # Mock EC2 client's describe_instances method\n    mock_ec2_client.describe_instances.return_value = mock_instances_response\n    \n    # Execute\n    result = list_instances()\n    \n    # Verify\n    mock_get_clients.assert_called_once()\n    # The describe_instances method is called twice now - once for instances with the cloudproxy-instance tag\n    # and once for legacy instances without this tag\n    assert mock_ec2_client.describe_instances.call_count == 2\n    assert result == mock_instances_response[\"Reservations\"]\n\n@patch('cloudproxy.providers.aws.functions.get_clients')\ndef test_list_instances_with_instance_config(mock_get_clients, mock_instances_response, test_instance_config):\n    \"\"\"Test listing EC2 instances with a specific instance configuration\"\"\"\n    # Setup mocks\n    mock_ec2 = MagicMock()\n    mock_ec2_client = MagicMock()\n    mock_get_clients.return_value = (mock_ec2, mock_ec2_client)\n    \n    # Mock EC2 client's describe_instances method\n    mock_ec2_client.describe_instances.return_value = mock_instances_response\n    \n    # Save original config to restore later\n    original_instances = settings.config[\"providers\"][\"aws\"][\"instances\"].copy()\n    \n    try:\n        # Add test instance to config for proper name lookup\n        settings.config[\"providers\"][\"aws\"][\"instances\"][\"test-instance\"] = test_instance_config\n        \n        # Execute\n        result = list_instances(test_instance_config)\n        \n        # Verify\n        mock_get_clients.assert_called_once_with(test_instance_config)\n        mock_ec2_client.describe_instances.assert_called_once()\n        \n        # Check that describe_instances was called with the correct filters\n        # The filter should include the cloudproxy-instance tag with value \"test-instance\"\n        filters = mock_ec2_client.describe_instances.call_args[1][\"Filters\"]\n        instance_tag_filter = next((f for f in filters if f[\"Name\"] == \"tag:cloudproxy-instance\"), None)\n        assert instance_tag_filter is not None\n        assert \"test-instance\" in instance_tag_filter[\"Values\"]\n        \n        assert result == mock_instances_response[\"Reservations\"]\n    finally:\n        # Restore original config\n        settings.config[\"providers\"][\"aws\"][\"instances\"] = original_instances "}
{"type": "test_file", "path": "tests/test_check.py", "content": "import pytest\nfrom unittest.mock import Mock, patch\nimport requests\n\nfrom cloudproxy.check import requests_retry_session, fetch_ip, check_alive\nfrom cloudproxy.providers import settings\n\n@pytest.fixture\ndef mock_session():\n    \"\"\"Create a mock session for testing\"\"\"\n    session = Mock(spec=requests.Session)\n    session.mount = Mock()\n    session.get = Mock()\n    return session\n\ndef test_requests_retry_session_defaults():\n    \"\"\"Test that requests_retry_session creates a session with default values\"\"\"\n    session = requests_retry_session()\n    assert isinstance(session, requests.Session)\n    \n    # Check that adapters are mounted\n    assert any(a.startswith(\"http://\") for a in session.adapters.keys())\n    assert any(a.startswith(\"https://\") for a in session.adapters.keys())\n\ndef test_requests_retry_session_custom_params():\n    \"\"\"Test that requests_retry_session accepts custom parameters\"\"\"\n    custom_session = Mock(spec=requests.Session)\n    custom_session.mount = Mock()\n    \n    result = requests_retry_session(\n        retries=5,\n        backoff_factor=0.5,\n        status_forcelist=(500, 501, 502),\n        session=custom_session\n    )\n    \n    assert result == custom_session\n    assert custom_session.mount.call_count == 2\n\n@patch('cloudproxy.check.requests_retry_session')\ndef test_fetch_ip_no_auth(mock_retry_session):\n    \"\"\"Test fetch_ip function with authentication disabled\"\"\"\n    # Setup\n    mock_response = Mock()\n    mock_response.text = \"192.168.1.1\"\n    mock_session = Mock()\n    mock_session.get.return_value = mock_response\n    mock_retry_session.return_value = mock_session\n    \n    # Set no_auth to True\n    original_no_auth = settings.config[\"no_auth\"]\n    settings.config[\"no_auth\"] = True\n    \n    try:\n        # Execute\n        result = fetch_ip(\"10.0.0.1\")\n        \n        # Verify\n        assert result == \"192.168.1.1\"\n        expected_proxies = {\n            \"http\": \"http://10.0.0.1:8899\",\n            \"https\": \"http://10.0.0.1:8899\",\n        }\n        mock_session.get.assert_called_once_with(\n            \"https://api.ipify.org\", proxies=expected_proxies, timeout=10\n        )\n    finally:\n        # Restore original setting\n        settings.config[\"no_auth\"] = original_no_auth\n\n@patch('cloudproxy.check.requests_retry_session')\ndef test_fetch_ip_with_auth(mock_retry_session):\n    \"\"\"Test fetch_ip function with authentication enabled\"\"\"\n    # Setup\n    mock_response = Mock()\n    mock_response.text = \"192.168.1.1\"\n    mock_session = Mock()\n    mock_session.get.return_value = mock_response\n    mock_retry_session.return_value = mock_session\n    \n    # Set no_auth to False and configure auth settings\n    original_no_auth = settings.config[\"no_auth\"]\n    original_username = settings.config[\"auth\"][\"username\"]\n    original_password = settings.config[\"auth\"][\"password\"]\n    \n    settings.config[\"no_auth\"] = False\n    settings.config[\"auth\"][\"username\"] = \"testuser\"\n    settings.config[\"auth\"][\"password\"] = \"testpass\"\n    \n    try:\n        # Execute\n        result = fetch_ip(\"10.0.0.1\")\n        \n        # Verify\n        assert result == \"192.168.1.1\"\n        expected_proxies = {\n            \"http\": \"http://testuser:testpass@10.0.0.1:8899\",\n            \"https\": \"http://testuser:testpass@10.0.0.1:8899\",\n        }\n        mock_session.get.assert_called_once_with(\n            \"https://api.ipify.org\", proxies=expected_proxies, timeout=10\n        )\n    finally:\n        # Restore original settings\n        settings.config[\"no_auth\"] = original_no_auth\n        settings.config[\"auth\"][\"username\"] = original_username\n        settings.config[\"auth\"][\"password\"] = original_password\n\n@patch('cloudproxy.check.requests.get')\ndef test_check_alive_success(mock_get):\n    \"\"\"Test check_alive function with a successful response\"\"\"\n    # Setup for successful response\n    mock_response = Mock()\n    mock_response.status_code = 200\n    mock_get.return_value = mock_response\n    \n    # Execute\n    result = check_alive(\"10.0.0.1\")\n    \n    # Verify\n    assert result is True\n    mock_get.assert_called_once_with(\n        \"http://ipecho.net/plain\", \n        proxies={'http': \"http://10.0.0.1:8899\"}, \n        timeout=10\n    )\n\n@patch('cloudproxy.check.requests.get')\ndef test_check_alive_auth_required(mock_get):\n    \"\"\"Test check_alive function with 407 status code\"\"\"\n    # Setup for auth required response\n    mock_response = Mock()\n    mock_response.status_code = 407  # Proxy Authentication Required\n    mock_get.return_value = mock_response\n    \n    # Execute\n    result = check_alive(\"10.0.0.1\")\n    \n    # Verify\n    assert result is True  # Should still return True for 407\n\n@patch('cloudproxy.check.requests.get')\ndef test_check_alive_error_status(mock_get):\n    \"\"\"Test check_alive function with error status code\"\"\"\n    # Setup for error response\n    mock_response = Mock()\n    mock_response.status_code = 500\n    mock_get.return_value = mock_response\n    \n    # Execute\n    result = check_alive(\"10.0.0.1\")\n    \n    # Verify\n    assert result is False\n\n@patch('cloudproxy.check.requests.get')\ndef test_check_alive_exception(mock_get):\n    \"\"\"Test check_alive function with exception\"\"\"\n    # Setup to raise exception\n    mock_get.side_effect = requests.exceptions.RequestException(\"Connection error\")\n    \n    # Execute\n    result = check_alive(\"10.0.0.1\")\n    \n    # Verify\n    assert result is False "}
{"type": "test_file", "path": "tests/test_check_multi_account.py", "content": "import pytest\nfrom unittest.mock import patch, MagicMock\nimport requests\n\nfrom cloudproxy.check import check_alive, fetch_ip\nfrom cloudproxy.providers import settings\n\n\n@pytest.fixture\ndef mock_proxy_data():\n    \"\"\"Fixture for mock proxy data from different provider instances.\"\"\"\n    return [\n        {\n            \"ip\": \"192.168.1.10\",\n            \"provider\": \"digitalocean\",\n            \"instance\": \"default\",\n            \"display_name\": \"London DO\"\n        },\n        {\n            \"ip\": \"192.168.1.20\",\n            \"provider\": \"digitalocean\",\n            \"instance\": \"nyc\",\n            \"display_name\": \"New York DO\"\n        },\n        {\n            \"ip\": \"192.168.1.30\",\n            \"provider\": \"aws\",\n            \"instance\": \"default\",\n            \"display_name\": \"US East AWS\"\n        },\n        {\n            \"ip\": \"192.168.1.40\",\n            \"provider\": \"aws\",\n            \"instance\": \"eu\",\n            \"display_name\": \"EU West AWS\"\n        },\n        {\n            \"ip\": \"192.168.1.50\",\n            \"provider\": \"hetzner\",\n            \"instance\": \"default\",\n            \"display_name\": \"Germany Hetzner\"\n        }\n    ]\n\n\n@pytest.fixture(autouse=True)\ndef setup_auth():\n    \"\"\"Setup authentication for all tests.\"\"\"\n    # Save original config\n    original_auth = settings.config.get(\"auth\", {}).copy() if \"auth\" in settings.config else {}\n    original_no_auth = settings.config.get(\"no_auth\", True)\n    \n    # Set test credentials\n    settings.config[\"auth\"] = {\n        \"username\": \"testuser\",\n        \"password\": \"testpass\"\n    }\n    settings.config[\"no_auth\"] = False\n    \n    yield\n    \n    # Restore original config\n    settings.config[\"auth\"] = original_auth\n    settings.config[\"no_auth\"] = original_no_auth\n\n\n@patch('cloudproxy.check.requests.get')\ndef test_check_alive_for_different_instances(mock_requests_get, mock_proxy_data):\n    \"\"\"Test check_alive function for proxies from different provider instances.\"\"\"\n    # Setup mock response with success status code\n    mock_response = MagicMock()\n    mock_response.status_code = 200\n    mock_requests_get.return_value = mock_response\n    \n    # Test check_alive for each proxy\n    for proxy in mock_proxy_data:\n        # Call function under test\n        result = check_alive(proxy[\"ip\"])\n        \n        # Verify result\n        assert result is True, f\"check_alive for {proxy['ip']} from {proxy['provider']}/{proxy['instance']} should return True\"\n        \n        # Verify correct proxy was used in the request\n        expected_proxy = {'http': f'http://{proxy[\"ip\"]}:8899'}\n        mock_requests_get.assert_called_with(\n            \"http://ipecho.net/plain\", \n            proxies=expected_proxy, \n            timeout=10\n        )\n        \n        # Reset mock for next iteration\n        mock_requests_get.reset_mock()\n\n\n@patch('cloudproxy.check.requests_retry_session')\ndef test_fetch_ip_with_auth_for_different_instances(mock_retry_session, mock_proxy_data):\n    \"\"\"Test fetch_ip function with authentication for proxies from different provider instances.\"\"\"\n    # Disable no_auth\n    settings.config[\"no_auth\"] = False\n    \n    # Setup mock response\n    mock_response = MagicMock()\n    mock_response.text = \"mocked-ip-response\"\n    \n    # Setup mock session\n    mock_session = MagicMock()\n    mock_session.get.return_value = mock_response\n    mock_retry_session.return_value = mock_session\n    \n    # Test fetch_ip for each proxy\n    for proxy in mock_proxy_data:\n        # Call function under test\n        result = fetch_ip(proxy[\"ip\"])\n        \n        # Verify result\n        assert result == \"mocked-ip-response\", f\"fetch_ip for {proxy['ip']} from {proxy['provider']}/{proxy['instance']} returned unexpected value\"\n        \n        # Verify correct proxies were used in the request\n        expected_proxies = {\n            'http': f'http://testuser:testpass@{proxy[\"ip\"]}:8899',\n            'https': f'http://testuser:testpass@{proxy[\"ip\"]}:8899'\n        }\n        mock_session.get.assert_called_with(\n            \"https://api.ipify.org\",\n            proxies=expected_proxies,\n            timeout=10\n        )\n        \n        # Reset mocks for next iteration\n        mock_session.reset_mock()\n        mock_retry_session.reset_mock()\n\n\n@patch('cloudproxy.check.requests_retry_session')\ndef test_fetch_ip_without_auth_for_different_instances(mock_retry_session, mock_proxy_data):\n    \"\"\"Test fetch_ip function without authentication for proxies from different provider instances.\"\"\"\n    # Enable no_auth\n    settings.config[\"no_auth\"] = True\n    \n    # Setup mock response\n    mock_response = MagicMock()\n    mock_response.text = \"mocked-ip-response\"\n    \n    # Setup mock session\n    mock_session = MagicMock()\n    mock_session.get.return_value = mock_response\n    mock_retry_session.return_value = mock_session\n    \n    # Test fetch_ip for each proxy\n    for proxy in mock_proxy_data:\n        # Call function under test\n        result = fetch_ip(proxy[\"ip\"])\n        \n        # Verify result\n        assert result == \"mocked-ip-response\", f\"fetch_ip for {proxy['ip']} from {proxy['provider']}/{proxy['instance']} returned unexpected value\"\n        \n        # Verify correct proxies were used in the request\n        expected_proxies = {\n            'http': f'http://{proxy[\"ip\"]}:8899',\n            'https': f'http://{proxy[\"ip\"]}:8899'\n        }\n        mock_session.get.assert_called_with(\n            \"https://api.ipify.org\",\n            proxies=expected_proxies,\n            timeout=10\n        )\n        \n        # Reset mocks for next iteration\n        mock_session.reset_mock()\n        mock_retry_session.reset_mock()\n\n\n@patch('cloudproxy.check.requests.get')\ndef test_check_alive_exception_handling_for_different_instances(mock_get, mock_proxy_data):\n    \"\"\"Test that check_alive properly handles exceptions for proxies from different provider instances.\"\"\"\n    # List of exceptions to test\n    exceptions = [\n        requests.exceptions.ConnectTimeout(\"Connection timed out\"),\n        requests.exceptions.ConnectionError(\"Connection refused\"),\n        requests.exceptions.ReadTimeout(\"Read timed out\"),\n        requests.exceptions.ProxyError(\"Proxy error\")\n    ]\n    \n    # Last proxy in the list will work\n    last_proxy = mock_proxy_data[-1]\n    \n    # Test each proxy\n    for i, proxy in enumerate(mock_proxy_data):\n        if i < len(mock_proxy_data) - 1:\n            # Configure exception for proxies except the last one\n            mock_get.side_effect = exceptions[i % len(exceptions)]\n        else:\n            # Configure success for the last proxy\n            mock_response = MagicMock()\n            mock_response.status_code = 200\n            mock_get.side_effect = None\n            mock_get.return_value = mock_response\n        \n        # Call function under test\n        result = check_alive(proxy[\"ip\"])\n        \n        # Verify result\n        if proxy[\"ip\"] == last_proxy[\"ip\"]:\n            assert result is True, f\"Proxy {proxy['ip']} should be alive\"\n        else:\n            assert result is False, f\"Proxy {proxy['ip']} should handle exception and return False\"\n        \n        # Reset mock for next iteration\n        mock_get.reset_mock() "}
{"type": "test_file", "path": "tests/test_main_routes.py", "content": "from fastapi.testclient import TestClient\nimport pytest\nfrom unittest.mock import patch, Mock\n\nfrom cloudproxy.main import app, get_ip_list\nfrom cloudproxy.providers.settings import delete_queue, restart_queue, config\n\n# Create test client\nclient = TestClient(app)\n\n# Fixture to preserve original settings\n@pytest.fixture\ndef setup_test_data():\n    \"\"\"Fixture to setup test data and restore original values after test\"\"\"\n    # Save original values\n    original_providers = config[\"providers\"].copy()\n    original_delete_queue = delete_queue.copy()\n    original_restart_queue = restart_queue.copy()\n    \n    # Setup test data\n    for provider in [\"digitalocean\", \"aws\", \"gcp\", \"hetzner\", \"azure\"]:\n        config[\"providers\"][provider][\"instances\"][\"default\"][\"ips\"] = []\n    \n    config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"ips\"] = [\"1.1.1.1\", \"2.2.2.2\"]\n    config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"ips\"] = [\"3.3.3.3\", \"4.4.4.4\"]\n    config[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"ips\"] = [\"5.5.5.5\", \"6.6.6.6\"]\n    config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"ips\"] = [\"7.7.7.7\", \"8.8.8.8\"]\n    config[\"providers\"][\"azure\"][\"instances\"][\"default\"][\"ips\"] = [\"9.9.9.9\", \"10.10.10.10\"]\n    \n    # Add a test instance for AWS\n    config[\"providers\"][\"aws\"][\"instances\"][\"production\"] = {\n        \"enabled\": True,\n        \"ips\": [\"11.11.11.11\", \"12.12.12.12\"],\n        \"scaling\": {\"min_scaling\": 2, \"max_scaling\": 4},\n        \"size\": \"t3.medium\",\n        \"region\": \"us-west-2\",\n        \"display_name\": \"AWS Production\"\n    }\n    \n    # Clear queues\n    delete_queue.clear()\n    restart_queue.clear()\n    \n    yield\n    \n    # Restore original values\n    config[\"providers\"] = original_providers\n    delete_queue.clear()\n    delete_queue.update(original_delete_queue)\n    restart_queue.clear()\n    restart_queue.update(original_restart_queue)\n\n# Tests for root endpoint with pagination\ndef test_root_endpoint_default_pagination(setup_test_data):\n    \"\"\"Test the root endpoint with default pagination (offset=0, limit=10)\"\"\"\n    response = client.get(\"/\")\n    assert response.status_code == 200\n    data = response.json()\n    \n    assert \"metadata\" in data\n    assert \"total\" in data\n    assert \"proxies\" in data\n    assert data[\"total\"] == 12  # Total IPs across all providers with AWS production instance\n    assert len(data[\"proxies\"]) == 10  # Default limit is 10, we have 12 total\n    \n    # Check that IPs from each provider are included\n    ip_values = [proxy[\"ip\"] for proxy in data[\"proxies\"]]\n    # The test should check for IPs that are actually in the data\n    assert \"1.1.1.1\" in ip_values\n    assert \"3.3.3.3\" in ip_values\n    assert \"5.5.5.5\" in ip_values\n    assert \"7.7.7.7\" in ip_values\n    # One of the actual IPs from AWS production instead of 9.9.9.9\n    assert \"11.11.11.11\" in ip_values\n\ndef test_root_endpoint_custom_pagination(setup_test_data):\n    \"\"\"Test the root endpoint with custom pagination parameters\"\"\"\n    response = client.get(\"/?offset=2&limit=3\")\n    assert response.status_code == 200\n    data = response.json()\n    \n    assert data[\"total\"] == 12  # Total IPs across all providers with AWS production instance\n    assert len(data[\"proxies\"]) == 3  # Requested limit of 3\n    \n    # Third, fourth, and fifth IPs based on the order in get_ip_list\n    ip_values = [proxy[\"ip\"] for proxy in data[\"proxies\"]]\n    # The exact IPs will depend on the order they're returned by get_ip_list\n    assert len(ip_values) == 3\n    \ndef test_root_endpoint_pagination_bounds(setup_test_data):\n    \"\"\"Test the root endpoint with pagination at the bounds\"\"\"\n    # Test offset beyond available data\n    response = client.get(\"/?offset=12\")\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"total\"] == 12  # Total IPs across all providers with AWS production instance\n    assert len(data[\"proxies\"]) == 0  # No proxies returned\n    \n    # Test with very large limit\n    response = client.get(\"/?limit=100\")\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"total\"] == 12  # Total should be 12 not 10\n    assert len(data[\"proxies\"]) == 12  # All proxies returned\n\ndef test_provider_instance_formatting(setup_test_data):\n    \"\"\"Test that proxies contain provider and instance information\"\"\"\n    response = client.get(\"/\")\n    assert response.status_code == 200\n    data = response.json()\n    \n    # Check that proxies from different providers have the correct provider and instance info\n    proxies = data[\"proxies\"]\n    \n    # Verify AWS default instance\n    aws_default_proxy = next((p for p in proxies if p[\"ip\"] in [\"3.3.3.3\", \"4.4.4.4\"]), None)\n    assert aws_default_proxy is not None\n    assert aws_default_proxy[\"provider\"] == \"aws\"\n    assert aws_default_proxy[\"instance\"] == \"default\"\n    \n    # Verify AWS production instance\n    aws_production_proxy = next((p for p in proxies if p[\"ip\"] in [\"11.11.11.11\", \"12.12.12.12\"]), None)\n    assert aws_production_proxy is not None\n    assert aws_production_proxy[\"provider\"] == \"aws\"\n    assert aws_production_proxy[\"instance\"] == \"production\"\n    assert aws_production_proxy[\"display_name\"] == \"AWS Production\"\n\n# Tests for /destroy endpoints\ndef test_destroy_get_endpoint_empty(setup_test_data):\n    \"\"\"Test the GET /destroy endpoint with empty queue\"\"\"\n    response = client.get(\"/destroy\")\n    assert response.status_code == 200\n    data = response.json()\n    \n    assert data[\"total\"] == 0\n    assert len(data[\"proxies\"]) == 0\n\ndef test_destroy_get_endpoint_with_items(setup_test_data):\n    \"\"\"Test the GET /destroy endpoint with items in queue\"\"\"\n    # Add some IPs to delete queue\n    delete_queue.add(\"1.1.1.1\")\n    delete_queue.add(\"3.3.3.3\")\n    \n    response = client.get(\"/destroy\")\n    assert response.status_code == 200\n    data = response.json()\n    \n    assert data[\"total\"] == 2\n    assert len(data[\"proxies\"]) == 2\n    \n    ip_values = [proxy[\"ip\"] for proxy in data[\"proxies\"]]\n    assert \"1.1.1.1\" in ip_values\n    assert \"3.3.3.3\" in ip_values\n\ndef test_destroy_delete_endpoint(setup_test_data):\n    \"\"\"Test the DELETE /destroy endpoint\"\"\"\n    # Add an IP to the providers list\n    ip_to_delete = \"9.9.9.9\"\n    config[\"providers\"][\"azure\"][\"instances\"][\"default\"][\"ips\"].append(ip_to_delete)\n    \n    # Schedule the IP for deletion\n    response = client.delete(f\"/destroy?ip_address={ip_to_delete}\")\n    assert response.status_code == 200\n    data = response.json()\n    \n    assert data[\"message\"] == \"Proxy scheduled for deletion\"\n    assert data[\"proxy\"][\"ip\"] == ip_to_delete\n    assert ip_to_delete in delete_queue\n\n# Tests for /restart endpoints\ndef test_restart_get_endpoint_empty(setup_test_data):\n    \"\"\"Test the GET /restart endpoint with empty queue\"\"\"\n    response = client.get(\"/restart\")\n    assert response.status_code == 200\n    data = response.json()\n    \n    assert data[\"total\"] == 0\n    assert len(data[\"proxies\"]) == 0\n\ndef test_restart_get_endpoint_with_items(setup_test_data):\n    \"\"\"Test the GET /restart endpoint with items in queue\"\"\"\n    # Add some IPs to restart queue\n    restart_queue.add(\"2.2.2.2\")\n    restart_queue.add(\"4.4.4.4\")\n    \n    response = client.get(\"/restart\")\n    assert response.status_code == 200\n    data = response.json()\n    \n    assert data[\"total\"] == 2\n    assert len(data[\"proxies\"]) == 2\n    \n    ip_values = [proxy[\"ip\"] for proxy in data[\"proxies\"]]\n    assert \"2.2.2.2\" in ip_values\n    assert \"4.4.4.4\" in ip_values\n\ndef test_restart_delete_endpoint(setup_test_data):\n    \"\"\"Test the DELETE /restart endpoint\"\"\"\n    # Use an existing IP from the providers list\n    ip_to_restart = \"5.5.5.5\"\n    \n    # Schedule the IP for restart\n    response = client.delete(f\"/restart?ip_address={ip_to_restart}\")\n    assert response.status_code == 200\n    data = response.json()\n    \n    assert data[\"message\"] == \"Proxy scheduled for restart\"\n    assert data[\"proxy\"][\"ip\"] == ip_to_restart\n    assert ip_to_restart in restart_queue\n\n# Tests for provider management endpoints\ndef test_providers_get_all(setup_test_data):\n    \"\"\"Test the GET /providers endpoint\"\"\"\n    response = client.get(\"/providers\")\n    assert response.status_code == 200\n    data = response.json()\n    \n    assert \"providers\" in data\n    assert \"metadata\" in data\n    assert \"digitalocean\" in data[\"providers\"]\n    assert \"aws\" in data[\"providers\"]\n    assert \"gcp\" in data[\"providers\"]\n    assert \"hetzner\" in data[\"providers\"]\n    assert \"azure\" in data[\"providers\"]\n    \n    # Check that aws has both instances\n    aws_provider = data[\"providers\"][\"aws\"]\n    assert \"instances\" in aws_provider\n    assert \"default\" in aws_provider[\"instances\"]\n    assert \"production\" in aws_provider[\"instances\"]\n    \n    # Check production instance data\n    production_instance = aws_provider[\"instances\"][\"production\"]\n    assert production_instance[\"display_name\"] == \"AWS Production\"\n    assert production_instance[\"ips\"] == [\"11.11.11.11\", \"12.12.12.12\"]\n\ndef test_provider_get_specific(setup_test_data):\n    \"\"\"Test the GET /providers/{provider} endpoint\"\"\"\n    response = client.get(\"/providers/digitalocean\")\n    assert response.status_code == 200\n    data = response.json()\n    \n    assert \"instances\" in data\n    assert \"default\" in data[\"instances\"]\n    \n    default_instance = data[\"instances\"][\"default\"]\n    assert default_instance[\"ips\"] == [\"1.1.1.1\", \"2.2.2.2\"]\n    assert \"scaling\" in default_instance\n    assert \"region\" in default_instance\n\ndef test_provider_instance_get(setup_test_data):\n    \"\"\"Test the GET /providers/{provider}/{instance} endpoint\"\"\"\n    response = client.get(\"/providers/aws/production\")\n    assert response.status_code == 200\n    data = response.json()\n    \n    assert \"metadata\" in data\n    assert \"message\" in data\n    assert \"provider\" in data\n    assert \"instance\" in data\n    assert \"config\" in data\n    \n    assert data[\"provider\"] == \"aws\"\n    assert data[\"instance\"] == \"production\"\n    assert data[\"message\"] == \"Provider 'aws' instance 'production' configuration retrieved successfully\"\n    \n    instance_config = data[\"config\"]\n    assert instance_config[\"display_name\"] == \"AWS Production\"\n    assert instance_config[\"ips\"] == [\"11.11.11.11\", \"12.12.12.12\"]\n    assert instance_config[\"size\"] == \"t3.medium\"\n    assert instance_config[\"region\"] == \"us-west-2\"\n    assert instance_config[\"scaling\"][\"min_scaling\"] == 2\n    assert instance_config[\"scaling\"][\"max_scaling\"] == 4\n\ndef test_provider_instance_get_not_found(setup_test_data):\n    \"\"\"Test the GET /providers/{provider}/{instance} with non-existent instance\"\"\"\n    response = client.get(\"/providers/aws/nonexistent\")\n    assert response.status_code == 404\n    data = response.json()\n    assert \"detail\" in data\n    assert \"not found\" in data[\"detail\"].lower()\n\ndef test_provider_patch_endpoint(setup_test_data):\n    \"\"\"Test the PATCH /providers/{provider} endpoint\"\"\"\n    # Save original scaling settings\n    original_min = config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"scaling\"][\"min_scaling\"]\n    original_max = config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"scaling\"][\"max_scaling\"]\n    \n    try:\n        # Update scaling settings\n        response = client.patch(\"/providers/aws\", json={\n            \"min_scaling\": 2,\n            \"max_scaling\": 5\n        })\n        assert response.status_code == 200\n        data = response.json()\n        \n        assert data[\"message\"] == \"Provider 'aws' scaling configuration updated successfully\"\n        assert data[\"provider\"][\"scaling\"][\"min_scaling\"] == 2\n        assert data[\"provider\"][\"scaling\"][\"max_scaling\"] == 5\n        \n        # Verify settings were actually updated in the config\n        assert config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"scaling\"][\"min_scaling\"] == 2\n        assert config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"scaling\"][\"max_scaling\"] == 5\n    finally:\n        # Restore original settings\n        config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"scaling\"][\"min_scaling\"] = original_min\n        config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"scaling\"][\"max_scaling\"] = original_max\n\ndef test_provider_instance_patch_endpoint(setup_test_data):\n    \"\"\"Test the PATCH /providers/{provider}/{instance} endpoint\"\"\"\n    # Save original scaling settings\n    original_min = config[\"providers\"][\"aws\"][\"instances\"][\"production\"][\"scaling\"][\"min_scaling\"]\n    original_max = config[\"providers\"][\"aws\"][\"instances\"][\"production\"][\"scaling\"][\"max_scaling\"]\n    \n    try:\n        # Update scaling settings for the production instance\n        response = client.patch(\"/providers/aws/production\", json={\n            \"min_scaling\": 3,\n            \"max_scaling\": 6\n        })\n        assert response.status_code == 200\n        data = response.json()\n        \n        assert data[\"message\"] == \"Provider 'aws' instance 'production' scaling configuration updated successfully\"\n        assert data[\"config\"][\"scaling\"][\"min_scaling\"] == 3\n        assert data[\"config\"][\"scaling\"][\"max_scaling\"] == 6\n        \n        # Verify settings were actually updated in the config\n        assert config[\"providers\"][\"aws\"][\"instances\"][\"production\"][\"scaling\"][\"min_scaling\"] == 3\n        assert config[\"providers\"][\"aws\"][\"instances\"][\"production\"][\"scaling\"][\"max_scaling\"] == 6\n        \n        # Verify the default instance was not affected\n        assert config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"scaling\"][\"min_scaling\"] != 3\n    finally:\n        # Restore original settings\n        config[\"providers\"][\"aws\"][\"instances\"][\"production\"][\"scaling\"][\"min_scaling\"] = original_min\n        config[\"providers\"][\"aws\"][\"instances\"][\"production\"][\"scaling\"][\"max_scaling\"] = original_max\n\ndef test_provider_instance_patch_invalid_scaling(setup_test_data):\n    \"\"\"Test the PATCH /providers/{provider}/{instance} with invalid scaling values\"\"\"\n    response = client.patch(\"/providers/aws/production\", json={\n        \"min_scaling\": 5,\n        \"max_scaling\": 3  # Invalid: max < min\n    })\n    assert response.status_code == 422  # Validation error\n    data = response.json()\n    assert \"detail\" in data\n\n# Edge cases and error handling\ndef test_random_no_proxies():\n    \"\"\"Test the /random endpoint when no proxies are available\"\"\"\n    # Save original IPs\n    original_providers = config[\"providers\"].copy()\n    \n    try:\n        # Clear all IPs\n        for provider in [\"digitalocean\", \"aws\", \"gcp\", \"hetzner\", \"azure\"]:\n            for instance in config[\"providers\"][provider][\"instances\"]:\n                config[\"providers\"][provider][\"instances\"][instance][\"ips\"] = []\n        \n        response = client.get(\"/random\")\n        assert response.status_code == 404\n        data = response.json()\n        assert \"detail\" in data\n        assert data[\"detail\"] == \"No proxies available\"\n    finally:\n        # Restore original IPs\n        config[\"providers\"] = original_providers\n\ndef test_provider_model_selection():\n    \"\"\"Test the provider model selection for different provider types\"\"\"\n    from cloudproxy.main import get_provider_model\n    \n    # Test DigitalOcean provider\n    do_config = {\n        \"instances\": {\n            \"default\": {\n                \"enabled\": True,\n                \"ips\": [\"1.1.1.1\"],\n                \"scaling\": {\"min_scaling\": 1, \"max_scaling\": 3},\n                \"size\": \"s-1vcpu-1gb\",\n                \"region\": \"nyc1\",\n                \"display_name\": \"DigitalOcean\"\n            }\n        }\n    }\n    do_model = get_provider_model(\"digitalocean\", do_config)\n    assert \"default\" in do_model.instances\n    assert do_model.instances[\"default\"].region == \"nyc1\"\n    \n    # Test AWS provider with multiple instances\n    aws_config = {\n        \"instances\": {\n            \"default\": {\n                \"enabled\": True,\n                \"ips\": [\"2.2.2.2\"],\n                \"scaling\": {\"min_scaling\": 1, \"max_scaling\": 3},\n                \"size\": \"t2.micro\",\n                \"region\": \"us-east-1\",\n                \"ami\": \"ami-12345\",\n                \"spot\": True,\n                \"display_name\": \"AWS Default\"\n            },\n            \"production\": {\n                \"enabled\": True,\n                \"ips\": [\"3.3.3.3\"],\n                \"scaling\": {\"min_scaling\": 2, \"max_scaling\": 4},\n                \"size\": \"t3.medium\",\n                \"region\": \"us-west-2\",\n                \"ami\": \"ami-67890\",\n                \"spot\": False,\n                \"display_name\": \"AWS Production\"\n            }\n        }\n    }\n    aws_model = get_provider_model(\"aws\", aws_config)\n    assert \"default\" in aws_model.instances\n    assert \"production\" in aws_model.instances\n    assert aws_model.instances[\"default\"].region == \"us-east-1\"\n    assert aws_model.instances[\"default\"].ami == \"ami-12345\"\n    assert aws_model.instances[\"default\"].spot is True\n    assert aws_model.instances[\"production\"].region == \"us-west-2\"\n    assert aws_model.instances[\"production\"].display_name == \"AWS Production\" "}
{"type": "test_file", "path": "tests/test_providers_digitalocean_firewall.py", "content": "import pytest\nfrom unittest.mock import Mock, patch, MagicMock\nimport digitalocean\nfrom cloudproxy.providers.digitalocean.functions import (\n    create_firewall, DOFirewallExistsException\n)\n\nclass TestDigitalOceanFirewall:\n    @pytest.fixture(autouse=True)\n    def setup_teardown(self):\n        \"\"\"Setup before each test and cleanup after\"\"\"\n        yield  # This is where the testing happens\n\n    def test_create_firewall(self, mocker):\n        \"\"\"Test successful firewall creation\"\"\"\n        # Mock the DigitalOcean client and firewall\n        mock_manager = Mock()\n        mock_firewall = Mock()\n        mock_firewall.id = \"123456\"\n        \n        # Mock the digitalocean.Manager and FirewallManager\n        mocker.patch(\n            'cloudproxy.providers.digitalocean.functions.digitalocean.Manager',\n            return_value=mock_manager\n        )\n        \n        # Mock the Firewall class\n        mock_fw = Mock()\n        mocker.patch(\n            'cloudproxy.providers.digitalocean.functions.digitalocean.Firewall',\n            return_value=mock_fw\n        )\n        \n        mock_manager.get_all_firewalls = MagicMock(return_value=[])\n        \n        # Call the function\n        create_firewall()\n        \n        # Verify the expected behaviors\n        mock_fw.create.assert_called_once()\n        \n    def test_create_firewall_already_exists(self, mocker):\n        \"\"\"Test handling of duplicate firewall name\"\"\"\n        # Mock the DataReadError class\n        class MockDataReadError(Exception):\n            pass\n            \n        mocker.patch(\n            'cloudproxy.providers.digitalocean.functions.digitalocean.DataReadError',\n            MockDataReadError\n        )\n        \n        # Mock the Firewall class\n        mock_fw = Mock()\n        mocker.patch(\n            'cloudproxy.providers.digitalocean.functions.digitalocean.Firewall',\n            return_value=mock_fw\n        )\n        \n        # Set up the error\n        mock_fw.create.side_effect = MockDataReadError('duplicate name')\n        \n        # Call the function and expect exception\n        with pytest.raises(DOFirewallExistsException) as exc_info:\n            create_firewall()\n        \n        # Verify the exception message\n        assert \"Firewall already exists\" in str(exc_info.value)\n        \n    def test_create_firewall_other_error(self, mocker):\n        \"\"\"Test handling of other errors during firewall creation\"\"\"\n        # Mock the Firewall class\n        mock_fw = Mock()\n        mocker.patch(\n            'cloudproxy.providers.digitalocean.functions.digitalocean.Firewall',\n            return_value=mock_fw\n        )\n        \n        # Set up a general exception\n        mock_fw.create.side_effect = Exception(\"API Error\")\n        \n        # Call the function - it should propagate the exception\n        with pytest.raises(Exception) as exc_info:\n            create_firewall()\n        \n        # Verify the exception message\n        assert \"API Error\" in str(exc_info.value)\n        mock_fw.create.assert_called_once() "}
{"type": "test_file", "path": "tests/test_proxy_model.py", "content": "import pytest\nfrom unittest.mock import patch, MagicMock\nimport ipaddress\nimport os\nfrom cloudproxy.providers import settings\nfrom cloudproxy.main import ProxyAddress, create_proxy_address\n\n\n@pytest.fixture(autouse=True)\ndef setup_auth():\n    \"\"\"Set up authentication config for all tests.\"\"\"\n    # Save original config\n    original_auth = settings.config.get(\"auth\", {}).copy() if \"auth\" in settings.config else {}\n    original_no_auth = settings.config.get(\"no_auth\", True)\n    \n    # Set test credentials\n    settings.config[\"auth\"] = {\n        \"username\": \"testuser\",\n        \"password\": \"testpass\"\n    }\n    settings.config[\"no_auth\"] = False\n    \n    # Print for debugging\n    print(f\"Setup auth. Config: {settings.config['auth']}\")\n    \n    yield\n    \n    # Restore original config\n    settings.config[\"auth\"] = original_auth\n    settings.config[\"no_auth\"] = original_no_auth\n\ndef test_proxy_address_with_provider_instance():\n    \"\"\"Test creating a ProxyAddress with provider and instance information.\"\"\"\n    proxy = ProxyAddress(\n        ip=\"192.168.1.1\",\n        port=8899,\n        auth_enabled=True,\n        provider=\"aws\",\n        instance=\"eu-west\",\n        display_name=\"Europe AWS\"\n    )\n    \n    assert str(proxy.ip) == \"192.168.1.1\"\n    assert proxy.port == 8899\n    assert proxy.auth_enabled is True\n    assert proxy.provider == \"aws\"\n    assert proxy.instance == \"eu-west\"\n    assert proxy.display_name == \"Europe AWS\"\n\ndef test_proxy_address_without_provider_instance():\n    \"\"\"Test creating a ProxyAddress without provider and instance information.\"\"\"\n    proxy = ProxyAddress(\n        ip=\"192.168.1.1\",\n        port=8899,\n        auth_enabled=True\n    )\n    \n    assert str(proxy.ip) == \"192.168.1.1\"\n    assert proxy.port == 8899\n    assert proxy.auth_enabled is True\n    assert proxy.provider is None\n    assert proxy.instance is None\n    assert proxy.display_name is None\n\ndef test_proxy_address_with_provider_without_instance():\n    \"\"\"Test creating a ProxyAddress with provider but without instance information.\"\"\"\n    proxy = ProxyAddress(\n        ip=\"192.168.1.1\",\n        port=8899,\n        auth_enabled=True,\n        provider=\"digitalocean\"\n    )\n    \n    assert str(proxy.ip) == \"192.168.1.1\"\n    assert proxy.port == 8899\n    assert proxy.auth_enabled is True\n    assert proxy.provider == \"digitalocean\"\n    assert proxy.instance is None\n    assert proxy.display_name is None\n\ndef test_proxy_address_url_with_auth():\n    \"\"\"Test that the URL field includes authentication when auth_enabled is True.\"\"\"\n    # Print for debugging\n    print(f\"Auth config: {settings.config.get('auth', {})}\")\n    \n    # Manually set the expected URL\n    expected_url = f\"http://testuser:testpass@192.168.1.1:8899\"\n    \n    # Create the proxy object with the URL set explicitly\n    proxy = ProxyAddress(\n        ip=\"192.168.1.1\",\n        port=8899,\n        auth_enabled=True,\n        provider=\"aws\",\n        instance=\"eu-west\",\n        url=expected_url\n    )\n    \n    # Debug print\n    print(f\"Proxy: {proxy}\")\n    print(f\"Proxy URL: {proxy.url}\")\n    print(f\"Expected URL: {expected_url}\")\n    \n    # Check URL is set correctly\n    assert proxy.url is not None, \"URL should not be None\"\n    assert \"testuser:testpass@\" in proxy.url\n    assert proxy.url == expected_url\n\ndef test_proxy_address_url_without_auth():\n    \"\"\"Test that the URL field doesn't include authentication when auth_enabled is False.\"\"\"\n    # Manually set the expected URL\n    expected_url = \"http://192.168.1.1:8899\"\n    \n    proxy = ProxyAddress(\n        ip=\"192.168.1.1\",\n        port=8899,\n        auth_enabled=False,\n        provider=\"aws\",\n        instance=\"eu-west\",\n        url=expected_url\n    )\n    \n    # Debug print\n    print(f\"Proxy: {proxy}\")\n    print(f\"Proxy URL: {proxy.url}\")\n    \n    assert proxy.url is not None, \"URL should not be None\"\n    assert \"testuser:testpass@\" not in proxy.url\n    assert proxy.url == expected_url\n\ndef test_create_proxy_address():\n    \"\"\"Test creating a proxy address using create_proxy_address function.\"\"\"\n    # Setup\n    settings.config[\"no_auth\"] = False\n    \n    # Test function\n    proxy = create_proxy_address(ip=\"192.168.1.1\")\n    \n    # Verify\n    assert str(proxy.ip) == \"192.168.1.1\"\n    assert proxy.port == 8899\n    assert proxy.auth_enabled is True\n    assert proxy.provider is None\n    assert proxy.instance is None\n    \n    # Cleanup\n    settings.config[\"no_auth\"] = False\n\ndef test_create_proxy_address_no_auth():\n    \"\"\"Test creating a proxy address with no_auth=True.\"\"\"\n    # Setup\n    settings.config[\"no_auth\"] = True\n    \n    # Test function\n    proxy = create_proxy_address(ip=\"192.168.1.1\")\n    \n    # Verify\n    assert str(proxy.ip) == \"192.168.1.1\"\n    assert proxy.port == 8899\n    assert proxy.auth_enabled is False\n    assert proxy.provider is None\n    assert proxy.instance is None\n    \n    # Cleanup\n    settings.config[\"no_auth\"] = False\n\ndef test_model_serialization_with_provider_info():\n    \"\"\"Test that ProxyAddress model correctly serializes with provider information.\"\"\"\n    proxy = ProxyAddress(\n        ip=\"192.168.1.1\",\n        port=8899,\n        auth_enabled=True,\n        provider=\"aws\",\n        instance=\"eu-west\",\n        display_name=\"Europe AWS\"\n    )\n    \n    serialized = proxy.model_dump()\n    assert str(serialized[\"ip\"]) == \"192.168.1.1\"\n    assert serialized[\"port\"] == 8899\n    assert serialized[\"auth_enabled\"] is True\n    assert serialized[\"provider\"] == \"aws\"\n    assert serialized[\"instance\"] == \"eu-west\"\n    assert serialized[\"display_name\"] == \"Europe AWS\" "}
{"type": "test_file", "path": "tests/test_providers_digitalocean_main.py", "content": "import pytest\nfrom cloudproxy.providers.digitalocean.main import do_deployment, do_start\nfrom cloudproxy.providers.digitalocean.functions import list_droplets, delete_proxy\nfrom tests.test_providers_digitalocean_functions import load_from_file\n\n\n@pytest.fixture\ndef droplets(mocker):\n    \"\"\"Fixture for droplets data.\"\"\"\n    data = load_from_file('test_providers_digitalocean_functions_droplets_all.json')\n    # Convert the dictionary droplets to Droplet objects\n    from tests.test_providers_digitalocean_functions import Droplet\n    droplet_objects = []\n    for droplet_dict in data['droplets']:\n        droplet = Droplet(droplet_dict['id'])\n        # Add tags attribute to the droplet\n        droplet.tags = [\"cloudproxy\"]\n        droplet_objects.append(droplet)\n    \n    mocker.patch('cloudproxy.providers.digitalocean.functions.digitalocean.Manager.get_all_droplets',\n                 return_value=droplet_objects)\n    return droplet_objects\n\n\n@pytest.fixture\ndef droplet_id():\n    \"\"\"Fixture for droplet ID.\"\"\"\n    return \"DROPLET-ID\"\n\n\ndef test_do_deployment(mocker, droplets, droplet_id):\n    mocker.patch(\n        'cloudproxy.providers.digitalocean.main.list_droplets',\n        return_value=droplets\n    )\n    mocker.patch(\n        'cloudproxy.providers.digitalocean.main.create_proxy',\n        return_value=True\n    )\n    mocker.patch(\n        'cloudproxy.providers.digitalocean.main.delete_proxy',\n        return_value=True\n    )\n    result = do_deployment(1)\n    assert isinstance(result, int)\n    assert result == 1\n\n\ndef test_initiatedo(mocker):\n    mocker.patch(\n        'cloudproxy.providers.digitalocean.main.do_deployment',\n        return_value=2\n    )\n    mocker.patch(\n        'cloudproxy.providers.digitalocean.main.do_check_alive',\n        return_value=[\"192.1.1.1\"]\n    )\n    mocker.patch(\n        'cloudproxy.providers.digitalocean.main.do_check_delete',\n        return_value=True\n    )\n    result = do_start()\n    assert isinstance(result, list)\n    assert result == [\"192.1.1.1\"]\n\n\ndef test_list_droplets(droplets):\n    \"\"\"Test listing droplets.\"\"\"\n    result = list_droplets()\n    assert isinstance(result, list)\n    assert len(result) > 0\n    assert result[0].id == 3164444  # Verify specific droplet data\n    # Store the result in a module-level variable if needed by other tests\n    global test_droplets\n    test_droplets = result\n\n\ndef test_delete_proxy(mocker, droplets):\n    \"\"\"Test deleting a proxy.\"\"\"\n    assert len(droplets) > 0\n    droplet_id = droplets[0].id\n    \n    # Mock the Manager and get_droplet method to avoid real API calls\n    mock_manager = mocker.patch('cloudproxy.providers.digitalocean.functions.get_manager')\n    mock_manager_instance = mocker.MagicMock()\n    mock_manager.return_value = mock_manager_instance\n    \n    # Mock the droplet that will be returned by get_droplet\n    mock_droplet = mocker.MagicMock()\n    mock_droplet.destroy.return_value = True\n    mock_manager_instance.get_droplet.return_value = mock_droplet\n    \n    # Test the delete_proxy function\n    assert delete_proxy(droplet_id) == True\n    \n    # Verify our mock was called correctly\n    mock_manager.assert_called_once()\n    mock_manager_instance.get_droplet.assert_called_once_with(droplet_id)\n    mock_droplet.destroy.assert_called_once()"}
{"type": "test_file", "path": "tests/test_providers_digitalocean_functions.py", "content": "import pytest\nfrom cloudproxy.providers.digitalocean.functions import create_proxy, delete_proxy, list_droplets, get_manager\nimport os\nimport json\nfrom unittest.mock import MagicMock, patch\nfrom cloudproxy.providers import settings\n\n\nclass Droplet:\n    def __init__(self, id):\n        self.id = id\n        self.tags = [\"cloudproxy\"]\n\n\ndef load_from_file(json_file):\n    cwd = os.path.dirname(__file__)\n    with open(os.path.join(cwd, json_file), 'r') as f:\n        return json.loads(f.read())\n\n\n@pytest.fixture\ndef droplets(mocker):\n    \"\"\"Fixture for droplets data.\"\"\"\n    data = load_from_file('test_providers_digitalocean_functions_droplets_all.json')\n    # Convert the dictionary droplets to Droplet objects\n    droplet_objects = []\n    for droplet_dict in data['droplets']:\n        droplet = Droplet(droplet_dict['id'])\n        # Add tags attribute to the droplet\n        droplet.tags = [\"cloudproxy\"]\n        droplet_objects.append(droplet)\n    \n    mocker.patch('cloudproxy.providers.digitalocean.functions.digitalocean.Manager.get_all_droplets',\n                 return_value=droplet_objects)\n    return droplet_objects\n\n\n@pytest.fixture\ndef instance_specific_droplets(mocker):\n    \"\"\"Fixture for instance-specific droplets data.\"\"\"\n    # Create droplets with instance-specific tags\n    droplet_objects = []\n    \n    # Default instance droplets\n    default_droplet1 = Droplet(1001)\n    default_droplet1.tags = [\"cloudproxy\", \"cloudproxy-default\"]\n    droplet_objects.append(default_droplet1)\n    \n    default_droplet2 = Droplet(1002)\n    default_droplet2.tags = [\"cloudproxy\", \"cloudproxy-default\"]\n    droplet_objects.append(default_droplet2)\n    \n    # Instance \"useast\" droplets\n    useast_droplet1 = Droplet(2001)\n    useast_droplet1.tags = [\"cloudproxy\", \"cloudproxy-useast\"]\n    droplet_objects.append(useast_droplet1)\n    \n    useast_droplet2 = Droplet(2002)\n    useast_droplet2.tags = [\"cloudproxy\", \"cloudproxy-useast\"]\n    droplet_objects.append(useast_droplet2)\n    \n    # Old-style droplets without instance tag\n    old_droplet = Droplet(3001)\n    old_droplet.tags = [\"cloudproxy\"]\n    droplet_objects.append(old_droplet)\n    \n    return droplet_objects\n\n\n@pytest.fixture\ndef test_instance_config():\n    \"\"\"Fixture for a test instance configuration.\"\"\"\n    return {\n        \"enabled\": True,\n        \"secrets\": {\n            \"access_token\": \"test-token-useast\"\n        },\n        \"size\": \"s-1vcpu-1gb\",\n        \"region\": \"nyc1\",\n        \"min_scaling\": 2,\n        \"max_scaling\": 5,\n        \"display_name\": \"US East\"\n    }\n\n\n@pytest.fixture\ndef droplet_id():\n    \"\"\"Fixture for droplet ID.\"\"\"\n    return \"DROPLET-ID\"\n\n\ndef test_list_droplets(droplets):\n    \"\"\"Test listing droplets.\"\"\"\n    result = list_droplets()\n    assert isinstance(result, list)\n    assert len(result) > 0\n    # Check that the first droplet has the correct ID\n    assert result[0].id == 3164444  # Verify specific droplet data\n\n\ndef test_create_proxy(mocker, droplet_id):\n    \"\"\"Test creating a proxy.\"\"\"\n    droplet = Droplet(droplet_id)\n    mocker.patch(\n        'cloudproxy.providers.digitalocean.functions.digitalocean.Droplet.create',\n        return_value=droplet\n    )\n    assert create_proxy() == True\n\n\ndef test_delete_proxy(mocker, droplets):\n    \"\"\"Test deleting a proxy.\"\"\"\n    assert len(droplets) > 0\n    droplet_id = droplets[0].id\n    \n    # Mock the Manager and get_droplet method to avoid real API calls\n    mock_manager = mocker.patch('cloudproxy.providers.digitalocean.functions.get_manager')\n    mock_manager_instance = mocker.MagicMock()\n    mock_manager.return_value = mock_manager_instance\n    \n    # Mock the droplet that will be returned by get_droplet\n    mock_droplet = mocker.MagicMock()\n    mock_droplet.destroy.return_value = True\n    mock_manager_instance.get_droplet.return_value = mock_droplet\n    \n    # Test the delete_proxy function\n    assert delete_proxy(droplet_id) == True\n    \n    # Verify our mock was called correctly\n    mock_manager.assert_called_once()\n    mock_manager_instance.get_droplet.assert_called_once_with(droplet_id)\n    mock_droplet.destroy.assert_called_once()\n\n\n@patch('cloudproxy.providers.digitalocean.functions.digitalocean.Manager')\ndef test_get_manager_default(mock_manager):\n    \"\"\"Test get_manager with default configuration.\"\"\"\n    # Setup mock\n    mock_manager_instance = MagicMock()\n    mock_manager.return_value = mock_manager_instance\n    \n    # Call function under test\n    result = get_manager()\n    \n    # Verify\n    mock_manager.assert_called_once()\n    assert mock_manager.call_args[1]['token'] == settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"secrets\"][\"access_token\"]\n    assert result == mock_manager_instance\n\n\n@patch('cloudproxy.providers.digitalocean.functions.digitalocean.Manager')\ndef test_get_manager_with_instance_config(mock_manager, test_instance_config):\n    \"\"\"Test get_manager with a specific instance configuration.\"\"\"\n    # Setup mock\n    mock_manager_instance = MagicMock()\n    mock_manager.return_value = mock_manager_instance\n    \n    # Call function under test\n    result = get_manager(test_instance_config)\n    \n    # Verify\n    mock_manager.assert_called_once()\n    assert mock_manager.call_args[1]['token'] == \"test-token-useast\"\n    assert result == mock_manager_instance\n\n\n@patch('cloudproxy.providers.digitalocean.functions.digitalocean.Droplet')\ndef test_create_proxy_with_instance_config(mock_droplet, test_instance_config):\n    \"\"\"Test creating a proxy with a specific instance configuration.\"\"\"\n    # Setup mock\n    mock_droplet_instance = MagicMock()\n    mock_droplet.return_value = mock_droplet_instance\n    \n    # Save the original config to restore later\n    original_config = settings.config[\"providers\"][\"digitalocean\"][\"instances\"].copy()\n    \n    # Add our test instance config\n    settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"useast\"] = test_instance_config\n    \n    try:\n        # Call function under test\n        result = create_proxy(test_instance_config)\n        \n        # Verify\n        mock_droplet.assert_called_once()\n        args, kwargs = mock_droplet.call_args\n        \n        # Verify token, region, and size from instance config were used\n        assert kwargs['token'] == \"test-token-useast\"\n        assert kwargs['region'] == \"nyc1\"\n        assert kwargs['size_slug'] == \"s-1vcpu-1gb\"\n        \n        # Verify that correct tags are set\n        assert \"cloudproxy\" in kwargs['tags']\n        assert \"cloudproxy-useast\" in kwargs['tags']\n        \n        # Verify name format includes instance identifier\n        assert \"cloudproxy-useast-\" in kwargs['name']\n        \n        assert result == True\n    finally:\n        # Restore original config\n        settings.config[\"providers\"][\"digitalocean\"][\"instances\"] = original_config\n\n\n@patch('cloudproxy.providers.digitalocean.functions.get_manager')\ndef test_delete_proxy_with_instance_config(mock_get_manager, mocker, test_instance_config):\n    \"\"\"Test deleting a proxy with a specific instance configuration.\"\"\"\n    # Setup mock\n    mock_manager = mocker.MagicMock()\n    mock_get_manager.return_value = mock_manager\n    \n    # Mock the droplet that will be returned by get_droplet\n    mock_droplet = mocker.MagicMock()\n    mock_droplet.destroy.return_value = True\n    mock_manager.get_droplet.return_value = mock_droplet\n    \n    # Call function under test\n    result = delete_proxy(1234, test_instance_config)\n    \n    # Verify\n    mock_get_manager.assert_called_once_with(test_instance_config)\n    mock_manager.get_droplet.assert_called_once_with(1234)\n    mock_droplet.destroy.assert_called_once()\n    assert result == True\n\n\n@patch('cloudproxy.providers.digitalocean.functions.get_manager')\ndef test_list_droplets_with_instance_config_default(mock_get_manager, instance_specific_droplets):\n    \"\"\"Test listing droplets using the default instance configuration.\"\"\"\n    # Setup mock\n    mock_manager = MagicMock()\n    mock_get_manager.return_value = mock_manager\n    \n    # First call returns droplets with instance tag\n    # Create a copy of the droplets with cloudproxy-default tag to return from the first call\n    default_droplets = [d for d in instance_specific_droplets if \"cloudproxy-default\" in d.tags]\n    \n    # Second call returns all cloudproxy droplets for filtering old ones\n    all_droplets = instance_specific_droplets.copy()\n    \n    # Configure get_all_droplets mock to return different values on each call\n    mock_manager.get_all_droplets.side_effect = [default_droplets, all_droplets]\n    \n    # Call function under test\n    result = list_droplets()  # Default instance\n    \n    # Verify\n    assert mock_manager.get_all_droplets.call_count == 2\n    \n    # Check that the first call used the right tag_name\n    assert mock_manager.get_all_droplets.call_args_list[0][1]['tag_name'] == 'cloudproxy-default'\n    \n    # Check that the second call used the right tag_name \n    assert mock_manager.get_all_droplets.call_args_list[1][1]['tag_name'] == 'cloudproxy'\n    \n    # Match droplet ids instead of just comparing length\n    old_droplets = [d for d in all_droplets if len(d.tags) == 1 and \"cloudproxy\" in d.tags]\n    \n    expected_ids = set()\n    for droplet in default_droplets:\n        expected_ids.add(droplet.id)\n    for droplet in old_droplets:\n        expected_ids.add(droplet.id)\n        \n    result_ids = set(droplet.id for droplet in result)\n    assert result_ids == expected_ids\n\n\n@patch('cloudproxy.providers.digitalocean.functions.get_manager')\ndef test_list_droplets_with_instance_config_specific(mock_get_manager, instance_specific_droplets, test_instance_config):\n    \"\"\"Test listing droplets using a specific instance configuration.\"\"\"\n    # Setup mock\n    mock_manager = MagicMock()\n    mock_get_manager.return_value = mock_manager\n    \n    # Setup specific instance droplets\n    useast_droplets = [d for d in instance_specific_droplets if 'cloudproxy-useast' in d.tags]\n    mock_manager.get_all_droplets.return_value = useast_droplets\n    \n    # Save the original config to restore later\n    original_config = settings.config[\"providers\"][\"digitalocean\"][\"instances\"].copy()\n    \n    # Add our test instance config\n    settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"useast\"] = test_instance_config\n    \n    try:\n        # Call function under test\n        result = list_droplets(test_instance_config)\n        \n        # Verify\n        mock_manager.get_all_droplets.assert_called_once_with(tag_name='cloudproxy-useast')\n        \n        # Should include only the specific instance's droplets\n        assert len(result) == len(useast_droplets)\n        result_ids = [d.id for d in result]\n        \n        # Check for useast instance droplets\n        assert 2001 in result_ids\n        assert 2002 in result_ids\n        \n        # Should NOT include other instances' droplets\n        assert 1001 not in result_ids\n        assert 1002 not in result_ids\n        assert 3001 not in result_ids\n    finally:\n        # Restore original config\n        settings.config[\"providers\"][\"digitalocean\"][\"instances\"] = original_config\n"}
{"type": "test_file", "path": "tests/test_providers_hetzner_functions.py", "content": "import pytest\nfrom unittest.mock import MagicMock, patch, Mock\nfrom cloudproxy.providers.hetzner.functions import (\n    get_client, create_proxy, delete_proxy, list_proxies\n)\nfrom cloudproxy.providers import settings\n\n\n@pytest.fixture\ndef mock_server():\n    \"\"\"Create a mock server instance.\"\"\"\n    server = Mock()\n    server.id = \"server-id-1\"\n    server.name = \"cloudproxy-default-uuid1\"\n    server.labels = {\"type\": \"cloudproxy\", \"instance\": \"default\"}\n    return server\n\n\n@pytest.fixture\ndef mock_servers():\n    \"\"\"Create a list of mock servers.\"\"\"\n    servers = []\n    \n    # Default instance servers\n    server1 = Mock()\n    server1.id = \"server-id-1\"\n    server1.name = \"cloudproxy-default-uuid1\"\n    server1.labels = {\"type\": \"cloudproxy\", \"instance\": \"default\"}\n    servers.append(server1)\n    \n    server2 = Mock()\n    server2.id = \"server-id-2\"\n    server2.name = \"cloudproxy-default-uuid2\"\n    server2.labels = {\"type\": \"cloudproxy\", \"instance\": \"default\"}\n    servers.append(server2)\n    \n    # Custom instance servers\n    server3 = Mock()\n    server3.id = \"server-id-3\"\n    server3.name = \"cloudproxy-europe-uuid3\"\n    server3.labels = {\"type\": \"cloudproxy\", \"instance\": \"europe\"}\n    servers.append(server3)\n    \n    server4 = Mock()\n    server4.id = \"server-id-4\"\n    server4.name = \"cloudproxy-europe-uuid4\"\n    server4.labels = {\"type\": \"cloudproxy\", \"instance\": \"europe\"}\n    servers.append(server4)\n    \n    # Old-style server without instance label\n    server5 = Mock()\n    server5.id = \"server-id-5\"\n    server5.name = \"cloudproxy-uuid5\"\n    server5.labels = {\"type\": \"cloudproxy\"}\n    servers.append(server5)\n    \n    return servers\n\n\n@pytest.fixture\ndef test_instance_config():\n    \"\"\"Fixture for a test instance configuration.\"\"\"\n    return {\n        \"enabled\": True,\n        \"secrets\": {\n            \"access_token\": \"europe-test-token\"\n        },\n        \"size\": \"cx11\",\n        \"location\": \"hel1\",\n        \"min_scaling\": 2,\n        \"max_scaling\": 5,\n        \"display_name\": \"Europe Hetzner\"\n    }\n\n\n@patch('cloudproxy.providers.hetzner.functions.Client')\ndef test_get_client_default(mock_client):\n    \"\"\"Test get_client with default configuration.\"\"\"\n    # Setup\n    mock_client_instance = MagicMock()\n    mock_client.return_value = mock_client_instance\n    \n    # Execute\n    result = get_client()\n    \n    # Verify\n    mock_client.assert_called_once_with(\n        token=settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"secrets\"][\"access_token\"]\n    )\n    assert result == mock_client_instance\n\n\n@patch('cloudproxy.providers.hetzner.functions.Client')\ndef test_get_client_with_instance_config(mock_client, test_instance_config):\n    \"\"\"Test get_client with a specific instance configuration.\"\"\"\n    # Setup\n    mock_client_instance = MagicMock()\n    mock_client.return_value = mock_client_instance\n    \n    # Execute\n    result = get_client(test_instance_config)\n    \n    # Verify\n    mock_client.assert_called_once_with(token=\"europe-test-token\")\n    assert result == mock_client_instance\n\n\n@patch('cloudproxy.providers.hetzner.functions.get_client')\n@patch('cloudproxy.providers.hetzner.functions.set_auth')\n@patch('cloudproxy.providers.hetzner.functions.uuid')\ndef test_create_proxy_default(mock_uuid, mock_set_auth, mock_get_client):\n    \"\"\"Test create_proxy with default configuration.\"\"\"\n    # Setup\n    mock_uuid.uuid4.return_value = \"test-uuid\"\n    mock_set_auth.return_value = \"user-data-script\"\n    \n    mock_client = MagicMock()\n    mock_get_client.return_value = mock_client\n    \n    mock_response = MagicMock()\n    mock_client.servers.create.return_value = mock_response\n    \n    # Execute\n    result = create_proxy()\n    \n    # Verify\n    assert mock_get_client.call_count == 1\n    mock_client.servers.create.assert_called_once()\n    assert \"cloudproxy-default-test-uuid\" in mock_client.servers.create.call_args[1][\"name\"]\n    assert result == mock_response\n\n\n@patch('cloudproxy.providers.hetzner.functions.get_client')\n@patch('cloudproxy.providers.hetzner.functions.set_auth')\n@patch('cloudproxy.providers.hetzner.functions.uuid')\ndef test_create_proxy_with_instance_config(mock_uuid, mock_set_auth, mock_get_client, test_instance_config):\n    \"\"\"Test create_proxy with a specific instance configuration.\"\"\"\n    # Setup\n    mock_uuid.uuid4.return_value = \"test-uuid\"\n    mock_set_auth.return_value = \"user-data-script\"\n    \n    mock_client = MagicMock()\n    mock_get_client.return_value = mock_client\n    \n    mock_response = MagicMock()\n    mock_client.servers.create.return_value = mock_response\n    \n    # Save the original config to restore later\n    original_config = settings.config[\"providers\"][\"hetzner\"][\"instances\"].copy()\n    \n    # Add our test instance config\n    settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"europe\"] = test_instance_config\n    \n    try:\n        # Execute\n        result = create_proxy(test_instance_config)\n        \n        # Verify\n        mock_get_client.assert_called_once_with(test_instance_config)\n        mock_client.servers.create.assert_called_once()\n        \n        # Check server name format and configuration\n        args, kwargs = mock_client.servers.create.call_args\n        assert kwargs['name'] == \"cloudproxy-europe-test-uuid\"\n        assert kwargs['labels'] == {\"type\": \"cloudproxy\", \"instance\": \"europe\"}\n        \n        # Check location is used from instance config\n        assert kwargs['location'] is not None\n        assert kwargs['location'].name == \"hel1\"\n        \n        assert result == mock_response\n    finally:\n        # Restore original config\n        settings.config[\"providers\"][\"hetzner\"][\"instances\"] = original_config\n\n\n@patch('cloudproxy.providers.hetzner.functions.get_client')\ndef test_delete_proxy_default(mock_get_client, mock_server):\n    \"\"\"Test delete_proxy with default configuration.\"\"\"\n    # Setup\n    mock_client = MagicMock()\n    mock_get_client.return_value = mock_client\n    \n    mock_servers = MagicMock()\n    mock_client.servers = mock_servers\n    \n    mock_servers.get_by_id.return_value = mock_server\n    mock_server.delete.return_value = \"delete-response\"\n    \n    # Execute\n    result = delete_proxy(\"server-id-1\")\n    \n    # Verify\n    mock_get_client.assert_called_once()\n    mock_servers.get_by_id.assert_called_once_with(\"server-id-1\")\n    mock_server.delete.assert_called_once()\n    assert result == \"delete-response\"\n\n\n@patch('cloudproxy.providers.hetzner.functions.get_client')\ndef test_delete_proxy_with_instance_config(mock_get_client, mock_server, test_instance_config):\n    \"\"\"Test delete_proxy with a specific instance configuration.\"\"\"\n    # Setup\n    mock_client = MagicMock()\n    mock_get_client.return_value = mock_client\n    \n    mock_servers = MagicMock()\n    mock_client.servers = mock_servers\n    \n    mock_servers.get_by_id.return_value = mock_server\n    mock_server.delete.return_value = \"delete-response\"\n    \n    # Execute\n    result = delete_proxy(\"server-id-1\", test_instance_config)\n    \n    # Verify\n    mock_get_client.assert_called_once_with(test_instance_config)\n    mock_servers.get_by_id.assert_called_once_with(\"server-id-1\")\n    mock_server.delete.assert_called_once()\n    assert result == \"delete-response\"\n\n\n@patch('cloudproxy.providers.hetzner.functions.get_client')\ndef test_list_proxies_default(mock_get_client, mock_servers):\n    \"\"\"Test list_proxies with default configuration.\"\"\"\n    # Setup\n    mock_client = MagicMock()\n    mock_get_client.return_value = mock_client\n    \n    # Filter mock servers for default instance first call\n    default_servers = [s for s in mock_servers if \"instance\" in s.labels and s.labels[\"instance\"] == \"default\"]\n    # All servers with type=cloudproxy for second call\n    type_servers = [s for s in mock_servers if \"type\" in s.labels and s.labels[\"type\"] == \"cloudproxy\"]\n    # Old-style servers for filtering\n    old_servers = [s for s in type_servers if \"instance\" not in s.labels]\n    \n    mock_client.servers.get_all.side_effect = [default_servers, type_servers]\n    \n    # Execute\n    result = list_proxies()\n    \n    # Verify\n    assert mock_get_client.call_count == 1\n    assert mock_client.servers.get_all.call_count == 2\n    \n    # First call should filter by label_selector for type=cloudproxy\n    first_call_args = mock_client.servers.get_all.call_args_list[0]\n    assert first_call_args[1][\"label_selector\"] == \"type=cloudproxy\"\n    \n    # Second call should also filter by type=cloudproxy\n    second_call_args = mock_client.servers.get_all.call_args_list[1]\n    assert second_call_args[1][\"label_selector\"] == \"type=cloudproxy\"\n    \n    # Check the result itself instead of just the length\n    default_and_old_servers = set()\n    for server in default_servers:\n        default_and_old_servers.add(server.id)\n    for server in old_servers:\n        default_and_old_servers.add(server.id)\n    \n    result_ids = set(proxy.id for proxy in result)\n    assert result_ids == default_and_old_servers\n\n\n@patch('cloudproxy.providers.hetzner.functions.get_client')\ndef test_list_proxies_with_instance_config(mock_get_client, mock_servers, test_instance_config):\n    \"\"\"Test list_proxies with a specific instance configuration.\"\"\"\n    # Setup\n    mock_client = MagicMock()\n    mock_get_client.return_value = mock_client\n    \n    # Filter mock servers for europe instance\n    europe_servers = [s for s in mock_servers if \"instance\" in s.labels and s.labels[\"instance\"] == \"europe\"]\n    mock_client.servers.get_all.return_value = europe_servers\n    \n    # Save the original config to restore later\n    original_config = settings.config[\"providers\"][\"hetzner\"][\"instances\"].copy()\n    \n    # Add our test instance config\n    settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"europe\"] = test_instance_config\n    \n    try:\n        # Execute\n        result = list_proxies(test_instance_config)\n        \n        # Verify\n        mock_get_client.assert_called_once_with(test_instance_config)\n        mock_client.servers.get_all.assert_called_once_with(label_selector=\"type=cloudproxy,instance=europe\")\n        \n        # Result should only include europe instance servers\n        assert len(result) == len(europe_servers)\n        \n        # Check specific server IDs\n        result_ids = [s.id for s in result]\n        # Europe instance servers\n        assert \"server-id-3\" in result_ids\n        assert \"server-id-4\" in result_ids\n        # Default instance and old servers should NOT be included\n        assert \"server-id-1\" not in result_ids\n        assert \"server-id-2\" not in result_ids\n        assert \"server-id-5\" not in result_ids\n    finally:\n        # Restore original config\n        settings.config[\"providers\"][\"hetzner\"][\"instances\"] = original_config "}
{"type": "test_file", "path": "tests/test_providers_manager_functions.py", "content": "import pytest\nfrom unittest.mock import patch, Mock\n\nfrom cloudproxy.providers import settings\nfrom cloudproxy.providers.manager import (\n    do_manager,\n    aws_manager,\n    gcp_manager,\n    hetzner_manager\n)\n\n\n# Fixture to save and restore settings\n@pytest.fixture\ndef setup_manager_test():\n    \"\"\"Fixture to save and restore provider IP lists\"\"\"\n    # Save original values\n    original_providers = settings.config[\"providers\"].copy()\n    \n    yield\n    \n    # Restore original values\n    settings.config[\"providers\"] = original_providers\n\n@pytest.fixture\ndef test_instance_config():\n    \"\"\"Test instance configuration for multiple providers\"\"\"\n    return {\n        \"aws\": {\n            \"enabled\": True,\n            \"ips\": [],\n            \"scaling\": {\"min_scaling\": 2, \"max_scaling\": 5},\n            \"size\": \"t3.micro\",\n            \"region\": \"us-west-2\",\n            \"display_name\": \"Test AWS\",\n            \"secrets\": {\n                \"access_key_id\": \"test-key\",\n                \"secret_access_key\": \"test-secret\"\n            }\n        },\n        \"digitalocean\": {\n            \"enabled\": True,\n            \"ips\": [],\n            \"scaling\": {\"min_scaling\": 1, \"max_scaling\": 3},\n            \"size\": \"s-2vcpu-2gb\",\n            \"region\": \"sfo2\",\n            \"display_name\": \"Test DO\",\n            \"secrets\": {\n                \"access_token\": \"test-token\"\n            }\n        },\n        \"gcp\": {\n            \"enabled\": True,\n            \"ips\": [],\n            \"scaling\": {\"min_scaling\": 1, \"max_scaling\": 2},\n            \"size\": \"e2-medium\",\n            \"zone\": \"us-west1-a\",\n            \"display_name\": \"Test GCP\",\n            \"secrets\": {\n                \"service_account_key\": \"test-key\"\n            }\n        },\n        \"hetzner\": {\n            \"enabled\": True,\n            \"ips\": [],\n            \"scaling\": {\"min_scaling\": 2, \"max_scaling\": 4},\n            \"size\": \"cx21\",\n            \"location\": \"fsn1\",\n            \"display_name\": \"Test Hetzner\",\n            \"secrets\": {\n                \"access_token\": \"test-token\"\n            }\n        }\n    }\n\n\n# Test DigitalOcean manager\n@patch('cloudproxy.providers.manager.do_start')\ndef test_do_manager(mock_do_start, setup_manager_test):\n    \"\"\"Test DigitalOcean manager function\"\"\"\n    # Setup - mock what do_start returns\n    expected_ips = [\"192.168.1.1\", \"192.168.1.2\"]\n    mock_do_start.return_value = expected_ips.copy()\n    \n    # Execute\n    result = do_manager()\n    \n    # Verify\n    assert result == expected_ips\n    assert settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"ips\"] == expected_ips\n    mock_do_start.assert_called_once()\n    \n    # Check that do_start was called with the default instance config\n    default_config = settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"]\n    mock_do_start.assert_called_once_with(default_config)\n\n@patch('cloudproxy.providers.manager.do_start')\ndef test_do_manager_custom_instance(mock_do_start, setup_manager_test, test_instance_config):\n    \"\"\"Test DigitalOcean manager function with custom instance name\"\"\"\n    # Setup - mock what do_start returns\n    expected_ips = [\"192.168.1.1\", \"192.168.1.2\"]\n    mock_do_start.return_value = expected_ips.copy()\n    \n    # Setup test instance in the config\n    settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"test\"] = test_instance_config[\"digitalocean\"]\n    \n    # Execute with custom instance name\n    result = do_manager(\"test\")\n    \n    # Verify\n    assert result == expected_ips\n    assert settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"test\"][\"ips\"] == expected_ips\n    \n    # Check that do_start was called with the test instance config\n    test_config = settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"test\"]\n    mock_do_start.assert_called_once_with(test_config)\n\n\n# Test AWS manager\n@patch('cloudproxy.providers.manager.aws_start')\ndef test_aws_manager(mock_aws_start, setup_manager_test):\n    \"\"\"Test AWS manager function\"\"\"\n    # Setup - mock what aws_start returns\n    expected_ips = [\"10.0.0.1\", \"10.0.0.2\"]\n    mock_aws_start.return_value = expected_ips.copy()\n    \n    # Execute\n    result = aws_manager()\n    \n    # Verify\n    assert result == expected_ips\n    assert settings.config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"ips\"] == expected_ips\n    \n    # Check that aws_start was called with the default instance config\n    default_config = settings.config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n    mock_aws_start.assert_called_once_with(default_config)\n\n@patch('cloudproxy.providers.manager.aws_start')\ndef test_aws_manager_custom_instance(mock_aws_start, setup_manager_test, test_instance_config):\n    \"\"\"Test AWS manager function with custom instance name\"\"\"\n    # Setup - mock what aws_start returns\n    expected_ips = [\"10.0.0.1\", \"10.0.0.2\"]\n    mock_aws_start.return_value = expected_ips.copy()\n    \n    # Setup test instance in the config\n    settings.config[\"providers\"][\"aws\"][\"instances\"][\"production\"] = test_instance_config[\"aws\"]\n    \n    # Execute with custom instance name\n    result = aws_manager(\"production\")\n    \n    # Verify\n    assert result == expected_ips\n    assert settings.config[\"providers\"][\"aws\"][\"instances\"][\"production\"][\"ips\"] == expected_ips\n    \n    # Check that aws_start was called with the test instance config\n    test_config = settings.config[\"providers\"][\"aws\"][\"instances\"][\"production\"]\n    mock_aws_start.assert_called_once_with(test_config)\n\n\n# Test GCP manager\n@patch('cloudproxy.providers.manager.gcp_start')\ndef test_gcp_manager(mock_gcp_start, setup_manager_test):\n    \"\"\"Test GCP manager function\"\"\"\n    # Setup - mock what gcp_start returns\n    expected_ips = [\"172.16.0.1\", \"172.16.0.2\"]\n    mock_gcp_start.return_value = expected_ips.copy()\n    \n    # Execute\n    result = gcp_manager()\n    \n    # Verify\n    assert result == expected_ips\n    assert settings.config[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"ips\"] == expected_ips\n    \n    # Check that gcp_start was called with the default instance config\n    default_config = settings.config[\"providers\"][\"gcp\"][\"instances\"][\"default\"]\n    mock_gcp_start.assert_called_once_with(default_config)\n\n@patch('cloudproxy.providers.manager.gcp_start')\ndef test_gcp_manager_custom_instance(mock_gcp_start, setup_manager_test, test_instance_config):\n    \"\"\"Test GCP manager function with custom instance name\"\"\"\n    # Setup - mock what gcp_start returns\n    expected_ips = [\"172.16.0.1\", \"172.16.0.2\"]\n    mock_gcp_start.return_value = expected_ips.copy()\n    \n    # Setup test instance in the config\n    settings.config[\"providers\"][\"gcp\"][\"instances\"][\"dev\"] = test_instance_config[\"gcp\"]\n    \n    # Execute with custom instance name\n    result = gcp_manager(\"dev\")\n    \n    # Verify\n    assert result == expected_ips\n    assert settings.config[\"providers\"][\"gcp\"][\"instances\"][\"dev\"][\"ips\"] == expected_ips\n    \n    # Check that gcp_start was called with the test instance config\n    test_config = settings.config[\"providers\"][\"gcp\"][\"instances\"][\"dev\"]\n    mock_gcp_start.assert_called_once_with(test_config)\n\n\n# Test Hetzner manager\n@patch('cloudproxy.providers.manager.hetzner_start')\ndef test_hetzner_manager(mock_hetzner_start, setup_manager_test):\n    \"\"\"Test Hetzner manager function\"\"\"\n    # Setup - mock what hetzner_start returns\n    expected_ips = [\"1.2.3.4\", \"5.6.7.8\"]\n    mock_hetzner_start.return_value = expected_ips.copy()\n    \n    # Execute\n    result = hetzner_manager()\n    \n    # Verify\n    assert result == expected_ips\n    assert settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"ips\"] == expected_ips\n    \n    # Check that hetzner_start was called with the default instance config\n    default_config = settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"]\n    mock_hetzner_start.assert_called_once_with(default_config)\n\n@patch('cloudproxy.providers.manager.hetzner_start')\ndef test_hetzner_manager_custom_instance(mock_hetzner_start, setup_manager_test, test_instance_config):\n    \"\"\"Test Hetzner manager function with custom instance name\"\"\"\n    # Setup - mock what hetzner_start returns\n    expected_ips = [\"1.2.3.4\", \"5.6.7.8\"]\n    mock_hetzner_start.return_value = expected_ips.copy()\n    \n    # Setup test instance in the config\n    settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"highcpu\"] = test_instance_config[\"hetzner\"]\n    \n    # Execute with custom instance name\n    result = hetzner_manager(\"highcpu\")\n    \n    # Verify\n    assert result == expected_ips\n    assert settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"highcpu\"][\"ips\"] == expected_ips\n    \n    # Check that hetzner_start was called with the test instance config\n    test_config = settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"highcpu\"]\n    mock_hetzner_start.assert_called_once_with(test_config)\n\n\n# Test error handling in managers\n@patch('cloudproxy.providers.manager.do_start')\ndef test_do_manager_empty_response(mock_do_start, setup_manager_test):\n    \"\"\"Test DigitalOcean manager with empty response\"\"\"\n    # Setup - mock an empty response\n    mock_do_start.return_value = []\n    \n    # Set initial IPs\n    settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"ips\"] = [\"old.ip.address\"]\n    \n    # Execute\n    result = do_manager()\n    \n    # Verify\n    assert result == []\n    assert settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"ips\"] == []\n\n\n@patch('cloudproxy.providers.manager.aws_start')\ndef test_aws_manager_exception(mock_aws_start, setup_manager_test):\n    \"\"\"Test AWS manager with an exception in aws_start\"\"\"\n    # Setup - mock aws_start to raise an exception\n    mock_aws_start.side_effect = Exception(\"Test exception\")\n    \n    # Set initial IPs\n    original_ips = [\"old.ip.address\"]\n    settings.config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"ips\"] = original_ips.copy()\n    \n    # Execute - this should catch the exception and return an empty list\n    with pytest.raises(Exception):\n        aws_manager()\n        \n@patch('cloudproxy.providers.manager.aws_start')\ndef test_aws_manager_custom_instance_exception(mock_aws_start, setup_manager_test, test_instance_config):\n    \"\"\"Test AWS manager with custom instance and an exception\"\"\"\n    # Setup - mock aws_start to raise an exception\n    mock_aws_start.side_effect = Exception(\"Test exception\")\n    \n    # Setup test instance in the config\n    settings.config[\"providers\"][\"aws\"][\"instances\"][\"prod\"] = test_instance_config[\"aws\"]\n    settings.config[\"providers\"][\"aws\"][\"instances\"][\"prod\"][\"ips\"] = [\"old.prod.ip\"]\n    \n    # Execute - this should catch the exception and return an empty list\n    with pytest.raises(Exception):\n        aws_manager(\"prod\") "}
{"type": "test_file", "path": "tests/test_providers_manager.py", "content": "import pytest\nfrom unittest.mock import patch, Mock\nfrom cloudproxy.providers import settings\nfrom cloudproxy.providers.manager import init_schedule\n\n# Test setup and teardown\n@pytest.fixture\ndef setup_provider_config():\n    \"\"\"Fixture to preserve and restore provider settings\"\"\"\n    # Save original settings\n    original_providers = settings.config[\"providers\"].copy()\n    \n    # Return function to restore settings\n    yield\n    \n    # Restore original settings\n    settings.config[\"providers\"] = original_providers\n\n# Tests for scheduler initialization\n@patch('cloudproxy.providers.manager.BackgroundScheduler')\ndef test_init_schedule_all_enabled(mock_scheduler_class, setup_provider_config):\n    \"\"\"Test scheduler initialization with all providers enabled\"\"\"\n    # Setup\n    mock_scheduler = Mock()\n    mock_scheduler_class.return_value = mock_scheduler\n    \n    # Configure all providers as enabled\n    for provider in [\"digitalocean\", \"aws\", \"gcp\", \"hetzner\"]:\n        settings.config[\"providers\"][provider][\"instances\"][\"default\"][\"enabled\"] = True\n    \n    # Remove the production instance for this test\n    if \"production\" in settings.config[\"providers\"][\"aws\"][\"instances\"]:\n        del settings.config[\"providers\"][\"aws\"][\"instances\"][\"production\"]\n    \n    # Execute\n    init_schedule()\n    \n    # Verify\n    mock_scheduler.start.assert_called_once()\n    assert mock_scheduler.add_job.call_count == 4  # One for each provider\n    \n    # Verify the correct methods were scheduled\n    calls = mock_scheduler.add_job.call_args_list\n    functions = [call[0][0].__name__ for call in calls]\n    \n    # Check that all provider managers were scheduled\n    assert \"do_manager\" in functions\n    assert \"aws_manager\" in functions\n    assert \"gcp_manager\" in functions\n    assert \"hetzner_manager\" in functions\n\n@patch('cloudproxy.providers.manager.BackgroundScheduler')\ndef test_init_schedule_all_disabled(mock_scheduler_class, setup_provider_config):\n    \"\"\"Test scheduler initialization with all providers disabled\"\"\"\n    # Setup\n    mock_scheduler = Mock()\n    mock_scheduler_class.return_value = mock_scheduler\n    \n    # Configure all providers as disabled\n    for provider in [\"digitalocean\", \"aws\", \"gcp\", \"hetzner\"]:\n        settings.config[\"providers\"][provider][\"instances\"][\"default\"][\"enabled\"] = False\n    \n    # Also disable the production instance if it exists\n    if \"production\" in settings.config[\"providers\"][\"aws\"][\"instances\"]:\n        settings.config[\"providers\"][\"aws\"][\"instances\"][\"production\"][\"enabled\"] = False\n    \n    # Execute\n    init_schedule()\n    \n    # Verify\n    mock_scheduler.start.assert_called_once()\n    assert mock_scheduler.add_job.call_count == 0  # No jobs should be added\n\n@patch('cloudproxy.providers.manager.BackgroundScheduler')\ndef test_init_schedule_mixed_providers(mock_scheduler_class, setup_provider_config):\n    \"\"\"Test scheduler initialization with some providers enabled and others disabled\"\"\"\n    # Setup\n    mock_scheduler = Mock()\n    mock_scheduler_class.return_value = mock_scheduler\n    \n    # Configure mix of enabled/disabled providers\n    settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"enabled\"] = True\n    settings.config[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"enabled\"] = False\n    settings.config[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"enabled\"] = True\n    settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"enabled\"] = False\n    \n    # Also disable the production instance if it exists\n    if \"production\" in settings.config[\"providers\"][\"aws\"][\"instances\"]:\n        settings.config[\"providers\"][\"aws\"][\"instances\"][\"production\"][\"enabled\"] = False\n    \n    # Execute\n    init_schedule()\n    \n    # Verify\n    mock_scheduler.start.assert_called_once()\n    assert mock_scheduler.add_job.call_count == 2  # Two jobs should be added\n    \n    # Verify the correct methods were scheduled\n    calls = mock_scheduler.add_job.call_args_list\n    functions = [call[0][0].__name__ for call in calls]\n    assert \"do_manager\" in functions\n    assert \"gcp_manager\" in functions\n\n@patch('cloudproxy.providers.manager.BackgroundScheduler')\ndef test_init_schedule_multiple_instances(mock_scheduler_class, setup_provider_config):\n    \"\"\"Test scheduler initialization with multiple instances of the same provider\"\"\"\n    # Setup\n    mock_scheduler = Mock()\n    mock_scheduler_class.return_value = mock_scheduler\n    \n    # Configure AWS with multiple instances\n    settings.config[\"providers\"][\"aws\"][\"instances\"] = {\n        \"default\": {\n            \"enabled\": True,\n            \"scaling\": {\"min_scaling\": 1, \"max_scaling\": 2},\n            \"size\": \"t2.micro\",\n            \"region\": \"us-east-1\",\n            \"display_name\": \"Default AWS\"\n        },\n        \"production\": {\n            \"enabled\": True,\n            \"scaling\": {\"min_scaling\": 2, \"max_scaling\": 5},\n            \"size\": \"t3.medium\",\n            \"region\": \"us-west-2\",\n            \"display_name\": \"Production AWS\"\n        },\n        \"testing\": {\n            \"enabled\": False,  # Disabled instance\n            \"scaling\": {\"min_scaling\": 1, \"max_scaling\": 1},\n            \"size\": \"t2.nano\",\n            \"region\": \"eu-west-1\",\n            \"display_name\": \"Testing AWS\"\n        }\n    }\n    \n    # Disable other providers for clarity\n    settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"enabled\"] = False\n    settings.config[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"enabled\"] = False\n    settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"enabled\"] = False\n    \n    # Execute\n    init_schedule()\n    \n    # Verify\n    mock_scheduler.start.assert_called_once()\n    assert mock_scheduler.add_job.call_count == 2  # Only the two enabled AWS instances\n    \n    # Verify all calls are to aws_manager but with different instance names\n    calls = mock_scheduler.add_job.call_args_list\n    \n    # Extract the job functions\n    job_functions = [call[0][0] for call in calls]\n    \n    # The scheduler uses a closure, so we can't directly access the instance name\n    # Instead, we'll check that we have the right number of aws_manager calls\n    assert mock_scheduler.add_job.call_count == 2\n    \n    # Check log message calls would indicate both AWS instances were enabled\n    # This is an indirect way to verify the right instances were scheduled\n\n@patch('cloudproxy.providers.manager.BackgroundScheduler')\ndef test_init_schedule_multiple_providers_with_instances(mock_scheduler_class, setup_provider_config):\n    \"\"\"Test scheduler initialization with multiple providers each with multiple instances\"\"\"\n    # Setup\n    mock_scheduler = Mock()\n    mock_scheduler_class.return_value = mock_scheduler\n    \n    # Configure AWS with multiple instances\n    settings.config[\"providers\"][\"aws\"][\"instances\"] = {\n        \"default\": {\n            \"enabled\": True,\n            \"scaling\": {\"min_scaling\": 1, \"max_scaling\": 2},\n            \"size\": \"t2.micro\",\n            \"region\": \"us-east-1\",\n            \"display_name\": \"Default AWS\"\n        },\n        \"production\": {\n            \"enabled\": True,\n            \"scaling\": {\"min_scaling\": 2, \"max_scaling\": 5},\n            \"size\": \"t3.medium\",\n            \"region\": \"us-west-2\",\n            \"display_name\": \"Production AWS\"\n        }\n    }\n    \n    # Configure DigitalOcean with multiple instances\n    settings.config[\"providers\"][\"digitalocean\"][\"instances\"] = {\n        \"default\": {\n            \"enabled\": True,\n            \"scaling\": {\"min_scaling\": 1, \"max_scaling\": 2},\n            \"size\": \"s-1vcpu-1gb\",\n            \"region\": \"nyc1\",\n            \"display_name\": \"Default DO\"\n        },\n        \"backup\": {\n            \"enabled\": True,\n            \"scaling\": {\"min_scaling\": 1, \"max_scaling\": 3},\n            \"size\": \"s-2vcpu-2gb\",\n            \"region\": \"lon1\",\n            \"display_name\": \"Backup DO\"\n        }\n    }\n    \n    # Disable other providers for clarity\n    settings.config[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"enabled\"] = False\n    settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"enabled\"] = False\n    \n    # Execute\n    init_schedule()\n    \n    # Verify\n    mock_scheduler.start.assert_called_once()\n    assert mock_scheduler.add_job.call_count == 4  # 2 AWS + 2 DO instances\n    \n    # Verify all calls are to the correct manager functions\n    calls = mock_scheduler.add_job.call_args_list\n    \n    # Extract all function names from the calls\n    function_names = set()\n    for call in calls:\n        func = call[0][0]\n        # The function is a closure that calls another function\n        # We need to extract the original function name from the closure\n        function_names.add(func.__name__)\n    \n    # There should be closures for both provider types\n    assert len(function_names) > 0 "}
{"type": "source_file", "path": "cloudproxy/__init__.py", "content": "from cloudproxy.providers import manager\n\nmanager.init_schedule()\n"}
{"type": "source_file", "path": "cloudproxy/providers/digitalocean/main.py", "content": "import datetime\nimport itertools\n\nimport dateparser\nfrom loguru import logger\n\nfrom cloudproxy.check import check_alive\nfrom cloudproxy.providers.digitalocean.functions import (\n    create_proxy,\n    list_droplets,\n    delete_proxy,\n    create_firewall,\n    DOFirewallExistsException,\n)\nfrom cloudproxy.providers import settings\nfrom cloudproxy.providers.settings import delete_queue, restart_queue, config\n\n\ndef do_deployment(min_scaling, instance_config=None):\n    \"\"\"\n    Deploy DigitalOcean droplets based on min_scaling requirements.\n    \n    Args:\n        min_scaling: The minimum number of droplets to maintain\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"]\n        \n    # Get instance display name for logging\n    display_name = instance_config.get(\"display_name\", \"default\")\n    \n    total_droplets = len(list_droplets(instance_config))\n    if min_scaling < total_droplets:\n        logger.info(f\"Overprovisioned: DO {display_name} destroying.....\")\n        for droplet in itertools.islice(list_droplets(instance_config), 0, (total_droplets - min_scaling)):\n            delete_proxy(droplet, instance_config)\n            logger.info(f\"Destroyed: DO {display_name} -> {str(droplet.ip_address)}\")\n            \n    if min_scaling - total_droplets < 1:\n        logger.info(f\"Minimum DO {display_name} Droplets met\")\n    else:\n        total_deploy = min_scaling - total_droplets\n        logger.info(f\"Deploying: {str(total_deploy)} DO {display_name} droplets\")\n        for _ in range(total_deploy):\n            create_proxy(instance_config)\n            logger.info(f\"Deployed DO {display_name} droplet\")\n    return len(list_droplets(instance_config))\n\n\ndef do_check_alive(instance_config=None):\n    \"\"\"\n    Check if DigitalOcean droplets are alive and operational.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"]\n        \n    # Get instance display name for logging\n    display_name = instance_config.get(\"display_name\", \"default\")\n    \n    ip_ready = []\n    for droplet in list_droplets(instance_config):\n        try:\n            elapsed = datetime.datetime.now(\n                datetime.timezone.utc\n            ) - dateparser.parse(droplet.created_at)\n            if config[\"age_limit\"] > 0 and elapsed > datetime.timedelta(seconds=config[\"age_limit\"]):\n                delete_proxy(droplet, instance_config)\n                logger.info(\n                    f\"Recycling DO {display_name} droplet, reached age limit -> {str(droplet.ip_address)}\"\n                )\n            elif check_alive(droplet.ip_address):\n                logger.info(f\"Alive: DO {display_name} -> {str(droplet.ip_address)}\")\n                ip_ready.append(droplet.ip_address)\n            else:\n                if elapsed > datetime.timedelta(minutes=10):\n                    delete_proxy(droplet, instance_config)\n                    logger.info(\n                        f\"Destroyed: took too long DO {display_name} -> {str(droplet.ip_address)}\"\n                    )\n                else:\n                    logger.info(f\"Waiting: DO {display_name} -> {str(droplet.ip_address)}\")\n        except TypeError:\n            logger.info(f\"Pending: DO {display_name} allocating\")\n    return ip_ready\n\n\ndef do_check_delete(instance_config=None):\n    \"\"\"\n    Check if any DigitalOcean droplets need to be deleted.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"]\n        \n    # Get instance display name for logging\n    display_name = instance_config.get(\"display_name\", \"default\")\n    \n    # Log current delete queue state\n    if delete_queue:\n        logger.info(f\"Current delete queue contains {len(delete_queue)} IP addresses: {', '.join(delete_queue)}\")\n    \n    droplets = list_droplets(instance_config)\n    if not droplets:\n        logger.info(f\"No DigitalOcean {display_name} droplets found to process for deletion\")\n        return\n        \n    logger.info(f\"Checking {len(droplets)} DigitalOcean {display_name} droplets for deletion\")\n    \n    for droplet in droplets:\n        try:\n            droplet_ip = str(droplet.ip_address)\n            \n            # Check if this droplet's IP is in the delete or restart queue\n            if droplet_ip in delete_queue or droplet_ip in restart_queue:\n                logger.info(f\"Found droplet {droplet.id} with IP {droplet_ip} in deletion queue - deleting now\")\n                \n                # Attempt to delete the droplet\n                delete_result = delete_proxy(droplet, instance_config)\n                \n                if delete_result:\n                    logger.info(f\"Successfully destroyed DigitalOcean {display_name} droplet -> {droplet_ip}\")\n                    \n                    # Remove from queues upon successful deletion\n                    if droplet_ip in delete_queue:\n                        delete_queue.remove(droplet_ip)\n                        logger.info(f\"Removed {droplet_ip} from delete queue\")\n                    if droplet_ip in restart_queue:\n                        restart_queue.remove(droplet_ip)\n                        logger.info(f\"Removed {droplet_ip} from restart queue\")\n                else:\n                    logger.warning(f\"Failed to destroy DigitalOcean {display_name} droplet -> {droplet_ip}\")\n        except Exception as e:\n            logger.error(f\"Error processing droplet for deletion: {e}\")\n            continue\n    \n    # Report on any IPs that remain in the queues but weren't found\n    remaining_delete = [ip for ip in delete_queue if any(ip == str(d.ip_address) for d in droplets)]\n    if remaining_delete:\n        logger.warning(f\"IPs remaining in delete queue that weren't found as droplets: {', '.join(remaining_delete)}\")\n\ndef do_fw(instance_config=None):\n    \"\"\"\n    Create a DigitalOcean firewall for proxy droplets.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"]\n        \n    # Get instance name for logging\n    instance_id = next(\n        (name for name, inst in config[\"providers\"][\"digitalocean\"][\"instances\"].items() \n         if inst == instance_config), \n        \"default\"\n    )\n    \n    try:\n        create_firewall(instance_config)\n        logger.info(f\"Created firewall 'cloudproxy-{instance_id}'\")\n    except DOFirewallExistsException as e:\n        pass\n    except Exception as e:\n        logger.error(e)\n\ndef do_start(instance_config=None):\n    \"\"\"\n    Start the DigitalOcean provider lifecycle.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \n    Returns:\n        list: List of ready IP addresses\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"]\n        \n    do_fw(instance_config)\n    do_check_delete(instance_config)\n    # First check which droplets are alive\n    ip_ready = do_check_alive(instance_config)\n    # Then handle deployment/scaling based on ready droplets\n    do_deployment(instance_config[\"scaling\"][\"min_scaling\"], instance_config)\n    # Final check for alive droplets\n    return do_check_alive(instance_config)\n"}
{"type": "source_file", "path": "cloudproxy/providers/aws/functions.py", "content": "import boto3\nimport os\nimport json\nimport botocore as botocore\nimport botocore.exceptions\n\nfrom cloudproxy.providers.config import set_auth\nfrom cloudproxy.providers.settings import config\n\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n\n# Module-level variables needed for tests\nec2 = None\nec2_client = None\n\ndef reset_clients():\n    \"\"\"\n    Reset the module-level client variables.\n    This is primarily used in tests to ensure a clean state.\n    \"\"\"\n    global ec2, ec2_client\n    ec2 = None\n    ec2_client = None\n\ndef get_clients(instance_config=None):\n    \"\"\"\n    Initialize and return AWS clients based on the provided configuration.\n    \n    Args:\n        instance_config: The specific instance configuration\n        \n    Returns:\n        tuple: (ec2_resource, ec2_client)\n    \"\"\"\n    # Use global variables to allow mocking in tests\n    global ec2, ec2_client\n    \n    # If clients are already set (likely by a test), return them\n    if ec2 is not None and ec2_client is not None:\n        return ec2, ec2_client\n        \n    if instance_config is None:\n        instance_config = config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n    \n    # Get AWS credentials from the instance configuration\n    aws_access_key_id = instance_config[\"secrets\"][\"access_key_id\"]\n    aws_secret_access_key = instance_config[\"secrets\"][\"secret_access_key\"]\n    region_name = instance_config[\"region\"]\n    \n    # Create AWS clients using the instance-specific credentials\n    ec2 = boto3.resource(\n        \"ec2\", \n        region_name=region_name,\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n    )\n    \n    ec2_client = boto3.client(\n        \"ec2\", \n        region_name=region_name,\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n    )\n    \n    return ec2, ec2_client\n\ndef get_tags(instance_config=None):\n    \"\"\"\n    Get tags for AWS resources based on the provided configuration.\n    \n    Args:\n        instance_config: The specific instance configuration\n        \n    Returns:\n        tuple: (tags, tag_specification)\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n    \n    # Use instance name in the tag if available\n    instance_name = instance_config.get(\"display_name\", \"default\")\n    instance_id = next(\n        (name for name, inst in config[\"providers\"][\"aws\"][\"instances\"].items() \n         if inst == instance_config), \n        \"default\"\n    )\n    \n    tags = [\n        {\"Key\": \"cloudproxy\", \"Value\": \"cloudproxy\"},\n        {\"Key\": \"cloudproxy-instance\", \"Value\": instance_id},\n        {\"Key\": \"Name\", \"Value\": f\"CloudProxy-{instance_name}\"}\n    ]\n    \n    tag_specification = [\n        {\"ResourceType\": \"instance\", \"Tags\": tags},\n    ]\n    \n    return tags, tag_specification\n\n\ndef create_proxy(instance_config=None):\n    \"\"\"\n    Create an AWS proxy instance.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n    \n    # Get clients and tags\n    ec2, ec2_client = get_clients(instance_config)\n    tags, tag_specification = get_tags(instance_config)\n    \n    # Find default VPC\n    vpcs = list((ec2.vpcs.filter()))\n    default_vpc = None\n    for vpc in vpcs:\n        response = ec2_client.describe_vpcs(\n            VpcIds=[\n                vpc.id,\n            ],\n        )\n\n        if response[\"Vpcs\"][0][\"IsDefault\"]:\n            default_vpc = response[\"Vpcs\"][0][\"VpcId\"]\n    \n    # Setup user data with appropriate authentication\n    user_data = set_auth(config[\"auth\"][\"username\"], config[\"auth\"][\"password\"])\n    \n    # Create security group if needed\n    try:\n        sg = ec2.create_security_group(\n            Description=f\"SG for CloudProxy {instance_config.get('display_name', 'default')}\",\n            GroupName=f\"cloudproxy-{next((name for name, inst in config['providers']['aws']['instances'].items() if inst == instance_config), 'default')}\",\n            VpcId=default_vpc\n        )\n        sg.authorize_ingress(\n            CidrIp=\"0.0.0.0/0\", IpProtocol=\"tcp\", FromPort=8899, ToPort=8899\n        )\n        sg.authorize_ingress(\n            CidrIp=\"0.0.0.0/0\", IpProtocol=\"tcp\", FromPort=22, ToPort=22\n        )\n    except botocore.exceptions.ClientError:\n        pass\n    \n    # Get security group ID\n    sg_id = ec2_client.describe_security_groups(GroupNames=[f\"cloudproxy-{next((name for name, inst in config['providers']['aws']['instances'].items() if inst == instance_config), 'default')}\"])\n    sg_id = sg_id[\"SecurityGroups\"][0][\"GroupId\"]\n    \n    # Create instance with appropriate spot configuration\n    if instance_config[\"spot\"] == 'persistent':\n        instance = ec2.create_instances(\n            ImageId=instance_config[\"ami\"],\n            MinCount=1,\n            MaxCount=1,\n            InstanceType=instance_config[\"size\"],\n            NetworkInterfaces=[\n                {\"DeviceIndex\": 0, \"AssociatePublicIpAddress\": True, \"Groups\": [sg_id]}\n            ],\n            InstanceMarketOptions={\n                \"MarketType\": \"spot\",\n                \"SpotOptions\": {\n                    \"InstanceInterruptionBehavior\": \"stop\",\n                    \"SpotInstanceType\": \"persistent\"\n                }\n            },\n            TagSpecifications=tag_specification,\n            UserData=user_data,\n        )\n    elif instance_config[\"spot\"] == 'one-time':\n            instance = ec2.create_instances(\n                ImageId=instance_config[\"ami\"],\n                MinCount=1,\n                MaxCount=1,\n                InstanceType=instance_config[\"size\"],\n                NetworkInterfaces=[\n                    {\"DeviceIndex\": 0, \"AssociatePublicIpAddress\": True, \"Groups\": [sg_id]}\n                ],\n                InstanceMarketOptions={\n                    \"MarketType\": \"spot\",\n                    \"SpotOptions\": {\n                        \"InstanceInterruptionBehavior\": \"terminate\",\n                        \"SpotInstanceType\": \"one-time\"\n                    }\n                },\n                TagSpecifications=tag_specification,\n                UserData=user_data,\n            )\n    else:\n        instance = ec2.create_instances(\n            ImageId=instance_config[\"ami\"],\n            MinCount=1,\n            MaxCount=1,\n            InstanceType=instance_config[\"size\"],\n            NetworkInterfaces=[\n                {\"DeviceIndex\": 0, \"AssociatePublicIpAddress\": True, \"Groups\": [sg_id]}\n            ],\n            TagSpecifications=tag_specification,\n            UserData=user_data,\n        )\n    return instance\n\n\ndef delete_proxy(instance_id, instance_config=None):\n    \"\"\"\n    Delete an AWS proxy instance.\n    \n    Args:\n        instance_id: ID of the instance to delete\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n    \n    # Get clients\n    ec2, ec2_client = get_clients(instance_config)\n    \n    ids = [instance_id]\n    deleted = ec2.instances.filter(InstanceIds=ids).terminate()\n    if instance_config[\"spot\"]:\n        associated_spot_instance_requests = ec2_client.describe_spot_instance_requests(\n            Filters=[\n                {\n                    'Name': 'instance-id',\n                    'Values': ids\n                }\n            ]\n        )\n        spot_instance_id_list = []\n        for spot_instance in associated_spot_instance_requests[\"SpotInstanceRequests\"]:\n            spot_instance_id_list.append(spot_instance.get(\"SpotInstanceRequestId\"))\n        if spot_instance_id_list:\n            ec2_client.cancel_spot_instance_requests(SpotInstanceRequestIds=spot_instance_id_list)\n    return deleted\n\n\ndef stop_proxy(instance_id, instance_config=None):\n    \"\"\"\n    Stop an AWS proxy instance.\n    \n    Args:\n        instance_id: ID of the instance to stop\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n    \n    # Get clients\n    ec2, ec2_client = get_clients(instance_config)\n    \n    ids = [instance_id]\n    stopped = ec2.instances.filter(InstanceIds=ids).stop()\n    return stopped\n\n\ndef start_proxy(instance_id, instance_config=None):\n    \"\"\"\n    Start an AWS proxy instance.\n    \n    Args:\n        instance_id: ID of the instance to start\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n    \n    # Get clients\n    ec2, ec2_client = get_clients(instance_config)\n    \n    ids = [instance_id]\n    try:\n        started = ec2.instances.filter(InstanceIds=ids).start()\n    except botocore.exceptions.ClientError as error:\n        if error.response['Error']['Code'] == 'IncorrectSpotRequestState':\n            return None\n        else:\n            raise error\n    return started\n\n\ndef list_instances(instance_config=None):\n    \"\"\"\n    List AWS proxy instances.\n    \n    Args:\n        instance_config: The specific instance configuration\n        \n    Returns:\n        list: List of AWS instances\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n    \n    # Get clients\n    ec2, ec2_client = get_clients(instance_config)\n    \n    # Get instance ID for this configuration\n    instance_id = next(\n        (name for name, inst in config[\"providers\"][\"aws\"][\"instances\"].items() \n         if inst == instance_config), \n        \"default\"\n    )\n    \n    # Filter instances for this specific instance\n    filters = [\n        {\"Name\": \"tag:cloudproxy\", \"Values\": [\"cloudproxy\"]},\n        {\"Name\": \"tag:cloudproxy-instance\", \"Values\": [instance_id]},\n        {\"Name\": \"instance-state-name\", \"Values\": [\"pending\", \"running\", \"stopped\", \"stopping\"]},\n    ]\n    instances = ec2_client.describe_instances(Filters=filters)\n    result = instances[\"Reservations\"]\n    \n    # If this is the default instance, also find instances created before multi-instance support\n    if instance_id == \"default\":\n        # Get all cloudproxy instances without filtering by instance tag\n        base_filters = [\n            {\"Name\": \"tag:cloudproxy\", \"Values\": [\"cloudproxy\"]},\n            {\"Name\": \"instance-state-name\", \"Values\": [\"pending\", \"running\", \"stopped\", \"stopping\"]},\n        ]\n        all_instances = ec2_client.describe_instances(Filters=base_filters)\n        \n        # Track IDs we already have in our result\n        existing_ids = set()\n        for reservation in result:\n            for instance in reservation[\"Instances\"]:\n                existing_ids.add(instance[\"InstanceId\"])\n        \n        # Add instances that don't have the cloudproxy-instance tag at all\n        for reservation in all_instances[\"Reservations\"]:\n            for instance in reservation[\"Instances\"]:\n                # Skip if we already have this instance\n                if instance[\"InstanceId\"] in existing_ids:\n                    continue\n                \n                # Check if this instance has any cloudproxy-instance tag\n                has_instance_tag = False\n                for tag in instance.get(\"Tags\", []):\n                    if tag[\"Key\"] == \"cloudproxy-instance\":\n                        has_instance_tag = True\n                        break\n                \n                # If no instance tag, add this reservation to our results\n                if not has_instance_tag:\n                    result.append(reservation)\n                    existing_ids.add(instance[\"InstanceId\"])\n    \n    return result\n"}
{"type": "source_file", "path": "cloudproxy/check.py", "content": "import requests as requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util import Retry\nfrom cloudproxy.providers import settings\n\n\ndef requests_retry_session(\n    retries=1,\n    backoff_factor=0.3,\n    status_forcelist=(500, 502, 504),\n    session=None,\n):\n    session = session or requests.Session()\n    retry = Retry(\n        total=retries,\n        read=retries,\n        connect=retries,\n        backoff_factor=backoff_factor,\n        status_forcelist=status_forcelist,\n    )\n    adapter = HTTPAdapter(max_retries=retry)\n    session.mount(\"http://\", adapter)\n    session.mount(\"https://\", adapter)\n    return session\n\n\ndef fetch_ip(ip_address):\n    if settings.config[\"no_auth\"]:\n        proxies = {\n            \"http\": \"http://\" + ip_address + \":8899\",\n            \"https\": \"http://\" + ip_address + \":8899\",\n        }\n    else:\n        auth = (\n            settings.config[\"auth\"][\"username\"] + \":\" + settings.config[\"auth\"][\"password\"]\n        )\n\n        proxies = {\n            \"http\": \"http://\" + auth + \"@\" + ip_address + \":8899\",\n            \"https\": \"http://\" + auth + \"@\" + ip_address + \":8899\",\n        }\n    \n    s = requests.Session()\n    s.proxies = proxies\n\n    fetched_ip = requests_retry_session(session=s).get(\n        \"https://api.ipify.org\", proxies=proxies, timeout=10\n    )\n    return fetched_ip.text\n\n\ndef check_alive(ip_address):\n    try:\n        result = requests.get(\"http://ipecho.net/plain\", proxies={'http': \"http://\" + ip_address + \":8899\"}, timeout=10)\n        if result.status_code in (200, 407):\n            return True\n        else:\n            return False\n    except:\n        return False\n"}
{"type": "source_file", "path": "cloudproxy/providers/config.py", "content": "import os\nimport requests\nfrom cloudproxy.providers import settings\n\n\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n\n\ndef set_auth(username, password):\n    \n    with open(os.path.join(__location__, \"user_data.sh\")) as file:\n        filedata = file.read()\n\n    if settings.config[\"no_auth\"]:\n        # Remove auth configuration for tinyproxy\n        filedata = filedata.replace('\\nBasicAuth PROXY_USERNAME PROXY_PASSWORD\\n', '\\n')\n    else:\n        # Replace username and password in tinyproxy config\n        filedata = filedata.replace(\"PROXY_USERNAME\", username)\n        filedata = filedata.replace(\"PROXY_PASSWORD\", password)\n\n    if settings.config[\"only_host_ip\"]:\n        ip_address = requests.get('https://ipecho.net/plain').text.strip()\n        # Update UFW rules\n        filedata = filedata.replace(\"sudo ufw allow 22/tcp\", f\"sudo ufw allow from {ip_address} to any port 22 proto tcp\")\n        filedata = filedata.replace(\"sudo ufw allow 8899/tcp\", f\"sudo ufw allow from {ip_address} to any port 8899 proto tcp\")\n        # Update tinyproxy access rule\n        filedata = filedata.replace(\"Allow 127.0.0.1\", f\"Allow 127.0.0.1\\nAllow {ip_address}\")\n\n    return filedata\n"}
{"type": "source_file", "path": "cloudproxy/providers/__init__.py", "content": ""}
{"type": "source_file", "path": "cloudproxy/providers/aws/main.py", "content": "import datetime\nimport itertools\n\nfrom loguru import logger\n\nfrom cloudproxy.check import check_alive\nfrom cloudproxy.providers.aws.functions import (\n    list_instances,\n    create_proxy,\n    delete_proxy,\n    stop_proxy,\n    start_proxy,\n)\nfrom cloudproxy.providers.settings import delete_queue, restart_queue, config\n\n\ndef aws_deployment(min_scaling, instance_config=None):\n    \"\"\"\n    Deploy AWS instances based on min_scaling requirements.\n    \n    Args:\n        min_scaling: The minimum number of instances to maintain\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n        \n    total_instances = len(list_instances(instance_config))\n    if min_scaling < total_instances:\n        logger.info(f\"Overprovisioned: AWS {instance_config.get('display_name', 'default')} destroying.....\")\n        for instance in itertools.islice(\n            list_instances(instance_config), 0, (total_instances - min_scaling)\n        ):\n            delete_proxy(instance[\"Instances\"][0][\"InstanceId\"], instance_config)\n            try:\n                msg = instance[\"Instances\"][0][\"PublicIpAddress\"]\n            except KeyError:\n                msg = instance[\"Instances\"][0][\"InstanceId\"]\n\n            logger.info(f\"Destroyed: AWS {instance_config.get('display_name', 'default')} -> \" + msg)\n    if min_scaling - total_instances < 1:\n        logger.info(f\"Minimum AWS {instance_config.get('display_name', 'default')} instances met\")\n    else:\n        total_deploy = min_scaling - total_instances\n        logger.info(f\"Deploying: {str(total_deploy)} AWS {instance_config.get('display_name', 'default')} instances\")\n        for _ in range(total_deploy):\n            create_proxy(instance_config)\n            logger.info(f\"Deployed AWS {instance_config.get('display_name', 'default')} instance\")\n    return len(list_instances(instance_config))\n\n\ndef aws_check_alive(instance_config=None):\n    \"\"\"\n    Check if AWS instances are alive and operational.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n        \n    ip_ready = []\n    for instance in list_instances(instance_config):\n        try:\n            elapsed = datetime.datetime.now(\n                datetime.timezone.utc\n            ) - instance[\"Instances\"][0][\"LaunchTime\"]\n            if config[\"age_limit\"] > 0 and elapsed > datetime.timedelta(seconds=config[\"age_limit\"]):\n                delete_proxy(instance[\"Instances\"][0][\"InstanceId\"], instance_config)\n                logger.info(\n                    f\"Recycling AWS {instance_config.get('display_name', 'default')} instance, reached age limit -> \" + instance[\"Instances\"][0][\"PublicIpAddress\"]\n                )\n            elif instance[\"Instances\"][0][\"State\"][\"Name\"] == \"stopped\":\n                logger.info(\n                    f\"Waking up: AWS {instance_config.get('display_name', 'default')} -> Instance \" + instance[\"Instances\"][0][\"InstanceId\"]\n                )\n                started = start_proxy(instance[\"Instances\"][0][\"InstanceId\"], instance_config)\n                if not started:\n                    logger.info(\n                        \"Could not wake up due to IncorrectSpotRequestState, trying again later.\"\n                    )\n            elif instance[\"Instances\"][0][\"State\"][\"Name\"] == \"stopping\":\n                logger.info(\n                    f\"Stopping: AWS {instance_config.get('display_name', 'default')} -> \" + instance[\"Instances\"][0][\"PublicIpAddress\"]\n                )\n            elif instance[\"Instances\"][0][\"State\"][\"Name\"] == \"pending\":\n                logger.info(\n                    f\"Pending: AWS {instance_config.get('display_name', 'default')} -> \" + instance[\"Instances\"][0][\"PublicIpAddress\"]\n                )\n            # Must be \"pending\" if none of the above, check if alive or not.\n            elif check_alive(instance[\"Instances\"][0][\"PublicIpAddress\"]):\n                logger.info(\n                    f\"Alive: AWS {instance_config.get('display_name', 'default')} -> \" + instance[\"Instances\"][0][\"PublicIpAddress\"]\n                )\n                ip_ready.append(instance[\"Instances\"][0][\"PublicIpAddress\"])\n            else:\n                if elapsed > datetime.timedelta(minutes=10):\n                    delete_proxy(instance[\"Instances\"][0][\"InstanceId\"], instance_config)\n                    logger.info(\n                        f\"Destroyed: took too long AWS {instance_config.get('display_name', 'default')} -> \"\n                        + instance[\"Instances\"][0][\"PublicIpAddress\"]\n                    )\n                else:\n                    logger.info(\n                        f\"Waiting: AWS {instance_config.get('display_name', 'default')} -> \" + instance[\"Instances\"][0][\"PublicIpAddress\"]\n                    )\n        except (TypeError, KeyError):\n            logger.info(f\"Pending: AWS {instance_config.get('display_name', 'default')} -> allocating ip\")\n    return ip_ready\n\n\ndef aws_check_delete(instance_config=None):\n    \"\"\"\n    Check if any AWS instances need to be deleted.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n        \n    for instance in list_instances(instance_config):\n        if instance[\"Instances\"][0].get(\"PublicIpAddress\") in delete_queue:\n            delete_proxy(instance[\"Instances\"][0][\"InstanceId\"], instance_config)\n            logger.info(\n                f\"Destroyed: not wanted AWS {instance_config.get('display_name', 'default')} -> \"\n                + instance[\"Instances\"][0][\"PublicIpAddress\"]\n            )\n            delete_queue.remove(instance[\"Instances\"][0][\"PublicIpAddress\"])\n\n\ndef aws_check_stop(instance_config=None):\n    \"\"\"\n    Check if any AWS instances need to be stopped.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n        \n    for instance in list_instances(instance_config):\n        if instance[\"Instances\"][0].get(\"PublicIpAddress\") in restart_queue:\n            stop_proxy(instance[\"Instances\"][0][\"InstanceId\"], instance_config)\n            logger.info(\n                f\"Stopped: getting new IP AWS {instance_config.get('display_name', 'default')} -> \"\n                + instance[\"Instances\"][0][\"PublicIpAddress\"]\n            )\n            restart_queue.remove(instance[\"Instances\"][0][\"PublicIpAddress\"])\n\n\ndef aws_start(instance_config=None):\n    \"\"\"\n    Start the AWS provider lifecycle.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \n    Returns:\n        list: List of ready IP addresses\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"aws\"][\"instances\"][\"default\"]\n        \n    aws_check_delete(instance_config)\n    aws_check_stop(instance_config)\n    aws_deployment(instance_config[\"scaling\"][\"min_scaling\"], instance_config)\n    ip_ready = aws_check_alive(instance_config)\n    return ip_ready\n"}
{"type": "source_file", "path": "cloudproxy/main.py", "content": "import os\nimport random\nimport sys\nimport re\nimport logging\nimport uuid\nfrom datetime import datetime, UTC\nfrom typing import List, Optional, Set, Dict, Any\n\nimport uvicorn\nfrom loguru import logger\nfrom fastapi import FastAPI, HTTPException, Query\nfrom fastapi.responses import JSONResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.openapi.docs import get_swagger_ui_html\nfrom fastapi.openapi.utils import get_openapi\nfrom pydantic import BaseModel, IPvAnyAddress, Field, field_validator\n\nfrom cloudproxy.providers import settings\nfrom cloudproxy.providers.settings import delete_queue, restart_queue\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n\ndef custom_openapi():\n    if app.openapi_schema:\n        return app.openapi_schema\n    openapi_schema = get_openapi(\n        title=\"CloudProxy API\",\n        version=\"1.0.0\",\n        description=\"\"\"\n        CloudProxy API allows you to manage proxy servers across multiple cloud providers.\n        \n        ## Features\n        * Deploy and scale proxies across multiple cloud providers\n        * Automatic proxy rotation\n        * Provider-specific configuration\n        * Health monitoring\n        \n        ## Authentication\n        Basic authentication is used for proxy access. Configure via environment variables:\n        * USERNAME\n        * PASSWORD\n        \"\"\",\n        routes=app.routes,\n    )\n    app.openapi_schema = openapi_schema\n    return app.openapi_schema\n\napp = FastAPI(\n    title=\"CloudProxy\",\n    description=\"Cloud-based Proxy Management API\",\n    version=\"1.0.0\",\n    docs_url=None,\n    redoc_url=None\n)\n\n@app.get(\"/docs\", include_in_schema=False)\nasync def custom_swagger_ui_html():\n    return get_swagger_ui_html(\n        openapi_url=\"/openapi.json\",\n        title=\"CloudProxy API\",\n        swagger_js_url=\"https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui-bundle.js\",\n        swagger_css_url=\"https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui.css\",\n        swagger_favicon_url=\"https://fastapi.tiangolo.com/img/favicon.png\"\n    )\n\n@app.get(\"/openapi.json\", include_in_schema=False)\nasync def get_openapi_endpoint():\n    return custom_openapi()\n\n# Check if UI directories exist before mounting\nui_dir = os.path.join(__location__, \"../cloudproxy-ui/dist\")\ncss_dir = os.path.join(__location__, \"../cloudproxy-ui/dist/css\")\njs_dir = os.path.join(__location__, \"../cloudproxy-ui/dist/js\")\n\nif os.path.exists(ui_dir):\n    app.mount(\"/ui\", StaticFiles(directory=ui_dir, html=True), name=\"static\")\n    if os.path.exists(css_dir):\n        app.mount(\"/css\", StaticFiles(directory=css_dir, html=True), name=\"cssstatic\")\n    if os.path.exists(js_dir):\n        app.mount(\"/js\", StaticFiles(directory=js_dir), name=\"jsstatic\")\nelse:\n    logger.warning(f\"UI directory {ui_dir} does not exist. UI will not be available.\")\n\norigins = [\"*\"]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Configure logging\nlogger.add(\"cloudproxy.log\", rotation=\"20 MB\")\n\nclass InterceptHandler(logging.Handler):\n    def emit(self, record):\n        # Get corresponding Loguru level if it exists\n        try:\n            level = logger.level(record.levelname).name\n        except ValueError:\n            level = record.levelno\n\n        # Find caller from where originated the logged message\n        frame, depth = logging.currentframe(), 2\n        while frame.f_code.co_filename == logging.__file__:\n            frame = frame.f_back\n            depth += 1\n\n        logger.opt(depth=depth, exception=record.exc_info).log(level, record.getMessage())\n\ndef main():\n    # Intercept everything at the root logger\n    logging.root.handlers = [InterceptHandler()]\n    logging.root.setLevel(logging.INFO)\n    \n    # Remove every other logger's handlers and propagate to root logger\n    for name in logging.root.manager.loggerDict.keys():\n        logging.getLogger(name).handlers = []\n        logging.getLogger(name).propagate = True\n    \n    # Start uvicorn with modified logging config\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n\n\n# Pydantic Models for Request/Response\nclass Metadata(BaseModel):\n    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    timestamp: datetime = Field(default_factory=lambda: datetime.now(UTC))\n\nclass ProxyAddress(BaseModel):\n    ip: IPvAnyAddress\n    port: int = 8899\n    auth_enabled: bool = True\n    url: Optional[str] = None\n    provider: Optional[str] = None\n    instance: Optional[str] = None\n    display_name: Optional[str] = None\n\n    @field_validator('url', mode='before')\n    @classmethod\n    def set_url(cls, v, info):\n        values = info.data\n        ip = str(values.get('ip'))\n        port = values.get('port', 8899)\n        if values.get('auth_enabled'):\n            return f\"http://{settings.config['auth']['username']}:{settings.config['auth']['password']}@{ip}:{port}\"\n        return f\"http://{ip}:{port}\"\n\nclass ProxyList(BaseModel):\n    metadata: Metadata = Field(default_factory=Metadata)\n    total: int\n    proxies: List[ProxyAddress]\n\nclass ProxyResponse(BaseModel):\n    metadata: Metadata = Field(default_factory=Metadata)\n    message: str\n    proxy: ProxyAddress\n\nclass ErrorResponse(BaseModel):\n    metadata: Metadata = Field(default_factory=Metadata)\n    error: str\n    detail: str\n\n# Helper function to convert IP string to ProxyAddress\ndef create_proxy_address(ip: str) -> ProxyAddress:\n    return ProxyAddress(\n        ip=ip,\n        auth_enabled=not settings.config[\"no_auth\"]\n    )\n\n# Updated get_ip_list function\ndef get_ip_list() -> List[ProxyAddress]:\n    ip_list = []\n    for provider_name, provider_config in settings.config[\"providers\"].items():\n        # Handle top-level IPs (for backward compatibility)\n        if \"ips\" in provider_config:\n            for ip in provider_config[\"ips\"]:\n                if ip not in delete_queue and ip not in restart_queue:\n                    proxy = create_proxy_address(ip)\n                    proxy.provider = provider_name\n                    proxy.instance = \"default\"  # Assume default instance for top-level IPs\n                    proxy.display_name = provider_config.get(\"display_name\", provider_name)\n                    ip_list.append(proxy)\n                    \n        # Skip providers that don't have an instances field (like azure)\n        if \"instances\" not in provider_config:\n            continue\n            \n        # Process each instance\n        for instance_name, instance_config in provider_config[\"instances\"].items():\n            if \"ips\" in instance_config:\n                for ip in instance_config[\"ips\"]:\n                    if ip not in delete_queue and ip not in restart_queue:\n                        proxy = create_proxy_address(ip)\n                        proxy.provider = provider_name\n                        proxy.instance = instance_name\n                        proxy.display_name = instance_config.get(\"display_name\")\n                        ip_list.append(proxy)\n                        \n    return ip_list\n\n# Updated API endpoints\n@app.get(\"/\", tags=[\"Proxies\"], response_model=ProxyList)\ndef read_root(\n    offset: int = Query(0, ge=0),\n    limit: int = Query(10, ge=1, le=100)\n):\n    \"\"\"\n    Get a list of all available proxy servers.\n    \n    Args:\n        offset: Number of items to skip (pagination)\n        limit: Maximum number of items to return\n        \n    Returns:\n        ProxyList: A paginated list of proxy servers with metadata\n    \"\"\"\n    all_proxies = get_ip_list()\n    return ProxyList(\n        total=len(all_proxies),\n        proxies=all_proxies[offset:offset + limit]\n    )\n\n@app.get(\"/random\", tags=[\"Proxies\"], response_model=ProxyResponse)\ndef read_random():\n    \"\"\"\n    Get a random proxy server from the available pool.\n    \n    Returns:\n        ProxyResponse: A single random proxy with metadata\n        \n    Raises:\n        HTTPException: If no proxies are available\n    \"\"\"\n    proxies = get_ip_list()\n    if not proxies:\n        raise HTTPException(\n            status_code=404,\n            detail=\"No proxies available\"\n        )\n    proxy = random.choice(proxies)\n    return ProxyResponse(\n        message=\"Random proxy retrieved successfully\",\n        proxy=proxy\n    )\n\n@app.get(\"/destroy\", tags=[\"Proxy Management\"], response_model=ProxyList)\ndef remove_proxy_list():\n    \"\"\"\n    Get a list of proxies scheduled for deletion.\n    \n    Returns:\n        ProxyList: A list of proxies queued for deletion with metadata\n    \"\"\"\n    proxies = [create_proxy_address(ip) for ip in delete_queue]\n    return ProxyList(\n        total=len(proxies),\n        proxies=proxies\n    )\n\n@app.delete(\"/destroy\", tags=[\"Proxy Management\"], response_model=ProxyResponse)\nasync def remove_proxy(ip_address: str):\n    \"\"\"\n    Schedule a proxy for deletion.\n    \n    Args:\n        ip_address: The IP address of the proxy to be deleted\n        \n    Returns:\n        ProxyResponse: Confirmation message with proxy details\n        \n    Raises:\n        HTTPException: If the IP address is invalid or not found\n    \"\"\"\n    try:\n        proxy = create_proxy_address(ip_address)\n        delete_queue.add(str(proxy.ip))\n        return ProxyResponse(\n            message=f\"Proxy scheduled for deletion\",\n            proxy=proxy\n        )\n    except ValueError:\n        raise HTTPException(\n            status_code=422,\n            detail=\"Invalid IP address format\"\n        )\n\n@app.get(\"/restart\", tags=[\"Proxy Management\"], response_model=ProxyList)\ndef restart_proxy_list():\n    \"\"\"\n    Get a list of proxies scheduled for restart.\n    \n    Returns:\n        ProxyList: A list of proxies queued for restart with metadata\n    \"\"\"\n    proxies = [create_proxy_address(ip) for ip in restart_queue]\n    return ProxyList(\n        total=len(proxies),\n        proxies=proxies\n    )\n\n@app.delete(\"/restart\", tags=[\"Proxy Management\"], response_model=ProxyResponse)\nasync def restart_proxy(ip_address: str):\n    \"\"\"\n    Schedule a proxy for restart.\n    \n    Args:\n        ip_address: The IP address of the proxy to be restarted\n        \n    Returns:\n        ProxyResponse: Confirmation message with proxy details\n        \n    Raises:\n        HTTPException: If the IP address is invalid or not found\n    \"\"\"\n    try:\n        proxy = create_proxy_address(ip_address)\n        restart_queue.add(str(proxy.ip))\n        return ProxyResponse(\n            message=f\"Proxy scheduled for restart\",\n            proxy=proxy\n        )\n    except ValueError:\n        raise HTTPException(\n            status_code=422,\n            detail=\"Invalid IP address format\"\n        )\n\n# Add new Pydantic models for providers\nclass ProviderScaling(BaseModel):\n    min_scaling: int = Field(ge=0, default=0)\n    max_scaling: int = Field(ge=0, default=0)\n\n# Provider instance model for multi-instance support\nclass ProviderInstance(BaseModel):\n    enabled: bool\n    ips: List[str] = []\n    scaling: ProviderScaling\n    size: str\n    region: Optional[str] = None\n    location: Optional[str] = None\n    datacenter: Optional[str] = None\n    zone: Optional[str] = None\n    image_project: Optional[str] = None\n    image_family: Optional[str] = None\n    ami: Optional[str] = None\n    spot: Optional[bool] = None\n    display_name: Optional[str] = None\n    project: Optional[str] = None\n\nclass BaseProvider(BaseModel):\n    enabled: bool = False\n    ips: List[str] = Field(default_factory=list)\n    region: str = \"\"\n    size: Optional[str] = \"\"\n    image: Optional[str] = \"\"\n    scaling: ProviderScaling = Field(default_factory=lambda: ProviderScaling())\n    instances: Dict[str, ProviderInstance] = Field(default_factory=dict)\n\nclass DigitalOceanProvider(BaseProvider):\n    region: str\n\nclass AWSProvider(BaseProvider):\n    region: Optional[str] = \"\"\n    ami: Optional[str] = \"\"\n    spot: bool = False\n\nclass GCPProvider(BaseProvider):\n    zone: str\n    image_project: str\n    image_family: str\n\nclass HetznerProvider(BaseProvider):\n    location: Optional[str] = None\n    datacenter: Optional[str] = None\n\nclass ProviderList(BaseModel):\n    metadata: Metadata = Field(default_factory=Metadata)\n    providers: Dict[str, BaseProvider]\n\nclass ProviderResponse(BaseModel):\n    metadata: Metadata = Field(default_factory=Metadata)\n    message: str\n    provider: Dict[str, Any]\n    instances: Dict[str, Any] = Field(default_factory=dict)\n\n# Update ProviderInstanceResponse model for instance-specific responses\nclass ProviderInstanceResponse(BaseModel):\n    metadata: Metadata = Field(default_factory=Metadata)\n    message: str\n    provider: str\n    instance: str\n    config: ProviderInstance\n\ndef get_provider_model(provider_name: str, provider_config: Dict) -> BaseProvider:\n    \"\"\"\n    Get the appropriate Pydantic model for a provider based on the provider name.\n    \n    Args:\n        provider_name: The name of the provider\n        provider_config: The provider configuration\n        \n    Returns:\n        A provider model instance\n    \"\"\"\n    # Extract instances separately from the config to handle it correctly\n    instances_dict = {}\n    if \"instances\" in provider_config:\n        for instance_name, instance_config in provider_config[\"instances\"].items():\n            # Ensure the scaling is a dict for the instance\n            if isinstance(instance_config.get(\"scaling\"), ProviderScaling):\n                instance_config[\"scaling\"] = {\n                    \"min_scaling\": instance_config[\"scaling\"].min_scaling,\n                    \"max_scaling\": instance_config[\"scaling\"].max_scaling\n                }\n            instances_dict[instance_name] = ProviderInstance(**instance_config)\n    \n    # Create a shallow copy to avoid modifying the original\n    config_copy = provider_config.copy()\n    \n    # Remove instances from the copy to avoid duplication\n    if \"instances\" in config_copy:\n        del config_copy[\"instances\"]\n    \n    # Ensure the scaling is a dict\n    if isinstance(config_copy.get(\"scaling\"), ProviderScaling):\n        config_copy[\"scaling\"] = {\n            \"min_scaling\": config_copy[\"scaling\"].min_scaling,\n            \"max_scaling\": config_copy[\"scaling\"].max_scaling\n        }\n    \n    # Get the appropriate provider model\n    if provider_name == \"digitalocean\":\n        # For DigitalOcean, ensure region is set (required by model)\n        if \"region\" not in config_copy and \"instances\" in provider_config and \"default\" in provider_config[\"instances\"]:\n            config_copy[\"region\"] = provider_config[\"instances\"][\"default\"].get(\"region\", \"\")\n        provider_model = DigitalOceanProvider(**config_copy, instances=instances_dict)\n    elif provider_name == \"aws\":\n        # For AWS, ensure region and ami are set (required by model)\n        if \"region\" not in config_copy and \"instances\" in provider_config and \"default\" in provider_config[\"instances\"]:\n            config_copy[\"region\"] = provider_config[\"instances\"][\"default\"].get(\"region\", \"\")\n        if \"ami\" not in config_copy and \"instances\" in provider_config and \"default\" in provider_config[\"instances\"]:\n            config_copy[\"ami\"] = provider_config[\"instances\"][\"default\"].get(\"ami\", \"\")\n        provider_model = AWSProvider(**config_copy, instances=instances_dict)\n    elif provider_name == \"gcp\":\n        provider_model = GCPProvider(**config_copy, instances=instances_dict)\n    elif provider_name == \"hetzner\":\n        provider_model = HetznerProvider(**config_copy, instances=instances_dict)\n    else:\n        provider_model = BaseProvider(**config_copy, instances=instances_dict)\n    \n    return provider_model\n\nclass AuthSettings(BaseModel):\n    username: str\n    password: str\n    auth_enabled: bool = True\n\n@app.get(\"/auth\", tags=[\"Authentication\"], response_model=AuthSettings)\ndef get_auth_settings():\n    \"\"\"\n    Get the current authentication settings.\n    \n    Returns:\n        AuthSettings: The current username and password configuration\n    \"\"\"\n    return AuthSettings(\n        username=settings.config[\"auth\"][\"username\"],\n        password=settings.config[\"auth\"][\"password\"],\n        auth_enabled=not settings.config[\"no_auth\"]\n    )\n\nclass ProviderUpdateRequest(BaseModel):\n    min_scaling: int = Field(ge=0, description=\"Minimum number of proxy instances\")\n    max_scaling: int = Field(ge=0, description=\"Maximum number of proxy instances\")\n\n    @field_validator('max_scaling')\n    @classmethod\n    def max_scaling_must_be_greater_than_min(cls, v, info):\n        values = info.data\n        if 'min_scaling' in values and v < values['min_scaling']:\n            raise ValueError('max_scaling must be greater than or equal to min_scaling')\n        return v\n\n@app.get(\"/providers\", tags=[\"Provider Management\"], response_model=ProviderList)\ndef providers():\n    \"\"\"\n    Get configuration and status for all providers.\n    \n    Returns:\n        ProviderList: Configuration and status for all providers, excluding sensitive information.\n    \"\"\"\n    providers_data = {}\n    for name, config in settings.config[\"providers\"].items():\n        provider_config = config.copy()\n        provider_config.pop(\"secrets\", None)\n        providers_data[name] = get_provider_model(name, provider_config)\n    \n    return ProviderList(\n        providers=providers_data\n    )\n\n@app.get(\"/providers/{provider}\", tags=[\"Provider Management\"], response_model=ProviderResponse)\nasync def get_provider(provider: str):\n    \"\"\"\n    Get configuration and status for a provider.\n    \n    Args:\n        provider: The name of the provider (digitalocean, aws, gcp, hetzner, azure)\n        \n    Returns:\n        ProviderResponse: Provider configuration and status with metadata\n        \n    Raises:\n        HTTPException: If the provider is not found\n    \"\"\"\n    if provider not in settings.config[\"providers\"]:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Provider '{provider}' not found\"\n        )\n\n    # Get the provider configuration directly\n    provider_config = settings.config[\"providers\"][provider]\n    \n    # Extract instances for top-level inclusion\n    instances = provider_config.get(\"instances\", {})\n    \n    # Create the exact expected response format without using models\n    provider_response = {\n        \"enabled\": provider_config.get(\"enabled\", False),\n        \"ips\": provider_config.get(\"ips\", []),\n        \"scaling\": {\n            \"min_scaling\": provider_config.get(\"scaling\", {}).get(\"min_scaling\", 0),\n            \"max_scaling\": provider_config.get(\"scaling\", {}).get(\"max_scaling\", 0)\n        },\n        \"size\": provider_config.get(\"size\", \"\"),\n        \"region\": provider_config.get(\"region\", \"\")\n    }\n    \n    # Return the provider configuration\n    return {\n        \"message\": f\"Provider '{provider}' configuration retrieved successfully\",\n        \"metadata\": Metadata().model_dump(),\n        \"provider\": provider_response,\n        \"instances\": instances\n    }\n\n@app.patch(\"/providers/{provider}\", tags=[\"Provider Management\"], response_model=ProviderResponse)\ndef configure(\n    provider: str,\n    update: ProviderUpdateRequest\n):\n    \"\"\"\n    Update scaling configuration for the default instance of a provider.\n    \n    Args:\n        provider: The name of the provider (digitalocean, aws, gcp, hetzner, azure)\n        update: The scaling configuration to update\n        \n    Returns:\n        ProviderResponse: Updated provider configuration with metadata\n        \n    Raises:\n        HTTPException: If the provider is not found\n    \"\"\"\n    if provider not in settings.config[\"providers\"]:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Provider '{provider}' not found\"\n        )\n\n    # If scaling is a dict, update directly\n    if isinstance(settings.config[\"providers\"][provider][\"instances\"][\"default\"][\"scaling\"], dict):\n        settings.config[\"providers\"][provider][\"instances\"][\"default\"][\"scaling\"][\"min_scaling\"] = update.min_scaling\n        settings.config[\"providers\"][provider][\"instances\"][\"default\"][\"scaling\"][\"max_scaling\"] = update.max_scaling\n    else:\n        # Create a new ProviderScaling object\n        settings.config[\"providers\"][provider][\"instances\"][\"default\"][\"scaling\"] = {\n            \"min_scaling\": update.min_scaling,\n            \"max_scaling\": update.max_scaling\n        }\n    \n    # Update top-level scaling for backward compatibility\n    if isinstance(settings.config[\"providers\"][provider][\"scaling\"], dict):\n        settings.config[\"providers\"][provider][\"scaling\"][\"min_scaling\"] = update.min_scaling\n        settings.config[\"providers\"][provider][\"scaling\"][\"max_scaling\"] = update.max_scaling\n    else:\n        settings.config[\"providers\"][provider][\"scaling\"] = {\n            \"min_scaling\": update.min_scaling,\n            \"max_scaling\": update.max_scaling\n        }\n    \n    # Get provider config\n    provider_config = settings.config[\"providers\"][provider]\n    \n    # Extract instances for top-level inclusion\n    instances = provider_config.get(\"instances\", {})\n    \n    # Create the exact expected response format without using models\n    provider_response = {\n        \"enabled\": provider_config.get(\"enabled\", False),\n        \"ips\": provider_config.get(\"ips\", []),\n        \"scaling\": {\n            \"min_scaling\": update.min_scaling,\n            \"max_scaling\": update.max_scaling\n        },\n        \"size\": provider_config.get(\"size\", \"\"),\n        \"region\": provider_config.get(\"region\", \"\")\n    }\n    \n    # Return the response with only the specific fields\n    return {\n        \"metadata\": Metadata().model_dump(),\n        \"message\": f\"Provider '{provider}' scaling configuration updated successfully\",\n        \"provider\": provider_response,\n        \"instances\": instances\n    }\n\n@app.get(\"/providers/{provider}/{instance}\", tags=[\"Provider Management\"], response_model=ProviderInstanceResponse)\ndef get_provider_instance(provider: str, instance: str):\n    \"\"\"\n    Get configuration and status for a specific provider instance.\n    \n    Args:\n        provider: The name of the provider (digitalocean, aws, gcp, hetzner)\n        instance: The name of the instance\n        \n    Returns:\n        ProviderInstanceResponse: Provider instance configuration and status with metadata\n        \n    Raises:\n        HTTPException: If the provider or instance is not found\n    \"\"\"\n    if provider not in settings.config[\"providers\"]:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Provider '{provider}' not found\"\n        )\n    \n    if instance not in settings.config[\"providers\"][provider][\"instances\"]:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Provider '{provider}' instance '{instance}' not found\"\n        )\n\n    instance_config = settings.config[\"providers\"][provider][\"instances\"][instance].copy()\n    instance_config.pop(\"secrets\", None)\n    \n    return ProviderInstanceResponse(\n        message=f\"Provider '{provider}' instance '{instance}' configuration retrieved successfully\",\n        provider=provider,\n        instance=instance,\n        config=ProviderInstance(**instance_config)\n    )\n\n@app.patch(\"/providers/{provider}/{instance}\", tags=[\"Provider Management\"], response_model=ProviderInstanceResponse)\ndef configure_instance(\n    provider: str,\n    instance: str,\n    update: ProviderUpdateRequest\n):\n    \"\"\"\n    Update scaling configuration for a specific instance of a provider.\n    \n    Args:\n        provider: The name of the provider (digitalocean, aws, gcp, hetzner)\n        instance: The name of the instance\n        update: The scaling configuration to update\n        \n    Returns:\n        ProviderInstanceResponse: Updated provider instance configuration with metadata\n        \n    Raises:\n        HTTPException: If the provider or instance is not found\n    \"\"\"\n    if provider not in settings.config[\"providers\"]:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Provider '{provider}' not found\"\n        )\n    \n    if instance not in settings.config[\"providers\"][provider][\"instances\"]:\n        raise HTTPException(\n            status_code=404,\n            detail=f\"Provider '{provider}' instance '{instance}' not found\"\n        )\n    \n    # If scaling is a dict, update directly\n    if isinstance(settings.config[\"providers\"][provider][\"instances\"][instance][\"scaling\"], dict):\n        settings.config[\"providers\"][provider][\"instances\"][instance][\"scaling\"][\"min_scaling\"] = update.min_scaling\n        settings.config[\"providers\"][provider][\"instances\"][instance][\"scaling\"][\"max_scaling\"] = update.max_scaling\n    else:\n        # Create a new scaling dictionary\n        settings.config[\"providers\"][provider][\"instances\"][instance][\"scaling\"] = {\n            \"min_scaling\": update.min_scaling,\n            \"max_scaling\": update.max_scaling\n        }\n    \n    # Update top-level scaling for backward compatibility if this is the default instance\n    if instance == \"default\":\n        if isinstance(settings.config[\"providers\"][provider][\"scaling\"], dict):\n            settings.config[\"providers\"][provider][\"scaling\"][\"min_scaling\"] = update.min_scaling\n            settings.config[\"providers\"][provider][\"scaling\"][\"max_scaling\"] = update.max_scaling\n        else:\n            settings.config[\"providers\"][provider][\"scaling\"] = {\n                \"min_scaling\": update.min_scaling,\n                \"max_scaling\": update.max_scaling\n            }\n    \n    instance_config = settings.config[\"providers\"][provider][\"instances\"][instance].copy()\n    instance_config.pop(\"secrets\", None)\n    \n    return ProviderInstanceResponse(\n        message=f\"Provider '{provider}' instance '{instance}' scaling configuration updated successfully\",\n        provider=provider,\n        instance=instance,\n        config=ProviderInstance(**instance_config)\n    )\n\nif __name__ == \"__main__\":\n    main()\n\n"}
{"type": "source_file", "path": "cloudproxy/providers/digitalocean/functions.py", "content": "import os\n\nimport digitalocean\nimport uuid as uuid\nfrom loguru import logger\n\nfrom cloudproxy.check import check_alive\nfrom cloudproxy.providers import settings\nfrom cloudproxy.providers.config import set_auth\n\n# Initialize manager with default instance configuration\nmanager = digitalocean.Manager(\n    token=settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"secrets\"][\"access_token\"]\n)\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n\n# Get default token\ntoken = settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"secrets\"][\"access_token\"]\n\nclass DOFirewallExistsException(Exception):\n    pass\n\ndef get_manager(instance_config=None):\n    \"\"\"\n    Get a DigitalOcean manager for the specific instance configuration.\n    \n    Args:\n        instance_config: The specific instance configuration\n        \n    Returns:\n        digitalocean.Manager: Manager instance for the configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"]\n    \n    return digitalocean.Manager(token=instance_config[\"secrets\"][\"access_token\"])\n\ndef create_proxy(instance_config=None):\n    \"\"\"\n    Create a DigitalOcean proxy droplet.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"]\n    \n    # Get instance name for tagging\n    instance_id = next(\n        (name for name, inst in settings.config[\"providers\"][\"digitalocean\"][\"instances\"].items() \n         if inst == instance_config), \n        \"default\"\n    )\n    \n    user_data = set_auth(\n        settings.config[\"auth\"][\"username\"], settings.config[\"auth\"][\"password\"]\n    )\n    \n    # Create droplet with instance-specific settings\n    do_manager = get_manager(instance_config)\n    droplet = digitalocean.Droplet(\n        token=instance_config[\"secrets\"][\"access_token\"],\n        name=f\"cloudproxy-{instance_id}-{str(uuid.uuid1())}\",\n        region=instance_config[\"region\"],\n        image=\"ubuntu-20-04-x64\",\n        size_slug=instance_config[\"size\"],\n        backups=False,\n        user_data=user_data,\n        tags=[\"cloudproxy\", f\"cloudproxy-{instance_id}\"],\n    )\n    droplet.create()\n    return True\n\n\ndef delete_proxy(droplet_id, instance_config=None):\n    \"\"\"\n    Delete a DigitalOcean proxy droplet.\n    \n    Args:\n        droplet_id: ID of the droplet to delete\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"]\n    \n    # Use instance-specific token\n    try:\n        # Handle both ID and Droplet object\n        if hasattr(droplet_id, 'id'):\n            # Get a fresh reference to the droplet to ensure we have the latest state\n            do_manager = get_manager(instance_config)\n            try:\n                droplet = do_manager.get_droplet(droplet_id.id)\n                logger.info(f\"Found droplet with ID: {droplet_id.id} - executing deletion\")\n            except Exception as e:\n                # If we can't get the droplet, it might already be deleted\n                if \"not found\" in str(e).lower() or \"404\" in str(e).lower():\n                    logger.info(f\"Droplet with ID {droplet_id.id} not found, considering it already deleted\")\n                    return True\n                raise  # Re-raise other errors\n                \n            # Now delete the droplet\n            deleted = droplet.destroy()\n            logger.info(f\"DigitalOcean deletion API call completed with result: {deleted}\")\n            return deleted\n        else:\n            # It's just an ID, not an object\n            do_manager = get_manager(instance_config)\n            try:\n                droplet = do_manager.get_droplet(droplet_id)\n                logger.info(f\"Found droplet with ID: {droplet_id} - executing deletion\")\n            except Exception as e:\n                # If we can't get the droplet, it might already be deleted\n                if \"not found\" in str(e).lower() or \"404\" in str(e).lower():\n                    logger.info(f\"Droplet with ID {droplet_id} not found, considering it already deleted\")\n                    return True\n                raise  # Re-raise other errors\n                \n            # Now delete the droplet\n            deleted = droplet.destroy()\n            logger.info(f\"DigitalOcean deletion API call completed with result: {deleted}\")\n            return deleted\n    except Exception as e:\n        # Log the error but don't fail if the droplet is already gone\n        if \"not found\" in str(e).lower() or \"404\" in str(e).lower():\n            # Droplet is already gone, consider it successfully deleted\n            logger.info(f\"Droplet deletion exception indicates it's already gone: {str(e)}\")\n            return True\n        else:\n            # Re-raise other errors\n            logger.error(f\"Error during droplet deletion: {str(e)}\")\n            raise\n\n\ndef list_droplets(instance_config=None):\n    \"\"\"\n    List DigitalOcean proxy droplets.\n    \n    Args:\n        instance_config: The specific instance configuration\n        \n    Returns:\n        list: List of droplets\n    \"\"\"\n    if instance_config is None:\n        instance_config = settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"]\n    \n    # Get instance name for tagging\n    instance_id = next(\n        (name for name, inst in settings.config[\"providers\"][\"digitalocean\"][\"instances\"].items() \n         if inst == instance_config), \n        \"default\"\n    )\n    \n    # Get instance-specific droplets by tag\n    do_manager = get_manager(instance_config)\n    \n    # First try with instance-specific tag\n    my_droplets = do_manager.get_all_droplets(tag_name=f\"cloudproxy-{instance_id}\")\n    \n    # If this is the default instance, also get droplets created before multi-instance support\n    if instance_id == \"default\":\n        # Get old droplets with just the cloudproxy tag (created before multi-instance support)\n        old_droplets = do_manager.get_all_droplets(tag_name=\"cloudproxy\")\n        \n        # Only add droplets that don't have the new instance-specific tag\n        if old_droplets:\n            existing_ids = {d.id for d in my_droplets}\n            for droplet in old_droplets:\n                # Check if this droplet has any instance-specific tags\n                has_instance_tag = False\n                for tag in droplet.tags:\n                    if tag.startswith(\"cloudproxy-\") and tag != \"cloudproxy\":\n                        has_instance_tag = True\n                        break\n                \n                # If no instance tag and not already in our list, add it\n                if not has_instance_tag and droplet.id not in existing_ids:\n                    my_droplets.append(droplet)\n    \n    return my_droplets\n\ndef create_firewall(instance_config=None):\n    \"\"\"\n    Create a DigitalOcean firewall for proxy droplets.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = settings.config[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"]\n    \n    # Get instance name for firewall naming\n    instance_id = next(\n        (name for name, inst in settings.config[\"providers\"][\"digitalocean\"][\"instances\"].items() \n         if inst == instance_config), \n        \"default\"\n    )\n    \n    fw = digitalocean.Firewall(\n            token=instance_config[\"secrets\"][\"access_token\"],\n            name=f\"cloudproxy-{instance_id}\",\n            inbound_rules=__create_inbound_fw_rules(),\n            outbound_rules=__create_outbound_fw_rules(),\n            tags=[f\"cloudproxy-{instance_id}\"]\n         )\n    try:\n        fw.create()\n    except digitalocean.DataReadError as dre:\n        if dre.args[0] == 'duplicate name':\n            raise DOFirewallExistsException(f\"Firewall already exists for 'cloudproxy-{instance_id}'\")\n        else:\n            raise\n\ndef __create_inbound_fw_rules():\n    return [\n        digitalocean.InboundRule(\n            protocol=\"tcp\", \n            ports=\"8899\", \n            sources=digitalocean.Sources(addresses=[\n                            \"0.0.0.0/0\",\n                            \"::/0\"]\n                        )\n            )\n    ]\n\ndef __create_outbound_fw_rules():\n    return [\n        digitalocean.OutboundRule(\n            protocol=\"tcp\",\n            ports=\"all\",\n            destinations=digitalocean.Destinations(addresses=[\n                            \"0.0.0.0/0\",\n                            \"::/0\"]\n                        )\n            ),\n        digitalocean.OutboundRule(\n            protocol=\"udp\",\n            ports=\"all\",\n            destinations=digitalocean.Destinations(addresses=[\n                            \"0.0.0.0/0\",\n                            \"::/0\"]\n                        )\n            )\n    ]\n"}
{"type": "source_file", "path": "cloudproxy/providers/digitalocean/__init__.py", "content": ""}
{"type": "source_file", "path": "cloudproxy/providers/manager.py", "content": "from apscheduler.schedulers.background import BackgroundScheduler\nfrom loguru import logger\nfrom cloudproxy.providers import settings\nfrom cloudproxy.providers.aws.main import aws_start\nfrom cloudproxy.providers.gcp.main import gcp_start\nfrom cloudproxy.providers.digitalocean.main import do_start\nfrom cloudproxy.providers.hetzner.main import hetzner_start\n\n\ndef do_manager(instance_name=\"default\"):\n    \"\"\"\n    DigitalOcean manager function for a specific instance.\n    \"\"\"\n    instance_config = settings.config[\"providers\"][\"digitalocean\"][\"instances\"][instance_name]\n    ip_list = do_start(instance_config)\n    settings.config[\"providers\"][\"digitalocean\"][\"instances\"][instance_name][\"ips\"] = [ip for ip in ip_list]\n    return ip_list\n\n\ndef aws_manager(instance_name=\"default\"):\n    \"\"\"\n    AWS manager function for a specific instance.\n    \"\"\"\n    instance_config = settings.config[\"providers\"][\"aws\"][\"instances\"][instance_name]\n    ip_list = aws_start(instance_config)\n    settings.config[\"providers\"][\"aws\"][\"instances\"][instance_name][\"ips\"] = [ip for ip in ip_list]\n    return ip_list\n\n\ndef gcp_manager(instance_name=\"default\"):\n    \"\"\"\n    GCP manager function for a specific instance.\n    \"\"\"\n    instance_config = settings.config[\"providers\"][\"gcp\"][\"instances\"][instance_name]\n    ip_list = gcp_start(instance_config)\n    settings.config[\"providers\"][\"gcp\"][\"instances\"][instance_name][\"ips\"] = [ip for ip in ip_list]\n    return ip_list\n\n\ndef hetzner_manager(instance_name=\"default\"):\n    \"\"\"\n    Hetzner manager function for a specific instance.\n    \"\"\"\n    instance_config = settings.config[\"providers\"][\"hetzner\"][\"instances\"][instance_name]\n    ip_list = hetzner_start(instance_config)\n    settings.config[\"providers\"][\"hetzner\"][\"instances\"][instance_name][\"ips\"] = [ip for ip in ip_list]\n    return ip_list\n\n\ndef init_schedule():\n    sched = BackgroundScheduler()\n    sched.start()\n    \n    # Define provider manager mapping\n    provider_managers = {\n        \"digitalocean\": do_manager,\n        \"aws\": aws_manager,\n        \"gcp\": gcp_manager,\n        \"hetzner\": hetzner_manager,\n    }\n    \n    # Schedule jobs for all provider instances\n    for provider_name, provider_config in settings.config[\"providers\"].items():\n        # Skip providers not in our manager mapping\n        if provider_name not in provider_managers:\n            continue\n            \n        for instance_name, instance_config in provider_config[\"instances\"].items():\n            if instance_config[\"enabled\"]:\n                manager_func = provider_managers.get(provider_name)\n                if manager_func:\n                    # Create a function that preserves the original name\n                    def scheduled_func(func=manager_func, instance=instance_name):\n                        return func(instance)\n                    \n                    # Preserve the original function name for testing\n                    scheduled_func.__name__ = manager_func.__name__\n                    \n                    sched.add_job(scheduled_func, \"interval\", seconds=20)\n                    logger.info(f\"{provider_name.capitalize()} {instance_name} enabled\")\n            else:\n                logger.info(f\"{provider_name.capitalize()} {instance_name} not enabled\")\n"}
{"type": "source_file", "path": "cloudproxy/providers/gcp/functions.py", "content": "import json\nimport uuid\n\nfrom loguru import logger\n\nimport googleapiclient.discovery\nfrom google.oauth2 import service_account\n\nfrom cloudproxy.providers.config import set_auth\nfrom cloudproxy.providers.settings import config\n\ngcp = config[\"providers\"][\"gcp\"]\nif gcp[\"enabled\"] == 'True':\n    try:\n        credentials = service_account.Credentials.from_service_account_info(\n            json.loads(gcp[\"secrets\"][\"service_account_key\"])\n        )\n        compute = googleapiclient.discovery.build('compute', 'v1', credentials=credentials)\n    except TypeError:\n        logger.error(\"GCP -> Invalid service account key\")\n\n\ndef create_proxy():\n    image_response = compute.images().getFromFamily(\n        project=gcp[\"image_project\"], \n        family=gcp[\"image_family\"]\n    ).execute()\n    source_disk_image = image_response['selfLink']\n\n    body = {\n        'name': 'cloudproxy-' + str(uuid.uuid4()),\n        'machineType': \n            f\"zones/{gcp['zone']}/machineTypes/{gcp['size']}\",\n        'tags': {\n            'items': [\n                'cloudproxy'\n            ]\n        },\n        \"labels\": {\n            'cloudproxy': 'cloudproxy'\n        },\n        'disks': [\n            {\n                'boot': True,\n                'autoDelete': True,\n                'initializeParams': {\n                    'sourceImage': source_disk_image,\n                }\n            }\n        ],\n        'networkInterfaces': [{\n            'network': 'global/networks/default',\n            'accessConfigs': [\n                {\n                    'name': 'External NAT', \n                    'type': 'ONE_TO_ONE_NAT',\n                    'networkTier': 'STANDARD'\n                }\n            ]\n        }],\n        'metadata': {\n            'items': [{\n                'key': 'startup-script',\n                'value': set_auth(config[\"auth\"][\"username\"], config[\"auth\"][\"password\"])\n            }]\n        }\n    }\n\n    return compute.instances().insert(\n        project=gcp[\"project\"],\n        zone=gcp[\"zone\"],\n        body=body\n    ).execute()\n\ndef delete_proxy(name):\n    try:\n        return compute.instances().delete(\n            project=gcp[\"project\"],\n            zone=gcp[\"zone\"],\n            instance=name\n        ).execute()\n    except(googleapiclient.errors.HttpError):\n        logger.info(f\"GCP --> HTTP Error when trying to delete proxy {name}. Probably has already been deleted.\")\n        return None\n\ndef stop_proxy(name):\n    try:\n        return compute.instances().stop(\n            project=gcp[\"project\"],\n            zone=gcp[\"zone\"],\n            instance=name\n        ).execute()\n    except(googleapiclient.errors.HttpError):\n        logger.info(f\"GCP --> HTTP Error when trying to stop proxy {name}. Probably has already been deleted.\")\n        return None\n\ndef start_proxy(name):\n    try:\n        return compute.instances().start(\n            project=gcp[\"project\"],\n            zone=gcp[\"zone\"],\n            instance=name\n        ).execute()\n    except(googleapiclient.errors.HttpError):\n        logger.info(f\"GCP --> HTTP Error when trying to start proxy {name}. Probably has already been deleted.\")\n        return None\n\ndef list_instances():\n    result = compute.instances().list(\n        project=gcp[\"project\"], \n        zone=gcp[\"zone\"], \n        filter='labels.cloudproxy eq cloudproxy'\n    ).execute()\n    return result['items'] if 'items' in result else []\n"}
{"type": "source_file", "path": "cloudproxy/providers/settings.py", "content": "import os\nfrom dotenv import load_dotenv\n\nconfig = {\n    \"auth\": {\"username\": \"\", \"password\": \"\"},\n    \"no_auth\": False,\n    \"only_host_ip\": False,\n    \"age_limit\": 0,\n    \"providers\": {\n        \"digitalocean\": {\n            \"instances\": {\n                \"default\": {\n            \"enabled\": False,\n            \"ips\": [],\n            \"scaling\": {\"min_scaling\": 0, \"max_scaling\": 0},\n            \"size\": \"\",\n            \"region\": \"\",\n                    \"display_name\": \"DigitalOcean\",\n            \"secrets\": {\"access_token\": \"\"},\n                }\n            }\n        },\n        \"aws\": {\n            \"instances\": {\n                \"default\": {\n            \"enabled\": False,\n            \"ips\": [],\n            \"scaling\": {\"min_scaling\": 0, \"max_scaling\": 0},\n            \"size\": \"\",\n            \"region\": \"\",\n            \"ami\": \"\",\n                    \"display_name\": \"AWS\",\n            \"secrets\": {\"access_key_id\": \"\", \"secret_access_key\": \"\"},\n            \"spot\": False,\n                }\n            }\n        },\n        \"gcp\": {\n            \"instances\": {\n                \"default\": {\n            \"enabled\": False,\n            \"project\": \"\",\n            \"ips\": [],\n            \"scaling\": {\"min_scaling\": 0, \"max_scaling\": 0},\n            \"size\": \"\",\n            \"zone\": \"\",\n            \"image_project\": \"\",\n            \"image_family\": \"\",\n                    \"display_name\": \"GCP\",\n            \"secrets\": {\"service_account_key\": \"\"},\n                }\n            }\n        },\n        \"hetzner\": {\n            \"instances\": {\n                \"default\": {\n            \"enabled\": False,\n            \"ips\": [],\n            \"scaling\": {\"min_scaling\": 0, \"max_scaling\": 0},\n            \"size\": \"\",\n            \"location\": \"\",\n            \"datacenter\": \"\",\n                    \"display_name\": \"Hetzner\",\n            \"secrets\": {\"access_token\": \"\"},\n                }\n            }\n        },\n        \"azure\": {\n            \"instances\": {\n                \"default\": {\n                    \"enabled\": False,\n                    \"ips\": [],\n                    \"scaling\": {\"min_scaling\": 0, \"max_scaling\": 0},\n                    \"size\": \"\",\n                    \"location\": \"\",\n                    \"display_name\": \"Azure\",\n                    \"secrets\": {\n                        \"subscription_id\": \"\",\n                        \"client_id\": \"\",\n                        \"client_secret\": \"\",\n                        \"tenant_id\": \"\",\n                        \"resource_group\": \"\"\n                    },\n                }\n            }\n        },\n    },\n}\n\ndelete_queue = set()\nrestart_queue = set()\n\nload_dotenv()\n\n# Set proxy authentication\nconfig[\"auth\"][\"username\"] = os.environ.get(\"PROXY_USERNAME\", \"changeme\")\nconfig[\"auth\"][\"password\"] = os.environ.get(\"PROXY_PASSWORD\", \"changeme\")\nconfig[\"age_limit\"] = int(os.environ.get('AGE_LIMIT', 0))\nconfig[\"no_auth\"] = config[\"auth\"][\"username\"] == \"changeme\" and config[\"auth\"][\"password\"] == \"changeme\"\nconfig[\"only_host_ip\"] = os.environ.get(\"ONLY_HOST_IP\", False)\n\n# Set DigitalOcean config - original format for backward compatibility\nconfig[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"enabled\"] = os.environ.get(\n    \"DIGITALOCEAN_ENABLED\", \"False\"\n) == \"True\"\nconfig[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"secrets\"][\"access_token\"] = os.environ.get(\n    \"DIGITALOCEAN_ACCESS_TOKEN\"\n)\nconfig[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"scaling\"][\"min_scaling\"] = int(\n    os.environ.get(\"DIGITALOCEAN_MIN_SCALING\", 2)\n)\nconfig[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"scaling\"][\"max_scaling\"] = int(\n    os.environ.get(\"DIGITALOCEAN_MAX_SCALING\", 2)\n)\nconfig[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"size\"] = os.environ.get(\n    \"DIGITALOCEAN_SIZE\", \"s-1vcpu-1gb\"\n)\nconfig[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"region\"] = os.environ.get(\n    \"DIGITALOCEAN_REGION\", \"lon1\"\n)\nconfig[\"providers\"][\"digitalocean\"][\"instances\"][\"default\"][\"display_name\"] = os.environ.get(\n    \"DIGITALOCEAN_DISPLAY_NAME\", \"DigitalOcean\"\n)\n\n# Set AWS Config - original format for backward compatibility\nconfig[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"enabled\"] = os.environ.get(\"AWS_ENABLED\", \"False\") == \"True\"\nconfig[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"secrets\"][\"access_key_id\"] = os.environ.get(\n    \"AWS_ACCESS_KEY_ID\"\n)\nconfig[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"secrets\"][\"secret_access_key\"] = os.environ.get(\n    \"AWS_SECRET_ACCESS_KEY\"\n)\nconfig[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"scaling\"][\"min_scaling\"] = int(\n    os.environ.get(\"AWS_MIN_SCALING\", 2)\n)\nconfig[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"scaling\"][\"max_scaling\"] = int(\n    os.environ.get(\"AWS_MAX_SCALING\", 2)\n)\nconfig[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"size\"] = os.environ.get(\"AWS_SIZE\", \"t2.micro\")\nconfig[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"region\"] = os.environ.get(\"AWS_REGION\", \"eu-west-2\")\nconfig[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"spot\"] = os.environ.get(\"AWS_SPOT\", \"False\") == \"True\"\nconfig[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"ami\"] = os.environ.get(\"AWS_AMI\", \"ami-096cb92bb3580c759\")\nconfig[\"providers\"][\"aws\"][\"instances\"][\"default\"][\"display_name\"] = os.environ.get(\"AWS_DISPLAY_NAME\", \"AWS\")\n\n# Set GCP Config - original format for backward compatibility\nconfig[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"enabled\"] = os.environ.get(\"GCP_ENABLED\", \"False\") == \"True\"\nconfig[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"project\"] = os.environ.get(\"GCP_PROJECT\")\nconfig[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"secrets\"][\"service_account_key\"] = os.environ.get(\n    \"GCP_SERVICE_ACCOUNT_KEY\"\n)\nconfig[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"scaling\"][\"min_scaling\"] = int(\n    os.environ.get(\"GCP_MIN_SCALING\", 2)\n)\nconfig[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"scaling\"][\"max_scaling\"] = int(\n    os.environ.get(\"GCP_MAX_SCALING\", 2)\n)\nconfig[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"size\"] = os.environ.get(\"GCP_SIZE\", \"f1-micro\")\nconfig[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"zone\"] = os.environ.get(\"GCP_REGION\", \"us-central1-a\")\nconfig[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"image_project\"] = os.environ.get(\"GCP_IMAGE_PROJECT\", \"ubuntu-os-cloud\")\nconfig[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"image_family\"] = os.environ.get(\"GCP_IMAGE_FAMILY\", \"ubuntu-minimal-2004-lts\")\nconfig[\"providers\"][\"gcp\"][\"instances\"][\"default\"][\"display_name\"] = os.environ.get(\"GCP_DISPLAY_NAME\", \"GCP\")\n\n# Set Hetzner config - original format for backward compatibility\nconfig[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"enabled\"] = os.environ.get(\n    \"HETZNER_ENABLED\", \"False\"\n) == \"True\"\nconfig[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"secrets\"][\"access_token\"] = os.environ.get(\n    \"HETZNER_ACCESS_TOKEN\"\n)\nconfig[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"scaling\"][\"min_scaling\"] = int(\n    os.environ.get(\"HETZNER_MIN_SCALING\", 2)\n)\nconfig[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"scaling\"][\"max_scaling\"] = int(\n    os.environ.get(\"HETZNER_MAX_SCALING\", 2)\n)\nconfig[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"size\"] = os.environ.get(\n    \"HETZNER_SIZE\", \"cx21\"\n)\nconfig[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"location\"] = os.environ.get(\n    \"HETZNER_LOCATION\", \"nbg1\"\n)\nconfig[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"display_name\"] = os.environ.get(\n    \"HETZNER_DISPLAY_NAME\", \"Hetzner\"\n)\n\n# Set Azure config - original format for backward compatibility\nconfig[\"providers\"][\"azure\"][\"instances\"][\"default\"][\"enabled\"] = os.environ.get(\n    \"AZURE_ENABLED\", \"False\"\n) == \"True\"\nconfig[\"providers\"][\"azure\"][\"instances\"][\"default\"][\"secrets\"][\"subscription_id\"] = os.environ.get(\n    \"AZURE_SUBSCRIPTION_ID\"\n)\nconfig[\"providers\"][\"azure\"][\"instances\"][\"default\"][\"secrets\"][\"client_id\"] = os.environ.get(\n    \"AZURE_CLIENT_ID\"\n)\nconfig[\"providers\"][\"azure\"][\"instances\"][\"default\"][\"secrets\"][\"client_secret\"] = os.environ.get(\n    \"AZURE_CLIENT_SECRET\"\n)\nconfig[\"providers\"][\"azure\"][\"instances\"][\"default\"][\"secrets\"][\"tenant_id\"] = os.environ.get(\n    \"AZURE_TENANT_ID\"\n)\nconfig[\"providers\"][\"azure\"][\"instances\"][\"default\"][\"secrets\"][\"resource_group\"] = os.environ.get(\n    \"AZURE_RESOURCE_GROUP\", \"cloudproxy-rg\"\n)\nconfig[\"providers\"][\"azure\"][\"instances\"][\"default\"][\"scaling\"][\"min_scaling\"] = int(\n    os.environ.get(\"AZURE_MIN_SCALING\", 2)\n)\nconfig[\"providers\"][\"azure\"][\"instances\"][\"default\"][\"scaling\"][\"max_scaling\"] = int(\n    os.environ.get(\"AZURE_MAX_SCALING\", 2)\n)\nconfig[\"providers\"][\"azure\"][\"instances\"][\"default\"][\"size\"] = os.environ.get(\n    \"AZURE_SIZE\", \"Standard_B1s\"\n)\nconfig[\"providers\"][\"azure\"][\"instances\"][\"default\"][\"location\"] = os.environ.get(\n    \"AZURE_LOCATION\", \"eastus\"\n)\nconfig[\"providers\"][\"azure\"][\"instances\"][\"default\"][\"display_name\"] = os.environ.get(\n    \"AZURE_DISPLAY_NAME\", \"Azure\"\n)\n\n# Check for additional provider instances using the new format pattern\nfor provider_key in config[\"providers\"].keys():\n    provider_upper = provider_key.upper()\n    \n    # Find all environment variables matching the pattern {PROVIDER}_INSTANCE_{NAME}_ENABLED\n    instance_vars = {key: value for key, value in os.environ.items() \n                    if key.startswith(f\"{provider_upper}_INSTANCE_\") and key.endswith(\"_ENABLED\")}\n    \n    for instance_var, enabled_value in instance_vars.items():\n        # Extract instance name from the environment variable key\n        # Format: {PROVIDER}_INSTANCE_{NAME}_ENABLED\n        instance_name = instance_var[len(f\"{provider_upper}_INSTANCE_\"):-8].lower()\n        \n        if enabled_value == \"True\":\n            # Create a new instance configuration\n            if instance_name not in config[\"providers\"][provider_key][\"instances\"]:\n                # Clone the default instance configuration as a starting point\n                config[\"providers\"][provider_key][\"instances\"][instance_name] = {\n                    \"enabled\": True,\n                    \"ips\": [],\n                    \"scaling\": {\"min_scaling\": 0, \"max_scaling\": 0},\n                    \"size\": \"\",\n                    \"display_name\": f\"{provider_key.capitalize()} {instance_name}\",\n                    \"secrets\": {},\n                }\n                \n                # Copy relevant fields from default instance\n                default_instance = config[\"providers\"][provider_key][\"instances\"][\"default\"]\n                for field in [\"region\", \"zone\", \"location\", \"ami\", \"spot\", \"datacenter\", \n                             \"image_project\", \"image_family\", \"project\"]:\n                    if field in default_instance:\n                        config[\"providers\"][provider_key][\"instances\"][instance_name][field] = default_instance[field]\n                \n                # Copy secrets structure (but not values)\n                for secret_key in default_instance[\"secrets\"].keys():\n                    config[\"providers\"][provider_key][\"instances\"][instance_name][\"secrets\"][secret_key] = None\n            \n            # Set instance-specific values from environment variables\n            instance_prefix = f\"{provider_upper}_INSTANCE_{instance_name.upper()}_\"\n            \n            # Process all environment variables for this instance\n            for env_key, env_value in os.environ.items():\n                if env_key.startswith(instance_prefix):\n                    # Extract the setting name\n                    setting_name = env_key[len(instance_prefix):].lower()\n                    \n                    if setting_name == \"min_scaling\":\n                        config[\"providers\"][provider_key][\"instances\"][instance_name][\"scaling\"][\"min_scaling\"] = int(env_value)\n                    elif setting_name == \"max_scaling\":\n                        config[\"providers\"][provider_key][\"instances\"][instance_name][\"scaling\"][\"max_scaling\"] = int(env_value)\n                    elif setting_name == \"display_name\":\n                        config[\"providers\"][provider_key][\"instances\"][instance_name][\"display_name\"] = env_value\n                    elif setting_name in [\"size\", \"region\", \"zone\", \"location\", \"ami\", \"project\", \n                                          \"image_project\", \"image_family\", \"datacenter\"]:\n                        config[\"providers\"][provider_key][\"instances\"][instance_name][setting_name] = env_value\n                    elif setting_name == \"spot\":\n                        config[\"providers\"][provider_key][\"instances\"][instance_name][\"spot\"] = env_value == \"True\"\n                    elif setting_name in default_instance[\"secrets\"]:\n                        # Handle secret values\n                        config[\"providers\"][provider_key][\"instances\"][instance_name][\"secrets\"][setting_name] = env_value\n\n# For backward compatibility - maintain top-level properties for the default instance\nfor provider_key in config[\"providers\"].keys():\n    default_instance = config[\"providers\"][provider_key][\"instances\"][\"default\"]\n    for key, value in default_instance.items():\n        if key != \"secrets\":  # Don't include secrets in top-level\n            config[\"providers\"][provider_key][key] = value\n"}
{"type": "source_file", "path": "cloudproxy/providers/scaleway/functions.py", "content": "import uuid as uuid\nimport json\nfrom cloudproxy.providers import settings\nfrom cloudproxy.providers.config import set_auth\nfrom scaleway.apis import ComputeAPI\nfrom slumber.exceptions import HttpClientError\n\ncompute_api = ComputeAPI(\n    auth_token=settings.config[\"providers\"][\"scaleway\"][\"secrets\"][\"access_token\"]\n)\n\n\ndef create_proxy():\n    user_data = set_auth(\n        settings.config[\"auth\"][\"username\"], settings.config[\"auth\"][\"password\"]\n    )\n    # try:\n    #     res = compute_api.query().images.get()\n    #     image = next(image for image in res[\"images\"] if \"Ubuntu 20.04\" in image[\"name\"])\n    # except HttpClientError as exc:\n    #     print(json.dumps(exc.response.json(), indent=2))\n    # try:\n    #     instance = compute_api.query().servers.post(\n    #         {\n    #             \"project\": settings.config[\"providers\"][\"scaleway\"][\"secrets\"][\n    #                 \"project\"\n    #             ],\n    #             \"name\": str(uuid.uuid1()),\n    #             \"commercial_type\": \"DEV1-M\",\n    #             \"image\": image['id'],\n    #             \"tags\": [\"cloudproxy\"]\n    #         }\n    #     )\n    # except HttpClientError as exc:\n    #     print(json.dumps(exc.response.json(), indent=2))\n    try:\n        data = (\n            compute_api.query()\n            .servers(\"cb2412b2-d983-46be-9ead-9c33777cfdea\")\n            .user_data(\"cloud-init\")\n            .get()\n        )\n        print(data.decode())\n        print(\n            compute_api.query()\n            .servers(\"cb2412b2-d983-46be-9ead-9c33777cfdea\")\n            .user_data(\"cloud-init\")\n            .patch()\n        )\n    except HttpClientError as exc:\n        print(json.dumps(exc.response.json(), indent=2))\n    return True\n\n\n# def delete_proxy(droplet_id):\n#     deleted =\n#     return deleted\n#\n#\n# def list_droplets():\n#     my_droplets =\n#     return my_droplets\n\ncreate_proxy()\n"}
{"type": "source_file", "path": "cloudproxy/providers/gcp/main.py", "content": "import datetime\nimport itertools\n\nfrom loguru import logger\n\nfrom cloudproxy.check import check_alive\nfrom cloudproxy.providers.gcp.functions import (\n    list_instances,\n    create_proxy,\n    delete_proxy,\n    stop_proxy,\n    start_proxy,\n)\nfrom cloudproxy.providers.settings import delete_queue, restart_queue, config\n\ndef gcp_deployment(min_scaling):\n    total_instances = len(list_instances())\n    if min_scaling < total_instances:\n        logger.info(\"Overprovisioned: GCP destroying.....\")\n        for instance in itertools.islice(\n            list_instances(), 0, (total_instances - min_scaling)\n        ):\n            access_configs = instance['networkInterfaces'][0]['accessConfigs'][0]\n            msg = f\"{instance['name']} {access_configs['natIP']}\"\n            delete_proxy(instance['name'])\n            logger.info(\"Destroyed: GCP -> \" + msg)\n    if min_scaling - total_instances < 1:\n        logger.info(\"Minimum GCP instances met\")\n    else:\n        total_deploy = min_scaling - total_instances\n        logger.info(\"Deploying: \" + str(total_deploy) + \" GCP instances\")\n        for _ in range(total_deploy):\n            create_proxy()\n            logger.info(\"Deployed\")\n    return len(list_instances())\n\ndef gcp_check_alive():\n    ip_ready = []\n    for instance in list_instances():\n        try:\n            elapsed = datetime.datetime.now(\n                datetime.timezone.utc\n            ) - datetime.datetime.strptime(instance[\"creationTimestamp\"], '%Y-%m-%dT%H:%M:%S.%f%z')\n            \n            if config[\"age_limit\"] > 0 and elapsed > datetime.timedelta(seconds=config[\"age_limit\"]):\n                access_configs = instance['networkInterfaces'][0]['accessConfigs'][0]\n                msg = f\"{instance['name']} {access_configs['natIP'] if 'natIP' in access_configs else ''}\"\n                delete_proxy(instance['name'])\n                logger.info(\"Recycling instance, reached age limit -> \" + msg)\n            \n            elif instance['status'] == \"TERMINATED\":\n                logger.info(\"Waking up: GCP -> Instance \" + instance['name'])\n                started = start_proxy(instance['name'])\n                if not started:\n                    logger.info(\"Could not wake up, trying again later.\")\n            \n            elif instance['status'] == \"STOPPING\":\n                access_configs = instance['networkInterfaces'][0]['accessConfigs'][0]\n                msg = f\"{instance['name']} {access_configs['natIP'] if 'natIP' in access_configs else ''}\"\n                logger.info(\"Stopping: GCP -> \" + msg)\n            \n            elif instance['status'] == \"PROVISIONING\" or instance['status'] == \"STAGING\":\n                access_configs = instance['networkInterfaces'][0]['accessConfigs'][0]\n                msg = f\"{instance['name']} {access_configs['natIP'] if 'natIP' in access_configs else ''}\"\n                logger.info(\"Provisioning: GCP -> \" + msg)\n            \n            # If none of the above, check if alive or not.\n            elif check_alive(instance['networkInterfaces'][0]['accessConfigs'][0]['natIP']):\n                access_configs = instance['networkInterfaces'][0]['accessConfigs'][0]\n                msg = f\"{instance['name']} {access_configs['natIP']}\"\n                logger.info(\"Alive: GCP -> \" + msg)\n                ip_ready.append(access_configs['natIP'])\n            \n            else:\n                access_configs = instance['networkInterfaces'][0]['accessConfigs'][0]\n                msg = f\"{instance['name']} {access_configs['natIP']}\"\n                if elapsed > datetime.timedelta(minutes=10):\n                    delete_proxy(instance['name'])\n                    logger.info(\"Destroyed: took too long GCP -> \" + msg)\n                else:\n                    logger.info(\"Waiting: GCP -> \" + msg)\n        except (TypeError, KeyError):\n            logger.info(\"Pending: GCP -> Allocating IP\")\n    return ip_ready\n\ndef gcp_check_delete():\n    for instance in list_instances():\n        access_configs = instance['networkInterfaces'][0]['accessConfigs'][0]\n        if 'natIP' in  access_configs and access_configs['natIP'] in delete_queue: \n            msg = f\"{instance['name']}, {access_configs['natIP']}\"\n            delete_proxy(instance['name'])\n            logger.info(\"Destroyed: not wanted -> \" + msg)\n            delete_queue.remove(access_configs['natIP'])\n\ndef gcp_check_stop():\n    for instance in list_instances():\n        access_configs = instance['networkInterfaces'][0]['accessConfigs'][0]\n        if 'natIP' in  access_configs and access_configs['natIP'] in restart_queue:\n            msg = f\"{instance['name']}, {access_configs['natIP']}\"\n            stop_proxy(instance['name'])\n            logger.info(\"Stopped: getting new IP -> \" + msg)\n            restart_queue.remove(access_configs['natIP'])\n\ndef gcp_start():\n    gcp_check_delete()\n    gcp_check_stop()\n    gcp_deployment(config[\"providers\"][\"gcp\"][\"scaling\"][\"min_scaling\"])\n    ip_ready = gcp_check_alive()\n    return ip_ready"}
{"type": "source_file", "path": "cloudproxy/providers/hetzner/main.py", "content": "import itertools\nimport datetime\n\nimport dateparser\nfrom loguru import logger\n\nfrom cloudproxy.check import check_alive\nfrom cloudproxy.providers import settings\nfrom cloudproxy.providers.hetzner.functions import list_proxies, delete_proxy, create_proxy\nfrom cloudproxy.providers.settings import config, delete_queue, restart_queue\n\n\ndef hetzner_deployment(min_scaling, instance_config=None):\n    \"\"\"\n    Deploy Hetzner servers based on min_scaling requirements.\n    \n    Args:\n        min_scaling: The minimum number of servers to maintain\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"]\n        \n    # Get instance display name for logging\n    display_name = instance_config.get(\"display_name\", \"default\")\n    \n    total_proxies = len(list_proxies(instance_config))\n    if min_scaling < total_proxies:\n        logger.info(f\"Overprovisioned: Hetzner {display_name} destroying.....\")\n        for proxy in itertools.islice(\n                list_proxies(instance_config), 0, (total_proxies - min_scaling)\n        ):\n            delete_proxy(proxy, instance_config)\n            logger.info(f\"Destroyed: Hetzner {display_name} -> {str(proxy.public_net.ipv4.ip)}\")\n            \n    if min_scaling - total_proxies < 1:\n        logger.info(f\"Minimum Hetzner {display_name} proxies met\")\n    else:\n        total_deploy = min_scaling - total_proxies\n        logger.info(f\"Deploying: {str(total_deploy)} Hetzner {display_name} proxy\")\n        for _ in range(total_deploy):\n            create_proxy(instance_config)\n            logger.info(f\"Deployed Hetzner {display_name} proxy\")\n            \n    return len(list_proxies(instance_config))\n\n\ndef hetzner_check_alive(instance_config=None):\n    \"\"\"\n    Check if Hetzner servers are alive and operational.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"]\n        \n    # Get instance display name for logging\n    display_name = instance_config.get(\"display_name\", \"default\")\n    \n    ip_ready = []\n    for proxy in list_proxies(instance_config):\n        elapsed = datetime.datetime.now(\n            datetime.timezone.utc\n        ) - dateparser.parse(str(proxy.created))\n        if config[\"age_limit\"] > 0 and elapsed > datetime.timedelta(seconds=config[\"age_limit\"]):\n            delete_proxy(proxy, instance_config)\n            logger.info(\n                f\"Recycling Hetzner {display_name} proxy, reached age limit -> {str(proxy.public_net.ipv4.ip)}\"\n            )\n        elif check_alive(proxy.public_net.ipv4.ip):\n            logger.info(f\"Alive: Hetzner {display_name} -> {str(proxy.public_net.ipv4.ip)}\")\n            ip_ready.append(proxy.public_net.ipv4.ip)\n        else:\n            if elapsed > datetime.timedelta(minutes=10):\n                delete_proxy(proxy, instance_config)\n                logger.info(\n                    f\"Destroyed: Hetzner {display_name} took too long -> {str(proxy.public_net.ipv4.ip)}\"\n                )\n            else:\n                logger.info(f\"Waiting: Hetzner {display_name} -> {str(proxy.public_net.ipv4.ip)}\")\n    return ip_ready\n\n\ndef hetzner_check_delete(instance_config=None):\n    \"\"\"\n    Check if any Hetzner servers need to be deleted.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"]\n        \n    # Get instance display name for logging\n    display_name = instance_config.get(\"display_name\", \"default\")\n    \n    # Log current delete queue state\n    if delete_queue:\n        logger.info(f\"Current delete queue contains {len(delete_queue)} IP addresses: {', '.join(delete_queue)}\")\n    \n    servers = list_proxies(instance_config)\n    if not servers:\n        logger.info(f\"No Hetzner {display_name} servers found to process for deletion\")\n        return\n        \n    logger.info(f\"Checking {len(servers)} Hetzner {display_name} servers for deletion\")\n    \n    for server in servers:\n        try:\n            server_ip = str(server.public_net.ipv4.ip)\n            \n            # Check if this server's IP is in the delete or restart queue\n            if server_ip in delete_queue or server_ip in restart_queue:\n                logger.info(f\"Found server {server.id} with IP {server_ip} in deletion queue - deleting now\")\n                \n                # Attempt to delete the server\n                delete_result = delete_proxy(server, instance_config)\n                \n                if delete_result:\n                    logger.info(f\"Successfully destroyed Hetzner {display_name} server -> {server_ip}\")\n                    \n                    # Remove from queues upon successful deletion\n                    if server_ip in delete_queue:\n                        delete_queue.remove(server_ip)\n                        logger.info(f\"Removed {server_ip} from delete queue\")\n                    if server_ip in restart_queue:\n                        restart_queue.remove(server_ip)\n                        logger.info(f\"Removed {server_ip} from restart queue\")\n                else:\n                    logger.warning(f\"Failed to destroy Hetzner {display_name} server -> {server_ip}\")\n        except Exception as e:\n            logger.error(f\"Error processing server for deletion: {e}\")\n            continue\n    \n    # Report on any IPs that remain in the queues but weren't found\n    remaining_delete = [ip for ip in delete_queue if any(ip == str(s.public_net.ipv4.ip) for s in servers)]\n    if remaining_delete:\n        logger.warning(f\"IPs remaining in delete queue that weren't found as servers: {', '.join(remaining_delete)}\")\n\n\ndef hetzner_start(instance_config=None):\n    \"\"\"\n    Start the Hetzner provider lifecycle.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \n    Returns:\n        list: List of ready IP addresses\n    \"\"\"\n    if instance_config is None:\n        instance_config = config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"]\n        \n    hetzner_check_delete(instance_config)\n    hetzner_deployment(instance_config[\"scaling\"][\"min_scaling\"], instance_config)\n    ip_ready = hetzner_check_alive(instance_config)\n    return ip_ready\n"}
{"type": "source_file", "path": "cloudproxy/providers/hetzner/functions.py", "content": "import os\nimport uuid\n\nfrom hcloud import Client\nfrom hcloud.images.domain import Image\nfrom hcloud.server_types.domain import ServerType\nfrom hcloud.datacenters.domain import Datacenter\nfrom hcloud.locations.domain import Location\nfrom loguru import logger\n\nfrom cloudproxy.providers import settings\nfrom cloudproxy.providers.config import set_auth\n\n# Initialize client with default instance configuration\nclient = Client(token=settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"][\"secrets\"][\"access_token\"])\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n\n# Remove this invalid logger configuration\n# logger = logging.getLogger(__name__)\n# loguru logger is already imported above\n\n\ndef get_client(instance_config=None):\n    \"\"\"\n    Get a Hetzner client for the specific instance configuration.\n    \n    Args:\n        instance_config: The specific instance configuration\n        \n    Returns:\n        Client: Hetzner client instance for the configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"]\n    \n    return Client(token=instance_config[\"secrets\"][\"access_token\"])\n\n\ndef create_proxy(instance_config=None):\n    \"\"\"\n    Create a Hetzner proxy server.\n    \n    Args:\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"]\n        \n    # Get instance name for labeling\n    instance_id = next(\n        (name for name, inst in settings.config[\"providers\"][\"hetzner\"][\"instances\"].items() \n         if inst == instance_config), \n        \"default\"\n    )\n    \n    # Get instance-specific client\n    hetzner_client = get_client(instance_config)\n    \n    # Prepare user data script\n    user_data = set_auth(settings.config[\"auth\"][\"username\"], settings.config[\"auth\"][\"password\"])\n    \n    # Determine location or datacenter parameter\n    datacenter = instance_config.get(\"datacenter\", None)\n    location = instance_config.get(\"location\", None)\n    \n    # Create the server with instance-specific settings\n    response = hetzner_client.servers.create(\n        name=f\"cloudproxy-{instance_id}-{str(uuid.uuid4())}\",\n        server_type=ServerType(instance_config[\"size\"]),\n        image=Image(name=\"ubuntu-20.04\"),\n        user_data=user_data,\n        datacenter=Datacenter(name=datacenter) if datacenter else None,\n        location=Location(name=location) if location else None,\n        labels={\"type\": \"cloudproxy\", \"instance\": instance_id}\n    )\n    \n    return response\n\n\ndef delete_proxy(server_id, instance_config=None):\n    \"\"\"\n    Delete a Hetzner proxy server.\n    \n    Args:\n        server_id: ID of the server to delete\n        instance_config: The specific instance configuration\n    \"\"\"\n    if instance_config is None:\n        instance_config = settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"]\n        \n    # Get instance-specific client\n    hetzner_client = get_client(instance_config)\n    \n    try:\n        # Handle both ID and Server object\n        server_obj = None\n        server_identifier = None\n        \n        if hasattr(server_id, 'id'):\n            # It's a server object\n            server_obj = server_id\n            server_identifier = server_id.id\n        else:\n            # It's an ID, we need to get the server first\n            server_identifier = server_id\n            try:\n                server_obj = hetzner_client.servers.get_by_id(server_id)\n            except Exception as e:\n                # If server not found, consider it already deleted\n                if \"not found\" in str(e).lower() or \"404\" in str(e) or \"does not exist\" in str(e).lower():\n                    logger.info(f\"Hetzner server with ID {server_id} not found, considering it already deleted\")\n                    return True\n                # Re-raise other errors\n                raise\n        \n        # If we got the server object successfully, delete it\n        if server_obj:\n            logger.info(f\"Deleting Hetzner server with ID: {server_identifier}\")\n            response = server_obj.delete()\n            logger.info(f\"Hetzner deletion API call completed with response: {response}\")\n            return response\n        else:\n            # This should not happen since we either return earlier or have a server object\n            logger.warning(f\"No valid Hetzner server object found for ID: {server_identifier}\")\n            return True\n            \n    except Exception as e:\n        # If the server is not found or any other error occurs\n        # during deletion, consider it already deleted\n        if \"not found\" in str(e).lower() or \"404\" in str(e) or \"attribute\" in str(e).lower() or \"does not exist\" in str(e).lower():\n            logger.info(f\"Exception during Hetzner server deletion indicates it's already gone: {str(e)}\")\n            return True\n        else:\n            # Re-raise other errors\n            logger.error(f\"Error during Hetzner server deletion: {str(e)}\")\n            raise\n\n\ndef list_proxies(instance_config=None):\n    \"\"\"\n    List Hetzner proxy servers.\n    \n    Args:\n        instance_config: The specific instance configuration\n        \n    Returns:\n        list: List of Hetzner servers\n    \"\"\"\n    if instance_config is None:\n        instance_config = settings.config[\"providers\"][\"hetzner\"][\"instances\"][\"default\"]\n        \n    # Get instance name for filtering\n    instance_id = next(\n        (name for name, inst in settings.config[\"providers\"][\"hetzner\"][\"instances\"].items() \n         if inst == instance_config), \n        \"default\"\n    )\n    \n    # Get instance-specific client\n    hetzner_client = get_client(instance_config)\n    \n    # Filter servers by labels\n    label_selector = \"type=cloudproxy\"\n    if instance_id != \"default\":\n        label_selector += f\",instance={instance_id}\"\n        \n    servers = hetzner_client.servers.get_all(label_selector=label_selector)\n    \n    # For default instance, also include servers created before multi-instance support\n    if instance_id == \"default\":\n        # Get old servers without instance label but with cloudproxy type\n        old_servers = hetzner_client.servers.get_all(label_selector=\"type=cloudproxy\")\n        # Filter out servers that have instance labels\n        old_servers = [s for s in old_servers if \"instance\" not in s.labels]\n        # Merge lists, avoiding duplicates\n        existing_ids = {s.id for s in servers}\n        servers.extend([s for s in old_servers if s.id not in existing_ids])\n    \n    return servers\n"}
