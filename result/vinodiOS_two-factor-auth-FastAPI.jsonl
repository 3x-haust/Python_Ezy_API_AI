{"repo_info": {"repo_name": "two-factor-auth-FastAPI", "repo_owner": "vinodiOS", "repo_url": "https://github.com/vinodiOS/two-factor-auth-FastAPI"}}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/aiosqlite/tests/__init__.py", "content": "# Copyright 2018 John Reese\n# Licensed under the MIT license\n\nfrom .smoke import SmokeTest\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/ecdsa/test_jacobi.py", "content": "import pickle\n\ntry:\n    import unittest2 as unittest\nexcept ImportError:\n    import unittest\n\nimport os\nimport sys\nimport signal\nimport pytest\nimport threading\nimport platform\nimport hypothesis.strategies as st\nfrom hypothesis import given, assume, settings, example\n\nfrom .ellipticcurve import CurveFp, PointJacobi, INFINITY\nfrom .ecdsa import (\n    generator_256,\n    curve_256,\n    generator_224,\n    generator_brainpoolp160r1,\n    curve_brainpoolp160r1,\n    generator_112r2,\n)\nfrom .numbertheory import inverse_mod\nfrom .util import randrange\n\n\nNO_OLD_SETTINGS = {}\nif sys.version_info > (2, 7):  # pragma: no branch\n    NO_OLD_SETTINGS[\"deadline\"] = 5000\n\n\nclass TestJacobi(unittest.TestCase):\n    def test___init__(self):\n        curve = object()\n        x = 2\n        y = 3\n        z = 1\n        order = 4\n        pj = PointJacobi(curve, x, y, z, order)\n\n        self.assertEqual(pj.order(), order)\n        self.assertIs(pj.curve(), curve)\n        self.assertEqual(pj.x(), x)\n        self.assertEqual(pj.y(), y)\n\n    def test_add_with_different_curves(self):\n        p_a = PointJacobi.from_affine(generator_256)\n        p_b = PointJacobi.from_affine(generator_224)\n\n        with self.assertRaises(ValueError):\n            p_a + p_b\n\n    def test_compare_different_curves(self):\n        self.assertNotEqual(generator_256, generator_224)\n\n    def test_equality_with_non_point(self):\n        pj = PointJacobi.from_affine(generator_256)\n\n        self.assertNotEqual(pj, \"value\")\n\n    def test_conversion(self):\n        pj = PointJacobi.from_affine(generator_256)\n        pw = pj.to_affine()\n\n        self.assertEqual(generator_256, pw)\n\n    def test_single_double(self):\n        pj = PointJacobi.from_affine(generator_256)\n        pw = generator_256.double()\n\n        pj = pj.double()\n\n        self.assertEqual(pj.x(), pw.x())\n        self.assertEqual(pj.y(), pw.y())\n\n    def test_double_with_zero_point(self):\n        pj = PointJacobi(curve_256, 0, 0, 1)\n\n        pj = pj.double()\n\n        self.assertIs(pj, INFINITY)\n\n    def test_double_with_zero_equivalent_point(self):\n        pj = PointJacobi(curve_256, 0, curve_256.p(), 1)\n\n        pj = pj.double()\n\n        self.assertIs(pj, INFINITY)\n\n    def test_double_with_zero_equivalent_point_non_1_z(self):\n        pj = PointJacobi(curve_256, 0, curve_256.p(), 2)\n\n        pj = pj.double()\n\n        self.assertIs(pj, INFINITY)\n\n    def test_compare_with_affine_point(self):\n        pj = PointJacobi.from_affine(generator_256)\n        pa = pj.to_affine()\n\n        self.assertEqual(pj, pa)\n        self.assertEqual(pa, pj)\n\n    def test_to_affine_with_zero_point(self):\n        pj = PointJacobi(curve_256, 0, 0, 1)\n\n        pa = pj.to_affine()\n\n        self.assertIs(pa, INFINITY)\n\n    def test_add_with_affine_point(self):\n        pj = PointJacobi.from_affine(generator_256)\n        pa = pj.to_affine()\n\n        s = pj + pa\n\n        self.assertEqual(s, pj.double())\n\n    def test_radd_with_affine_point(self):\n        pj = PointJacobi.from_affine(generator_256)\n        pa = pj.to_affine()\n\n        s = pa + pj\n\n        self.assertEqual(s, pj.double())\n\n    def test_add_with_infinity(self):\n        pj = PointJacobi.from_affine(generator_256)\n\n        s = pj + INFINITY\n\n        self.assertEqual(s, pj)\n\n    def test_add_zero_point_to_affine(self):\n        pa = PointJacobi.from_affine(generator_256).to_affine()\n        pj = PointJacobi(curve_256, 0, 0, 1)\n\n        s = pj + pa\n\n        self.assertIs(s, pa)\n\n    def test_multiply_by_zero(self):\n        pj = PointJacobi.from_affine(generator_256)\n\n        pj = pj * 0\n\n        self.assertIs(pj, INFINITY)\n\n    def test_zero_point_multiply_by_one(self):\n        pj = PointJacobi(curve_256, 0, 0, 1)\n\n        pj = pj * 1\n\n        self.assertIs(pj, INFINITY)\n\n    def test_multiply_by_one(self):\n        pj = PointJacobi.from_affine(generator_256)\n        pw = generator_256 * 1\n\n        pj = pj * 1\n\n        self.assertEqual(pj.x(), pw.x())\n        self.assertEqual(pj.y(), pw.y())\n\n    def test_multiply_by_two(self):\n        pj = PointJacobi.from_affine(generator_256)\n        pw = generator_256 * 2\n\n        pj = pj * 2\n\n        self.assertEqual(pj.x(), pw.x())\n        self.assertEqual(pj.y(), pw.y())\n\n    def test_rmul_by_two(self):\n        pj = PointJacobi.from_affine(generator_256)\n        pw = generator_256 * 2\n\n        pj = 2 * pj\n\n        self.assertEqual(pj, pw)\n\n    def test_compare_non_zero_with_infinity(self):\n        pj = PointJacobi.from_affine(generator_256)\n\n        self.assertNotEqual(pj, INFINITY)\n\n    def test_compare_zero_point_with_infinity(self):\n        pj = PointJacobi(curve_256, 0, 0, 1)\n\n        self.assertEqual(pj, INFINITY)\n\n    def test_compare_double_with_multiply(self):\n        pj = PointJacobi.from_affine(generator_256)\n        dbl = pj.double()\n        mlpl = pj * 2\n\n        self.assertEqual(dbl, mlpl)\n\n    @settings(max_examples=10)\n    @given(\n        st.integers(\n            min_value=0, max_value=int(generator_brainpoolp160r1.order())\n        )\n    )\n    def test_multiplications(self, mul):\n        pj = PointJacobi.from_affine(generator_brainpoolp160r1)\n        pw = pj.to_affine() * mul\n\n        pj = pj * mul\n\n        self.assertEqual((pj.x(), pj.y()), (pw.x(), pw.y()))\n        self.assertEqual(pj, pw)\n\n    @settings(max_examples=10)\n    @given(\n        st.integers(\n            min_value=0, max_value=int(generator_brainpoolp160r1.order())\n        )\n    )\n    @example(0)\n    @example(int(generator_brainpoolp160r1.order()))\n    def test_precompute(self, mul):\n        precomp = generator_brainpoolp160r1\n        self.assertTrue(precomp._PointJacobi__precompute)\n        pj = PointJacobi.from_affine(generator_brainpoolp160r1)\n\n        a = precomp * mul\n        b = pj * mul\n\n        self.assertEqual(a, b)\n\n    @settings(max_examples=10)\n    @given(\n        st.integers(\n            min_value=1, max_value=int(generator_brainpoolp160r1.order())\n        ),\n        st.integers(\n            min_value=1, max_value=int(generator_brainpoolp160r1.order())\n        ),\n    )\n    @example(3, 3)\n    def test_add_scaled_points(self, a_mul, b_mul):\n        j_g = PointJacobi.from_affine(generator_brainpoolp160r1)\n        a = PointJacobi.from_affine(j_g * a_mul)\n        b = PointJacobi.from_affine(j_g * b_mul)\n\n        c = a + b\n\n        self.assertEqual(c, j_g * (a_mul + b_mul))\n\n    @settings(max_examples=10)\n    @given(\n        st.integers(\n            min_value=1, max_value=int(generator_brainpoolp160r1.order())\n        ),\n        st.integers(\n            min_value=1, max_value=int(generator_brainpoolp160r1.order())\n        ),\n        st.integers(min_value=1, max_value=int(curve_brainpoolp160r1.p() - 1)),\n    )\n    def test_add_one_scaled_point(self, a_mul, b_mul, new_z):\n        j_g = PointJacobi.from_affine(generator_brainpoolp160r1)\n        a = PointJacobi.from_affine(j_g * a_mul)\n        b = PointJacobi.from_affine(j_g * b_mul)\n\n        p = curve_brainpoolp160r1.p()\n\n        assume(inverse_mod(new_z, p))\n\n        new_zz = new_z * new_z % p\n\n        b = PointJacobi(\n            curve_brainpoolp160r1,\n            b.x() * new_zz % p,\n            b.y() * new_zz * new_z % p,\n            new_z,\n        )\n\n        c = a + b\n\n        self.assertEqual(c, j_g * (a_mul + b_mul))\n\n    @settings(max_examples=10)\n    @given(\n        st.integers(\n            min_value=1, max_value=int(generator_brainpoolp160r1.order())\n        ),\n        st.integers(\n            min_value=1, max_value=int(generator_brainpoolp160r1.order())\n        ),\n        st.integers(min_value=1, max_value=int(curve_brainpoolp160r1.p() - 1)),\n    )\n    @example(1, 1, 1)\n    @example(3, 3, 3)\n    @example(2, int(generator_brainpoolp160r1.order() - 2), 1)\n    @example(2, int(generator_brainpoolp160r1.order() - 2), 3)\n    def test_add_same_scale_points(self, a_mul, b_mul, new_z):\n        j_g = PointJacobi.from_affine(generator_brainpoolp160r1)\n        a = PointJacobi.from_affine(j_g * a_mul)\n        b = PointJacobi.from_affine(j_g * b_mul)\n\n        p = curve_brainpoolp160r1.p()\n\n        assume(inverse_mod(new_z, p))\n\n        new_zz = new_z * new_z % p\n\n        a = PointJacobi(\n            curve_brainpoolp160r1,\n            a.x() * new_zz % p,\n            a.y() * new_zz * new_z % p,\n            new_z,\n        )\n        b = PointJacobi(\n            curve_brainpoolp160r1,\n            b.x() * new_zz % p,\n            b.y() * new_zz * new_z % p,\n            new_z,\n        )\n\n        c = a + b\n\n        self.assertEqual(c, j_g * (a_mul + b_mul))\n\n    def test_add_same_scale_points_static(self):\n        j_g = generator_brainpoolp160r1\n        p = curve_brainpoolp160r1.p()\n        a = j_g * 11\n        a.scale()\n        z1 = 13\n        x = PointJacobi(\n            curve_brainpoolp160r1,\n            a.x() * z1 ** 2 % p,\n            a.y() * z1 ** 3 % p,\n            z1,\n        )\n        y = PointJacobi(\n            curve_brainpoolp160r1,\n            a.x() * z1 ** 2 % p,\n            a.y() * z1 ** 3 % p,\n            z1,\n        )\n\n        c = a + a\n\n        self.assertEqual(c, x + y)\n\n    @settings(max_examples=14)\n    @given(\n        st.integers(\n            min_value=1, max_value=int(generator_brainpoolp160r1.order())\n        ),\n        st.integers(\n            min_value=1, max_value=int(generator_brainpoolp160r1.order())\n        ),\n        st.lists(\n            st.integers(\n                min_value=1, max_value=int(curve_brainpoolp160r1.p() - 1)\n            ),\n            min_size=2,\n            max_size=2,\n            unique=True,\n        ),\n    )\n    @example(2, 2, [2, 1])\n    @example(2, 2, [2, 3])\n    @example(2, int(generator_brainpoolp160r1.order() - 2), [2, 3])\n    @example(2, int(generator_brainpoolp160r1.order() - 2), [2, 1])\n    def test_add_different_scale_points(self, a_mul, b_mul, new_z):\n        j_g = PointJacobi.from_affine(generator_brainpoolp160r1)\n        a = PointJacobi.from_affine(j_g * a_mul)\n        b = PointJacobi.from_affine(j_g * b_mul)\n\n        p = curve_brainpoolp160r1.p()\n\n        assume(inverse_mod(new_z[0], p))\n        assume(inverse_mod(new_z[1], p))\n\n        new_zz0 = new_z[0] * new_z[0] % p\n        new_zz1 = new_z[1] * new_z[1] % p\n\n        a = PointJacobi(\n            curve_brainpoolp160r1,\n            a.x() * new_zz0 % p,\n            a.y() * new_zz0 * new_z[0] % p,\n            new_z[0],\n        )\n        b = PointJacobi(\n            curve_brainpoolp160r1,\n            b.x() * new_zz1 % p,\n            b.y() * new_zz1 * new_z[1] % p,\n            new_z[1],\n        )\n\n        c = a + b\n\n        self.assertEqual(c, j_g * (a_mul + b_mul))\n\n    def test_add_different_scale_points_static(self):\n        j_g = generator_brainpoolp160r1\n        p = curve_brainpoolp160r1.p()\n        a = j_g * 11\n        a.scale()\n        z1 = 13\n        x = PointJacobi(\n            curve_brainpoolp160r1,\n            a.x() * z1 ** 2 % p,\n            a.y() * z1 ** 3 % p,\n            z1,\n        )\n        z2 = 29\n        y = PointJacobi(\n            curve_brainpoolp160r1,\n            a.x() * z2 ** 2 % p,\n            a.y() * z2 ** 3 % p,\n            z2,\n        )\n\n        c = a + a\n\n        self.assertEqual(c, x + y)\n\n    def test_add_point_3_times(self):\n        j_g = PointJacobi.from_affine(generator_256)\n\n        self.assertEqual(j_g * 3, j_g + j_g + j_g)\n\n    def test_mul_without_order(self):\n        j_g = PointJacobi(curve_256, generator_256.x(), generator_256.y(), 1)\n\n        self.assertEqual(j_g * generator_256.order(), INFINITY)\n\n    def test_mul_add_inf(self):\n        j_g = PointJacobi.from_affine(generator_256)\n\n        self.assertEqual(j_g, j_g.mul_add(1, INFINITY, 1))\n\n    def test_mul_add_same(self):\n        j_g = PointJacobi.from_affine(generator_256)\n\n        self.assertEqual(j_g * 2, j_g.mul_add(1, j_g, 1))\n\n    def test_mul_add_precompute(self):\n        j_g = PointJacobi.from_affine(generator_brainpoolp160r1, True)\n        b = PointJacobi.from_affine(j_g * 255, True)\n\n        self.assertEqual(j_g * 256, j_g + b)\n        self.assertEqual(j_g * (5 + 255 * 7), j_g * 5 + b * 7)\n        self.assertEqual(j_g * (5 + 255 * 7), j_g.mul_add(5, b, 7))\n\n    def test_mul_add_precompute_large(self):\n        j_g = PointJacobi.from_affine(generator_brainpoolp160r1, True)\n        b = PointJacobi.from_affine(j_g * 255, True)\n\n        self.assertEqual(j_g * 256, j_g + b)\n        self.assertEqual(\n            j_g * (0xFF00 + 255 * 0xF0F0), j_g * 0xFF00 + b * 0xF0F0\n        )\n        self.assertEqual(\n            j_g * (0xFF00 + 255 * 0xF0F0), j_g.mul_add(0xFF00, b, 0xF0F0)\n        )\n\n    def test_mul_add_to_mul(self):\n        j_g = PointJacobi.from_affine(generator_256)\n\n        a = j_g * 3\n        b = j_g.mul_add(2, j_g, 1)\n\n        self.assertEqual(a, b)\n\n    def test_mul_add_differnt(self):\n        j_g = PointJacobi.from_affine(generator_256)\n\n        w_a = j_g * 2\n\n        self.assertEqual(j_g.mul_add(1, w_a, 1), j_g * 3)\n\n    def test_mul_add_slightly_different(self):\n        j_g = PointJacobi.from_affine(generator_256)\n\n        w_a = j_g * 2\n        w_b = j_g * 3\n\n        self.assertEqual(w_a.mul_add(1, w_b, 3), w_a * 1 + w_b * 3)\n\n    def test_mul_add(self):\n        j_g = PointJacobi.from_affine(generator_256)\n\n        w_a = generator_256 * 255\n        w_b = generator_256 * (0xA8 * 0xF0)\n        j_b = j_g * 0xA8\n\n        ret = j_g.mul_add(255, j_b, 0xF0)\n\n        self.assertEqual(ret.to_affine(), w_a + w_b)\n\n    def test_mul_add_large(self):\n        j_g = PointJacobi.from_affine(generator_256)\n        b = PointJacobi.from_affine(j_g * 255)\n\n        self.assertEqual(j_g * 256, j_g + b)\n        self.assertEqual(\n            j_g * (0xFF00 + 255 * 0xF0F0), j_g * 0xFF00 + b * 0xF0F0\n        )\n        self.assertEqual(\n            j_g * (0xFF00 + 255 * 0xF0F0), j_g.mul_add(0xFF00, b, 0xF0F0)\n        )\n\n    def test_mul_add_with_infinity_as_result(self):\n        j_g = PointJacobi.from_affine(generator_256)\n\n        order = generator_256.order()\n\n        b = PointJacobi.from_affine(generator_256 * 256)\n\n        self.assertEqual(j_g.mul_add(order % 256, b, order // 256), INFINITY)\n\n    def test_mul_add_without_order(self):\n        j_g = PointJacobi(curve_256, generator_256.x(), generator_256.y(), 1)\n\n        order = generator_256.order()\n\n        w_b = generator_256 * 34\n        w_b.scale()\n\n        b = PointJacobi(curve_256, w_b.x(), w_b.y(), 1)\n\n        self.assertEqual(j_g.mul_add(order % 34, b, order // 34), INFINITY)\n\n    def test_mul_add_with_doubled_negation_of_itself(self):\n        j_g = PointJacobi.from_affine(generator_256 * 17)\n\n        dbl_neg = 2 * (-j_g)\n\n        self.assertEqual(j_g.mul_add(4, dbl_neg, 2), INFINITY)\n\n    def test_equality(self):\n        pj1 = PointJacobi(curve=CurveFp(23, 1, 1, 1), x=2, y=3, z=1, order=1)\n        pj2 = PointJacobi(curve=CurveFp(23, 1, 1, 1), x=2, y=3, z=1, order=1)\n        self.assertEqual(pj1, pj2)\n\n    def test_equality_with_invalid_object(self):\n        j_g = PointJacobi.from_affine(generator_256)\n\n        self.assertNotEqual(j_g, 12)\n\n    def test_equality_with_wrong_curves(self):\n        p_a = PointJacobi.from_affine(generator_256)\n        p_b = PointJacobi.from_affine(generator_224)\n\n        self.assertNotEqual(p_a, p_b)\n\n    def test_pickle(self):\n        pj = PointJacobi(curve=CurveFp(23, 1, 1, 1), x=2, y=3, z=1, order=1)\n        self.assertEqual(pickle.loads(pickle.dumps(pj)), pj)\n\n    @settings(**NO_OLD_SETTINGS)\n    @given(st.integers(min_value=1, max_value=10))\n    def test_multithreading(self, thread_num):\n        # ensure that generator's precomputation table is filled\n        generator_112r2 * 2\n\n        # create a fresh point that doesn't have a filled precomputation table\n        gen = generator_112r2\n        gen = PointJacobi(gen.curve(), gen.x(), gen.y(), 1, gen.order(), True)\n\n        self.assertEqual(gen._PointJacobi__precompute, [])\n\n        def runner(generator):\n            order = generator.order()\n            for _ in range(10):\n                generator * randrange(order)\n\n        threads = []\n        for _ in range(thread_num):\n            threads.append(threading.Thread(target=runner, args=(gen,)))\n\n        for t in threads:\n            t.start()\n\n        runner(gen)\n\n        for t in threads:\n            t.join()\n\n        self.assertEqual(\n            gen._PointJacobi__precompute,\n            generator_112r2._PointJacobi__precompute,\n        )\n\n    @pytest.mark.skipif(\n        platform.system() == \"Windows\",\n        reason=\"there are no signals on Windows\",\n    )\n    def test_multithreading_with_interrupts(self):\n        thread_num = 10\n        # ensure that generator's precomputation table is filled\n        generator_112r2 * 2\n\n        # create a fresh point that doesn't have a filled precomputation table\n        gen = generator_112r2\n        gen = PointJacobi(gen.curve(), gen.x(), gen.y(), 1, gen.order(), True)\n\n        self.assertEqual(gen._PointJacobi__precompute, [])\n\n        def runner(generator):\n            order = generator.order()\n            for _ in range(50):\n                generator * randrange(order)\n\n        def interrupter(barrier_start, barrier_end, lock_exit):\n            # wait until MainThread can handle KeyboardInterrupt\n            barrier_start.release()\n            barrier_end.acquire()\n            os.kill(os.getpid(), signal.SIGINT)\n            lock_exit.release()\n\n        threads = []\n        for _ in range(thread_num):\n            threads.append(threading.Thread(target=runner, args=(gen,)))\n\n        barrier_start = threading.Lock()\n        barrier_start.acquire()\n        barrier_end = threading.Lock()\n        barrier_end.acquire()\n        lock_exit = threading.Lock()\n        lock_exit.acquire()\n\n        threads.append(\n            threading.Thread(\n                target=interrupter,\n                args=(barrier_start, barrier_end, lock_exit),\n            )\n        )\n\n        for t in threads:\n            t.start()\n\n        with self.assertRaises(KeyboardInterrupt):\n            # signal to interrupter that we can now handle the signal\n            barrier_start.acquire()\n            barrier_end.release()\n            runner(gen)\n            # use the lock to ensure we never go past the scope of\n            # assertRaises before the os.kill is called\n            lock_exit.acquire()\n\n        for t in threads:\n            t.join()\n\n        self.assertEqual(\n            gen._PointJacobi__precompute,\n            generator_112r2._PointJacobi__precompute,\n        )\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/ecdsa/test_der.py", "content": "# compatibility with Python 2.6, for that we need unittest2 package,\n# which is not available on 3.3 or 3.4\nimport warnings\nfrom binascii import hexlify\n\ntry:\n    import unittest2 as unittest\nexcept ImportError:\n    import unittest\nfrom six import b\nimport hypothesis.strategies as st\nfrom hypothesis import given\nimport pytest\nfrom ._compat import str_idx_as_int\nfrom .curves import NIST256p, NIST224p\nfrom .der import (\n    remove_integer,\n    UnexpectedDER,\n    read_length,\n    encode_bitstring,\n    remove_bitstring,\n    remove_object,\n    encode_oid,\n    remove_constructed,\n    remove_octet_string,\n    remove_sequence,\n)\n\n\nclass TestRemoveInteger(unittest.TestCase):\n    # DER requires the integers to be 0-padded only if they would be\n    # interpreted as negative, check if those errors are detected\n    def test_non_minimal_encoding(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_integer(b(\"\\x02\\x02\\x00\\x01\"))\n\n    def test_negative_with_high_bit_set(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_integer(b(\"\\x02\\x01\\x80\"))\n\n    def test_minimal_with_high_bit_set(self):\n        val, rem = remove_integer(b(\"\\x02\\x02\\x00\\x80\"))\n\n        self.assertEqual(val, 0x80)\n        self.assertEqual(rem, b\"\")\n\n    def test_two_zero_bytes_with_high_bit_set(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_integer(b(\"\\x02\\x03\\x00\\x00\\xff\"))\n\n    def test_zero_length_integer(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_integer(b(\"\\x02\\x00\"))\n\n    def test_empty_string(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_integer(b(\"\"))\n\n    def test_encoding_of_zero(self):\n        val, rem = remove_integer(b(\"\\x02\\x01\\x00\"))\n\n        self.assertEqual(val, 0)\n        self.assertEqual(rem, b\"\")\n\n    def test_encoding_of_127(self):\n        val, rem = remove_integer(b(\"\\x02\\x01\\x7f\"))\n\n        self.assertEqual(val, 127)\n        self.assertEqual(rem, b\"\")\n\n    def test_encoding_of_128(self):\n        val, rem = remove_integer(b(\"\\x02\\x02\\x00\\x80\"))\n\n        self.assertEqual(val, 128)\n        self.assertEqual(rem, b\"\")\n\n    def test_wrong_tag(self):\n        with self.assertRaises(UnexpectedDER) as e:\n            remove_integer(b\"\\x01\\x02\\x00\\x80\")\n\n        self.assertIn(\"wanted type 'integer'\", str(e.exception))\n\n    def test_wrong_length(self):\n        with self.assertRaises(UnexpectedDER) as e:\n            remove_integer(b\"\\x02\\x03\\x00\\x80\")\n\n        self.assertIn(\"Length longer\", str(e.exception))\n\n\nclass TestReadLength(unittest.TestCase):\n    # DER requires the lengths between 0 and 127 to be encoded using the short\n    # form and lengths above that encoded with minimal number of bytes\n    # necessary\n    def test_zero_length(self):\n        self.assertEqual((0, 1), read_length(b(\"\\x00\")))\n\n    def test_two_byte_zero_length(self):\n        with self.assertRaises(UnexpectedDER):\n            read_length(b(\"\\x81\\x00\"))\n\n    def test_two_byte_small_length(self):\n        with self.assertRaises(UnexpectedDER):\n            read_length(b(\"\\x81\\x7f\"))\n\n    def test_long_form_with_zero_length(self):\n        with self.assertRaises(UnexpectedDER):\n            read_length(b(\"\\x80\"))\n\n    def test_smallest_two_byte_length(self):\n        self.assertEqual((128, 2), read_length(b(\"\\x81\\x80\")))\n\n    def test_zero_padded_length(self):\n        with self.assertRaises(UnexpectedDER):\n            read_length(b(\"\\x82\\x00\\x80\"))\n\n    def test_two_three_byte_length(self):\n        self.assertEqual((256, 3), read_length(b\"\\x82\\x01\\x00\"))\n\n    def test_empty_string(self):\n        with self.assertRaises(UnexpectedDER):\n            read_length(b(\"\"))\n\n    def test_length_overflow(self):\n        with self.assertRaises(UnexpectedDER):\n            read_length(b(\"\\x83\\x01\\x00\"))\n\n\nclass TestEncodeBitstring(unittest.TestCase):\n    # DER requires BIT STRINGS to include a number of padding bits in the\n    # encoded byte string, that padding must be between 0 and 7\n\n    def test_old_call_convention(self):\n        \"\"\"This is the old way to use the function.\"\"\"\n        warnings.simplefilter(\"always\")\n        with pytest.warns(DeprecationWarning) as warns:\n            der = encode_bitstring(b\"\\x00\\xff\")\n\n        self.assertEqual(len(warns), 1)\n        self.assertIn(\n            \"unused= needs to be specified\", warns[0].message.args[0]\n        )\n\n        self.assertEqual(der, b\"\\x03\\x02\\x00\\xff\")\n\n    def test_new_call_convention(self):\n        \"\"\"This is how it should be called now.\"\"\"\n        warnings.simplefilter(\"always\")\n        with pytest.warns(None) as warns:\n            der = encode_bitstring(b\"\\xff\", 0)\n\n        # verify that new call convention doesn't raise Warnings\n        self.assertEqual(len(warns), 0)\n\n        self.assertEqual(der, b\"\\x03\\x02\\x00\\xff\")\n\n    def test_implicit_unused_bits(self):\n        \"\"\"\n        Writing bit string with already included the number of unused bits.\n        \"\"\"\n        warnings.simplefilter(\"always\")\n        with pytest.warns(None) as warns:\n            der = encode_bitstring(b\"\\x00\\xff\", None)\n\n        # verify that new call convention doesn't raise Warnings\n        self.assertEqual(len(warns), 0)\n\n        self.assertEqual(der, b\"\\x03\\x02\\x00\\xff\")\n\n    def test_explicit_unused_bits(self):\n        der = encode_bitstring(b\"\\xff\\xf0\", 4)\n\n        self.assertEqual(der, b\"\\x03\\x03\\x04\\xff\\xf0\")\n\n    def test_empty_string(self):\n        self.assertEqual(encode_bitstring(b\"\", 0), b\"\\x03\\x01\\x00\")\n\n    def test_invalid_unused_count(self):\n        with self.assertRaises(ValueError):\n            encode_bitstring(b\"\\xff\\x00\", 8)\n\n    def test_invalid_unused_with_empty_string(self):\n        with self.assertRaises(ValueError):\n            encode_bitstring(b\"\", 1)\n\n    def test_non_zero_padding_bits(self):\n        with self.assertRaises(ValueError):\n            encode_bitstring(b\"\\xff\", 2)\n\n\nclass TestRemoveBitstring(unittest.TestCase):\n    def test_old_call_convention(self):\n        \"\"\"This is the old way to call the function.\"\"\"\n        warnings.simplefilter(\"always\")\n        with pytest.warns(DeprecationWarning) as warns:\n            bits, rest = remove_bitstring(b\"\\x03\\x02\\x00\\xff\")\n\n        self.assertEqual(len(warns), 1)\n        self.assertIn(\n            \"expect_unused= needs to be specified\", warns[0].message.args[0]\n        )\n\n        self.assertEqual(bits, b\"\\x00\\xff\")\n        self.assertEqual(rest, b\"\")\n\n    def test_new_call_convention(self):\n        warnings.simplefilter(\"always\")\n        with pytest.warns(None) as warns:\n            bits, rest = remove_bitstring(b\"\\x03\\x02\\x00\\xff\", 0)\n\n        self.assertEqual(len(warns), 0)\n\n        self.assertEqual(bits, b\"\\xff\")\n        self.assertEqual(rest, b\"\")\n\n    def test_implicit_unexpected_unused(self):\n        warnings.simplefilter(\"always\")\n        with pytest.warns(None) as warns:\n            bits, rest = remove_bitstring(b\"\\x03\\x02\\x00\\xff\", None)\n\n        self.assertEqual(len(warns), 0)\n\n        self.assertEqual(bits, (b\"\\xff\", 0))\n        self.assertEqual(rest, b\"\")\n\n    def test_with_padding(self):\n        ret, rest = remove_bitstring(b\"\\x03\\x02\\x04\\xf0\", None)\n\n        self.assertEqual(ret, (b\"\\xf0\", 4))\n        self.assertEqual(rest, b\"\")\n\n    def test_not_a_bitstring(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_bitstring(b\"\\x02\\x02\\x00\\xff\", None)\n\n    def test_empty_encoding(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_bitstring(b\"\\x03\\x00\", None)\n\n    def test_empty_string(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_bitstring(b\"\", None)\n\n    def test_no_length(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_bitstring(b\"\\x03\", None)\n\n    def test_unexpected_number_of_unused_bits(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_bitstring(b\"\\x03\\x02\\x00\\xff\", 1)\n\n    def test_invalid_encoding_of_unused_bits(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_bitstring(b\"\\x03\\x03\\x08\\xff\\x00\", None)\n\n    def test_invalid_encoding_of_empty_string(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_bitstring(b\"\\x03\\x01\\x01\", None)\n\n    def test_invalid_padding_bits(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_bitstring(b\"\\x03\\x02\\x01\\xff\", None)\n\n\nclass TestStrIdxAsInt(unittest.TestCase):\n    def test_str(self):\n        self.assertEqual(115, str_idx_as_int(\"str\", 0))\n\n    def test_bytes(self):\n        self.assertEqual(115, str_idx_as_int(b\"str\", 0))\n\n    def test_bytearray(self):\n        self.assertEqual(115, str_idx_as_int(bytearray(b\"str\"), 0))\n\n\nclass TestEncodeOid(unittest.TestCase):\n    def test_pub_key_oid(self):\n        oid_ecPublicKey = encode_oid(1, 2, 840, 10045, 2, 1)\n        self.assertEqual(hexlify(oid_ecPublicKey), b(\"06072a8648ce3d0201\"))\n\n    def test_nist224p_oid(self):\n        self.assertEqual(hexlify(NIST224p.encoded_oid), b(\"06052b81040021\"))\n\n    def test_nist256p_oid(self):\n        self.assertEqual(\n            hexlify(NIST256p.encoded_oid), b\"06082a8648ce3d030107\"\n        )\n\n    def test_large_second_subid(self):\n        # from X.690, section 8.19.5\n        oid = encode_oid(2, 999, 3)\n        self.assertEqual(oid, b\"\\x06\\x03\\x88\\x37\\x03\")\n\n    def test_with_two_subids(self):\n        oid = encode_oid(2, 999)\n        self.assertEqual(oid, b\"\\x06\\x02\\x88\\x37\")\n\n    def test_zero_zero(self):\n        oid = encode_oid(0, 0)\n        self.assertEqual(oid, b\"\\x06\\x01\\x00\")\n\n    def test_with_wrong_types(self):\n        with self.assertRaises((TypeError, AssertionError)):\n            encode_oid(0, None)\n\n    def test_with_small_first_large_second(self):\n        with self.assertRaises(AssertionError):\n            encode_oid(1, 40)\n\n    def test_small_first_max_second(self):\n        oid = encode_oid(1, 39)\n        self.assertEqual(oid, b\"\\x06\\x01\\x4f\")\n\n    def test_with_invalid_first(self):\n        with self.assertRaises(AssertionError):\n            encode_oid(3, 39)\n\n\nclass TestRemoveObject(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.oid_ecPublicKey = encode_oid(1, 2, 840, 10045, 2, 1)\n\n    def test_pub_key_oid(self):\n        oid, rest = remove_object(self.oid_ecPublicKey)\n        self.assertEqual(rest, b\"\")\n        self.assertEqual(oid, (1, 2, 840, 10045, 2, 1))\n\n    def test_with_extra_bytes(self):\n        oid, rest = remove_object(self.oid_ecPublicKey + b\"more\")\n        self.assertEqual(rest, b\"more\")\n        self.assertEqual(oid, (1, 2, 840, 10045, 2, 1))\n\n    def test_with_large_second_subid(self):\n        # from X.690, section 8.19.5\n        oid, rest = remove_object(b\"\\x06\\x03\\x88\\x37\\x03\")\n        self.assertEqual(rest, b\"\")\n        self.assertEqual(oid, (2, 999, 3))\n\n    def test_with_padded_first_subid(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_object(b\"\\x06\\x02\\x80\\x00\")\n\n    def test_with_padded_second_subid(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_object(b\"\\x06\\x04\\x88\\x37\\x80\\x01\")\n\n    def test_with_missing_last_byte_of_multi_byte(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_object(b\"\\x06\\x03\\x88\\x37\\x83\")\n\n    def test_with_two_subids(self):\n        oid, rest = remove_object(b\"\\x06\\x02\\x88\\x37\")\n        self.assertEqual(rest, b\"\")\n        self.assertEqual(oid, (2, 999))\n\n    def test_zero_zero(self):\n        oid, rest = remove_object(b\"\\x06\\x01\\x00\")\n        self.assertEqual(rest, b\"\")\n        self.assertEqual(oid, (0, 0))\n\n    def test_empty_string(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_object(b\"\")\n\n    def test_missing_length(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_object(b\"\\x06\")\n\n    def test_empty_oid(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_object(b\"\\x06\\x00\")\n\n    def test_empty_oid_overflow(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_object(b\"\\x06\\x01\")\n\n    def test_with_wrong_type(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_object(b\"\\x04\\x02\\x88\\x37\")\n\n    def test_with_too_long_length(self):\n        with self.assertRaises(UnexpectedDER):\n            remove_object(b\"\\x06\\x03\\x88\\x37\")\n\n\nclass TestRemoveConstructed(unittest.TestCase):\n    def test_simple(self):\n        data = b\"\\xa1\\x02\\xff\\xaa\"\n\n        tag, body, rest = remove_constructed(data)\n\n        self.assertEqual(tag, 0x01)\n        self.assertEqual(body, b\"\\xff\\xaa\")\n        self.assertEqual(rest, b\"\")\n\n    def test_with_malformed_tag(self):\n        data = b\"\\x01\\x02\\xff\\xaa\"\n\n        with self.assertRaises(UnexpectedDER) as e:\n            remove_constructed(data)\n\n        self.assertIn(\"constructed tag\", str(e.exception))\n\n\nclass TestRemoveOctetString(unittest.TestCase):\n    def test_simple(self):\n        data = b\"\\x04\\x03\\xaa\\xbb\\xcc\"\n        body, rest = remove_octet_string(data)\n        self.assertEqual(body, b\"\\xaa\\xbb\\xcc\")\n        self.assertEqual(rest, b\"\")\n\n    def test_with_malformed_tag(self):\n        data = b\"\\x03\\x03\\xaa\\xbb\\xcc\"\n        with self.assertRaises(UnexpectedDER) as e:\n            remove_octet_string(data)\n\n        self.assertIn(\"octetstring\", str(e.exception))\n\n\nclass TestRemoveSequence(unittest.TestCase):\n    def test_simple(self):\n        data = b\"\\x30\\x02\\xff\\xaa\"\n        body, rest = remove_sequence(data)\n        self.assertEqual(body, b\"\\xff\\xaa\")\n        self.assertEqual(rest, b\"\")\n\n    def test_with_empty_string(self):\n        with self.assertRaises(UnexpectedDER) as e:\n            remove_sequence(b\"\")\n\n        self.assertIn(\"Empty string\", str(e.exception))\n\n    def test_with_wrong_tag(self):\n        data = b\"\\x20\\x02\\xff\\xaa\"\n\n        with self.assertRaises(UnexpectedDER) as e:\n            remove_sequence(data)\n\n        self.assertIn(\"wanted type 'sequence'\", str(e.exception))\n\n    def test_with_wrong_length(self):\n        data = b\"\\x30\\x03\\xff\\xaa\"\n\n        with self.assertRaises(UnexpectedDER) as e:\n            remove_sequence(data)\n\n        self.assertIn(\"Length longer\", str(e.exception))\n\n\n@st.composite\ndef st_oid(draw, max_value=2 ** 512, max_size=50):\n    \"\"\"\n    Hypothesis strategy that returns valid OBJECT IDENTIFIERs as tuples\n\n    :param max_value: maximum value of any single sub-identifier\n    :param max_size: maximum length of the generated OID\n    \"\"\"\n    first = draw(st.integers(min_value=0, max_value=2))\n    if first < 2:\n        second = draw(st.integers(min_value=0, max_value=39))\n    else:\n        second = draw(st.integers(min_value=0, max_value=max_value))\n    rest = draw(\n        st.lists(\n            st.integers(min_value=0, max_value=max_value), max_size=max_size\n        )\n    )\n    return (first, second) + tuple(rest)\n\n\n@given(st_oid())\ndef test_oids(ids):\n    encoded_oid = encode_oid(*ids)\n    decoded_oid, rest = remove_object(encoded_oid)\n    assert rest == b\"\"\n    assert decoded_oid == ids\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/ecdsa/test_ecdh.py", "content": "import os\nimport shutil\nimport subprocess\nimport pytest\nfrom binascii import unhexlify\n\ntry:\n    import unittest2 as unittest\nexcept ImportError:\n    import unittest\n\nfrom .curves import (\n    NIST192p,\n    NIST224p,\n    NIST256p,\n    NIST384p,\n    NIST521p,\n    BRAINPOOLP160r1,\n)\nfrom .curves import curves\nfrom .ecdh import (\n    ECDH,\n    InvalidCurveError,\n    InvalidSharedSecretError,\n    NoKeyError,\n    NoCurveError,\n)\nfrom .keys import SigningKey, VerifyingKey\n\n\n@pytest.mark.parametrize(\n    \"vcurve\", curves, ids=[curve.name for curve in curves]\n)\ndef test_ecdh_each(vcurve):\n    ecdh1 = ECDH(curve=vcurve)\n    ecdh2 = ECDH(curve=vcurve)\n\n    ecdh2.generate_private_key()\n    ecdh1.load_received_public_key(ecdh2.get_public_key())\n    ecdh2.load_received_public_key(ecdh1.generate_private_key())\n\n    secret1 = ecdh1.generate_sharedsecret_bytes()\n    secret2 = ecdh2.generate_sharedsecret_bytes()\n    assert secret1 == secret2\n\n\ndef test_ecdh_both_keys_present():\n    key1 = SigningKey.generate(BRAINPOOLP160r1)\n    key2 = SigningKey.generate(BRAINPOOLP160r1)\n\n    ecdh1 = ECDH(BRAINPOOLP160r1, key1, key2.verifying_key)\n    ecdh2 = ECDH(private_key=key2, public_key=key1.verifying_key)\n\n    secret1 = ecdh1.generate_sharedsecret_bytes()\n    secret2 = ecdh2.generate_sharedsecret_bytes()\n\n    assert secret1 == secret2\n\n\ndef test_ecdh_no_public_key():\n    ecdh1 = ECDH(curve=NIST192p)\n\n    with pytest.raises(NoKeyError):\n        ecdh1.generate_sharedsecret_bytes()\n\n    ecdh1.generate_private_key()\n\n    with pytest.raises(NoKeyError):\n        ecdh1.generate_sharedsecret_bytes()\n\n\nclass TestECDH(unittest.TestCase):\n    def test_load_key_from_wrong_curve(self):\n        ecdh1 = ECDH()\n        ecdh1.set_curve(NIST192p)\n\n        key1 = SigningKey.generate(BRAINPOOLP160r1)\n\n        with self.assertRaises(InvalidCurveError) as e:\n            ecdh1.load_private_key(key1)\n\n        self.assertIn(\"Curve mismatch\", str(e.exception))\n\n    def test_generate_without_curve(self):\n        ecdh1 = ECDH()\n\n        with self.assertRaises(NoCurveError) as e:\n            ecdh1.generate_private_key()\n\n        self.assertIn(\"Curve must be set\", str(e.exception))\n\n    def test_load_bytes_without_curve_set(self):\n        ecdh1 = ECDH()\n\n        with self.assertRaises(NoCurveError) as e:\n            ecdh1.load_private_key_bytes(b\"\\x01\" * 32)\n\n        self.assertIn(\"Curve must be set\", str(e.exception))\n\n    def test_set_curve_from_received_public_key(self):\n        ecdh1 = ECDH()\n\n        key1 = SigningKey.generate(BRAINPOOLP160r1)\n\n        ecdh1.load_received_public_key(key1.verifying_key)\n\n        self.assertEqual(ecdh1.curve, BRAINPOOLP160r1)\n\n\ndef test_ecdh_wrong_public_key_curve():\n    ecdh1 = ECDH(curve=NIST192p)\n    ecdh1.generate_private_key()\n    ecdh2 = ECDH(curve=NIST256p)\n    ecdh2.generate_private_key()\n\n    with pytest.raises(InvalidCurveError):\n        ecdh1.load_received_public_key(ecdh2.get_public_key())\n\n    with pytest.raises(InvalidCurveError):\n        ecdh2.load_received_public_key(ecdh1.get_public_key())\n\n    ecdh1.public_key = ecdh2.get_public_key()\n    ecdh2.public_key = ecdh1.get_public_key()\n\n    with pytest.raises(InvalidCurveError):\n        ecdh1.generate_sharedsecret_bytes()\n\n    with pytest.raises(InvalidCurveError):\n        ecdh2.generate_sharedsecret_bytes()\n\n\ndef test_ecdh_invalid_shared_secret_curve():\n    ecdh1 = ECDH(curve=NIST256p)\n    ecdh1.generate_private_key()\n\n    ecdh1.load_received_public_key(\n        SigningKey.generate(NIST256p).get_verifying_key()\n    )\n\n    ecdh1.private_key.privkey.secret_multiplier = ecdh1.private_key.curve.order\n\n    with pytest.raises(InvalidSharedSecretError):\n        ecdh1.generate_sharedsecret_bytes()\n\n\n# https://github.com/scogliani/ecc-test-vectors/blob/master/ecdh_kat/secp192r1.txt\n# https://github.com/scogliani/ecc-test-vectors/blob/master/ecdh_kat/secp256r1.txt\n# https://github.com/coruus/nist-testvectors/blob/master/csrc.nist.gov/groups/STM/cavp/documents/components/ecccdhtestvectors/KAS_ECC_CDH_PrimitiveTest.txt\n@pytest.mark.parametrize(\n    \"curve,privatekey,pubkey,secret\",\n    [\n        pytest.param(\n            NIST192p,\n            \"f17d3fea367b74d340851ca4270dcb24c271f445bed9d527\",\n            \"42ea6dd9969dd2a61fea1aac7f8e98edcc896c6e55857cc0\"\n            \"dfbe5d7c61fac88b11811bde328e8a0d12bf01a9d204b523\",\n            \"803d8ab2e5b6e6fca715737c3a82f7ce3c783124f6d51cd0\",\n            id=\"NIST192p-1\",\n        ),\n        pytest.param(\n            NIST192p,\n            \"56e853349d96fe4c442448dacb7cf92bb7a95dcf574a9bd5\",\n            \"deb5712fa027ac8d2f22c455ccb73a91e17b6512b5e030e7\"\n            \"7e2690a02cc9b28708431a29fb54b87b1f0c14e011ac2125\",\n            \"c208847568b98835d7312cef1f97f7aa298283152313c29d\",\n            id=\"NIST192p-2\",\n        ),\n        pytest.param(\n            NIST192p,\n            \"c6ef61fe12e80bf56f2d3f7d0bb757394519906d55500949\",\n            \"4edaa8efc5a0f40f843663ec5815e7762dddc008e663c20f\"\n            \"0a9f8dc67a3e60ef6d64b522185d03df1fc0adfd42478279\",\n            \"87229107047a3b611920d6e3b2c0c89bea4f49412260b8dd\",\n            id=\"NIST192p-3\",\n        ),\n        pytest.param(\n            NIST192p,\n            \"e6747b9c23ba7044f38ff7e62c35e4038920f5a0163d3cda\",\n            \"8887c276edeed3e9e866b46d58d895c73fbd80b63e382e88\"\n            \"04c5097ba6645e16206cfb70f7052655947dd44a17f1f9d5\",\n            \"eec0bed8fc55e1feddc82158fd6dc0d48a4d796aaf47d46c\",\n            id=\"NIST192p-4\",\n        ),\n        pytest.param(\n            NIST192p,\n            \"beabedd0154a1afcfc85d52181c10f5eb47adc51f655047d\",\n            \"0d045f30254adc1fcefa8a5b1f31bf4e739dd327cd18d594\"\n            \"542c314e41427c08278a08ce8d7305f3b5b849c72d8aff73\",\n            \"716e743b1b37a2cd8479f0a3d5a74c10ba2599be18d7e2f4\",\n            id=\"NIST192p-5\",\n        ),\n        pytest.param(\n            NIST192p,\n            \"cf70354226667321d6e2baf40999e2fd74c7a0f793fa8699\",\n            \"fb35ca20d2e96665c51b98e8f6eb3d79113508d8bccd4516\"\n            \"368eec0d5bfb847721df6aaff0e5d48c444f74bf9cd8a5a7\",\n            \"f67053b934459985a315cb017bf0302891798d45d0e19508\",\n            id=\"NIST192p-6\",\n        ),\n        pytest.param(\n            NIST224p,\n            \"8346a60fc6f293ca5a0d2af68ba71d1dd389e5e40837942df3e43cbd\",\n            \"af33cd0629bc7e996320a3f40368f74de8704fa37b8fab69abaae280\"\n            \"882092ccbba7930f419a8a4f9bb16978bbc3838729992559a6f2e2d7\",\n            \"7d96f9a3bd3c05cf5cc37feb8b9d5209d5c2597464dec3e9983743e8\",\n            id=\"NIST224p\",\n        ),\n        pytest.param(\n            NIST256p,\n            \"7d7dc5f71eb29ddaf80d6214632eeae03d9058af1fb6d22ed80badb62bc1a534\",\n            \"700c48f77f56584c5cc632ca65640db91b6bacce3a4df6b42ce7cc838833d287\"\n            \"db71e509e3fd9b060ddb20ba5c51dcc5948d46fbf640dfe0441782cab85fa4ac\",\n            \"46fc62106420ff012e54a434fbdd2d25ccc5852060561e68040dd7778997bd7b\",\n            id=\"NIST256p-1\",\n        ),\n        pytest.param(\n            NIST256p,\n            \"38f65d6dce47676044d58ce5139582d568f64bb16098d179dbab07741dd5caf5\",\n            \"809f04289c64348c01515eb03d5ce7ac1a8cb9498f5caa50197e58d43a86a7ae\"\n            \"b29d84e811197f25eba8f5194092cb6ff440e26d4421011372461f579271cda3\",\n            \"057d636096cb80b67a8c038c890e887d1adfa4195e9b3ce241c8a778c59cda67\",\n            id=\"NIST256p-2\",\n        ),\n        pytest.param(\n            NIST256p,\n            \"1accfaf1b97712b85a6f54b148985a1bdc4c9bec0bd258cad4b3d603f49f32c8\",\n            \"a2339c12d4a03c33546de533268b4ad667debf458b464d77443636440ee7fec3\"\n            \"ef48a3ab26e20220bcda2c1851076839dae88eae962869a497bf73cb66faf536\",\n            \"2d457b78b4614132477618a5b077965ec90730a8c81a1c75d6d4ec68005d67ec\",\n            id=\"NIST256p-3\",\n        ),\n        pytest.param(\n            NIST256p,\n            \"207c43a79bfee03db6f4b944f53d2fb76cc49ef1c9c4d34d51b6c65c4db6932d\",\n            \"df3989b9fa55495719b3cf46dccd28b5153f7808191dd518eff0c3cff2b705ed\"\n            \"422294ff46003429d739a33206c8752552c8ba54a270defc06e221e0feaf6ac4\",\n            \"96441259534b80f6aee3d287a6bb17b5094dd4277d9e294f8fe73e48bf2a0024\",\n            id=\"NIST256p-4\",\n        ),\n        pytest.param(\n            NIST256p,\n            \"59137e38152350b195c9718d39673d519838055ad908dd4757152fd8255c09bf\",\n            \"41192d2813e79561e6a1d6f53c8bc1a433a199c835e141b05a74a97b0faeb922\"\n            \"1af98cc45e98a7e041b01cf35f462b7562281351c8ebf3ffa02e33a0722a1328\",\n            \"19d44c8d63e8e8dd12c22a87b8cd4ece27acdde04dbf47f7f27537a6999a8e62\",\n            id=\"NIST256p-5\",\n        ),\n        pytest.param(\n            NIST256p,\n            \"f5f8e0174610a661277979b58ce5c90fee6c9b3bb346a90a7196255e40b132ef\",\n            \"33e82092a0f1fb38f5649d5867fba28b503172b7035574bf8e5b7100a3052792\"\n            \"f2cf6b601e0a05945e335550bf648d782f46186c772c0f20d3cd0d6b8ca14b2f\",\n            \"664e45d5bba4ac931cd65d52017e4be9b19a515f669bea4703542a2c525cd3d3\",\n            id=\"NIST256p-6\",\n        ),\n        pytest.param(\n            NIST384p,\n            \"3cc3122a68f0d95027ad38c067916ba0eb8c38894d22e1b1\"\n            \"5618b6818a661774ad463b205da88cf699ab4d43c9cf98a1\",\n            \"a7c76b970c3b5fe8b05d2838ae04ab47697b9eaf52e76459\"\n            \"2efda27fe7513272734466b400091adbf2d68c58e0c50066\"\n            \"ac68f19f2e1cb879aed43a9969b91a0839c4c38a49749b66\"\n            \"1efedf243451915ed0905a32b060992b468c64766fc8437a\",\n            \"5f9d29dc5e31a163060356213669c8ce132e22f57c9a04f4\"\n            \"0ba7fcead493b457e5621e766c40a2e3d4d6a04b25e533f1\",\n            id=\"NIST384p\",\n        ),\n        pytest.param(\n            NIST521p,\n            \"017eecc07ab4b329068fba65e56a1f8890aa935e57134ae0ffcce802735151f4ea\"\n            \"c6564f6ee9974c5e6887a1fefee5743ae2241bfeb95d5ce31ddcb6f9edb4d6fc47\",\n            \"00685a48e86c79f0f0875f7bc18d25eb5fc8c0b07e5da4f4370f3a949034085433\"\n            \"4b1e1b87fa395464c60626124a4e70d0f785601d37c09870ebf176666877a2046d\"\n            \"01ba52c56fc8776d9e8f5db4f0cc27636d0b741bbe05400697942e80b739884a83\"\n            \"bde99e0f6716939e632bc8986fa18dccd443a348b6c3e522497955a4f3c302f676\",\n            \"005fc70477c3e63bc3954bd0df3ea0d1f41ee21746ed95fc5e1fdf90930d5e1366\"\n            \"72d72cc770742d1711c3c3a4c334a0ad9759436a4d3c5bf6e74b9578fac148c831\",\n            id=\"NIST521p\",\n        ),\n    ],\n)\ndef test_ecdh_NIST(curve, privatekey, pubkey, secret):\n    ecdh = ECDH(curve=curve)\n    ecdh.load_private_key_bytes(unhexlify(privatekey))\n    ecdh.load_received_public_key_bytes(unhexlify(pubkey))\n\n    sharedsecret = ecdh.generate_sharedsecret_bytes()\n\n    assert sharedsecret == unhexlify(secret)\n\n\npem_local_private_key = (\n    \"-----BEGIN EC PRIVATE KEY-----\\n\"\n    \"MF8CAQEEGF7IQgvW75JSqULpiQQ8op9WH6Uldw6xxaAKBggqhkjOPQMBAaE0AzIA\\n\"\n    \"BLiBd9CE7xf15FY5QIAoNg+fWbSk1yZOYtoGUdzkejWkxbRc9RWTQjqLVXucIJnz\\n\"\n    \"bA==\\n\"\n    \"-----END EC PRIVATE KEY-----\\n\"\n)\nder_local_private_key = (\n    \"305f02010104185ec8420bd6ef9252a942e989043ca29f561fa525770eb1c5a00a06082a864\"\n    \"8ce3d030101a13403320004b88177d084ef17f5e45639408028360f9f59b4a4d7264e62da06\"\n    \"51dce47a35a4c5b45cf51593423a8b557b9c2099f36c\"\n)\npem_remote_public_key = (\n    \"-----BEGIN PUBLIC KEY-----\\n\"\n    \"MEkwEwYHKoZIzj0CAQYIKoZIzj0DAQEDMgAEuIF30ITvF/XkVjlAgCg2D59ZtKTX\\n\"\n    \"Jk5i2gZR3OR6NaTFtFz1FZNCOotVe5wgmfNs\\n\"\n    \"-----END PUBLIC KEY-----\\n\"\n)\nder_remote_public_key = (\n    \"3049301306072a8648ce3d020106082a8648ce3d03010103320004b88177d084ef17f5e4563\"\n    \"9408028360f9f59b4a4d7264e62da0651dce47a35a4c5b45cf51593423a8b557b9c2099f36c\"\n)\ngshared_secret = \"8f457e34982478d1c34b9cd2d0c15911b72dd60d869e2cea\"\n\n\ndef test_ecdh_pem():\n    ecdh = ECDH()\n    ecdh.load_private_key_pem(pem_local_private_key)\n    ecdh.load_received_public_key_pem(pem_remote_public_key)\n\n    sharedsecret = ecdh.generate_sharedsecret_bytes()\n\n    assert sharedsecret == unhexlify(gshared_secret)\n\n\ndef test_ecdh_der():\n    ecdh = ECDH()\n    ecdh.load_private_key_der(unhexlify(der_local_private_key))\n    ecdh.load_received_public_key_der(unhexlify(der_remote_public_key))\n\n    sharedsecret = ecdh.generate_sharedsecret_bytes()\n\n    assert sharedsecret == unhexlify(gshared_secret)\n\n\n# Exception classes used by run_openssl.\nclass RunOpenSslError(Exception):\n    pass\n\n\ndef run_openssl(cmd):\n    OPENSSL = \"openssl\"\n    p = subprocess.Popen(\n        [OPENSSL] + cmd.split(),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n    )\n    stdout, ignored = p.communicate()\n    if p.returncode != 0:\n        raise RunOpenSslError(\n            \"cmd '%s %s' failed: rc=%s, stdout/err was %s\"\n            % (OPENSSL, cmd, p.returncode, stdout)\n        )\n    return stdout.decode()\n\n\nOPENSSL_SUPPORTED_CURVES = set(\n    c.split(\":\")[0].strip()\n    for c in run_openssl(\"ecparam -list_curves\").split(\"\\n\")\n)\n\n\n@pytest.mark.parametrize(\n    \"vcurve\", curves, ids=[curve.name for curve in curves]\n)\ndef test_ecdh_with_openssl(vcurve):\n    assert vcurve.openssl_name\n\n    if vcurve.openssl_name not in OPENSSL_SUPPORTED_CURVES:\n        pytest.skip(\"system openssl does not support \" + vcurve.openssl_name)\n\n    try:\n        hlp = run_openssl(\"pkeyutl -help\")\n        if hlp.find(\"-derive\") == 0:  # pragma: no cover\n            pytest.skip(\"system openssl does not support `pkeyutl -derive`\")\n    except RunOpenSslError:  # pragma: no cover\n        pytest.skip(\"system openssl could not be executed\")\n\n    if os.path.isdir(\"t\"):  # pragma: no branch\n        shutil.rmtree(\"t\")\n    os.mkdir(\"t\")\n    run_openssl(\n        \"ecparam -name %s -genkey -out t/privkey1.pem\" % vcurve.openssl_name\n    )\n    run_openssl(\n        \"ecparam -name %s -genkey -out t/privkey2.pem\" % vcurve.openssl_name\n    )\n    run_openssl(\"ec -in t/privkey1.pem -pubout -out t/pubkey1.pem\")\n\n    ecdh1 = ECDH(curve=vcurve)\n    ecdh2 = ECDH(curve=vcurve)\n    with open(\"t/privkey1.pem\") as e:\n        key = e.read()\n    ecdh1.load_private_key_pem(key)\n    with open(\"t/privkey2.pem\") as e:\n        key = e.read()\n    ecdh2.load_private_key_pem(key)\n\n    with open(\"t/pubkey1.pem\") as e:\n        key = e.read()\n    vk1 = VerifyingKey.from_pem(key)\n    assert vk1.to_string() == ecdh1.get_public_key().to_string()\n    vk2 = ecdh2.get_public_key()\n    with open(\"t/pubkey2.pem\", \"wb\") as e:\n        e.write(vk2.to_pem())\n\n    ecdh1.load_received_public_key(vk2)\n    ecdh2.load_received_public_key(vk1)\n    secret1 = ecdh1.generate_sharedsecret_bytes()\n    secret2 = ecdh2.generate_sharedsecret_bytes()\n\n    assert secret1 == secret2\n\n    run_openssl(\n        \"pkeyutl -derive -inkey t/privkey1.pem -peerkey t/pubkey2.pem -out t/secret1\"\n    )\n    run_openssl(\n        \"pkeyutl -derive -inkey t/privkey2.pem -peerkey t/pubkey1.pem -out t/secret2\"\n    )\n\n    with open(\"t/secret1\", \"rb\") as e:\n        ssl_secret1 = e.read()\n    with open(\"t/secret1\", \"rb\") as e:\n        ssl_secret2 = e.read()\n\n    assert len(ssl_secret1) == vk1.curve.verifying_key_length // 2\n    assert len(secret1) == vk1.curve.verifying_key_length // 2\n\n    assert ssl_secret1 == ssl_secret2\n    assert secret1 == ssl_secret1\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/ecdsa/test_numbertheory.py", "content": "import operator\nfrom functools import reduce\n\ntry:\n    import unittest2 as unittest\nexcept ImportError:\n    import unittest\nimport hypothesis.strategies as st\nimport pytest\nfrom hypothesis import given, settings, example\n\ntry:\n    from hypothesis import HealthCheck\n\n    HC_PRESENT = True\nexcept ImportError:  # pragma: no cover\n    HC_PRESENT = False\nfrom .numbertheory import (\n    SquareRootError,\n    factorization,\n    gcd,\n    lcm,\n    jacobi,\n    inverse_mod,\n    is_prime,\n    next_prime,\n    smallprimes,\n    square_root_mod_prime,\n)\n\n\nBIGPRIMES = (\n    999671,\n    999683,\n    999721,\n    999727,\n    999749,\n    999763,\n    999769,\n    999773,\n    999809,\n    999853,\n    999863,\n    999883,\n    999907,\n    999917,\n    999931,\n    999953,\n    999959,\n    999961,\n    999979,\n    999983,\n)\n\n\n@pytest.mark.parametrize(\n    \"prime, next_p\", [(p, q) for p, q in zip(BIGPRIMES[:-1], BIGPRIMES[1:])]\n)\ndef test_next_prime(prime, next_p):\n    assert next_prime(prime) == next_p\n\n\n@pytest.mark.parametrize(\"val\", [-1, 0, 1])\ndef test_next_prime_with_nums_less_2(val):\n    assert next_prime(val) == 2\n\n\n@pytest.mark.parametrize(\"prime\", smallprimes)\ndef test_square_root_mod_prime_for_small_primes(prime):\n    squares = set()\n    for num in range(0, 1 + prime // 2):\n        sq = num * num % prime\n        squares.add(sq)\n        root = square_root_mod_prime(sq, prime)\n        # tested for real with TestNumbertheory.test_square_root_mod_prime\n        assert root * root % prime == sq\n\n    for nonsquare in range(0, prime):\n        if nonsquare in squares:\n            continue\n        with pytest.raises(SquareRootError):\n            square_root_mod_prime(nonsquare, prime)\n\n\ndef test_square_root_mod_prime_for_2():\n    a = square_root_mod_prime(1, 2)\n    assert a == 1\n\n\ndef test_square_root_mod_prime_for_small_prime():\n    root = square_root_mod_prime(98 ** 2 % 101, 101)\n    assert root * root % 101 == 9\n\n\ndef test_square_root_mod_prime_for_p_congruent_5():\n    p = 13\n    assert p % 8 == 5\n\n    root = square_root_mod_prime(3, p)\n    assert root * root % p == 3\n\n\ndef test_square_root_mod_prime_for_p_congruent_5_large_d():\n    p = 29\n    assert p % 8 == 5\n\n    root = square_root_mod_prime(4, p)\n    assert root * root % p == 4\n\n\n@st.composite\ndef st_two_nums_rel_prime(draw):\n    # 521-bit is the biggest curve we operate on, use 1024 for a bit\n    # of breathing space\n    mod = draw(st.integers(min_value=2, max_value=2 ** 1024))\n    num = draw(\n        st.integers(min_value=1, max_value=mod - 1).filter(\n            lambda x: gcd(x, mod) == 1\n        )\n    )\n    return num, mod\n\n\n@st.composite\ndef st_primes(draw, *args, **kwargs):\n    if \"min_value\" not in kwargs:  # pragma: no branch\n        kwargs[\"min_value\"] = 1\n    prime = draw(\n        st.sampled_from(smallprimes)\n        | st.integers(*args, **kwargs).filter(is_prime)\n    )\n    return prime\n\n\n@st.composite\ndef st_num_square_prime(draw):\n    prime = draw(st_primes(max_value=2 ** 1024))\n    num = draw(st.integers(min_value=0, max_value=1 + prime // 2))\n    sq = num * num % prime\n    return sq, prime\n\n\n@st.composite\ndef st_comp_with_com_fac(draw):\n    \"\"\"\n    Strategy that returns lists of numbers, all having a common factor.\n    \"\"\"\n    primes = draw(\n        st.lists(st_primes(max_value=2 ** 512), min_size=1, max_size=10)\n    )\n    # select random prime(s) that will make the common factor of composites\n    com_fac_primes = draw(\n        st.lists(st.sampled_from(primes), min_size=1, max_size=20)\n    )\n    com_fac = reduce(operator.mul, com_fac_primes, 1)\n\n    # select at most 20 lists (returned numbers),\n    # each having at most 30 primes (factors) including none (then the number\n    # will be 1)\n    comp_primes = draw(\n        st.integers(min_value=1, max_value=20).flatmap(\n            lambda n: st.lists(\n                st.lists(st.sampled_from(primes), max_size=30),\n                min_size=1,\n                max_size=n,\n            )\n        )\n    )\n\n    return [reduce(operator.mul, nums, 1) * com_fac for nums in comp_primes]\n\n\n@st.composite\ndef st_comp_no_com_fac(draw):\n    \"\"\"\n    Strategy that returns lists of numbers that don't have a common factor.\n    \"\"\"\n    primes = draw(\n        st.lists(\n            st_primes(max_value=2 ** 512), min_size=2, max_size=10, unique=True\n        )\n    )\n    # first select the primes that will create the uncommon factor\n    # between returned numbers\n    uncom_fac_primes = draw(\n        st.lists(\n            st.sampled_from(primes),\n            min_size=1,\n            max_size=len(primes) - 1,\n            unique=True,\n        )\n    )\n    uncom_fac = reduce(operator.mul, uncom_fac_primes, 1)\n\n    # then build composites from leftover primes\n    leftover_primes = [i for i in primes if i not in uncom_fac_primes]\n\n    assert leftover_primes\n    assert uncom_fac_primes\n\n    # select at most 20 lists, each having at most 30 primes\n    # selected from the leftover_primes list\n    number_primes = draw(\n        st.integers(min_value=1, max_value=20).flatmap(\n            lambda n: st.lists(\n                st.lists(st.sampled_from(leftover_primes), max_size=30),\n                min_size=1,\n                max_size=n,\n            )\n        )\n    )\n\n    numbers = [reduce(operator.mul, nums, 1) for nums in number_primes]\n\n    insert_at = draw(st.integers(min_value=0, max_value=len(numbers)))\n    numbers.insert(insert_at, uncom_fac)\n    return numbers\n\n\nHYP_SETTINGS = {}\nif HC_PRESENT:  # pragma: no branch\n    HYP_SETTINGS[\"suppress_health_check\"] = [\n        HealthCheck.filter_too_much,\n        HealthCheck.too_slow,\n    ]\n    # the factorization() sometimes takes a long time to finish\n    HYP_SETTINGS[\"deadline\"] = 5000\n\n\nHYP_SLOW_SETTINGS = dict(HYP_SETTINGS)\nHYP_SLOW_SETTINGS[\"max_examples\"] = 10\n\n\nclass TestIsPrime(unittest.TestCase):\n    def test_very_small_prime(self):\n        assert is_prime(23)\n\n    def test_very_small_composite(self):\n        assert not is_prime(22)\n\n    def test_small_prime(self):\n        assert is_prime(123456791)\n\n    def test_special_composite(self):\n        assert not is_prime(10261)\n\n    def test_medium_prime_1(self):\n        # nextPrime[2^256]\n        assert is_prime(2 ** 256 + 0x129)\n\n    def test_medium_prime_2(self):\n        # nextPrime(2^256+0x129)\n        assert is_prime(2 ** 256 + 0x12D)\n\n    def test_medium_trivial_composite(self):\n        assert not is_prime(2 ** 256 + 0x130)\n\n    def test_medium_non_trivial_composite(self):\n        assert not is_prime(2 ** 256 + 0x12F)\n\n    def test_large_prime(self):\n        # nextPrime[2^2048]\n        assert is_prime(2 ** 2048 + 0x3D5)\n\n\nclass TestNumbertheory(unittest.TestCase):\n    def test_gcd(self):\n        assert gcd(3 * 5 * 7, 3 * 5 * 11, 3 * 5 * 13) == 3 * 5\n        assert gcd([3 * 5 * 7, 3 * 5 * 11, 3 * 5 * 13]) == 3 * 5\n        assert gcd(3) == 3\n\n    @unittest.skipUnless(\n        HC_PRESENT,\n        \"Hypothesis 2.0.0 can't be made tolerant of hard to \"\n        \"meet requirements (like `is_prime()`), the test \"\n        \"case times-out on it\",\n    )\n    @settings(**HYP_SLOW_SETTINGS)\n    @given(st_comp_with_com_fac())\n    def test_gcd_with_com_factor(self, numbers):\n        n = gcd(numbers)\n        assert 1 in numbers or n != 1\n        for i in numbers:\n            assert i % n == 0\n\n    @unittest.skipUnless(\n        HC_PRESENT,\n        \"Hypothesis 2.0.0 can't be made tolerant of hard to \"\n        \"meet requirements (like `is_prime()`), the test \"\n        \"case times-out on it\",\n    )\n    @settings(**HYP_SLOW_SETTINGS)\n    @given(st_comp_no_com_fac())\n    def test_gcd_with_uncom_factor(self, numbers):\n        n = gcd(numbers)\n        assert n == 1\n\n    @given(\n        st.lists(\n            st.integers(min_value=1, max_value=2 ** 8192),\n            min_size=1,\n            max_size=20,\n        )\n    )\n    def test_gcd_with_random_numbers(self, numbers):\n        n = gcd(numbers)\n        for i in numbers:\n            # check that at least it's a divider\n            assert i % n == 0\n\n    def test_lcm(self):\n        assert lcm(3, 5 * 3, 7 * 3) == 3 * 5 * 7\n        assert lcm([3, 5 * 3, 7 * 3]) == 3 * 5 * 7\n        assert lcm(3) == 3\n\n    @given(\n        st.lists(\n            st.integers(min_value=1, max_value=2 ** 8192),\n            min_size=1,\n            max_size=20,\n        )\n    )\n    def test_lcm_with_random_numbers(self, numbers):\n        n = lcm(numbers)\n        for i in numbers:\n            assert n % i == 0\n\n    @unittest.skipUnless(\n        HC_PRESENT,\n        \"Hypothesis 2.0.0 can't be made tolerant of hard to \"\n        \"meet requirements (like `is_prime()`), the test \"\n        \"case times-out on it\",\n    )\n    @settings(**HYP_SETTINGS)\n    @given(st_num_square_prime())\n    def test_square_root_mod_prime(self, vals):\n        square, prime = vals\n\n        calc = square_root_mod_prime(square, prime)\n        assert calc * calc % prime == square\n\n    @settings(**HYP_SETTINGS)\n    @given(st.integers(min_value=1, max_value=10 ** 12))\n    @example(265399 * 1526929)\n    @example(373297 ** 2 * 553991)\n    def test_factorization(self, num):\n        factors = factorization(num)\n        mult = 1\n        for i in factors:\n            mult *= i[0] ** i[1]\n        assert mult == num\n\n    def test_factorisation_smallprimes(self):\n        exp = 101 * 103\n        assert 101 in smallprimes\n        assert 103 in smallprimes\n        factors = factorization(exp)\n        mult = 1\n        for i in factors:\n            mult *= i[0] ** i[1]\n        assert mult == exp\n\n    def test_factorisation_not_smallprimes(self):\n        exp = 1231 * 1237\n        assert 1231 not in smallprimes\n        assert 1237 not in smallprimes\n        factors = factorization(exp)\n        mult = 1\n        for i in factors:\n            mult *= i[0] ** i[1]\n        assert mult == exp\n\n    def test_jacobi_with_zero(self):\n        assert jacobi(0, 3) == 0\n\n    def test_jacobi_with_one(self):\n        assert jacobi(1, 3) == 1\n\n    @settings(**HYP_SETTINGS)\n    @given(st.integers(min_value=3, max_value=1000).filter(lambda x: x % 2))\n    def test_jacobi(self, mod):\n        if is_prime(mod):\n            squares = set()\n            for root in range(1, mod):\n                assert jacobi(root * root, mod) == 1\n                squares.add(root * root % mod)\n            for i in range(1, mod):\n                if i not in squares:\n                    assert jacobi(i, mod) == -1\n        else:\n            factors = factorization(mod)\n            for a in range(1, mod):\n                c = 1\n                for i in factors:\n                    c *= jacobi(a, i[0]) ** i[1]\n                assert c == jacobi(a, mod)\n\n    @given(st_two_nums_rel_prime())\n    def test_inverse_mod(self, nums):\n        num, mod = nums\n\n        inv = inverse_mod(num, mod)\n\n        assert 0 < inv < mod\n        assert num * inv % mod == 1\n\n    def test_inverse_mod_with_zero(self):\n        assert 0 == inverse_mod(0, 11)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/aiosqlite/tests/smoke.py", "content": "# Copyright 2018 John Reese\n# Licensed under the MIT license\nimport asyncio\nimport sqlite3\nimport sys\nfrom pathlib import Path\nfrom sqlite3 import OperationalError\nfrom threading import Thread\nfrom unittest import SkipTest, skipIf, skipUnless\n\nimport aiounittest\n\nimport aiosqlite\nfrom .helpers import setup_logger\n\nTEST_DB = Path(\"test.db\")\n\n# pypy uses non-standard text factory for low-level sqlite implementation\ntry:\n    from _sqlite3 import _unicode_text_factory as default_text_factory\nexcept ImportError:\n    default_text_factory = str\n\n\nclass SmokeTest(aiounittest.AsyncTestCase):\n    @classmethod\n    def setUpClass(cls):\n        setup_logger()\n\n    def setUp(self):\n        if TEST_DB.exists():\n            TEST_DB.unlink()\n\n    def tearDown(self):\n        if TEST_DB.exists():\n            TEST_DB.unlink()\n\n    async def test_connection_await(self):\n        db = await aiosqlite.connect(TEST_DB)\n        self.assertIsInstance(db, aiosqlite.Connection)\n\n        async with db.execute(\"select 1, 2\") as cursor:\n            rows = await cursor.fetchall()\n            self.assertEqual(rows, [(1, 2)])\n\n        await db.close()\n\n    async def test_connection_context(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            self.assertIsInstance(db, aiosqlite.Connection)\n\n            async with db.execute(\"select 1, 2\") as cursor:\n                rows = await cursor.fetchall()\n                self.assertEqual(rows, [(1, 2)])\n\n    async def test_connection_locations(self):\n        class Fake:  # pylint: disable=too-few-public-methods\n            def __str__(self):\n                return str(TEST_DB)\n\n        locs = (\"test.db\", b\"test.db\", Path(\"test.db\"), Fake())\n\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.execute(\"create table foo (i integer, k integer)\")\n            await db.execute(\"insert into foo (i, k) values (1, 5)\")\n            await db.commit()\n\n            cursor = await db.execute(\"select * from foo\")\n            rows = await cursor.fetchall()\n\n        for loc in locs:\n            async with aiosqlite.connect(loc) as db:\n                cursor = await db.execute(\"select * from foo\")\n                self.assertEqual(await cursor.fetchall(), rows)\n\n    async def test_multiple_connections(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.execute(\n                \"create table multiple_connections \"\n                \"(i integer primary key asc, k integer)\"\n            )\n\n        async def do_one_conn(i):\n            async with aiosqlite.connect(TEST_DB) as db:\n                await db.execute(\"insert into multiple_connections (k) values (?)\", [i])\n                await db.commit()\n\n        await asyncio.gather(*[do_one_conn(i) for i in range(10)])\n\n        async with aiosqlite.connect(TEST_DB) as db:\n            cursor = await db.execute(\"select * from multiple_connections\")\n            rows = await cursor.fetchall()\n\n        assert len(rows) == 10\n\n    async def test_multiple_queries(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.execute(\n                \"create table multiple_queries \"\n                \"(i integer primary key asc, k integer)\"\n            )\n\n            await asyncio.gather(\n                *[\n                    db.execute(\"insert into multiple_queries (k) values (?)\", [i])\n                    for i in range(10)\n                ]\n            )\n\n            await db.commit()\n\n        async with aiosqlite.connect(TEST_DB) as db:\n            cursor = await db.execute(\"select * from multiple_queries\")\n            rows = await cursor.fetchall()\n\n        assert len(rows) == 10\n\n    async def test_iterable_cursor(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            cursor = await db.cursor()\n            await cursor.execute(\n                \"create table iterable_cursor \" \"(i integer primary key asc, k integer)\"\n            )\n            await cursor.executemany(\n                \"insert into iterable_cursor (k) values (?)\", [[i] for i in range(10)]\n            )\n            await db.commit()\n\n        async with aiosqlite.connect(TEST_DB) as db:\n            cursor = await db.execute(\"select * from iterable_cursor\")\n            rows = []\n            async for row in cursor:\n                rows.append(row)\n\n        assert len(rows) == 10\n\n    async def test_multi_loop_usage(self):\n        results = {}\n\n        def runner(k, conn):\n            async def query():\n                async with conn.execute(\"select * from foo\") as cursor:\n                    rows = await cursor.fetchall()\n                    self.assertEqual(len(rows), 2)\n                    return rows\n\n            with self.subTest(k):\n                loop = asyncio.new_event_loop()\n                rows = loop.run_until_complete(query())\n                loop.close()\n                results[k] = rows\n\n        async with aiosqlite.connect(\":memory:\") as db:\n            await db.execute(\"create table foo (id int, name varchar)\")\n            await db.execute(\n                \"insert into foo values (?, ?), (?, ?)\", (1, \"Sally\", 2, \"Janet\")\n            )\n            await db.commit()\n\n            threads = [Thread(target=runner, args=(k, db)) for k in range(4)]\n            for thread in threads:\n                thread.start()\n            for thread in threads:\n                thread.join()\n\n        self.assertEqual(len(results), 4)\n        for rows in results.values():\n            self.assertEqual(len(rows), 2)\n\n    async def test_context_cursor(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            async with db.cursor() as cursor:\n                await cursor.execute(\n                    \"create table context_cursor \"\n                    \"(i integer primary key asc, k integer)\"\n                )\n                await cursor.executemany(\n                    \"insert into context_cursor (k) values (?)\",\n                    [[i] for i in range(10)],\n                )\n                await db.commit()\n\n        async with aiosqlite.connect(TEST_DB) as db:\n            async with db.execute(\"select * from context_cursor\") as cursor:\n                rows = []\n                async for row in cursor:\n                    rows.append(row)\n\n        assert len(rows) == 10\n\n    async def test_cursor_return_self(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            cursor = await db.cursor()\n\n            result = await cursor.execute(\n                \"create table test_cursor_return_self (i integer, k integer)\"\n            )\n            self.assertEqual(result, cursor, \"cursor execute returns itself\")\n\n            result = await cursor.executemany(\n                \"insert into test_cursor_return_self values (?, ?)\", [(1, 1), (2, 2)]\n            )\n            self.assertEqual(result, cursor)\n\n            result = await cursor.executescript(\n                \"insert into test_cursor_return_self values (3, 3);\"\n                \"insert into test_cursor_return_self values (4, 4);\"\n                \"insert into test_cursor_return_self values (5, 5);\"\n            )\n            self.assertEqual(result, cursor)\n\n    async def test_connection_properties(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            self.assertEqual(db.total_changes, 0)\n\n            async with db.cursor() as cursor:\n                self.assertFalse(db.in_transaction)\n                await cursor.execute(\n                    \"create table test_properties \"\n                    \"(i integer primary key asc, k integer, d text)\"\n                )\n                await cursor.execute(\n                    \"insert into test_properties (k, d) values (1, 'hi')\"\n                )\n                self.assertTrue(db.in_transaction)\n                await db.commit()\n                self.assertFalse(db.in_transaction)\n\n            self.assertEqual(db.total_changes, 1)\n\n            self.assertIsNone(db.row_factory)\n            self.assertEqual(db.text_factory, default_text_factory)\n\n            async with db.cursor() as cursor:\n                await cursor.execute(\"select * from test_properties\")\n                row = await cursor.fetchone()\n                self.assertIsInstance(row, tuple)\n                self.assertEqual(row, (1, 1, \"hi\"))\n                with self.assertRaises(TypeError):\n                    _ = row[\"k\"]\n\n            db.row_factory = aiosqlite.Row\n            db.text_factory = bytes\n            self.assertEqual(db.row_factory, aiosqlite.Row)\n            self.assertEqual(db.text_factory, bytes)\n\n            async with db.cursor() as cursor:\n                await cursor.execute(\"select * from test_properties\")\n                row = await cursor.fetchone()\n                self.assertIsInstance(row, aiosqlite.Row)\n                self.assertEqual(row[1], 1)\n                self.assertEqual(row[2], b\"hi\")\n                self.assertEqual(row[\"k\"], 1)\n                self.assertEqual(row[\"d\"], b\"hi\")\n\n    async def test_fetch_all(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.execute(\n                \"create table test_fetch_all (i integer primary key asc, k integer)\"\n            )\n            await db.execute(\n                \"insert into test_fetch_all (k) values (10), (24), (16), (32)\"\n            )\n            await db.commit()\n\n        async with aiosqlite.connect(TEST_DB) as db:\n            cursor = await db.execute(\"select k from test_fetch_all where k < 30\")\n            rows = await cursor.fetchall()\n            self.assertEqual(rows, [(10,), (24,), (16,)])\n\n    async def test_enable_load_extension(self):\n        \"\"\"Assert that after enabling extension loading, they can be loaded\"\"\"\n        async with aiosqlite.connect(TEST_DB) as db:\n            try:\n                await db.enable_load_extension(True)\n                await db.load_extension(\"test\")\n            except OperationalError as e:\n                assert \"not authorized\" not in e.args\n            except AttributeError:\n                raise SkipTest(\n                    \"python was not compiled with sqlite3 \"\n                    \"extension support, so we can't test it\"\n                )\n\n    async def test_set_progress_handler(self):\n        \"\"\"Assert that after setting a progress handler returning 1, DB operations are aborted\"\"\"\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.set_progress_handler(lambda: 1, 1)\n            with self.assertRaises(OperationalError):\n                await db.execute(\n                    \"create table test_progress_handler (i integer primary key asc, k integer)\"\n                )\n\n    async def test_create_function(self):\n        \"\"\"Assert that after creating a custom function, it can be used\"\"\"\n\n        def no_arg():\n            return \"no arg\"\n\n        def one_arg(num):\n            return num * 2\n\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.create_function(\"no_arg\", 0, no_arg)\n            await db.create_function(\"one_arg\", 1, one_arg)\n\n            async with db.execute(\"SELECT no_arg();\") as res:\n                row = await res.fetchone()\n                self.assertEqual(row[0], \"no arg\")\n\n            async with db.execute(\"SELECT one_arg(10);\") as res:\n                row = await res.fetchone()\n                self.assertEqual(row[0], 20)\n\n    @skipUnless(sys.version_info < (3, 8), \"Python < 3.8 specific behaviour\")\n    async def test_create_function_deterministic_pre38(self):\n        \"\"\"Make sure the deterministic parameter cannot be used in old Python versions\"\"\"\n\n        def one_arg(num):\n            return num * 2\n\n        async with aiosqlite.connect(TEST_DB) as db:\n            with self.assertWarnsRegex(UserWarning, \"registered as non-deterministic\"):\n                await db.create_function(\"one_arg\", 1, one_arg, deterministic=True)\n\n            await db.execute(\"create table foo (id int, bar int)\")\n\n            # Deterministic parameter is only available in Python 3.8+ so this\n            # won't be deterministic\n            with self.assertRaisesRegex(\n                OperationalError,\n                \"non-deterministic functions prohibited in index expressions\",\n            ):\n                await db.execute(\"create index t on foo(one_arg(bar))\")\n\n    @skipIf(sys.version_info < (3, 8), \"Python 3.8+ specific behaviour\")\n    async def test_create_function_deterministic_post38(self):\n        \"\"\"Assert that after creating a deterministic custom function, it can be used.\n\n        https://sqlite.org/deterministic.html\n        \"\"\"\n\n        def one_arg(num):\n            return num * 2\n\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.create_function(\"one_arg\", 1, one_arg, deterministic=True)\n            await db.execute(\"create table foo (id int, bar int)\")\n\n            # Non-deterministic functions cannot be used in indexes\n            await db.execute(\"create index t on foo(one_arg(bar))\")\n\n    async def test_set_trace_callback(self):\n        statements = []\n\n        def callback(statement: str):\n            statements.append(statement)\n\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.set_trace_callback(callback)\n\n            await db.execute(\"select 10\")\n            self.assertIn(\"select 10\", statements)\n\n    async def test_connect_error(self):\n        bad_db = Path(\"/something/that/shouldnt/exist.db\")\n        with self.assertRaisesRegex(OperationalError, \"unable to open database\"):\n            async with aiosqlite.connect(bad_db) as db:\n                self.assertIsNone(db)  # should never be reached\n\n        with self.assertRaisesRegex(OperationalError, \"unable to open database\"):\n            db = await aiosqlite.connect(bad_db)\n            self.assertIsNone(db)  # should never be reached\n\n    async def test_iterdump(self):\n        async with aiosqlite.connect(\":memory:\") as db:\n            await db.execute(\"create table foo (i integer, k charvar(250))\")\n            await db.executemany(\n                \"insert into foo values (?, ?)\", [(1, \"hello\"), (2, \"world\")]\n            )\n\n            lines = [line async for line in db.iterdump()]\n            self.assertEqual(\n                lines,\n                [\n                    \"BEGIN TRANSACTION;\",\n                    \"CREATE TABLE foo (i integer, k charvar(250));\",\n                    \"INSERT INTO \\\"foo\\\" VALUES(1,'hello');\",\n                    \"INSERT INTO \\\"foo\\\" VALUES(2,'world');\",\n                    \"COMMIT;\",\n                ],\n            )\n\n    async def test_cursor_on_closed_connection(self):\n        db = await aiosqlite.connect(TEST_DB)\n\n        cursor = await db.execute(\"select 1, 2\")\n        await db.close()\n        with self.assertRaisesRegex(ValueError, \"Connection closed\"):\n            await cursor.fetchall()\n        with self.assertRaisesRegex(ValueError, \"Connection closed\"):\n            await cursor.fetchall()\n\n    async def test_cursor_on_closed_connection_loop(self):\n        db = await aiosqlite.connect(TEST_DB)\n\n        cursor = await db.execute(\"select 1, 2\")\n        tasks = []\n        for i in range(100):\n            if i == 50:\n                tasks.append(asyncio.ensure_future(db.close()))\n            tasks.append(asyncio.ensure_future(cursor.fetchall()))\n        for task in tasks:\n            try:\n                await task\n            except sqlite3.ProgrammingError:\n                pass\n\n    @skipIf(sys.version_info < (3, 7), \"Test backup() on 3.7+\")\n    async def test_backup_aiosqlite(self):\n        def progress(a, b, c):\n            print(a, b, c)\n\n        async with aiosqlite.connect(\":memory:\") as db1, aiosqlite.connect(\n            \":memory:\"\n        ) as db2:\n            await db1.execute(\"create table foo (i integer, k charvar(250))\")\n            await db1.executemany(\n                \"insert into foo values (?, ?)\", [(1, \"hello\"), (2, \"world\")]\n            )\n            await db1.commit()\n\n            with self.assertRaisesRegex(OperationalError, \"no such table: foo\"):\n                await db2.execute(\"select * from foo\")\n\n            await db1.backup(db2, progress=progress)\n\n            async with db2.execute(\"select * from foo\") as cursor:\n                rows = await cursor.fetchall()\n                self.assertEqual(rows, [(1, \"hello\"), (2, \"world\")])\n\n    @skipIf(sys.version_info < (3, 7), \"Test backup() on 3.7+\")\n    async def test_backup_sqlite(self):\n        async with aiosqlite.connect(\":memory:\") as db1:\n            with sqlite3.connect(\":memory:\") as db2:\n                await db1.execute(\"create table foo (i integer, k charvar(250))\")\n                await db1.executemany(\n                    \"insert into foo values (?, ?)\", [(1, \"hello\"), (2, \"world\")]\n                )\n                await db1.commit()\n\n                with self.assertRaisesRegex(OperationalError, \"no such table: foo\"):\n                    db2.execute(\"select * from foo\")\n\n                await db1.backup(db2)\n\n                cursor = db2.execute(\"select * from foo\")\n                rows = cursor.fetchall()\n                self.assertEqual(rows, [(1, \"hello\"), (2, \"world\")])\n\n    @skipUnless(sys.version_info < (3, 7), \"Test short circuit fail on Py 3.6\")\n    async def test_backup_py36(self):\n        async with aiosqlite.connect(\":memory:\") as db1, aiosqlite.connect(\n            \":memory:\"\n        ) as db2:\n            with self.assertRaisesRegex(RuntimeError, \"backup().+3.7\"):\n                await db1.backup(db2)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/test_generator_nested.py", "content": "import unittest\nfrom greenlet import greenlet\n\n\nclass genlet(greenlet):\n\n    def __init__(self, *args, **kwds):\n        self.args = args\n        self.kwds = kwds\n        self.child = None\n\n    def run(self):\n        fn, = self.fn\n        fn(*self.args, **self.kwds)\n\n    def __iter__(self):\n        return self\n\n    def set_child(self, child):\n        self.child = child\n\n    def __next__(self):\n        if self.child:\n            child = self.child\n            while child.child:\n                tmp = child\n                child = child.child\n                tmp.child = None\n\n            result = child.switch()\n        else:\n            self.parent = greenlet.getcurrent()\n            result = self.switch()\n\n        if self:\n            return result\n        else:\n            raise StopIteration\n\n    # Hack: Python < 2.6 compatibility\n    next = __next__\n\n\ndef Yield(value, level=1):\n    g = greenlet.getcurrent()\n\n    while level != 0:\n        if not isinstance(g, genlet):\n            raise RuntimeError('yield outside a genlet')\n        if level > 1:\n            g.parent.set_child(g)\n        g = g.parent\n        level -= 1\n\n    g.switch(value)\n\n\ndef Genlet(func):\n    class Genlet(genlet):\n        fn = (func,)\n    return Genlet\n\n# ____________________________________________________________\n\n\ndef g1(n, seen):\n    for i in range(n):\n        seen.append(i + 1)\n        yield i\n\n\ndef g2(n, seen):\n    for i in range(n):\n        seen.append(i + 1)\n        Yield(i)\n\ng2 = Genlet(g2)\n\n\ndef nested(i):\n    Yield(i)\n\n\ndef g3(n, seen):\n    for i in range(n):\n        seen.append(i + 1)\n        nested(i)\ng3 = Genlet(g3)\n\n\ndef a(n):\n    if n == 0:\n        return\n    for ii in ax(n - 1):\n        Yield(ii)\n    Yield(n)\nax = Genlet(a)\n\n\ndef perms(l):\n    if len(l) > 1:\n        for e in l:\n            # No syntactical sugar for generator expressions\n            [Yield([e] + p) for p in perms([x for x in l if x != e])]\n    else:\n        Yield(l)\nperms = Genlet(perms)\n\n\ndef gr1(n):\n    for ii in range(1, n):\n        Yield(ii)\n        Yield(ii * ii, 2)\n\ngr1 = Genlet(gr1)\n\n\ndef gr2(n, seen):\n    for ii in gr1(n):\n        seen.append(ii)\n\ngr2 = Genlet(gr2)\n\n\nclass NestedGeneratorTests(unittest.TestCase):\n    def test_layered_genlets(self):\n        seen = []\n        for ii in gr2(5, seen):\n            seen.append(ii)\n        self.assertEqual(seen, [1, 1, 2, 4, 3, 9, 4, 16])\n\n    def test_permutations(self):\n        gen_perms = perms(list(range(4)))\n        permutations = list(gen_perms)\n        self.assertEqual(len(permutations), 4 * 3 * 2 * 1)\n        self.assertTrue([0, 1, 2, 3] in permutations)\n        self.assertTrue([3, 2, 1, 0] in permutations)\n        res = []\n        for ii in zip(perms(list(range(4))), perms(list(range(3)))):\n            res.append(ii)\n        self.assertEqual(\n            res,\n            [([0, 1, 2, 3], [0, 1, 2]), ([0, 1, 3, 2], [0, 2, 1]),\n             ([0, 2, 1, 3], [1, 0, 2]), ([0, 2, 3, 1], [1, 2, 0]),\n             ([0, 3, 1, 2], [2, 0, 1]), ([0, 3, 2, 1], [2, 1, 0])])\n        # XXX Test to make sure we are working as a generator expression\n\n    def test_genlet_simple(self):\n        for g in [g1, g2, g3]:\n            seen = []\n            for k in range(3):\n                for j in g(5, seen):\n                    seen.append(j)\n            self.assertEqual(seen, 3 * [1, 0, 2, 1, 3, 2, 4, 3, 5, 4])\n\n    def test_genlet_bad(self):\n        try:\n            Yield(10)\n        except RuntimeError:\n            pass\n\n    def test_nested_genlets(self):\n        seen = []\n        for ii in ax(5):\n            seen.append(ii)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/test_tracing.py", "content": "import sys\nimport unittest\nimport greenlet\n\nclass SomeError(Exception):\n    pass\n\nclass GreenletTracer(object):\n    oldtrace = None\n\n    def __init__(self, error_on_trace=False):\n        self.actions = []\n        self.error_on_trace = error_on_trace\n\n    def __call__(self, *args):\n        self.actions.append(args)\n        if self.error_on_trace:\n            raise SomeError\n\n    def __enter__(self):\n        self.oldtrace = greenlet.settrace(self)\n        return self.actions\n\n    def __exit__(self, *args):\n        greenlet.settrace(self.oldtrace)\n\n\nclass TestGreenletTracing(unittest.TestCase):\n    \"\"\"\n    Tests of ``greenlet.settrace()``\n    \"\"\"\n\n    def test_greenlet_tracing(self):\n        main = greenlet.getcurrent()\n        def dummy():\n            pass\n        def dummyexc():\n            raise SomeError()\n\n        with GreenletTracer() as actions:\n            g1 = greenlet.greenlet(dummy)\n            g1.switch()\n            g2 = greenlet.greenlet(dummyexc)\n            self.assertRaises(SomeError, g2.switch)\n\n        self.assertEqual(actions, [\n            ('switch', (main, g1)),\n            ('switch', (g1, main)),\n            ('switch', (main, g2)),\n            ('throw', (g2, main)),\n        ])\n\n    def test_exception_disables_tracing(self):\n        main = greenlet.getcurrent()\n        def dummy():\n            main.switch()\n        g = greenlet.greenlet(dummy)\n        g.switch()\n        with GreenletTracer(error_on_trace=True) as actions:\n            self.assertRaises(SomeError, g.switch)\n            self.assertEqual(greenlet.gettrace(), None)\n\n        self.assertEqual(actions, [\n            ('switch', (main, g)),\n        ])\n\n\nclass PythonTracer(object):\n    oldtrace = None\n\n    def __init__(self):\n        self.actions = []\n\n    def __call__(self, frame, event, arg):\n        # Record the co_name so we have an idea what function we're in.\n        self.actions.append((event, frame.f_code.co_name))\n\n    def __enter__(self):\n        self.oldtrace = sys.setprofile(self)\n        return self.actions\n\n    def __exit__(self, *args):\n        sys.setprofile(self.oldtrace)\n\ndef tpt_callback():\n    return 42\n\nclass TestPythonTracing(unittest.TestCase):\n    \"\"\"\n    Tests of the interaction of ``sys.settrace()``\n    with greenlet facilities.\n\n    NOTE: Most of this is probably CPython specific.\n    \"\"\"\n\n    maxDiff = None\n\n    def test_trace_events_trivial(self):\n        with PythonTracer() as actions:\n            tpt_callback()\n        # If we use the sys.settrace instead of setprofile, we get\n        # this:\n\n        # self.assertEqual(actions, [\n        #     ('call', 'tpt_callback'),\n        #     ('call', '__exit__'),\n        # ])\n\n        self.assertEqual(actions, [\n            ('return', '__enter__'),\n            ('call', 'tpt_callback'),\n            ('return', 'tpt_callback'),\n            ('call', '__exit__'),\n            ('c_call', '__exit__'),\n        ])\n\n    def _trace_switch(self, glet):\n        with PythonTracer() as actions:\n            glet.switch()\n        return actions\n\n    def _check_trace_events_func_already_set(self, glet):\n        actions = self._trace_switch(glet)\n        self.assertEqual(actions, [\n            ('return', '__enter__'),\n            ('c_call', '_trace_switch'),\n            ('call', 'run'),\n            ('call', 'tpt_callback'),\n            ('return', 'tpt_callback'),\n            ('return', 'run'),\n            ('c_return', '_trace_switch'),\n            ('call', '__exit__'),\n            ('c_call', '__exit__'),\n        ])\n\n    def test_trace_events_into_greenlet_func_already_set(self):\n        def run():\n            return tpt_callback()\n\n        self._check_trace_events_func_already_set(greenlet.greenlet(run))\n\n    def test_trace_events_into_greenlet_subclass_already_set(self):\n        class X(greenlet.greenlet):\n            def run(self):\n                return tpt_callback()\n        self._check_trace_events_func_already_set(X())\n\n    def _check_trace_events_from_greenlet_sets_profiler(self, g, tracer):\n        g.switch()\n        tpt_callback()\n        tracer.__exit__()\n        self.assertEqual(tracer.actions, [\n            ('return', '__enter__'),\n            ('call', 'tpt_callback'),\n            ('return', 'tpt_callback'),\n            ('return', 'run'),\n            ('call', 'tpt_callback'),\n            ('return', 'tpt_callback'),\n            ('call', '__exit__'),\n            ('c_call', '__exit__'),\n        ])\n\n\n    def test_trace_events_from_greenlet_func_sets_profiler(self):\n        tracer = PythonTracer()\n        def run():\n            tracer.__enter__()\n            return tpt_callback()\n\n        self._check_trace_events_from_greenlet_sets_profiler(greenlet.greenlet(run),\n                                                             tracer)\n\n    def test_trace_events_from_greenlet_subclass_sets_profiler(self):\n        tracer = PythonTracer()\n        class X(greenlet.greenlet):\n            def run(self):\n                tracer.__enter__()\n                return tpt_callback()\n\n        self._check_trace_events_from_greenlet_sets_profiler(X(), tracer)\n\n\n    def test_trace_events_multiple_greenlets_switching(self):\n        tracer = PythonTracer()\n\n        g1 = None\n        g2 = None\n\n        def g1_run():\n            tracer.__enter__()\n            tpt_callback()\n            g2.switch()\n            tpt_callback()\n            return 42\n\n        def g2_run():\n            tpt_callback()\n            tracer.__exit__()\n            tpt_callback()\n            g1.switch()\n\n        g1 = greenlet.greenlet(g1_run)\n        g2 = greenlet.greenlet(g2_run)\n\n        x = g1.switch()\n        self.assertEqual(x, 42)\n        tpt_callback() # ensure not in the trace\n        self.assertEqual(tracer.actions, [\n            ('return', '__enter__'),\n            ('call', 'tpt_callback'),\n            ('return', 'tpt_callback'),\n            ('c_call', 'g1_run'),\n            ('call', 'g2_run'),\n            ('call', 'tpt_callback'),\n            ('return', 'tpt_callback'),\n            ('call', '__exit__'),\n            ('c_call', '__exit__'),\n        ])\n\n    def test_trace_events_multiple_greenlets_switching_siblings(self):\n        # Like the first version, but get both greenlets running first\n        # as \"siblings\" and then establish the tracing.\n        tracer = PythonTracer()\n\n        g1 = None\n        g2 = None\n\n        def g1_run():\n            greenlet.getcurrent().parent.switch()\n            tracer.__enter__()\n            tpt_callback()\n            g2.switch()\n            tpt_callback()\n            return 42\n\n        def g2_run():\n            greenlet.getcurrent().parent.switch()\n\n            tpt_callback()\n            tracer.__exit__()\n            tpt_callback()\n            g1.switch()\n\n        g1 = greenlet.greenlet(g1_run)\n        g2 = greenlet.greenlet(g2_run)\n\n        # Start g1\n        g1.switch()\n        # And it immediately returns control to us.\n        # Start g2\n        g2.switch()\n        # Which also returns. Now kick of the real part of the\n        # test.\n        x = g1.switch()\n        self.assertEqual(x, 42)\n\n        tpt_callback() # ensure not in the trace\n        self.assertEqual(tracer.actions, [\n            ('return', '__enter__'),\n            ('call', 'tpt_callback'),\n            ('return', 'tpt_callback'),\n            ('c_call', 'g1_run'),\n            ('call', 'tpt_callback'),\n            ('return', 'tpt_callback'),\n            ('call', '__exit__'),\n            ('c_call', '__exit__'),\n        ])\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/aiosqlite/tests/perf.py", "content": "# Copyright 2018 John Reese\n# Licensed under the MIT license\n\n\"\"\"\nSimple perf tests for aiosqlite and the asyncio run loop.\n\"\"\"\nimport string\nimport time\n\nimport aiounittest\n\nimport aiosqlite\nfrom .smoke import setup_logger\n\nTEST_DB = \":memory:\"\nTARGET = 2.0\nRESULTS = {}\n\n\ndef timed(fn, name=None):\n    \"\"\"\n    Decorator for perf testing a block of async code.\n\n    Expects the wrapped function to return an async generator.\n    The generator should do setup, then yield when ready to start perf testing.\n    The decorator will then pump the generator repeatedly until the target\n    time has been reached, then close the generator and print perf results.\n    \"\"\"\n\n    name = name or fn.__name__\n\n    async def wrapper(*args, **kwargs):\n        gen = fn(*args, **kwargs)\n\n        await gen.asend(None)\n        count = 0\n        before = time.time()\n\n        while True:\n            count += 1\n            value = time.time() - before < TARGET\n            try:\n                if value:\n                    await gen.asend(value)\n                else:\n                    await gen.aclose()\n                    break\n\n            except StopAsyncIteration:\n                break\n\n            except Exception as e:\n                print(f\"exception occurred: {e}\")\n                return\n\n        duration = time.time() - before\n\n        RESULTS[name] = (count, duration)\n\n    return wrapper\n\n\nclass PerfTest(aiounittest.AsyncTestCase):\n    @classmethod\n    def setUpClass(cls):\n        print(f\"Running perf tests for at least {TARGET:.1f}s each...\")\n        setup_logger()\n\n    @classmethod\n    def tearDownClass(cls):\n        print(f\"\\n{'Perf Test':<25} Iterations  Duration  {'Rate':>11}\")\n        for name in sorted(RESULTS):\n            count, duration = RESULTS[name]\n            rate = count / duration\n            name = name.replace(\"test_\", \"\")\n            print(f\"{name:<25} {count:>10}  {duration:>7.1f}s  {rate:>9.1f}/s\")\n\n    @timed\n    async def test_atomics(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.execute(\"create table perf (i integer primary key asc, k integer)\")\n            await db.execute(\"insert into perf (k) values (2), (3)\")\n            await db.commit()\n\n            while True:\n                yield\n                async with db.execute(\"select last_insert_rowid()\") as cursor:\n                    _row_id = await cursor.fetchone()\n\n    @timed\n    async def test_inserts(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.execute(\"create table perf (i integer primary key asc, k integer)\")\n            await db.commit()\n\n            while True:\n                yield\n                await db.execute(\"insert into perf (k) values (1), (2), (3)\")\n                await db.commit()\n\n    @timed\n    async def test_insert_ids(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.execute(\"create table perf (i integer primary key asc, k integer)\")\n            await db.commit()\n\n            while True:\n                yield\n                cursor = await db.execute(\"insert into perf (k) values (1)\")\n                await cursor.execute(\"select last_insert_rowid()\")\n                await cursor.fetchone()\n                await db.commit()\n\n    @timed\n    async def test_insert_macro_ids(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.execute(\"create table perf (i integer primary key asc, k integer)\")\n            await db.commit()\n\n            while True:\n                yield\n                await db.execute_insert(\"insert into perf (k) values (1)\")\n                await db.commit()\n\n    @timed\n    async def test_select(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.execute(\"create table perf (i integer primary key asc, k integer)\")\n            for i in range(100):\n                await db.execute(\"insert into perf (k) values (%d)\" % (i,))\n            await db.commit()\n\n            while True:\n                yield\n                cursor = await db.execute(\"select i, k from perf\")\n                assert len(await cursor.fetchall()) == 100\n\n    @timed\n    async def test_select_macro(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.execute(\"create table perf (i integer primary key asc, k integer)\")\n            for i in range(100):\n                await db.execute(\"insert into perf (k) values (%d)\" % (i,))\n            await db.commit()\n\n            while True:\n                yield\n                assert len(await db.execute_fetchall(\"select i, k from perf\")) == 100\n\n    async def test_iterable_cursor_perf(self):\n        async with aiosqlite.connect(TEST_DB) as db:\n            await db.execute(\n                \"create table ic_perf (\"\n                \"i integer primary key asc, k integer, a integer, b integer, c char(16))\"\n            )\n            for batch in range(128):  # add 128k rows\n                r_start = batch * 1024\n                await db.executemany(\n                    \"insert into ic_perf (k, a, b, c) values(?, 1, 2, ?)\",\n                    [\n                        *[\n                            (i, string.ascii_lowercase)\n                            for i in range(r_start, r_start + 1024)\n                        ]\n                    ],\n                )\n                await db.commit()\n\n            async def test_perf(chunk_size: int):\n                while True:\n                    async with db.execute(\"SELECT * FROM ic_perf\") as cursor:\n                        cursor.iter_chunk_size = chunk_size\n                        async for _ in cursor:\n                            yield\n\n            for chunk_size in [2 ** i for i in range(4, 11)]:\n                await timed(test_perf, f\"iterable_cursor @ {chunk_size}\")(chunk_size)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/aiosqlite/tests/helpers.py", "content": "# Copyright 2018 John Reese\n# Licensed under the MIT license\n\nimport logging\nimport sys\n\n\ndef setup_logger():\n    log = logging.getLogger(\"\")\n    log.setLevel(logging.INFO)\n\n    logging.addLevelName(logging.ERROR, \"E\")\n    logging.addLevelName(logging.WARNING, \"W\")\n    logging.addLevelName(logging.INFO, \"I\")\n    logging.addLevelName(logging.DEBUG, \"V\")\n\n    date_fmt = r\"%H:%M:%S\"\n    verbose_fmt = (\n        \"%(asctime)s,%(msecs)d %(levelname)s \"\n        \"%(module)s:%(funcName)s():%(lineno)d   \"\n        \"%(message)s\"\n    )\n\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setLevel(logging.INFO)\n    handler.setFormatter(logging.Formatter(verbose_fmt, date_fmt))\n    log.addHandler(handler)\n\n    return log\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/ecdsa/test_malformed_sigs.py", "content": "from __future__ import with_statement, division\n\nimport hashlib\n\ntry:\n    from hashlib import algorithms_available\nexcept ImportError:  # pragma: no cover\n    algorithms_available = [\n        \"md5\",\n        \"sha1\",\n        \"sha224\",\n        \"sha256\",\n        \"sha384\",\n        \"sha512\",\n    ]\nfrom functools import partial\nimport pytest\nimport sys\nimport hypothesis.strategies as st\nfrom hypothesis import note, assume, given, settings, example\n\nfrom .keys import SigningKey\nfrom .keys import BadSignatureError\nfrom .util import sigencode_der, sigencode_string\nfrom .util import sigdecode_der, sigdecode_string\nfrom .curves import curves\nfrom .der import (\n    encode_integer,\n    encode_bitstring,\n    encode_octet_string,\n    encode_oid,\n    encode_sequence,\n    encode_constructed,\n)\n\n\nexample_data = b\"some data to sign\"\n\"\"\"Since the data is hashed for processing, really any string will do.\"\"\"\n\n\nhash_and_size = [\n    (name, hashlib.new(name).digest_size) for name in algorithms_available\n]\n\"\"\"Pairs of hash names and their output sizes.\nNeeded for pairing with curves as we don't support hashes\nbigger than order sizes of curves.\"\"\"\n\n\nkeys_and_sigs = []\n\"\"\"Name of the curve+hash combination, VerifyingKey and DER signature.\"\"\"\n\n\n# for hypothesis strategy shrinking we want smallest curves and hashes first\nfor curve in sorted(curves, key=lambda x: x.baselen):\n    for hash_alg in [\n        name\n        for name, size in sorted(hash_and_size, key=lambda x: x[1])\n        if 0 < size <= curve.baselen\n    ]:\n        sk = SigningKey.generate(\n            curve, hashfunc=partial(hashlib.new, hash_alg)\n        )\n\n        keys_and_sigs.append(\n            (\n                \"{0} {1}\".format(curve, hash_alg),\n                sk.verifying_key,\n                sk.sign(example_data, sigencode=sigencode_der),\n            )\n        )\n\n\n# first make sure that the signatures can be verified\n@pytest.mark.parametrize(\n    \"verifying_key,signature\",\n    [pytest.param(vk, sig, id=name) for name, vk, sig in keys_and_sigs],\n)\ndef test_signatures(verifying_key, signature):\n    assert verifying_key.verify(\n        signature, example_data, sigdecode=sigdecode_der\n    )\n\n\n@st.composite\ndef st_fuzzed_sig(draw, keys_and_sigs):\n    \"\"\"\n    Hypothesis strategy that generates pairs of VerifyingKey and malformed\n    signatures created by fuzzing of a valid signature.\n    \"\"\"\n    name, verifying_key, old_sig = draw(st.sampled_from(keys_and_sigs))\n    note(\"Configuration: {0}\".format(name))\n\n    sig = bytearray(old_sig)\n\n    # decide which bytes should be removed\n    to_remove = draw(\n        st.lists(st.integers(min_value=0, max_value=len(sig) - 1), unique=True)\n    )\n    to_remove.sort()\n    for i in reversed(to_remove):\n        del sig[i]\n    note(\"Remove bytes: {0}\".format(to_remove))\n\n    # decide which bytes of the original signature should be changed\n    if sig:  # pragma: no branch\n        xors = draw(\n            st.dictionaries(\n                st.integers(min_value=0, max_value=len(sig) - 1),\n                st.integers(min_value=1, max_value=255),\n            )\n        )\n        for i, val in xors.items():\n            sig[i] ^= val\n        note(\"xors: {0}\".format(xors))\n\n    # decide where new data should be inserted\n    insert_pos = draw(st.integers(min_value=0, max_value=len(sig)))\n    # NIST521p signature is about 140 bytes long, test slightly longer\n    insert_data = draw(st.binary(max_size=256))\n\n    sig = sig[:insert_pos] + insert_data + sig[insert_pos:]\n    note(\n        \"Inserted at position {0} bytes: {1!r}\".format(insert_pos, insert_data)\n    )\n\n    sig = bytes(sig)\n    # make sure that there was performed at least one mutation on the data\n    assume(to_remove or xors or insert_data)\n    # and that the mutations didn't cancel each-other out\n    assume(sig != old_sig)\n\n    return verifying_key, sig\n\n\nparams = {}\n# not supported in hypothesis 2.0.0\nif sys.version_info >= (2, 7):  # pragma: no branch\n    from hypothesis import HealthCheck\n\n    # deadline=5s because NIST521p are slow to verify\n    params[\"deadline\"] = 5000\n    params[\"suppress_health_check\"] = [\n        HealthCheck.data_too_large,\n        HealthCheck.filter_too_much,\n        HealthCheck.too_slow,\n    ]\n\nslow_params = dict(params)\nslow_params[\"max_examples\"] = 10\n\n\n@settings(**params)\n@given(st_fuzzed_sig(keys_and_sigs))\ndef test_fuzzed_der_signatures(args):\n    verifying_key, sig = args\n\n    with pytest.raises(BadSignatureError):\n        verifying_key.verify(sig, example_data, sigdecode=sigdecode_der)\n\n\n@st.composite\ndef st_random_der_ecdsa_sig_value(draw):\n    \"\"\"\n    Hypothesis strategy for selecting random values and encoding them\n    to ECDSA-Sig-Value object::\n\n        ECDSA-Sig-Value ::= SEQUENCE {\n            r INTEGER,\n            s INTEGER\n        }\n    \"\"\"\n    name, verifying_key, _ = draw(st.sampled_from(keys_and_sigs))\n    note(\"Configuration: {0}\".format(name))\n    order = int(verifying_key.curve.order)\n\n    # the encode_integer doesn't suport negative numbers, would be nice\n    # to generate them too, but we have coverage for remove_integer()\n    # verifying that it doesn't accept them, so meh.\n    # Test all numbers around the ones that can show up (around order)\n    # way smaller and slightly bigger\n    r = draw(\n        st.integers(min_value=0, max_value=order << 4)\n        | st.integers(min_value=order >> 2, max_value=order + 1)\n    )\n    s = draw(\n        st.integers(min_value=0, max_value=order << 4)\n        | st.integers(min_value=order >> 2, max_value=order + 1)\n    )\n\n    sig = encode_sequence(encode_integer(r), encode_integer(s))\n\n    return verifying_key, sig\n\n\n@settings(**slow_params)\n@given(st_random_der_ecdsa_sig_value())\ndef test_random_der_ecdsa_sig_value(params):\n    \"\"\"\n    Check if random values encoded in ECDSA-Sig-Value structure are rejected\n    as signature.\n    \"\"\"\n    verifying_key, sig = params\n\n    with pytest.raises(BadSignatureError):\n        verifying_key.verify(sig, example_data, sigdecode=sigdecode_der)\n\n\ndef st_der_integer(*args, **kwargs):\n    \"\"\"\n    Hypothesis strategy that returns a random positive integer as DER\n    INTEGER.\n    Parameters are passed to hypothesis.strategy.integer.\n    \"\"\"\n    if \"min_value\" not in kwargs:  # pragma: no branch\n        kwargs[\"min_value\"] = 0\n    return st.builds(encode_integer, st.integers(*args, **kwargs))\n\n\n@st.composite\ndef st_der_bit_string(draw, *args, **kwargs):\n    \"\"\"\n    Hypothesis strategy that returns a random DER BIT STRING.\n    Parameters are passed to hypothesis.strategy.binary.\n    \"\"\"\n    data = draw(st.binary(*args, **kwargs))\n    if data:\n        unused = draw(st.integers(min_value=0, max_value=7))\n        data = bytearray(data)\n        data[-1] &= -(2 ** unused)\n        data = bytes(data)\n    else:\n        unused = 0\n    return encode_bitstring(data, unused)\n\n\ndef st_der_octet_string(*args, **kwargs):\n    \"\"\"\n    Hypothesis strategy that returns a random DER OCTET STRING object.\n    Parameters are passed to hypothesis.strategy.binary\n    \"\"\"\n    return st.builds(encode_octet_string, st.binary(*args, **kwargs))\n\n\ndef st_der_null():\n    \"\"\"\n    Hypothesis strategy that returns DER NULL object.\n    \"\"\"\n    return st.just(b\"\\x05\\x00\")\n\n\n@st.composite\ndef st_der_oid(draw):\n    \"\"\"\n    Hypothesis strategy that returns DER OBJECT IDENTIFIER objects.\n    \"\"\"\n    first = draw(st.integers(min_value=0, max_value=2))\n    if first < 2:\n        second = draw(st.integers(min_value=0, max_value=39))\n    else:\n        second = draw(st.integers(min_value=0, max_value=2 ** 512))\n    rest = draw(\n        st.lists(st.integers(min_value=0, max_value=2 ** 512), max_size=50)\n    )\n    return encode_oid(first, second, *rest)\n\n\ndef st_der():\n    \"\"\"\n    Hypothesis strategy that returns random DER structures.\n\n    A valid DER structure is any primitive object, an octet encoding\n    of a valid DER structure, sequence of valid DER objects or a constructed\n    encoding of any of the above.\n    \"\"\"\n    return st.recursive(\n        st.just(b\"\")\n        | st_der_integer(max_value=2 ** 4096)\n        | st_der_bit_string(max_size=1024 ** 2)\n        | st_der_octet_string(max_size=1024 ** 2)\n        | st_der_null()\n        | st_der_oid(),\n        lambda children: st.builds(\n            lambda x: encode_octet_string(x), st.one_of(children)\n        )\n        | st.builds(lambda x: encode_bitstring(x, 0), st.one_of(children))\n        | st.builds(\n            lambda x: encode_sequence(*x), st.lists(children, max_size=200)\n        )\n        | st.builds(\n            lambda tag, x: encode_constructed(tag, x),\n            st.integers(min_value=0, max_value=0x3F),\n            st.one_of(children),\n        ),\n        max_leaves=40,\n    )\n\n\n@settings(**params)\n@given(st.sampled_from(keys_and_sigs), st_der())\ndef test_random_der_as_signature(params, der):\n    \"\"\"Check if random DER structures are rejected as signature\"\"\"\n    name, verifying_key, _ = params\n\n    with pytest.raises(BadSignatureError):\n        verifying_key.verify(der, example_data, sigdecode=sigdecode_der)\n\n\n@settings(**params)\n@given(st.sampled_from(keys_and_sigs), st.binary(max_size=1024 ** 2))\n@example(\n    keys_and_sigs[0], encode_sequence(encode_integer(0), encode_integer(0))\n)\n@example(\n    keys_and_sigs[0],\n    encode_sequence(encode_integer(1), encode_integer(1)) + b\"\\x00\",\n)\n@example(keys_and_sigs[0], encode_sequence(*[encode_integer(1)] * 3))\ndef test_random_bytes_as_signature(params, der):\n    \"\"\"Check if random bytes are rejected as signature\"\"\"\n    name, verifying_key, _ = params\n\n    with pytest.raises(BadSignatureError):\n        verifying_key.verify(der, example_data, sigdecode=sigdecode_der)\n\n\nkeys_and_string_sigs = [\n    (\n        name,\n        verifying_key,\n        sigencode_string(\n            *sigdecode_der(sig, verifying_key.curve.order),\n            order=verifying_key.curve.order\n        ),\n    )\n    for name, verifying_key, sig in keys_and_sigs\n]\n\"\"\"\nName of the curve+hash combination, VerifyingKey and signature as a\nbyte string.\n\"\"\"\n\n\n@settings(**params)\n@given(st_fuzzed_sig(keys_and_string_sigs))\ndef test_fuzzed_string_signatures(params):\n    verifying_key, sig = params\n\n    with pytest.raises(BadSignatureError):\n        verifying_key.verify(sig, example_data, sigdecode=sigdecode_string)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/test_throw.py", "content": "import sys\nimport unittest\n\nfrom greenlet import greenlet\n\n\ndef switch(*args):\n    return greenlet.getcurrent().parent.switch(*args)\n\n\nclass ThrowTests(unittest.TestCase):\n    def test_class(self):\n        def f():\n            try:\n                switch(\"ok\")\n            except RuntimeError:\n                switch(\"ok\")\n                return\n            switch(\"fail\")\n        g = greenlet(f)\n        res = g.switch()\n        self.assertEqual(res, \"ok\")\n        res = g.throw(RuntimeError)\n        self.assertEqual(res, \"ok\")\n\n    def test_val(self):\n        def f():\n            try:\n                switch(\"ok\")\n            except RuntimeError:\n                val = sys.exc_info()[1]\n                if str(val) == \"ciao\":\n                    switch(\"ok\")\n                    return\n            switch(\"fail\")\n\n        g = greenlet(f)\n        res = g.switch()\n        self.assertEqual(res, \"ok\")\n        res = g.throw(RuntimeError(\"ciao\"))\n        self.assertEqual(res, \"ok\")\n\n        g = greenlet(f)\n        res = g.switch()\n        self.assertEqual(res, \"ok\")\n        res = g.throw(RuntimeError, \"ciao\")\n        self.assertEqual(res, \"ok\")\n\n    def test_kill(self):\n        def f():\n            switch(\"ok\")\n            switch(\"fail\")\n        g = greenlet(f)\n        res = g.switch()\n        self.assertEqual(res, \"ok\")\n        res = g.throw()\n        self.assertTrue(isinstance(res, greenlet.GreenletExit))\n        self.assertTrue(g.dead)\n        res = g.throw()    # immediately eaten by the already-dead greenlet\n        self.assertTrue(isinstance(res, greenlet.GreenletExit))\n\n    def test_throw_goes_to_original_parent(self):\n        main = greenlet.getcurrent()\n\n        def f1():\n            try:\n                main.switch(\"f1 ready to catch\")\n            except IndexError:\n                return \"caught\"\n            else:\n                return \"normal exit\"\n\n        def f2():\n            main.switch(\"from f2\")\n\n        g1 = greenlet(f1)\n        g2 = greenlet(f2, parent=g1)\n        self.assertRaises(IndexError, g2.throw, IndexError)\n        self.assertTrue(g2.dead)\n        self.assertTrue(g1.dead)\n\n        g1 = greenlet(f1)\n        g2 = greenlet(f2, parent=g1)\n        res = g1.switch()\n        self.assertEqual(res, \"f1 ready to catch\")\n        res = g2.throw(IndexError)\n        self.assertEqual(res, \"caught\")\n        self.assertTrue(g2.dead)\n        self.assertTrue(g1.dead)\n\n        g1 = greenlet(f1)\n        g2 = greenlet(f2, parent=g1)\n        res = g1.switch()\n        self.assertEqual(res, \"f1 ready to catch\")\n        res = g2.switch()\n        self.assertEqual(res, \"from f2\")\n        res = g2.throw(IndexError)\n        self.assertEqual(res, \"caught\")\n        self.assertTrue(g2.dead)\n        self.assertTrue(g1.dead)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/ecdsa/test_pyecdsa.py", "content": "from __future__ import with_statement, division\n\ntry:\n    import unittest2 as unittest\nexcept ImportError:\n    import unittest\nimport os\nimport sys\nimport shutil\nimport subprocess\nimport pytest\nfrom binascii import hexlify, unhexlify\nfrom hashlib import sha1, sha256, sha384, sha512\nimport hashlib\nfrom functools import partial\n\nfrom hypothesis import given\nimport hypothesis.strategies as st\n\nfrom six import b, print_, binary_type\nfrom .keys import SigningKey, VerifyingKey\nfrom .keys import BadSignatureError, MalformedPointError, BadDigestError\nfrom . import util\nfrom .util import sigencode_der, sigencode_strings\nfrom .util import sigdecode_der, sigdecode_strings\nfrom .util import number_to_string, encoded_oid_ecPublicKey, MalformedSignature\nfrom .curves import Curve, UnknownCurveError\nfrom .curves import (\n    SECP112r1,\n    SECP112r2,\n    SECP128r1,\n    SECP160r1,\n    NIST192p,\n    NIST224p,\n    NIST256p,\n    NIST384p,\n    NIST521p,\n    SECP256k1,\n    BRAINPOOLP160r1,\n    BRAINPOOLP192r1,\n    BRAINPOOLP224r1,\n    BRAINPOOLP256r1,\n    BRAINPOOLP320r1,\n    BRAINPOOLP384r1,\n    BRAINPOOLP512r1,\n    curves,\n)\nfrom .ecdsa import (\n    curve_brainpoolp224r1,\n    curve_brainpoolp256r1,\n    curve_brainpoolp384r1,\n    curve_brainpoolp512r1,\n)\nfrom .ellipticcurve import Point\nfrom . import der\nfrom . import rfc6979\nfrom . import ecdsa\n\n\nclass SubprocessError(Exception):\n    pass\n\n\ndef run_openssl(cmd):\n    OPENSSL = \"openssl\"\n    p = subprocess.Popen(\n        [OPENSSL] + cmd.split(),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n    )\n    stdout, ignored = p.communicate()\n    if p.returncode != 0:\n        raise SubprocessError(\n            \"cmd '%s %s' failed: rc=%s, stdout/err was %s\"\n            % (OPENSSL, cmd, p.returncode, stdout)\n        )\n    return stdout.decode()\n\n\nclass ECDSA(unittest.TestCase):\n    def test_basic(self):\n        priv = SigningKey.generate()\n        pub = priv.get_verifying_key()\n\n        data = b(\"blahblah\")\n        sig = priv.sign(data)\n\n        self.assertTrue(pub.verify(sig, data))\n        self.assertRaises(BadSignatureError, pub.verify, sig, data + b(\"bad\"))\n\n        pub2 = VerifyingKey.from_string(pub.to_string())\n        self.assertTrue(pub2.verify(sig, data))\n\n    def test_deterministic(self):\n        data = b(\"blahblah\")\n        secexp = int(\"9d0219792467d7d37b4d43298a7d0c05\", 16)\n\n        priv = SigningKey.from_secret_exponent(secexp, SECP256k1, sha256)\n        pub = priv.get_verifying_key()\n\n        k = rfc6979.generate_k(\n            SECP256k1.generator.order(), secexp, sha256, sha256(data).digest()\n        )\n\n        sig1 = priv.sign(data, k=k)\n        self.assertTrue(pub.verify(sig1, data))\n\n        sig2 = priv.sign(data, k=k)\n        self.assertTrue(pub.verify(sig2, data))\n\n        sig3 = priv.sign_deterministic(data, sha256)\n        self.assertTrue(pub.verify(sig3, data))\n\n        self.assertEqual(sig1, sig2)\n        self.assertEqual(sig1, sig3)\n\n    def test_bad_usage(self):\n        # sk=SigningKey() is wrong\n        self.assertRaises(TypeError, SigningKey)\n        self.assertRaises(TypeError, VerifyingKey)\n\n    def test_lengths(self):\n        default = NIST192p\n        priv = SigningKey.generate()\n        pub = priv.get_verifying_key()\n        self.assertEqual(len(pub.to_string()), default.verifying_key_length)\n        sig = priv.sign(b(\"data\"))\n        self.assertEqual(len(sig), default.signature_length)\n        for curve in (\n            NIST192p,\n            NIST224p,\n            NIST256p,\n            NIST384p,\n            NIST521p,\n            BRAINPOOLP160r1,\n            BRAINPOOLP192r1,\n            BRAINPOOLP224r1,\n            BRAINPOOLP256r1,\n            BRAINPOOLP320r1,\n            BRAINPOOLP384r1,\n            BRAINPOOLP512r1,\n        ):\n            priv = SigningKey.generate(curve=curve)\n            pub1 = priv.get_verifying_key()\n            pub2 = VerifyingKey.from_string(pub1.to_string(), curve)\n            self.assertEqual(pub1.to_string(), pub2.to_string())\n            self.assertEqual(len(pub1.to_string()), curve.verifying_key_length)\n            sig = priv.sign(b(\"data\"))\n            self.assertEqual(len(sig), curve.signature_length)\n\n    def test_serialize(self):\n        seed = b(\"secret\")\n        curve = NIST192p\n        secexp1 = util.randrange_from_seed__trytryagain(seed, curve.order)\n        secexp2 = util.randrange_from_seed__trytryagain(seed, curve.order)\n        self.assertEqual(secexp1, secexp2)\n        priv1 = SigningKey.from_secret_exponent(secexp1, curve)\n        priv2 = SigningKey.from_secret_exponent(secexp2, curve)\n        self.assertEqual(\n            hexlify(priv1.to_string()), hexlify(priv2.to_string())\n        )\n        self.assertEqual(priv1.to_pem(), priv2.to_pem())\n        pub1 = priv1.get_verifying_key()\n        pub2 = priv2.get_verifying_key()\n        data = b(\"data\")\n        sig1 = priv1.sign(data)\n        sig2 = priv2.sign(data)\n        self.assertTrue(pub1.verify(sig1, data))\n        self.assertTrue(pub2.verify(sig1, data))\n        self.assertTrue(pub1.verify(sig2, data))\n        self.assertTrue(pub2.verify(sig2, data))\n        self.assertEqual(hexlify(pub1.to_string()), hexlify(pub2.to_string()))\n\n    def test_nonrandom(self):\n        s = b(\"all the entropy in the entire world, compressed into one line\")\n\n        def not_much_entropy(numbytes):\n            return s[:numbytes]\n\n        # we control the entropy source, these two keys should be identical:\n        priv1 = SigningKey.generate(entropy=not_much_entropy)\n        priv2 = SigningKey.generate(entropy=not_much_entropy)\n        self.assertEqual(\n            hexlify(priv1.get_verifying_key().to_string()),\n            hexlify(priv2.get_verifying_key().to_string()),\n        )\n        # likewise, signatures should be identical. Obviously you'd never\n        # want to do this with keys you care about, because the secrecy of\n        # the private key depends upon using different random numbers for\n        # each signature\n        sig1 = priv1.sign(b(\"data\"), entropy=not_much_entropy)\n        sig2 = priv2.sign(b(\"data\"), entropy=not_much_entropy)\n        self.assertEqual(hexlify(sig1), hexlify(sig2))\n\n    def assertTruePrivkeysEqual(self, priv1, priv2):\n        self.assertEqual(\n            priv1.privkey.secret_multiplier, priv2.privkey.secret_multiplier\n        )\n        self.assertEqual(\n            priv1.privkey.public_key.generator,\n            priv2.privkey.public_key.generator,\n        )\n\n    def test_privkey_creation(self):\n        s = b(\"all the entropy in the entire world, compressed into one line\")\n\n        def not_much_entropy(numbytes):\n            return s[:numbytes]\n\n        priv1 = SigningKey.generate()\n        self.assertEqual(priv1.baselen, NIST192p.baselen)\n\n        priv1 = SigningKey.generate(curve=NIST224p)\n        self.assertEqual(priv1.baselen, NIST224p.baselen)\n\n        priv1 = SigningKey.generate(entropy=not_much_entropy)\n        self.assertEqual(priv1.baselen, NIST192p.baselen)\n        priv2 = SigningKey.generate(entropy=not_much_entropy)\n        self.assertEqual(priv2.baselen, NIST192p.baselen)\n        self.assertTruePrivkeysEqual(priv1, priv2)\n\n        priv1 = SigningKey.from_secret_exponent(secexp=3)\n        self.assertEqual(priv1.baselen, NIST192p.baselen)\n        priv2 = SigningKey.from_secret_exponent(secexp=3)\n        self.assertTruePrivkeysEqual(priv1, priv2)\n\n        priv1 = SigningKey.from_secret_exponent(secexp=4, curve=NIST224p)\n        self.assertEqual(priv1.baselen, NIST224p.baselen)\n\n    def test_privkey_strings(self):\n        priv1 = SigningKey.generate()\n        s1 = priv1.to_string()\n        self.assertEqual(type(s1), binary_type)\n        self.assertEqual(len(s1), NIST192p.baselen)\n        priv2 = SigningKey.from_string(s1)\n        self.assertTruePrivkeysEqual(priv1, priv2)\n\n        s1 = priv1.to_pem()\n        self.assertEqual(type(s1), binary_type)\n        self.assertTrue(s1.startswith(b(\"-----BEGIN EC PRIVATE KEY-----\")))\n        self.assertTrue(s1.strip().endswith(b(\"-----END EC PRIVATE KEY-----\")))\n        priv2 = SigningKey.from_pem(s1)\n        self.assertTruePrivkeysEqual(priv1, priv2)\n\n        s1 = priv1.to_der()\n        self.assertEqual(type(s1), binary_type)\n        priv2 = SigningKey.from_der(s1)\n        self.assertTruePrivkeysEqual(priv1, priv2)\n\n        priv1 = SigningKey.generate(curve=NIST256p)\n        s1 = priv1.to_pem()\n        self.assertEqual(type(s1), binary_type)\n        self.assertTrue(s1.startswith(b(\"-----BEGIN EC PRIVATE KEY-----\")))\n        self.assertTrue(s1.strip().endswith(b(\"-----END EC PRIVATE KEY-----\")))\n        priv2 = SigningKey.from_pem(s1)\n        self.assertTruePrivkeysEqual(priv1, priv2)\n\n        s1 = priv1.to_der()\n        self.assertEqual(type(s1), binary_type)\n        priv2 = SigningKey.from_der(s1)\n        self.assertTruePrivkeysEqual(priv1, priv2)\n\n    def test_privkey_strings_brainpool(self):\n        priv1 = SigningKey.generate(curve=BRAINPOOLP512r1)\n        s1 = priv1.to_pem()\n        self.assertEqual(type(s1), binary_type)\n        self.assertTrue(s1.startswith(b(\"-----BEGIN EC PRIVATE KEY-----\")))\n        self.assertTrue(s1.strip().endswith(b(\"-----END EC PRIVATE KEY-----\")))\n        priv2 = SigningKey.from_pem(s1)\n        self.assertTruePrivkeysEqual(priv1, priv2)\n\n        s1 = priv1.to_der()\n        self.assertEqual(type(s1), binary_type)\n        priv2 = SigningKey.from_der(s1)\n        self.assertTruePrivkeysEqual(priv1, priv2)\n\n    def assertTruePubkeysEqual(self, pub1, pub2):\n        self.assertEqual(pub1.pubkey.point, pub2.pubkey.point)\n        self.assertEqual(pub1.pubkey.generator, pub2.pubkey.generator)\n        self.assertEqual(pub1.curve, pub2.curve)\n\n    def test_pubkey_strings(self):\n        priv1 = SigningKey.generate()\n        pub1 = priv1.get_verifying_key()\n        s1 = pub1.to_string()\n        self.assertEqual(type(s1), binary_type)\n        self.assertEqual(len(s1), NIST192p.verifying_key_length)\n        pub2 = VerifyingKey.from_string(s1)\n        self.assertTruePubkeysEqual(pub1, pub2)\n\n        priv1 = SigningKey.generate(curve=NIST256p)\n        pub1 = priv1.get_verifying_key()\n        s1 = pub1.to_string()\n        self.assertEqual(type(s1), binary_type)\n        self.assertEqual(len(s1), NIST256p.verifying_key_length)\n        pub2 = VerifyingKey.from_string(s1, curve=NIST256p)\n        self.assertTruePubkeysEqual(pub1, pub2)\n\n        pub1_der = pub1.to_der()\n        self.assertEqual(type(pub1_der), binary_type)\n        pub2 = VerifyingKey.from_der(pub1_der)\n        self.assertTruePubkeysEqual(pub1, pub2)\n\n        self.assertRaises(\n            der.UnexpectedDER, VerifyingKey.from_der, pub1_der + b(\"junk\")\n        )\n        badpub = VerifyingKey.from_der(pub1_der)\n\n        class FakeGenerator:\n            def order(self):\n                return 123456789\n\n        class FakeCurveFp:\n            def p(self):\n                return int(\n                    \"6525534529039240705020950546962731340\"\n                    \"4541085228058844382513856749047873406763\"\n                )\n\n        badcurve = Curve(\n            \"unknown\", FakeCurveFp(), FakeGenerator(), (1, 2, 3, 4, 5, 6), None\n        )\n        badpub.curve = badcurve\n        badder = badpub.to_der()\n        self.assertRaises(UnknownCurveError, VerifyingKey.from_der, badder)\n\n        pem = pub1.to_pem()\n        self.assertEqual(type(pem), binary_type)\n        self.assertTrue(pem.startswith(b(\"-----BEGIN PUBLIC KEY-----\")), pem)\n        self.assertTrue(\n            pem.strip().endswith(b(\"-----END PUBLIC KEY-----\")), pem\n        )\n        pub2 = VerifyingKey.from_pem(pem)\n        self.assertTruePubkeysEqual(pub1, pub2)\n\n    def test_pubkey_strings_brainpool(self):\n        priv1 = SigningKey.generate(curve=BRAINPOOLP512r1)\n        pub1 = priv1.get_verifying_key()\n        s1 = pub1.to_string()\n        self.assertEqual(type(s1), binary_type)\n        self.assertEqual(len(s1), BRAINPOOLP512r1.verifying_key_length)\n        pub2 = VerifyingKey.from_string(s1, curve=BRAINPOOLP512r1)\n        self.assertTruePubkeysEqual(pub1, pub2)\n\n        pub1_der = pub1.to_der()\n        self.assertEqual(type(pub1_der), binary_type)\n        pub2 = VerifyingKey.from_der(pub1_der)\n        self.assertTruePubkeysEqual(pub1, pub2)\n\n    def test_vk_to_der_with_invalid_point_encoding(self):\n        sk = SigningKey.generate()\n        vk = sk.verifying_key\n\n        with self.assertRaises(ValueError):\n            vk.to_der(\"raw\")\n\n    def test_sk_to_der_with_invalid_point_encoding(self):\n        sk = SigningKey.generate()\n\n        with self.assertRaises(ValueError):\n            sk.to_der(\"raw\")\n\n    def test_vk_from_der_garbage_after_curve_oid(self):\n        type_oid_der = encoded_oid_ecPublicKey\n        curve_oid_der = der.encode_oid(*(1, 2, 840, 10045, 3, 1, 1)) + b(\n            \"garbage\"\n        )\n        enc_type_der = der.encode_sequence(type_oid_der, curve_oid_der)\n        point_der = der.encode_bitstring(b\"\\x00\\xff\", None)\n        to_decode = der.encode_sequence(enc_type_der, point_der)\n\n        with self.assertRaises(der.UnexpectedDER):\n            VerifyingKey.from_der(to_decode)\n\n    def test_vk_from_der_invalid_key_type(self):\n        type_oid_der = der.encode_oid(*(1, 2, 3))\n        curve_oid_der = der.encode_oid(*(1, 2, 840, 10045, 3, 1, 1))\n        enc_type_der = der.encode_sequence(type_oid_der, curve_oid_der)\n        point_der = der.encode_bitstring(b\"\\x00\\xff\", None)\n        to_decode = der.encode_sequence(enc_type_der, point_der)\n\n        with self.assertRaises(der.UnexpectedDER):\n            VerifyingKey.from_der(to_decode)\n\n    def test_vk_from_der_garbage_after_point_string(self):\n        type_oid_der = encoded_oid_ecPublicKey\n        curve_oid_der = der.encode_oid(*(1, 2, 840, 10045, 3, 1, 1))\n        enc_type_der = der.encode_sequence(type_oid_der, curve_oid_der)\n        point_der = der.encode_bitstring(b\"\\x00\\xff\", None) + b(\"garbage\")\n        to_decode = der.encode_sequence(enc_type_der, point_der)\n\n        with self.assertRaises(der.UnexpectedDER):\n            VerifyingKey.from_der(to_decode)\n\n    def test_vk_from_der_invalid_bitstring(self):\n        type_oid_der = encoded_oid_ecPublicKey\n        curve_oid_der = der.encode_oid(*(1, 2, 840, 10045, 3, 1, 1))\n        enc_type_der = der.encode_sequence(type_oid_der, curve_oid_der)\n        point_der = der.encode_bitstring(b\"\\x08\\xff\", None)\n        to_decode = der.encode_sequence(enc_type_der, point_der)\n\n        with self.assertRaises(der.UnexpectedDER):\n            VerifyingKey.from_der(to_decode)\n\n    def test_vk_from_der_with_invalid_length_of_encoding(self):\n        type_oid_der = encoded_oid_ecPublicKey\n        curve_oid_der = der.encode_oid(*(1, 2, 840, 10045, 3, 1, 1))\n        enc_type_der = der.encode_sequence(type_oid_der, curve_oid_der)\n        point_der = der.encode_bitstring(b\"\\xff\" * 64, 0)\n        to_decode = der.encode_sequence(enc_type_der, point_der)\n\n        with self.assertRaises(MalformedPointError):\n            VerifyingKey.from_der(to_decode)\n\n    def test_vk_from_der_with_raw_encoding(self):\n        type_oid_der = encoded_oid_ecPublicKey\n        curve_oid_der = der.encode_oid(*(1, 2, 840, 10045, 3, 1, 1))\n        enc_type_der = der.encode_sequence(type_oid_der, curve_oid_der)\n        point_der = der.encode_bitstring(b\"\\xff\" * 48, 0)\n        to_decode = der.encode_sequence(enc_type_der, point_der)\n\n        with self.assertRaises(der.UnexpectedDER):\n            VerifyingKey.from_der(to_decode)\n\n    def test_signature_strings(self):\n        priv1 = SigningKey.generate()\n        pub1 = priv1.get_verifying_key()\n        data = b(\"data\")\n\n        sig = priv1.sign(data)\n        self.assertEqual(type(sig), binary_type)\n        self.assertEqual(len(sig), NIST192p.signature_length)\n        self.assertTrue(pub1.verify(sig, data))\n\n        sig = priv1.sign(data, sigencode=sigencode_strings)\n        self.assertEqual(type(sig), tuple)\n        self.assertEqual(len(sig), 2)\n        self.assertEqual(type(sig[0]), binary_type)\n        self.assertEqual(type(sig[1]), binary_type)\n        self.assertEqual(len(sig[0]), NIST192p.baselen)\n        self.assertEqual(len(sig[1]), NIST192p.baselen)\n        self.assertTrue(pub1.verify(sig, data, sigdecode=sigdecode_strings))\n\n        sig_der = priv1.sign(data, sigencode=sigencode_der)\n        self.assertEqual(type(sig_der), binary_type)\n        self.assertTrue(pub1.verify(sig_der, data, sigdecode=sigdecode_der))\n\n    def test_sig_decode_strings_with_invalid_count(self):\n        with self.assertRaises(MalformedSignature):\n            sigdecode_strings([b(\"one\"), b(\"two\"), b(\"three\")], 0xFF)\n\n    def test_sig_decode_strings_with_wrong_r_len(self):\n        with self.assertRaises(MalformedSignature):\n            sigdecode_strings([b(\"one\"), b(\"two\")], 0xFF)\n\n    def test_sig_decode_strings_with_wrong_s_len(self):\n        with self.assertRaises(MalformedSignature):\n            sigdecode_strings([b(\"\\xa0\"), b(\"\\xb0\\xff\")], 0xFF)\n\n    def test_verify_with_too_long_input(self):\n        sk = SigningKey.generate()\n        vk = sk.verifying_key\n\n        with self.assertRaises(BadDigestError):\n            vk.verify_digest(None, b(\"\\x00\") * 128)\n\n    def test_sk_from_secret_exponent_with_wrong_sec_exponent(self):\n        with self.assertRaises(MalformedPointError):\n            SigningKey.from_secret_exponent(0)\n\n    def test_sk_from_string_with_wrong_len_string(self):\n        with self.assertRaises(MalformedPointError):\n            SigningKey.from_string(b(\"\\x01\"))\n\n    def test_sk_from_der_with_junk_after_sequence(self):\n        ver_der = der.encode_integer(1)\n        to_decode = der.encode_sequence(ver_der) + b(\"garbage\")\n\n        with self.assertRaises(der.UnexpectedDER):\n            SigningKey.from_der(to_decode)\n\n    def test_sk_from_der_with_wrong_version(self):\n        ver_der = der.encode_integer(0)\n        to_decode = der.encode_sequence(ver_der)\n\n        with self.assertRaises(der.UnexpectedDER):\n            SigningKey.from_der(to_decode)\n\n    def test_sk_from_der_invalid_const_tag(self):\n        ver_der = der.encode_integer(1)\n        privkey_der = der.encode_octet_string(b(\"\\x00\\xff\"))\n        curve_oid_der = der.encode_oid(*(1, 2, 3))\n        const_der = der.encode_constructed(1, curve_oid_der)\n        to_decode = der.encode_sequence(\n            ver_der, privkey_der, const_der, curve_oid_der\n        )\n\n        with self.assertRaises(der.UnexpectedDER):\n            SigningKey.from_der(to_decode)\n\n    def test_sk_from_der_garbage_after_privkey_oid(self):\n        ver_der = der.encode_integer(1)\n        privkey_der = der.encode_octet_string(b(\"\\x00\\xff\"))\n        curve_oid_der = der.encode_oid(*(1, 2, 3)) + b(\"garbage\")\n        const_der = der.encode_constructed(0, curve_oid_der)\n        to_decode = der.encode_sequence(\n            ver_der, privkey_der, const_der, curve_oid_der\n        )\n\n        with self.assertRaises(der.UnexpectedDER):\n            SigningKey.from_der(to_decode)\n\n    def test_sk_from_der_with_short_privkey(self):\n        ver_der = der.encode_integer(1)\n        privkey_der = der.encode_octet_string(b(\"\\x00\\xff\"))\n        curve_oid_der = der.encode_oid(*(1, 2, 840, 10045, 3, 1, 1))\n        const_der = der.encode_constructed(0, curve_oid_der)\n        to_decode = der.encode_sequence(\n            ver_der, privkey_der, const_der, curve_oid_der\n        )\n\n        sk = SigningKey.from_der(to_decode)\n        self.assertEqual(sk.privkey.secret_multiplier, 255)\n\n    def test_sk_from_p8_der_with_wrong_version(self):\n        ver_der = der.encode_integer(2)\n        algorithm_der = der.encode_sequence(\n            der.encode_oid(1, 2, 840, 10045, 2, 1),\n            der.encode_oid(1, 2, 840, 10045, 3, 1, 1),\n        )\n        privkey_der = der.encode_octet_string(\n            der.encode_sequence(\n                der.encode_integer(1), der.encode_octet_string(b\"\\x00\\xff\")\n            )\n        )\n        to_decode = der.encode_sequence(ver_der, algorithm_der, privkey_der)\n\n        with self.assertRaises(der.UnexpectedDER):\n            SigningKey.from_der(to_decode)\n\n    def test_sk_from_p8_der_with_wrong_algorithm(self):\n        ver_der = der.encode_integer(1)\n        algorithm_der = der.encode_sequence(\n            der.encode_oid(1, 2, 3), der.encode_oid(1, 2, 840, 10045, 3, 1, 1)\n        )\n        privkey_der = der.encode_octet_string(\n            der.encode_sequence(\n                der.encode_integer(1), der.encode_octet_string(b\"\\x00\\xff\")\n            )\n        )\n        to_decode = der.encode_sequence(ver_der, algorithm_der, privkey_der)\n\n        with self.assertRaises(der.UnexpectedDER):\n            SigningKey.from_der(to_decode)\n\n    def test_sk_from_p8_der_with_trailing_junk_after_algorithm(self):\n        ver_der = der.encode_integer(1)\n        algorithm_der = der.encode_sequence(\n            der.encode_oid(1, 2, 840, 10045, 2, 1),\n            der.encode_oid(1, 2, 840, 10045, 3, 1, 1),\n            der.encode_octet_string(b\"junk\"),\n        )\n        privkey_der = der.encode_octet_string(\n            der.encode_sequence(\n                der.encode_integer(1), der.encode_octet_string(b\"\\x00\\xff\")\n            )\n        )\n        to_decode = der.encode_sequence(ver_der, algorithm_der, privkey_der)\n\n        with self.assertRaises(der.UnexpectedDER):\n            SigningKey.from_der(to_decode)\n\n    def test_sk_from_p8_der_with_trailing_junk_after_key(self):\n        ver_der = der.encode_integer(1)\n        algorithm_der = der.encode_sequence(\n            der.encode_oid(1, 2, 840, 10045, 2, 1),\n            der.encode_oid(1, 2, 840, 10045, 3, 1, 1),\n        )\n        privkey_der = der.encode_octet_string(\n            der.encode_sequence(\n                der.encode_integer(1), der.encode_octet_string(b\"\\x00\\xff\")\n            )\n            + der.encode_integer(999)\n        )\n        to_decode = der.encode_sequence(\n            ver_der,\n            algorithm_der,\n            privkey_der,\n            der.encode_octet_string(b\"junk\"),\n        )\n\n        with self.assertRaises(der.UnexpectedDER):\n            SigningKey.from_der(to_decode)\n\n    def test_sign_with_too_long_hash(self):\n        sk = SigningKey.from_secret_exponent(12)\n\n        with self.assertRaises(BadDigestError):\n            sk.sign_digest(b(\"\\xff\") * 64)\n\n    def test_hashfunc(self):\n        sk = SigningKey.generate(curve=NIST256p, hashfunc=sha256)\n        data = b(\"security level is 128 bits\")\n        sig = sk.sign(data)\n        vk = VerifyingKey.from_string(\n            sk.get_verifying_key().to_string(), curve=NIST256p, hashfunc=sha256\n        )\n        self.assertTrue(vk.verify(sig, data))\n\n        sk2 = SigningKey.generate(curve=NIST256p)\n        sig2 = sk2.sign(data, hashfunc=sha256)\n        vk2 = VerifyingKey.from_string(\n            sk2.get_verifying_key().to_string(),\n            curve=NIST256p,\n            hashfunc=sha256,\n        )\n        self.assertTrue(vk2.verify(sig2, data))\n\n        vk3 = VerifyingKey.from_string(\n            sk.get_verifying_key().to_string(), curve=NIST256p\n        )\n        self.assertTrue(vk3.verify(sig, data, hashfunc=sha256))\n\n    def test_public_key_recovery(self):\n        # Create keys\n        curve = BRAINPOOLP160r1\n\n        sk = SigningKey.generate(curve=curve)\n        vk = sk.get_verifying_key()\n\n        # Sign a message\n        data = b(\"blahblah\")\n        signature = sk.sign(data)\n\n        # Recover verifying keys\n        recovered_vks = VerifyingKey.from_public_key_recovery(\n            signature, data, curve\n        )\n\n        # Test if each pk is valid\n        for recovered_vk in recovered_vks:\n            # Test if recovered vk is valid for the data\n            self.assertTrue(recovered_vk.verify(signature, data))\n\n            # Test if properties are equal\n            self.assertEqual(vk.curve, recovered_vk.curve)\n            self.assertEqual(\n                vk.default_hashfunc, recovered_vk.default_hashfunc\n            )\n\n        # Test if original vk is the list of recovered keys\n        self.assertIn(\n            vk.pubkey.point,\n            [recovered_vk.pubkey.point for recovered_vk in recovered_vks],\n        )\n\n    def test_public_key_recovery_with_custom_hash(self):\n        # Create keys\n        curve = BRAINPOOLP160r1\n\n        sk = SigningKey.generate(curve=curve, hashfunc=sha256)\n        vk = sk.get_verifying_key()\n\n        # Sign a message\n        data = b(\"blahblah\")\n        signature = sk.sign(data)\n\n        # Recover verifying keys\n        recovered_vks = VerifyingKey.from_public_key_recovery(\n            signature, data, curve, hashfunc=sha256, allow_truncate=True\n        )\n\n        # Test if each pk is valid\n        for recovered_vk in recovered_vks:\n            # Test if recovered vk is valid for the data\n            self.assertTrue(recovered_vk.verify(signature, data))\n\n            # Test if properties are equal\n            self.assertEqual(vk.curve, recovered_vk.curve)\n            self.assertEqual(sha256, recovered_vk.default_hashfunc)\n\n        # Test if original vk is the list of recovered keys\n        self.assertIn(\n            vk.pubkey.point,\n            [recovered_vk.pubkey.point for recovered_vk in recovered_vks],\n        )\n\n    def test_encoding(self):\n        sk = SigningKey.from_secret_exponent(123456789)\n        vk = sk.verifying_key\n\n        exp = b(\n            \"\\x0c\\xe0\\x1d\\xe0d\\x1c\\x8eS\\x8a\\xc0\\x9eK\\xa8x !\\xd5\\xc2\\xc3\"\n            \"\\xfd\\xc8\\xa0c\\xff\\xfb\\x02\\xb9\\xc4\\x84)\\x1a\\x0f\\x8b\\x87\\xa4\"\n            \"z\\x8a#\\xb5\\x97\\xecO\\xb6\\xa0HQ\\x89*\"\n        )\n        self.assertEqual(vk.to_string(), exp)\n        self.assertEqual(vk.to_string(\"raw\"), exp)\n        self.assertEqual(vk.to_string(\"uncompressed\"), b(\"\\x04\") + exp)\n        self.assertEqual(vk.to_string(\"compressed\"), b(\"\\x02\") + exp[:24])\n        self.assertEqual(vk.to_string(\"hybrid\"), b(\"\\x06\") + exp)\n\n    def test_decoding(self):\n        sk = SigningKey.from_secret_exponent(123456789)\n        vk = sk.verifying_key\n\n        enc = b(\n            \"\\x0c\\xe0\\x1d\\xe0d\\x1c\\x8eS\\x8a\\xc0\\x9eK\\xa8x !\\xd5\\xc2\\xc3\"\n            \"\\xfd\\xc8\\xa0c\\xff\\xfb\\x02\\xb9\\xc4\\x84)\\x1a\\x0f\\x8b\\x87\\xa4\"\n            \"z\\x8a#\\xb5\\x97\\xecO\\xb6\\xa0HQ\\x89*\"\n        )\n\n        from_raw = VerifyingKey.from_string(enc)\n        self.assertEqual(from_raw.pubkey.point, vk.pubkey.point)\n\n        from_uncompressed = VerifyingKey.from_string(b(\"\\x04\") + enc)\n        self.assertEqual(from_uncompressed.pubkey.point, vk.pubkey.point)\n\n        from_compressed = VerifyingKey.from_string(b(\"\\x02\") + enc[:24])\n        self.assertEqual(from_compressed.pubkey.point, vk.pubkey.point)\n\n        from_uncompressed = VerifyingKey.from_string(b(\"\\x06\") + enc)\n        self.assertEqual(from_uncompressed.pubkey.point, vk.pubkey.point)\n\n    def test_uncompressed_decoding_as_only_alowed(self):\n        enc = b(\n            \"\\x04\"\n            \"\\x0c\\xe0\\x1d\\xe0d\\x1c\\x8eS\\x8a\\xc0\\x9eK\\xa8x !\\xd5\\xc2\\xc3\"\n            \"\\xfd\\xc8\\xa0c\\xff\\xfb\\x02\\xb9\\xc4\\x84)\\x1a\\x0f\\x8b\\x87\\xa4\"\n            \"z\\x8a#\\xb5\\x97\\xecO\\xb6\\xa0HQ\\x89*\"\n        )\n        vk = VerifyingKey.from_string(enc, valid_encodings=(\"uncompressed\",))\n        sk = SigningKey.from_secret_exponent(123456789)\n\n        self.assertEqual(vk, sk.verifying_key)\n\n    def test_raw_decoding_with_blocked_format(self):\n        enc = b(\n            \"\\x0c\\xe0\\x1d\\xe0d\\x1c\\x8eS\\x8a\\xc0\\x9eK\\xa8x !\\xd5\\xc2\\xc3\"\n            \"\\xfd\\xc8\\xa0c\\xff\\xfb\\x02\\xb9\\xc4\\x84)\\x1a\\x0f\\x8b\\x87\\xa4\"\n            \"z\\x8a#\\xb5\\x97\\xecO\\xb6\\xa0HQ\\x89*\"\n        )\n        with self.assertRaises(MalformedPointError) as exp:\n            VerifyingKey.from_string(enc, valid_encodings=(\"hybrid\",))\n\n        self.assertIn(\"hybrid\", str(exp.exception))\n\n    def test_decoding_with_unknown_format(self):\n        with self.assertRaises(ValueError) as e:\n            VerifyingKey.from_string(b\"\", valid_encodings=(\"raw\", \"foobar\"))\n\n        self.assertIn(\"Only uncompressed, compressed\", str(e.exception))\n\n    def test_uncompressed_decoding_with_blocked_format(self):\n        enc = b(\n            \"\\x04\"\n            \"\\x0c\\xe0\\x1d\\xe0d\\x1c\\x8eS\\x8a\\xc0\\x9eK\\xa8x !\\xd5\\xc2\\xc3\"\n            \"\\xfd\\xc8\\xa0c\\xff\\xfb\\x02\\xb9\\xc4\\x84)\\x1a\\x0f\\x8b\\x87\\xa4\"\n            \"z\\x8a#\\xb5\\x97\\xecO\\xb6\\xa0HQ\\x89*\"\n        )\n        with self.assertRaises(MalformedPointError) as exp:\n            VerifyingKey.from_string(enc, valid_encodings=(\"hybrid\",))\n\n        self.assertIn(\"Invalid X9.62 encoding\", str(exp.exception))\n\n    def test_hybrid_decoding_with_blocked_format(self):\n        enc = b(\n            \"\\x06\"\n            \"\\x0c\\xe0\\x1d\\xe0d\\x1c\\x8eS\\x8a\\xc0\\x9eK\\xa8x !\\xd5\\xc2\\xc3\"\n            \"\\xfd\\xc8\\xa0c\\xff\\xfb\\x02\\xb9\\xc4\\x84)\\x1a\\x0f\\x8b\\x87\\xa4\"\n            \"z\\x8a#\\xb5\\x97\\xecO\\xb6\\xa0HQ\\x89*\"\n        )\n        with self.assertRaises(MalformedPointError) as exp:\n            VerifyingKey.from_string(enc, valid_encodings=(\"uncompressed\",))\n\n        self.assertIn(\"Invalid X9.62 encoding\", str(exp.exception))\n\n    def test_compressed_decoding_with_blocked_format(self):\n        enc = b(\n            \"\\x02\"\n            \"\\x0c\\xe0\\x1d\\xe0d\\x1c\\x8eS\\x8a\\xc0\\x9eK\\xa8x !\\xd5\\xc2\\xc3\"\n            \"\\xfd\\xc8\\xa0c\\xff\\xfb\\x02\\xb9\\xc4\\x84)\\x1a\\x0f\\x8b\\x87\\xa4\"\n            \"z\\x8a#\\xb5\\x97\\xecO\\xb6\\xa0HQ\\x89*\"\n        )[:25]\n        with self.assertRaises(MalformedPointError) as exp:\n            VerifyingKey.from_string(enc, valid_encodings=(\"hybrid\", \"raw\"))\n\n        self.assertIn(\"(hybrid, raw)\", str(exp.exception))\n\n    def test_decoding_with_malformed_uncompressed(self):\n        enc = b(\n            \"\\x0c\\xe0\\x1d\\xe0d\\x1c\\x8eS\\x8a\\xc0\\x9eK\\xa8x !\\xd5\\xc2\\xc3\"\n            \"\\xfd\\xc8\\xa0c\\xff\\xfb\\x02\\xb9\\xc4\\x84)\\x1a\\x0f\\x8b\\x87\\xa4\"\n            \"z\\x8a#\\xb5\\x97\\xecO\\xb6\\xa0HQ\\x89*\"\n        )\n\n        with self.assertRaises(MalformedPointError):\n            VerifyingKey.from_string(b(\"\\x02\") + enc)\n\n    def test_decoding_with_malformed_compressed(self):\n        enc = b(\n            \"\\x0c\\xe0\\x1d\\xe0d\\x1c\\x8eS\\x8a\\xc0\\x9eK\\xa8x !\\xd5\\xc2\\xc3\"\n            \"\\xfd\\xc8\\xa0c\\xff\\xfb\\x02\\xb9\\xc4\\x84)\\x1a\\x0f\\x8b\\x87\\xa4\"\n            \"z\\x8a#\\xb5\\x97\\xecO\\xb6\\xa0HQ\\x89*\"\n        )\n\n        with self.assertRaises(MalformedPointError):\n            VerifyingKey.from_string(b(\"\\x01\") + enc[:24])\n\n    def test_decoding_with_inconsistent_hybrid(self):\n        enc = b(\n            \"\\x0c\\xe0\\x1d\\xe0d\\x1c\\x8eS\\x8a\\xc0\\x9eK\\xa8x !\\xd5\\xc2\\xc3\"\n            \"\\xfd\\xc8\\xa0c\\xff\\xfb\\x02\\xb9\\xc4\\x84)\\x1a\\x0f\\x8b\\x87\\xa4\"\n            \"z\\x8a#\\xb5\\x97\\xecO\\xb6\\xa0HQ\\x89*\"\n        )\n\n        with self.assertRaises(MalformedPointError):\n            VerifyingKey.from_string(b(\"\\x07\") + enc)\n\n    def test_decoding_with_point_not_on_curve(self):\n        enc = b(\n            \"\\x0c\\xe0\\x1d\\xe0d\\x1c\\x8eS\\x8a\\xc0\\x9eK\\xa8x !\\xd5\\xc2\\xc3\"\n            \"\\xfd\\xc8\\xa0c\\xff\\xfb\\x02\\xb9\\xc4\\x84)\\x1a\\x0f\\x8b\\x87\\xa4\"\n            \"z\\x8a#\\xb5\\x97\\xecO\\xb6\\xa0HQ\\x89*\"\n        )\n\n        with self.assertRaises(MalformedPointError):\n            VerifyingKey.from_string(enc[:47] + b(\"\\x00\"))\n\n    def test_decoding_with_point_at_infinity(self):\n        # decoding it is unsupported, as it's not necessary to encode it\n        with self.assertRaises(MalformedPointError):\n            VerifyingKey.from_string(b(\"\\x00\"))\n\n    def test_not_lying_on_curve(self):\n        enc = number_to_string(NIST192p.curve.p(), NIST192p.curve.p() + 1)\n\n        with self.assertRaises(MalformedPointError):\n            VerifyingKey.from_string(b(\"\\x02\") + enc)\n\n    def test_from_string_with_invalid_curve_too_short_ver_key_len(self):\n        # both verifying_key_length and baselen are calculated internally\n        # by the Curve constructor, but since we depend on them verify\n        # that inconsistent values are detected\n        curve = Curve(\"test\", ecdsa.curve_192, ecdsa.generator_192, (1, 2))\n        curve.verifying_key_length = 16\n        curve.baselen = 32\n\n        with self.assertRaises(MalformedPointError):\n            VerifyingKey.from_string(b(\"\\x00\") * 16, curve)\n\n    def test_from_string_with_invalid_curve_too_long_ver_key_len(self):\n        # both verifying_key_length and baselen are calculated internally\n        # by the Curve constructor, but since we depend on them verify\n        # that inconsistent values are detected\n        curve = Curve(\"test\", ecdsa.curve_192, ecdsa.generator_192, (1, 2))\n        curve.verifying_key_length = 16\n        curve.baselen = 16\n\n        with self.assertRaises(MalformedPointError):\n            VerifyingKey.from_string(b(\"\\x00\") * 16, curve)\n\n\n@pytest.mark.parametrize(\n    \"val,even\", [(i, j) for i in range(256) for j in [True, False]]\n)\ndef test_VerifyingKey_decode_with_small_values(val, even):\n    enc = number_to_string(val, NIST192p.order)\n\n    if even:\n        enc = b(\"\\x02\") + enc\n    else:\n        enc = b(\"\\x03\") + enc\n\n    # small values can both be actual valid public keys and not, verify that\n    # only expected exceptions are raised if they are not\n    try:\n        vk = VerifyingKey.from_string(enc)\n        assert isinstance(vk, VerifyingKey)\n    except MalformedPointError:\n        assert True\n\n\nparams = []\nfor curve in curves:\n    for enc in [\"raw\", \"uncompressed\", \"compressed\", \"hybrid\"]:\n        params.append(\n            pytest.param(curve, enc, id=\"{0}-{1}\".format(curve.name, enc))\n        )\n\n\n@pytest.mark.parametrize(\"curve,encoding\", params)\ndef test_VerifyingKey_encode_decode(curve, encoding):\n    sk = SigningKey.generate(curve=curve)\n    vk = sk.verifying_key\n\n    encoded = vk.to_string(encoding)\n\n    from_enc = VerifyingKey.from_string(encoded, curve=curve)\n\n    assert vk.pubkey.point == from_enc.pubkey.point\n\n\nclass OpenSSL(unittest.TestCase):\n    # test interoperability with OpenSSL tools. Note that openssl's ECDSA\n    # sign/verify arguments changed between 0.9.8 and 1.0.0: the early\n    # versions require \"-ecdsa-with-SHA1\", the later versions want just\n    # \"-SHA1\" (or to leave out that argument entirely, which means the\n    # signature will use some default digest algorithm, probably determined\n    # by the key, probably always SHA1).\n    #\n    # openssl ecparam -name secp224r1 -genkey -out privkey.pem\n    # openssl ec -in privkey.pem -text -noout # get the priv/pub keys\n    # openssl dgst -ecdsa-with-SHA1 -sign privkey.pem -out data.sig data.txt\n    # openssl asn1parse -in data.sig -inform DER\n    #  data.sig is 64 bytes, probably 56b plus ASN1 overhead\n    # openssl dgst -ecdsa-with-SHA1 -prverify privkey.pem -signature data.sig data.txt ; echo $?\n    # openssl ec -in privkey.pem -pubout -out pubkey.pem\n    # openssl ec -in privkey.pem -pubout -outform DER -out pubkey.der\n\n    OPENSSL_SUPPORTED_CURVES = set(\n        c.split(\":\")[0].strip()\n        for c in run_openssl(\"ecparam -list_curves\").split(\"\\n\")\n    )\n\n    def get_openssl_messagedigest_arg(self, hash_name):\n        v = run_openssl(\"version\")\n        # e.g. \"OpenSSL 1.0.0 29 Mar 2010\", or \"OpenSSL 1.0.0a 1 Jun 2010\",\n        # or \"OpenSSL 0.9.8o 01 Jun 2010\"\n        vs = v.split()[1].split(\".\")\n        if vs >= [\"1\", \"0\", \"0\"]:  # pragma: no cover\n            return \"-{0}\".format(hash_name)\n        else:  # pragma: no cover\n            return \"-ecdsa-with-{0}\".format(hash_name)\n\n    # sk: 1:OpenSSL->python  2:python->OpenSSL\n    # vk: 3:OpenSSL->python  4:python->OpenSSL\n    # sig: 5:OpenSSL->python 6:python->OpenSSL\n\n    @pytest.mark.skipif(\n        \"secp112r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp112r1\",\n    )\n    def test_from_openssl_secp112r1(self):\n        return self.do_test_from_openssl(SECP112r1)\n\n    @pytest.mark.skipif(\n        \"secp112r2\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp112r2\",\n    )\n    def test_from_openssl_secp112r2(self):\n        return self.do_test_from_openssl(SECP112r2)\n\n    @pytest.mark.skipif(\n        \"secp128r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp128r1\",\n    )\n    def test_from_openssl_secp128r1(self):\n        return self.do_test_from_openssl(SECP128r1)\n\n    @pytest.mark.skipif(\n        \"secp160r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp160r1\",\n    )\n    def test_from_openssl_secp160r1(self):\n        return self.do_test_from_openssl(SECP160r1)\n\n    @pytest.mark.skipif(\n        \"prime192v1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support prime192v1\",\n    )\n    def test_from_openssl_nist192p(self):\n        return self.do_test_from_openssl(NIST192p)\n\n    @pytest.mark.skipif(\n        \"prime192v1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support prime192v1\",\n    )\n    def test_from_openssl_nist192p_sha256(self):\n        return self.do_test_from_openssl(NIST192p, \"SHA256\")\n\n    @pytest.mark.skipif(\n        \"secp224r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp224r1\",\n    )\n    def test_from_openssl_nist224p(self):\n        return self.do_test_from_openssl(NIST224p)\n\n    @pytest.mark.skipif(\n        \"prime256v1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support prime256v1\",\n    )\n    def test_from_openssl_nist256p(self):\n        return self.do_test_from_openssl(NIST256p)\n\n    @pytest.mark.skipif(\n        \"prime256v1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support prime256v1\",\n    )\n    def test_from_openssl_nist256p_sha384(self):\n        return self.do_test_from_openssl(NIST256p, \"SHA384\")\n\n    @pytest.mark.skipif(\n        \"prime256v1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support prime256v1\",\n    )\n    def test_from_openssl_nist256p_sha512(self):\n        return self.do_test_from_openssl(NIST256p, \"SHA512\")\n\n    @pytest.mark.skipif(\n        \"secp384r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp384r1\",\n    )\n    def test_from_openssl_nist384p(self):\n        return self.do_test_from_openssl(NIST384p)\n\n    @pytest.mark.skipif(\n        \"secp521r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp521r1\",\n    )\n    def test_from_openssl_nist521p(self):\n        return self.do_test_from_openssl(NIST521p)\n\n    @pytest.mark.skipif(\n        \"secp256k1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp256k1\",\n    )\n    def test_from_openssl_secp256k1(self):\n        return self.do_test_from_openssl(SECP256k1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP160r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP160r1\",\n    )\n    def test_from_openssl_brainpoolp160r1(self):\n        return self.do_test_from_openssl(BRAINPOOLP160r1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP192r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP192r1\",\n    )\n    def test_from_openssl_brainpoolp192r1(self):\n        return self.do_test_from_openssl(BRAINPOOLP192r1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP224r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP224r1\",\n    )\n    def test_from_openssl_brainpoolp224r1(self):\n        return self.do_test_from_openssl(BRAINPOOLP224r1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP256r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP256r1\",\n    )\n    def test_from_openssl_brainpoolp256r1(self):\n        return self.do_test_from_openssl(BRAINPOOLP256r1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP320r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP320r1\",\n    )\n    def test_from_openssl_brainpoolp320r1(self):\n        return self.do_test_from_openssl(BRAINPOOLP320r1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP384r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP384r1\",\n    )\n    def test_from_openssl_brainpoolp384r1(self):\n        return self.do_test_from_openssl(BRAINPOOLP384r1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP512r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP512r1\",\n    )\n    def test_from_openssl_brainpoolp512r1(self):\n        return self.do_test_from_openssl(BRAINPOOLP512r1)\n\n    def do_test_from_openssl(self, curve, hash_name=\"SHA1\"):\n        curvename = curve.openssl_name\n        assert curvename\n        # OpenSSL: create sk, vk, sign.\n        # Python: read vk(3), checksig(5), read sk(1), sign, check\n        mdarg = self.get_openssl_messagedigest_arg(hash_name)\n        if os.path.isdir(\"t\"):  # pragma: no cover\n            shutil.rmtree(\"t\")\n        os.mkdir(\"t\")\n        run_openssl(\"ecparam -name %s -genkey -out t/privkey.pem\" % curvename)\n        run_openssl(\"ec -in t/privkey.pem -pubout -out t/pubkey.pem\")\n        data = b(\"data\")\n        with open(\"t/data.txt\", \"wb\") as e:\n            e.write(data)\n        run_openssl(\n            \"dgst %s -sign t/privkey.pem -out t/data.sig t/data.txt\" % mdarg\n        )\n        run_openssl(\n            \"dgst %s -verify t/pubkey.pem -signature t/data.sig t/data.txt\"\n            % mdarg\n        )\n        with open(\"t/pubkey.pem\", \"rb\") as e:\n            pubkey_pem = e.read()\n        vk = VerifyingKey.from_pem(pubkey_pem)  # 3\n        with open(\"t/data.sig\", \"rb\") as e:\n            sig_der = e.read()\n        self.assertTrue(\n            vk.verify(\n                sig_der,\n                data,  # 5\n                hashfunc=partial(hashlib.new, hash_name),\n                sigdecode=sigdecode_der,\n            )\n        )\n\n        with open(\"t/privkey.pem\") as e:\n            fp = e.read()\n        sk = SigningKey.from_pem(fp)  # 1\n        sig = sk.sign(data, hashfunc=partial(hashlib.new, hash_name))\n        self.assertTrue(\n            vk.verify(sig, data, hashfunc=partial(hashlib.new, hash_name))\n        )\n\n        run_openssl(\n            \"pkcs8 -topk8 -nocrypt \"\n            \"-in t/privkey.pem -outform pem -out t/privkey-p8.pem\"\n        )\n        with open(\"t/privkey-p8.pem\", \"rb\") as e:\n            privkey_p8_pem = e.read()\n        sk_from_p8 = SigningKey.from_pem(privkey_p8_pem)\n        self.assertEqual(sk, sk_from_p8)\n\n    @pytest.mark.skipif(\n        \"secp112r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp112r1\",\n    )\n    def test_to_openssl_secp112r1(self):\n        self.do_test_to_openssl(SECP112r1)\n\n    @pytest.mark.skipif(\n        \"secp112r2\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp112r2\",\n    )\n    def test_to_openssl_secp112r2(self):\n        self.do_test_to_openssl(SECP112r2)\n\n    @pytest.mark.skipif(\n        \"secp128r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp128r1\",\n    )\n    def test_to_openssl_secp128r1(self):\n        self.do_test_to_openssl(SECP128r1)\n\n    @pytest.mark.skipif(\n        \"secp160r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp160r1\",\n    )\n    def test_to_openssl_secp160r1(self):\n        self.do_test_to_openssl(SECP160r1)\n\n    @pytest.mark.skipif(\n        \"prime192v1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support prime192v1\",\n    )\n    def test_to_openssl_nist192p(self):\n        self.do_test_to_openssl(NIST192p)\n\n    @pytest.mark.skipif(\n        \"prime192v1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support prime192v1\",\n    )\n    def test_to_openssl_nist192p_sha256(self):\n        self.do_test_to_openssl(NIST192p, \"SHA256\")\n\n    @pytest.mark.skipif(\n        \"secp224r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp224r1\",\n    )\n    def test_to_openssl_nist224p(self):\n        self.do_test_to_openssl(NIST224p)\n\n    @pytest.mark.skipif(\n        \"prime256v1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support prime256v1\",\n    )\n    def test_to_openssl_nist256p(self):\n        self.do_test_to_openssl(NIST256p)\n\n    @pytest.mark.skipif(\n        \"prime256v1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support prime256v1\",\n    )\n    def test_to_openssl_nist256p_sha384(self):\n        self.do_test_to_openssl(NIST256p, \"SHA384\")\n\n    @pytest.mark.skipif(\n        \"prime256v1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support prime256v1\",\n    )\n    def test_to_openssl_nist256p_sha512(self):\n        self.do_test_to_openssl(NIST256p, \"SHA512\")\n\n    @pytest.mark.skipif(\n        \"secp384r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp384r1\",\n    )\n    def test_to_openssl_nist384p(self):\n        self.do_test_to_openssl(NIST384p)\n\n    @pytest.mark.skipif(\n        \"secp521r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp521r1\",\n    )\n    def test_to_openssl_nist521p(self):\n        self.do_test_to_openssl(NIST521p)\n\n    @pytest.mark.skipif(\n        \"secp256k1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support secp256k1\",\n    )\n    def test_to_openssl_secp256k1(self):\n        self.do_test_to_openssl(SECP256k1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP160r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP160r1\",\n    )\n    def test_to_openssl_brainpoolp160r1(self):\n        self.do_test_to_openssl(BRAINPOOLP160r1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP192r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP192r1\",\n    )\n    def test_to_openssl_brainpoolp192r1(self):\n        self.do_test_to_openssl(BRAINPOOLP192r1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP224r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP224r1\",\n    )\n    def test_to_openssl_brainpoolp224r1(self):\n        self.do_test_to_openssl(BRAINPOOLP224r1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP256r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP256r1\",\n    )\n    def test_to_openssl_brainpoolp256r1(self):\n        self.do_test_to_openssl(BRAINPOOLP256r1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP320r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP320r1\",\n    )\n    def test_to_openssl_brainpoolp320r1(self):\n        self.do_test_to_openssl(BRAINPOOLP320r1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP384r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP384r1\",\n    )\n    def test_to_openssl_brainpoolp384r1(self):\n        self.do_test_to_openssl(BRAINPOOLP384r1)\n\n    @pytest.mark.skipif(\n        \"brainpoolP512r1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support brainpoolP512r1\",\n    )\n    def test_to_openssl_brainpoolp512r1(self):\n        self.do_test_to_openssl(BRAINPOOLP512r1)\n\n    def do_test_to_openssl(self, curve, hash_name=\"SHA1\"):\n        curvename = curve.openssl_name\n        assert curvename\n        # Python: create sk, vk, sign.\n        # OpenSSL: read vk(4), checksig(6), read sk(2), sign, check\n        mdarg = self.get_openssl_messagedigest_arg(hash_name)\n        if os.path.isdir(\"t\"):  # pragma: no cover\n            shutil.rmtree(\"t\")\n        os.mkdir(\"t\")\n        sk = SigningKey.generate(curve=curve)\n        vk = sk.get_verifying_key()\n        data = b(\"data\")\n        with open(\"t/pubkey.der\", \"wb\") as e:\n            e.write(vk.to_der())  # 4\n        with open(\"t/pubkey.pem\", \"wb\") as e:\n            e.write(vk.to_pem())  # 4\n        sig_der = sk.sign(\n            data,\n            hashfunc=partial(hashlib.new, hash_name),\n            sigencode=sigencode_der,\n        )\n\n        with open(\"t/data.sig\", \"wb\") as e:\n            e.write(sig_der)  # 6\n        with open(\"t/data.txt\", \"wb\") as e:\n            e.write(data)\n        with open(\"t/baddata.txt\", \"wb\") as e:\n            e.write(data + b(\"corrupt\"))\n\n        self.assertRaises(\n            SubprocessError,\n            run_openssl,\n            \"dgst %s -verify t/pubkey.der -keyform DER -signature t/data.sig t/baddata.txt\"\n            % mdarg,\n        )\n        run_openssl(\n            \"dgst %s -verify t/pubkey.der -keyform DER -signature t/data.sig t/data.txt\"\n            % mdarg\n        )\n\n        with open(\"t/privkey.pem\", \"wb\") as e:\n            e.write(sk.to_pem())  # 2\n        run_openssl(\n            \"dgst %s -sign t/privkey.pem -out t/data.sig2 t/data.txt\" % mdarg\n        )\n        run_openssl(\n            \"dgst %s -verify t/pubkey.pem -signature t/data.sig2 t/data.txt\"\n            % mdarg\n        )\n\n        with open(\"t/privkey-explicit.pem\", \"wb\") as e:\n            e.write(sk.to_pem(curve_parameters_encoding=\"explicit\"))\n        run_openssl(\n            \"dgst %s -sign t/privkey-explicit.pem -out t/data.sig2 t/data.txt\"\n            % mdarg\n        )\n        run_openssl(\n            \"dgst %s -verify t/pubkey.pem -signature t/data.sig2 t/data.txt\"\n            % mdarg\n        )\n\n        with open(\"t/privkey-p8.pem\", \"wb\") as e:\n            e.write(sk.to_pem(format=\"pkcs8\"))\n        run_openssl(\n            \"dgst %s -sign t/privkey-p8.pem -out t/data.sig3 t/data.txt\"\n            % mdarg\n        )\n        run_openssl(\n            \"dgst %s -verify t/pubkey.pem -signature t/data.sig3 t/data.txt\"\n            % mdarg\n        )\n\n        with open(\"t/privkey-p8-explicit.pem\", \"wb\") as e:\n            e.write(\n                sk.to_pem(format=\"pkcs8\", curve_parameters_encoding=\"explicit\")\n            )\n        run_openssl(\n            \"dgst %s -sign t/privkey-p8-explicit.pem -out t/data.sig3 t/data.txt\"\n            % mdarg\n        )\n        run_openssl(\n            \"dgst %s -verify t/pubkey.pem -signature t/data.sig3 t/data.txt\"\n            % mdarg\n        )\n\n\nclass TooSmallCurve(unittest.TestCase):\n    OPENSSL_SUPPORTED_CURVES = set(\n        c.split(\":\")[0].strip()\n        for c in run_openssl(\"ecparam -list_curves\").split(\"\\n\")\n    )\n\n    @pytest.mark.skipif(\n        \"prime192v1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support prime192v1\",\n    )\n    def test_sign_too_small_curve_dont_allow_truncate_raises(self):\n        sk = SigningKey.generate(curve=NIST192p)\n        data = b(\"data\")\n        with self.assertRaises(BadDigestError):\n            sk.sign(\n                data,\n                hashfunc=partial(hashlib.new, \"SHA256\"),\n                sigencode=sigencode_der,\n                allow_truncate=False,\n            )\n\n    @pytest.mark.skipif(\n        \"prime192v1\" not in OPENSSL_SUPPORTED_CURVES,\n        reason=\"system openssl does not support prime192v1\",\n    )\n    def test_verify_too_small_curve_dont_allow_truncate_raises(self):\n        sk = SigningKey.generate(curve=NIST192p)\n        vk = sk.get_verifying_key()\n        data = b(\"data\")\n        sig_der = sk.sign(\n            data,\n            hashfunc=partial(hashlib.new, \"SHA256\"),\n            sigencode=sigencode_der,\n            allow_truncate=True,\n        )\n        with self.assertRaises(BadDigestError):\n            vk.verify(\n                sig_der,\n                data,\n                hashfunc=partial(hashlib.new, \"SHA256\"),\n                sigdecode=sigdecode_der,\n                allow_truncate=False,\n            )\n\n\nclass DER(unittest.TestCase):\n    def test_integer(self):\n        self.assertEqual(der.encode_integer(0), b(\"\\x02\\x01\\x00\"))\n        self.assertEqual(der.encode_integer(1), b(\"\\x02\\x01\\x01\"))\n        self.assertEqual(der.encode_integer(127), b(\"\\x02\\x01\\x7f\"))\n        self.assertEqual(der.encode_integer(128), b(\"\\x02\\x02\\x00\\x80\"))\n        self.assertEqual(der.encode_integer(256), b(\"\\x02\\x02\\x01\\x00\"))\n        # self.assertEqual(der.encode_integer(-1), b(\"\\x02\\x01\\xff\"))\n\n        def s(n):\n            return der.remove_integer(der.encode_integer(n) + b(\"junk\"))\n\n        self.assertEqual(s(0), (0, b(\"junk\")))\n        self.assertEqual(s(1), (1, b(\"junk\")))\n        self.assertEqual(s(127), (127, b(\"junk\")))\n        self.assertEqual(s(128), (128, b(\"junk\")))\n        self.assertEqual(s(256), (256, b(\"junk\")))\n        self.assertEqual(\n            s(1234567890123456789012345678901234567890),\n            (1234567890123456789012345678901234567890, b(\"junk\")),\n        )\n\n    def test_number(self):\n        self.assertEqual(der.encode_number(0), b(\"\\x00\"))\n        self.assertEqual(der.encode_number(127), b(\"\\x7f\"))\n        self.assertEqual(der.encode_number(128), b(\"\\x81\\x00\"))\n        self.assertEqual(der.encode_number(3 * 128 + 7), b(\"\\x83\\x07\"))\n        # self.assertEqual(der.read_number(\"\\x81\\x9b\" + \"more\"), (155, 2))\n        # self.assertEqual(der.encode_number(155), b(\"\\x81\\x9b\"))\n        for n in (0, 1, 2, 127, 128, 3 * 128 + 7, 840, 10045):  # , 155):\n            x = der.encode_number(n) + b(\"more\")\n            n1, llen = der.read_number(x)\n            self.assertEqual(n1, n)\n            self.assertEqual(x[llen:], b(\"more\"))\n\n    def test_length(self):\n        self.assertEqual(der.encode_length(0), b(\"\\x00\"))\n        self.assertEqual(der.encode_length(127), b(\"\\x7f\"))\n        self.assertEqual(der.encode_length(128), b(\"\\x81\\x80\"))\n        self.assertEqual(der.encode_length(255), b(\"\\x81\\xff\"))\n        self.assertEqual(der.encode_length(256), b(\"\\x82\\x01\\x00\"))\n        self.assertEqual(der.encode_length(3 * 256 + 7), b(\"\\x82\\x03\\x07\"))\n        self.assertEqual(der.read_length(b(\"\\x81\\x9b\") + b(\"more\")), (155, 2))\n        self.assertEqual(der.encode_length(155), b(\"\\x81\\x9b\"))\n        for n in (0, 1, 2, 127, 128, 255, 256, 3 * 256 + 7, 155):\n            x = der.encode_length(n) + b(\"more\")\n            n1, llen = der.read_length(x)\n            self.assertEqual(n1, n)\n            self.assertEqual(x[llen:], b(\"more\"))\n\n    def test_sequence(self):\n        x = der.encode_sequence(b(\"ABC\"), b(\"DEF\")) + b(\"GHI\")\n        self.assertEqual(x, b(\"\\x30\\x06ABCDEFGHI\"))\n        x1, rest = der.remove_sequence(x)\n        self.assertEqual(x1, b(\"ABCDEF\"))\n        self.assertEqual(rest, b(\"GHI\"))\n\n    def test_constructed(self):\n        x = der.encode_constructed(0, NIST224p.encoded_oid)\n        self.assertEqual(hexlify(x), b(\"a007\") + b(\"06052b81040021\"))\n        x = der.encode_constructed(1, unhexlify(b(\"0102030a0b0c\")))\n        self.assertEqual(hexlify(x), b(\"a106\") + b(\"0102030a0b0c\"))\n\n\nclass Util(unittest.TestCase):\n    def test_trytryagain(self):\n        tta = util.randrange_from_seed__trytryagain\n        for i in range(1000):\n            seed = \"seed-%d\" % i\n            for order in (\n                2 ** 8 - 2,\n                2 ** 8 - 1,\n                2 ** 8,\n                2 ** 8 + 1,\n                2 ** 8 + 2,\n                2 ** 16 - 1,\n                2 ** 16 + 1,\n            ):\n                n = tta(seed, order)\n                self.assertTrue(1 <= n < order, (1, n, order))\n        # this trytryagain *does* provide long-term stability\n        self.assertEqual(\n            (\"%x\" % (tta(\"seed\", NIST224p.order))).encode(),\n            b(\"6fa59d73bf0446ae8743cf748fc5ac11d5585a90356417e97155c3bc\"),\n        )\n\n    def test_trytryagain_single(self):\n        tta = util.randrange_from_seed__trytryagain\n        order = 2 ** 8 - 2\n        seed = b\"text\"\n        n = tta(seed, order)\n        # known issue: https://github.com/warner/python-ecdsa/issues/221\n        if sys.version_info < (3, 0):  # pragma: no branch\n            self.assertEqual(n, 228)\n        else:\n            self.assertEqual(n, 18)\n\n    @given(st.integers(min_value=0, max_value=10 ** 200))\n    def test_randrange(self, i):\n        # util.randrange does not provide long-term stability: we might\n        # change the algorithm in the future.\n        entropy = util.PRNG(\"seed-%d\" % i)\n        for order in (\n            2 ** 8 - 2,\n            2 ** 8 - 1,\n            2 ** 8,\n            2 ** 16 - 1,\n            2 ** 16 + 1,\n        ):\n            # that oddball 2**16+1 takes half our runtime\n            n = util.randrange(order, entropy=entropy)\n            self.assertTrue(1 <= n < order, (1, n, order))\n\n    def OFF_test_prove_uniformity(self):  # pragma: no cover\n        order = 2 ** 8 - 2\n        counts = dict([(i, 0) for i in range(1, order)])\n        assert 0 not in counts\n        assert order not in counts\n        for i in range(1000000):\n            seed = \"seed-%d\" % i\n            n = util.randrange_from_seed__trytryagain(seed, order)\n            counts[n] += 1\n        # this technique should use the full range\n        self.assertTrue(counts[order - 1])\n        for i in range(1, order):\n            print_(\"%3d: %s\" % (i, \"*\" * (counts[i] // 100)))\n\n\nclass RFC6979(unittest.TestCase):\n    # https://tools.ietf.org/html/rfc6979#appendix-A.1\n    def _do(self, generator, secexp, hsh, hash_func, expected):\n        actual = rfc6979.generate_k(generator.order(), secexp, hash_func, hsh)\n        self.assertEqual(expected, actual)\n\n    def test_SECP256k1(self):\n        \"\"\"RFC doesn't contain test vectors for SECP256k1 used in bitcoin.\n        This vector has been computed by Golang reference implementation instead.\"\"\"\n        self._do(\n            generator=SECP256k1.generator,\n            secexp=int(\"9d0219792467d7d37b4d43298a7d0c05\", 16),\n            hsh=sha256(b(\"sample\")).digest(),\n            hash_func=sha256,\n            expected=int(\n                \"8fa1f95d514760e498f28957b824ee6ec39ed64826ff4fecc2b5739ec45b91cd\",\n                16,\n            ),\n        )\n\n    def test_SECP256k1_2(self):\n        self._do(\n            generator=SECP256k1.generator,\n            secexp=int(\n                \"cca9fbcc1b41e5a95d369eaa6ddcff73b61a4efaa279cfc6567e8daa39cbaf50\",\n                16,\n            ),\n            hsh=sha256(b(\"sample\")).digest(),\n            hash_func=sha256,\n            expected=int(\n                \"2df40ca70e639d89528a6b670d9d48d9165fdc0febc0974056bdce192b8e16a3\",\n                16,\n            ),\n        )\n\n    def test_SECP256k1_3(self):\n        self._do(\n            generator=SECP256k1.generator,\n            secexp=0x1,\n            hsh=sha256(b(\"Satoshi Nakamoto\")).digest(),\n            hash_func=sha256,\n            expected=0x8F8A276C19F4149656B280621E358CCE24F5F52542772691EE69063B74F15D15,\n        )\n\n    def test_SECP256k1_4(self):\n        self._do(\n            generator=SECP256k1.generator,\n            secexp=0x1,\n            hsh=sha256(\n                b(\n                    \"All those moments will be lost in time, like tears in rain. Time to die...\"\n                )\n            ).digest(),\n            hash_func=sha256,\n            expected=0x38AA22D72376B4DBC472E06C3BA403EE0A394DA63FC58D88686C611ABA98D6B3,\n        )\n\n    def test_SECP256k1_5(self):\n        self._do(\n            generator=SECP256k1.generator,\n            secexp=0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364140,\n            hsh=sha256(b(\"Satoshi Nakamoto\")).digest(),\n            hash_func=sha256,\n            expected=0x33A19B60E25FB6F4435AF53A3D42D493644827367E6453928554F43E49AA6F90,\n        )\n\n    def test_SECP256k1_6(self):\n        self._do(\n            generator=SECP256k1.generator,\n            secexp=0xF8B8AF8CE3C7CCA5E300D33939540C10D45CE001B8F252BFBC57BA0342904181,\n            hsh=sha256(b(\"Alan Turing\")).digest(),\n            hash_func=sha256,\n            expected=0x525A82B70E67874398067543FD84C83D30C175FDC45FDEEE082FE13B1D7CFDF1,\n        )\n\n    def test_1(self):\n        # Basic example of the RFC, it also tests 'try-try-again' from Step H of rfc6979\n        self._do(\n            generator=Point(\n                None,\n                0,\n                0,\n                int(\"4000000000000000000020108A2E0CC0D99F8A5EF\", 16),\n            ),\n            secexp=int(\"09A4D6792295A7F730FC3F2B49CBC0F62E862272F\", 16),\n            hsh=unhexlify(\n                b(\n                    \"AF2BDBE1AA9B6EC1E2ADE1D694F41FC71A831D0268E9891562113D8A62ADD1BF\"\n                )\n            ),\n            hash_func=sha256,\n            expected=int(\"23AF4074C90A02B3FE61D286D5C87F425E6BDD81B\", 16),\n        )\n\n    def test_2(self):\n        self._do(\n            generator=NIST192p.generator,\n            secexp=int(\"6FAB034934E4C0FC9AE67F5B5659A9D7D1FEFD187EE09FD4\", 16),\n            hsh=sha1(b(\"sample\")).digest(),\n            hash_func=sha1,\n            expected=int(\n                \"37D7CA00D2C7B0E5E412AC03BD44BA837FDD5B28CD3B0021\", 16\n            ),\n        )\n\n    def test_3(self):\n        self._do(\n            generator=NIST192p.generator,\n            secexp=int(\"6FAB034934E4C0FC9AE67F5B5659A9D7D1FEFD187EE09FD4\", 16),\n            hsh=sha256(b(\"sample\")).digest(),\n            hash_func=sha256,\n            expected=int(\n                \"32B1B6D7D42A05CB449065727A84804FB1A3E34D8F261496\", 16\n            ),\n        )\n\n    def test_4(self):\n        self._do(\n            generator=NIST192p.generator,\n            secexp=int(\"6FAB034934E4C0FC9AE67F5B5659A9D7D1FEFD187EE09FD4\", 16),\n            hsh=sha512(b(\"sample\")).digest(),\n            hash_func=sha512,\n            expected=int(\n                \"A2AC7AB055E4F20692D49209544C203A7D1F2C0BFBC75DB1\", 16\n            ),\n        )\n\n    def test_5(self):\n        self._do(\n            generator=NIST192p.generator,\n            secexp=int(\"6FAB034934E4C0FC9AE67F5B5659A9D7D1FEFD187EE09FD4\", 16),\n            hsh=sha1(b(\"test\")).digest(),\n            hash_func=sha1,\n            expected=int(\n                \"D9CF9C3D3297D3260773A1DA7418DB5537AB8DD93DE7FA25\", 16\n            ),\n        )\n\n    def test_6(self):\n        self._do(\n            generator=NIST192p.generator,\n            secexp=int(\"6FAB034934E4C0FC9AE67F5B5659A9D7D1FEFD187EE09FD4\", 16),\n            hsh=sha256(b(\"test\")).digest(),\n            hash_func=sha256,\n            expected=int(\n                \"5C4CE89CF56D9E7C77C8585339B006B97B5F0680B4306C6C\", 16\n            ),\n        )\n\n    def test_7(self):\n        self._do(\n            generator=NIST192p.generator,\n            secexp=int(\"6FAB034934E4C0FC9AE67F5B5659A9D7D1FEFD187EE09FD4\", 16),\n            hsh=sha512(b(\"test\")).digest(),\n            hash_func=sha512,\n            expected=int(\n                \"0758753A5254759C7CFBAD2E2D9B0792EEE44136C9480527\", 16\n            ),\n        )\n\n    def test_8(self):\n        self._do(\n            generator=NIST521p.generator,\n            secexp=int(\n                \"0FAD06DAA62BA3B25D2FB40133DA757205DE67F5BB0018FEE8C86E1B68C7E75CAA896EB32F1F47C70855836A6D16FCC1466F6D8FBEC67DB89EC0C08B0E996B83538\",\n                16,\n            ),\n            hsh=sha1(b(\"sample\")).digest(),\n            hash_func=sha1,\n            expected=int(\n                \"089C071B419E1C2820962321787258469511958E80582E95D8378E0C2CCDB3CB42BEDE42F50E3FA3C71F5A76724281D31D9C89F0F91FC1BE4918DB1C03A5838D0F9\",\n                16,\n            ),\n        )\n\n    def test_9(self):\n        self._do(\n            generator=NIST521p.generator,\n            secexp=int(\n                \"0FAD06DAA62BA3B25D2FB40133DA757205DE67F5BB0018FEE8C86E1B68C7E75CAA896EB32F1F47C70855836A6D16FCC1466F6D8FBEC67DB89EC0C08B0E996B83538\",\n                16,\n            ),\n            hsh=sha256(b(\"sample\")).digest(),\n            hash_func=sha256,\n            expected=int(\n                \"0EDF38AFCAAECAB4383358B34D67C9F2216C8382AAEA44A3DAD5FDC9C32575761793FEF24EB0FC276DFC4F6E3EC476752F043CF01415387470BCBD8678ED2C7E1A0\",\n                16,\n            ),\n        )\n\n    def test_10(self):\n        self._do(\n            generator=NIST521p.generator,\n            secexp=int(\n                \"0FAD06DAA62BA3B25D2FB40133DA757205DE67F5BB0018FEE8C86E1B68C7E75CAA896EB32F1F47C70855836A6D16FCC1466F6D8FBEC67DB89EC0C08B0E996B83538\",\n                16,\n            ),\n            hsh=sha512(b(\"test\")).digest(),\n            hash_func=sha512,\n            expected=int(\n                \"16200813020EC986863BEDFC1B121F605C1215645018AEA1A7B215A564DE9EB1B38A67AA1128B80CE391C4FB71187654AAA3431027BFC7F395766CA988C964DC56D\",\n                16,\n            ),\n        )\n\n\nclass ECDH(unittest.TestCase):\n    def _do(self, curve, generator, dA, x_qA, y_qA, dB, x_qB, y_qB, x_Z, y_Z):\n        qA = dA * generator\n        qB = dB * generator\n        Z = dA * qB\n        self.assertEqual(Point(curve, x_qA, y_qA), qA)\n        self.assertEqual(Point(curve, x_qB, y_qB), qB)\n        self.assertTrue(\n            (dA * qB)\n            == (dA * dB * generator)\n            == (dB * dA * generator)\n            == (dB * qA)\n        )\n        self.assertEqual(Point(curve, x_Z, y_Z), Z)\n\n\nclass RFC6932(ECDH):\n    # https://tools.ietf.org/html/rfc6932#appendix-A.1\n\n    def test_brainpoolP224r1(self):\n        self._do(\n            curve=curve_brainpoolp224r1,\n            generator=BRAINPOOLP224r1.generator,\n            dA=int(\n                \"7C4B7A2C8A4BAD1FBB7D79CC0955DB7C6A4660CA64CC4778159B495E\", 16\n            ),\n            x_qA=int(\n                \"B104A67A6F6E85E14EC1825E1539E8ECDBBF584922367DD88C6BDCF2\", 16\n            ),\n            y_qA=int(\n                \"46D782E7FDB5F60CD8404301AC5949C58EDB26BC68BA07695B750A94\", 16\n            ),\n            dB=int(\n                \"63976D4AAE6CD0F6DD18DEFEF55D96569D0507C03E74D6486FFA28FB\", 16\n            ),\n            x_qB=int(\n                \"2A97089A9296147B71B21A4B574E1278245B536F14D8C2B9D07A874E\", 16\n            ),\n            y_qB=int(\n                \"9B900D7C77A709A797276B8CA1BA61BB95B546FC29F862E44D59D25B\", 16\n            ),\n            x_Z=int(\n                \"312DFD98783F9FB77B9704945A73BEB6DCCBE3B65D0F967DCAB574EB\", 16\n            ),\n            y_Z=int(\n                \"6F800811D64114B1C48C621AB3357CF93F496E4238696A2A012B3C98\", 16\n            ),\n        )\n\n    def test_brainpoolP256r1(self):\n        self._do(\n            curve=curve_brainpoolp256r1,\n            generator=BRAINPOOLP256r1.generator,\n            dA=int(\n                \"041EB8B1E2BC681BCE8E39963B2E9FC415B05283313DD1A8BCC055F11AE\"\n                \"49699\",\n                16,\n            ),\n            x_qA=int(\n                \"78028496B5ECAAB3C8B6C12E45DB1E02C9E4D26B4113BC4F015F60C5C\"\n                \"CC0D206\",\n                16,\n            ),\n            y_qA=int(\n                \"A2AE1762A3831C1D20F03F8D1E3C0C39AFE6F09B4D44BBE80CD100987\"\n                \"B05F92B\",\n                16,\n            ),\n            dB=int(\n                \"06F5240EACDB9837BC96D48274C8AA834B6C87BA9CC3EEDD81F99A16B8D\"\n                \"804D3\",\n                16,\n            ),\n            x_qB=int(\n                \"8E07E219BA588916C5B06AA30A2F464C2F2ACFC1610A3BE2FB240B635\"\n                \"341F0DB\",\n                16,\n            ),\n            y_qB=int(\n                \"148EA1D7D1E7E54B9555B6C9AC90629C18B63BEE5D7AA6949EBBF47B2\"\n                \"4FDE40D\",\n                16,\n            ),\n            x_Z=int(\n                \"05E940915549E9F6A4A75693716E37466ABA79B4BF2919877A16DD2CC2\"\n                \"E23708\",\n                16,\n            ),\n            y_Z=int(\n                \"6BC23B6702BC5A019438CEEA107DAAD8B94232FFBBC350F3B137628FE6\"\n                \"FD134C\",\n                16,\n            ),\n        )\n\n    def test_brainpoolP384r1(self):\n        self._do(\n            curve=curve_brainpoolp384r1,\n            generator=BRAINPOOLP384r1.generator,\n            dA=int(\n                \"014EC0755B78594BA47FB0A56F6173045B4331E74BA1A6F47322E70D79D\"\n                \"828D97E095884CA72B73FDABD5910DF0FA76A\",\n                16,\n            ),\n            x_qA=int(\n                \"45CB26E4384DAF6FB776885307B9A38B7AD1B5C692E0C32F012533277\"\n                \"8F3B8D3F50CA358099B30DEB5EE69A95C058B4E\",\n                16,\n            ),\n            y_qA=int(\n                \"8173A1C54AFFA7E781D0E1E1D12C0DC2B74F4DF58E4A4E3AF7026C5D3\"\n                \"2DC530A2CD89C859BB4B4B768497F49AB8CC859\",\n                16,\n            ),\n            dB=int(\n                \"6B461CB79BD0EA519A87D6828815D8CE7CD9B3CAA0B5A8262CBCD550A01\"\n                \"5C90095B976F3529957506E1224A861711D54\",\n                16,\n            ),\n            x_qB=int(\n                \"01BF92A92EE4BE8DED1A911125C209B03F99E3161CFCC986DC7711383\"\n                \"FC30AF9CE28CA3386D59E2C8D72CE1E7B4666E8\",\n                16,\n            ),\n            y_qB=int(\n                \"3289C4A3A4FEE035E39BDB885D509D224A142FF9FBCC5CFE5CCBB3026\"\n                \"8EE47487ED8044858D31D848F7A95C635A347AC\",\n                16,\n            ),\n            x_Z=int(\n                \"04CC4FF3DCCCB07AF24E0ACC529955B36D7C807772B92FCBE48F3AFE9A\"\n                \"2F370A1F98D3FA73FD0C0747C632E12F1423EC\",\n                16,\n            ),\n            y_Z=int(\n                \"7F465F90BD69AFB8F828A214EB9716D66ABC59F17AF7C75EE7F1DE22AB\"\n                \"5D05085F5A01A9382D05BF72D96698FE3FF64E\",\n                16,\n            ),\n        )\n\n    def test_brainpoolP512r1(self):\n        self._do(\n            curve=curve_brainpoolp512r1,\n            generator=BRAINPOOLP512r1.generator,\n            dA=int(\n                \"636B6BE0482A6C1C41AA7AE7B245E983392DB94CECEA2660A379CFE1595\"\n                \"59E357581825391175FC195D28BAC0CF03A7841A383B95C262B98378287\"\n                \"4CCE6FE333\",\n                16,\n            ),\n            x_qA=int(\n                \"0562E68B9AF7CBFD5565C6B16883B777FF11C199161ECC427A39D17EC\"\n                \"2166499389571D6A994977C56AD8252658BA8A1B72AE42F4FB7532151\"\n                \"AFC3EF0971CCDA\",\n                16,\n            ),\n            y_qA=int(\n                \"A7CA2D8191E21776A89860AFBC1F582FAA308D551C1DC6133AF9F9C3C\"\n                \"AD59998D70079548140B90B1F311AFB378AA81F51B275B2BE6B7DEE97\"\n                \"8EFC7343EA642E\",\n                16,\n            ),\n            dB=int(\n                \"0AF4E7F6D52EDD52907BB8DBAB3992A0BB696EC10DF11892FF205B66D38\"\n                \"1ECE72314E6A6EA079CEA06961DBA5AE6422EF2E9EE803A1F236FB96A17\"\n                \"99B86E5C8B\",\n                16,\n            ),\n            x_qB=int(\n                \"5A7954E32663DFF11AE24712D87419F26B708AC2B92877D6BFEE2BFC4\"\n                \"3714D89BBDB6D24D807BBD3AEB7F0C325F862E8BADE4F74636B97EAAC\"\n                \"E739E11720D323\",\n                16,\n            ),\n            y_qB=int(\n                \"96D14621A9283A1BED84DE8DD64836B2C0758B11441179DC0C54C0D49\"\n                \"A47C03807D171DD544B72CAAEF7B7CE01C7753E2CAD1A861ECA55A719\"\n                \"54EE1BA35E04BE\",\n                16,\n            ),\n            x_Z=int(\n                \"1EE8321A4BBF93B9CF8921AB209850EC9B7066D1984EF08C2BB7232362\"\n                \"08AC8F1A483E79461A00E0D5F6921CE9D360502F85C812BEDEE23AC5B2\"\n                \"10E5811B191E\",\n                16,\n            ),\n            y_Z=int(\n                \"2632095B7B936174B41FD2FAF369B1D18DCADEED7E410A7E251F083109\"\n                \"7C50D02CFED02607B6A2D5ADB4C0006008562208631875B58B54ECDA5A\"\n                \"4F9FE9EAABA6\",\n                16,\n            ),\n        )\n\n\nclass RFC7027(ECDH):\n    # https://tools.ietf.org/html/rfc7027#appendix-A\n\n    def test_brainpoolP256r1(self):\n        self._do(\n            curve=curve_brainpoolp256r1,\n            generator=BRAINPOOLP256r1.generator,\n            dA=int(\n                \"81DB1EE100150FF2EA338D708271BE38300CB54241D79950F77B0630398\"\n                \"04F1D\",\n                16,\n            ),\n            x_qA=int(\n                \"44106E913F92BC02A1705D9953A8414DB95E1AAA49E81D9E85F929A8E\"\n                \"3100BE5\",\n                16,\n            ),\n            y_qA=int(\n                \"8AB4846F11CACCB73CE49CBDD120F5A900A69FD32C272223F789EF10E\"\n                \"B089BDC\",\n                16,\n            ),\n            dB=int(\n                \"55E40BC41E37E3E2AD25C3C6654511FFA8474A91A0032087593852D3E7D\"\n                \"76BD3\",\n                16,\n            ),\n            x_qB=int(\n                \"8D2D688C6CF93E1160AD04CC4429117DC2C41825E1E9FCA0ADDD34E6F\"\n                \"1B39F7B\",\n                16,\n            ),\n            y_qB=int(\n                \"990C57520812BE512641E47034832106BC7D3E8DD0E4C7F1136D70065\"\n                \"47CEC6A\",\n                16,\n            ),\n            x_Z=int(\n                \"89AFC39D41D3B327814B80940B042590F96556EC91E6AE7939BCE31F3A\"\n                \"18BF2B\",\n                16,\n            ),\n            y_Z=int(\n                \"49C27868F4ECA2179BFD7D59B1E3BF34C1DBDE61AE12931648F43E5963\"\n                \"2504DE\",\n                16,\n            ),\n        )\n\n    def test_brainpoolP384r1(self):\n        self._do(\n            curve=curve_brainpoolp384r1,\n            generator=BRAINPOOLP384r1.generator,\n            dA=int(\n                \"1E20F5E048A5886F1F157C74E91BDE2B98C8B52D58E5003D57053FC4B0B\"\n                \"D65D6F15EB5D1EE1610DF870795143627D042\",\n                16,\n            ),\n            x_qA=int(\n                \"68B665DD91C195800650CDD363C625F4E742E8134667B767B1B476793\"\n                \"588F885AB698C852D4A6E77A252D6380FCAF068\",\n                16,\n            ),\n            y_qA=int(\n                \"55BC91A39C9EC01DEE36017B7D673A931236D2F1F5C83942D049E3FA2\"\n                \"0607493E0D038FF2FD30C2AB67D15C85F7FAA59\",\n                16,\n            ),\n            dB=int(\n                \"032640BC6003C59260F7250C3DB58CE647F98E1260ACCE4ACDA3DD869F7\"\n                \"4E01F8BA5E0324309DB6A9831497ABAC96670\",\n                16,\n            ),\n            x_qB=int(\n                \"4D44326F269A597A5B58BBA565DA5556ED7FD9A8A9EB76C25F46DB69D\"\n                \"19DC8CE6AD18E404B15738B2086DF37E71D1EB4\",\n                16,\n            ),\n            y_qB=int(\n                \"62D692136DE56CBE93BF5FA3188EF58BC8A3A0EC6C1E151A21038A42E\"\n                \"9185329B5B275903D192F8D4E1F32FE9CC78C48\",\n                16,\n            ),\n            x_Z=int(\n                \"0BD9D3A7EA0B3D519D09D8E48D0785FB744A6B355E6304BC51C229FBBC\"\n                \"E239BBADF6403715C35D4FB2A5444F575D4F42\",\n                16,\n            ),\n            y_Z=int(\n                \"0DF213417EBE4D8E40A5F76F66C56470C489A3478D146DECF6DF0D94BA\"\n                \"E9E598157290F8756066975F1DB34B2324B7BD\",\n                16,\n            ),\n        )\n\n    def test_brainpoolP512r1(self):\n        self._do(\n            curve=curve_brainpoolp512r1,\n            generator=BRAINPOOLP512r1.generator,\n            dA=int(\n                \"16302FF0DBBB5A8D733DAB7141C1B45ACBC8715939677F6A56850A38BD8\"\n                \"7BD59B09E80279609FF333EB9D4C061231FB26F92EEB04982A5F1D1764C\"\n                \"AD57665422\",\n                16,\n            ),\n            x_qA=int(\n                \"0A420517E406AAC0ACDCE90FCD71487718D3B953EFD7FBEC5F7F27E28\"\n                \"C6149999397E91E029E06457DB2D3E640668B392C2A7E737A7F0BF044\"\n                \"36D11640FD09FD\",\n                16,\n            ),\n            y_qA=int(\n                \"72E6882E8DB28AAD36237CD25D580DB23783961C8DC52DFA2EC138AD4\"\n                \"72A0FCEF3887CF62B623B2A87DE5C588301EA3E5FC269B373B60724F5\"\n                \"E82A6AD147FDE7\",\n                16,\n            ),\n            dB=int(\n                \"230E18E1BCC88A362FA54E4EA3902009292F7F8033624FD471B5D8ACE49\"\n                \"D12CFABBC19963DAB8E2F1EBA00BFFB29E4D72D13F2224562F405CB8050\"\n                \"3666B25429\",\n                16,\n            ),\n            x_qB=int(\n                \"9D45F66DE5D67E2E6DB6E93A59CE0BB48106097FF78A081DE781CDB31\"\n                \"FCE8CCBAAEA8DD4320C4119F1E9CD437A2EAB3731FA9668AB268D871D\"\n                \"EDA55A5473199F\",\n                16,\n            ),\n            y_qB=int(\n                \"2FDC313095BCDD5FB3A91636F07A959C8E86B5636A1E930E8396049CB\"\n                \"481961D365CC11453A06C719835475B12CB52FC3C383BCE35E27EF194\"\n                \"512B71876285FA\",\n                16,\n            ),\n            x_Z=int(\n                \"A7927098655F1F9976FA50A9D566865DC530331846381C87256BAF3226\"\n                \"244B76D36403C024D7BBF0AA0803EAFF405D3D24F11A9B5C0BEF679FE1\"\n                \"454B21C4CD1F\",\n                16,\n            ),\n            y_Z=int(\n                \"7DB71C3DEF63212841C463E881BDCF055523BD368240E6C3143BD8DEF8\"\n                \"B3B3223B95E0F53082FF5E412F4222537A43DF1C6D25729DDB51620A83\"\n                \"2BE6A26680A2\",\n                16,\n            ),\n        )\n\n\n# https://tools.ietf.org/html/rfc4754#page-5\n@pytest.mark.parametrize(\n    \"w, gwx, gwy, k, msg, md, r, s, curve\",\n    [\n        pytest.param(\n            \"DC51D3866A15BACDE33D96F992FCA99DA7E6EF0934E7097559C27F1614C88A7F\",\n            \"2442A5CC0ECD015FA3CA31DC8E2BBC70BF42D60CBCA20085E0822CB04235E970\",\n            \"6FC98BD7E50211A4A27102FA3549DF79EBCB4BF246B80945CDDFE7D509BBFD7D\",\n            \"9E56F509196784D963D1C0A401510EE7ADA3DCC5DEE04B154BF61AF1D5A6DECE\",\n            b\"abc\",\n            sha256,\n            \"CB28E0999B9C7715FD0A80D8E47A77079716CBBF917DD72E97566EA1C066957C\",\n            \"86FA3BB4E26CAD5BF90B7F81899256CE7594BB1EA0C89212748BFF3B3D5B0315\",\n            NIST256p,\n            id=\"ECDSA-256\",\n        ),\n        pytest.param(\n            \"0BEB646634BA87735D77AE4809A0EBEA865535DE4C1E1DCB692E84708E81A5AF\"\n            \"62E528C38B2A81B35309668D73524D9F\",\n            \"96281BF8DD5E0525CA049C048D345D3082968D10FEDF5C5ACA0C64E6465A97EA\"\n            \"5CE10C9DFEC21797415710721F437922\",\n            \"447688BA94708EB6E2E4D59F6AB6D7EDFF9301D249FE49C33096655F5D502FAD\"\n            \"3D383B91C5E7EDAA2B714CC99D5743CA\",\n            \"B4B74E44D71A13D568003D7489908D564C7761E229C58CBFA18950096EB7463B\"\n            \"854D7FA992F934D927376285E63414FA\",\n            b\"abc\",\n            sha384,\n            \"FB017B914E29149432D8BAC29A514640B46F53DDAB2C69948084E2930F1C8F7E\"\n            \"08E07C9C63F2D21A07DCB56A6AF56EB3\",\n            \"B263A1305E057F984D38726A1B46874109F417BCA112674C528262A40A629AF1\"\n            \"CBB9F516CE0FA7D2FF630863A00E8B9F\",\n            NIST384p,\n            id=\"ECDSA-384\",\n        ),\n        pytest.param(\n            \"0065FDA3409451DCAB0A0EAD45495112A3D813C17BFD34BDF8C1209D7DF58491\"\n            \"20597779060A7FF9D704ADF78B570FFAD6F062E95C7E0C5D5481C5B153B48B37\"\n            \"5FA1\",\n            \"0151518F1AF0F563517EDD5485190DF95A4BF57B5CBA4CF2A9A3F6474725A35F\"\n            \"7AFE0A6DDEB8BEDBCD6A197E592D40188901CECD650699C9B5E456AEA5ADD190\"\n            \"52A8\",\n            \"006F3B142EA1BFFF7E2837AD44C9E4FF6D2D34C73184BBAD90026DD5E6E85317\"\n            \"D9DF45CAD7803C6C20035B2F3FF63AFF4E1BA64D1C077577DA3F4286C58F0AEA\"\n            \"E643\",\n            \"00C1C2B305419F5A41344D7E4359933D734096F556197A9B244342B8B62F46F9\"\n            \"373778F9DE6B6497B1EF825FF24F42F9B4A4BD7382CFC3378A540B1B7F0C1B95\"\n            \"6C2F\",\n            b\"abc\",\n            sha512,\n            \"0154FD3836AF92D0DCA57DD5341D3053988534FDE8318FC6AAAAB68E2E6F4339\"\n            \"B19F2F281A7E0B22C269D93CF8794A9278880ED7DBB8D9362CAEACEE54432055\"\n            \"2251\",\n            \"017705A7030290D1CEB605A9A1BB03FF9CDD521E87A696EC926C8C10C8362DF4\"\n            \"975367101F67D1CF9BCCBF2F3D239534FA509E70AAC851AE01AAC68D62F86647\"\n            \"2660\",\n            NIST521p,\n            id=\"ECDSA-521\",\n        ),\n    ],\n)\ndef test_RFC4754_vectors(w, gwx, gwy, k, msg, md, r, s, curve):\n    sk = SigningKey.from_string(unhexlify(w), curve)\n    vk = VerifyingKey.from_string(unhexlify(gwx + gwy), curve)\n    assert sk.verifying_key == vk\n    sig = sk.sign(msg, hashfunc=md, sigencode=sigencode_strings, k=int(k, 16))\n\n    assert sig == (unhexlify(r), unhexlify(s))\n\n    assert vk.verify(sig, msg, md, sigdecode_strings)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/test_cpp.py", "content": "from __future__ import print_function\nfrom __future__ import absolute_import\n\nimport unittest\n\nimport greenlet\nfrom . import _test_extension_cpp\n\n\nclass CPPTests(unittest.TestCase):\n    def test_exception_switch(self):\n        greenlets = []\n        for i in range(4):\n            g = greenlet.greenlet(_test_extension_cpp.test_exception_switch)\n            g.switch(i)\n            greenlets.append(g)\n        for i, g in enumerate(greenlets):\n            self.assertEqual(g.switch(), i)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/ecdsa/test_keys.py", "content": "try:\n    import unittest2 as unittest\nexcept ImportError:\n    import unittest\n\ntry:\n    buffer\nexcept NameError:\n    buffer = memoryview\n\nimport os\nimport array\nimport pytest\nimport hashlib\n\nfrom .keys import VerifyingKey, SigningKey, MalformedPointError\nfrom .der import unpem, UnexpectedDER\nfrom .util import (\n    sigencode_string,\n    sigencode_der,\n    sigencode_strings,\n    sigdecode_string,\n    sigdecode_der,\n    sigdecode_strings,\n)\nfrom .curves import NIST256p, Curve, BRAINPOOLP160r1\nfrom .ellipticcurve import Point, PointJacobi, CurveFp, INFINITY\nfrom .ecdsa import generator_brainpoolp160r1\n\n\nclass TestVerifyingKeyFromString(unittest.TestCase):\n    \"\"\"\n    Verify that ecdsa.keys.VerifyingKey.from_string() can be used with\n    bytes-like objects\n    \"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        cls.key_bytes = (\n            b\"\\x04L\\xa2\\x95\\xdb\\xc7Z\\xd7\\x1f\\x93\\nz\\xcf\\x97\\xcf\"\n            b\"\\xd7\\xc2\\xd9o\\xfe8}X!\\xae\\xd4\\xfah\\xfa^\\rpI\\xba\\xd1\"\n            b\"Y\\xfb\\x92xa\\xebo+\\x9cG\\xfav\\xca\"\n        )\n        cls.vk = VerifyingKey.from_string(cls.key_bytes)\n\n    def test_bytes(self):\n        self.assertIsNotNone(self.vk)\n        self.assertIsInstance(self.vk, VerifyingKey)\n        self.assertEqual(\n            self.vk.pubkey.point.x(),\n            105419898848891948935835657980914000059957975659675736097,\n        )\n        self.assertEqual(\n            self.vk.pubkey.point.y(),\n            4286866841217412202667522375431381222214611213481632495306,\n        )\n\n    def test_bytes_memoryview(self):\n        vk = VerifyingKey.from_string(buffer(self.key_bytes))\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_bytearray(self):\n        vk = VerifyingKey.from_string(bytearray(self.key_bytes))\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_bytesarray_memoryview(self):\n        vk = VerifyingKey.from_string(buffer(bytearray(self.key_bytes)))\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_array_array_of_bytes(self):\n        arr = array.array(\"B\", self.key_bytes)\n        vk = VerifyingKey.from_string(arr)\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_array_array_of_bytes_memoryview(self):\n        arr = array.array(\"B\", self.key_bytes)\n        vk = VerifyingKey.from_string(buffer(arr))\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_array_array_of_ints(self):\n        arr = array.array(\"I\", self.key_bytes)\n        vk = VerifyingKey.from_string(arr)\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_array_array_of_ints_memoryview(self):\n        arr = array.array(\"I\", self.key_bytes)\n        vk = VerifyingKey.from_string(buffer(arr))\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_bytes_uncompressed(self):\n        vk = VerifyingKey.from_string(b\"\\x04\" + self.key_bytes)\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_bytearray_uncompressed(self):\n        vk = VerifyingKey.from_string(bytearray(b\"\\x04\" + self.key_bytes))\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_bytes_compressed(self):\n        vk = VerifyingKey.from_string(b\"\\x02\" + self.key_bytes[:24])\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_bytearray_compressed(self):\n        vk = VerifyingKey.from_string(bytearray(b\"\\x02\" + self.key_bytes[:24]))\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n\nclass TestVerifyingKeyFromDer(unittest.TestCase):\n    \"\"\"\n    Verify that ecdsa.keys.VerifyingKey.from_der() can be used with\n    bytes-like objects.\n    \"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        prv_key_str = (\n            \"-----BEGIN EC PRIVATE KEY-----\\n\"\n            \"MF8CAQEEGF7IQgvW75JSqULpiQQ8op9WH6Uldw6xxaAKBggqhkjOPQMBAaE0AzIA\\n\"\n            \"BLiBd9CE7xf15FY5QIAoNg+fWbSk1yZOYtoGUdzkejWkxbRc9RWTQjqLVXucIJnz\\n\"\n            \"bA==\\n\"\n            \"-----END EC PRIVATE KEY-----\\n\"\n        )\n        key_str = (\n            \"-----BEGIN PUBLIC KEY-----\\n\"\n            \"MEkwEwYHKoZIzj0CAQYIKoZIzj0DAQEDMgAEuIF30ITvF/XkVjlAgCg2D59ZtKTX\\n\"\n            \"Jk5i2gZR3OR6NaTFtFz1FZNCOotVe5wgmfNs\\n\"\n            \"-----END PUBLIC KEY-----\\n\"\n        )\n        cls.key_pem = key_str\n\n        cls.key_bytes = unpem(key_str)\n        assert isinstance(cls.key_bytes, bytes)\n        cls.vk = VerifyingKey.from_pem(key_str)\n        cls.sk = SigningKey.from_pem(prv_key_str)\n\n        key_str = (\n            \"-----BEGIN PUBLIC KEY-----\\n\"\n            \"MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE4H3iRbG4TSrsSRb/gusPQB/4YcN8\\n\"\n            \"Poqzgjau4kfxBPyZimeRfuY/9g/wMmPuhGl4BUve51DsnKJFRr8psk0ieA==\\n\"\n            \"-----END PUBLIC KEY-----\\n\"\n        )\n        cls.vk2 = VerifyingKey.from_pem(key_str)\n\n        cls.sk2 = SigningKey.generate(vk.curve)\n\n    def test_load_key_with_explicit_parameters(self):\n        pub_key_str = (\n            \"-----BEGIN PUBLIC KEY-----\\n\"\n            \"MIIBSzCCAQMGByqGSM49AgEwgfcCAQEwLAYHKoZIzj0BAQIhAP////8AAAABAAAA\\n\"\n            \"AAAAAAAAAAAA////////////////MFsEIP////8AAAABAAAAAAAAAAAAAAAA////\\n\"\n            \"///////////8BCBaxjXYqjqT57PrvVV2mIa8ZR0GsMxTsPY7zjw+J9JgSwMVAMSd\\n\"\n            \"NgiG5wSTamZ44ROdJreBn36QBEEEaxfR8uEsQkf4vOblY6RA8ncDfYEt6zOg9KE5\\n\"\n            \"RdiYwpZP40Li/hp/m47n60p8D54WK84zV2sxXs7LtkBoN79R9QIhAP////8AAAAA\\n\"\n            \"//////////+85vqtpxeehPO5ysL8YyVRAgEBA0IABIr1UkgYs5jmbFc7it1/YI2X\\n\"\n            \"T//IlaEjMNZft1owjqpBYH2ErJHk4U5Pp4WvWq1xmHwIZlsH7Ig4KmefCfR6SmU=\\n\"\n            \"-----END PUBLIC KEY-----\"\n        )\n        pk = VerifyingKey.from_pem(pub_key_str)\n\n        pk_exp = VerifyingKey.from_string(\n            b\"\\x04\\x8a\\xf5\\x52\\x48\\x18\\xb3\\x98\\xe6\\x6c\\x57\\x3b\\x8a\\xdd\\x7f\"\n            b\"\\x60\\x8d\\x97\\x4f\\xff\\xc8\\x95\\xa1\\x23\\x30\\xd6\\x5f\\xb7\\x5a\\x30\"\n            b\"\\x8e\\xaa\\x41\\x60\\x7d\\x84\\xac\\x91\\xe4\\xe1\\x4e\\x4f\\xa7\\x85\\xaf\"\n            b\"\\x5a\\xad\\x71\\x98\\x7c\\x08\\x66\\x5b\\x07\\xec\\x88\\x38\\x2a\\x67\\x9f\"\n            b\"\\x09\\xf4\\x7a\\x4a\\x65\",\n            curve=NIST256p,\n        )\n        self.assertEqual(pk, pk_exp)\n\n    def test_load_key_with_explicit_with_explicit_disabled(self):\n        pub_key_str = (\n            \"-----BEGIN PUBLIC KEY-----\\n\"\n            \"MIIBSzCCAQMGByqGSM49AgEwgfcCAQEwLAYHKoZIzj0BAQIhAP////8AAAABAAAA\\n\"\n            \"AAAAAAAAAAAA////////////////MFsEIP////8AAAABAAAAAAAAAAAAAAAA////\\n\"\n            \"///////////8BCBaxjXYqjqT57PrvVV2mIa8ZR0GsMxTsPY7zjw+J9JgSwMVAMSd\\n\"\n            \"NgiG5wSTamZ44ROdJreBn36QBEEEaxfR8uEsQkf4vOblY6RA8ncDfYEt6zOg9KE5\\n\"\n            \"RdiYwpZP40Li/hp/m47n60p8D54WK84zV2sxXs7LtkBoN79R9QIhAP////8AAAAA\\n\"\n            \"//////////+85vqtpxeehPO5ysL8YyVRAgEBA0IABIr1UkgYs5jmbFc7it1/YI2X\\n\"\n            \"T//IlaEjMNZft1owjqpBYH2ErJHk4U5Pp4WvWq1xmHwIZlsH7Ig4KmefCfR6SmU=\\n\"\n            \"-----END PUBLIC KEY-----\"\n        )\n        with self.assertRaises(UnexpectedDER):\n            VerifyingKey.from_pem(\n                pub_key_str, valid_curve_encodings=[\"named_curve\"]\n            )\n\n    def test_load_key_with_disabled_format(self):\n        with self.assertRaises(MalformedPointError) as e:\n            VerifyingKey.from_der(self.key_bytes, valid_encodings=[\"raw\"])\n\n        self.assertIn(\"enabled (raw) encodings\", str(e.exception))\n\n    def test_custom_hashfunc(self):\n        vk = VerifyingKey.from_der(self.key_bytes, hashlib.sha256)\n\n        self.assertIs(vk.default_hashfunc, hashlib.sha256)\n\n    def test_from_pem_with_custom_hashfunc(self):\n        vk = VerifyingKey.from_pem(self.key_pem, hashlib.sha256)\n\n        self.assertIs(vk.default_hashfunc, hashlib.sha256)\n\n    def test_bytes(self):\n        vk = VerifyingKey.from_der(self.key_bytes)\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_bytes_memoryview(self):\n        vk = VerifyingKey.from_der(buffer(self.key_bytes))\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_bytearray(self):\n        vk = VerifyingKey.from_der(bytearray(self.key_bytes))\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_bytesarray_memoryview(self):\n        vk = VerifyingKey.from_der(buffer(bytearray(self.key_bytes)))\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_array_array_of_bytes(self):\n        arr = array.array(\"B\", self.key_bytes)\n        vk = VerifyingKey.from_der(arr)\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_array_array_of_bytes_memoryview(self):\n        arr = array.array(\"B\", self.key_bytes)\n        vk = VerifyingKey.from_der(buffer(arr))\n\n        self.assertEqual(self.vk.to_string(), vk.to_string())\n\n    def test_equality_on_verifying_keys(self):\n        self.assertEqual(self.vk, self.sk.get_verifying_key())\n\n    def test_inequality_on_verifying_keys(self):\n        self.assertNotEqual(self.vk, self.vk2)\n\n    def test_inequality_on_verifying_keys_not_implemented(self):\n        self.assertNotEqual(self.vk, None)\n\n    def test_VerifyingKey_inequality_on_same_curve(self):\n        self.assertNotEqual(self.vk, self.sk2.verifying_key)\n\n    def test_SigningKey_inequality_on_same_curve(self):\n        self.assertNotEqual(self.sk, self.sk2)\n\n    def test_inequality_on_wrong_types(self):\n        self.assertNotEqual(self.vk, self.sk)\n\n    def test_from_public_point_old(self):\n        pj = self.vk.pubkey.point\n        point = Point(pj.curve(), pj.x(), pj.y())\n\n        vk = VerifyingKey.from_public_point(point, self.vk.curve)\n\n        self.assertEqual(vk, self.vk)\n\n\nclass TestSigningKey(unittest.TestCase):\n    \"\"\"\n    Verify that ecdsa.keys.SigningKey.from_der() can be used with\n    bytes-like objects.\n    \"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        prv_key_str = (\n            \"-----BEGIN EC PRIVATE KEY-----\\n\"\n            \"MF8CAQEEGF7IQgvW75JSqULpiQQ8op9WH6Uldw6xxaAKBggqhkjOPQMBAaE0AzIA\\n\"\n            \"BLiBd9CE7xf15FY5QIAoNg+fWbSk1yZOYtoGUdzkejWkxbRc9RWTQjqLVXucIJnz\\n\"\n            \"bA==\\n\"\n            \"-----END EC PRIVATE KEY-----\\n\"\n        )\n        cls.sk1 = SigningKey.from_pem(prv_key_str)\n\n        prv_key_str = (\n            \"-----BEGIN PRIVATE KEY-----\\n\"\n            \"MG8CAQAwEwYHKoZIzj0CAQYIKoZIzj0DAQEEVTBTAgEBBBheyEIL1u+SUqlC6YkE\\n\"\n            \"PKKfVh+lJXcOscWhNAMyAAS4gXfQhO8X9eRWOUCAKDYPn1m0pNcmTmLaBlHc5Ho1\\n\"\n            \"pMW0XPUVk0I6i1V7nCCZ82w=\\n\"\n            \"-----END PRIVATE KEY-----\\n\"\n        )\n        cls.sk1_pkcs8 = SigningKey.from_pem(prv_key_str)\n\n        prv_key_str = (\n            \"-----BEGIN EC PRIVATE KEY-----\\n\"\n            \"MHcCAQEEIKlL2EAm5NPPZuXwxRf4nXMk0A80y6UUbiQ17be/qFhRoAoGCCqGSM49\\n\"\n            \"AwEHoUQDQgAE4H3iRbG4TSrsSRb/gusPQB/4YcN8Poqzgjau4kfxBPyZimeRfuY/\\n\"\n            \"9g/wMmPuhGl4BUve51DsnKJFRr8psk0ieA==\\n\"\n            \"-----END EC PRIVATE KEY-----\\n\"\n        )\n        cls.sk2 = SigningKey.from_pem(prv_key_str)\n\n    def test_decoding_explicit_curve_parameters(self):\n        prv_key_str = (\n            \"-----BEGIN PRIVATE KEY-----\\n\"\n            \"MIIBeQIBADCCAQMGByqGSM49AgEwgfcCAQEwLAYHKoZIzj0BAQIhAP////8AAAAB\\n\"\n            \"AAAAAAAAAAAAAAAA////////////////MFsEIP////8AAAABAAAAAAAAAAAAAAAA\\n\"\n            \"///////////////8BCBaxjXYqjqT57PrvVV2mIa8ZR0GsMxTsPY7zjw+J9JgSwMV\\n\"\n            \"AMSdNgiG5wSTamZ44ROdJreBn36QBEEEaxfR8uEsQkf4vOblY6RA8ncDfYEt6zOg\\n\"\n            \"9KE5RdiYwpZP40Li/hp/m47n60p8D54WK84zV2sxXs7LtkBoN79R9QIhAP////8A\\n\"\n            \"AAAA//////////+85vqtpxeehPO5ysL8YyVRAgEBBG0wawIBAQQgIXtREfUmR16r\\n\"\n            \"ZbmvDGD2lAEFPZa2DLPyz0czSja58yChRANCAASK9VJIGLOY5mxXO4rdf2CNl0//\\n\"\n            \"yJWhIzDWX7daMI6qQWB9hKyR5OFOT6eFr1qtcZh8CGZbB+yIOCpnnwn0ekpl\\n\"\n            \"-----END PRIVATE KEY-----\\n\"\n        )\n\n        sk = SigningKey.from_pem(prv_key_str)\n\n        sk2 = SigningKey.from_string(\n            b\"\\x21\\x7b\\x51\\x11\\xf5\\x26\\x47\\x5e\\xab\\x65\\xb9\\xaf\\x0c\\x60\\xf6\"\n            b\"\\x94\\x01\\x05\\x3d\\x96\\xb6\\x0c\\xb3\\xf2\\xcf\\x47\\x33\\x4a\\x36\\xb9\"\n            b\"\\xf3\\x20\",\n            curve=NIST256p,\n        )\n\n        self.assertEqual(sk, sk2)\n\n    def test_decoding_explicit_curve_parameters_with_explicit_disabled(self):\n        prv_key_str = (\n            \"-----BEGIN PRIVATE KEY-----\\n\"\n            \"MIIBeQIBADCCAQMGByqGSM49AgEwgfcCAQEwLAYHKoZIzj0BAQIhAP////8AAAAB\\n\"\n            \"AAAAAAAAAAAAAAAA////////////////MFsEIP////8AAAABAAAAAAAAAAAAAAAA\\n\"\n            \"///////////////8BCBaxjXYqjqT57PrvVV2mIa8ZR0GsMxTsPY7zjw+J9JgSwMV\\n\"\n            \"AMSdNgiG5wSTamZ44ROdJreBn36QBEEEaxfR8uEsQkf4vOblY6RA8ncDfYEt6zOg\\n\"\n            \"9KE5RdiYwpZP40Li/hp/m47n60p8D54WK84zV2sxXs7LtkBoN79R9QIhAP////8A\\n\"\n            \"AAAA//////////+85vqtpxeehPO5ysL8YyVRAgEBBG0wawIBAQQgIXtREfUmR16r\\n\"\n            \"ZbmvDGD2lAEFPZa2DLPyz0czSja58yChRANCAASK9VJIGLOY5mxXO4rdf2CNl0//\\n\"\n            \"yJWhIzDWX7daMI6qQWB9hKyR5OFOT6eFr1qtcZh8CGZbB+yIOCpnnwn0ekpl\\n\"\n            \"-----END PRIVATE KEY-----\\n\"\n        )\n\n        with self.assertRaises(UnexpectedDER):\n            SigningKey.from_pem(\n                prv_key_str, valid_curve_encodings=[\"named_curve\"]\n            )\n\n    def test_equality_on_signing_keys(self):\n        sk = SigningKey.from_secret_exponent(\n            self.sk1.privkey.secret_multiplier, self.sk1.curve\n        )\n        self.assertEqual(self.sk1, sk)\n        self.assertEqual(self.sk1_pkcs8, sk)\n\n    def test_verify_with_precompute(self):\n        sig = self.sk1.sign(b\"message\")\n\n        vk = self.sk1.verifying_key\n\n        vk.precompute()\n\n        self.assertTrue(vk.verify(sig, b\"message\"))\n\n    def test_compare_verifying_key_with_precompute(self):\n        vk1 = self.sk1.verifying_key\n        vk1.precompute()\n\n        vk2 = self.sk1_pkcs8.verifying_key\n\n        self.assertEqual(vk1, vk2)\n\n    def test_verify_with_lazy_precompute(self):\n        sig = self.sk2.sign(b\"other message\")\n\n        vk = self.sk2.verifying_key\n\n        vk.precompute(lazy=True)\n\n        self.assertTrue(vk.verify(sig, b\"other message\"))\n\n    def test_inequality_on_signing_keys(self):\n        self.assertNotEqual(self.sk1, self.sk2)\n\n    def test_inequality_on_signing_keys_not_implemented(self):\n        self.assertNotEqual(self.sk1, None)\n\n\nclass TestTrivialCurve(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # To test what happens with r or s in signing happens to be zero we\n        # need to find a scalar that creates one of the points on a curve that\n        # has x coordinate equal to zero.\n        # Even for secp112r2 curve that's non trivial so use this toy\n        # curve, for which we can iterate over all points quickly\n        curve = CurveFp(163, 84, 58)\n        gen = PointJacobi(curve, 2, 87, 1, 167, generator=True)\n\n        cls.toy_curve = Curve(\"toy_p8\", curve, gen, (1, 2, 0))\n\n        cls.sk = SigningKey.from_secret_exponent(\n            140, cls.toy_curve, hashfunc=hashlib.sha1,\n        )\n\n    def test_generator_sanity(self):\n        gen = self.toy_curve.generator\n\n        self.assertEqual(gen * gen.order(), INFINITY)\n\n    def test_public_key_sanity(self):\n        self.assertEqual(self.sk.verifying_key.to_string(), b\"\\x98\\x1e\")\n\n    def test_deterministic_sign(self):\n        sig = self.sk.sign_deterministic(b\"message\")\n\n        self.assertEqual(sig, b\"-.\")\n\n        self.assertTrue(self.sk.verifying_key.verify(sig, b\"message\"))\n\n    def test_deterministic_sign_random_message(self):\n        msg = os.urandom(32)\n        sig = self.sk.sign_deterministic(msg)\n        self.assertEqual(len(sig), 2)\n        self.assertTrue(self.sk.verifying_key.verify(sig, msg))\n\n    def test_deterministic_sign_that_rises_R_zero_error(self):\n        # the raised RSZeroError is caught and handled internally by\n        # sign_deterministic methods\n        msg = b\"\\x00\\x4f\"\n        sig = self.sk.sign_deterministic(msg)\n        self.assertEqual(sig, b\"\\x36\\x9e\")\n        self.assertTrue(self.sk.verifying_key.verify(sig, msg))\n\n    def test_deterministic_sign_that_rises_S_zero_error(self):\n        msg = b\"\\x01\\x6d\"\n        sig = self.sk.sign_deterministic(msg)\n        self.assertEqual(sig, b\"\\x49\\x6c\")\n        self.assertTrue(self.sk.verifying_key.verify(sig, msg))\n\n\n# test VerifyingKey.verify()\nprv_key_str = (\n    \"-----BEGIN EC PRIVATE KEY-----\\n\"\n    \"MF8CAQEEGF7IQgvW75JSqULpiQQ8op9WH6Uldw6xxaAKBggqhkjOPQMBAaE0AzIA\\n\"\n    \"BLiBd9CE7xf15FY5QIAoNg+fWbSk1yZOYtoGUdzkejWkxbRc9RWTQjqLVXucIJnz\\n\"\n    \"bA==\\n\"\n    \"-----END EC PRIVATE KEY-----\\n\"\n)\nkey_bytes = unpem(prv_key_str)\nassert isinstance(key_bytes, bytes)\nsk = SigningKey.from_der(key_bytes)\nvk = sk.verifying_key\n\ndata = (\n    b\"some string for signing\"\n    b\"contents don't really matter\"\n    b\"but do include also some crazy values: \"\n    b\"\\x00\\x01\\t\\r\\n\\x00\\x00\\x00\\xff\\xf0\"\n)\nassert len(data) % 4 == 0\nsha1 = hashlib.sha1()\nsha1.update(data)\ndata_hash = sha1.digest()\nassert isinstance(data_hash, bytes)\nsig_raw = sk.sign(data, sigencode=sigencode_string)\nassert isinstance(sig_raw, bytes)\nsig_der = sk.sign(data, sigencode=sigencode_der)\nassert isinstance(sig_der, bytes)\nsig_strings = sk.sign(data, sigencode=sigencode_strings)\nassert isinstance(sig_strings[0], bytes)\n\nverifiers = []\nfor modifier, fun in [\n    (\"bytes\", lambda x: x),\n    (\"bytes memoryview\", lambda x: buffer(x)),\n    (\"bytearray\", lambda x: bytearray(x)),\n    (\"bytearray memoryview\", lambda x: buffer(bytearray(x))),\n    (\"array.array of bytes\", lambda x: array.array(\"B\", x)),\n    (\"array.array of bytes memoryview\", lambda x: buffer(array.array(\"B\", x))),\n    (\"array.array of ints\", lambda x: array.array(\"I\", x)),\n    (\"array.array of ints memoryview\", lambda x: buffer(array.array(\"I\", x))),\n]:\n    if \"ints\" in modifier:\n        conv = lambda x: x\n    else:\n        conv = fun\n    for sig_format, signature, decoder, mod_apply in [\n        (\"raw\", sig_raw, sigdecode_string, lambda x: conv(x)),\n        (\"der\", sig_der, sigdecode_der, lambda x: conv(x)),\n        (\n            \"strings\",\n            sig_strings,\n            sigdecode_strings,\n            lambda x: tuple(conv(i) for i in x),\n        ),\n    ]:\n        for method_name, vrf_mthd, vrf_data in [\n            (\"verify\", vk.verify, data),\n            (\"verify_digest\", vk.verify_digest, data_hash),\n        ]:\n            verifiers.append(\n                pytest.param(\n                    signature,\n                    decoder,\n                    mod_apply,\n                    fun,\n                    vrf_mthd,\n                    vrf_data,\n                    id=\"{2}-{0}-{1}\".format(modifier, sig_format, method_name),\n                )\n            )\n\n\n@pytest.mark.parametrize(\n    \"signature,decoder,mod_apply,fun,vrf_mthd,vrf_data\", verifiers\n)\ndef test_VerifyingKey_verify(\n    signature, decoder, mod_apply, fun, vrf_mthd, vrf_data\n):\n    sig = mod_apply(signature)\n\n    assert vrf_mthd(sig, fun(vrf_data), sigdecode=decoder)\n\n\n# test SigningKey.from_string()\nprv_key_bytes = (\n    b\"^\\xc8B\\x0b\\xd6\\xef\\x92R\\xa9B\\xe9\\x89\\x04<\\xa2\"\n    b\"\\x9fV\\x1f\\xa5%w\\x0e\\xb1\\xc5\"\n)\nassert len(prv_key_bytes) == 24\nconverters = []\nfor modifier, convert in [\n    (\"bytes\", lambda x: x),\n    (\"bytes memoryview\", buffer),\n    (\"bytearray\", bytearray),\n    (\"bytearray memoryview\", lambda x: buffer(bytearray(x))),\n    (\"array.array of bytes\", lambda x: array.array(\"B\", x)),\n    (\"array.array of bytes memoryview\", lambda x: buffer(array.array(\"B\", x))),\n    (\"array.array of ints\", lambda x: array.array(\"I\", x)),\n    (\"array.array of ints memoryview\", lambda x: buffer(array.array(\"I\", x))),\n]:\n    converters.append(pytest.param(convert, id=modifier))\n\n\n@pytest.mark.parametrize(\"convert\", converters)\ndef test_SigningKey_from_string(convert):\n    key = convert(prv_key_bytes)\n    sk = SigningKey.from_string(key)\n\n    assert sk.to_string() == prv_key_bytes\n\n\n# test SigningKey.from_der()\nprv_key_str = (\n    \"-----BEGIN EC PRIVATE KEY-----\\n\"\n    \"MF8CAQEEGF7IQgvW75JSqULpiQQ8op9WH6Uldw6xxaAKBggqhkjOPQMBAaE0AzIA\\n\"\n    \"BLiBd9CE7xf15FY5QIAoNg+fWbSk1yZOYtoGUdzkejWkxbRc9RWTQjqLVXucIJnz\\n\"\n    \"bA==\\n\"\n    \"-----END EC PRIVATE KEY-----\\n\"\n)\nkey_bytes = unpem(prv_key_str)\nassert isinstance(key_bytes, bytes)\n\n# last two converters are for array.array of ints, those require input\n# that's multiple of 4, which no curve we support produces\n@pytest.mark.parametrize(\"convert\", converters[:-2])\ndef test_SigningKey_from_der(convert):\n    key = convert(key_bytes)\n    sk = SigningKey.from_der(key)\n\n    assert sk.to_string() == prv_key_bytes\n\n\n# test SigningKey.sign_deterministic()\nextra_entropy = b\"\\x0a\\x0b\\x0c\\x0d\\x0e\\x0f\\x10\\x11\"\n\n\n@pytest.mark.parametrize(\"convert\", converters)\ndef test_SigningKey_sign_deterministic(convert):\n    sig = sk.sign_deterministic(\n        convert(data), extra_entropy=convert(extra_entropy)\n    )\n\n    vk.verify(sig, data)\n\n\n# test SigningKey.sign_digest_deterministic()\n@pytest.mark.parametrize(\"convert\", converters)\ndef test_SigningKey_sign_digest_deterministic(convert):\n    sig = sk.sign_digest_deterministic(\n        convert(data_hash), extra_entropy=convert(extra_entropy)\n    )\n\n    vk.verify(sig, data)\n\n\n@pytest.mark.parametrize(\"convert\", converters)\ndef test_SigningKey_sign(convert):\n    sig = sk.sign(convert(data))\n\n    vk.verify(sig, data)\n\n\n@pytest.mark.parametrize(\"convert\", converters)\ndef test_SigningKey_sign_digest(convert):\n    sig = sk.sign_digest(convert(data_hash))\n\n    vk.verify(sig, data)\n\n\ndef test_SigningKey_with_unlikely_value():\n    sk = SigningKey.from_secret_exponent(NIST256p.order - 1, curve=NIST256p)\n    vk = sk.verifying_key\n    sig = sk.sign(b\"hello\")\n    assert vk.verify(sig, b\"hello\")\n\n\ndef test_SigningKey_with_custom_curve_old_point():\n    generator = generator_brainpoolp160r1\n    generator = Point(\n        generator.curve(), generator.x(), generator.y(), generator.order(),\n    )\n\n    curve = Curve(\n        \"BRAINPOOLP160r1\",\n        generator.curve(),\n        generator,\n        (1, 3, 36, 3, 3, 2, 8, 1, 1, 1),\n    )\n\n    sk = SigningKey.from_secret_exponent(12, curve)\n\n    sk2 = SigningKey.from_secret_exponent(12, BRAINPOOLP160r1)\n\n    assert sk.privkey == sk2.privkey\n\n\ndef test_VerifyingKey_inequality_with_different_curves():\n    sk1 = SigningKey.from_secret_exponent(2, BRAINPOOLP160r1)\n    sk2 = SigningKey.from_secret_exponent(2, NIST256p)\n\n    assert sk1.verifying_key != sk2.verifying_key\n\n\ndef test_VerifyingKey_inequality_with_different_secret_points():\n    sk1 = SigningKey.from_secret_exponent(2, BRAINPOOLP160r1)\n    sk2 = SigningKey.from_secret_exponent(3, BRAINPOOLP160r1)\n\n    assert sk1.verifying_key != sk2.verifying_key\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/test_gc.py", "content": "import gc\nimport sys\nimport unittest\nimport weakref\n\nimport greenlet\n\n\nclass GCTests(unittest.TestCase):\n    def test_dead_circular_ref(self):\n        o = weakref.ref(greenlet.greenlet(greenlet.getcurrent).switch())\n        gc.collect()\n        self.assertTrue(o() is None)\n        self.assertFalse(gc.garbage, gc.garbage)\n\n    if greenlet.GREENLET_USE_GC:\n        # These only work with greenlet gc support\n\n        def test_circular_greenlet(self):\n            class circular_greenlet(greenlet.greenlet):\n                pass\n            o = circular_greenlet()\n            o.self = o\n            o = weakref.ref(o)\n            gc.collect()\n            self.assertTrue(o() is None)\n            self.assertFalse(gc.garbage, gc.garbage)\n\n        def test_inactive_ref(self):\n            class inactive_greenlet(greenlet.greenlet):\n                def __init__(self):\n                    greenlet.greenlet.__init__(self, run=self.run)\n\n                def run(self):\n                    pass\n            o = inactive_greenlet()\n            o = weakref.ref(o)\n            gc.collect()\n            self.assertTrue(o() is None)\n            self.assertFalse(gc.garbage, gc.garbage)\n\n        def test_finalizer_crash(self):\n            # This test is designed to crash when active greenlets\n            # are made garbage collectable, until the underlying\n            # problem is resolved. How does it work:\n            # - order of object creation is important\n            # - array is created first, so it is moved to unreachable first\n            # - we create a cycle between a greenlet and this array\n            # - we create an object that participates in gc, is only\n            #   referenced by a greenlet, and would corrupt gc lists\n            #   on destruction, the easiest is to use an object with\n            #   a finalizer\n            # - because array is the first object in unreachable it is\n            #   cleared first, which causes all references to greenlet\n            #   to disappear and causes greenlet to be destroyed, but since\n            #   it is still live it causes a switch during gc, which causes\n            #   an object with finalizer to be destroyed, which causes stack\n            #   corruption and then a crash\n            class object_with_finalizer(object):\n                def __del__(self):\n                    pass\n            array = []\n            parent = greenlet.getcurrent()\n            def greenlet_body():\n                greenlet.getcurrent().object = object_with_finalizer()\n                try:\n                    parent.switch()\n                finally:\n                    del greenlet.getcurrent().object\n            g = greenlet.greenlet(greenlet_body)\n            g.array = array\n            array.append(g)\n            g.switch()\n            del array\n            del g\n            greenlet.getcurrent()\n            gc.collect()\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/ecdsa/test_curves.py", "content": "try:\n    import unittest2 as unittest\nexcept ImportError:\n    import unittest\n\nimport base64\nimport pytest\nfrom .curves import Curve, NIST256p, curves, UnknownCurveError, PRIME_FIELD_OID\nfrom .ellipticcurve import CurveFp, PointJacobi\nfrom . import der\nfrom .util import number_to_string\n\n\nclass TestParameterEncoding(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        # minimal, but with cofactor (excludes seed when compared to\n        # OpenSSL output)\n        cls.base64_params = (\n            \"MIHgAgEBMCwGByqGSM49AQECIQD/////AAAAAQAAAAAAAAAAAAAAAP/////////\"\n            \"//////zBEBCD/////AAAAAQAAAAAAAAAAAAAAAP///////////////AQgWsY12K\"\n            \"o6k+ez671VdpiGvGUdBrDMU7D2O848PifSYEsEQQRrF9Hy4SxCR/i85uVjpEDyd\"\n            \"wN9gS3rM6D0oTlF2JjClk/jQuL+Gn+bjufrSnwPnhYrzjNXazFezsu2QGg3v1H1\"\n            \"AiEA/////wAAAAD//////////7zm+q2nF56E87nKwvxjJVECAQE=\"\n        )\n\n    def test_from_pem(self):\n        pem_params = (\n            \"-----BEGIN EC PARAMETERS-----\\n\"\n            \"MIHgAgEBMCwGByqGSM49AQECIQD/////AAAAAQAAAAAAAAAAAAAAAP/////////\\n\"\n            \"//////zBEBCD/////AAAAAQAAAAAAAAAAAAAAAP///////////////AQgWsY12K\\n\"\n            \"o6k+ez671VdpiGvGUdBrDMU7D2O848PifSYEsEQQRrF9Hy4SxCR/i85uVjpEDyd\\n\"\n            \"wN9gS3rM6D0oTlF2JjClk/jQuL+Gn+bjufrSnwPnhYrzjNXazFezsu2QGg3v1H1\\n\"\n            \"AiEA/////wAAAAD//////////7zm+q2nF56E87nKwvxjJVECAQE=\\n\"\n            \"-----END EC PARAMETERS-----\\n\"\n        )\n        curve = Curve.from_pem(pem_params)\n\n        self.assertIs(curve, NIST256p)\n\n    def test_from_pem_with_explicit_when_explicit_disabled(self):\n        pem_params = (\n            \"-----BEGIN EC PARAMETERS-----\\n\"\n            \"MIHgAgEBMCwGByqGSM49AQECIQD/////AAAAAQAAAAAAAAAAAAAAAP/////////\\n\"\n            \"//////zBEBCD/////AAAAAQAAAAAAAAAAAAAAAP///////////////AQgWsY12K\\n\"\n            \"o6k+ez671VdpiGvGUdBrDMU7D2O848PifSYEsEQQRrF9Hy4SxCR/i85uVjpEDyd\\n\"\n            \"wN9gS3rM6D0oTlF2JjClk/jQuL+Gn+bjufrSnwPnhYrzjNXazFezsu2QGg3v1H1\\n\"\n            \"AiEA/////wAAAAD//////////7zm+q2nF56E87nKwvxjJVECAQE=\\n\"\n            \"-----END EC PARAMETERS-----\\n\"\n        )\n        with self.assertRaises(der.UnexpectedDER) as e:\n            Curve.from_pem(pem_params, [\"named_curve\"])\n\n        self.assertIn(\"explicit curve parameters not\", str(e.exception))\n\n    def test_from_pem_with_named_curve_with_named_curve_disabled(self):\n        pem_params = (\n            \"-----BEGIN EC PARAMETERS-----\\n\"\n            \"BggqhkjOPQMBBw==\\n\"\n            \"-----END EC PARAMETERS-----\\n\"\n        )\n        with self.assertRaises(der.UnexpectedDER) as e:\n            Curve.from_pem(pem_params, [\"explicit\"])\n\n        self.assertIn(\"named_curve curve parameters not\", str(e.exception))\n\n    def test_from_pem_with_wrong_header(self):\n        pem_params = (\n            \"-----BEGIN PARAMETERS-----\\n\"\n            \"MIHgAgEBMCwGByqGSM49AQECIQD/////AAAAAQAAAAAAAAAAAAAAAP/////////\\n\"\n            \"//////zBEBCD/////AAAAAQAAAAAAAAAAAAAAAP///////////////AQgWsY12K\\n\"\n            \"o6k+ez671VdpiGvGUdBrDMU7D2O848PifSYEsEQQRrF9Hy4SxCR/i85uVjpEDyd\\n\"\n            \"wN9gS3rM6D0oTlF2JjClk/jQuL+Gn+bjufrSnwPnhYrzjNXazFezsu2QGg3v1H1\\n\"\n            \"AiEA/////wAAAAD//////////7zm+q2nF56E87nKwvxjJVECAQE=\\n\"\n            \"-----END PARAMETERS-----\\n\"\n        )\n        with self.assertRaises(der.UnexpectedDER) as e:\n            Curve.from_pem(pem_params)\n\n        self.assertIn(\"PARAMETERS PEM header\", str(e.exception))\n\n    def test_to_pem(self):\n        pem_params = (\n            b\"-----BEGIN EC PARAMETERS-----\\n\"\n            b\"BggqhkjOPQMBBw==\\n\"\n            b\"-----END EC PARAMETERS-----\\n\"\n        )\n        encoding = NIST256p.to_pem()\n\n        self.assertEqual(pem_params, encoding)\n\n    def test_compare_with_different_object(self):\n        self.assertNotEqual(NIST256p, 256)\n\n    def test_named_curve_params_der(self):\n        encoded = NIST256p.to_der()\n\n        # just the encoding of the NIST256p OID (prime256v1)\n        self.assertEqual(b\"\\x06\\x08\\x2a\\x86\\x48\\xce\\x3d\\x03\\x01\\x07\", encoded)\n\n    def test_verify_that_default_is_named_curve_der(self):\n        encoded_default = NIST256p.to_der()\n        encoded_named = NIST256p.to_der(\"named_curve\")\n\n        self.assertEqual(encoded_default, encoded_named)\n\n    def test_encoding_to_explicit_params(self):\n        encoded = NIST256p.to_der(\"explicit\")\n\n        self.assertEqual(encoded, bytes(base64.b64decode(self.base64_params)))\n\n    def test_encoding_to_explicit_compressed_params(self):\n        encoded = NIST256p.to_der(\"explicit\", \"compressed\")\n\n        compressed_base_point = (\n            \"MIHAAgEBMCwGByqGSM49AQECIQD/////AAAAAQAAAAAAAAAAAAAAAP//////////\"\n            \"/////zBEBCD/////AAAAAQAAAAAAAAAAAAAAAP///////////////AQgWsY12Ko6\"\n            \"k+ez671VdpiGvGUdBrDMU7D2O848PifSYEsEIQNrF9Hy4SxCR/i85uVjpEDydwN9\"\n            \"gS3rM6D0oTlF2JjClgIhAP////8AAAAA//////////+85vqtpxeehPO5ysL8YyVR\"\n            \"AgEB\"\n        )\n\n        self.assertEqual(\n            encoded, bytes(base64.b64decode(compressed_base_point))\n        )\n\n    def test_decoding_explicit_from_openssl(self):\n        # generated with openssl 1.1.1k using\n        # openssl ecparam -name P-256 -param_enc explicit -out /tmp/file.pem\n        p256_explicit = (\n            \"MIH3AgEBMCwGByqGSM49AQECIQD/////AAAAAQAAAAAAAAAAAAAAAP//////////\"\n            \"/////zBbBCD/////AAAAAQAAAAAAAAAAAAAAAP///////////////AQgWsY12Ko6\"\n            \"k+ez671VdpiGvGUdBrDMU7D2O848PifSYEsDFQDEnTYIhucEk2pmeOETnSa3gZ9+\"\n            \"kARBBGsX0fLhLEJH+Lzm5WOkQPJ3A32BLeszoPShOUXYmMKWT+NC4v4af5uO5+tK\"\n            \"fA+eFivOM1drMV7Oy7ZAaDe/UfUCIQD/////AAAAAP//////////vOb6racXnoTz\"\n            \"ucrC/GMlUQIBAQ==\"\n        )\n\n        decoded = Curve.from_der(bytes(base64.b64decode(p256_explicit)))\n\n        self.assertEqual(NIST256p, decoded)\n\n    def test_decoding_well_known_from_explicit_params(self):\n        curve = Curve.from_der(bytes(base64.b64decode(self.base64_params)))\n\n        self.assertIs(curve, NIST256p)\n\n    def test_decoding_with_incorrect_valid_encodings(self):\n        with self.assertRaises(ValueError) as e:\n            Curve.from_der(b\"\", [\"explicitCA\"])\n\n        self.assertIn(\"Only named_curve\", str(e.exception))\n\n    def test_compare_curves_with_different_generators(self):\n        curve_fp = CurveFp(23, 1, 7)\n        base_a = PointJacobi(curve_fp, 13, 3, 1, 9, generator=True)\n        base_b = PointJacobi(curve_fp, 1, 20, 1, 9, generator=True)\n\n        curve_a = Curve(\"unknown\", curve_fp, base_a, None)\n        curve_b = Curve(\"unknown\", curve_fp, base_b, None)\n\n        self.assertNotEqual(curve_a, curve_b)\n\n    def test_default_encode_for_custom_curve(self):\n        curve_fp = CurveFp(23, 1, 7)\n        base_point = PointJacobi(curve_fp, 13, 3, 1, 9, generator=True)\n\n        curve = Curve(\"unknown\", curve_fp, base_point, None)\n\n        encoded = curve.to_der()\n\n        decoded = Curve.from_der(encoded)\n\n        self.assertEqual(curve, decoded)\n\n        expected = \"MCECAQEwDAYHKoZIzj0BAQIBFzAGBAEBBAEHBAMEDQMCAQk=\"\n\n        self.assertEqual(encoded, bytes(base64.b64decode(expected)))\n\n    def test_named_curve_encode_for_custom_curve(self):\n        curve_fp = CurveFp(23, 1, 7)\n        base_point = PointJacobi(curve_fp, 13, 3, 1, 9, generator=True)\n\n        curve = Curve(\"unknown\", curve_fp, base_point, None)\n\n        with self.assertRaises(UnknownCurveError) as e:\n            curve.to_der(\"named_curve\")\n\n        self.assertIn(\"Can't encode curve\", str(e.exception))\n\n    def test_try_decoding_binary_explicit(self):\n        sect113r1_explicit = (\n            \"MIGRAgEBMBwGByqGSM49AQIwEQIBcQYJKoZIzj0BAgMCAgEJMDkEDwAwiCUMpufH\"\n            \"/mSc6Fgg9wQPAOi+5NPiJgdEGIvg6ccjAxUAEOcjqxTWluZ2h1YVF1b+v4/LSakE\"\n            \"HwQAnXNhbzX0qxQH1zViwQ8ApSgwJ3lY7oTRMV7TGIYCDwEAAAAAAAAA2czsijnl\"\n            \"bwIBAg==\"\n        )\n\n        with self.assertRaises(UnknownCurveError) as e:\n            Curve.from_der(base64.b64decode(sect113r1_explicit))\n\n        self.assertIn(\"Characteristic 2 curves unsupported\", str(e.exception))\n\n    def test_decode_malformed_named_curve(self):\n        bad_der = der.encode_oid(*NIST256p.oid) + der.encode_integer(1)\n\n        with self.assertRaises(der.UnexpectedDER) as e:\n            Curve.from_der(bad_der)\n\n        self.assertIn(\"Unexpected data after OID\", str(e.exception))\n\n    def test_decode_malformed_explicit_garbage_after_ECParam(self):\n        bad_der = bytes(\n            base64.b64decode(self.base64_params)\n        ) + der.encode_integer(1)\n\n        with self.assertRaises(der.UnexpectedDER) as e:\n            Curve.from_der(bad_der)\n\n        self.assertIn(\"Unexpected data after ECParameters\", str(e.exception))\n\n    def test_decode_malformed_unknown_version_number(self):\n        bad_der = der.encode_sequence(der.encode_integer(2))\n\n        with self.assertRaises(der.UnexpectedDER) as e:\n            Curve.from_der(bad_der)\n\n        self.assertIn(\"Unknown parameter encoding format\", str(e.exception))\n\n    def test_decode_malformed_unknown_field_type(self):\n        curve_p = NIST256p.curve.p()\n        bad_der = der.encode_sequence(\n            der.encode_integer(1),\n            der.encode_sequence(\n                der.encode_oid(1, 2, 3), der.encode_integer(curve_p)\n            ),\n            der.encode_sequence(\n                der.encode_octet_string(\n                    number_to_string(NIST256p.curve.a() % curve_p, curve_p)\n                ),\n                der.encode_octet_string(\n                    number_to_string(NIST256p.curve.b(), curve_p)\n                ),\n            ),\n            der.encode_octet_string(\n                NIST256p.generator.to_bytes(\"uncompressed\")\n            ),\n            der.encode_integer(NIST256p.generator.order()),\n        )\n\n        with self.assertRaises(UnknownCurveError) as e:\n            Curve.from_der(bad_der)\n\n        self.assertIn(\"Unknown field type: (1, 2, 3)\", str(e.exception))\n\n    def test_decode_malformed_garbage_after_prime(self):\n        curve_p = NIST256p.curve.p()\n        bad_der = der.encode_sequence(\n            der.encode_integer(1),\n            der.encode_sequence(\n                der.encode_oid(*PRIME_FIELD_OID),\n                der.encode_integer(curve_p),\n                der.encode_integer(1),\n            ),\n            der.encode_sequence(\n                der.encode_octet_string(\n                    number_to_string(NIST256p.curve.a() % curve_p, curve_p)\n                ),\n                der.encode_octet_string(\n                    number_to_string(NIST256p.curve.b(), curve_p)\n                ),\n            ),\n            der.encode_octet_string(\n                NIST256p.generator.to_bytes(\"uncompressed\")\n            ),\n            der.encode_integer(NIST256p.generator.order()),\n        )\n\n        with self.assertRaises(der.UnexpectedDER) as e:\n            Curve.from_der(bad_der)\n\n        self.assertIn(\"Prime-p element\", str(e.exception))\n\n\n@pytest.mark.parametrize(\"curve\", curves, ids=[i.name for i in curves])\ndef test_curve_params_encode_decode_named(curve):\n    ret = Curve.from_der(curve.to_der(\"named_curve\"))\n\n    assert curve == ret\n\n\n@pytest.mark.parametrize(\"curve\", curves, ids=[i.name for i in curves])\ndef test_curve_params_encode_decode_explicit(curve):\n    ret = Curve.from_der(curve.to_der(\"explicit\"))\n\n    assert curve == ret\n\n\n@pytest.mark.parametrize(\"curve\", curves, ids=[i.name for i in curves])\ndef test_curve_params_encode_decode_default(curve):\n    ret = Curve.from_der(curve.to_der())\n\n    assert curve == ret\n\n\n@pytest.mark.parametrize(\"curve\", curves, ids=[i.name for i in curves])\ndef test_curve_params_encode_decode_explicit_compressed(curve):\n    ret = Curve.from_der(curve.to_der(\"explicit\", \"compressed\"))\n\n    assert curve == ret\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/test_extension_interface.py", "content": "from __future__ import print_function\nfrom __future__ import absolute_import\n\nimport sys\nimport unittest\n\nimport greenlet\nfrom . import _test_extension\n\n\nclass CAPITests(unittest.TestCase):\n    def test_switch(self):\n        self.assertEqual(\n            50, _test_extension.test_switch(greenlet.greenlet(lambda: 50)))\n\n    def test_switch_kwargs(self):\n        def foo(x, y):\n            return x * y\n        g = greenlet.greenlet(foo)\n        self.assertEqual(6, _test_extension.test_switch_kwargs(g, x=3, y=2))\n\n    def test_setparent(self):\n        def foo():\n            def bar():\n                greenlet.getcurrent().parent.switch()\n\n                # This final switch should go back to the main greenlet, since\n                # the test_setparent() function in the C extension should have\n                # reparented this greenlet.\n                greenlet.getcurrent().parent.switch()\n                raise AssertionError(\"Should never have reached this code\")\n            child = greenlet.greenlet(bar)\n            child.switch()\n            greenlet.getcurrent().parent.switch(child)\n            greenlet.getcurrent().parent.throw(\n                AssertionError(\"Should never reach this code\"))\n        foo_child = greenlet.greenlet(foo).switch()\n        self.assertEqual(None, _test_extension.test_setparent(foo_child))\n\n    def test_getcurrent(self):\n        _test_extension.test_getcurrent()\n\n    def test_new_greenlet(self):\n        self.assertEqual(-15, _test_extension.test_new_greenlet(lambda: -15))\n\n    def test_raise_greenlet_dead(self):\n        self.assertRaises(\n            greenlet.GreenletExit, _test_extension.test_raise_dead_greenlet)\n\n    def test_raise_greenlet_error(self):\n        self.assertRaises(\n            greenlet.error, _test_extension.test_raise_greenlet_error)\n\n    def test_throw(self):\n        seen = []\n\n        def foo():\n            try:\n                greenlet.getcurrent().parent.switch()\n            except ValueError:\n                seen.append(sys.exc_info()[1])\n            except greenlet.GreenletExit:\n                raise AssertionError\n        g = greenlet.greenlet(foo)\n        g.switch()\n        _test_extension.test_throw(g)\n        self.assertEqual(len(seen), 1)\n        self.assertTrue(\n            isinstance(seen[0], ValueError),\n            \"ValueError was not raised in foo()\")\n        self.assertEqual(\n            str(seen[0]),\n            'take that sucka!',\n            \"message doesn't match\")\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/test_generator.py", "content": "import unittest\nfrom greenlet import greenlet\n\n\nclass genlet(greenlet):\n\n    def __init__(self, *args, **kwds):\n        self.args = args\n        self.kwds = kwds\n\n    def run(self):\n        fn, = self.fn\n        fn(*self.args, **self.kwds)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        self.parent = greenlet.getcurrent()\n        result = self.switch()\n        if self:\n            return result\n        else:\n            raise StopIteration\n\n    # Hack: Python < 2.6 compatibility\n    next = __next__\n\n\ndef Yield(value):\n    g = greenlet.getcurrent()\n    while not isinstance(g, genlet):\n        if g is None:\n            raise RuntimeError('yield outside a genlet')\n        g = g.parent\n    g.parent.switch(value)\n\n\ndef generator(func):\n    class generator(genlet):\n        fn = (func,)\n    return generator\n\n# ____________________________________________________________\n\n\nclass GeneratorTests(unittest.TestCase):\n    def test_generator(self):\n        seen = []\n\n        def g(n):\n            for i in range(n):\n                seen.append(i)\n                Yield(i)\n        g = generator(g)\n        for k in range(3):\n            for j in g(5):\n                seen.append(j)\n        self.assertEqual(seen, 3 * [0, 0, 1, 1, 2, 2, 3, 3, 4, 4])\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/ecdsa/test_ellipticcurve.py", "content": "import pytest\n\ntry:\n    import unittest2 as unittest\nexcept ImportError:\n    import unittest\nfrom hypothesis import given, settings\nimport hypothesis.strategies as st\n\ntry:\n    from hypothesis import HealthCheck\n\n    HC_PRESENT = True\nexcept ImportError:  # pragma: no cover\n    HC_PRESENT = False\nfrom .numbertheory import inverse_mod\nfrom .ellipticcurve import CurveFp, INFINITY, Point\n\n\nHYP_SETTINGS = {}\nif HC_PRESENT:  # pragma: no branch\n    HYP_SETTINGS[\"suppress_health_check\"] = [HealthCheck.too_slow]\n    HYP_SETTINGS[\"deadline\"] = 5000\n\n\n# NIST Curve P-192:\np = 6277101735386680763835789423207666416083908700390324961279\nr = 6277101735386680763835789423176059013767194773182842284081\n# s = 0x3045ae6fc8422f64ed579528d38120eae12196d5\n# c = 0x3099d2bbbfcb2538542dcd5fb078b6ef5f3d6fe2c745de65\nb = 0x64210519E59C80E70FA7E9AB72243049FEB8DEECC146B9B1\nGx = 0x188DA80EB03090F67CBF20EB43A18800F4FF0AFD82FF1012\nGy = 0x07192B95FFC8DA78631011ED6B24CDD573F977A11E794811\n\nc192 = CurveFp(p, -3, b)\np192 = Point(c192, Gx, Gy, r)\n\nc_23 = CurveFp(23, 1, 1)\ng_23 = Point(c_23, 13, 7, 7)\n\n\nHYP_SLOW_SETTINGS = dict(HYP_SETTINGS)\nHYP_SLOW_SETTINGS[\"max_examples\"] = 10\n\n\n@settings(**HYP_SLOW_SETTINGS)\n@given(st.integers(min_value=1, max_value=r + 1))\ndef test_p192_mult_tests(multiple):\n    inv_m = inverse_mod(multiple, r)\n\n    p1 = p192 * multiple\n    assert p1 * inv_m == p192\n\n\ndef add_n_times(point, n):\n    ret = INFINITY\n    i = 0\n    while i <= n:\n        yield ret\n        ret = ret + point\n        i += 1\n\n\n# From X9.62 I.1 (p. 96):\n@pytest.mark.parametrize(\n    \"p, m, check\",\n    [(g_23, n, exp) for n, exp in enumerate(add_n_times(g_23, 8))],\n    ids=[\"g_23 test with mult {0}\".format(i) for i in range(9)],\n)\ndef test_add_and_mult_equivalence(p, m, check):\n    assert p * m == check\n\n\nclass TestCurve(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.c_23 = CurveFp(23, 1, 1)\n\n    def test_equality_curves(self):\n        self.assertEqual(self.c_23, CurveFp(23, 1, 1))\n\n    def test_inequality_curves(self):\n        c192 = CurveFp(p, -3, b)\n        self.assertNotEqual(self.c_23, c192)\n\n    def test_usability_in_a_hashed_collection_curves(self):\n        {self.c_23: None}\n\n    def test_hashability_curves(self):\n        hash(self.c_23)\n\n    def test_conflation_curves(self):\n        ne1, ne2, ne3 = CurveFp(24, 1, 1), CurveFp(23, 2, 1), CurveFp(23, 1, 2)\n        eq1, eq2, eq3 = CurveFp(23, 1, 1), CurveFp(23, 1, 1), self.c_23\n        self.assertEqual(len(set((c_23, eq1, eq2, eq3))), 1)\n        self.assertEqual(len(set((c_23, ne1, ne2, ne3))), 4)\n        self.assertDictEqual({c_23: None}, {eq1: None})\n        self.assertIn(eq2, {eq3: None})\n\n\nclass TestPoint(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.c_23 = CurveFp(23, 1, 1)\n        cls.g_23 = Point(cls.c_23, 13, 7, 7)\n\n        p = 6277101735386680763835789423207666416083908700390324961279\n        r = 6277101735386680763835789423176059013767194773182842284081\n        # s = 0x3045ae6fc8422f64ed579528d38120eae12196d5\n        # c = 0x3099d2bbbfcb2538542dcd5fb078b6ef5f3d6fe2c745de65\n        b = 0x64210519E59C80E70FA7E9AB72243049FEB8DEECC146B9B1\n        Gx = 0x188DA80EB03090F67CBF20EB43A18800F4FF0AFD82FF1012\n        Gy = 0x07192B95FFC8DA78631011ED6B24CDD573F977A11E794811\n\n        cls.c192 = CurveFp(p, -3, b)\n        cls.p192 = Point(cls.c192, Gx, Gy, r)\n\n    def test_p192(self):\n        # Checking against some sample computations presented\n        # in X9.62:\n        d = 651056770906015076056810763456358567190100156695615665659\n        Q = d * self.p192\n        self.assertEqual(\n            Q.x(), 0x62B12D60690CDCF330BABAB6E69763B471F994DD702D16A5\n        )\n\n        k = 6140507067065001063065065565667405560006161556565665656654\n        R = k * self.p192\n        self.assertEqual(\n            R.x(), 0x885052380FF147B734C330C43D39B2C4A89F29B0F749FEAD\n        )\n        self.assertEqual(\n            R.y(), 0x9CF9FA1CBEFEFB917747A3BB29C072B9289C2547884FD835\n        )\n\n        u1 = 2563697409189434185194736134579731015366492496392189760599\n        u2 = 6266643813348617967186477710235785849136406323338782220568\n        temp = u1 * self.p192 + u2 * Q\n        self.assertEqual(\n            temp.x(), 0x885052380FF147B734C330C43D39B2C4A89F29B0F749FEAD\n        )\n        self.assertEqual(\n            temp.y(), 0x9CF9FA1CBEFEFB917747A3BB29C072B9289C2547884FD835\n        )\n\n    def test_double_infinity(self):\n        p1 = INFINITY\n        p3 = p1.double()\n        self.assertEqual(p1, p3)\n        self.assertEqual(p3.x(), p1.x())\n        self.assertEqual(p3.y(), p3.y())\n\n    def test_double(self):\n        x1, y1, x3, y3 = (3, 10, 7, 12)\n\n        p1 = Point(self.c_23, x1, y1)\n        p3 = p1.double()\n        self.assertEqual(p3.x(), x3)\n        self.assertEqual(p3.y(), y3)\n\n    def test_multiply(self):\n        x1, y1, m, x3, y3 = (3, 10, 2, 7, 12)\n        p1 = Point(self.c_23, x1, y1)\n        p3 = p1 * m\n        self.assertEqual(p3.x(), x3)\n        self.assertEqual(p3.y(), y3)\n\n    # Trivial tests from X9.62 B.3:\n    def test_add(self):\n        \"\"\"We expect that on curve c, (x1,y1) + (x2, y2 ) = (x3, y3).\"\"\"\n\n        x1, y1, x2, y2, x3, y3 = (3, 10, 9, 7, 17, 20)\n        p1 = Point(self.c_23, x1, y1)\n        p2 = Point(self.c_23, x2, y2)\n        p3 = p1 + p2\n        self.assertEqual(p3.x(), x3)\n        self.assertEqual(p3.y(), y3)\n\n    def test_add_as_double(self):\n        \"\"\"We expect that on curve c, (x1,y1) + (x2, y2 ) = (x3, y3).\"\"\"\n\n        x1, y1, x2, y2, x3, y3 = (3, 10, 3, 10, 7, 12)\n        p1 = Point(self.c_23, x1, y1)\n        p2 = Point(self.c_23, x2, y2)\n        p3 = p1 + p2\n        self.assertEqual(p3.x(), x3)\n        self.assertEqual(p3.y(), y3)\n\n    def test_equality_points(self):\n        self.assertEqual(self.g_23, Point(self.c_23, 13, 7, 7))\n\n    def test_inequality_points(self):\n        c = CurveFp(100, -3, 100)\n        p = Point(c, 100, 100, 100)\n        self.assertNotEqual(self.g_23, p)\n\n    def test_inequality_points_diff_types(self):\n        c = CurveFp(100, -3, 100)\n        self.assertNotEqual(self.g_23, c)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/ecdsa/test_rw_lock.py", "content": "# Copyright Mateusz Kobos, (c) 2011\n# https://code.activestate.com/recipes/577803-reader-writer-lock-with-priority-for-writers/\n# released under the MIT licence\n\ntry:\n    import unittest2 as unittest\nexcept ImportError:\n    import unittest\nimport threading\nimport time\nimport copy\nfrom ._rwlock import RWLock\n\n\nclass Writer(threading.Thread):\n    def __init__(\n        self, buffer_, rw_lock, init_sleep_time, sleep_time, to_write\n    ):\n        \"\"\"\n        @param buffer_: common buffer_ shared by the readers and writers\n        @type buffer_: list\n        @type rw_lock: L{RWLock}\n        @param init_sleep_time: sleep time before doing any action\n        @type init_sleep_time: C{float}\n        @param sleep_time: sleep time while in critical section\n        @type sleep_time: C{float}\n        @param to_write: data that will be appended to the buffer\n        \"\"\"\n        threading.Thread.__init__(self)\n        self.__buffer = buffer_\n        self.__rw_lock = rw_lock\n        self.__init_sleep_time = init_sleep_time\n        self.__sleep_time = sleep_time\n        self.__to_write = to_write\n        self.entry_time = None\n        \"\"\"Time of entry to the critical section\"\"\"\n        self.exit_time = None\n        \"\"\"Time of exit from the critical section\"\"\"\n\n    def run(self):\n        time.sleep(self.__init_sleep_time)\n        self.__rw_lock.writer_acquire()\n        self.entry_time = time.time()\n        time.sleep(self.__sleep_time)\n        self.__buffer.append(self.__to_write)\n        self.exit_time = time.time()\n        self.__rw_lock.writer_release()\n\n\nclass Reader(threading.Thread):\n    def __init__(self, buffer_, rw_lock, init_sleep_time, sleep_time):\n        \"\"\"\n        @param buffer_: common buffer shared by the readers and writers\n        @type buffer_: list\n        @type rw_lock: L{RWLock}\n        @param init_sleep_time: sleep time before doing any action\n        @type init_sleep_time: C{float}\n        @param sleep_time: sleep time while in critical section\n        @type sleep_time: C{float}\n        \"\"\"\n        threading.Thread.__init__(self)\n        self.__buffer = buffer_\n        self.__rw_lock = rw_lock\n        self.__init_sleep_time = init_sleep_time\n        self.__sleep_time = sleep_time\n        self.buffer_read = None\n        \"\"\"a copy of a the buffer read while in critical section\"\"\"\n        self.entry_time = None\n        \"\"\"Time of entry to the critical section\"\"\"\n        self.exit_time = None\n        \"\"\"Time of exit from the critical section\"\"\"\n\n    def run(self):\n        time.sleep(self.__init_sleep_time)\n        self.__rw_lock.reader_acquire()\n        self.entry_time = time.time()\n        time.sleep(self.__sleep_time)\n        self.buffer_read = copy.deepcopy(self.__buffer)\n        self.exit_time = time.time()\n        self.__rw_lock.reader_release()\n\n\nclass RWLockTestCase(unittest.TestCase):\n    def test_readers_nonexclusive_access(self):\n        (buffer_, rw_lock, threads) = self.__init_variables()\n\n        threads.append(Reader(buffer_, rw_lock, 0, 0))\n        threads.append(Writer(buffer_, rw_lock, 0.2, 0.4, 1))\n        threads.append(Reader(buffer_, rw_lock, 0.3, 0.3))\n        threads.append(Reader(buffer_, rw_lock, 0.5, 0))\n\n        self.__start_and_join_threads(threads)\n\n        ## The third reader should enter after the second one but it should\n        ## exit before the second one exits\n        ## (i.e. the readers should be in the critical section\n        ## at the same time)\n\n        self.assertEqual([], threads[0].buffer_read)\n        self.assertEqual([1], threads[2].buffer_read)\n        self.assertEqual([1], threads[3].buffer_read)\n        self.assertTrue(threads[1].exit_time <= threads[2].entry_time)\n        self.assertTrue(threads[2].entry_time <= threads[3].entry_time)\n        self.assertTrue(threads[3].exit_time < threads[2].exit_time)\n\n    def test_writers_exclusive_access(self):\n        (buffer_, rw_lock, threads) = self.__init_variables()\n\n        threads.append(Writer(buffer_, rw_lock, 0, 0.4, 1))\n        threads.append(Writer(buffer_, rw_lock, 0.1, 0, 2))\n        threads.append(Reader(buffer_, rw_lock, 0.2, 0))\n\n        self.__start_and_join_threads(threads)\n\n        ## The second writer should wait for the first one to exit\n\n        self.assertEqual([1, 2], threads[2].buffer_read)\n        self.assertTrue(threads[0].exit_time <= threads[1].entry_time)\n        self.assertTrue(threads[1].exit_time <= threads[2].exit_time)\n\n    def test_writer_priority(self):\n        (buffer_, rw_lock, threads) = self.__init_variables()\n\n        threads.append(Writer(buffer_, rw_lock, 0, 0, 1))\n        threads.append(Reader(buffer_, rw_lock, 0.1, 0.4))\n        threads.append(Writer(buffer_, rw_lock, 0.2, 0, 2))\n        threads.append(Reader(buffer_, rw_lock, 0.3, 0))\n        threads.append(Reader(buffer_, rw_lock, 0.3, 0))\n\n        self.__start_and_join_threads(threads)\n\n        ## The second writer should go before the second and the third reader\n\n        self.assertEqual([1], threads[1].buffer_read)\n        self.assertEqual([1, 2], threads[3].buffer_read)\n        self.assertEqual([1, 2], threads[4].buffer_read)\n        self.assertTrue(threads[0].exit_time < threads[1].entry_time)\n        self.assertTrue(threads[1].exit_time <= threads[2].entry_time)\n        self.assertTrue(threads[2].exit_time <= threads[3].entry_time)\n        self.assertTrue(threads[2].exit_time <= threads[4].entry_time)\n\n    def test_many_writers_priority(self):\n        (buffer_, rw_lock, threads) = self.__init_variables()\n\n        threads.append(Writer(buffer_, rw_lock, 0, 0, 1))\n        threads.append(Reader(buffer_, rw_lock, 0.1, 0.6))\n        threads.append(Writer(buffer_, rw_lock, 0.2, 0.1, 2))\n        threads.append(Reader(buffer_, rw_lock, 0.3, 0))\n        threads.append(Reader(buffer_, rw_lock, 0.4, 0))\n        threads.append(Writer(buffer_, rw_lock, 0.5, 0.1, 3))\n\n        self.__start_and_join_threads(threads)\n\n        ## The two last writers should go first -- after the first reader and\n        ## before the second and the third reader\n\n        self.assertEqual([1], threads[1].buffer_read)\n        self.assertEqual([1, 2, 3], threads[3].buffer_read)\n        self.assertEqual([1, 2, 3], threads[4].buffer_read)\n        self.assertTrue(threads[0].exit_time < threads[1].entry_time)\n        self.assertTrue(threads[1].exit_time <= threads[2].entry_time)\n        self.assertTrue(threads[1].exit_time <= threads[5].entry_time)\n        self.assertTrue(threads[2].exit_time <= threads[3].entry_time)\n        self.assertTrue(threads[2].exit_time <= threads[4].entry_time)\n        self.assertTrue(threads[5].exit_time <= threads[3].entry_time)\n        self.assertTrue(threads[5].exit_time <= threads[4].entry_time)\n\n    @staticmethod\n    def __init_variables():\n        buffer_ = []\n        rw_lock = RWLock()\n        threads = []\n        return (buffer_, rw_lock, threads)\n\n    @staticmethod\n    def __start_and_join_threads(threads):\n        for t in threads:\n            t.start()\n        for t in threads:\n            t.join()\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/test_greenlet.py", "content": "import gc\nimport sys\nimport time\nimport threading\nimport unittest\nfrom abc import ABCMeta, abstractmethod\n\nfrom greenlet import greenlet\n\n# We manually manage locks in many tests\n# pylint:disable=consider-using-with\n\nclass SomeError(Exception):\n    pass\n\n\ndef fmain(seen):\n    try:\n        greenlet.getcurrent().parent.switch()\n    except:\n        seen.append(sys.exc_info()[0])\n        raise\n    raise SomeError\n\n\ndef send_exception(g, exc):\n    # note: send_exception(g, exc)  can be now done with  g.throw(exc).\n    # the purpose of this test is to explicitely check the propagation rules.\n    def crasher(exc):\n        raise exc\n    g1 = greenlet(crasher, parent=g)\n    g1.switch(exc)\n\n\nclass TestGreenlet(unittest.TestCase):\n    def test_simple(self):\n        lst = []\n\n        def f():\n            lst.append(1)\n            greenlet.getcurrent().parent.switch()\n            lst.append(3)\n        g = greenlet(f)\n        lst.append(0)\n        g.switch()\n        lst.append(2)\n        g.switch()\n        lst.append(4)\n        self.assertEqual(lst, list(range(5)))\n\n    def test_parent_equals_None(self):\n        g = greenlet(parent=None)\n        self.assertIsNotNone(g)\n        self.assertIs(g.parent, greenlet.getcurrent())\n\n    def test_run_equals_None(self):\n        g = greenlet(run=None)\n        self.assertIsNotNone(g)\n        self.assertIsNone(g.run)\n\n    def test_two_children(self):\n        lst = []\n\n        def f():\n            lst.append(1)\n            greenlet.getcurrent().parent.switch()\n            lst.extend([1, 1])\n        g = greenlet(f)\n        h = greenlet(f)\n        g.switch()\n        self.assertEqual(len(lst), 1)\n        h.switch()\n        self.assertEqual(len(lst), 2)\n        h.switch()\n        self.assertEqual(len(lst), 4)\n        self.assertEqual(h.dead, True)\n        g.switch()\n        self.assertEqual(len(lst), 6)\n        self.assertEqual(g.dead, True)\n\n    def test_two_recursive_children(self):\n        lst = []\n\n        def f():\n            lst.append(1)\n            greenlet.getcurrent().parent.switch()\n\n        def g():\n            lst.append(1)\n            g = greenlet(f)\n            g.switch()\n            lst.append(1)\n        g = greenlet(g)\n        g.switch()\n        self.assertEqual(len(lst), 3)\n        self.assertEqual(sys.getrefcount(g), 2)\n\n    def test_threads(self):\n        success = []\n\n        def f():\n            self.test_simple()\n            success.append(True)\n        ths = [threading.Thread(target=f) for i in range(10)]\n        for th in ths:\n            th.start()\n        for th in ths:\n            th.join()\n        self.assertEqual(len(success), len(ths))\n\n    def test_exception(self):\n        seen = []\n        g1 = greenlet(fmain)\n        g2 = greenlet(fmain)\n        g1.switch(seen)\n        g2.switch(seen)\n        g2.parent = g1\n        self.assertEqual(seen, [])\n        self.assertRaises(SomeError, g2.switch)\n        self.assertEqual(seen, [SomeError])\n        g2.switch()\n        self.assertEqual(seen, [SomeError])\n\n    def test_send_exception(self):\n        seen = []\n        g1 = greenlet(fmain)\n        g1.switch(seen)\n        self.assertRaises(KeyError, send_exception, g1, KeyError)\n        self.assertEqual(seen, [KeyError])\n\n    def test_dealloc(self):\n        seen = []\n        g1 = greenlet(fmain)\n        g2 = greenlet(fmain)\n        g1.switch(seen)\n        g2.switch(seen)\n        self.assertEqual(seen, [])\n        del g1\n        gc.collect()\n        self.assertEqual(seen, [greenlet.GreenletExit])\n        del g2\n        gc.collect()\n        self.assertEqual(seen, [greenlet.GreenletExit, greenlet.GreenletExit])\n\n    def test_dealloc_other_thread(self):\n        seen = []\n        someref = []\n        lock = threading.Lock()\n        lock.acquire()\n        lock2 = threading.Lock()\n        lock2.acquire()\n\n        def f():\n            g1 = greenlet(fmain)\n            g1.switch(seen)\n            someref.append(g1)\n            del g1\n            gc.collect()\n            lock.release()\n            lock2.acquire()\n            greenlet()   # trigger release\n            lock.release()\n            lock2.acquire()\n        t = threading.Thread(target=f)\n        t.start()\n        lock.acquire()\n        self.assertEqual(seen, [])\n        self.assertEqual(len(someref), 1)\n        del someref[:]\n        gc.collect()\n        # g1 is not released immediately because it's from another thread\n        self.assertEqual(seen, [])\n        lock2.release()\n        lock.acquire()\n        self.assertEqual(seen, [greenlet.GreenletExit])\n        lock2.release()\n        t.join()\n\n    def test_frame(self):\n        def f1():\n            f = sys._getframe(0) # pylint:disable=protected-access\n            self.assertEqual(f.f_back, None)\n            greenlet.getcurrent().parent.switch(f)\n            return \"meaning of life\"\n        g = greenlet(f1)\n        frame = g.switch()\n        self.assertTrue(frame is g.gr_frame)\n        self.assertTrue(g)\n\n        from_g = g.switch()\n        self.assertFalse(g)\n        self.assertEqual(from_g, 'meaning of life')\n        self.assertEqual(g.gr_frame, None)\n\n    def test_thread_bug(self):\n        def runner(x):\n            g = greenlet(lambda: time.sleep(x))\n            g.switch()\n        t1 = threading.Thread(target=runner, args=(0.2,))\n        t2 = threading.Thread(target=runner, args=(0.3,))\n        t1.start()\n        t2.start()\n        t1.join()\n        t2.join()\n\n    def test_switch_kwargs(self):\n        def run(a, b):\n            self.assertEqual(a, 4)\n            self.assertEqual(b, 2)\n            return 42\n        x = greenlet(run).switch(a=4, b=2)\n        self.assertEqual(x, 42)\n\n    def test_switch_kwargs_to_parent(self):\n        def run(x):\n            greenlet.getcurrent().parent.switch(x=x)\n            greenlet.getcurrent().parent.switch(2, x=3)\n            return x, x ** 2\n        g = greenlet(run)\n        self.assertEqual({'x': 3}, g.switch(3))\n        self.assertEqual(((2,), {'x': 3}), g.switch())\n        self.assertEqual((3, 9), g.switch())\n\n    def test_switch_to_another_thread(self):\n        data = {}\n        error = None\n        created_event = threading.Event()\n        done_event = threading.Event()\n\n        def run():\n            data['g'] = greenlet(lambda: None)\n            created_event.set()\n            done_event.wait()\n        thread = threading.Thread(target=run)\n        thread.start()\n        created_event.wait()\n        try:\n            data['g'].switch()\n        except greenlet.error:\n            error = sys.exc_info()[1]\n        self.assertIsNotNone(error, \"greenlet.error was not raised!\")\n        done_event.set()\n        thread.join()\n\n    def test_exc_state(self):\n        def f():\n            try:\n                raise ValueError('fun')\n            except: # pylint:disable=bare-except\n                exc_info = sys.exc_info()\n                greenlet(h).switch()\n                self.assertEqual(exc_info, sys.exc_info())\n\n        def h():\n            self.assertEqual(sys.exc_info(), (None, None, None))\n\n        greenlet(f).switch()\n\n    def test_instance_dict(self):\n        def f():\n            greenlet.getcurrent().test = 42\n        def deldict(g):\n            del g.__dict__\n        def setdict(g, value):\n            g.__dict__ = value\n        g = greenlet(f)\n        self.assertEqual(g.__dict__, {})\n        g.switch()\n        self.assertEqual(g.test, 42)\n        self.assertEqual(g.__dict__, {'test': 42})\n        g.__dict__ = g.__dict__\n        self.assertEqual(g.__dict__, {'test': 42})\n        self.assertRaises(TypeError, deldict, g)\n        self.assertRaises(TypeError, setdict, g, 42)\n\n    def test_threaded_reparent(self):\n        data = {}\n        created_event = threading.Event()\n        done_event = threading.Event()\n\n        def run():\n            data['g'] = greenlet(lambda: None)\n            created_event.set()\n            done_event.wait()\n\n        def blank():\n            greenlet.getcurrent().parent.switch()\n\n        def setparent(g, value):\n            g.parent = value\n\n        thread = threading.Thread(target=run)\n        thread.start()\n        created_event.wait()\n        g = greenlet(blank)\n        g.switch()\n        self.assertRaises(ValueError, setparent, g, data['g'])\n        done_event.set()\n        thread.join()\n\n    def test_deepcopy(self):\n        import copy\n        self.assertRaises(TypeError, copy.copy, greenlet())\n        self.assertRaises(TypeError, copy.deepcopy, greenlet())\n\n    def test_parent_restored_on_kill(self):\n        hub = greenlet(lambda: None)\n        main = greenlet.getcurrent()\n        result = []\n        def worker():\n            try:\n                # Wait to be killed\n                main.switch()\n            except greenlet.GreenletExit:\n                # Resurrect and switch to parent\n                result.append(greenlet.getcurrent().parent)\n                result.append(greenlet.getcurrent())\n                hub.switch()\n        g = greenlet(worker, parent=hub)\n        g.switch()\n        del g\n        self.assertTrue(result)\n        self.assertEqual(result[0], main)\n        self.assertEqual(result[1].parent, hub)\n\n    def test_parent_return_failure(self):\n        # No run causes AttributeError on switch\n        g1 = greenlet()\n        # Greenlet that implicitly switches to parent\n        g2 = greenlet(lambda: None, parent=g1)\n        # AttributeError should propagate to us, no fatal errors\n        self.assertRaises(AttributeError, g2.switch)\n\n    def test_throw_exception_not_lost(self):\n        class mygreenlet(greenlet):\n            def __getattribute__(self, name):\n                try:\n                    raise Exception()\n                except: # pylint:disable=bare-except\n                    pass\n                return greenlet.__getattribute__(self, name)\n        g = mygreenlet(lambda: None)\n        self.assertRaises(SomeError, g.throw, SomeError())\n\n    def test_throw_doesnt_crash(self):\n        result = []\n        def worker():\n            greenlet.getcurrent().parent.switch()\n        def creator():\n            g = greenlet(worker)\n            g.switch()\n            result.append(g)\n        t = threading.Thread(target=creator)\n        t.start()\n        t.join()\n        self.assertRaises(greenlet.error, result[0].throw, SomeError())\n\n    def test_recursive_startup(self):\n        class convoluted(greenlet):\n            def __init__(self):\n                greenlet.__init__(self)\n                self.count = 0\n            def __getattribute__(self, name):\n                if name == 'run' and self.count == 0:\n                    self.count = 1\n                    self.switch(43)\n                return greenlet.__getattribute__(self, name)\n            def run(self, value):\n                while True:\n                    self.parent.switch(value)\n        g = convoluted()\n        self.assertEqual(g.switch(42), 43)\n\n    def test_unexpected_reparenting(self):\n        another = []\n        def worker():\n            g = greenlet(lambda: None)\n            another.append(g)\n            g.switch()\n        t = threading.Thread(target=worker)\n        t.start()\n        t.join()\n        class convoluted(greenlet):\n            def __getattribute__(self, name):\n                if name == 'run':\n                    self.parent = another[0] # pylint:disable=attribute-defined-outside-init\n                return greenlet.__getattribute__(self, name)\n        g = convoluted(lambda: None)\n        self.assertRaises(greenlet.error, g.switch)\n\n    def test_threaded_updatecurrent(self):\n        # released when main thread should execute\n        lock1 = threading.Lock()\n        lock1.acquire()\n        # released when another thread should execute\n        lock2 = threading.Lock()\n        lock2.acquire()\n        class finalized(object):\n            def __del__(self):\n                # happens while in green_updatecurrent() in main greenlet\n                # should be very careful not to accidentally call it again\n                # at the same time we must make sure another thread executes\n                lock2.release()\n                lock1.acquire()\n                # now ts_current belongs to another thread\n        def deallocator():\n            greenlet.getcurrent().parent.switch()\n        def fthread():\n            lock2.acquire()\n            greenlet.getcurrent()\n            del g[0]\n            lock1.release()\n            lock2.acquire()\n            greenlet.getcurrent()\n            lock1.release()\n        main = greenlet.getcurrent()\n        g = [greenlet(deallocator)]\n        g[0].bomb = finalized()\n        g[0].switch()\n        t = threading.Thread(target=fthread)\n        t.start()\n        # let another thread grab ts_current and deallocate g[0]\n        lock2.release()\n        lock1.acquire()\n        # this is the corner stone\n        # getcurrent() will notice that ts_current belongs to another thread\n        # and start the update process, which would notice that g[0] should\n        # be deallocated, and that will execute an object's finalizer. Now,\n        # that object will let another thread run so it can grab ts_current\n        # again, which would likely crash the interpreter if there's no\n        # check for this case at the end of green_updatecurrent(). This test\n        # passes if getcurrent() returns correct result, but it's likely\n        # to randomly crash if it's not anyway.\n        self.assertEqual(greenlet.getcurrent(), main)\n        # wait for another thread to complete, just in case\n        t.join()\n\n    def test_dealloc_switch_args_not_lost(self):\n        seen = []\n        def worker():\n            # wait for the value\n            value = greenlet.getcurrent().parent.switch()\n            # delete all references to ourself\n            del worker[0]\n            initiator.parent = greenlet.getcurrent().parent\n            # switch to main with the value, but because\n            # ts_current is the last reference to us we\n            # return immediately\n            try:\n                greenlet.getcurrent().parent.switch(value)\n            finally:\n                seen.append(greenlet.getcurrent())\n        def initiator():\n            return 42 # implicitly falls thru to parent\n        worker = [greenlet(worker)]\n        worker[0].switch() # prime worker\n        initiator = greenlet(initiator, worker[0])\n        value = initiator.switch()\n        self.assertTrue(seen)\n        self.assertEqual(value, 42)\n\n\n\n    def test_tuple_subclass(self):\n        if sys.version_info[0] > 2:\n            # There's no apply in Python 3.x\n            def _apply(func, a, k):\n                func(*a, **k)\n        else:\n            _apply = apply # pylint:disable=undefined-variable\n\n        class mytuple(tuple):\n            def __len__(self):\n                greenlet.getcurrent().switch()\n                return tuple.__len__(self)\n        args = mytuple()\n        kwargs = dict(a=42)\n        def switchapply():\n            _apply(greenlet.getcurrent().parent.switch, args, kwargs)\n        g = greenlet(switchapply)\n        self.assertEqual(g.switch(), kwargs)\n\n    def test_abstract_subclasses(self):\n        AbstractSubclass = ABCMeta(\n            'AbstractSubclass',\n            (greenlet,),\n            {'run': abstractmethod(lambda self: None)})\n\n        class BadSubclass(AbstractSubclass):\n            pass\n\n        class GoodSubclass(AbstractSubclass):\n            def run(self):\n                pass\n\n        GoodSubclass() # should not raise\n        self.assertRaises(TypeError, BadSubclass)\n\n    def test_implicit_parent_with_threads(self):\n        if not gc.isenabled():\n            return # cannot test with disabled gc\n        N = gc.get_threshold()[0]\n        if N < 50:\n            return # cannot test with such a small N\n        def attempt():\n            lock1 = threading.Lock()\n            lock1.acquire()\n            lock2 = threading.Lock()\n            lock2.acquire()\n            recycled = [False]\n            def another_thread():\n                lock1.acquire() # wait for gc\n                greenlet.getcurrent() # update ts_current\n                lock2.release() # release gc\n            t = threading.Thread(target=another_thread)\n            t.start()\n            class gc_callback(object):\n                def __del__(self):\n                    lock1.release()\n                    lock2.acquire()\n                    recycled[0] = True\n            class garbage(object):\n                def __init__(self):\n                    self.cycle = self\n                    self.callback = gc_callback()\n            l = []\n            x = range(N*2)\n            current = greenlet.getcurrent()\n            g = garbage()\n            for _ in x:\n                g = None # lose reference to garbage\n                if recycled[0]:\n                    # gc callback called prematurely\n                    t.join()\n                    return False\n                last = greenlet()\n                if recycled[0]:\n                    break # yes! gc called in green_new\n                l.append(last) # increase allocation counter\n            else:\n                # gc callback not called when expected\n                gc.collect()\n                if recycled[0]:\n                    t.join()\n                return False\n            self.assertEqual(last.parent, current)\n            for g in l:\n                self.assertEqual(g.parent, current)\n            return True\n        for _ in range(5):\n            if attempt():\n                break\n\n    def test_issue_245_reference_counting_subclass_no_threads(self):\n        # https://github.com/python-greenlet/greenlet/issues/245\n        # Before the fix, this crashed pretty reliably on\n        # Python 3.10, at least on macOS; but much less reliably on other\n        # interpreters (memory layout must have changed).\n        # The threaded test crashed more reliably on more interpreters.\n        from greenlet import getcurrent\n        from greenlet import GreenletExit\n\n        class Greenlet(greenlet):\n            pass\n\n        initial_refs = sys.getrefcount(Greenlet)\n        # This has to be an instance variable because\n        # Python 2 raises a SyntaxError if we delete a local\n        # variable referenced in an inner scope.\n        self.glets = [] # pylint:disable=attribute-defined-outside-init\n\n        def greenlet_main():\n            try:\n                getcurrent().parent.switch()\n            except GreenletExit:\n                self.glets.append(getcurrent())\n\n        # Before the\n        for _ in range(10):\n            Greenlet(greenlet_main).switch()\n\n        del self.glets\n        self.assertEqual(sys.getrefcount(Greenlet), initial_refs)\n\n    def test_issue_245_reference_counting_subclass_threads(self):\n        # https://github.com/python-greenlet/greenlet/issues/245\n        from threading import Thread\n        from threading import Event\n\n        from greenlet import getcurrent\n\n        class MyGreenlet(greenlet):\n            pass\n\n        glets = []\n        ref_cleared = Event()\n\n        def greenlet_main():\n            getcurrent().parent.switch()\n\n        def thread_main(greenlet_running_event):\n            mine = MyGreenlet(greenlet_main)\n            glets.append(mine)\n            # The greenlets being deleted must be active\n            mine.switch()\n            # Don't keep any reference to it in this thread\n            del mine\n            # Let main know we published our greenlet.\n            greenlet_running_event.set()\n            # Wait for main to let us know the references are\n            # gone and the greenlet objects no longer reachable\n            ref_cleared.wait()\n            # The creating thread must call getcurrent() (or a few other\n            # greenlet APIs) because that's when the thread-local list of dead\n            # greenlets gets cleared.\n            getcurrent()\n\n        # We start with 3 references to the subclass:\n        # - This module\n        # - Its __mro__\n        # - The __subclassess__ attribute of greenlet\n        # - (If we call gc.get_referents(), we find four entries, including\n        #   some other tuple ``(greenlet)`` that I'm not sure about but must be part\n        #   of the machinery.)\n        #\n        # On Python 3.10 it's often enough to just run 3 threads; on Python 2.7,\n        # more threads are needed, and the results are still\n        # non-deterministic. Presumably the memory layouts are different\n        initial_refs = sys.getrefcount(MyGreenlet)\n        thread_ready_events = []\n        for _ in range(\n                initial_refs + 45\n        ):\n            event = Event()\n            thread = Thread(target=thread_main, args=(event,))\n            thread_ready_events.append(event)\n            thread.start()\n\n\n        for done_event in thread_ready_events:\n            done_event.wait()\n\n\n        del glets[:]\n        ref_cleared.set()\n        # Let any other thread run; it will crash the interpreter\n        # if not fixed (or silently corrupt memory and we possibly crash\n        # later).\n        time.sleep(1)\n        self.assertEqual(sys.getrefcount(MyGreenlet), initial_refs)\n\n\nclass TestRepr(unittest.TestCase):\n\n    def assertEndsWith(self, got, suffix):\n        self.assertTrue(got.endswith(suffix), (got, suffix))\n\n    def test_main_while_running(self):\n        r = repr(greenlet.getcurrent())\n        self.assertEndsWith(r, \" current active started main>\")\n\n    def test_main_in_background(self):\n        main = greenlet.getcurrent()\n        def run():\n            return repr(main)\n\n        g = greenlet(run)\n        r = g.switch()\n        self.assertEndsWith(r, ' suspended active started main>')\n\n    def test_initial(self):\n        r = repr(greenlet())\n        self.assertEndsWith(r, ' pending>')\n\n    def test_main_from_other_thread(self):\n        main = greenlet.getcurrent()\n\n        class T(threading.Thread):\n            original_main = thread_main = None\n            main_glet = None\n            def run(self):\n                self.original_main = repr(main)\n                self.main_glet = greenlet.getcurrent()\n                self.thread_main = repr(self.main_glet)\n\n        t = T()\n        t.start()\n        t.join(10)\n\n        self.assertEndsWith(t.original_main, ' suspended active started main>')\n        self.assertEndsWith(t.thread_main, ' current active started main>')\n\n        r = repr(t.main_glet)\n        # main greenlets, even from dead threads, never really appear dead\n        # TODO: Can we find a better way to differentiate that?\n        assert not t.main_glet.dead\n        self.assertEndsWith(r, ' suspended active started main>')\n\n    def test_dead(self):\n        g = greenlet(lambda: None)\n        g.switch()\n        self.assertEndsWith(repr(g), ' dead>')\n        self.assertNotIn('suspended', repr(g))\n        self.assertNotIn('started', repr(g))\n        self.assertNotIn('active', repr(g))\n\n    def test_formatting_produces_native_str(self):\n        # https://github.com/python-greenlet/greenlet/issues/218\n        # %s formatting on Python 2 was producing unicode, not str.\n\n        g_dead = greenlet(lambda: None)\n        g_not_started = greenlet(lambda: None)\n        g_cur = greenlet.getcurrent()\n\n        for g in g_dead, g_not_started, g_cur:\n\n            self.assertIsInstance(\n                '%s' % (g,),\n                str\n            )\n            self.assertIsInstance(\n                '%r' % (g,),\n                str,\n            )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/passlib/tests/test_apps.py", "content": "\"\"\"test passlib.apps\"\"\"\n#=============================================================================\n# imports\n#=============================================================================\nfrom __future__ import with_statement\n# core\nimport logging; log = logging.getLogger(__name__)\n# site\n# pkg\nfrom passlib import apps, hash as hashmod\nfrom passlib.tests.utils import TestCase\n# module\n\n#=============================================================================\n# test predefined app contexts\n#=============================================================================\nclass AppsTest(TestCase):\n    \"\"\"perform general tests to make sure contexts work\"\"\"\n    # NOTE: these tests are not really comprehensive,\n    #       since they would do little but duplicate\n    #       the presets in apps.py\n    #\n    #       they mainly try to ensure no typos\n    #       or dynamic behavior foul-ups.\n\n    def test_master_context(self):\n        ctx = apps.master_context\n        self.assertGreater(len(ctx.schemes()), 50)\n\n    def test_custom_app_context(self):\n        ctx = apps.custom_app_context\n        self.assertEqual(ctx.schemes(), (\"sha512_crypt\", \"sha256_crypt\"))\n        for hash in [\n            ('$6$rounds=41128$VoQLvDjkaZ6L6BIE$4pt.1Ll1XdDYduEwEYPCMOBiR6W6'\n                'znsyUEoNlcVXpv2gKKIbQolgmTGe6uEEVJ7azUxuc8Tf7zV9SD2z7Ij751'),\n            ('$5$rounds=31817$iZGmlyBQ99JSB5n6$p4E.pdPBWx19OajgjLRiOW0itGny'\n                 'xDGgMlDcOsfaI17'),\n        ]:\n            self.assertTrue(ctx.verify(\"test\", hash))\n\n    def test_django16_context(self):\n        ctx = apps.django16_context\n        for hash in [\n            'pbkdf2_sha256$29000$ZsgquwnCyBs2$fBxRQpfKd2PIeMxtkKPy0h7SrnrN+EU/cm67aitoZ2s=',\n            'sha1$0d082$cdb462ae8b6be8784ef24b20778c4d0c82d5957f',\n            'md5$b887a$37767f8a745af10612ad44c80ff52e92',\n            'crypt$95a6d$95x74hLDQKXI2',\n            '098f6bcd4621d373cade4e832627b4f6',\n        ]:\n            self.assertTrue(ctx.verify(\"test\", hash))\n\n        self.assertEqual(ctx.identify(\"!\"), \"django_disabled\")\n        self.assertFalse(ctx.verify(\"test\", \"!\"))\n\n    def test_django_context(self):\n        ctx = apps.django_context\n        for hash in [\n            'pbkdf2_sha256$29000$ZsgquwnCyBs2$fBxRQpfKd2PIeMxtkKPy0h7SrnrN+EU/cm67aitoZ2s=',\n        ]:\n            self.assertTrue(ctx.verify(\"test\", hash))\n\n        self.assertEqual(ctx.identify(\"!\"), \"django_disabled\")\n        self.assertFalse(ctx.verify(\"test\", \"!\"))\n\n    def test_ldap_nocrypt_context(self):\n        ctx = apps.ldap_nocrypt_context\n        for hash in [\n            '{SSHA}cPusOzd6d5n3OjSVK3R329ZGCNyFcC7F',\n            'test',\n        ]:\n            self.assertTrue(ctx.verify(\"test\", hash))\n\n        self.assertIs(ctx.identify('{CRYPT}$5$rounds=31817$iZGmlyBQ99JSB5'\n                'n6$p4E.pdPBWx19OajgjLRiOW0itGnyxDGgMlDcOsfaI17'), None)\n\n    def test_ldap_context(self):\n        ctx = apps.ldap_context\n        for hash in [\n            ('{CRYPT}$5$rounds=31817$iZGmlyBQ99JSB5n6$p4E.pdPBWx19OajgjLRiOW0'\n                'itGnyxDGgMlDcOsfaI17'),\n            '{SSHA}cPusOzd6d5n3OjSVK3R329ZGCNyFcC7F',\n            'test',\n        ]:\n            self.assertTrue(ctx.verify(\"test\", hash))\n\n    def test_ldap_mysql_context(self):\n        ctx = apps.mysql_context\n        for hash in [\n            '*94BDCEBE19083CE2A1F959FD02F964C7AF4CFC29',\n            '378b243e220ca493',\n        ]:\n            self.assertTrue(ctx.verify(\"test\", hash))\n\n    def test_postgres_context(self):\n        ctx = apps.postgres_context\n        hash = 'md55d9c68c6c50ed3d02a2fcf54f63993b6'\n        self.assertTrue(ctx.verify(\"test\", hash, user='user'))\n\n    def test_phppass_context(self):\n        ctx = apps.phpass_context\n        for hash in [\n            '$P$8Ja1vJsKa5qyy/b3mCJGXM7GyBnt6..',\n            '$H$8b95CoYQnQ9Y6fSTsACyphNh5yoM02.',\n            '_cD..aBxeRhYFJvtUvsI',\n        ]:\n            self.assertTrue(ctx.verify(\"test\", hash))\n\n        h1 = \"$2a$04$yjDgE74RJkeqC0/1NheSSOrvKeu9IbKDpcQf/Ox3qsrRS/Kw42qIS\"\n        if hashmod.bcrypt.has_backend():\n            self.assertTrue(ctx.verify(\"test\", h1))\n            self.assertEqual(ctx.default_scheme(), \"bcrypt\")\n            self.assertEqual(ctx.handler().name, \"bcrypt\")\n        else:\n            self.assertEqual(ctx.identify(h1), \"bcrypt\")\n            self.assertEqual(ctx.default_scheme(), \"phpass\")\n            self.assertEqual(ctx.handler().name, \"phpass\")\n\n    def test_phpbb3_context(self):\n        ctx = apps.phpbb3_context\n        for hash in [\n            '$P$8Ja1vJsKa5qyy/b3mCJGXM7GyBnt6..',\n            '$H$8b95CoYQnQ9Y6fSTsACyphNh5yoM02.',\n        ]:\n            self.assertTrue(ctx.verify(\"test\", hash))\n        self.assertTrue(ctx.hash(\"test\").startswith(\"$H$\"))\n\n    def test_roundup_context(self):\n        ctx = apps.roundup_context\n        for hash in [\n            '{PBKDF2}9849$JMTYu3eOUSoFYExprVVqbQ$N5.gV.uR1.BTgLSvi0qyPiRlGZ0',\n            '{SHA}a94a8fe5ccb19ba61c4c0873d391e987982fbbd3',\n            '{CRYPT}dptOmKDriOGfU',\n            '{plaintext}test',\n        ]:\n            self.assertTrue(ctx.verify(\"test\", hash))\n\n#=============================================================================\n# eof\n#=============================================================================\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/h11/tests/test_events.py", "content": "from http import HTTPStatus\n\nimport pytest\n\nfrom .. import _events\nfrom .._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    Event,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom .._util import LocalProtocolError\n\n\ndef test_events() -> None:\n    with pytest.raises(LocalProtocolError):\n        # Missing Host:\n        req = Request(\n            method=\"GET\", target=\"/\", headers=[(\"a\", \"b\")], http_version=\"1.1\"\n        )\n    # But this is okay (HTTP/1.0)\n    req = Request(method=\"GET\", target=\"/\", headers=[(\"a\", \"b\")], http_version=\"1.0\")\n    # fields are normalized\n    assert req.method == b\"GET\"\n    assert req.target == b\"/\"\n    assert req.headers == [(b\"a\", b\"b\")]\n    assert req.http_version == b\"1.0\"\n\n    # This is also okay -- has a Host (with weird capitalization, which is ok)\n    req = Request(\n        method=\"GET\",\n        target=\"/\",\n        headers=[(\"a\", \"b\"), (\"hOSt\", \"example.com\")],\n        http_version=\"1.1\",\n    )\n    # we normalize header capitalization\n    assert req.headers == [(b\"a\", b\"b\"), (b\"host\", b\"example.com\")]\n\n    # Multiple host is bad too\n    with pytest.raises(LocalProtocolError):\n        req = Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"a\"), (\"Host\", \"a\")],\n            http_version=\"1.1\",\n        )\n    # Even for HTTP/1.0\n    with pytest.raises(LocalProtocolError):\n        req = Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"a\"), (\"Host\", \"a\")],\n            http_version=\"1.0\",\n        )\n\n    # Header values are validated\n    for bad_char in \"\\x00\\r\\n\\f\\v\":\n        with pytest.raises(LocalProtocolError):\n            req = Request(\n                method=\"GET\",\n                target=\"/\",\n                headers=[(\"Host\", \"a\"), (\"Foo\", \"asd\" + bad_char)],\n                http_version=\"1.0\",\n            )\n\n    # But for compatibility we allow non-whitespace control characters, even\n    # though they're forbidden by the spec.\n    Request(\n        method=\"GET\",\n        target=\"/\",\n        headers=[(\"Host\", \"a\"), (\"Foo\", \"asd\\x01\\x02\\x7f\")],\n        http_version=\"1.0\",\n    )\n\n    # Request target is validated\n    for bad_byte in b\"\\x00\\x20\\x7f\\xee\":\n        target = bytearray(b\"/\")\n        target.append(bad_byte)\n        with pytest.raises(LocalProtocolError):\n            Request(\n                method=\"GET\", target=target, headers=[(\"Host\", \"a\")], http_version=\"1.1\"\n            )\n\n    # Request method is validated\n    with pytest.raises(LocalProtocolError):\n        Request(\n            method=\"GET / HTTP/1.1\",\n            target=target,\n            headers=[(\"Host\", \"a\")],\n            http_version=\"1.1\",\n        )\n\n    ir = InformationalResponse(status_code=100, headers=[(\"Host\", \"a\")])\n    assert ir.status_code == 100\n    assert ir.headers == [(b\"host\", b\"a\")]\n    assert ir.http_version == b\"1.1\"\n\n    with pytest.raises(LocalProtocolError):\n        InformationalResponse(status_code=200, headers=[(\"Host\", \"a\")])\n\n    resp = Response(status_code=204, headers=[], http_version=\"1.0\")  # type: ignore[arg-type]\n    assert resp.status_code == 204\n    assert resp.headers == []\n    assert resp.http_version == b\"1.0\"\n\n    with pytest.raises(LocalProtocolError):\n        resp = Response(status_code=100, headers=[], http_version=\"1.0\")  # type: ignore[arg-type]\n\n    with pytest.raises(LocalProtocolError):\n        Response(status_code=\"100\", headers=[], http_version=\"1.0\")  # type: ignore[arg-type]\n\n    with pytest.raises(LocalProtocolError):\n        InformationalResponse(status_code=b\"100\", headers=[], http_version=\"1.0\")  # type: ignore[arg-type]\n\n    d = Data(data=b\"asdf\")\n    assert d.data == b\"asdf\"\n\n    eom = EndOfMessage()\n    assert eom.headers == []\n\n    cc = ConnectionClosed()\n    assert repr(cc) == \"ConnectionClosed()\"\n\n\ndef test_intenum_status_code() -> None:\n    # https://github.com/python-hyper/h11/issues/72\n\n    r = Response(status_code=HTTPStatus.OK, headers=[], http_version=\"1.0\")  # type: ignore[arg-type]\n    assert r.status_code == HTTPStatus.OK\n    assert type(r.status_code) is not type(HTTPStatus.OK)\n    assert type(r.status_code) is int\n\n\ndef test_header_casing() -> None:\n    r = Request(\n        method=\"GET\",\n        target=\"/\",\n        headers=[(\"Host\", \"example.org\"), (\"Connection\", \"keep-alive\")],\n        http_version=\"1.1\",\n    )\n    assert len(r.headers) == 2\n    assert r.headers[0] == (b\"host\", b\"example.org\")\n    assert r.headers == [(b\"host\", b\"example.org\"), (b\"connection\", b\"keep-alive\")]\n    assert r.headers.raw_items() == [\n        (b\"Host\", b\"example.org\"),\n        (b\"Connection\", b\"keep-alive\"),\n    ]\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/test_leaks.py", "content": "import unittest\nimport sys\nimport gc\n\nimport time\nimport weakref\nimport threading\n\nimport greenlet\n\nclass TestLeaks(unittest.TestCase):\n\n    def test_arg_refs(self):\n        args = ('a', 'b', 'c')\n        refcount_before = sys.getrefcount(args)\n        # pylint:disable=unnecessary-lambda\n        g = greenlet.greenlet(\n            lambda *args: greenlet.getcurrent().parent.switch(*args))\n        for _ in range(100):\n            g.switch(*args)\n        self.assertEqual(sys.getrefcount(args), refcount_before)\n\n    def test_kwarg_refs(self):\n        kwargs = {}\n        # pylint:disable=unnecessary-lambda\n        g = greenlet.greenlet(\n            lambda **kwargs: greenlet.getcurrent().parent.switch(**kwargs))\n        for _ in range(100):\n            g.switch(**kwargs)\n        self.assertEqual(sys.getrefcount(kwargs), 2)\n\n    assert greenlet.GREENLET_USE_GC # Option to disable this was removed in 1.0\n\n    def recycle_threads(self):\n        # By introducing a thread that does sleep we allow other threads,\n        # that have triggered their __block condition, but did not have a\n        # chance to deallocate their thread state yet, to finally do so.\n        # The way it works is by requiring a GIL switch (different thread),\n        # which does a GIL release (sleep), which might do a GIL switch\n        # to finished threads and allow them to clean up.\n        def worker():\n            time.sleep(0.001)\n        t = threading.Thread(target=worker)\n        t.start()\n        time.sleep(0.001)\n        t.join()\n\n    def test_threaded_leak(self):\n        gg = []\n        def worker():\n            # only main greenlet present\n            gg.append(weakref.ref(greenlet.getcurrent()))\n        for _ in range(2):\n            t = threading.Thread(target=worker)\n            t.start()\n            t.join()\n            del t\n        greenlet.getcurrent() # update ts_current\n        self.recycle_threads()\n        greenlet.getcurrent() # update ts_current\n        gc.collect()\n        greenlet.getcurrent() # update ts_current\n        for g in gg:\n            self.assertIsNone(g())\n\n    def test_threaded_adv_leak(self):\n        gg = []\n        def worker():\n            # main and additional *finished* greenlets\n            ll = greenlet.getcurrent().ll = []\n            def additional():\n                ll.append(greenlet.getcurrent())\n            for _ in range(2):\n                greenlet.greenlet(additional).switch()\n            gg.append(weakref.ref(greenlet.getcurrent()))\n        for _ in range(2):\n            t = threading.Thread(target=worker)\n            t.start()\n            t.join()\n            del t\n        greenlet.getcurrent() # update ts_current\n        self.recycle_threads()\n        greenlet.getcurrent() # update ts_current\n        gc.collect()\n        greenlet.getcurrent() # update ts_current\n        for g in gg:\n            self.assertIsNone(g())\n\n    def test_issue251_killing_cross_thread_leaks_list(self, manually_collect_background=True):\n        # See https://github.com/python-greenlet/greenlet/issues/251\n        # Killing a greenlet (probably not the main one)\n        # in one thread from another thread would\n        # result in leaking a list (the ts_delkey list).\n\n        # For the test to be valid, even empty lists have to be tracked by the\n        # GC\n        assert gc.is_tracked([])\n\n        def count_objects(kind=list):\n            # pylint:disable=unidiomatic-typecheck\n            # Collect the garbage.\n            for _ in range(3):\n                gc.collect()\n            gc.collect()\n            return sum(\n                1\n                for x in gc.get_objects()\n                if type(x) is kind\n            )\n\n        # XXX: The main greenlet of a dead thread is only released\n        # when one of the proper greenlet APIs is used from a different\n        # running thread. See #252 (https://github.com/python-greenlet/greenlet/issues/252)\n        greenlet.getcurrent()\n        greenlets_before = count_objects(greenlet.greenlet)\n\n        background_glet_running = threading.Event()\n        background_glet_killed = threading.Event()\n        background_greenlets = []\n        def background_greenlet():\n            # Throw control back to the main greenlet.\n            greenlet.getcurrent().parent.switch()\n\n        def background_thread():\n            glet = greenlet.greenlet(background_greenlet)\n            background_greenlets.append(glet)\n            glet.switch() # Be sure it's active.\n            # Control is ours again.\n            del glet # Delete one reference from the thread it runs in.\n            background_glet_running.set()\n            background_glet_killed.wait()\n            # To trigger the background collection of the dead\n            # greenlet, thus clearing out the contents of the list, we\n            # need to run some APIs. See issue 252.\n            if manually_collect_background:\n                greenlet.getcurrent()\n\n\n        t = threading.Thread(target=background_thread)\n        t.start()\n        background_glet_running.wait()\n\n        lists_before = count_objects()\n\n        assert len(background_greenlets) == 1\n        self.assertFalse(background_greenlets[0].dead)\n        # Delete the last reference to the background greenlet\n        # from a different thread. This puts it in the background thread's\n        # ts_delkey list.\n        del background_greenlets[:]\n        background_glet_killed.set()\n\n        # Now wait for the background thread to die.\n        t.join(10)\n        del t\n\n        # Free the background main greenlet by forcing greenlet to notice a difference.\n        greenlet.getcurrent()\n        greenlets_after = count_objects(greenlet.greenlet)\n\n        lists_after = count_objects()\n        # On 2.7, we observe that lists_after is smaller than\n        # lists_before. No idea what lists got cleaned up. All the\n        # Python 3 versions match exactly.\n        self.assertLessEqual(lists_after, lists_before)\n\n        self.assertEqual(greenlets_before, greenlets_after)\n\n    @unittest.expectedFailure\n    def test_issue251_issue252_need_to_collect_in_background(self):\n        # This still fails because the leak of the list\n        # still exists when we don't call a greenlet API before exiting the\n        # thread. The proximate cause is that neither of the two greenlets\n        # from the background thread are actually being destroyed, even though\n        # the GC is in fact visiting both objects.\n        # It's not clear where that leak is? For some reason the thread-local dict\n        # holding it isn't being cleaned up.\n        self.test_issue251_killing_cross_thread_leaks_list(manually_collect_background=False)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/aiosqlite/tests/__main__.py", "content": "# Copyright 2018 John Reese\n# Licensed under the MIT license\n\nimport unittest\n\nif __name__ == \"__main__\":\n    unittest.main(module=\"aiosqlite.tests\", verbosity=2)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/h11/tests/test_headers.py", "content": "import pytest\n\nfrom .._events import Request\nfrom .._headers import (\n    get_comma_header,\n    has_expect_100_continue,\n    Headers,\n    normalize_and_validate,\n    set_comma_header,\n)\nfrom .._util import LocalProtocolError\n\n\ndef test_normalize_and_validate() -> None:\n    assert normalize_and_validate([(\"foo\", \"bar\")]) == [(b\"foo\", b\"bar\")]\n    assert normalize_and_validate([(b\"foo\", b\"bar\")]) == [(b\"foo\", b\"bar\")]\n\n    # no leading/trailing whitespace in names\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(b\"foo \", \"bar\")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(b\" foo\", \"bar\")])\n\n    # no weird characters in names\n    with pytest.raises(LocalProtocolError) as excinfo:\n        normalize_and_validate([(b\"foo bar\", b\"baz\")])\n    assert \"foo bar\" in str(excinfo.value)\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(b\"foo\\x00bar\", b\"baz\")])\n    # Not even 8-bit characters:\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(b\"foo\\xffbar\", b\"baz\")])\n    # And not even the control characters we allow in values:\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(b\"foo\\x01bar\", b\"baz\")])\n\n    # no return or NUL characters in values\n    with pytest.raises(LocalProtocolError) as excinfo:\n        normalize_and_validate([(\"foo\", \"bar\\rbaz\")])\n    assert \"bar\\\\rbaz\" in str(excinfo.value)\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"foo\", \"bar\\nbaz\")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"foo\", \"bar\\x00baz\")])\n    # no leading/trailing whitespace\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"foo\", \"barbaz  \")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"foo\", \"  barbaz\")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"foo\", \"barbaz\\t\")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"foo\", \"\\tbarbaz\")])\n\n    # content-length\n    assert normalize_and_validate([(\"Content-Length\", \"1\")]) == [\n        (b\"content-length\", b\"1\")\n    ]\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"Content-Length\", \"asdf\")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"Content-Length\", \"1x\")])\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"Content-Length\", \"1\"), (\"Content-Length\", \"2\")])\n    assert normalize_and_validate(\n        [(\"Content-Length\", \"0\"), (\"Content-Length\", \"0\")]\n    ) == [(b\"content-length\", b\"0\")]\n    assert normalize_and_validate([(\"Content-Length\", \"0 , 0\")]) == [\n        (b\"content-length\", b\"0\")\n    ]\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate(\n            [(\"Content-Length\", \"1\"), (\"Content-Length\", \"1\"), (\"Content-Length\", \"2\")]\n        )\n    with pytest.raises(LocalProtocolError):\n        normalize_and_validate([(\"Content-Length\", \"1 , 1,2\")])\n\n    # transfer-encoding\n    assert normalize_and_validate([(\"Transfer-Encoding\", \"chunked\")]) == [\n        (b\"transfer-encoding\", b\"chunked\")\n    ]\n    assert normalize_and_validate([(\"Transfer-Encoding\", \"cHuNkEd\")]) == [\n        (b\"transfer-encoding\", b\"chunked\")\n    ]\n    with pytest.raises(LocalProtocolError) as excinfo:\n        normalize_and_validate([(\"Transfer-Encoding\", \"gzip\")])\n    assert excinfo.value.error_status_hint == 501  # Not Implemented\n    with pytest.raises(LocalProtocolError) as excinfo:\n        normalize_and_validate(\n            [(\"Transfer-Encoding\", \"chunked\"), (\"Transfer-Encoding\", \"gzip\")]\n        )\n    assert excinfo.value.error_status_hint == 501  # Not Implemented\n\n\ndef test_get_set_comma_header() -> None:\n    headers = normalize_and_validate(\n        [\n            (\"Connection\", \"close\"),\n            (\"whatever\", \"something\"),\n            (\"connectiON\", \"fOo,, , BAR\"),\n        ]\n    )\n\n    assert get_comma_header(headers, b\"connection\") == [b\"close\", b\"foo\", b\"bar\"]\n\n    headers = set_comma_header(headers, b\"newthing\", [\"a\", \"b\"])  # type: ignore\n\n    with pytest.raises(LocalProtocolError):\n        set_comma_header(headers, b\"newthing\", [\"  a\", \"b\"])  # type: ignore\n\n    assert headers == [\n        (b\"connection\", b\"close\"),\n        (b\"whatever\", b\"something\"),\n        (b\"connection\", b\"fOo,, , BAR\"),\n        (b\"newthing\", b\"a\"),\n        (b\"newthing\", b\"b\"),\n    ]\n\n    headers = set_comma_header(headers, b\"whatever\", [\"different thing\"])  # type: ignore\n\n    assert headers == [\n        (b\"connection\", b\"close\"),\n        (b\"connection\", b\"fOo,, , BAR\"),\n        (b\"newthing\", b\"a\"),\n        (b\"newthing\", b\"b\"),\n        (b\"whatever\", b\"different thing\"),\n    ]\n\n\ndef test_has_100_continue() -> None:\n    assert has_expect_100_continue(\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"example.com\"), (\"Expect\", \"100-continue\")],\n        )\n    )\n    assert not has_expect_100_continue(\n        Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"example.com\")])\n    )\n    # Case insensitive\n    assert has_expect_100_continue(\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"example.com\"), (\"Expect\", \"100-Continue\")],\n        )\n    )\n    # Doesn't work in HTTP/1.0\n    assert not has_expect_100_continue(\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"example.com\"), (\"Expect\", \"100-continue\")],\n            http_version=\"1.0\",\n        )\n    )\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/h11/tests/test_receivebuffer.py", "content": "import re\nfrom typing import Tuple\n\nimport pytest\n\nfrom .._receivebuffer import ReceiveBuffer\n\n\ndef test_receivebuffer() -> None:\n    b = ReceiveBuffer()\n    assert not b\n    assert len(b) == 0\n    assert bytes(b) == b\"\"\n\n    b += b\"123\"\n    assert b\n    assert len(b) == 3\n    assert bytes(b) == b\"123\"\n\n    assert bytes(b) == b\"123\"\n\n    assert b.maybe_extract_at_most(2) == b\"12\"\n    assert b\n    assert len(b) == 1\n    assert bytes(b) == b\"3\"\n\n    assert bytes(b) == b\"3\"\n\n    assert b.maybe_extract_at_most(10) == b\"3\"\n    assert bytes(b) == b\"\"\n\n    assert b.maybe_extract_at_most(10) is None\n    assert not b\n\n    ################################################################\n    # maybe_extract_until_next\n    ################################################################\n\n    b += b\"123\\n456\\r\\n789\\r\\n\"\n\n    assert b.maybe_extract_next_line() == b\"123\\n456\\r\\n\"\n    assert bytes(b) == b\"789\\r\\n\"\n\n    assert b.maybe_extract_next_line() == b\"789\\r\\n\"\n    assert bytes(b) == b\"\"\n\n    b += b\"12\\r\"\n    assert b.maybe_extract_next_line() is None\n    assert bytes(b) == b\"12\\r\"\n\n    b += b\"345\\n\\r\"\n    assert b.maybe_extract_next_line() is None\n    assert bytes(b) == b\"12\\r345\\n\\r\"\n\n    # here we stopped at the middle of b\"\\r\\n\" delimiter\n\n    b += b\"\\n6789aaa123\\r\\n\"\n    assert b.maybe_extract_next_line() == b\"12\\r345\\n\\r\\n\"\n    assert b.maybe_extract_next_line() == b\"6789aaa123\\r\\n\"\n    assert b.maybe_extract_next_line() is None\n    assert bytes(b) == b\"\"\n\n    ################################################################\n    # maybe_extract_lines\n    ################################################################\n\n    b += b\"123\\r\\na: b\\r\\nfoo:bar\\r\\n\\r\\ntrailing\"\n    lines = b.maybe_extract_lines()\n    assert lines == [b\"123\", b\"a: b\", b\"foo:bar\"]\n    assert bytes(b) == b\"trailing\"\n\n    assert b.maybe_extract_lines() is None\n\n    b += b\"\\r\\n\\r\"\n    assert b.maybe_extract_lines() is None\n\n    assert b.maybe_extract_at_most(100) == b\"trailing\\r\\n\\r\"\n    assert not b\n\n    # Empty body case (as happens at the end of chunked encoding if there are\n    # no trailing headers, e.g.)\n    b += b\"\\r\\ntrailing\"\n    assert b.maybe_extract_lines() == []\n    assert bytes(b) == b\"trailing\"\n\n\n@pytest.mark.parametrize(\n    \"data\",\n    [\n        pytest.param(\n            (\n                b\"HTTP/1.1 200 OK\\r\\n\",\n                b\"Content-type: text/plain\\r\\n\",\n                b\"Connection: close\\r\\n\",\n                b\"\\r\\n\",\n                b\"Some body\",\n            ),\n            id=\"with_crlf_delimiter\",\n        ),\n        pytest.param(\n            (\n                b\"HTTP/1.1 200 OK\\n\",\n                b\"Content-type: text/plain\\n\",\n                b\"Connection: close\\n\",\n                b\"\\n\",\n                b\"Some body\",\n            ),\n            id=\"with_lf_only_delimiter\",\n        ),\n        pytest.param(\n            (\n                b\"HTTP/1.1 200 OK\\n\",\n                b\"Content-type: text/plain\\r\\n\",\n                b\"Connection: close\\n\",\n                b\"\\n\",\n                b\"Some body\",\n            ),\n            id=\"with_mixed_crlf_and_lf\",\n        ),\n    ],\n)\ndef test_receivebuffer_for_invalid_delimiter(data: Tuple[bytes]) -> None:\n    b = ReceiveBuffer()\n\n    for line in data:\n        b += line\n\n    lines = b.maybe_extract_lines()\n\n    assert lines == [\n        b\"HTTP/1.1 200 OK\",\n        b\"Content-type: text/plain\",\n        b\"Connection: close\",\n    ]\n    assert bytes(b) == b\"Some body\"\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/h11/tests/helpers.py", "content": "from typing import cast, List, Type, Union, ValuesView\n\nfrom .._connection import Connection, NEED_DATA, PAUSED\nfrom .._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    Event,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom .._state import CLIENT, CLOSED, DONE, MUST_CLOSE, SERVER\nfrom .._util import Sentinel\n\ntry:\n    from typing import Literal\nexcept ImportError:\n    from typing_extensions import Literal  # type: ignore\n\n\ndef get_all_events(conn: Connection) -> List[Event]:\n    got_events = []\n    while True:\n        event = conn.next_event()\n        if event in (NEED_DATA, PAUSED):\n            break\n        event = cast(Event, event)\n        got_events.append(event)\n        if type(event) is ConnectionClosed:\n            break\n    return got_events\n\n\ndef receive_and_get(conn: Connection, data: bytes) -> List[Event]:\n    conn.receive_data(data)\n    return get_all_events(conn)\n\n\n# Merges adjacent Data events, converts payloads to bytestrings, and removes\n# chunk boundaries.\ndef normalize_data_events(in_events: List[Event]) -> List[Event]:\n    out_events: List[Event] = []\n    for event in in_events:\n        if type(event) is Data:\n            event = Data(data=bytes(event.data), chunk_start=False, chunk_end=False)\n        if out_events and type(out_events[-1]) is type(event) is Data:\n            out_events[-1] = Data(\n                data=out_events[-1].data + event.data,\n                chunk_start=out_events[-1].chunk_start,\n                chunk_end=out_events[-1].chunk_end,\n            )\n        else:\n            out_events.append(event)\n    return out_events\n\n\n# Given that we want to write tests that push some events through a Connection\n# and check that its state updates appropriately... we might as make a habit\n# of pushing them through two Connections with a fake network link in\n# between.\nclass ConnectionPair:\n    def __init__(self) -> None:\n        self.conn = {CLIENT: Connection(CLIENT), SERVER: Connection(SERVER)}\n        self.other = {CLIENT: SERVER, SERVER: CLIENT}\n\n    @property\n    def conns(self) -> ValuesView[Connection]:\n        return self.conn.values()\n\n    # expect=\"match\" if expect=send_events; expect=[...] to say what expected\n    def send(\n        self,\n        role: Type[Sentinel],\n        send_events: Union[List[Event], Event],\n        expect: Union[List[Event], Event, Literal[\"match\"]] = \"match\",\n    ) -> bytes:\n        if not isinstance(send_events, list):\n            send_events = [send_events]\n        data = b\"\"\n        closed = False\n        for send_event in send_events:\n            new_data = self.conn[role].send(send_event)\n            if new_data is None:\n                closed = True\n            else:\n                data += new_data\n        # send uses b\"\" to mean b\"\", and None to mean closed\n        # receive uses b\"\" to mean closed, and None to mean \"try again\"\n        # so we have to translate between the two conventions\n        if data:\n            self.conn[self.other[role]].receive_data(data)\n        if closed:\n            self.conn[self.other[role]].receive_data(b\"\")\n        got_events = get_all_events(self.conn[self.other[role]])\n        if expect == \"match\":\n            expect = send_events\n        if not isinstance(expect, list):\n            expect = [expect]\n        assert got_events == expect\n        return data\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/test_stack_saved.py", "content": "import greenlet\nimport unittest\n\n\nclass Test(unittest.TestCase):\n\n    def test_stack_saved(self):\n        main = greenlet.getcurrent()\n        self.assertEqual(main._stack_saved, 0)\n\n        def func():\n            main.switch(main._stack_saved)\n\n        g = greenlet.greenlet(func)\n        x = g.switch()\n        assert x > 0, x\n        assert g._stack_saved > 0, g._stack_saved\n        g.switch()\n        assert g._stack_saved == 0, g._stack_saved\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/h11/tests/test_against_stdlib_http.py", "content": "import json\nimport os.path\nimport socket\nimport socketserver\nimport threading\nfrom contextlib import closing, contextmanager\nfrom http.server import SimpleHTTPRequestHandler\nfrom typing import Callable, Generator\nfrom urllib.request import urlopen\n\nimport h11\n\n\n@contextmanager\ndef socket_server(\n    handler: Callable[..., socketserver.BaseRequestHandler]\n) -> Generator[socketserver.TCPServer, None, None]:\n    httpd = socketserver.TCPServer((\"127.0.0.1\", 0), handler)\n    thread = threading.Thread(\n        target=httpd.serve_forever, kwargs={\"poll_interval\": 0.01}\n    )\n    thread.daemon = True\n    try:\n        thread.start()\n        yield httpd\n    finally:\n        httpd.shutdown()\n\n\ntest_file_path = os.path.join(os.path.dirname(__file__), \"data/test-file\")\nwith open(test_file_path, \"rb\") as f:\n    test_file_data = f.read()\n\n\nclass SingleMindedRequestHandler(SimpleHTTPRequestHandler):\n    def translate_path(self, path: str) -> str:\n        return test_file_path\n\n\ndef test_h11_as_client() -> None:\n    with socket_server(SingleMindedRequestHandler) as httpd:\n        with closing(socket.create_connection(httpd.server_address)) as s:\n            c = h11.Connection(h11.CLIENT)\n\n            s.sendall(\n                c.send(  # type: ignore[arg-type]\n                    h11.Request(\n                        method=\"GET\", target=\"/foo\", headers=[(\"Host\", \"localhost\")]\n                    )\n                )\n            )\n            s.sendall(c.send(h11.EndOfMessage()))  # type: ignore[arg-type]\n\n            data = bytearray()\n            while True:\n                event = c.next_event()\n                print(event)\n                if event is h11.NEED_DATA:\n                    # Use a small read buffer to make things more challenging\n                    # and exercise more paths :-)\n                    c.receive_data(s.recv(10))\n                    continue\n                if type(event) is h11.Response:\n                    assert event.status_code == 200\n                if type(event) is h11.Data:\n                    data += event.data\n                if type(event) is h11.EndOfMessage:\n                    break\n            assert bytes(data) == test_file_data\n\n\nclass H11RequestHandler(socketserver.BaseRequestHandler):\n    def handle(self) -> None:\n        with closing(self.request) as s:\n            c = h11.Connection(h11.SERVER)\n            request = None\n            while True:\n                event = c.next_event()\n                if event is h11.NEED_DATA:\n                    # Use a small read buffer to make things more challenging\n                    # and exercise more paths :-)\n                    c.receive_data(s.recv(10))\n                    continue\n                if type(event) is h11.Request:\n                    request = event\n                if type(event) is h11.EndOfMessage:\n                    break\n            assert request is not None\n            info = json.dumps(\n                {\n                    \"method\": request.method.decode(\"ascii\"),\n                    \"target\": request.target.decode(\"ascii\"),\n                    \"headers\": {\n                        name.decode(\"ascii\"): value.decode(\"ascii\")\n                        for (name, value) in request.headers\n                    },\n                }\n            )\n            s.sendall(c.send(h11.Response(status_code=200, headers=[])))  # type: ignore[arg-type]\n            s.sendall(c.send(h11.Data(data=info.encode(\"ascii\"))))\n            s.sendall(c.send(h11.EndOfMessage()))\n\n\ndef test_h11_as_server() -> None:\n    with socket_server(H11RequestHandler) as httpd:\n        host, port = httpd.server_address\n        url = \"http://{}:{}/some-path\".format(host, port)\n        with closing(urlopen(url)) as f:\n            assert f.getcode() == 200\n            data = f.read()\n    info = json.loads(data.decode(\"ascii\"))\n    print(info)\n    assert info[\"method\"] == \"GET\"\n    assert info[\"target\"] == \"/some-path\"\n    assert \"urllib\" in info[\"headers\"][\"user-agent\"]\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/passlib/tests/_test_bad_register.py", "content": "\"\"\"helper for method in test_registry.py\"\"\"\nfrom passlib.registry import register_crypt_handler\nimport passlib.utils.handlers as uh\n\nclass dummy_bad(uh.StaticHandler):\n    name = \"dummy_bad\"\n\nclass alt_dummy_bad(uh.StaticHandler):\n    name = \"dummy_bad\"\n\n# NOTE: if passlib.tests is being run from symlink (e.g. via gaeunit),\n#       this module may be imported a second time as test._test_bad_registry.\n#       we don't want it to do anything in that case.\nif __name__.startswith(\"passlib.tests\"):\n    register_crypt_handler(alt_dummy_bad)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/h11/tests/test_io.py", "content": "from typing import Any, Callable, Generator, List\n\nimport pytest\n\nfrom .._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    Event,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom .._headers import Headers, normalize_and_validate\nfrom .._readers import (\n    _obsolete_line_fold,\n    ChunkedReader,\n    ContentLengthReader,\n    Http10Reader,\n    READERS,\n)\nfrom .._receivebuffer import ReceiveBuffer\nfrom .._state import (\n    CLIENT,\n    CLOSED,\n    DONE,\n    IDLE,\n    MIGHT_SWITCH_PROTOCOL,\n    MUST_CLOSE,\n    SEND_BODY,\n    SEND_RESPONSE,\n    SERVER,\n    SWITCHED_PROTOCOL,\n)\nfrom .._util import LocalProtocolError\nfrom .._writers import (\n    ChunkedWriter,\n    ContentLengthWriter,\n    Http10Writer,\n    write_any_response,\n    write_headers,\n    write_request,\n    WRITERS,\n)\nfrom .helpers import normalize_data_events\n\nSIMPLE_CASES = [\n    (\n        (CLIENT, IDLE),\n        Request(\n            method=\"GET\",\n            target=\"/a\",\n            headers=[(\"Host\", \"foo\"), (\"Connection\", \"close\")],\n        ),\n        b\"GET /a HTTP/1.1\\r\\nHost: foo\\r\\nConnection: close\\r\\n\\r\\n\",\n    ),\n    (\n        (SERVER, SEND_RESPONSE),\n        Response(status_code=200, headers=[(\"Connection\", \"close\")], reason=b\"OK\"),\n        b\"HTTP/1.1 200 OK\\r\\nConnection: close\\r\\n\\r\\n\",\n    ),\n    (\n        (SERVER, SEND_RESPONSE),\n        Response(status_code=200, headers=[], reason=b\"OK\"),  # type: ignore[arg-type]\n        b\"HTTP/1.1 200 OK\\r\\n\\r\\n\",\n    ),\n    (\n        (SERVER, SEND_RESPONSE),\n        InformationalResponse(\n            status_code=101, headers=[(\"Upgrade\", \"websocket\")], reason=b\"Upgrade\"\n        ),\n        b\"HTTP/1.1 101 Upgrade\\r\\nUpgrade: websocket\\r\\n\\r\\n\",\n    ),\n    (\n        (SERVER, SEND_RESPONSE),\n        InformationalResponse(status_code=101, headers=[], reason=b\"Upgrade\"),  # type: ignore[arg-type]\n        b\"HTTP/1.1 101 Upgrade\\r\\n\\r\\n\",\n    ),\n]\n\n\ndef dowrite(writer: Callable[..., None], obj: Any) -> bytes:\n    got_list: List[bytes] = []\n    writer(obj, got_list.append)\n    return b\"\".join(got_list)\n\n\ndef tw(writer: Any, obj: Any, expected: Any) -> None:\n    got = dowrite(writer, obj)\n    assert got == expected\n\n\ndef makebuf(data: bytes) -> ReceiveBuffer:\n    buf = ReceiveBuffer()\n    buf += data\n    return buf\n\n\ndef tr(reader: Any, data: bytes, expected: Any) -> None:\n    def check(got: Any) -> None:\n        assert got == expected\n        # Headers should always be returned as bytes, not e.g. bytearray\n        # https://github.com/python-hyper/wsproto/pull/54#issuecomment-377709478\n        for name, value in getattr(got, \"headers\", []):\n            assert type(name) is bytes\n            assert type(value) is bytes\n\n    # Simple: consume whole thing\n    buf = makebuf(data)\n    check(reader(buf))\n    assert not buf\n\n    # Incrementally growing buffer\n    buf = ReceiveBuffer()\n    for i in range(len(data)):\n        assert reader(buf) is None\n        buf += data[i : i + 1]\n    check(reader(buf))\n\n    # Trailing data\n    buf = makebuf(data)\n    buf += b\"trailing\"\n    check(reader(buf))\n    assert bytes(buf) == b\"trailing\"\n\n\ndef test_writers_simple() -> None:\n    for ((role, state), event, binary) in SIMPLE_CASES:\n        tw(WRITERS[role, state], event, binary)\n\n\ndef test_readers_simple() -> None:\n    for ((role, state), event, binary) in SIMPLE_CASES:\n        tr(READERS[role, state], binary, event)\n\n\ndef test_writers_unusual() -> None:\n    # Simple test of the write_headers utility routine\n    tw(\n        write_headers,\n        normalize_and_validate([(\"foo\", \"bar\"), (\"baz\", \"quux\")]),\n        b\"foo: bar\\r\\nbaz: quux\\r\\n\\r\\n\",\n    )\n    tw(write_headers, Headers([]), b\"\\r\\n\")\n\n    # We understand HTTP/1.0, but we don't speak it\n    with pytest.raises(LocalProtocolError):\n        tw(\n            write_request,\n            Request(\n                method=\"GET\",\n                target=\"/\",\n                headers=[(\"Host\", \"foo\"), (\"Connection\", \"close\")],\n                http_version=\"1.0\",\n            ),\n            None,\n        )\n    with pytest.raises(LocalProtocolError):\n        tw(\n            write_any_response,\n            Response(\n                status_code=200, headers=[(\"Connection\", \"close\")], http_version=\"1.0\"\n            ),\n            None,\n        )\n\n\ndef test_readers_unusual() -> None:\n    # Reading HTTP/1.0\n    tr(\n        READERS[CLIENT, IDLE],\n        b\"HEAD /foo HTTP/1.0\\r\\nSome: header\\r\\n\\r\\n\",\n        Request(\n            method=\"HEAD\",\n            target=\"/foo\",\n            headers=[(\"Some\", \"header\")],\n            http_version=\"1.0\",\n        ),\n    )\n\n    # check no-headers, since it's only legal with HTTP/1.0\n    tr(\n        READERS[CLIENT, IDLE],\n        b\"HEAD /foo HTTP/1.0\\r\\n\\r\\n\",\n        Request(method=\"HEAD\", target=\"/foo\", headers=[], http_version=\"1.0\"),  # type: ignore[arg-type]\n    )\n\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.0 200 OK\\r\\nSome: header\\r\\n\\r\\n\",\n        Response(\n            status_code=200,\n            headers=[(\"Some\", \"header\")],\n            http_version=\"1.0\",\n            reason=b\"OK\",\n        ),\n    )\n\n    # single-character header values (actually disallowed by the ABNF in RFC\n    # 7230 -- this is a bug in the standard that we originally copied...)\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.0 200 OK\\r\\n\" b\"Foo: a a a a a \\r\\n\\r\\n\",\n        Response(\n            status_code=200,\n            headers=[(\"Foo\", \"a a a a a\")],\n            http_version=\"1.0\",\n            reason=b\"OK\",\n        ),\n    )\n\n    # Empty headers -- also legal\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.0 200 OK\\r\\n\" b\"Foo:\\r\\n\\r\\n\",\n        Response(\n            status_code=200, headers=[(\"Foo\", \"\")], http_version=\"1.0\", reason=b\"OK\"\n        ),\n    )\n\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.0 200 OK\\r\\n\" b\"Foo: \\t \\t \\r\\n\\r\\n\",\n        Response(\n            status_code=200, headers=[(\"Foo\", \"\")], http_version=\"1.0\", reason=b\"OK\"\n        ),\n    )\n\n    # Tolerate broken servers that leave off the response code\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.0 200\\r\\n\" b\"Foo: bar\\r\\n\\r\\n\",\n        Response(\n            status_code=200, headers=[(\"Foo\", \"bar\")], http_version=\"1.0\", reason=b\"\"\n        ),\n    )\n\n    # Tolerate headers line endings (\\r\\n and \\n)\n    #    \\n\\r\\b between headers and body\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.1 200 OK\\r\\nSomeHeader: val\\n\\r\\n\",\n        Response(\n            status_code=200,\n            headers=[(\"SomeHeader\", \"val\")],\n            http_version=\"1.1\",\n            reason=\"OK\",\n        ),\n    )\n\n    #   delimited only with \\n\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.1 200 OK\\nSomeHeader1: val1\\nSomeHeader2: val2\\n\\n\",\n        Response(\n            status_code=200,\n            headers=[(\"SomeHeader1\", \"val1\"), (\"SomeHeader2\", \"val2\")],\n            http_version=\"1.1\",\n            reason=\"OK\",\n        ),\n    )\n\n    #   mixed \\r\\n and \\n\n    tr(\n        READERS[SERVER, SEND_RESPONSE],\n        b\"HTTP/1.1 200 OK\\r\\nSomeHeader1: val1\\nSomeHeader2: val2\\n\\r\\n\",\n        Response(\n            status_code=200,\n            headers=[(\"SomeHeader1\", \"val1\"), (\"SomeHeader2\", \"val2\")],\n            http_version=\"1.1\",\n            reason=\"OK\",\n        ),\n    )\n\n    # obsolete line folding\n    tr(\n        READERS[CLIENT, IDLE],\n        b\"HEAD /foo HTTP/1.1\\r\\n\"\n        b\"Host: example.com\\r\\n\"\n        b\"Some: multi-line\\r\\n\"\n        b\" header\\r\\n\"\n        b\"\\tnonsense\\r\\n\"\n        b\"    \\t   \\t\\tI guess\\r\\n\"\n        b\"Connection: close\\r\\n\"\n        b\"More-nonsense: in the\\r\\n\"\n        b\"    last header  \\r\\n\\r\\n\",\n        Request(\n            method=\"HEAD\",\n            target=\"/foo\",\n            headers=[\n                (\"Host\", \"example.com\"),\n                (\"Some\", \"multi-line header nonsense I guess\"),\n                (\"Connection\", \"close\"),\n                (\"More-nonsense\", \"in the last header\"),\n            ],\n        ),\n    )\n\n    with pytest.raises(LocalProtocolError):\n        tr(\n            READERS[CLIENT, IDLE],\n            b\"HEAD /foo HTTP/1.1\\r\\n\" b\"  folded: line\\r\\n\\r\\n\",\n            None,\n        )\n\n    with pytest.raises(LocalProtocolError):\n        tr(\n            READERS[CLIENT, IDLE],\n            b\"HEAD /foo HTTP/1.1\\r\\n\" b\"foo  : line\\r\\n\\r\\n\",\n            None,\n        )\n    with pytest.raises(LocalProtocolError):\n        tr(\n            READERS[CLIENT, IDLE],\n            b\"HEAD /foo HTTP/1.1\\r\\n\" b\"foo\\t: line\\r\\n\\r\\n\",\n            None,\n        )\n    with pytest.raises(LocalProtocolError):\n        tr(\n            READERS[CLIENT, IDLE],\n            b\"HEAD /foo HTTP/1.1\\r\\n\" b\"foo\\t: line\\r\\n\\r\\n\",\n            None,\n        )\n    with pytest.raises(LocalProtocolError):\n        tr(READERS[CLIENT, IDLE], b\"HEAD /foo HTTP/1.1\\r\\n\" b\": line\\r\\n\\r\\n\", None)\n\n\ndef test__obsolete_line_fold_bytes() -> None:\n    # _obsolete_line_fold has a defensive cast to bytearray, which is\n    # necessary to protect against O(n^2) behavior in case anyone ever passes\n    # in regular bytestrings... but right now we never pass in regular\n    # bytestrings. so this test just exists to get some coverage on that\n    # defensive cast.\n    assert list(_obsolete_line_fold([b\"aaa\", b\"bbb\", b\"  ccc\", b\"ddd\"])) == [\n        b\"aaa\",\n        bytearray(b\"bbb ccc\"),\n        b\"ddd\",\n    ]\n\n\ndef _run_reader_iter(\n    reader: Any, buf: bytes, do_eof: bool\n) -> Generator[Any, None, None]:\n    while True:\n        event = reader(buf)\n        if event is None:\n            break\n        yield event\n        # body readers have undefined behavior after returning EndOfMessage,\n        # because this changes the state so they don't get called again\n        if type(event) is EndOfMessage:\n            break\n    if do_eof:\n        assert not buf\n        yield reader.read_eof()\n\n\ndef _run_reader(*args: Any) -> List[Event]:\n    events = list(_run_reader_iter(*args))\n    return normalize_data_events(events)\n\n\ndef t_body_reader(thunk: Any, data: bytes, expected: Any, do_eof: bool = False) -> None:\n    # Simple: consume whole thing\n    print(\"Test 1\")\n    buf = makebuf(data)\n    assert _run_reader(thunk(), buf, do_eof) == expected\n\n    # Incrementally growing buffer\n    print(\"Test 2\")\n    reader = thunk()\n    buf = ReceiveBuffer()\n    events = []\n    for i in range(len(data)):\n        events += _run_reader(reader, buf, False)\n        buf += data[i : i + 1]\n    events += _run_reader(reader, buf, do_eof)\n    assert normalize_data_events(events) == expected\n\n    is_complete = any(type(event) is EndOfMessage for event in expected)\n    if is_complete and not do_eof:\n        buf = makebuf(data + b\"trailing\")\n        assert _run_reader(thunk(), buf, False) == expected\n\n\ndef test_ContentLengthReader() -> None:\n    t_body_reader(lambda: ContentLengthReader(0), b\"\", [EndOfMessage()])\n\n    t_body_reader(\n        lambda: ContentLengthReader(10),\n        b\"0123456789\",\n        [Data(data=b\"0123456789\"), EndOfMessage()],\n    )\n\n\ndef test_Http10Reader() -> None:\n    t_body_reader(Http10Reader, b\"\", [EndOfMessage()], do_eof=True)\n    t_body_reader(Http10Reader, b\"asdf\", [Data(data=b\"asdf\")], do_eof=False)\n    t_body_reader(\n        Http10Reader, b\"asdf\", [Data(data=b\"asdf\"), EndOfMessage()], do_eof=True\n    )\n\n\ndef test_ChunkedReader() -> None:\n    t_body_reader(ChunkedReader, b\"0\\r\\n\\r\\n\", [EndOfMessage()])\n\n    t_body_reader(\n        ChunkedReader,\n        b\"0\\r\\nSome: header\\r\\n\\r\\n\",\n        [EndOfMessage(headers=[(\"Some\", \"header\")])],\n    )\n\n    t_body_reader(\n        ChunkedReader,\n        b\"5\\r\\n01234\\r\\n\"\n        + b\"10\\r\\n0123456789abcdef\\r\\n\"\n        + b\"0\\r\\n\"\n        + b\"Some: header\\r\\n\\r\\n\",\n        [\n            Data(data=b\"012340123456789abcdef\"),\n            EndOfMessage(headers=[(\"Some\", \"header\")]),\n        ],\n    )\n\n    t_body_reader(\n        ChunkedReader,\n        b\"5\\r\\n01234\\r\\n\" + b\"10\\r\\n0123456789abcdef\\r\\n\" + b\"0\\r\\n\\r\\n\",\n        [Data(data=b\"012340123456789abcdef\"), EndOfMessage()],\n    )\n\n    # handles upper and lowercase hex\n    t_body_reader(\n        ChunkedReader,\n        b\"aA\\r\\n\" + b\"x\" * 0xAA + b\"\\r\\n\" + b\"0\\r\\n\\r\\n\",\n        [Data(data=b\"x\" * 0xAA), EndOfMessage()],\n    )\n\n    # refuses arbitrarily long chunk integers\n    with pytest.raises(LocalProtocolError):\n        # Technically this is legal HTTP/1.1, but we refuse to process chunk\n        # sizes that don't fit into 20 characters of hex\n        t_body_reader(ChunkedReader, b\"9\" * 100 + b\"\\r\\nxxx\", [Data(data=b\"xxx\")])\n\n    # refuses garbage in the chunk count\n    with pytest.raises(LocalProtocolError):\n        t_body_reader(ChunkedReader, b\"10\\x00\\r\\nxxx\", None)\n\n    # handles (and discards) \"chunk extensions\" omg wtf\n    t_body_reader(\n        ChunkedReader,\n        b\"5; hello=there\\r\\n\"\n        + b\"xxxxx\"\n        + b\"\\r\\n\"\n        + b'0; random=\"junk\"; some=more; canbe=lonnnnngg\\r\\n\\r\\n',\n        [Data(data=b\"xxxxx\"), EndOfMessage()],\n    )\n\n\ndef test_ContentLengthWriter() -> None:\n    w = ContentLengthWriter(5)\n    assert dowrite(w, Data(data=b\"123\")) == b\"123\"\n    assert dowrite(w, Data(data=b\"45\")) == b\"45\"\n    assert dowrite(w, EndOfMessage()) == b\"\"\n\n    w = ContentLengthWriter(5)\n    with pytest.raises(LocalProtocolError):\n        dowrite(w, Data(data=b\"123456\"))\n\n    w = ContentLengthWriter(5)\n    dowrite(w, Data(data=b\"123\"))\n    with pytest.raises(LocalProtocolError):\n        dowrite(w, Data(data=b\"456\"))\n\n    w = ContentLengthWriter(5)\n    dowrite(w, Data(data=b\"123\"))\n    with pytest.raises(LocalProtocolError):\n        dowrite(w, EndOfMessage())\n\n    w = ContentLengthWriter(5)\n    dowrite(w, Data(data=b\"123\")) == b\"123\"\n    dowrite(w, Data(data=b\"45\")) == b\"45\"\n    with pytest.raises(LocalProtocolError):\n        dowrite(w, EndOfMessage(headers=[(\"Etag\", \"asdf\")]))\n\n\ndef test_ChunkedWriter() -> None:\n    w = ChunkedWriter()\n    assert dowrite(w, Data(data=b\"aaa\")) == b\"3\\r\\naaa\\r\\n\"\n    assert dowrite(w, Data(data=b\"a\" * 20)) == b\"14\\r\\n\" + b\"a\" * 20 + b\"\\r\\n\"\n\n    assert dowrite(w, Data(data=b\"\")) == b\"\"\n\n    assert dowrite(w, EndOfMessage()) == b\"0\\r\\n\\r\\n\"\n\n    assert (\n        dowrite(w, EndOfMessage(headers=[(\"Etag\", \"asdf\"), (\"a\", \"b\")]))\n        == b\"0\\r\\nEtag: asdf\\r\\na: b\\r\\n\\r\\n\"\n    )\n\n\ndef test_Http10Writer() -> None:\n    w = Http10Writer()\n    assert dowrite(w, Data(data=b\"1234\")) == b\"1234\"\n    assert dowrite(w, EndOfMessage()) == b\"\"\n\n    with pytest.raises(LocalProtocolError):\n        dowrite(w, EndOfMessage(headers=[(\"Etag\", \"asdf\")]))\n\n\ndef test_reject_garbage_after_request_line() -> None:\n    with pytest.raises(LocalProtocolError):\n        tr(READERS[SERVER, SEND_RESPONSE], b\"HTTP/1.0 200 OK\\x00xxxx\\r\\n\\r\\n\", None)\n\n\ndef test_reject_garbage_after_response_line() -> None:\n    with pytest.raises(LocalProtocolError):\n        tr(\n            READERS[CLIENT, IDLE],\n            b\"HEAD /foo HTTP/1.1 xxxxxx\\r\\n\" b\"Host: a\\r\\n\\r\\n\",\n            None,\n        )\n\n\ndef test_reject_garbage_in_header_line() -> None:\n    with pytest.raises(LocalProtocolError):\n        tr(\n            READERS[CLIENT, IDLE],\n            b\"HEAD /foo HTTP/1.1\\r\\n\" b\"Host: foo\\x00bar\\r\\n\\r\\n\",\n            None,\n        )\n\n\ndef test_reject_non_vchar_in_path() -> None:\n    for bad_char in b\"\\x00\\x20\\x7f\\xee\":\n        message = bytearray(b\"HEAD /\")\n        message.append(bad_char)\n        message.extend(b\" HTTP/1.1\\r\\nHost: foobar\\r\\n\\r\\n\")\n        with pytest.raises(LocalProtocolError):\n            tr(READERS[CLIENT, IDLE], message, None)\n\n\n# https://github.com/python-hyper/h11/issues/57\ndef test_allow_some_garbage_in_cookies() -> None:\n    tr(\n        READERS[CLIENT, IDLE],\n        b\"HEAD /foo HTTP/1.1\\r\\n\"\n        b\"Host: foo\\r\\n\"\n        b\"Set-Cookie: ___utmvafIumyLc=kUd\\x01UpAt; path=/; Max-Age=900\\r\\n\"\n        b\"\\r\\n\",\n        Request(\n            method=\"HEAD\",\n            target=\"/foo\",\n            headers=[\n                (\"Host\", \"foo\"),\n                (\"Set-Cookie\", \"___utmvafIumyLc=kUd\\x01UpAt; path=/; Max-Age=900\"),\n            ],\n        ),\n    )\n\n\ndef test_host_comes_first() -> None:\n    tw(\n        write_headers,\n        normalize_and_validate([(\"foo\", \"bar\"), (\"Host\", \"example.com\")]),\n        b\"Host: example.com\\r\\nfoo: bar\\r\\n\\r\\n\",\n    )\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/passlib/tests/__main__.py", "content": "import os\nfrom nose import run\nrun(\n    defaultTest=os.path.dirname(__file__),\n)\n\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/h11/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/multipart/tests/compat.py", "content": "try:\n    import unittest2 as unittest\nexcept ImportError:\n    import unittest\n\nimport os\nimport re\nimport sys\nimport types\nimport functools\n\n\ndef ensure_in_path(path):\n    \"\"\"\n    Ensure that a given path is in the sys.path array\n    \"\"\"\n    if not os.path.isdir(path):\n        raise RuntimeError('Tried to add nonexisting path')\n\n    def _samefile(x, y):\n        try:\n            return os.path.samefile(x, y)\n        except (IOError, OSError):\n            return False\n        except AttributeError:\n            # Probably on Windows.\n            path1 = os.path.abspath(x).lower()\n            path2 = os.path.abspath(y).lower()\n            return path1 == path2\n\n    # Remove existing copies of it.\n    for pth in sys.path:\n        if _samefile(pth, path):\n            sys.path.remove(pth)\n\n    # Add it at the beginning.\n    sys.path.insert(0, path)\n\n\n# Check if pytest is imported.  If so, we use it to create marking decorators.\n# If not, we just create a function that does nothing.\ntry:\n    import pytest\nexcept ImportError:\n    pytest = None\n\nif pytest is not None:\n    slow_test = pytest.mark.slow_test\n    xfail = pytest.mark.xfail\n\nelse:\n    slow_test = lambda x: x\n\n    def xfail(*args, **kwargs):\n        if len(args) > 0 and isinstance(args[0], types.FunctionType):\n            return args[0]\n\n        return lambda x: x\n\n\n# We don't use the py.test parametrizing function, since it seems to break\n# with unittest.TestCase subclasses.\ndef parametrize(field_names, field_values):\n    # If we're not given a list of field names, we make it.\n    if not isinstance(field_names, (tuple, list)):\n        field_names = (field_names,)\n        field_values = [(val,) for val in field_values]\n\n    # Create a decorator that saves this list of field names and values on the\n    # function for later parametrizing.\n    def decorator(func):\n        func.__dict__['param_names'] = field_names\n        func.__dict__['param_values'] = field_values\n        return func\n\n    return decorator\n\n\n# This is a metaclass that actually performs the parametrization.\nclass ParametrizingMetaclass(type):\n    IDENTIFIER_RE = re.compile('[^A-Za-z0-9]')\n\n    def __new__(klass, name, bases, attrs):\n        new_attrs = attrs.copy()\n        for attr_name, attr in attrs.items():\n            # We only care about functions\n            if not isinstance(attr, types.FunctionType):\n                continue\n\n            param_names = attr.__dict__.pop('param_names', None)\n            param_values = attr.__dict__.pop('param_values', None)\n            if param_names is None or param_values is None:\n                continue\n\n            # Create multiple copies of the function.\n            for i, values in enumerate(param_values):\n                assert len(param_names) == len(values)\n\n                # Get a repr of the values, and fix it to be a valid identifier\n                human = '_'.join(\n                    [klass.IDENTIFIER_RE.sub('', repr(x)) for x in values]\n                )\n\n                # Create a new name.\n                # new_name = attr.__name__ + \"_%d\" % i\n                new_name = attr.__name__ + \"__\" + human\n\n                # Create a replacement function.\n                def create_new_func(func, names, values):\n                    # Create a kwargs dictionary.\n                    kwargs = dict(zip(names, values))\n\n                    @functools.wraps(func)\n                    def new_func(self):\n                        return func(self, **kwargs)\n\n                    # Manually set the name and return the new function.\n                    new_func.__name__ = new_name\n                    return new_func\n\n                # Actually create the new function.\n                new_func = create_new_func(attr, param_names, values)\n\n                # Save this new function in our attrs dict.\n                new_attrs[new_name] = new_func\n\n            # Remove the old attribute from our new dictionary.\n            del new_attrs[attr_name]\n\n        # We create the class as normal, except we use our new attributes.\n        return type.__new__(klass, name, bases, new_attrs)\n\n\n# This is a class decorator that actually applies the above metaclass.\ndef parametrize_class(klass):\n    return ParametrizingMetaclass(klass.__name__,\n                                  klass.__bases__,\n                                  klass.__dict__)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/h11/tests/test_connection.py", "content": "from typing import Any, cast, Dict, List, Optional, Tuple, Type\n\nimport pytest\n\nfrom .._connection import _body_framing, _keep_alive, Connection, NEED_DATA, PAUSED\nfrom .._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    Event,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom .._state import (\n    CLIENT,\n    CLOSED,\n    DONE,\n    ERROR,\n    IDLE,\n    MIGHT_SWITCH_PROTOCOL,\n    MUST_CLOSE,\n    SEND_BODY,\n    SEND_RESPONSE,\n    SERVER,\n    SWITCHED_PROTOCOL,\n)\nfrom .._util import LocalProtocolError, RemoteProtocolError, Sentinel\nfrom .helpers import ConnectionPair, get_all_events, receive_and_get\n\n\ndef test__keep_alive() -> None:\n    assert _keep_alive(\n        Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"Example.com\")])\n    )\n    assert not _keep_alive(\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"Example.com\"), (\"Connection\", \"close\")],\n        )\n    )\n    assert not _keep_alive(\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"Example.com\"), (\"Connection\", \"a, b, cLOse, foo\")],\n        )\n    )\n    assert not _keep_alive(\n        Request(method=\"GET\", target=\"/\", headers=[], http_version=\"1.0\")  # type: ignore[arg-type]\n    )\n\n    assert _keep_alive(Response(status_code=200, headers=[]))  # type: ignore[arg-type]\n    assert not _keep_alive(Response(status_code=200, headers=[(\"Connection\", \"close\")]))\n    assert not _keep_alive(\n        Response(status_code=200, headers=[(\"Connection\", \"a, b, cLOse, foo\")])\n    )\n    assert not _keep_alive(Response(status_code=200, headers=[], http_version=\"1.0\"))  # type: ignore[arg-type]\n\n\ndef test__body_framing() -> None:\n    def headers(cl: Optional[int], te: bool) -> List[Tuple[str, str]]:\n        headers = []\n        if cl is not None:\n            headers.append((\"Content-Length\", str(cl)))\n        if te:\n            headers.append((\"Transfer-Encoding\", \"chunked\"))\n        return headers\n\n    def resp(\n        status_code: int = 200, cl: Optional[int] = None, te: bool = False\n    ) -> Response:\n        return Response(status_code=status_code, headers=headers(cl, te))\n\n    def req(cl: Optional[int] = None, te: bool = False) -> Request:\n        h = headers(cl, te)\n        h += [(\"Host\", \"example.com\")]\n        return Request(method=\"GET\", target=\"/\", headers=h)\n\n    # Special cases where the headers are ignored:\n    for kwargs in [{}, {\"cl\": 100}, {\"te\": True}, {\"cl\": 100, \"te\": True}]:\n        kwargs = cast(Dict[str, Any], kwargs)\n        for meth, r in [\n            (b\"HEAD\", resp(**kwargs)),\n            (b\"GET\", resp(status_code=204, **kwargs)),\n            (b\"GET\", resp(status_code=304, **kwargs)),\n        ]:\n            assert _body_framing(meth, r) == (\"content-length\", (0,))\n\n    # Transfer-encoding\n    for kwargs in [{\"te\": True}, {\"cl\": 100, \"te\": True}]:\n        kwargs = cast(Dict[str, Any], kwargs)\n        for meth, r in [(None, req(**kwargs)), (b\"GET\", resp(**kwargs))]:  # type: ignore\n            assert _body_framing(meth, r) == (\"chunked\", ())\n\n    # Content-Length\n    for meth, r in [(None, req(cl=100)), (b\"GET\", resp(cl=100))]:  # type: ignore\n        assert _body_framing(meth, r) == (\"content-length\", (100,))\n\n    # No headers\n    assert _body_framing(None, req()) == (\"content-length\", (0,))  # type: ignore\n    assert _body_framing(b\"GET\", resp()) == (\"http/1.0\", ())\n\n\ndef test_Connection_basics_and_content_length() -> None:\n    with pytest.raises(ValueError):\n        Connection(\"CLIENT\")  # type: ignore\n\n    p = ConnectionPair()\n    assert p.conn[CLIENT].our_role is CLIENT\n    assert p.conn[CLIENT].their_role is SERVER\n    assert p.conn[SERVER].our_role is SERVER\n    assert p.conn[SERVER].their_role is CLIENT\n\n    data = p.send(\n        CLIENT,\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"example.com\"), (\"Content-Length\", \"10\")],\n        ),\n    )\n    assert data == (\n        b\"GET / HTTP/1.1\\r\\n\" b\"Host: example.com\\r\\n\" b\"Content-Length: 10\\r\\n\\r\\n\"\n    )\n\n    for conn in p.conns:\n        assert conn.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n    assert p.conn[CLIENT].our_state is SEND_BODY\n    assert p.conn[CLIENT].their_state is SEND_RESPONSE\n    assert p.conn[SERVER].our_state is SEND_RESPONSE\n    assert p.conn[SERVER].their_state is SEND_BODY\n\n    assert p.conn[CLIENT].their_http_version is None\n    assert p.conn[SERVER].their_http_version == b\"1.1\"\n\n    data = p.send(SERVER, InformationalResponse(status_code=100, headers=[]))  # type: ignore[arg-type]\n    assert data == b\"HTTP/1.1 100 \\r\\n\\r\\n\"\n\n    data = p.send(SERVER, Response(status_code=200, headers=[(\"Content-Length\", \"11\")]))\n    assert data == b\"HTTP/1.1 200 \\r\\nContent-Length: 11\\r\\n\\r\\n\"\n\n    for conn in p.conns:\n        assert conn.states == {CLIENT: SEND_BODY, SERVER: SEND_BODY}\n\n    assert p.conn[CLIENT].their_http_version == b\"1.1\"\n    assert p.conn[SERVER].their_http_version == b\"1.1\"\n\n    data = p.send(CLIENT, Data(data=b\"12345\"))\n    assert data == b\"12345\"\n    data = p.send(\n        CLIENT, Data(data=b\"67890\"), expect=[Data(data=b\"67890\"), EndOfMessage()]\n    )\n    assert data == b\"67890\"\n    data = p.send(CLIENT, EndOfMessage(), expect=[])\n    assert data == b\"\"\n\n    for conn in p.conns:\n        assert conn.states == {CLIENT: DONE, SERVER: SEND_BODY}\n\n    data = p.send(SERVER, Data(data=b\"1234567890\"))\n    assert data == b\"1234567890\"\n    data = p.send(SERVER, Data(data=b\"1\"), expect=[Data(data=b\"1\"), EndOfMessage()])\n    assert data == b\"1\"\n    data = p.send(SERVER, EndOfMessage(), expect=[])\n    assert data == b\"\"\n\n    for conn in p.conns:\n        assert conn.states == {CLIENT: DONE, SERVER: DONE}\n\n\ndef test_chunked() -> None:\n    p = ConnectionPair()\n\n    p.send(\n        CLIENT,\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            headers=[(\"Host\", \"example.com\"), (\"Transfer-Encoding\", \"chunked\")],\n        ),\n    )\n    data = p.send(CLIENT, Data(data=b\"1234567890\", chunk_start=True, chunk_end=True))\n    assert data == b\"a\\r\\n1234567890\\r\\n\"\n    data = p.send(CLIENT, Data(data=b\"abcde\", chunk_start=True, chunk_end=True))\n    assert data == b\"5\\r\\nabcde\\r\\n\"\n    data = p.send(CLIENT, Data(data=b\"\"), expect=[])\n    assert data == b\"\"\n    data = p.send(CLIENT, EndOfMessage(headers=[(\"hello\", \"there\")]))\n    assert data == b\"0\\r\\nhello: there\\r\\n\\r\\n\"\n\n    p.send(\n        SERVER, Response(status_code=200, headers=[(\"Transfer-Encoding\", \"chunked\")])\n    )\n    p.send(SERVER, Data(data=b\"54321\", chunk_start=True, chunk_end=True))\n    p.send(SERVER, Data(data=b\"12345\", chunk_start=True, chunk_end=True))\n    p.send(SERVER, EndOfMessage())\n\n    for conn in p.conns:\n        assert conn.states == {CLIENT: DONE, SERVER: DONE}\n\n\ndef test_chunk_boundaries() -> None:\n    conn = Connection(our_role=SERVER)\n\n    request = (\n        b\"POST / HTTP/1.1\\r\\n\"\n        b\"Host: example.com\\r\\n\"\n        b\"Transfer-Encoding: chunked\\r\\n\"\n        b\"\\r\\n\"\n    )\n    conn.receive_data(request)\n    assert conn.next_event() == Request(\n        method=\"POST\",\n        target=\"/\",\n        headers=[(\"Host\", \"example.com\"), (\"Transfer-Encoding\", \"chunked\")],\n    )\n    assert conn.next_event() is NEED_DATA\n\n    conn.receive_data(b\"5\\r\\nhello\\r\\n\")\n    assert conn.next_event() == Data(data=b\"hello\", chunk_start=True, chunk_end=True)\n\n    conn.receive_data(b\"5\\r\\nhel\")\n    assert conn.next_event() == Data(data=b\"hel\", chunk_start=True, chunk_end=False)\n\n    conn.receive_data(b\"l\")\n    assert conn.next_event() == Data(data=b\"l\", chunk_start=False, chunk_end=False)\n\n    conn.receive_data(b\"o\\r\\n\")\n    assert conn.next_event() == Data(data=b\"o\", chunk_start=False, chunk_end=True)\n\n    conn.receive_data(b\"5\\r\\nhello\")\n    assert conn.next_event() == Data(data=b\"hello\", chunk_start=True, chunk_end=True)\n\n    conn.receive_data(b\"\\r\\n\")\n    assert conn.next_event() == NEED_DATA\n\n    conn.receive_data(b\"0\\r\\n\\r\\n\")\n    assert conn.next_event() == EndOfMessage()\n\n\ndef test_client_talking_to_http10_server() -> None:\n    c = Connection(CLIENT)\n    c.send(Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"example.com\")]))\n    c.send(EndOfMessage())\n    assert c.our_state is DONE\n    # No content-length, so Http10 framing for body\n    assert receive_and_get(c, b\"HTTP/1.0 200 OK\\r\\n\\r\\n\") == [\n        Response(status_code=200, headers=[], http_version=\"1.0\", reason=b\"OK\")  # type: ignore[arg-type]\n    ]\n    assert c.our_state is MUST_CLOSE\n    assert receive_and_get(c, b\"12345\") == [Data(data=b\"12345\")]\n    assert receive_and_get(c, b\"67890\") == [Data(data=b\"67890\")]\n    assert receive_and_get(c, b\"\") == [EndOfMessage(), ConnectionClosed()]\n    assert c.their_state is CLOSED\n\n\ndef test_server_talking_to_http10_client() -> None:\n    c = Connection(SERVER)\n    # No content-length, so no body\n    # NB: no host header\n    assert receive_and_get(c, b\"GET / HTTP/1.0\\r\\n\\r\\n\") == [\n        Request(method=\"GET\", target=\"/\", headers=[], http_version=\"1.0\"),  # type: ignore[arg-type]\n        EndOfMessage(),\n    ]\n    assert c.their_state is MUST_CLOSE\n\n    # We automatically Connection: close back at them\n    assert (\n        c.send(Response(status_code=200, headers=[]))  # type: ignore[arg-type]\n        == b\"HTTP/1.1 200 \\r\\nConnection: close\\r\\n\\r\\n\"\n    )\n\n    assert c.send(Data(data=b\"12345\")) == b\"12345\"\n    assert c.send(EndOfMessage()) == b\"\"\n    assert c.our_state is MUST_CLOSE\n\n    # Check that it works if they do send Content-Length\n    c = Connection(SERVER)\n    # NB: no host header\n    assert receive_and_get(c, b\"POST / HTTP/1.0\\r\\nContent-Length: 10\\r\\n\\r\\n1\") == [\n        Request(\n            method=\"POST\",\n            target=\"/\",\n            headers=[(\"Content-Length\", \"10\")],\n            http_version=\"1.0\",\n        ),\n        Data(data=b\"1\"),\n    ]\n    assert receive_and_get(c, b\"234567890\") == [Data(data=b\"234567890\"), EndOfMessage()]\n    assert c.their_state is MUST_CLOSE\n    assert receive_and_get(c, b\"\") == [ConnectionClosed()]\n\n\ndef test_automatic_transfer_encoding_in_response() -> None:\n    # Check that in responses, the user can specify either Transfer-Encoding:\n    # chunked or no framing at all, and in both cases we automatically select\n    # the right option depending on whether the peer speaks HTTP/1.0 or\n    # HTTP/1.1\n    for user_headers in [\n        [(\"Transfer-Encoding\", \"chunked\")],\n        [],\n        # In fact, this even works if Content-Length is set,\n        # because if both are set then Transfer-Encoding wins\n        [(\"Transfer-Encoding\", \"chunked\"), (\"Content-Length\", \"100\")],\n    ]:\n        user_headers = cast(List[Tuple[str, str]], user_headers)\n        p = ConnectionPair()\n        p.send(\n            CLIENT,\n            [\n                Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"example.com\")]),\n                EndOfMessage(),\n            ],\n        )\n        # When speaking to HTTP/1.1 client, all of the above cases get\n        # normalized to Transfer-Encoding: chunked\n        p.send(\n            SERVER,\n            Response(status_code=200, headers=user_headers),\n            expect=Response(\n                status_code=200, headers=[(\"Transfer-Encoding\", \"chunked\")]\n            ),\n        )\n\n        # When speaking to HTTP/1.0 client, all of the above cases get\n        # normalized to no-framing-headers\n        c = Connection(SERVER)\n        receive_and_get(c, b\"GET / HTTP/1.0\\r\\n\\r\\n\")\n        assert (\n            c.send(Response(status_code=200, headers=user_headers))\n            == b\"HTTP/1.1 200 \\r\\nConnection: close\\r\\n\\r\\n\"\n        )\n        assert c.send(Data(data=b\"12345\")) == b\"12345\"\n\n\ndef test_automagic_connection_close_handling() -> None:\n    p = ConnectionPair()\n    # If the user explicitly sets Connection: close, then we notice and\n    # respect it\n    p.send(\n        CLIENT,\n        [\n            Request(\n                method=\"GET\",\n                target=\"/\",\n                headers=[(\"Host\", \"example.com\"), (\"Connection\", \"close\")],\n            ),\n            EndOfMessage(),\n        ],\n    )\n    for conn in p.conns:\n        assert conn.states[CLIENT] is MUST_CLOSE\n    # And if the client sets it, the server automatically echoes it back\n    p.send(\n        SERVER,\n        # no header here...\n        [Response(status_code=204, headers=[]), EndOfMessage()],  # type: ignore[arg-type]\n        # ...but oh look, it arrived anyway\n        expect=[\n            Response(status_code=204, headers=[(\"connection\", \"close\")]),\n            EndOfMessage(),\n        ],\n    )\n    for conn in p.conns:\n        assert conn.states == {CLIENT: MUST_CLOSE, SERVER: MUST_CLOSE}\n\n\ndef test_100_continue() -> None:\n    def setup() -> ConnectionPair:\n        p = ConnectionPair()\n        p.send(\n            CLIENT,\n            Request(\n                method=\"GET\",\n                target=\"/\",\n                headers=[\n                    (\"Host\", \"example.com\"),\n                    (\"Content-Length\", \"100\"),\n                    (\"Expect\", \"100-continue\"),\n                ],\n            ),\n        )\n        for conn in p.conns:\n            assert conn.client_is_waiting_for_100_continue\n        assert not p.conn[CLIENT].they_are_waiting_for_100_continue\n        assert p.conn[SERVER].they_are_waiting_for_100_continue\n        return p\n\n    # Disabled by 100 Continue\n    p = setup()\n    p.send(SERVER, InformationalResponse(status_code=100, headers=[]))  # type: ignore[arg-type]\n    for conn in p.conns:\n        assert not conn.client_is_waiting_for_100_continue\n        assert not conn.they_are_waiting_for_100_continue\n\n    # Disabled by a real response\n    p = setup()\n    p.send(\n        SERVER, Response(status_code=200, headers=[(\"Transfer-Encoding\", \"chunked\")])\n    )\n    for conn in p.conns:\n        assert not conn.client_is_waiting_for_100_continue\n        assert not conn.they_are_waiting_for_100_continue\n\n    # Disabled by the client going ahead and sending stuff anyway\n    p = setup()\n    p.send(CLIENT, Data(data=b\"12345\"))\n    for conn in p.conns:\n        assert not conn.client_is_waiting_for_100_continue\n        assert not conn.they_are_waiting_for_100_continue\n\n\ndef test_max_incomplete_event_size_countermeasure() -> None:\n    # Infinitely long headers are definitely not okay\n    c = Connection(SERVER)\n    c.receive_data(b\"GET / HTTP/1.0\\r\\nEndless: \")\n    assert c.next_event() is NEED_DATA\n    with pytest.raises(RemoteProtocolError):\n        while True:\n            c.receive_data(b\"a\" * 1024)\n            c.next_event()\n\n    # Checking that the same header is accepted / rejected depending on the\n    # max_incomplete_event_size setting:\n    c = Connection(SERVER, max_incomplete_event_size=5000)\n    c.receive_data(b\"GET / HTTP/1.0\\r\\nBig: \")\n    c.receive_data(b\"a\" * 4000)\n    c.receive_data(b\"\\r\\n\\r\\n\")\n    assert get_all_events(c) == [\n        Request(\n            method=\"GET\", target=\"/\", http_version=\"1.0\", headers=[(\"big\", \"a\" * 4000)]\n        ),\n        EndOfMessage(),\n    ]\n\n    c = Connection(SERVER, max_incomplete_event_size=4000)\n    c.receive_data(b\"GET / HTTP/1.0\\r\\nBig: \")\n    c.receive_data(b\"a\" * 4000)\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n    # Temporarily exceeding the size limit is fine, as long as its done with\n    # complete events:\n    c = Connection(SERVER, max_incomplete_event_size=5000)\n    c.receive_data(b\"GET / HTTP/1.0\\r\\nContent-Length: 10000\")\n    c.receive_data(b\"\\r\\n\\r\\n\" + b\"a\" * 10000)\n    assert get_all_events(c) == [\n        Request(\n            method=\"GET\",\n            target=\"/\",\n            http_version=\"1.0\",\n            headers=[(\"Content-Length\", \"10000\")],\n        ),\n        Data(data=b\"a\" * 10000),\n        EndOfMessage(),\n    ]\n\n    c = Connection(SERVER, max_incomplete_event_size=100)\n    # Two pipelined requests to create a way-too-big receive buffer... but\n    # it's fine because we're not checking\n    c.receive_data(\n        b\"GET /1 HTTP/1.1\\r\\nHost: a\\r\\n\\r\\n\"\n        b\"GET /2 HTTP/1.1\\r\\nHost: b\\r\\n\\r\\n\" + b\"X\" * 1000\n    )\n    assert get_all_events(c) == [\n        Request(method=\"GET\", target=\"/1\", headers=[(\"host\", \"a\")]),\n        EndOfMessage(),\n    ]\n    # Even more data comes in, still no problem\n    c.receive_data(b\"X\" * 1000)\n    # We can respond and reuse to get the second pipelined request\n    c.send(Response(status_code=200, headers=[]))  # type: ignore[arg-type]\n    c.send(EndOfMessage())\n    c.start_next_cycle()\n    assert get_all_events(c) == [\n        Request(method=\"GET\", target=\"/2\", headers=[(\"host\", \"b\")]),\n        EndOfMessage(),\n    ]\n    # But once we unpause and try to read the next message, and find that it's\n    # incomplete and the buffer is *still* way too large, then *that's* a\n    # problem:\n    c.send(Response(status_code=200, headers=[]))  # type: ignore[arg-type]\n    c.send(EndOfMessage())\n    c.start_next_cycle()\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n\ndef test_reuse_simple() -> None:\n    p = ConnectionPair()\n    p.send(\n        CLIENT,\n        [Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"a\")]), EndOfMessage()],\n    )\n    p.send(\n        SERVER,\n        [\n            Response(status_code=200, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n            EndOfMessage(),\n        ],\n    )\n    for conn in p.conns:\n        assert conn.states == {CLIENT: DONE, SERVER: DONE}\n        conn.start_next_cycle()\n\n    p.send(\n        CLIENT,\n        [\n            Request(method=\"DELETE\", target=\"/foo\", headers=[(\"Host\", \"a\")]),\n            EndOfMessage(),\n        ],\n    )\n    p.send(\n        SERVER,\n        [\n            Response(status_code=404, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n            EndOfMessage(),\n        ],\n    )\n\n\ndef test_pipelining() -> None:\n    # Client doesn't support pipelining, so we have to do this by hand\n    c = Connection(SERVER)\n    assert c.next_event() is NEED_DATA\n    # 3 requests all bunched up\n    c.receive_data(\n        b\"GET /1 HTTP/1.1\\r\\nHost: a.com\\r\\nContent-Length: 5\\r\\n\\r\\n\"\n        b\"12345\"\n        b\"GET /2 HTTP/1.1\\r\\nHost: a.com\\r\\nContent-Length: 5\\r\\n\\r\\n\"\n        b\"67890\"\n        b\"GET /3 HTTP/1.1\\r\\nHost: a.com\\r\\n\\r\\n\"\n    )\n    assert get_all_events(c) == [\n        Request(\n            method=\"GET\",\n            target=\"/1\",\n            headers=[(\"Host\", \"a.com\"), (\"Content-Length\", \"5\")],\n        ),\n        Data(data=b\"12345\"),\n        EndOfMessage(),\n    ]\n    assert c.their_state is DONE\n    assert c.our_state is SEND_RESPONSE\n\n    assert c.next_event() is PAUSED\n\n    c.send(Response(status_code=200, headers=[]))  # type: ignore[arg-type]\n    c.send(EndOfMessage())\n    assert c.their_state is DONE\n    assert c.our_state is DONE\n\n    c.start_next_cycle()\n\n    assert get_all_events(c) == [\n        Request(\n            method=\"GET\",\n            target=\"/2\",\n            headers=[(\"Host\", \"a.com\"), (\"Content-Length\", \"5\")],\n        ),\n        Data(data=b\"67890\"),\n        EndOfMessage(),\n    ]\n    assert c.next_event() is PAUSED\n    c.send(Response(status_code=200, headers=[]))  # type: ignore[arg-type]\n    c.send(EndOfMessage())\n    c.start_next_cycle()\n\n    assert get_all_events(c) == [\n        Request(method=\"GET\", target=\"/3\", headers=[(\"Host\", \"a.com\")]),\n        EndOfMessage(),\n    ]\n    # Doesn't pause this time, no trailing data\n    assert c.next_event() is NEED_DATA\n    c.send(Response(status_code=200, headers=[]))  # type: ignore[arg-type]\n    c.send(EndOfMessage())\n\n    # Arrival of more data triggers pause\n    assert c.next_event() is NEED_DATA\n    c.receive_data(b\"SADF\")\n    assert c.next_event() is PAUSED\n    assert c.trailing_data == (b\"SADF\", False)\n    # If EOF arrives while paused, we don't see that either:\n    c.receive_data(b\"\")\n    assert c.trailing_data == (b\"SADF\", True)\n    assert c.next_event() is PAUSED\n    c.receive_data(b\"\")\n    assert c.next_event() is PAUSED\n    # Can't call receive_data with non-empty buf after closing it\n    with pytest.raises(RuntimeError):\n        c.receive_data(b\"FDSA\")\n\n\ndef test_protocol_switch() -> None:\n    for (req, deny, accept) in [\n        (\n            Request(\n                method=\"CONNECT\",\n                target=\"example.com:443\",\n                headers=[(\"Host\", \"foo\"), (\"Content-Length\", \"1\")],\n            ),\n            Response(status_code=404, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n            Response(status_code=200, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n        ),\n        (\n            Request(\n                method=\"GET\",\n                target=\"/\",\n                headers=[(\"Host\", \"foo\"), (\"Content-Length\", \"1\"), (\"Upgrade\", \"a, b\")],\n            ),\n            Response(status_code=200, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n            InformationalResponse(status_code=101, headers=[(\"Upgrade\", \"a\")]),\n        ),\n        (\n            Request(\n                method=\"CONNECT\",\n                target=\"example.com:443\",\n                headers=[(\"Host\", \"foo\"), (\"Content-Length\", \"1\"), (\"Upgrade\", \"a, b\")],\n            ),\n            Response(status_code=404, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n            # Accept CONNECT, not upgrade\n            Response(status_code=200, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n        ),\n        (\n            Request(\n                method=\"CONNECT\",\n                target=\"example.com:443\",\n                headers=[(\"Host\", \"foo\"), (\"Content-Length\", \"1\"), (\"Upgrade\", \"a, b\")],\n            ),\n            Response(status_code=404, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n            # Accept Upgrade, not CONNECT\n            InformationalResponse(status_code=101, headers=[(\"Upgrade\", \"b\")]),\n        ),\n    ]:\n\n        def setup() -> ConnectionPair:\n            p = ConnectionPair()\n            p.send(CLIENT, req)\n            # No switch-related state change stuff yet; the client has to\n            # finish the request before that kicks in\n            for conn in p.conns:\n                assert conn.states[CLIENT] is SEND_BODY\n            p.send(CLIENT, [Data(data=b\"1\"), EndOfMessage()])\n            for conn in p.conns:\n                assert conn.states[CLIENT] is MIGHT_SWITCH_PROTOCOL\n            assert p.conn[SERVER].next_event() is PAUSED\n            return p\n\n        # Test deny case\n        p = setup()\n        p.send(SERVER, deny)\n        for conn in p.conns:\n            assert conn.states == {CLIENT: DONE, SERVER: SEND_BODY}\n        p.send(SERVER, EndOfMessage())\n        # Check that re-use is still allowed after a denial\n        for conn in p.conns:\n            conn.start_next_cycle()\n\n        # Test accept case\n        p = setup()\n        p.send(SERVER, accept)\n        for conn in p.conns:\n            assert conn.states == {CLIENT: SWITCHED_PROTOCOL, SERVER: SWITCHED_PROTOCOL}\n            conn.receive_data(b\"123\")\n            assert conn.next_event() is PAUSED\n            conn.receive_data(b\"456\")\n            assert conn.next_event() is PAUSED\n            assert conn.trailing_data == (b\"123456\", False)\n\n        # Pausing in might-switch, then recovery\n        # (weird artificial case where the trailing data actually is valid\n        # HTTP for some reason, because this makes it easier to test the state\n        # logic)\n        p = setup()\n        sc = p.conn[SERVER]\n        sc.receive_data(b\"GET / HTTP/1.0\\r\\n\\r\\n\")\n        assert sc.next_event() is PAUSED\n        assert sc.trailing_data == (b\"GET / HTTP/1.0\\r\\n\\r\\n\", False)\n        sc.send(deny)\n        assert sc.next_event() is PAUSED\n        sc.send(EndOfMessage())\n        sc.start_next_cycle()\n        assert get_all_events(sc) == [\n            Request(method=\"GET\", target=\"/\", headers=[], http_version=\"1.0\"),  # type: ignore[arg-type]\n            EndOfMessage(),\n        ]\n\n        # When we're DONE, have no trailing data, and the connection gets\n        # closed, we report ConnectionClosed(). When we're in might-switch or\n        # switched, we don't.\n        p = setup()\n        sc = p.conn[SERVER]\n        sc.receive_data(b\"\")\n        assert sc.next_event() is PAUSED\n        assert sc.trailing_data == (b\"\", True)\n        p.send(SERVER, accept)\n        assert sc.next_event() is PAUSED\n\n        p = setup()\n        sc = p.conn[SERVER]\n        sc.receive_data(b\"\")\n        assert sc.next_event() is PAUSED\n        sc.send(deny)\n        assert sc.next_event() == ConnectionClosed()\n\n        # You can't send after switching protocols, or while waiting for a\n        # protocol switch\n        p = setup()\n        with pytest.raises(LocalProtocolError):\n            p.conn[CLIENT].send(\n                Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"a\")])\n            )\n        p = setup()\n        p.send(SERVER, accept)\n        with pytest.raises(LocalProtocolError):\n            p.conn[SERVER].send(Data(data=b\"123\"))\n\n\ndef test_close_simple() -> None:\n    # Just immediately closing a new connection without anything having\n    # happened yet.\n    for (who_shot_first, who_shot_second) in [(CLIENT, SERVER), (SERVER, CLIENT)]:\n\n        def setup() -> ConnectionPair:\n            p = ConnectionPair()\n            p.send(who_shot_first, ConnectionClosed())\n            for conn in p.conns:\n                assert conn.states == {\n                    who_shot_first: CLOSED,\n                    who_shot_second: MUST_CLOSE,\n                }\n            return p\n\n        # You can keep putting b\"\" into a closed connection, and you keep\n        # getting ConnectionClosed() out:\n        p = setup()\n        assert p.conn[who_shot_second].next_event() == ConnectionClosed()\n        assert p.conn[who_shot_second].next_event() == ConnectionClosed()\n        p.conn[who_shot_second].receive_data(b\"\")\n        assert p.conn[who_shot_second].next_event() == ConnectionClosed()\n        # Second party can close...\n        p = setup()\n        p.send(who_shot_second, ConnectionClosed())\n        for conn in p.conns:\n            assert conn.our_state is CLOSED\n            assert conn.their_state is CLOSED\n        # But trying to receive new data on a closed connection is a\n        # RuntimeError (not ProtocolError, because the problem here isn't\n        # violation of HTTP, it's violation of physics)\n        p = setup()\n        with pytest.raises(RuntimeError):\n            p.conn[who_shot_second].receive_data(b\"123\")\n        # And receiving new data on a MUST_CLOSE connection is a ProtocolError\n        p = setup()\n        p.conn[who_shot_first].receive_data(b\"GET\")\n        with pytest.raises(RemoteProtocolError):\n            p.conn[who_shot_first].next_event()\n\n\ndef test_close_different_states() -> None:\n    req = [\n        Request(method=\"GET\", target=\"/foo\", headers=[(\"Host\", \"a\")]),\n        EndOfMessage(),\n    ]\n    resp = [\n        Response(status_code=200, headers=[(b\"transfer-encoding\", b\"chunked\")]),\n        EndOfMessage(),\n    ]\n\n    # Client before request\n    p = ConnectionPair()\n    p.send(CLIENT, ConnectionClosed())\n    for conn in p.conns:\n        assert conn.states == {CLIENT: CLOSED, SERVER: MUST_CLOSE}\n\n    # Client after request\n    p = ConnectionPair()\n    p.send(CLIENT, req)\n    p.send(CLIENT, ConnectionClosed())\n    for conn in p.conns:\n        assert conn.states == {CLIENT: CLOSED, SERVER: SEND_RESPONSE}\n\n    # Server after request -> not allowed\n    p = ConnectionPair()\n    p.send(CLIENT, req)\n    with pytest.raises(LocalProtocolError):\n        p.conn[SERVER].send(ConnectionClosed())\n    p.conn[CLIENT].receive_data(b\"\")\n    with pytest.raises(RemoteProtocolError):\n        p.conn[CLIENT].next_event()\n\n    # Server after response\n    p = ConnectionPair()\n    p.send(CLIENT, req)\n    p.send(SERVER, resp)\n    p.send(SERVER, ConnectionClosed())\n    for conn in p.conns:\n        assert conn.states == {CLIENT: MUST_CLOSE, SERVER: CLOSED}\n\n    # Both after closing (ConnectionClosed() is idempotent)\n    p = ConnectionPair()\n    p.send(CLIENT, req)\n    p.send(SERVER, resp)\n    p.send(CLIENT, ConnectionClosed())\n    p.send(SERVER, ConnectionClosed())\n    p.send(CLIENT, ConnectionClosed())\n    p.send(SERVER, ConnectionClosed())\n\n    # In the middle of sending -> not allowed\n    p = ConnectionPair()\n    p.send(\n        CLIENT,\n        Request(\n            method=\"GET\", target=\"/\", headers=[(\"Host\", \"a\"), (\"Content-Length\", \"10\")]\n        ),\n    )\n    with pytest.raises(LocalProtocolError):\n        p.conn[CLIENT].send(ConnectionClosed())\n    p.conn[SERVER].receive_data(b\"\")\n    with pytest.raises(RemoteProtocolError):\n        p.conn[SERVER].next_event()\n\n\n# Receive several requests and then client shuts down their side of the\n# connection; we can respond to each\ndef test_pipelined_close() -> None:\n    c = Connection(SERVER)\n    # 2 requests then a close\n    c.receive_data(\n        b\"GET /1 HTTP/1.1\\r\\nHost: a.com\\r\\nContent-Length: 5\\r\\n\\r\\n\"\n        b\"12345\"\n        b\"GET /2 HTTP/1.1\\r\\nHost: a.com\\r\\nContent-Length: 5\\r\\n\\r\\n\"\n        b\"67890\"\n    )\n    c.receive_data(b\"\")\n    assert get_all_events(c) == [\n        Request(\n            method=\"GET\",\n            target=\"/1\",\n            headers=[(\"host\", \"a.com\"), (\"content-length\", \"5\")],\n        ),\n        Data(data=b\"12345\"),\n        EndOfMessage(),\n    ]\n    assert c.states[CLIENT] is DONE\n    c.send(Response(status_code=200, headers=[]))  # type: ignore[arg-type]\n    c.send(EndOfMessage())\n    assert c.states[SERVER] is DONE\n    c.start_next_cycle()\n    assert get_all_events(c) == [\n        Request(\n            method=\"GET\",\n            target=\"/2\",\n            headers=[(\"host\", \"a.com\"), (\"content-length\", \"5\")],\n        ),\n        Data(data=b\"67890\"),\n        EndOfMessage(),\n        ConnectionClosed(),\n    ]\n    assert c.states == {CLIENT: CLOSED, SERVER: SEND_RESPONSE}\n    c.send(Response(status_code=200, headers=[]))  # type: ignore[arg-type]\n    c.send(EndOfMessage())\n    assert c.states == {CLIENT: CLOSED, SERVER: MUST_CLOSE}\n    c.send(ConnectionClosed())\n    assert c.states == {CLIENT: CLOSED, SERVER: CLOSED}\n\n\ndef test_sendfile() -> None:\n    class SendfilePlaceholder:\n        def __len__(self) -> int:\n            return 10\n\n    placeholder = SendfilePlaceholder()\n\n    def setup(\n        header: Tuple[str, str], http_version: str\n    ) -> Tuple[Connection, Optional[List[bytes]]]:\n        c = Connection(SERVER)\n        receive_and_get(\n            c, \"GET / HTTP/{}\\r\\nHost: a\\r\\n\\r\\n\".format(http_version).encode(\"ascii\")\n        )\n        headers = []\n        if header:\n            headers.append(header)\n        c.send(Response(status_code=200, headers=headers))\n        return c, c.send_with_data_passthrough(Data(data=placeholder))  # type: ignore\n\n    c, data = setup((\"Content-Length\", \"10\"), \"1.1\")\n    assert data == [placeholder]  # type: ignore\n    # Raises an error if the connection object doesn't think we've sent\n    # exactly 10 bytes\n    c.send(EndOfMessage())\n\n    _, data = setup((\"Transfer-Encoding\", \"chunked\"), \"1.1\")\n    assert placeholder in data  # type: ignore\n    data[data.index(placeholder)] = b\"x\" * 10  # type: ignore\n    assert b\"\".join(data) == b\"a\\r\\nxxxxxxxxxx\\r\\n\"  # type: ignore\n\n    c, data = setup(None, \"1.0\")  # type: ignore\n    assert data == [placeholder]  # type: ignore\n    assert c.our_state is SEND_BODY\n\n\ndef test_errors() -> None:\n    # After a receive error, you can't receive\n    for role in [CLIENT, SERVER]:\n        c = Connection(our_role=role)\n        c.receive_data(b\"gibberish\\r\\n\\r\\n\")\n        with pytest.raises(RemoteProtocolError):\n            c.next_event()\n        # Now any attempt to receive continues to raise\n        assert c.their_state is ERROR\n        assert c.our_state is not ERROR\n        print(c._cstate.states)\n        with pytest.raises(RemoteProtocolError):\n            c.next_event()\n        # But we can still yell at the client for sending us gibberish\n        if role is SERVER:\n            assert (\n                c.send(Response(status_code=400, headers=[]))  # type: ignore[arg-type]\n                == b\"HTTP/1.1 400 \\r\\nConnection: close\\r\\n\\r\\n\"\n            )\n\n    # After an error sending, you can no longer send\n    # (This is especially important for things like content-length errors,\n    # where there's complex internal state being modified)\n    def conn(role: Type[Sentinel]) -> Connection:\n        c = Connection(our_role=role)\n        if role is SERVER:\n            # Put it into the state where it *could* send a response...\n            receive_and_get(c, b\"GET / HTTP/1.0\\r\\n\\r\\n\")\n            assert c.our_state is SEND_RESPONSE\n        return c\n\n    for role in [CLIENT, SERVER]:\n        if role is CLIENT:\n            # This HTTP/1.0 request won't be detected as bad until after we go\n            # through the state machine and hit the writing code\n            good = Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"example.com\")])\n            bad = Request(\n                method=\"GET\",\n                target=\"/\",\n                headers=[(\"Host\", \"example.com\")],\n                http_version=\"1.0\",\n            )\n        elif role is SERVER:\n            good = Response(status_code=200, headers=[])  # type: ignore[arg-type,assignment]\n            bad = Response(status_code=200, headers=[], http_version=\"1.0\")  # type: ignore[arg-type,assignment]\n        # Make sure 'good' actually is good\n        c = conn(role)\n        c.send(good)\n        assert c.our_state is not ERROR\n        # Do that again, but this time sending 'bad' first\n        c = conn(role)\n        with pytest.raises(LocalProtocolError):\n            c.send(bad)\n        assert c.our_state is ERROR\n        assert c.their_state is not ERROR\n        # Now 'good' is not so good\n        with pytest.raises(LocalProtocolError):\n            c.send(good)\n\n        # And check send_failed() too\n        c = conn(role)\n        c.send_failed()\n        assert c.our_state is ERROR\n        assert c.their_state is not ERROR\n        # This is idempotent\n        c.send_failed()\n        assert c.our_state is ERROR\n        assert c.their_state is not ERROR\n\n\ndef test_idle_receive_nothing() -> None:\n    # At one point this incorrectly raised an error\n    for role in [CLIENT, SERVER]:\n        c = Connection(role)\n        assert c.next_event() is NEED_DATA\n\n\ndef test_connection_drop() -> None:\n    c = Connection(SERVER)\n    c.receive_data(b\"GET /\")\n    assert c.next_event() is NEED_DATA\n    c.receive_data(b\"\")\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n\ndef test_408_request_timeout() -> None:\n    # Should be able to send this spontaneously as a server without seeing\n    # anything from client\n    p = ConnectionPair()\n    p.send(SERVER, Response(status_code=408, headers=[(b\"connection\", b\"close\")]))\n\n\n# This used to raise IndexError\ndef test_empty_request() -> None:\n    c = Connection(SERVER)\n    c.receive_data(b\"\\r\\n\")\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n\n# This used to raise IndexError\ndef test_empty_response() -> None:\n    c = Connection(CLIENT)\n    c.send(Request(method=\"GET\", target=\"/\", headers=[(\"Host\", \"a\")]))\n    c.receive_data(b\"\\r\\n\")\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n\n@pytest.mark.parametrize(\n    \"data\",\n    [\n        b\"\\x00\",\n        b\"\\x20\",\n        b\"\\x16\\x03\\x01\\x00\\xa5\",  # Typical start of a TLS Client Hello\n    ],\n)\ndef test_early_detection_of_invalid_request(data: bytes) -> None:\n    c = Connection(SERVER)\n    # Early detection should occur before even receiving a `\\r\\n`\n    c.receive_data(data)\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n\n@pytest.mark.parametrize(\n    \"data\",\n    [\n        b\"\\x00\",\n        b\"\\x20\",\n        b\"\\x16\\x03\\x03\\x00\\x31\",  # Typical start of a TLS Server Hello\n    ],\n)\ndef test_early_detection_of_invalid_response(data: bytes) -> None:\n    c = Connection(CLIENT)\n    # Early detection should occur before even receiving a `\\r\\n`\n    c.receive_data(data)\n    with pytest.raises(RemoteProtocolError):\n        c.next_event()\n\n\n# This used to give different headers for HEAD and GET.\n# The correct way to handle HEAD is to put whatever headers we *would* have\n# put if it were a GET -- even though we know that for HEAD, those headers\n# will be ignored.\ndef test_HEAD_framing_headers() -> None:\n    def setup(method: bytes, http_version: bytes) -> Connection:\n        c = Connection(SERVER)\n        c.receive_data(\n            method + b\" / HTTP/\" + http_version + b\"\\r\\n\" + b\"Host: example.com\\r\\n\\r\\n\"\n        )\n        assert type(c.next_event()) is Request\n        assert type(c.next_event()) is EndOfMessage\n        return c\n\n    for method in [b\"GET\", b\"HEAD\"]:\n        # No Content-Length, HTTP/1.1 peer, should use chunked\n        c = setup(method, b\"1.1\")\n        assert (\n            c.send(Response(status_code=200, headers=[])) == b\"HTTP/1.1 200 \\r\\n\"  # type: ignore[arg-type]\n            b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n        )\n\n        # No Content-Length, HTTP/1.0 peer, frame with connection: close\n        c = setup(method, b\"1.0\")\n        assert (\n            c.send(Response(status_code=200, headers=[])) == b\"HTTP/1.1 200 \\r\\n\"  # type: ignore[arg-type]\n            b\"Connection: close\\r\\n\\r\\n\"\n        )\n\n        # Content-Length + Transfer-Encoding, TE wins\n        c = setup(method, b\"1.1\")\n        assert (\n            c.send(\n                Response(\n                    status_code=200,\n                    headers=[\n                        (\"Content-Length\", \"100\"),\n                        (\"Transfer-Encoding\", \"chunked\"),\n                    ],\n                )\n            )\n            == b\"HTTP/1.1 200 \\r\\n\"\n            b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n        )\n\n\ndef test_special_exceptions_for_lost_connection_in_message_body() -> None:\n    c = Connection(SERVER)\n    c.receive_data(\n        b\"POST / HTTP/1.1\\r\\n\" b\"Host: example.com\\r\\n\" b\"Content-Length: 100\\r\\n\\r\\n\"\n    )\n    assert type(c.next_event()) is Request\n    assert c.next_event() is NEED_DATA\n    c.receive_data(b\"12345\")\n    assert c.next_event() == Data(data=b\"12345\")\n    c.receive_data(b\"\")\n    with pytest.raises(RemoteProtocolError) as excinfo:\n        c.next_event()\n    assert \"received 5 bytes\" in str(excinfo.value)\n    assert \"expected 100\" in str(excinfo.value)\n\n    c = Connection(SERVER)\n    c.receive_data(\n        b\"POST / HTTP/1.1\\r\\n\"\n        b\"Host: example.com\\r\\n\"\n        b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n    )\n    assert type(c.next_event()) is Request\n    assert c.next_event() is NEED_DATA\n    c.receive_data(b\"8\\r\\n012345\")\n    assert c.next_event().data == b\"012345\"  # type: ignore\n    c.receive_data(b\"\")\n    with pytest.raises(RemoteProtocolError) as excinfo:\n        c.next_event()\n    assert \"incomplete chunked read\" in str(excinfo.value)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/h11/tests/test_helpers.py", "content": "from .._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    Event,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom .helpers import normalize_data_events\n\n\ndef test_normalize_data_events() -> None:\n    assert normalize_data_events(\n        [\n            Data(data=bytearray(b\"1\")),\n            Data(data=b\"2\"),\n            Response(status_code=200, headers=[]),  # type: ignore[arg-type]\n            Data(data=b\"3\"),\n            Data(data=b\"4\"),\n            EndOfMessage(),\n            Data(data=b\"5\"),\n            Data(data=b\"6\"),\n            Data(data=b\"7\"),\n        ]\n    ) == [\n        Data(data=b\"12\"),\n        Response(status_code=200, headers=[]),  # type: ignore[arg-type]\n        Data(data=b\"34\"),\n        EndOfMessage(),\n        Data(data=b\"567\"),\n    ]\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/passlib/tests/test_apache.py", "content": "\"\"\"tests for passlib.apache -- (c) Assurance Technologies 2008-2011\"\"\"\n#=============================================================================\n# imports\n#=============================================================================\nfrom __future__ import with_statement\n# core\nfrom logging import getLogger\nimport os\nimport subprocess\n# site\n# pkg\nfrom passlib import apache, registry\nfrom passlib.exc import MissingBackendError\nfrom passlib.utils.compat import irange\nfrom passlib.tests.backports import unittest\nfrom passlib.tests.utils import TestCase, get_file, set_file, ensure_mtime_changed\nfrom passlib.utils.compat import u\nfrom passlib.utils import to_bytes\nfrom passlib.utils.handlers import to_unicode_for_identify\n# module\nlog = getLogger(__name__)\n\n#=============================================================================\n# helpers\n#=============================================================================\n\ndef backdate_file_mtime(path, offset=10):\n    \"\"\"backdate file's mtime by specified amount\"\"\"\n    # NOTE: this is used so we can test code which detects mtime changes,\n    #       without having to actually *pause* for that long.\n    atime = os.path.getatime(path)\n    mtime = os.path.getmtime(path)-offset\n    os.utime(path, (atime, mtime))\n\n#=============================================================================\n# detect external HTPASSWD tool\n#=============================================================================\n\n\nhtpasswd_path = os.environ.get(\"PASSLIB_TEST_HTPASSWD_PATH\") or \"htpasswd\"\n\n\ndef _call_htpasswd(args, stdin=None):\n    \"\"\"\n    helper to run htpasswd cmd\n    \"\"\"\n    if stdin is not None:\n        stdin = stdin.encode(\"utf-8\")\n    proc = subprocess.Popen([htpasswd_path] + args, stdout=subprocess.PIPE,\n                            stderr=subprocess.STDOUT, stdin=subprocess.PIPE if stdin else None)\n    out, err = proc.communicate(stdin)\n    rc = proc.wait()\n    out = to_unicode_for_identify(out or \"\")\n    return out, rc\n\n\ndef _call_htpasswd_verify(path, user, password):\n    \"\"\"\n    wrapper for htpasswd verify\n    \"\"\"\n    out, rc = _call_htpasswd([\"-vi\", path, user], password)\n    return not rc\n\n\ndef _detect_htpasswd():\n    \"\"\"\n    helper to check if htpasswd is present\n    \"\"\"\n    try:\n        out, rc = _call_htpasswd([])\n    except OSError:\n        # TODO: under py3, could trap the more specific FileNotFoundError\n        # cmd not found\n        return False, False\n    # when called w/o args, it should print usage to stderr & return rc=2\n    if not rc:\n        log.warning(\"htpasswd test returned with rc=0\")\n    have_bcrypt = \" -B \" in out\n    return True, have_bcrypt\n\n\nHAVE_HTPASSWD, HAVE_HTPASSWD_BCRYPT = _detect_htpasswd()\n\nrequires_htpasswd_cmd = unittest.skipUnless(HAVE_HTPASSWD, \"requires `htpasswd` cmdline tool\")\n\n\n#=============================================================================\n# htpasswd\n#=============================================================================\nclass HtpasswdFileTest(TestCase):\n    \"\"\"test HtpasswdFile class\"\"\"\n    descriptionPrefix = \"HtpasswdFile\"\n\n    # sample with 4 users\n    sample_01 = (b'user2:2CHkkwa2AtqGs\\n'\n                 b'user3:{SHA}3ipNV1GrBtxPmHFC21fCbVCSXIo=\\n'\n                 b'user4:pass4\\n'\n                 b'user1:$apr1$t4tc7jTh$GPIWVUo8sQKJlUdV8V5vu0\\n')\n\n    # sample 1 with user 1, 2 deleted; 4 changed\n    sample_02 = b'user3:{SHA}3ipNV1GrBtxPmHFC21fCbVCSXIo=\\nuser4:pass4\\n'\n\n    # sample 1 with user2 updated, user 1 first entry removed, and user 5 added\n    sample_03 = (b'user2:pass2x\\n'\n                 b'user3:{SHA}3ipNV1GrBtxPmHFC21fCbVCSXIo=\\n'\n                 b'user4:pass4\\n'\n                 b'user1:$apr1$t4tc7jTh$GPIWVUo8sQKJlUdV8V5vu0\\n'\n                 b'user5:pass5\\n')\n\n    # standalone sample with 8-bit username\n    sample_04_utf8 = b'user\\xc3\\xa6:2CHkkwa2AtqGs\\n'\n    sample_04_latin1 = b'user\\xe6:2CHkkwa2AtqGs\\n'\n\n    sample_dup = b'user1:pass1\\nuser1:pass2\\n'\n\n    # sample with bcrypt & sha256_crypt hashes\n    sample_05 = (b'user2:2CHkkwa2AtqGs\\n'\n                 b'user3:{SHA}3ipNV1GrBtxPmHFC21fCbVCSXIo=\\n'\n                 b'user4:pass4\\n'\n                 b'user1:$apr1$t4tc7jTh$GPIWVUo8sQKJlUdV8V5vu0\\n'\n                 b'user5:$2a$12$yktDxraxijBZ360orOyCOePFGhuis/umyPNJoL5EbsLk.s6SWdrRO\\n'\n                 b'user6:$5$rounds=110000$cCRp/xUUGVgwR4aP$'\n                     b'p0.QKFS5qLNRqw1/47lXYiAcgIjJK.WjCO8nrEKuUK.\\n')\n\n    def test_00_constructor_autoload(self):\n        \"\"\"test constructor autoload\"\"\"\n        # check with existing file\n        path = self.mktemp()\n        set_file(path, self.sample_01)\n        ht = apache.HtpasswdFile(path)\n        self.assertEqual(ht.to_string(), self.sample_01)\n        self.assertEqual(ht.path, path)\n        self.assertTrue(ht.mtime)\n\n        # check changing path\n        ht.path = path + \"x\"\n        self.assertEqual(ht.path, path + \"x\")\n        self.assertFalse(ht.mtime)\n\n        # check new=True\n        ht = apache.HtpasswdFile(path, new=True)\n        self.assertEqual(ht.to_string(), b\"\")\n        self.assertEqual(ht.path, path)\n        self.assertFalse(ht.mtime)\n\n        # check autoload=False (deprecated alias for new=True)\n        with self.assertWarningList(\"``autoload=False`` is deprecated\"):\n            ht = apache.HtpasswdFile(path, autoload=False)\n        self.assertEqual(ht.to_string(), b\"\")\n        self.assertEqual(ht.path, path)\n        self.assertFalse(ht.mtime)\n\n        # check missing file\n        os.remove(path)\n        self.assertRaises(IOError, apache.HtpasswdFile, path)\n\n        # NOTE: \"default_scheme\" option checked via set_password() test, among others\n\n    def test_00_from_path(self):\n        path = self.mktemp()\n        set_file(path, self.sample_01)\n        ht = apache.HtpasswdFile.from_path(path)\n        self.assertEqual(ht.to_string(), self.sample_01)\n        self.assertEqual(ht.path, None)\n        self.assertFalse(ht.mtime)\n\n    def test_01_delete(self):\n        \"\"\"test delete()\"\"\"\n        ht = apache.HtpasswdFile.from_string(self.sample_01)\n        self.assertTrue(ht.delete(\"user1\")) # should delete both entries\n        self.assertTrue(ht.delete(\"user2\"))\n        self.assertFalse(ht.delete(\"user5\")) # user not present\n        self.assertEqual(ht.to_string(), self.sample_02)\n\n        # invalid user\n        self.assertRaises(ValueError, ht.delete, \"user:\")\n\n    def test_01_delete_autosave(self):\n        path = self.mktemp()\n        sample = b'user1:pass1\\nuser2:pass2\\n'\n        set_file(path, sample)\n\n        ht = apache.HtpasswdFile(path)\n        ht.delete(\"user1\")\n        self.assertEqual(get_file(path), sample)\n\n        ht = apache.HtpasswdFile(path, autosave=True)\n        ht.delete(\"user1\")\n        self.assertEqual(get_file(path), b\"user2:pass2\\n\")\n\n    def test_02_set_password(self):\n        \"\"\"test set_password()\"\"\"\n        ht = apache.HtpasswdFile.from_string(\n            self.sample_01, default_scheme=\"plaintext\")\n        self.assertTrue(ht.set_password(\"user2\", \"pass2x\"))\n        self.assertFalse(ht.set_password(\"user5\", \"pass5\"))\n        self.assertEqual(ht.to_string(), self.sample_03)\n\n        # test legacy default kwd\n        with self.assertWarningList(\"``default`` is deprecated\"):\n            ht = apache.HtpasswdFile.from_string(self.sample_01, default=\"plaintext\")\n        self.assertTrue(ht.set_password(\"user2\", \"pass2x\"))\n        self.assertFalse(ht.set_password(\"user5\", \"pass5\"))\n        self.assertEqual(ht.to_string(), self.sample_03)\n\n        # invalid user\n        self.assertRaises(ValueError, ht.set_password, \"user:\", \"pass\")\n\n        # test that legacy update() still works\n        with self.assertWarningList(\"update\\(\\) is deprecated\"):\n            ht.update(\"user2\", \"test\")\n        self.assertTrue(ht.check_password(\"user2\", \"test\"))\n\n    def test_02_set_password_autosave(self):\n        path = self.mktemp()\n        sample = b'user1:pass1\\n'\n        set_file(path, sample)\n\n        ht = apache.HtpasswdFile(path)\n        ht.set_password(\"user1\", \"pass2\")\n        self.assertEqual(get_file(path), sample)\n\n        ht = apache.HtpasswdFile(path, default_scheme=\"plaintext\", autosave=True)\n        ht.set_password(\"user1\", \"pass2\")\n        self.assertEqual(get_file(path), b\"user1:pass2\\n\")\n\n    def test_02_set_password_default_scheme(self):\n        \"\"\"test set_password() -- default_scheme\"\"\"\n\n        def check(scheme):\n            ht = apache.HtpasswdFile(default_scheme=scheme)\n            ht.set_password(\"user1\", \"pass1\")\n            return ht.context.identify(ht.get_hash(\"user1\"))\n\n        # explicit scheme\n        self.assertEqual(check(\"sha256_crypt\"), \"sha256_crypt\")\n        self.assertEqual(check(\"des_crypt\"), \"des_crypt\")\n\n        # unknown scheme\n        self.assertRaises(KeyError, check, \"xxx\")\n\n        # alias resolution\n        self.assertEqual(check(\"portable\"), apache.htpasswd_defaults[\"portable\"])\n        self.assertEqual(check(\"portable_apache_22\"), apache.htpasswd_defaults[\"portable_apache_22\"])\n        self.assertEqual(check(\"host_apache_22\"), apache.htpasswd_defaults[\"host_apache_22\"])\n\n        # default\n        self.assertEqual(check(None), apache.htpasswd_defaults[\"portable_apache_22\"])\n\n    def test_03_users(self):\n        \"\"\"test users()\"\"\"\n        ht = apache.HtpasswdFile.from_string(self.sample_01)\n        ht.set_password(\"user5\", \"pass5\")\n        ht.delete(\"user3\")\n        ht.set_password(\"user3\", \"pass3\")\n        self.assertEqual(sorted(ht.users()), [\"user1\", \"user2\", \"user3\", \"user4\", \"user5\"])\n\n    def test_04_check_password(self):\n        \"\"\"test check_password()\"\"\"\n        ht = apache.HtpasswdFile.from_string(self.sample_05)\n        self.assertRaises(TypeError, ht.check_password, 1, 'pass9')\n        self.assertTrue(ht.check_password(\"user9\",\"pass9\") is None)\n\n        # users 1..6 of sample_01 run through all the main hash formats,\n        # to make sure they're recognized.\n        for i in irange(1, 7):\n            i = str(i)\n            try:\n                self.assertTrue(ht.check_password(\"user\"+i, \"pass\"+i))\n                self.assertTrue(ht.check_password(\"user\"+i, \"pass9\") is False)\n            except MissingBackendError:\n                if i == \"5\":\n                    # user5 uses bcrypt, which is apparently not available right now\n                    continue\n                raise\n\n        self.assertRaises(ValueError, ht.check_password, \"user:\", \"pass\")\n\n        # test that legacy verify() still works\n        with self.assertWarningList([\"verify\\(\\) is deprecated\"]*2):\n            self.assertTrue(ht.verify(\"user1\", \"pass1\"))\n            self.assertFalse(ht.verify(\"user1\", \"pass2\"))\n\n    def test_05_load(self):\n        \"\"\"test load()\"\"\"\n        # setup empty file\n        path = self.mktemp()\n        set_file(path, \"\")\n        backdate_file_mtime(path, 5)\n        ha = apache.HtpasswdFile(path, default_scheme=\"plaintext\")\n        self.assertEqual(ha.to_string(), b\"\")\n\n        # make changes, check load_if_changed() does nothing\n        ha.set_password(\"user1\", \"pass1\")\n        ha.load_if_changed()\n        self.assertEqual(ha.to_string(), b\"user1:pass1\\n\")\n\n        # change file\n        set_file(path, self.sample_01)\n        ha.load_if_changed()\n        self.assertEqual(ha.to_string(), self.sample_01)\n\n        # make changes, check load() overwrites them\n        ha.set_password(\"user5\", \"pass5\")\n        ha.load()\n        self.assertEqual(ha.to_string(), self.sample_01)\n\n        # test load w/ no path\n        hb = apache.HtpasswdFile()\n        self.assertRaises(RuntimeError, hb.load)\n        self.assertRaises(RuntimeError, hb.load_if_changed)\n\n        # test load w/ dups and explicit path\n        set_file(path, self.sample_dup)\n        hc = apache.HtpasswdFile()\n        hc.load(path)\n        self.assertTrue(hc.check_password('user1','pass1'))\n\n    # NOTE: load_string() tested via from_string(), which is used all over this file\n\n    def test_06_save(self):\n        \"\"\"test save()\"\"\"\n        # load from file\n        path = self.mktemp()\n        set_file(path, self.sample_01)\n        ht = apache.HtpasswdFile(path)\n\n        # make changes, check they saved\n        ht.delete(\"user1\")\n        ht.delete(\"user2\")\n        ht.save()\n        self.assertEqual(get_file(path), self.sample_02)\n\n        # test save w/ no path\n        hb = apache.HtpasswdFile(default_scheme=\"plaintext\")\n        hb.set_password(\"user1\", \"pass1\")\n        self.assertRaises(RuntimeError, hb.save)\n\n        # test save w/ explicit path\n        hb.save(path)\n        self.assertEqual(get_file(path), b\"user1:pass1\\n\")\n\n    def test_07_encodings(self):\n        \"\"\"test 'encoding' kwd\"\"\"\n        # test bad encodings cause failure in constructor\n        self.assertRaises(ValueError, apache.HtpasswdFile, encoding=\"utf-16\")\n\n        # check sample utf-8\n        ht = apache.HtpasswdFile.from_string(self.sample_04_utf8, encoding=\"utf-8\",\n                                             return_unicode=True)\n        self.assertEqual(ht.users(), [ u(\"user\\u00e6\") ])\n\n        # test deprecated encoding=None\n        with self.assertWarningList(\"``encoding=None`` is deprecated\"):\n            ht = apache.HtpasswdFile.from_string(self.sample_04_utf8, encoding=None)\n        self.assertEqual(ht.users(), [ b'user\\xc3\\xa6' ])\n\n        # check sample latin-1\n        ht = apache.HtpasswdFile.from_string(self.sample_04_latin1,\n                                              encoding=\"latin-1\", return_unicode=True)\n        self.assertEqual(ht.users(), [ u(\"user\\u00e6\") ])\n\n    def test_08_get_hash(self):\n        \"\"\"test get_hash()\"\"\"\n        ht = apache.HtpasswdFile.from_string(self.sample_01)\n        self.assertEqual(ht.get_hash(\"user3\"), b\"{SHA}3ipNV1GrBtxPmHFC21fCbVCSXIo=\")\n        self.assertEqual(ht.get_hash(\"user4\"), b\"pass4\")\n        self.assertEqual(ht.get_hash(\"user5\"), None)\n\n        with self.assertWarningList(\"find\\(\\) is deprecated\"):\n            self.assertEqual(ht.find(\"user4\"), b\"pass4\")\n\n    def test_09_to_string(self):\n        \"\"\"test to_string\"\"\"\n\n        # check with known sample\n        ht = apache.HtpasswdFile.from_string(self.sample_01)\n        self.assertEqual(ht.to_string(), self.sample_01)\n\n        # test blank\n        ht = apache.HtpasswdFile()\n        self.assertEqual(ht.to_string(), b\"\")\n\n    def test_10_repr(self):\n        ht = apache.HtpasswdFile(\"fakepath\", autosave=True, new=True, encoding=\"latin-1\")\n        repr(ht)\n\n    def test_11_malformed(self):\n        self.assertRaises(ValueError, apache.HtpasswdFile.from_string,\n            b'realm:user1:pass1\\n')\n        self.assertRaises(ValueError, apache.HtpasswdFile.from_string,\n            b'pass1\\n')\n\n    def test_12_from_string(self):\n        # forbid path kwd\n        self.assertRaises(TypeError, apache.HtpasswdFile.from_string,\n                          b'', path=None)\n\n    def test_13_whitespace(self):\n        \"\"\"whitespace & comment handling\"\"\"\n\n        # per htpasswd source (https://github.com/apache/httpd/blob/trunk/support/htpasswd.c),\n        # lines that match \"^\\s*(#.*)?$\" should be ignored\n        source = to_bytes(\n            '\\n'\n            'user2:pass2\\n'\n            'user4:pass4\\n'\n            'user7:pass7\\r\\n'\n            ' \\t \\n'\n            'user1:pass1\\n'\n            ' # legacy users\\n'\n            '#user6:pass6\\n'\n            'user5:pass5\\n\\n'\n        )\n\n        # loading should see all users (except user6, who was commented out)\n        ht = apache.HtpasswdFile.from_string(source)\n        self.assertEqual(sorted(ht.users()), [\"user1\", \"user2\", \"user4\", \"user5\", \"user7\"])\n\n        # update existing user\n        ht.set_hash(\"user4\", \"althash4\")\n        self.assertEqual(sorted(ht.users()), [\"user1\", \"user2\", \"user4\", \"user5\", \"user7\"])\n\n        # add a new user\n        ht.set_hash(\"user6\", \"althash6\")\n        self.assertEqual(sorted(ht.users()), [\"user1\", \"user2\", \"user4\", \"user5\", \"user6\", \"user7\"])\n\n        # delete existing user\n        ht.delete(\"user7\")\n        self.assertEqual(sorted(ht.users()), [\"user1\", \"user2\", \"user4\", \"user5\", \"user6\"])\n\n        # re-serialization should preserve whitespace\n        target = to_bytes(\n            '\\n'\n            'user2:pass2\\n'\n            'user4:althash4\\n'\n            ' \\t \\n'\n            'user1:pass1\\n'\n            ' # legacy users\\n'\n            '#user6:pass6\\n'\n            'user5:pass5\\n'\n            'user6:althash6\\n'\n        )\n        self.assertEqual(ht.to_string(), target)\n\n    @requires_htpasswd_cmd\n    def test_htpasswd_cmd_verify(self):\n        \"\"\"\n        verify \"htpasswd\" command can read output\n        \"\"\"\n        path = self.mktemp()\n        ht = apache.HtpasswdFile(path=path, new=True)\n\n        def hash_scheme(pwd, scheme):\n            return ht.context.handler(scheme).hash(pwd)\n\n        # base scheme\n        ht.set_hash(\"user1\", hash_scheme(\"password\",\"apr_md5_crypt\"))\n\n        # 2.2-compat scheme\n        host_no_bcrypt = apache.htpasswd_defaults[\"host_apache_22\"]\n        ht.set_hash(\"user2\", hash_scheme(\"password\", host_no_bcrypt))\n\n        # 2.4-compat scheme\n        host_best = apache.htpasswd_defaults[\"host\"]\n        ht.set_hash(\"user3\", hash_scheme(\"password\", host_best))\n\n        # unsupported scheme -- should always fail to verify\n        ht.set_hash(\"user4\", \"$xxx$foo$bar$baz\")\n\n        # make sure htpasswd properly recognizes hashes\n        ht.save()\n\n        self.assertFalse(_call_htpasswd_verify(path, \"user1\", \"wrong\"))\n        self.assertFalse(_call_htpasswd_verify(path, \"user2\", \"wrong\"))\n        self.assertFalse(_call_htpasswd_verify(path, \"user3\", \"wrong\"))\n        self.assertFalse(_call_htpasswd_verify(path, \"user4\", \"wrong\"))\n\n        self.assertTrue(_call_htpasswd_verify(path, \"user1\", \"password\"))\n        self.assertTrue(_call_htpasswd_verify(path, \"user2\", \"password\"))\n        self.assertTrue(_call_htpasswd_verify(path, \"user3\", \"password\"))\n\n    @requires_htpasswd_cmd\n    @unittest.skipUnless(registry.has_backend(\"bcrypt\"), \"bcrypt support required\")\n    def test_htpasswd_cmd_verify_bcrypt(self):\n        \"\"\"\n        verify \"htpasswd\" command can read bcrypt format\n\n        this tests for regression of issue 95, where we output \"$2b$\" instead of \"$2y$\";\n        fixed in v1.7.2.\n        \"\"\"\n        path = self.mktemp()\n        ht = apache.HtpasswdFile(path=path, new=True)\n        def hash_scheme(pwd, scheme):\n            return ht.context.handler(scheme).hash(pwd)\n        ht.set_hash(\"user1\", hash_scheme(\"password\", \"bcrypt\"))\n        ht.save()\n        self.assertFalse(_call_htpasswd_verify(path, \"user1\", \"wrong\"))\n        if HAVE_HTPASSWD_BCRYPT:\n            self.assertTrue(_call_htpasswd_verify(path, \"user1\", \"password\"))\n        else:\n            # apache2.2 should fail, acting like it's an unknown hash format\n            self.assertFalse(_call_htpasswd_verify(path, \"user1\", \"password\"))\n\n    #===================================================================\n    # eoc\n    #===================================================================\n\n#=============================================================================\n# htdigest\n#=============================================================================\nclass HtdigestFileTest(TestCase):\n    \"\"\"test HtdigestFile class\"\"\"\n    descriptionPrefix = \"HtdigestFile\"\n\n    # sample with 4 users\n    sample_01 = (b'user2:realm:549d2a5f4659ab39a80dac99e159ab19\\n'\n                 b'user3:realm:a500bb8c02f6a9170ae46af10c898744\\n'\n                 b'user4:realm:ab7b5d5f28ccc7666315f508c7358519\\n'\n                 b'user1:realm:2a6cf53e7d8f8cf39d946dc880b14128\\n')\n\n    # sample 1 with user 1, 2 deleted; 4 changed\n    sample_02 = (b'user3:realm:a500bb8c02f6a9170ae46af10c898744\\n'\n                 b'user4:realm:ab7b5d5f28ccc7666315f508c7358519\\n')\n\n    # sample 1 with user2 updated, user 1 first entry removed, and user 5 added\n    sample_03 = (b'user2:realm:5ba6d8328943c23c64b50f8b29566059\\n'\n                 b'user3:realm:a500bb8c02f6a9170ae46af10c898744\\n'\n                 b'user4:realm:ab7b5d5f28ccc7666315f508c7358519\\n'\n                 b'user1:realm:2a6cf53e7d8f8cf39d946dc880b14128\\n'\n                 b'user5:realm:03c55fdc6bf71552356ad401bdb9af19\\n')\n\n    # standalone sample with 8-bit username & realm\n    sample_04_utf8 = b'user\\xc3\\xa6:realm\\xc3\\xa6:549d2a5f4659ab39a80dac99e159ab19\\n'\n    sample_04_latin1 = b'user\\xe6:realm\\xe6:549d2a5f4659ab39a80dac99e159ab19\\n'\n\n    def test_00_constructor_autoload(self):\n        \"\"\"test constructor autoload\"\"\"\n        # check with existing file\n        path = self.mktemp()\n        set_file(path, self.sample_01)\n        ht = apache.HtdigestFile(path)\n        self.assertEqual(ht.to_string(), self.sample_01)\n\n        # check without autoload\n        ht = apache.HtdigestFile(path, new=True)\n        self.assertEqual(ht.to_string(), b\"\")\n\n        # check missing file\n        os.remove(path)\n        self.assertRaises(IOError, apache.HtdigestFile, path)\n\n        # NOTE: default_realm option checked via other tests.\n\n    def test_01_delete(self):\n        \"\"\"test delete()\"\"\"\n        ht = apache.HtdigestFile.from_string(self.sample_01)\n        self.assertTrue(ht.delete(\"user1\", \"realm\"))\n        self.assertTrue(ht.delete(\"user2\", \"realm\"))\n        self.assertFalse(ht.delete(\"user5\", \"realm\"))\n        self.assertFalse(ht.delete(\"user3\", \"realm5\"))\n        self.assertEqual(ht.to_string(), self.sample_02)\n\n        # invalid user\n        self.assertRaises(ValueError, ht.delete, \"user:\", \"realm\")\n\n        # invalid realm\n        self.assertRaises(ValueError, ht.delete, \"user\", \"realm:\")\n\n    def test_01_delete_autosave(self):\n        path = self.mktemp()\n        set_file(path, self.sample_01)\n\n        ht = apache.HtdigestFile(path)\n        self.assertTrue(ht.delete(\"user1\", \"realm\"))\n        self.assertFalse(ht.delete(\"user3\", \"realm5\"))\n        self.assertFalse(ht.delete(\"user5\", \"realm\"))\n        self.assertEqual(get_file(path), self.sample_01)\n\n        ht.autosave = True\n        self.assertTrue(ht.delete(\"user2\", \"realm\"))\n        self.assertEqual(get_file(path), self.sample_02)\n\n    def test_02_set_password(self):\n        \"\"\"test update()\"\"\"\n        ht = apache.HtdigestFile.from_string(self.sample_01)\n        self.assertTrue(ht.set_password(\"user2\", \"realm\", \"pass2x\"))\n        self.assertFalse(ht.set_password(\"user5\", \"realm\", \"pass5\"))\n        self.assertEqual(ht.to_string(), self.sample_03)\n\n        # default realm\n        self.assertRaises(TypeError, ht.set_password, \"user2\", \"pass3\")\n        ht.default_realm = \"realm2\"\n        ht.set_password(\"user2\", \"pass3\")\n        ht.check_password(\"user2\", \"realm2\", \"pass3\")\n\n        # invalid user\n        self.assertRaises(ValueError, ht.set_password, \"user:\", \"realm\", \"pass\")\n        self.assertRaises(ValueError, ht.set_password, \"u\"*256, \"realm\", \"pass\")\n\n        # invalid realm\n        self.assertRaises(ValueError, ht.set_password, \"user\", \"realm:\", \"pass\")\n        self.assertRaises(ValueError, ht.set_password, \"user\", \"r\"*256, \"pass\")\n\n        # test that legacy update() still works\n        with self.assertWarningList(\"update\\(\\) is deprecated\"):\n            ht.update(\"user2\", \"realm2\", \"test\")\n        self.assertTrue(ht.check_password(\"user2\", \"test\"))\n\n    # TODO: test set_password autosave\n\n    def test_03_users(self):\n        \"\"\"test users()\"\"\"\n        ht = apache.HtdigestFile.from_string(self.sample_01)\n        ht.set_password(\"user5\", \"realm\", \"pass5\")\n        ht.delete(\"user3\", \"realm\")\n        ht.set_password(\"user3\", \"realm\", \"pass3\")\n        self.assertEqual(sorted(ht.users(\"realm\")), [\"user1\", \"user2\", \"user3\", \"user4\", \"user5\"])\n\n        self.assertRaises(TypeError, ht.users, 1)\n\n    def test_04_check_password(self):\n        \"\"\"test check_password()\"\"\"\n        ht = apache.HtdigestFile.from_string(self.sample_01)\n        self.assertRaises(TypeError, ht.check_password, 1, 'realm', 'pass5')\n        self.assertRaises(TypeError, ht.check_password, 'user', 1, 'pass5')\n        self.assertIs(ht.check_password(\"user5\", \"realm\",\"pass5\"), None)\n        for i in irange(1,5):\n            i = str(i)\n            self.assertTrue(ht.check_password(\"user\"+i, \"realm\", \"pass\"+i))\n            self.assertIs(ht.check_password(\"user\"+i, \"realm\", \"pass5\"), False)\n\n        # default realm\n        self.assertRaises(TypeError, ht.check_password, \"user5\", \"pass5\")\n        ht.default_realm = \"realm\"\n        self.assertTrue(ht.check_password(\"user1\", \"pass1\"))\n        self.assertIs(ht.check_password(\"user5\", \"pass5\"), None)\n\n        # test that legacy verify() still works\n        with self.assertWarningList([\"verify\\(\\) is deprecated\"]*2):\n            self.assertTrue(ht.verify(\"user1\", \"realm\", \"pass1\"))\n            self.assertFalse(ht.verify(\"user1\", \"realm\", \"pass2\"))\n\n        # invalid user\n        self.assertRaises(ValueError, ht.check_password, \"user:\", \"realm\", \"pass\")\n\n    def test_05_load(self):\n        \"\"\"test load()\"\"\"\n        # setup empty file\n        path = self.mktemp()\n        set_file(path, \"\")\n        backdate_file_mtime(path, 5)\n        ha = apache.HtdigestFile(path)\n        self.assertEqual(ha.to_string(), b\"\")\n\n        # make changes, check load_if_changed() does nothing\n        ha.set_password(\"user1\", \"realm\", \"pass1\")\n        ha.load_if_changed()\n        self.assertEqual(ha.to_string(), b'user1:realm:2a6cf53e7d8f8cf39d946dc880b14128\\n')\n\n        # change file\n        set_file(path, self.sample_01)\n        ha.load_if_changed()\n        self.assertEqual(ha.to_string(), self.sample_01)\n\n        # make changes, check load_if_changed overwrites them\n        ha.set_password(\"user5\", \"realm\", \"pass5\")\n        ha.load()\n        self.assertEqual(ha.to_string(), self.sample_01)\n\n        # test load w/ no path\n        hb = apache.HtdigestFile()\n        self.assertRaises(RuntimeError, hb.load)\n        self.assertRaises(RuntimeError, hb.load_if_changed)\n\n        # test load w/ explicit path\n        hc = apache.HtdigestFile()\n        hc.load(path)\n        self.assertEqual(hc.to_string(), self.sample_01)\n\n        # change file, test deprecated force=False kwd\n        ensure_mtime_changed(path)\n        set_file(path, \"\")\n        with self.assertWarningList(r\"load\\(force=False\\) is deprecated\"):\n            ha.load(force=False)\n        self.assertEqual(ha.to_string(), b\"\")\n\n    def test_06_save(self):\n        \"\"\"test save()\"\"\"\n        # load from file\n        path = self.mktemp()\n        set_file(path, self.sample_01)\n        ht = apache.HtdigestFile(path)\n\n        # make changes, check they saved\n        ht.delete(\"user1\", \"realm\")\n        ht.delete(\"user2\", \"realm\")\n        ht.save()\n        self.assertEqual(get_file(path), self.sample_02)\n\n        # test save w/ no path\n        hb = apache.HtdigestFile()\n        hb.set_password(\"user1\", \"realm\", \"pass1\")\n        self.assertRaises(RuntimeError, hb.save)\n\n        # test save w/ explicit path\n        hb.save(path)\n        self.assertEqual(get_file(path), hb.to_string())\n\n    def test_07_realms(self):\n        \"\"\"test realms() & delete_realm()\"\"\"\n        ht = apache.HtdigestFile.from_string(self.sample_01)\n\n        self.assertEqual(ht.delete_realm(\"x\"), 0)\n        self.assertEqual(ht.realms(), ['realm'])\n\n        self.assertEqual(ht.delete_realm(\"realm\"), 4)\n        self.assertEqual(ht.realms(), [])\n        self.assertEqual(ht.to_string(), b\"\")\n\n    def test_08_get_hash(self):\n        \"\"\"test get_hash()\"\"\"\n        ht = apache.HtdigestFile.from_string(self.sample_01)\n        self.assertEqual(ht.get_hash(\"user3\", \"realm\"), \"a500bb8c02f6a9170ae46af10c898744\")\n        self.assertEqual(ht.get_hash(\"user4\", \"realm\"), \"ab7b5d5f28ccc7666315f508c7358519\")\n        self.assertEqual(ht.get_hash(\"user5\", \"realm\"), None)\n\n        with self.assertWarningList(\"find\\(\\) is deprecated\"):\n            self.assertEqual(ht.find(\"user4\", \"realm\"), \"ab7b5d5f28ccc7666315f508c7358519\")\n\n    def test_09_encodings(self):\n        \"\"\"test encoding parameter\"\"\"\n        # test bad encodings cause failure in constructor\n        self.assertRaises(ValueError, apache.HtdigestFile, encoding=\"utf-16\")\n\n        # check sample utf-8\n        ht = apache.HtdigestFile.from_string(self.sample_04_utf8, encoding=\"utf-8\", return_unicode=True)\n        self.assertEqual(ht.realms(), [ u(\"realm\\u00e6\") ])\n        self.assertEqual(ht.users(u(\"realm\\u00e6\")), [ u(\"user\\u00e6\") ])\n\n        # check sample latin-1\n        ht = apache.HtdigestFile.from_string(self.sample_04_latin1, encoding=\"latin-1\", return_unicode=True)\n        self.assertEqual(ht.realms(), [ u(\"realm\\u00e6\") ])\n        self.assertEqual(ht.users(u(\"realm\\u00e6\")), [ u(\"user\\u00e6\") ])\n\n    def test_10_to_string(self):\n        \"\"\"test to_string()\"\"\"\n\n        # check sample\n        ht = apache.HtdigestFile.from_string(self.sample_01)\n        self.assertEqual(ht.to_string(), self.sample_01)\n\n        # check blank\n        ht = apache.HtdigestFile()\n        self.assertEqual(ht.to_string(), b\"\")\n\n    def test_11_malformed(self):\n        self.assertRaises(ValueError, apache.HtdigestFile.from_string,\n            b'realm:user1:pass1:other\\n')\n        self.assertRaises(ValueError, apache.HtdigestFile.from_string,\n            b'user1:pass1\\n')\n\n    #===================================================================\n    # eoc\n    #===================================================================\n\n#=============================================================================\n# eof\n#=============================================================================\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/test_version.py", "content": "#! /usr/bin/env python\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport unittest\n\nimport greenlet\n\nclass VersionTests(unittest.TestCase):\n    def test_version(self):\n        def find_dominating_file(name):\n            if os.path.exists(name):\n                return name\n\n            tried = []\n            here = os.path.abspath(os.path.dirname(__file__))\n            for i in range(10):\n                up = ['..'] * i\n                path = [here] + up + [name]\n                fname = os.path.join(*path)\n                fname = os.path.abspath(fname)\n                tried.append(fname)\n                if os.path.exists(fname):\n                    return fname\n            raise AssertionError(\"Could not find file \" + name + \"; checked \" + str(tried))\n\n        try:\n            setup_py = find_dominating_file('setup.py')\n        except AssertionError as e:\n            raise unittest.SkipTest(\"Unable to find setup.py; must be out of tree. \" + str(e))\n\n\n        invoke_setup = \"%s %s --version\" % (sys.executable, setup_py)\n        with os.popen(invoke_setup) as f:\n            sversion = f.read().strip()\n\n        self.assertEqual(sversion, greenlet.__version__)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/test_contextvars.py", "content": "import unittest\nimport gc\nimport sys\n\nfrom functools import partial\n\nfrom greenlet import greenlet\nfrom greenlet import getcurrent\n\n\ntry:\n    from contextvars import Context\n    from contextvars import ContextVar\n    from contextvars import copy_context\nexcept ImportError:\n    Context = ContextVar = copy_context = None\n\n# We don't support testing if greenlet's built-in context var support is disabled.\n@unittest.skipUnless(Context is not None, \"ContextVar not supported\")\nclass ContextVarsTests(unittest.TestCase):\n    def _new_ctx_run(self, *args, **kwargs):\n        return copy_context().run(*args, **kwargs)\n\n    def _increment(self, greenlet_id, ctx_var, callback, counts, expect):\n        if expect is None:\n            self.assertIsNone(ctx_var.get())\n        else:\n            self.assertEqual(ctx_var.get(), expect)\n        ctx_var.set(greenlet_id)\n        for _ in range(2):\n            counts[ctx_var.get()] += 1\n            callback()\n\n    def _test_context(self, propagate_by):\n        id_var = ContextVar(\"id\", default=None)\n        id_var.set(0)\n\n        callback = getcurrent().switch\n        counts = dict((i, 0) for i in range(5))\n\n        lets = [\n            greenlet(partial(\n                partial(\n                    copy_context().run,\n                    self._increment\n                ) if propagate_by == \"run\" else self._increment,\n                greenlet_id=i,\n                ctx_var=id_var,\n                callback=callback,\n                counts=counts,\n                expect=(\n                    i - 1 if propagate_by == \"share\" else\n                    0 if propagate_by in (\"set\", \"run\") else None\n                )\n            ))\n            for i in range(1, 5)\n        ]\n\n        for let in lets:\n            if propagate_by == \"set\":\n                let.gr_context = copy_context()\n            elif propagate_by == \"share\":\n                let.gr_context = getcurrent().gr_context\n\n        for i in range(2):\n            counts[id_var.get()] += 1\n            for let in lets:\n                let.switch()\n\n        if propagate_by == \"run\":\n            # Must leave each context.run() in reverse order of entry\n            for let in reversed(lets):\n                let.switch()\n        else:\n            # No context.run(), so fine to exit in any order.\n            for let in lets:\n                let.switch()\n\n        for let in lets:\n            self.assertTrue(let.dead)\n            # When using run(), we leave the run() as the greenlet dies,\n            # and there's no context \"underneath\". When not using run(),\n            # gr_context still reflects the context the greenlet was\n            # running in.\n            self.assertEqual(let.gr_context is None, propagate_by == \"run\")\n\n        if propagate_by == \"share\":\n            self.assertEqual(counts, {0: 1, 1: 1, 2: 1, 3: 1, 4: 6})\n        else:\n            self.assertEqual(set(counts.values()), set([2]))\n\n    def test_context_propagated_by_context_run(self):\n        self._new_ctx_run(self._test_context, \"run\")\n\n    def test_context_propagated_by_setting_attribute(self):\n        self._new_ctx_run(self._test_context, \"set\")\n\n    def test_context_not_propagated(self):\n        self._new_ctx_run(self._test_context, None)\n\n    def test_context_shared(self):\n        self._new_ctx_run(self._test_context, \"share\")\n\n    def test_break_ctxvars(self):\n        let1 = greenlet(copy_context().run)\n        let2 = greenlet(copy_context().run)\n        let1.switch(getcurrent().switch)\n        let2.switch(getcurrent().switch)\n        # Since let2 entered the current context and let1 exits its own, the\n        # interpreter emits:\n        # RuntimeError: cannot exit context: thread state references a different context object\n        let1.switch()\n\n    def test_not_broken_if_using_attribute_instead_of_context_run(self):\n        let1 = greenlet(getcurrent().switch)\n        let2 = greenlet(getcurrent().switch)\n        let1.gr_context = copy_context()\n        let2.gr_context = copy_context()\n        let1.switch()\n        let2.switch()\n        let1.switch()\n        let2.switch()\n\n    def test_context_assignment_while_running(self):\n        id_var = ContextVar(\"id\", default=None)\n\n        def target():\n            self.assertIsNone(id_var.get())\n            self.assertIsNone(gr.gr_context)\n\n            # Context is created on first use\n            id_var.set(1)\n            self.assertIsInstance(gr.gr_context, Context)\n            self.assertEqual(id_var.get(), 1)\n            self.assertEqual(gr.gr_context[id_var], 1)\n\n            # Clearing the context makes it get re-created as another\n            # empty context when next used\n            old_context = gr.gr_context\n            gr.gr_context = None  # assign None while running\n            self.assertIsNone(id_var.get())\n            self.assertIsNone(gr.gr_context)\n            id_var.set(2)\n            self.assertIsInstance(gr.gr_context, Context)\n            self.assertEqual(id_var.get(), 2)\n            self.assertEqual(gr.gr_context[id_var], 2)\n\n            new_context = gr.gr_context\n            getcurrent().parent.switch((old_context, new_context))\n            # parent switches us back to old_context\n\n            self.assertEqual(id_var.get(), 1)\n            gr.gr_context = new_context  # assign non-None while running\n            self.assertEqual(id_var.get(), 2)\n\n            getcurrent().parent.switch()\n            # parent switches us back to no context\n            self.assertIsNone(id_var.get())\n            self.assertIsNone(gr.gr_context)\n            gr.gr_context = old_context\n            self.assertEqual(id_var.get(), 1)\n\n            getcurrent().parent.switch()\n            # parent switches us back to no context\n            self.assertIsNone(id_var.get())\n            self.assertIsNone(gr.gr_context)\n\n        gr = greenlet(target)\n\n        with self.assertRaisesRegex(AttributeError, \"can't delete attr\"):\n            del gr.gr_context\n\n        self.assertIsNone(gr.gr_context)\n        old_context, new_context = gr.switch()\n        self.assertIs(new_context, gr.gr_context)\n        self.assertEqual(old_context[id_var], 1)\n        self.assertEqual(new_context[id_var], 2)\n        self.assertEqual(new_context.run(id_var.get), 2)\n        gr.gr_context = old_context  # assign non-None while suspended\n        gr.switch()\n        self.assertIs(gr.gr_context, new_context)\n        gr.gr_context = None  # assign None while suspended\n        gr.switch()\n        self.assertIs(gr.gr_context, old_context)\n        gr.gr_context = None\n        gr.switch()\n        self.assertIsNone(gr.gr_context)\n\n        # Make sure there are no reference leaks\n        gr = None\n        gc.collect()\n        self.assertEqual(sys.getrefcount(old_context), 2)\n        self.assertEqual(sys.getrefcount(new_context), 2)\n\n    def test_context_assignment_different_thread(self):\n        import threading\n\n        ctx = Context()\n        var = ContextVar(\"var\", default=None)\n        is_running = threading.Event()\n        should_suspend = threading.Event()\n        did_suspend = threading.Event()\n        should_exit = threading.Event()\n        holder = []\n\n        def greenlet_in_thread_fn():\n            var.set(1)\n            is_running.set()\n            should_suspend.wait()\n            var.set(2)\n            getcurrent().parent.switch()\n            holder.append(var.get())\n\n        def thread_fn():\n            gr = greenlet(greenlet_in_thread_fn)\n            gr.gr_context = ctx\n            holder.append(gr)\n            gr.switch()\n            did_suspend.set()\n            should_exit.wait()\n            gr.switch()\n\n        thread = threading.Thread(target=thread_fn, daemon=True)\n        thread.start()\n        is_running.wait()\n        gr = holder[0]\n\n        # Can't access or modify context if the greenlet is running\n        # in a different thread\n        with self.assertRaisesRegex(ValueError, \"running in a different\"):\n            getattr(gr, 'gr_context')\n        with self.assertRaisesRegex(ValueError, \"running in a different\"):\n            gr.gr_context = None\n\n        should_suspend.set()\n        did_suspend.wait()\n\n        # OK to access and modify context if greenlet is suspended\n        self.assertIs(gr.gr_context, ctx)\n        self.assertEqual(gr.gr_context[var], 2)\n        gr.gr_context = None\n\n        should_exit.set()\n        thread.join()\n\n        self.assertEqual(holder, [gr, None])\n\n        # Context can still be accessed/modified when greenlet is dead:\n        self.assertIsNone(gr.gr_context)\n        gr.gr_context = ctx\n        self.assertIs(gr.gr_context, ctx)\n\n@unittest.skipIf(Context is not None, \"ContextVar supported\")\nclass NoContextVarsTests(unittest.TestCase):\n    def test_contextvars_errors(self):\n        let1 = greenlet(getcurrent().switch)\n        self.assertFalse(hasattr(let1, 'gr_context'))\n        with self.assertRaises(AttributeError):\n            getattr(let1, 'gr_context')\n        with self.assertRaises(AttributeError):\n            let1.gr_context = None\n        let1.switch()\n        with self.assertRaises(AttributeError):\n            getattr(let1, 'gr_context')\n        with self.assertRaises(AttributeError):\n            let1.gr_context = None\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/multipart/tests/test_multipart.py", "content": "# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nimport glob\nimport yaml\nimport base64\nimport random\nimport tempfile\nfrom .compat import (\n    parametrize,\n    parametrize_class,\n    slow_test,\n    unittest,\n)\nfrom io import BytesIO\nfrom six import binary_type, text_type\n\nfrom mock import MagicMock, Mock, patch\n\nfrom ..multipart import *\n\n\n# Get the current directory for our later test cases.\ncurr_dir = os.path.abspath(os.path.dirname(__file__))\n\n\ndef force_bytes(val):\n    if isinstance(val, text_type):\n        val = val.encode(sys.getfilesystemencoding())\n\n    return val\n\n\nclass TestField(unittest.TestCase):\n    def setUp(self):\n        self.f = Field('foo')\n\n    def test_name(self):\n        self.assertEqual(self.f.field_name, 'foo')\n\n    def test_data(self):\n        self.f.write(b'test123')\n        self.assertEqual(self.f.value, b'test123')\n\n    def test_cache_expiration(self):\n        self.f.write(b'test')\n        self.assertEqual(self.f.value, b'test')\n        self.f.write(b'123')\n        self.assertEqual(self.f.value, b'test123')\n\n    def test_finalize(self):\n        self.f.write(b'test123')\n        self.f.finalize()\n        self.assertEqual(self.f.value, b'test123')\n\n    def test_close(self):\n        self.f.write(b'test123')\n        self.f.close()\n        self.assertEqual(self.f.value, b'test123')\n\n    def test_from_value(self):\n        f = Field.from_value(b'name', b'value')\n        self.assertEqual(f.field_name, b'name')\n        self.assertEqual(f.value, b'value')\n\n        f2 = Field.from_value(b'name', None)\n        self.assertEqual(f2.value, None)\n\n    def test_equality(self):\n        f1 = Field.from_value(b'name', b'value')\n        f2 = Field.from_value(b'name', b'value')\n\n        self.assertEqual(f1, f2)\n\n    def test_equality_with_other(self):\n        f = Field.from_value(b'foo', b'bar')\n        self.assertFalse(f == b'foo')\n        self.assertFalse(b'foo' == f)\n\n    def test_set_none(self):\n        f = Field(b'foo')\n        self.assertEqual(f.value, b'')\n\n        f.set_none()\n        self.assertEqual(f.value, None)\n\n\nclass TestFile(unittest.TestCase):\n    def setUp(self):\n        self.c = {}\n        self.d = force_bytes(tempfile.mkdtemp())\n        self.f = File(b'foo.txt', config=self.c)\n\n    def assert_data(self, data):\n        f = self.f.file_object\n        f.seek(0)\n        self.assertEqual(f.read(), data)\n        f.seek(0)\n        f.truncate()\n\n    def assert_exists(self):\n        full_path = os.path.join(self.d, self.f.actual_file_name)\n        self.assertTrue(os.path.exists(full_path))\n\n    def test_simple(self):\n        self.f.write(b'foobar')\n        self.assert_data(b'foobar')\n\n    def test_invalid_write(self):\n        m = Mock()\n        m.write.return_value = 5\n        self.f._fileobj = m\n        v = self.f.write(b'foobar')\n        self.assertEqual(v, 5)\n\n    def test_file_fallback(self):\n        self.c['MAX_MEMORY_FILE_SIZE'] = 1\n\n        self.f.write(b'1')\n        self.assertTrue(self.f.in_memory)\n        self.assert_data(b'1')\n\n        self.f.write(b'123')\n        self.assertFalse(self.f.in_memory)\n        self.assert_data(b'123')\n\n        # Test flushing too.\n        old_obj = self.f.file_object\n        self.f.flush_to_disk()\n        self.assertFalse(self.f.in_memory)\n        self.assertIs(self.f.file_object, old_obj)\n\n    def test_file_fallback_with_data(self):\n        self.c['MAX_MEMORY_FILE_SIZE'] = 10\n\n        self.f.write(b'1' * 10)\n        self.assertTrue(self.f.in_memory)\n\n        self.f.write(b'2' * 10)\n        self.assertFalse(self.f.in_memory)\n\n        self.assert_data(b'11111111112222222222')\n\n    def test_file_name(self):\n        # Write to this dir.\n        self.c['UPLOAD_DIR'] = self.d\n        self.c['MAX_MEMORY_FILE_SIZE'] = 10\n\n        # Write.\n        self.f.write(b'12345678901')\n        self.assertFalse(self.f.in_memory)\n\n        # Assert that the file exists\n        self.assertIsNotNone(self.f.actual_file_name)\n        self.assert_exists()\n\n    def test_file_full_name(self):\n        # Write to this dir.\n        self.c['UPLOAD_DIR'] = self.d\n        self.c['UPLOAD_KEEP_FILENAME'] = True\n        self.c['MAX_MEMORY_FILE_SIZE'] = 10\n\n        # Write.\n        self.f.write(b'12345678901')\n        self.assertFalse(self.f.in_memory)\n\n        # Assert that the file exists\n        self.assertEqual(self.f.actual_file_name, b'foo')\n        self.assert_exists()\n\n    def test_file_full_name_with_ext(self):\n        self.c['UPLOAD_DIR'] = self.d\n        self.c['UPLOAD_KEEP_FILENAME'] = True\n        self.c['UPLOAD_KEEP_EXTENSIONS'] = True\n        self.c['MAX_MEMORY_FILE_SIZE'] = 10\n\n        # Write.\n        self.f.write(b'12345678901')\n        self.assertFalse(self.f.in_memory)\n\n        # Assert that the file exists\n        self.assertEqual(self.f.actual_file_name, b'foo.txt')\n        self.assert_exists()\n\n    def test_file_full_name_with_ext(self):\n        self.c['UPLOAD_DIR'] = self.d\n        self.c['UPLOAD_KEEP_FILENAME'] = True\n        self.c['UPLOAD_KEEP_EXTENSIONS'] = True\n        self.c['MAX_MEMORY_FILE_SIZE'] = 10\n\n        # Write.\n        self.f.write(b'12345678901')\n        self.assertFalse(self.f.in_memory)\n\n        # Assert that the file exists\n        self.assertEqual(self.f.actual_file_name, b'foo.txt')\n        self.assert_exists()\n\n    def test_no_dir_with_extension(self):\n        self.c['UPLOAD_KEEP_EXTENSIONS'] = True\n        self.c['MAX_MEMORY_FILE_SIZE'] = 10\n\n        # Write.\n        self.f.write(b'12345678901')\n        self.assertFalse(self.f.in_memory)\n\n        # Assert that the file exists\n        ext = os.path.splitext(self.f.actual_file_name)[1]\n        self.assertEqual(ext, b'.txt')\n        self.assert_exists()\n\n    def test_invalid_dir_with_name(self):\n        # Write to this dir.\n        self.c['UPLOAD_DIR'] = force_bytes(os.path.join('/', 'tmp', 'notexisting'))\n        self.c['UPLOAD_KEEP_FILENAME'] = True\n        self.c['MAX_MEMORY_FILE_SIZE'] = 5\n\n        # Write.\n        with self.assertRaises(FileError):\n            self.f.write(b'1234567890')\n\n    def test_invalid_dir_no_name(self):\n        # Write to this dir.\n        self.c['UPLOAD_DIR'] = force_bytes(os.path.join('/', 'tmp', 'notexisting'))\n        self.c['UPLOAD_KEEP_FILENAME'] = False\n        self.c['MAX_MEMORY_FILE_SIZE'] = 5\n\n        # Write.\n        with self.assertRaises(FileError):\n            self.f.write(b'1234567890')\n\n    # TODO: test uploading two files with the same name.\n\n\nclass TestParseOptionsHeader(unittest.TestCase):\n    def test_simple(self):\n        t, p = parse_options_header('application/json')\n        self.assertEqual(t, b'application/json')\n        self.assertEqual(p, {})\n\n    def test_blank(self):\n        t, p = parse_options_header('')\n        self.assertEqual(t, b'')\n        self.assertEqual(p, {})\n\n    def test_single_param(self):\n        t, p = parse_options_header('application/json;par=val')\n        self.assertEqual(t, b'application/json')\n        self.assertEqual(p, {b'par': b'val'})\n\n    def test_single_param_with_spaces(self):\n        t, p = parse_options_header(b'application/json;     par=val')\n        self.assertEqual(t, b'application/json')\n        self.assertEqual(p, {b'par': b'val'})\n\n    def test_multiple_params(self):\n        t, p = parse_options_header(b'application/json;par=val;asdf=foo')\n        self.assertEqual(t, b'application/json')\n        self.assertEqual(p, {b'par': b'val', b'asdf': b'foo'})\n\n    def test_quoted_param(self):\n        t, p = parse_options_header(b'application/json;param=\"quoted\"')\n        self.assertEqual(t, b'application/json')\n        self.assertEqual(p, {b'param': b'quoted'})\n\n    def test_quoted_param_with_semicolon(self):\n        t, p = parse_options_header(b'application/json;param=\"quoted;with;semicolons\"')\n        self.assertEqual(p[b'param'], b'quoted;with;semicolons')\n\n    def test_quoted_param_with_escapes(self):\n        t, p = parse_options_header(b'application/json;param=\"This \\\\\" is \\\\\" a \\\\\" quote\"')\n        self.assertEqual(p[b'param'], b'This \" is \" a \" quote')\n\n    def test_handles_ie6_bug(self):\n        t, p = parse_options_header(b'text/plain; filename=\"C:\\\\this\\\\is\\\\a\\\\path\\\\file.txt\"')\n\n        self.assertEqual(p[b'filename'], b'file.txt')\n\n\nclass TestBaseParser(unittest.TestCase):\n    def setUp(self):\n        self.b = BaseParser()\n        self.b.callbacks = {}\n\n    def test_callbacks(self):\n        # The stupid list-ness is to get around lack of nonlocal on py2\n        l = [0]\n        def on_foo():\n            l[0] += 1\n\n        self.b.set_callback('foo', on_foo)\n        self.b.callback('foo')\n        self.assertEqual(l[0], 1)\n\n        self.b.set_callback('foo', None)\n        self.b.callback('foo')\n        self.assertEqual(l[0], 1)\n\n\nclass TestQuerystringParser(unittest.TestCase):\n    def assert_fields(self, *args, **kwargs):\n        if kwargs.pop('finalize', True):\n            self.p.finalize()\n\n        self.assertEqual(self.f, list(args))\n        if kwargs.get('reset', True):\n            self.f = []\n\n    def setUp(self):\n        self.reset()\n\n    def reset(self):\n        self.f = []\n\n        name_buffer = []\n        data_buffer = []\n\n        def on_field_name(data, start, end):\n            name_buffer.append(data[start:end])\n\n        def on_field_data(data, start, end):\n            data_buffer.append(data[start:end])\n\n        def on_field_end():\n            self.f.append((\n                b''.join(name_buffer),\n                b''.join(data_buffer)\n            ))\n\n            del name_buffer[:]\n            del data_buffer[:]\n\n        callbacks = {\n            'on_field_name': on_field_name,\n            'on_field_data': on_field_data,\n            'on_field_end': on_field_end\n        }\n\n        self.p = QuerystringParser(callbacks)\n\n    def test_simple_querystring(self):\n        self.p.write(b'foo=bar')\n\n        self.assert_fields((b'foo', b'bar'))\n\n    def test_querystring_blank_beginning(self):\n        self.p.write(b'&foo=bar')\n\n        self.assert_fields((b'foo', b'bar'))\n\n    def test_querystring_blank_end(self):\n        self.p.write(b'foo=bar&')\n\n        self.assert_fields((b'foo', b'bar'))\n\n    def test_multiple_querystring(self):\n        self.p.write(b'foo=bar&asdf=baz')\n\n        self.assert_fields(\n            (b'foo', b'bar'),\n            (b'asdf', b'baz')\n        )\n\n    def test_streaming_simple(self):\n        self.p.write(b'foo=bar&')\n        self.assert_fields(\n            (b'foo', b'bar'),\n            finalize=False\n        )\n\n        self.p.write(b'asdf=baz')\n        self.assert_fields(\n            (b'asdf', b'baz')\n        )\n\n    def test_streaming_break(self):\n        self.p.write(b'foo=one')\n        self.assert_fields(finalize=False)\n\n        self.p.write(b'two')\n        self.assert_fields(finalize=False)\n\n        self.p.write(b'three')\n        self.assert_fields(finalize=False)\n\n        self.p.write(b'&asd')\n        self.assert_fields(\n            (b'foo', b'onetwothree'),\n            finalize=False\n        )\n\n        self.p.write(b'f=baz')\n        self.assert_fields(\n            (b'asdf', b'baz')\n        )\n\n    def test_semicolon_seperator(self):\n        self.p.write(b'foo=bar;asdf=baz')\n\n        self.assert_fields(\n            (b'foo', b'bar'),\n            (b'asdf', b'baz')\n        )\n\n    def test_too_large_field(self):\n        self.p.max_size = 15\n\n        # Note: len = 8\n        self.p.write(b\"foo=bar&\")\n        self.assert_fields((b'foo', b'bar'), finalize=False)\n\n        # Note: len = 8, only 7 bytes processed\n        self.p.write(b'a=123456')\n        self.assert_fields((b'a', b'12345'))\n\n    def test_invalid_max_size(self):\n        with self.assertRaises(ValueError):\n            p = QuerystringParser(max_size=-100)\n\n    def test_strict_parsing_pass(self):\n        data = b'foo=bar&another=asdf'\n        for first, last in split_all(data):\n            self.reset()\n            self.p.strict_parsing = True\n\n            print(\"%r / %r\" % (first, last))\n\n            self.p.write(first)\n            self.p.write(last)\n            self.assert_fields((b'foo', b'bar'), (b'another', b'asdf'))\n\n    def test_strict_parsing_fail_double_sep(self):\n        data = b'foo=bar&&another=asdf'\n        for first, last in split_all(data):\n            self.reset()\n            self.p.strict_parsing = True\n\n            cnt = 0\n            with self.assertRaises(QuerystringParseError) as cm:\n                cnt += self.p.write(first)\n                cnt += self.p.write(last)\n                self.p.finalize()\n\n            # The offset should occur at 8 bytes into the data (as a whole),\n            # so we calculate the offset into the chunk.\n            if cm is not None:\n                self.assertEqual(cm.exception.offset, 8 - cnt)\n\n    def test_double_sep(self):\n        data = b'foo=bar&&another=asdf'\n        for first, last in split_all(data):\n            print(\" %r / %r \" % (first, last))\n            self.reset()\n\n            cnt = 0\n            cnt += self.p.write(first)\n            cnt += self.p.write(last)\n\n            self.assert_fields((b'foo', b'bar'), (b'another', b'asdf'))\n\n    def test_strict_parsing_fail_no_value(self):\n        self.p.strict_parsing = True\n        with self.assertRaises(QuerystringParseError) as cm:\n            self.p.write(b'foo=bar&blank&another=asdf')\n\n        if cm is not None:\n            self.assertEqual(cm.exception.offset, 8)\n\n    def test_success_no_value(self):\n        self.p.write(b'foo=bar&blank&another=asdf')\n        self.assert_fields(\n            (b'foo', b'bar'),\n            (b'blank', b''),\n            (b'another', b'asdf')\n        )\n\n\nclass TestOctetStreamParser(unittest.TestCase):\n    def setUp(self):\n        self.d = []\n        self.started = 0\n        self.finished = 0\n\n        def on_start():\n            self.started += 1\n\n        def on_data(data, start, end):\n            self.d.append(data[start:end])\n\n        def on_end():\n            self.finished += 1\n\n        callbacks = {\n            'on_start': on_start,\n            'on_data': on_data,\n            'on_end': on_end\n        }\n\n        self.p = OctetStreamParser(callbacks)\n\n    def assert_data(self, data, finalize=True):\n        self.assertEqual(b''.join(self.d), data)\n        self.d = []\n\n    def assert_started(self, val=True):\n        if val:\n            self.assertEqual(self.started, 1)\n        else:\n            self.assertEqual(self.started, 0)\n\n    def assert_finished(self, val=True):\n        if val:\n            self.assertEqual(self.finished, 1)\n        else:\n            self.assertEqual(self.finished, 0)\n\n    def test_simple(self):\n        # Assert is not started\n        self.assert_started(False)\n\n        # Write something, it should then be started + have data\n        self.p.write(b'foobar')\n        self.assert_started()\n        self.assert_data(b'foobar')\n\n        # Finalize, and check\n        self.assert_finished(False)\n        self.p.finalize()\n        self.assert_finished()\n\n    def test_multiple_chunks(self):\n        self.p.write(b'foo')\n        self.p.write(b'bar')\n        self.p.write(b'baz')\n        self.p.finalize()\n\n        self.assert_data(b'foobarbaz')\n        self.assert_finished()\n\n    def test_max_size(self):\n        self.p.max_size = 5\n\n        self.p.write(b'0123456789')\n        self.p.finalize()\n\n        self.assert_data(b'01234')\n        self.assert_finished()\n\n    def test_invalid_max_size(self):\n        with self.assertRaises(ValueError):\n            q = OctetStreamParser(max_size='foo')\n\n\nclass TestBase64Decoder(unittest.TestCase):\n    # Note: base64('foobar') == 'Zm9vYmFy'\n    def setUp(self):\n        self.f = BytesIO()\n        self.d = Base64Decoder(self.f)\n\n    def assert_data(self, data, finalize=True):\n        if finalize:\n            self.d.finalize()\n\n        self.f.seek(0)\n        self.assertEqual(self.f.read(), data)\n        self.f.seek(0)\n        self.f.truncate()\n\n    def test_simple(self):\n        self.d.write(b'Zm9vYmFy')\n        self.assert_data(b'foobar')\n\n    def test_bad(self):\n        with self.assertRaises(DecodeError):\n            self.d.write(b'Zm9v!mFy')\n\n    def test_split_properly(self):\n        self.d.write(b'Zm9v')\n        self.d.write(b'YmFy')\n        self.assert_data(b'foobar')\n\n    def test_bad_split(self):\n        buff = b'Zm9v'\n        for i in range(1, 4):\n            first, second = buff[:i], buff[i:]\n\n            self.setUp()\n            self.d.write(first)\n            self.d.write(second)\n            self.assert_data(b'foo')\n\n    def test_long_bad_split(self):\n        buff = b'Zm9vYmFy'\n        for i in range(5, 8):\n            first, second = buff[:i], buff[i:]\n\n            self.setUp()\n            self.d.write(first)\n            self.d.write(second)\n            self.assert_data(b'foobar')\n\n    def test_close_and_finalize(self):\n        parser = Mock()\n        f = Base64Decoder(parser)\n\n        f.finalize()\n        parser.finalize.assert_called_once_with()\n\n        f.close()\n        parser.close.assert_called_once_with()\n\n    def test_bad_length(self):\n        self.d.write(b'Zm9vYmF')        # missing ending 'y'\n\n        with self.assertRaises(DecodeError):\n            self.d.finalize()\n\n\nclass TestQuotedPrintableDecoder(unittest.TestCase):\n    def setUp(self):\n        self.f = BytesIO()\n        self.d = QuotedPrintableDecoder(self.f)\n\n    def assert_data(self, data, finalize=True):\n        if finalize:\n            self.d.finalize()\n\n        self.f.seek(0)\n        self.assertEqual(self.f.read(), data)\n        self.f.seek(0)\n        self.f.truncate()\n\n    def test_simple(self):\n        self.d.write(b'foobar')\n        self.assert_data(b'foobar')\n\n    def test_with_escape(self):\n        self.d.write(b'foo=3Dbar')\n        self.assert_data(b'foo=bar')\n\n    def test_with_newline_escape(self):\n        self.d.write(b'foo=\\r\\nbar')\n        self.assert_data(b'foobar')\n\n    def test_with_only_newline_escape(self):\n        self.d.write(b'foo=\\nbar')\n        self.assert_data(b'foobar')\n\n    def test_with_split_escape(self):\n        self.d.write(b'foo=3')\n        self.d.write(b'Dbar')\n        self.assert_data(b'foo=bar')\n\n    def test_with_split_newline_escape_1(self):\n        self.d.write(b'foo=\\r')\n        self.d.write(b'\\nbar')\n        self.assert_data(b'foobar')\n\n    def test_with_split_newline_escape_2(self):\n        self.d.write(b'foo=')\n        self.d.write(b'\\r\\nbar')\n        self.assert_data(b'foobar')\n\n    def test_close_and_finalize(self):\n        parser = Mock()\n        f = QuotedPrintableDecoder(parser)\n\n        f.finalize()\n        parser.finalize.assert_called_once_with()\n\n        f.close()\n        parser.close.assert_called_once_with()\n\n    def test_not_aligned(self):\n        \"\"\"\n        https://github.com/andrew-d/python-multipart/issues/6\n        \"\"\"\n        self.d.write(b'=3AX')\n        self.assert_data(b':X')\n\n        # Additional offset tests\n        self.d.write(b'=3')\n        self.d.write(b'AX')\n        self.assert_data(b':X')\n\n        self.d.write(b'q=3AX')\n        self.assert_data(b'q:X')\n\n\n# Load our list of HTTP test cases.\nhttp_tests_dir = os.path.join(curr_dir, 'test_data', 'http')\n\n# Read in all test cases and load them.\nNON_PARAMETRIZED_TESTS = set(['single_field_blocks'])\nhttp_tests = []\nfor f in os.listdir(http_tests_dir):\n    # Only load the HTTP test cases.\n    fname, ext = os.path.splitext(f)\n    if fname in NON_PARAMETRIZED_TESTS:\n        continue\n\n    if ext == '.http':\n        # Get the YAML file and load it too.\n        yaml_file = os.path.join(http_tests_dir, fname + '.yaml')\n\n        # Load both.\n        with open(os.path.join(http_tests_dir, f), 'rb') as f:\n            test_data = f.read()\n\n        with open(yaml_file, 'rb') as f:\n            yaml_data = yaml.load(f)\n\n        http_tests.append({\n            'name': fname,\n            'test': test_data,\n            'result': yaml_data\n        })\n\n\ndef split_all(val):\n    \"\"\"\n    This function will split an array all possible ways.  For example:\n        split_all([1,2,3,4])\n    will give:\n        ([1], [2,3,4]), ([1,2], [3,4]), ([1,2,3], [4])\n    \"\"\"\n    for i in range(1, len(val) - 1):\n        yield (val[:i], val[i:])\n\n\n@parametrize_class\nclass TestFormParser(unittest.TestCase):\n    def make(self, boundary, config={}):\n        self.ended = False\n        self.files = []\n        self.fields = []\n\n        def on_field(f):\n            self.fields.append(f)\n\n        def on_file(f):\n            self.files.append(f)\n\n        def on_end():\n            self.ended = True\n\n        # Get a form-parser instance.\n        self.f = FormParser('multipart/form-data', on_field, on_file, on_end,\n                            boundary=boundary, config=config)\n\n    def assert_file_data(self, f, data):\n        o = f.file_object\n        o.seek(0)\n        file_data = o.read()\n        self.assertEqual(file_data, data)\n\n    def assert_file(self, field_name, file_name, data):\n        # Find this file.\n        found = None\n        for f in self.files:\n            if f.field_name == field_name:\n                found = f\n                break\n\n        # Assert that we found it.\n        self.assertIsNotNone(found)\n\n        try:\n            # Assert about this file.\n            self.assert_file_data(found, data)\n            self.assertEqual(found.file_name, file_name)\n\n            # Remove it from our list.\n            self.files.remove(found)\n        finally:\n            # Close our file\n            found.close()\n\n    def assert_field(self, name, value):\n        # Find this field in our fields list.\n        found = None\n        for f in self.fields:\n            if f.field_name == name:\n                found = f\n                break\n\n        # Assert that it exists and matches.\n        self.assertIsNotNone(found)\n        self.assertEqual(value, found.value)\n\n        # Remove it for future iterations.\n        self.fields.remove(found)\n\n    @parametrize('param', http_tests)\n    def test_http(self, param):\n        # Firstly, create our parser with the given boundary.\n        boundary = param['result']['boundary']\n        if isinstance(boundary, text_type):\n            boundary = boundary.encode('latin-1')\n        self.make(boundary)\n\n        # Now, we feed the parser with data.\n        exc = None\n        try:\n            processed = self.f.write(param['test'])\n            self.f.finalize()\n        except MultipartParseError as e:\n            processed = 0\n            exc = e\n\n        # print(repr(param))\n        # print(\"\")\n        # print(repr(self.fields))\n        # print(repr(self.files))\n\n        # Do we expect an error?\n        if 'error' in param['result']['expected']:\n            self.assertIsNotNone(exc)\n            self.assertEqual(param['result']['expected']['error'], exc.offset)\n            return\n\n        # No error!\n        self.assertEqual(processed, len(param['test']))\n\n        # Assert that the parser gave us the appropriate fields/files.\n        for e in param['result']['expected']:\n            # Get our type and name.\n            type = e['type']\n            name = e['name'].encode('latin-1')\n\n            if type == 'field':\n                self.assert_field(name, e['data'])\n\n            elif type == 'file':\n                self.assert_file(\n                    name,\n                    e['file_name'].encode('latin-1'),\n                    e['data']\n                )\n\n            else:\n                assert False\n\n    def test_random_splitting(self):\n        \"\"\"\n        This test runs a simple multipart body with one field and one file\n        through every possible split.\n        \"\"\"\n        # Load test data.\n        test_file = 'single_field_single_file.http'\n        with open(os.path.join(http_tests_dir, test_file), 'rb') as f:\n            test_data = f.read()\n\n        # We split the file through all cases.\n        for first, last in split_all(test_data):\n            # Create form parser.\n            self.make('boundary')\n\n            # Feed with data in 2 chunks.\n            i = 0\n            i += self.f.write(first)\n            i += self.f.write(last)\n            self.f.finalize()\n\n            # Assert we processed everything.\n            self.assertEqual(i, len(test_data))\n\n            # Assert that our file and field are here.\n            self.assert_field(b'field', b'test1')\n            self.assert_file(b'file', b'file.txt', b'test2')\n\n    def test_feed_single_bytes(self):\n        \"\"\"\n        This test parses a simple multipart body 1 byte at a time.\n        \"\"\"\n        # Load test data.\n        test_file = 'single_field_single_file.http'\n        with open(os.path.join(http_tests_dir, test_file), 'rb') as f:\n            test_data = f.read()\n\n        # Create form parser.\n        self.make('boundary')\n\n        # Write all bytes.\n        # NOTE: Can't simply do `for b in test_data`, since that gives\n        # an integer when iterating over a bytes object on Python 3.\n        i = 0\n        for x in range(len(test_data)):\n            b = test_data[x:x + 1]\n            i += self.f.write(b)\n\n        self.f.finalize()\n\n        # Assert we processed everything.\n        self.assertEqual(i, len(test_data))\n\n        # Assert that our file and field are here.\n        self.assert_field(b'field', b'test1')\n        self.assert_file(b'file', b'file.txt', b'test2')\n\n    def test_feed_blocks(self):\n        \"\"\"\n        This test parses a simple multipart body 1 byte at a time.\n        \"\"\"\n        # Load test data.\n        test_file = 'single_field_blocks.http'\n        with open(os.path.join(http_tests_dir, test_file), 'rb') as f:\n            test_data = f.read()\n\n        for c in range(1, len(test_data) + 1):\n            # Skip first `d` bytes - not interesting\n            for d in range(c):\n\n                # Create form parser.\n                self.make('boundary')\n                # Skip\n                i = 0\n                self.f.write(test_data[:d])\n                i += d\n                for x in range(d, len(test_data), c):\n                    # Write a chunk to achieve condition\n                    #     `i == data_length - 1`\n                    # in boundary search loop (multipatr.py:1302)\n                    b = test_data[x:x + c]\n                    i += self.f.write(b)\n\n                self.f.finalize()\n\n                # Assert we processed everything.\n                self.assertEqual(i, len(test_data))\n\n                # Assert that our field is here.\n                self.assert_field(b'field',\n                                  b'0123456789ABCDEFGHIJ0123456789ABCDEFGHIJ')\n\n    @slow_test\n    def test_request_body_fuzz(self):\n        \"\"\"\n        This test randomly fuzzes the request body to ensure that no strange\n        exceptions are raised and we don't end up in a strange state.  The\n        fuzzing consists of randomly doing one of the following:\n            - Adding a random byte at a random offset\n            - Randomly deleting a single byte\n            - Randomly swapping two bytes\n        \"\"\"\n        # Load test data.\n        test_file = 'single_field_single_file.http'\n        with open(os.path.join(http_tests_dir, test_file), 'rb') as f:\n            test_data = f.read()\n\n        iterations = 1000\n        successes = 0\n        failures = 0\n        exceptions = 0\n\n        print(\"Running %d iterations of fuzz testing:\" % (iterations,))\n        for i in range(iterations):\n            # Create a bytearray to mutate.\n            fuzz_data = bytearray(test_data)\n\n            # Pick what we're supposed to do.\n            choice = random.choice([1, 2, 3])\n            if choice == 1:\n                # Add a random byte.\n                i = random.randrange(len(test_data))\n                b = random.randrange(256)\n\n                fuzz_data.insert(i, b)\n                msg = \"Inserting byte %r at offset %d\" % (b, i)\n\n            elif choice == 2:\n                # Remove a random byte.\n                i = random.randrange(len(test_data))\n                del fuzz_data[i]\n\n                msg = \"Deleting byte at offset %d\" % (i,)\n\n            elif choice == 3:\n                # Swap two bytes.\n                i = random.randrange(len(test_data) - 1)\n                fuzz_data[i], fuzz_data[i + 1] = fuzz_data[i + 1], fuzz_data[i]\n\n                msg = \"Swapping bytes %d and %d\" % (i, i + 1)\n\n            # Print message, so if this crashes, we can inspect the output.\n            print(\"  \" + msg)\n\n            # Create form parser.\n            self.make('boundary')\n\n            # Feed with data, and ignore form parser exceptions.\n            i = 0\n            try:\n                i = self.f.write(bytes(fuzz_data))\n                self.f.finalize()\n            except FormParserError:\n                exceptions += 1\n            else:\n                if i == len(fuzz_data):\n                    successes += 1\n                else:\n                    failures += 1\n\n        print(\"--------------------------------------------------\")\n        print(\"Successes:  %d\" % (successes,))\n        print(\"Failures:   %d\" % (failures,))\n        print(\"Exceptions: %d\" % (exceptions,))\n\n    @slow_test\n    def test_request_body_fuzz_random_data(self):\n        \"\"\"\n        This test will fuzz the multipart parser with some number of iterations\n        of randomly-generated data.\n        \"\"\"\n        iterations = 1000\n        successes = 0\n        failures = 0\n        exceptions = 0\n\n        print(\"Running %d iterations of fuzz testing:\" % (iterations,))\n        for i in range(iterations):\n            data_size = random.randrange(100, 4096)\n            data = os.urandom(data_size)\n            print(\"  Testing with %d random bytes...\" % (data_size,))\n\n            # Create form parser.\n            self.make('boundary')\n\n            # Feed with data, and ignore form parser exceptions.\n            i = 0\n            try:\n                i = self.f.write(bytes(data))\n                self.f.finalize()\n            except FormParserError:\n                exceptions += 1\n            else:\n                if i == len(data):\n                    successes += 1\n                else:\n                    failures += 1\n\n        print(\"--------------------------------------------------\")\n        print(\"Successes:  %d\" % (successes,))\n        print(\"Failures:   %d\" % (failures,))\n        print(\"Exceptions: %d\" % (exceptions,))\n\n    def test_bad_start_boundary(self):\n        self.make('boundary')\n        data = b'--boundary\\rfoobar'\n        with self.assertRaises(MultipartParseError):\n            self.f.write(data)\n\n        self.make('boundary')\n        data = b'--boundaryfoobar'\n        with self.assertRaises(MultipartParseError):\n            i = self.f.write(data)\n\n    def test_octet_stream(self):\n        files = []\n        def on_file(f):\n            files.append(f)\n        on_field = Mock()\n        on_end = Mock()\n\n        f = FormParser('application/octet-stream', on_field, on_file, on_end=on_end, file_name=b'foo.txt')\n        self.assertTrue(isinstance(f.parser, OctetStreamParser))\n\n        f.write(b'test')\n        f.write(b'1234')\n        f.finalize()\n\n        # Assert that we only recieved a single file, with the right data, and that we're done.\n        self.assertFalse(on_field.called)\n        self.assertEqual(len(files), 1)\n        self.assert_file_data(files[0], b'test1234')\n        self.assertTrue(on_end.called)\n\n    def test_querystring(self):\n        fields = []\n        def on_field(f):\n            fields.append(f)\n        on_file = Mock()\n        on_end = Mock()\n\n        def simple_test(f):\n            # Reset tracking.\n            del fields[:]\n            on_file.reset_mock()\n            on_end.reset_mock()\n\n            # Write test data.\n            f.write(b'foo=bar')\n            f.write(b'&test=asdf')\n            f.finalize()\n\n            # Assert we only recieved 2 fields...\n            self.assertFalse(on_file.called)\n            self.assertEqual(len(fields), 2)\n\n            # ...assert that we have the correct data...\n            self.assertEqual(fields[0].field_name, b'foo')\n            self.assertEqual(fields[0].value, b'bar')\n\n            self.assertEqual(fields[1].field_name, b'test')\n            self.assertEqual(fields[1].value, b'asdf')\n\n            # ... and assert that we've finished.\n            self.assertTrue(on_end.called)\n\n        f = FormParser('application/x-www-form-urlencoded', on_field, on_file, on_end=on_end)\n        self.assertTrue(isinstance(f.parser, QuerystringParser))\n        simple_test(f)\n\n        f = FormParser('application/x-url-encoded', on_field, on_file, on_end=on_end)\n        self.assertTrue(isinstance(f.parser, QuerystringParser))\n        simple_test(f)\n\n    def test_close_methods(self):\n        parser = Mock()\n        f = FormParser('application/x-url-encoded', None, None)\n        f.parser = parser\n\n        f.finalize()\n        parser.finalize.assert_called_once_with()\n\n        f.close()\n        parser.close.assert_called_once_with()\n\n    def test_bad_content_type(self):\n        # We should raise a ValueError for a bad Content-Type\n        with self.assertRaises(ValueError):\n            f = FormParser('application/bad', None, None)\n\n    def test_no_boundary_given(self):\n        # We should raise a FormParserError when parsing a multipart message\n        # without a boundary.\n        with self.assertRaises(FormParserError):\n            f = FormParser('multipart/form-data', None, None)\n\n    def test_bad_content_transfer_encoding(self):\n        data = b'----boundary\\r\\nContent-Disposition: form-data; name=\"file\"; filename=\"test.txt\"\\r\\nContent-Type: text/plain\\r\\nContent-Transfer-Encoding: badstuff\\r\\n\\r\\nTest\\r\\n----boundary--\\r\\n'\n\n        files = []\n        def on_file(f):\n            files.append(f)\n        on_field = Mock()\n        on_end = Mock()\n\n        # Test with erroring.\n        config = {'UPLOAD_ERROR_ON_BAD_CTE': True}\n        f = FormParser('multipart/form-data', on_field, on_file,\n                       on_end=on_end, boundary='--boundary', config=config)\n\n        with self.assertRaises(FormParserError):\n            f.write(data)\n            f.finalize()\n\n        # Test without erroring.\n        config = {'UPLOAD_ERROR_ON_BAD_CTE': False}\n        f = FormParser('multipart/form-data', on_field, on_file,\n                       on_end=on_end, boundary='--boundary', config=config)\n\n        f.write(data)\n        f.finalize()\n        self.assert_file_data(files[0], b'Test')\n\n    def test_handles_None_fields(self):\n        fields = []\n        def on_field(f):\n            fields.append(f)\n        on_file = Mock()\n        on_end = Mock()\n\n        f = FormParser('application/x-www-form-urlencoded', on_field, on_file, on_end=on_end)\n        f.write(b'foo=bar&another&baz=asdf')\n        f.finalize()\n\n        self.assertEqual(fields[0].field_name, b'foo')\n        self.assertEqual(fields[0].value, b'bar')\n\n        self.assertEqual(fields[1].field_name, b'another')\n        self.assertEqual(fields[1].value, None)\n\n        self.assertEqual(fields[2].field_name, b'baz')\n        self.assertEqual(fields[2].value, b'asdf')\n\n    def test_max_size_multipart(self):\n        # Load test data.\n        test_file = 'single_field_single_file.http'\n        with open(os.path.join(http_tests_dir, test_file), 'rb') as f:\n            test_data = f.read()\n\n        # Create form parser.\n        self.make('boundary')\n\n        # Set the maximum length that we can process to be halfway through the\n        # given data.\n        self.f.parser.max_size = len(test_data) / 2\n\n        i = self.f.write(test_data)\n        self.f.finalize()\n\n        # Assert we processed the correct amount.\n        self.assertEqual(i, len(test_data) / 2)\n\n    def test_max_size_form_parser(self):\n        # Load test data.\n        test_file = 'single_field_single_file.http'\n        with open(os.path.join(http_tests_dir, test_file), 'rb') as f:\n            test_data = f.read()\n\n        # Create form parser setting the maximum length that we can process to\n        # be halfway through the given data.\n        size = len(test_data) / 2\n        self.make('boundary', config={'MAX_BODY_SIZE': size})\n\n        i = self.f.write(test_data)\n        self.f.finalize()\n\n        # Assert we processed the correct amount.\n        self.assertEqual(i, len(test_data) / 2)\n\n    def test_octet_stream_max_size(self):\n        files = []\n        def on_file(f):\n            files.append(f)\n        on_field = Mock()\n        on_end = Mock()\n\n        f = FormParser('application/octet-stream', on_field, on_file,\n                       on_end=on_end, file_name=b'foo.txt',\n                       config={'MAX_BODY_SIZE': 10})\n\n        f.write(b'0123456789012345689')\n        f.finalize()\n\n        self.assert_file_data(files[0], b'0123456789')\n\n    def test_invalid_max_size_multipart(self):\n        with self.assertRaises(ValueError):\n            q = MultipartParser(b'bound', max_size='foo')\n\n\nclass TestHelperFunctions(unittest.TestCase):\n    def test_create_form_parser(self):\n        r = create_form_parser({'Content-Type': 'application/octet-stream'},\n                               None, None)\n        self.assertTrue(isinstance(r, FormParser))\n\n    def test_create_form_parser_error(self):\n        headers = {}\n        with self.assertRaises(ValueError):\n            create_form_parser(headers, None, None)\n\n    def test_parse_form(self):\n        on_field = Mock()\n        on_file = Mock()\n\n        parse_form(\n            {'Content-Type': 'application/octet-stream',\n             },\n            BytesIO(b'123456789012345'),\n            on_field,\n            on_file\n        )\n\n        on_file.assert_called_once()\n\n        # Assert that the first argument of the call (a File object) has size\n        # 15 - i.e. all data is written.\n        self.assertEqual(on_file.call_args[0][0].size, 15)\n\n    def test_parse_form_content_length(self):\n        files = []\n        def on_file(file):\n            files.append(file)\n\n        parse_form(\n            {'Content-Type': 'application/octet-stream',\n             'Content-Length': '10'\n             },\n            BytesIO(b'123456789012345'),\n            None,\n            on_file\n        )\n\n        self.assertEqual(len(files), 1)\n        self.assertEqual(files[0].size, 10)\n\n\n\ndef suite():\n    suite = unittest.TestSuite()\n    suite.addTest(unittest.makeSuite(TestFile))\n    suite.addTest(unittest.makeSuite(TestParseOptionsHeader))\n    suite.addTest(unittest.makeSuite(TestBaseParser))\n    suite.addTest(unittest.makeSuite(TestQuerystringParser))\n    suite.addTest(unittest.makeSuite(TestOctetStreamParser))\n    suite.addTest(unittest.makeSuite(TestBase64Decoder))\n    suite.addTest(unittest.makeSuite(TestQuotedPrintableDecoder))\n    suite.addTest(unittest.makeSuite(TestFormParser))\n    suite.addTest(unittest.makeSuite(TestHelperFunctions))\n\n    return suite\n\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/passlib/tests/__init__.py", "content": "\"\"\"passlib tests\"\"\"\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/h11/tests/test_util.py", "content": "import re\nimport sys\nimport traceback\nfrom typing import NoReturn\n\nimport pytest\n\nfrom .._util import (\n    bytesify,\n    LocalProtocolError,\n    ProtocolError,\n    RemoteProtocolError,\n    Sentinel,\n    validate,\n)\n\n\ndef test_ProtocolError() -> None:\n    with pytest.raises(TypeError):\n        ProtocolError(\"abstract base class\")\n\n\ndef test_LocalProtocolError() -> None:\n    try:\n        raise LocalProtocolError(\"foo\")\n    except LocalProtocolError as e:\n        assert str(e) == \"foo\"\n        assert e.error_status_hint == 400\n\n    try:\n        raise LocalProtocolError(\"foo\", error_status_hint=418)\n    except LocalProtocolError as e:\n        assert str(e) == \"foo\"\n        assert e.error_status_hint == 418\n\n    def thunk() -> NoReturn:\n        raise LocalProtocolError(\"a\", error_status_hint=420)\n\n    try:\n        try:\n            thunk()\n        except LocalProtocolError as exc1:\n            orig_traceback = \"\".join(traceback.format_tb(sys.exc_info()[2]))\n            exc1._reraise_as_remote_protocol_error()\n    except RemoteProtocolError as exc2:\n        assert type(exc2) is RemoteProtocolError\n        assert exc2.args == (\"a\",)\n        assert exc2.error_status_hint == 420\n        new_traceback = \"\".join(traceback.format_tb(sys.exc_info()[2]))\n        assert new_traceback.endswith(orig_traceback)\n\n\ndef test_validate() -> None:\n    my_re = re.compile(br\"(?P<group1>[0-9]+)\\.(?P<group2>[0-9]+)\")\n    with pytest.raises(LocalProtocolError):\n        validate(my_re, b\"0.\")\n\n    groups = validate(my_re, b\"0.1\")\n    assert groups == {\"group1\": b\"0\", \"group2\": b\"1\"}\n\n    # successful partial matches are an error - must match whole string\n    with pytest.raises(LocalProtocolError):\n        validate(my_re, b\"0.1xx\")\n    with pytest.raises(LocalProtocolError):\n        validate(my_re, b\"0.1\\n\")\n\n\ndef test_validate_formatting() -> None:\n    my_re = re.compile(br\"foo\")\n\n    with pytest.raises(LocalProtocolError) as excinfo:\n        validate(my_re, b\"\", \"oops\")\n    assert \"oops\" in str(excinfo.value)\n\n    with pytest.raises(LocalProtocolError) as excinfo:\n        validate(my_re, b\"\", \"oops {}\")\n    assert \"oops {}\" in str(excinfo.value)\n\n    with pytest.raises(LocalProtocolError) as excinfo:\n        validate(my_re, b\"\", \"oops {} xx\", 10)\n    assert \"oops 10 xx\" in str(excinfo.value)\n\n\ndef test_make_sentinel() -> None:\n    class S(Sentinel, metaclass=Sentinel):\n        pass\n\n    assert repr(S) == \"S\"\n    assert S == S\n    assert type(S).__name__ == \"S\"\n    assert S in {S}\n    assert type(S) is S\n\n    class S2(Sentinel, metaclass=Sentinel):\n        pass\n\n    assert repr(S2) == \"S2\"\n    assert S != S2\n    assert S not in {S2}\n    assert type(S) is not type(S2)\n\n\ndef test_bytesify() -> None:\n    assert bytesify(b\"123\") == b\"123\"\n    assert bytesify(bytearray(b\"123\")) == b\"123\"\n    assert bytesify(\"123\") == b\"123\"\n\n    with pytest.raises(UnicodeEncodeError):\n        bytesify(\"\\u1234\")\n\n    with pytest.raises(TypeError):\n        bytesify(10)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/greenlet/tests/test_weakref.py", "content": "import gc\nimport greenlet\nimport weakref\nimport unittest\n\n\nclass WeakRefTests(unittest.TestCase):\n    def test_dead_weakref(self):\n        def _dead_greenlet():\n            g = greenlet.greenlet(lambda: None)\n            g.switch()\n            return g\n        o = weakref.ref(_dead_greenlet())\n        gc.collect()\n        self.assertEqual(o(), None)\n\n    def test_inactive_weakref(self):\n        o = weakref.ref(greenlet.greenlet())\n        gc.collect()\n        self.assertEqual(o(), None)\n\n    def test_dealloc_weakref(self):\n        seen = []\n        def worker():\n            try:\n                greenlet.getcurrent().parent.switch()\n            finally:\n                seen.append(g())\n        g = greenlet.greenlet(worker)\n        g.switch()\n        g2 = greenlet.greenlet(lambda: None, g)\n        g = weakref.ref(g2)\n        g2 = None\n        self.assertEqual(seen, [None])\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/multipart/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/h11/tests/test_state.py", "content": "import pytest\n\nfrom .._events import (\n    ConnectionClosed,\n    Data,\n    EndOfMessage,\n    Event,\n    InformationalResponse,\n    Request,\n    Response,\n)\nfrom .._state import (\n    _SWITCH_CONNECT,\n    _SWITCH_UPGRADE,\n    CLIENT,\n    CLOSED,\n    ConnectionState,\n    DONE,\n    IDLE,\n    MIGHT_SWITCH_PROTOCOL,\n    MUST_CLOSE,\n    SEND_BODY,\n    SEND_RESPONSE,\n    SERVER,\n    SWITCHED_PROTOCOL,\n)\nfrom .._util import LocalProtocolError\n\n\ndef test_ConnectionState() -> None:\n    cs = ConnectionState()\n\n    # Basic event-triggered transitions\n\n    assert cs.states == {CLIENT: IDLE, SERVER: IDLE}\n\n    cs.process_event(CLIENT, Request)\n    # The SERVER-Request special case:\n    assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n\n    # Illegal transitions raise an error and nothing happens\n    with pytest.raises(LocalProtocolError):\n        cs.process_event(CLIENT, Request)\n    assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n\n    cs.process_event(SERVER, InformationalResponse)\n    assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n\n    cs.process_event(SERVER, Response)\n    assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_BODY}\n\n    cs.process_event(CLIENT, EndOfMessage)\n    cs.process_event(SERVER, EndOfMessage)\n    assert cs.states == {CLIENT: DONE, SERVER: DONE}\n\n    # State-triggered transition\n\n    cs.process_event(SERVER, ConnectionClosed)\n    assert cs.states == {CLIENT: MUST_CLOSE, SERVER: CLOSED}\n\n\ndef test_ConnectionState_keep_alive() -> None:\n    # keep_alive = False\n    cs = ConnectionState()\n    cs.process_event(CLIENT, Request)\n    cs.process_keep_alive_disabled()\n    cs.process_event(CLIENT, EndOfMessage)\n    assert cs.states == {CLIENT: MUST_CLOSE, SERVER: SEND_RESPONSE}\n\n    cs.process_event(SERVER, Response)\n    cs.process_event(SERVER, EndOfMessage)\n    assert cs.states == {CLIENT: MUST_CLOSE, SERVER: MUST_CLOSE}\n\n\ndef test_ConnectionState_keep_alive_in_DONE() -> None:\n    # Check that if keep_alive is disabled when the CLIENT is already in DONE,\n    # then this is sufficient to immediately trigger the DONE -> MUST_CLOSE\n    # transition\n    cs = ConnectionState()\n    cs.process_event(CLIENT, Request)\n    cs.process_event(CLIENT, EndOfMessage)\n    assert cs.states[CLIENT] is DONE\n    cs.process_keep_alive_disabled()\n    assert cs.states[CLIENT] is MUST_CLOSE\n\n\ndef test_ConnectionState_switch_denied() -> None:\n    for switch_type in (_SWITCH_CONNECT, _SWITCH_UPGRADE):\n        for deny_early in (True, False):\n            cs = ConnectionState()\n            cs.process_client_switch_proposal(switch_type)\n            cs.process_event(CLIENT, Request)\n            cs.process_event(CLIENT, Data)\n            assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n\n            assert switch_type in cs.pending_switch_proposals\n\n            if deny_early:\n                # before client reaches DONE\n                cs.process_event(SERVER, Response)\n                assert not cs.pending_switch_proposals\n\n            cs.process_event(CLIENT, EndOfMessage)\n\n            if deny_early:\n                assert cs.states == {CLIENT: DONE, SERVER: SEND_BODY}\n            else:\n                assert cs.states == {\n                    CLIENT: MIGHT_SWITCH_PROTOCOL,\n                    SERVER: SEND_RESPONSE,\n                }\n\n                cs.process_event(SERVER, InformationalResponse)\n                assert cs.states == {\n                    CLIENT: MIGHT_SWITCH_PROTOCOL,\n                    SERVER: SEND_RESPONSE,\n                }\n\n                cs.process_event(SERVER, Response)\n                assert cs.states == {CLIENT: DONE, SERVER: SEND_BODY}\n                assert not cs.pending_switch_proposals\n\n\n_response_type_for_switch = {\n    _SWITCH_UPGRADE: InformationalResponse,\n    _SWITCH_CONNECT: Response,\n    None: Response,\n}\n\n\ndef test_ConnectionState_protocol_switch_accepted() -> None:\n    for switch_event in [_SWITCH_UPGRADE, _SWITCH_CONNECT]:\n        cs = ConnectionState()\n        cs.process_client_switch_proposal(switch_event)\n        cs.process_event(CLIENT, Request)\n        cs.process_event(CLIENT, Data)\n        assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n\n        cs.process_event(CLIENT, EndOfMessage)\n        assert cs.states == {CLIENT: MIGHT_SWITCH_PROTOCOL, SERVER: SEND_RESPONSE}\n\n        cs.process_event(SERVER, InformationalResponse)\n        assert cs.states == {CLIENT: MIGHT_SWITCH_PROTOCOL, SERVER: SEND_RESPONSE}\n\n        cs.process_event(SERVER, _response_type_for_switch[switch_event], switch_event)\n        assert cs.states == {CLIENT: SWITCHED_PROTOCOL, SERVER: SWITCHED_PROTOCOL}\n\n\ndef test_ConnectionState_double_protocol_switch() -> None:\n    # CONNECT + Upgrade is legal! Very silly, but legal. So we support\n    # it. Because sometimes doing the silly thing is easier than not.\n    for server_switch in [None, _SWITCH_UPGRADE, _SWITCH_CONNECT]:\n        cs = ConnectionState()\n        cs.process_client_switch_proposal(_SWITCH_UPGRADE)\n        cs.process_client_switch_proposal(_SWITCH_CONNECT)\n        cs.process_event(CLIENT, Request)\n        cs.process_event(CLIENT, EndOfMessage)\n        assert cs.states == {CLIENT: MIGHT_SWITCH_PROTOCOL, SERVER: SEND_RESPONSE}\n        cs.process_event(\n            SERVER, _response_type_for_switch[server_switch], server_switch\n        )\n        if server_switch is None:\n            assert cs.states == {CLIENT: DONE, SERVER: SEND_BODY}\n        else:\n            assert cs.states == {CLIENT: SWITCHED_PROTOCOL, SERVER: SWITCHED_PROTOCOL}\n\n\ndef test_ConnectionState_inconsistent_protocol_switch() -> None:\n    for client_switches, server_switch in [\n        ([], _SWITCH_CONNECT),\n        ([], _SWITCH_UPGRADE),\n        ([_SWITCH_UPGRADE], _SWITCH_CONNECT),\n        ([_SWITCH_CONNECT], _SWITCH_UPGRADE),\n    ]:\n        cs = ConnectionState()\n        for client_switch in client_switches:  # type: ignore[attr-defined]\n            cs.process_client_switch_proposal(client_switch)\n        cs.process_event(CLIENT, Request)\n        with pytest.raises(LocalProtocolError):\n            cs.process_event(SERVER, Response, server_switch)\n\n\ndef test_ConnectionState_keepalive_protocol_switch_interaction() -> None:\n    # keep_alive=False + pending_switch_proposals\n    cs = ConnectionState()\n    cs.process_client_switch_proposal(_SWITCH_UPGRADE)\n    cs.process_event(CLIENT, Request)\n    cs.process_keep_alive_disabled()\n    cs.process_event(CLIENT, Data)\n    assert cs.states == {CLIENT: SEND_BODY, SERVER: SEND_RESPONSE}\n\n    # the protocol switch \"wins\"\n    cs.process_event(CLIENT, EndOfMessage)\n    assert cs.states == {CLIENT: MIGHT_SWITCH_PROTOCOL, SERVER: SEND_RESPONSE}\n\n    # but when the server denies the request, keep_alive comes back into play\n    cs.process_event(SERVER, Response)\n    assert cs.states == {CLIENT: MUST_CLOSE, SERVER: SEND_BODY}\n\n\ndef test_ConnectionState_reuse() -> None:\n    cs = ConnectionState()\n\n    with pytest.raises(LocalProtocolError):\n        cs.start_next_cycle()\n\n    cs.process_event(CLIENT, Request)\n    cs.process_event(CLIENT, EndOfMessage)\n\n    with pytest.raises(LocalProtocolError):\n        cs.start_next_cycle()\n\n    cs.process_event(SERVER, Response)\n    cs.process_event(SERVER, EndOfMessage)\n\n    cs.start_next_cycle()\n    assert cs.states == {CLIENT: IDLE, SERVER: IDLE}\n\n    # No keepalive\n\n    cs.process_event(CLIENT, Request)\n    cs.process_keep_alive_disabled()\n    cs.process_event(CLIENT, EndOfMessage)\n    cs.process_event(SERVER, Response)\n    cs.process_event(SERVER, EndOfMessage)\n\n    with pytest.raises(LocalProtocolError):\n        cs.start_next_cycle()\n\n    # One side closed\n\n    cs = ConnectionState()\n    cs.process_event(CLIENT, Request)\n    cs.process_event(CLIENT, EndOfMessage)\n    cs.process_event(CLIENT, ConnectionClosed)\n    cs.process_event(SERVER, Response)\n    cs.process_event(SERVER, EndOfMessage)\n\n    with pytest.raises(LocalProtocolError):\n        cs.start_next_cycle()\n\n    # Succesful protocol switch\n\n    cs = ConnectionState()\n    cs.process_client_switch_proposal(_SWITCH_UPGRADE)\n    cs.process_event(CLIENT, Request)\n    cs.process_event(CLIENT, EndOfMessage)\n    cs.process_event(SERVER, InformationalResponse, _SWITCH_UPGRADE)\n\n    with pytest.raises(LocalProtocolError):\n        cs.start_next_cycle()\n\n    # Failed protocol switch\n\n    cs = ConnectionState()\n    cs.process_client_switch_proposal(_SWITCH_UPGRADE)\n    cs.process_event(CLIENT, Request)\n    cs.process_event(CLIENT, EndOfMessage)\n    cs.process_event(SERVER, Response)\n    cs.process_event(SERVER, EndOfMessage)\n\n    cs.start_next_cycle()\n    assert cs.states == {CLIENT: IDLE, SERVER: IDLE}\n\n\ndef test_server_request_is_illegal() -> None:\n    # There used to be a bug in how we handled the Request special case that\n    # made this allowed...\n    cs = ConnectionState()\n    with pytest.raises(LocalProtocolError):\n        cs.process_event(SERVER, Request)\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/passlib/tests/backports.py", "content": "\"\"\"backports of needed unittest2 features\"\"\"\n#=============================================================================\n# imports\n#=============================================================================\nfrom __future__ import with_statement\n# core\nimport logging; log = logging.getLogger(__name__)\nimport re\nimport sys\n##from warnings import warn\n# site\n# pkg\nfrom passlib.utils.compat import PY26\n# local\n__all__ = [\n    \"TestCase\",\n    \"unittest\",\n    # TODO: deprecate these exports in favor of \"unittest.XXX\"\n    \"skip\", \"skipIf\", \"skipUnless\",\n]\n\n#=============================================================================\n# import latest unittest module available\n#=============================================================================\ntry:\n    import unittest2 as unittest\nexcept ImportError:\n    if PY26:\n        raise ImportError(\"Passlib's tests require 'unittest2' under Python 2.6 (as of Passlib 1.7)\")\n    # python 2.7 and python 3.2 both have unittest2 features (at least, the ones we use)\n    import unittest\n\n#=============================================================================\n# unittest aliases\n#=============================================================================\nskip = unittest.skip\nskipIf = unittest.skipIf\nskipUnless = unittest.skipUnless\nSkipTest = unittest.SkipTest\n\n#=============================================================================\n# custom test harness\n#=============================================================================\nclass TestCase(unittest.TestCase):\n    \"\"\"backports a number of unittest2 features in TestCase\"\"\"\n\n    #===================================================================\n    # backport some unittest2 names\n    #===================================================================\n\n    #---------------------------------------------------------------\n    # backport assertRegex() alias from 3.2 to 2.7\n    # was present in 2.7 under an alternate name\n    #---------------------------------------------------------------\n    if not hasattr(unittest.TestCase, \"assertRegex\"):\n        assertRegex = unittest.TestCase.assertRegexpMatches\n\n    if not hasattr(unittest.TestCase, \"assertRaisesRegex\"):\n        assertRaisesRegex = unittest.TestCase.assertRaisesRegexp\n\n    #===================================================================\n    # eoc\n    #===================================================================\n\n#=============================================================================\n# eof\n#=============================================================================\n"}
{"type": "test_file", "path": "env/lib/python3.9/site-packages/ecdsa/test_ecdsa.py", "content": "from __future__ import print_function\nimport sys\nimport hypothesis.strategies as st\nfrom hypothesis import given, settings, note, example\n\ntry:\n    import unittest2 as unittest\nexcept ImportError:\n    import unittest\nimport pytest\nfrom .ecdsa import (\n    Private_key,\n    Public_key,\n    Signature,\n    generator_192,\n    digest_integer,\n    ellipticcurve,\n    point_is_valid,\n    generator_224,\n    generator_256,\n    generator_384,\n    generator_521,\n    generator_secp256k1,\n    curve_192,\n    InvalidPointError,\n    curve_112r2,\n    generator_112r2,\n    int_to_string,\n)\n\n\nHYP_SETTINGS = {}\n# old hypothesis doesn't have the \"deadline\" setting\nif sys.version_info > (2, 7):  # pragma: no branch\n    # SEC521p is slow, allow long execution for it\n    HYP_SETTINGS[\"deadline\"] = 5000\n\n\nclass TestP192FromX9_62(unittest.TestCase):\n    \"\"\"Check test vectors from X9.62\"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        cls.d = 651056770906015076056810763456358567190100156695615665659\n        cls.Q = cls.d * generator_192\n        cls.k = 6140507067065001063065065565667405560006161556565665656654\n        cls.R = cls.k * generator_192\n\n        cls.msg = 968236873715988614170569073515315707566766479517\n        cls.pubk = Public_key(generator_192, generator_192 * cls.d)\n        cls.privk = Private_key(cls.pubk, cls.d)\n        cls.sig = cls.privk.sign(cls.msg, cls.k)\n\n    def test_point_multiplication(self):\n        assert self.Q.x() == 0x62B12D60690CDCF330BABAB6E69763B471F994DD702D16A5\n\n    def test_point_multiplication_2(self):\n        assert self.R.x() == 0x885052380FF147B734C330C43D39B2C4A89F29B0F749FEAD\n        assert self.R.y() == 0x9CF9FA1CBEFEFB917747A3BB29C072B9289C2547884FD835\n\n    def test_mult_and_addition(self):\n        u1 = 2563697409189434185194736134579731015366492496392189760599\n        u2 = 6266643813348617967186477710235785849136406323338782220568\n        temp = u1 * generator_192 + u2 * self.Q\n        assert temp.x() == 0x885052380FF147B734C330C43D39B2C4A89F29B0F749FEAD\n        assert temp.y() == 0x9CF9FA1CBEFEFB917747A3BB29C072B9289C2547884FD835\n\n    def test_signature(self):\n        r, s = self.sig.r, self.sig.s\n        assert r == 3342403536405981729393488334694600415596881826869351677613\n        assert s == 5735822328888155254683894997897571951568553642892029982342\n\n    def test_verification(self):\n        assert self.pubk.verifies(self.msg, self.sig)\n\n    def test_rejection(self):\n        assert not self.pubk.verifies(self.msg - 1, self.sig)\n\n\nclass TestPublicKey(unittest.TestCase):\n    def test_equality_public_keys(self):\n        gen = generator_192\n        x = 0xC58D61F88D905293BCD4CD0080BCB1B7F811F2FFA41979F6\n        y = 0x8804DC7A7C4C7F8B5D437F5156F3312CA7D6DE8A0E11867F\n        point = ellipticcurve.Point(gen.curve(), x, y)\n        pub_key1 = Public_key(gen, point)\n        pub_key2 = Public_key(gen, point)\n        self.assertEqual(pub_key1, pub_key2)\n\n    def test_inequality_public_key(self):\n        gen = generator_192\n        x1 = 0xC58D61F88D905293BCD4CD0080BCB1B7F811F2FFA41979F6\n        y1 = 0x8804DC7A7C4C7F8B5D437F5156F3312CA7D6DE8A0E11867F\n        point1 = ellipticcurve.Point(gen.curve(), x1, y1)\n\n        x2 = 0x6A223D00BD22C52833409A163E057E5B5DA1DEF2A197DD15\n        y2 = 0x7B482604199367F1F303F9EF627F922F97023E90EAE08ABF\n        point2 = ellipticcurve.Point(gen.curve(), x2, y2)\n\n        pub_key1 = Public_key(gen, point1)\n        pub_key2 = Public_key(gen, point2)\n        self.assertNotEqual(pub_key1, pub_key2)\n\n    def test_inequality_different_curves(self):\n        gen = generator_192\n        x1 = 0xC58D61F88D905293BCD4CD0080BCB1B7F811F2FFA41979F6\n        y1 = 0x8804DC7A7C4C7F8B5D437F5156F3312CA7D6DE8A0E11867F\n        point1 = ellipticcurve.Point(gen.curve(), x1, y1)\n\n        x2 = 0x722BA0FB6B8FC8898A4C6AB49E66\n        y2 = 0x2B7344BB57A7ABC8CA0F1A398C7D\n        point2 = ellipticcurve.Point(generator_112r2.curve(), x2, y2)\n\n        pub_key1 = Public_key(gen, point1)\n        pub_key2 = Public_key(generator_112r2, point2)\n        self.assertNotEqual(pub_key1, pub_key2)\n\n    def test_inequality_public_key_not_implemented(self):\n        gen = generator_192\n        x = 0xC58D61F88D905293BCD4CD0080BCB1B7F811F2FFA41979F6\n        y = 0x8804DC7A7C4C7F8B5D437F5156F3312CA7D6DE8A0E11867F\n        point = ellipticcurve.Point(gen.curve(), x, y)\n        pub_key = Public_key(gen, point)\n        self.assertNotEqual(pub_key, None)\n\n    def test_public_key_with_generator_without_order(self):\n        gen = ellipticcurve.PointJacobi(\n            generator_192.curve(), generator_192.x(), generator_192.y(), 1\n        )\n\n        x = 0xC58D61F88D905293BCD4CD0080BCB1B7F811F2FFA41979F6\n        y = 0x8804DC7A7C4C7F8B5D437F5156F3312CA7D6DE8A0E11867F\n        point = ellipticcurve.Point(gen.curve(), x, y)\n\n        with self.assertRaises(InvalidPointError) as e:\n            Public_key(gen, point)\n\n        self.assertIn(\"Generator point must have order\", str(e.exception))\n\n    def test_public_point_on_curve_not_scalar_multiple_of_base_point(self):\n        x = 2\n        y = 0xBE6AA4938EF7CFE6FE29595B6B00\n        # we need a curve with cofactor != 1\n        point = ellipticcurve.PointJacobi(curve_112r2, x, y, 1)\n\n        self.assertTrue(curve_112r2.contains_point(x, y))\n\n        with self.assertRaises(InvalidPointError) as e:\n            Public_key(generator_112r2, point)\n\n        self.assertIn(\"Generator point order\", str(e.exception))\n\n    def test_point_is_valid_with_not_scalar_multiple_of_base_point(self):\n        x = 2\n        y = 0xBE6AA4938EF7CFE6FE29595B6B00\n\n        self.assertFalse(point_is_valid(generator_112r2, x, y))\n\n    # the tests to verify the extensiveness of tests in ecdsa.ecdsa\n    # if PointJacobi gets modified to calculate the x and y mod p the tests\n    # below will need to use a fake/mock object\n    def test_invalid_point_x_negative(self):\n        pt = ellipticcurve.PointJacobi(curve_192, -1, 0, 1)\n\n        with self.assertRaises(InvalidPointError) as e:\n            Public_key(generator_192, pt)\n\n        self.assertIn(\"The public point has x or y\", str(e.exception))\n\n    def test_invalid_point_x_equal_p(self):\n        pt = ellipticcurve.PointJacobi(curve_192, curve_192.p(), 0, 1)\n\n        with self.assertRaises(InvalidPointError) as e:\n            Public_key(generator_192, pt)\n\n        self.assertIn(\"The public point has x or y\", str(e.exception))\n\n    def test_invalid_point_y_negative(self):\n        pt = ellipticcurve.PointJacobi(curve_192, 0, -1, 1)\n\n        with self.assertRaises(InvalidPointError) as e:\n            Public_key(generator_192, pt)\n\n        self.assertIn(\"The public point has x or y\", str(e.exception))\n\n    def test_invalid_point_y_equal_p(self):\n        pt = ellipticcurve.PointJacobi(curve_192, 0, curve_192.p(), 1)\n\n        with self.assertRaises(InvalidPointError) as e:\n            Public_key(generator_192, pt)\n\n        self.assertIn(\"The public point has x or y\", str(e.exception))\n\n\nclass TestPublicKeyVerifies(unittest.TestCase):\n    # test all the different ways that a signature can be publicly invalid\n    @classmethod\n    def setUpClass(cls):\n        gen = generator_192\n        x = 0xC58D61F88D905293BCD4CD0080BCB1B7F811F2FFA41979F6\n        y = 0x8804DC7A7C4C7F8B5D437F5156F3312CA7D6DE8A0E11867F\n        point = ellipticcurve.Point(gen.curve(), x, y)\n\n        cls.pub_key = Public_key(gen, point)\n\n    def test_sig_with_r_zero(self):\n        sig = Signature(0, 1)\n\n        self.assertFalse(self.pub_key.verifies(1, sig))\n\n    def test_sig_with_r_order(self):\n        sig = Signature(generator_192.order(), 1)\n\n        self.assertFalse(self.pub_key.verifies(1, sig))\n\n    def test_sig_with_s_zero(self):\n        sig = Signature(1, 0)\n\n        self.assertFalse(self.pub_key.verifies(1, sig))\n\n    def test_sig_with_s_order(self):\n        sig = Signature(1, generator_192.order())\n\n        self.assertFalse(self.pub_key.verifies(1, sig))\n\n\nclass TestPrivateKey(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        gen = generator_192\n        x = 0xC58D61F88D905293BCD4CD0080BCB1B7F811F2FFA41979F6\n        y = 0x8804DC7A7C4C7F8B5D437F5156F3312CA7D6DE8A0E11867F\n        point = ellipticcurve.Point(gen.curve(), x, y)\n        cls.pub_key = Public_key(gen, point)\n\n    def test_equality_private_keys(self):\n        pr_key1 = Private_key(self.pub_key, 100)\n        pr_key2 = Private_key(self.pub_key, 100)\n        self.assertEqual(pr_key1, pr_key2)\n\n    def test_inequality_private_keys(self):\n        pr_key1 = Private_key(self.pub_key, 100)\n        pr_key2 = Private_key(self.pub_key, 200)\n        self.assertNotEqual(pr_key1, pr_key2)\n\n    def test_inequality_private_keys_not_implemented(self):\n        pr_key = Private_key(self.pub_key, 100)\n        self.assertNotEqual(pr_key, None)\n\n\n# Testing point validity, as per ECDSAVS.pdf B.2.2:\nP192_POINTS = [\n    (\n        generator_192,\n        0xCD6D0F029A023E9AACA429615B8F577ABEE685D8257CC83A,\n        0x00019C410987680E9FB6C0B6ECC01D9A2647C8BAE27721BACDFC,\n        False,\n    ),\n    (\n        generator_192,\n        0x00017F2FCE203639E9EAF9FB50B81FC32776B30E3B02AF16C73B,\n        0x95DA95C5E72DD48E229D4748D4EEE658A9A54111B23B2ADB,\n        False,\n    ),\n    (\n        generator_192,\n        0x4F77F8BC7FCCBADD5760F4938746D5F253EE2168C1CF2792,\n        0x000147156FF824D131629739817EDB197717C41AAB5C2A70F0F6,\n        False,\n    ),\n    (\n        generator_192,\n        0xC58D61F88D905293BCD4CD0080BCB1B7F811F2FFA41979F6,\n        0x8804DC7A7C4C7F8B5D437F5156F3312CA7D6DE8A0E11867F,\n        True,\n    ),\n    (\n        generator_192,\n        0xCDF56C1AA3D8AFC53C521ADF3FFB96734A6A630A4A5B5A70,\n        0x97C1C44A5FB229007B5EC5D25F7413D170068FFD023CAA4E,\n        True,\n    ),\n    (\n        generator_192,\n        0x89009C0DC361C81E99280C8E91DF578DF88CDF4B0CDEDCED,\n        0x27BE44A529B7513E727251F128B34262A0FD4D8EC82377B9,\n        True,\n    ),\n    (\n        generator_192,\n        0x6A223D00BD22C52833409A163E057E5B5DA1DEF2A197DD15,\n        0x7B482604199367F1F303F9EF627F922F97023E90EAE08ABF,\n        True,\n    ),\n    (\n        generator_192,\n        0x6DCCBDE75C0948C98DAB32EA0BC59FE125CF0FB1A3798EDA,\n        0x0001171A3E0FA60CF3096F4E116B556198DE430E1FBD330C8835,\n        False,\n    ),\n    (\n        generator_192,\n        0xD266B39E1F491FC4ACBBBC7D098430931CFA66D55015AF12,\n        0x193782EB909E391A3148B7764E6B234AA94E48D30A16DBB2,\n        False,\n    ),\n    (\n        generator_192,\n        0x9D6DDBCD439BAA0C6B80A654091680E462A7D1D3F1FFEB43,\n        0x6AD8EFC4D133CCF167C44EB4691C80ABFFB9F82B932B8CAA,\n        False,\n    ),\n    (\n        generator_192,\n        0x146479D944E6BDA87E5B35818AA666A4C998A71F4E95EDBC,\n        0xA86D6FE62BC8FBD88139693F842635F687F132255858E7F6,\n        False,\n    ),\n    (\n        generator_192,\n        0xE594D4A598046F3598243F50FD2C7BD7D380EDB055802253,\n        0x509014C0C4D6B536E3CA750EC09066AF39B4C8616A53A923,\n        False,\n    ),\n]\n\n\n@pytest.mark.parametrize(\"generator,x,y,expected\", P192_POINTS)\ndef test_point_validity(generator, x, y, expected):\n    \"\"\"\n    `generator` defines the curve; is `(x, y)` a point on\n    this curve? `expected` is True if the right answer is Yes.\n    \"\"\"\n    assert point_is_valid(generator, x, y) == expected\n\n\n# Trying signature-verification tests from ECDSAVS.pdf B.2.4:\nCURVE_192_KATS = [\n    (\n        generator_192,\n        int(\n            \"0x84ce72aa8699df436059f052ac51b6398d2511e49631bcb7e71f89c499b9ee\"\n            \"425dfbc13a5f6d408471b054f2655617cbbaf7937b7c80cd8865cf02c8487d30\"\n            \"d2b0fbd8b2c4e102e16d828374bbc47b93852f212d5043c3ea720f086178ff79\"\n            \"8cc4f63f787b9c2e419efa033e7644ea7936f54462dc21a6c4580725f7f0e7d1\"\n            \"58\",\n            16,\n        ),\n        0xD9DBFB332AA8E5FF091E8CE535857C37C73F6250FFB2E7AC,\n        0x282102E364FEDED3AD15DDF968F88D8321AA268DD483EBC4,\n        0x64DCA58A20787C488D11D6DD96313F1B766F2D8EFE122916,\n        0x1ECBA28141E84AB4ECAD92F56720E2CC83EB3D22DEC72479,\n        True,\n    ),\n    (\n        generator_192,\n        int(\n            \"0x94bb5bacd5f8ea765810024db87f4224ad71362a3c28284b2b9f39fab86db1\"\n            \"2e8beb94aae899768229be8fdb6c4f12f28912bb604703a79ccff769c1607f5a\"\n            \"91450f30ba0460d359d9126cbd6296be6d9c4bb96c0ee74cbb44197c207f6db3\"\n            \"26ab6f5a659113a9034e54be7b041ced9dcf6458d7fb9cbfb2744d999f7dfd63\"\n            \"f4\",\n            16,\n        ),\n        0x3E53EF8D3112AF3285C0E74842090712CD324832D4277AE7,\n        0xCC75F8952D30AEC2CBB719FC6AA9934590B5D0FF5A83ADB7,\n        0x8285261607283BA18F335026130BAB31840DCFD9C3E555AF,\n        0x356D89E1B04541AFC9704A45E9C535CE4A50929E33D7E06C,\n        True,\n    ),\n    (\n        generator_192,\n        int(\n            \"0xf6227a8eeb34afed1621dcc89a91d72ea212cb2f476839d9b4243c66877911\"\n            \"b37b4ad6f4448792a7bbba76c63bdd63414b6facab7dc71c3396a73bd7ee14cd\"\n            \"d41a659c61c99b779cecf07bc51ab391aa3252386242b9853ea7da67fd768d30\"\n            \"3f1b9b513d401565b6f1eb722dfdb96b519fe4f9bd5de67ae131e64b40e78c42\"\n            \"dd\",\n            16,\n        ),\n        0x16335DBE95F8E8254A4E04575D736BEFB258B8657F773CB7,\n        0x421B13379C59BC9DCE38A1099CA79BBD06D647C7F6242336,\n        0x4141BD5D64EA36C5B0BD21EF28C02DA216ED9D04522B1E91,\n        0x159A6AA852BCC579E821B7BB0994C0861FB08280C38DAA09,\n        False,\n    ),\n    (\n        generator_192,\n        int(\n            \"0x16b5f93afd0d02246f662761ed8e0dd9504681ed02a253006eb36736b56309\"\n            \"7ba39f81c8e1bce7a16c1339e345efabbc6baa3efb0612948ae51103382a8ee8\"\n            \"bc448e3ef71e9f6f7a9676694831d7f5dd0db5446f179bcb737d4a526367a447\"\n            \"bfe2c857521c7f40b6d7d7e01a180d92431fb0bbd29c04a0c420a57b3ed26ccd\"\n            \"8a\",\n            16,\n        ),\n        0xFD14CDF1607F5EFB7B1793037B15BDF4BAA6F7C16341AB0B,\n        0x83FA0795CC6C4795B9016DAC928FD6BAC32F3229A96312C4,\n        0x8DFDB832951E0167C5D762A473C0416C5C15BC1195667DC1,\n        0x1720288A2DC13FA1EC78F763F8FE2FF7354A7E6FDDE44520,\n        False,\n    ),\n    (\n        generator_192,\n        int(\n            \"0x08a2024b61b79d260e3bb43ef15659aec89e5b560199bc82cf7c65c77d3919\"\n            \"2e03b9a895d766655105edd9188242b91fbde4167f7862d4ddd61e5d4ab55196\"\n            \"683d4f13ceb90d87aea6e07eb50a874e33086c4a7cb0273a8e1c4408f4b846bc\"\n            \"eae1ebaac1b2b2ea851a9b09de322efe34cebe601653efd6ddc876ce8c2f2072\"\n            \"fb\",\n            16,\n        ),\n        0x674F941DC1A1F8B763C9334D726172D527B90CA324DB8828,\n        0x65ADFA32E8B236CB33A3E84CF59BFB9417AE7E8EDE57A7FF,\n        0x9508B9FDD7DAF0D8126F9E2BC5A35E4C6D800B5B804D7796,\n        0x36F2BF6B21B987C77B53BB801B3435A577E3D493744BFAB0,\n        False,\n    ),\n    (\n        generator_192,\n        int(\n            \"0x1843aba74b0789d4ac6b0b8923848023a644a7b70afa23b1191829bbe4397c\"\n            \"e15b629bf21a8838298653ed0c19222b95fa4f7390d1b4c844d96e645537e0aa\"\n            \"e98afb5c0ac3bd0e4c37f8daaff25556c64e98c319c52687c904c4de7240a1cc\"\n            \"55cd9756b7edaef184e6e23b385726e9ffcba8001b8f574987c1a3fedaaa83ca\"\n            \"6d\",\n            16,\n        ),\n        0x10ECCA1AAD7220B56A62008B35170BFD5E35885C4014A19F,\n        0x04EB61984C6C12ADE3BC47F3C629ECE7AA0A033B9948D686,\n        0x82BFA4E82C0DFE9274169B86694E76CE993FD83B5C60F325,\n        0xA97685676C59A65DBDE002FE9D613431FB183E8006D05633,\n        False,\n    ),\n    (\n        generator_192,\n        int(\n            \"0x5a478f4084ddd1a7fea038aa9732a822106385797d02311aeef4d0264f824f\"\n            \"698df7a48cfb6b578cf3da416bc0799425bb491be5b5ecc37995b85b03420a98\"\n            \"f2c4dc5c31a69a379e9e322fbe706bbcaf0f77175e05cbb4fa162e0da82010a2\"\n            \"78461e3e974d137bc746d1880d6eb02aa95216014b37480d84b87f717bb13f76\"\n            \"e1\",\n            16,\n        ),\n        0x6636653CB5B894CA65C448277B29DA3AD101C4C2300F7C04,\n        0xFDF1CBB3FC3FD6A4F890B59E554544175FA77DBDBEB656C1,\n        0xEAC2DDECDDFB79931A9C3D49C08DE0645C783A24CB365E1C,\n        0x3549FEE3CFA7E5F93BC47D92D8BA100E881A2A93C22F8D50,\n        False,\n    ),\n    (\n        generator_192,\n        int(\n            \"0xc598774259a058fa65212ac57eaa4f52240e629ef4c310722088292d1d4af6\"\n            \"c39b49ce06ba77e4247b20637174d0bd67c9723feb57b5ead232b47ea452d5d7\"\n            \"a089f17c00b8b6767e434a5e16c231ba0efa718a340bf41d67ea2d295812ff1b\"\n            \"9277daacb8bc27b50ea5e6443bcf95ef4e9f5468fe78485236313d53d1c68f6b\"\n            \"a2\",\n            16,\n        ),\n        0xA82BD718D01D354001148CD5F69B9EBF38FF6F21898F8AAA,\n        0xE67CEEDE07FC2EBFAFD62462A51E4B6C6B3D5B537B7CAF3E,\n        0x4D292486C620C3DE20856E57D3BB72FCDE4A73AD26376955,\n        0xA85289591A6081D5728825520E62FF1C64F94235C04C7F95,\n        False,\n    ),\n    (\n        generator_192,\n        int(\n            \"0xca98ed9db081a07b7557f24ced6c7b9891269a95d2026747add9e9eb80638a\"\n            \"961cf9c71a1b9f2c29744180bd4c3d3db60f2243c5c0b7cc8a8d40a3f9a7fc91\"\n            \"0250f2187136ee6413ffc67f1a25e1c4c204fa9635312252ac0e0481d89b6d53\"\n            \"808f0c496ba87631803f6c572c1f61fa049737fdacce4adff757afed4f05beb6\"\n            \"58\",\n            16,\n        ),\n        0x7D3B016B57758B160C4FCA73D48DF07AE3B6B30225126C2F,\n        0x4AF3790D9775742BDE46F8DA876711BE1B65244B2B39E7EC,\n        0x95F778F5F656511A5AB49A5D69DDD0929563C29CBC3A9E62,\n        0x75C87FC358C251B4C83D2DD979FAAD496B539F9F2EE7A289,\n        False,\n    ),\n    (\n        generator_192,\n        int(\n            \"0x31dd9a54c8338bea06b87eca813d555ad1850fac9742ef0bbe40dad400e102\"\n            \"88acc9c11ea7dac79eb16378ebea9490e09536099f1b993e2653cd50240014c9\"\n            \"0a9c987f64545abc6a536b9bd2435eb5e911fdfde2f13be96ea36ad38df4ae9e\"\n            \"a387b29cced599af777338af2794820c9cce43b51d2112380a35802ab7e396c9\"\n            \"7a\",\n            16,\n        ),\n        0x9362F28C4EF96453D8A2F849F21E881CD7566887DA8BEB4A,\n        0xE64D26D8D74C48A024AE85D982EE74CD16046F4EE5333905,\n        0xF3923476A296C88287E8DE914B0B324AD5A963319A4FE73B,\n        0xF0BAEED7624ED00D15244D8BA2AEDE085517DBDEC8AC65F5,\n        True,\n    ),\n    (\n        generator_192,\n        int(\n            \"0xb2b94e4432267c92f9fdb9dc6040c95ffa477652761290d3c7de312283f645\"\n            \"0d89cc4aabe748554dfb6056b2d8e99c7aeaad9cdddebdee9dbc099839562d90\"\n            \"64e68e7bb5f3a6bba0749ca9a538181fc785553a4000785d73cc207922f63e8c\"\n            \"e1112768cb1de7b673aed83a1e4a74592f1268d8e2a4e9e63d414b5d442bd045\"\n            \"6d\",\n            16,\n        ),\n        0xCC6FC032A846AAAC25533EB033522824F94E670FA997ECEF,\n        0xE25463EF77A029ECCDA8B294FD63DD694E38D223D30862F1,\n        0x066B1D07F3A40E679B620EDA7F550842A35C18B80C5EBE06,\n        0xA0B0FB201E8F2DF65E2C4508EF303BDC90D934016F16B2DC,\n        False,\n    ),\n    (\n        generator_192,\n        int(\n            \"0x4366fcadf10d30d086911de30143da6f579527036937007b337f7282460eae\"\n            \"5678b15cccda853193ea5fc4bc0a6b9d7a31128f27e1214988592827520b214e\"\n            \"ed5052f7775b750b0c6b15f145453ba3fee24a085d65287e10509eb5d5f602c4\"\n            \"40341376b95c24e5c4727d4b859bfe1483d20538acdd92c7997fa9c614f0f839\"\n            \"d7\",\n            16,\n        ),\n        0x955C908FE900A996F7E2089BEE2F6376830F76A19135E753,\n        0xBA0C42A91D3847DE4A592A46DC3FDAF45A7CC709B90DE520,\n        0x1F58AD77FC04C782815A1405B0925E72095D906CBF52A668,\n        0xF2E93758B3AF75EDF784F05A6761C9B9A6043C66B845B599,\n        False,\n    ),\n    (\n        generator_192,\n        int(\n            \"0x543f8af57d750e33aa8565e0cae92bfa7a1ff78833093421c2942cadf99866\"\n            \"70a5ff3244c02a8225e790fbf30ea84c74720abf99cfd10d02d34377c3d3b412\"\n            \"69bea763384f372bb786b5846f58932defa68023136cd571863b304886e95e52\"\n            \"e7877f445b9364b3f06f3c28da12707673fecb4b8071de06b6e0a3c87da160ce\"\n            \"f3\",\n            16,\n        ),\n        0x31F7FA05576D78A949B24812D4383107A9A45BB5FCCDD835,\n        0x8DC0EB65994A90F02B5E19BD18B32D61150746C09107E76B,\n        0xBE26D59E4E883DDE7C286614A767B31E49AD88789D3A78FF,\n        0x8762CA831C1CE42DF77893C9B03119428E7A9B819B619068,\n        False,\n    ),\n    (\n        generator_192,\n        int(\n            \"0xd2e8454143ce281e609a9d748014dcebb9d0bc53adb02443a6aac2ffe6cb009f\"\n            \"387c346ecb051791404f79e902ee333ad65e5c8cb38dc0d1d39a8dc90add502357\"\n            \"2720e5b94b190d43dd0d7873397504c0c7aef2727e628eb6a74411f2e400c65670\"\n            \"716cb4a815dc91cbbfeb7cfe8c929e93184c938af2c078584da045e8f8d1\",\n            16,\n        ),\n        0x66AA8EDBBDB5CF8E28CEB51B5BDA891CAE2DF84819FE25C0,\n        0x0C6BC2F69030A7CE58D4A00E3B3349844784A13B8936F8DA,\n        0xA4661E69B1734F4A71B788410A464B71E7FFE42334484F23,\n        0x738421CF5E049159D69C57A915143E226CAC8355E149AFE9,\n        False,\n    ),\n    (\n        generator_192,\n        int(\n            \"0x6660717144040f3e2f95a4e25b08a7079c702a8b29babad5a19a87654bc5c5af\"\n            \"a261512a11b998a4fb36b5d8fe8bd942792ff0324b108120de86d63f65855e5461\"\n            \"184fc96a0a8ffd2ce6d5dfb0230cbbdd98f8543e361b3205f5da3d500fdc8bac6d\"\n            \"b377d75ebef3cb8f4d1ff738071ad0938917889250b41dd1d98896ca06fb\",\n            16,\n        ),\n        0xBCFACF45139B6F5F690A4C35A5FFFA498794136A2353FC77,\n        0x6F4A6C906316A6AFC6D98FE1F0399D056F128FE0270B0F22,\n        0x9DB679A3DAFE48F7CCAD122933ACFE9DA0970B71C94C21C1,\n        0x984C2DB99827576C0A41A5DA41E07D8CC768BC82F18C9DA9,\n        False,\n    ),\n]\n\n\n@pytest.mark.parametrize(\"gen,msg,qx,qy,r,s,expected\", CURVE_192_KATS)\ndef test_signature_validity(gen, msg, qx, qy, r, s, expected):\n    \"\"\"\n    `msg` = message, `qx` and `qy` represent the base point on\n    elliptic curve of `gen`, `r` and `s` are the signature, and\n    `expected` is True iff the signature is expected to be valid.\"\"\"\n    pubk = Public_key(gen, ellipticcurve.Point(gen.curve(), qx, qy))\n    assert expected == pubk.verifies(digest_integer(msg), Signature(r, s))\n\n\n@pytest.mark.parametrize(\n    \"gen,msg,qx,qy,r,s,expected\", [x for x in CURVE_192_KATS if x[6]]\n)\ndef test_pk_recovery(gen, msg, r, s, qx, qy, expected):\n    del expected\n    sign = Signature(r, s)\n    pks = sign.recover_public_keys(digest_integer(msg), gen)\n\n    assert pks\n\n    # Test if the signature is valid for all found public keys\n    for pk in pks:\n        q = pk.point\n        test_signature_validity(gen, msg, q.x(), q.y(), r, s, True)\n\n    # Test if the original public key is in the set of found keys\n    original_q = ellipticcurve.Point(gen.curve(), qx, qy)\n    points = [pk.point for pk in pks]\n    assert original_q in points\n\n\n@st.composite\ndef st_random_gen_key_msg_nonce(draw):\n    \"\"\"Hypothesis strategy for test_sig_verify().\"\"\"\n    name_gen = {\n        \"generator_192\": generator_192,\n        \"generator_224\": generator_224,\n        \"generator_256\": generator_256,\n        \"generator_secp256k1\": generator_secp256k1,\n        \"generator_384\": generator_384,\n        \"generator_521\": generator_521,\n    }\n    name = draw(st.sampled_from(sorted(name_gen.keys())))\n    note(\"Generator used: {0}\".format(name))\n    generator = name_gen[name]\n    order = int(generator.order())\n\n    key = draw(st.integers(min_value=1, max_value=order))\n    msg = draw(st.integers(min_value=1, max_value=order))\n    nonce = draw(\n        st.integers(min_value=1, max_value=order + 1)\n        | st.integers(min_value=order >> 1, max_value=order)\n    )\n    return generator, key, msg, nonce\n\n\nSIG_VER_SETTINGS = dict(HYP_SETTINGS)\nSIG_VER_SETTINGS[\"max_examples\"] = 10\n\n\n@settings(**SIG_VER_SETTINGS)\n@example((generator_224, 4, 1, 1))\n@given(st_random_gen_key_msg_nonce())\ndef test_sig_verify(args):\n    \"\"\"\n    Check if signing and verification works for arbitrary messages and\n    that signatures for other messages are rejected.\n    \"\"\"\n    generator, sec_mult, msg, nonce = args\n\n    pubkey = Public_key(generator, generator * sec_mult)\n    privkey = Private_key(pubkey, sec_mult)\n\n    signature = privkey.sign(msg, nonce)\n\n    assert pubkey.verifies(msg, signature)\n\n    assert not pubkey.verifies(msg - 1, signature)\n\n\ndef test_int_to_string_with_zero():\n    assert int_to_string(0) == b\"\\x00\"\n"}
{"type": "source_file", "path": "core/otp.py", "content": "import pyotp\n\nclass OTP:\n    @staticmethod\n    def verify_otp(secret, otp):\n        totp = pyotp.TOTP(secret)\n        return totp.verify(otp)"}
{"type": "source_file", "path": "database/repository/login.py", "content": "import email\nfrom db.models.users import User\nfrom sqlalchemy.orm import Session\n\n\ndef get_user(email: str, db: Session):\n    user = db.query(User).filter(User.email == email).first()\n    return user"}
{"type": "source_file", "path": "database/base_class.py", "content": "from typing import Any\nfrom sqlalchemy.ext.declarative import as_declarative\nfrom sqlalchemy.ext.declarative import declared_attr\n\n@as_declarative()\nclass Base:\n    id: Any\n    __name__: str\n\n    @declared_attr\n    def __tablename__(cls) -> str:\n        return cls.__name__.lower()"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiofiles/tempfile/__init__.py", "content": "# Imports\r\nimport asyncio\r\nfrom tempfile import (\r\n    TemporaryFile as syncTemporaryFile,\r\n    NamedTemporaryFile as syncNamedTemporaryFile,\r\n    SpooledTemporaryFile as syncSpooledTemporaryFile,\r\n    TemporaryDirectory as syncTemporaryDirectory,\r\n    _TemporaryFileWrapper as syncTemporaryFileWrapper,\r\n)\r\nfrom io import FileIO, TextIOBase, BufferedReader, BufferedWriter, BufferedRandom\r\nfrom functools import partial, singledispatch\r\nfrom ..base import AiofilesContextManager\r\nfrom ..threadpool.text import AsyncTextIOWrapper\r\nfrom ..threadpool.binary import AsyncBufferedIOBase, AsyncBufferedReader, AsyncFileIO\r\nfrom .temptypes import AsyncSpooledTemporaryFile, AsyncTemporaryDirectory\r\n\r\n__all__ = [\r\n    \"NamedTemporaryFile\",\r\n    \"TemporaryFile\",\r\n    \"SpooledTemporaryFile\",\r\n    \"TemporaryDirectory\",\r\n]\r\n\r\n\r\n# ================================================================\r\n# Public methods for async open and return of temp file/directory\r\n# objects with async interface\r\n# ================================================================\r\ndef NamedTemporaryFile(\r\n    mode=\"w+b\",\r\n    buffering=-1,\r\n    encoding=None,\r\n    newline=None,\r\n    suffix=None,\r\n    prefix=None,\r\n    dir=None,\r\n    delete=True,\r\n    loop=None,\r\n    executor=None,\r\n):\r\n    \"\"\"Async open a named temporary file\"\"\"\r\n    return AiofilesContextManager(\r\n        _temporary_file(\r\n            named=True,\r\n            mode=mode,\r\n            buffering=buffering,\r\n            encoding=encoding,\r\n            newline=newline,\r\n            suffix=suffix,\r\n            prefix=prefix,\r\n            dir=dir,\r\n            delete=delete,\r\n            loop=loop,\r\n            executor=executor,\r\n        )\r\n    )\r\n\r\n\r\ndef TemporaryFile(\r\n    mode=\"w+b\",\r\n    buffering=-1,\r\n    encoding=None,\r\n    newline=None,\r\n    suffix=None,\r\n    prefix=None,\r\n    dir=None,\r\n    loop=None,\r\n    executor=None,\r\n):\r\n    \"\"\"Async open an unnamed temporary file\"\"\"\r\n    return AiofilesContextManager(\r\n        _temporary_file(\r\n            named=False,\r\n            mode=mode,\r\n            buffering=buffering,\r\n            encoding=encoding,\r\n            newline=newline,\r\n            suffix=suffix,\r\n            prefix=prefix,\r\n            dir=dir,\r\n            loop=loop,\r\n            executor=executor,\r\n        )\r\n    )\r\n\r\n\r\ndef SpooledTemporaryFile(\r\n    max_size=0,\r\n    mode=\"w+b\",\r\n    buffering=-1,\r\n    encoding=None,\r\n    newline=None,\r\n    suffix=None,\r\n    prefix=None,\r\n    dir=None,\r\n    loop=None,\r\n    executor=None,\r\n):\r\n    \"\"\"Async open a spooled temporary file\"\"\"\r\n    return AiofilesContextManager(\r\n        _spooled_temporary_file(\r\n            max_size=max_size,\r\n            mode=mode,\r\n            buffering=buffering,\r\n            encoding=encoding,\r\n            newline=newline,\r\n            suffix=suffix,\r\n            prefix=prefix,\r\n            dir=dir,\r\n            loop=loop,\r\n            executor=executor,\r\n        )\r\n    )\r\n\r\n\r\ndef TemporaryDirectory(suffix=None, prefix=None, dir=None, loop=None, executor=None):\r\n    \"\"\"Async open a temporary directory\"\"\"\r\n    return AiofilesContextManagerTempDir(\r\n        _temporary_directory(\r\n            suffix=suffix, prefix=prefix, dir=dir, loop=loop, executor=executor\r\n        )\r\n    )\r\n\r\n\r\n# =========================================================\r\n# Internal coroutines to open new temp files/directories\r\n# =========================================================\r\nasync def _temporary_file(\r\n    named=True,\r\n    mode=\"w+b\",\r\n    buffering=-1,\r\n    encoding=None,\r\n    newline=None,\r\n    suffix=None,\r\n    prefix=None,\r\n    dir=None,\r\n    delete=True,\r\n    loop=None,\r\n    executor=None,\r\n    max_size=0,\r\n):\r\n    \"\"\"Async method to open a temporary file with async interface\"\"\"\r\n    if loop is None:\r\n        loop = asyncio.get_event_loop()\r\n\r\n    if named:\r\n        cb = partial(\r\n            syncNamedTemporaryFile,\r\n            mode=mode,\r\n            buffering=buffering,\r\n            encoding=encoding,\r\n            newline=newline,\r\n            suffix=suffix,\r\n            prefix=prefix,\r\n            dir=dir,\r\n            delete=delete,\r\n        )\r\n    else:\r\n        cb = partial(\r\n            syncTemporaryFile,\r\n            mode=mode,\r\n            buffering=buffering,\r\n            encoding=encoding,\r\n            newline=newline,\r\n            suffix=suffix,\r\n            prefix=prefix,\r\n            dir=dir,\r\n        )\r\n\r\n    f = await loop.run_in_executor(executor, cb)\r\n\r\n    # Wrap based on type of underlying IO object\r\n    if type(f) is syncTemporaryFileWrapper:\r\n        # _TemporaryFileWrapper was used (named files)\r\n        result = wrap(f.file, f, loop=loop, executor=executor)\r\n        # add delete property\r\n        result.delete = f.delete\r\n        return result\r\n    else:\r\n        # IO object was returned directly without wrapper\r\n        return wrap(f, f, loop=loop, executor=executor)\r\n\r\n\r\nasync def _spooled_temporary_file(\r\n    max_size=0,\r\n    mode=\"w+b\",\r\n    buffering=-1,\r\n    encoding=None,\r\n    newline=None,\r\n    suffix=None,\r\n    prefix=None,\r\n    dir=None,\r\n    loop=None,\r\n    executor=None,\r\n):\r\n    \"\"\"Open a spooled temporary file with async interface\"\"\"\r\n    if loop is None:\r\n        loop = asyncio.get_event_loop()\r\n\r\n    cb = partial(\r\n        syncSpooledTemporaryFile,\r\n        max_size=max_size,\r\n        mode=mode,\r\n        buffering=buffering,\r\n        encoding=encoding,\r\n        newline=newline,\r\n        suffix=suffix,\r\n        prefix=prefix,\r\n        dir=dir,\r\n    )\r\n\r\n    f = await loop.run_in_executor(executor, cb)\r\n\r\n    # Single interface provided by SpooledTemporaryFile for all modes\r\n    return AsyncSpooledTemporaryFile(f, loop=loop, executor=executor)\r\n\r\n\r\nasync def _temporary_directory(\r\n    suffix=None, prefix=None, dir=None, loop=None, executor=None\r\n):\r\n    \"\"\"Async method to open a temporary directory with async interface\"\"\"\r\n    if loop is None:\r\n        loop = asyncio.get_event_loop()\r\n\r\n    cb = partial(syncTemporaryDirectory, suffix, prefix, dir)\r\n    f = await loop.run_in_executor(executor, cb)\r\n\r\n    return AsyncTemporaryDirectory(f, loop=loop, executor=executor)\r\n\r\n\r\nclass AiofilesContextManagerTempDir(AiofilesContextManager):\r\n    \"\"\"With returns the directory location, not the object (matching sync lib)\"\"\"\r\n\r\n    async def __aenter__(self):\r\n        self._obj = await self._coro\r\n        return self._obj.name\r\n\r\n\r\n@singledispatch\r\ndef wrap(base_io_obj, file, *, loop=None, executor=None):\r\n    \"\"\"Wrap the object with interface based on type of underlying IO\"\"\"\r\n    raise TypeError(\"Unsupported IO type: {}\".format(base_io_obj))\r\n\r\n\r\n@wrap.register(TextIOBase)\r\ndef _(base_io_obj, file, *, loop=None, executor=None):\r\n    return AsyncTextIOWrapper(file, loop=loop, executor=executor)\r\n\r\n\r\n@wrap.register(BufferedWriter)\r\ndef _(base_io_obj, file, *, loop=None, executor=None):\r\n    return AsyncBufferedIOBase(file, loop=loop, executor=executor)\r\n\r\n\r\n@wrap.register(BufferedReader)\r\n@wrap.register(BufferedRandom)\r\ndef _(base_io_obj, file, *, loop=None, executor=None):\r\n    return AsyncBufferedReader(file, loop=loop, executor=executor)\r\n\r\n\r\n@wrap.register(FileIO)\r\ndef _(base_io_obj, file, *, loop=None, executor=None):\r\n    return AsyncFileIO(file, loop=loop, executor=executor)\r\n"}
{"type": "source_file", "path": "database/repository/users.py", "content": "from core.hashing import Hasher\nfrom database.models.users import User\nfrom schemas.users import UserCreate\nfrom sqlalchemy.orm import Session\n\n\ndef create_new_user(user: UserCreate, db: Session):\n    user = User(\n        email=user.email,\n        hashed_password=Hasher.get_password_hash(user.password),\n        secret = user.secret\n    )\n    db.add(user)\n    db.commit()\n    db.refresh(user)\n    return user\n\n\ndef get_user_by_email(email: str, db: Session):\n    user = db.query(User).filter(User.email == email).first()\n    return user"}
{"type": "source_file", "path": "database/session.py", "content": "from typing import Generator\n\nfrom core.config import settings\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\n\nSQLALCHEMY_DATABASE_URL = \"sqlite:///./two_fa_app.db\"\nengine = create_engine(\n     SQLALCHEMY_DATABASE_URL, connect_args={\"check_same_thread\": False}\n)\n\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\n\ndef get_db() -> Generator:\n    try:\n        db = SessionLocal()\n        yield db\n    finally:\n        db.close()"}
{"type": "source_file", "path": "database/base.py", "content": "from database.base_class import Base\nfrom database.models.users import User"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiofiles/base.py", "content": "\"\"\"Various base classes.\"\"\"\nfrom types import coroutine\nfrom collections.abc import Coroutine\n\n\nclass AsyncBase:\n    def __init__(self, file, loop, executor):\n        self._file = file\n        self._loop = loop\n        self._executor = executor\n\n    def __aiter__(self):\n        \"\"\"We are our own iterator.\"\"\"\n        return self\n\n    def __repr__(self):\n        return super().__repr__() + \" wrapping \" + repr(self._file)\n\n    async def __anext__(self):\n        \"\"\"Simulate normal file iteration.\"\"\"\n        line = await self.readline()\n        if line:\n            return line\n        else:\n            raise StopAsyncIteration\n\n\nclass _ContextManager(Coroutine):\n    __slots__ = (\"_coro\", \"_obj\")\n\n    def __init__(self, coro):\n        self._coro = coro\n        self._obj = None\n\n    def send(self, value):\n        return self._coro.send(value)\n\n    def throw(self, typ, val=None, tb=None):\n        if val is None:\n            return self._coro.throw(typ)\n        elif tb is None:\n            return self._coro.throw(typ, val)\n        else:\n            return self._coro.throw(typ, val, tb)\n\n    def close(self):\n        return self._coro.close()\n\n    @property\n    def gi_frame(self):\n        return self._coro.gi_frame\n\n    @property\n    def gi_running(self):\n        return self._coro.gi_running\n\n    @property\n    def gi_code(self):\n        return self._coro.gi_code\n\n    def __next__(self):\n        return self.send(None)\n\n    @coroutine\n    def __iter__(self):\n        resp = yield from self._coro\n        return resp\n\n    def __await__(self):\n        resp = yield from self._coro\n        return resp\n\n    async def __anext__(self):\n        resp = await self._coro\n        return resp\n\n    async def __aenter__(self):\n        self._obj = await self._coro\n        return self._obj\n\n    async def __aexit__(self, exc_type, exc, tb):\n        self._obj.close()\n        self._obj = None\n\n\nclass AiofilesContextManager(_ContextManager):\n    \"\"\"An adjusted async context manager for aiofiles.\"\"\"\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self._obj.close()\n        self._obj = None\n"}
{"type": "source_file", "path": "core/hashing.py", "content": "from passlib.context import CryptContext\n\npwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\n\nclass Hasher:\n    @staticmethod\n    def verify_password(plain_password, hashed_password):\n        return pwd_context.verify(plain_password, hashed_password)\n\n    @staticmethod\n    def get_password_hash(password):\n        return pwd_context.hash(password)"}
{"type": "source_file", "path": "database/models/users.py", "content": "from database.base_class import Base\nfrom sqlalchemy import Column\nfrom sqlalchemy import Integer\nfrom sqlalchemy import String\n\nclass User(Base):\n    id = Column(Integer, primary_key=True, index=True)\n    email = Column(String, unique=True, nullable=False)\n    hashed_password = Column(String, nullable=False)\n    secret = Column(String, nullable=False)"}
{"type": "source_file", "path": "database/utils.py", "content": "import databases\nfrom database.session import SQLALCHEMY_DATABASE_URL\n\n\nasync def check_db_connected():\n    try:\n        if not str(SQLALCHEMY_DATABASE_URL).__contains__(\"sqlite\"):\n            database = databases.Database(SQLALCHEMY_DATABASE_URL)\n            if not database.is_connected:\n                await database.connect()\n                await database.execute(\"SELECT 1\")\n        print(\"Database is connected\", \"\\N{smiling face with smiling eyes}\")\n    except Exception as e:\n        print(\n            \"Looks like db is missing or is there is some problem in connection,see below traceback\"\n        )\n        raise e\n\n\nasync def check_db_disconnected():\n    try:\n        if not str(SQLALCHEMY_DATABASE_URL).__contains__(\"sqlite\"):\n            database = databases.Database(SQLALCHEMY_DATABASE_URL)\n            if database.is_connected:\n                await database.disconnect()\n        print(\"Database is Disconnected\",\"\\N{sleeping face}\")\n    except Exception as e:\n        raise e"}
{"type": "source_file", "path": "core/config.py", "content": "\nimport os\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\n\nenv_path = Path(\".\") / \".env\"\nload_dotenv(dotenv_path=env_path)\n\n\nclass Settings:\n    PROJECT_NAME: str = \"FastAPI-2FA\"\n    PROJECT_VERSION: str = \"1.0.0\"\n\n    \nsettings = Settings()"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiofiles/__init__.py", "content": "\"\"\"Utilities for asyncio-friendly file handling.\"\"\"\nfrom .threadpool import open\nfrom . import tempfile\n\n__all__ = [\"open\", \"tempfile\"]\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/_yaml/__init__.py", "content": "# This is a stub package designed to roughly emulate the _yaml\n# extension module, which previously existed as a standalone module\n# and has been moved into the `yaml` package namespace.\n# It does not perfectly mimic its old counterpart, but should get\n# close enough for anyone who's relying on it even when they shouldn't.\nimport yaml\n\n# in some circumstances, the yaml module we imoprted may be from a different version, so we need\n# to tread carefully when poking at it here (it may not have the attributes we expect)\nif not getattr(yaml, '__with_libyaml__', False):\n    from sys import version_info\n\n    exc = ModuleNotFoundError if version_info >= (3, 6) else ImportError\n    raise exc(\"No module named '_yaml'\")\nelse:\n    from yaml._yaml import *\n    import warnings\n    warnings.warn(\n        'The _yaml extension module is now located at yaml._yaml'\n        ' and its location is subject to change.  To use the'\n        ' LibYAML-based parser and emitter, import from `yaml`:'\n        ' `from yaml import CLoader as Loader, CDumper as Dumper`.',\n        DeprecationWarning\n    )\n    del warnings\n    # Don't `del yaml` here because yaml is actually an existing\n    # namespace member of _yaml.\n\n__name__ = '_yaml'\n# If the module is top-level (i.e. not a part of any specific package)\n# then the attribute should be set to ''.\n# https://docs.python.org/3.8/library/types.html\n__package__ = ''\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/_distutils_hack/__init__.py", "content": "import sys\nimport os\nimport re\nimport importlib\nimport warnings\n\n\nis_pypy = '__pypy__' in sys.builtin_module_names\n\n\ndef warn_distutils_present():\n    if 'distutils' not in sys.modules:\n        return\n    if is_pypy and sys.version_info < (3, 7):\n        # PyPy for 3.6 unconditionally imports distutils, so bypass the warning\n        # https://foss.heptapod.net/pypy/pypy/-/blob/be829135bc0d758997b3566062999ee8b23872b4/lib-python/3/site.py#L250\n        return\n    warnings.warn(\n        \"Distutils was imported before Setuptools, but importing Setuptools \"\n        \"also replaces the `distutils` module in `sys.modules`. This may lead \"\n        \"to undesirable behaviors or errors. To avoid these issues, avoid \"\n        \"using distutils directly, ensure that setuptools is installed in the \"\n        \"traditional way (e.g. not an editable install), and/or make sure \"\n        \"that setuptools is always imported before distutils.\")\n\n\ndef clear_distutils():\n    if 'distutils' not in sys.modules:\n        return\n    warnings.warn(\"Setuptools is replacing distutils.\")\n    mods = [name for name in sys.modules if re.match(r'distutils\\b', name)]\n    for name in mods:\n        del sys.modules[name]\n\n\ndef enabled():\n    \"\"\"\n    Allow selection of distutils by environment variable.\n    \"\"\"\n    which = os.environ.get('SETUPTOOLS_USE_DISTUTILS', 'stdlib')\n    return which == 'local'\n\n\ndef ensure_local_distutils():\n    clear_distutils()\n    distutils = importlib.import_module('setuptools._distutils')\n    distutils.__name__ = 'distutils'\n    sys.modules['distutils'] = distutils\n\n    # sanity check that submodules load as expected\n    core = importlib.import_module('distutils.core')\n    assert '_distutils' in core.__file__, core.__file__\n\n\ndef do_override():\n    \"\"\"\n    Ensure that the local copy of distutils is preferred over stdlib.\n\n    See https://github.com/pypa/setuptools/issues/417#issuecomment-392298401\n    for more motivation.\n    \"\"\"\n    if enabled():\n        warn_distutils_present()\n        ensure_local_distutils()\n\n\nclass DistutilsMetaFinder:\n    def find_spec(self, fullname, path, target=None):\n        if path is not None:\n            return\n\n        method_name = 'spec_for_{fullname}'.format(**locals())\n        method = getattr(self, method_name, lambda: None)\n        return method()\n\n    def spec_for_distutils(self):\n        import importlib.abc\n        import importlib.util\n\n        class DistutilsLoader(importlib.abc.Loader):\n\n            def create_module(self, spec):\n                return importlib.import_module('setuptools._distutils')\n\n            def exec_module(self, module):\n                pass\n\n        return importlib.util.spec_from_loader('distutils', DistutilsLoader())\n\n    def spec_for_pip(self):\n        \"\"\"\n        Ensure stdlib distutils when running under pip.\n        See pypa/pip#8761 for rationale.\n        \"\"\"\n        if self.pip_imported_during_build():\n            return\n        clear_distutils()\n        self.spec_for_distutils = lambda: None\n\n    @staticmethod\n    def pip_imported_during_build():\n        \"\"\"\n        Detect if pip is being imported in a build script. Ref #2355.\n        \"\"\"\n        import traceback\n        return any(\n            frame.f_globals['__file__'].endswith('setup.py')\n            for frame, line in traceback.walk_stack(None)\n        )\n\n\nDISTUTILS_FINDER = DistutilsMetaFinder()\n\n\ndef add_shim():\n    sys.meta_path.insert(0, DISTUTILS_FINDER)\n\n\ndef remove_shim():\n    try:\n        sys.meta_path.remove(DISTUTILS_FINDER)\n    except ValueError:\n        pass\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiofiles/threadpool/utils.py", "content": "import functools\nfrom types import coroutine\n\n\ndef delegate_to_executor(*attrs):\n    def cls_builder(cls):\n        for attr_name in attrs:\n            setattr(cls, attr_name, _make_delegate_method(attr_name))\n        return cls\n\n    return cls_builder\n\n\ndef proxy_method_directly(*attrs):\n    def cls_builder(cls):\n        for attr_name in attrs:\n            setattr(cls, attr_name, _make_proxy_method(attr_name))\n        return cls\n\n    return cls_builder\n\n\ndef proxy_property_directly(*attrs):\n    def cls_builder(cls):\n        for attr_name in attrs:\n            setattr(cls, attr_name, _make_proxy_property(attr_name))\n        return cls\n\n    return cls_builder\n\n\ndef cond_delegate_to_executor(*attrs):\n    def cls_builder(cls):\n        for attr_name in attrs:\n            setattr(cls, attr_name, _make_cond_delegate_method(attr_name))\n        return cls\n\n    return cls_builder\n\n\ndef _make_delegate_method(attr_name):\n    @coroutine\n    def method(self, *args, **kwargs):\n        cb = functools.partial(getattr(self._file, attr_name), *args, **kwargs)\n        return (yield from self._loop.run_in_executor(self._executor, cb))\n\n    return method\n\n\ndef _make_proxy_method(attr_name):\n    def method(self, *args, **kwargs):\n        return getattr(self._file, attr_name)(*args, **kwargs)\n\n    return method\n\n\ndef _make_proxy_property(attr_name):\n    def proxy_property(self):\n        return getattr(self._file, attr_name)\n\n    return property(proxy_property)\n\n\ndef _make_cond_delegate_method(attr_name):\n    \"\"\"For spooled temp files, delegate only if rolled to file object\"\"\"\n\n    async def method(self, *args, **kwargs):\n        if self._file._rolled:\n            cb = functools.partial(getattr(self._file, attr_name), *args, **kwargs)\n            return await self._loop.run_in_executor(self._executor, cb)\n        else:\n            return getattr(self._file, attr_name)(*args, **kwargs)\n\n    return method\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/_distutils_hack/override.py", "content": "__import__('_distutils_hack').do_override()\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiofiles/os.py", "content": "\"\"\"Async executor versions of file functions from the os module.\"\"\"\nimport asyncio\nfrom functools import partial, wraps\nimport os\n\n\ndef wrap(func):\n    @wraps(func)\n    async def run(*args, loop=None, executor=None, **kwargs):\n        if loop is None:\n            loop = asyncio.get_event_loop()\n        pfunc = partial(func, *args, **kwargs)\n        return await loop.run_in_executor(executor, pfunc)\n\n    return run\n\n\nfrom . import ospath as path\n\n\nstat = wrap(os.stat)\nrename = wrap(os.rename)\nreplace = wrap(os.replace)\nremove = wrap(os.remove)\nmkdir = wrap(os.mkdir)\nmakedirs = wrap(os.makedirs)\nrmdir = wrap(os.rmdir)\nremovedirs = wrap(os.removedirs)\n\nif hasattr(os, \"sendfile\"):\n    sendfile = wrap(os.sendfile)\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiosqlite/__init__.py", "content": "# Copyright 2018 John Reese\n# Licensed under the MIT license\n\n\"\"\"asyncio bridge to the standard sqlite3 module\"\"\"\n\nfrom sqlite3 import (  # pylint: disable=redefined-builtin\n    DatabaseError,\n    Error,\n    IntegrityError,\n    NotSupportedError,\n    OperationalError,\n    ProgrammingError,\n    Row,\n    Warning,\n    register_adapter,\n    register_converter,\n    sqlite_version,\n    sqlite_version_info,\n)\n\n__author__ = \"John Reese\"\nfrom .__version__ import __version__\nfrom .core import Connection, Cursor, connect\n\n__all__ = [\n    \"__version__\",\n    \"register_adapter\",\n    \"register_converter\",\n    \"sqlite_version\",\n    \"sqlite_version_info\",\n    \"connect\",\n    \"Connection\",\n    \"Cursor\",\n    \"Row\",\n    \"Warning\",\n    \"Error\",\n    \"DatabaseError\",\n    \"IntegrityError\",\n    \"ProgrammingError\",\n    \"OperationalError\",\n    \"NotSupportedError\",\n]\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_core/_resources.py", "content": "from ..abc import AsyncResource\nfrom ._tasks import CancelScope\n\n\nasync def aclose_forcefully(resource: AsyncResource) -> None:\n    \"\"\"\n    Close an asynchronous resource in a cancelled scope.\n\n    Doing this closes the resource without waiting on anything.\n\n    :param resource: the resource to close\n\n    \"\"\"\n    with CancelScope() as scope:\n        scope.cancel()\n        await resource.aclose()\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiosqlite/__version__.py", "content": "__version__ = \"0.17.0\"\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiosqlite/cursor.py", "content": "# Copyright 2018 John Reese\n# Licensed under the MIT license\n\nimport sqlite3\nfrom typing import TYPE_CHECKING, Any, AsyncIterator, Iterable, Optional, Tuple\n\nif TYPE_CHECKING:\n    from .core import Connection\n\n\nclass Cursor:\n    def __init__(self, conn: \"Connection\", cursor: sqlite3.Cursor) -> None:\n        self.iter_chunk_size = conn._iter_chunk_size\n        self._conn = conn\n        self._cursor = cursor\n\n    def __aiter__(self) -> AsyncIterator[sqlite3.Row]:\n        \"\"\"The cursor proxy is also an async iterator.\"\"\"\n        return self._fetch_chunked()\n\n    async def _fetch_chunked(self):\n        while True:\n            rows = await self.fetchmany(self.iter_chunk_size)\n            if not rows:\n                return\n            for row in rows:\n                yield row\n\n    async def _execute(self, fn, *args, **kwargs):\n        \"\"\"Execute the given function on the shared connection's thread.\"\"\"\n        return await self._conn._execute(fn, *args, **kwargs)\n\n    async def execute(self, sql: str, parameters: Iterable[Any] = None) -> \"Cursor\":\n        \"\"\"Execute the given query.\"\"\"\n        if parameters is None:\n            parameters = []\n        await self._execute(self._cursor.execute, sql, parameters)\n        return self\n\n    async def executemany(\n        self, sql: str, parameters: Iterable[Iterable[Any]]\n    ) -> \"Cursor\":\n        \"\"\"Execute the given multiquery.\"\"\"\n        await self._execute(self._cursor.executemany, sql, parameters)\n        return self\n\n    async def executescript(self, sql_script: str) -> \"Cursor\":\n        \"\"\"Execute a user script.\"\"\"\n        await self._execute(self._cursor.executescript, sql_script)\n        return self\n\n    async def fetchone(self) -> Optional[sqlite3.Row]:\n        \"\"\"Fetch a single row.\"\"\"\n        return await self._execute(self._cursor.fetchone)\n\n    async def fetchmany(self, size: int = None) -> Iterable[sqlite3.Row]:\n        \"\"\"Fetch up to `cursor.arraysize` number of rows.\"\"\"\n        args: Tuple[int, ...] = ()\n        if size is not None:\n            args = (size,)\n        return await self._execute(self._cursor.fetchmany, *args)\n\n    async def fetchall(self) -> Iterable[sqlite3.Row]:\n        \"\"\"Fetch all remaining rows.\"\"\"\n        return await self._execute(self._cursor.fetchall)\n\n    async def close(self) -> None:\n        \"\"\"Close the cursor.\"\"\"\n        await self._execute(self._cursor.close)\n\n    @property\n    def rowcount(self) -> int:\n        return self._cursor.rowcount\n\n    @property\n    def lastrowid(self) -> int:\n        return self._cursor.lastrowid\n\n    @property\n    def arraysize(self) -> int:\n        return self._cursor.arraysize\n\n    @arraysize.setter\n    def arraysize(self, value: int) -> None:\n        self._cursor.arraysize = value\n\n    @property\n    def description(self) -> Tuple[Tuple]:\n        return self._cursor.description\n\n    @property\n    def connection(self) -> sqlite3.Connection:\n        return self._cursor.connection\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.close()\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_core/_signals.py", "content": "from typing import AsyncIterator\n\nfrom ._compat import DeprecatedAsyncContextManager\nfrom ._eventloop import get_asynclib\n\n\ndef open_signal_receiver(*signals: int) -> DeprecatedAsyncContextManager[AsyncIterator[int]]:\n    \"\"\"\n    Start receiving operating system signals.\n\n    :param signals: signals to receive (e.g. ``signal.SIGINT``)\n    :return: an asynchronous context manager for an asynchronous iterator which yields signal\n        numbers\n\n    .. warning:: Windows does not support signals natively so it is best to avoid relying on this\n        in cross-platform applications.\n\n    .. warning:: On asyncio, this permanently replaces any previous signal handler for the given\n        signals, as set via :meth:`~asyncio.loop.add_signal_handler`.\n\n    \"\"\"\n    return get_asynclib().open_signal_receiver(*signals)\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiofiles/threadpool/binary.py", "content": "from ..base import AsyncBase\nfrom .utils import (\n    delegate_to_executor,\n    proxy_method_directly,\n    proxy_property_directly,\n)\n\n\n@delegate_to_executor(\n    \"close\",\n    \"flush\",\n    \"isatty\",\n    \"read\",\n    \"read1\",\n    \"readinto\",\n    \"readline\",\n    \"readlines\",\n    \"seek\",\n    \"seekable\",\n    \"tell\",\n    \"truncate\",\n    \"writable\",\n    \"write\",\n    \"writelines\",\n)\n@proxy_method_directly(\"detach\", \"fileno\", \"readable\")\n@proxy_property_directly(\"closed\", \"raw\", \"name\", \"mode\")\nclass AsyncBufferedIOBase(AsyncBase):\n    \"\"\"The asyncio executor version of io.BufferedWriter.\"\"\"\n\n\n@delegate_to_executor(\"peek\")\nclass AsyncBufferedReader(AsyncBufferedIOBase):\n    \"\"\"The asyncio executor version of io.BufferedReader and Random.\"\"\"\n\n\n@delegate_to_executor(\n    \"close\",\n    \"flush\",\n    \"isatty\",\n    \"read\",\n    \"readall\",\n    \"readinto\",\n    \"readline\",\n    \"readlines\",\n    \"seek\",\n    \"seekable\",\n    \"tell\",\n    \"truncate\",\n    \"writable\",\n    \"write\",\n    \"writelines\",\n)\n@proxy_method_directly(\"fileno\", \"readable\")\n@proxy_property_directly(\"closed\", \"name\", \"mode\")\nclass AsyncFileIO(AsyncBase):\n    \"\"\"The asyncio executor version of io.FileIO.\"\"\"\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/__init__.py", "content": "__all__ = (\n    'maybe_async',\n    'maybe_async_cm',\n    'run',\n    'sleep',\n    'sleep_forever',\n    'sleep_until',\n    'current_time',\n    'get_all_backends',\n    'get_cancelled_exc_class',\n    'BrokenResourceError',\n    'BrokenWorkerProcess',\n    'BusyResourceError',\n    'ClosedResourceError',\n    'DelimiterNotFound',\n    'EndOfStream',\n    'ExceptionGroup',\n    'IncompleteRead',\n    'TypedAttributeLookupError',\n    'WouldBlock',\n    'AsyncFile',\n    'Path',\n    'open_file',\n    'wrap_file',\n    'aclose_forcefully',\n    'open_signal_receiver',\n    'connect_tcp',\n    'connect_unix',\n    'create_tcp_listener',\n    'create_unix_listener',\n    'create_udp_socket',\n    'create_connected_udp_socket',\n    'getaddrinfo',\n    'getnameinfo',\n    'wait_socket_readable',\n    'wait_socket_writable',\n    'create_memory_object_stream',\n    'run_process',\n    'open_process',\n    'create_lock',\n    'CapacityLimiter',\n    'CapacityLimiterStatistics',\n    'Condition',\n    'ConditionStatistics',\n    'Event',\n    'EventStatistics',\n    'Lock',\n    'LockStatistics',\n    'Semaphore',\n    'SemaphoreStatistics',\n    'create_condition',\n    'create_event',\n    'create_semaphore',\n    'create_capacity_limiter',\n    'open_cancel_scope',\n    'fail_after',\n    'move_on_after',\n    'current_effective_deadline',\n    'TASK_STATUS_IGNORED',\n    'CancelScope',\n    'create_task_group',\n    'TaskInfo',\n    'get_current_task',\n    'get_running_tasks',\n    'wait_all_tasks_blocked',\n    'run_sync_in_worker_thread',\n    'run_async_from_thread',\n    'run_sync_from_thread',\n    'current_default_worker_thread_limiter',\n    'create_blocking_portal',\n    'start_blocking_portal',\n    'typed_attribute',\n    'TypedAttributeSet',\n    'TypedAttributeProvider'\n)\n\nfrom typing import Any\n\nfrom ._core._compat import maybe_async, maybe_async_cm\nfrom ._core._eventloop import (\n    current_time, get_all_backends, get_cancelled_exc_class, run, sleep, sleep_forever,\n    sleep_until)\nfrom ._core._exceptions import (\n    BrokenResourceError, BrokenWorkerProcess, BusyResourceError, ClosedResourceError,\n    DelimiterNotFound, EndOfStream, ExceptionGroup, IncompleteRead, TypedAttributeLookupError,\n    WouldBlock)\nfrom ._core._fileio import AsyncFile, Path, open_file, wrap_file\nfrom ._core._resources import aclose_forcefully\nfrom ._core._signals import open_signal_receiver\nfrom ._core._sockets import (\n    connect_tcp, connect_unix, create_connected_udp_socket, create_tcp_listener, create_udp_socket,\n    create_unix_listener, getaddrinfo, getnameinfo, wait_socket_readable, wait_socket_writable)\nfrom ._core._streams import create_memory_object_stream\nfrom ._core._subprocesses import open_process, run_process\nfrom ._core._synchronization import (\n    CapacityLimiter, CapacityLimiterStatistics, Condition, ConditionStatistics, Event,\n    EventStatistics, Lock, LockStatistics, Semaphore, SemaphoreStatistics, create_capacity_limiter,\n    create_condition, create_event, create_lock, create_semaphore)\nfrom ._core._tasks import (\n    TASK_STATUS_IGNORED, CancelScope, create_task_group, current_effective_deadline, fail_after,\n    move_on_after, open_cancel_scope)\nfrom ._core._testing import TaskInfo, get_current_task, get_running_tasks, wait_all_tasks_blocked\nfrom ._core._typedattr import TypedAttributeProvider, TypedAttributeSet, typed_attribute\n\n# Re-exported here, for backwards compatibility\n# isort: off\nfrom .to_thread import current_default_worker_thread_limiter, run_sync_in_worker_thread\nfrom .from_thread import (\n    create_blocking_portal, run_async_from_thread, run_sync_from_thread, start_blocking_portal)\n\n# Re-export imports so they look like they live directly in this package\nkey: str\nvalue: Any\nfor key, value in list(locals().items()):\n    if getattr(value, '__module__', '').startswith('anyio.'):\n        value.__module__ = __name__\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_core/_compat.py", "content": "from abc import ABCMeta, abstractmethod\nfrom contextlib import AbstractContextManager\nfrom types import TracebackType\nfrom typing import (\n    TYPE_CHECKING, Any, AsyncContextManager, Callable, ContextManager, Generator, Generic,\n    Iterable, List, Optional, Tuple, Type, TypeVar, Union, overload)\nfrom warnings import warn\n\nif TYPE_CHECKING:\n    from ._testing import TaskInfo\nelse:\n    TaskInfo = object\n\nT = TypeVar('T')\nAnyDeprecatedAwaitable = Union['DeprecatedAwaitable', 'DeprecatedAwaitableFloat',\n                               'DeprecatedAwaitableList[T]', TaskInfo]\n\n\n@overload\nasync def maybe_async(__obj: TaskInfo) -> TaskInfo:\n    ...\n\n\n@overload\nasync def maybe_async(__obj: 'DeprecatedAwaitableFloat') -> float:\n    ...\n\n\n@overload\nasync def maybe_async(__obj: 'DeprecatedAwaitableList[T]') -> List[T]:\n    ...\n\n\n@overload\nasync def maybe_async(__obj: 'DeprecatedAwaitable') -> None:\n    ...\n\n\nasync def maybe_async(__obj: 'AnyDeprecatedAwaitable[T]') -> Union[TaskInfo, float, List[T], None]:\n    \"\"\"\n    Await on the given object if necessary.\n\n    This function is intended to bridge the gap between AnyIO 2.x and 3.x where some functions and\n    methods were converted from coroutine functions into regular functions.\n\n    Do **not** try to use this for any other purpose!\n\n    :return: the result of awaiting on the object if coroutine, or the object itself otherwise\n\n    .. versionadded:: 2.2\n\n    \"\"\"\n    return __obj._unwrap()\n\n\nclass _ContextManagerWrapper:\n    def __init__(self, cm: ContextManager[T]):\n        self._cm = cm\n\n    async def __aenter__(self) -> T:\n        return self._cm.__enter__()\n\n    async def __aexit__(self, exc_type: Optional[Type[BaseException]],\n                        exc_val: Optional[BaseException],\n                        exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        return self._cm.__exit__(exc_type, exc_val, exc_tb)\n\n\ndef maybe_async_cm(cm: Union[ContextManager[T], AsyncContextManager[T]]) -> AsyncContextManager[T]:\n    \"\"\"\n    Wrap a regular context manager as an async one if necessary.\n\n    This function is intended to bridge the gap between AnyIO 2.x and 3.x where some functions and\n    methods were changed to return regular context managers instead of async ones.\n\n    :param cm: a regular or async context manager\n    :return: an async context manager\n\n    .. versionadded:: 2.2\n\n    \"\"\"\n    if not isinstance(cm, AbstractContextManager):\n        raise TypeError('Given object is not an context manager')\n\n    return _ContextManagerWrapper(cm)\n\n\ndef _warn_deprecation(awaitable: 'AnyDeprecatedAwaitable[Any]', stacklevel: int = 1) -> None:\n    warn(f'Awaiting on {awaitable._name}() is deprecated. Use \"await '\n         f'anyio.maybe_async({awaitable._name}(...)) if you have to support both AnyIO 2.x '\n         f'and 3.x, or just remove the \"await\" if you are completely migrating to AnyIO 3+.',\n         DeprecationWarning, stacklevel=stacklevel + 1)\n\n\nclass DeprecatedAwaitable:\n    def __init__(self, func: Callable[..., 'DeprecatedAwaitable']):\n        self._name = f'{func.__module__}.{func.__qualname__}'\n\n    def __await__(self) -> Generator[None, None, None]:\n        _warn_deprecation(self)\n        if False:\n            yield\n\n    def __reduce__(self) -> Tuple[Type[None], Tuple[()]]:\n        return type(None), ()\n\n    def _unwrap(self) -> None:\n        return None\n\n\nclass DeprecatedAwaitableFloat(float):\n    def __new__(\n        cls, x: float, func: Callable[..., 'DeprecatedAwaitableFloat']\n    ) -> 'DeprecatedAwaitableFloat':\n        return super().__new__(cls, x)\n\n    def __init__(self, x: float, func: Callable[..., 'DeprecatedAwaitableFloat']):\n        self._name = f'{func.__module__}.{func.__qualname__}'\n\n    def __await__(self) -> Generator[None, None, float]:\n        _warn_deprecation(self)\n        if False:\n            yield\n\n        return float(self)\n\n    def __reduce__(self) -> Tuple[Type[float], Tuple[float]]:\n        return float, (float(self),)\n\n    def _unwrap(self) -> float:\n        return float(self)\n\n\nclass DeprecatedAwaitableList(List[T]):\n    def __init__(self, iterable: Iterable[T] = (), *,\n                 func: Callable[..., 'DeprecatedAwaitableList[T]']):\n        super().__init__(iterable)\n        self._name = f'{func.__module__}.{func.__qualname__}'\n\n    def __await__(self) -> Generator[None, None, List[T]]:\n        _warn_deprecation(self)\n        if False:\n            yield\n\n        return list(self)\n\n    def __reduce__(self) -> Tuple[Type[List[T]], Tuple[List[T]]]:\n        return list, (list(self),)\n\n    def _unwrap(self) -> List[T]:\n        return list(self)\n\n\nclass DeprecatedAsyncContextManager(Generic[T], metaclass=ABCMeta):\n    @abstractmethod\n    def __enter__(self) -> T:\n        pass\n\n    @abstractmethod\n    def __exit__(self, exc_type: Optional[Type[BaseException]],\n                 exc_val: Optional[BaseException],\n                 exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        pass\n\n    async def __aenter__(self) -> T:\n        warn(f'Using {self.__class__.__name__} as an async context manager has been deprecated. '\n             f'Use \"async with anyio.maybe_async_cm(yourcontextmanager) as foo:\" if you have to '\n             f'support both AnyIO 2.x and 3.x, or just remove the \"async\" from \"async with\" if '\n             f'you are completely migrating to AnyIO 3+.', DeprecationWarning)\n        return self.__enter__()\n\n    async def __aexit__(self, exc_type: Optional[Type[BaseException]],\n                        exc_val: Optional[BaseException],\n                        exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        return self.__exit__(exc_type, exc_val, exc_tb)\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_core/_synchronization.py", "content": "from collections import deque\nfrom dataclasses import dataclass\nfrom types import TracebackType\nfrom typing import Deque, Optional, Tuple, Type\nfrom warnings import warn\n\nfrom ..lowlevel import cancel_shielded_checkpoint, checkpoint, checkpoint_if_cancelled\nfrom ._compat import DeprecatedAwaitable\nfrom ._eventloop import get_asynclib\nfrom ._exceptions import BusyResourceError, WouldBlock\nfrom ._tasks import CancelScope\nfrom ._testing import TaskInfo, get_current_task\n\n\n@dataclass(frozen=True)\nclass EventStatistics:\n    \"\"\"\n    :ivar int tasks_waiting: number of tasks waiting on :meth:`~.Event.wait`\n    \"\"\"\n\n    tasks_waiting: int\n\n\n@dataclass(frozen=True)\nclass CapacityLimiterStatistics:\n    \"\"\"\n    :ivar int borrowed_tokens: number of tokens currently borrowed by tasks\n    :ivar float total_tokens: total number of available tokens\n    :ivar tuple borrowers: tasks or other objects currently holding tokens borrowed from this\n        limiter\n    :ivar int tasks_waiting: number of tasks waiting on :meth:`~.CapacityLimiter.acquire` or\n        :meth:`~.CapacityLimiter.acquire_on_behalf_of`\n    \"\"\"\n\n    borrowed_tokens: int\n    total_tokens: float\n    borrowers: Tuple[object, ...]\n    tasks_waiting: int\n\n\n@dataclass(frozen=True)\nclass LockStatistics:\n    \"\"\"\n    :ivar bool locked: flag indicating if this lock is locked or not\n    :ivar ~anyio.TaskInfo owner: task currently holding the lock (or ``None`` if the lock is not\n        held by any task)\n    :ivar int tasks_waiting: number of tasks waiting on :meth:`~.Lock.acquire`\n    \"\"\"\n\n    locked: bool\n    owner: Optional[TaskInfo]\n    tasks_waiting: int\n\n\n@dataclass(frozen=True)\nclass ConditionStatistics:\n    \"\"\"\n    :ivar int tasks_waiting: number of tasks blocked on :meth:`~.Condition.wait`\n    :ivar ~anyio.LockStatistics lock_statistics: statistics of the underlying :class:`~.Lock`\n    \"\"\"\n\n    tasks_waiting: int\n    lock_statistics: LockStatistics\n\n\n@dataclass(frozen=True)\nclass SemaphoreStatistics:\n    \"\"\"\n    :ivar int tasks_waiting: number of tasks waiting on :meth:`~.Semaphore.acquire`\n\n    \"\"\"\n    tasks_waiting: int\n\n\nclass Event:\n    def __new__(cls) -> 'Event':\n        return get_asynclib().Event()\n\n    def set(self) -> DeprecatedAwaitable:\n        \"\"\"Set the flag, notifying all listeners.\"\"\"\n        raise NotImplementedError\n\n    def is_set(self) -> bool:\n        \"\"\"Return ``True`` if the flag is set, ``False`` if not.\"\"\"\n        raise NotImplementedError\n\n    async def wait(self) -> None:\n        \"\"\"\n        Wait until the flag has been set.\n\n        If the flag has already been set when this method is called, it returns immediately.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def statistics(self) -> EventStatistics:\n        \"\"\"Return statistics about the current state of this event.\"\"\"\n        raise NotImplementedError\n\n\nclass Lock:\n    _owner_task: Optional[TaskInfo] = None\n\n    def __init__(self) -> None:\n        self._waiters: Deque[Tuple[TaskInfo, Event]] = deque()\n\n    async def __aenter__(self) -> None:\n        await self.acquire()\n\n    async def __aexit__(self, exc_type: Optional[Type[BaseException]],\n                        exc_val: Optional[BaseException],\n                        exc_tb: Optional[TracebackType]) -> None:\n        self.release()\n\n    async def acquire(self) -> None:\n        \"\"\"Acquire the lock.\"\"\"\n        await checkpoint_if_cancelled()\n        try:\n            self.acquire_nowait()\n        except WouldBlock:\n            task = get_current_task()\n            event = Event()\n            token = task, event\n            self._waiters.append(token)\n            try:\n                await event.wait()\n            except BaseException:\n                if not event.is_set():\n                    self._waiters.remove(token)\n                elif self._owner_task == task:\n                    self.release()\n\n                raise\n\n            assert self._owner_task == task\n        else:\n            try:\n                await cancel_shielded_checkpoint()\n            except BaseException:\n                self.release()\n                raise\n\n    def acquire_nowait(self) -> None:\n        \"\"\"\n        Acquire the lock, without blocking.\n\n        :raises ~WouldBlock: if the operation would block\n\n        \"\"\"\n        task = get_current_task()\n        if self._owner_task == task:\n            raise RuntimeError('Attempted to acquire an already held Lock')\n\n        if self._owner_task is not None:\n            raise WouldBlock\n\n        self._owner_task = task\n\n    def release(self) -> DeprecatedAwaitable:\n        \"\"\"Release the lock.\"\"\"\n        if self._owner_task != get_current_task():\n            raise RuntimeError('The current task is not holding this lock')\n\n        if self._waiters:\n            self._owner_task, event = self._waiters.popleft()\n            event.set()\n        else:\n            del self._owner_task\n\n        return DeprecatedAwaitable(self.release)\n\n    def locked(self) -> bool:\n        \"\"\"Return True if the lock is currently held.\"\"\"\n        return self._owner_task is not None\n\n    def statistics(self) -> LockStatistics:\n        \"\"\"\n        Return statistics about the current state of this lock.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return LockStatistics(self.locked(), self._owner_task, len(self._waiters))\n\n\nclass Condition:\n    _owner_task: Optional[TaskInfo] = None\n\n    def __init__(self, lock: Optional[Lock] = None):\n        self._lock = lock or Lock()\n        self._waiters: Deque[Event] = deque()\n\n    async def __aenter__(self) -> None:\n        await self.acquire()\n\n    async def __aexit__(self, exc_type: Optional[Type[BaseException]],\n                        exc_val: Optional[BaseException],\n                        exc_tb: Optional[TracebackType]) -> None:\n        self.release()\n\n    def _check_acquired(self) -> None:\n        if self._owner_task != get_current_task():\n            raise RuntimeError('The current task is not holding the underlying lock')\n\n    async def acquire(self) -> None:\n        \"\"\"Acquire the underlying lock.\"\"\"\n        await self._lock.acquire()\n        self._owner_task = get_current_task()\n\n    def acquire_nowait(self) -> None:\n        \"\"\"\n        Acquire the underlying lock, without blocking.\n\n        :raises ~WouldBlock: if the operation would block\n\n        \"\"\"\n        self._lock.acquire_nowait()\n        self._owner_task = get_current_task()\n\n    def release(self) -> DeprecatedAwaitable:\n        \"\"\"Release the underlying lock.\"\"\"\n        self._lock.release()\n        return DeprecatedAwaitable(self.release)\n\n    def locked(self) -> bool:\n        \"\"\"Return True if the lock is set.\"\"\"\n        return self._lock.locked()\n\n    def notify(self, n: int = 1) -> None:\n        \"\"\"Notify exactly n listeners.\"\"\"\n        self._check_acquired()\n        for _ in range(n):\n            try:\n                event = self._waiters.popleft()\n            except IndexError:\n                break\n\n            event.set()\n\n    def notify_all(self) -> None:\n        \"\"\"Notify all the listeners.\"\"\"\n        self._check_acquired()\n        for event in self._waiters:\n            event.set()\n\n        self._waiters.clear()\n\n    async def wait(self) -> None:\n        \"\"\"Wait for a notification.\"\"\"\n        await checkpoint()\n        event = Event()\n        self._waiters.append(event)\n        self.release()\n        try:\n            await event.wait()\n        except BaseException:\n            if not event.is_set():\n                self._waiters.remove(event)\n\n            raise\n        finally:\n            with CancelScope(shield=True):\n                await self.acquire()\n\n    def statistics(self) -> ConditionStatistics:\n        \"\"\"\n        Return statistics about the current state of this condition.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return ConditionStatistics(len(self._waiters), self._lock.statistics())\n\n\nclass Semaphore:\n    def __init__(self, initial_value: int, *, max_value: Optional[int] = None):\n        if not isinstance(initial_value, int):\n            raise TypeError('initial_value must be an integer')\n        if initial_value < 0:\n            raise ValueError('initial_value must be >= 0')\n        if max_value is not None:\n            if not isinstance(max_value, int):\n                raise TypeError('max_value must be an integer or None')\n            if max_value < initial_value:\n                raise ValueError('max_value must be equal to or higher than initial_value')\n\n        self._value = initial_value\n        self._max_value = max_value\n        self._waiters: Deque[Event] = deque()\n\n    async def __aenter__(self) -> 'Semaphore':\n        await self.acquire()\n        return self\n\n    async def __aexit__(self, exc_type: Optional[Type[BaseException]],\n                        exc_val: Optional[BaseException],\n                        exc_tb: Optional[TracebackType]) -> None:\n        self.release()\n\n    async def acquire(self) -> None:\n        \"\"\"Decrement the semaphore value, blocking if necessary.\"\"\"\n        await checkpoint_if_cancelled()\n        try:\n            self.acquire_nowait()\n        except WouldBlock:\n            event = Event()\n            self._waiters.append(event)\n            try:\n                await event.wait()\n            except BaseException:\n                if not event.is_set():\n                    self._waiters.remove(event)\n                else:\n                    self.release()\n\n                raise\n        else:\n            try:\n                await cancel_shielded_checkpoint()\n            except BaseException:\n                self.release()\n                raise\n\n    def acquire_nowait(self) -> None:\n        \"\"\"\n        Acquire the underlying lock, without blocking.\n\n        :raises ~WouldBlock: if the operation would block\n\n        \"\"\"\n        if self._value == 0:\n            raise WouldBlock\n\n        self._value -= 1\n\n    def release(self) -> DeprecatedAwaitable:\n        \"\"\"Increment the semaphore value.\"\"\"\n        if self._max_value is not None and self._value == self._max_value:\n            raise ValueError('semaphore released too many times')\n\n        if self._waiters:\n            self._waiters.popleft().set()\n        else:\n            self._value += 1\n\n        return DeprecatedAwaitable(self.release)\n\n    @property\n    def value(self) -> int:\n        \"\"\"The current value of the semaphore.\"\"\"\n        return self._value\n\n    @property\n    def max_value(self) -> Optional[int]:\n        \"\"\"The maximum value of the semaphore.\"\"\"\n        return self._max_value\n\n    def statistics(self) -> SemaphoreStatistics:\n        \"\"\"\n        Return statistics about the current state of this semaphore.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return SemaphoreStatistics(len(self._waiters))\n\n\nclass CapacityLimiter:\n    def __new__(cls, total_tokens: float) -> 'CapacityLimiter':\n        return get_asynclib().CapacityLimiter(total_tokens)\n\n    async def __aenter__(self) -> None:\n        raise NotImplementedError\n\n    async def __aexit__(self, exc_type: Optional[Type[BaseException]],\n                        exc_val: Optional[BaseException],\n                        exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        raise NotImplementedError\n\n    @property\n    def total_tokens(self) -> float:\n        \"\"\"\n        The total number of tokens available for borrowing.\n\n        This is a read-write property. If the total number of tokens is increased, the\n        proportionate number of tasks waiting on this limiter will be granted their tokens.\n\n        .. versionchanged:: 3.0\n            The property is now writable.\n\n        \"\"\"\n        raise NotImplementedError\n\n    @total_tokens.setter\n    def total_tokens(self, value: float) -> None:\n        raise NotImplementedError\n\n    async def set_total_tokens(self, value: float) -> None:\n        warn('CapacityLimiter.set_total_tokens has been deprecated. Set the value of the'\n             '\"total_tokens\" attribute directly.', DeprecationWarning)\n        self.total_tokens = value\n\n    @property\n    def borrowed_tokens(self) -> int:\n        \"\"\"The number of tokens that have currently been borrowed.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def available_tokens(self) -> float:\n        \"\"\"The number of tokens currently available to be borrowed\"\"\"\n        raise NotImplementedError\n\n    def acquire_nowait(self) -> DeprecatedAwaitable:\n        \"\"\"\n        Acquire a token for the current task without waiting for one to become available.\n\n        :raises ~anyio.WouldBlock: if there are no tokens available for borrowing\n\n        \"\"\"\n        raise NotImplementedError\n\n    def acquire_on_behalf_of_nowait(self, borrower: object) -> DeprecatedAwaitable:\n        \"\"\"\n        Acquire a token without waiting for one to become available.\n\n        :param borrower: the entity borrowing a token\n        :raises ~anyio.WouldBlock: if there are no tokens available for borrowing\n\n        \"\"\"\n        raise NotImplementedError\n\n    async def acquire(self) -> None:\n        \"\"\"\n        Acquire a token for the current task, waiting if necessary for one to become available.\n\n        \"\"\"\n        raise NotImplementedError\n\n    async def acquire_on_behalf_of(self, borrower: object) -> None:\n        \"\"\"\n        Acquire a token, waiting if necessary for one to become available.\n\n        :param borrower: the entity borrowing a token\n\n        \"\"\"\n        raise NotImplementedError\n\n    def release(self) -> None:\n        \"\"\"\n        Release the token held by the current task.\n        :raises RuntimeError: if the current task has not borrowed a token from this limiter.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def release_on_behalf_of(self, borrower: object) -> None:\n        \"\"\"\n        Release the token held by the given borrower.\n\n        :raises RuntimeError: if the borrower has not borrowed a token from this limiter.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def statistics(self) -> CapacityLimiterStatistics:\n        \"\"\"\n        Return statistics about the current state of this limiter.\n\n        .. versionadded:: 3.0\n\n        \"\"\"\n        raise NotImplementedError\n\n\ndef create_lock() -> Lock:\n    \"\"\"\n    Create an asynchronous lock.\n\n    :return: a lock object\n\n    .. deprecated:: 3.0\n       Use :class:`~Lock` directly.\n\n    \"\"\"\n    warn('create_lock() is deprecated -- use Lock() directly', DeprecationWarning)\n    return Lock()\n\n\ndef create_condition(lock: Optional[Lock] = None) -> Condition:\n    \"\"\"\n    Create an asynchronous condition.\n\n    :param lock: the lock to base the condition object on\n    :return: a condition object\n\n    .. deprecated:: 3.0\n       Use :class:`~Condition` directly.\n\n    \"\"\"\n    warn('create_condition() is deprecated -- use Condition() directly', DeprecationWarning)\n    return Condition(lock=lock)\n\n\ndef create_event() -> Event:\n    \"\"\"\n    Create an asynchronous event object.\n\n    :return: an event object\n\n    .. deprecated:: 3.0\n       Use :class:`~Event` directly.\n\n    \"\"\"\n    warn('create_event() is deprecated -- use Event() directly', DeprecationWarning)\n    return get_asynclib().Event()\n\n\ndef create_semaphore(value: int, *, max_value: Optional[int] = None) -> Semaphore:\n    \"\"\"\n    Create an asynchronous semaphore.\n\n    :param value: the semaphore's initial value\n    :param max_value: if set, makes this a \"bounded\" semaphore that raises :exc:`ValueError` if the\n        semaphore's value would exceed this number\n    :return: a semaphore object\n\n    .. deprecated:: 3.0\n       Use :class:`~Semaphore` directly.\n\n    \"\"\"\n    warn('create_semaphore() is deprecated -- use Semaphore() directly', DeprecationWarning)\n    return Semaphore(value, max_value=max_value)\n\n\ndef create_capacity_limiter(total_tokens: float) -> CapacityLimiter:\n    \"\"\"\n    Create a capacity limiter.\n\n    :param total_tokens: the total number of tokens available for borrowing (can be an integer or\n        :data:`math.inf`)\n    :return: a capacity limiter object\n\n    .. deprecated:: 3.0\n       Use :class:`~CapacityLimiter` directly.\n\n    \"\"\"\n    warn('create_capacity_limiter() is deprecated -- use CapacityLimiter() directly',\n         DeprecationWarning)\n    return get_asynclib().CapacityLimiter(total_tokens)\n\n\nclass ResourceGuard:\n    __slots__ = 'action', '_guarded'\n\n    def __init__(self, action: str):\n        self.action = action\n        self._guarded = False\n\n    def __enter__(self) -> None:\n        if self._guarded:\n            raise BusyResourceError(self.action)\n\n        self._guarded = True\n\n    def __exit__(self, exc_type: Optional[Type[BaseException]],\n                 exc_val: Optional[BaseException],\n                 exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        self._guarded = False\n        return None\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_backends/__init__.py", "content": ""}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiofiles/threadpool/text.py", "content": "from ..base import AsyncBase\nfrom .utils import (\n    delegate_to_executor,\n    proxy_method_directly,\n    proxy_property_directly,\n)\n\n\n@delegate_to_executor(\n    \"close\",\n    \"flush\",\n    \"isatty\",\n    \"read\",\n    \"readable\",\n    \"readline\",\n    \"readlines\",\n    \"seek\",\n    \"seekable\",\n    \"tell\",\n    \"truncate\",\n    \"write\",\n    \"writable\",\n    \"writelines\",\n)\n@proxy_method_directly(\"detach\", \"fileno\", \"readable\")\n@proxy_property_directly(\n    \"buffer\",\n    \"closed\",\n    \"encoding\",\n    \"errors\",\n    \"line_buffering\",\n    \"newlines\",\n    \"name\",\n    \"mode\",\n)\nclass AsyncTextIOWrapper(AsyncBase):\n    \"\"\"The asyncio executor version of io.TextIOWrapper.\"\"\"\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_core/_subprocesses.py", "content": "from io import BytesIO\nfrom os import PathLike\nfrom subprocess import DEVNULL, PIPE, CalledProcessError, CompletedProcess\nfrom typing import AsyncIterable, List, Mapping, Optional, Sequence, Union, cast\n\nfrom ..abc import Process\nfrom ._eventloop import get_asynclib\nfrom ._tasks import create_task_group\n\n\nasync def run_process(command: Union[str, Sequence[str]], *, input: Optional[bytes] = None,\n                      stdout: int = PIPE, stderr: int = PIPE, check: bool = True,\n                      cwd: Union[str, bytes, 'PathLike[str]', None] = None,\n                      env: Optional[Mapping[str, str]] = None, start_new_session: bool = False,\n                      ) -> 'CompletedProcess[bytes]':\n    \"\"\"\n    Run an external command in a subprocess and wait until it completes.\n\n    .. seealso:: :func:`subprocess.run`\n\n    :param command: either a string to pass to the shell, or an iterable of strings containing the\n        executable name or path and its arguments\n    :param input: bytes passed to the standard input of the subprocess\n    :param stdout: either :data:`subprocess.PIPE` or :data:`subprocess.DEVNULL`\n    :param stderr: one of :data:`subprocess.PIPE`, :data:`subprocess.DEVNULL` or\n        :data:`subprocess.STDOUT`\n    :param check: if ``True``, raise :exc:`~subprocess.CalledProcessError` if the process\n        terminates with a return code other than 0\n    :param cwd: If not ``None``, change the working directory to this before running the command\n    :param env: if not ``None``, this mapping replaces the inherited environment variables from the\n        parent process\n    :param start_new_session: if ``true`` the setsid() system call will be made in the child\n        process prior to the execution of the subprocess. (POSIX only)\n    :return: an object representing the completed process\n    :raises ~subprocess.CalledProcessError: if ``check`` is ``True`` and the process exits with a\n        nonzero return code\n\n    \"\"\"\n    async def drain_stream(stream: AsyncIterable[bytes], index: int) -> None:\n        buffer = BytesIO()\n        async for chunk in stream:\n            buffer.write(chunk)\n\n        stream_contents[index] = buffer.getvalue()\n\n    async with await open_process(command, stdin=PIPE if input else DEVNULL, stdout=stdout,\n                                  stderr=stderr, cwd=cwd, env=env,\n                                  start_new_session=start_new_session) as process:\n        stream_contents: List[Optional[bytes]] = [None, None]\n        try:\n            async with create_task_group() as tg:\n                if process.stdout:\n                    tg.start_soon(drain_stream, process.stdout, 0)\n                if process.stderr:\n                    tg.start_soon(drain_stream, process.stderr, 1)\n                if process.stdin and input:\n                    await process.stdin.send(input)\n                    await process.stdin.aclose()\n\n                await process.wait()\n        except BaseException:\n            process.kill()\n            raise\n\n    output, errors = stream_contents\n    if check and process.returncode != 0:\n        raise CalledProcessError(cast(int, process.returncode), command, output, errors)\n\n    return CompletedProcess(command, cast(int, process.returncode), output, errors)\n\n\nasync def open_process(command: Union[str, Sequence[str]], *, stdin: int = PIPE,\n                       stdout: int = PIPE, stderr: int = PIPE,\n                       cwd: Union[str, bytes, 'PathLike[str]', None] = None,\n                       env: Optional[Mapping[str, str]] = None,\n                       start_new_session: bool = False) -> Process:\n    \"\"\"\n    Start an external command in a subprocess.\n\n    .. seealso:: :class:`subprocess.Popen`\n\n    :param command: either a string to pass to the shell, or an iterable of strings containing the\n        executable name or path and its arguments\n    :param stdin: either :data:`subprocess.PIPE` or :data:`subprocess.DEVNULL`\n    :param stdout: either :data:`subprocess.PIPE` or :data:`subprocess.DEVNULL`\n    :param stderr: one of :data:`subprocess.PIPE`, :data:`subprocess.DEVNULL` or\n        :data:`subprocess.STDOUT`\n    :param cwd: If not ``None``, the working directory is changed before executing\n    :param env: If env is not ``None``, it must be a mapping that defines the environment\n        variables for the new process\n    :param start_new_session: if ``true`` the setsid() system call will be made in the child\n        process prior to the execution of the subprocess. (POSIX only)\n    :return: an asynchronous process object\n\n    \"\"\"\n    shell = isinstance(command, str)\n    return await get_asynclib().open_process(command, shell=shell, stdin=stdin, stdout=stdout,\n                                             stderr=stderr, cwd=cwd, env=env,\n                                             start_new_session=start_new_session)\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiofiles/ospath.py", "content": "\"\"\"Async executor versions of file functions from the os.path module.\"\"\"\n\nfrom .os import wrap\nfrom os import path\n\nexists = wrap(path.exists)\nisfile = wrap(path.isfile)\nisdir = wrap(path.isdir)\ngetsize = wrap(path.getsize)\ngetmtime = wrap(path.getmtime)\ngetatime = wrap(path.getatime)\ngetctime = wrap(path.getctime)\nsamefile = wrap(path.samefile)\nsameopenfile = wrap(path.sameopenfile)\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_core/_streams.py", "content": "import math\nfrom typing import Any, Optional, Tuple, Type, TypeVar, overload\n\nfrom ..streams.memory import (\n    MemoryObjectReceiveStream, MemoryObjectSendStream, MemoryObjectStreamState)\n\nT_Item = TypeVar('T_Item')\n\n\n@overload\ndef create_memory_object_stream(\n    max_buffer_size: float, item_type: Type[T_Item]\n) -> Tuple[MemoryObjectSendStream[T_Item], MemoryObjectReceiveStream[T_Item]]:\n    ...\n\n\n@overload\ndef create_memory_object_stream(\n    max_buffer_size: float = 0\n) -> Tuple[MemoryObjectSendStream[Any], MemoryObjectReceiveStream[Any]]:\n    ...\n\n\ndef create_memory_object_stream(\n    max_buffer_size: float = 0, item_type: Optional[Type[T_Item]] = None\n) -> Tuple[MemoryObjectSendStream[Any], MemoryObjectReceiveStream[Any]]:\n    \"\"\"\n    Create a memory object stream.\n\n    :param max_buffer_size: number of items held in the buffer until ``send()`` starts blocking\n    :param item_type: type of item, for marking the streams with the right generic type for\n        static typing (not used at run time)\n    :return: a tuple of (send stream, receive stream)\n\n    \"\"\"\n    if max_buffer_size != math.inf and not isinstance(max_buffer_size, int):\n        raise ValueError('max_buffer_size must be either an integer or math.inf')\n    if max_buffer_size < 0:\n        raise ValueError('max_buffer_size cannot be negative')\n\n    state: MemoryObjectStreamState = MemoryObjectStreamState(max_buffer_size)\n    return MemoryObjectSendStream(state), MemoryObjectReceiveStream(state)\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_core/_eventloop.py", "content": "import math\nimport sys\nimport threading\nfrom contextlib import contextmanager\nfrom importlib import import_module\nfrom typing import Any, Callable, Coroutine, Dict, Generator, Optional, Tuple, Type, TypeVar\n\nimport sniffio\n\n# This must be updated when new backends are introduced\nfrom ._compat import DeprecatedAwaitableFloat\n\nBACKENDS = 'asyncio', 'trio'\n\nT_Retval = TypeVar('T_Retval')\nthreadlocals = threading.local()\n\n\ndef run(func: Callable[..., Coroutine[Any, Any, T_Retval]], *args: object,\n        backend: str = 'asyncio', backend_options: Optional[Dict[str, Any]] = None) -> T_Retval:\n    \"\"\"\n    Run the given coroutine function in an asynchronous event loop.\n\n    The current thread must not be already running an event loop.\n\n    :param func: a coroutine function\n    :param args: positional arguments to ``func``\n    :param backend: name of the asynchronous event loop implementation – currently either\n        ``asyncio`` or ``trio``\n    :param backend_options: keyword arguments to call the backend ``run()`` implementation with\n        (documented :ref:`here <backend options>`)\n    :return: the return value of the coroutine function\n    :raises RuntimeError: if an asynchronous event loop is already running in this thread\n    :raises LookupError: if the named backend is not found\n\n    \"\"\"\n    try:\n        asynclib_name = sniffio.current_async_library()\n    except sniffio.AsyncLibraryNotFoundError:\n        pass\n    else:\n        raise RuntimeError(f'Already running {asynclib_name} in this thread')\n\n    try:\n        asynclib = import_module(f'..._backends._{backend}', package=__name__)\n    except ImportError as exc:\n        raise LookupError(f'No such backend: {backend}') from exc\n\n    token = None\n    if sniffio.current_async_library_cvar.get(None) is None:\n        # Since we're in control of the event loop, we can cache the name of the async library\n        token = sniffio.current_async_library_cvar.set(backend)\n\n    try:\n        backend_options = backend_options or {}\n        return asynclib.run(func, *args, **backend_options)\n    finally:\n        if token:\n            sniffio.current_async_library_cvar.reset(token)\n\n\nasync def sleep(delay: float) -> None:\n    \"\"\"\n    Pause the current task for the specified duration.\n\n    :param delay: the duration, in seconds\n\n    \"\"\"\n    return await get_asynclib().sleep(delay)\n\n\nasync def sleep_forever() -> None:\n    \"\"\"\n    Pause the current task until it's cancelled.\n\n    This is a shortcut for ``sleep(math.inf)``.\n\n    .. versionadded:: 3.1\n\n    \"\"\"\n    await sleep(math.inf)\n\n\nasync def sleep_until(deadline: float) -> None:\n    \"\"\"\n    Pause the current task until the given time.\n\n    :param deadline: the absolute time to wake up at (according to the internal monotonic clock of\n        the event loop)\n\n    .. versionadded:: 3.1\n\n    \"\"\"\n    now = current_time()\n    await sleep(max(deadline - now, 0))\n\n\ndef current_time() -> DeprecatedAwaitableFloat:\n    \"\"\"\n    Return the current value of the event loop's internal clock.\n\n    :return: the clock value (seconds)\n\n    \"\"\"\n    return DeprecatedAwaitableFloat(get_asynclib().current_time(), current_time)\n\n\ndef get_all_backends() -> Tuple[str, ...]:\n    \"\"\"Return a tuple of the names of all built-in backends.\"\"\"\n    return BACKENDS\n\n\ndef get_cancelled_exc_class() -> Type[BaseException]:\n    \"\"\"Return the current async library's cancellation exception class.\"\"\"\n    return get_asynclib().CancelledError\n\n\n#\n# Private API\n#\n\n@contextmanager\ndef claim_worker_thread(backend: str) -> Generator[Any, None, None]:\n    module = sys.modules['anyio._backends._' + backend]\n    threadlocals.current_async_module = module\n    try:\n        yield\n    finally:\n        del threadlocals.current_async_module\n\n\ndef get_asynclib(asynclib_name: Optional[str] = None) -> Any:\n    if asynclib_name is None:\n        asynclib_name = sniffio.current_async_library()\n\n    modulename = 'anyio._backends._' + asynclib_name\n    try:\n        return sys.modules[modulename]\n    except KeyError:\n        return import_module(modulename)\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_core/__init__.py", "content": ""}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiosqlite/context.py", "content": "# Copyright 2018\n# Licensed under the MIT license\n\n\nfrom functools import wraps\nfrom typing import Any, Callable, Coroutine, Generator, TypeVar\n\nfrom typing_extensions import AsyncContextManager\n\nfrom .cursor import Cursor\n\n_T = TypeVar(\"_T\")\n\n\nclass Result(AsyncContextManager[_T], Coroutine[Any, Any, _T]):\n    __slots__ = (\"_coro\", \"_obj\")\n\n    def __init__(self, coro: Coroutine[Any, Any, _T]):\n        self._coro = coro\n        self._obj: _T\n\n    def send(self, value) -> None:\n        return self._coro.send(value)\n\n    def throw(self, typ, val=None, tb=None) -> None:\n        if val is None:\n            return self._coro.throw(typ)\n\n        if tb is None:\n            return self._coro.throw(typ, val)\n\n        return self._coro.throw(typ, val, tb)\n\n    def close(self) -> None:\n        return self._coro.close()\n\n    def __await__(self) -> Generator[Any, None, _T]:\n        return self._coro.__await__()\n\n    async def __aenter__(self) -> _T:\n        self._obj = await self._coro\n        return self._obj\n\n    async def __aexit__(self, exc_type, exc, tb) -> None:\n        if isinstance(self._obj, Cursor):\n            await self._obj.close()\n\n\ndef contextmanager(\n    method: Callable[..., Coroutine[Any, Any, _T]]\n) -> Callable[..., Result[_T]]:\n    @wraps(method)\n    def wrapper(self, *args, **kwargs) -> Result[_T]:\n        return Result(method(self, *args, **kwargs))\n\n    return wrapper\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_backends/_trio.py", "content": "import array\nimport math\nimport socket\nfrom concurrent.futures import Future\nfrom contextvars import copy_context\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom io import IOBase\nfrom os import PathLike\nfrom signal import Signals\nfrom types import TracebackType\nfrom typing import (\n    Any, Awaitable, Callable, Collection, ContextManager, Coroutine, Deque, Dict, Generic, List,\n    Mapping, NoReturn, Optional, Sequence, Set, Tuple, Type, TypeVar, Union, cast)\n\nimport sniffio\nimport trio.from_thread\nfrom outcome import Error, Outcome, Value\nfrom trio.socket import SocketType as TrioSocketType\nfrom trio.to_thread import run_sync\n\nfrom .. import CapacityLimiterStatistics, EventStatistics, TaskInfo, abc\nfrom .._core._compat import DeprecatedAsyncContextManager, DeprecatedAwaitable, T\nfrom .._core._eventloop import claim_worker_thread\nfrom .._core._exceptions import (\n    BrokenResourceError, BusyResourceError, ClosedResourceError, EndOfStream)\nfrom .._core._exceptions import ExceptionGroup as BaseExceptionGroup\nfrom .._core._sockets import convert_ipv6_sockaddr\nfrom .._core._synchronization import CapacityLimiter as BaseCapacityLimiter\nfrom .._core._synchronization import Event as BaseEvent\nfrom .._core._synchronization import ResourceGuard\nfrom .._core._tasks import CancelScope as BaseCancelScope\nfrom ..abc import IPSockAddrType, UDPPacketType\n\ntry:\n    from trio import lowlevel as trio_lowlevel\nexcept ImportError:\n    from trio import hazmat as trio_lowlevel  # type: ignore[no-redef]\n    from trio.hazmat import wait_readable, wait_writable\nelse:\n    from trio.lowlevel import wait_readable, wait_writable\n\ntry:\n    from trio.lowlevel import open_process as trio_open_process  # type: ignore[attr-defined]\nexcept ImportError:\n    from trio import open_process as trio_open_process\n\nT_Retval = TypeVar('T_Retval')\nT_SockAddr = TypeVar('T_SockAddr', str, IPSockAddrType)\n\n\n#\n# Event loop\n#\n\nrun = trio.run\ncurrent_token = trio.lowlevel.current_trio_token\nRunVar = trio.lowlevel.RunVar\n\n\n#\n# Miscellaneous\n#\n\nsleep = trio.sleep\n\n\n#\n# Timeouts and cancellation\n#\n\nclass CancelScope(BaseCancelScope):\n    def __new__(cls, original: Optional[trio.CancelScope] = None,\n                **kwargs: object) -> 'CancelScope':\n        return object.__new__(cls)\n\n    def __init__(self, original: Optional[trio.CancelScope] = None, **kwargs: Any) -> None:\n        self.__original = original or trio.CancelScope(**kwargs)\n\n    def __enter__(self) -> 'CancelScope':\n        self.__original.__enter__()\n        return self\n\n    def __exit__(self, exc_type: Optional[Type[BaseException]],\n                 exc_val: Optional[BaseException],\n                 exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        return self.__original.__exit__(exc_type, exc_val, exc_tb)\n\n    def cancel(self) -> DeprecatedAwaitable:\n        self.__original.cancel()\n        return DeprecatedAwaitable(self.cancel)\n\n    @property\n    def deadline(self) -> float:\n        return self.__original.deadline\n\n    @deadline.setter\n    def deadline(self, value: float) -> None:\n        self.__original.deadline = value\n\n    @property\n    def cancel_called(self) -> bool:\n        return self.__original.cancel_called\n\n    @property\n    def shield(self) -> bool:\n        return self.__original.shield\n\n    @shield.setter\n    def shield(self, value: bool) -> None:\n        self.__original.shield = value\n\n\nCancelledError = trio.Cancelled\ncheckpoint = trio.lowlevel.checkpoint\ncheckpoint_if_cancelled = trio.lowlevel.checkpoint_if_cancelled\ncancel_shielded_checkpoint = trio.lowlevel.cancel_shielded_checkpoint\ncurrent_effective_deadline = trio.current_effective_deadline\ncurrent_time = trio.current_time\n\n\n#\n# Task groups\n#\n\nclass ExceptionGroup(BaseExceptionGroup, trio.MultiError):\n    pass\n\n\nclass TaskGroup(abc.TaskGroup):\n    def __init__(self) -> None:\n        self._active = False\n        self._nursery_manager = trio.open_nursery()\n        self.cancel_scope = None  # type: ignore[assignment]\n\n    async def __aenter__(self) -> 'TaskGroup':\n        self._active = True\n        self._nursery = await self._nursery_manager.__aenter__()\n        self.cancel_scope = CancelScope(self._nursery.cancel_scope)\n        return self\n\n    async def __aexit__(self, exc_type: Optional[Type[BaseException]],\n                        exc_val: Optional[BaseException],\n                        exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        try:\n            return await self._nursery_manager.__aexit__(exc_type, exc_val, exc_tb)\n        except trio.MultiError as exc:\n            raise ExceptionGroup(exc.exceptions) from None\n        finally:\n            self._active = False\n\n    def start_soon(self, func: Callable, *args: object, name: object = None) -> None:\n        if not self._active:\n            raise RuntimeError('This task group is not active; no new tasks can be started.')\n\n        self._nursery.start_soon(func, *args, name=name)\n\n    async def start(self, func: Callable[..., Coroutine],\n                    *args: object, name: object = None) -> object:\n        if not self._active:\n            raise RuntimeError('This task group is not active; no new tasks can be started.')\n\n        return await self._nursery.start(func, *args, name=name)\n\n#\n# Threads\n#\n\n\nasync def run_sync_in_worker_thread(\n        func: Callable[..., T_Retval], *args: object, cancellable: bool = False,\n        limiter: Optional[trio.CapacityLimiter] = None) -> T_Retval:\n    def wrapper() -> T_Retval:\n        with claim_worker_thread('trio'):\n            return func(*args)\n\n    # TODO: remove explicit context copying when trio 0.20 is the minimum requirement\n    context = copy_context()\n    context.run(sniffio.current_async_library_cvar.set, None)\n    return await run_sync(context.run, wrapper, cancellable=cancellable, limiter=limiter)\n\n\n# TODO: remove this workaround when trio 0.20 is the minimum requirement\ndef run_async_from_thread(fn: Callable[..., Awaitable[T_Retval]], *args: Any) -> T_Retval:\n    async def wrapper() -> T_Retval:\n        retval: T_Retval\n\n        async def inner() -> None:\n            nonlocal retval\n            __tracebackhide__ = True\n            retval = await fn(*args)\n\n        async with trio.open_nursery() as n:\n            context.run(n.start_soon, inner)\n\n        __tracebackhide__ = True\n        return retval\n\n    context = copy_context()\n    context.run(sniffio.current_async_library_cvar.set, 'trio')\n    return trio.from_thread.run(wrapper)\n\n\ndef run_sync_from_thread(fn: Callable[..., T_Retval], *args: Any) -> T_Retval:\n    # TODO: remove explicit context copying when trio 0.20 is the minimum requirement\n    retval = trio.from_thread.run_sync(copy_context().run, fn, *args)\n    return cast(T_Retval, retval)\n\n\nclass BlockingPortal(abc.BlockingPortal):\n    def __new__(cls) -> 'BlockingPortal':\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._token = trio.lowlevel.current_trio_token()\n\n    def _spawn_task_from_thread(self, func: Callable, args: tuple, kwargs: Dict[str, Any],\n                                name: object, future: Future) -> None:\n        context = copy_context()\n        context.run(sniffio.current_async_library_cvar.set, 'trio')\n        trio.from_thread.run_sync(\n            context.run, partial(self._task_group.start_soon, name=name), self._call_func, func,\n            args, kwargs, future, trio_token=self._token)\n\n\n#\n# Subprocesses\n#\n\n@dataclass(eq=False)\nclass ReceiveStreamWrapper(abc.ByteReceiveStream):\n    _stream: trio.abc.ReceiveStream\n\n    async def receive(self, max_bytes: Optional[int] = None) -> bytes:\n        try:\n            data = await self._stream.receive_some(max_bytes)\n        except trio.ClosedResourceError as exc:\n            raise ClosedResourceError from exc.__cause__\n        except trio.BrokenResourceError as exc:\n            raise BrokenResourceError from exc.__cause__\n\n        if data:\n            return data\n        else:\n            raise EndOfStream\n\n    async def aclose(self) -> None:\n        await self._stream.aclose()\n\n\n@dataclass(eq=False)\nclass SendStreamWrapper(abc.ByteSendStream):\n    _stream: trio.abc.SendStream\n\n    async def send(self, item: bytes) -> None:\n        try:\n            await self._stream.send_all(item)\n        except trio.ClosedResourceError as exc:\n            raise ClosedResourceError from exc.__cause__\n        except trio.BrokenResourceError as exc:\n            raise BrokenResourceError from exc.__cause__\n\n    async def aclose(self) -> None:\n        await self._stream.aclose()\n\n\n@dataclass(eq=False)\nclass Process(abc.Process):\n    _process: trio.Process\n    _stdin: Optional[abc.ByteSendStream]\n    _stdout: Optional[abc.ByteReceiveStream]\n    _stderr: Optional[abc.ByteReceiveStream]\n\n    async def aclose(self) -> None:\n        if self._stdin:\n            await self._stdin.aclose()\n        if self._stdout:\n            await self._stdout.aclose()\n        if self._stderr:\n            await self._stderr.aclose()\n\n        await self.wait()\n\n    async def wait(self) -> int:\n        return await self._process.wait()\n\n    def terminate(self) -> None:\n        self._process.terminate()\n\n    def kill(self) -> None:\n        self._process.kill()\n\n    def send_signal(self, signal: Signals) -> None:\n        self._process.send_signal(signal)\n\n    @property\n    def pid(self) -> int:\n        return self._process.pid\n\n    @property\n    def returncode(self) -> Optional[int]:\n        return self._process.returncode\n\n    @property\n    def stdin(self) -> Optional[abc.ByteSendStream]:\n        return self._stdin\n\n    @property\n    def stdout(self) -> Optional[abc.ByteReceiveStream]:\n        return self._stdout\n\n    @property\n    def stderr(self) -> Optional[abc.ByteReceiveStream]:\n        return self._stderr\n\n\nasync def open_process(command: Union[str, Sequence[str]], *, shell: bool,\n                       stdin: int, stdout: int, stderr: int,\n                       cwd: Union[str, bytes, PathLike, None] = None,\n                       env: Optional[Mapping[str, str]] = None,\n                       start_new_session: bool = False) -> Process:\n    process = await trio_open_process(command, stdin=stdin, stdout=stdout, stderr=stderr,\n                                      shell=shell, cwd=cwd, env=env,\n                                      start_new_session=start_new_session)\n    stdin_stream = SendStreamWrapper(process.stdin) if process.stdin else None\n    stdout_stream = ReceiveStreamWrapper(process.stdout) if process.stdout else None\n    stderr_stream = ReceiveStreamWrapper(process.stderr) if process.stderr else None\n    return Process(process, stdin_stream, stdout_stream, stderr_stream)\n\n\nclass _ProcessPoolShutdownInstrument(trio.abc.Instrument):\n    def after_run(self) -> None:\n        super().after_run()\n\n\ncurrent_default_worker_process_limiter: RunVar = RunVar(\n    'current_default_worker_process_limiter')\n\n\nasync def _shutdown_process_pool(workers: Set[Process]) -> None:\n    process: Process\n    try:\n        await sleep(math.inf)\n    except trio.Cancelled:\n        for process in workers:\n            if process.returncode is None:\n                process.kill()\n\n        with CancelScope(shield=True):\n            for process in workers:\n                await process.aclose()\n\n\ndef setup_process_pool_exit_at_shutdown(workers: Set[Process]) -> None:\n    trio.lowlevel.spawn_system_task(_shutdown_process_pool, workers)\n\n\n#\n# Sockets and networking\n#\n\nclass _TrioSocketMixin(Generic[T_SockAddr]):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        self._trio_socket = trio_socket\n        self._closed = False\n\n    def _check_closed(self) -> None:\n        if self._closed:\n            raise ClosedResourceError\n        if self._trio_socket.fileno() < 0:\n            raise BrokenResourceError\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._trio_socket._sock  # type: ignore[attr-defined]\n\n    async def aclose(self) -> None:\n        if self._trio_socket.fileno() >= 0:\n            self._closed = True\n            self._trio_socket.close()\n\n    def _convert_socket_error(self, exc: BaseException) -> 'NoReturn':\n        if isinstance(exc, trio.ClosedResourceError):\n            raise ClosedResourceError from exc\n        elif self._trio_socket.fileno() < 0 and self._closed:\n            raise ClosedResourceError from None\n        elif isinstance(exc, OSError):\n            raise BrokenResourceError from exc\n        else:\n            raise exc\n\n\nclass SocketStream(_TrioSocketMixin, abc.SocketStream):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        super().__init__(trio_socket)\n        self._receive_guard = ResourceGuard('reading from')\n        self._send_guard = ResourceGuard('writing to')\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        with self._receive_guard:\n            try:\n                data = await self._trio_socket.recv(max_bytes)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n            if data:\n                return data\n            else:\n                raise EndOfStream\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            view = memoryview(item)\n            while view:\n                try:\n                    bytes_sent = await self._trio_socket.send(view)\n                except BaseException as exc:\n                    self._convert_socket_error(exc)\n\n                view = view[bytes_sent:]\n\n    async def send_eof(self) -> None:\n        self._trio_socket.shutdown(socket.SHUT_WR)\n\n\nclass UNIXSocketStream(SocketStream, abc.UNIXSocketStream):\n    async def receive_fds(self, msglen: int, maxfds: int) -> Tuple[bytes, List[int]]:\n        if not isinstance(msglen, int) or msglen < 0:\n            raise ValueError('msglen must be a non-negative integer')\n        if not isinstance(maxfds, int) or maxfds < 1:\n            raise ValueError('maxfds must be a positive integer')\n\n        fds = array.array(\"i\")\n        await checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    message, ancdata, flags, addr = await self._trio_socket.recvmsg(\n                        msglen, socket.CMSG_LEN(maxfds * fds.itemsize))\n                except BaseException as exc:\n                    self._convert_socket_error(exc)\n                else:\n                    if not message and not ancdata:\n                        raise EndOfStream\n\n                    break\n\n        for cmsg_level, cmsg_type, cmsg_data in ancdata:\n            if cmsg_level != socket.SOL_SOCKET or cmsg_type != socket.SCM_RIGHTS:\n                raise RuntimeError(f'Received unexpected ancillary data; message = {message!r}, '\n                                   f'cmsg_level = {cmsg_level}, cmsg_type = {cmsg_type}')\n\n            fds.frombytes(cmsg_data[:len(cmsg_data) - (len(cmsg_data) % fds.itemsize)])\n\n        return message, list(fds)\n\n    async def send_fds(self, message: bytes, fds: Collection[Union[int, IOBase]]) -> None:\n        if not message:\n            raise ValueError('message must not be empty')\n        if not fds:\n            raise ValueError('fds must not be empty')\n\n        filenos: List[int] = []\n        for fd in fds:\n            if isinstance(fd, int):\n                filenos.append(fd)\n            elif isinstance(fd, IOBase):\n                filenos.append(fd.fileno())\n\n        fdarray = array.array(\"i\", filenos)\n        await checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    await self._trio_socket.sendmsg(\n                        [message],\n                        [(socket.SOL_SOCKET, socket.SCM_RIGHTS,  # type: ignore[list-item]\n                          fdarray)]\n                    )\n                    break\n                except BaseException as exc:\n                    self._convert_socket_error(exc)\n\n\nclass TCPSocketListener(_TrioSocketMixin, abc.SocketListener):\n    def __init__(self, raw_socket: socket.socket):\n        super().__init__(trio.socket.from_stdlib_socket(raw_socket))\n        self._accept_guard = ResourceGuard('accepting connections from')\n\n    async def accept(self) -> SocketStream:\n        with self._accept_guard:\n            try:\n                trio_socket, _addr = await self._trio_socket.accept()\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n        trio_socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        return SocketStream(trio_socket)\n\n\nclass UNIXSocketListener(_TrioSocketMixin, abc.SocketListener):\n    def __init__(self, raw_socket: socket.socket):\n        super().__init__(trio.socket.from_stdlib_socket(raw_socket))\n        self._accept_guard = ResourceGuard('accepting connections from')\n\n    async def accept(self) -> UNIXSocketStream:\n        with self._accept_guard:\n            try:\n                trio_socket, _addr = await self._trio_socket.accept()\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n        return UNIXSocketStream(trio_socket)\n\n\nclass UDPSocket(_TrioSocketMixin[IPSockAddrType], abc.UDPSocket):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        super().__init__(trio_socket)\n        self._receive_guard = ResourceGuard('reading from')\n        self._send_guard = ResourceGuard('writing to')\n\n    async def receive(self) -> Tuple[bytes, IPSockAddrType]:\n        with self._receive_guard:\n            try:\n                data, addr = await self._trio_socket.recvfrom(65536)\n                return data, convert_ipv6_sockaddr(addr)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n    async def send(self, item: UDPPacketType) -> None:\n        with self._send_guard:\n            try:\n                await self._trio_socket.sendto(*item)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n\nclass ConnectedUDPSocket(_TrioSocketMixin[IPSockAddrType], abc.ConnectedUDPSocket):\n    def __init__(self, trio_socket: TrioSocketType) -> None:\n        super().__init__(trio_socket)\n        self._receive_guard = ResourceGuard('reading from')\n        self._send_guard = ResourceGuard('writing to')\n\n    async def receive(self) -> bytes:\n        with self._receive_guard:\n            try:\n                return await self._trio_socket.recv(65536)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            try:\n                await self._trio_socket.send(item)\n            except BaseException as exc:\n                self._convert_socket_error(exc)\n\n\nasync def connect_tcp(host: str, port: int,\n                      local_address: Optional[IPSockAddrType] = None) -> SocketStream:\n    family = socket.AF_INET6 if ':' in host else socket.AF_INET\n    trio_socket = trio.socket.socket(family)\n    trio_socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n    if local_address:\n        await trio_socket.bind(local_address)\n\n    try:\n        await trio_socket.connect((host, port))\n    except BaseException:\n        trio_socket.close()\n        raise\n\n    return SocketStream(trio_socket)\n\n\nasync def connect_unix(path: str) -> UNIXSocketStream:\n    trio_socket = trio.socket.socket(socket.AF_UNIX)\n    try:\n        await trio_socket.connect(path)\n    except BaseException:\n        trio_socket.close()\n        raise\n\n    return UNIXSocketStream(trio_socket)\n\n\nasync def create_udp_socket(\n    family: socket.AddressFamily,\n    local_address: Optional[IPSockAddrType],\n    remote_address: Optional[IPSockAddrType],\n    reuse_port: bool\n) -> Union[UDPSocket, ConnectedUDPSocket]:\n    trio_socket = trio.socket.socket(family=family, type=socket.SOCK_DGRAM)\n\n    if reuse_port:\n        trio_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n\n    if local_address:\n        await trio_socket.bind(local_address)\n\n    if remote_address:\n        await trio_socket.connect(remote_address)\n        return ConnectedUDPSocket(trio_socket)\n    else:\n        return UDPSocket(trio_socket)\n\n\ngetaddrinfo = trio.socket.getaddrinfo\ngetnameinfo = trio.socket.getnameinfo\n\n\nasync def wait_socket_readable(sock: socket.socket) -> None:\n    try:\n        await wait_readable(sock)\n    except trio.ClosedResourceError as exc:\n        raise ClosedResourceError().with_traceback(exc.__traceback__) from None\n    except trio.BusyResourceError:\n        raise BusyResourceError('reading from') from None\n\n\nasync def wait_socket_writable(sock: socket.socket) -> None:\n    try:\n        await wait_writable(sock)\n    except trio.ClosedResourceError as exc:\n        raise ClosedResourceError().with_traceback(exc.__traceback__) from None\n    except trio.BusyResourceError:\n        raise BusyResourceError('writing to') from None\n\n\n#\n# Synchronization\n#\n\nclass Event(BaseEvent):\n    def __new__(cls) -> 'Event':\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        self.__original = trio.Event()\n\n    def is_set(self) -> bool:\n        return self.__original.is_set()\n\n    async def wait(self) -> None:\n        return await self.__original.wait()\n\n    def statistics(self) -> EventStatistics:\n        orig_statistics = self.__original.statistics()\n        return EventStatistics(tasks_waiting=orig_statistics.tasks_waiting)\n\n    def set(self) -> DeprecatedAwaitable:\n        self.__original.set()\n        return DeprecatedAwaitable(self.set)\n\n\nclass CapacityLimiter(BaseCapacityLimiter):\n    def __new__(cls, *args: object, **kwargs: object) -> \"CapacityLimiter\":\n        return object.__new__(cls)\n\n    def __init__(self, *args: Any, original: Optional[trio.CapacityLimiter] = None) -> None:\n        self.__original = original or trio.CapacityLimiter(*args)\n\n    async def __aenter__(self) -> None:\n        return await self.__original.__aenter__()\n\n    async def __aexit__(self, exc_type: Optional[Type[BaseException]],\n                        exc_val: Optional[BaseException],\n                        exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        return await self.__original.__aexit__(exc_type, exc_val, exc_tb)\n\n    @property\n    def total_tokens(self) -> float:\n        return self.__original.total_tokens\n\n    @total_tokens.setter\n    def total_tokens(self, value: float) -> None:\n        self.__original.total_tokens = value\n\n    @property\n    def borrowed_tokens(self) -> int:\n        return self.__original.borrowed_tokens\n\n    @property\n    def available_tokens(self) -> float:\n        return self.__original.available_tokens\n\n    def acquire_nowait(self) -> DeprecatedAwaitable:\n        self.__original.acquire_nowait()\n        return DeprecatedAwaitable(self.acquire_nowait)\n\n    def acquire_on_behalf_of_nowait(self, borrower: object) -> DeprecatedAwaitable:\n        self.__original.acquire_on_behalf_of_nowait(borrower)\n        return DeprecatedAwaitable(self.acquire_on_behalf_of_nowait)\n\n    async def acquire(self) -> None:\n        await self.__original.acquire()\n\n    async def acquire_on_behalf_of(self, borrower: object) -> None:\n        await self.__original.acquire_on_behalf_of(borrower)\n\n    def release(self) -> None:\n        return self.__original.release()\n\n    def release_on_behalf_of(self, borrower: object) -> None:\n        return self.__original.release_on_behalf_of(borrower)\n\n    def statistics(self) -> CapacityLimiterStatistics:\n        orig = self.__original.statistics()\n        return CapacityLimiterStatistics(\n            borrowed_tokens=orig.borrowed_tokens, total_tokens=orig.total_tokens,\n            borrowers=orig.borrowers, tasks_waiting=orig.tasks_waiting)\n\n\n_capacity_limiter_wrapper: RunVar = RunVar('_capacity_limiter_wrapper')\n\n\ndef current_default_thread_limiter() -> CapacityLimiter:\n    try:\n        return _capacity_limiter_wrapper.get()\n    except LookupError:\n        limiter = CapacityLimiter(original=trio.to_thread.current_default_thread_limiter())\n        _capacity_limiter_wrapper.set(limiter)\n        return limiter\n\n\n#\n# Signal handling\n#\n\nclass _SignalReceiver(DeprecatedAsyncContextManager[T]):\n    def __init__(self, cm: ContextManager[T]):\n        self._cm = cm\n\n    def __enter__(self) -> T:\n        return self._cm.__enter__()\n\n    def __exit__(self, exc_type: Optional[Type[BaseException]],\n                 exc_val: Optional[BaseException],\n                 exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        return self._cm.__exit__(exc_type, exc_val, exc_tb)\n\n\ndef open_signal_receiver(*signals: Signals) -> _SignalReceiver:\n    cm = trio.open_signal_receiver(*signals)\n    return _SignalReceiver(cm)\n\n#\n# Testing and debugging\n#\n\n\ndef get_current_task() -> TaskInfo:\n    task = trio_lowlevel.current_task()\n\n    parent_id = None\n    if task.parent_nursery and task.parent_nursery.parent_task:\n        parent_id = id(task.parent_nursery.parent_task)\n\n    return TaskInfo(id(task), parent_id, task.name, task.coro)\n\n\ndef get_running_tasks() -> List[TaskInfo]:\n    root_task = trio_lowlevel.current_root_task()\n    task_infos = [TaskInfo(id(root_task), None, root_task.name, root_task.coro)]\n    nurseries = root_task.child_nurseries\n    while nurseries:\n        new_nurseries: List[trio.Nursery] = []\n        for nursery in nurseries:\n            for task in nursery.child_tasks:\n                task_infos.append(\n                    TaskInfo(id(task), id(nursery.parent_task), task.name, task.coro))\n                new_nurseries.extend(task.child_nurseries)\n\n        nurseries = new_nurseries\n\n    return task_infos\n\n\ndef wait_all_tasks_blocked() -> Awaitable[None]:\n    import trio.testing\n    return trio.testing.wait_all_tasks_blocked()\n\n\nclass TestRunner(abc.TestRunner):\n    def __init__(self, **options: Any) -> None:\n        from collections import deque\n        from queue import Queue\n\n        self._call_queue: \"Queue[Callable[..., object]]\" = Queue()\n        self._result_queue: Deque[Outcome] = deque()\n        self._stop_event: Optional[trio.Event] = None\n        self._nursery: Optional[trio.Nursery] = None\n        self._options = options\n\n    async def _trio_main(self) -> None:\n        self._stop_event = trio.Event()\n        async with trio.open_nursery() as self._nursery:\n            await self._stop_event.wait()\n\n    async def _call_func(self, func: Callable[..., Awaitable[object]],\n                         args: tuple, kwargs: dict) -> None:\n        try:\n            retval = await func(*args, **kwargs)\n        except BaseException as exc:\n            self._result_queue.append(Error(exc))\n        else:\n            self._result_queue.append(Value(retval))\n\n    def _main_task_finished(self, outcome: object) -> None:\n        self._nursery = None\n\n    def close(self) -> None:\n        if self._stop_event:\n            self._stop_event.set()\n            while self._nursery is not None:\n                self._call_queue.get()()\n\n    def call(self, func: Callable[..., Awaitable[T_Retval]],\n             *args: object, **kwargs: object) -> T_Retval:\n        if self._nursery is None:\n            trio.lowlevel.start_guest_run(\n                self._trio_main, run_sync_soon_threadsafe=self._call_queue.put,\n                done_callback=self._main_task_finished, **self._options)\n            while self._nursery is None:\n                self._call_queue.get()()\n\n        self._nursery.start_soon(self._call_func, func, args, kwargs)\n        while not self._result_queue:\n            self._call_queue.get()()\n\n        outcome = self._result_queue.pop()\n        return outcome.unwrap()\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_core/_exceptions.py", "content": "from traceback import format_exception\nfrom typing import List\n\n\nclass BrokenResourceError(Exception):\n    \"\"\"\n    Raised when trying to use a resource that has been rendered unusable due to external causes\n    (e.g. a send stream whose peer has disconnected).\n    \"\"\"\n\n\nclass BrokenWorkerProcess(Exception):\n    \"\"\"\n    Raised by :func:`run_sync_in_process` if the worker process terminates abruptly or otherwise\n    misbehaves.\n    \"\"\"\n\n\nclass BusyResourceError(Exception):\n    \"\"\"Raised when two tasks are trying to read from or write to the same resource concurrently.\"\"\"\n\n    def __init__(self, action: str):\n        super().__init__(f'Another task is already {action} this resource')\n\n\nclass ClosedResourceError(Exception):\n    \"\"\"Raised when trying to use a resource that has been closed.\"\"\"\n\n\nclass DelimiterNotFound(Exception):\n    \"\"\"\n    Raised during :meth:`~anyio.streams.buffered.BufferedByteReceiveStream.receive_until` if the\n    maximum number of bytes has been read without the delimiter being found.\n    \"\"\"\n\n    def __init__(self, max_bytes: int) -> None:\n        super().__init__(f'The delimiter was not found among the first {max_bytes} bytes')\n\n\nclass EndOfStream(Exception):\n    \"\"\"Raised when trying to read from a stream that has been closed from the other end.\"\"\"\n\n\nclass ExceptionGroup(BaseException):\n    \"\"\"\n    Raised when multiple exceptions have been raised in a task group.\n\n    :var ~typing.Sequence[BaseException] exceptions: the sequence of exceptions raised together\n    \"\"\"\n\n    SEPARATOR = '----------------------------\\n'\n\n    exceptions: List[BaseException]\n\n    def __str__(self) -> str:\n        tracebacks = [''.join(format_exception(type(exc), exc, exc.__traceback__))\n                      for exc in self.exceptions]\n        return f'{len(self.exceptions)} exceptions were raised in the task group:\\n' \\\n               f'{self.SEPARATOR}{self.SEPARATOR.join(tracebacks)}'\n\n    def __repr__(self) -> str:\n        exception_reprs = ', '.join(repr(exc) for exc in self.exceptions)\n        return f'<{self.__class__.__name__}: {exception_reprs}>'\n\n\nclass IncompleteRead(Exception):\n    \"\"\"\n    Raised during :meth:`~anyio.streams.buffered.BufferedByteReceiveStream.receive_exactly` or\n    :meth:`~anyio.streams.buffered.BufferedByteReceiveStream.receive_until` if the\n    connection is closed before the requested amount of bytes has been read.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__('The stream was closed before the read operation could be completed')\n\n\nclass TypedAttributeLookupError(LookupError):\n    \"\"\"\n    Raised by :meth:`~anyio.TypedAttributeProvider.extra` when the given typed attribute is not\n    found and no default value has been given.\n    \"\"\"\n\n\nclass WouldBlock(Exception):\n    \"\"\"Raised by ``X_nowait`` functions if ``X()`` would block.\"\"\"\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_core/_fileio.py", "content": "import os\nimport pathlib\nimport sys\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom os import PathLike\nfrom typing import (\n    IO, TYPE_CHECKING, Any, AnyStr, AsyncIterator, Callable, Generic, Iterable, Iterator, List,\n    Optional, Sequence, Tuple, Union, cast, overload)\n\nfrom .. import to_thread\nfrom ..abc import AsyncResource\n\nif sys.version_info >= (3, 8):\n    from typing import Final\nelse:\n    from typing_extensions import Final\n\nif TYPE_CHECKING:\n    from _typeshed import OpenBinaryMode, OpenTextMode, ReadableBuffer, WriteableBuffer\nelse:\n    ReadableBuffer = OpenBinaryMode = OpenTextMode = WriteableBuffer = object\n\n\nclass AsyncFile(AsyncResource, Generic[AnyStr]):\n    \"\"\"\n    An asynchronous file object.\n\n    This class wraps a standard file object and provides async friendly versions of the following\n    blocking methods (where available on the original file object):\n\n    * read\n    * read1\n    * readline\n    * readlines\n    * readinto\n    * readinto1\n    * write\n    * writelines\n    * truncate\n    * seek\n    * tell\n    * flush\n\n    All other methods are directly passed through.\n\n    This class supports the asynchronous context manager protocol which closes the underlying file\n    at the end of the context block.\n\n    This class also supports asynchronous iteration::\n\n        async with await open_file(...) as f:\n            async for line in f:\n                print(line)\n    \"\"\"\n\n    def __init__(self, fp: IO[AnyStr]) -> None:\n        self._fp: Any = fp\n\n    def __getattr__(self, name: str) -> object:\n        return getattr(self._fp, name)\n\n    @property\n    def wrapped(self) -> IO[AnyStr]:\n        \"\"\"The wrapped file object.\"\"\"\n        return self._fp\n\n    async def __aiter__(self) -> AsyncIterator[AnyStr]:\n        while True:\n            line = await self.readline()\n            if line:\n                yield line\n            else:\n                break\n\n    async def aclose(self) -> None:\n        return await to_thread.run_sync(self._fp.close)\n\n    async def read(self, size: int = -1) -> AnyStr:\n        return await to_thread.run_sync(self._fp.read, size)\n\n    async def read1(self: 'AsyncFile[bytes]', size: int = -1) -> bytes:\n        return await to_thread.run_sync(self._fp.read1, size)\n\n    async def readline(self) -> AnyStr:\n        return await to_thread.run_sync(self._fp.readline)\n\n    async def readlines(self) -> List[AnyStr]:\n        return await to_thread.run_sync(self._fp.readlines)\n\n    async def readinto(self: 'AsyncFile[bytes]', b: WriteableBuffer) -> bytes:\n        return await to_thread.run_sync(self._fp.readinto, b)\n\n    async def readinto1(self: 'AsyncFile[bytes]', b: WriteableBuffer) -> bytes:\n        return await to_thread.run_sync(self._fp.readinto1, b)\n\n    @overload\n    async def write(self: 'AsyncFile[bytes]', b: ReadableBuffer) -> int: ...\n\n    @overload\n    async def write(self: 'AsyncFile[str]', b: str) -> int: ...\n\n    async def write(self, b: Union[ReadableBuffer, str]) -> int:\n        return await to_thread.run_sync(self._fp.write, b)\n\n    @overload\n    async def writelines(self: 'AsyncFile[bytes]', lines: Iterable[ReadableBuffer]) -> None: ...\n\n    @overload\n    async def writelines(self: 'AsyncFile[str]', lines: Iterable[str]) -> None: ...\n\n    async def writelines(self, lines: Union[Iterable[ReadableBuffer], Iterable[str]]) -> None:\n        return await to_thread.run_sync(self._fp.writelines, lines)\n\n    async def truncate(self, size: Optional[int] = None) -> int:\n        return await to_thread.run_sync(self._fp.truncate, size)\n\n    async def seek(self, offset: int, whence: Optional[int] = os.SEEK_SET) -> int:\n        return await to_thread.run_sync(self._fp.seek, offset, whence)\n\n    async def tell(self) -> int:\n        return await to_thread.run_sync(self._fp.tell)\n\n    async def flush(self) -> None:\n        return await to_thread.run_sync(self._fp.flush)\n\n\n@overload\nasync def open_file(file: Union[str, 'PathLike[str]', int], mode: OpenBinaryMode,\n                    buffering: int = ..., encoding: Optional[str] = ...,\n                    errors: Optional[str] = ..., newline: Optional[str] = ..., closefd: bool = ...,\n                    opener: Optional[Callable[[str, int], int]] = ...) -> AsyncFile[bytes]:\n    ...\n\n\n@overload\nasync def open_file(file: Union[str, 'PathLike[str]', int], mode: OpenTextMode = ...,\n                    buffering: int = ..., encoding: Optional[str] = ...,\n                    errors: Optional[str] = ..., newline: Optional[str] = ..., closefd: bool = ...,\n                    opener: Optional[Callable[[str, int], int]] = ...) -> AsyncFile[str]:\n    ...\n\n\nasync def open_file(file: Union[str, 'PathLike[str]', int], mode: str = 'r', buffering: int = -1,\n                    encoding: Optional[str] = None, errors: Optional[str] = None,\n                    newline: Optional[str] = None, closefd: bool = True,\n                    opener: Optional[Callable[[str, int], int]] = None) -> AsyncFile[Any]:\n    \"\"\"\n    Open a file asynchronously.\n\n    The arguments are exactly the same as for the builtin :func:`open`.\n\n    :return: an asynchronous file object\n\n    \"\"\"\n    fp = await to_thread.run_sync(open, file, mode, buffering, encoding, errors, newline,\n                                  closefd, opener)\n    return AsyncFile(fp)\n\n\ndef wrap_file(file: IO[AnyStr]) -> AsyncFile[AnyStr]:\n    \"\"\"\n    Wrap an existing file as an asynchronous file.\n\n    :param file: an existing file-like object\n    :return: an asynchronous file object\n\n    \"\"\"\n    return AsyncFile(file)\n\n\n@dataclass(eq=False)\nclass _PathIterator(AsyncIterator['Path']):\n    iterator: Iterator['PathLike[str]']\n\n    async def __anext__(self) -> 'Path':\n        nextval = await to_thread.run_sync(next, self.iterator, None, cancellable=True)\n        if nextval is None:\n            raise StopAsyncIteration from None\n\n        return Path(cast('PathLike[str]', nextval))\n\n\nclass Path:\n    \"\"\"\n    An asynchronous version of :class:`pathlib.Path`.\n\n    This class cannot be substituted for :class:`pathlib.Path` or :class:`pathlib.PurePath`, but\n    it is compatible with the :class:`os.PathLike` interface.\n\n    It implements the Python 3.10 version of :class:`pathlib.Path` interface, except for the\n    deprecated :meth:`~pathlib.Path.link_to` method.\n\n    Any methods that do disk I/O need to be awaited on. These methods are:\n\n    * :meth:`~pathlib.Path.absolute`\n    * :meth:`~pathlib.Path.chmod`\n    * :meth:`~pathlib.Path.cwd`\n    * :meth:`~pathlib.Path.exists`\n    * :meth:`~pathlib.Path.expanduser`\n    * :meth:`~pathlib.Path.group`\n    * :meth:`~pathlib.Path.hardlink_to`\n    * :meth:`~pathlib.Path.home`\n    * :meth:`~pathlib.Path.is_block_device`\n    * :meth:`~pathlib.Path.is_char_device`\n    * :meth:`~pathlib.Path.is_dir`\n    * :meth:`~pathlib.Path.is_fifo`\n    * :meth:`~pathlib.Path.is_file`\n    * :meth:`~pathlib.Path.is_mount`\n    * :meth:`~pathlib.Path.lchmod`\n    * :meth:`~pathlib.Path.lstat`\n    * :meth:`~pathlib.Path.mkdir`\n    * :meth:`~pathlib.Path.open`\n    * :meth:`~pathlib.Path.owner`\n    * :meth:`~pathlib.Path.read_bytes`\n    * :meth:`~pathlib.Path.read_text`\n    * :meth:`~pathlib.Path.readlink`\n    * :meth:`~pathlib.Path.rename`\n    * :meth:`~pathlib.Path.replace`\n    * :meth:`~pathlib.Path.rmdir`\n    * :meth:`~pathlib.Path.samefile`\n    * :meth:`~pathlib.Path.stat`\n    * :meth:`~pathlib.Path.touch`\n    * :meth:`~pathlib.Path.unlink`\n    * :meth:`~pathlib.Path.write_bytes`\n    * :meth:`~pathlib.Path.write_text`\n\n    Additionally, the following methods return an async iterator yielding :class:`~.Path` objects:\n\n    * :meth:`~pathlib.Path.glob`\n    * :meth:`~pathlib.Path.iterdir`\n    * :meth:`~pathlib.Path.rglob`\n    \"\"\"\n\n    __slots__ = '_path', '__weakref__'\n\n    __weakref__: Any\n\n    def __init__(self, *args: Union[str, 'PathLike[str]']) -> None:\n        self._path: Final[pathlib.Path] = pathlib.Path(*args)\n\n    def __fspath__(self) -> str:\n        return self._path.__fspath__()\n\n    def __str__(self) -> str:\n        return self._path.__str__()\n\n    def __repr__(self) -> str:\n        return f'{self.__class__.__name__}({self.as_posix()!r})'\n\n    def __bytes__(self) -> bytes:\n        return self._path.__bytes__()\n\n    def __hash__(self) -> int:\n        return self._path.__hash__()\n\n    def __eq__(self, other: object) -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__eq__(target)\n\n    def __lt__(self, other: 'Path') -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__lt__(target)\n\n    def __le__(self, other: 'Path') -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__le__(target)\n\n    def __gt__(self, other: 'Path') -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__gt__(target)\n\n    def __ge__(self, other: 'Path') -> bool:\n        target = other._path if isinstance(other, Path) else other\n        return self._path.__ge__(target)\n\n    def __truediv__(self, other: Any) -> 'Path':\n        return Path(self._path / other)\n\n    def __rtruediv__(self, other: Any) -> 'Path':\n        return Path(other) / self\n\n    @property\n    def parts(self) -> Tuple[str, ...]:\n        return self._path.parts\n\n    @property\n    def drive(self) -> str:\n        return self._path.drive\n\n    @property\n    def root(self) -> str:\n        return self._path.root\n\n    @property\n    def anchor(self) -> str:\n        return self._path.anchor\n\n    @property\n    def parents(self) -> Sequence['Path']:\n        return tuple(Path(p) for p in self._path.parents)\n\n    @property\n    def parent(self) -> 'Path':\n        return Path(self._path.parent)\n\n    @property\n    def name(self) -> str:\n        return self._path.name\n\n    @property\n    def suffix(self) -> str:\n        return self._path.suffix\n\n    @property\n    def suffixes(self) -> List[str]:\n        return self._path.suffixes\n\n    @property\n    def stem(self) -> str:\n        return self._path.stem\n\n    async def absolute(self) -> 'Path':\n        path = await to_thread.run_sync(self._path.absolute)\n        return Path(path)\n\n    def as_posix(self) -> str:\n        return self._path.as_posix()\n\n    def as_uri(self) -> str:\n        return self._path.as_uri()\n\n    def match(self, path_pattern: str) -> bool:\n        return self._path.match(path_pattern)\n\n    def is_relative_to(self, *other: Union[str, 'PathLike[str]']) -> bool:\n        try:\n            self.relative_to(*other)\n            return True\n        except ValueError:\n            return False\n\n    async def chmod(self, mode: int, *, follow_symlinks: bool = True) -> None:\n        func = partial(os.chmod, follow_symlinks=follow_symlinks)\n        return await to_thread.run_sync(func, self._path, mode)\n\n    @classmethod\n    async def cwd(cls) -> 'Path':\n        path = await to_thread.run_sync(pathlib.Path.cwd)\n        return cls(path)\n\n    async def exists(self) -> bool:\n        return await to_thread.run_sync(self._path.exists, cancellable=True)\n\n    async def expanduser(self) -> 'Path':\n        return Path(await to_thread.run_sync(self._path.expanduser, cancellable=True))\n\n    def glob(self, pattern: str) -> AsyncIterator['Path']:\n        gen = self._path.glob(pattern)\n        return _PathIterator(gen)\n\n    async def group(self) -> str:\n        return await to_thread.run_sync(self._path.group, cancellable=True)\n\n    async def hardlink_to(self, target: Union[str, pathlib.Path, 'Path']) -> None:\n        if isinstance(target, Path):\n            target = target._path\n\n        await to_thread.run_sync(os.link, target, self)\n\n    @classmethod\n    async def home(cls) -> 'Path':\n        home_path = await to_thread.run_sync(pathlib.Path.home)\n        return cls(home_path)\n\n    def is_absolute(self) -> bool:\n        return self._path.is_absolute()\n\n    async def is_block_device(self) -> bool:\n        return await to_thread.run_sync(self._path.is_block_device, cancellable=True)\n\n    async def is_char_device(self) -> bool:\n        return await to_thread.run_sync(self._path.is_char_device, cancellable=True)\n\n    async def is_dir(self) -> bool:\n        return await to_thread.run_sync(self._path.is_dir, cancellable=True)\n\n    async def is_fifo(self) -> bool:\n        return await to_thread.run_sync(self._path.is_fifo, cancellable=True)\n\n    async def is_file(self) -> bool:\n        return await to_thread.run_sync(self._path.is_file, cancellable=True)\n\n    async def is_mount(self) -> bool:\n        return await to_thread.run_sync(os.path.ismount, self._path, cancellable=True)\n\n    def is_reserved(self) -> bool:\n        return self._path.is_reserved()\n\n    async def is_socket(self) -> bool:\n        return await to_thread.run_sync(self._path.is_socket, cancellable=True)\n\n    async def is_symlink(self) -> bool:\n        return await to_thread.run_sync(self._path.is_symlink, cancellable=True)\n\n    def iterdir(self) -> AsyncIterator['Path']:\n        gen = self._path.iterdir()\n        return _PathIterator(gen)\n\n    def joinpath(self, *args: Union[str, 'PathLike[str]']) -> 'Path':\n        return Path(self._path.joinpath(*args))\n\n    async def lchmod(self, mode: int) -> None:\n        await to_thread.run_sync(self._path.lchmod, mode)\n\n    async def lstat(self) -> os.stat_result:\n        return await to_thread.run_sync(self._path.lstat, cancellable=True)\n\n    async def mkdir(self, mode: int = 0o777, parents: bool = False,\n                    exist_ok: bool = False) -> None:\n        await to_thread.run_sync(self._path.mkdir, mode, parents, exist_ok)\n\n    @overload\n    async def open(self, mode: OpenBinaryMode, buffering: int = ..., encoding: Optional[str] = ...,\n                   errors: Optional[str] = ..., newline: Optional[str] = ...) -> AsyncFile[bytes]:\n        ...\n\n    @overload\n    async def open(self, mode: OpenTextMode = ..., buffering: int = ...,\n                   encoding: Optional[str] = ..., errors: Optional[str] = ...,\n                   newline: Optional[str] = ...) -> AsyncFile[str]:\n        ...\n\n    async def open(self, mode: str = 'r', buffering: int = -1, encoding: Optional[str] = None,\n                   errors: Optional[str] = None, newline: Optional[str] = None) -> AsyncFile[Any]:\n        fp = await to_thread.run_sync(self._path.open, mode, buffering, encoding, errors, newline)\n        return AsyncFile(fp)\n\n    async def owner(self) -> str:\n        return await to_thread.run_sync(self._path.owner, cancellable=True)\n\n    async def read_bytes(self) -> bytes:\n        return await to_thread.run_sync(self._path.read_bytes)\n\n    async def read_text(self, encoding: Optional[str] = None, errors: Optional[str] = None) -> str:\n        return await to_thread.run_sync(self._path.read_text, encoding, errors)\n\n    def relative_to(self, *other: Union[str, 'PathLike[str]']) -> 'Path':\n        return Path(self._path.relative_to(*other))\n\n    async def readlink(self) -> 'Path':\n        target = await to_thread.run_sync(os.readlink, self._path)\n        return Path(cast(str, target))\n\n    async def rename(self, target: Union[str, pathlib.PurePath, 'Path']) -> 'Path':\n        if isinstance(target, Path):\n            target = target._path\n\n        await to_thread.run_sync(self._path.rename, target)\n        return Path(target)\n\n    async def replace(self, target: Union[str, pathlib.PurePath, 'Path']) -> 'Path':\n        if isinstance(target, Path):\n            target = target._path\n\n        await to_thread.run_sync(self._path.replace, target)\n        return Path(target)\n\n    async def resolve(self, strict: bool = False) -> 'Path':\n        func = partial(self._path.resolve, strict=strict)\n        return Path(await to_thread.run_sync(func, cancellable=True))\n\n    def rglob(self, pattern: str) -> AsyncIterator['Path']:\n        gen = self._path.rglob(pattern)\n        return _PathIterator(gen)\n\n    async def rmdir(self) -> None:\n        await to_thread.run_sync(self._path.rmdir)\n\n    async def samefile(self, other_path: Union[str, bytes, int, pathlib.Path, 'Path']) -> bool:\n        if isinstance(other_path, Path):\n            other_path = other_path._path\n\n        return await to_thread.run_sync(self._path.samefile, other_path, cancellable=True)\n\n    async def stat(self, *, follow_symlinks: bool = True) -> os.stat_result:\n        func = partial(os.stat, follow_symlinks=follow_symlinks)\n        return await to_thread.run_sync(func, self._path, cancellable=True)\n\n    async def symlink_to(self, target: Union[str, pathlib.Path, 'Path'],\n                         target_is_directory: bool = False) -> None:\n        if isinstance(target, Path):\n            target = target._path\n\n        await to_thread.run_sync(self._path.symlink_to, target, target_is_directory)\n\n    async def touch(self, mode: int = 0o666, exist_ok: bool = True) -> None:\n        await to_thread.run_sync(self._path.touch, mode, exist_ok)\n\n    async def unlink(self, missing_ok: bool = False) -> None:\n        try:\n            await to_thread.run_sync(self._path.unlink)\n        except FileNotFoundError:\n            if not missing_ok:\n                raise\n\n    def with_name(self, name: str) -> 'Path':\n        return Path(self._path.with_name(name))\n\n    def with_stem(self, stem: str) -> 'Path':\n        return Path(self._path.with_name(stem + self._path.suffix))\n\n    def with_suffix(self, suffix: str) -> 'Path':\n        return Path(self._path.with_suffix(suffix))\n\n    async def write_bytes(self, data: bytes) -> int:\n        return await to_thread.run_sync(self._path.write_bytes, data)\n\n    async def write_text(self, data: str, encoding: Optional[str] = None,\n                         errors: Optional[str] = None, newline: Optional[str] = None) -> int:\n        # Path.write_text() does not support the \"newline\" parameter before Python 3.10\n        def sync_write_text() -> int:\n            with self._path.open('w', encoding=encoding, errors=errors, newline=newline) as fp:\n                return fp.write(data)\n\n        return await to_thread.run_sync(sync_write_text)\n\n\nPathLike.register(Path)\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_core/_tasks.py", "content": "import math\nfrom types import TracebackType\nfrom typing import Optional, Type\nfrom warnings import warn\n\nfrom ..abc._tasks import TaskGroup, TaskStatus\nfrom ._compat import DeprecatedAsyncContextManager, DeprecatedAwaitable, DeprecatedAwaitableFloat\nfrom ._eventloop import get_asynclib\n\n\nclass _IgnoredTaskStatus(TaskStatus):\n    def started(self, value: object = None) -> None:\n        pass\n\n\nTASK_STATUS_IGNORED = _IgnoredTaskStatus()\n\n\nclass CancelScope(DeprecatedAsyncContextManager['CancelScope']):\n    \"\"\"\n    Wraps a unit of work that can be made separately cancellable.\n\n    :param deadline: The time (clock value) when this scope is cancelled automatically\n    :param shield: ``True`` to shield the cancel scope from external cancellation\n    \"\"\"\n\n    def __new__(cls, *, deadline: float = math.inf, shield: bool = False) -> 'CancelScope':\n        return get_asynclib().CancelScope(shield=shield, deadline=deadline)\n\n    def cancel(self) -> DeprecatedAwaitable:\n        \"\"\"Cancel this scope immediately.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def deadline(self) -> float:\n        \"\"\"\n        The time (clock value) when this scope is cancelled automatically.\n\n        Will be ``float('inf')`` if no timeout has been set.\n\n        \"\"\"\n        raise NotImplementedError\n\n    @deadline.setter\n    def deadline(self, value: float) -> None:\n        raise NotImplementedError\n\n    @property\n    def cancel_called(self) -> bool:\n        \"\"\"``True`` if :meth:`cancel` has been called.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def shield(self) -> bool:\n        \"\"\"\n        ``True`` if this scope is shielded from external cancellation.\n\n        While a scope is shielded, it will not receive cancellations from outside.\n\n        \"\"\"\n        raise NotImplementedError\n\n    @shield.setter\n    def shield(self, value: bool) -> None:\n        raise NotImplementedError\n\n    def __enter__(self) -> 'CancelScope':\n        raise NotImplementedError\n\n    def __exit__(self, exc_type: Optional[Type[BaseException]],\n                 exc_val: Optional[BaseException],\n                 exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        raise NotImplementedError\n\n\ndef open_cancel_scope(*, shield: bool = False) -> CancelScope:\n    \"\"\"\n    Open a cancel scope.\n\n    :param shield: ``True`` to shield the cancel scope from external cancellation\n    :return: a cancel scope\n\n    .. deprecated:: 3.0\n       Use :class:`~CancelScope` directly.\n\n    \"\"\"\n    warn('open_cancel_scope() is deprecated -- use CancelScope() directly', DeprecationWarning)\n    return get_asynclib().CancelScope(shield=shield)\n\n\nclass FailAfterContextManager(DeprecatedAsyncContextManager[CancelScope]):\n    def __init__(self, cancel_scope: CancelScope):\n        self._cancel_scope = cancel_scope\n\n    def __enter__(self) -> CancelScope:\n        return self._cancel_scope.__enter__()\n\n    def __exit__(self, exc_type: Optional[Type[BaseException]],\n                 exc_val: Optional[BaseException],\n                 exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        retval = self._cancel_scope.__exit__(exc_type, exc_val, exc_tb)\n        if self._cancel_scope.cancel_called:\n            raise TimeoutError\n\n        return retval\n\n\ndef fail_after(delay: Optional[float], shield: bool = False) -> FailAfterContextManager:\n    \"\"\"\n    Create a context manager which raises a :class:`TimeoutError` if does not finish in time.\n\n    :param delay: maximum allowed time (in seconds) before raising the exception, or ``None`` to\n        disable the timeout\n    :param shield: ``True`` to shield the cancel scope from external cancellation\n    :return: a context manager that yields a cancel scope\n    :rtype: :class:`~typing.ContextManager`\\\\[:class:`~anyio.abc.CancelScope`\\\\]\n\n    \"\"\"\n    deadline = (get_asynclib().current_time() + delay) if delay is not None else math.inf\n    cancel_scope = get_asynclib().CancelScope(deadline=deadline, shield=shield)\n    return FailAfterContextManager(cancel_scope)\n\n\ndef move_on_after(delay: Optional[float], shield: bool = False) -> CancelScope:\n    \"\"\"\n    Create a cancel scope with a deadline that expires after the given delay.\n\n    :param delay: maximum allowed time (in seconds) before exiting the context block, or ``None``\n        to disable the timeout\n    :param shield: ``True`` to shield the cancel scope from external cancellation\n    :return: a cancel scope\n\n    \"\"\"\n    deadline = (get_asynclib().current_time() + delay) if delay is not None else math.inf\n    return get_asynclib().CancelScope(deadline=deadline, shield=shield)\n\n\ndef current_effective_deadline() -> DeprecatedAwaitableFloat:\n    \"\"\"\n    Return the nearest deadline among all the cancel scopes effective for the current task.\n\n    :return: a clock value from the event loop's internal clock (``float('inf')`` if there is no\n        deadline in effect)\n    :rtype: float\n\n    \"\"\"\n    return DeprecatedAwaitableFloat(get_asynclib().current_effective_deadline(),\n                                    current_effective_deadline)\n\n\ndef create_task_group() -> 'TaskGroup':\n    \"\"\"\n    Create a task group.\n\n    :return: a task group\n\n    \"\"\"\n    return get_asynclib().TaskGroup()\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_core/_sockets.py", "content": "import socket\nimport ssl\nimport sys\nfrom ipaddress import IPv6Address, ip_address\nfrom os import PathLike, chmod\nfrom pathlib import Path\nfrom socket import AddressFamily, SocketKind\nfrom typing import Awaitable, List, Optional, Tuple, Union, cast, overload\n\nfrom .. import to_thread\nfrom ..abc import (\n    ConnectedUDPSocket, IPAddressType, IPSockAddrType, SocketListener, SocketStream, UDPSocket,\n    UNIXSocketStream)\nfrom ..streams.stapled import MultiListener\nfrom ..streams.tls import TLSStream\nfrom ._eventloop import get_asynclib\nfrom ._resources import aclose_forcefully\nfrom ._synchronization import Event\nfrom ._tasks import create_task_group, move_on_after\n\nif sys.version_info >= (3, 8):\n    from typing import Literal\nelse:\n    from typing_extensions import Literal\n\nIPPROTO_IPV6 = getattr(socket, 'IPPROTO_IPV6', 41)  # https://bugs.python.org/issue29515\n\nGetAddrInfoReturnType = List[Tuple[AddressFamily, SocketKind, int, str, Tuple[str, int]]]\nAnyIPAddressFamily = Literal[AddressFamily.AF_UNSPEC, AddressFamily.AF_INET,\n                             AddressFamily.AF_INET6]\nIPAddressFamily = Literal[AddressFamily.AF_INET, AddressFamily.AF_INET6]\n\n\n# tls_hostname given\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType, remote_port: int, *, local_host: Optional[IPAddressType] = ...,\n    ssl_context: Optional[ssl.SSLContext] = ..., tls_standard_compatible: bool = ...,\n    tls_hostname: str, happy_eyeballs_delay: float = ...\n) -> TLSStream:\n    ...\n\n\n# ssl_context given\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType, remote_port: int, *, local_host: Optional[IPAddressType] = ...,\n    ssl_context: ssl.SSLContext, tls_standard_compatible: bool = ...,\n    tls_hostname: Optional[str] = ..., happy_eyeballs_delay: float = ...\n) -> TLSStream:\n    ...\n\n\n# tls=True\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType, remote_port: int, *, local_host: Optional[IPAddressType] = ...,\n    tls: Literal[True], ssl_context: Optional[ssl.SSLContext] = ...,\n    tls_standard_compatible: bool = ..., tls_hostname: Optional[str] = ...,\n    happy_eyeballs_delay: float = ...\n) -> TLSStream:\n    ...\n\n\n# tls=False\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType, remote_port: int, *, local_host: Optional[IPAddressType] = ...,\n    tls: Literal[False], ssl_context: Optional[ssl.SSLContext] = ...,\n    tls_standard_compatible: bool = ..., tls_hostname: Optional[str] = ...,\n    happy_eyeballs_delay: float = ...\n) -> SocketStream:\n    ...\n\n\n# No TLS arguments\n@overload\nasync def connect_tcp(\n    remote_host: IPAddressType, remote_port: int, *, local_host: Optional[IPAddressType] = ...,\n    happy_eyeballs_delay: float = ...\n) -> SocketStream:\n    ...\n\n\nasync def connect_tcp(\n    remote_host: IPAddressType, remote_port: int, *, local_host: Optional[IPAddressType] = None,\n    tls: bool = False, ssl_context: Optional[ssl.SSLContext] = None,\n    tls_standard_compatible: bool = True, tls_hostname: Optional[str] = None,\n    happy_eyeballs_delay: float = 0.25\n) -> Union[SocketStream, TLSStream]:\n    \"\"\"\n    Connect to a host using the TCP protocol.\n\n    This function implements the stateless version of the Happy Eyeballs algorithm (RFC 6555).\n    If ``address`` is a host name that resolves to multiple IP addresses, each one is tried until\n    one connection attempt succeeds. If the first attempt does not connected within 250\n    milliseconds, a second attempt is started using the next address in the list, and so on.\n    On IPv6 enabled systems, an IPv6 address (if available) is tried first.\n\n    When the connection has been established, a TLS handshake will be done if either\n    ``ssl_context`` or ``tls_hostname`` is not ``None``, or if ``tls`` is ``True``.\n\n    :param remote_host: the IP address or host name to connect to\n    :param remote_port: port on the target host to connect to\n    :param local_host: the interface address or name to bind the socket to before connecting\n    :param tls: ``True`` to do a TLS handshake with the connected stream and return a\n        :class:`~anyio.streams.tls.TLSStream` instead\n    :param ssl_context: the SSL context object to use (if omitted, a default context is created)\n    :param tls_standard_compatible: If ``True``, performs the TLS shutdown handshake before closing\n        the stream and requires that the server does this as well. Otherwise,\n        :exc:`~ssl.SSLEOFError` may be raised during reads from the stream.\n        Some protocols, such as HTTP, require this option to be ``False``.\n        See :meth:`~ssl.SSLContext.wrap_socket` for details.\n    :param tls_hostname: host name to check the server certificate against (defaults to the value\n        of ``remote_host``)\n    :param happy_eyeballs_delay: delay (in seconds) before starting the next connection attempt\n    :return: a socket stream object if no TLS handshake was done, otherwise a TLS stream\n    :raises OSError: if the connection attempt fails\n\n    \"\"\"\n    # Placed here due to https://github.com/python/mypy/issues/7057\n    connected_stream: Optional[SocketStream] = None\n\n    async def try_connect(remote_host: str, event: Event) -> None:\n        nonlocal connected_stream\n        try:\n            stream = await asynclib.connect_tcp(remote_host, remote_port, local_address)\n        except OSError as exc:\n            oserrors.append(exc)\n            return\n        else:\n            if connected_stream is None:\n                connected_stream = stream\n                tg.cancel_scope.cancel()\n            else:\n                await stream.aclose()\n        finally:\n            event.set()\n\n    asynclib = get_asynclib()\n    local_address: Optional[IPSockAddrType] = None\n    family = socket.AF_UNSPEC\n    if local_host:\n        gai_res = await getaddrinfo(str(local_host), None)\n        family, *_, local_address = gai_res[0]\n\n    target_host = str(remote_host)\n    try:\n        addr_obj = ip_address(remote_host)\n    except ValueError:\n        # getaddrinfo() will raise an exception if name resolution fails\n        gai_res = await getaddrinfo(target_host, remote_port, family=family,\n                                    type=socket.SOCK_STREAM)\n\n        # Organize the list so that the first address is an IPv6 address (if available) and the\n        # second one is an IPv4 addresses. The rest can be in whatever order.\n        v6_found = v4_found = False\n        target_addrs: List[Tuple[socket.AddressFamily, str]] = []\n        for af, *rest, sa in gai_res:\n            if af == socket.AF_INET6 and not v6_found:\n                v6_found = True\n                target_addrs.insert(0, (af, sa[0]))\n            elif af == socket.AF_INET and not v4_found and v6_found:\n                v4_found = True\n                target_addrs.insert(1, (af, sa[0]))\n            else:\n                target_addrs.append((af, sa[0]))\n    else:\n        if isinstance(addr_obj, IPv6Address):\n            target_addrs = [(socket.AF_INET6, addr_obj.compressed)]\n        else:\n            target_addrs = [(socket.AF_INET, addr_obj.compressed)]\n\n    oserrors: List[OSError] = []\n    async with create_task_group() as tg:\n        for i, (af, addr) in enumerate(target_addrs):\n            event = Event()\n            tg.start_soon(try_connect, addr, event)\n            with move_on_after(happy_eyeballs_delay):\n                await event.wait()\n\n    if connected_stream is None:\n        cause = oserrors[0] if len(oserrors) == 1 else asynclib.ExceptionGroup(oserrors)\n        raise OSError('All connection attempts failed') from cause\n\n    if tls or tls_hostname or ssl_context:\n        try:\n            return await TLSStream.wrap(connected_stream, server_side=False,\n                                        hostname=tls_hostname or str(remote_host),\n                                        ssl_context=ssl_context,\n                                        standard_compatible=tls_standard_compatible)\n        except BaseException:\n            await aclose_forcefully(connected_stream)\n            raise\n\n    return connected_stream\n\n\nasync def connect_unix(path: Union[str, 'PathLike[str]']) -> UNIXSocketStream:\n    \"\"\"\n    Connect to the given UNIX socket.\n\n    Not available on Windows.\n\n    :param path: path to the socket\n    :return: a socket stream object\n\n    \"\"\"\n    path = str(Path(path))\n    return await get_asynclib().connect_unix(path)\n\n\nasync def create_tcp_listener(\n    *, local_host: Optional[IPAddressType] = None, local_port: int = 0,\n    family: AnyIPAddressFamily = socket.AddressFamily.AF_UNSPEC, backlog: int = 65536,\n    reuse_port: bool = False\n) -> MultiListener[SocketStream]:\n    \"\"\"\n    Create a TCP socket listener.\n\n    :param local_port: port number to listen on\n    :param local_host: IP address of the interface to listen on. If omitted, listen on all IPv4\n        and IPv6 interfaces. To listen on all interfaces on a specific address family, use\n        ``0.0.0.0`` for IPv4 or ``::`` for IPv6.\n    :param family: address family (used if ``interface`` was omitted)\n    :param backlog: maximum number of queued incoming connections (up to a maximum of 2**16, or\n        65536)\n    :param reuse_port: ``True`` to allow multiple sockets to bind to the same address/port\n        (not supported on Windows)\n    :return: a list of listener objects\n\n    \"\"\"\n    asynclib = get_asynclib()\n    backlog = min(backlog, 65536)\n    local_host = str(local_host) if local_host is not None else None\n    gai_res = await getaddrinfo(local_host, local_port, family=family,  # type: ignore[arg-type]\n                                type=socket.SOCK_STREAM,\n                                flags=socket.AI_PASSIVE | socket.AI_ADDRCONFIG)\n    listeners: List[SocketListener] = []\n    try:\n        # The set() is here to work around a glibc bug:\n        # https://sourceware.org/bugzilla/show_bug.cgi?id=14969\n        for fam, *_, sockaddr in sorted(set(gai_res)):\n            raw_socket = socket.socket(fam)\n            raw_socket.setblocking(False)\n\n            # For Windows, enable exclusive address use. For others, enable address reuse.\n            if sys.platform == 'win32':\n                raw_socket.setsockopt(socket.SOL_SOCKET, socket.SO_EXCLUSIVEADDRUSE, 1)\n            else:\n                raw_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n\n            if reuse_port:\n                raw_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n\n            # If only IPv6 was requested, disable dual stack operation\n            if fam == socket.AF_INET6:\n                raw_socket.setsockopt(IPPROTO_IPV6, socket.IPV6_V6ONLY, 1)\n\n            raw_socket.bind(sockaddr)\n            raw_socket.listen(backlog)\n            listener = asynclib.TCPSocketListener(raw_socket)\n            listeners.append(listener)\n    except BaseException:\n        for listener in listeners:\n            await listener.aclose()\n\n        raise\n\n    return MultiListener(listeners)\n\n\nasync def create_unix_listener(\n        path: Union[str, 'PathLike[str]'], *, mode: Optional[int] = None,\n        backlog: int = 65536) -> SocketListener:\n    \"\"\"\n    Create a UNIX socket listener.\n\n    Not available on Windows.\n\n    :param path: path of the socket\n    :param mode: permissions to set on the socket\n    :param backlog: maximum number of queued incoming connections (up to a maximum of 2**16, or\n        65536)\n    :return: a listener object\n\n    .. versionchanged:: 3.0\n        If a socket already exists on the file system in the given path, it will be removed first.\n\n    \"\"\"\n    path_str = str(path)\n    path = Path(path)\n    if path.is_socket():\n        path.unlink()\n\n    backlog = min(backlog, 65536)\n    raw_socket = socket.socket(socket.AF_UNIX)\n    raw_socket.setblocking(False)\n    try:\n        await to_thread.run_sync(raw_socket.bind, path_str, cancellable=True)\n        if mode is not None:\n            await to_thread.run_sync(chmod, path_str, mode, cancellable=True)\n\n        raw_socket.listen(backlog)\n        return get_asynclib().UNIXSocketListener(raw_socket)\n    except BaseException:\n        raw_socket.close()\n        raise\n\n\nasync def create_udp_socket(\n    family: AnyIPAddressFamily = AddressFamily.AF_UNSPEC, *,\n    local_host: Optional[IPAddressType] = None, local_port: int = 0, reuse_port: bool = False\n) -> UDPSocket:\n    \"\"\"\n    Create a UDP socket.\n\n    If ``port`` has been given, the socket will be bound to this port on the local machine,\n    making this socket suitable for providing UDP based services.\n\n    :param family: address family (``AF_INET`` or ``AF_INET6``) – automatically determined from\n        ``local_host`` if omitted\n    :param local_host: IP address or host name of the local interface to bind to\n    :param local_port: local port to bind to\n    :param reuse_port: ``True`` to allow multiple sockets to bind to the same address/port\n        (not supported on Windows)\n    :return: a UDP socket\n\n    \"\"\"\n    if family is AddressFamily.AF_UNSPEC and not local_host:\n        raise ValueError('Either \"family\" or \"local_host\" must be given')\n\n    if local_host:\n        gai_res = await getaddrinfo(str(local_host), local_port, family=family,\n                                    type=socket.SOCK_DGRAM,\n                                    flags=socket.AI_PASSIVE | socket.AI_ADDRCONFIG)\n        family = cast(AnyIPAddressFamily, gai_res[0][0])\n        local_address = gai_res[0][-1]\n    elif family is AddressFamily.AF_INET6:\n        local_address = ('::', 0)\n    else:\n        local_address = ('0.0.0.0', 0)\n\n    return await get_asynclib().create_udp_socket(family, local_address, None, reuse_port)\n\n\nasync def create_connected_udp_socket(\n    remote_host: IPAddressType, remote_port: int, *,\n    family: AnyIPAddressFamily = AddressFamily.AF_UNSPEC,\n    local_host: Optional[IPAddressType] = None, local_port: int = 0, reuse_port: bool = False\n) -> ConnectedUDPSocket:\n    \"\"\"\n    Create a connected UDP socket.\n\n    Connected UDP sockets can only communicate with the specified remote host/port, and any packets\n    sent from other sources are dropped.\n\n    :param remote_host: remote host to set as the default target\n    :param remote_port: port on the remote host to set as the default target\n    :param family: address family (``AF_INET`` or ``AF_INET6``) – automatically determined from\n        ``local_host`` or ``remote_host`` if omitted\n    :param local_host: IP address or host name of the local interface to bind to\n    :param local_port: local port to bind to\n    :param reuse_port: ``True`` to allow multiple sockets to bind to the same address/port\n        (not supported on Windows)\n    :return: a connected UDP socket\n\n    \"\"\"\n    local_address = None\n    if local_host:\n        gai_res = await getaddrinfo(str(local_host), local_port, family=family,\n                                    type=socket.SOCK_DGRAM,\n                                    flags=socket.AI_PASSIVE | socket.AI_ADDRCONFIG)\n        family = cast(AnyIPAddressFamily, gai_res[0][0])\n        local_address = gai_res[0][-1]\n\n    gai_res = await getaddrinfo(str(remote_host), remote_port, family=family,\n                                type=socket.SOCK_DGRAM)\n    family = cast(AnyIPAddressFamily, gai_res[0][0])\n    remote_address = gai_res[0][-1]\n\n    return await get_asynclib().create_udp_socket(family, local_address, remote_address,\n                                                  reuse_port)\n\n\nasync def getaddrinfo(host: Union[bytearray, bytes, str], port: Union[str, int, None], *,\n                      family: Union[int, AddressFamily] = 0, type: Union[int, SocketKind] = 0,\n                      proto: int = 0, flags: int = 0) -> GetAddrInfoReturnType:\n    \"\"\"\n    Look up a numeric IP address given a host name.\n\n    Internationalized domain names are translated according to the (non-transitional) IDNA 2008\n    standard.\n\n    .. note:: 4-tuple IPv6 socket addresses are automatically converted to 2-tuples of\n        (host, port), unlike what :func:`socket.getaddrinfo` does.\n\n    :param host: host name\n    :param port: port number\n    :param family: socket family (`'AF_INET``, ...)\n    :param type: socket type (``SOCK_STREAM``, ...)\n    :param proto: protocol number\n    :param flags: flags to pass to upstream ``getaddrinfo()``\n    :return: list of tuples containing (family, type, proto, canonname, sockaddr)\n\n    .. seealso:: :func:`socket.getaddrinfo`\n\n    \"\"\"\n    # Handle unicode hostnames\n    if isinstance(host, str):\n        try:\n            encoded_host = host.encode('ascii')\n        except UnicodeEncodeError:\n            import idna\n            encoded_host = idna.encode(host, uts46=True)\n    else:\n        encoded_host = host\n\n    gai_res = await get_asynclib().getaddrinfo(encoded_host, port, family=family, type=type,\n                                               proto=proto, flags=flags)\n    return [(family, type, proto, canonname, convert_ipv6_sockaddr(sockaddr))\n            for family, type, proto, canonname, sockaddr in gai_res]\n\n\ndef getnameinfo(sockaddr: IPSockAddrType, flags: int = 0) -> Awaitable[Tuple[str, str]]:\n    \"\"\"\n    Look up the host name of an IP address.\n\n    :param sockaddr: socket address (e.g. (ipaddress, port) for IPv4)\n    :param flags: flags to pass to upstream ``getnameinfo()``\n    :return: a tuple of (host name, service name)\n\n    .. seealso:: :func:`socket.getnameinfo`\n\n    \"\"\"\n    return get_asynclib().getnameinfo(sockaddr, flags)\n\n\ndef wait_socket_readable(sock: socket.socket) -> Awaitable[None]:\n    \"\"\"\n    Wait until the given socket has data to be read.\n\n    This does **NOT** work on Windows when using the asyncio backend with a proactor event loop\n    (default on py3.8+).\n\n    .. warning:: Only use this on raw sockets that have not been wrapped by any higher level\n        constructs like socket streams!\n\n    :param sock: a socket object\n    :raises ~anyio.ClosedResourceError: if the socket was closed while waiting for the\n        socket to become readable\n    :raises ~anyio.BusyResourceError: if another task is already waiting for the socket\n        to become readable\n\n    \"\"\"\n    return get_asynclib().wait_socket_readable(sock)\n\n\ndef wait_socket_writable(sock: socket.socket) -> Awaitable[None]:\n    \"\"\"\n    Wait until the given socket can be written to.\n\n    This does **NOT** work on Windows when using the asyncio backend with a proactor event loop\n    (default on py3.8+).\n\n    .. warning:: Only use this on raw sockets that have not been wrapped by any higher level\n        constructs like socket streams!\n\n    :param sock: a socket object\n    :raises ~anyio.ClosedResourceError: if the socket was closed while waiting for the\n        socket to become writable\n    :raises ~anyio.BusyResourceError: if another task is already waiting for the socket\n        to become writable\n\n    \"\"\"\n    return get_asynclib().wait_socket_writable(sock)\n\n\n#\n# Private API\n#\n\ndef convert_ipv6_sockaddr(\n    sockaddr: Union[Tuple[str, int, int, int], Tuple[str, int]]\n) -> Tuple[str, int]:\n    \"\"\"\n    Convert a 4-tuple IPv6 socket address to a 2-tuple (address, port) format.\n\n    If the scope ID is nonzero, it is added to the address, separated with ``%``.\n    Otherwise the flow id and scope id are simply cut off from the tuple.\n    Any other kinds of socket addresses are returned as-is.\n\n    :param sockaddr: the result of :meth:`~socket.socket.getsockname`\n    :return: the converted socket address\n\n    \"\"\"\n    # This is more complicated than it should be because of MyPy\n    if isinstance(sockaddr, tuple) and len(sockaddr) == 4:\n        host, port, flowinfo, scope_id = cast(Tuple[str, int, int, int], sockaddr)\n        if scope_id:\n            # Add scope_id to the address\n            return f\"{host}%{scope_id}\", port\n        else:\n            return host, port\n    else:\n        return cast(Tuple[str, int], sockaddr)\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiofiles/tempfile/temptypes.py", "content": "\"\"\"Async wrappers for spooled temp files and temp directory objects\"\"\"\r\n\r\n# Imports\r\nimport asyncio\r\nfrom types import coroutine\r\n\r\nfrom ..base import AsyncBase\r\nfrom ..threadpool.utils import (\r\n    delegate_to_executor,\r\n    proxy_property_directly,\r\n    cond_delegate_to_executor,\r\n)\r\nfrom functools import partial\r\n\r\n\r\n@delegate_to_executor(\"fileno\", \"rollover\")\r\n@cond_delegate_to_executor(\r\n    \"close\",\r\n    \"flush\",\r\n    \"isatty\",\r\n    \"newlines\",\r\n    \"read\",\r\n    \"readline\",\r\n    \"readlines\",\r\n    \"seek\",\r\n    \"tell\",\r\n    \"truncate\",\r\n)\r\n@proxy_property_directly(\"closed\", \"encoding\", \"mode\", \"name\", \"softspace\")\r\nclass AsyncSpooledTemporaryFile(AsyncBase):\r\n    \"\"\"Async wrapper for SpooledTemporaryFile class\"\"\"\r\n\r\n    async def _check(self):\r\n        if self._file._rolled:\r\n            return\r\n        max_size = self._file._max_size\r\n        if max_size and self._file.tell() > max_size:\r\n            await self.rollover()\r\n\r\n    async def write(self, s):\r\n        \"\"\"Implementation to anticipate rollover\"\"\"\r\n        if self._file._rolled:\r\n            cb = partial(self._file.write, s)\r\n            return await self._loop.run_in_executor(self._executor, cb)\r\n        else:\r\n            file = self._file._file  # reference underlying base IO object\r\n            rv = file.write(s)\r\n            await self._check()\r\n            return rv\r\n\r\n    async def writelines(self, iterable):\r\n        \"\"\"Implementation to anticipate rollover\"\"\"\r\n        if self._file._rolled:\r\n            cb = partial(self._file.writelines, iterable)\r\n            return await self._loop.run_in_executor(self._executor, cb)\r\n        else:\r\n            file = self._file._file  # reference underlying base IO object\r\n            rv = file.writelines(iterable)\r\n            await self._check()\r\n            return rv\r\n\r\n\r\n@delegate_to_executor(\"cleanup\")\r\n@proxy_property_directly(\"name\")\r\nclass AsyncTemporaryDirectory:\r\n    \"\"\"Async wrapper for TemporaryDirectory class\"\"\"\r\n\r\n    def __init__(self, file, loop, executor):\r\n        self._file = file\r\n        self._loop = loop\r\n        self._executor = executor\r\n\r\n    async def close(self):\r\n        await self.cleanup()\r\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiofiles/threadpool/__init__.py", "content": "\"\"\"Handle files using a thread pool executor.\"\"\"\nimport asyncio\nfrom types import coroutine\n\nfrom io import (\n    FileIO,\n    TextIOBase,\n    BufferedReader,\n    BufferedWriter,\n    BufferedRandom,\n)\nfrom functools import partial, singledispatch\n\nfrom .binary import AsyncBufferedIOBase, AsyncBufferedReader, AsyncFileIO\nfrom .text import AsyncTextIOWrapper\nfrom ..base import AiofilesContextManager\n\nsync_open = open\n\n__all__ = (\"open\",)\n\n\ndef open(\n    file,\n    mode=\"r\",\n    buffering=-1,\n    encoding=None,\n    errors=None,\n    newline=None,\n    closefd=True,\n    opener=None,\n    *,\n    loop=None,\n    executor=None\n):\n    return AiofilesContextManager(\n        _open(\n            file,\n            mode=mode,\n            buffering=buffering,\n            encoding=encoding,\n            errors=errors,\n            newline=newline,\n            closefd=closefd,\n            opener=opener,\n            loop=loop,\n            executor=executor,\n        )\n    )\n\n\n@coroutine\ndef _open(\n    file,\n    mode=\"r\",\n    buffering=-1,\n    encoding=None,\n    errors=None,\n    newline=None,\n    closefd=True,\n    opener=None,\n    *,\n    loop=None,\n    executor=None\n):\n    \"\"\"Open an asyncio file.\"\"\"\n    if loop is None:\n        loop = asyncio.get_event_loop()\n    cb = partial(\n        sync_open,\n        file,\n        mode=mode,\n        buffering=buffering,\n        encoding=encoding,\n        errors=errors,\n        newline=newline,\n        closefd=closefd,\n        opener=opener,\n    )\n    f = yield from loop.run_in_executor(executor, cb)\n\n    return wrap(f, loop=loop, executor=executor)\n\n\n@singledispatch\ndef wrap(file, *, loop=None, executor=None):\n    raise TypeError(\"Unsupported io type: {}.\".format(file))\n\n\n@wrap.register(TextIOBase)\ndef _(file, *, loop=None, executor=None):\n    return AsyncTextIOWrapper(file, loop=loop, executor=executor)\n\n\n@wrap.register(BufferedWriter)\ndef _(file, *, loop=None, executor=None):\n    return AsyncBufferedIOBase(file, loop=loop, executor=executor)\n\n\n@wrap.register(BufferedReader)\n@wrap.register(BufferedRandom)\ndef _(file, *, loop=None, executor=None):\n    return AsyncBufferedReader(file, loop=loop, executor=executor)\n\n\n@wrap.register(FileIO)\ndef _(file, *, loop=None, executor=None):\n    return AsyncFileIO(file, loop, executor)\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/aiosqlite/core.py", "content": "# Copyright 2018 John Reese\n# Licensed under the MIT license\n\n\"\"\"\nCore implementation of aiosqlite proxies\n\"\"\"\n\nimport asyncio\nimport logging\nimport sqlite3\nimport sys\nimport warnings\nfrom functools import partial\nfrom pathlib import Path\nfrom queue import Empty, Queue\nfrom threading import Thread\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Generator,\n    Iterable,\n    Optional,\n    Type,\n    Union,\n)\nfrom warnings import warn\n\nfrom .context import contextmanager\nfrom .cursor import Cursor\n\n__all__ = [\"connect\", \"Connection\", \"Cursor\"]\n\nLOG = logging.getLogger(\"aiosqlite\")\n\n\ndef get_loop(future: asyncio.Future) -> asyncio.AbstractEventLoop:\n    if sys.version_info >= (3, 7):\n        return future.get_loop()\n    else:\n        return future._loop\n\n\nclass Connection(Thread):\n    def __init__(\n        self,\n        connector: Callable[[], sqlite3.Connection],\n        iter_chunk_size: int,\n        loop: Optional[asyncio.AbstractEventLoop] = None,\n    ) -> None:\n        super().__init__()\n        self._running = True\n        self._connection: Optional[sqlite3.Connection] = None\n        self._connector = connector\n        self._tx: Queue = Queue()\n        self._iter_chunk_size = iter_chunk_size\n\n        if loop is not None:\n            warn(\n                \"aiosqlite.Connection no longer uses the `loop` parameter\",\n                DeprecationWarning,\n            )\n\n    @property\n    def _conn(self) -> sqlite3.Connection:\n        if self._connection is None:\n            raise ValueError(\"no active connection\")\n\n        return self._connection\n\n    def _execute_insert(\n        self, sql: str, parameters: Iterable[Any]\n    ) -> Optional[sqlite3.Row]:\n        cursor = self._conn.execute(sql, parameters)\n        cursor.execute(\"SELECT last_insert_rowid()\")\n        return cursor.fetchone()\n\n    def _execute_fetchall(\n        self, sql: str, parameters: Iterable[Any]\n    ) -> Iterable[sqlite3.Row]:\n        cursor = self._conn.execute(sql, parameters)\n        return cursor.fetchall()\n\n    def run(self) -> None:\n        \"\"\"\n        Execute function calls on a separate thread.\n\n        :meta private:\n        \"\"\"\n        while True:\n            # Continues running until all queue items are processed,\n            # even after connection is closed (so we can finalize all\n            # futures)\n            try:\n                future, function = self._tx.get(timeout=0.1)\n            except Empty:\n                if self._running:\n                    continue\n                break\n            try:\n                LOG.debug(\"executing %s\", function)\n                result = function()\n                LOG.debug(\"operation %s completed\", function)\n\n                def set_result(fut, result):\n                    if not fut.done():\n                        fut.set_result(result)\n\n                get_loop(future).call_soon_threadsafe(set_result, future, result)\n            except BaseException as e:\n                LOG.debug(\"returning exception %s\", e)\n\n                def set_exception(fut, e):\n                    if not fut.done():\n                        fut.set_exception(e)\n\n                get_loop(future).call_soon_threadsafe(set_exception, future, e)\n\n    async def _execute(self, fn, *args, **kwargs):\n        \"\"\"Queue a function with the given arguments for execution.\"\"\"\n        if not self._running or not self._connection:\n            raise ValueError(\"Connection closed\")\n\n        function = partial(fn, *args, **kwargs)\n        future = asyncio.get_event_loop().create_future()\n\n        self._tx.put_nowait((future, function))\n\n        return await future\n\n    async def _connect(self) -> \"Connection\":\n        \"\"\"Connect to the actual sqlite database.\"\"\"\n        if self._connection is None:\n            try:\n                future = asyncio.get_event_loop().create_future()\n                self._tx.put_nowait((future, self._connector))\n                self._connection = await future\n            except Exception:\n                self._running = False\n                self._connection = None\n                raise\n\n        return self\n\n    def __await__(self) -> Generator[Any, None, \"Connection\"]:\n        self.start()\n        return self._connect().__await__()\n\n    async def __aenter__(self) -> \"Connection\":\n        return await self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n        await self.close()\n\n    @contextmanager\n    async def cursor(self) -> Cursor:\n        \"\"\"Create an aiosqlite cursor wrapping a sqlite3 cursor object.\"\"\"\n        return Cursor(self, await self._execute(self._conn.cursor))\n\n    async def commit(self) -> None:\n        \"\"\"Commit the current transaction.\"\"\"\n        await self._execute(self._conn.commit)\n\n    async def rollback(self) -> None:\n        \"\"\"Roll back the current transaction.\"\"\"\n        await self._execute(self._conn.rollback)\n\n    async def close(self) -> None:\n        \"\"\"Complete queued queries/cursors and close the connection.\"\"\"\n        try:\n            await self._execute(self._conn.close)\n        except Exception:\n            LOG.info(\"exception occurred while closing connection\")\n            raise\n        finally:\n            self._running = False\n            self._connection = None\n\n    @contextmanager\n    async def execute(self, sql: str, parameters: Iterable[Any] = None) -> Cursor:\n        \"\"\"Helper to create a cursor and execute the given query.\"\"\"\n        if parameters is None:\n            parameters = []\n        cursor = await self._execute(self._conn.execute, sql, parameters)\n        return Cursor(self, cursor)\n\n    @contextmanager\n    async def execute_insert(\n        self, sql: str, parameters: Iterable[Any] = None\n    ) -> Optional[sqlite3.Row]:\n        \"\"\"Helper to insert and get the last_insert_rowid.\"\"\"\n        if parameters is None:\n            parameters = []\n        return await self._execute(self._execute_insert, sql, parameters)\n\n    @contextmanager\n    async def execute_fetchall(\n        self, sql: str, parameters: Iterable[Any] = None\n    ) -> Iterable[sqlite3.Row]:\n        \"\"\"Helper to execute a query and return all the data.\"\"\"\n        if parameters is None:\n            parameters = []\n        return await self._execute(self._execute_fetchall, sql, parameters)\n\n    @contextmanager\n    async def executemany(\n        self, sql: str, parameters: Iterable[Iterable[Any]]\n    ) -> Cursor:\n        \"\"\"Helper to create a cursor and execute the given multiquery.\"\"\"\n        cursor = await self._execute(self._conn.executemany, sql, parameters)\n        return Cursor(self, cursor)\n\n    @contextmanager\n    async def executescript(self, sql_script: str) -> Cursor:\n        \"\"\"Helper to create a cursor and execute a user script.\"\"\"\n        cursor = await self._execute(self._conn.executescript, sql_script)\n        return Cursor(self, cursor)\n\n    async def interrupt(self) -> None:\n        \"\"\"Interrupt pending queries.\"\"\"\n        return self._conn.interrupt()\n\n    async def create_function(\n        self, name: str, num_params: int, func: Callable, deterministic: bool = False\n    ) -> None:\n        \"\"\"\n        Create user-defined function that can be later used\n        within SQL statements. Must be run within the same thread\n        that query executions take place so instead of executing directly\n        against the connection, we defer this to `run` function.\n\n        In Python 3.8 and above, if *deterministic* is true, the created\n        function is marked as deterministic, which allows SQLite to perform\n        additional optimizations. This flag is supported by SQLite 3.8.3 or\n        higher, ``NotSupportedError`` will be raised if used with older\n        versions.\n        \"\"\"\n        if sys.version_info >= (3, 8):\n            await self._execute(\n                self._conn.create_function,\n                name,\n                num_params,\n                func,\n                deterministic=deterministic,\n            )\n        else:\n            if deterministic:\n                warnings.warn(\n                    \"Deterministic function support is only available on \"\n                    'Python 3.8+. Function \"{}\" will be registered as '\n                    \"non-deterministic as per SQLite defaults.\".format(name)\n                )\n\n            await self._execute(self._conn.create_function, name, num_params, func)\n\n    @property\n    def in_transaction(self) -> bool:\n        return self._conn.in_transaction\n\n    @property\n    def isolation_level(self) -> str:\n        return self._conn.isolation_level\n\n    @isolation_level.setter\n    def isolation_level(self, value: str) -> None:\n        self._conn.isolation_level = value\n\n    @property\n    def row_factory(self) -> \"Optional[Type]\":  # py3.5.2 compat (#24)\n        return self._conn.row_factory\n\n    @row_factory.setter\n    def row_factory(self, factory: \"Optional[Type]\") -> None:  # py3.5.2 compat (#24)\n        self._conn.row_factory = factory\n\n    @property\n    def text_factory(self) -> Type:\n        return self._conn.text_factory\n\n    @text_factory.setter\n    def text_factory(self, factory: Type) -> None:\n        self._conn.text_factory = factory\n\n    @property\n    def total_changes(self) -> int:\n        return self._conn.total_changes\n\n    async def enable_load_extension(self, value: bool) -> None:\n        await self._execute(self._conn.enable_load_extension, value)  # type: ignore\n\n    async def load_extension(self, path: str):\n        await self._execute(self._conn.load_extension, path)  # type: ignore\n\n    async def set_progress_handler(\n        self, handler: Callable[[], Optional[int]], n: int\n    ) -> None:\n        await self._execute(self._conn.set_progress_handler, handler, n)\n\n    async def set_trace_callback(self, handler: Callable) -> None:\n        await self._execute(self._conn.set_trace_callback, handler)\n\n    async def iterdump(self) -> AsyncIterator[str]:\n        \"\"\"\n        Return an async iterator to dump the database in SQL text format.\n\n        Example::\n\n            async for line in db.iterdump():\n                ...\n\n        \"\"\"\n        dump_queue: Queue = Queue()\n\n        def dumper():\n            try:\n                for line in self._conn.iterdump():\n                    dump_queue.put_nowait(line)\n                dump_queue.put_nowait(None)\n\n            except Exception:\n                LOG.exception(\"exception while dumping db\")\n                dump_queue.put_nowait(None)\n                raise\n\n        fut = self._execute(dumper)\n        task = asyncio.ensure_future(fut)\n\n        while True:\n            try:\n                line: Optional[str] = dump_queue.get_nowait()\n                if line is None:\n                    break\n                yield line\n\n            except Empty:\n                if task.done():\n                    LOG.warning(\"iterdump completed unexpectedly\")\n                    break\n\n                await asyncio.sleep(0.01)\n\n        await task\n\n    async def backup(\n        self,\n        target: Union[\"Connection\", sqlite3.Connection],\n        *,\n        pages: int = 0,\n        progress: Optional[Callable[[int, int, int], None]] = None,\n        name: str = \"main\",\n        sleep: float = 0.250\n    ) -> None:\n        \"\"\"\n        Make a backup of the current database to the target database.\n\n        Takes either a standard sqlite3 or aiosqlite Connection object as the target.\n        \"\"\"\n        if sys.version_info < (3, 7):\n            raise RuntimeError(\"backup() method is only available on Python 3.7+\")\n\n        if isinstance(target, Connection):\n            target = target._conn\n\n        await self._execute(\n            self._conn.backup,\n            target,\n            pages=pages,\n            progress=progress,\n            name=name,\n            sleep=sleep,\n        )\n\n\ndef connect(\n    database: Union[str, Path],\n    *,\n    iter_chunk_size=64,\n    loop: Optional[asyncio.AbstractEventLoop] = None,\n    **kwargs: Any\n) -> Connection:\n    \"\"\"Create and return a connection proxy to the sqlite database.\"\"\"\n\n    if loop is not None:\n        warn(\n            \"aiosqlite.connect() no longer uses the `loop` parameter\",\n            DeprecationWarning,\n        )\n\n    def connector() -> sqlite3.Connection:\n        if isinstance(database, str):\n            loc = database\n        elif isinstance(database, bytes):\n            loc = database.decode(\"utf-8\")\n        else:\n            loc = str(database)\n\n        return sqlite3.connect(loc, **kwargs)\n\n    return Connection(connector, iter_chunk_size)\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/abc/_streams.py", "content": "from abc import abstractmethod\nfrom typing import Any, Callable, Generic, Optional, TypeVar, Union\n\nfrom .._core._exceptions import EndOfStream\nfrom .._core._typedattr import TypedAttributeProvider\nfrom ._resources import AsyncResource\nfrom ._tasks import TaskGroup\n\nT_Item = TypeVar('T_Item')\nT_Stream = TypeVar('T_Stream')\n\n\nclass UnreliableObjectReceiveStream(Generic[T_Item], AsyncResource, TypedAttributeProvider):\n    \"\"\"\n    An interface for receiving objects.\n\n    This interface makes no guarantees that the received messages arrive in the order in which they\n    were sent, or that no messages are missed.\n\n    Asynchronously iterating over objects of this type will yield objects matching the given type\n    parameter.\n    \"\"\"\n\n    def __aiter__(self) -> \"UnreliableObjectReceiveStream[T_Item]\":\n        return self\n\n    async def __anext__(self) -> T_Item:\n        try:\n            return await self.receive()\n        except EndOfStream:\n            raise StopAsyncIteration\n\n    @abstractmethod\n    async def receive(self) -> T_Item:\n        \"\"\"\n        Receive the next item.\n\n        :raises ~anyio.ClosedResourceError: if the receive stream has been explicitly\n            closed\n        :raises ~anyio.EndOfStream: if this stream has been closed from the other end\n        :raises ~anyio.BrokenResourceError: if this stream has been rendered unusable\n            due to external causes\n        \"\"\"\n\n\nclass UnreliableObjectSendStream(Generic[T_Item], AsyncResource, TypedAttributeProvider):\n    \"\"\"\n    An interface for sending objects.\n\n    This interface makes no guarantees that the messages sent will reach the recipient(s) in the\n    same order in which they were sent, or at all.\n    \"\"\"\n\n    @abstractmethod\n    async def send(self, item: T_Item) -> None:\n        \"\"\"\n        Send an item to the peer(s).\n\n        :param item: the item to send\n        :raises ~anyio.ClosedResourceError: if the send stream has been explicitly\n            closed\n        :raises ~anyio.BrokenResourceError: if this stream has been rendered unusable\n            due to external causes\n        \"\"\"\n\n\nclass UnreliableObjectStream(UnreliableObjectReceiveStream[T_Item],\n                             UnreliableObjectSendStream[T_Item]):\n    \"\"\"\n    A bidirectional message stream which does not guarantee the order or reliability of message\n    delivery.\n    \"\"\"\n\n\nclass ObjectReceiveStream(UnreliableObjectReceiveStream[T_Item]):\n    \"\"\"\n    A receive message stream which guarantees that messages are received in the same order in\n    which they were sent, and that no messages are missed.\n    \"\"\"\n\n\nclass ObjectSendStream(UnreliableObjectSendStream[T_Item]):\n    \"\"\"\n    A send message stream which guarantees that messages are delivered in the same order in which\n    they were sent, without missing any messages in the middle.\n    \"\"\"\n\n\nclass ObjectStream(ObjectReceiveStream[T_Item], ObjectSendStream[T_Item],\n                   UnreliableObjectStream[T_Item]):\n    \"\"\"\n    A bidirectional message stream which guarantees the order and reliability of message delivery.\n    \"\"\"\n\n    @abstractmethod\n    async def send_eof(self) -> None:\n        \"\"\"\n        Send an end-of-file indication to the peer.\n\n        You should not try to send any further data to this stream after calling this method.\n        This method is idempotent (does nothing on successive calls).\n        \"\"\"\n\n\nclass ByteReceiveStream(AsyncResource, TypedAttributeProvider):\n    \"\"\"\n    An interface for receiving bytes from a single peer.\n\n    Iterating this byte stream will yield a byte string of arbitrary length, but no more than\n    65536 bytes.\n    \"\"\"\n\n    def __aiter__(self) -> 'ByteReceiveStream':\n        return self\n\n    async def __anext__(self) -> bytes:\n        try:\n            return await self.receive()\n        except EndOfStream:\n            raise StopAsyncIteration\n\n    @abstractmethod\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        \"\"\"\n        Receive at most ``max_bytes`` bytes from the peer.\n\n        .. note:: Implementors of this interface should not return an empty :class:`bytes` object,\n            and users should ignore them.\n\n        :param max_bytes: maximum number of bytes to receive\n        :return: the received bytes\n        :raises ~anyio.EndOfStream: if this stream has been closed from the other end\n        \"\"\"\n\n\nclass ByteSendStream(AsyncResource, TypedAttributeProvider):\n    \"\"\"An interface for sending bytes to a single peer.\"\"\"\n\n    @abstractmethod\n    async def send(self, item: bytes) -> None:\n        \"\"\"\n        Send the given bytes to the peer.\n\n        :param item: the bytes to send\n        \"\"\"\n\n\nclass ByteStream(ByteReceiveStream, ByteSendStream):\n    \"\"\"A bidirectional byte stream.\"\"\"\n\n    @abstractmethod\n    async def send_eof(self) -> None:\n        \"\"\"\n        Send an end-of-file indication to the peer.\n\n        You should not try to send any further data to this stream after calling this method.\n        This method is idempotent (does nothing on successive calls).\n        \"\"\"\n\n\n#: Type alias for all unreliable bytes-oriented receive streams.\nAnyUnreliableByteReceiveStream = Union[UnreliableObjectReceiveStream[bytes], ByteReceiveStream]\n#: Type alias for all unreliable bytes-oriented send streams.\nAnyUnreliableByteSendStream = Union[UnreliableObjectSendStream[bytes], ByteSendStream]\n#: Type alias for all unreliable bytes-oriented streams.\nAnyUnreliableByteStream = Union[UnreliableObjectStream[bytes], ByteStream]\n#: Type alias for all bytes-oriented receive streams.\nAnyByteReceiveStream = Union[ObjectReceiveStream[bytes], ByteReceiveStream]\n#: Type alias for all bytes-oriented send streams.\nAnyByteSendStream = Union[ObjectSendStream[bytes], ByteSendStream]\n#: Type alias for all bytes-oriented streams.\nAnyByteStream = Union[ObjectStream[bytes], ByteStream]\n\n\nclass Listener(Generic[T_Stream], AsyncResource, TypedAttributeProvider):\n    \"\"\"An interface for objects that let you accept incoming connections.\"\"\"\n\n    @abstractmethod\n    async def serve(self, handler: Callable[[T_Stream], Any],\n                    task_group: Optional[TaskGroup] = None) -> None:\n        \"\"\"\n        Accept incoming connections as they come in and start tasks to handle them.\n\n        :param handler: a callable that will be used to handle each accepted connection\n        :param task_group: the task group that will be used to start tasks for handling each\n            accepted connection (if omitted, an ad-hoc task group will be created)\n        \"\"\"\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/abc/__init__.py", "content": "__all__ = ('AsyncResource', 'IPAddressType', 'IPSockAddrType', 'SocketAttribute', 'SocketStream',\n           'SocketListener', 'UDPSocket', 'UNIXSocketStream', 'UDPPacketType',\n           'ConnectedUDPSocket', 'UnreliableObjectReceiveStream', 'UnreliableObjectSendStream',\n           'UnreliableObjectStream', 'ObjectReceiveStream', 'ObjectSendStream', 'ObjectStream',\n           'ByteReceiveStream', 'ByteSendStream', 'ByteStream', 'AnyUnreliableByteReceiveStream',\n           'AnyUnreliableByteSendStream', 'AnyUnreliableByteStream', 'AnyByteReceiveStream',\n           'AnyByteSendStream', 'AnyByteStream', 'Listener', 'Process', 'Event',\n           'Condition', 'Lock', 'Semaphore', 'CapacityLimiter', 'CancelScope', 'TaskGroup',\n           'TaskStatus', 'TestRunner', 'BlockingPortal')\n\nfrom typing import Any\n\nfrom ._resources import AsyncResource\nfrom ._sockets import (\n    ConnectedUDPSocket, IPAddressType, IPSockAddrType, SocketAttribute, SocketListener,\n    SocketStream, UDPPacketType, UDPSocket, UNIXSocketStream)\nfrom ._streams import (\n    AnyByteReceiveStream, AnyByteSendStream, AnyByteStream, AnyUnreliableByteReceiveStream,\n    AnyUnreliableByteSendStream, AnyUnreliableByteStream, ByteReceiveStream, ByteSendStream,\n    ByteStream, Listener, ObjectReceiveStream, ObjectSendStream, ObjectStream,\n    UnreliableObjectReceiveStream, UnreliableObjectSendStream, UnreliableObjectStream)\nfrom ._subprocesses import Process\nfrom ._tasks import TaskGroup, TaskStatus\nfrom ._testing import TestRunner\n\n# Re-exported here, for backwards compatibility\n# isort: off\nfrom .._core._synchronization import CapacityLimiter, Condition, Event, Lock, Semaphore\nfrom .._core._tasks import CancelScope\nfrom ..from_thread import BlockingPortal\n\n# Re-export imports so they look like they live directly in this package\nkey: str\nvalue: Any\nfor key, value in list(locals().items()):\n    if getattr(value, '__module__', '').startswith('anyio.abc.'):\n        value.__module__ = __name__\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/abc/_resources.py", "content": "from abc import ABCMeta, abstractmethod\nfrom types import TracebackType\nfrom typing import Optional, Type, TypeVar\n\nT = TypeVar(\"T\")\n\n\nclass AsyncResource(metaclass=ABCMeta):\n    \"\"\"\n    Abstract base class for all closeable asynchronous resources.\n\n    Works as an asynchronous context manager which returns the instance itself on enter, and calls\n    :meth:`aclose` on exit.\n    \"\"\"\n\n    async def __aenter__(self: T) -> T:\n        return self\n\n    async def __aexit__(self, exc_type: Optional[Type[BaseException]],\n                        exc_val: Optional[BaseException],\n                        exc_tb: Optional[TracebackType]) -> None:\n        await self.aclose()\n\n    @abstractmethod\n    async def aclose(self) -> None:\n        \"\"\"Close the resource.\"\"\"\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/abc/_sockets.py", "content": "import socket\nfrom abc import abstractmethod\nfrom io import IOBase\nfrom ipaddress import IPv4Address, IPv6Address\nfrom socket import AddressFamily\nfrom types import TracebackType\nfrom typing import (\n    Any, AsyncContextManager, Callable, Collection, Dict, List, Mapping, Optional, Tuple, Type,\n    TypeVar, Union)\n\nfrom .._core._typedattr import TypedAttributeProvider, TypedAttributeSet, typed_attribute\nfrom ._streams import ByteStream, Listener, T_Stream, UnreliableObjectStream\nfrom ._tasks import TaskGroup\n\nIPAddressType = Union[str, IPv4Address, IPv6Address]\nIPSockAddrType = Tuple[str, int]\nSockAddrType = Union[IPSockAddrType, str]\nUDPPacketType = Tuple[bytes, IPSockAddrType]\nT_Retval = TypeVar('T_Retval')\n\n\nclass _NullAsyncContextManager:\n    async def __aenter__(self) -> None:\n        pass\n\n    async def __aexit__(self, exc_type: Optional[Type[BaseException]],\n                        exc_val: Optional[BaseException],\n                        exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        return None\n\n\nclass SocketAttribute(TypedAttributeSet):\n    #: the address family of the underlying socket\n    family: AddressFamily = typed_attribute()\n    #: the local socket address of the underlying socket\n    local_address: SockAddrType = typed_attribute()\n    #: for IP addresses, the local port the underlying socket is bound to\n    local_port: int = typed_attribute()\n    #: the underlying stdlib socket object\n    raw_socket: socket.socket = typed_attribute()\n    #: the remote address the underlying socket is connected to\n    remote_address: SockAddrType = typed_attribute()\n    #: for IP addresses, the remote port the underlying socket is connected to\n    remote_port: int = typed_attribute()\n\n\nclass _SocketProvider(TypedAttributeProvider):\n    @property\n    def extra_attributes(self) -> Mapping[Any, Callable[[], Any]]:\n        from .._core._sockets import convert_ipv6_sockaddr as convert\n\n        attributes: Dict[Any, Callable[[], Any]] = {\n            SocketAttribute.family: lambda: self._raw_socket.family,\n            SocketAttribute.local_address: lambda: convert(self._raw_socket.getsockname()),\n            SocketAttribute.raw_socket: lambda: self._raw_socket\n        }\n        try:\n            peername: Optional[Tuple[str, int]] = convert(self._raw_socket.getpeername())\n        except OSError:\n            peername = None\n\n        # Provide the remote address for connected sockets\n        if peername is not None:\n            attributes[SocketAttribute.remote_address] = lambda: peername\n\n        # Provide local and remote ports for IP based sockets\n        if self._raw_socket.family in (AddressFamily.AF_INET, AddressFamily.AF_INET6):\n            attributes[SocketAttribute.local_port] = lambda: self._raw_socket.getsockname()[1]\n            if peername is not None:\n                remote_port = peername[1]\n                attributes[SocketAttribute.remote_port] = lambda: remote_port\n\n        return attributes\n\n    @property\n    @abstractmethod\n    def _raw_socket(self) -> socket.socket:\n        pass\n\n\nclass SocketStream(ByteStream, _SocketProvider):\n    \"\"\"\n    Transports bytes over a socket.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n\n\nclass UNIXSocketStream(SocketStream):\n    @abstractmethod\n    async def send_fds(self, message: bytes, fds: Collection[Union[int, IOBase]]) -> None:\n        \"\"\"\n        Send file descriptors along with a message to the peer.\n\n        :param message: a non-empty bytestring\n        :param fds: a collection of files (either numeric file descriptors or open file or socket\n            objects)\n        \"\"\"\n\n    @abstractmethod\n    async def receive_fds(self, msglen: int, maxfds: int) -> Tuple[bytes, List[int]]:\n        \"\"\"\n        Receive file descriptors along with a message from the peer.\n\n        :param msglen: length of the message to expect from the peer\n        :param maxfds: maximum number of file descriptors to expect from the peer\n        :return: a tuple of (message, file descriptors)\n        \"\"\"\n\n\nclass SocketListener(Listener[SocketStream], _SocketProvider):\n    \"\"\"\n    Listens to incoming socket connections.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n\n    @abstractmethod\n    async def accept(self) -> SocketStream:\n        \"\"\"Accept an incoming connection.\"\"\"\n\n    async def serve(self, handler: Callable[[T_Stream], Any],\n                    task_group: Optional[TaskGroup] = None) -> None:\n        from .. import create_task_group\n\n        context_manager: AsyncContextManager\n        if task_group is None:\n            task_group = context_manager = create_task_group()\n        else:\n            # Can be replaced with AsyncExitStack once on py3.7+\n            context_manager = _NullAsyncContextManager()\n\n        async with context_manager:\n            while True:\n                stream = await self.accept()\n                task_group.start_soon(handler, stream)\n\n\nclass UDPSocket(UnreliableObjectStream[UDPPacketType], _SocketProvider):\n    \"\"\"\n    Represents an unconnected UDP socket.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n\n    async def sendto(self, data: bytes, host: str, port: int) -> None:\n        \"\"\"Alias for :meth:`~.UnreliableObjectSendStream.send` ((data, (host, port))).\"\"\"\n        return await self.send((data, (host, port)))\n\n\nclass ConnectedUDPSocket(UnreliableObjectStream[bytes], _SocketProvider):\n    \"\"\"\n    Represents an connected UDP socket.\n\n    Supports all relevant extra attributes from :class:`~SocketAttribute`.\n    \"\"\"\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_core/_typedattr.py", "content": "import sys\nfrom typing import Any, Callable, Dict, Mapping, TypeVar, Union, overload\n\nfrom ._exceptions import TypedAttributeLookupError\n\nif sys.version_info >= (3, 8):\n    from typing import final\nelse:\n    from typing_extensions import final\n\nT_Attr = TypeVar('T_Attr')\nT_Default = TypeVar('T_Default')\nundefined = object()\n\n\ndef typed_attribute() -> Any:\n    \"\"\"Return a unique object, used to mark typed attributes.\"\"\"\n    return object()\n\n\nclass TypedAttributeSet:\n    \"\"\"\n    Superclass for typed attribute collections.\n\n    Checks that every public attribute of every subclass has a type annotation.\n    \"\"\"\n\n    def __init_subclass__(cls) -> None:\n        annotations: Dict[str, Any] = getattr(cls, '__annotations__', {})\n        for attrname in dir(cls):\n            if not attrname.startswith('_') and attrname not in annotations:\n                raise TypeError(f'Attribute {attrname!r} is missing its type annotation')\n\n        super().__init_subclass__()\n\n\nclass TypedAttributeProvider:\n    \"\"\"Base class for classes that wish to provide typed extra attributes.\"\"\"\n\n    @property\n    def extra_attributes(self) -> Mapping[T_Attr, Callable[[], T_Attr]]:\n        \"\"\"\n        A mapping of the extra attributes to callables that return the corresponding values.\n\n        If the provider wraps another provider, the attributes from that wrapper should also be\n        included in the returned mapping (but the wrapper may override the callables from the\n        wrapped instance).\n\n        \"\"\"\n        return {}\n\n    @overload\n    def extra(self, attribute: T_Attr) -> T_Attr:\n        ...\n\n    @overload\n    def extra(self, attribute: T_Attr, default: T_Default) -> Union[T_Attr, T_Default]:\n        ...\n\n    @final\n    def extra(self, attribute: Any, default: object = undefined) -> object:\n        \"\"\"\n        extra(attribute, default=undefined)\n\n        Return the value of the given typed extra attribute.\n\n        :param attribute: the attribute (member of a :class:`~TypedAttributeSet`) to look for\n        :param default: the value that should be returned if no value is found for the attribute\n        :raises ~anyio.TypedAttributeLookupError: if the search failed and no default value was\n            given\n\n        \"\"\"\n        try:\n            return self.extra_attributes[attribute]()\n        except KeyError:\n            if default is undefined:\n                raise TypedAttributeLookupError('Attribute not found') from None\n            else:\n                return default\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/_backends/_asyncio.py", "content": "import array\nimport asyncio\nimport concurrent.futures\nimport math\nimport socket\nimport sys\nfrom asyncio.base_events import _run_until_complete_cb  # type: ignore[attr-defined]\nfrom collections import OrderedDict, deque\nfrom concurrent.futures import Future\nfrom contextvars import Context, copy_context\nfrom dataclasses import dataclass\nfrom functools import partial, wraps\nfrom inspect import (\n    CORO_RUNNING, CORO_SUSPENDED, GEN_RUNNING, GEN_SUSPENDED, getcoroutinestate, getgeneratorstate)\nfrom io import IOBase\nfrom os import PathLike\nfrom queue import Queue\nfrom socket import AddressFamily, SocketKind\nfrom threading import Thread\nfrom types import TracebackType\nfrom typing import (\n    Any, Awaitable, Callable, Collection, Coroutine, Deque, Dict, Generator, Iterable, List,\n    Mapping, Optional, Sequence, Set, Tuple, Type, TypeVar, Union, cast)\nfrom weakref import WeakKeyDictionary\n\nimport sniffio\n\nfrom .. import CapacityLimiterStatistics, EventStatistics, TaskInfo, abc\nfrom .._core._compat import DeprecatedAsyncContextManager, DeprecatedAwaitable\nfrom .._core._eventloop import claim_worker_thread, threadlocals\nfrom .._core._exceptions import (\n    BrokenResourceError, BusyResourceError, ClosedResourceError, EndOfStream)\nfrom .._core._exceptions import ExceptionGroup as BaseExceptionGroup\nfrom .._core._exceptions import WouldBlock\nfrom .._core._sockets import GetAddrInfoReturnType, convert_ipv6_sockaddr\nfrom .._core._synchronization import CapacityLimiter as BaseCapacityLimiter\nfrom .._core._synchronization import Event as BaseEvent\nfrom .._core._synchronization import ResourceGuard\nfrom .._core._tasks import CancelScope as BaseCancelScope\nfrom ..abc import IPSockAddrType, UDPPacketType\nfrom ..lowlevel import RunVar\n\nif sys.version_info >= (3, 8):\n    get_coro = asyncio.Task.get_coro\nelse:\n    def get_coro(task: asyncio.Task) -> Union[Generator, Awaitable[Any]]:\n        return task._coro\n\nif sys.version_info >= (3, 7):\n    from asyncio import all_tasks, create_task, current_task, get_running_loop\n    from asyncio import run as native_run\n\n    def _get_task_callbacks(task: asyncio.Task) -> Iterable[Callable]:\n        return [cb for cb, context in task._callbacks]  # type: ignore[attr-defined]\nelse:\n    _T = TypeVar('_T')\n\n    def _get_task_callbacks(task: asyncio.Task) -> Iterable[Callable]:\n        return task._callbacks\n\n    def native_run(main, *, debug=False):\n        # Snatched from Python 3.7\n        from asyncio import coroutines, events, tasks\n\n        def _cancel_all_tasks(loop):\n            to_cancel = all_tasks(loop)\n            if not to_cancel:\n                return\n\n            for task in to_cancel:\n                task.cancel()\n\n            loop.run_until_complete(\n                tasks.gather(*to_cancel, loop=loop, return_exceptions=True))\n\n            for task in to_cancel:\n                if task.cancelled():\n                    continue\n                if task.exception() is not None:\n                    loop.call_exception_handler({\n                        'message': 'unhandled exception during asyncio.run() shutdown',\n                        'exception': task.exception(),\n                        'task': task,\n                    })\n\n        if events._get_running_loop() is not None:\n            raise RuntimeError(\n                \"asyncio.run() cannot be called from a running event loop\")\n\n        if not coroutines.iscoroutine(main):\n            raise ValueError(f\"a coroutine was expected, got {main!r}\")\n\n        loop = events.new_event_loop()\n        try:\n            events.set_event_loop(loop)\n            loop.set_debug(debug)\n            return loop.run_until_complete(main)\n        finally:\n            try:\n                _cancel_all_tasks(loop)\n                loop.run_until_complete(loop.shutdown_asyncgens())\n            finally:\n                events.set_event_loop(None)\n                loop.close()\n\n    def create_task(coro: Union[Generator[Any, None, _T], Awaitable[_T]], *,\n                    name: object = None) -> asyncio.Task:\n        return get_running_loop().create_task(coro)\n\n    def get_running_loop() -> asyncio.AbstractEventLoop:\n        loop = asyncio._get_running_loop()\n        if loop is not None:\n            return loop\n        else:\n            raise RuntimeError('no running event loop')\n\n    def all_tasks(loop: Optional[asyncio.AbstractEventLoop] = None) -> Set[asyncio.Task]:\n        \"\"\"Return a set of all tasks for the loop.\"\"\"\n        from asyncio import Task\n\n        if loop is None:\n            loop = get_running_loop()\n\n        return {t for t in Task.all_tasks(loop) if not t.done()}\n\n    def current_task(loop: Optional[asyncio.AbstractEventLoop] = None) -> Optional[asyncio.Task]:\n        if loop is None:\n            loop = get_running_loop()\n\n        return asyncio.Task.current_task(loop)\n\nT_Retval = TypeVar('T_Retval')\n\n# Check whether there is native support for task names in asyncio (3.8+)\n_native_task_names = hasattr(asyncio.Task, 'get_name')\n\n\n_root_task: RunVar[Optional[asyncio.Task]] = RunVar('_root_task')\n\n\ndef find_root_task() -> asyncio.Task:\n    root_task = _root_task.get(None)\n    if root_task is not None and not root_task.done():\n        return root_task\n\n    # Look for a task that has been started via run_until_complete()\n    for task in all_tasks():\n        if task._callbacks and not task.done():\n            for cb in _get_task_callbacks(task):\n                if (cb is _run_until_complete_cb\n                        or getattr(cb, '__module__', None) == 'uvloop.loop'):\n                    _root_task.set(task)\n                    return task\n\n    # Look up the topmost task in the AnyIO task tree, if possible\n    task = cast(asyncio.Task, current_task())\n    state = _task_states.get(task)\n    if state:\n        cancel_scope = state.cancel_scope\n        while cancel_scope and cancel_scope._parent_scope is not None:\n            cancel_scope = cancel_scope._parent_scope\n\n        if cancel_scope is not None:\n            return cast(asyncio.Task, cancel_scope._host_task)\n\n    return task\n\n\ndef get_callable_name(func: Callable) -> str:\n    module = getattr(func, '__module__', None)\n    qualname = getattr(func, '__qualname__', None)\n    return '.'.join([x for x in (module, qualname) if x])\n\n\n#\n# Event loop\n#\n\n_run_vars = WeakKeyDictionary()  # type: WeakKeyDictionary[asyncio.AbstractEventLoop, Any]\n\ncurrent_token = get_running_loop\n\n\ndef _task_started(task: asyncio.Task) -> bool:\n    \"\"\"Return ``True`` if the task has been started and has not finished.\"\"\"\n    coro = get_coro(task)\n    try:\n        return getcoroutinestate(coro) in (CORO_RUNNING, CORO_SUSPENDED)\n    except AttributeError:\n        try:\n            return getgeneratorstate(cast(Generator, coro)) in (GEN_RUNNING, GEN_SUSPENDED)\n        except AttributeError:\n            # task coro is async_genenerator_asend https://bugs.python.org/issue37771\n            raise Exception(f\"Cannot determine if task {task} has started or not\")\n\n\ndef _maybe_set_event_loop_policy(policy: Optional[asyncio.AbstractEventLoopPolicy],\n                                 use_uvloop: bool) -> None:\n    # On CPython, use uvloop when possible if no other policy has been given and if not\n    # explicitly disabled\n    if policy is None and use_uvloop and sys.implementation.name == 'cpython':\n        try:\n            import uvloop\n        except ImportError:\n            pass\n        else:\n            # Test for missing shutdown_default_executor() (uvloop 0.14.0 and earlier)\n            if (not hasattr(asyncio.AbstractEventLoop, 'shutdown_default_executor')\n                    or hasattr(uvloop.loop.Loop, 'shutdown_default_executor')):\n                policy = uvloop.EventLoopPolicy()\n\n    if policy is not None:\n        asyncio.set_event_loop_policy(policy)\n\n\ndef run(func: Callable[..., Awaitable[T_Retval]], *args: object,\n        debug: bool = False, use_uvloop: bool = False,\n        policy: Optional[asyncio.AbstractEventLoopPolicy] = None) -> T_Retval:\n    @wraps(func)\n    async def wrapper() -> T_Retval:\n        task = cast(asyncio.Task, current_task())\n        task_state = TaskState(None, get_callable_name(func), None)\n        _task_states[task] = task_state\n        if _native_task_names:\n            task.set_name(task_state.name)\n\n        try:\n            return await func(*args)\n        finally:\n            del _task_states[task]\n\n    _maybe_set_event_loop_policy(policy, use_uvloop)\n    return native_run(wrapper(), debug=debug)\n\n\n#\n# Miscellaneous\n#\n\nsleep = asyncio.sleep\n\n\n#\n# Timeouts and cancellation\n#\n\nCancelledError = asyncio.CancelledError\n\n\nclass CancelScope(BaseCancelScope):\n    def __new__(cls, *, deadline: float = math.inf, shield: bool = False) -> \"CancelScope\":\n        return object.__new__(cls)\n\n    def __init__(self, deadline: float = math.inf, shield: bool = False):\n        self._deadline = deadline\n        self._shield = shield\n        self._parent_scope: Optional[CancelScope] = None\n        self._cancel_called = False\n        self._active = False\n        self._timeout_handle: Optional[asyncio.TimerHandle] = None\n        self._cancel_handle: Optional[asyncio.Handle] = None\n        self._tasks: Set[asyncio.Task] = set()\n        self._host_task: Optional[asyncio.Task] = None\n        self._timeout_expired = False\n\n    def __enter__(self) -> \"CancelScope\":\n        if self._active:\n            raise RuntimeError(\n                \"Each CancelScope may only be used for a single 'with' block\"\n            )\n\n        self._host_task = host_task = cast(asyncio.Task, current_task())\n        self._tasks.add(host_task)\n        try:\n            task_state = _task_states[host_task]\n        except KeyError:\n            task_name = host_task.get_name() if _native_task_names else None\n            task_state = TaskState(None, task_name, self)\n            _task_states[host_task] = task_state\n        else:\n            self._parent_scope = task_state.cancel_scope\n            task_state.cancel_scope = self\n\n        self._timeout()\n        self._active = True\n        return self\n\n    def __exit__(self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException],\n                 exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        if not self._active:\n            raise RuntimeError('This cancel scope is not active')\n        if current_task() is not self._host_task:\n            raise RuntimeError('Attempted to exit cancel scope in a different task than it was '\n                               'entered in')\n\n        assert self._host_task is not None\n        host_task_state = _task_states.get(self._host_task)\n        if host_task_state is None or host_task_state.cancel_scope is not self:\n            raise RuntimeError(\"Attempted to exit a cancel scope that isn't the current tasks's \"\n                               \"current cancel scope\")\n\n        self._active = False\n        if self._timeout_handle:\n            self._timeout_handle.cancel()\n            self._timeout_handle = None\n\n        self._tasks.remove(self._host_task)\n\n        host_task_state.cancel_scope = self._parent_scope\n\n        # Restart the cancellation effort in the farthest directly cancelled parent scope if this\n        # one was shielded\n        if self._shield:\n            self._deliver_cancellation_to_parent()\n\n        if exc_val is not None:\n            exceptions = exc_val.exceptions if isinstance(exc_val, ExceptionGroup) else [exc_val]\n            if all(isinstance(exc, CancelledError) for exc in exceptions):\n                if self._timeout_expired:\n                    return True\n                elif not self._cancel_called:\n                    # Task was cancelled natively\n                    return None\n                elif not self._parent_cancelled():\n                    # This scope was directly cancelled\n                    return True\n\n        return None\n\n    def _timeout(self) -> None:\n        if self._deadline != math.inf:\n            loop = get_running_loop()\n            if loop.time() >= self._deadline:\n                self._timeout_expired = True\n                self.cancel()\n            else:\n                self._timeout_handle = loop.call_at(self._deadline, self._timeout)\n\n    def _deliver_cancellation(self) -> None:\n        \"\"\"\n        Deliver cancellation to directly contained tasks and nested cancel scopes.\n\n        Schedule another run at the end if we still have tasks eligible for cancellation.\n        \"\"\"\n        should_retry = False\n        current = current_task()\n        for task in self._tasks:\n            if task._must_cancel:  # type: ignore[attr-defined]\n                continue\n\n            # The task is eligible for cancellation if it has started and is not in a cancel\n            # scope shielded from this one\n            cancel_scope = _task_states[task].cancel_scope\n            while cancel_scope is not self:\n                if cancel_scope is None or cancel_scope._shield:\n                    break\n                else:\n                    cancel_scope = cancel_scope._parent_scope\n            else:\n                should_retry = True\n                if task is not current and (task is self._host_task or _task_started(task)):\n                    task.cancel()\n\n        # Schedule another callback if there are still tasks left\n        if should_retry:\n            self._cancel_handle = get_running_loop().call_soon(self._deliver_cancellation)\n        else:\n            self._cancel_handle = None\n\n    def _deliver_cancellation_to_parent(self) -> None:\n        \"\"\"Start cancellation effort in the farthest directly cancelled parent scope\"\"\"\n        scope = self._parent_scope\n        scope_to_cancel: Optional[CancelScope] = None\n        while scope is not None:\n            if scope._cancel_called and scope._cancel_handle is None:\n                scope_to_cancel = scope\n\n            # No point in looking beyond any shielded scope\n            if scope._shield:\n                break\n\n            scope = scope._parent_scope\n\n        if scope_to_cancel is not None:\n            scope_to_cancel._deliver_cancellation()\n\n    def _parent_cancelled(self) -> bool:\n        # Check whether any parent has been cancelled\n        cancel_scope = self._parent_scope\n        while cancel_scope is not None and not cancel_scope._shield:\n            if cancel_scope._cancel_called:\n                return True\n            else:\n                cancel_scope = cancel_scope._parent_scope\n\n        return False\n\n    def cancel(self) -> DeprecatedAwaitable:\n        if not self._cancel_called:\n            if self._timeout_handle:\n                self._timeout_handle.cancel()\n                self._timeout_handle = None\n\n            self._cancel_called = True\n            self._deliver_cancellation()\n\n        return DeprecatedAwaitable(self.cancel)\n\n    @property\n    def deadline(self) -> float:\n        return self._deadline\n\n    @deadline.setter\n    def deadline(self, value: float) -> None:\n        self._deadline = float(value)\n        if self._timeout_handle is not None:\n            self._timeout_handle.cancel()\n            self._timeout_handle = None\n\n        if self._active and not self._cancel_called:\n            self._timeout()\n\n    @property\n    def cancel_called(self) -> bool:\n        return self._cancel_called\n\n    @property\n    def shield(self) -> bool:\n        return self._shield\n\n    @shield.setter\n    def shield(self, value: bool) -> None:\n        if self._shield != value:\n            self._shield = value\n            if not value:\n                self._deliver_cancellation_to_parent()\n\n\nasync def checkpoint() -> None:\n    await sleep(0)\n\n\nasync def checkpoint_if_cancelled() -> None:\n    task = current_task()\n    if task is None:\n        return\n\n    try:\n        cancel_scope = _task_states[task].cancel_scope\n    except KeyError:\n        return\n\n    while cancel_scope:\n        if cancel_scope.cancel_called:\n            await sleep(0)\n        elif cancel_scope.shield:\n            break\n        else:\n            cancel_scope = cancel_scope._parent_scope\n\n\nasync def cancel_shielded_checkpoint() -> None:\n    with CancelScope(shield=True):\n        await sleep(0)\n\n\ndef current_effective_deadline() -> float:\n    try:\n        cancel_scope = _task_states[current_task()].cancel_scope  # type: ignore[index]\n    except KeyError:\n        return math.inf\n\n    deadline = math.inf\n    while cancel_scope:\n        deadline = min(deadline, cancel_scope.deadline)\n        if cancel_scope.shield:\n            break\n        else:\n            cancel_scope = cancel_scope._parent_scope\n\n    return deadline\n\n\ndef current_time() -> float:\n    return get_running_loop().time()\n\n\n#\n# Task states\n#\n\nclass TaskState:\n    \"\"\"\n    Encapsulates auxiliary task information that cannot be added to the Task instance itself\n    because there are no guarantees about its implementation.\n    \"\"\"\n\n    __slots__ = 'parent_id', 'name', 'cancel_scope'\n\n    def __init__(self, parent_id: Optional[int], name: Optional[str],\n                 cancel_scope: Optional[CancelScope]):\n        self.parent_id = parent_id\n        self.name = name\n        self.cancel_scope = cancel_scope\n\n\n_task_states = WeakKeyDictionary()  # type: WeakKeyDictionary[asyncio.Task, TaskState]\n\n\n#\n# Task groups\n#\n\nclass ExceptionGroup(BaseExceptionGroup):\n    def __init__(self, exceptions: List[BaseException]):\n        super().__init__()\n        self.exceptions = exceptions\n\n\nclass _AsyncioTaskStatus(abc.TaskStatus):\n    def __init__(self, future: asyncio.Future, parent_id: int):\n        self._future = future\n        self._parent_id = parent_id\n\n    def started(self, value: object = None) -> None:\n        try:\n            self._future.set_result(value)\n        except asyncio.InvalidStateError:\n            raise RuntimeError(\"called 'started' twice on the same task status\") from None\n\n        task = cast(asyncio.Task, current_task())\n        _task_states[task].parent_id = self._parent_id\n\n\nclass TaskGroup(abc.TaskGroup):\n    def __init__(self) -> None:\n        self.cancel_scope: CancelScope = CancelScope()\n        self._active = False\n        self._exceptions: List[BaseException] = []\n\n    async def __aenter__(self) -> \"TaskGroup\":\n        self.cancel_scope.__enter__()\n        self._active = True\n        return self\n\n    async def __aexit__(self, exc_type: Optional[Type[BaseException]],\n                        exc_val: Optional[BaseException],\n                        exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        ignore_exception = self.cancel_scope.__exit__(exc_type, exc_val, exc_tb)\n        if exc_val is not None:\n            self.cancel_scope.cancel()\n            self._exceptions.append(exc_val)\n\n        while self.cancel_scope._tasks:\n            try:\n                await asyncio.wait(self.cancel_scope._tasks)\n            except asyncio.CancelledError:\n                self.cancel_scope.cancel()\n\n        self._active = False\n        if not self.cancel_scope._parent_cancelled():\n            exceptions = self._filter_cancellation_errors(self._exceptions)\n        else:\n            exceptions = self._exceptions\n\n        try:\n            if len(exceptions) > 1:\n                if all(isinstance(e, CancelledError) and not e.args for e in exceptions):\n                    # Tasks were cancelled natively, without a cancellation message\n                    raise CancelledError\n                else:\n                    raise ExceptionGroup(exceptions)\n            elif exceptions and exceptions[0] is not exc_val:\n                raise exceptions[0]\n        except BaseException as exc:\n            # Clear the context here, as it can only be done in-flight.\n            # If the context is not cleared, it can result in recursive tracebacks (see #145).\n            exc.__context__ = None\n            raise\n\n        return ignore_exception\n\n    @staticmethod\n    def _filter_cancellation_errors(exceptions: Sequence[BaseException]) -> List[BaseException]:\n        filtered_exceptions: List[BaseException] = []\n        for exc in exceptions:\n            if isinstance(exc, ExceptionGroup):\n                new_exceptions = TaskGroup._filter_cancellation_errors(exc.exceptions)\n                if len(new_exceptions) > 1:\n                    filtered_exceptions.append(exc)\n                elif len(new_exceptions) == 1:\n                    filtered_exceptions.append(new_exceptions[0])\n                elif new_exceptions:\n                    new_exc = ExceptionGroup(new_exceptions)\n                    new_exc.__cause__ = exc.__cause__\n                    new_exc.__context__ = exc.__context__\n                    new_exc.__traceback__ = exc.__traceback__\n                    filtered_exceptions.append(new_exc)\n            elif not isinstance(exc, CancelledError) or exc.args:\n                filtered_exceptions.append(exc)\n\n        return filtered_exceptions\n\n    async def _run_wrapped_task(\n            self, coro: Coroutine, task_status_future: Optional[asyncio.Future]) -> None:\n        # This is the code path for Python 3.6 and 3.7 on which asyncio freaks out if a task raises\n        # a BaseException.\n        __traceback_hide__ = __tracebackhide__ = True  # noqa: F841\n        task = cast(asyncio.Task, current_task())\n        try:\n            await coro\n        except BaseException as exc:\n            if task_status_future is None or task_status_future.done():\n                self._exceptions.append(exc)\n                self.cancel_scope.cancel()\n            else:\n                task_status_future.set_exception(exc)\n        else:\n            if task_status_future is not None and not task_status_future.done():\n                task_status_future.set_exception(\n                    RuntimeError('Child exited without calling task_status.started()'))\n        finally:\n            if task in self.cancel_scope._tasks:\n                self.cancel_scope._tasks.remove(task)\n                del _task_states[task]\n\n    def _spawn(self, func: Callable[..., Coroutine], args: tuple, name: object,\n               task_status_future: Optional[asyncio.Future] = None) -> asyncio.Task:\n        def task_done(_task: asyncio.Task) -> None:\n            # This is the code path for Python 3.8+\n            assert _task in self.cancel_scope._tasks\n            self.cancel_scope._tasks.remove(_task)\n            del _task_states[_task]\n\n            try:\n                exc = _task.exception()\n            except CancelledError as e:\n                while isinstance(e.__context__, CancelledError):\n                    e = e.__context__\n\n                exc = e\n\n            if exc is not None:\n                if task_status_future is None or task_status_future.done():\n                    self._exceptions.append(exc)\n                    self.cancel_scope.cancel()\n                else:\n                    task_status_future.set_exception(exc)\n            elif task_status_future is not None and not task_status_future.done():\n                task_status_future.set_exception(\n                    RuntimeError('Child exited without calling task_status.started()'))\n\n        if not self._active:\n            raise RuntimeError('This task group is not active; no new tasks can be started.')\n\n        options = {}\n        name = get_callable_name(func) if name is None else str(name)\n        if _native_task_names:\n            options['name'] = name\n\n        kwargs = {}\n        if task_status_future:\n            parent_id = id(current_task())\n            kwargs['task_status'] = _AsyncioTaskStatus(task_status_future,\n                                                       id(self.cancel_scope._host_task))\n        else:\n            parent_id = id(self.cancel_scope._host_task)\n\n        coro = func(*args, **kwargs)\n        if not asyncio.iscoroutine(coro):\n            raise TypeError(f'Expected an async function, but {func} appears to be synchronous')\n\n        foreign_coro = not hasattr(coro, 'cr_frame') and not hasattr(coro, 'gi_frame')\n        if foreign_coro or sys.version_info < (3, 8):\n            coro = self._run_wrapped_task(coro, task_status_future)\n\n        task = create_task(coro, **options)\n        if not foreign_coro and sys.version_info >= (3, 8):\n            task.add_done_callback(task_done)\n\n        # Make the spawned task inherit the task group's cancel scope\n        _task_states[task] = TaskState(parent_id=parent_id, name=name,\n                                       cancel_scope=self.cancel_scope)\n        self.cancel_scope._tasks.add(task)\n        return task\n\n    def start_soon(self, func: Callable[..., Coroutine], *args: object,\n                   name: object = None) -> None:\n        self._spawn(func, args, name)\n\n    async def start(self, func: Callable[..., Coroutine], *args: object,\n                    name: object = None) -> None:\n        future: asyncio.Future = asyncio.Future()\n        task = self._spawn(func, args, name, future)\n\n        # If the task raises an exception after sending a start value without a switch point\n        # between, the task group is cancelled and this method never proceeds to process the\n        # completed future. That's why we have to have a shielded cancel scope here.\n        with CancelScope(shield=True):\n            try:\n                return await future\n            except CancelledError:\n                task.cancel()\n                raise\n\n\n#\n# Threads\n#\n\n_Retval_Queue_Type = Tuple[Optional[T_Retval], Optional[BaseException]]\n\n\nclass WorkerThread(Thread):\n    MAX_IDLE_TIME = 10  # seconds\n\n    def __init__(self, root_task: asyncio.Task, workers: Set['WorkerThread'],\n                 idle_workers: Deque['WorkerThread']):\n        super().__init__(name='AnyIO worker thread')\n        self.root_task = root_task\n        self.workers = workers\n        self.idle_workers = idle_workers\n        self.loop = root_task._loop\n        self.queue: Queue[Union[Tuple[Context, Callable, tuple, asyncio.Future], None]] = Queue(2)\n        self.idle_since = current_time()\n        self.stopping = False\n\n    def _report_result(self, future: asyncio.Future, result: Any,\n                       exc: Optional[BaseException]) -> None:\n        self.idle_since = current_time()\n        if not self.stopping:\n            self.idle_workers.append(self)\n\n        if not future.cancelled():\n            if exc is not None:\n                future.set_exception(exc)\n            else:\n                future.set_result(result)\n\n    def run(self) -> None:\n        with claim_worker_thread('asyncio'):\n            threadlocals.loop = self.loop\n            while True:\n                item = self.queue.get()\n                if item is None:\n                    # Shutdown command received\n                    return\n\n                context, func, args, future = item\n                if not future.cancelled():\n                    result = None\n                    exception: Optional[BaseException] = None\n                    try:\n                        result = context.run(func, *args)\n                    except BaseException as exc:\n                        exception = exc\n\n                    if not self.loop.is_closed():\n                        self.loop.call_soon_threadsafe(\n                            self._report_result, future, result, exception)\n\n                self.queue.task_done()\n\n    def stop(self, f: Optional[asyncio.Task] = None) -> None:\n        self.stopping = True\n        self.queue.put_nowait(None)\n        self.workers.discard(self)\n        try:\n            self.idle_workers.remove(self)\n        except ValueError:\n            pass\n\n\n_threadpool_idle_workers: RunVar[Deque[WorkerThread]] = RunVar('_threadpool_idle_workers')\n_threadpool_workers: RunVar[Set[WorkerThread]] = RunVar('_threadpool_workers')\n\n\nasync def run_sync_in_worker_thread(\n        func: Callable[..., T_Retval], *args: object, cancellable: bool = False,\n        limiter: Optional['CapacityLimiter'] = None) -> T_Retval:\n    await checkpoint()\n\n    # If this is the first run in this event loop thread, set up the necessary variables\n    try:\n        idle_workers = _threadpool_idle_workers.get()\n        workers = _threadpool_workers.get()\n    except LookupError:\n        idle_workers = deque()\n        workers = set()\n        _threadpool_idle_workers.set(idle_workers)\n        _threadpool_workers.set(workers)\n\n    async with (limiter or current_default_thread_limiter()):\n        with CancelScope(shield=not cancellable):\n            future: asyncio.Future = asyncio.Future()\n            root_task = find_root_task()\n            if not idle_workers:\n                worker = WorkerThread(root_task, workers, idle_workers)\n                worker.start()\n                workers.add(worker)\n                root_task.add_done_callback(worker.stop)\n            else:\n                worker = idle_workers.pop()\n\n                # Prune any other workers that have been idle for MAX_IDLE_TIME seconds or longer\n                now = current_time()\n                while idle_workers:\n                    if now - idle_workers[0].idle_since < WorkerThread.MAX_IDLE_TIME:\n                        break\n\n                    expired_worker = idle_workers.popleft()\n                    expired_worker.root_task.remove_done_callback(expired_worker.stop)\n                    expired_worker.stop()\n\n            context = copy_context()\n            context.run(sniffio.current_async_library_cvar.set, None)\n            worker.queue.put_nowait((context, func, args, future))\n            return await future\n\n\ndef run_sync_from_thread(func: Callable[..., T_Retval], *args: object,\n                         loop: Optional[asyncio.AbstractEventLoop] = None) -> T_Retval:\n    @wraps(func)\n    def wrapper() -> None:\n        try:\n            f.set_result(func(*args))\n        except BaseException as exc:\n            f.set_exception(exc)\n            if not isinstance(exc, Exception):\n                raise\n\n    f: concurrent.futures.Future[T_Retval] = Future()\n    loop = loop or threadlocals.loop\n    if sys.version_info < (3, 7):\n        loop.call_soon_threadsafe(copy_context().run, wrapper)\n    else:\n        loop.call_soon_threadsafe(wrapper)\n\n    return f.result()\n\n\ndef run_async_from_thread(\n    func: Callable[..., Coroutine[Any, Any, T_Retval]], *args: object\n) -> T_Retval:\n    f: concurrent.futures.Future[T_Retval] = asyncio.run_coroutine_threadsafe(\n        func(*args), threadlocals.loop)\n    return f.result()\n\n\nclass BlockingPortal(abc.BlockingPortal):\n    def __new__(cls) -> \"BlockingPortal\":\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        super().__init__()\n        self._loop = get_running_loop()\n\n    def _spawn_task_from_thread(self, func: Callable, args: tuple, kwargs: Dict[str, Any],\n                                name: object, future: Future) -> None:\n        run_sync_from_thread(\n            partial(self._task_group.start_soon, name=name), self._call_func, func, args, kwargs,\n            future, loop=self._loop)\n\n\n#\n# Subprocesses\n#\n\n@dataclass(eq=False)\nclass StreamReaderWrapper(abc.ByteReceiveStream):\n    _stream: asyncio.StreamReader\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        data = await self._stream.read(max_bytes)\n        if data:\n            return data\n        else:\n            raise EndOfStream\n\n    async def aclose(self) -> None:\n        self._stream.feed_eof()\n\n\n@dataclass(eq=False)\nclass StreamWriterWrapper(abc.ByteSendStream):\n    _stream: asyncio.StreamWriter\n\n    async def send(self, item: bytes) -> None:\n        self._stream.write(item)\n        await self._stream.drain()\n\n    async def aclose(self) -> None:\n        self._stream.close()\n\n\n@dataclass(eq=False)\nclass Process(abc.Process):\n    _process: asyncio.subprocess.Process\n    _stdin: Optional[StreamWriterWrapper]\n    _stdout: Optional[StreamReaderWrapper]\n    _stderr: Optional[StreamReaderWrapper]\n\n    async def aclose(self) -> None:\n        if self._stdin:\n            await self._stdin.aclose()\n        if self._stdout:\n            await self._stdout.aclose()\n        if self._stderr:\n            await self._stderr.aclose()\n\n        await self.wait()\n\n    async def wait(self) -> int:\n        return await self._process.wait()\n\n    def terminate(self) -> None:\n        self._process.terminate()\n\n    def kill(self) -> None:\n        self._process.kill()\n\n    def send_signal(self, signal: int) -> None:\n        self._process.send_signal(signal)\n\n    @property\n    def pid(self) -> int:\n        return self._process.pid\n\n    @property\n    def returncode(self) -> Optional[int]:\n        return self._process.returncode\n\n    @property\n    def stdin(self) -> Optional[abc.ByteSendStream]:\n        return self._stdin\n\n    @property\n    def stdout(self) -> Optional[abc.ByteReceiveStream]:\n        return self._stdout\n\n    @property\n    def stderr(self) -> Optional[abc.ByteReceiveStream]:\n        return self._stderr\n\n\nasync def open_process(command: Union[str, Sequence[str]], *, shell: bool,\n                       stdin: int, stdout: int, stderr: int,\n                       cwd: Union[str, bytes, PathLike, None] = None,\n                       env: Optional[Mapping[str, str]] = None,\n                       start_new_session: bool = False) -> Process:\n    await checkpoint()\n    if shell:\n        process = await asyncio.create_subprocess_shell(\n            command, stdin=stdin, stdout=stdout,  # type: ignore[arg-type]\n            stderr=stderr, cwd=cwd, env=env, start_new_session=start_new_session,\n        )\n    else:\n        process = await asyncio.create_subprocess_exec(*command, stdin=stdin, stdout=stdout,\n                                                       stderr=stderr, cwd=cwd, env=env,\n                                                       start_new_session=start_new_session)\n\n    stdin_stream = StreamWriterWrapper(process.stdin) if process.stdin else None\n    stdout_stream = StreamReaderWrapper(process.stdout) if process.stdout else None\n    stderr_stream = StreamReaderWrapper(process.stderr) if process.stderr else None\n    return Process(process, stdin_stream, stdout_stream, stderr_stream)\n\n\ndef _forcibly_shutdown_process_pool_on_exit(workers: Set[Process], _task: object) -> None:\n    \"\"\"\n    Forcibly shuts down worker processes belonging to this event loop.\"\"\"\n    child_watcher: Optional[asyncio.AbstractChildWatcher]\n    try:\n        child_watcher = asyncio.get_event_loop_policy().get_child_watcher()\n    except NotImplementedError:\n        child_watcher = None\n\n    # Close as much as possible (w/o async/await) to avoid warnings\n    for process in workers:\n        if process.returncode is None:\n            continue\n\n        process._stdin._stream._transport.close()  # type: ignore[union-attr]\n        process._stdout._stream._transport.close()  # type: ignore[union-attr]\n        process._stderr._stream._transport.close()  # type: ignore[union-attr]\n        process.kill()\n        if child_watcher:\n            child_watcher.remove_child_handler(process.pid)\n\n\nasync def _shutdown_process_pool_on_exit(workers: Set[Process]) -> None:\n    \"\"\"\n    Shuts down worker processes belonging to this event loop.\n\n    NOTE: this only works when the event loop was started using asyncio.run() or anyio.run().\n\n    \"\"\"\n    process: Process\n    try:\n        await sleep(math.inf)\n    except asyncio.CancelledError:\n        for process in workers:\n            if process.returncode is None:\n                process.kill()\n\n        for process in workers:\n            await process.aclose()\n\n\ndef setup_process_pool_exit_at_shutdown(workers: Set[Process]) -> None:\n    kwargs = {'name': 'AnyIO process pool shutdown task'} if _native_task_names else {}\n    create_task(_shutdown_process_pool_on_exit(workers), **kwargs)\n    find_root_task().add_done_callback(partial(_forcibly_shutdown_process_pool_on_exit, workers))\n\n\n#\n# Sockets and networking\n#\n\n\nclass StreamProtocol(asyncio.Protocol):\n    read_queue: Deque[bytes]\n    read_event: asyncio.Event\n    write_event: asyncio.Event\n    exception: Optional[Exception] = None\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        self.read_queue = deque()\n        self.read_event = asyncio.Event()\n        self.write_event = asyncio.Event()\n        self.write_event.set()\n        cast(asyncio.Transport, transport).set_write_buffer_limits(0)\n\n    def connection_lost(self, exc: Optional[Exception]) -> None:\n        if exc:\n            self.exception = BrokenResourceError()\n            self.exception.__cause__ = exc\n\n        self.read_event.set()\n        self.write_event.set()\n\n    def data_received(self, data: bytes) -> None:\n        self.read_queue.append(data)\n        self.read_event.set()\n\n    def eof_received(self) -> Optional[bool]:\n        self.read_event.set()\n        return True\n\n    def pause_writing(self) -> None:\n        self.write_event = asyncio.Event()\n\n    def resume_writing(self) -> None:\n        self.write_event.set()\n\n\nclass DatagramProtocol(asyncio.DatagramProtocol):\n    read_queue: Deque[Tuple[bytes, IPSockAddrType]]\n    read_event: asyncio.Event\n    write_event: asyncio.Event\n    exception: Optional[Exception] = None\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        self.read_queue = deque(maxlen=100)  # arbitrary value\n        self.read_event = asyncio.Event()\n        self.write_event = asyncio.Event()\n        self.write_event.set()\n\n    def connection_lost(self, exc: Optional[Exception]) -> None:\n        self.read_event.set()\n        self.write_event.set()\n\n    def datagram_received(self, data: bytes, addr: IPSockAddrType) -> None:\n        addr = convert_ipv6_sockaddr(addr)\n        self.read_queue.append((data, addr))\n        self.read_event.set()\n\n    def error_received(self, exc: Exception) -> None:\n        self.exception = exc\n\n    def pause_writing(self) -> None:\n        self.write_event.clear()\n\n    def resume_writing(self) -> None:\n        self.write_event.set()\n\n\nclass SocketStream(abc.SocketStream):\n    def __init__(self, transport: asyncio.Transport, protocol: StreamProtocol):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard('reading from')\n        self._send_guard = ResourceGuard('writing to')\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info('socket')\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        with self._receive_guard:\n            await checkpoint()\n\n            if not self._protocol.read_event.is_set() and not self._transport.is_closing():\n                self._transport.resume_reading()\n                await self._protocol.read_event.wait()\n                self._transport.pause_reading()\n\n            try:\n                chunk = self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                elif self._protocol.exception:\n                    raise self._protocol.exception\n                else:\n                    raise EndOfStream from None\n\n            if len(chunk) > max_bytes:\n                # Split the oversized chunk\n                chunk, leftover = chunk[:max_bytes], chunk[max_bytes:]\n                self._protocol.read_queue.appendleft(leftover)\n\n            # If the read queue is empty, clear the flag so that the next call will block until\n            # data is available\n            if not self._protocol.read_queue:\n                self._protocol.read_event.clear()\n\n        return chunk\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            await checkpoint()\n\n            if self._closed:\n                raise ClosedResourceError\n            elif self._protocol.exception is not None:\n                raise self._protocol.exception\n\n            try:\n                self._transport.write(item)\n            except RuntimeError as exc:\n                if self._transport.is_closing():\n                    raise BrokenResourceError from exc\n                else:\n                    raise\n\n            await self._protocol.write_event.wait()\n\n    async def send_eof(self) -> None:\n        try:\n            self._transport.write_eof()\n        except OSError:\n            pass\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            try:\n                self._transport.write_eof()\n            except OSError:\n                pass\n\n            self._transport.close()\n            await sleep(0)\n            self._transport.abort()\n\n\nclass UNIXSocketStream(abc.SocketStream):\n    _receive_future: Optional[asyncio.Future] = None\n    _send_future: Optional[asyncio.Future] = None\n    _closing = False\n\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._loop = get_running_loop()\n        self._receive_guard = ResourceGuard('reading from')\n        self._send_guard = ResourceGuard('writing to')\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n    def _wait_until_readable(self, loop: asyncio.AbstractEventLoop) -> asyncio.Future:\n        def callback(f: object) -> None:\n            del self._receive_future\n            loop.remove_reader(self.__raw_socket)\n\n        f = self._receive_future = asyncio.Future()\n        self._loop.add_reader(self.__raw_socket, f.set_result, None)\n        f.add_done_callback(callback)\n        return f\n\n    def _wait_until_writable(self, loop: asyncio.AbstractEventLoop) -> asyncio.Future:\n        def callback(f: object) -> None:\n            del self._send_future\n            loop.remove_writer(self.__raw_socket)\n\n        f = self._send_future = asyncio.Future()\n        self._loop.add_writer(self.__raw_socket, f.set_result, None)\n        f.add_done_callback(callback)\n        return f\n\n    async def send_eof(self) -> None:\n        with self._send_guard:\n            self._raw_socket.shutdown(socket.SHUT_WR)\n\n    async def receive(self, max_bytes: int = 65536) -> bytes:\n        loop = get_running_loop()\n        await checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    data = self.__raw_socket.recv(max_bytes)\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    if not data:\n                        raise EndOfStream\n\n                    return data\n\n    async def send(self, item: bytes) -> None:\n        loop = get_running_loop()\n        await checkpoint()\n        with self._send_guard:\n            view = memoryview(item)\n            while view:\n                try:\n                    bytes_sent = self.__raw_socket.send(item)\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    view = view[bytes_sent:]\n\n    async def receive_fds(self, msglen: int, maxfds: int) -> Tuple[bytes, List[int]]:\n        if not isinstance(msglen, int) or msglen < 0:\n            raise ValueError('msglen must be a non-negative integer')\n        if not isinstance(maxfds, int) or maxfds < 1:\n            raise ValueError('maxfds must be a positive integer')\n\n        loop = get_running_loop()\n        fds = array.array(\"i\")\n        await checkpoint()\n        with self._receive_guard:\n            while True:\n                try:\n                    message, ancdata, flags, addr = self.__raw_socket.recvmsg(\n                        msglen, socket.CMSG_LEN(maxfds * fds.itemsize))\n                except BlockingIOError:\n                    await self._wait_until_readable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n                else:\n                    if not message and not ancdata:\n                        raise EndOfStream\n\n                    break\n\n        for cmsg_level, cmsg_type, cmsg_data in ancdata:\n            if cmsg_level != socket.SOL_SOCKET or cmsg_type != socket.SCM_RIGHTS:\n                raise RuntimeError(f'Received unexpected ancillary data; message = {message!r}, '\n                                   f'cmsg_level = {cmsg_level}, cmsg_type = {cmsg_type}')\n\n            fds.frombytes(cmsg_data[:len(cmsg_data) - (len(cmsg_data) % fds.itemsize)])\n\n        return message, list(fds)\n\n    async def send_fds(self, message: bytes, fds: Collection[Union[int, IOBase]]) -> None:\n        if not message:\n            raise ValueError('message must not be empty')\n        if not fds:\n            raise ValueError('fds must not be empty')\n\n        loop = get_running_loop()\n        filenos: List[int] = []\n        for fd in fds:\n            if isinstance(fd, int):\n                filenos.append(fd)\n            elif isinstance(fd, IOBase):\n                filenos.append(fd.fileno())\n\n        fdarray = array.array(\"i\", filenos)\n        await checkpoint()\n        with self._send_guard:\n            while True:\n                try:\n                    # The ignore can be removed after mypy picks up\n                    # https://github.com/python/typeshed/pull/5545\n                    self.__raw_socket.sendmsg(\n                        [message],\n                        [(socket.SOL_SOCKET, socket.SCM_RIGHTS, fdarray)]\n                    )\n                    break\n                except BlockingIOError:\n                    await self._wait_until_writable(loop)\n                except OSError as exc:\n                    if self._closing:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n\n    async def aclose(self) -> None:\n        if not self._closing:\n            self._closing = True\n            if self.__raw_socket.fileno() != -1:\n                self.__raw_socket.close()\n\n            if self._receive_future:\n                self._receive_future.set_result(None)\n            if self._send_future:\n                self._send_future.set_result(None)\n\n\nclass TCPSocketListener(abc.SocketListener):\n    _accept_scope: Optional[CancelScope] = None\n    _closed = False\n\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._loop = cast(asyncio.BaseEventLoop, get_running_loop())\n        self._accept_guard = ResourceGuard('accepting connections from')\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n    async def accept(self) -> abc.SocketStream:\n        if self._closed:\n            raise ClosedResourceError\n\n        with self._accept_guard:\n            await checkpoint()\n            with CancelScope() as self._accept_scope:\n                try:\n                    client_sock, _addr = await self._loop.sock_accept(self._raw_socket)\n                except asyncio.CancelledError:\n                    # Workaround for https://bugs.python.org/issue41317\n                    try:\n                        self._loop.remove_reader(self._raw_socket)\n                    except (ValueError, NotImplementedError):\n                        pass\n\n                    if self._closed:\n                        raise ClosedResourceError from None\n\n                    raise\n                finally:\n                    self._accept_scope = None\n\n        client_sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        transport, protocol = await self._loop.connect_accepted_socket(StreamProtocol, client_sock)\n        return SocketStream(cast(asyncio.Transport, transport), cast(StreamProtocol, protocol))\n\n    async def aclose(self) -> None:\n        if self._closed:\n            return\n\n        self._closed = True\n        if self._accept_scope:\n            # Workaround for https://bugs.python.org/issue41317\n            try:\n                self._loop.remove_reader(self._raw_socket)\n            except (ValueError, NotImplementedError):\n                pass\n\n            self._accept_scope.cancel()\n            await sleep(0)\n\n        self._raw_socket.close()\n\n\nclass UNIXSocketListener(abc.SocketListener):\n    def __init__(self, raw_socket: socket.socket):\n        self.__raw_socket = raw_socket\n        self._loop = get_running_loop()\n        self._accept_guard = ResourceGuard('accepting connections from')\n        self._closed = False\n\n    async def accept(self) -> abc.SocketStream:\n        await checkpoint()\n        with self._accept_guard:\n            while True:\n                try:\n                    client_sock, _ = self.__raw_socket.accept()\n                    client_sock.setblocking(False)\n                    return UNIXSocketStream(client_sock)\n                except BlockingIOError:\n                    f: asyncio.Future = asyncio.Future()\n                    self._loop.add_reader(self.__raw_socket, f.set_result, None)\n                    f.add_done_callback(lambda _: self._loop.remove_reader(self.__raw_socket))\n                    await f\n                except OSError as exc:\n                    if self._closed:\n                        raise ClosedResourceError from None\n                    else:\n                        raise BrokenResourceError from exc\n\n    async def aclose(self) -> None:\n        self._closed = True\n        self.__raw_socket.close()\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self.__raw_socket\n\n\nclass UDPSocket(abc.UDPSocket):\n    def __init__(self, transport: asyncio.DatagramTransport, protocol: DatagramProtocol):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard('reading from')\n        self._send_guard = ResourceGuard('writing to')\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info('socket')\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            self._transport.close()\n\n    async def receive(self) -> Tuple[bytes, IPSockAddrType]:\n        with self._receive_guard:\n            await checkpoint()\n\n            # If the buffer is empty, ask for more data\n            if not self._protocol.read_queue and not self._transport.is_closing():\n                self._protocol.read_event.clear()\n                await self._protocol.read_event.wait()\n\n            try:\n                return self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                else:\n                    raise BrokenResourceError from None\n\n    async def send(self, item: UDPPacketType) -> None:\n        with self._send_guard:\n            await checkpoint()\n            await self._protocol.write_event.wait()\n            if self._closed:\n                raise ClosedResourceError\n            elif self._transport.is_closing():\n                raise BrokenResourceError\n            else:\n                self._transport.sendto(*item)\n\n\nclass ConnectedUDPSocket(abc.ConnectedUDPSocket):\n    def __init__(self, transport: asyncio.DatagramTransport, protocol: DatagramProtocol):\n        self._transport = transport\n        self._protocol = protocol\n        self._receive_guard = ResourceGuard('reading from')\n        self._send_guard = ResourceGuard('writing to')\n        self._closed = False\n\n    @property\n    def _raw_socket(self) -> socket.socket:\n        return self._transport.get_extra_info('socket')\n\n    async def aclose(self) -> None:\n        if not self._transport.is_closing():\n            self._closed = True\n            self._transport.close()\n\n    async def receive(self) -> bytes:\n        with self._receive_guard:\n            await checkpoint()\n\n            # If the buffer is empty, ask for more data\n            if not self._protocol.read_queue and not self._transport.is_closing():\n                self._protocol.read_event.clear()\n                await self._protocol.read_event.wait()\n\n            try:\n                packet = self._protocol.read_queue.popleft()\n            except IndexError:\n                if self._closed:\n                    raise ClosedResourceError from None\n                else:\n                    raise BrokenResourceError from None\n\n            return packet[0]\n\n    async def send(self, item: bytes) -> None:\n        with self._send_guard:\n            await checkpoint()\n            await self._protocol.write_event.wait()\n            if self._closed:\n                raise ClosedResourceError\n            elif self._transport.is_closing():\n                raise BrokenResourceError\n            else:\n                self._transport.sendto(item)\n\n\nasync def connect_tcp(host: str, port: int,\n                      local_addr: Optional[Tuple[str, int]] = None) -> SocketStream:\n    transport, protocol = cast(\n        Tuple[asyncio.Transport, StreamProtocol],\n        await get_running_loop().create_connection(StreamProtocol, host, port,\n                                                   local_addr=local_addr)\n    )\n    transport.pause_reading()\n    return SocketStream(transport, protocol)\n\n\nasync def connect_unix(path: str) -> UNIXSocketStream:\n    await checkpoint()\n    loop = get_running_loop()\n    raw_socket = socket.socket(socket.AF_UNIX)\n    raw_socket.setblocking(False)\n    while True:\n        try:\n            raw_socket.connect(path)\n        except BlockingIOError:\n            f: asyncio.Future = asyncio.Future()\n            loop.add_writer(raw_socket, f.set_result, None)\n            f.add_done_callback(lambda _: loop.remove_writer(raw_socket))\n            await f\n        except BaseException:\n            raw_socket.close()\n            raise\n        else:\n            return UNIXSocketStream(raw_socket)\n\n\nasync def create_udp_socket(\n    family: socket.AddressFamily,\n    local_address: Optional[IPSockAddrType],\n    remote_address: Optional[IPSockAddrType],\n    reuse_port: bool\n) -> Union[UDPSocket, ConnectedUDPSocket]:\n    result = await get_running_loop().create_datagram_endpoint(\n        DatagramProtocol, local_addr=local_address, remote_addr=remote_address, family=family,\n        reuse_port=reuse_port)\n    transport = cast(asyncio.DatagramTransport, result[0])\n    protocol = cast(DatagramProtocol, result[1])\n    if protocol.exception:\n        transport.close()\n        raise protocol.exception\n\n    if not remote_address:\n        return UDPSocket(transport, protocol)\n    else:\n        return ConnectedUDPSocket(transport, protocol)\n\n\nasync def getaddrinfo(host: Union[bytearray, bytes, str], port: Union[str, int, None], *,\n                      family: Union[int, AddressFamily] = 0, type: Union[int, SocketKind] = 0,\n                      proto: int = 0, flags: int = 0) -> GetAddrInfoReturnType:\n    # https://github.com/python/typeshed/pull/4304\n    result = await get_running_loop().getaddrinfo(\n        host, port, family=family, type=type, proto=proto, flags=flags)  # type: ignore[arg-type]\n    return cast(GetAddrInfoReturnType, result)\n\n\nasync def getnameinfo(sockaddr: IPSockAddrType, flags: int = 0) -> Tuple[str, str]:\n    return await get_running_loop().getnameinfo(sockaddr, flags)\n\n\n_read_events: RunVar[Dict[Any, asyncio.Event]] = RunVar('read_events')\n_write_events: RunVar[Dict[Any, asyncio.Event]] = RunVar('write_events')\n\n\nasync def wait_socket_readable(sock: socket.socket) -> None:\n    await checkpoint()\n    try:\n        read_events = _read_events.get()\n    except LookupError:\n        read_events = {}\n        _read_events.set(read_events)\n\n    if read_events.get(sock):\n        raise BusyResourceError('reading from') from None\n\n    loop = get_running_loop()\n    event = read_events[sock] = asyncio.Event()\n    loop.add_reader(sock, event.set)\n    try:\n        await event.wait()\n    finally:\n        if read_events.pop(sock, None) is not None:\n            loop.remove_reader(sock)\n            readable = True\n        else:\n            readable = False\n\n    if not readable:\n        raise ClosedResourceError\n\n\nasync def wait_socket_writable(sock: socket.socket) -> None:\n    await checkpoint()\n    try:\n        write_events = _write_events.get()\n    except LookupError:\n        write_events = {}\n        _write_events.set(write_events)\n\n    if write_events.get(sock):\n        raise BusyResourceError('writing to') from None\n\n    loop = get_running_loop()\n    event = write_events[sock] = asyncio.Event()\n    loop.add_writer(sock.fileno(), event.set)\n    try:\n        await event.wait()\n    finally:\n        if write_events.pop(sock, None) is not None:\n            loop.remove_writer(sock)\n            writable = True\n        else:\n            writable = False\n\n    if not writable:\n        raise ClosedResourceError\n\n\n#\n# Synchronization\n#\n\nclass Event(BaseEvent):\n    def __new__(cls) -> \"Event\":\n        return object.__new__(cls)\n\n    def __init__(self) -> None:\n        self._event = asyncio.Event()\n\n    def set(self) -> DeprecatedAwaitable:\n        self._event.set()\n        return DeprecatedAwaitable(self.set)\n\n    def is_set(self) -> bool:\n        return self._event.is_set()\n\n    async def wait(self) -> None:\n        if await self._event.wait():\n            await checkpoint()\n\n    def statistics(self) -> EventStatistics:\n        return EventStatistics(len(self._event._waiters))  # type: ignore[attr-defined]\n\n\nclass CapacityLimiter(BaseCapacityLimiter):\n    _total_tokens: float = 0\n\n    def __new__(cls, total_tokens: float) -> \"CapacityLimiter\":\n        return object.__new__(cls)\n\n    def __init__(self, total_tokens: float):\n        self._borrowers: Set[Any] = set()\n        self._wait_queue: Dict[Any, asyncio.Event] = OrderedDict()\n        self.total_tokens = total_tokens\n\n    async def __aenter__(self) -> None:\n        await self.acquire()\n\n    async def __aexit__(self, exc_type: Optional[Type[BaseException]],\n                        exc_val: Optional[BaseException],\n                        exc_tb: Optional[TracebackType]) -> None:\n        self.release()\n\n    @property\n    def total_tokens(self) -> float:\n        return self._total_tokens\n\n    @total_tokens.setter\n    def total_tokens(self, value: float) -> None:\n        if not isinstance(value, int) and not math.isinf(value):\n            raise TypeError('total_tokens must be an int or math.inf')\n        if value < 1:\n            raise ValueError('total_tokens must be >= 1')\n\n        old_value = self._total_tokens\n        self._total_tokens = value\n        events = []\n        for event in self._wait_queue.values():\n            if value <= old_value:\n                break\n\n            if not event.is_set():\n                events.append(event)\n                old_value += 1\n\n        for event in events:\n            event.set()\n\n    @property\n    def borrowed_tokens(self) -> int:\n        return len(self._borrowers)\n\n    @property\n    def available_tokens(self) -> float:\n        return self._total_tokens - len(self._borrowers)\n\n    def acquire_nowait(self) -> DeprecatedAwaitable:\n        self.acquire_on_behalf_of_nowait(current_task())\n        return DeprecatedAwaitable(self.acquire_nowait)\n\n    def acquire_on_behalf_of_nowait(self, borrower: object) -> DeprecatedAwaitable:\n        if borrower in self._borrowers:\n            raise RuntimeError(\"this borrower is already holding one of this CapacityLimiter's \"\n                               \"tokens\")\n\n        if self._wait_queue or len(self._borrowers) >= self._total_tokens:\n            raise WouldBlock\n\n        self._borrowers.add(borrower)\n        return DeprecatedAwaitable(self.acquire_on_behalf_of_nowait)\n\n    async def acquire(self) -> None:\n        return await self.acquire_on_behalf_of(current_task())\n\n    async def acquire_on_behalf_of(self, borrower: object) -> None:\n        await checkpoint_if_cancelled()\n        try:\n            self.acquire_on_behalf_of_nowait(borrower)\n        except WouldBlock:\n            event = asyncio.Event()\n            self._wait_queue[borrower] = event\n            try:\n                await event.wait()\n            except BaseException:\n                self._wait_queue.pop(borrower, None)\n                raise\n\n            self._borrowers.add(borrower)\n        else:\n            try:\n                await cancel_shielded_checkpoint()\n            except BaseException:\n                self.release()\n                raise\n\n    def release(self) -> None:\n        self.release_on_behalf_of(current_task())\n\n    def release_on_behalf_of(self, borrower: object) -> None:\n        try:\n            self._borrowers.remove(borrower)\n        except KeyError:\n            raise RuntimeError(\"this borrower isn't holding any of this CapacityLimiter's \"\n                               \"tokens\") from None\n\n        # Notify the next task in line if this limiter has free capacity now\n        if self._wait_queue and len(self._borrowers) < self._total_tokens:\n            event = self._wait_queue.popitem()[1]\n            event.set()\n\n    def statistics(self) -> CapacityLimiterStatistics:\n        return CapacityLimiterStatistics(self.borrowed_tokens, self.total_tokens,\n                                         tuple(self._borrowers), len(self._wait_queue))\n\n\n_default_thread_limiter: RunVar[CapacityLimiter] = RunVar('_default_thread_limiter')\n\n\ndef current_default_thread_limiter() -> CapacityLimiter:\n    try:\n        return _default_thread_limiter.get()\n    except LookupError:\n        limiter = CapacityLimiter(40)\n        _default_thread_limiter.set(limiter)\n        return limiter\n\n\n#\n# Operating system signals\n#\n\nclass _SignalReceiver(DeprecatedAsyncContextManager[\"_SignalReceiver\"]):\n    def __init__(self, signals: Tuple[int, ...]):\n        self._signals = signals\n        self._loop = get_running_loop()\n        self._signal_queue: Deque[int] = deque()\n        self._future: asyncio.Future = asyncio.Future()\n        self._handled_signals: Set[int] = set()\n\n    def _deliver(self, signum: int) -> None:\n        self._signal_queue.append(signum)\n        if not self._future.done():\n            self._future.set_result(None)\n\n    def __enter__(self) -> \"_SignalReceiver\":\n        for sig in set(self._signals):\n            self._loop.add_signal_handler(sig, self._deliver, sig)\n            self._handled_signals.add(sig)\n\n        return self\n\n    def __exit__(self, exc_type: Optional[Type[BaseException]],\n                 exc_val: Optional[BaseException],\n                 exc_tb: Optional[TracebackType]) -> Optional[bool]:\n        for sig in self._handled_signals:\n            self._loop.remove_signal_handler(sig)\n        return None\n\n    def __aiter__(self) -> \"_SignalReceiver\":\n        return self\n\n    async def __anext__(self) -> int:\n        await checkpoint()\n        if not self._signal_queue:\n            self._future = asyncio.Future()\n            await self._future\n\n        return self._signal_queue.popleft()\n\n\ndef open_signal_receiver(*signals: int) -> _SignalReceiver:\n    return _SignalReceiver(signals)\n\n\n#\n# Testing and debugging\n#\n\ndef _create_task_info(task: asyncio.Task) -> TaskInfo:\n    task_state = _task_states.get(task)\n    if task_state is None:\n        name = task.get_name() if _native_task_names else None\n        parent_id = None\n    else:\n        name = task_state.name\n        parent_id = task_state.parent_id\n\n    return TaskInfo(id(task), parent_id, name, get_coro(task))\n\n\ndef get_current_task() -> TaskInfo:\n    return _create_task_info(current_task())  # type: ignore[arg-type]\n\n\ndef get_running_tasks() -> List[TaskInfo]:\n    return [_create_task_info(task) for task in all_tasks() if not task.done()]\n\n\nasync def wait_all_tasks_blocked() -> None:\n    await checkpoint()\n    this_task = current_task()\n    while True:\n        for task in all_tasks():\n            if task is this_task:\n                continue\n\n            if task._fut_waiter is None or task._fut_waiter.done():  # type: ignore[attr-defined]\n                await sleep(0.1)\n                break\n        else:\n            return\n\n\nclass TestRunner(abc.TestRunner):\n    def __init__(self, debug: bool = False, use_uvloop: bool = False,\n                 policy: Optional[asyncio.AbstractEventLoopPolicy] = None):\n        _maybe_set_event_loop_policy(policy, use_uvloop)\n        self._loop = asyncio.new_event_loop()\n        self._loop.set_debug(debug)\n        asyncio.set_event_loop(self._loop)\n\n    def _cancel_all_tasks(self) -> None:\n        to_cancel = all_tasks(self._loop)\n        if not to_cancel:\n            return\n\n        for task in to_cancel:\n            task.cancel()\n\n        self._loop.run_until_complete(asyncio.gather(*to_cancel, return_exceptions=True))\n\n        for task in to_cancel:\n            if task.cancelled():\n                continue\n            if task.exception() is not None:\n                raise cast(BaseException, task.exception())\n\n    def close(self) -> None:\n        try:\n            self._cancel_all_tasks()\n            self._loop.run_until_complete(self._loop.shutdown_asyncgens())\n        finally:\n            asyncio.set_event_loop(None)\n            self._loop.close()\n\n    def call(self, func: Callable[..., Awaitable[T_Retval]],\n             *args: object, **kwargs: object) -> T_Retval:\n        def exception_handler(loop: asyncio.AbstractEventLoop, context: Dict[str, Any]) -> None:\n            exceptions.append(context['exception'])\n\n        exceptions: List[BaseException] = []\n        self._loop.set_exception_handler(exception_handler)\n        try:\n            retval: T_Retval = self._loop.run_until_complete(func(*args, **kwargs))\n        except Exception as exc:\n            retval = None  # type: ignore[assignment]\n            exceptions.append(exc)\n        finally:\n            self._loop.set_exception_handler(None)\n\n        if len(exceptions) == 1:\n            raise exceptions[0]\n        elif exceptions:\n            raise ExceptionGroup(exceptions)\n\n        return retval\n"}
{"type": "source_file", "path": "env/lib/python3.9/site-packages/anyio/abc/_subprocesses.py", "content": "from abc import abstractmethod\nfrom signal import Signals\nfrom typing import Optional\n\nfrom ._resources import AsyncResource\nfrom ._streams import ByteReceiveStream, ByteSendStream\n\n\nclass Process(AsyncResource):\n    \"\"\"An asynchronous version of :class:`subprocess.Popen`.\"\"\"\n\n    @abstractmethod\n    async def wait(self) -> int:\n        \"\"\"\n        Wait until the process exits.\n\n        :return: the exit code of the process\n        \"\"\"\n\n    @abstractmethod\n    def terminate(self) -> None:\n        \"\"\"\n        Terminates the process, gracefully if possible.\n\n        On Windows, this calls ``TerminateProcess()``.\n        On POSIX systems, this sends ``SIGTERM`` to the process.\n\n        .. seealso:: :meth:`subprocess.Popen.terminate`\n        \"\"\"\n\n    @abstractmethod\n    def kill(self) -> None:\n        \"\"\"\n        Kills the process.\n\n        On Windows, this calls ``TerminateProcess()``.\n        On POSIX systems, this sends ``SIGKILL`` to the process.\n\n        .. seealso:: :meth:`subprocess.Popen.kill`\n        \"\"\"\n\n    @abstractmethod\n    def send_signal(self, signal: Signals) -> None:\n        \"\"\"\n        Send a signal to the subprocess.\n\n        .. seealso:: :meth:`subprocess.Popen.send_signal`\n\n        :param signal: the signal number (e.g. :data:`signal.SIGHUP`)\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def pid(self) -> int:\n        \"\"\"The process ID of the process.\"\"\"\n\n    @property\n    @abstractmethod\n    def returncode(self) -> Optional[int]:\n        \"\"\"\n        The return code of the process. If the process has not yet terminated, this will be\n        ``None``.\n        \"\"\"\n\n    @property\n    @abstractmethod\n    def stdin(self) -> Optional[ByteSendStream]:\n        \"\"\"The stream for the standard input of the process.\"\"\"\n\n    @property\n    @abstractmethod\n    def stdout(self) -> Optional[ByteReceiveStream]:\n        \"\"\"The stream for the standard output of the process.\"\"\"\n\n    @property\n    @abstractmethod\n    def stderr(self) -> Optional[ByteReceiveStream]:\n        \"\"\"The stream for the standard error output of the process.\"\"\"\n"}
