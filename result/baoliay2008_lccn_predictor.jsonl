{"repo_info": {"repo_name": "lccn_predictor", "repo_owner": "baoliay2008", "repo_url": "https://github.com/baoliay2008/lccn_predictor"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/app/core/test_elo.py", "content": "import numpy as np\nimport pytest\n\nfrom app.core.elo import elo_delta\nfrom tests.utils import RATING_DELTA_PRECISION, read_data_contest_prediction_first\n\n\n@pytest.fixture\ndef data_contest_prediction_first():\n    return read_data_contest_prediction_first()\n\n\ndef test_elo_delta(data_contest_prediction_first):\n    \"\"\"\n    Test function for the elo_delta function.\n\n     Raises:\n         AssertionError: If not all errors are within the specified precision.\n    \"\"\"\n\n    ks, ranks, old_ratings, new_ratings = data_contest_prediction_first\n\n    delta_ratings = elo_delta(ranks, old_ratings, ks)\n    testing_new_ratings = old_ratings + delta_ratings\n\n    errors = np.abs(new_ratings - testing_new_ratings)\n    assert np.all(\n        errors < RATING_DELTA_PRECISION\n    ), f\"Elo delta test failed. Some errors are not within {RATING_DELTA_PRECISION=}.\"\n"}
{"type": "test_file", "path": "tests/app/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/utils.py", "content": "from typing import Final\n\nimport numpy as np\n\n# Ensure that the prediction error for rating deltas for EACH participant is within the specified precision limit\nRATING_DELTA_PRECISION: Final[float] = 0.05\n\n\ndef read_data_contest_prediction_first():\n    with open(\"tests/tests_data/contest_prediction_1.npy\", \"rb\") as f:\n        data = np.load(f)\n        ks = data[:, 0]\n        ranks = data[:, 1]\n        old_ratings = data[:, 2]\n        new_ratings = data[:, 3]\n        return ks, ranks, old_ratings, new_ratings\n"}
{"type": "test_file", "path": "tests/app/core/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/app/core/test_fft.py", "content": "import numpy as np\nimport pytest\n\nfrom app.core.fft import fft_delta\nfrom tests.utils import RATING_DELTA_PRECISION, read_data_contest_prediction_first\n\n\n@pytest.fixture\ndef data_contest_prediction_first():\n    return read_data_contest_prediction_first()\n\n\ndef test_fft_delta(data_contest_prediction_first):\n    \"\"\"\n    Test function for the fft_delta function.\n\n    Raises:\n        AssertionError: If not all errors are within the specified precision.\n    \"\"\"\n\n    ks, ranks, old_ratings, new_ratings = data_contest_prediction_first\n\n    delta_ratings = fft_delta(ranks, old_ratings, ks)\n    testing_new_ratings = old_ratings + delta_ratings\n\n    errors = np.abs(new_ratings - testing_new_ratings)\n    assert np.all(\n        errors < RATING_DELTA_PRECISION\n    ), f\"FFT delta test failed. Some errors are not within {RATING_DELTA_PRECISION=}.\"\n"}
{"type": "source_file", "path": "app/utils.py", "content": "import asyncio\nimport math\nimport sys\nfrom asyncio import iscoroutinefunction\nfrom datetime import datetime, timedelta\nfrom functools import partial, wraps\nfrom typing import Any, Callable, Coroutine, List, Sequence, Union\n\nfrom loguru import logger\n\nfrom app.constants import BIWEEKLY_CONTEST_BASE, WEEKLY_CONTEST_BASE\n\n\nasync def gather_with_limited_concurrency(\n    crts: Sequence[Coroutine], max_con_num: int = 10, return_exceptions: bool = False\n) -> List[Union[Exception, Any]]:\n    \"\"\"\n    limit the concurrent number of tasks in `asyncio.gather`\n    by using `asyncio.Semaphore`\n    :param crts: coroutines\n    :param max_con_num: Maximum number of concurrent tasks\n    :param return_exceptions: Whether to return exceptions in the result\n    :return: List of results or exceptions\n    \"\"\"\n\n    async def crt_with_semaphore(crt: Coroutine):\n        async with semaphore:\n            return await crt\n\n    semaphore = asyncio.Semaphore(max_con_num)\n    tasks = [crt_with_semaphore(crt) for crt in crts]\n    return await asyncio.gather(*tasks, return_exceptions=return_exceptions)\n\n\ndef get_passed_weeks(t: datetime, base_t: datetime) -> int:\n    \"\"\"\n    Calculate how many weeks passed from base_t to t\n    :param t:\n    :param base_t:\n    :return:\n    \"\"\"\n    return math.floor((t - base_t).total_seconds() / (7 * 24 * 60 * 60))\n\n\ndef get_contest_start_time(contest_name: str) -> datetime:\n    \"\"\"\n    It's a simple, bold, but EFFECTIVE conjecture here, take two baselines separately,\n    then from the expected `contest_name` calculate its start time, just let it run on server periodically, no bother.\n    It's just unnecessary to use dynamic configuration things instead.\n    This conjecture worked precisely in the past, hopefully will still work well in the future.\n    :param contest_name:\n    :return:\n    \"\"\"\n    contest_num = int(contest_name.split(\"-\")[-1])\n    if contest_name.lower().startswith(\"weekly\"):\n        start_time = WEEKLY_CONTEST_BASE.dt + timedelta(\n            weeks=contest_num - WEEKLY_CONTEST_BASE.num\n        )\n    else:\n        start_time = BIWEEKLY_CONTEST_BASE.dt + timedelta(\n            weeks=(contest_num - BIWEEKLY_CONTEST_BASE.num) * 2\n        )\n    logger.info(f\"{contest_name=} {start_time=}\")\n    return start_time\n\n\ndef start_loguru(process: str = \"main\") -> None:\n    \"\"\"\n    error-prone warning: misuse process parameter (for example, use main in fastapi process\n    or reassign a different value) will mess up logs.\n    TODO: could set a global singleton variable to make sure this function will only be called once in a single process.\n    :param process: \"main\" for main.py backend process, \"api\" for fastapi http-server process.\n    :return:\n    \"\"\"\n    from app.config import get_yaml_config\n\n    try:\n        loguru_config = get_yaml_config().get(\"loguru\").get(process)\n        logger.add(\n            sink=loguru_config[\"sink\"],\n            rotation=loguru_config[\"rotation\"],\n            level=loguru_config[\"level\"],\n        )\n    except Exception as e:\n        logger.exception(\n            f\"Failed to start loguru, check loguru config in config.yaml file. error={e}\"\n        )\n        sys.exit(1)\n\n\ndef exception_logger(func: Callable[..., Any], reraise: bool) -> Callable[..., Any]:\n    \"\"\"\n    A decorator to write logs and try-catch for the key functions you want to keep eyes on.\n    :param func:\n    :param reraise:\n    :return:\n    \"\"\"\n\n    @wraps(func)\n    async def async_wrapper(*args, **kwargs):\n        try:\n            logger.info(f\"{func.__name__} is about to run.\")\n            res = await func(*args, **kwargs)\n            logger.success(f\"{func.__name__} is finished.\")\n            return res\n        except Exception as e:\n            logger.exception(f\"{func.__name__} error={e}.\")\n            if reraise:\n                raise e\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            logger.info(f\"{func.__name__} is about to run.\")\n            res = func(*args, **kwargs)\n            logger.success(f\"{func.__name__} is finished.\")\n            return res\n        except Exception as e:\n            logger.exception(f\"{func.__name__} args={args} kwargs={kwargs} error={e}.\")\n            if reraise:\n                raise e\n\n    return async_wrapper if iscoroutinefunction(func) else wrapper\n\n\nexception_logger_reraise = partial(exception_logger, reraise=True)\nexception_logger_silence = partial(exception_logger, reraise=False)\n"}
{"type": "source_file", "path": "main.py", "content": "import asyncio\n\nfrom loguru import logger\n\nfrom app.db.mongodb import start_async_mongodb\nfrom app.schedulers import start_scheduler\nfrom app.utils import start_loguru\n\n\nasync def start() -> None:\n    start_loguru()\n    await start_async_mongodb()\n    await start_scheduler()\n    logger.success(\"started all entry functions\")\n\n\nif __name__ == \"__main__\":\n    loop = asyncio.new_event_loop()\n    loop.create_task(start())\n    try:\n        loop.run_forever()\n    except (KeyboardInterrupt, SystemExit) as e:\n        logger.critical(f\"Closing loop. {e=}\")\n    finally:\n        loop.close()\n        logger.critical(\"Closed loop.\")\n"}
{"type": "source_file", "path": "api/utils.py", "content": "from fastapi import HTTPException\nfrom loguru import logger\n\nfrom app.db.models import Contest\n\n\nasync def check_contest_name(contest_name: str) -> None:\n    \"\"\"\n    Check whether a contest_name is valid.\n    - Valid: silently passed\n    - Invalid: just raise HTTPException (fastapi will return error msg gracefully)\n    :param contest_name:\n    :return:\n    \"\"\"\n    contest = await Contest.find_one(Contest.titleSlug == contest_name)\n    if not contest:\n        msg = f\"contest not found for {contest_name=}\"\n        logger.error(msg)\n        raise HTTPException(status_code=400, detail=msg)\n"}
{"type": "source_file", "path": "api/deprecated/ssr.py", "content": "import asyncio\nimport math\nfrom datetime import datetime\nfrom typing import Final, List, Literal, Optional\n\nfrom fastapi import Body, FastAPI, Form, HTTPException, Request\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi.templating import Jinja2Templates\nfrom loguru import logger\nfrom pydantic import BaseModel\n\nfrom app.db.models import Contest, ContestRecordArchive, ContestRecordPredict, Question\nfrom app.db.mongodb import start_async_mongodb\nfrom app.utils import start_loguru\n\n\nclass KeyUniqueContestRecord(BaseModel):\n    contest_name: str\n    username: str\n    data_region: str\n\n\napp = FastAPI()\napp.mount(\"/static\", StaticFiles(directory=\"api/deprecated/static\"), name=\"static\")\ntemplates = Jinja2Templates(directory=\"api/deprecated/templates\")\n\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    start_loguru(process=\"api\")\n    await start_async_mongodb()\n\n\n@app.get(\"/\", response_class=HTMLResponse)\nasync def index_page_get(\n    request: Request,\n):\n    logger.info(f\"index_page_get {request.client=}\")\n    predict_contests = (\n        #  Contest.predict_time != None,  # beanie does not support `is not None` here.\n        await Contest.find(\n            Contest.predict_time > datetime(2000, 1, 1),\n        )\n        .sort(-Contest.startTime)\n        .to_list()\n    )\n    logger.trace(f\"{predict_contests=}\")\n    return templates.TemplateResponse(\n        \"index.html\",\n        {\n            \"request\": request,\n            \"predict_contests\": predict_contests,\n        },\n    )\n\n\n@app.get(\"/{contest_name}/{page}\", response_class=HTMLResponse)\nasync def contest_page_get(\n    request: Request,\n    contest_name: str,\n    page: int = 1,\n):\n    logger.info(f\"{request.client=} {contest_name=}, {page=}\")\n    total_num = await ContestRecordPredict.find(\n        ContestRecordPredict.contest_name == contest_name,\n        ContestRecordPredict.score != 0,\n    ).count()\n    max_page = math.ceil(total_num / 25)\n    pagination_list = [i for i in range(page - 4, page + 5) if 1 <= i <= max_page]\n    records = (\n        await ContestRecordPredict.find(\n            ContestRecordPredict.contest_name == contest_name,\n            ContestRecordPredict.score != 0,\n        )\n        .sort(ContestRecordPredict.rank)\n        .skip(25 * (page - 1))\n        .limit(25)\n        .to_list()\n    )\n    return templates.TemplateResponse(\n        \"contest.html\",\n        {\n            \"request\": request,\n            \"contest_name\": contest_name,\n            \"user_list\": records,\n            \"current_page\": page,\n            \"max_page\": max_page,\n            \"pagination_list\": pagination_list,\n        },\n    )\n\n\n@app.post(\"/{contest_name}/query_user\", response_class=HTMLResponse)\nasync def contest_user_post(\n    request: Request,\n    contest_name: str,\n    username: Optional[str] = Form(None),\n):\n    logger.info(f\"{request.client=}, {contest_name=}, {username=}\")\n    record = await ContestRecordPredict.find_one(\n        ContestRecordPredict.contest_name == contest_name,\n        ContestRecordPredict.username == username,\n        ContestRecordPredict.score != 0,\n    )\n    return templates.TemplateResponse(\n        \"contest.html\",\n        {\n            \"request\": request,\n            \"contest_name\": contest_name,\n            \"user_list\": [record] if record else [],\n            \"current_page\": None,\n        },\n    )\n\n\n@app.post(\"/user_rank_list\")\nasync def contest_user_rank_list(\n    request: Request,\n    unique_contest_record: KeyUniqueContestRecord,\n):\n    logger.info(f\"{request.client=} {unique_contest_record=}\")\n    contest = await Contest.find_one(\n        Contest.titleSlug == unique_contest_record.contest_name\n    )\n    if not contest:\n        logger.error(f\"contest not found for {unique_contest_record=}\")\n        return {}\n    start_time = contest.startTime\n    record = await ContestRecordArchive.find_one(\n        ContestRecordArchive.contest_name == unique_contest_record.contest_name,\n        ContestRecordArchive.username == unique_contest_record.username,\n        ContestRecordArchive.data_region == unique_contest_record.data_region,\n    )\n    if not record:\n        logger.error(f\"user contest record not found for {unique_contest_record=}\")\n    data = [[\"Minute\", \"User\", \"Rank\"],] + [\n        [minute + 1, unique_contest_record.username, x]\n        for minute, x in enumerate(\n            record.real_time_rank if record and record.real_time_rank else []\n        )\n    ]\n    logger.trace(f\"{unique_contest_record=} {data=}\")\n    return {\n        \"real_time_rank\": data,\n        \"start_time\": start_time,\n    }\n\n\n@app.post(\"/questions_finished_list\")\nasync def contest_questions_finished_list(\n    request: Request,\n    contest_name: str = Body(embed=True),\n):\n    logger.info(f\"{request.client=} {contest_name=}\")\n    data = [[\"Minute\", \"Question\", \"Count\"]]\n    contest = await Contest.find_one(\n        Contest.titleSlug == contest_name,\n    )\n    if not contest:\n        logger.error(f\"contest not found for {contest_name=}\")\n        return {}\n    questions = await Question.find(Question.contest_name == contest_name).to_list()\n    if not questions:\n        logger.error(f\"{questions=}, no data now\")\n        return {\"real_time_count\": data}\n    questions.sort(key=lambda q: q.credit)\n    logger.trace(f\"{questions=}\")\n    for i, question in enumerate(questions):\n        data.extend(\n            [\n                [minute + 1, f\"Q{i+1}\", count]\n                for minute, count in enumerate(question.real_time_count)\n            ]\n        )\n    logger.trace(f\"{contest_name=} {data=}\")\n    return {\"real_time_count\": data}\n\n\nDATA_REGION = Literal[\"CN\", \"US\"]\n\n\nclass UniqueUser(BaseModel):\n    username: str\n    data_region: DATA_REGION\n\n\nclass QueryPredictedRecords(BaseModel):\n    contest_name: str\n    users: List[UniqueUser]\n\n\nclass ProjectionPredictedResult(BaseModel):\n    old_rating: Optional[float] = None\n    new_rating: Optional[float] = None\n    delta_rating: Optional[float] = None\n\n\n@app.post(\"/predict_records\")\nasync def contest_predict_records(\n    request: Request,\n    query: QueryPredictedRecords,\n):\n    \"\"\"\n    Query multiple predicted records in a contest.\n    :param request:\n    :param query:\n    :return:\n    \"\"\"\n    logger.info(f\"{request.client=} {query=}\")\n    MAX_USERS: Final[int] = 26\n    contest = await Contest.find_one(Contest.titleSlug == query.contest_name)\n    if not contest:\n        logger.error(f\"contest not found for {query.contest_name=}\")\n        raise HTTPException(\n            status_code=400, detail=f\"contest not found for {query.contest_name=}\"\n        )\n    if (users_count := len(query.users)) > MAX_USERS:\n        logger.error(f\"{users_count=} per request, denied.\")\n        raise HTTPException(\n            status_code=400,\n            detail=f\"request denied because {users_count=}, which is bigger than maximum value={MAX_USERS}\",\n        )\n    tasks = (\n        ContestRecordPredict.find_one(\n            ContestRecordPredict.contest_name == query.contest_name,\n            ContestRecordPredict.data_region == user.data_region,\n            ContestRecordPredict.username == user.username,\n            projection_model=ProjectionPredictedResult,\n        )\n        for user in query.users\n    )\n    return await asyncio.gather(*tasks)\n"}
{"type": "source_file", "path": "api/entry.py", "content": "import time\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom loguru import logger\n\nfrom app.config import get_yaml_config\nfrom app.db.mongodb import start_async_mongodb\nfrom app.utils import start_loguru\n\nfrom .routers import contest_records, contests, questions\n\napp = FastAPI()\nyaml_config = get_yaml_config().get(\"fastapi\")\n\n\napp.include_router(contests.router, prefix=\"/api/v1\")\napp.include_router(contest_records.router, prefix=\"/api/v1\")\napp.include_router(questions.router, prefix=\"/api/v1\")\n\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    start_loguru(process=\"api\")\n    await start_async_mongodb()\n\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=yaml_config.get(\"CORS_allow_origins\"),\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.middleware(\"http\")\nasync def log_requests(request: Request, call_next):\n    t1 = time.time()\n    response = await call_next(request)\n    t2 = time.time()\n    logger.info(\n        f\"Received request: {request.client.host} {request.method} {request.url.path} \"\n        f\"Cost {(t2 - t1) * 1e3:.2f} ms {response.status_code=}\"\n    )\n    return response\n"}
{"type": "source_file", "path": "app/core/predictor.py", "content": "from datetime import datetime\nfrom typing import List\n\nimport numpy as np\nfrom beanie.odm.operators.update.general import Set\nfrom loguru import logger\n\nfrom app.core.elo import elo_delta\nfrom app.db.models import Contest, ContestRecordPredict, User\nfrom app.utils import exception_logger_reraise, gather_with_limited_concurrency\n\n\nasync def update_rating_immediately(\n    records: List[ContestRecordPredict],\n) -> None:\n    \"\"\"\n    Update users' rating and attendedContestsCount (if it's biweekly contest)\n    :param records:\n    :return:\n    \"\"\"\n    logger.info(\"immediately write predicted result back into User collection\")\n    tasks = [\n        User.find_one(\n            User.username == record.username,\n            User.data_region == record.data_region,\n        ).update(\n            Set(\n                {\n                    User.rating: record.new_rating,\n                    User.attendedContestsCount: record.attendedContestsCount + 1,\n                    User.update_time: datetime.utcnow(),\n                }\n            )\n        )\n        for record in records\n    ]\n    await gather_with_limited_concurrency(tasks, max_con_num=50)\n    logger.success(\"finished updating User using predicted result\")\n\n\n@exception_logger_reraise\nasync def predict_contest(\n    contest_name: str,\n) -> None:\n    \"\"\"\n    Core predict function using official elo rating algorithm\n    :param contest_name:\n    :return:\n    \"\"\"\n    records = (\n        await ContestRecordPredict.find(\n            ContestRecordPredict.contest_name == contest_name,\n            ContestRecordPredict.score != 0,\n        )\n        .sort(ContestRecordPredict.rank)\n        .to_list()\n    )\n\n    rank_array = np.array([record.rank for record in records])\n    rating_array = np.array([record.old_rating for record in records])\n    k_array = np.array([record.attendedContestsCount for record in records])\n    # core prediction\n    delta_rating_array = elo_delta(rank_array, rating_array, k_array)\n    new_rating_array = rating_array + delta_rating_array\n\n    # update ContestRecordPredict collection\n    predict_time = datetime.utcnow()\n    for i, record in enumerate(records):\n        record.delta_rating = delta_rating_array[i]\n        record.new_rating = new_rating_array[i]\n        record.predict_time = predict_time\n    tasks = [record.save() for record in records]\n    await gather_with_limited_concurrency(tasks, max_con_num=50)\n    logger.success(\"predict_contest finished updating ContestRecordPredict\")\n\n    if contest_name.lower().startswith(\"bi\"):\n        # for biweekly contests only, because next day's weekly contest needs the latest rating\n        await update_rating_immediately(records)\n\n    # update Contest collection to indicate that this contest has been predicted.\n    # by design, predictions should only be run once.\n    await Contest.find_one(Contest.titleSlug == contest_name).update(\n        Set(\n            {\n                Contest.predict_time: datetime.utcnow(),\n            }\n        )\n    )\n    logger.info(\"finished updating predict_time in Contest database\")\n"}
{"type": "source_file", "path": "app/crawler/user.py", "content": "from typing import Tuple\n\nfrom app.crawler.utils import multi_http_request\nfrom app.db.models import DATA_REGION\n\n\nasync def request_user_rating_and_attended_contests_count(\n    data_region: DATA_REGION,\n    username: str,\n) -> Tuple[float | None, int | None]:\n    \"\"\"\n    request user's rating, attended contests count\n    :param data_region:\n    :param username:\n    :return:\n    \"\"\"\n    if data_region == \"CN\":\n        req = (\n            await multi_http_request(\n                {\n                    (data_region, username): {\n                        \"url\": \"https://leetcode.cn/graphql/noj-go/\",\n                        \"method\": \"POST\",\n                        \"json\": {\n                            \"query\": \"\"\"\n                                 query userContestRankingInfo($userSlug: String!) {\n                                        userContestRanking(userSlug: $userSlug) {\n                                            attendedContestsCount\n                                            rating\n                                        }\n                                    }\n                                 \"\"\",\n                            \"variables\": {\"userSlug\": username},\n                        },\n                    }\n                }\n            )\n        )[0]\n    else:\n        req = (\n            await multi_http_request(\n                {\n                    (data_region, username): {\n                        \"url\": \"https://leetcode.com/graphql/\",\n                        \"method\": \"POST\",\n                        \"json\": {\n                            \"query\": \"\"\"\n                                 query getContestRankingData($username: String!) {\n                                    userContestRanking(username: $username) {\n                                        attendedContestsCount\n                                        rating\n                                    }\n                                 }\n                                 \"\"\",\n                            \"variables\": {\"username\": username},\n                        },\n                    }\n                }\n            )\n        )[0]\n    if req is None:\n        raise RuntimeError(f\"HTTP request failed for {data_region=} {username=}\")\n    if (graphql_res := req.json().get(\"data\", {}).get(\"userContestRanking\")) is None:\n        # Watch out: `None` means that it cannot request information about this user, it should be a new user.\n        return None, None\n    else:\n        return graphql_res.get(\"rating\"), graphql_res.get(\"attendedContestsCount\")\n"}
{"type": "source_file", "path": "app/constants.py", "content": "from datetime import datetime\nfrom typing import Final, NamedTuple\n\nDEFAULT_NEW_USER_ATTENDED_CONTESTS_COUNT: Final[int] = 0\nDEFAULT_NEW_USER_RATING: Final[float] = 1500.0\n\n\nclass CronTimePointWkdHrMin(NamedTuple):\n    weekday: int\n    hour: int\n    minute: int\n\n\n# Observed that leetcode would update more user's result within 10 minutes after ending,\n# so safely set minute to 15 instead 0 in order to wait for final result.\nWEEKLY_CONTEST_START = CronTimePointWkdHrMin(\n    6,  # Sunday\n    2,  # hour\n    30,  # minute\n)\nBIWEEKLY_CONTEST_START = CronTimePointWkdHrMin(\n    5,  # Saturday\n    14,  # hour\n    30,  # minute\n)\n\n\nclass SingleContestDatetime(NamedTuple):\n    num: int\n    dt: datetime\n\n\n# Take \"weekly-contest-294\" and \"biweekly-contest-78\" as two baselines\nWEEKLY_CONTEST_BASE = SingleContestDatetime(\n    294,\n    datetime(2022, 5, 22, 2, 30),\n)\nBIWEEKLY_CONTEST_BASE = SingleContestDatetime(\n    78,\n    datetime(2022, 5, 14, 14, 30),\n)\n"}
{"type": "source_file", "path": "api/deprecated/__init__.py", "content": ""}
{"type": "source_file", "path": "app/crawler/utils.py", "content": "import asyncio\nfrom collections import defaultdict, deque\nfrom typing import Any, Dict, List, Optional\n\nimport httpx\nfrom loguru import logger\n\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X x.y; rv:42.0) Gecko/20100101 Firefox/42.0\",\n}\n\n\nasync def multi_http_request(\n    multi_requests: Dict,\n    concurrent_num: int = 5,\n    retry_num: int = 10,\n) -> List[Optional[httpx.Response]]:\n    \"\"\"\n    Simple HTTP requests queue with speed control and retry automatically, hopefully can get corresponding response.\n    Failed response would be `None` but not a `response` object, so invokers MUST verify for None values.\n    Notice that `multi_requests` is `Dict` but not `Sequence` so that data accessing would be easier.\n    Because all the stuff are in memory, so DO NOT pass a long `multi_requests` in especially when `response` is huge.\n    :param multi_requests:\n    :param concurrent_num:\n    :param retry_num:\n    :return:\n    \"\"\"\n    # response_mapper value means: [int: retried times / Response: successful result]\n    response_mapper: Dict[Any, int | httpx.Response] = defaultdict(int)\n    crawler_queue = deque(multi_requests.items())\n    total_num = len(crawler_queue)\n    # gradually adjust wait_time by detect number of failed requests in the last round.\n    wait_time = 0\n    while crawler_queue:\n        requests_list = list()\n        # gradually increase wait_time according to max retry times\n        # wait_time = response_mapper[job_queue[-1][0]]\n        while len(requests_list) < concurrent_num and crawler_queue:\n            key, request = crawler_queue.popleft()\n            if response_mapper[key] >= retry_num:\n                logger.error(\n                    f\"request reached max retry_num. {key=}, req={multi_requests[key]}\"\n                )\n                continue\n            requests_list.append((key, request))\n        if not requests_list:\n            break\n        logger.info(\n            f\"remaining={len(crawler_queue) / total_num * 100 :.2f}% wait_time={wait_time} \"\n            f\"requests_list={[(key, response_mapper[key]) for key, request in requests_list]}\"\n        )\n        await asyncio.sleep(wait_time)\n        async with httpx.AsyncClient(headers=headers) as client:\n            tasks = [client.request(**request) for key, request in requests_list]\n            response_list = await asyncio.gather(*tasks, return_exceptions=True)\n            wait_time = 0\n            for response, (key, request) in zip(response_list, requests_list):\n                if isinstance(response, httpx.Response) and response.status_code == 200:\n                    # TODO: Very high memory usage here when saving response directly, say, if run 20000 requests.\n                    response_mapper[key] = response\n                else:\n                    # response could be an Exception here\n                    logger.warning(\n                        f\"multi_http_request error: {request=} \"\n                        f\"response.status_code: \"\n                        f\"{response.status_code if isinstance(response, httpx.Response) else response}\"\n                    )\n                    response_mapper[key] += 1\n                    wait_time += 1\n                    crawler_queue.append((key, request))\n    return [\n        None if isinstance(response, int) else response\n        for key, response in response_mapper.items()\n    ]\n"}
{"type": "source_file", "path": "api/routers/__init__.py", "content": ""}
{"type": "source_file", "path": "app/crawler/question.py", "content": "from typing import Dict, List, Optional\n\nfrom app.crawler.utils import multi_http_request\nfrom app.db.models import DATA_REGION\n\n\nasync def request_question_list(\n    contest_name: str,\n    data_region: DATA_REGION = \"CN\",\n) -> Optional[List[Dict]]:\n    \"\"\"\n    Send HTTP request to get questions data of a given contest\n    :param contest_name:\n    :param data_region:\n    :return:\n    \"\"\"\n    if data_region == \"US\":\n        url = f\"https://leetcode.com/contest/api/info/{contest_name}/\"\n    elif data_region == \"CN\":\n        url = f\"https://leetcode.cn/contest/api/info/{contest_name}/\"\n    else:\n        raise ValueError(f\"{data_region=}\")\n    data = (\n        await multi_http_request(\n            {\n                \"req\": {\n                    \"url\": url,\n                    \"method\": \"GET\",\n                }\n            }\n        )\n    )[0].json()\n    question_list = data.get(\"questions\")\n    if data_region == \"CN\":\n        for question in question_list:\n            question[\"title\"] = question[\"english_title\"]\n    return question_list\n"}
{"type": "source_file", "path": "app/core/fft.py", "content": "from typing import Final\n\nimport numpy as np\nfrom scipy.signal import fftconvolve\n\nfrom app.core.elo import delta_coefficients\n\nEXPAND_SIZE: Final[int] = 100\nMAX_RATING: Final[int] = 4000 * EXPAND_SIZE\n\n\ndef pre_calc_convolution(old_rating: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Pre-calculate convolution values for the Elo rating update.\n    :param old_rating:\n    :return:\n    \"\"\"\n    f = 1 / (\n        1 + np.power(10, np.arange(-MAX_RATING, MAX_RATING + 1) / (400 * EXPAND_SIZE))\n    )\n    g = np.bincount(np.round(old_rating * EXPAND_SIZE).astype(int))\n    convolution = fftconvolve(f, g, mode=\"full\")\n    convolution = convolution[: 2 * MAX_RATING + 1]\n    return convolution\n\n\ndef get_expected_rank(convolution: np.ndarray, x: int) -> float:\n    \"\"\"\n    Get the expected rank based on pre-calculated convolution values.\n    :param convolution:\n    :param x:\n    :return:\n    \"\"\"\n    return convolution[x + MAX_RATING] + 0.5\n\n\ndef get_equation_left(convolution: np.ndarray, x: int) -> float:\n    \"\"\"\n    Get the left side of equation for expected rating based on pre-calculated convolution values\n    :param convolution:\n    :param x:\n    :return:\n    \"\"\"\n    return convolution[x + MAX_RATING] + 1\n\n\ndef binary_search_expected_rating(convolution: np.ndarray, mean_rank: float) -> int:\n    \"\"\"\n    Perform binary search to find the expected rating for a given mean rank.\n    :param convolution:\n    :param mean_rank:\n    :return:\n    \"\"\"\n    lo, hi = 0, MAX_RATING\n    while lo < hi:\n        mid = (lo + hi) // 2\n        if get_equation_left(convolution, mid) < mean_rank:\n            hi = mid\n        else:\n            lo = mid + 1\n    return mid\n\n\ndef get_expected_rating(rank: int, rating: float, convolution: np.ndarray) -> float:\n    \"\"\"\n    Calculate the expected rating based on current rank, rating, and pre-calculated convolution.\n    :param rank:\n    :param rating:\n    :param convolution:\n    :return:\n    \"\"\"\n    expected_rank = get_expected_rank(convolution, round(rating * EXPAND_SIZE))\n    mean_rank = np.sqrt(expected_rank * rank)\n    return binary_search_expected_rating(convolution, mean_rank) / EXPAND_SIZE\n\n\ndef fft_delta(ranks: np.ndarray, ratings: np.ndarray, ks: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculate Elo rating changes using Fast Fourier Transform (FFT)\n    :param ranks:\n    :param ratings:\n    :param ks:\n    :return:\n    \"\"\"\n    convolution = pre_calc_convolution(ratings)\n    expected_ratings = list()\n    for i in range(len(ranks)):\n        rank = ranks[i]\n        rating = ratings[i]\n        expected_ratings.append(get_expected_rating(rank, rating, convolution))\n    delta_ratings = (np.array(expected_ratings) - ratings) * delta_coefficients(ks)\n    return delta_ratings\n"}
{"type": "source_file", "path": "api/routers/questions.py", "content": "import asyncio\nfrom typing import List, Optional\n\nfrom fastapi import APIRouter, HTTPException, Request\nfrom loguru import logger\nfrom pydantic import BaseModel, NonNegativeInt, conlist\n\nfrom api.utils import check_contest_name\nfrom app.db.models import Question\n\nrouter = APIRouter(\n    prefix=\"/questions\",\n    tags=[\"questions\"],\n)\n\n\nclass QueryOfQuestions(BaseModel):\n    contest_name: Optional[str] = None\n    question_id_list: Optional[\n        conlist(NonNegativeInt, min_length=1, max_length=4)\n    ] = None\n\n\n@router.post(\"/\")\nasync def questions(\n    request: Request,\n    query: QueryOfQuestions,\n) -> List[Question]:\n    \"\"\"\n    Query questions for a given contest.\n    Questions number must between 1 and 4 inclusively.\n    :param request:\n    :param query:\n    :return:\n    \"\"\"\n    if not (bool(query.contest_name) ^ bool(query.question_id_list)):\n        msg = \"contest_name OR question_id_list must be given!\"\n        logger.error(msg)\n        raise HTTPException(status_code=400, detail=msg)\n    # if `contest_name` is given, use it to query\n    if query.contest_name:\n        await check_contest_name(query.contest_name)\n        return await Question.find(\n            Question.contest_name == query.contest_name\n        ).to_list()\n    # or use `question_id_list` to query\n    else:\n        tasks = (\n            Question.find_one(Question.question_id == question_id)\n            for question_id in query.question_id_list\n        )\n        return await asyncio.gather(*tasks)\n    # notice that if both parameters are given, only use `contest_name`\n"}
{"type": "source_file", "path": "app/__init__.py", "content": ""}
{"type": "source_file", "path": "app/core/__init__.py", "content": ""}
{"type": "source_file", "path": "app/core/elo.py", "content": "from functools import lru_cache\nfrom typing import Final\n\nimport numpy as np\nfrom numba import jit\n\n\n@lru_cache\ndef pre_sum_of_sigma(k: int) -> float:\n    \"\"\"\n    Series cache\n    :param k:\n    :return:\n    \"\"\"\n    # if not isinstance(k, int):\n    #     raise TypeError(\"k must be an integer\")\n    if k < 0:\n        raise ValueError(f\"{k=}, pre_sum's index less than zero!\")\n    return (5 / 7) ** k + pre_sum_of_sigma(k - 1) if k >= 1 else 1\n\n\n@lru_cache\ndef adjustment_for_delta_coefficient(k: int) -> float:\n    \"\"\"\n    This function could also be `return 1 / (1 + sum((5 / 7) ** i for i in range(k + 1)))`\n    but use a `pre_sum_of_sigma` function(which is also cached) is faster.\n    When k is big enough, result approximately equals to 2/9.\n    :param k:\n    :return:\n    \"\"\"\n    return 1 / (1 + pre_sum_of_sigma(k)) if k <= 100 else 2 / 9\n\n\ndef delta_coefficients(ks: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculate delta coefficients for the given input array.\n    :param ks:\n    :return:\n    \"\"\"\n    vectorized_func = np.vectorize(adjustment_for_delta_coefficient)\n    return vectorized_func(ks)\n\n\n@jit(nopython=True, fastmath=True, parallel=True)\ndef expected_win_rate(vector: np.ndarray, scalar: float) -> np.ndarray:\n    \"\"\"\n    Calculate the expected win rate based on the Elo rating system.\n    Test result had shown this function has a quite decent performance.\n    :param vector:\n    :param scalar:\n    :return:\n    \"\"\"\n    return 1 / (1 + np.power(10, (scalar - vector) / 400))\n\n\n@jit(nopython=True, fastmath=True, parallel=True)\ndef binary_search_expected_rating(mean_rank: int, all_rating: np.ndarray) -> float:\n    \"\"\"\n    Perform binary search to find the rating corresponding to the given mean rank.\n    :param mean_rank:\n    :param all_rating:\n    :return:\n    \"\"\"\n    target = mean_rank - 1\n    lo, hi = 0, 4000\n    max_iteration = 25\n    precision: Final[float] = 0.01\n    while hi - lo > precision and max_iteration >= 0:\n        mid = lo + (hi - lo) / 2\n        if np.sum(expected_win_rate(all_rating, mid)) < target:\n            hi = mid\n        else:\n            lo = mid\n        max_iteration -= 1\n    return mid\n\n\n@jit(nopython=True, fastmath=True, parallel=True)\ndef get_expected_rating(rank: int, rating: float, all_rating: np.ndarray) -> float:\n    \"\"\"\n    Calculate the expected rating based on the given rank, player rating, and array of all ratings.\n    :param rank:\n    :param rating:\n    :param all_rating:\n    :return:\n    \"\"\"\n    expected_rank = np.sum(expected_win_rate(all_rating, rating)) + 0.5\n    mean_rank = np.sqrt(expected_rank * rank)\n    return binary_search_expected_rating(mean_rank, all_rating)\n\n\ndef elo_delta(ranks: np.ndarray, ratings: np.ndarray, ks: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculate the Elo rating changes (delta) based on the given ranks, current ratings, and coefficients.\n    :param ranks:\n    :param ratings:\n    :param ks:\n    :return:\n    \"\"\"\n    expected_ratings = list()\n    for i in range(len(ranks)):\n        rank = ranks[i]\n        rating = ratings[i]\n        expected_ratings.append(get_expected_rating(rank, rating, ratings))\n    delta_ratings = (np.array(expected_ratings) - ratings) * delta_coefficients(ks)\n    return delta_ratings\n"}
{"type": "source_file", "path": "app/config.py", "content": "import yaml\n\nyaml_config = None\n\n\ndef get_yaml_config():\n    \"\"\"\n    Parse `config.yaml`\n    :return:\n    \"\"\"\n    global yaml_config\n    if yaml_config is None:\n        with open(\"config.yaml\", \"r\") as yaml_file:\n            yaml_config = yaml.safe_load(yaml_file)\n    return yaml_config\n"}
{"type": "source_file", "path": "api/__init__.py", "content": ""}
{"type": "source_file", "path": "app/db/components.py", "content": "from datetime import datetime\nfrom typing import List, Literal, Optional\n\nfrom pydantic import BaseModel, Field\n\n\nclass PredictionEvent(BaseModel):\n    name: str\n    description: Optional[str] = None\n    timestamp: datetime = Field(default_factory=datetime.utcnow)\n    status: Literal[\"Ongoing\", \"Passed\", \"Failed\"] = \"Ongoing\"\n\n\nclass UserContestHistoryRecord(BaseModel):\n    contest_title: str\n    finishTimeInSeconds: int\n    # Actually, `rating` here is `new_rating` in ContestRecord\n    rating: float\n    ranking: int\n    solved_questions_id: Optional[List[int]] = None\n"}
{"type": "source_file", "path": "app/crawler/__init__.py", "content": ""}
{"type": "source_file", "path": "app/handler/question.py", "content": "import asyncio\nfrom datetime import datetime, timedelta\nfrom typing import List\n\nfrom beanie.odm.operators.update.general import Set\nfrom loguru import logger\n\nfrom app.crawler.question import request_question_list\nfrom app.db.models import Question, Submission\nfrom app.utils import gather_with_limited_concurrency, get_contest_start_time\n\n\nasync def real_time_count_at_time_point(\n    contest_name: str,\n    question_id: int,\n    time_point: datetime,\n) -> int:\n    \"\"\"\n    For a single question, count its finished submission at a given time point.\n    :param contest_name:\n    :param question_id:\n    :param time_point:\n    :return:\n    \"\"\"\n    return await Submission.find(\n        Submission.contest_name == contest_name,\n        Submission.question_id == question_id,\n        Submission.date <= time_point,\n    ).count()\n\n\nasync def save_questions_real_time_count(\n    contest_name: str,\n    delta_minutes: int = 1,\n) -> None:\n    \"\"\"\n    For every delta_minutes, count accepted submissions for each question.\n    :param contest_name:\n    :param delta_minutes:\n    :return:\n    \"\"\"\n    time_series = list()\n    start_time = get_contest_start_time(contest_name)\n    end_time = start_time + timedelta(minutes=90)\n    while (start_time := start_time + timedelta(minutes=delta_minutes)) <= end_time:\n        time_series.append(start_time)\n    logger.info(f\"{contest_name=} {time_series=}\")\n    questions = await Question.find(\n        Question.contest_name == contest_name,\n    ).to_list()\n    for question in questions:\n        tasks = [\n            real_time_count_at_time_point(\n                contest_name, question.question_id, time_point\n            )\n            for time_point in time_series\n        ]\n        question.real_time_count = await gather_with_limited_concurrency(tasks)\n        await question.save()\n    logger.success(\"finished\")\n\n\nasync def save_questions(\n    contest_name: str,\n) -> List[Question]:\n    \"\"\"\n    For the past contests, fetch questions list and fill into MongoDB\n    :param contest_name:\n    :return:\n    \"\"\"\n    try:\n        question_list = await request_question_list(contest_name)\n        time_point = datetime.utcnow()\n        additional_fields = {\n            \"contest_name\": contest_name,\n        }\n        questions = list()\n        for idx, question in enumerate(question_list):\n            question.pop(\"id\")\n            question.update({\"qi\": idx + 1})\n            questions.append(Question.model_validate(question | additional_fields))\n        tasks = (\n            Question.find_one(\n                Question.question_id == question_obj.question_id,\n                Question.contest_name == contest_name,\n            ).upsert(\n                Set(\n                    {\n                        Question.credit: question_obj.credit,\n                        Question.title: question_obj.title,\n                        Question.title_slug: question_obj.title_slug,\n                        Question.update_time: question_obj.update_time,\n                        Question.qi: question_obj.qi,\n                    }\n                ),\n                on_insert=question_obj,\n            )\n            for question_obj in questions\n        )\n        await asyncio.gather(*tasks)\n        # Old questions may change, could delete here.\n        await Question.find(\n            Question.contest_name == contest_name,\n            Question.update_time < time_point,\n        ).delete()\n        logger.success(\"finished\")\n        return questions\n    except Exception as e:\n        logger.error(f\"failed to fill questions fields for {contest_name=} {e=}\")\n"}
{"type": "source_file", "path": "app/db/models.py", "content": "from datetime import datetime\nfrom typing import Counter, List, Literal, Optional, Tuple\n\nfrom beanie import Document\nfrom pydantic import Field\nfrom pymongo import IndexModel\n\nfrom app.db.components import PredictionEvent, UserContestHistoryRecord\n\nDATA_REGION = Literal[\"CN\", \"US\"]\n\n\nclass Contest(Document):\n    titleSlug: str\n    title: str\n    startTime: datetime\n    duration: int\n    endTime: datetime\n    past: bool\n    update_time: datetime = Field(default_factory=datetime.utcnow)\n    predict_time: Optional[datetime] = None\n    user_num_us: Optional[int] = None\n    user_num_cn: Optional[int] = None\n    convolution_array: Optional[int] = None\n    prediction_progress: Optional[List[PredictionEvent]] = None\n\n    class Settings:\n        indexes = [\n            IndexModel(\"titleSlug\", unique=True),\n            \"title\",\n            \"startTime\",\n            \"endTime\",\n            \"predict_time\",\n        ]\n\n\nclass ContestRecord(Document):\n    contest_name: str\n    contest_id: int\n    username: str\n    user_slug: str\n    data_region: DATA_REGION\n    country_code: Optional[str] = None\n    country_name: Optional[str] = None\n    rank: int\n    score: int\n    finish_time: datetime\n    attendedContestsCount: Optional[int] = None\n    old_rating: Optional[float] = None\n    new_rating: Optional[float] = None\n    delta_rating: Optional[float] = None\n\n    class Settings:\n        indexes = [\n            \"contest_name\",\n            \"username\",\n            \"user_slug\",\n            \"rank\",\n            \"data_region\",\n        ]\n\n\nclass ContestRecordPredict(ContestRecord):\n    # Predicted records' will be inserted only once, won't update any fields.\n    # Records in this collection can be used to calculated MSE directly even after a long time because it won't change.\n    insert_time: datetime = Field(default_factory=datetime.utcnow)\n    predict_time: Optional[datetime] = None\n\n\nclass ContestRecordArchive(ContestRecord):\n    # Archived records will be updated.\n    # LeetCode would rejudge some submissions(cheat detection, adding test cases, etc.)\n    update_time: datetime = Field(default_factory=datetime.utcnow)\n    real_time_rank: Optional[list] = None\n\n\nclass Question(Document):\n    question_id: int\n    credit: int\n    title: str\n    title_slug: str\n    update_time: datetime = Field(default_factory=datetime.utcnow)\n    contest_name: str\n    qi: int\n    real_time_count: Optional[List[int]] = None\n    # For every question, save the quantiles of users' ratings who passed that question.\n    user_ratings_quantiles: Optional[List[float]] = None\n    # For every question, save the rating bins for users who passed each question.\n    # Each tuple represents a rating range, and the second element is the count of users within that range.\n    # For example, if 888 users have ratings in the range `[1100, 1150)`, the tuple would be `(1100, 888)`.\n    # The default range is 50, maintaining consistency with the distribution chart on LeetCode user homepages.\n    user_ratings_bins: Optional[List[Tuple[int, int]]] = None\n    average_fail_count: Optional[int] = None\n    lang_counter: Optional[Counter] = None\n    difficulty: Optional[float] = None\n    # For every question, save the first 10 users who finished this question\n    first_ten_users: Optional[List[Tuple[str, datetime]]] = None\n    topics: Optional[List[str]] = None\n\n    class Settings:\n        indexes = [\n            \"question_id\",\n            \"title_slug\",\n            \"contest_name\",\n        ]\n\n\nclass Submission(Document):\n    # these four can be used as compound Index\n    contest_name: str\n    username: str\n    data_region: DATA_REGION\n    question_id: int\n    date: datetime\n    fail_count: int\n    credit: int\n    submission_id: int\n    status: int\n    contest_id: int\n    update_time: datetime = Field(default_factory=datetime.utcnow)\n    # watch out: US data_region doesn't have `lang` field before weekly-contest-364\n    lang: Optional[str] = None\n\n    class Settings:\n        indexes = [\n            \"contest_name\",\n            \"username\",\n            \"data_region\",\n            \"question_id\",\n            \"date\",\n        ]\n\n\nclass User(Document):\n    username: str\n    user_slug: str\n    data_region: DATA_REGION\n    attendedContestsCount: int\n    rating: float\n    update_time: datetime = Field(default_factory=datetime.utcnow)\n    contest_history: Optional[List[UserContestHistoryRecord]] = None\n    avatar_url: Optional[str] = None\n\n    class Settings:\n        indexes = [\n            \"username\",\n            \"user_slug\",\n            \"data_region\",\n            \"rating\",\n        ]\n"}
{"type": "source_file", "path": "app/db/views.py", "content": "from pydantic import BaseModel\n\nfrom app.db.models import DATA_REGION\n\n\nclass UserKey(BaseModel):\n    # Unique key of User collection, DON'T miss `data_region` when dealing with User models\n    username: str\n    data_region: DATA_REGION\n"}
{"type": "source_file", "path": "app/db/mongodb.py", "content": "import sys\nimport urllib.parse\nfrom typing import Optional\n\n# just for temporary autocompleting, given that motor doesn't have type annotations yet,\n# see https://jira.mongodb.org/browse/MOTOR-331\n# and https://www.mongodb.com/community/forums/t/support-for-type-hint/107593\nfrom beanie import init_beanie\nfrom loguru import logger\nfrom motor.core import (  # bad idea to use these three here,\n    AgnosticClient,\n    AgnosticCollection,\n    AgnosticDatabase,\n)\nfrom motor.motor_asyncio import AsyncIOMotorClient\n\nfrom app.config import get_yaml_config\nfrom app.db.models import (\n    Contest,\n    ContestRecordArchive,\n    ContestRecordPredict,\n    Question,\n    Submission,\n    User,\n)\n\nasync_mongodb_client = None\n\n\ndef get_mongodb_config():\n    \"\"\"\n    Get Mongodb config in `config.yaml`\n    :return:\n    \"\"\"\n    config = get_yaml_config()\n    return config.get(\"mongodb\")\n\n\ndef get_async_mongodb_client() -> AgnosticClient:\n    \"\"\"\n    Raw Motor client handler, use it when beanie cannot work\n    :return:\n    \"\"\"\n    global async_mongodb_client\n    if async_mongodb_client is None:\n        ip = get_mongodb_config().get(\"ip\")\n        port = get_mongodb_config().get(\"port\")\n        username = urllib.parse.quote_plus(get_mongodb_config().get(\"username\"))\n        password = urllib.parse.quote_plus(get_mongodb_config().get(\"password\"))\n        db = get_mongodb_config().get(\"db\")\n        async_mongodb_client = AsyncIOMotorClient(\n            f\"mongodb://{username}:{password}@{ip}:{port}/{db}\",\n            # connectTimeoutMS=None,\n        )\n    return async_mongodb_client\n\n\ndef get_async_mongodb_database(db_name: Optional[str] = None) -> AgnosticDatabase:\n    \"\"\"\n    Raw Motor database handler, use it when beanie cannot work\n    :param db_name:\n    :return:\n    \"\"\"\n    if db_name is None:\n        db_name = get_mongodb_config().get(\"db\")\n    client = get_async_mongodb_client()\n    return client[db_name]\n\n\ndef get_async_mongodb_collection(col_name: str) -> AgnosticCollection:\n    \"\"\"\n    Raw Motor collection handler, use it when beanie cannot work\n    :param col_name:\n    :return:\n    \"\"\"\n    db = get_async_mongodb_database()\n    return db[col_name]\n\n\nasync def start_async_mongodb() -> None:\n    \"\"\"\n    Start beanie when process started.\n    :return:\n    \"\"\"\n    try:\n        async_mongodb_database = get_async_mongodb_database()\n        await init_beanie(\n            database=async_mongodb_database,\n            document_models=[\n                Contest,\n                ContestRecordPredict,\n                ContestRecordArchive,\n                User,\n                Submission,\n                Question,\n            ],\n        )\n        logger.success(\"started mongodb connection\")\n    except Exception as e:\n        logger.exception(f\"Failed to start mongodb. error={e}\")\n        sys.exit(1)\n"}
{"type": "source_file", "path": "app/handler/user.py", "content": "from datetime import datetime, timedelta\nfrom typing import List\n\nfrom beanie.odm.operators.update.general import Set\nfrom loguru import logger\n\nfrom app.constants import (\n    DEFAULT_NEW_USER_ATTENDED_CONTESTS_COUNT,\n    DEFAULT_NEW_USER_RATING,\n)\nfrom app.crawler.user import request_user_rating_and_attended_contests_count\nfrom app.db.models import DATA_REGION, ContestRecordArchive, ContestRecordPredict, User\nfrom app.db.mongodb import get_async_mongodb_collection\nfrom app.db.views import UserKey\nfrom app.utils import exception_logger_reraise, gather_with_limited_concurrency\n\n\nasync def upsert_users_rating_and_attended_contests_count(\n    data_region: DATA_REGION,\n    username: str,\n    save_new_user: bool = True,\n) -> None:\n    \"\"\"\n    Upsert users rating and attendedContestsCount by sending HTTP request to get latest data.\n    :param data_region:\n    :param username:\n    :param save_new_user:\n    :return:\n    \"\"\"\n    try:\n        (\n            rating,\n            attended_contests_count,\n        ) = await request_user_rating_and_attended_contests_count(data_region, username)\n        if rating is None:\n            logger.info(\n                f\"graphql data is None, new user found, {data_region=} {username=}\"\n            )\n            if not save_new_user:\n                logger.info(f\"{save_new_user=} do nothing.\")\n                return\n            rating = DEFAULT_NEW_USER_RATING\n            attended_contests_count = DEFAULT_NEW_USER_ATTENDED_CONTESTS_COUNT\n        user = User(\n            username=username,\n            user_slug=username,\n            data_region=data_region,\n            attendedContestsCount=attended_contests_count,\n            rating=rating,\n        )\n        await User.find_one(\n            User.username == user.username,\n            User.data_region == user.data_region,\n        ).upsert(\n            Set(\n                {\n                    User.update_time: user.update_time,\n                    User.attendedContestsCount: user.attendedContestsCount,\n                    User.rating: user.rating,\n                }\n            ),\n            on_insert=user,\n        )\n    except Exception as e:\n        logger.exception(f\"user update error. {data_region=} {username=} Exception={e}\")\n\n\n@exception_logger_reraise\nasync def update_all_users_in_database(\n    batch_size: int = 100,\n) -> None:\n    \"\"\"\n    For all users in the User collection, update their rating and attended_contests_count.\n    :param batch_size:\n    :return:\n    \"\"\"\n    total_count = await User.count()\n    logger.info(f\"User collection now has {total_count=}\")\n    for i in range(0, total_count, batch_size):\n        logger.info(f\"progress = {i / total_count* 100 :.2f}%\")\n        docs: List[UserKey] = await (\n            User.find_all()\n            .sort(-User.rating)\n            .skip(i)\n            .limit(batch_size)\n            .project(UserKey)\n            .to_list()\n        )\n        cn_tasks = []\n        us_tasks = []\n        for doc in docs:\n            if doc.data_region == \"CN\":\n                cn_tasks.append(\n                    upsert_users_rating_and_attended_contests_count(\n                        doc.data_region, doc.username, False\n                    )\n                )\n            else:\n                us_tasks.append(\n                    upsert_users_rating_and_attended_contests_count(\n                        doc.data_region, doc.username, False\n                    )\n                )\n        await gather_with_limited_concurrency(\n            [\n                # CN site has a strong rate limit\n                gather_with_limited_concurrency(cn_tasks, 1),\n                gather_with_limited_concurrency(us_tasks, 5),\n            ],\n            30,\n        )\n\n\n@exception_logger_reraise\nasync def save_users_of_contest(\n    contest_name: str,\n    predict: bool,\n) -> None:\n    \"\"\"\n    Update all users' rating and attendedContestsCount.\n    For the ContestRecordPredict collection, don't update users who have a zero score or were updated recently.\n    :param contest_name:\n    :param predict:\n    :return:\n    \"\"\"\n    if predict:\n        col = get_async_mongodb_collection(ContestRecordPredict.__name__)\n        pipeline = [\n            {\"$match\": {\"contest_name\": contest_name, \"score\": {\"$ne\": 0}}},\n            {\n                \"$lookup\": {\n                    \"from\": \"User\",\n                    \"let\": {\"data_region\": \"$data_region\", \"username\": \"$username\"},\n                    \"pipeline\": [\n                        {\n                            \"$match\": {\n                                \"$expr\": {\n                                    \"$and\": [\n                                        {\"$eq\": [\"$data_region\", \"$$data_region\"]},\n                                        {\"$eq\": [\"$username\", \"$$username\"]},\n                                        {\n                                            \"$gte\": [\n                                                \"$update_time\",\n                                                datetime.utcnow() - timedelta(hours=36),\n                                            ]\n                                        },\n                                    ]\n                                }\n                            }\n                        },\n                    ],\n                    \"as\": \"recent_updated_user\",\n                }\n            },\n            {\"$match\": {\"recent_updated_user\": {\"$eq\": []}}},\n            {\"$project\": {\"_id\": 0, \"data_region\": 1, \"username\": 1}},\n        ]\n    else:\n        col = get_async_mongodb_collection(ContestRecordArchive.__name__)\n        pipeline = [\n            {\"$match\": {\"contest_name\": contest_name}},\n            {\"$project\": {\"_id\": 0, \"data_region\": 1, \"username\": 1}},\n        ]\n    cursor = col.aggregate(pipeline)\n    docs = await cursor.to_list(length=None)\n    logger.info(f\"docs length = {len(docs)}\")\n    cn_tasks = []\n    us_tasks = []\n    for doc in docs:\n        if doc[\"data_region\"] == \"CN\":\n            cn_tasks.append(\n                upsert_users_rating_and_attended_contests_count(\n                    doc[\"data_region\"], doc[\"username\"]\n                )\n            )\n        else:\n            us_tasks.append(\n                upsert_users_rating_and_attended_contests_count(\n                    doc[\"data_region\"], doc[\"username\"]\n                )\n            )\n    await gather_with_limited_concurrency(\n        [\n            # CN site has a strong rate limit\n            gather_with_limited_concurrency(cn_tasks, 1),\n            gather_with_limited_concurrency(us_tasks, 5),\n        ],\n        30,\n    )\n"}
{"type": "source_file", "path": "app/db/__init__.py", "content": ""}
{"type": "source_file", "path": "app/schedulers.py", "content": "import asyncio\nfrom datetime import datetime, timedelta\nfrom typing import Optional\n\nimport pytz\nfrom apscheduler.schedulers.asyncio import AsyncIOScheduler\nfrom loguru import logger\n\nfrom app.constants import (\n    BIWEEKLY_CONTEST_BASE,\n    BIWEEKLY_CONTEST_START,\n    WEEKLY_CONTEST_BASE,\n    WEEKLY_CONTEST_START,\n    CronTimePointWkdHrMin,\n)\nfrom app.core.predictor import predict_contest\nfrom app.handler.contest import (\n    is_cn_contest_data_ready,\n    save_recent_and_next_two_contests,\n)\nfrom app.handler.contest_record import (\n    save_archive_contest_records,\n    save_predict_contest_records,\n)\nfrom app.utils import exception_logger_reraise, get_passed_weeks\n\nglobal_scheduler: Optional[AsyncIOScheduler] = None\n\n\n@exception_logger_reraise\nasync def save_last_two_contest_records() -> None:\n    \"\"\"\n    Update last weekly contest, and last biweekly contest.\n    Upsert contest records in ContestRecordArchive, its users will also be updated in the save_archive_contest function.\n    :return:\n    \"\"\"\n    utc = datetime.utcnow()\n\n    biweekly_passed_weeks = get_passed_weeks(utc, BIWEEKLY_CONTEST_BASE.dt)\n    last_biweekly_contest_name = (\n        f\"biweekly-contest-{biweekly_passed_weeks // 2 + BIWEEKLY_CONTEST_BASE.num}\"\n    )\n    logger.info(f\"{last_biweekly_contest_name=} update archive contests\")\n    await save_archive_contest_records(\n        contest_name=last_biweekly_contest_name, data_region=\"CN\"\n    )\n\n    weekly_passed_weeks = get_passed_weeks(utc, WEEKLY_CONTEST_BASE.dt)\n    last_weekly_contest_name = (\n        f\"weekly-contest-{weekly_passed_weeks + WEEKLY_CONTEST_BASE.num}\"\n    )\n    logger.info(f\"{last_weekly_contest_name=} update archive contests\")\n    await save_archive_contest_records(\n        contest_name=last_weekly_contest_name, data_region=\"CN\"\n    )\n\n\n@exception_logger_reraise\nasync def composed_predict_jobs(\n    contest_name: str,\n    max_try_times: int = 300,\n) -> None:\n    \"\"\"\n    All three steps which should be run when the contest is just over\n    :param contest_name:\n    :param max_try_times:\n    :return:\n    \"\"\"\n    tried_times = 1\n    while (\n        not (cn_data_is_ready := await is_cn_contest_data_ready(contest_name))\n        and tried_times < max_try_times\n    ):\n        await asyncio.sleep(60)\n        tried_times += 1\n    if not cn_data_is_ready:\n        logger.error(\n            f\"give up after failed {tried_times=} times. CONTINUE WITH INCOMPLETE DATA\"\n        )\n    await save_recent_and_next_two_contests()\n    await save_predict_contest_records(contest_name=contest_name, data_region=\"CN\")\n    await predict_contest(contest_name=contest_name)\n    await save_archive_contest_records(\n        contest_name=contest_name, data_region=\"CN\", save_users=False\n    )\n\n\nasync def pre_save_predict_users(contest_name: str) -> None:\n    \"\"\"\n    Cache CN and US users during contest\n    :param contest_name:\n    :return:\n    \"\"\"\n    await save_predict_contest_records(contest_name, \"CN\")\n    await save_predict_contest_records(contest_name, \"US\")\n\n\nasync def add_prediction_schedulers(contest_name: str) -> None:\n    \"\"\"\n    First add two save_predict_contest_records jobs to caching participants' info (mainly whose latest rating)\n    by doing so can improve the speed of real calculation job greatly\n    because we are wasting most of the time at fetching participants' info. (`save_users_of_contest` function)\n    Then add one composed_predict_jobs (real calculation jobs)\n    :param contest_name:\n    :return:\n    \"\"\"\n    utc = datetime.utcnow()\n    global global_scheduler\n    for pre_save_time in [utc + timedelta(minutes=25), utc + timedelta(minutes=70)]:\n        # preparation for prediction running, get users in advance.\n        global_scheduler.add_job(\n            pre_save_predict_users,\n            kwargs={\"contest_name\": contest_name},\n            trigger=\"date\",\n            run_date=pre_save_time,\n        )\n    # postpone 5 minutes to wait for LeetCode updating final result.\n    predict_run_time = utc + timedelta(minutes=95)\n    # real prediction running function.\n    global_scheduler.add_job(\n        composed_predict_jobs,\n        kwargs={\"contest_name\": contest_name},\n        trigger=\"date\",\n        run_date=predict_run_time,\n    )\n\n\nasync def scheduler_entry() -> None:\n    \"\"\"\n    Dispatch jobs at every minute.\n    :return:\n    \"\"\"\n    global global_scheduler\n    utc = datetime.utcnow()\n    time_point = CronTimePointWkdHrMin(utc.weekday(), utc.hour, utc.minute)\n    if time_point == WEEKLY_CONTEST_START:\n        passed_weeks = get_passed_weeks(utc, WEEKLY_CONTEST_BASE.dt)\n        contest_name = f\"weekly-contest-{passed_weeks + WEEKLY_CONTEST_BASE.num}\"\n        logger.info(f\"parsed {contest_name=}\")\n        await add_prediction_schedulers(contest_name)\n    elif time_point == BIWEEKLY_CONTEST_START:\n        passed_weeks = get_passed_weeks(utc, BIWEEKLY_CONTEST_BASE.dt)\n        if passed_weeks % 2 != 0:\n            logger.info(\n                f\"will not run biweekly prediction, passed_weeks={passed_weeks} is odd for now={utc}\"\n            )\n            return\n        contest_name = (\n            f\"biweekly-contest-{passed_weeks // 2 + BIWEEKLY_CONTEST_BASE.num}\"\n        )\n        logger.info(f\"parsed {contest_name=}\")\n        await add_prediction_schedulers(contest_name)\n    elif (\n        2 <= time_point.weekday <= 5  # Wednesday, Tuesday, Friday and Saturday 00:00\n        and time_point.hour == 0\n        and time_point.minute == 0\n    ):\n        # do other low-priority jobs such as updating user's rating and participated contest count.\n        global_scheduler.add_job(\n            save_recent_and_next_two_contests,\n            trigger=\"date\",\n            run_date=utc + timedelta(minutes=1),\n        )\n        global_scheduler.add_job(\n            save_last_two_contest_records,\n            trigger=\"date\",\n            run_date=utc + timedelta(minutes=10),\n        )\n    else:\n        logger.trace(f\"job_dispatcher nothing to do for {utc=} {time_point=}\")\n    if len(job_list := global_scheduler.get_jobs()) > 1:\n        # logging when there are more schedulers besides scheduler_entry itself.\n        logger.info(f\"global_scheduler jobs={'; '.join(str(job) for job in job_list)}\")\n\n\nasync def start_scheduler() -> None:\n    \"\"\"\n    Add `scheduler_entry` interval job when main process started.\n    :return:\n    \"\"\"\n    global global_scheduler\n    if global_scheduler is not None:\n        logger.error(\"global_scheduler could only be started once.\")\n    global_scheduler = AsyncIOScheduler(timezone=pytz.utc)\n    global_scheduler.add_job(scheduler_entry, \"interval\", minutes=1)\n    global_scheduler.start()\n    logger.success(\"started schedulers\")\n"}
{"type": "source_file", "path": "app/handler/__init__.py", "content": ""}
{"type": "source_file", "path": "app/handler/submission.py", "content": "from datetime import datetime, timedelta\nfrom typing import Dict, List, Tuple\n\nfrom beanie.odm.operators.update.general import Set\nfrom loguru import logger\n\nfrom app.db.models import ContestRecordArchive, Submission\nfrom app.db.mongodb import get_async_mongodb_collection\nfrom app.db.views import UserKey\nfrom app.handler.question import save_questions, save_questions_real_time_count\nfrom app.utils import (\n    exception_logger_reraise,\n    gather_with_limited_concurrency,\n    get_contest_start_time,\n)\n\n\nasync def aggregate_rank_at_time_point(\n    contest_name: str,\n    time_point: datetime,\n) -> Tuple[Dict[Tuple[str, str], int], int]:\n    \"\"\"\n    For a single time_point, rank all the participants.\n    Be careful that every wrong submission should add a 5-minutes penalty.\n    :param contest_name:\n    :param time_point:\n    :return:\n    \"\"\"\n    # hard to use beanie here, so use raw MongoDB aggregation\n    col = get_async_mongodb_collection(Submission.__name__)\n    rank_map = dict()\n    last_credit_sum = None\n    last_penalty_date = None\n    tie_rank = raw_rank = 0\n    pipeline = [\n        {\"$match\": {\"contest_name\": contest_name, \"date\": {\"$lte\": time_point}}},\n        {\n            \"$group\": {\n                \"_id\": {\"username\": \"$username\", \"data_region\": \"$data_region\"},\n                \"credit_sum\": {\"$sum\": \"$credit\"},\n                \"fail_count_sum\": {\"$sum\": \"$fail_count\"},\n                \"date_max\": {\"$max\": \"$date\"},\n            }\n        },\n        {\n            \"$addFields\": {\n                \"penalty_date\": {\n                    \"$dateAdd\": {\n                        \"startDate\": \"$date_max\",\n                        \"unit\": \"minute\",\n                        \"amount\": {\"$multiply\": [5, \"$fail_count_sum\"]},\n                    }\n                },\n                \"username\": \"$_id.username\",\n                \"data_region\": \"$_id.data_region\",\n            }\n        },\n        {\"$unset\": [\"_id\"]},\n        {\"$sort\": {\"credit_sum\": -1, \"penalty_date\": 1}},\n        {\n            \"$project\": {\n                \"_id\": 0,\n                \"data_region\": 1,\n                \"username\": 1,\n                \"credit_sum\": 1,\n                \"fail_count_sum\": 1,\n                \"penalty_date\": 1,\n            }\n        },\n    ]\n    async for doc in col.aggregate(pipeline):\n        raw_rank += 1\n        if (\n            doc[\"credit_sum\"] == last_credit_sum\n            and doc[\"penalty_date\"] == last_penalty_date\n        ):\n            rank_map[(doc[\"username\"], doc[\"data_region\"])] = tie_rank\n        else:\n            tie_rank = raw_rank\n            rank_map[(doc[\"username\"], doc[\"data_region\"])] = raw_rank\n        last_credit_sum = doc[\"credit_sum\"]\n        last_penalty_date = doc[\"penalty_date\"]\n    return rank_map, raw_rank\n\n\nasync def save_real_time_rank(\n    contest_name: str,\n    delta_minutes: int = 1,\n) -> None:\n    \"\"\"\n    For every delta_minutes, invoke `aggregate_rank_at_time_point` function to get ranking on single time_point\n    Then append every user's ranking to a list and save it\n    :param contest_name:\n    :param delta_minutes:\n    :return:\n    \"\"\"\n    logger.info(\"started running real_time_rank update function\")\n    users = (\n        await ContestRecordArchive.find(\n            ContestRecordArchive.contest_name == contest_name,\n            ContestRecordArchive.score != 0,  # No need to query users who have 0 score\n        )\n        .project(UserKey)\n        .to_list()\n    )\n    real_time_rank_map = {(user.username, user.data_region): list() for user in users}\n    start_time = get_contest_start_time(contest_name)\n    end_time = start_time + timedelta(minutes=90)\n    i = 1\n    while (start_time := start_time + timedelta(minutes=delta_minutes)) <= end_time:\n        rank_map, last_rank = await aggregate_rank_at_time_point(\n            contest_name, start_time\n        )\n        last_rank += 1\n        for (username, data_region), rank in rank_map.items():\n            real_time_rank_map[(username, data_region)].append(rank)\n        for k in real_time_rank_map.keys():\n            if len(real_time_rank_map[k]) != i:\n                real_time_rank_map[k].append(last_rank)\n        i += 1\n    tasks = [\n        ContestRecordArchive.find_one(\n            ContestRecordArchive.contest_name == contest_name,\n            ContestRecordArchive.username == username,\n            ContestRecordArchive.data_region == data_region,\n        ).update(\n            Set(\n                {\n                    ContestRecordArchive.real_time_rank: rank_list,\n                }\n            )\n        )\n        for (username, data_region), rank_list in real_time_rank_map.items()\n    ]\n    logger.info(\"updating real_time_rank field in ContestRecordArchive collection\")\n    await gather_with_limited_concurrency(tasks, max_con_num=5)\n    logger.success(f\"finished updating real_time_rank for {contest_name=}\")\n\n\n@exception_logger_reraise\nasync def save_submission(\n    contest_name: str,\n    contest_record_list: List[Dict],\n    nested_submission_list: List[Dict],\n) -> None:\n    \"\"\"\n    Save all of submission-related data to MongoDB\n    :param contest_name:\n    :param contest_record_list:\n    :param nested_submission_list:\n    :return:\n    \"\"\"\n    time_point = datetime.utcnow()\n    questions = await save_questions(contest_name)\n    question_credit_mapper = {\n        question.question_id: question.credit for question in questions\n    }\n    submissions = list()\n    for contest_record_dict, nested_submission_dict in zip(\n        contest_record_list, nested_submission_list\n    ):\n        for question_id, submission_dict in nested_submission_dict.items():\n            submission_dict.pop(\"id\")\n            submission_dict |= {\n                \"contest_name\": contest_name,\n                \"username\": contest_record_dict[\"username\"],\n                \"credit\": question_credit_mapper[int(question_id)],\n            }\n        submissions.extend(\n            [\n                Submission.model_validate(submission_dict)\n                for submission_dict in nested_submission_dict.values()\n            ]\n        )\n    tasks = [\n        Submission.find_one(\n            Submission.contest_name == submission.contest_name,\n            Submission.username == submission.username,\n            Submission.data_region == submission.data_region,\n            Submission.question_id == submission.question_id,\n        ).upsert(\n            Set(\n                {\n                    Submission.date: submission.date,\n                    Submission.fail_count: submission.fail_count,\n                    Submission.credit: submission.credit,\n                    Submission.update_time: submission.update_time,\n                    Submission.lang: submission.lang,\n                }\n            ),\n            on_insert=submission,\n        )\n        for submission in submissions\n    ]\n    logger.info(\"updating Submission collection\")\n    await gather_with_limited_concurrency(tasks, max_con_num=5)\n    # Old submissions may be rejudged, must be deleted here, or will cause error when plotting.\n    await Submission.find(\n        Submission.contest_name == contest_name,\n        Submission.update_time < time_point,\n    ).delete()\n    logger.success(\"finished updating submissions, begin to save real_time_rank\")\n    await save_questions_real_time_count(contest_name)\n    await save_real_time_rank(contest_name)\n"}
