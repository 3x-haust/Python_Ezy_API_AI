{"repo_info": {"repo_name": "gallama", "repo_owner": "remichu-ai", "repo_url": "https://github.com/remichu-ai/gallama"}}
{"type": "test_file", "path": "examples/test_video_using_websocket.py", "content": "import asyncio\nimport websockets\nimport json\nimport uuid\nfrom enum import Enum\nfrom typing import List, Optional\nimport os\nimport struct\nimport cv2\nimport numpy as np\nimport time\n\nfrom openai import AsyncOpenAI\n\nasync def main():\n    client = AsyncOpenAI(api_key=\"test\", websocket_base_url=\"ws://127.0.0.1:8000/v1\")\n\n    async with client.beta.realtime.connect(model=\"gpt-4o-realtime-preview-2024-10-01\") as connection:\n        await connection.session.update(session={\n            'modalities': ['text'],\n            'video': {\n                'video_stream': True,\n                'video_max_resolution': \"720p\"\n                # 'video_max_resolution': \"540p\"\n                # 'video_max_resolution': None\n            }\n        })\n\n        await connection.conversation.item.create(\n            item={\n                \"type\": \"message\",\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"input_text\", \"text\": \"what do you see in the video?\"}],\n                # \"content\": [{\"type\": \"input_text\", \"text\": \"how many scissor do you see?\"}],\n                # \"content\": [{\"type\": \"input_text\", \"text\": \"do you see any coffee appliances\"}],\n            }\n        )\n\n        frames_per_second = 2  # Change this value as needed\n        await send_video_frames(\n            video_path=\"/home/remichu/work/ML/gallama/experiment/media/kitchen.mp4\",\n            frames_per_second=frames_per_second\n        )\n\n        await connection.response.create()\n\n        start = time.time()\n        async for event in connection:\n            if event.type == 'response.text.delta':\n                print(event.delta, flush=True, end=\"\")\n\n            elif event.type == 'response.text.done':\n                print(\"text done\")\n\n            elif event.type == \"response.done\":\n                print(f\"time taken: {time.time() - start}\")\n                break\n\n\nasync def send_video_frames(video_path: str, frames_per_second: int = 1):\n    # Connect to the video WebSocket server\n    async with websockets.connect('ws://localhost:8000/video') as websocket:\n        # Open the video file\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            print(f\"Error: Could not open video {video_path}\")\n            return\n\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        frame_interval = int(fps / frames_per_second)  # Calculate the interval between frames\n        frame_count = 0\n\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Extract frames based on the specified frames_per_second\n            if frame_count % frame_interval == 0:\n                # Convert the frame to PNG format\n                _, buffer = cv2.imencode('.png', frame)\n                frame_data = buffer.tobytes()\n\n                # Pack the frame data with a timestamp (optional)\n                timestamp = frame_count / fps  # Calculate the timestamp\n                packed_data = struct.pack('>d', timestamp) + frame_data\n\n                # Send the frame data to the WebSocket server\n                await websocket.send(packed_data)\n                print(f\"Sent frame at {timestamp:.2f} seconds\")\n\n            frame_count += 1\n\n        cap.release()\n        print(\"Finished sending video frames.\")\n\nif __name__ == \"__main__\":\n    print(\"Starting WebSocket LLM and video test...\")\n    asyncio.run(main())"}
{"type": "test_file", "path": "src/tests/make_server.py", "content": "from argparse import Namespace\nfrom typing import List, Dict\n\n\ndef setup_make_server_args(\n        model_id: List[Dict[str, str]] = None,\n        verbose: bool = False,\n        detached: bool = False,\n        host: str = \"127.0.0.1\",\n        port: int = 8000,\n        reload: bool = False\n) -> Namespace:\n    \"\"\"\n    Create an argparse.Namespace object with the given parameters.\n\n    Args:\n        model_id (List[Dict[str, str]], optional): List of dictionaries containing model IDs. Defaults to None.\n        verbose (bool, optional): Turn on more verbose logging. Defaults to False.\n        detached (bool, optional): Log to ZeroMQ. Defaults to False.\n        host (str, optional): The host to bind to. Defaults to \"127.0.0.1\".\n        port (int, optional): The port to bind to. Defaults to 8000.\n        reload (bool, optional): Enable auto-reload. Defaults to False.\n\n    Returns:\n        Namespace: An argparse.Namespace object with the specified attributes.\n    \"\"\"\n    args = Namespace(\n        model_id=model_id,\n        verbose=verbose,\n        detached=detached,\n        host=host,\n        port=port,\n        reload=reload\n    )\n    return args\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example model_id\n    model_id = [{\"model_id\": \"Mixtral-8x7B\", \"gpus\": \"1.0\", \"cache_size\": \"2048\"}]\n\n    # Create args\n    args = setup_make_server_args(\n        model_id=model_id,\n        verbose=True,\n        port=8080\n    )\n\n    # Call make_server with the created args\n    make_server(args)"}
{"type": "test_file", "path": "src/tests/test_openai_server.py", "content": "# TODO\n# import pytest\n# from fastapi.testclient import TestClient\n# from gallama.server import run_from_script, manager_app\n#\n# class TestArgs:\n#     def __init__(self):\n#         self.model_id = [{\"model_id\": \"mistral\"}]\n#         self.detached = True\n#         self.host = \"127.0.0.1\"\n#         self.port = 8000\n#         self.draft_model_id = \"mistral\"\n#         self.verbose = False\n#         self.strict_mode = False\n#\n# args = TestArgs()\n#\n# app = run_from_script(args)     # start an api service\n#\n# client = TestClient(manager_app)\n#\n# def test_read_root():\n#     response = client.get(\"/\")\n#     assert response.status_code == 200\n#     assert response.json() == {\"Hello\": \"World\"}\n#\n# def test_chat_completion():\n#     payload = {\n#         \"model\": \"Mixtral-8x7B\",\n#         \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n#         \"temperature\": 0.7,\n#         \"stream\": False\n#     }\n#     response = client.post(\"/v1/chat/completions\", json=payload)\n#     assert response.status_code == 200\n#     assert \"choices\" in response.json()\n#     assert len(response.json()[\"choices\"]) > 0\n#     assert \"message\" in response.json()[\"choices\"][0]\n#\n# def test_generate():\n#     payload = {\n#         \"prompt\": \"Once upon a time\",\n#         \"model\": \"Mixtral-8x7B\",\n#         \"stream\": False,\n#         \"max_tokens\": 50\n#     }\n#     response = client.post(\"/v1/completions\", json=payload)\n#     assert response.status_code == 200\n#     assert \"choices\" in response.json()\n#     assert len(response.json()[\"choices\"]) > 0\n#     assert \"text\" in response.json()[\"choices\"][0]\n#\n# def test_embeddings():\n#     payload = {\n#         \"input\": \"The quick brown fox jumps over the lazy dog\",\n#         \"model\": \"Mixtral-8x7B\"\n#     }\n#     response = client.post(\"/v1/embeddings\", json=payload)\n#     assert response.status_code == 200\n#     assert \"data\" in response.json()\n#     assert len(response.json()[\"data\"]) > 0\n#     assert \"embedding\" in response.json()[\"data\"][0]\n#\n# def test_get_models():\n#     response = client.get(\"/v1/models\")\n#     assert response.status_code == 200\n#     assert \"data\" in response.json()\n#     assert len(response.json()[\"data\"]) > 0\n#     assert \"id\" in response.json()[\"data\"][0]\n#\n# def test_load_model():\n#     payload = {\n#         \"model_id\": \"Mixtral-8x7B\",\n#         \"gpus\": [1.0],\n#         \"cache_size\": 2048\n#     }\n#     response = client.post(\"/load_model\", json=payload)\n#     assert response.status_code == 200\n#\n# def test_delete_model():\n#     response = client.post(\"/delete_model\", json={\"model_name\": \"Mixtral-8x7B\"})\n#     assert response.status_code == 200\n#\n# def test_get_status():\n#     response = client.get(\"/status\")\n#     assert response.status_code == 200\n#     assert \"status\" in response.json()\n#\n# def test_health_check():\n#     response = client.get(\"/health\")\n#     assert response.status_code == 200\n#     assert response.json() == {\"status\": \"healthy\"}"}
{"type": "source_file", "path": "src/gallama/backend/llm/__init__.py", "content": "\nfrom .thinking_template import THINKING_TEMPLATE\n\n# selective import as not all backend available on all platform\ntry:\n    from .engine import ModelExllama\nexcept ImportError:\n    ModelExllama = None\n\ntry:\n    from .engine import ModelLlamaCpp\nexcept ImportError:\n    ModelLlamaCpp = None\n\ntry:\n    from .engine import ModelTransformers\nexcept ImportError:\n    ModelTransformers = None\n\n# try:\nfrom .engine import ModelMLXVLM\n# except ImportError:\n#     ModelMLXVLM = None"}
{"type": "source_file", "path": "docker_initializer.py", "content": "from split_lang import LangSplitter\n\nlang_splitter = LangSplitter()\nlang_splitter.split_by_lang(\"Sample Text\")"}
{"type": "source_file", "path": "examples/streamlit_chatbot.py", "content": "# do install streamlit first:\n# pip install streamlit openai\n# pip install mistralai anthropic\n# run this code by run following command in the terminal\n# 1/ cd to where the streamlit_chatbot.py is\n# 2/ streamlit run streamlit_chatbot.py\n\n\n# If you want to use this code with openai, claude or mistralai, you can update the API Key in the below code:\n# just search for this comment: #You can put your api key here\n\n\nimport streamlit as st\nfrom openai import OpenAI\n\n\n# optional installation\ntry:\n    from mistralai.client import MistralClient\n    from anthropic import Anthropic\nexcept:\n    MistralClient = None\n    Anthropic = None\n\n\n##wrapper class#####################################################\nclass LLMWrapper:\n    def __init__(self, api_key, model, stream=True, base_url=None):\n        self.api_key = api_key\n        self.model = model\n        self.stream = stream\n        self.base_url = base_url\n        if model in [\"gpt-4o-mini\", \"gpt-4-turbo\", \"gpt-4o\"]:\n            self.provider = \"openai\"\n        elif model in [\"mistral-small-latest\", \"mistral-medium-latest\"]:\n            self.provider = \"openai\"\n        elif model in [\"claude-sonnet\"]:\n            self.provider = \"anthropic\"\n        else:\n            self.provider = \"local\"\n\n        self.llm = self._init_model(self.provider, self.api_key, self.base_url)\n\n        # simulate openai\n        self.chat = self.chat(**vars(self))\n\n    def _init_model(self, provider, api_key, base_url):\n        if self.provider == \"openai\":\n            return OpenAI(api_key=api_key)\n        elif self.provider == \"mistral\":\n            return MistralClient(api_key=api_key)\n        elif self.provider == \"anthropic\":\n            return Anthropic(api_key=api_key)\n        elif self.provider == \"local\":\n            if base_url is None:\n                return OpenAI(api_key=api_key, base_url=\"http://localhost:8000/v1\")\n            else:\n                return OpenAI(api_key=api_key, base_url=base_url)\n\n    class chat:\n        def __init__(self, **kwargs):\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n\n            # simulate openai\n            self.completions = self.completions(**vars(self))\n\n        class completions:\n            def __init__(self, **kwargs):\n                for key, value in kwargs.items():\n                    setattr(self, key, value)\n\n            def create(self, messages, model=None, stream=None):\n                stream_setting = stream if stream else self.stream\n\n                if self.provider == \"openai\" or self.provider == \"local\":\n                    return self.llm.chat.completions.create(\n                        messages=messages,\n                        stream=stream_setting,\n                        model=model if model else self.model,\n                    )\n                elif self.provider == \"mistral\":\n                    if stream_setting:\n                        return self.llm.chat_stream(\n                            messages=messages,\n                            model=model if model else self.model,\n                        )\n                    else:\n                        return self.llm.chat(\n                            messages=messages,\n                            model=model if model else self.model,\n                        )\n                elif self.provider == \"anthropic\":\n                    return self.llm.messages.create(\n                        max_tokens=4096,\n                        messages=messages,\n                        stream=stream_setting,\n                        model=\"claude-3-5-sonnet-20240620\",\n                        # model = model if model else self.model,\n                    )\n\n\n##wrapper class#####################################################\n\nst.title(\"💬 Chatbot\")\nst.caption(\"🚀 A streamlit chatbot powered by OpenAI LLM\")\n\nadd_selectbox = st.sidebar.selectbox(\n    \"API Backend?\",\n    (\n    \"local\", \"gpt-4o-mini\", \"gpt-4o\", \"mistral-medium-latest\", \"mistral-small-latest\", \"mistral-large-latest\", \"claude-sonnet\")\n)\n\nif add_selectbox == add_selectbox == \"gpt-4o-mini\" or add_selectbox == \"gpt-4o\":\n    client = LLMWrapper(api_key=\"NA\", model=add_selectbox)          # You can put your api key here\nelif add_selectbox == \"local\":\n    client = LLMWrapper(api_key=\"NA\", base_url=\"http://localhost:8000/v1\",\n                        model='/home/remichu/work/ML/model/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4')\nelif add_selectbox == \"mistral-large-latest\"  or \"mistral-medium-latest\" or add_selectbox == \"mistral-small-latest\":\n    client = LLMWrapper(api_key=\"NA\", model=add_selectbox)          # You can put your api key here\nelif add_selectbox == \"claude-sonnet\":\n    client = LLMWrapper(\n        api_key=\"NA\",           # You can put your api key here\n        model=add_selectbox)\n\nst.session_state[\"model\"] = add_selectbox\n\n\ndef reset_conversation():\n    # st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"How can I help you?\"}]\n    st.session_state[\"messages\"] = []\n\n\nst.sidebar.button('Clear', on_click=reset_conversation)\n\nif \"messages\" not in st.session_state:\n    # st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"How can I help you?\"}]\n    st.session_state[\"messages\"] = []\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\nif prompt := st.chat_input(\"What is up?\"):\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n\n    with st.chat_message(\"assistant\"):\n        message_placeholder = st.empty()\n        stream = client.chat.completions.create(\n            model=st.session_state[\"model\"],\n            messages=[\n                {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n                for m in st.session_state.messages\n            ],\n            stream=True,\n        )\n        response = \"\"\n        if add_selectbox != \"claude-sonnet\":\n            st.write_stream(stream)\n        else:\n            for chunk in stream:\n                try:\n                    if chunk.delta:\n                        # print(chunk.delta.text)\n                        response = response + chunk.delta.text\n                        message_placeholder.markdown(response + \"| \")\n                except Exception as e:\n                    pass\n\n        # remove the ending \"|\"\n        message_placeholder.markdown(response)\n\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n\n# streamlit run Chatbot.py\n# if __name__ == \"__main__\":\n#    main()"}
{"type": "source_file", "path": "src/gallama/api_response/__init__.py", "content": ""}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/__init__.py", "content": "from .exllama.exllamav2 import ModelExllama\nfrom .llamacpp.llamacpp import ModelLlamaCpp\nfrom .transformers.transformers import ModelTransformers\nfrom .mlx_vllm.mlx_vlm import ModelMLXVLM"}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/base.py", "content": "from abc import ABC, abstractmethod\nfrom typing import Dict, List, Union, Optional, Literal, Callable, Tuple\nimport asyncio\nimport re       # for text processing of the thinking\nfrom fastapi import HTTPException, Request\nimport textwrap\n\n# logger\nfrom gallama.logger import logger\n\n# format enforcement\nfrom lmformatenforcer.tokenenforcer import TokenEnforcerTokenizerData\nfrom formatron.formatter import FormatterBuilder\nfrom formatron.schemas.pydantic import ClassSchema\nfrom gallama.backend.llm.format_enforcer import FormatEnforcer\n\n# thinking\nfrom gallama.backend.llm.thinking_template import THINKING_TEMPLATE, Thinking\n\n# function calling\nfrom gallama.backend.llm.tools import Tools, create_function_models_v2, create_function_models_formatron\n\nfrom gallama.utils.utils import get_token_length\nfrom gallama.api_response.chat_response import get_response_from_queue   # helper function to collect result from queue\nfrom gallama.data_classes import (\n    ModelSpec,\n    ChatMLQuery,\n    GenEnd,\n    GenText,\n    GenQueue,\n    GenStart,\n    GenerationStats,\n    QueueContext,\n    GenQueueDynamic,\n    VideoFrame,\n    MultiModalTextContent,\n    MultiModalImageContent\n)\nfrom ....config.config_manager import ConfigManager\n# handle prompting\nfrom gallama.backend.llm.prompt_engine import PromptEngine\nfrom dataclasses import dataclass\n\n# video handling\nfrom ....data_classes import VideoFrameCollection\n\nconfig_manager = ConfigManager()\n\n@dataclass\nclass ToolCallV2:\n    gen_dynamic_queue: List[GenQueueDynamic]\n    stop_event: asyncio.Event\n    generate_kwargs: dict\n\nclass ModelInterface(ABC):\n    @abstractmethod\n    def __init__(self, model_spec:ModelSpec):\n        # initialization share the same code to keep the frontend consistent\n        # if a backend does not support the value, please set it in the __init__ or load_model()\n\n        # load prompt engine\n        self.prompt_eng = PromptEngine(prompt_format=model_spec.prompt_template)\n\n        # model_spec capture cli argument\n        # model_config is from yml file\n        self.model_id = model_spec.model_id\n        # self.model_name = model_spec.model_name or model_config[\"model_name\"]\n        self.model_name = model_spec.model_name\n        # self.max_seq_len = model_spec.max_seq_len or model_config.get(\"max_seq_len\", None)\n        self.max_seq_len = model_spec.max_seq_len\n        if self.max_seq_len is not None:\n            self.max_seq_len = (self.max_seq_len//256) * 256     # for paged attention\n\n        # self.gpus = model_spec.gpus or model_config.get(\"gpus\") or \"auto\"\n        self.gpus = model_spec.gpus or \"auto\"\n        # self.cache_size = model_spec.cache_size or model_config.get(\"cache_size\") or self.max_seq_len   # default to max_seq_len if not set\n        self.cache_size = model_spec.cache_size or self.max_seq_len   # default to max_seq_len if not set\n        if self.cache_size is not None:\n            self.cache_size = (self.cache_size//256) * 256     # for paged attention\n            if self.max_seq_len is not None:\n                # cache size must be greater or equal to max_seq_len\n                self.cache_size = max(self.cache_size, self.max_seq_len)\n\n        # self.cache_quant = model_spec.cache_quant or model_config.get(\"cache_quant\") or \"Q4\"\n        self.cache_quant = model_spec.cache_quant or \"Q6\"       # default to cache quant 6\n        # self.backend = model_spec.backend or model_config[\"backend\"] or \"exllama\"\n        self.backend = model_spec.backend   # default should be set as exllama if not defined\n        # self.tensor_parallel = model_spec.tensor_parallel or model_config.get(\"tensor_parallel\", False)\n        self.tensor_parallel = model_spec.tensor_parallel or False      # tensor parallel is False unless explicitly\n\n        # transformers specific arguments\n        # self.backend_extra_args = model_spec.get(\"backend_extra_args\") or {}\n        self.backend_extra_args = model_spec.backend_extra_args or {}\n\n\n        # handle draft model\n        draft_model_config = {}\n        if model_spec.draft_model_id:\n            draft_model_config = config_manager.get_model_config(model_spec.draft_model_name)\n            if not draft_model_config:\n                raise HTTPException(f\"Model config for '{model_spec.draft_model_name}' not exist\")\n\n\n        # draft model is via cli only\n        self.draft_model_id = draft_model_config.get(\"model_id\")\n        self.draft_model_name = model_spec.draft_model_name or None\n        self.draft_gpus = model_spec.draft_gpus or draft_model_config.get(\"draft_gpus\") or \"auto\"\n        self.draft_cache_size = self.cache_size   # set to the same as main model\n        self.draft_cache_quant = model_spec.draft_cache_quant or draft_model_config.get(\"cache_quant\") or \"Q4\"\n        # assert (self.draft_model_id is None) == (self.draft_model_name is None)\n\n        # get the eos_token_str by merging the default config with anything set by user\n        self.eos_token_str = list(set(model_spec.eos_token_list + self.prompt_eng.eos_token_list))\n        self.eos_token_str_set = set(self.eos_token_str)    # set for some more efficient operation\n\n        # load_model method in each subclass should set the following parameters:\n        self.model = None\n        self.tokenizer = None\n        self.cache = None\n        self.draft_model = None\n        self.draft_cache = None\n        self.processor = None       # processor is for processing of vision input\n        self.eos_token_ids = None\n\n        # placeholder\n        # pipeline is a wrapper for model so that generation can be more standardized\n        # this is because each backend expose different interface\n        self.pipeline = None\n\n        # format enforcer\n        self.formatter = FormatEnforcer()\n\n        # standard tool template\n        self.TOOL_THINKING = THINKING_TEMPLATE[\"tool_necessity_evaluation\"]\n        self.TOOL_FORCE_THINKING = THINKING_TEMPLATE[\"tool_forced_evaluation\"]\n\n        # standard modalities\n        # this set should contain either \"text\", \"image\", \"video\"\n        self.modalities = {\"text\"}     # TODO current backend just assume that image is supported\n\n    ## *************** the following method must be implemented by each backend ********\n    @property\n    def support_concurrency(self) -> bool:\n        \"\"\"\n        whether this backend/ model support concurrent request\n        \"\"\"\n        return False\n\n    @property\n    def support_tool(self) -> bool:\n        \"\"\"\n        whether this backend/ model support format enforcement for tool\n        \"\"\"\n        return True\n\n    @abstractmethod\n    def load_model(self):\n        \"\"\"Load the model, tokenizer, cache, and optional processor.\"\"\"\n        pass\n\n    @abstractmethod\n    def generate_eos_tokens_id(self) -> List[int]:\n        \"\"\"Generate the end-of-sequence token IDs.\"\"\"\n        pass\n\n    @abstractmethod\n    async def generate(\n        self,\n        prompt: str,\n        gen_queue: Union[GenQueue, QueueContext, List[QueueContext]],\n        request: Optional[Request] = None,\n        gen_type: Union[str, GenStart] = \"text\", # the generated result will be store to this queue\n        temperature: float = 0.01,\n        top_p: float = 0.8,\n        formatter: FormatterBuilder | TokenEnforcerTokenizerData = None,\n        stop_words: Union[List[str], str] = None,\n        prefix_strings: Optional[Union[str, List[str]]] = None,\n        banned_strings: list[str] | None = None,\n        max_tokens: int = None,\n        quiet: bool = False,\n        messages: List = None,  # query.message is used for multimodal\n        video: List[VideoFrame] = None,\n        stop_event: asyncio.Event = None,\n        **kwargs,\n    ) -> (str, GenerationStats):\n        pass\n\n    @property\n    def support_video(self) -> bool:\n        \"\"\"\n        whether this backend/ model support concurrent request\n        \"\"\"\n        return \"video\" in self.modalities\n\n    @property\n    def support_format_enforcer(self) -> bool:\n        return True\n\n    @property\n    def video_token_by_backend(self) -> str:\n        return \"\"\n\n    ## ******************************************************************************\n    def validate_video_support(self, query: ChatMLQuery):\n        \"\"\"\n        Validate if the current model have video support.\n        Convert video frame to base64 images if the current model does not support video.\n        The converted images will be attached to the last message's content.\n        After conversion, the query.video is cleared.\n        \"\"\"\n        # If no video frames exist, nothing to do.\n        # If no messages, nothing to do\n        # If already support video, nothing to do\n        if not query.video or not query.messages or self.support_video:\n            # add in video token if there is video\n            if query.video and self.video_token_by_backend:\n                last_message = query.messages[-1]\n\n                if isinstance(last_message.content, str):\n                    last_message.content += self.video_token_by_backend\n                elif isinstance(last_message.content, list):\n                    last_message.content.append(\n                        MultiModalTextContent(type=\"text\", text=self.video_token_by_backend)\n                    )\n\n            return query\n\n        # raise error if image is not support\n        if query.video and \"image\" not in self.modalities:\n            raise Exception(\"Error while generating chat response: \" + str(e))\n\n        # Convert all video frames to base64-encoded PNG Data URIs.\n        # We assume query.video is a list of VideoFrame objects.\n        logger.debug(\"Converting video frame to base64 images\")\n        base64_images = VideoFrameCollection.convert_frames_to_base64(query.video)\n\n        # If there are messages, update the last message.\n        last_message = query.messages[-1]\n\n        # If the content is a string, convert it into a list.\n        if isinstance(last_message.content, str):\n            # Preserve existing text if any.\n            if last_message.content:\n                last_message.content = [MultiModalTextContent(type=\"text\", text=last_message.content)]\n            else:\n                last_message.content = []\n\n        # Append each base64 image as a new image content.\n        for img in base64_images:\n            last_message.content.append(\n                MultiModalImageContent(\n                    type=\"image_url\",\n                    image_url=MultiModalImageContent.ImageDetail(\n                        url=img,\n                        detail=\"high\"\n                    )\n                )\n            )\n\n        # Remove video frames from the query.\n        query.video = None\n\n        return query\n\n    async def chat(\n        self,\n        query: ChatMLQuery,\n        prompt_eng: PromptEngine,\n        gen_queue: GenQueue,\n        request: Request,\n        stop_event: asyncio.Event = asyncio.Event()\n    ):\n        \"\"\"\n        This function will route the request to chat with tool or without tool accordingly\n        \"\"\"\n\n        query = self.validate_video_support(query)\n\n        # set the suitable method for tool calling\n        if (query.tools or query.tool_choice != \"none\") and self.support_tool:\n            chat_method = self.chat_with_tool_v2\n        else:\n            # enforce tool_choice to none if it is backend issue\n            if not self.support_tool:\n                logger.info(\"Tool not supported for this backend\")\n                query.tool_choice = \"none\"\n\n            chat_method = self.chat_no_tool\n\n        try:\n            await chat_method(\n                query=query,\n                prompt_eng=prompt_eng,\n                gen_queue=gen_queue,\n                request=request,\n                stop_event=stop_event,\n            )\n        except Exception as e:\n            if stop_event.is_set():\n                logger.info(\"Stop event is set, aborting chat\")\n            else:\n                logger.error(\"Error while generating response: \" + str(e))\n                raise Exception(\"Error while generating chat response: \" + str(e))\n        return True\n\n    async def chat_raw(\n        self,\n        prompt: str,\n        gen_queue: asyncio.Queue,\n        request: Request,\n        # stream: bool = False,\n        max_tokens: int = None,\n        quiet=False,    # to disable any logging, mostly used for initialization cache prefill\n        stop_event: asyncio.Event = asyncio.Event()\n    ):\n        \"\"\"\n        This function handle chat with input as a string prompt.\n        This is mostly used for internal generation of the engine where the input is only a string\n        \"\"\"\n        return await self.generate(\n            prompt,\n            max_tokens=max_tokens,\n            gen_queue=gen_queue,\n            quiet=quiet,\n            request=request,\n            stop_event=stop_event,\n        )\n\n    def validate_token_length(self, token_length: int):\n        \"\"\"\n        validate that token_length is within max sequence length\n        \"\"\"\n        # if max_seq_len == None meaning there is no token length yet\n        if self.max_seq_len and token_length > self.max_seq_len:\n            raise HTTPException(status_code=400, detail=f\"Token length exceeds max length of {self.max_seq_len}\")\n\n\n    async def chat_no_tool(\n        self,\n        query: ChatMLQuery,\n        prompt_eng: PromptEngine,\n        gen_queue, request: Request,\n        stop_event: asyncio.Event = None\n    ):\n\n        prompt = prompt_eng.get_prompt(\n            query,\n            thinking_template=query.thinking_template,\n            backend=self.backend\n        )\n\n        formatter_prefix_regex = self.formatter.regex(\n            query.regex_prefix_pattern,\n            backend=self.backend,\n            preference=query.guided_decoding_backend\n        ) if query.regex_prefix_pattern else None\n\n        formatter_regex = self.formatter.regex(\n            query.regex_pattern,\n            backend=self.backend,\n            preference=query.guided_decoding_backend\n        ) if query.regex_pattern else None\n\n        token_length_prompt = get_token_length(self.tokenizer, prompt)\n        self.validate_token_length(token_length_prompt)\n\n        # think template prompting\n        if query.thinking_template:\n            try:\n                thinking = Thinking(query.thinking_template)\n            except Exception as e:\n                raise HTTPException(status_code=400, detail=f\"thinking_template is not valid XML string\")\n\n            # Not returning thinking\n            thinking_queue = GenQueue()\n            if query.return_thinking:\n                # return thinking to front end\n                queue_group = [\n                    QueueContext.create(gen_queue=thinking_queue, include_GenEnd=True, include_GenStats=False),\n                    QueueContext.create(gen_queue=gen_queue, include_GenEnd=False, include_GenStats=False)\n                ]\n            else:\n                # not return thinking to front end\n                queue_group = [\n                    QueueContext.create(gen_queue=thinking_queue, include_GenEnd=True, include_GenStats=False),\n                ]\n\n            await self.generate(\n                prompt,\n                messages=query.messages,\n                gen_type=\"thinking\",\n                gen_queue=queue_group,\n                temperature=query.temperature,\n                top_p=query.top_p,\n                prefix_strings=f\"<{thinking.root_tag}>\",\n                stop_words=thinking.root_key_stop_words,\n                request=request,\n                stop_event=stop_event,\n                video=query.video,\n            )\n            thinking_response, _ = await get_response_from_queue(thinking_queue)\n\n            # Get the new prompt with thinking response\n            prompt = prompt_eng.get_prompt(\n                query,\n                thinking_template=query.thinking_template,\n                thinking_response=thinking_response,\n                backend=self.backend\n            )\n\n        # 1st response if there is regex to match the regex pattern\n        first_response = \"\"\n\n        if query.regex_prefix_pattern:\n            prefix_queue = GenQueue()\n            queue_group = [\n                QueueContext.create(gen_queue=prefix_queue, include_GenEnd=True, include_GenStats=False),\n                QueueContext.create(gen_queue=gen_queue, include_GenEnd=False, include_GenStats=False)\n            ]\n\n            await self.generate(\n                prompt,\n                messages=query.messages,\n                gen_queue=queue_group,\n                temperature=query.temperature,\n                top_p=query.top_p,\n                formatter=formatter_prefix_regex,\n                prefix_strings=query.prefix_strings,\n                request=request,\n                # stop_words=query.stop_words,\n                stop_event=stop_event,\n                video=query.video,\n            )\n\n            first_response, _ = await get_response_from_queue(prefix_queue)\n\n            # append generated content to the full prompt\n            prompt = prompt.strip() + first_response.strip()\n\n        # Final generation to return to client\n        # set prefix string\n        prefix_strings = None if query.regex_prefix_pattern else query.prefix_strings\n\n        # Handle Artifact mode: overwrite prefix_strings if in artifact mode\n        stop_words_to_use = query.stop_words\n        banned_strings = None\n        if query.artifact and query.artifact == \"Fast\":\n            prefix_strings = None\n            manual_prefix_string = \"<answer>\\n \"\n            prompt += manual_prefix_string\n\n            # ban XML comment format which could mess up the parsing of output\n            banned_strings = [\"<![CDATA[\", \"<!--\"]\n\n            # add the stopword for artifact tag to the answer\n            if isinstance(stop_words_to_use, list):\n                stop_words_to_use.append(\"</answer>\")\n            elif isinstance(stop_words_to_use, str):\n                stop_words_to_use = [stop_words_to_use, \"</answer>\"]\n            else:\n                stop_words_to_use = \"</answer>\"\n\n            # add the initial string as prefix_strings can not be used together with banned_strings\n            chunk = GenText(content=manual_prefix_string)\n            gen_queue.put_nowait(chunk)\n\n        await self.generate(\n            prompt=prompt,\n            messages=query.messages,\n            gen_queue=gen_queue,\n            **{\n                'temperature': query.temperature,\n                'top_p': query.top_p,\n                'formatter': formatter_regex,\n                'stop_words': stop_words_to_use,\n                'max_tokens': query.max_tokens,\n                'prefix_strings': prefix_strings,  # already generated as part of the prefix string\n                'banned_strings': banned_strings,\n                'request': request,\n                'stop_event': stop_event,\n                'video': query.video,\n            }\n        )\n\n    async def chat_with_tool(\n        self,\n        query: ChatMLQuery,\n        prompt_eng,\n        gen_queue,\n        request: Request,\n        stop_event: asyncio.Event = None\n    ):\n        # use_tool marker\n        use_tool_bool = False  # this will be set to True if tool is used\n        fall_back_bool = False  # this will decide if fallback generation with regex enforcement is required\n\n        # tool class have the method to handle converting processing or tool requirement, schema and response\n        tool_handler = Tools(\n            prompt_eng=prompt_eng,\n            tools=query.tools,\n            tool_choice=query.tool_choice,\n        )\n\n        # substitute the actual list of tool to the thinking template\n        tool_thinking_queue = GenQueue()\n\n        tool_thinking_to_use = self.TOOL_THINKING\n        if query.tool_choice != \"auto\":\n            tool_thinking_to_use = self.TOOL_FORCE_THINKING\n\n        tool_thinking_formatted = tool_thinking_to_use.xml.format_map(\n            {\"fstring_available_tools\": tool_handler.tool_name_list}\n        )\n\n        # perform generation with tool thinking to evaluate if it is necessity to call a tool\n        prompt = prompt_eng.get_prompt(\n            query,\n            pydantic_tool_dict=tool_handler.tool_dict,\n            thinking_template=tool_thinking_formatted,\n            answer_format_schema=False,\n            backend=self.backend\n            # leading_prompt=leading_prompt,\n        )\n\n        await self.generate(\n            prompt,\n            messages=query.messages,\n            gen_queue=tool_thinking_queue,\n            temperature=query.temperature,\n            top_p=query.top_p,\n            stop_words=tool_thinking_to_use.root_key_stop_words,\n            prefix_strings=f\"<{tool_thinking_to_use.root_tag}>\",\n            request=request,\n            # formatter=formatter_regex  # no longer enforce format\n            stop_event=stop_event,\n            video=query.video,\n        )\n\n        # evaluate tool usage necessity\n        tool_thinking_response, _ = await get_response_from_queue(tool_thinking_queue)\n\n        # see if llm able to generate the xml format correctly\n        if query.tool_choice != \"auto\":\n            use_tool_bool = True\n        else:\n            try:\n                # parse the xml object\n                tool_thinking_response_dict = Thinking.parse_xml_to_dict(tool_thinking_response)\n                tool_decision = tool_thinking_response_dict[tool_thinking_to_use.root_tag][\"final_decision\"][\n                    \"is_tool_needed\"]  # TODO implement a less hardcoding way\n\n                if tool_decision.lower().strip() == \"yes\":\n                    use_tool_bool = True\n                elif tool_decision.lower().strip() == \"no\":\n                    use_tool_bool = False\n                else:\n                    # the format was not enforce, to perform fall back check\n                    fall_back_bool = True\n\n            except Exception as e:\n                logger.error(f\"XML parsing failed: {e}\")\n                # Fallback: check for the presence of Yes/No in the raw XML\n                yes_pattern = r'<is_tool_needed>\\s*Yes\\s*</is_tool_needed>'\n                no_pattern = r'<is_tool_needed>\\s*No\\s*</is_tool_needed>'\n\n                if re.search(yes_pattern, tool_thinking_response, re.IGNORECASE):\n                    use_tool_bool = True\n                elif re.search(no_pattern, tool_thinking_response, re.IGNORECASE):\n                    use_tool_bool = False\n                else:\n                    fall_back_bool = True\n\n        logger.info(tool_thinking_response)\n\n        # Fall back plan\n        fall_back_prompt = \"\"\n        if fall_back_bool:\n            logger.info(\"Tool Analysis fallback\")\n            # generate fall back response with regex enforcement:\n            fall_back_prompt = (\"\\n Fill in the blank in below sentence with either 'needed' or 'not needed'\"\n                                \"In summary, tool calling is {blank}\\n\"\n                                \"Answer: blank=\")\n            prompt += tool_thinking_response + fall_back_prompt\n\n            # perform generation with tool thinking to evaluate if it is necessity\n            tool_thinking_queue_fallback = GenQueue()\n\n            formatter_regex = self.formatter.regex('(needed|not needed)', backend=self.backend, preference=query.guided_decoding_backend)\n\n            await self.generate(\n                prompt,\n                messages=query.messages,\n                gen_queue=tool_thinking_queue_fallback,\n                temperature=query.temperature,\n                top_p=query.top_p,\n                request=request,\n                # prefix_strings=\"n\",\n                # stop_words=TOOL_THINKING.root_key_stop_words,\n                formatter=formatter_regex,  # no longer enforce format\n                stop_event = stop_event,\n                video=query.video,\n            )\n\n            # evaluate tool usage necessity\n            tool_thinking_decision_fallback, _ = await get_response_from_queue(tool_thinking_queue_fallback)\n\n            # decide if tool call is required\n            if \"not needed\" in tool_thinking_decision_fallback.lower():\n                use_tool_bool = False\n            elif \"needed\" in tool_thinking_decision_fallback.lower():\n                use_tool_bool = True\n\n        # USE TOOL\n        if use_tool_bool:\n            # create the pydantic schema to enforce generation\n            tool_combined_pydantic_lmfe = create_function_models_v2(tool_handler.tool_dict)\n\n            class ToolCalling_LMFE(ClassSchema):\n                \"\"\" The format to call one or multiple tools \"\"\"\n                functions_calling: List[Union[tuple(tool_combined_pydantic_lmfe)]] = []\n\n            # create the pydantic schema to enforce generation for formatron which use ClassSchema\n            tool_combined_pydantic_formatron = create_function_models_formatron(tool_handler.tool_dict_formatron)\n            class ToolCalling_formatron(ClassSchema):\n                \"\"\" The format to call one or multiple tools \"\"\"\n                functions_calling: List[Union[tuple(tool_combined_pydantic_formatron)]] = []\n\n            formatter_json = self.formatter.json(\n                pydantic_model_lmfe=ToolCalling_LMFE,\n                pydantic_model_formatron=ToolCalling_formatron,\n                backend=self.backend,\n                preference = query.guided_decoding_backend\n            )\n\n            # Experiment feature, formulate function calling as python programming. Which is more natural than a random Json output as part of conversation\n            tool_as_code_prompt = \"\"\"\ndef run_function(arg_dict):\n    function_calls = arg_dict[\"functions_calling\"]\n\n    if function_calls == []:\n        print(\"No function/tool calling needed\")\n        return\n\n    for call in function_calls:\n        function_name = call[\"name\"]\n        arguments = call[\"arguments\"]\n        globals()[function_name](**arguments)\n\n# Perform function calling if need to\narg_dict = \"\"\"\n\n            # get final prompt\n            prompt = prompt_eng.get_prompt(\n                query,\n                thinking_template=self.TOOL_THINKING.xml,\n                thinking_response=tool_thinking_response,\n                pydantic_tool_dict=tool_handler.tool_dict,\n                answer_format_schema=True,\n                leading_prompt=(\n                    f\"{fall_back_prompt}\\n\"\n                    'Now i will convert my answer above into \"functions_calling\" format by continuing this continue this code.\\n'\n                    f\"{tool_as_code_prompt}\"\n                ),\n                backend=self.backend,\n            )\n            logger.info(prompt)\n\n            # generate\n            await self.generate(\n                prompt,\n                messages=query.messages,\n                gen_queue=gen_queue,\n                gen_type=GenStart(gen_type=\"tool\"),\n                temperature=query.temperature,\n                top_p=query.top_p,\n                # stop_words=TOOL_THINKING.root_key_stop_words,\n                prefix_strings=['{\\n \"functions_calling\": ['],\n                formatter=formatter_json,\n                max_tokens=query.max_tokens,\n                request=request,\n                stop_event=stop_event,\n                video=query.video,\n            )\n\n        # NOT USE TOOL\n        if not use_tool_bool:\n            prompt = prompt_eng.get_prompt(\n                query,\n                backend=self.backend,\n            )\n\n            if query.tool_choice == \"auto\":\n                # Normal generation\n                await self.generate(\n                    prompt,\n                    messages=query.messages,\n                    gen_queue=gen_queue,\n                    gen_type=GenStart(gen_type=\"text\"),\n                    temperature=query.temperature,\n                    prefix_strings=query.prefix_strings,\n                    max_tokens=query.max_tokens,\n                    request=request,\n                    stop_event=stop_event,\n                    video=query.video,\n                )\n            else:\n                # tool choice is forced -> return empty tool calling\n                gen_queue.put_nowait(GenStart(gen_type=\"tool\"))\n                gen_queue.put_nowait(GenText(content='{\"functions_calling\":[]}'))\n                gen_queue.put_nowait(GenerationStats())\n                gen_queue.put_nowait(GenEnd())\n\n\n    async def _tool_calling_task(\n        self,\n        prompt_eng: PromptEngine,\n        query:ChatMLQuery,\n        request: Request,\n        tool_handler: Tools,\n        tool_as_code_prompt: str\n    ) -> ToolCallV2:\n\n        # queue for generating output\n        gen_queue_dynamic = [GenQueueDynamic()]\n\n        # create the pydantic schema to enforce generation\n        tool_combined_pydantic_lmfe = create_function_models_v2(tool_handler.tool_dict)\n\n        class ToolCalling_LMFE(ClassSchema):\n            \"\"\" The format to call one or multiple tools \"\"\"\n            functions_calling: List[Union[tuple(tool_combined_pydantic_lmfe)]] = []\n\n        # create the pydantic schema to enforce generation for formatron which use ClassSchema\n        tool_combined_pydantic_formatron = create_function_models_formatron(\n            tool_handler.tool_dict_formatron)\n\n        class ToolCalling_formatron(ClassSchema):\n            \"\"\" The format to call one or multiple tools \"\"\"\n            functions_calling: List[Union[tuple(tool_combined_pydantic_formatron)]] = []\n\n        formatter_json = self.formatter.json(\n            pydantic_model_lmfe=ToolCalling_LMFE,\n            pydantic_model_formatron=ToolCalling_formatron,\n            backend=self.backend,\n            preference=query.guided_decoding_backend\n        )\n\n        # shar prompt with tool decision for better KV\n        tool_answer_as_code = \"\"\"\n    \n# Perform function calling if need to by continuing this code. Return {\"functions_calling\": []} if function calling is not needed.\nresponse(to=\"function\", arg_dict=\"\"\"\n\n        if query.tool_instruction_position == \"prefix\":\n            _prefix_prompt = tool_as_code_prompt\n            _leading_prompt = tool_answer_as_code\n        else:\n            _prefix_prompt = \"\"\n            _leading_prompt = tool_as_code_prompt + tool_answer_as_code\n\n        prompt = prompt_eng.get_prompt(\n            query,\n            pydantic_tool_dict=tool_handler.tool_dict,\n            # pydantic_tool_code=tool_handler.tool_def_as_code,\n            answer_format_schema=True,\n            backend=self.backend,\n            prefix_prompt=_prefix_prompt,\n            leading_prompt=_leading_prompt,\n        )\n\n        stop_event = asyncio.Event()\n\n        generate_kwargs = {\n            \"prompt\": prompt,\n            \"messages\": query.messages,\n            \"gen_queue\": gen_queue_dynamic,\n            \"gen_type\": GenStart(gen_type=\"tool\"),\n            \"temperature\": query.temperature,\n            \"top_p\": query.top_p,\n            \"prefix_strings\": ['{\\n \"functions_calling\": ['],\n            \"formatter\": formatter_json,\n            \"max_tokens\": query.max_tokens,\n            \"request\": request,\n            \"stop_event\": stop_event,\n            \"video\": query.video\n        }\n\n        return ToolCallV2(\n            gen_dynamic_queue=gen_queue_dynamic,\n            stop_event=stop_event,\n            generate_kwargs=generate_kwargs\n        )\n\n    async def _tool_decision_task(\n        self,\n        prompt_eng: PromptEngine,\n        query: ChatMLQuery,\n        request: Request,\n        tool_handler: Tools,\n        tool_as_code_prompt: str\n    ) -> Tuple[ToolCallV2, Callable[[str], bool]]:\n        # queue for generating output\n        gen_queue_dynamic = [GenQueueDynamic()]\n\n        # create prompt\n        # reuse tool call to share the kv cache as possible\n        _tool_answer_prefix = \"to=\"\n        _tool_thinking_length_guide = \"\"\n        if query.tool_call_thinking:\n            _tool_answer_prefix = \"thinking=\"\n            _tool_thinking_length_guide = \"# briefly think if tool thinking is needed. Never redo a tool with same arguments.\"\n\n        tool_decision_answer_as_code_prompt = f\"\"\"\n{_tool_thinking_length_guide}\nresponse(\"\"\" + _tool_answer_prefix\n\n        def tool_decision_check_fn(tool_decision_answer: str) -> bool:\n            \"\"\" return True if tool usage is true\"\"\"\n            if tool_decision_answer.lower().endswith('\"function\"') or tool_decision_answer.lower().endswith(\"'function'\"):\n                logger.info(f\"Tool decision: {tool_decision_answer}\")\n                return True\n            else:\n                return False\n\n        def tool_decision_check_with_thinking_fn(tool_decision_answer: str) -> bool:\n            \"\"\" return True if tool usage is true\"\"\"\n            if tool_decision_answer.lower().endswith('\"function\"') or tool_decision_answer.lower().endswith(\"'function'\"):\n                logger.info(f\"Tool decision: internal_thinking: {tool_decision_answer}\")\n                return True\n            else:\n                return False\n\n        if query.tool_instruction_position==\"prefix\":\n            _prefix_prompt = tool_as_code_prompt\n            _postfix_prompt = tool_decision_answer_as_code_prompt\n        else:\n            _prefix_prompt = \"\"\n            _postfix_prompt = tool_as_code_prompt + tool_decision_answer_as_code_prompt\n\n        prompt = prompt_eng.get_prompt(\n            query,\n            pydantic_tool_dict=tool_handler.tool_dict,\n            # pydantic_tool_code=tool_handler.tool_def_as_code,\n            answer_format_schema=True,\n            backend=self.backend,\n            prefix_prompt=_prefix_prompt,\n            leading_prompt=_postfix_prompt\n        )\n\n        formatter_regex = self.formatter.regex(\n            regex_pattern=r'\"user\"|\"function\"',\n            backend=self.backend,\n            preference=query.guided_decoding_backend\n        )\n\n        stop_event = asyncio.Event()\n\n        generate_kwargs = {\n            \"prompt\": prompt,\n            \"messages\": query.messages,\n            \"gen_queue\": gen_queue_dynamic,\n            \"gen_type\": GenStart(gen_type=\"tool\"),\n            \"temperature\": query.temperature,\n            \"top_p\": query.top_p,\n            \"prefix_strings\": '\"',\n            \"formatter\": formatter_regex if not query.tool_call_thinking else None,\n            \"max_tokens\": query.tool_call_thinking_token,\n            \"request\": request,\n            \"stop_event\": stop_event,\n            \"stop_words\": ['\"user\"', '\"function\"', \"'user'\", \"'function'\"],\n            \"video\": query.video\n        }\n\n        tool_decision_check_fn_to_use = tool_decision_check_fn if query.tool_call_thinking else tool_decision_check_with_thinking_fn\n\n        return ToolCallV2(\n            gen_dynamic_queue=gen_queue_dynamic,\n            stop_event=stop_event,\n            generate_kwargs=generate_kwargs\n        ), tool_decision_check_fn_to_use\n\n\n    async def _no_tool_task(\n        self,\n        prompt_eng: PromptEngine,\n        query:ChatMLQuery,\n        request: Request\n    ) -> ToolCallV2:\n\n        gen_queue_dynamic = [GenQueueDynamic()]\n\n        prompt = prompt_eng.get_prompt(\n            query,\n            backend=self.backend,\n        )\n\n        stop_event = asyncio.Event()\n\n        generate_kwargs = {\n            \"prompt\": prompt,\n            \"messages\": query.messages,\n            \"gen_queue\": gen_queue_dynamic,\n            \"gen_type\": GenStart(gen_type=\"text\"),\n            \"temperature\": query.temperature,\n            \"top_p\": query.top_p,\n            \"prefix_strings\": query.prefix_strings,\n            \"max_tokens\": query.max_tokens,\n            \"request\": request,\n            \"stop_event\": stop_event,\n            \"video\": query.video,\n        }\n\n        return ToolCallV2(\n            gen_dynamic_queue=gen_queue_dynamic,\n            stop_event=stop_event,\n            generate_kwargs=generate_kwargs\n        )\n\n\n\n    async def chat_with_tool_v2(\n        self,\n        query: ChatMLQuery,\n        prompt_eng: PromptEngine,\n        gen_queue: GenQueueDynamic,\n        request: Request,\n        stop_event: asyncio.Event = None\n    ):\n        \"\"\"\n        handle tool calling llm generation\n        main purpose is to handle auto model of generation by generate both tool and non tool usage at the same time\n        dynamically swap to the best answer by LLM\n        This will reduce the wait time from generating sequentially\n        \"\"\"\n\n        # tool class have the method to handle converting processing or tool requirement, schema and response\n        tool_handler = Tools(\n            prompt_eng=prompt_eng,\n            tools=query.tools,\n            tool_choice=query.tool_choice,\n        )\n\n        # create prompt\n        _tool_thinking_fn_header = \"\"\n        _tool_answer_prefix = \"to=\"\n        if query.tool_call_thinking:\n            _tool_thinking_fn_header = 'internal_thinking: str=\"\",'\n            _tool_answer_prefix = \"thinking=\"\n\n        tool_as_code_prompt = \"\"\"\nFunction/ Tool calling Instruction:\n# Use Function calling if:\n- It is needed to answer user question.\n- Be conservative and only use function calling if it is necessary and suitable.\n\n# Reply directly to user if:\n- It is uncertain if user wants function calling.\n- Function calling is not related to user's question.\n- Clarification is needed. \n- Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.\n\nWhen a tool/ function is requested, it's result will be run by the system and be returned with reference to the tool call if available\ne.g. Result of tool call reference id <tool_call_id>.\n```python\ndef run_function(arg_dict):\n    function_calls = arg_dict[\"functions_calling\"]\n\n    if function_calls == []:\n        print(\"No function/tool calling needed\")\n        return\n\n    for call in function_calls:\n        function_name = call[\"name\"]\n        arguments = call[\"arguments\"]\n        globals()[function_name](**arguments)\n        \nallow_functions = [\"\"\" + tool_handler.tool_name_list + \"\"\"]\n\ndef response(\"\"\" + _tool_thinking_fn_header + \"\"\"to: Literal[\"user\",\"function\"], arg_dict: Optional[List[]]=[]):\n   if to==\"user\":\n       # NO function calling needed\n       break  # exit for a normal answer\n   elif to==\"function\" and arg_dict!=[]:\n       run_function(arg_dict)\n   else:\n      raise Exception(\"Either response to user without function calling or function calling is required.\")\n```\nExample:\n# No function calling needed:\nresponse(thinking=\"because... so function calling is not required \", to=\"user\")\nmy answer is...\n\n# Function calling needed:\nresponse(thinking=\"because... so function calling is required\", to=\"function\", arg_dict={\n  \"functions_calling\": [{\n      \"name\": \"function_name\",\n      \"arguments\": {\"argument_name\": \"value\"}\n    }\n]}) \n# answer to user is prohibited if using function calling\n\n# Example of follow-up answer after function calling result provided by user\nUser: what is 2^3?\nAssistant:                                                                                                                                                             \npower_number_tool({number: 2, power: 3})\n8\nAssistant: 2^3 is 8\n---\nIMPORTANT: 'Request for tool call with reference id' and 'Result of tool call reference id' are auto populated by the system for reference\n            DO NOT output Request for tool call with reference id by yourself\nEnd of Function Calling Instruction\n---\n\"\"\"\n\n\n        tool_call = await self._tool_calling_task(\n            prompt_eng=prompt_eng,\n            query=query,\n            request=request,\n            tool_handler=tool_handler,\n            tool_as_code_prompt=tool_as_code_prompt\n        )\n\n        tool_decision, tool_decision_check_fn = await self._tool_decision_task(\n            prompt_eng=prompt_eng,\n            query=query,\n            request=request,\n            tool_handler=tool_handler,\n            tool_as_code_prompt=tool_as_code_prompt\n        )\n\n\n        no_tool = await self._no_tool_task(\n            prompt_eng=prompt_eng,\n            query=query,\n            request=request\n        )\n\n        tasks = []  # to track the task running\n\n        tool_decision_task = None\n        tool_task = None\n        no_tool_task = None\n\n\n        # ensure result is out for decision\n        tool_decision_outcome: Literal[\"user\", \"function\"] = \"function\"\n\n        if self.support_concurrency:\n            # tool task will always be run. It is assumed that if this function is called, tool generation is required\n            tool_task = asyncio.create_task(self.generate(**tool_call.generate_kwargs))\n            tasks.append(tool_task)\n\n            # if mode is auto, run the tool usage decision generation\n            if query.tool_choice == \"auto\":\n                tool_decision_task = asyncio.create_task(self.generate(**tool_decision.generate_kwargs))\n                tasks.append(tool_decision_task)\n\n                no_tool_task = asyncio.create_task(self.generate(**no_tool.generate_kwargs))\n                tasks.append(no_tool_task)\n\n                tool_decision_outcome, _ = await get_response_from_queue(tool_decision.gen_dynamic_queue)\n\n\n            if tool_decision_check_fn(tool_decision_outcome):\n                logger.info(\"Tool auto decision: function\")\n\n                _has_tool = True\n\n                # check first if tool managed to generate\n                if tool_call.gen_dynamic_queue[0].qsize() < 3:\n                    await asyncio.sleep(0.3)    # wait for 0.3 more second\n\n                    if tool_call.gen_dynamic_queue[0].qsize() < 3:\n                        _has_tool = False\n\n                if _has_tool:\n                    if no_tool:\n                        no_tool.stop_event.set()\n\n                    # swap queue for output to function calling\n                    gen_queue.swap(tool_call.gen_dynamic_queue[0])\n                else:\n                    # tool not managed to generate, fall back to standard reply\n                    tool_call.stop_event.set()\n\n                    # swap queue non function calling\n                    gen_queue.swap(no_tool.gen_dynamic_queue[0])\n\n            else:\n                logger.info(\"Tool auto decision: user\")\n                if tool_call:\n                    tool_call.stop_event.set()\n\n                # swap queue non function calling\n                gen_queue.swap(no_tool.gen_dynamic_queue[0])\n\n            # Wait for the remaining tasks to complete (if needed)\n            try:\n                await asyncio.gather(*tasks)\n            except Exception as e:\n                logger.error(f\"Error in one of the tasks: {e}\")\n\n        else:   # non concurrency backend\n            # first generate what is the decision regarding whether use tool or not\n            tool_decision_task = await self.generate(**tool_decision.generate_kwargs)\n            tool_decision_outcome, _ = await get_response_from_queue(tool_decision.gen_dynamic_queue)\n\n            # handle tool/ non tool generation\n            if tool_decision_check_fn(tool_decision_outcome):\n                logger.info(\"Tool auto decision: function\")\n                tool_task = asyncio.create_task(self.generate(**tool_call.generate_kwargs))\n\n                # swap queue for output to function calling\n                gen_queue.swap(tool_call.gen_dynamic_queue[0])\n            else:\n                logger.info(\"Tool auto decision: user\")\n                no_tool_task = asyncio.create_task(self.generate(**no_tool.generate_kwargs))\n\n                # swap queue non function calling\n                gen_queue.swap(no_tool.gen_dynamic_queue[0])\n\n\n\n\n"}
{"type": "source_file", "path": "src/gallama/backend/embedding/embedding.py", "content": "from ...data_classes.data_class import (\n    EmbeddingRequest,\n    EmbeddingResponse,\n    EmbeddingObject,\n    ModelSpec\n)\nfrom ...utils.utils import floats_to_base64\nfrom typing import Dict\nimport logging\nfrom infinity_emb import EngineArgs, AsyncEmbeddingEngine\n\nfor handler in logging.root.handlers[:]:\n    logging.root.removeHandler(handler)      # disable logging from infinity\n\n\nclass EmbeddingModel:\n    def __init__(self, model_spec: ModelSpec):\n\n        self.model_id = model_spec.model_id\n        self.model_name = model_spec.model_name\n        self.model = self.load_embedding_model()\n\n    def get_visible_gpu_indices(self) -> str:\n        \"\"\"\n        Generate a string of GPU indices based on allocated GPUs.\n        If no GPUs are specified, return all available GPU indices.\n\n        Returns:\n            str: A comma-separated string of GPU indices with allocated VRAM,\n                 or all available GPU indices if none are specified.\n        \"\"\"\n        if self.gpus is None or self.gpus == \"auto\":\n            import torch\n            return ','.join(str(i) for i in range(torch.cuda.device_count()))\n\n        if all(vram == 0 for vram in self.gpus):\n            return \"\"  # No GPUs allocated\n\n        visible_devices = [str(i) for i, vram in enumerate(self.gpus) if vram > 0]\n        return ','.join(visible_devices)\n\n    def load_embedding_model(self):\n        # load model\n        emb_model = AsyncEmbeddingEngine.from_args(\n            EngineArgs(\n                model_name_or_path=self.model_id,\n                engine=\"torch\",\n                embedding_dtype=\"float32\",\n                bettertransformer=False,    # not compatible with many model\n                dtype=\"auto\"\n            )\n        )\n\n        return emb_model           # TODO support multiple model\n\n    async def text_embeddings(\n        self,\n        query: EmbeddingRequest,\n    ) -> EmbeddingResponse:\n\n        # initialize the input\n        input_texts = []\n        if isinstance(query.input, str):\n            input_texts = [query.input]\n        elif isinstance(query.input, list):\n            input_texts = query.input\n        else:\n            raise Exception(\"Data not supported\")\n\n        async with self.model:\n            embeddings, usage = await self.model.embed(sentences=input_texts)\n\n        # create the return embedding data\n        emb_response_list = []\n        for idx, text_emb in enumerate(embeddings):\n            emb_response_list.append(\n                EmbeddingObject(\n                    index=idx,\n                    embedding=text_emb if query.encoding_format == \"float\" else floats_to_base64(text_emb),\n                )\n            )\n\n        # return the response\n        return EmbeddingResponse(\n            model=self.model_name,\n            usage=EmbeddingResponse.Usage(\n                prompt_tokens=usage,\n                total_tokens=usage,\n            ),\n            data=emb_response_list,\n        )\n"}
{"type": "source_file", "path": "src/gallama/api_response/stream_parser.py", "content": "import re\nfrom typing import List, Tuple, Optional, Union\nfrom gallama.data_classes.data_class import TextTag, ArtifactTag\n\n\nclass StreamParser:\n    def __init__(self):\n        self.buffer = \"\"\n        self.current_element = None\n        self.current_tag = None\n        self.current_content = \"\"\n        self.MALFORMED_CHECK_LENGTH = 300\n        self.in_answer_tag = False\n        # self.xml_prefix = \"```xml\\n\"\n        self.xml_prefix = \"\"\n        self.root_key = \"<answer>\"\n        self.full_root_key = f\"{self.xml_prefix}{self.root_key}\"\n        self.tag_pattern = re.compile(r'(<artifact\\s+(?:(?:identifier|type|title|language)=\"[^\"]*\"\\s*)*>)|(<text>)')\n        self.comment_pattern = re.compile(r'<!--.*?-->', re.DOTALL)\n\n    def process_stream(self, new_data: str) -> List[Tuple[Union[TextTag, ArtifactTag], str]]:\n        self.buffer += new_data\n        self.remove_comments()\n\n        results = []\n\n        while True:\n            if not self.in_answer_tag:\n                start_index = self.buffer.find(self.full_root_key)\n                if start_index != -1:\n                    self.buffer = self.buffer[start_index + len(self.full_root_key):]\n                    self.in_answer_tag = True\n                elif len(self.buffer) >= self.MALFORMED_CHECK_LENGTH:\n                    results.append((TextTag(), self.buffer))\n                    self.buffer = \"\"\n                else:\n                    break\n\n            if self.current_element is None:\n                match = self.tag_pattern.search(self.buffer)\n                if match:\n                    element_type = \"artifact\" if match.group(1) else \"text\"\n                    start_index = match.start()\n                    tag_length = match.end() - match.start()\n\n                    self.current_element = element_type\n                    if element_type == \"artifact\":\n                        self.current_tag = self._parse_artifact_tag(self.buffer[start_index:match.end()])\n                    else:\n                        self.current_tag = TextTag()\n\n                    self.buffer = self.buffer[start_index + tag_length:]\n                    self.current_content = \"\"\n                elif len(self.buffer) >= self.MALFORMED_CHECK_LENGTH:\n                    results.append((TextTag(), self.buffer))\n                    self.buffer = \"\"\n                else:\n                    break\n            elif len(self.buffer) <= len(f'</{self.current_element}>'):\n                break\n            else:\n                end_tag = f'</{self.current_element}>'\n                end_index = self.buffer.find(end_tag)\n                if end_index != -1:\n                    content = self.buffer[:end_index]\n                    results.append((self.current_tag, content))\n                    self.buffer = self.buffer[end_index + len(end_tag):]\n                    self.current_element = None\n                    self.current_tag = None\n                    self.current_content = \"\"\n                else:\n                    if self.buffer:\n                        content_chunk = self.buffer[:len(self.buffer) - len(end_tag)]\n                        self.buffer = self.buffer[len(self.buffer) - len(end_tag):]\n                        results.append((self.current_tag, content_chunk))\n                    break\n\n        return results\n\n    def remove_comments(self):\n        comment_pattern = re.compile(r'<!--.*?-->', re.DOTALL)\n        self.buffer = comment_pattern.sub('', self.buffer)\n\n    def _parse_artifact_tag(self, tag_content: str) -> ArtifactTag:\n        attributes = re.findall(r'(\\w+)=\"([^\"]*)\"', tag_content)\n        attr_dict = dict(attributes)\n        return ArtifactTag(\n            artifact_type=attr_dict.get('type', 'code'),\n            identifier=attr_dict.get('identifier', ''),\n            title=attr_dict.get('title', ''),\n            language=attr_dict.get('language')\n        )\n\n    def get_current_state(self) -> Tuple[Optional[str], str]:\n        return self.current_element, self.current_content\n\n    def parse_full_response(self, response: str) -> List[Tuple[Union[TextTag, ArtifactTag], str]]:\n        if not response.startswith(self.xml_prefix):\n            response = f\"{self.xml_prefix}{response}\"\n        response = self.comment_pattern.sub('', response)\n        return self.process_stream(response)"}
{"type": "source_file", "path": "src/gallama/backend/__init__.py", "content": "# LLM\nfrom .llm import ModelExllama, ModelLlamaCpp, ModelTransformers\n\n# Embedding\nfrom .embedding.embedding import EmbeddingModel"}
{"type": "source_file", "path": "src/gallama/api_response/chat_response.py", "content": "from fastapi import Request\nfrom ..data_classes.data_class import (\n    ChatMLQuery,\n    ChatCompletionResponse,\n    Choice,\n    ChatMessage,\n    UsageResponse,\n    OneTool,\n    ToolCallResponse,\n    StreamChoice,\n    CompletionResponse,\n    CompletionStreamResponse,\n    CompletionChoice,\n    ChoiceDelta,\n    ChoiceDeltaToolCall,\n    ChoiceDeltaToolCallFunction,\n    TextTag,\n)\nfrom ..data_classes.generation_data_class import (\n    GenerationStats,\n    GenQueue,\n    GenText,\n    GenEnd,\n    GenStart,\n    GenQueueDynamic\n)\n\nfrom .stream_parser import StreamParser\nfrom typing import AsyncIterator, List\nfrom gallama.utils.utils import get_response_uid, get_response_tool_uid\nfrom gallama.logger.logger import logger\nfrom pydantic.json import pydantic_encoder\nimport time\nimport json\nimport asyncio\n\n\nasync def get_response_from_queue(\n    gen_queue: GenQueue | GenQueueDynamic | List[GenQueue|GenQueueDynamic],\n    request: Request = None,\n):\n    \"\"\" function to get the text generated in queue to be used for other part of the library\"\"\"\n    response = \"\"\n\n    # if it is a list, assume it is the first element\n    gen_queue_to_use = gen_queue\n    if isinstance(gen_queue, list):\n        gen_queue_to_use = gen_queue[0]\n\n    eos = False\n    gen_stats = None\n    while not eos:\n        try:\n            result = gen_queue_to_use.get_nowait()\n\n            if isinstance(result, GenText):\n                response += result.content\n            elif isinstance(result, GenerationStats):\n                gen_stats = result\n            elif isinstance(result, GenStart):\n                pass\n            elif isinstance(result, GenEnd):\n                eos = True\n                gen_queue_to_use.task_done()\n                logger.info(\"----------------------LLM Response---------------\\n\" + response.strip())\n\n        except asyncio.QueueEmpty:\n            await asyncio.sleep(0.01)    # short sleep before trying again\n\n    return response, gen_stats\n\n\nasync def chat_completion_response_stream(\n    query: ChatMLQuery,\n    gen_queue: GenQueue,\n    model_name: str,\n    request: Request,\n) -> AsyncIterator[dict]:\n    unique_id = get_response_uid()\n    created = int(time.time())\n    full_response = \"\"\n    eos = False\n    gen_type = \"text\"  # Default generation type\n    gen_stats = None\n\n    # no streaming of tool use at the moment\n    accumulated_tool_text = \"\"\n\n    while not eos:\n        accumulated_text = \"\"\n        accumulated_thinking = \"\"\n\n\n        try:\n            while True:\n                item = gen_queue.get_nowait()\n                if isinstance(item, GenText):\n                    if item.text_type == \"tool\":\n                        accumulated_tool_text += item.content\n                    elif item.text_type == \"thinking\":\n                        accumulated_thinking += item.content\n                    else:  # text type\n                        accumulated_text += item.content\n                elif isinstance(item, GenEnd):\n                    eos = True\n                    break\n                elif isinstance(item, GenStart):\n                    gen_type = item.gen_type\n                elif isinstance(item, GenerationStats):\n                    gen_stats = item\n\n        except asyncio.QueueEmpty:\n            await asyncio.sleep(0.01)  # Sleep for 100ms (adjust as needed)\n\n\n        if accumulated_text or accumulated_thinking:\n            full_response += accumulated_thinking + accumulated_text\n\n            if gen_type == \"text\" or gen_type == \"thinking\":\n                if query.return_thinking is True and accumulated_thinking:\n                    if accumulated_text:\n                        accumulated_text = \"\\n\" + accumulated_text\n                    accumulated_text = accumulated_thinking + accumulated_text\n\n                if accumulated_thinking and query.return_thinking == \"separate\":\n                    chunk_data = ChatCompletionResponse(\n                        unique_id=unique_id,\n                        model=model_name,\n                        object=\"chat.completion.chunk\",\n                        created=created,\n                        choices=[\n                            StreamChoice(\n                                index=0,\n                                delta=ChoiceDelta(\n                                    thinking=accumulated_thinking,\n                                )\n                            )\n                        ]\n                    )\n                    yield {\"data\": json.dumps(chunk_data.model_dump(exclude_unset=True), default=pydantic_encoder, ensure_ascii=False)}\n\n                if accumulated_text:\n                    chunk_data = ChatCompletionResponse(\n                        unique_id=unique_id,\n                        model=model_name,\n                        object=\"chat.completion.chunk\",\n                        created=created,\n                        choices=[\n                            StreamChoice(\n                                index=0,\n                                delta=ChoiceDelta(\n                                    content=accumulated_text,\n                                )\n                            )\n                        ]\n                    )\n                    yield {\"data\": json.dumps(chunk_data.model_dump(exclude_unset=True), default=pydantic_encoder, ensure_ascii=False)}\n\n        # currently not support partial streaming of tool use\n        if eos and accumulated_tool_text:\n            # We'll try to parse it as JSON but if it fails, we'll just stream it as is\n            try:\n                # If this is the start of a tool call (containing the function name)\n                response_dict = json.loads(accumulated_tool_text)\n\n                # successfully parse JSON, convert the tool used into response format\n                tools_list = []  # the list of tool to call\n                for index, tool in enumerate(response_dict['functions_calling']):\n                    tool_id = get_response_tool_uid()\n                    tools_list.append(\n                        ToolCallResponse(\n                            id=tool_id,\n                            index=index,\n                            function=OneTool(\n                                name=tool['name'],\n                                arguments=json.dumps(tool['arguments']),\n                            )\n                        )\n                    )\n\n                    chunk_data = ChatCompletionResponse(\n                        unique_id=unique_id,\n                        model=model_name,\n                        object=\"chat.completion.chunk\",\n                        created=created,\n                        choices=[\n                            StreamChoice(\n                                index=0,\n                                delta=ChoiceDelta(\n                                    tool_calls=[\n                                        ChoiceDeltaToolCall(\n                                            index=index,\n                                            id=get_response_tool_uid(),\n                                            function=ChoiceDeltaToolCallFunction(\n                                                name=tool.get(\"name\"),\n                                                arguments=json.dumps(tool.get(\"arguments\",\"\"))\n                                            ),\n                                            type=\"function\"\n                                        )\n                                    ]\n                                )\n                            )\n                        ]\n                    )\n\n                    yield {\"data\": json.dumps(chunk_data.model_dump(exclude_unset=True), default=pydantic_encoder, ensure_ascii=False)}\n\n            except json.JSONDecodeError:\n                # If parsing fails, just stream the raw text\n                chunk_data = ChatCompletionResponse(\n                    unique_id=unique_id,\n                    model=model_name,\n                    object=\"chat.completion.chunk\",\n                    created=created,\n                    choices=[\n                        StreamChoice(\n                            index=0,\n                            delta=ChoiceDelta(\n                                tool_calls=[\n                                    ChoiceDeltaToolCall(\n                                        function=ChoiceDeltaToolCallFunction(\n                                            arguments=accumulated_tool_text\n                                        )\n                                    )\n                                ]\n                            )\n                        )\n                    ]\n                )\n                yield {\"data\": json.dumps(chunk_data.model_dump(exclude_unset=True), default=pydantic_encoder, ensure_ascii=False)}\n\n        if eos:\n            logger.info(f\"----------------------LLM Response---------------\\n{full_response.strip()}\")\n\n            if gen_stats and query.stream_options and query.stream_options.include_usage:\n                usage_data = ChatCompletionResponse(\n                    unique_id=unique_id,\n                    model=model_name,\n                    object=\"chat.completion.chunk\",\n                    choices=[],\n                    usage=UsageResponse(\n                        prompt_tokens=gen_stats.input_tokens_count,\n                        completion_tokens=gen_stats.output_tokens_count,\n                        total_tokens=gen_stats.total_tokens_count,\n                    ),\n                )\n                yield {\"data\": json.dumps(usage_data.model_dump(exclude_unset=True))}\n\n            if gen_stats:\n                logger.info(f\"{model_name} | LLM speed {gen_stats.generation_speed:.1f}/s tokens\")\n\n            # Send final chunk with finish_reason\n            chunk_data = ChatCompletionResponse(\n                unique_id=unique_id,\n                model=model_name,\n                object=\"chat.completion.chunk\",\n                created=created,\n                choices=[\n                    StreamChoice(\n                        index=0,\n                        delta=ChoiceDelta(),\n                        finish_reason=\"tool_calls\" if accumulated_tool_text else \"stop\"\n                    )\n                ]\n            )\n            yield {\"data\": json.dumps(chunk_data.model_dump(exclude_unset=True), default=pydantic_encoder, ensure_ascii=False)}\n            yield {\"data\": \"[DONE]\"}\n        else:\n            await asyncio.sleep(0.1)\n\n\nasync def chat_completion_response(\n    query: ChatMLQuery,\n    gen_queue: GenQueue,\n    # response: str,\n    # gen_stats: GenerationStats,\n    model_name: str,\n    request: Request,\n    # mode: Literal[\"text\", \"tool\"] = \"text\"\n) -> ChatCompletionResponse:\n\n    response = \"\"\n    response_thinking = \"\"\n    response_all = \"\"\n    # global result_queue\n    # completed_event = asyncio.Event()\n    gen_type = \"text\"\n    gen_stats = None\n    eos = False\n    while not eos:\n        try:\n\n            result = gen_queue.get_nowait()\n            if isinstance(result, GenText) and result.text_type==\"text\":\n                response += result.content\n                response_all += result.content\n            elif isinstance(result, GenText) and result.text_type==\"tool\":\n                response += result.content\n                response_all += result.content\n            elif isinstance(result, GenText) and result.text_type==\"thinking\":\n                response_thinking += result.content\n                response_all += result.content\n            elif isinstance(result, GenerationStats):\n                gen_stats = result        # Not applicable for completion endpoint\n            elif isinstance(result, GenStart):\n                gen_type = result\n                gen_type = result.gen_type      # get the gen_type e.g. text, tool, thinking\n            elif isinstance(result, GenEnd):\n                eos = True\n                gen_queue.task_done()\n                logger.info(\"----------------------LLM Response---------------\\n\" + response_all.strip())\n\n        except asyncio.QueueEmpty:\n            await asyncio.sleep(0.01)    # short sleep before trying again\n\n\n    unique_id = get_response_uid()\n    response = response.strip()\n\n    if gen_type == \"text\" or gen_type==\"thinking\":\n\n        # whether to return separate or together\n        if query.return_thinking is False:\n            response_thinking = \"\"\n        elif query.return_thinking is True:\n            response = response_thinking + \"\\n\" + response\n            # response_thinking = \"\"     # still return the response_thinking for user to be able to segregate\n        elif query.return_thinking == \"separate\":\n            pass\n\n        response_obj = ChatCompletionResponse(\n            unique_id=unique_id,\n            model=model_name,\n            choices=[\n                Choice(\n                    index=0,\n                    message=ChatMessage(\n                        role=\"assistant\",\n                        content=response,\n                        thinking=response_thinking,\n                    ),\n                    finish_reason=\"stop\",\n                )\n            ],\n            usage=UsageResponse(\n                prompt_tokens=gen_stats.input_tokens_count,\n                completion_tokens=gen_stats.output_tokens_count,\n                total_tokens=gen_stats.total_tokens_count,\n            ),\n        )\n    elif gen_type == \"tool\":\n        try:\n            response_dict = json.loads(response)\n        except:\n            # since out put is not tool, return it as text instead #TODO find better solution\n            response_obj = ChatCompletionResponse(\n                unique_id=unique_id,\n                model=model_name,\n                choices=[\n                    Choice(\n                        index=0,\n                        message=ChatMessage(\n                            role=\"assistant\",\n                            content=response,\n                        ),\n                        finish_reason=\"stop\",\n                    )\n                ],\n                usage=UsageResponse(\n                    prompt_tokens=gen_stats.input_tokens_count,\n                    completion_tokens=gen_stats.output_tokens_count,\n                    total_tokens=gen_stats.total_tokens_count,\n                ),\n            )\n\n        # successfully parse JSON, convert the tool used into response format\n        tools_list = []  # the list of tool to call\n        for index, tool in enumerate(response_dict['functions_calling']):\n            tool_id = get_response_tool_uid()\n            tools_list.append(\n                ToolCallResponse(\n                    id=tool_id,\n                    index=index,\n                    function=OneTool(\n                        name=tool['name'],\n                        arguments=json.dumps(tool['arguments']),\n                    )\n                )\n            )\n\n        response_obj = ChatCompletionResponse(\n            model=model_name,\n            choices=[\n                Choice(\n                    index=0,\n                    message=ChatMessage(\n                        role=\"assistant\",\n                        content=None,\n                        tool_calls=tools_list,\n                    ),\n                    finish_reason=\"tool_calls\",\n                )\n            ],\n            usage=UsageResponse(\n                prompt_tokens=gen_stats.input_tokens_count,\n                completion_tokens=gen_stats.output_tokens_count,\n                total_tokens=gen_stats.total_tokens_count,\n            ),\n        )\n\n    assert response_obj is not None\n    logger.debug(\"----------------------LLM API Response---------------\\n\" + json.dumps(response_obj.model_dump(), indent=2))\n    logger.info(f\"{model_name} | LLM speed {gen_stats.generation_speed:.1f}/s tokens\")\n\n    return response_obj\n\n\nasync def completion_response(\n    gen_queue: GenQueue,\n    model_name: str,\n    request: Request,\n) -> CompletionResponse:\n    response, gen_stats = await get_response_from_queue(gen_queue, request=request)\n\n    if response:\n        completion_response = CompletionResponse(\n            model=model_name,\n            choices=[\n                CompletionChoice(\n                    text=response.strip(),\n                    index=0,\n                    logprobs=None,\n                    finish_reason=\"stop\"  # You may want to determine this based on actual finish reason\n                )\n            ],\n            usage=UsageResponse(\n                prompt_tokens=gen_stats.input_tokens_count if gen_stats else 0,\n                completion_tokens=gen_stats.output_tokens_count if gen_stats else 0,\n                total_tokens=gen_stats.total_tokens_count if gen_stats else 0\n            )\n        )\n\n        # Use model_dump() and json.dumps() instead of json() method\n        logger.info(\n            f\"----------------------LLM API Response---------------\\n{json.dumps(completion_response.model_dump(), indent=2)}\")\n        return completion_response\n\n\nasync def completion_response_stream(\n    request: Request,\n    gen_queue: GenQueue,\n    model_name: str\n) -> AsyncIterator:\n    unique_id = get_response_uid()\n    full_response = \"\"\n    eos = False\n    gen_stats = None\n    while not eos:\n        # if await request.is_disconnected():\n        #     logger.info(\"Request disconnected, stopping queue processing\")\n        #     break\n\n        accumulated_text = \"\"\n        try:\n            while True:\n                result = gen_queue.get_nowait()\n                if isinstance(result, GenText):\n                    accumulated_text += result.content\n                elif isinstance(result, GenEnd):\n                    eos = True\n                    gen_queue.task_done()\n                    break\n                elif isinstance(result, GenStart):\n                    pass\n                elif isinstance(result, GenerationStats):\n                    gen_stats = result\n        except asyncio.QueueEmpty:\n            pass\n        if accumulated_text:\n            full_response += accumulated_text\n            chunk_data = CompletionStreamResponse(\n                id=unique_id,\n                object=\"text_completion\",\n                created=int(time.time()),\n                model=model_name,\n                system_fingerprint=\"fp_44709d6fcb\",\n                choices=[\n                    CompletionChoice(\n                        text=accumulated_text,\n                        index=0,\n                        logprobs=None,\n                        finish_reason=None\n                    )\n                ]\n            )\n            json_data = json.dumps(chunk_data.model_dump())\n            if json_data.strip():\n                logger.debug(f\"Yielding: {json_data!r}\")\n                yield json_data\n        if eos:\n            logger.info(f\"----------------------LLM Response---------------\\n{full_response.strip()}\")\n            if gen_stats is not None:\n                logger.info(f\"{model_name} | LLM speed {gen_stats.generation_speed:.1f}/s tokens\")\n            yield \"[DONE]\"\n            break\n        else:\n            try:\n                if await asyncio.wait_for(request.is_disconnected(), timeout=0.1):\n                    logger.info(\"Client disconnected, stopping stream\")\n                    break\n            except asyncio.TimeoutError:\n                pass\n    if not eos:\n        logger.info(\"Stream ended before receiving GenEnd\")\n\n\nasync def chat_completion_response_artifact_stream(\n    query: ChatMLQuery,\n    gen_queue: GenQueue,\n    model_name: str,\n    request: Request,\n) -> AsyncIterator[dict]:\n    unique_id = get_response_uid()\n    created = int(time.time())\n    full_response = \"\"\n    response_thinking = \"\"\n    eos = False\n    gen_type = \"text\"  # Default generation type\n    gen_stats = None\n\n    # last_log_time = time.time()\n    # log_interval = 1  # Log every 5 seconds\n\n    artifact_parser = StreamParser()\n    malformed_data = False\n    MALFORMED_CHECK_LENGTH = 70     # the length limit of text so that the content_type block appear\n    content_type = None    # either text or code\n\n    while not eos:\n        accumulated_text = \"\"\n        accumulated_thinking = \"\"\n\n        # if await request.is_disconnected():\n        #     logger.info(\"Request disconnected, stopping queue processing\")\n        #     break\n\n        try:\n            # Collect all available items from the queue\n            while True:\n                item = gen_queue.get_nowait()\n                if isinstance(item, GenText) and item.text_type==\"text\":\n                    accumulated_text += item.content\n                if isinstance(item, GenText) and item.text_type==\"tool\":\n                    accumulated_text += item.content\n                elif isinstance(item, GenText) and item.text_type==\"thinking\":\n                    accumulated_thinking += item.content\n                elif isinstance(item, GenEnd):\n                    eos = True\n                    break\n                elif isinstance(item, GenStart):\n                    gen_type = item.gen_type\n                elif isinstance(item, GenerationStats):\n                    gen_stats = item\n        except asyncio.QueueEmpty:\n            pass\n\n        if accumulated_thinking and query.return_thinking is not False:\n            full_response += accumulated_thinking\n            chunk_data = ChatCompletionResponse(\n                unique_id=unique_id,\n                model=model_name,\n                object=\"chat.completion.chunk\",\n                created=created,\n                choices=[\n                    StreamChoice(\n                        index=0,\n                        delta=ChatMessage(\n                            role=\"assistant\",\n                            content=\"\",\n                            thinking=accumulated_thinking,\n                        ),\n                    )\n                ],\n            )\n            yield {\"data\": json.dumps(chunk_data.model_dump(exclude_unset=True), default=pydantic_encoder,\n                                      ensure_ascii=False)}\n\n        if accumulated_text:\n            full_response += accumulated_text\n\n            if gen_type == \"text\":\n                parsed_chunks = artifact_parser.process_stream(accumulated_text)\n\n                for chunk_type, chunk_content in parsed_chunks:\n                    if chunk_content:\n                        chunk_data = ChatCompletionResponse(\n                            unique_id=unique_id,\n                            model=model_name,\n                            object=\"chat.completion.chunk\",\n                            created=created,\n                            choices=[\n                                StreamChoice(\n                                    index=0,\n                                    delta=ChatMessage(\n                                        role=\"assistant\",\n                                        content=chunk_content,\n                                        artifact_meta=chunk_type.model_dump()\n\n                                    ),\n                                ),\n                            ],\n                        )\n                        yield {\"data\": json.dumps(chunk_data.model_dump(exclude_unset=True))}\n\n            elif gen_type == \"tool\":\n                # artifact do not affect tool usage\n                # Accumulate tool usage data\n                # Note: This assumes that tool data is complete in a single chunk\n                # If tool data can span multiple chunks, you'll need to implement a more sophisticated accumulation strategy\n                tool_response = json.loads(accumulated_text)\n                tools_list = []\n                for index, tool in enumerate(tool_response.get('functions_calling', [])):\n                    tool_id = get_response_tool_uid()\n                    tools_list.append(\n                        ToolCallResponse(\n                            id=tool_id,\n                            index=index,\n                            function=OneTool(\n                                name=tool['name'],\n                                arguments=json.dumps(tool['arguments']),\n                            )\n                        )\n                    )\n                chunk_data = ChatCompletionResponse(\n                    unique_id=unique_id,\n                    model=model_name,\n                    object=\"chat.completion.chunk\",\n                    created=created,\n                    choices=[\n                        StreamChoice(\n                            index=0,\n                            delta=ChatMessage(\n                                role=\"assistant\",\n                                tool_calls=tools_list,\n                            ),\n                            finish_reason=\"tool_calls\",\n                        )\n                    ],\n                )\n                yield {\"data\": json.dumps(chunk_data.dict(exclude_unset=True))}\n\n        if eos:\n            # Log the full response at the end\n            logger.info(f\"----------------------LLM Response---------------\\n{full_response.strip()}\")\n\n            # Include generation stats if available and requested\n            if gen_stats and query.stream_options and query.stream_options.include_usage:\n                usage_data = ChatCompletionResponse(\n                    unique_id=unique_id,\n                    model=model_name,\n                    object=\"chat.completion.chunk\",\n                    choices=[],\n                    usage=UsageResponse(\n                        prompt_tokens=gen_stats.input_tokens_count,\n                        completion_tokens=gen_stats.output_tokens_count,\n                        total_tokens=gen_stats.total_tokens_count,\n                    ),\n                )\n                yield {\"data\": json.dumps(usage_data.model_dump(exclude_unset=True))}\n\n            if gen_stats:\n                logger.info(f\"{model_name} | LLM speed {gen_stats.generation_speed:.1f}/s tokens\")\n\n            # Send the ending DONE message\n            yield {\"data\": \"[DONE]\"}\n        else:\n            await asyncio.sleep(0.1)  # Short sleep before next iteration if not at end of stream\n\n\nasync def chat_completion_response_artifact(\n    query: ChatMLQuery,\n    gen_queue: GenQueue,\n    model_name: str,\n    request: Request,\n) -> ChatCompletionResponse:\n    response = \"\"\n    response_thinking = \"\"\n    response_all = \"\"\n\n    response_obj = None\n    gen_type = GenStart(gen_type=\"text\")\n    gen_stats = None\n    eos = False\n\n    while not eos:\n        try:\n            # if await request.is_disconnected():\n            #     logger.info(\"Request disconnected, stopping queue processing\")\n            #     break\n\n            result = gen_queue.get_nowait()\n            if isinstance(result, GenText) and result.text_type==\"text\":\n                response += result.content\n                response_all += result.content\n            elif isinstance(result, GenText) and result.text_type==\"tool\":\n                response += result.content\n                response_all += result.content\n            elif isinstance(result, GenText) and result.text_type==\"thinking\":\n                response_thinking += result.content\n                response_all += result.content\n            elif isinstance(result, GenerationStats):\n                gen_stats = result\n            elif isinstance(result, GenStart):\n                gen_type = result\n            elif isinstance(result, GenEnd):\n                eos = True\n                gen_queue.task_done()\n                logger.info(\"----------------------LLM Response---------------\\n\" + response_all.strip())\n        except asyncio.QueueEmpty:\n            await asyncio.sleep(0.01)\n\n    unique_id = get_response_uid()\n    response = response.strip()\n\n    if gen_type.gen_type == \"text\" or gen_type==\"thinking\":\n        choices = []\n\n        # return thinking first\n        if response_thinking and query.return_thinking is not False:    # currently always return as separate\n            choices.append(\n                Choice(\n                    index=0,    # only 1 thinking per response for now\n                    message=ChatMessage(\n                        role=\"assistant\",\n                        content='',\n                        thinking=response_thinking,\n                    ),\n                    finish_reason=\"stop\"\n                )\n            )\n\n        parser = StreamParser()\n        parsed_chunks = parser.parse_full_response(response)\n\n        if parsed_chunks:\n            # If parsing was successful, create a structured response\n\n            for idx, (chunk_type, chunk_content) in enumerate(parsed_chunks):\n                choices.append(\n                    Choice(\n                        index=idx,\n                        message=ChatMessage(\n                            role=\"assistant\",\n                            content=chunk_content,\n                            artifact_meta=chunk_type.model_dump(),\n                        ),\n                        finish_reason=\"stop\"\n                    )\n                )\n        else:\n            # If parsing failed, treat the entire response as a single text chunk\n            choices = [\n                Choice(\n                    index=0,\n                    message=ChatMessage(\n                        role=\"assistant\",\n                        content=response,\n                        artifact_meta=TextTag().model_dump()\n                    ),\n                    finish_reason=\"stop\"\n                )\n            ]\n\n        response_obj = ChatCompletionResponse(\n            unique_id=unique_id,\n            model=model_name,\n            choices=choices,\n            usage=UsageResponse(\n                prompt_tokens=gen_stats.input_tokens_count if gen_stats else 0,\n                completion_tokens=gen_stats.output_tokens_count if gen_stats else 0,\n                total_tokens=gen_stats.total_tokens_count if gen_stats else 0,\n            ),\n        )\n\n    elif gen_type.gen_type == \"tool\":\n        try:\n            response_dict = json.loads(response)\n        except:\n            # since output is not tool, return it as text instead #TODO find better solution\n            response_obj = ChatCompletionResponse(\n                unique_id=unique_id,\n                model=model_name,\n                choices=[\n                    Choice(\n                        index=0,\n                        message=ChatMessage(\n                            role=\"assistant\",\n                            content=response,\n                            artifact_meta=TextTag().model_dump()\n                        ),\n                        finish_reason=\"stop\",\n                    )\n                ],\n                usage=UsageResponse(\n                    prompt_tokens=gen_stats.input_tokens_count,\n                    completion_tokens=gen_stats.output_tokens_count,\n                    total_tokens=gen_stats.total_tokens_count,\n                ),\n            )\n        else:\n            # successfully parse JSON, convert the tool used into response format\n            tools_list = []  # the list of tool to call\n            for index, tool in enumerate(response_dict['functions_calling']):\n                tool_id = get_response_tool_uid()\n                tools_list.append(\n                    ToolCallResponse(\n                        id=tool_id,\n                        index=index,\n                        function=OneTool(\n                            name=tool['name'],\n                            arguments=json.dumps(tool['arguments']),\n                        )\n                    )\n                )\n\n            response_obj = ChatCompletionResponse(\n                model=model_name,\n                choices=[\n                    Choice(\n                        index=0,\n                        message=ChatMessage(\n                            role=\"assistant\",\n                            content=None,\n                            tool_calls=tools_list,\n                        ),\n                        finish_reason=\"tool_calls\",\n                    )\n                ],\n                usage=UsageResponse(\n                    prompt_tokens=gen_stats.input_tokens_count,\n                    completion_tokens=gen_stats.output_tokens_count,\n                    total_tokens=gen_stats.total_tokens_count,\n                ),\n            )\n\n    assert response_obj is not None\n    logger.debug(\"----------------------LLM API Response---------------\\n\" + json.dumps(response_obj.model_dump(), indent=2))\n    logger.info(f\"{model_name} | LLM speed {gen_stats.generation_speed:.1f}/s tokens\")\n\n    return response_obj\n"}
{"type": "source_file", "path": "src/gallama/app.py", "content": "# using absolute import here as this file will be run alone\nimport sys\nfrom fastapi import FastAPI, Request, APIRouter\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom gallama.data_classes import (\n    ModelSpec,\n    GenQueue,\n    GenQueueDynamic\n)\nimport signal\nimport argparse\nimport uvicorn\nfrom fastapi.exceptions import RequestValidationError\nimport json\nfrom fastapi.responses import JSONResponse\nfrom gallama.utils import parse_request_body\nfrom gallama.config.config_manager import ConfigManager\nfrom gallama.data import ARTIFACT_SYSTEM_PROMPT\nimport os\nfrom contextlib import asynccontextmanager\nfrom logging import DEBUG\nfrom gallama.logger.logger import get_logger\nimport base64\nfrom gallama.dependencies import get_model_manager\nfrom gallama.routes import (\n    chat_router,\n    embedding_router,\n    model_management_router,\n    audio_router,\n    ws_stt_router,\n    ws_llm_router,\n    ws_tts_router,\n    ws_video_router\n)\nfrom tempfile import SpooledTemporaryFile\n\n# Add this after your imports to clear logging from 3rd party module\n\n#streaming example\n#https://blog.gopenai.com/how-to-stream-output-in-llm-based-applications-to-boost-user-experience-e9fcf582777a\n\nlogger = get_logger()\n\n# add endpoint\nrouter = APIRouter()\nrouter.include_router(chat_router)\nrouter.include_router(embedding_router)\nrouter.include_router(model_management_router)\nrouter.include_router(audio_router)\nrouter.include_router(ws_stt_router)\nrouter.include_router(ws_llm_router)\nrouter.include_router(ws_tts_router)\nrouter.include_router(ws_video_router)\n\n\nconfig_manager = ConfigManager()\nmodel_manager = get_model_manager()\nmodel_ready = False\n\n@router.get(\"/\")\nasync def read_root():\n    return {\"Hello\": \"PAI\"}\n\n\n@router.options(\"/v1/chat/completions\")\nasync def options_handler(request: Request):\n    return JSONResponse(\n        status_code=200,\n        content={\"message\": \"Local OpenAI response for OPTIONS request\"},\n        headers={\n            \"Access-Control-Allow-Origin\": \"*\",\n            \"Access-Control-Allow-Methods\": \"GET, POST, OPTIONS\",\n            \"Access-Control-Allow-Headers\": \"\",\n        },\n    )\n\n\n\nasync def startup_event():\n    # run some dummy generation so that cache is initialized\n    logger.info(\"Generator initialization\")\n\n    # warm up LLM\n    if model_manager.llm_dict:\n        gen_queue = GenQueue()\n        for _model_name, _model in model_manager.llm_dict.items():\n            await _model.chat_raw(\n                prompt=f\"{ARTIFACT_SYSTEM_PROMPT}\\nWrite a 500 words story on Llama\",\n                # stream=False,\n                max_tokens=50,\n                gen_queue=gen_queue,\n                quiet=True,\n                request=None,\n            )\n\n            logger.info(f\"LLM| {_model_name} | warmed up\")\n        gen_queue = None\n\n    if model_manager.stt_dict:\n        logger.info(\"STT warmed up NOT YET IMPLEMENTED\")\n\n    if model_manager.tts_dict:\n        for _model_name, tts in model_manager.tts_dict.items():\n            _, _ = await tts.text_to_speech(\n                text=\"hello\",\n                stream=False,\n                batching=False,\n                batch_size=1\n            )\n            logger.info(f\"TTS| {_model_name} | warmed up\")\n\n    if model_manager.embedding_dict:\n        logger.info(\"Embedding warmed up NOT YET IMPLEMENTED\")\n\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup code\n    await startup_event()\n    try:\n        yield\n    finally:\n        # Cleanup code\n        logger.info(\"Cleaning up ZMQ connections...\")\n        for handler in logger.handlers:\n            if hasattr(handler, 'close'):\n                handler.close()\n\n\ndef make_server(args):\n    global logger\n    global draft_spec_dict\n\n    # Add signal handlers for graceful shutdown\n    def signal_handler(signum, frame):\n        logger.info(\"Received shutdown signal, cleaning up...\")\n        for handler in logger.handlers:\n            if hasattr(handler, 'close'):\n                handler.close()\n        sys.exit(0)\n\n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n\n    # load yaml file of model info\n    logger.info(args)\n    model_dict = {}\n    draft_spec_dict = {}    # for speculative decoding\n\n    # parse cli argument to get model info\n    # Handle model specification\n    model_spec = None\n    if args.model_spec:\n        try:\n            # Decode and parse the ModelSpec from base64 JSON\n            model_json = base64.b64decode(args.model_spec).decode('utf-8')\n            model_spec = ModelSpec.model_validate_json(model_json)\n        except Exception as e:\n            logger.error(f\"Failed to parse model specification: {e}\")\n            sys.exit(1)\n\n        # set the env for log to zmq\n        # Note: This assumes you want to use the first model's ID if multiple are provided\n        os.environ['MODEL_NAME'] = model_spec.model_name\n        os.environ['MODEL_PORT'] = str(args.port)\n\n    # set logger level\n    if args.verbose:\n        logger.setLevel(DEBUG)\n        os.environ[\"LOCAL_OPEN_AI_VERBOSE\"] = '2'   # turn on verbosity for all\n\n    args.model_spec = model_spec\n    logger.info(\"Parsed Arguments:\" + str(args))  # Debug statement\n\n    if args.detached:\n        # send logging to zmq so that it will show in the parent log\n        logger = get_logger(name=\"child\", to_zmq=True, to_console=False)\n    else:\n        # keep the default logger declared on top\n        pass\n\n    app = FastAPI(lifespan=lifespan)\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"], #origins,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    app.include_router(router)\n\n    @app.middleware(\"http\")\n    @app.middleware(\"https\")\n    async def log_requests(request: Request, call_next):\n        try:\n            # if request.method in (\"POST\", \"PUT\", \"PATCH\"):\n            #     # Parse body and preserve it\n            #     body_content, is_multipart = await parse_request_body(request)\n            #\n            #     # Handle file uploads explicitly\n            #     if is_multipart or isinstance(body_content, SpooledTemporaryFile):\n            #         request_content = \"File upload content not logged\"\n            #     elif isinstance(body_content, dict):\n            #         request_content = json.dumps(body_content, indent=2)\n            #     elif isinstance(body_content, str):\n            #         try:\n            #             parsed_content = json.loads(body_content)\n            #             request_content = json.dumps(parsed_content, indent=2)\n            #         except json.JSONDecodeError:\n            #             request_content = body_content\n            #     else:\n            #         request_content = str(body_content)\n            #\n            #     logger.debug(\n            #         f\"API Request:\\n\"\n            #         f\"Method: {request.method}\\n\"\n            #         f\"URL: {request.url}\\n\"\n            #         f\"Headers: {dict(request.headers)}\\n\"\n            #         f\"Content-Type: {request.headers.get('content-type', 'Not specified')}\\n\"\n            #         f\"Body: {request_content if request_content else 'No Body'}\"\n            #     )\n\n            # Proceed with the request\n            response = await call_next(request)\n            return response\n\n        except Exception as e:\n            logger.error(f\"Middleware error: {str(e)}\", exc_info=True)\n            return JSONResponse(\n                status_code=500,\n                content={\"detail\": \"Internal server error in middleware\"}\n            )\n\n    if model_spec:\n        # load model\n        model_manager.load_model(model_spec)\n        model_manager.model_ready = True\n\n    uvicorn.run(app, host=args.host, port=args.port, log_config=None)    # disable log_config to use our custom logger\n\n\ndef parse_dict(arg):\n    \"\"\"Parses a key=value string and returns a dictionary.\"\"\"\n    result = {}\n    for pair in arg.split():\n        key, value = pair.split('=')\n        result[key] = value.strip(\"'\")  # Strip single quotes here as well\n    return result\n\n\nif __name__ == \"__main__\":\n    arg_parser = argparse.ArgumentParser(description=\"Launch local AI model\")\n    arg_parser.add_argument(\"--model-spec\", type=str, help=\"Base64 encoded JSON ModelSpec object\")\n    arg_parser.add_argument('-v', \"--verbose\", action='store_true', help=\"Turn on more verbose logging\")\n    arg_parser.add_argument('-d', \"--detached\", action='store_true', help=\"Log to ZeroMQ\")\n    arg_parser.add_argument(\"--host\", type=str, default=\"127.0.0.1\", help=\"The host to bind to.\")\n    arg_parser.add_argument('-p', \"--port\", type=int, default=8000, help=\"The port to bind to.\")\n    arg_parser.add_argument(\"--reload\", action=\"store_true\", help=\"Enable auto-reload.\")\n\n    args = arg_parser.parse_args()\n\n    make_server(args)\n"}
{"type": "source_file", "path": "src/gallama/__init__.py", "content": "from . import cli\nfrom gallama.logger.logger import logger\nfrom gallama.backend.llm.thinking_template import THINKING_TEMPLATE, Thinking\nfrom . import app\nfrom .app import make_server\nfrom .config import ConfigManager\nfrom .dependencies import model_manager\nfrom .dependencies_server import get_server_manager, get_server_logger, DEFAULT_ZMQ_URL"}
{"type": "source_file", "path": "src/gallama/backend/embedding/__init__.py", "content": ""}
{"type": "source_file", "path": "src/gallama/backend/stt/base.py", "content": "from typing import Literal\nfrom abc import ABC, abstractmethod\nfrom ...data_classes import ModelSpec\n\nclass ASRBase(ABC):\n\n    def __init__(\n        self,\n        model_spec: ModelSpec,\n        # language: str=\"auto\",\n        # modelsize=None,\n        # cache_dir=None,\n        # model_dir=None,\n        # quant: Literal[\"8.0\", \"16.0\"] = \"16.0\",\n        # device: TODO to implement\n    ):\n\n        if model_spec.language == \"auto\":\n            self.original_language = None\n        else:\n            self.original_language = model_spec.language\n\n        self.quant = model_spec.quant\n\n        self.sep = \" \"   # join transcribe words with this character (\" \" for whisper_timestamped,\n                         # \"\" for faster-whisper because it emits the spaces when neeeded)\n\n        self.transcribe_kargs = {}\n        self.model = self.load_model(model_spec)\n\n    @abstractmethod\n    def load_model(self, model_spec: ModelSpec):\n        raise NotImplemented(\"must be implemented in the child class\")\n\n    @abstractmethod\n    def segment_to_timestamped_words(self, segments):\n        raise NotImplemented(\"must be implemented in the child class\")\n\n    @abstractmethod\n    def transcribe(self, audio, init_prompt: str = \"\", temperature: float = 0.0):\n        raise NotImplemented(\"must be implemented in the child class\")\n\n    @abstractmethod\n    def transcribe_to_segment(self, audio, init_prompt=\"\"):\n        raise NotImplemented(\"must be implemented in the child class\")\n\n    @abstractmethod\n    def segments_end_ts(self, res):\n        raise NotImplemented(\"must be implemented in the child class\")\n\n    def use_vad(self):\n        raise NotImplemented(\"must be implemented in the child class\")"}
{"type": "source_file", "path": "src/gallama/backend/stt/mlx_whisper/model.py", "content": "from ..base import ASRBase\nfrom typing import Literal, List, Union, BinaryIO, Optional\nimport numpy as np\n\nfrom ....logger import logger\nfrom ....data_classes import ModelSpec, TranscriptionResponse, TimeStampedWord, LanguageType\nimport soundfile as sf\n\n\n# faster whisper wont work. For now we import to reuse the data classes\n# TODO create the data classes in gallama\ntry:\n    from faster_whisper.transcribe import TranscriptionOptions, TranscriptionInfo, Segment\nexcept ImportError:\n    TranscriptionOptions, TranscriptionInfo, Segment = None, None, None, None, None\n\n\nimport mlx_whisper as mlx_model\nimport mlx.core as mx\n\n\nclass ASRMLXWhisper(ASRBase):\n    \"\"\"\n    An implementation of ASRBase that uses the MLX library as backend.\n    \"\"\"\n\n\n    def load_model(self, model_spec: ModelSpec):\n        \"\"\"\n        For MLX, we simply store the model parameters for use with the MLX transcribe function.\n        \"\"\"\n        self.model_id = model_spec.model_id  # path or HF repo for the MLX converted weights\n        self.model_name = model_spec.model_name\n        self.quant = model_spec.quant\n\n        # mlx whisper doesnt have dedicated load method\n        # but it will load when transcribe is ran\n        mlx_model.transcribe(\n            audio=np.random.randn(44100 * 2),\n            path_or_hf_repo=model_spec.model_id,\n        )\n\n        # Use fp16 if quantization is \"16.0\"; otherwise, assume fp32.\n        # self.fp16 = True if self.quant == \"16.0\" else False\n        return mlx_model\n\n    # @staticmethod\n    # def binaryio_to_numpy(audio_io: BinaryIO) -> Tuple[np.ndarray, int]:\n    #     \"\"\"\n    #     Convert a BinaryIO audio stream into a numpy array.\n    #     Returns a tuple of (audio_data, sample_rate).\n    #     \"\"\"\n    #     audio_io.seek(0)  # Ensure the file pointer is at the beginning\n    #     audio_data, sample_rate = sf.read(audio_io)\n    #     return audio_data, sample_rate\n\n    def transcribe_to_segment(\n        self,\n        audio: Union[str, np.ndarray],\n        init_prompt: str = \"\",\n        temperature: float = 0.0,\n        language: LanguageType = None,\n        batch: bool = False,\n        batch_size: int = 8,\n    ) -> List[dict]:\n        \"\"\"\n        Transcribe audio into a list of segments.\n        Calls the MLX transcribe function with word timestamps enabled.\n        \"\"\"\n\n        if not isinstance(audio, np.ndarray):\n            audio_to_use, sample_rate = sf.read(audio)\n        else:\n            audio_to_use = audio\n\n        response = self.model.transcribe(\n            audio=audio_to_use,\n            path_or_hf_repo=self.model_id,  # eventhough loaded, we still need to use this\n            temperature=temperature,\n            condition_on_previous_text=True,\n            initial_prompt=init_prompt,\n            word_timestamps=True,\n            hallucination_silence_threshold=1,\n            **self.transcribe_kargs  # any extra keyword arguments set via use_vad(), etc.\n        )\n        segments = response.get(\"segments\", [])\n        return segments\n\n    def transcribe(\n        self,\n        audio: Union[str, np.ndarray],\n        init_prompt: str = \"\",\n        temperature: float = 0.0,\n        language=None,\n        include_segments: bool = False,\n        batch: bool = False,\n    ) -> TranscriptionResponse:\n        \"\"\"\n        Transcribe audio to full text and return a TranscriptionResponse.\n        The response includes the text, detected language, duration (from the last segment),\n        and word-level timestamps (either from MLX if available, or approximated).\n        \"\"\"\n\n        if not isinstance(audio, np.ndarray):\n            audio_to_use, sample_rate = sf.read(audio)\n        else:\n            audio_to_use = audio\n\n\n        response = self.model.transcribe(\n            audio=audio_to_use,\n            path_or_hf_repo=self.model_id,  # eventhough loaded, we still need to use this\n            temperature=temperature,\n            condition_on_previous_text=True,\n            initial_prompt=init_prompt,\n            word_timestamps=True,\n            hallucination_silence_threshold=1,\n            **self.transcribe_kargs  # any extra keyword arguments set via use_vad(), etc.\n        )\n        text = response.get(\"text\", \"\")\n        segments = response.get(\"segments\", [])\n        lang = response.get(\"language\", None)\n        duration = segments[-1][\"end\"] if segments else None\n        # Process segments to extract or approximate word-level timestamps.\n        words = self.segment_to_timestamped_words(segments)\n        return TranscriptionResponse(\n            text=text,\n            segments=segments if include_segments else None,\n            words=words,\n            language=lang,\n            duration=duration\n        )\n\n    def segment_to_timestamped_words(self, segments: List[dict]) -> List[TimeStampedWord]:\n        \"\"\"\n        Converts a list of segments into word-level timestamps.\n        If a segment includes a \"words\" key, it uses that data.\n        Otherwise, it splits the segment text and assigns equal intervals.\n        \"\"\"\n        output = []\n        for segment in segments:\n            # Skip segments that are likely silence.\n            if segment.get(\"no_speech_prob\", 0) > 0.9:\n                continue\n            if \"words\" in segment and segment[\"words\"]:\n                for word in segment[\"words\"]:\n                    output.append((word[\"start\"], word[\"end\"], word[\"word\"]))\n            else:\n                # Fallback: split the segment text and assign approximate timestamps.\n                words = segment[\"text\"].split()\n                if not words:\n                    continue\n                duration = segment[\"end\"] - segment[\"start\"]\n                word_duration = duration / len(words)\n                for i, w in enumerate(words):\n                    start = segment[\"start\"] + i * word_duration\n                    end = start + word_duration\n                    output.append((start, end, w))\n        return output\n\n    def segment_to_long_text(self, segments: List[Segment]) -> str:\n        \"\"\"\n        Converts a list of segments into a full text.\n\n        Args:\n            segments (List[Segment]): Transcription segments with words and metadata.\n\n        Returns:\n            List[Tuple[float, float, str]]: List of (start, end, word) tuples.\n        \"\"\"\n\n        output = []\n        for segment in segments:\n            if segment.no_speech_prob > 0.9:    # silence segment\n                continue\n            for word in segment.words:\n                output.append(word.word)\n\n        return self.sep.join(output)\n\n    def segments_end_ts(self, segments: List[dict]) -> List[float]:\n        \"\"\"\n        Returns a list of ending timestamps from the provided segments.\n        \"\"\"\n        return [segment.get(\"end\", 0.0) for segment in segments]\n\n    def use_vad(self):\n        \"\"\"\n        Enable voice activity detection (VAD) settings.\n        For example, here we set a no-speech threshold in the extra keyword arguments.\n        \"\"\"\n        self.transcribe_kargs[\"no_speech_threshold\"] = 0.6\n\n    def set_translate_task(self):\n        self.transcribe_kargs[\"task\"] = \"translate\""}
{"type": "source_file", "path": "src/gallama/backend/stt/vad.py", "content": "from dataclasses import dataclass\nimport numpy as np\nfrom typing import Optional, List, Dict, Tuple\nimport torch\nfrom silero_vad import VADIterator, load_silero_vad\nfrom .audio_buffer import AudioBufferWithTiming\nfrom ...data_classes.realtime_client_proto import TurnDetectionConfig\nfrom gallama.logger import logger\n\n\n\n\nclass VADProcessor:\n    def __init__(self, config: TurnDetectionConfig):\n        self.config = config\n        self.sampling_rate = 16000\n\n        # Initialize Silero VAD\n        try:\n            self.model = load_silero_vad()\n            self.model.eval()\n            self.vad_iterator = VADIterator(\n                self.model,\n                sampling_rate=self.sampling_rate,\n                threshold=self.config.threshold\n            )\n        except Exception as e:\n            logger.error(f\"Error initializing Silero VAD: {str(e)}\")\n            raise\n\n        # Speech detection state\n        self.speech_start_ms = None\n        self.last_speech_ms = None\n        self.continuous_silence_ms = 0\n        self.continuous_speaking_ms = 0\n\n        logger.info(f\"VADProcessor initialized with threshold={self.config.threshold}\")\n        self.min_silence_ms = self.config.silence_duration_ms\n        logger.info(f\"Min silence duration: {self.min_silence_ms}ms\")\n        self.min_speak_ms = self.config.prefix_padding_ms\n        self.window_size_samples = 512\n        self.window_stride_samples = 512\n\n        # Event state tracking\n        self.is_speaking = False\n        self.speech_start_sent = False\n        self.speech_end_sent = False\n        self.prob_speech_start = None\n        self.prob_speech_end = None\n        self.potential_speech_start_flag = None\n        self.potential_speech_end_flag = None\n\n    def get_windows_from_buffer(self, audio_buffer: AudioBufferWithTiming, current_offset: int):\n        audio_chunk = audio_buffer.get_unprocessed_audio_vad()\n        if len(audio_chunk) == 0:\n            return []\n\n        windows = []\n        for i in range(0, len(audio_chunk) - self.window_size_samples + 1, self.window_stride_samples):\n            window = audio_chunk[i:i + self.window_size_samples]\n            if len(window) == self.window_size_samples:\n                windows.append({\n                    'data': torch.from_numpy(window).float(),\n                    'index': i  # Remove the division by window_stride_samples\n                })\n\n        return windows\n\n    def process_chunk(self, audio_buffer: AudioBufferWithTiming, current_offset: int, is_final: bool = False) -> Tuple[Dict, Dict]:\n        windows = self.get_windows_from_buffer(audio_buffer, current_offset)\n        if not windows:\n            return None, None\n\n        speech_start_object = None\n        speech_end_object = None\n\n        for window in windows:\n            # Get absolute time accounting for buffer offset and VAD-specific processing position\n            absolute_time_ms = audio_buffer.get_time_ms(\n                audio_buffer.last_processed_sample_vad + window['index']  # Use raw sample index\n            )\n            logger.debug(\n                f\"Window timing - index: {window['index']}, absolute_time: {absolute_time_ms}ms, last_processed: {audio_buffer.last_processed_sample_vad}\")\n\n            prob = self.model(window['data'], self.sampling_rate).item()\n            # logger.info(f\"VAD chunk window index {window['index']}, prob: {prob}\")\n\n            if prob >= self.config.threshold:\n                self.last_speech_ms = absolute_time_ms\n\n                if not self.is_speaking:\n                    if not self.potential_speech_start_flag:\n                        self.potential_speech_start_flag = True\n                        self.speech_start_ms = absolute_time_ms\n\n                    self.continuous_speaking_ms += (self.window_size_samples / self.sampling_rate * 1000)\n\n                    if self.continuous_speaking_ms >= self.min_speak_ms:\n                        self.is_speaking = True\n                        self.speech_start_sent = True\n                        self.prob_speech_start = prob\n                        speech_start_object = {\n                            'speech_detected': True,\n                            'speech_ended': False,\n                            'start_time': self.speech_start_ms,\n                            'end_time': None,\n                            'confidence': prob\n                        }\n                else:\n                    if self.potential_speech_end_flag:\n                        self.potential_speech_end_flag = False\n                        self.continuous_silence_ms = 0\n\n            else:\n                if self.is_speaking:\n                    self.continuous_silence_ms += (self.window_size_samples / self.sampling_rate * 1000)\n\n                    if not self.potential_speech_end_flag:\n                        self.potential_speech_end_flag = True\n\n                    if self.continuous_silence_ms > self.min_silence_ms and self.speech_start_sent:\n                        self.prob_speech_end = prob\n                        speech_end_object = {\n                            'speech_detected': True,\n                            'speech_ended': True,\n                            'start_time': self.speech_start_ms,\n                            'end_time': self.last_speech_ms,\n                            'duration_ms': self.last_speech_ms - self.speech_start_ms,\n                            'confidence': 1 - prob\n                        }\n                        self.reset()\n\n        # Mark VAD processing as complete after processing all windows\n        audio_buffer.mark_vad_processing_complete(is_final)\n        return speech_start_object, speech_end_object\n\n    def reset(self):\n        \"\"\"Reset VAD state.\"\"\"\n        if self.vad_iterator:\n            self.vad_iterator.reset_states()\n\n        # Reset timing state\n        self.speech_start_ms = None\n        self.last_speech_ms = None\n        self.continuous_silence_ms = 0\n        self.continuous_speaking_ms = 0\n\n        # Reset detection state\n        self.is_speaking = False\n        self.speech_start_sent = False\n        self.speech_end_sent = False\n        self.prob_speech_start = None\n        self.prob_speech_end = None\n        self.potential_speech_start_flag = None\n        self.potential_speech_end_flag = None"}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/mlx_vllm/__init__.py", "content": "from .mlx_vlm import ModelMLXVLM"}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/exllama/inference_json_lmfe_wrapper.py", "content": "from exllamav2 import ExLlamaV2, ExLlamaV2Tokenizer\nfrom exllamav2.generator.filters import ExLlamaV2Filter\nfrom functools import lru_cache\nfrom lmformatenforcer.integrations.exllamav2 import build_token_enforcer_tokenizer_data\nfrom lmformatenforcer import TokenEnforcer, CharacterLevelParser\nfrom typing import List\n\n\n# Temporary wrapper for lm-format-enforcer  as they didnt update their method with new api changes from exllama\n# try to use formatron as alternative for higher performance and better support\n\n\n@lru_cache(32)\ndef _get_lmfe_tokenizer_data(tokenizer: ExLlamaV2Tokenizer):\n    return build_token_enforcer_tokenizer_data(tokenizer)\n\n\nclass ExLlamaV2TokenEnforcerFilter(ExLlamaV2Filter):\n\n    token_sequence: List[int]\n\n    def __init__(\n        self,\n        model: ExLlamaV2,\n        tokenizer: ExLlamaV2Tokenizer,\n        character_level_parser: CharacterLevelParser,\n    ):\n        super().__init__(model, tokenizer)\n        tokenizer_data = _get_lmfe_tokenizer_data(tokenizer)\n        self.token_enforcer = TokenEnforcer(tokenizer_data, character_level_parser)\n        self.token_sequence = []\n\n    def begin(self, prefix_str: str) -> None:\n        self.token_sequence = []\n\n    def feed(self, token) -> None:\n        self.token_sequence.append(int(token[0][0]))\n\n    def next(self):\n        allowed_tokens = self.token_enforcer.get_allowed_tokens(self.token_sequence)\n        return sorted(allowed_tokens), []\n\n    def use_background_worker(self):\n        return True"}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/transformers/model_support/llama3_2_vision/__init__.py", "content": ""}
{"type": "source_file", "path": "src/gallama/backend/tts/base.py", "content": "from typing import Any, AsyncIterator, Dict, List\nimport asyncio\nfrom .text_processor import TextToTextSegment\nfrom .TTSQueueHandler import TTSQueueHandler\nfrom ...data_classes import ModelSpec, TTSEvent\nfrom ...logger import logger\nimport time\n\n\nclass TTSBase:\n    \"\"\"this is the base interface for all TTS models\"\"\"\n\n    def __init__(self, model_spec: ModelSpec):\n        self.model_name = model_spec.model_name\n        self.model = None\n        self.model_path = model_spec.model_id or None\n\n        self.voice_list: List[str] = None\n        self.default_voice: Dict = None\n\n        self.voice: Dict = model_spec.voice\n        if self.voice:\n            self.voice_list = list(self.voice.keys())\n            self.default_voice = self.voice[self.voice_list[0]]\n\n        # backend specific arguments\n        self.backend_extra_args = model_spec.backend_extra_args\n\n        # each subclass must implement the model loading method\n        self.load_model(model_spec)\n\n        # for stopping\n        self._stop_event = asyncio.Event()\n        self._current_session_tracker: set = None\n\n\n    def load_model(self, model_spec: ModelSpec):\n        raise NotImplementedError(\n            \"The load_model method must be implemented by the derived class.\"\n        )\n\n    def stop(self):\n        \"\"\"Stop current processing\"\"\"\n        if self._current_session_tracker is not None:\n            self._stop_event.set()\n            if self._current_session_tracker:\n                self._current_session_tracker.clear()\n            self._current_session_tracker = None\n\n\n    async def text_to_speech(\n        self,\n        text: str,\n        language:str = \"auto\",\n        stream: bool = False,    # non stream return numpy array of whole speech, True will return iterator instead\n        speed_factor: float = 1.0,\n        queue: asyncio.Queue = None,\n        **kwargs: Any\n    ):\n        # If stream=False: Tuple of (sample_rate, concatenated_audio_data)\n        # If stream=True: None (audio chunks are sent to the provided queue)\n\n        raise NotImplementedError(\n            \"The synthesize method must be implemented by the derived class.\"\n        )\n\n    async def text_stream_to_speech_to_queue(\n        self,\n        text_queue: asyncio.Queue,\n        queue: asyncio.Queue,\n        language: str = \"auto\",\n        speed_factor: float = 1.0,\n        stream: bool = True,\n        **kwargs: Any\n    ) -> None:\n        pipeline = TextToTextSegment(\n            quick_start=True,\n            initial_segment_size=1,  # Start with single sentences\n            max_segment_size=4,  # Maximum of 4 sentences per segment\n            segment_size_increase_interval=1  # Increase segment size every 3 segments\n        )\n\n        # Create a set to track generations specific to this streaming session\n        self._current_session_tracker = set()\n        self._stop_event.clear()\n\n        try:\n            processing_task = asyncio.create_task(\n                pipeline.process_text_stream_async(input_queue=text_queue)\n            )\n\n            last_log_time = time.time()\n\n            while not self._stop_event.is_set():\n                try:\n                    segment = await pipeline.get_next_segment(timeout=0.1, raise_timeout=True)\n\n                    if segment and not self._stop_event.is_set():\n                        segment = segment.strip()\n                        await self.text_to_speech(\n                            queue=queue,\n                            text=segment,\n                            language=language,\n                            stream=True,\n                            speed_factor=speed_factor,\n                            session_tracker=self._current_session_tracker,\n                        )\n\n                except asyncio.TimeoutError:\n                    # Check if processing is complete\n                    if (not self._current_session_tracker and\n                            pipeline.processing_queue.qsize() == 0 and\n                            pipeline.text_buffer == \"\" and\n                            processing_task.done()):\n                        break\n                    elif self._stop_event.is_set():\n                        break\n                    else:\n                        continue\n\n        except Exception as e:\n            logger.error(f\"Error in text_stream_to_speech_to_queue: {str(e)}\", exc_info=True)\n            await queue.put(Exception(f\"Text-to-speech error: {str(e)}\"))\n            if hasattr(self, 'model') and hasattr(self.model, 'stop'):\n                self.model.stop()\n            raise\n\n        finally:\n            logger.info(\"Cleaning up pipeline\")\n            # Cancel the processing task if it's still running\n            if not processing_task.done():\n                processing_task.cancel()\n                try:\n                    await processing_task\n                except asyncio.CancelledError:\n                    pass\n\n            await queue.put(TTSEvent(type=\"text_end\"))\n            await pipeline.stop_processing()\n            await pipeline.reset()\n            await pipeline.clear_queue()\n\n            # Clear session tracker\n            self._current_session_tracker = None\n            self._stop_event.clear()\n"}
{"type": "source_file", "path": "src/gallama/backend/stt/hypothesis.py", "content": "from typing import List, Tuple\nfrom gallama.logger.logger import logger\nfrom ...data_classes import TimeStampedWord\n\n\nclass HypothesisBuffer:\n\n    def __init__(self):\n        \"\"\"\n        This class manages hypotheses (potential recognized words) from the ASR system and determines which words can be committed as finalized transcriptions.\n        \"\"\"\n        self.commited_in_buffer = []  # Stores finalized (committed) transcriptions.\n        self.buffer = []  # Stores new hypotheses inserted but not yet processed.\n        self.new: List[Tuple[float, float, str]] = []  # Stores new hypotheses inserted but not yet processed.\n\n        self.last_commited_time = 0  # Timestamp of the last committed word.\n        self.last_commited_word = None  # Last committed word.\n        self.is_final = False  # New flag for final processing\n\n\n        # Parameters for detecting repetition patterns\n        self.min_word_gap = 0.1  # Minimum expected gap between distinct words\n        self.max_merge_window = 2.0  # Maximum window to consider for merging\n\n    def detect_repetition_pattern(self, words):\n        \"\"\"\n        Analyze sequence of words to detect abnormal repetition patterns\n        Returns list of indices that are likely part of a repetition pattern\n        \"\"\"\n        if len(words) < 2:\n            return []\n\n        repetition_indices = []\n        for i in range(1, len(words)):\n            prev_time = words[i - 1][1]  # End time of previous word\n            curr_time = words[i][0]  # Start time of current word\n            time_gap = curr_time - prev_time\n\n            # Check for suspiciously small gaps between words\n            if time_gap < self.min_word_gap:\n                # Look for repetition of single characters or very short segments\n                if len(words[i][2]) <= 2 and words[i][2] == words[i - 1][2]:\n                    repetition_indices.extend([i - 1, i])\n\n        return list(set(repetition_indices))\n\n    def merge_adjacent_segments(self, words):\n        \"\"\"\n        Merge adjacent word segments that might be part of the same word\n        \"\"\"\n        if len(words) < 2:\n            return words\n\n        merged = []\n        i = 0\n        while i < len(words):\n            if i == len(words) - 1:\n                merged.append(words[i])\n                break\n\n            curr_word = words[i]\n            next_word = words[i + 1]\n\n            # Check if these segments should be merged\n            time_gap = next_word[0] - curr_word[1]\n            if time_gap < self.min_word_gap and len(curr_word[2]) <= 2 and len(next_word[2]) <= 2:\n                # Merge the segments\n                merged_text = curr_word[2] + next_word[2]\n                merged_segment = (curr_word[0], next_word[1], merged_text)\n                merged.append(merged_segment)\n                i += 2\n            else:\n                merged.append(curr_word)\n                i += 1\n\n        return merged\n\n    def insert(self, new: TimeStampedWord, offset: float):  # TODO\n        \"\"\"\n        Adjusts the timestamps of the new hypotheses by offset.\n        Filters out any hypotheses that are older than last_commited_time - 0.1.\n        Removes overlapping words between commited_in_buffer and new (using n-gram matching).\n\n        How the algorithm works:\n        compare self.commited_in_buffer and new. It inserts only the words in new that extend the commited_in_buffer, it means they are roughly behind last_commited_time and new in content\n        the new tail is added to self.new\n        \"\"\"\n\n        # Apply offset to timestamps\n        new = [(a + offset, b + offset, t) for a, b, t in new]\n\n        # Filter based on timestamp\n        filtered_new = [(a, b, t) for a, b, t in new if a > self.last_commited_time - 0.5]\n\n        if filtered_new:\n            # Detect repetition patterns\n            rep_indices = self.detect_repetition_pattern(filtered_new)\n\n            # Remove detected repetitions\n            filtered_new = [word for i, word in enumerate(filtered_new)\n                            if i not in rep_indices]\n\n            # Merge adjacent segments that might be part of the same word\n            filtered_new = self.merge_adjacent_segments(filtered_new)\n\n            self.new = filtered_new\n\n            # Perform n-gram matching with existing buffer\n            if self.new and abs(self.new[0][0] - self.last_commited_time) < 1:\n                if self.commited_in_buffer:\n                    cn = len(self.commited_in_buffer)\n                    nn = len(self.new)\n                    for i in range(1, min(min(cn, nn), 5) + 1):\n                        c = \" \".join([self.commited_in_buffer[-j][2]\n                                      for j in range(1, i + 1)][::-1])\n                        tail = \" \".join(self.new[j - 1][2]\n                                        for j in range(1, i + 1))\n                        if c == tail:\n                            for j in range(i):\n                                self.new.pop(0)\n                            break\n\n    def flush(self, force_commit=False):\n        \"\"\"\n        Commits words to the final transcription.\n        Args:\n            force_commit: If True, commits remaining words without waiting for confirmation\n        \"\"\"\n        commit = []\n\n        if force_commit:\n            # Commit all remaining words in buffer and new\n            remaining_words = []\n            if self.buffer:\n                remaining_words.extend(self.buffer)\n            if self.new:\n                remaining_words.extend(self.new)\n\n            # Sort by start time\n            remaining_words.sort(key=lambda x: x[0])\n\n            # Remove duplicates while preserving order\n            seen_words = {}  # Using dict to preserve order\n            for na, nb, nt in remaining_words:\n                # If we see the same word again, keep the one with the later timestamp\n                if nt in seen_words:\n                    prev_na, prev_nb, _ = seen_words[nt]\n                    if nb > prev_nb:\n                        seen_words[nt] = (na, nb, nt)\n                else:\n                    seen_words[nt] = (na, nb, nt)\n\n            # Convert back to list\n            commit = list(seen_words.values())\n\n            # Update last committed info\n            if commit:\n                self.last_commited_word = commit[-1][2]\n                self.last_commited_time = commit[-1][1]\n\n            self.buffer = []\n            self.new = []\n\n        else:\n            # Normal matching-based commitment\n            while self.new and self.buffer:\n                na, nb, nt = self.new[0]\n                if nt == self.buffer[0][2]:\n                    commit.append((na, nb, nt))\n                    self.last_commited_word = nt\n                    self.last_commited_time = nb\n                    self.buffer.pop(0)\n                    self.new.pop(0)\n                else:\n                    break\n\n            self.buffer = self.new\n            self.new = []\n\n        self.commited_in_buffer.extend(commit)\n        return commit\n\n    def set_final(self):\n        \"\"\"Mark the buffer as final, triggering complete word commitment\"\"\"\n        self.is_final = True\n        return self.flush(force_commit=True)\n\n    def pop_committed(self, time):\n        while self.commited_in_buffer and self.commited_in_buffer[0][1] <= time:\n            self.commited_in_buffer.pop(0)\n\n    def complete(self):\n        \"\"\"Returns all remaining words in the buffer\"\"\"\n        return self.buffer\n"}
{"type": "source_file", "path": "src/gallama/backend/stt/asr_processor.py", "content": "from .hypothesis import HypothesisBuffer\nfrom .base import ASRBase\nimport numpy as np\nfrom typing import Union, BinaryIO, Optional\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nimport os\nfrom datetime import datetime\nimport soundfile as sf\nfrom ...data_classes import TranscriptionResponse, LanguageType\nfrom ...data_classes.realtime_client_proto import TurnDetectionConfig\nfrom .audio_buffer import AudioBufferWithTiming\nfrom .vad import VADProcessor\nfrom gallama.logger import logger\n\n\nclass ASRProcessor:\n    \"\"\"\n    ASRProcessor processes audio buffers for real-time ASR (Automatic Speech Recognition).\n    It manages an audio buffer, transcribes audio using an ASR backend, and trims processed portions\n    to maintain efficiency. Supports trimming based on sentences or audio segments.\n    \"\"\"\n\n    SAMPLING_RATE = 16000  # Standard audio sampling rate\n\n    def __init__(\n        self,\n        asr: ASRBase,\n        tokenizer=None,\n        buffer_trimming=(\"segment\", 15),\n        min_context_needed=3.0,  # require 3 second for a content to be process -> higher number better accuracy\n        debug_audio_dir=None,  # New parameter for debug audio directory\n        vad_config: Optional[TurnDetectionConfig] = None,\n    ):\n        \"\"\"\n        Initializes the ASR processor.\n\n        Args:\n            asr: ASR backend object for transcribing audio.\n            tokenizer: Sentence tokenizer for the target language. Required if trimming mode is \"sentence\".\n            buffer_trimming: Tuple (mode, duration).\n                             - mode: \"sentence\" or \"segment\" (trimming method).\n                             - duration: Max buffer length (in seconds) before trimming occurs.\n            debug_audio_dir: Directory to store debug audio files\n        \"\"\"\n        self.asr = asr\n        self.tokenizer = tokenizer\n        self.debug_audio_dir = debug_audio_dir\n        self.min_context_needed = min_context_needed\n\n        if debug_audio_dir and not os.path.exists(debug_audio_dir):\n            os.makedirs(debug_audio_dir)\n\n        self.trimming_mode, self.trimming_duration = buffer_trimming\n\n        # Initialize two separate buffers\n        self.audio_buffer = AudioBufferWithTiming(sample_rate=self.SAMPLING_RATE)\n        self.vad_audio_buffer = AudioBufferWithTiming(sample_rate=self.SAMPLING_RATE)\n\n        self.transcript_buffer = HypothesisBuffer()\n        self.transcript_buffer.last_committed_time = 0\n        self.committed_transcriptions = []\n        self.buffer_count = 0\n\n        self.vad_config = vad_config if vad_config else TurnDetectionConfig()\n        self.vad_enable: bool = self.vad_config.create_response\n\n        self.vad = None\n        if self.vad_enable:\n            self.initialize_vad(self.vad_config)\n\n        self.vad_speech_active = False\n        self.vad_start_sent = False\n        self.vad_end_sent = False\n\n    def add_audio_chunk(self, audio_chunk):\n        \"\"\"\n        Adds an audio chunk to the appropriate buffer based on VAD mode.\n        \"\"\"\n        if self.vad_enable:\n            self.vad_audio_buffer.add_chunk(audio_chunk)\n        else:\n            self.audio_buffer.add_chunk(audio_chunk)\n\n    def reset_buffers_for_mode_change(self, new_vad_enable: bool):\n        \"\"\"\n        Reset the appropriate buffer when switching between VAD and non-VAD modes.\n        \"\"\"\n        if new_vad_enable and not self.vad_enable:\n            # Switching from non-VAD to VAD mode: reset the VAD buffer\n            logger.info(\"Switching to VAD mode: resetting VAD buffer\")\n            self.vad_audio_buffer.reset()\n        elif not new_vad_enable and self.vad_enable:\n            # Switching from VAD to non-VAD mode: reset the non-VAD buffer\n            logger.info(\"Switching to non-VAD mode: resetting non-VAD buffer\")\n            self.audio_buffer.reset()\n\n        # Update the VAD enable flag\n        self.vad_enable = new_vad_enable\n\n\n    def process_audio(self, is_final: bool = False):\n        \"\"\"\n        Processes the current audio buffer, transcribes it, and handles buffer trimming.\n        \"\"\"\n        if self.vad_enable:\n            buffer = self.vad_audio_buffer\n        else:\n            buffer = self.audio_buffer\n\n        if len(buffer) == 0:\n            logger.debug(\"Audio buffer is empty, returning early\")\n            return None, None, \"\", []\n\n        buffer.mark_processing_start()\n\n        vad_events = []\n        speech_start_ms = None\n        speech_end_ms = None\n\n        if self.vad_enable and self.vad:\n            current_offset = buffer.last_processed_sample_vad\n            vad_result_start, vad_result_end = self.vad.process_chunk(buffer, current_offset)\n\n            logger.info(f\"VAD result_start: {vad_result_start}\")\n            logger.info(f\"VAD result_end: {vad_result_end}\")\n\n            if vad_result_start and vad_result_start['speech_detected'] and not self.vad_speech_active and not self.vad_start_sent:\n                self.vad_speech_active = True\n                self.vad_start_sent = True\n                self.vad_end_sent = False\n                speech_start_ms = vad_result_start.get('start_time', 0.0)\n                vad_events.append({\n                    'type': 'start',\n                    'timestamp_ms': int(speech_start_ms),\n                    'confidence': vad_result_start.get('confidence', 0.0)\n                })\n                logger.info(f\"Speech start event queued at {speech_start_ms}ms\")\n\n            if vad_result_end and (vad_result_end['speech_ended'] or (is_final and self.vad_speech_active)) and not self.vad_end_sent:\n                is_final = True\n                self.vad_speech_active = False\n                self.vad_end_sent = True\n                self.vad_start_sent = False\n                speech_end_ms = vad_result_end.get(\"end_time\") if self.vad else buffer.get_time_ms(current_offset)\n                vad_events.append({\n                    'type': 'end',\n                    'timestamp_ms': int(speech_end_ms),\n                    'confidence': vad_result_end.get('confidence', 0.0)\n                })\n                logger.info(f\"Speech end event queued at {speech_end_ms}ms\")\n\n        audio_to_process = buffer.get_unprocessed_audio()\n        if len(audio_to_process) == 0:\n            logger.debug(\"No new audio to process\")\n            return None, None, \"\", vad_events\n\n        if self.vad_enable and speech_start_ms is not None:\n            start_sample = int((speech_start_ms / 1000) * self.SAMPLING_RATE)\n            if speech_end_ms is not None:\n                end_sample = min(int((speech_end_ms / 1000) * self.SAMPLING_RATE), len(audio_to_process))\n            else:\n                end_sample = len(audio_to_process)\n            audio_to_process = audio_to_process[start_sample:end_sample]\n\n        current_duration_ms = (len(audio_to_process) / self.SAMPLING_RATE) * 1000\n        current_duration = current_duration_ms / 1000\n        if current_duration < self.min_context_needed and not is_final:\n            buffer.is_processing = False\n            return None, None, \"\", vad_events\n\n        try:\n            prompt, context = self.construct_prompt()\n            asr_results = self.asr.transcribe_to_segment(\n                audio_to_process,\n                init_prompt=prompt,\n                language=self.vad_config.language if self.vad_enable else None,\n            )\n            timestamped_words = self.asr.segment_to_timestamped_words(asr_results)\n            buffer_start_time = ((buffer.start_offset + buffer.last_processed_sample) / self.SAMPLING_RATE)\n            self.transcript_buffer.insert(timestamped_words, buffer_start_time)\n\n            if is_final:\n                confirmed_transcriptions = self.transcript_buffer.set_final()\n            else:\n                confirmed_transcriptions = self.transcript_buffer.flush()\n\n            self.committed_transcriptions.extend(confirmed_transcriptions)\n\n            if not is_final:\n                if self.trimming_mode == \"sentence\" and confirmed_transcriptions:\n                    if current_duration > self.trimming_duration:\n                        self.trim_to_last_completed_sentence()\n                elif self.trimming_mode == \"segment\":\n                    if current_duration > self.trimming_duration:\n                        self.trim_to_last_completed_segment(asr_results)\n\n            buffer.mark_processing_complete(is_final)\n\n            start_time, end_time, transcription = self.format_output(confirmed_transcriptions)\n            return start_time, end_time, transcription, vad_events\n\n        except Exception as e:\n            logger.error(f\"Error in process_audio: {str(e)}\")\n            buffer.is_processing = False\n            return None, None, \"\", vad_events\n\n    def initialize_vad(self, vad_config: TurnDetectionConfig):\n        logger.info(\"Initializing VAD processor\")\n        self.vad = VADProcessor(vad_config)\n\n    def update_vad_config(self, vad_config: TurnDetectionConfig):\n        \"\"\"\n        Update the VAD configuration and reset buffers if the mode changes.\n        \"\"\"\n        new_vad_enable = vad_config.create_response if vad_config else False\n\n        # Check if the VAD mode is changing\n        if new_vad_enable != self.vad_enable:\n            self.reset_buffers_for_mode_change(new_vad_enable)\n\n        # Update the VAD configuration\n        self.vad_config = vad_config if vad_config else TurnDetectionConfig()\n\n        if new_vad_enable:\n            # Initialize VAD and the VAD audio buffer if not already initialized\n            if self.vad_audio_buffer is None:\n                self.vad_audio_buffer = AudioBufferWithTiming(sample_rate=self.SAMPLING_RATE)\n            self.initialize_vad(self.vad_config)\n            self.vad_enable = True\n        else:\n            self.vad = None\n            self.vad_enable = False\n\n        # Reset VAD state tracking\n        self.vad_speech_active = False\n        self.vad_start_sent = False\n        self.vad_end_sent = False\n\n\n    def reset_state(self, initial_offset=0):\n        \"\"\"Resets the internal state of the processor.\"\"\"\n        self.audio_buffer.reset()\n        if self.vad_enable:\n            self.vad_audio_buffer.reset()  # Clear the VAD-specific buffer\n\n        self.transcript_buffer = HypothesisBuffer()\n        self.transcript_buffer.last_committed_time = initial_offset * 1000\n        self.committed_transcriptions = []\n        # Reset VAD state tracking\n        self.vad_speech_active = False\n        self.vad_start_sent = False\n        self.vad_end_sent = False\n        if self.vad:\n            self.vad.reset()\n\n\n    def save_debug_audio(self, audio_data: np.ndarray, suffix: str = \"\"):\n        \"\"\"\n        Saves the current audio buffer to a WAV file for debugging.\n\n        Args:\n            audio_data: Audio data to save\n            suffix: Optional suffix to add to the filename\n        \"\"\"\n        if self.debug_audio_dir:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            filename = f\"audio_buffer_{timestamp}_{self.buffer_count}{suffix}.wav\"\n            filepath = os.path.join(self.debug_audio_dir, filename)\n\n            try:\n                sf.write(filepath, audio_data, self.SAMPLING_RATE)\n                logger.info(f\"Saved debug audio to {filepath}\")\n            except Exception as e:\n                logger.error(f\"Error saving debug audio: {str(e)}\")\n\n            self.buffer_count += 1\n\n\n    def trim_buffer_at(self, timestamp):\n        \"\"\"\n        Trims the audio buffer and transcription buffer at a specified timestamp.\n\n        Args:\n            timestamp: The time (in seconds) to trim the buffers up to.\n        \"\"\"\n        self.transcript_buffer.pop_committed(timestamp)\n        cut_samples = int((timestamp - (self.audio_buffer.start_offset / self.SAMPLING_RATE)) * self.SAMPLING_RATE)\n        self.audio_buffer.clear_until(cut_samples)\n\n    # Rest of the methods remain the same as they don't directly interact with the buffer\n    def transcribe(\n        self,\n        audio: Union[str, BinaryIO, np.ndarray],\n        init_prompt: str = \"\",\n        temperature: float = 0.0,\n        language: Optional[LanguageType] = None,\n        include_segments: bool = False,\n        batch: bool = False,\n    ) -> TranscriptionResponse:\n\n        return self.asr.transcribe(\n            audio,\n            init_prompt=init_prompt,\n            temperature=temperature,\n            language=language,\n            include_segments=include_segments,\n            batch=batch\n        )\n\n    async def transcribe_async(\n        self, audio: Union[str, BinaryIO, np.ndarray],\n        init_prompt: str = \"\",\n        temperature: float = 0.0,\n        language: str = None,\n        include_segments: bool = False,\n        batch: bool = False,\n    ) -> TranscriptionResponse:\n        loop = asyncio.get_event_loop()\n        with ThreadPoolExecutor() as pool:\n            return await loop.run_in_executor(\n                pool,\n                self.transcribe,\n                audio, init_prompt, temperature, language, include_segments, batch\n            )\n\n    def construct_prompt(self):\n        \"\"\"\n        Constructs a prompt from previously confirmed transcriptions for context during ASR.\n\n        Returns:\n            Tuple (prompt, context):\n                - prompt: A 200-character string of previous context for the ASR model.\n                - context: The remaining transcriptions within the current audio buffer for logging/debugging.\n        \"\"\"\n        # Find the first committed transcription outside the current buffer\n        start_index = max(0, len(self.committed_transcriptions) - 1)\n        while start_index > 0 and self.committed_transcriptions[start_index - 1][1] > self.audio_buffer.get_time_ms(self.audio_buffer.last_processed_sample):\n            start_index -= 1\n\n        past_context = self.committed_transcriptions[:start_index]\n        past_texts = [text for _, _, text in past_context]\n\n        # Build prompt (up to 200 characters)\n        prompt, current_length = [], 0\n        while past_texts and current_length < 200:\n            last_text = past_texts.pop()\n            current_length += len(last_text) + 1\n            prompt.append(last_text)\n\n        remaining_transcriptions = self.committed_transcriptions[start_index:]\n        return self.asr.sep.join(reversed(prompt)), self.asr.sep.join(text for _, _, text in remaining_transcriptions)\n\n    def trim_to_last_completed_sentence(self):\n        \"\"\"\n        Trims the audio buffer and committed transcriptions up to the end of the last completed sentence.\n        \"\"\"\n        if not self.committed_transcriptions:\n            return\n\n        sentences = self.segment_transcriptions_into_sentences(self.committed_transcriptions)\n        if len(sentences) < 2:\n            return  # Not enough sentences to trim\n\n        # Keep the last two sentences; trim up to the second-last one\n        trim_timestamp = sentences[-2][1]\n        self.trim_buffer_at(trim_timestamp)\n\n    def trim_to_last_completed_segment(self, transcription_results):\n        \"\"\"\n        Trims the audio buffer to the last completed audio segment.\n\n        Args:\n            transcription_results: Transcription results from the ASR system.\n        \"\"\"\n        if not self.committed_transcriptions:\n            return\n\n        segment_end_times = self.asr.segments_end_ts(transcription_results)\n        if len(segment_end_times) > 1:\n            trim_timestamp = segment_end_times[-2] + (self.audio_buffer.start_offset / self.SAMPLING_RATE)\n            if trim_timestamp <= self.committed_transcriptions[-1][1]:\n                self.trim_buffer_at(trim_timestamp)\n\n    def segment_transcriptions_into_sentences(self, words):\n        \"\"\"\n        Segments committed transcriptions into sentences using the tokenizer.\n\n        Args:\n            words: List of transcribed words with timestamps.\n\n        Returns:\n            List of tuples [(start_time, end_time, sentence), ...].\n        \"\"\"\n        if not self.tokenizer:\n            raise ValueError(\"Tokenizer is required for sentence segmentation.\")\n\n        complete_words = list(words)\n        text = \" \".join(word[2] for word in complete_words)\n        sentences = self.tokenizer.split(text)\n\n        segmented = []\n        for sentence in sentences:\n            start, end = None, None\n            while complete_words:\n                word_start, word_end, word = complete_words.pop(0)\n                if start is None and sentence.startswith(word):\n                    start = word_start\n                if sentence.strip() == word:\n                    end = word_end\n                    segmented.append((start, end, sentence.strip()))\n                    break\n                sentence = sentence[len(word):].strip()\n        return segmented\n\n    def format_output(self, sentences):\n        \"\"\"\n        Formats the confirmed sentences for output.\n\n        Args:\n            sentences: List of sentences [(start_time, end_time, sentence), ...].\n\n        Returns:\n            A tuple (start_time, end_time, combined_text) or (None, None, \"\") if empty.\n        \"\"\"\n        if not sentences:\n            return None, None, \"\"\n\n        start_time = sentences[0][0]\n        end_time = sentences[-1][1]\n        combined_text = self.asr.sep.join(sentence[2] for sentence in sentences)\n        return start_time, end_time, combined_text"}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/transformers/transformers.py", "content": "from ..base import ModelInterface\nfrom typing import Optional, Dict, List, Union\nimport transformers\nimport time                                 # for compute of generation time\nimport asyncio\nfrom fastapi import Request                 # for type hint\nfrom threading import Thread\nfrom importlib import import_module\n\n# custom model support\nfrom qwen_vl_utils import process_vision_info\n\n# format enforcement with formatron\ntry:\n    from formatron.formatter import FormatterBuilder\n    from formatron.integrations.transformers import create_formatter_logits_processor_list\nexcept ImportError:\n    FormatterBuilder = None\n    create_formatter_logits_processor_list = None\n\n# format enforcement with lmfe\ntry:\n    from lmformatenforcer.tokenenforcer import TokenEnforcerTokenizerData\n    from lmformatenforcer.integrations.transformers import (\n        build_transformers_prefix_allowed_tokens_fn,\n    )\nexcept ImportError:\n    TokenEnforcerTokenizerData = None\n    build_transformers_prefix_allowed_tokens_fn = None\n\n\nfrom .model_support.llama3_2_vision.text_streamer import CustomTextIteratorStreamer\n\n\nfrom gallama.utils import is_flash_attention_installed\nfrom gallama.logger.logger import logger\n\n# custom data classes\nfrom gallama.data_classes import (\n    ModelSpec,\n    GenStart,\n    GenEnd,\n    GenQueue,\n    GenText,\n    GenerationStats,\n    QueueContext,\n    GenQueueDynamic\n)\n\nclass ModelTransformers(ModelInterface):\n    def __init__(self, model_spec:ModelSpec):\n        super().__init__(model_spec)\n        self.model, self.tokenizer, self.processor = self.load_model()\n\n\n    def load_model(self):\n        \"\"\"Load the model, tokenizer, cache, and optional processor.\"\"\"\n        # processor is for multimodal\n        model, tokenizer, processor = self.load_model_transformers(\n            model_id=self.model_id,\n            gpus=self.gpus,\n        )\n\n        # load draft model\n        if self.draft_model_id is not None:\n            raise \"Draft model currently not supported for llama cpp backend\"\n\n        self.eos_token_ids = self.generate_eos_tokens_id()\n\n        return model, tokenizer, processor\n\n    def load_model_transformers(\n        self,\n        model_id,\n        gpus,\n    ):\n        \"\"\"This function return the model and its tokenizer\"\"\"\n        logger.info(\"Loading model: \" + model_id)\n\n        cache = None  # in case not a backend with separate cache like llama cpp\n        tokenizer = None\n        processor = None\n\n        # helper function for dynamic class loading\n        def get_class(class_string):\n            module_name, class_name = class_string.rsplit('.', 1)\n            module = import_module(module_name)\n            return getattr(module, class_name)\n\n        # arguments for model loading\n        model_kargs = {\n            'pretrained_model_name_or_path': self.model_id,\n            'torch_dtype' : \"auto\",\n            'device_map': \"auto\",\n        }\n\n        tokenizer_args = {\n            'pretrained_model_name_or_path': self.model_id,\n        }\n\n        # check if flash attention enabled\n        flash_installed, flash_version = is_flash_attention_installed()\n        if flash_installed:\n            model_kargs[\"attn_implementation\"]  = \"flash_attention_2\"\n\n\n        # determine the class to use for loading\n        logger.info(self.backend_extra_args)\n        if self.backend_extra_args.get('model_class'):\n            model_class = get_class(self.backend_extra_args['model_class'])\n\n            model_extra_kwargs = self.backend_extra_args.get('model_class_extra_kwargs')\n            if model_extra_kwargs:\n                model_kargs.update(model_extra_kwargs)      # update any extra argument\n        else:\n            model_class = transformers.AutoModelForCausalLM\n\n        if self.backend_extra_args.get('tokenizer_class'):\n            tokenizer_class = get_class(self.backend_extra_args['tokenizer_class'])\n        else:\n            tokenizer_class = transformers.AutoTokenizer\n\n        if self.backend_extra_args.get('processor_class'):\n            processor_class = get_class(self.backend_extra_args['processor_class'])\n        else:\n            processor_class = None\n\n        # currently speculative decoding not supported by model specific for LLama CPP python\n        if isinstance(gpus, str) and gpus == \"auto\":\n            logger.info(model_kargs)\n            model = model_class.from_pretrained(**model_kargs)\n            tokenizer = tokenizer_class.from_pretrained(**tokenizer_args)\n            if processor_class:\n                processor = processor_class.from_pretrained(**tokenizer_args)\n\n        elif isinstance(gpus, list):\n            raise \"Specifying GPU for transformers is not supported\"\n\n        else:\n            raise ValueError(\"Device map should be either 'auto', 'gpu' split\")\n\n        # set max_seq_len based on model    TODO: To find more reliable method\n        try:\n            self.max_seq_len = model.config.max_position_embeddings\n        except:\n            # for llama 3.2\n            self.max_seq_len = model.config.text_config.max_position_embeddings\n\n        return model, tokenizer, processor\n\n\n\n    def generate_eos_tokens_id(self) -> List[int]:\n        \"\"\"Generate the end-of-sequence token IDs.\"\"\"\n\n        # currently the stop word for llama cpp work by string and not by token id\n        # hence usage of this is not required\n        return []\n\n    # *************** generation method from here\n    async def _run_generation(\n        self,\n        #prompt,\n        input_ids,\n        max_tokens,\n        temperature,\n        stop,\n        gen_queue_list,\n        top_p=0.8,\n        prefix_strings=None,\n        stop_word_to_return=\"\",\n        gen_type_str: str = \"text\",\n        logits_processor=None,              # for formatron format enforcement\n        prefix_allowed_tokens_fn=None       # for lmfe format enforcement\n    ):\n        # Tokenize the prompt\n        input_ids = input_ids.to(self.model.device)\n\n        # Create generation config\n        generation_config = transformers.GenerationConfig(\n            max_new_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            stop_strings=stop,\n            repetition_penalty=1.1,\n            do_sample=True,\n        )\n\n        # Create streamer\n        streamer = CustomTextIteratorStreamer(\n            tokenizer=self.processor if self.processor else self.tokenizer,\n            skip_prompt=True,\n            skip_special_tokens=True,\n        )\n\n        # Run the llm's generate function in a separate thread\n        thread = Thread(\n            target=self.model.generate,\n            kwargs={\n                **input_ids,\n                'generation_config': generation_config,\n                'streamer': streamer,\n                'tokenizer': self.tokenizer,\n                'logits_processor': logits_processor,\n                'prefix_allowed_tokens_fn': prefix_allowed_tokens_fn,\n            }\n        )\n        thread.start()\n\n        generate_text = \"\"\n        try:\n            for chunk in streamer:\n                chunk_text = GenText(content=chunk, text_type=gen_type_str)\n                generate_text += chunk\n                for g_queue in gen_queue_list:\n                    await g_queue.get_queue().put(chunk_text)\n\n                # Yield control back to the event loop\n                await asyncio.sleep(0)\n        finally:\n            # Wait for the generation thread to finish\n            thread.join()\n\n        # Handle stop word if present\n        if stop_word_to_return:\n            chunk_text = GenText(content=stop_word_to_return, text_type=gen_type_str)\n            generate_text += chunk_text.content\n            for g_queue in gen_queue_list:\n                await g_queue.get_queue().put(chunk_text)\n\n        return generate_text\n\n\n    async def generate(\n        self,\n        prompt: str,\n        gen_queue: Union[GenQueue, QueueContext, List[QueueContext]],\n        request: Optional[Request] = None,\n        gen_type: Union[str, GenStart] = \"text\",    # the generated result will be store to this queue\n        temperature: float = 0.01,\n        top_p: float = 0.8,\n        formatter: FormatterBuilder | TokenEnforcerTokenizerData = None,\n        stop_words: Union[List[str], str] = None,\n        prefix_strings: Optional[Union[str, List[str]]] = None,\n        banned_strings: list[str] | None = None,\n        max_tokens: int = None,\n        quiet=False,\n        messages: List = None,  # query.message for multimodal\n        **kwargs,\n    ) -> (str, GenerationStats):\n\n        if not quiet:\n            logger.info(\"----------------------Prompt---------------\\n\" + prompt)\n            logger.debug(\"----------------------temperature---------\\n\" + str(temperature))\n\n\n        # make gen_queue to List[QueueContext] for standardize downstream handling\n        gen_queue_list = None\n        # Convert gen_queue to List[GenQueueDynamic] format to standardize downstream handling\n        gen_queue_list = []\n        if isinstance(gen_queue, GenQueueDynamic):\n            gen_queue_list = [gen_queue]\n        elif isinstance(gen_queue, GenQueue):\n            # Wrap the GenQueue in a GenQueueDynamic\n            gen_queue_list = [GenQueueDynamic(existing_queue=gen_queue, include_GenStats=True, include_GenEnd=True)]\n        elif isinstance(gen_queue, list):\n            # Ensure all items in the list are GenQueueDynamic objects\n            for queue in gen_queue:\n                if isinstance(queue, GenQueueDynamic):\n                    gen_queue_list.append(queue)\n                elif isinstance(queue, GenQueue):\n                    # Wrap the GenQueue in a GenQueueDynamic\n                    gen_queue_list.append(\n                        GenQueueDynamic(existing_queue=queue, include_GenStats=True, include_GenEnd=True))\n                else:\n                    raise TypeError(\"gen_queue list must contain only GenQueue or GenQueueDynamic objects\")\n        else:\n            raise TypeError(\"gen_queue must be either a GenQueue, GenQueueDynamic, or a list of GenQueueDynamic\")\n\n        # vision support\n        image_inputs, video_inputs = None, None\n        if messages:\n            messages_as_dicts = [message.dict() for message in messages]\n\n            # convert OpenAI to qwen format -> TODO find more generalized method\n            # OpenAI format for image_url:\n            # {\n            #     \"type\": \"image\",\n            #     \"image\": {\n            #         \"image_url\": {\n            #             \"url\": \"url here\"\n            #         }\n            #     }\n            # }\n            # convert to qwen2 VL format:\n            # {\n            #     \"type\": \"image_url\",\n            #     \"image_url\": \"url here\"\n            # }\n            for one_message in messages_as_dicts:\n                if isinstance(one_message[\"content\"], list):\n                    for message in one_message[\"content\"]:\n                        if message.get(\"type\") == \"image_url\":\n                            message[\"type\"] = \"image\"\n                            message[\"image\"] = message[\"image_url\"][\"url\"]\n                            message.pop(\"image_url\", None)\n\n\n            image_inputs, video_inputs = process_vision_info(messages_as_dicts)\n\n        # convert prompt to token id\n        if image_inputs is None and video_inputs is None:\n            input_ids = self.tokenizer(prompt, return_tensors=\"pt\")\n        else:   # multimodal\n            input_ids = self.processor(\n                text=[prompt],\n                images=image_inputs,\n                #videos=video_inputs,       # TODO currently Llama doesnt support videos, comment out for now.\n                #padding=True,\n                add_special_tokens=False,\n                return_tensors=\"pt\",\n            )\n\n        self.validate_token_length(len(input_ids))\n\n        # format enforcer\n        logits_processor = None\n        prefix_allowed_tokens_fn = None\n        if formatter:\n            if isinstance(formatter, FormatterBuilder):\n                logits_processor = create_formatter_logits_processor_list(self.tokenizer, formatter)\n            else:\n                prefix_allowed_tokens_fn = build_transformers_prefix_allowed_tokens_fn(self.tokenizer, formatter)\n\n        # find stop conditions\n        stop_word_to_return = \"\"\n        if stop_words:\n            if isinstance(stop_words, str):\n                stop_word_to_return = stop_words\n                stop_words = [stop_words]\n\n            elif isinstance(stop_words, list):\n                stop_word_to_return = stop_words[0]\n\n            if not self.eos_token_str:\n                raise Exception(\"EOS token not set in model_config\")\n            stop_conditions = self.eos_token_str + stop_words  # concat the 2 list\n            logger.debug(\"stop_words: \" + str(stop_conditions))\n        else:\n            stop_conditions = self.eos_token_str\n\n        max_tokens_to_use = min(\n            self.max_seq_len - len(input_ids),\n            max_tokens, 4096) if max_tokens else min(self.max_seq_len - len(input_ids), 4096)\n\n        # kickstart the generation and let down stream know gen type\n        if isinstance(gen_type, str):\n            gen_type_str = gen_type\n            gen_type = GenStart(gen_type=gen_type)\n        else:\n            gen_type_str = gen_type.gen_type  # get out the generation type in str format\n\n        for g_queue in gen_queue_list:\n            g_queue.put_nowait(gen_type)\n\n        # Create a task to check for disconnection\n        # pass\n\n        # generate\n        start_time = time.time()\n        generate_text = await self._run_generation(\n            input_ids=input_ids,\n            logits_processor=logits_processor,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            max_tokens=max_tokens_to_use,\n            temperature=temperature,\n            stop=stop_words,\n            gen_queue_list=gen_queue_list,\n            top_p=top_p,\n            prefix_strings=None,\n            stop_word_to_return=\"\",\n            gen_type_str=gen_type_str,\n        )\n\n        duration = time.time() - start_time\n\n        gen_stats = GenerationStats()\n        for g_queue in gen_queue_list:\n            if g_queue.include_GenStats:\n                g_queue.put_nowait(gen_stats)\n\n        # this to signal the end of generation\n        for g_queue in gen_queue_list:\n            if g_queue.include_GenEnd:\n                g_queue.put_nowait(GenEnd())\n\n        logger.debug(\"----------------------LLM Raw Response---------------\\n\" + generate_text)"}
{"type": "source_file", "path": "src/gallama/backend/tts/__init__.py", "content": "from .model.kokoro import TTSKokoro\n\ntry:\n    from .model.gpt_sovits import TTS_GPT_SoVITS\nexcept ModuleNotFoundError:\n    pass"}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/llamacpp/__init__.py", "content": ""}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/exllama/__init__.py", "content": ""}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/transformers/__init__.py", "content": ""}
{"type": "source_file", "path": "src/gallama/backend/stt/__init__.py", "content": "try:\n    from .faster_whisper.model import ASRFasterWhisper\nexcept ModuleNotFoundError:\n    ASRFasterWhisper = None\n\ntry:\n    from .mlx_whisper.model import ASRMLXWhisper\nexcept ModuleNotFoundError:\n    ASRMLXWhisper = None\n\n\nfrom .asr_processor import ASRProcessor"}
{"type": "source_file", "path": "src/gallama/backend/llm/tools.py", "content": "from pydantic import BaseModel, Field, ValidationError, create_model\nfrom typing import Literal, Type, Union, Optional, List, Any, Dict\nfrom formatron.schemas.pydantic import ClassSchema, Schema\nfrom pathlib import Path\nfrom tempfile import TemporaryDirectory\nfrom ...data_classes import ToolSpec\nimport json\nfrom datamodel_code_generator import generate, InputFileType, DataModelType\nfrom ...logger.logger import logger\n\n\nclass Tools:\n    def __init__(self, prompt_eng, tools: [List[ToolSpec]], tool_choice):\n        self.prompt_eng = prompt_eng\n        self.tools = tools\n        self.tools_list = self.create_pydantic_model_from_tools(self.tools, mode=\"pydantic_v2\")\n        self.tools_list_formatron = self.create_pydantic_model_from_tools(self.tools, mode=\"formatron\")\n        self.tool_dict = {tool.schema()['title']: tool for tool in self.tools_list}\n        self.tool_dict_formatron = {tool.schema()['title']: tool for tool in self.tools_list_formatron}\n        self.answer_format = None\n        self.json_parser = None\n        self.tool_choice = tool_choice\n\n        # initialize tool as code\n        self.tool_def_as_code: str = self.create_tool_def_as_code()\n        logger.debug(f\"tool_def_as_code: {self.tool_def_as_code}\")\n\n    def create_tool_def_as_code(self):\n        _temp_tool_list = [\n            self.generate_pydantic_model_from_json_schema(tool.model_json_schema())\n            for tool in self.tools_list\n        ]\n        return self.append_code_without_duplicate_imports(_temp_tool_list)\n\n    @property\n    def tool_name_list(self):\n        return \", \".join(f'\"{name}\"' for name in self.tool_dict.keys())\n\n\n    @staticmethod\n    def type_from_json_schema(schema: dict):\n        \"\"\"Map JSON Schema types to Python types with appropriate handling for enums and additional types.\"\"\"\n        if 'anyOf' in schema:\n            return Union[tuple(Tools.type_from_json_schema(sub_schema) for sub_schema in schema['anyOf'])]\n\n        schema_type = schema['type']\n\n        type_mapping = {\n            'string': str,\n            'integer': int,\n            'number': float,\n            'boolean': bool,\n            'object': Dict[str, Any],  # Represents JSON objects\n            'array': List[Any],        # Represents JSON arrays\n            'null': type(None)         # Represents JSON null\n        }\n\n        if schema_type not in type_mapping:\n            raise ValueError(f\"Unsupported JSON schema type: {schema_type}\")\n\n        if 'enum' in schema:\n            # Define the type as a Python Literal with enumerated values.\n            return Literal[tuple(schema['enum'])]\n\n        if schema_type == 'object':\n            # Recursively create a Pydantic model for nested objects.\n            return Tools.create_pydantic_model_v2({\n                'name': schema.get('title', 'NestedObject'),\n                'description': schema.get('description', ''),\n                'parameters': schema\n            })\n\n        if schema_type == 'array':\n            # Handle array type with items\n            items_schema = schema.get('items', {})\n            items_type = Tools.type_from_json_schema(items_schema)\n            return List[items_type]\n\n        return type_mapping[schema_type]\n\n\n    @staticmethod\n    def create_pydantic_model_v1(function_info: dict) -> Type[BaseModel]:\n        parameters = function_info['parameters']\n        properties = parameters.get('properties', {})\n        required_properties = set(function_info['parameters'].get('required', []))\n\n        # Redefined how fields are set up\n        attributes = {}\n        for prop_name, prop_info in properties.items():\n            field_type = Tools.type_from_json_schema(prop_info)\n            field_description = prop_info.get('description', None)\n\n            if prop_name in required_properties:\n                # Required properties should not have a default value\n                attributes[prop_name] = (field_type, ..., field_description)\n            else:\n                # Optional properties default to None\n                attributes[prop_name] = (Optional[field_type], None, field_description)\n\n        # Dynamically create Pydantic model with explicit annotations\n        #print(attributes)\n        namespace = {'__annotations__': {}}\n        for attr_name, (attr_type, default, field_description) in attributes.items():\n            namespace['__annotations__'][attr_name] = attr_type\n            if default is not ...:\n                namespace[attr_name] = Field(default=default, description=field_description)\n            else:\n                namespace[attr_name] = Field(..., description=field_description)\n\n        # Dynamically create Pydantic model with explicit annotations\n        # The comma is crucial here—it signifies that this is not merely a parenthesized expression but a tuple.\n        model = type(function_info['name'], (BaseModel,), namespace)\n        model.__doc__ = function_info['description']\n\n        return model\n\n    @staticmethod\n    def create_pydantic_model_v2(function_info: dict) -> Type[BaseModel]:\n        parameters = function_info['parameters']\n        properties = parameters.get('properties', {})\n        required_properties = set(parameters.get('required', []))\n\n        # Redefined how fields are set up\n        attributes = {}\n        for prop_name, prop_info in properties.items():\n            field_type = Tools.type_from_json_schema(prop_info)\n            field_description = prop_info.get('description', None)\n\n            if prop_name in required_properties:\n                # Required properties should not have a default value\n                attributes[prop_name] = (field_type, Field(description=field_description))\n            else:\n                # Optional properties default to None\n                attributes[prop_name] = (Optional[field_type], Field(default=None, description=field_description))\n\n        # Dynamically create Pydantic model with explicit annotations\n        namespace = {'__annotations__': {}}\n        for attr_name, (attr_type, field_info) in attributes.items():\n            namespace['__annotations__'][attr_name] = attr_type\n            namespace[attr_name] = field_info\n\n        model = type(function_info['name'], (BaseModel,), namespace)\n        model.__doc__ = function_info.get('description', '')\n\n        return model\n\n    @staticmethod\n    def create_class_schema(function_info: dict) -> Type[ClassSchema]:\n        parameters = function_info['parameters']\n        properties = parameters.get('properties', {})\n        required_properties = set(parameters.get('required', []))\n\n        attributes = {}\n        for prop_name, prop_info in properties.items():\n            field_type = Tools.type_from_json_schema(prop_info)\n            field_description = prop_info.get('description', None)\n\n            if prop_name in required_properties:\n                attributes[prop_name] = (field_type, Field(description=field_description))\n            else:\n                attributes[prop_name] = (Optional[field_type], Field(default=None, description=field_description))\n\n        namespace = {'__annotations__': {}}\n        for attr_name, (attr_type, field_info) in attributes.items():\n            namespace['__annotations__'][attr_name] = attr_type\n            namespace[attr_name] = field_info\n\n        model = type(function_info['name'], (ClassSchema,), namespace)\n        model.__doc__ = function_info.get('description', '')\n\n        return model\n\n    @staticmethod\n    def create_pydantic_model_from_tools(tools: list, mode: Literal[\"pydantic_v2\", \"formatron\", \"pydantic_v1\"]=\"pydantic_v2\"):\n        \"\"\"\n        this function create a list of pydantic model based on\n        tool_list in the API call\n        \"\"\"\n        models = []\n        for tool in tools:\n            function_info = tool.dict()['function']\n            if mode == \"pydantic_v2\":\n                model = Tools.create_pydantic_model_v2(function_info)\n            elif mode == \"formatron\":\n                model = Tools.create_class_schema(function_info)\n            elif mode == \"pydantic_v1\":\n                model = Tools.create_pydantic_model_v1(function_info)\n            else:\n                raise ValueError(f\"Unsupported mode: {mode}\")\n            models.append(model)\n\n        return models\n\n    @staticmethod\n    def replace_refs_with_definitions_v2(schema: Dict[str, Any], defs: Dict[str, Any] = None) -> Dict[str, Any]:\n        \"\"\"\n        Recursively replace all `$ref` in schema with definitions from `$defs`.\n        \"\"\"\n        if defs is None:\n            defs = schema.get('$defs', {})\n\n        if isinstance(schema, dict):\n            if '$ref' in schema:\n                # $ref in Pydantic v2 will point to '#/$defs/Child'\n                ref_path = schema['$ref']\n                assert ref_path.startswith('#/$defs/'), f\"Unhandled $ref format: {ref_path}\"\n                ref_name = ref_path.split('/')[-1]\n                # Proceed to replace with the actual schema from $defs\n                # Making a copy of the definition to avoid modifications\n                return Tools.replace_refs_with_definitions_v2(defs[ref_name], defs)\n            else:\n                # Recursively replace in all dictionary items\n                return {k: Tools.replace_refs_with_definitions_v2(v, defs) for k, v in schema.items()}\n        elif isinstance(schema, list):\n            # Recursively replace in all list items\n            return [Tools.replace_refs_with_definitions_v2(item, defs) for item in schema]\n        return schema\n\n\n    @staticmethod\n    def get_schema_without_refs_from_pydantic_v2(model: BaseModel) -> Dict[str, Any]:\n        \"\"\"\n        Generate JSON schema from a Pydantic model and replace all `$ref` with actual definitions.\n        \"\"\"\n        raw_schema = model.schema()\n        return Tools.replace_refs_with_definitions_v2(raw_schema)\n\n    @staticmethod\n    def replace_refs_with_definitions_v1(schema: Dict[str, Any], definitions: Dict[str, Any] = None) -> Dict[str, Any]:\n        \"\"\"\n        Recursively replace all `$ref` in schema with definitions.\n        \"\"\"\n        if definitions is None:\n            definitions = schema.get('definitions', {})\n\n        if isinstance(schema, dict):\n            if '$ref' in schema:\n                ref_path = schema['$ref']\n                assert ref_path.startswith('#/definitions/'), f\"Unhandled $ref format: {ref_path}\"\n                ref_name = ref_path.split('/')[-1]\n                # Proceed to replace with the actual schema definition\n                # Important: We make a deep copy to avoid unintentional modifications\n                return Tools.replace_refs_with_definitions_v1(definitions[ref_name], definitions)\n            else:\n                # Recursively replace in all dictionary items\n                return {k: Tools.replace_refs_with_definitions_v1(v, definitions) for k, v in schema.items()}\n        elif isinstance(schema, list):\n            # Recursively replace in all list items\n            return [Tools.replace_refs_with_definitions_v1(item, definitions) for item in schema]\n        return schema\n\n    @staticmethod\n    def get_schema_without_refs_from_pydantic_v1(model: BaseModel) -> Dict[str, Any]:\n        \"\"\"\n        Generate JSON schema from a Pydantic model and replace all `$ref`.\n        \"\"\"\n        raw_schema = model.schema()\n        return Tools.replace_refs_with_definitions_v1(raw_schema)\n\n    @staticmethod\n    def generate_pydantic_model_from_json_schema(json_schema: dict) -> str:\n        \"\"\"\n        Generates Pydantic model code from a JSON schema using datamodel-code-generator.\n\n        Args:\n            json_schema (dict): The JSON schema to generate the Pydantic model from.\n\n        Returns:\n            str: The generated Pydantic model code as a string, with comments removed,\n                 duplicate imports removed, and normalized newlines.\n        \"\"\"\n        with TemporaryDirectory() as temporary_directory_name:\n            temporary_directory = Path(temporary_directory_name)\n\n            # Write the JSON schema to a temporary file\n            schema_file = temporary_directory / 'schema.json'\n            with open(schema_file, 'w') as f:\n                json.dump(json_schema, f)\n\n            # Define the output file for the generated model\n            output = temporary_directory / 'model.py'\n\n            # Generate the Pydantic model code\n            generate(\n                input_=schema_file,  # Pass the file path instead of the raw dictionary\n                input_file_type=InputFileType.JsonSchema,\n                output=output,\n                output_model_type=DataModelType.PydanticV2BaseModel,\n            )\n\n            # Read the generated code from the output file\n            with open(output, 'r') as file:\n                generated_code = file.read()\n\n        # Remove the comment lines at the top\n        generated_code = '\\n'.join(\n            line for line in generated_code.splitlines()\n            if not line.strip().startswith('#')\n        )\n\n        # Normalize newlines to ensure a maximum of one empty line\n        generated_code = '\\n'.join(\n            line for line in generated_code.splitlines()\n            if line.strip()  # Remove empty lines\n        )\n        generated_code = generated_code.replace('\\n\\n', '\\n')  # Ensure max one empty line\n\n        return generated_code\n\n    @staticmethod\n    def append_code_without_duplicate_imports(code_list: list[str]) -> str:\n        \"\"\"\n        Appends multiple code strings into one, removing duplicate imports and normalizing newlines.\n\n        Args:\n            code_list (list[str]): A list of code strings to combine.\n\n        Returns:\n            str: The combined code with duplicate imports removed and normalized newlines.\n        \"\"\"\n        if not code_list:\n            return \"\"\n\n        # Initialize the combined code with the first code string\n        combined_code = code_list[0]\n\n        # Iterate over the remaining code strings\n        for new_code in code_list[1:]:\n            # Split the code into lines\n            existing_lines = combined_code.splitlines()\n            new_lines = new_code.splitlines()\n\n            # Extract imports from existing code\n            existing_imports = set(\n                line for line in existing_lines\n                if line.strip().startswith(('from ', 'import '))\n            )\n\n            # Filter out duplicate imports from new code\n            new_lines_filtered = [\n                line for line in new_lines\n                if not (line.strip().startswith(('from ', 'import ')) and line in existing_imports)\n            ]\n\n            # Append the filtered new code to the combined code\n            combined_code += '\\n' + '\\n'.join(new_lines_filtered)\n\n        # Normalize newlines to ensure a maximum of one empty line\n        combined_code = '\\n'.join(\n            line for line in combined_code.splitlines()\n            if line.strip()  # Remove empty lines\n        )\n        combined_code = combined_code.replace('\\n\\n', '\\n')  # Ensure max one empty line\n\n        return combined_code\n\n\n\ndef create_function_models_v1(functions: Dict[str, Type[BaseModel]]) -> List[Type[BaseModel]]:\n    \"\"\" create a list of pydantic models v1 for the function schemas that passed in via OpenAI request call\"\"\"\n    function_model_list: List[Type[BaseModel]] = []\n    for func_name, arg_model in functions.items():\n        # Dynamic Pydantic model creation\n        NewModel = create_model(\n            func_name.title(),\n            name=(Literal[func_name], ...),  # ... mean required\n            arguments=(arg_model, ...),\n            __config__=type('Config', (BaseModel.Config,), {'arbitrary_types_allowed': True})\n            # Nested Config class\n        )\n        function_model_list.append(NewModel)\n    return function_model_list\n\ndef create_function_models_v2(functions: Dict[str, Type[BaseModel]]) -> List[Type[BaseModel]]:\n    \"\"\"Create a list of Pydantic models v2 for the function schemas passed in via OpenAI request call.\"\"\"\n    function_model_list: List[Type[BaseModel]] = []\n    for func_name, arg_model in functions.items():\n        class Config:\n            arbitrary_types_allowed = True\n\n        NewModel = create_model(\n            func_name.title(),\n            name=(Literal[func_name], Field(...)),  # '...' means required\n            arguments=(arg_model, Field(...)),\n            __config__=Config\n        )\n        function_model_list.append(NewModel)\n    return function_model_list\n\n\ndef create_function_models_formatron(functions: Dict[str, Type[ClassSchema]]) -> List[Type[ClassSchema]]:\n    \"\"\"Create a list of ClassSchema models for the function schemas passed in via OpenAI request call.\"\"\"\n    function_model_list: List[Type[ClassSchema]] = []\n    for func_name, arg_model in functions.items():\n        class Config:\n            arbitrary_types_allowed = True\n\n        # Create a new ClassSchema subclass\n        class NewModel(ClassSchema):\n            name: Literal[func_name] = Field(...)\n            arguments: arg_model = Field(...)\n\n            class Config:\n                arbitrary_types_allowed = True\n\n        # Set the name of the class to match the function name\n        NewModel.__name__ = func_name.title()\n\n        function_model_list.append(NewModel)\n    return function_model_list"}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/exllama/exllamav2.py", "content": "from gallama.backend.llm.engine.base import ModelInterface\nfrom typing import Optional, Dict, List, Union\nimport torch\nimport asyncio\nfrom fastapi import Request                 # for type hint\nfrom importlib.metadata import version      # for checking of exllama version\nfrom functools import lru_cache             # for image caching\nimport uuid                                 # use for generating id for api return\nimport traceback\n\nfrom gallama.logger.logger import logger\nfrom gallama.data_classes import (\n    BaseMessage,\n    ModelSpec,\n    GenStart,\n    GenEnd,\n    GenQueue,\n    GenText,\n    GenerationStats,\n    QueueContext,\n    GenQueueDynamic,\n    VideoFrame\n)\nfrom gallama.utils.utils import get_image\n\n\ntry:\n    from exllamav2 import (\n        ExLlamaV2,\n        ExLlamaV2Tokenizer,\n        ExLlamaV2Cache,\n        ExLlamaV2Cache_Q4,\n        ExLlamaV2Cache_Q6,\n        ExLlamaV2Cache_Q8,\n        ExLlamaV2Config,\n    )\n    from exllamav2.generator import (\n        ExLlamaV2Sampler,\n        ExLlamaV2DynamicGeneratorAsync,\n        ExLlamaV2DynamicJobAsync,\n    )\n\n    from exllamav2.generator.filters import ExLlamaV2PrefixFilter\n\n    if version('exllamav2') == '0.2.1' or version('exllamav2') == '0.2.2':\n        raise \"Please use version 0.2.3 onwards. There is some bug with v0.2.1 and 0.2.2 related with format enforcement\"\n\nexcept ImportError:\n    ExLlamaV2 = None\n    ExLlamaV2Tokenizer = None\n    ExLlamaV2Cache = None\n    ExLlamaV2Cache_Q4 = None\n    ExLlamaV2Cache_Q6 = None\n    ExLlamaV2Cache_Q8 = None\n    ExLlamaV2Config = None\n    ExLlamaV2Sampler = None\n    ExLlamaV2DynamicGeneratorAsync = None\n    ExLlamaV2DynamicJobAsync = None\n    ExLlamaV2PrefixFilter = None\n\n\n# tensor parallel from v0.1.9 onward\ntry:\n    from exllamav2 import ExLlamaV2Cache_TP\nexcept ImportError:\n    # optional dependency\n    ExLlamaV2Cache_TP = None\n\n# vision support from v0.2.4 onward\ntry:\n    from exllamav2 import ExLlamaV2VisionTower\nexcept ImportError:\n    ExLlamaV2VisionTower = None\n\n# format enforcement with formatron\ntry:\n    from formatron.formatter import FormatterBuilder\n    from formatron.integrations.exllamav2 import create_formatter_filter\nexcept ImportError:\n    FormatterBuilder = None\n    create_formatter_filter = None\n\n# format enforcement with lmfe\ntry:\n    from lmformatenforcer.tokenenforcer import TokenEnforcerTokenizerData\n    from lmformatenforcer.integrations.exllamav2 import (\n        ExLlamaV2TokenEnforcerFilter,\n        build_token_enforcer_tokenizer_data\n    )\n    from lmformatenforcer import JsonSchemaParser\nexcept ImportError:\n    TokenEnforcerTokenizerData = None\n    ExLlamaV2TokenEnforcerFilter = None\n    build_token_enforcer_tokenizer_data = None\n    JsonSchemaParser = None\n\ntry:\n    # lm format enforcer does not work correctly without update with latest api from exllama\n    # this wrapper class aim as stop gap solution and formatron is recommended instead\n    from gallama.backend.llm.engine.exllama.inference_json_lmfe_wrapper import ExLlamaV2TokenEnforcerFilter as ExLlamaV2TokenEnforcerFilterTemp\nexcept ImportError:\n    TokenEnforcerTokenizerData = None\n    ExLlamaV2TokenEnforcerFilter = None\n    build_token_enforcer_tokenizer_data = None\n    ExLlamaV2TokenEnforcerFilterTemp = None\n    JsonSchemaParser = None\n\n\nclass ModelExllama(ModelInterface):\n    def __init__(self, model_spec:ModelSpec):\n        super().__init__(model_spec)\n        self.model, self.tokenizer, self.cache, self.processor = self.load_model()\n\n    @property\n    def support_concurrency(self) -> bool:\n        \"\"\"\n        whether this backend/ model support concurrent request\n        \"\"\"\n        return True\n\n    def load_model(self):\n        \"\"\"Load the model, tokenizer, cache, and optional processor.\"\"\"\n\n        model, tokenizer, cache, processor = self.load_model_exllama(\n            model_id=self.model_id,\n            backend=self.backend,\n            max_seq_len=self.max_seq_len,\n            cache_size=self.cache_size,\n            cache_quant=self.cache_quant,\n            gpus=self.gpus,\n            reserve_vram=self._reserve_vram,\n            tensor_parallel=self.tensor_parallel,\n        )\n\n        # load draft model\n        if self.draft_model_id:\n            # tokenizer and processor already set above\n            self.draft_model, _, self.draft_cache, _ = self.load_model_exllama(\n                model_id=self.draft_model_id,\n                backend=self.backend,\n                max_seq_len=self.max_seq_len,  # draft model max_seq_len must be same as main model\n                cache_size=self.draft_cache_size,\n                cache_quant=self.draft_cache_quant,\n                gpus=self.draft_gpus,\n                reserve_vram=self._reserve_vram,\n            )\n\n        self.eos_token_ids = self.generate_eos_tokens_id()\n\n        return model, tokenizer, cache, processor\n\n\n    @property\n    def _reserve_vram(self):\n        try:\n            reserve_block_size = 1024 ** 2\n            num_devices = torch.cuda.device_count()\n            #reserved_vram = [192 * 1024**2] + [64 * 1024**2] * (num_devices - 1)\n            #reserved_vram = [256 * 1024 ** 2] + [96 * 1024 ** 2] * (num_devices - 1)\n\n            # GPU1 is the main GPU for my PC\n            # The below is lower threshold than exllamav2 default setting\n            reserve_per_gpu = [32 for _ in range(num_devices)]\n            main_gpu = 0    # TODO pass it to front end\n            reserve_per_gpu[main_gpu] = 64\n            reserved_vram = [_reserve * reserve_block_size for _reserve in reserve_per_gpu]\n            return reserved_vram\n        except:\n            # for non cuda env e.g. macbook\n            return None\n\n    def load_model_exllama(self, model_id, backend, cache_size, cache_quant, gpus, reserve_vram, max_seq_len=None, tensor_parallel=False):\n        \"\"\"This function return the model and its tokenizer\"\"\"\n        logger.info(\"Loading model: \" + model_id)\n\n        # initialize\n        cache = None\n        tokenizer = None\n\n        config = ExLlamaV2Config(model_id)\n        if max_seq_len is not None:\n            config.max_seq_len = max_seq_len\n        else:\n            # set the self.max_seq_len using model config file as it is None at the moment\n            max_seq_len = config.max_seq_len\n            self.max_seq_len = config.max_seq_len\n\n        model = ExLlamaV2(config)\n        tokenizer = ExLlamaV2Tokenizer(config)\n\n        # a simple dict to help map cache quant\n        cache_quant_dict = {\n            \"FP16\": ExLlamaV2Cache,\n            \"Q4\": ExLlamaV2Cache_Q4,\n            \"Q6\": ExLlamaV2Cache_Q6,\n            \"Q8\": ExLlamaV2Cache_Q8,\n        }\n\n        # cache size need to minimally max_seq_len size\n        cache_size_to_use = cache_size if cache_size else config.max_seq_len\n        cache_size_to_use = (cache_size_to_use//256) * 256      # round to multiplier of 256 for paged attention\n        # ensure cache_size is minimally max_seq_len\n        cache_size_to_use = max(cache_size_to_use, max_seq_len)\n\n        # get the cache quantization to use\n        cache_quant_to_use = cache_quant_dict[cache_quant]\n\n        logger.info(\"max_seq_len: \" + str(self.max_seq_len))\n        logger.info(\"cache_size: \" + str(cache_size_to_use))\n        logger.info(\"Cache Quantization: \" + str(cache_quant))\n\n        assert cache_quant_to_use is not None\n        assert (isinstance(gpus, str) and gpus == \"auto\") or (isinstance(gpus, list)), \\\n            \"Device map should be either 'auto', 'gpu' split\"\n\n        if not tensor_parallel:\n            if isinstance(gpus, str) and gpus == \"auto\":\n                cache = cache_quant_to_use(model, max_seq_len=cache_size_to_use, lazy=True)\n                model.load_autosplit(cache, reserve_vram=reserve_vram, progress=True)\n            elif isinstance(gpus, list):      # user specify the gpus split\n                logger.info(\"Custom GPU Allocation in GB: \" + str(gpus))\n                model.load(gpu_split=gpus, progress=True)\n                cache = cache_quant_to_use(model, max_seq_len=cache_size_to_use, lazy=not model.loaded)\n        else:\n            # tensor parallel mode\n            logger.info(\"ExllamaV2 Tensor Parallel enabled\")\n            if ExLlamaV2Cache_TP:       # ensure that tensor parallel is available\n                model.load_tp(progress=True, gpu_split = gpus if isinstance(gpus, list) else None)\n                cache = ExLlamaV2Cache_TP(\n                    model,\n                    max_seq_len = cache_size_to_use,\n                    base = cache_quant_to_use,\n                )\n            else:\n                raise ValueError(\"ExllamaV2 was not installed with tensor parallel\")\n\n        # load vision processor\n        processor = None\n        if ExLlamaV2VisionTower:\n            try:\n                processor = ExLlamaV2VisionTower(config)\n                processor.load(progress=True)\n            except:\n                processor = None\n\n        # if processor is not None, meaning at least image is supported\n        if processor:\n            self.modalities.add(\"image\")\n\n        # check if video is supported\n        if processor and processor.video_preprocess_func:\n            self.modalities.add(\"video\")\n\n        logger.info(f\"Supported Modalities: {self.modalities}\")\n\n        return model, tokenizer, cache, processor\n\n\n    @property\n    def video_token_by_backend(self) -> str:\n        \"\"\" exllama use this specific token for video embedding\"\"\"\n        return \"{{VIDEO-PlaceHolderTokenHere}}\"\n\n\n    def generate_eos_tokens_id(self) -> List[int]:\n        \"\"\"Generate the end-of-sequence token IDs.\"\"\"\n        if self.eos_token_str:\n            # exllama\n            if ExLlamaV2Tokenizer and isinstance(self.tokenizer, ExLlamaV2Tokenizer):\n                eos_token_ids = [self.tokenizer.single_id(token) for token in self.eos_token_str]\n                return eos_token_ids\n        else:\n            return []\n\n\n    # ********* from here is generation methods\n    # helper function for job disconnection. Currently only exllama support this\n    @staticmethod\n    async def check_disconnection(\n        request: Request,\n        job: ExLlamaV2DynamicJobAsync,\n        gen_queue_list: Union[GenQueue, QueueContext, List[QueueContext]],\n        stop_event: asyncio.Event = None,\n    ):\n        \"\"\"\n        Helper function that handle stopping generation mid-stream\n        \"\"\"\n        try:\n            while True:\n                if await request.is_disconnected():\n                    logger.info(\"User disconnected\")\n                    if job:\n                        await job.cancel()\n\n                    # add GenEnd to signal the end of generation\n                    chunk = GenEnd()\n                    for g_queue in gen_queue_list:\n                        try:\n                            await g_queue.get_queue().put(chunk)\n                        except Exception as e:\n                            logger.error(f\"Error putting GenEnd into queue: {str(e)}\")\n\n                    # break the while loop\n                    break\n\n                # Use asyncio.wait_for to implement a timeout\n                try:\n                    await asyncio.wait_for(asyncio.sleep(1), timeout=1.1)\n                except asyncio.TimeoutError:\n                    # This allows us to check for cancellation more frequently\n                    pass\n\n        except asyncio.CancelledError:\n            logger.debug(\"Disconnection check was cancelled\")\n        except Exception as e:\n            raise\n            if not stop_event or (stop_event and not stop_event.is_set()):\n                logger.error(f\"An error occurred in check_disconnection: {str(e)}\", exc_info=True)\n        finally:\n            logger.debug(\"Exiting check_disconnection\")\n\n\n    class ExllamaV2Pipeline:\n        \"\"\" class as wrapper for objects required for Exllama V2 text generation\"\"\"\n\n        def __init__(\n            self,\n            cache: Union[ExLlamaV2Cache, ExLlamaV2Cache_Q4],\n            generator: ExLlamaV2DynamicGeneratorAsync,\n            lm_enforcer_tokenizer_data: TokenEnforcerTokenizerData,\n        ):\n            self.cache = cache\n            self.generator = generator\n            self.lm_enforcer_tokenizer_data = lm_enforcer_tokenizer_data\n\n    async def _get_pipeline_async(self):\n        \"\"\"\n        create generator for exllama and also build the tokenizer data for lmfe\n        as ExLlamaV2DynamicGeneratorAsync is async function, it can not be run in the class __init__\n        this will be run in the first text generation to ensure that generator is initialized\n        \"\"\"\n\n        lm_enforcer_tokenizer_data = build_token_enforcer_tokenizer_data(self.tokenizer)\n\n        generator = ExLlamaV2DynamicGeneratorAsync(\n            model=self.model,\n            cache=self.cache,\n            tokenizer=self.tokenizer,\n            max_seq_len=self.max_seq_len,\n            draft_model=self.draft_model,\n            draft_cache=self.draft_cache,\n            num_draft_tokens=5,\n        )\n\n        return self.ExllamaV2Pipeline(\n            cache=self.cache,\n            generator=generator,\n            lm_enforcer_tokenizer_data=lm_enforcer_tokenizer_data,\n        )\n\n    @staticmethod\n    def _get_exllama_gen_settings(\n        temperature: float = 0.01,\n        top_p: float = 0.8,\n        **kwargs,\n    ):\n        # settings\n        settings = ExLlamaV2Sampler.Settings()\n        settings.temperature = temperature\n        settings.min_temp = 0.15\n        settings.top_k = 50\n        settings.top_p = top_p\n        settings.min_p = 0.05\n        settings.token_repetition_penalty = 1.1\n        settings.token_frequency_penalty = 0.05\n        settings.token_repetition_range = 1024\n        # settings.token_repetition_decay: int = 0.98\n        settings.temperature_last = False\n\n        return settings\n\n    @staticmethod\n    def get_stop_word(text, stop_words) -> Union[str, None]:\n        \"\"\" this function will match the stop word used given the text that llm ended generation with and a list of stop_words.\"\"\"\n\n        # sort the list by length to find the longest first\n        sorted_stop_words = sorted(stop_words, key=len, reverse=True)\n\n        text = text.lstrip()  # Remove trailing whitespace\n        for stop_word in stop_words:\n            if stop_word in text:\n                return stop_word\n\n        return None\n\n    @staticmethod\n    @lru_cache(1024)     # TODO set this dynamically\n    def get_image_embedding_cached(processor, model, tokenizer, url):\n        \"\"\"\n        function to return image embedding for exllama\n        lru_cache to cache frequently used image\n        \"\"\"\n        img = get_image(url=url)\n\n        return processor.get_image_embeddings(\n            model=model,\n            tokenizer=tokenizer,\n            image=img,\n            text_alias=None,    # passing None will let the llm generate its own embedding\n        )\n\n    @staticmethod\n    # @lru_cache(32)     # unhashable type list\n    def get_video_embedding_cached(processor, model, tokenizer, video: List[VideoFrame]):\n        \"\"\"\n        function to return image embedding for exllama\n        lru_cache to cache frequently used image\n        \"\"\"\n\n        return processor.get_video_embeddings(\n            model=model,\n            tokenizer=tokenizer,\n            video=video,\n            text_alias=None,    # passing None will let the llm generate its own embedding\n        )\n\n    def _generate_image_embeddings(self, prompt, image_list):\n        \"\"\"Generate embeddings for images and update prompt\"\"\"\n        # in prompt processing step, each image was substituted with the following token\n        # TODO move this token to better place\n\n        image_token = \"{{IMG-PlaceHolderTokenHere}}\"\n\n        # Validate image token count matches number of images\n        assert prompt.count(image_token) == len(image_list), \"Image token mismatch\"\n\n        # Generate embeddings\n        image_embeddings = [\n            self.get_image_embedding_cached(\n                processor=self.processor,\n                model=self.model,\n                tokenizer=self.tokenizer,\n                url=url\n            ) for url in image_list\n        ]\n\n        # Replace image tokens with embeddings\n        for emb in image_embeddings:\n            prompt = prompt.replace(image_token, emb.text_alias, 1)\n\n        return prompt, image_embeddings\n\n    def _generate_video_embeddings(self, prompt, video: List[VideoFrame]):\n        \"\"\"Generate embeddings for video and update prompt\"\"\"\n        # in prompt processing step, each image was substituted with the following token\n        # TODO move this token to better place\n        video_token = \"{{VIDEO-PlaceHolderTokenHere}}\"\n\n        # Validate only 1 video\n        assert prompt.count(video_token) == 1, \"Video support currently limit to 1 token mismatch\"\n\n        # get the image from each of the VideoFrame object\n        _video = [ f.image for f in video ]\n\n        video_embeddings = self.get_video_embedding_cached(\n            processor=self.processor,\n            model=self.model,\n            tokenizer=self.tokenizer,\n            video=_video\n        )\n\n        # replace prompt token\n        prompt = prompt.replace(video_token, video_embeddings.text_alias, 1)\n\n        return prompt, video_embeddings\n\n    def _process_vision_inputs(self, prompt, messages: List[BaseMessage], video: List[VideoFrame] = None):\n        \"\"\"Handle image embedding and token replacement for vision inputs\"\"\"\n\n        _prompt = prompt\n        _vision_embeddings = []\n\n        if not messages:\n            return prompt, None\n\n        image_list = []\n        vision_required = False\n\n        # Extract image URLs from messages\n        for message in messages:\n            if isinstance(message.content, list):\n                image_urls = [\n                    msg.image_url.url\n                    for msg in message.content\n                    if msg.type == \"image_url\"\n                ]\n                image_list.extend(image_urls)\n\n        vision_required = True if len(image_list) >0 else False\n\n        # Process image embeddings if vision is required\n        if vision_required and self.processor:\n            _prompt,_vision_embeddings = self._generate_image_embeddings(prompt, image_list)\n\n        # handle video input\n        if video:\n            _prompt, _video_embeddings = self._generate_video_embeddings(_prompt, video)\n            _vision_embeddings.append(_video_embeddings)\n\n        return _prompt, _vision_embeddings\n\n\n    def _get_format_enforcer_filter(self, formatter):\n        \"\"\"Determine appropriate format enforcer filter\"\"\"\n        if isinstance(formatter, (TokenEnforcerTokenizerData, JsonSchemaParser)):\n            # Logic for LM format enforcer\n            exllama_version = version('exllamav2')\n            return [\n                ExLlamaV2TokenEnforcerFilterTemp(\n                    model=self.model,\n                    tokenizer=self.tokenizer,\n                    character_level_parser=formatter,\n                ) if exllama_version > '0.2.0' else\n                ExLlamaV2TokenEnforcerFilter(\n                    character_level_parser=formatter,\n                    tokenizer_data=self.pipeline.lm_enforcer_tokenizer_data\n                )\n            ]\n        elif FormatterBuilder and isinstance(formatter, FormatterBuilder):\n            # Logic for Formatron\n            return [create_formatter_filter(self.model, self.tokenizer, formatter)]\n\n        raise ValueError(\"Unsupported formatter type\")\n\n    def _create_generation_filters(\n        self,\n        formatter: Optional[Union[TokenEnforcerTokenizerData, FormatterBuilder]] = None,\n        prefix_strings: Optional[Union[str, List[str]]] = None\n    ) -> List:\n        \"\"\"Create filters for token generation\"\"\"\n        filters = []\n\n        # Format enforcer filters\n        if formatter:\n            filters.extend(self._get_format_enforcer_filter(formatter))\n\n        # Prefix filters\n        if prefix_strings:\n            filters.append(\n                ExLlamaV2PrefixFilter(self.model, self.tokenizer, prefix_strings)\n            )\n\n        return filters\n\n    async def generate(\n        self,\n        prompt: str,\n        gen_queue: Union[GenQueue, GenQueueDynamic, List[GenQueueDynamic]],\n        request: Optional[Request] = None,  # for disconnection check\n        gen_type: Union[str, GenStart] = \"text\",  # the generated result will be stored in this queue\n        temperature: float = 0.01,\n        top_p: float = 0.8,\n        formatter: Optional[Union[FormatterBuilder, TokenEnforcerTokenizerData]] = None,\n        stop_words: Union[List[str], str] = None,\n        prefix_strings: Optional[Union[str, List[str]]] = None,\n        banned_strings: list[str] | None = None,\n        max_tokens: int = None,\n        quiet=False,\n        messages: List[BaseMessage] = None,  # query.message for multimodal\n        video: List[VideoFrame] = None,\n        stop_event: asyncio.Event = None,\n        **kwargs,\n    ) -> (str, GenerationStats):\n        try:\n            # Ensure that the generator is initialized\n            if self.pipeline is None:\n                self.pipeline = await self._get_pipeline_async()\n\n            # Convert gen_queue to List[GenQueueDynamic] format to standardize downstream handling\n            gen_queue_list = []\n            if isinstance(gen_queue, GenQueueDynamic):\n                gen_queue_list = [gen_queue]\n            elif isinstance(gen_queue, GenQueue):\n                # Wrap the GenQueue in a GenQueueDynamic\n                gen_queue_list = [GenQueueDynamic(existing_queue=gen_queue, include_GenStats=True, include_GenEnd=True)]\n            elif isinstance(gen_queue, list):\n                # Ensure all items in the list are GenQueueDynamic objects\n                for queue in gen_queue:\n                    if isinstance(queue, GenQueueDynamic):\n                        gen_queue_list.append(queue)\n                    elif isinstance(queue, GenQueue):\n                        # Wrap the GenQueue in a GenQueueDynamic\n                        gen_queue_list.append(\n                            GenQueueDynamic(existing_queue=queue, include_GenStats=True, include_GenEnd=True))\n                    else:\n                        raise TypeError(\"gen_queue list must contain only GenQueue or GenQueueDynamic objects\")\n            else:\n                raise TypeError(\"gen_queue must be either a GenQueue, GenQueueDynamic, or a list of GenQueueDynamic\")\n\n            # Get generation settings\n            settings = self._get_exllama_gen_settings(temperature, top_p=top_p)\n\n            # Vision support - get image embedding and construct the prompt with placeholder tokens for images\n            prompt, image_embeddings = self._process_vision_inputs(prompt, messages, video)\n\n            # Convert prompt to token IDs\n            if image_embeddings:\n                input_ids = self.tokenizer.encode(\n                    prompt,\n                    encode_special_tokens=True,\n                    embeddings=image_embeddings,\n                )\n            else:\n                input_ids = self.tokenizer.encode(prompt)\n\n            self.validate_token_length(len(input_ids[0]))\n\n            # Create filters for format enforcement\n            filters = self._create_generation_filters(formatter, prefix_strings)\n\n            # Find stop conditions\n            if stop_words:\n                if isinstance(stop_words, str):\n                    stop_words = [stop_words]\n\n                if not self.eos_token_str:\n                    raise Exception(\"EOS token not set in model_config\")\n                stop_conditions = self.eos_token_str + stop_words  # Concatenate the two lists\n                logger.debug(\"stop_words: \" + str(stop_conditions))\n            else:\n                stop_conditions = self.eos_token_str\n\n            job_id = uuid.uuid4().hex\n\n            # Calculate max tokens to use\n            max_tokens_to_use = min(\n                self.max_seq_len - len(input_ids[0]),\n                max_tokens, 4096) if max_tokens else min(self.max_seq_len - len(input_ids[0]), 4096)\n\n\n            if not quiet:\n                logger.info(\"----------------------Prompt---------------\\n\" + prompt)\n                logger.debug(\"----------------------temperature---------\\n\" + str(temperature))\n\n            # Prepare arguments for the job\n            argument_list = {\n                \"generator\": self.pipeline.generator,\n                \"input_ids\": input_ids,\n                \"max_new_tokens\": max_tokens_to_use,\n                \"gen_settings\": settings,\n                \"stop_conditions\": stop_conditions,\n                \"banned_strings\": banned_strings,\n                \"decode_special_tokens\": True,\n                \"filters\": filters,\n                \"token_healing\": True,\n                \"identifier\": job_id,\n            }\n\n            # Add image embeddings if available\n            if image_embeddings:\n                argument_list[\"embeddings\"] = image_embeddings\n\n            # Create the job\n            job = ExLlamaV2DynamicJobAsync(**argument_list)\n\n            generate_text = \"\"\n            gen_stats = None\n            eos = False\n\n            # Kick-start the generation and let downstream know the generation type\n            if isinstance(gen_type, str):\n                gen_type_str = gen_type\n                gen_type = GenStart(gen_type=gen_type)\n            else:\n                gen_type_str = gen_type.gen_type  # Get the generation type in string format\n\n            for g_queue in gen_queue_list:\n                g_queue.put_nowait(gen_type)\n\n            # Create a task to check for disconnection\n            disconnect_check_task = None\n            if request:\n                disconnect_check_task = asyncio.create_task(self.check_disconnection(request, job, gen_queue_list, stop_event=stop_event))\n\n            try:\n                # Start the generation\n                async for result in job:\n                    if eos or stop_event.is_set():\n                        await job.cancel()\n                        break\n\n                    chunk_text = result.get(\"text\", \"\")\n                    if chunk_text:\n                        # logger.info(f\"chunk_text: {chunk_text}\")\n                        chunk = GenText(content=chunk_text, text_type=gen_type_str)\n                        for g_queue in gen_queue_list:\n                            if chunk_text not in self.eos_token_str_set:  # Formatron returns EOS token\n                                g_queue.put_nowait(chunk)\n\n                    # Handle EOS signal\n                    if result[\"eos\"]:\n                        eos = True\n                        # logger.info(f\"eos result {result}\")\n                        # If the stop word occurred is from the stop_words and not LLM result token -> include in result\n                        if stop_words and result.get(\"held\") and result.get(\"held\").get(\"text\"):\n                            ending_string = result[\"held\"][\"text\"].rstrip()\n\n                            if ending_string:\n                                # Find the stop word that was used to end the string\n                                stop_word_used = self.get_stop_word(ending_string, stop_words)\n\n                                if stop_word_used:\n                                    # If generation ended with one of the stop words\n                                    # -> return that stop word as the last token\n                                    chunk = GenText(content=stop_word_used, text_type=gen_type_str)\n                                    for g_queue in gen_queue_list:\n                                        g_queue.put_nowait(chunk)\n\n                        gen_stats = GenerationStats(\n                            input_tokens_count=result[\"prompt_tokens\"],\n                            output_tokens_count=result[\"new_tokens\"],\n                            time_to_first_token=result[\"time_prefill\"],\n                            time_generate=result[\"time_generate\"],\n                        )\n\n                        for g_queue in gen_queue_list:\n                            if g_queue.include_GenStats:\n                                g_queue.put_nowait(gen_stats)\n\n                        # Signal the end of generation\n                        for g_queue in gen_queue_list:\n                            if g_queue.include_GenEnd:\n                                g_queue.put_nowait(GenEnd())\n\n            except Exception as e:\n                logger.error(e)\n            finally:\n                if disconnect_check_task:\n                    disconnect_check_task.cancel()\n                    try:\n                        await disconnect_check_task\n                    except:\n                        pass\n        except Exception as e:\n            logger.error(e)\n            raise\n"}
{"type": "source_file", "path": "src/gallama/backend/tts/TTSQueueHandler.py", "content": "from typing import Any, AsyncIterator\nimport asyncio\nfrom gallama.logger.logger import logger\n\n\nclass TTSQueueHandler:\n    \"\"\"Handles text segment processing and audio streaming for TTS with proper synchronization\"\"\"\n\n    def __init__(self, tts_engine: \"TTSBase\"):\n        self.tts_engine = tts_engine\n        self.text_queue = asyncio.Queue()  # Queue for text segments\n        self.audio_queue = asyncio.Queue()  # Queue for audio chunks\n        self._processing = False\n        self._current_task = None\n\n    async def start(self):\n        \"\"\"Start the processing loop\"\"\"\n        self._processing = True\n        self._current_task = asyncio.create_task(self._process_segments())\n\n    async def stop(self):\n        \"\"\"Stop the processing loop\"\"\"\n        self._processing = False\n        if self._current_task:\n            await self._current_task\n            self._current_task = None\n        # Clear queues\n        while not self.text_queue.empty():\n            try:\n                self.text_queue.get_nowait()\n            except asyncio.QueueEmpty:\n                break\n        while not self.audio_queue.empty():\n            try:\n                self.audio_queue.get_nowait()\n            except asyncio.QueueEmpty:\n                break\n\n    async def add_segment(self, text: str):\n        \"\"\"Add a text segment to be processed\"\"\"\n        await self.text_queue.put(text)\n\n    async def get_audio(self):\n        \"\"\"Get the next audio chunk\"\"\"\n        return await self.audio_queue.get()\n\n    async def _process_segments(self):\n        \"\"\"Main processing loop that handles text segments one at a time\"\"\"\n        while self._processing:\n            try:\n                # Get the next text segment\n                text = await self.text_queue.get()\n                if text is None:  # Check for stop signal\n                    break\n\n                # Create a temporary queue for this segment's audio chunks\n                segment_queue = asyncio.Queue()\n\n                # Process the current segment\n                try:\n                    await self.tts_engine.text_to_speech(\n                        text=text,\n                        queue=segment_queue,\n                        stream=True,\n                        language=\"auto\"\n                    )\n\n                    # Forward all audio chunks to the main audio queue\n                    while True:\n                        chunk = await segment_queue.get()\n                        if chunk is None:  # End of segment\n                            break\n                        if isinstance(chunk, Exception):\n                            raise chunk\n                        await self.audio_queue.put(chunk)\n\n                except Exception as e:\n                    logger.error(f\"Error processing segment: {str(e)}\")\n                    await self.audio_queue.put(Exception(f\"TTS error: {str(e)}\"))\n                    break\n\n                # Mark this segment as done\n                self.text_queue.task_done()\n\n            except asyncio.CancelledError:\n                break\n            except Exception as e:\n                logger.error(f\"Unexpected error in segment processing: {str(e)}\")\n                await self.audio_queue.put(Exception(f\"Unexpected error: {str(e)}\"))\n                break\n\n        # Signal end of processing\n        await self.audio_queue.put(None)"}
{"type": "source_file", "path": "src/gallama/backend/llm/prompt_engine.py", "content": "import json\nimport yaml\nfrom typing import List, Dict, Union, Literal\nfrom gallama.data_classes.data_class import BaseMessage, ChatMLQuery, ToolCall, MultiModalTextContent, MultiModalImageContent\nfrom pydantic import BaseModel\nfrom copy import deepcopy\nfrom gallama.utils.utils import parse_xml_to_dict\nfrom fastapi import HTTPException\nfrom pathlib import Path\nfrom textwrap import dedent\nfrom gallama.data import ARTIFACT_SYSTEM_PROMPT\nimport uuid\n\nclass PromptEngine:\n    def __init__(self, prompt_format: str):\n        self.system_msg_enabled = False\n        self.tool_enabled = False\n        self.model_prompt_all = self.get_prompt_token()\n        if not self.model_prompt_all.get(prompt_format):\n            raise ValueError(f'Prompt format {prompt_format} not found in data/model_token.yaml')\n\n        self.model_prompt = self.model_prompt_all.get(prompt_format)\n        self.system_msg_enabled = self.model_prompt.get(\"system_msg_enabled\")\n        self.tool_enabled = self.model_prompt.get(\"tool_enabled\")\n        self.eos_token_list = self.model_prompt.get(\"eos_token_list\")\n\n    @staticmethod\n    def get_prompt_token() -> Dict:\n        \"\"\"Get the absolute path to the data directory.\"\"\"\n        yaml_file = Path(__file__).parent.parent.parent / 'data' / 'model_token.yaml'\n        with open(yaml_file, 'r') as file:\n            yaml_data = yaml.safe_load(file)\n        return yaml_data\n\n    def get_conversation_start_token(self):\n        return self.model_prompt.get(\"conversation_start\", \"\")\n\n    def get_conversation_end_token(self):\n        return self.model_prompt.get(\"conversation_end\", \"\")\n\n    def leading_prompt_token(self):\n        return self.model_prompt.get(\"leading_prompt_token\", \"\")\n\n    def get_user_start_token(self):\n        return self.model_prompt.get(\"user_start\", \"\")\n\n    def get_user_end_token(self):\n        return self.model_prompt.get(\"user_end\", \"\")\n\n    def get_sys_start_token(self):\n        return self.model_prompt.get(\"system_start\", \"\")\n\n    def get_sys_end_token(self):\n        return self.model_prompt.get(\"system_end\", \"\")\n\n    def get_assistant_start_token(self):\n        return self.model_prompt.get(\"assistant_start\", \"\")\n\n    def get_assistant_end_token(self):\n        return self.model_prompt.get(\"assistant_end\", \"\")\n\n    def get_tool_start_token(self):\n        return self.model_prompt.get(\"tool_start\", \"\")\n\n    def get_tool_end_token(self):\n        return self.model_prompt.get(\"tool_end\", \"\")\n\n    def get_tool_result_start_token(self):\n        return self.model_prompt.get(\"tool_result_start\", \"\")\n\n    def get_tool_result_end_token(self):\n        return self.model_prompt.get(\"tool_result_end\", \"\")\n\n    def get_tool_call_start_token(self):\n        return self.model_prompt.get(\"tool_call_start\", \"\")\n\n    def get_tool_call_end_token(self):\n        return self.model_prompt.get(\"tool_call_start\", \"\")\n\n    def get_vision_start_token(self):\n        return self.model_prompt.get(\"vision_start\", \"\")\n\n    def get_vision_end_token(self):\n        return self.model_prompt.get(\"vision_end\", \"\")\n\n    def get_image_pad_token(self):\n        return self.model_prompt.get(\"image_pad\", \"\")\n\n    def _get_role_token(self, role, token_type: Literal[\"start\", \"end\"]):\n        if token_type == \"start\":\n            if role == \"system\":\n                return self.get_sys_start_token()\n            elif role == \"user\":\n                return self.get_user_start_token()\n            elif role == \"assistant\":\n                return self.get_assistant_start_token()\n            elif role == \"tool\":\n                return self.get_tool_start_token()\n            elif role == \"tool_result\":\n                return self.get_tool_result_start_token()\n            elif role == \"tool_call\":\n                return self.get_tool_call_start_token()\n        elif token_type == \"end\":\n            if role == \"system\":\n                return self.get_sys_end_token()\n            elif role == \"user\":\n                return self.get_user_end_token()\n            elif role == \"assistant\":\n                return self.get_assistant_end_token()\n            elif role == \"tool\":\n                return self.get_tool_end_token()\n            elif role == \"tool_result\":\n                return self.get_tool_result_end_token()\n            elif role == \"tool_call\":\n                return self.get_tool_call_end_token()\n        else:\n            return \"\"\n\n    @staticmethod\n    def _get_message_type(msg: BaseMessage) -> str:\n        if msg.tool_call_id:\n            return \"tool_result\"\n        elif msg.role == \"tool\":\n            return \"tool_call\"\n        elif msg.role == \"assistant\":\n            return \"assistant\"\n        elif msg.role == \"system\":\n            return \"system\"\n        elif msg.role == \"user\":\n            return \"user\"\n        else:\n            raise ValueError(f\"Unknown message type {msg.role}\")\n\n    def _format_tool_call(self, tool_call: ToolCall) -> str:\n        \"\"\"\n            get string representation of tool call like normal python function calling\n            Example Output: get_current_weather(location='Boston', unit='Fahrenheit')\n        \"\"\"\n\n        func_name = tool_call.function.name\n        args_str = tool_call.function.arguments.replace('\"', '')\n        return f\"{func_name}({args_str})\"\n\n    def _format_tool_result(self, msg: BaseMessage) -> str:\n        content = msg.content if msg.content else \"\"\n\n        try:\n            content = self._get_role_token(role=\"user\",token_type=\"start\") + f\"Result of tool call reference id {msg.tool_call_id}:\\n\" + str(json.dumps(json.loads(content), indent=2)) + \"\\n---\\n\\n\"\n        except:\n            content = self._get_role_token(role=\"user\",token_type=\"start\") + f\"Result of tool call reference id {msg.tool_call_id}:\\n\" + str(content) + \"\\n---\\n\\n\"\n\n        return content + self._get_role_token(role=\"user\",token_type=\"end\")\n\n    def _format_tool_result_msg(self, msg: BaseMessage) -> str:\n        \"\"\"one msg might have multiple tool calls\"\"\"\n        # tool_call_str = f\"Please help to call these tool:\\n\"\n        tool_call_str = \"---\"\n\n        for tool_call in msg.tool_calls:\n            tool_call_str += f\"Request for tool call with reference id {tool_call.id}:\\n\"\n            tool_call_str += self._format_tool_call(tool_call)\n            tool_call_str += \"\\n\\n\"\n\n        tool_call_str += \"---\"\n        return tool_call_str\n\n    def _format_tool_msg(self, pydantic_tool_list: Dict[str, BaseModel], pretty:bool = False) -> str:\n        # append tools calling if it is a prompt\n        tools_json = (\"\\nBelow are the functions available to you to use. Only use the tool if it is necessary\\n\")\n\n        if self.tool_enabled:\n            tools_json += self.get_tool_start_token()\n\n        for tool_name, tool in pydantic_tool_list.items():\n            if pretty:\n                tools_json = tools_json + tool_name + \":\\n\" + str(json.dumps(tool.schema(), indent=2)) + \"\\n---\\n\"\n            else:\n                tools_json = tools_json + tool_name + \":\\n\" + str(json.dumps(tool.schema())) + \"\\n---\\n\"\n\n        if self.tool_enabled:\n            tools_json += self.get_tool_end_token()\n\n        return tools_json\n\n    def _format_tool_msg_as_code(self, pydantic_tool_as_code: str) -> str:\n        # append tools calling if it is a prompt\n        tools_as_code = (\"\\nBelow are the functions available to you to use.\\n\"\n                      \"The function definition and arguments are presented as pydantic class.\\n\\n\")\n\n        if self.tool_enabled:\n            tools_as_code += pydantic_tool_as_code + \"\\n\"\n\n        return tools_as_code\n\n    def _get_one_msg(self, msg: BaseMessage) -> str:\n        content = msg.content if msg.content else \"\"\n\n        prompt = \"\"\n        tool_call_str = \"\"\n\n        if self._get_message_type(msg) == \"tool_result\":\n            return self._format_tool_result(msg)\n        elif self._get_message_type(msg) == \"tool_call\":\n            return self._format_tool_result_msg(msg)\n        elif self._get_message_type(msg) in [\"system\", \"user\"]:\n            return content\n        elif self._get_message_type(msg) in [\"assistant\"]:\n            # check if there is tool call\n            if msg.tool_calls:\n                content += self._format_tool_result_msg(msg)\n\n            return content\n\n    def _get_one_msg_grp(self, grp: List[BaseMessage]) -> str:\n        prompt = self._get_role_token(role=self._get_message_type(grp[0]), token_type=\"start\")\n\n        for msg in grp:\n            prompt = prompt + self._get_one_msg(msg) + \"\\n\\n\"\n\n        prompt += self._get_role_token(role=self._get_message_type(grp[0]), token_type=\"end\")\n        return prompt\n\n    def _regroup_msg(self, msg_List: List[BaseMessage]):\n        # group msg with same role into one array\n        regrouped_msg = []\n        temp_array = []\n\n        # first create a list of msg with the role converted\n        for idx, msg in enumerate(msg_List):\n            temp_msg = deepcopy(msg)\n            if temp_msg.role == \"tool\" and not self.tool_enabled:\n                temp_msg.role = \"user\"      # convert to user as this LLM model does not have tool role\n            elif temp_msg.role == \"system\" and not self.system_msg_enabled:\n                temp_msg.role = \"user\"  # convert to user as this LLM model does not have system role\n\n            temp_array.append(temp_msg)\n\n        # now grouping message that was from the same role together\n        # this is cause model might have been trained with alternated role prompting\n        # and repeat of the same role will not give good response\n        previous_msg = msg_List[0]\n        grouped_array = []\n        for idx, msg in enumerate(temp_array):\n            if msg.role == previous_msg.role:\n                grouped_array.append(msg)\n            else:\n                if grouped_array:\n                    # create new array\n                    regrouped_msg.append(grouped_array.copy())\n                    grouped_array = [msg]     # reset temp array\n                else:\n                    grouped_array.append(msg)\n\n            previous_msg = msg\n\n        # add the last group\n        if temp_array:\n            regrouped_msg.append(grouped_array.copy())\n\n        # logger.debug(\"overall regroup:\\n\" + str(regrouped_msg))\n        return regrouped_msg\n\n    def convert_multimodal_content_list_to_string(\n        self,\n        content: List[Union[MultiModalTextContent, MultiModalImageContent]],\n        exllama_vision_token: bool = False,     # to use placeholder token for exllama for not. TODO - refractor\n    ) -> str:\n        \"\"\"\n        convert multimodal content list to string\n        e.g.\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Describe this image.\"\n                },\n            ]\n        to:\n            <|vision_start|><|image_pad|><|vision_end|>Describe this image.\n        \"\"\"\n\n        content_str = \"\"\n\n        for chunk in content:\n            if isinstance(chunk, MultiModalTextContent):\n                content_str += chunk.text\n            elif isinstance(chunk, MultiModalImageContent):\n                if not exllama_vision_token:\n                    content_str += self.get_vision_start_token() + self.get_image_pad_token() + self.get_vision_end_token()   # TODO\n                else:\n                    # use a standard token as place holder, TODO - refractor\n                    # content_str += \"{{IMG-\" + f\"{uuid.uuid4().hex}\" + \"}}\"\n                    content_str += \"{{IMG-PlaceHolderTokenHere}}\"   #TODO use a constant instead\n            else:\n                raise ValueError(\"Unexpected content type \")\n\n        return content_str\n\n\n    def get_prompt(\n        self,\n        query: ChatMLQuery,\n        pydantic_tool_dict: List[BaseModel] = None,\n        answer_format_schema: bool = True,  # whether to add the instruction for tool calling answer schema\n        prefix_prompt: str = \"\",\n        leading_prompt: str = \"\",\n        use_thinking: bool = True,\n        thinking_template: str = None,\n        thinking_response: str = None,\n        backend: Literal[\"exllama\", \"llama_cpp\", \"transformers\", \"embedding\"] = \"exllama\",    # skip model pseudo token and use exllama placeholder token # TODO - code refractoring\n        pydantic_tool_code: str = None,     # the code representation of tool\n    ) -> str:\n        exllama_vision_token = (backend==\"exllama\")     # vision token in exllama is handled and assigned from the embedding itself\n\n        def _create_leading_prompt(original_prompt, query_leading_prompt, model_leading_prompt):\n            # add leading prompt from user or model\n            # this is usually the role e.g. assistant:, Tom to Jerry:\n            _leading_prompt = \"\"\n            if query_leading_prompt:\n                _leading_prompt = model_leading_prompt + query_leading_prompt\n            elif model_leading_prompt:\n                _leading_prompt = model_leading_prompt\n\n            if not _leading_prompt.endswith(\"\\n\"):\n                _leading_prompt += \"\\n\"\n\n            return _leading_prompt\n\n        thinking_example = \"\"\n\n        # thinking_example = dedent(\"\"\"\n        # ### Thinking example:\n        # Question: Is dog or cat faster? Answer in Capital letter\n        # Apply thinking template:\n        # <format_restriction>Any specific format requirement from user</format_restriction>\n        # My thought process using the thinking template:\n        # <format_restriction>User request for answer in capital letter</format_restriction>\n        # Final answer:\n        # CAT\n        # End of Thinking Example.\n        # \"\"\").strip()\n\n        msg_list_copy = []\n\n        prompt = self.get_conversation_start_token()     # use arrange to story prompt\n\n        # add the default system prompt for artifact\n        if query.artifact != \"No\":\n            prompt += ARTIFACT_SYSTEM_PROMPT\n\n        # standardize multimodal message\n        for message in query.messages:\n            if isinstance(message.content, list):\n                msg_copy = deepcopy(message)\n                msg_copy.content = self.convert_multimodal_content_list_to_string(\n                    content=msg_copy.content,\n                    exllama_vision_token=exllama_vision_token,\n                )\n                msg_list_copy.append(deepcopy(msg_copy))\n            else:\n                msg_list_copy.append(deepcopy(message))\n\n        # group message together for the same role\n        msg_groups = self._regroup_msg(msg_list_copy)\n\n        # concat all msg\n        for idx, msg_group in enumerate(msg_groups):\n            prompt = prompt + self._get_one_msg_grp(msg_group)\n\n        # if there is tool, return the prompt with tool\n        if pydantic_tool_dict and query.tool_choice != \"none\":\n            if pydantic_tool_code:\n                if query.tool_schema_position==\"postfix\":\n                    prompt = prompt + self._format_tool_msg_as_code(pydantic_tool_code)\n                else:\n                    prompt = self._format_tool_msg_as_code(pydantic_tool_code) + prompt\n            else:\n                if query.tool_schema_position == \"postfix\":\n                    prompt = prompt + self._format_tool_msg(pydantic_tool_dict)\n                else:\n                    prompt = self._format_tool_msg(pydantic_tool_dict) + prompt\n\n#             if answer_format_schema:\n#                 prompt += \"\"\"\n# IMPORTANT: If you use tool/ functions, please answer using the following schema using json format.\n# Each item in the array under \"functions_calling\" is the \"name\" of the function to call and its \"arguments\".\n#\n# {\n#   \"functions_calling\": [\n#     {\n#       \"name\": the name of function/ tool to call,\n#       \"arguments\": {argument_name: argument_value}\n#     }\n# }\n#\n#\n# If no functions_calling/ tool needed, the response can be:\n# {\n#   \"functions_calling\": []\n# }\n# End of Example of answer with Tool/ Function_calling usage.\n#\n# \"\"\"\n        # TODO move the thinking_template check to request receiver\n        # initialize thinking_template_dict\n        thinking_template_dict = {}\n        root_key = None\n        if thinking_template:\n            try:\n                thinking_template_dict = parse_xml_to_dict(thinking_template)\n                if len(list(thinking_template_dict.keys())) > 1:\n                    raise HTTPException(status_code=400, detail=f\"thinking_template must be a XML with only 1 root key\")\n\n            except Exception as e:\n                raise HTTPException(status_code=400, detail=f\"thinking_template is not valid XML string\")\n\n            # find the XML root key\n            root_key = list(thinking_template_dict.keys())[0]\n\n        # prompt creation with thinking_template\n        if use_thinking and thinking_template and not thinking_response:\n            prompt += _create_leading_prompt(prompt, query.leading_prompt, self.leading_prompt_token())\n\n            prompt += \"\\nNow, before answering the question, I am required to apply XML thinking template to guide my internal thinking process.\\n\" + \\\n                      f\"{thinking_example}\\n\" + \\\n                      \"The thinking template i need to apply to answer this question is as follow:\\n\" + \\\n                      thinking_template + \"\\n\" + \\\n                      f\"My thinking using the above XML template as follow:\\n\\n```xml\\n\"\n            # add ending token\n            # prompt += self.get_conversation_end_token()\n\n        elif use_thinking and thinking_template and thinking_response:\n            # the thinking result is ready here\n            # root key need to be added as we use it for leading prompt in the prompt creation above so it is not part of the response\n            prompt += \"\\nNow, before answering the question, I am required to apply XML thinking template to guide my internal thinking process.\\n\" + \\\n                      f\"{thinking_example}\\n\" + \\\n                      \"Now, the thinking template i need to apply to answer this question is:\\n\" + \\\n                      thinking_template + \"\\n\" + \\\n                      f\"My thinking using the XML template as follow:\\n```xml\\n{thinking_response}\\n\" + \\\n                      \"Now answer the question. Remember that the thinking above is INVISIBLE to user, \" + \\\n                      \"and you are PROHIBITED to mentioned to user about the existence of Thinking section. You are ALLOWED to reiterate points from thinking section to user if necessary.\"\n\n            # add ending token\n            prompt += self.get_conversation_end_token()\n\n            # for thinking response, we put it before the leading prompt\n            prompt += _create_leading_prompt(prompt, query.leading_prompt, self.leading_prompt_token())\n\n        else:\n            # add ending token\n            prompt += self.get_conversation_end_token()\n\n            prompt += _create_leading_prompt(prompt, query.leading_prompt, self.leading_prompt_token())\n\n\n        # add leading_prompt from code which is usually tool related\n        # e.g. ```json\n        if leading_prompt:      # leading prompt that passed to this function take highest priority\n            prompt += leading_prompt + \"\\n\"\n\n        return prefix_prompt + prompt\n\n        # match tool call result #TODO\n\n"}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/transformers/model_support/llama3_2_vision/text_streamer.py", "content": "from transformers import TextIteratorStreamer\nfrom queue import Queue\nfrom typing import Optional\n\n\nclass CustomTextIteratorStreamer(TextIteratorStreamer):\n    def __init__(\n            self,\n            tokenizer,  # Replace 'tokenizer' with 'processor'\n            skip_prompt: bool = False,\n            timeout: Optional[float] = None,\n            **decode_kwargs\n    ):\n        super().__init__(tokenizer, skip_prompt, timeout, **decode_kwargs)\n        self.tokenizer = tokenizer  # Store the processor instead of tokenizer\n\n    def put(self, value):\n        \"\"\"\n        Receives tokens, decodes them, and puts them in the queue.\n        \"\"\"\n        if len(value.shape) > 1 and value.shape[0] > 1:\n            raise ValueError(\"CustomTextIteratorStreamer only supports batch size 1\")\n\n        # Use value[0] instead of value\n        if len(value.shape) > 1:\n            value = value[0]\n\n        if self.skip_prompt and self.next_tokens_are_prompt:\n            self.next_tokens_are_prompt = False\n            return\n\n        # Add the new token to the cache\n        self.token_cache.extend(value.tolist())\n\n        # Use the processor to decode instead of tokenizer\n        text = self.tokenizer.decode(self.token_cache, **self.decode_kwargs)\n\n        # Process the text similarly to the original TextStreamer\n        if text.endswith(\"\\n\"):\n            printable_text = text[self.print_len:]\n            self.token_cache = []\n            self.print_len = 0\n        elif len(text) > 0 and self._is_chinese_char(ord(text[-1])):\n            printable_text = text[self.print_len:]\n            self.print_len += len(printable_text)\n        else:\n            printable_text = text[self.print_len: text.rfind(\" \") + 1]\n            self.print_len += len(printable_text)\n\n        self.on_finalized_text(printable_text)\n\n    def end(self):\n        \"\"\"Flushes any remaining cache and signals the end of the stream.\"\"\"\n        if len(self.token_cache) > 0:\n            # Use the processor to decode instead of tokenizer\n            text = self.tokenizer.decode(self.token_cache, **self.decode_kwargs)\n            printable_text = text[self.print_len:]\n            self.token_cache = []\n            self.print_len = 0\n        else:\n            printable_text = \"\"\n\n        self.next_tokens_are_prompt = True\n        self.on_finalized_text(printable_text, stream_end=True)"}
{"type": "source_file", "path": "src/gallama/backend/llm/format_enforcer.py", "content": "from typing import Literal\nfrom formatron.schemas.pydantic import ClassSchema\nfrom lmformatenforcer import JsonSchemaParser, RegexParser\nfrom lmformatenforcer.tokenenforcer import TokenEnforcerTokenizerData\nfrom gallama.logger.logger import logger\nfrom pydantic import BaseModel\nfrom gallama.backend.llm.tools import Tools\n\n\n# experimental support for formatron\ntry:\n    from formatron.formatter import FormatterBuilder\n    from formatron.integrations.exllamav2 import create_formatter_filter\n\nexcept:\n    FormatterBuilder = None\n    create_formatter_filter = None\n\nclass FormatEnforcer:\n    \"\"\" this class will help to create filter for generation enforcement\"\"\"\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def get_default_engine(\n        backend: Literal[\"exllama\", \"llama_cpp\", \"transformers\"] = \"exllama\",\n        preference: Literal[\"auto\", \"formatron\", \"lm-format-enforcer\"] = \"auto\",\n    ) -> Literal[\"formatron\", \"lm_enforcer\"]:\n        \"\"\" this function will select the format enforcer engine to use if not selected by user\"\"\"\n\n        if preference != \"auto\":\n            logger.info(f\"guided encoding preference: {preference}\")\n\n        # formatron doesnt support llama cpp at the moment\n        if backend == \"llama_cpp\":\n            return \"lm_enforcer\"\n        elif backend == \"exllama\" or backend==\"transformers\":\n            # use formatron if it is available if it is exllama\n            if preference == \"auto\":\n                if FormatterBuilder:\n                    return \"formatron\"\n                else:\n                    return \"lm_enforcer\"\n            else:\n                if preference == \"formatron\" and FormatterBuilder:\n                    return \"formatron\"\n                elif preference == \"lm-format-enforcer\":\n                    return \"lm_enforcer\"\n                else:\n                    raise \"Invalid backend\"\n        else:\n            raise \"Invalid backend\"\n\n\n\n    def regex(\n        self,\n        regex_pattern: str,\n        filter_engine: Literal[\n        \"formatron\", \"lm_enforcer\"] = None,\n        backend: Literal[\"exllama\", \"llama_cpp\", \"transformers\"] = \"exllama\",\n        preference: Literal[\"auto\", \"formatron\", \"lm-format-enforcer\"] = \"auto\",\n    ) -> FormatterBuilder | TokenEnforcerTokenizerData:\n\n        logger.info(backend)\n        # set the filter engine to use\n        if not filter_engine:\n            filter_engine = FormatEnforcer.get_default_engine(backend=backend, preference=preference)  # if engine is specified, use it\n\n        # create filter if engine is lm_enforcer\n        if filter_engine == \"lm_enforcer\":\n            return RegexParser(regex_pattern)\n\n        # create filter if engine is formatron\n        if filter_engine == \"formatron\":\n            f = FormatterBuilder()\n            _regex = f.regex(regex_pattern, capture_name='regex')\n            f.append_line(f\"{_regex}\")\n            return f\n\n    def json(\n        self,\n        pydantic_model_lmfe: BaseModel,\n        pydantic_model_formatron: ClassSchema,\n        filter_engine: Literal[\"formatron\", \"lm_enforcer\"] = None,\n        backend: Literal[\"llama_cpp\", \"exllama\", \"transformers\"] = \"exllama\",\n        preference: Literal[\"auto\", \"formatron\", \"lm-format-enforcer\"] = \"auto\",\n    ) -> FormatterBuilder | TokenEnforcerTokenizerData:\n        \"\"\" this function will return the filters for format enforcer to generate json output based on Pyantic model\"\"\"\n\n        # set the filter engine to use\n        if not filter_engine:\n            filter_engine = FormatEnforcer.get_default_engine(backend=backend, preference=preference)  # if engine is specified, use it\n\n        assert filter_engine == \"lm_enforcer\" or filter_engine == \"formatron\"\n\n        # create filter if engine is lm_enforcer\n        # if filter_engine == \"lm_enforcer\" or filter_engine == \"formatron\":  # TODO currently formatron and nested pydantic model is having issue\n        if filter_engine == \"lm_enforcer\":  # TODO currently formatron and nested pydantic model is having issue\n            json_schema = Tools.replace_refs_with_definitions_v2(pydantic_model_lmfe.model_json_schema())\n            return JsonSchemaParser(json_schema)\n\n        # create filter if engine is formatron\n        elif filter_engine == \"formatron\":\n            f = FormatterBuilder()\n            f.append_line(f\"{f.json(pydantic_model_formatron, capture_name='json')}\")\n            return f\n"}
{"type": "source_file", "path": "src/gallama/backend/llm/thinking_template.py", "content": "import os\nimport sys\n# Get the directory of the current script\n# current_dir = os.path.dirname(os.path.abspath(__file__))\nfrom pathlib import Path\nfrom lxml import etree\nfrom typing import Dict, Optional, List, Any\nfrom gallama.logger import logger\nimport re\n\n\ndef get_package_root():\n    \"\"\"\n    Get the root directory of the installed package or the current script directory.\n    \"\"\"\n    if getattr(sys, 'frozen', False):\n        # If the application is frozen (e.g., PyInstaller executable)\n        return Path(sys.executable).parent.parent\n    elif __package__:\n        # If it's an installed package\n        return Path(__file__).parent.parent.parent\n    else:\n        # If it's a standalone script\n        return Path(__file__).resolve().parent.parent.parent\n\n# Use the function to get the package root\ncurrent_dir = get_package_root()\n\nclass Thinking:\n    def __init__(self, xml: str, regex: Optional[str] = None, xml_is_file: bool = False, regex_is_file: bool = False):\n        self.xml: str = self._process_input(xml, xml_is_file, self.read_xml_file_to_string)\n        self.regex: Optional[str] = self._process_input(regex, regex_is_file, self.read_regex_file_to_string) if regex else None\n        self.xml_dict: Dict[str, Any] = {}\n        self.root_tag: Optional[str] = None\n        self.root_key_stop_words: List[str] = []\n        self._parse_xml()\n        self._generate_stop_words()\n\n    def _process_input(self, input_value: Optional[str], is_file: bool, read_func) -> Optional[str]:\n        if input_value is None:\n            return None\n        if not isinstance(input_value, str):\n            raise ValueError(\"Input must be a string (content or file path)\")\n        if is_file:\n            if not os.path.isfile(input_value):\n                raise FileNotFoundError(f\"File not found: {input_value}\")\n            return read_func(input_value)\n        return input_value\n\n    def _parse_xml(self):\n        root = etree.fromstring(self.xml)\n        self.root_tag = root.tag\n        self.xml_dict = self.parse_xml_to_dict(self.xml)\n\n    def _generate_stop_words(self):\n        # self.root_key_stop_words = [f\"</{self.root_tag}>\", f\"</{self.root_tag}>\\n\", f\"</{self.root_tag}>\\n```\"]\n        self.root_key_stop_words = [f\"</{self.root_tag}>\"]\n\n    @staticmethod\n    def read_xml_file_to_string(file_path: str) -> str:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            xml_string = file.read()\n        return xml_string\n\n    @staticmethod\n    def read_regex_file_to_string(file_path: str) -> str:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            regex_string = file.read().strip()\n        return regex_string\n\n    @staticmethod\n    def parse_xml_to_dict(xml_string: str) -> Dict[str, Any]:\n        root = etree.fromstring(xml_string)\n\n        def xml_to_dict(element):\n            children = list(element)\n            if not children:\n                return element.text\n            result = {}\n            for child in children:\n                child_dict = xml_to_dict(child)\n                if child.tag in result:\n                    if not isinstance(result[child.tag], list):\n                        result[child.tag] = [result[child.tag]]\n                    result[child.tag].append(child_dict)\n                else:\n                    result[child.tag] = child_dict\n            return result\n\n        return {root.tag: xml_to_dict(root)}\n\n    def validate_with_regex(self, value: str) -> str:\n        if self.regex:\n            pattern = re.compile(self.regex)\n            if not pattern.match(value):\n                raise ValueError(f\"Value '{value}' does not match the regex pattern.\")\n        return value\n\n    def get_stop_words(self) -> List[str]:\n        return self.root_key_stop_words\n\n\ndef find_xml_regex_pairs(directory:str = f\"{current_dir}/data/thinking_template/\"):\n    # Define a named tuple for our pairs, now including the XML filename without extension\n    thinking_dict = dict()\n\n\n    # Walk through the directory\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith('.xml'):\n                xml_file = os.path.join(root, file)\n                regex_file = os.path.join(root, file[:-4] + '.regex')\n\n                # Get the XML filename without extension\n                xml_filename, _ = os.path.splitext(file)\n\n                # Check if the corresponding .regex file exists\n                if not os.path.exists(regex_file):\n                    regex_file = None\n\n                thinking_dict[xml_filename] = Thinking(\n                    xml=xml_file,\n                    xml_is_file=True,\n                    regex=regex_file,\n                    regex_is_file=True\n                )\n\n    return thinking_dict\n\nTHINKING_TEMPLATE = find_xml_regex_pairs(f\"{current_dir}/data/thinking_template/\")\n\n"}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/mlx_vllm/mlx_vlm.py", "content": "from ..base import ModelInterface\nfrom typing import Optional, Dict, List, Union\nfrom fastapi import Request                 # for type hint\nimport time\n\n# for async running of mlx\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\ntry:\n    from mlx_vlm import load, generate, stream_generate\n    from mlx_vlm.prompt_utils import apply_chat_template\n    from mlx_vlm.utils import load_config\n    from mlx_vlm.video_generate import process_vision_info\nexcept:\n    load = None\n    generate = None\n    stream_generate = None\n    apply_chat_template = None\n    load_config = None\n    process_vision_info = None\n\n\n# custom data classes\nfrom gallama.data_classes import (\n    ModelSpec,\n    GenStart,\n    GenEnd,\n    GenQueue,\n    GenText,\n    GenerationStats,\n    QueueContext,\n    GenQueueDynamic\n)\n\nfrom gallama.logger.logger import logger\n\n\n\nclass ModelMLXVLM(ModelInterface):\n    def __init__(self, model_spec:ModelSpec):\n        super().__init__(model_spec)\n\n        self.model, self.tokenizer, self.processor = self.load_model()\n\n    @property\n    def support_tool(self) -> bool:\n        \"\"\"\n        Currently no format enforcement\n        \"\"\"\n        return False\n\n    def load_model(self):\n        \"\"\"Load the model, tokenizer, cache, and optional processor.\"\"\"\n\n        model, processor = load(path_or_hf_repo=self.model_id)\n        config = load_config(model_path=self.model_id)\n\n        # set max_seq_len\n        self.max_seq_len = config.get('max_position_embeddings')\n\n        # load draft model\n        if self.draft_model_id is not None:\n            raise \"Draft model currently not supported for mlx vllm backend\"\n\n        self.eos_token_ids = self.generate_eos_tokens_id()\n        return model, processor.tokenizer, processor\n\n\n    def generate_eos_tokens_id(self) -> List[int]:\n        \"\"\"Generate the end-of-sequence token IDs.\"\"\"\n\n        # currently the stop word for mlx vllm work by string and not by token id\n        # hence usage of this is not required\n        return []\n\n    # generation method from here ----------------------\n    def _run_generation(\n        self,\n        loop: asyncio.AbstractEventLoop,\n        prompt,\n        image_list: List[str],\n        max_tokens: int = 4096,\n        temperature: float = 1.0,\n        # stop,\n        gen_queue_list: List[QueueContext] = None,\n        top_p=0.8,\n        # prefix_strings=None,\n        # stop_word_to_return=\"\",\n        gen_type_str: str = \"text\",\n        # logits_processor=None,              # for formatron format enforcement\n        # prefix_allowed_tokens_fn=None       # for lmfe format enforcement\n    ):\n\n        # Run the llm's generate function in a separate thread\n        generator = stream_generate(\n            **{\n                'model': self.model,\n                'processor': self.processor,\n                'prompt': prompt,\n                'image': image_list,\n                'max_tokens': max_tokens,\n                'temperature': temperature,\n                'top_p': top_p,\n                'repetition_penalty': 1.1,\n            }\n        )\n\n        generate_text = \"\"\n        last_chunk = None\n        for chunk in generator:\n            last_chunk = chunk\n            chunk_text = GenText(content=chunk.text, text_type=gen_type_str)\n            generate_text += chunk_text.content\n            for g_queue in gen_queue_list:\n                future = asyncio.run_coroutine_threadsafe(\n                    g_queue.put(chunk_text),\n                    loop\n                )\n                future.result()\n\n\n        return generate_text, last_chunk\n\n\n\n    async def generate(\n        self,\n        prompt: str,\n        gen_queue: Union[GenQueue, QueueContext, List[QueueContext]],\n        request: Optional[Request] = None,\n        gen_type: Union[str, GenStart] = \"text\",    # the generated result will be store to this queue\n        temperature: float = 0.01,\n        top_p: float = 0.8,\n        formatter = None,   # not supported\n        stop_words: Union[List[str], str] = None,\n        prefix_strings: Optional[Union[str, List[str]]] = None,\n        banned_strings: list[str] | None = None,\n        max_tokens: int = None,\n        quiet=False,\n        messages: List = None,  # query.message for multimodal\n        **kwargs,\n    ) -> (str, GenerationStats):\n\n        loop = asyncio.get_event_loop()\n\n        if not quiet:\n            logger.info(\"----------------------Prompt---------------\\n\" + prompt)\n            logger.debug(\"----------------------temperature---------\\n\" + str(temperature))\n\n        # make gen_queue to List[QueueContext] for standardize downstream handling\n        gen_queue_list = None\n        # Convert gen_queue to List[GenQueueDynamic] format to standardize downstream handling\n        gen_queue_list = []\n        if isinstance(gen_queue, GenQueueDynamic):\n            gen_queue_list = [gen_queue]\n        elif isinstance(gen_queue, GenQueue):\n            # Wrap the GenQueue in a GenQueueDynamic\n            gen_queue_list = [GenQueueDynamic(existing_queue=gen_queue, include_GenStats=True, include_GenEnd=True)]\n        elif isinstance(gen_queue, list):\n            # Ensure all items in the list are GenQueueDynamic objects\n            for queue in gen_queue:\n                if isinstance(queue, GenQueueDynamic):\n                    gen_queue_list.append(queue)\n                elif isinstance(queue, GenQueue):\n                    # Wrap the GenQueue in a GenQueueDynamic\n                    gen_queue_list.append(\n                        GenQueueDynamic(existing_queue=queue, include_GenStats=True, include_GenEnd=True))\n                else:\n                    raise TypeError(\"gen_queue list must contain only GenQueue or GenQueueDynamic objects\")\n        else:\n            raise TypeError(\"gen_queue must be either a GenQueue, GenQueueDynamic, or a list of GenQueueDynamic\")\n\n        # vision support\n        image_inputs, video_inputs = None, None\n\n        # for now support only image\n\n        image_list = []\n        if messages:\n            # convert pydantic to basic dictionary\n            messages_as_dicts = [message.dict() for message in messages]\n\n            # convert OpenAI to qwen format -> TODO find more generalized method\n            # OpenAI format for image_url:\n            # {\n            #     \"type\": \"image\",\n            #     \"image\": {\n            #         \"image_url\": {\n            #             \"url\": \"url here\"\n            #         }\n            #     }\n            # }\n            # convert to qwen2 VL format:\n            # {\n            #     \"type\": \"image_url\",\n            #     \"image_url\": \"url here\"\n            # }\n            for one_message in messages_as_dicts:\n                if isinstance(one_message[\"content\"], list):\n                    for message in one_message[\"content\"]:\n                        if message.get(\"type\") == \"image_url\":\n                            image_list.append(message[\"image_url\"][\"url\"])\n                            # message[\"type\"] = \"image\"\n                            # message[\"image\"] = message[\"image_url\"][\"url\"]\n                            # message.pop(\"image_url\", None)\n\n            # image_inputs, video_inputs = process_vision_info(messages_as_dicts)\n\n\n        # # convert prompt to token id\n        # if image_inputs is None and video_inputs is None:\n        #     input_ids = self.tokenizer(prompt, return_tensors=\"pt\")\n        # else:   # multimodal\n        #     input_ids = self.processor(\n        #         text=[prompt],\n        #         images=image_inputs,\n        #         #videos=video_inputs,       # TODO currently Llama doesnt support videos, comment out for now.\n        #         #padding=True,\n        #         add_special_tokens=False,\n        #         return_tensors=\"pt\",\n        #     )\n\n        input_ids = self.tokenizer.encode(prompt)\n        self.validate_token_length(len(input_ids))\n\n        # # format enforcer\n        # logits_processor = None\n        # prefix_allowed_tokens_fn = None\n        # if formatter:\n        #     if isinstance(formatter, FormatterBuilder):\n        #         logits_processor = create_formatter_logits_processor_list(self.tokenizer, formatter)\n        #     else:\n        #         prefix_allowed_tokens_fn = build_transformers_prefix_allowed_tokens_fn(self.tokenizer, formatter)\n\n        # manual end of string not supported yet -------\n        # # find stop conditions\n        # stop_word_to_return = \"\"\n        # if stop_words:\n        #     if isinstance(stop_words, str):\n        #         stop_word_to_return = stop_words\n        #         stop_words = [stop_words]\n        #\n        #     elif isinstance(stop_words, list):\n        #         stop_word_to_return = stop_words[0]\n        #\n        #     if not self.eos_token_str:\n        #         raise Exception(\"EOS token not set in model_config\")\n        #     stop_conditions = self.eos_token_str + stop_words  # concat the 2 list\n        #     logger.debug(\"stop_words: \" + str(stop_conditions))\n        # else:\n        #     stop_conditions = self.eos_token_str\n\n        max_tokens_to_use = min(\n            self.max_seq_len - len(input_ids),\n            max_tokens, 4096) if max_tokens else min(self.max_seq_len - len(input_ids), 4096)\n\n        # kickstart the generation and let down stream know gen type\n        if isinstance(gen_type, str):\n            gen_type_str = gen_type\n            gen_type = GenStart(gen_type=gen_type)\n        else:\n            gen_type_str = gen_type.gen_type  # get out the generation type in str format\n\n        for g_queue in gen_queue_list:\n            g_queue.put_nowait(gen_type)\n\n        # Create a task to check for disconnection\n        # pass\n\n        # generate\n\n        start_time = time.time()\n\n        with ThreadPoolExecutor() as pool:\n            generate_text, last_chunk = await loop.run_in_executor(\n                pool,\n                self._run_generation,\n                loop, # <-- Passing the main event loop\n                prompt,\n                image_list,\n                max_tokens_to_use,\n                temperature,\n                gen_queue_list,\n                top_p,\n                gen_type_str,\n            )\n\n        duration = time.time() - start_time\n\n        gen_stats = GenerationStats(\n            input_tokens_count=last_chunk.prompt_tokens,\n            output_tokens_count=last_chunk.generation_tokens,\n            time_generate=duration,\n        )\n        for g_queue in gen_queue_list:\n            if g_queue.include_GenStats:\n                g_queue.put_nowait(gen_stats)\n\n        # this to signal the end of generation\n        for g_queue in gen_queue_list:\n            if g_queue.include_GenEnd:\n                g_queue.put_nowait(GenEnd())\n\n        logger.debug(\"----------------------LLM Raw Response---------------\\n\" + generate_text)"}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/transformers/model_support/__init__.py", "content": ""}
{"type": "source_file", "path": "src/gallama/backend/llm/engine/llamacpp/llamacpp.py", "content": "from ..base import ModelInterface\nfrom typing import Optional, Dict, List, Union\nfrom fastapi import Request                 # for type hint\nimport time\n\n# for async running of llama cpp\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\ntry:\n    from llama_cpp import Llama\n    from llama_cpp import LogitsProcessorList\nexcept:\n    Llama = None\n    LogitsProcessorList = None\n\n# format enforcement with formatron\ntry:\n    from formatron.formatter import FormatterBuilder\nexcept ImportError:\n    FormatterBuilder = None\n\n# format enforcement with llama cpp\ntry:\n    from lmformatenforcer.tokenenforcer import TokenEnforcerTokenizerData\n    from lmformatenforcer.integrations.llamacpp import (\n        build_llamacpp_logits_processor,\n        build_token_enforcer_tokenizer_data as build_token_enforcer_tokenizer_data_llama_cpp\n    )\nexcept:\n    # llama_cpp optional dependency\n    build_llamacpp_logits_processor = None\n    build_token_enforcer_tokenizer_data_llama_cpp = None\n    LogitsProcessorList = None\n\n# custom data classes\nfrom gallama.data_classes import (\n    ModelSpec,\n    GenStart,\n    GenEnd,\n    GenQueue,\n    GenText,\n    GenerationStats,\n    QueueContext,\n    GenQueueDynamic\n)\nfrom gallama.logger.logger import logger\n\n\nclass ModelLlamaCpp(ModelInterface):\n    def __init__(self, model_spec:ModelSpec):\n        super().__init__(model_spec)\n\n        self.model, self.tokenizer = self.load_model()\n\n        # initialize lmfe tokenizer data\n        self.lm_enforcer_tokenizer_data = build_token_enforcer_tokenizer_data_llama_cpp(self.model)\n\n\n    def load_model(self):\n        \"\"\"Load the model, tokenizer, cache, and optional processor.\"\"\"\n\n        model, tokenizer = self.load_model_llama_cpp(\n            model_id=self.model_id,\n            gpus=self.gpus,\n        )\n\n        # load draft model\n        if self.draft_model_id is not None:\n            raise \"Draft model currently not supported for llama cpp backend\"\n\n        self.eos_token_ids = self.generate_eos_tokens_id()\n        return model, tokenizer\n\n\n    def load_model_llama_cpp(self, model_id, gpus):\n        \"\"\"This function return the model and its tokenizer\"\"\"\n        logger.info(\"Loading model: \" + model_id)\n\n        # currently speculative decoding not supported by model specific for LLama CPP python\n        if isinstance(gpus, str) and gpus == \"auto\":\n            model = Llama(\n                model_path=self.model_id,\n                n_gpu_layers=-1,\n                seed=1,\n                n_ctx=self.max_seq_len if self.max_seq_len else 0,  # Uncomment to increase the context window\n                flash_attn=True,\n                offload_kqv=True,\n                # draft_model=draf_model_id,\n            )\n        elif isinstance(gpus, list):  # user specify the gpus split\n            model = Llama(\n                model_path=self.model_id,\n                n_gpu_layers=-1,\n                seed=1,\n                n_ctx=self.max_seq_len if self.max_seq_len else 0,  # Uncomment to increase the context window\n                flash_attn=True,\n                offload_kqv=True,\n                tensor_split=gpus,\n                # draf_model_id=draf_model_id,\n            )\n        else:\n            raise ValueError(\"Device map should be either 'auto', 'gpu' split\")\n\n        # set max_seq_len based on model\n        self.max_seq_len = model._model.n_ctx_train()\n        tokenizer = model       # llama cpp doesnt have separate tokenizer object\n\n        self.eos_token_ids = self.generate_eos_tokens_id()\n\n        return model, tokenizer\n\n\n\n    def generate_eos_tokens_id(self) -> List[int]:\n        \"\"\"Generate the end-of-sequence token IDs.\"\"\"\n\n        # currently the stop word for llama cpp work by string and not by token id\n        # hence usage of this is not required\n        return []\n\n\n    # ************* method for generation from here\n    def _run_generator_and_queue(self, prompt, logits_processor, max_tokens, temperature, stop, gen_queue_list,\n                                 top_p=0.8, prefix_strings=None, stop_word_to_return=\"\", gen_type_str: str=\"text\"):\n\n        loop = asyncio.get_event_loop()\n\n        generator = self.model(\n            prompt=prompt,\n            #suffix=prefix_strings,\n            logits_processor=logits_processor,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            min_p=0.05,\n            stop=stop,\n            repeat_penalty=1.1,\n            stream=True,\n        )\n\n        generate_text = \"\"\n        for chunk in generator:\n            chunk_text = GenText(content=chunk['choices'][0]['text'], text_type=gen_type_str)\n            generate_text += chunk_text.content\n            for g_queue in gen_queue_list:\n                # g_queue.get_queue().put_nowait(chunk_text)\n                future = asyncio.run_coroutine_threadsafe(\n                    g_queue.put(chunk_text),\n                    loop\n                )\n                future.result()\n\n        # return stop_word if there is\n        if stop_word_to_return:\n            chunk_text = GenText(content=stop_word_to_return, text_type=gen_type_str)\n            generate_text += chunk_text.content\n            for g_queue in gen_queue_list:\n                # g_queue.get_queue().put_nowait(chunk_text)\n                future = asyncio.run_coroutine_threadsafe(\n                    g_queue.put(chunk_text),\n                    loop\n                )\n                future.result()\n        return generate_text\n\n    async def generate(\n        self,\n        prompt: str,\n        gen_queue: Union[GenQueue, QueueContext, List[QueueContext]],\n        request: Optional[Request] = None,          # for disconnection check, however, no stopping feature for llama cpp\n        gen_type: Union[str, GenStart] = \"text\",    # the generated result will be store to this queue\n        temperature: float = 0.01,\n        top_p: float = 0.8,\n        formatter: FormatterBuilder | TokenEnforcerTokenizerData = None,\n        stop_words: Union[List[str], str] = None,\n        prefix_strings: Optional[Union[str, List[str]]] = None,\n        banned_strings: list[str] | None = None,\n        max_tokens: int = None,\n        quiet=False,\n        messages: List = None,  # query.message for multimodal\n        **kwargs,\n    ) -> (str, GenerationStats):\n\n        loop = asyncio.get_event_loop()\n\n        if not quiet:\n            logger.info(\"----------------------Prompt---------------\\n\" + prompt)\n            logger.debug(\"----------------------temperature---------\\n\" + str(temperature))\n\n        # ensure that generator is initialized\n        # if self.pipeline is None:\n        #     self.pipeline = await self._get_pipeline_async()\n\n        # make gen_queue to List[QueueContext] for standardize downstream handline\n        # Convert gen_queue to List[GenQueueDynamic] format to standardize downstream handling\n        gen_queue_list = []\n        if isinstance(gen_queue, GenQueueDynamic):\n            gen_queue_list = [gen_queue]\n        elif isinstance(gen_queue, GenQueue):\n            # Wrap the GenQueue in a GenQueueDynamic\n            gen_queue_list = [GenQueueDynamic(existing_queue=gen_queue, include_GenStats=True, include_GenEnd=True)]\n        elif isinstance(gen_queue, list):\n            # Ensure all items in the list are GenQueueDynamic objects\n            for queue in gen_queue:\n                if isinstance(queue, GenQueueDynamic):\n                    gen_queue_list.append(queue)\n                elif isinstance(queue, GenQueue):\n                    # Wrap the GenQueue in a GenQueueDynamic\n                    gen_queue_list.append(\n                        GenQueueDynamic(existing_queue=queue, include_GenStats=True, include_GenEnd=True))\n                else:\n                    raise TypeError(\"gen_queue list must contain only GenQueue or GenQueueDynamic objects\")\n        else:\n            raise TypeError(\"gen_queue must be either a GenQueue, GenQueueDynamic, or a list of GenQueueDynamic\")\n\n        # convert prompt to token id\n        input_ids = self.tokenizer.tokenize(prompt.encode(\"utf-8\"), add_bos=False)\n        self.validate_token_length(len(input_ids))\n\n        # format enforcer\n        logits_processors = None\n        if formatter:\n            logits_processors = LogitsProcessorList([\n                build_llamacpp_logits_processor(\n                    llm=self.pipeline.lm_enforcer_tokenizer_data,\n                    character_level_parser=formatter,\n                )\n            ])\n\n        start_time = time.time()\n\n        # find stop conditions\n        stop_word_to_return = \"\"\n        if stop_words:\n            if isinstance(stop_words, str):\n                stop_word_to_return = stop_words\n                stop_words = [stop_words]\n\n            elif isinstance(stop_words, list):\n                stop_word_to_return = stop_words[0]\n\n            if not self.eos_token_str:\n                raise Exception(\"EOS token not set in model_config\")\n            stop_conditions = self.eos_token_str + stop_words  # concat the 2 list\n            logger.debug(\"stop_words: \" + str(stop_conditions))\n        else:\n            stop_conditions = self.eos_token_str\n\n        max_tokens_to_use = min(\n            self.max_seq_len - len(input_ids),\n            max_tokens, 4096) if max_tokens else min(self.max_seq_len - len(input_ids),\n                                                     4096\n                                                     )\n\n        # kick-start the generation and let down stream know gen type\n        if isinstance(gen_type, str):\n            gen_type_str = gen_type\n            gen_type = GenStart(gen_type=gen_type)\n        else:\n            gen_type_str = gen_type.gen_type  # get out the generation type in str format\n\n        for g_queue in gen_queue_list:\n            # g_queue.get_queue().put_nowait(gen_type)\n            future = asyncio.run_coroutine_threadsafe(\n                g_queue.put(gen_type),\n                loop\n            )\n            future.result()\n\n        # Create a task to check for disconnection\n        # llama cpp python generator is not async, hence running fake async..\n        # Run the synchronous generator in a separate thread\n        with ThreadPoolExecutor() as pool:\n            generate_text = await loop.run_in_executor(\n                pool,\n                self._run_generator_and_queue,\n                prompt, logits_processors, max_tokens_to_use, temperature, stop_conditions, gen_queue_list,\n                top_p, prefix_strings, stop_word_to_return, gen_type_str\n            )\n\n        duration = time.time() - start_time\n\n        gen_stats = GenerationStats()\n        for g_queue in gen_queue_list:\n            if g_queue.include_GenStats:\n                # g_queue.get_queue().put_nowait(gen_stats)\n                future = asyncio.run_coroutine_threadsafe(\n                    g_queue.put(gen_stats),\n                    loop\n                )\n                future.result()\n\n\n        # this to signal the end of generation\n        for g_queue in gen_queue_list:\n            if g_queue.include_GenEnd:\n                # g_queue.get_queue().put_nowait(GenEnd())\n                future = asyncio.run_coroutine_threadsafe(\n                    g_queue.put(GenEnd()),\n                    loop\n                )\n                future.result()\n        logger.debug(\"----------------------LLM Raw Response---------------\\n\" + generate_text)"}
{"type": "source_file", "path": "src/gallama/backend/stt/faster_whisper/model.py", "content": "from ..base import ASRBase\nfrom ....logger import logger\nfrom ....data_classes import ModelSpec, TranscriptionResponse, LanguageType, TimeStampedWord\n\nimport dataclasses\nfrom typing import Literal, List, Union, BinaryIO, Tuple\nimport numpy as np\n\ntry:\n    from faster_whisper import WhisperModel, BatchedInferencePipeline\n    from faster_whisper.transcribe import TranscriptionOptions, TranscriptionInfo, Segment\nexcept ImportError:\n    WhisperModel, BatchedInferencePipeline, TranscriptionOptions, TranscriptionInfo, Segment = None, None, None, None, None\n\n\n\nclass ASRFasterWhisper(ASRBase):\n    \"\"\"Uses faster-whisper library as the backend. Works much faster, appx 4-times (in offline mode). For GPU, it requires installation with a specific CUDNN version.\n    \"\"\"\n\n    class ModelWrapper:\n        def __init__(self, model, model_batch):\n            self.model = model\n            self.model_batch = model_batch\n\n        def transcribe(self, _type: Literal[\"single\", \"batch\"] = \"single\", **kwargs):\n            if _type == \"single\":\n                logger.debug(\"using single transcribe\")\n                return self.model.transcribe(**kwargs)\n            elif _type == \"batch\":\n                logger.debug(\"using batch transcribe\")\n                return self.model_batch.transcribe(**kwargs)\n            else:\n                raise ValueError(\"type must be either 'single' or 'batch'\")\n\n        def detect_language(self, **kwargs):\n            return self.model.detect_language(**kwargs)\n\n    def load_model(\n        self,\n        model_spec: ModelSpec,\n    ):\n\n        # set seperator for faster whisper to \"\"\n        self.sep = \"\"                       # seperator for faster whisper is \"\"\n        self.compute_type = \"float16\"       # to store the quantization\n        self.model_id = model_spec.model_id      # to store path to model\n        self.model_name = model_spec.model_name\n        self.device = \"cuda\" # if device == \"gpu\" else \"cpu\"\n\n\n        if self.quant == \"8.0\":\n            self.compute_type = \"int8_float16\"\n\n        # load model\n        model = WhisperModel(\n            self.model_id,\n            device=self.device,\n            compute_type=self.compute_type,\n            # download_root=cache_dir\n        )\n\n        model_batched = BatchedInferencePipeline(model)\n\n        return self.ModelWrapper(model, model_batched)\n\n    @staticmethod\n    def _filter_lanague(allowed_lanague: List[str], all_language_probs: List[Tuple[str, float]]):\n        \"\"\" find the most probable language from a list of language and their probability\"\"\"\n        most_probable_langauge = all_language_probs[0][0]\n\n        for langauge, prob in all_language_probs:\n            if langauge in allowed_lanague:\n                return langauge\n\n        return most_probable_langauge\n\n    def transcribe_to_segment(\n        self,\n        audio: Union[str, BinaryIO, np.ndarray],\n        init_prompt: str = \"\",\n        temperature: float = 0.0,\n        language: LanguageType = None,\n        batch: bool = False,\n        batch_size: int = 8,\n    ) -> List[Segment]:\n        \"\"\"\n        basic function to transcribe audio to segment data type\n\n        Example of segment data:\n        Segment(id=1, seek=0, start=0.0, end=4.0, text=' Then you g',\n        tokens=[50365, 1396, 291, ],\n        avg_logprob=-0.4401855356991291,\n        compression_ratio=1.0185185185185186,\n        no_speech_prob=0.06640625,\n        words=None,\n        temperature=0.0)\n        \"\"\"\n        if language:\n            logger.info(f\"language is set to {language}\")\n\n        # using default parameters mostly\n        language = language if language else self.original_language\n        if isinstance(language, list) and len(language) == 1:\n            language = language[0]\n\n        if language == \"auto\":\n            language = None\n\n        # transcription for multilinguage with restrictred set of lanaguage\n        if isinstance(language, list) and len(language) > 1:\n            _language, _language_probability, all_language_probs = self.model.detect_language(audio=audio)\n            logger.info(\"allowed language list: %s\", language)\n            if _language in language:\n                most_probable_language = _language\n            else:\n                most_probable_language = self._filter_lanague(language, all_language_probs)\n\n            segments, info = self.model.transcribe(\n                _type=\"single\", # only work with single\n                audio=audio,\n                language=most_probable_language,\n                multilingual=False if language else True,\n                task=\"transcribe\",\n                initial_prompt=init_prompt,\n                beam_size=5,                        # tested: beam_size=5 is faster and better\n                word_timestamps=True,               # timestamp is used for sequence matching when streaming\n                condition_on_previous_text=True,\n                temperature=temperature,\n                vad_filter=True,\n                repetition_penalty=1.0,\n                **self.transcribe_kargs\n            )\n        else:  # all other scenario\n            kwargs_args = {\n                \"_type\": \"single\",\n                \"audio\": audio,\n                \"language\": language,\n                \"multilingual\": False if language else True,\n                \"task\": \"transcribe\",\n                \"initial_prompt\": init_prompt,\n                \"beam_size\": 5,  # tested: beam_size=5 is faster and better\n                \"word_timestamps\": True,  # timestamp is used for sequence matching when streaming\n                \"condition_on_previous_text\": True,\n                \"temperature\": temperature,\n                \"vad_filter\": True,\n                \"repetition_penalty\": 1.0,\n                ** self.transcribe_kargs\n            }\n\n            if batch:\n                kwargs_args.update(\n                    {\n                        \"_type\": \"batch\",\n                        \"batch_size\": batch_size if batch else 8,\n                    }\n                )\n\n            segments, info = self.model.transcribe(**kwargs_args)\n\n        logger.debug(info)  # info contains language detection result\n\n        return list(segments)\n\n    def transcribe(\n        self,\n        audio: Union[str, BinaryIO, np.ndarray],\n        init_prompt: str = \"\",\n        temperature: float = 0.0,\n        language: LanguageType = None,\n        include_segments: bool = False,\n        batch: bool = False,\n    ) -> TranscriptionResponse:\n        \"\"\"\n        similar to transcribe_to_segment, however, will concat all the words into the full text\n        \"\"\"\n\n        # using default parameters mostly\n        segments = self.transcribe_to_segment(\n            audio,\n            init_prompt=init_prompt,\n            temperature=temperature,\n            language=language,\n            batch=batch\n        )\n        segments = list(segments)   # convert iterable to list\n\n        transcribed_text = self.segment_to_long_text(segments)\n\n        if not include_segments:\n            return TranscriptionResponse(text=transcribed_text)\n        else:\n            # convert to dictionary for segment\n            segments_dict = [dataclasses.asdict(segment) for segment in segments]\n\n            return TranscriptionResponse(\n                text=transcribed_text,\n                segments=segments_dict    # convert to dictionary\n            )\n\n\n    def segment_to_timestamped_words(self, segments: List[Segment]) -> List[Tuple]:\n        \"\"\"\n        Converts a list of segments into a list of word-level timestamps.\n\n        Args:\n            segments (List[Segment]): Transcription segments with words and metadata.\n\n        Returns:\n            List[Tuple[float, float, str]]: List of (start, end, word) tuples.\n        \"\"\"\n        output = []\n        for segment in segments:\n            if segment.no_speech_prob > 0.9:    # silence segment\n                continue\n            for word in segment.words:\n                output.append((word.start, word.end, word.word))\n        return output\n\n\n    def segment_to_long_text(self, segments: List[Segment]) -> str:\n        \"\"\"\n        Converts a list of segments into a full text.\n\n        Args:\n            segments (List[Segment]): Transcription segments with words and metadata.\n\n        Returns:\n            List[Tuple[float, float, str]]: List of (start, end, word) tuples.\n        \"\"\"\n\n        output = []\n        for segment in segments:\n            if segment.no_speech_prob > 0.9:    # silence segment\n                continue\n            for word in segment.words:\n                output.append(word.word)\n\n        return self.sep.join(output)\n\n\n    def segments_end_ts(self, res):\n        \"\"\"\n        return a list of ending time stamp from a list of segments\n        \"\"\"\n        return [s.end for s in res]\n\n    def use_vad(self):\n        self.transcribe_kargs[\"vad_filter\"] = True\n\n    def set_translate_task(self):\n        self.transcribe_kargs[\"task\"] = \"translate\""}
{"type": "source_file", "path": "src/gallama/backend/tts/model/gpt_sovits_source/GPT_SoVITS/AR/data/dataset.py", "content": "# modified from https://github.com/yangdongchao/SoundStorm/blob/master/soundstorm/s1/AR/data/dataset.py\n# reference: https://github.com/lifeiteng/vall-e\nimport pdb\nimport sys\n\n# sys.path.append(\"/data/docker/liujing04/gpt-vits/mq-vits-s1bert_no_bert\")\nimport traceback, os\nfrom typing import Dict\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\nimport torch, json\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer\n\nversion = os.environ.get('version',None)\n\nfrom text import cleaned_text_to_sequence\n\n# from config import exp_dir\n\n\ndef batch_sequences(sequences: List[np.array], axis: int = 0, pad_value: int = 0):\n    seq = sequences[0]\n    ndim = seq.ndim\n    if axis < 0:\n        axis += ndim\n    dtype = seq.dtype\n    pad_value = dtype.type(pad_value)\n    seq_lengths = [seq.shape[axis] for seq in sequences]\n    max_length = np.max(seq_lengths)\n\n    padded_sequences = []\n    for seq, length in zip(sequences, seq_lengths):\n        padding = (\n            [(0, 0)] * axis + [(0, max_length - length)] + [(0, 0)] * (ndim - axis - 1)\n        )\n        padded_seq = np.pad(seq, padding, mode=\"constant\", constant_values=pad_value)\n        padded_sequences.append(padded_seq)\n    batch = np.stack(padded_sequences)\n    return batch\n\n\nclass Text2SemanticDataset(Dataset):\n    \"\"\"dataset class for text tokens to semantic model training.\"\"\"\n\n    def __init__(\n        self,\n        phoneme_path: str,\n        semantic_path: str,\n        max_sample: int = None,\n        max_sec: int = 100,\n        pad_val: int = 1024,\n        # min value of phoneme/sec\n        min_ps_ratio: int = 3,\n        # max value of phoneme/sec\n        max_ps_ratio: int = 25,\n    ) -> None:\n        super().__init__()\n\n        self.semantic_data = pd.read_csv(\n            semantic_path, delimiter=\"\\t\", encoding=\"utf-8\"\n        )\n        # get dict\n        self.path2 = phoneme_path  # \"%s/2-name2text.txt\"%exp_dir#phoneme_path\n        self.path3 = \"%s/3-bert\" % (\n            os.path.dirname(phoneme_path)\n        )  # \"%s/3-bert\"%exp_dir#bert_dir\n        self.path6 = semantic_path  # \"%s/6-name2semantic.tsv\"%exp_dir#semantic_path\n        assert os.path.exists(self.path2)\n        assert os.path.exists(self.path6)\n        self.phoneme_data = {}\n        with open(self.path2, \"r\", encoding=\"utf8\") as f:\n            lines = f.read().strip(\"\\n\").split(\"\\n\")\n\n        for line in lines:\n            tmp = line.split(\"\\t\")\n            if len(tmp) != 4:\n                continue\n            self.phoneme_data[tmp[0]] = [tmp[1], tmp[2], tmp[3]]\n\n        # self.phoneme_data = np.load(phoneme_path, allow_pickle=True).item()\n        # pad for semantic tokens\n        self.PAD: int = pad_val\n        # self.hz = 25\n        # with open(\"/data/docker/liujing04/gpt-vits/mq-vits-s1bert_no_bert/configs/s2.json\", \"r\") as f:data = f.read()\n        # data=json.loads(data)[\"model\"][\"semantic_frame_rate\"]#50hz\n        # self.hz=int(data[:-2])#\n        self.hz = int(os.environ.get(\"hz\", \"25hz\")[:-2])\n\n        # max seconds of semantic token\n        self.max_sec = max_sec\n        self.min_ps_ratio = min_ps_ratio\n        self.max_ps_ratio = max_ps_ratio\n\n        if max_sample is not None:\n            self.semantic_data = self.semantic_data[:max_sample]\n\n        # {idx: (semantic, phoneme)}\n        # semantic list, phoneme list\n        self.semantic_phoneme = []\n        self.item_names = []\n\n        self.inited = False\n\n        if not self.inited:\n            # 调用初始化函数\n            self.init_batch()\n            self.inited = True\n            del self.semantic_data\n            del self.phoneme_data\n        # self.tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")\n        # self.tokenizer = AutoTokenizer.from_pretrained(\"/data/docker/liujing04/bert-vits2/Bert-VITS2-master20231106/bert/chinese-roberta-wwm-ext-large\")\n\n    def init_batch(self):\n        semantic_data_len = len(self.semantic_data)\n        phoneme_data_len = len(self.phoneme_data.keys())\n        print(\"semantic_data_len:\", semantic_data_len)\n        print(\"phoneme_data_len:\", phoneme_data_len)\n        print(self.semantic_data)\n        idx = 0\n        num_not_in = 0\n        num_deleted_bigger = 0\n        num_deleted_ps = 0\n        for i in range(semantic_data_len):\n            # 先依次遍历\n            # get str\n            item_name = self.semantic_data.iloc[i,0]\n            # print(self.phoneme_data)\n            try:\n                phoneme, word2ph, text = self.phoneme_data[item_name]\n            except Exception:\n                traceback.print_exc()\n                # print(f\"{item_name} not in self.phoneme_data !\")\n                num_not_in += 1\n                continue\n\n            semantic_str = self.semantic_data.iloc[i,1]\n            # get token list\n            semantic_ids = [int(idx) for idx in semantic_str.split(\" \")]\n            # (T), 是否需要变成 (1, T) -> 不需要，因为需要求 len\n            # 过滤掉太长的样本\n            if (\n                len(semantic_ids) > self.max_sec * self.hz\n            ):  #########1###根据token个数推测总时长过滤时长60s（config里）#40*25=1k\n                num_deleted_bigger += 1\n                continue\n            # (T, ), 这个速度不会很慢，所以可以在一开始就处理，无需在 __getitem__ 里面单个处理####\n            phoneme = phoneme.split(\" \")\n\n            try:\n                phoneme_ids = cleaned_text_to_sequence(phoneme, version)\n            except:\n                traceback.print_exc()\n                # print(f\"{item_name} not in self.phoneme_data !\")\n                num_not_in += 1\n                continue\n            # if len(phoneme_ids) >400:###########2：改为恒定限制为semantic/2.5就行\n            if (\n                len(phoneme_ids) > self.max_sec * self.hz / 2.5\n            ):  ###########2：改为恒定限制为semantic/2.5就行\n                num_deleted_ps += 1\n                continue\n            # if len(semantic_ids) > 1000:###########3\n            #     num_deleted_bigger += 1\n            #     continue\n\n            ps_ratio = len(phoneme_ids) / (len(semantic_ids) / self.hz)\n\n            if (\n                ps_ratio > self.max_ps_ratio or ps_ratio < self.min_ps_ratio\n            ):  ##########4#3~25#每秒多少个phone\n                num_deleted_ps += 1\n                # print(item_name)\n                continue\n\n            self.semantic_phoneme.append((semantic_ids, phoneme_ids))\n            idx += 1\n            self.item_names.append(item_name)\n\n        min_num = 100  # 20直接不补#30补了也不存ckpt\n        leng = len(self.semantic_phoneme)\n        if leng < min_num:\n            tmp1 = self.semantic_phoneme\n            tmp2 = self.item_names\n            self.semantic_phoneme = []\n            self.item_names = []\n            for _ in range(max(2, int(min_num / leng))):\n                self.semantic_phoneme += tmp1\n                self.item_names += tmp2\n        if num_not_in > 0:\n            print(f\"there are {num_not_in} semantic datas not in phoneme datas\")\n        if num_deleted_bigger > 0:\n            print(\n                f\"deleted {num_deleted_bigger} audios who's duration are bigger than {self.max_sec} seconds\"\n            )\n        if num_deleted_ps > 0:\n            # 4702 for LibriTTS, LirbriTTS 是标注数据, 是否需要筛？=> 需要，有值为 100 的极端值\n            print(\n                f\"deleted {num_deleted_ps} audios who's phoneme/sec are bigger than {self.max_ps_ratio} or smaller than {self.min_ps_ratio}\"\n            )\n        \"\"\"\n        there are 31 semantic datas not in phoneme datas\n        deleted 34 audios who's duration are bigger than 54 seconds\n        deleted 3190 audios who's phoneme/sec are bigger than 25 or smaller than 3\n        dataset.__len__(): 366463\n\n        \"\"\"\n        # 345410 for LibriTTS\n        print(\"dataset.__len__():\", self.__len__())\n\n    def __get_item_names__(self) -> List[str]:\n        return self.item_names\n\n    def __len__(self) -> int:\n        return len(self.semantic_phoneme)\n\n    def __getitem__(self, idx: int) -> Dict:\n        semantic_ids, phoneme_ids = self.semantic_phoneme[idx]\n        item_name = self.item_names[idx]\n        phoneme_ids_len = len(phoneme_ids)\n        # semantic tokens target\n        semantic_ids_len = len(semantic_ids)\n\n        flag = 0\n        path_bert = \"%s/%s.pt\" % (self.path3, item_name)\n        if os.path.exists(path_bert) == True:\n            bert_feature = torch.load(path_bert, map_location=\"cpu\")\n        else:\n            flag = 1\n        if flag == 1:\n            # bert_feature=torch.zeros_like(phoneme_ids,dtype=torch.float32)\n            bert_feature = None\n        else:\n            assert bert_feature.shape[-1] == len(phoneme_ids)\n        return {\n            \"idx\": idx,\n            \"phoneme_ids\": phoneme_ids,\n            \"phoneme_ids_len\": phoneme_ids_len,\n            \"semantic_ids\": semantic_ids,\n            \"semantic_ids_len\": semantic_ids_len,\n            \"bert_feature\": bert_feature,\n        }\n\n    def get_sample_length(self, idx: int):\n        semantic_ids = self.semantic_phoneme[idx][0]\n        sec = 1.0 * len(semantic_ids) / self.hz\n        return sec\n\n    def collate(self, examples: List[Dict]) -> Dict:\n        sample_index: List[int] = []\n        phoneme_ids: List[torch.Tensor] = []\n        phoneme_ids_lens: List[int] = []\n        semantic_ids: List[torch.Tensor] = []\n        semantic_ids_lens: List[int] = []\n        # return\n\n        for item in examples:\n            sample_index.append(item[\"idx\"])\n            phoneme_ids.append(np.array(item[\"phoneme_ids\"], dtype=np.int64))\n            semantic_ids.append(np.array(item[\"semantic_ids\"], dtype=np.int64))\n            phoneme_ids_lens.append(item[\"phoneme_ids_len\"])\n            semantic_ids_lens.append(item[\"semantic_ids_len\"])\n\n        # pad 0\n        phoneme_ids = batch_sequences(phoneme_ids)\n        semantic_ids = batch_sequences(semantic_ids, pad_value=self.PAD)\n\n        # # convert each batch to torch.tensor\n        phoneme_ids = torch.tensor(phoneme_ids)\n        semantic_ids = torch.tensor(semantic_ids)\n        phoneme_ids_lens = torch.tensor(phoneme_ids_lens)\n        semantic_ids_lens = torch.tensor(semantic_ids_lens)\n        bert_padded = torch.FloatTensor(len(examples), 1024, max(phoneme_ids_lens))\n        bert_padded.zero_()\n\n        for idx, item in enumerate(examples):\n            bert = item[\"bert_feature\"]\n            if bert != None:\n                bert_padded[idx, :, : bert.shape[-1]] = bert\n\n        return {\n            # List[int]\n            \"ids\": sample_index,\n            # torch.Tensor (B, max_phoneme_length)\n            \"phoneme_ids\": phoneme_ids,\n            # torch.Tensor (B)\n            \"phoneme_ids_len\": phoneme_ids_lens,\n            # torch.Tensor (B, max_semantic_ids_length)\n            \"semantic_ids\": semantic_ids,\n            # torch.Tensor (B)\n            \"semantic_ids_len\": semantic_ids_lens,\n            # torch.Tensor (B, 1024, max_phoneme_length)\n            \"bert_feature\": bert_padded,\n        }\n\n\nif __name__ == \"__main__\":\n    root_dir = \"/data/docker/liujing04/gpt-vits/prepare/dump_mix/\"\n    dataset = Text2SemanticDataset(\n        phoneme_path=root_dir + \"phoneme_train.npy\",\n        semantic_path=root_dir + \"semantic_train.tsv\",\n    )\n\n    batch_size = 12\n    dataloader = DataLoader(\n        dataset, batch_size=batch_size, collate_fn=dataset.collate, shuffle=False\n    )\n    for i, batch in enumerate(dataloader):\n        if i % 1000 == 0:\n            print(i)\n        # if i == 0:\n        #     print('batch[\"ids\"]:', batch[\"ids\"])\n        # print('batch[\"phoneme_ids\"]:', batch[\"phoneme_ids\"],\n        #       batch[\"phoneme_ids\"].shape)\n        # print('batch[\"phoneme_ids_len\"]:', batch[\"phoneme_ids_len\"],\n        #       batch[\"phoneme_ids_len\"].shape)\n        # print('batch[\"semantic_ids\"]:', batch[\"semantic_ids\"],\n        #       batch[\"semantic_ids\"].shape)\n        # print('batch[\"semantic_ids_len\"]:', batch[\"semantic_ids_len\"],\n        #       batch[\"semantic_ids_len\"].shape)\n"}
{"type": "source_file", "path": "src/gallama/backend/tts/model/gpt_sovits_source/GPT_SoVITS/AR/models/__init__.py", "content": ""}
{"type": "source_file", "path": "src/gallama/backend/tts/model/gpt_sovits_source/GPT_SoVITS/AR/__init__.py", "content": ""}
{"type": "source_file", "path": "src/gallama/backend/tts/model/__init__.py", "content": "from .kokoro import TTSKokoro\n\n# currently GPT Sovit is not hard dependency\ntry:\n    from .gpt_sovits import TTS_GPT_SoVITS\nexcept ModuleNotFoundError:\n    pass\n"}
{"type": "source_file", "path": "src/gallama/backend/tts/model/gpt_sovits_source/GPT_SoVITS/AR/data/bucket_sampler.py", "content": "# modified from https://github.com/yangdongchao/SoundStorm/blob/master/soundstorm/s1/AR/data/bucket_sampler.py\n# reference: https://github.com/lifeiteng/vall-e\nimport itertools\nimport math\nimport random\nfrom random import shuffle\nfrom typing import Iterator\nfrom typing import Optional\nfrom typing import TypeVar\n\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import Sampler\n\n__all__ = [\n    \"DistributedBucketSampler\",\n]\n\nT_co = TypeVar(\"T_co\", covariant=True)\n\n\nclass DistributedBucketSampler(Sampler[T_co]):\n    r\"\"\"\n    sort the dataset wrt. input length\n    divide samples into buckets\n    sort within buckets\n    divide buckets into batches\n    sort batches\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: Dataset,\n        num_replicas: Optional[int] = None,\n        rank: Optional[int] = None,\n        shuffle: bool = True,\n        seed: int = 0,\n        drop_last: bool = False,\n        batch_size: int = 32,\n    ) -> None:\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size() if torch.cuda.is_available() else 1\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank() if torch.cuda.is_available() else 0\n            if torch.cuda.is_available():\n                torch.cuda.set_device(rank)\n        if rank >= num_replicas or rank < 0:\n            raise ValueError(\n                \"Invalid rank {}, rank should be in the interval\"\n                \" [0, {}]\".format(rank, num_replicas - 1)\n            )\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.drop_last = drop_last\n        # If the dataset length is evenly divisible by # of replicas, then there\n        # is no need to drop any data, since the dataset will be split equally.\n        if (\n            self.drop_last and len(self.dataset) % self.num_replicas != 0\n        ):  # type: ignore[arg-type]\n            # Split to nearest available length that is evenly divisible.\n            # This is to ensure each rank receives the same amount of data when\n            # using this Sampler.\n            self.num_samples = math.ceil(\n                (len(self.dataset) - self.num_replicas)\n                / self.num_replicas  # type: ignore[arg-type]\n            )\n        else:\n            self.num_samples = math.ceil(\n                len(self.dataset) / self.num_replicas\n            )  # type: ignore[arg-type]\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n        self.seed = seed\n        self.batch_size = batch_size\n        self.id_with_length = self._get_sample_lengths()\n        self.id_buckets = self.make_buckets(bucket_width=2.0)\n\n    def _get_sample_lengths(self):\n        id_with_lengths = []\n        for i in range(len(self.dataset)):\n            id_with_lengths.append((i, self.dataset.get_sample_length(i)))\n        id_with_lengths.sort(key=lambda x: x[1])\n        return id_with_lengths\n\n    def make_buckets(self, bucket_width: float = 2.0):\n        buckets = []\n        cur = []\n        max_sec = bucket_width\n        for id, sec in self.id_with_length:\n            if sec < max_sec:\n                cur.append(id)\n            else:\n                buckets.append(cur)\n                cur = [id]\n                max_sec += bucket_width\n        if len(cur) > 0:\n            buckets.append(cur)\n        return buckets\n\n    def __iter__(self) -> Iterator[T_co]:\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            random.seed(self.epoch + self.seed)\n            shuffled_bucket = []\n            for buc in self.id_buckets:\n                buc_copy = buc.copy()\n                shuffle(buc_copy)\n                shuffled_bucket.append(buc_copy)\n            grouped_batch_size = self.batch_size * self.num_replicas\n            shuffled_bucket = list(itertools.chain(*shuffled_bucket))\n            n_batch = int(math.ceil(len(shuffled_bucket) / grouped_batch_size))\n            batches = [\n                shuffled_bucket[b * grouped_batch_size : (b + 1) * grouped_batch_size]\n                for b in range(n_batch)\n            ]\n            shuffle(batches)\n            indices = list(itertools.chain(*batches))\n        else:\n            # type: ignore[arg-type]\n            indices = list(range(len(self.dataset)))\n\n        if not self.drop_last:\n            # add extra samples to make it evenly divisible\n            padding_size = self.total_size - len(indices)\n            if padding_size <= len(indices):\n                indices += indices[:padding_size]\n            else:\n                indices += (indices * math.ceil(padding_size / len(indices)))[\n                    :padding_size\n                ]\n        else:\n            # remove tail of data to make it evenly divisible.\n            indices = indices[: self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self) -> int:\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -> None:\n        r\"\"\"\n        Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas\n        use a different random ordering for each epoch. Otherwise, the next iteration of this\n        sampler will yield the same ordering.\n\n        Args:\n            epoch (int): Epoch number.\n        \"\"\"\n        self.epoch = epoch\n"}
{"type": "source_file", "path": "src/gallama/backend/tts/model/gpt_sovits.py", "content": "from ..base import TTSBase\nfrom .gpt_sovits_source import TTS, TTS_Config\nfrom typing import Dict, Any, Tuple, AsyncIterator\nimport numpy as np\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nfrom gallama.data_classes import ModelSpec\nfrom gallama.logger.logger import logger\nimport time\n\n\nclass TSS_ConfigModified(TTS_Config):\n    def __init__(self, backend_extra_args):\n        \"\"\" overwrite the original logic to change the path to point to gallama folder\"\"\"\n\n        self.config = backend_extra_args\n\n        # get argument set in config file:\n        backend_arg_list = [\"bert_base_path\", \"cnhuhbert_base_path\", \"device\", \"is_half\", \"t2s_weights_path\", \"version\", \"vits_weights_path\"]\n\n        for arg_name in backend_arg_list:\n            value_to_set = backend_extra_args.get(arg_name)\n\n            if value_to_set is None:\n                raise ValueError(f\"For GPT_SoVits argument {arg_name} is required and must be set in model_config.yaml\")\n\n            # set this as attribute of the class\n            setattr(self, arg_name, value_to_set)\n\n        self.max_sec = None\n        self.hz:int = 50\n        self.semantic_frame_rate:str = \"25hz\"\n        self.segment_size:int = 20480\n        self.filter_length:int = 2048\n        self.sampling_rate:int = 32000\n        self.hop_length:int = 640\n        self.win_length:int = 2048\n        self.n_speakers:int = 300\n\n    def save_configs(self, configs_path:str=None)->None:\n        pass\n\n\nclass AsyncTTSWrapper:\n    def __init__(self, tts_instance, executor: ThreadPoolExecutor = None):\n        self.tts = tts_instance\n        self.executor = executor or ThreadPoolExecutor(max_workers=1)\n\n    async def stream_audio(\n        self,\n        inputs: Dict[str, Any],\n        queue: asyncio.Queue,\n        session_tracker: set = None,  # Set to track generations for a specific session\n    ) -> None:\n        \"\"\"\n        Feeds audio fragments directly into the provided queue for upstream consumption.\n\n        Args:\n            inputs: Dictionary of TTS parameters including text, ref_audio_path, etc.\n            queue: asyncio.Queue where audio chunks will be placed\n            session_tracker: Optional set to track generations for this specific session\n        \"\"\"\n\n        task_id = str(time.time())\n        if session_tracker is not None:\n            session_tracker.add(task_id)\n\n        loop = asyncio.get_running_loop()\n\n        def run_tts():\n            try:\n                for audio_chunk in self.tts.run(inputs):\n                    future = asyncio.run_coroutine_threadsafe(\n                        queue.put(audio_chunk),\n                        loop\n                    )\n                    future.result()\n\n            except Exception as e:\n                logger.error(f\"Error in TTS generation {task_id}: {e}\")\n                future = asyncio.run_coroutine_threadsafe(\n                    queue.put(Exception(f\"TTS error: {str(e)}\")),\n                    loop\n                )\n                future.result()\n            finally:\n                # Remove this task from session tracker if it exists\n                if session_tracker is not None:\n                    session_tracker.discard(task_id)\n\n        loop.run_in_executor(self.executor, run_tts)\n\n\n    def stop(self):\n        \"\"\"Stops the TTS processing\"\"\"\n        self.tts.stop()\n\n\nclass TTS_GPT_SoVITS(TTSBase):\n    def __init__(self, model_spec: ModelSpec):\n        super().__init__(model_spec)\n\n\n    def load_model(self, model_spec: ModelSpec):\n        # create the config object with parameters from gallama yaml config file\n        config = TSS_ConfigModified(self.backend_extra_args)\n        self.model = AsyncTTSWrapper(TTS(config))\n\n        # set other required parameter for speech generation\n        # self.ref_audio_path = self.backend_extra_args.get('ref_audio_path')\n        # self.ref_audio_transcription = self.backend_extra_args.get('ref_audio_transcription')\n        self.chunk_size_in_s = self.backend_extra_args.get('chunk_size_in_s', 0.5)\n\n\n    async def text_to_speech(\n        self,\n        text: str,\n        language: str = \"auto\",\n        stream: bool = False,\n        batching: bool = False,\n        batch_size: int = 1,\n        queue: asyncio.Queue = None,\n        voice: str = None,\n        speed_factor: float = None,\n        session_tracker: set = None,\n        **kwargs: Any\n    ) -> None | Tuple[int, np.ndarray] | AsyncIterator[Tuple[int, np.ndarray]] | str:\n        \"\"\"\n        Generate audio chunks from text and put them into an asyncio Queue.\n\n        Args:\n            text: Text to convert to speech\n            language: Language of the text (default: \"auto\")\n            stream: Whether to stream the audio in chunks (default: False)\n            voice: the name of the voice to use (default: None), if None, will use default voice\n            speed_factor: Speed factor for the audio playback (default: 1.0)\n            batching: whether to using parallel batching, will be faster but require more memory (default: False)\n            batch_size: batch size if use batching (default: 1)\n            queue: Optional asyncio Queue to receive audio chunks\n            session_tracker: Optional set to track generations for this specific session\n            kwargs: Additional parameters to pass to the text_to_speech function\n\n        Returns:\n            If stream=False: Tuple of (sample_rate, concatenated_audio_data)\n            If stream=True: None (audio chunks are sent to the provided queue)\n        \"\"\"\n        logger.debug(f\"Converting TTS: {text}\")\n\n        if stream and not queue:\n            # for streaming, the result will be written directly to the queue hence it is required parameter\n            raise Exception(\"For streaming mode, the queue must be provided\")\n\n        if voice:\n            if voice in self.voice_list:\n                voice_to_use = self.voice[voice]\n            else:\n                voice_to_use = self.default_voice\n        else:\n            voice_to_use = self.default_voice\n\n        if speed_factor:\n            speed_factor_to_use = speed_factor\n        elif voice_to_use:\n            speed_factor_to_use = voice_to_use.speed_factor or 1.0\n        else:\n            raise Exception(\"There must be at least one voice to use\")\n\n        params = {\n            \"text\": text,\n            \"text_lang\": language,\n            \"ref_audio_path\": voice_to_use.ref_audio_path,\n            \"aux_ref_audio_paths\": [],\n            \"prompt_text\": voice_to_use.ref_audio_transcription,\n            \"prompt_lang\": \"en\",\n            \"top_k\": 5,\n            \"top_p\": 1,\n            \"temperature\": 1,\n            \"text_split_method\": \"cut0\",\n            \"batch_size\": batch_size,\n            \"batch_threshold\": 0.75,\n            # \"split_bucket\": not stream,  # Disable split_bucket when streaming\n            \"split_bucket\": True,  # Disable split_bucket when streaming\n            \"return_fragment\": stream,  # Enable fragments when streaming\n            # \"return_fragment\": False,  # Enable fragments when streaming\n            \"speed_factor\": speed_factor_to_use,  # Use the provided speed_factor\n            \"fragment_interval\": self.chunk_size_in_s,\n            \"seed\": -1,\n            # \"parallel_infer\": batching,\n            \"parallel_infer\": 3,\n            \"repetition_penalty\": 1.35,\n            **kwargs  # Allow overriding of any parameters\n        }\n\n        if stream:\n            if queue is None:\n                queue = asyncio.Queue()\n            await self.model.stream_audio(\n                inputs=params,\n                queue=queue,\n                session_tracker=session_tracker\n            )\n            return None  # Since we're using a queue, no direct return value needed\n        else:\n            # For non-streaming mode, create a temporary queue and collect all chunks\n            temp_queue = asyncio.Queue()\n            temp_session_tracker = set()  # Create temporary session tracker if none provided\n            session_tracker_to_use = session_tracker if session_tracker is not None else temp_session_tracker\n\n            await self.model.stream_audio(\n                inputs=params,\n                queue=temp_queue,\n                session_tracker=session_tracker_to_use\n            )\n\n            try:\n                # Collect all audio chunks into a single array\n                audio_chunks = []\n                sample_rate = None\n\n                # Process chunks while the session is active or queue has items\n                while session_tracker_to_use or not temp_queue.empty():\n                    try:\n                        # Use asyncio.wait_for with a timeout for the queue.get()\n                        chunk = await asyncio.wait_for(temp_queue.get(), timeout=0.1)\n                        if isinstance(chunk, Exception):\n                            raise chunk\n\n                        current_sample_rate, audio_data = chunk\n                        if sample_rate is None:\n                            sample_rate = current_sample_rate\n                        elif sample_rate != current_sample_rate:\n                            raise ValueError(\n                                f\"Inconsistent sample rates detected: {sample_rate} vs {current_sample_rate}\")\n\n                        if audio_data.shape[0] > 0:  # Only add non-empty chunks\n                            audio_chunks.append(audio_data)\n                        temp_queue.task_done()\n                    except asyncio.TimeoutError:\n                        # Timeout is expected when queue is empty, just continue the loop\n                        continue\n\n                if not audio_chunks:\n                    return 0, np.array([], dtype=np.int16)\n\n                return sample_rate, np.concatenate(audio_chunks)\n\n            except Exception as e:\n                logger.error(f\"Error during audio generation: {e}\")\n                self.model.stop()\n                raise"}
{"type": "source_file", "path": "src/gallama/backend/tts/model/gpt_sovits_source/GPT_SoVITS/AR/data/data_module.py", "content": "# modified from https://github.com/yangdongchao/SoundStorm/blob/master/soundstorm/s1/AR/data/data_module.py\n# reference: https://github.com/lifeiteng/vall-e\nfrom pytorch_lightning import LightningDataModule\nfrom AR.data.bucket_sampler import DistributedBucketSampler\nfrom AR.data.dataset import Text2SemanticDataset\nfrom torch.utils.data import DataLoader\n\n\nclass Text2SemanticDataModule(LightningDataModule):\n    def __init__(\n        self,\n        config,\n        train_semantic_path,\n        train_phoneme_path,\n        dev_semantic_path=None,\n        dev_phoneme_path=None,\n    ):\n        super().__init__()\n        self.config = config\n        self.train_semantic_path = train_semantic_path\n        self.train_phoneme_path = train_phoneme_path\n        self.dev_semantic_path = dev_semantic_path\n        self.dev_phoneme_path = dev_phoneme_path\n        self.num_workers = self.config[\"data\"][\"num_workers\"]\n\n    def prepare_data(self):\n        pass\n\n    def setup(self, stage=None, output_logs=False):\n        self._train_dataset = Text2SemanticDataset(\n            phoneme_path=self.train_phoneme_path,\n            semantic_path=self.train_semantic_path,\n            max_sec=self.config[\"data\"][\"max_sec\"],\n            pad_val=self.config[\"data\"][\"pad_val\"],\n        )\n        self._dev_dataset = self._train_dataset\n        # self._dev_dataset = Text2SemanticDataset(\n        #     phoneme_path=self.dev_phoneme_path,\n        #     semantic_path=self.dev_semantic_path,\n        #     max_sample=self.config['data']['max_eval_sample'],\n        #     max_sec=self.config['data']['max_sec'],\n        #     pad_val=self.config['data']['pad_val'])\n\n    def train_dataloader(self):\n        batch_size=self.config[\"train\"][\"batch_size\"]//2 if self.config[\"train\"].get(\"if_dpo\",False)==True else self.config[\"train\"][\"batch_size\"]\n        batch_size = max(min(batch_size,len(self._train_dataset)//4),1)#防止不保存\n        sampler = DistributedBucketSampler(self._train_dataset, batch_size=batch_size)\n        return DataLoader(\n            self._train_dataset,\n            batch_size=batch_size,\n            sampler=sampler,\n            collate_fn=self._train_dataset.collate,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            prefetch_factor=16,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self._dev_dataset,\n            batch_size=1,\n            shuffle=False,\n            collate_fn=self._train_dataset.collate,\n            num_workers=max(self.num_workers, 12),\n            persistent_workers=True,\n            prefetch_factor=16,\n        )\n\n    # 这个会使用到嘛？\n    def test_dataloader(self):\n        return DataLoader(\n            self._dev_dataset,\n            batch_size=1,\n            shuffle=False,\n            collate_fn=self._train_dataset.collate,\n        )\n"}
{"type": "source_file", "path": "src/gallama/backend/tts/model/gpt_sovits_source/GPT_SoVITS/AR/models/t2s_lightning_module.py", "content": "# modified from https://github.com/yangdongchao/SoundStorm/blob/master/soundstorm/s1/AR/models/t2s_lightning_module.py\n# reference: https://github.com/lifeiteng/vall-e\nimport os, sys\n\nnow_dir = os.getcwd()\nsys.path.append(now_dir)\nfrom typing import Dict\n\nimport torch\nfrom pytorch_lightning import LightningModule\nfrom AR.models.t2s_model import Text2SemanticDecoder\nfrom AR.modules.lr_schedulers import WarmupCosineLRSchedule\nfrom AR.modules.optim import ScaledAdam\n\nclass Text2SemanticLightningModule(LightningModule):\n    def __init__(self, config, output_dir, is_train=True):\n        super().__init__()\n        self.config = config\n        self.top_k = 3\n        self.model = Text2SemanticDecoder(config=config, top_k=self.top_k)\n        pretrained_s1 = config.get(\"pretrained_s1\")\n        if pretrained_s1 and is_train:\n            # print(self.load_state_dict(torch.load(pretrained_s1,map_location=\"cpu\")[\"state_dict\"]))\n            print(\n                self.load_state_dict(\n                    torch.load(pretrained_s1, map_location=\"cpu\")[\"weight\"]\n                )\n            )\n        if is_train:\n            self.automatic_optimization = False\n            self.save_hyperparameters()\n            self.eval_dir = output_dir / \"eval\"\n            self.eval_dir.mkdir(parents=True, exist_ok=True)\n\n    def training_step(self, batch: Dict, batch_idx: int):\n        opt = self.optimizers()\n        scheduler = self.lr_schedulers()\n        forward=self.model.forward if self.config[\"train\"].get(\"if_dpo\",False)==True else self.model.forward_old\n        loss, acc = forward(\n            batch[\"phoneme_ids\"],\n            batch[\"phoneme_ids_len\"],\n            batch[\"semantic_ids\"],\n            batch[\"semantic_ids_len\"],\n            batch[\"bert_feature\"],\n        )\n        self.manual_backward(loss)\n        if batch_idx > 0 and batch_idx % 4 == 0:\n            opt.step()\n            opt.zero_grad()\n            scheduler.step()\n\n        self.log(\n            \"total_loss\",\n            loss,\n            on_step=True,\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"lr\",\n            scheduler.get_last_lr()[0],\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n        self.log(\n            f\"top_{self.top_k}_acc\",\n            acc,\n            on_step=True,\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n\n    def validation_step(self, batch: Dict, batch_idx: int):\n        return\n\n    # # get loss\n    # loss, acc = self.model.forward(\n    #     batch['phoneme_ids'], batch['phoneme_ids_len'],\n    #     batch['semantic_ids'], batch['semantic_ids_len'],\n    #     batch['bert_feature']\n    # )\n    #\n    # self.log(\n    #     \"val_total_loss\",\n    #     loss,\n    #     on_step=True,\n    #     on_epoch=True,\n    #     prog_bar=True,\n    #     sync_dist=True)\n    # self.log(\n    #     f\"val_top_{self.top_k}_acc\",\n    #     acc,\n    #     on_step=True,\n    #     on_epoch=True,\n    #     prog_bar=True,\n    #     sync_dist=True)\n    #\n    # # get infer output\n    # semantic_len = batch['semantic_ids'].size(1)\n    # prompt_len = min(int(semantic_len * 0.5), 150)\n    # prompt = batch['semantic_ids'][:, :prompt_len]\n    # pred_semantic = self.model.infer(batch['phoneme_ids'],\n    #                                  batch['phoneme_ids_len'], prompt,\n    #                                  batch['bert_feature']\n    #                                  )\n    # save_name = f'semantic_toks_{batch_idx}.pt'\n    # save_path = os.path.join(self.eval_dir, save_name)\n    # torch.save(pred_semantic.detach().cpu(), save_path)\n\n    def configure_optimizers(self):\n        model_parameters = self.model.parameters()\n        parameters_names = []\n        parameters_names.append(\n            [name_param_pair[0] for name_param_pair in self.model.named_parameters()]\n        )\n        lm_opt = ScaledAdam(\n            model_parameters,\n            lr=0.01,\n            betas=(0.9, 0.95),\n            clipping_scale=2.0,\n            parameters_names=parameters_names,\n            show_dominant_parameters=False,\n            clipping_update_period=1000,\n        )\n\n        return {\n            \"optimizer\": lm_opt,\n            \"lr_scheduler\": {\n                \"scheduler\": WarmupCosineLRSchedule(\n                    lm_opt,\n                    init_lr=self.config[\"optimizer\"][\"lr_init\"],\n                    peak_lr=self.config[\"optimizer\"][\"lr\"],\n                    end_lr=self.config[\"optimizer\"][\"lr_end\"],\n                    warmup_steps=self.config[\"optimizer\"][\"warmup_steps\"],\n                    total_steps=self.config[\"optimizer\"][\"decay_steps\"],\n                )\n            },\n        }\n"}
{"type": "source_file", "path": "src/gallama/backend/tts/model/gpt_sovits_source/GPT_SoVITS/AR/data/__init__.py", "content": ""}
{"type": "source_file", "path": "src/gallama/backend/stt/audio_buffer.py", "content": "from dataclasses import dataclass\nimport numpy as np\nfrom typing import Optional, Tuple\nfrom gallama.logger import logger\n\n@dataclass\nclass VADEvent:\n    event_type: str  # \"speech_start\" or \"speech_end\"\n    timestamp_ms: float\n    confidence: float\n\n\nclass AudioBufferWithTiming:\n    \"\"\"Manages audio data with precise timing information.\"\"\"\n\n    def __init__(self, sample_rate: int, max_length_minutes: float = 30.0, trim_duration_minutes: float = 5.0):\n        self.buffer = np.array([], dtype=np.float32)\n        self.sample_rate = sample_rate\n        self.total_samples = 0\n        self.start_offset = 0  # Track offset after trimming\n        self.last_processed_sample = 0  # Track the last processed sample position for ASR\n        self.last_processed_sample_vad = 0  # Track the last processed sample position for VAD\n        self.is_processing = False  # Flag to track if processing is ongoing\n        self.max_length_minutes = max_length_minutes\n        self.trim_duration_minutes = trim_duration_minutes\n        self.max_length_samples = int(self.max_length_minutes * 60 * self.sample_rate)\n        self.trim_length_samples = int(self.trim_duration_minutes * 60 * self.sample_rate)\n\n    def __len__(self) -> int:\n        \"\"\"Return the length of the underlying audio buffer.\"\"\"\n        return len(self.buffer)\n\n    def add_chunk(self, chunk: np.ndarray) -> None:\n        \"\"\"Add a new audio chunk to the buffer.\"\"\"\n        self.buffer = np.append(self.buffer, chunk)\n        self.total_samples += len(chunk)\n\n        # Check if the buffer exceeds the maximum length\n        if len(self.buffer) > self.max_length_samples:\n            # Instead of just removing excess, remove trim_duration worth of samples\n            self.buffer = self.buffer[self.trim_length_samples:]\n            self.start_offset += self.trim_length_samples\n            self.last_processed_sample = max(0, self.last_processed_sample - self.trim_length_samples)\n            self.last_processed_sample_vad = max(0, self.last_processed_sample_vad - self.trim_length_samples)\n\n    def get_time_ms(self, sample_index: int) -> float:\n        \"\"\"Convert sample index to milliseconds from stream start.\"\"\"\n        absolute_sample = sample_index + self.start_offset\n        return (absolute_sample / self.sample_rate) * 1000\n\n    def get_current_duration_ms(self) -> float:\n        \"\"\"Get current buffer duration in milliseconds.\"\"\"\n        return (len(self.buffer) / self.sample_rate) * 1000\n\n    def clear_until(self, sample_index: int) -> None:\n        \"\"\"Clear buffer up to given sample index, updating timing information.\"\"\"\n        if sample_index > 0:\n            self.buffer = self.buffer[sample_index:]\n            self.start_offset += sample_index\n            # Update both processing trackers relative to the new buffer position\n            self.last_processed_sample = max(0, self.last_processed_sample - sample_index)\n            self.last_processed_sample_vad = max(0, self.last_processed_sample_vad - sample_index)\n\n    def get_samples_for_duration(self, duration_ms: float) -> Optional[np.ndarray]:\n        \"\"\"Get samples corresponding to specified duration from the start.\"\"\"\n        num_samples = int((duration_ms / 1000) * self.sample_rate)\n        if num_samples <= len(self.buffer):\n            return self.buffer[:num_samples]\n        return None\n\n    def get_latest_samples(self, duration_ms: float) -> Optional[np.ndarray]:\n        \"\"\"Get the most recent samples of specified duration.\"\"\"\n        num_samples = int((duration_ms / 1000) * self.sample_rate)\n        if num_samples <= len(self.buffer):\n            return self.buffer[-num_samples:]\n        return None\n\n    def mark_processing_start(self) -> None:\n        \"\"\"Mark the start of audio processing.\"\"\"\n        self.is_processing = True\n\n    def mark_processing_complete(self, is_final: bool = False) -> None:\n        \"\"\"\n        Mark the completion of audio processing.\n        Updates the last_processed_sample pointer to current buffer position.\n\n        Args:\n            is_final (bool): Whether this was the final processing of the audio stream\n        \"\"\"\n        if is_final:\n            self.last_processed_sample = len(self.buffer)\n            self.last_processed_sample_vad = len(self.buffer)\n        else:\n            # Move the pointer to end of current buffer\n            # self.last_processed_sample = len(self.buffer)\n            pass\n\n        self.is_processing = False\n\n    def get_unprocessed_audio(self) -> np.ndarray:\n        \"\"\"\n        Get the portion of audio that hasn't been processed yet by ASR.\n        Returns the audio from last_processed_sample to the end of the buffer.\n        \"\"\"\n        if self.last_processed_sample >= len(self.buffer):\n            return np.array([], dtype=np.float32)\n\n        return self.buffer[self.last_processed_sample:]\n\n    def get_unprocessed_audio_vad(self) -> np.ndarray:\n        logger.info(\n            f\"VAD processing state - last_processed: {self.last_processed_sample_vad}, buffer_len: {len(self.buffer)}, start_offset: {self.start_offset}\")\n        if self.last_processed_sample_vad >= len(self.buffer):\n            return np.array([], dtype=np.float32)\n        return self.buffer[self.last_processed_sample_vad:]\n\n    def mark_vad_processing_complete(self, is_final: bool = False) -> None:\n        \"\"\"\n        Mark the completion of VAD processing.\n        Updates the last_processed_sample_vad pointer to current buffer position.\n\n        Args:\n            is_final (bool): Whether this was the final processing of the audio stream\n        \"\"\"\n        if is_final:\n            self.last_processed_sample_vad = len(self.buffer)\n        else:\n            self.last_processed_sample_vad = len(self.buffer)\n\n    def reset(self) -> None:\n        \"\"\"Reset the buffer state.\"\"\"\n        self.buffer = np.array([], dtype=np.float32)\n        self.total_samples = 0\n        self.start_offset = 0\n        self.last_processed_sample = 0\n        self.last_processed_sample_vad = 0\n        self.is_processing = False"}
{"type": "source_file", "path": "src/gallama/backend/tts/model/gpt_sovits_source/GPT_SoVITS/AR/models/t2s_lightning_module_onnx.py", "content": "# modified from https://github.com/yangdongchao/SoundStorm/blob/master/soundstorm/s1/AR/models/t2s_lightning_module.py\n# reference: https://github.com/lifeiteng/vall-e\nimport os, sys\n\nnow_dir = os.getcwd()\nsys.path.append(now_dir)\nfrom typing import Dict\n\nimport torch\nfrom pytorch_lightning import LightningModule\nfrom AR.models.t2s_model_onnx import Text2SemanticDecoder\nfrom AR.modules.lr_schedulers import WarmupCosineLRSchedule\nfrom AR.modules.optim import ScaledAdam\n\n\nclass Text2SemanticLightningModule(LightningModule):\n    def __init__(self, config, output_dir, is_train=True):\n        super().__init__()\n        self.config = config\n        self.top_k = 3\n        self.model = Text2SemanticDecoder(config=config, top_k=self.top_k)\n        pretrained_s1 = config.get(\"pretrained_s1\")\n        if pretrained_s1 and is_train:\n            # print(self.load_state_dict(torch.load(pretrained_s1,map_location=\"cpu\")[\"state_dict\"]))\n            print(\n                self.load_state_dict(\n                    torch.load(pretrained_s1, map_location=\"cpu\")[\"weight\"]\n                )\n            )\n        if is_train:\n            self.automatic_optimization = False\n            self.save_hyperparameters()\n            self.eval_dir = output_dir / \"eval\"\n            self.eval_dir.mkdir(parents=True, exist_ok=True)\n\n    def training_step(self, batch: Dict, batch_idx: int):\n        opt = self.optimizers()\n        scheduler = self.lr_schedulers()\n        loss, acc = self.model.forward(\n            batch[\"phoneme_ids\"],\n            batch[\"phoneme_ids_len\"],\n            batch[\"semantic_ids\"],\n            batch[\"semantic_ids_len\"],\n            batch[\"bert_feature\"],\n        )\n        self.manual_backward(loss)\n        if batch_idx > 0 and batch_idx % 4 == 0:\n            opt.step()\n            opt.zero_grad()\n            scheduler.step()\n\n        self.log(\n            \"total_loss\",\n            loss,\n            on_step=True,\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"lr\",\n            scheduler.get_last_lr()[0],\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n        self.log(\n            f\"top_{self.top_k}_acc\",\n            acc,\n            on_step=True,\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n\n    def validation_step(self, batch: Dict, batch_idx: int):\n        return\n\n    def configure_optimizers(self):\n        model_parameters = self.model.parameters()\n        parameters_names = []\n        parameters_names.append(\n            [name_param_pair[0] for name_param_pair in self.model.named_parameters()]\n        )\n        lm_opt = ScaledAdam(\n            model_parameters,\n            lr=0.01,\n            betas=(0.9, 0.95),\n            clipping_scale=2.0,\n            parameters_names=parameters_names,\n            show_dominant_parameters=False,\n            clipping_update_period=1000,\n        )\n\n        return {\n            \"optimizer\": lm_opt,\n            \"lr_scheduler\": {\n                \"scheduler\": WarmupCosineLRSchedule(\n                    lm_opt,\n                    init_lr=self.config[\"optimizer\"][\"lr_init\"],\n                    peak_lr=self.config[\"optimizer\"][\"lr\"],\n                    end_lr=self.config[\"optimizer\"][\"lr_end\"],\n                    warmup_steps=self.config[\"optimizer\"][\"warmup_steps\"],\n                    total_steps=self.config[\"optimizer\"][\"decay_steps\"],\n                )\n            },\n        }\n"}
