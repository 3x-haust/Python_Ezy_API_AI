{"repo_info": {"repo_name": "vanilla-aiagents", "repo_owner": "Azure-Samples", "repo_url": "https://github.com/Azure-Samples/vanilla-aiagents"}}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_askable.py", "content": "import unittest\nimport os, logging, sys\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom vanilla_aiagents.conversation import Conversation\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.user import User\nfrom vanilla_aiagents.team import Team\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nfrom dotenv import load_dotenv\nload_dotenv(override=True)\n\nclass TestAskable(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM({\n            \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n            \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n            \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n        })\n        self.askable = Agent(id=\"foo\", llm=self.llm, description=\"Call this agent to answer questions by the user\", system_message = \"\"\"You are an AI assistant\"\"\")\n\n    def test_basics(self):\n        \n        self.askable.id = \"bar\"\n        self.assertEqual(self.askable.id, \"bar\", \"Expected id to be 'bar'\")\n        \n        self.askable.description = \"Call this agent to help the user\"\n        self.assertEqual(self.askable.description, \"Call this agent to help the user\", \"Expected description to be 'Call this agent to help the user'\")\n\n    # def test_ask(self):\n    #     conversation = Conversation()\n    #     conversation.messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n    #     result = self.askable.ask(conversation)\n\nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "vanilla_aiagents/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_agent_func_call.py", "content": "from typing import Annotated\nimport unittest\nimport os, logging, sys\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.user import User\nfrom vanilla_aiagents.team import Team\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n\nclass TestAgent(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM(\n            {\n                \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n                \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n                \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n                \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n            }\n        )\n\n        logging.basicConfig(level=logging.INFO)\n        logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.llm\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.team\").setLevel(logging.DEBUG)\n\n    def test_func_call(self):\n        agent1 = Agent(\n            id=\"agent\",\n            llm=self.llm,\n            description=\"Call this agent to answer questions by the user. This agent can answer about accounts\",\n            system_message=\"\"\"You are an AI assistant\n        Your task is to help the user with their questions.\n        Always respond with the best answer you can generate.\n        If you don't know the answer, respond with \"I don't know\".\n        Always be polite and helpful.\n        \"\"\",\n        )\n\n        @agent1.register_tool(description=\"get user balance\")\n        def get_user_balance() -> Annotated[str, \"The user balance in USD\"]:\n            return \"100\"\n\n        user = User(id=\"user\", mode=\"unattended\")\n\n        flow = Team(\n            id=\"team\",\n            description=\"\",\n            members=[agent1, user],\n            llm=self.llm,\n            stop_callback=lambda conv: len(conv.messages) > 6,\n        )\n        workflow = Workflow(askable=flow)\n\n        workflow.run(\"Hi! Which is my account balance?\")\n\n        self.assertGreaterEqual(\n            len(workflow.conversation.messages),\n            3,\n            \"Expected at least 3 messages in the conversation\",\n        )\n        self.assertEqual(\n            workflow.conversation.messages[-1][\"name\"],\n            \"agent\",\n            \"Expected agent to respond with greeting\",\n        )\n        self.assertIn(\n            \"100\",\n            workflow.conversation.messages[-1][\"content\"],\n            \"Expected agent to respond with balance 100$\",\n        )\n\n    def test_multiple_func_call(self):\n        agent1 = Agent(\n            id=\"agent\",\n            llm=self.llm,\n            description=\"Call this agent to play guess the number game with the use\",\n            system_message=\"\"\"You are an AI assistant\n        Your task is to play a game with the user.\n        You first generate a random number between 1 and 100. Then save it as a conversation variable named \"number\".\n        The user will try to guess the number.\n        If the user's guess is too high, respond with \"Too high\".\n        If the user's guess is too low, respond with \"Too low\".\n        \"\"\",\n        )\n\n        @agent1.register_tool(description=\"Generate a random number\")\n        def random() -> Annotated[str, \"A random number\"]:\n            # Better to use a fixed number for testing\n            return \"42\"\n\n        user = User(id=\"user\", mode=\"unattended\")\n\n        flow = Team(\n            id=\"team\",\n            description=\"\",\n            members=[agent1, user],\n            llm=self.llm,\n            stop_callback=lambda conv: len(conv.messages) > 2,\n        )\n        workflow = Workflow(askable=flow)\n\n        workflow.run(\"Hi! Let's play a game. Guess the number.\")\n\n        self.assertIn(\n            \"number\",\n            workflow.conversation.variables,\n            \"Expected agent to have generated and saved a random number\",\n        )\n        self.assertEqual(\n            workflow.conversation.variables[\"number\"],\n            \"42\",\n            \"Expected agent to have generated and saved a random number\",\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_coding.py", "content": "import unittest\nimport os, logging, sys\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom vanilla_aiagents.azure_coding_agent import AzureCodingAgent\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.team import Team\nfrom vanilla_aiagents.llm import AzureOpenAILLM\nfrom vanilla_aiagents.coding_agent import LocalCodingAgent\n\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n\nclass TestAgent(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM(\n            {\n                \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n                \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n                \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n                \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n            }\n        )\n\n        logging.basicConfig(level=logging.INFO)\n        logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.coding_agent\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.llm\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.team\").setLevel(logging.DEBUG)\n\n        localCodingAgent = LocalCodingAgent(\n            id=\"agent1\", description=\"Agent 1\", llm=self.llm\n        )\n        azureCodingAgent = AzureCodingAgent(\n            id=\"agent2\", description=\"Agent 2\", llm=self.llm\n        )\n\n        flow = Team(\n            id=\"team\",\n            description=\"An agent capable of writing and executing code\",\n            members=[localCodingAgent],\n            llm=self.llm,\n            stop_callback=lambda conv: len(conv.messages) > 2,\n        )\n        self.local_workflow = Workflow(askable=flow)\n        self.azure_workflow = Workflow(askable=azureCodingAgent)\n\n    # Not working yet on GH actions\n    # def test_basic_math(self):\n    #     self.local_workflow.restart()\n    #     self.local_workflow.run(\"Which is the square root of 256?\")\n\n    #     self.assertIn(\"16\", self.local_workflow.conversation.messages[-1][\"content\"], \"Expected agent to respond 16\")\n\n    def test_yfinance(self):\n        self.local_workflow.restart()\n        self.local_workflow.run(\n            \"What is latest quote for Apple Inc. stock? Generate code to get the quote using yfinance and return only the latest quote (include $)\"\n        )\n\n        self.assertIn(\n            \"$\",\n            self.local_workflow.conversation.messages[-1][\"content\"],\n            \"Expected agent to respond with a quote\",\n        )\n\n    def test_azure_basic_math(self):\n        self.azure_workflow.restart()\n        self.azure_workflow.run(\"Which is the square root of 256?\")\n\n        self.assertIn(\n            \"16\",\n            self.azure_workflow.conversation.messages[-1][\"content\"],\n            \"Expected agent to respond 16\",\n        )\n\n    def test_azure_yfinance(self):\n        self.azure_workflow.restart()\n        self.azure_workflow.run(\n            \"What is latest quote for Apple Inc. stock? Generate code to get the quote using yfinance and return only the latest quote (include $)\"\n        )\n\n        self.assertIn(\n            \"$\",\n            self.azure_workflow.conversation.messages[-1][\"content\"],\n            \"Expected agent to respond with a quote\",\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "vanilla_aiagents/tests/generate_coverage_badge.py", "content": "import os\nimport coverage\nimport requests\n\ndef generate_badge(coverage_percentage):\n    color = \"red\"\n    if coverage_percentage >= 85:\n        color = \"brightgreen\"\n    elif coverage_percentage >= 75:\n        color = \"yellow\"\n    elif coverage_percentage >= 50:\n        color = \"orange\"\n\n    badge_url = f\"https://img.shields.io/badge/coverage-{coverage_percentage}%25-{color}.svg\"\n    response = requests.get(badge_url)\n    with open(\"coverage-badge.svg\", \"wb\") as f:\n        f.write(response.content)\n\nif __name__ == \"__main__\":\n    cov = coverage.Coverage()\n    cov.load()\n    coverage_percentage = round(cov.report())\n    generate_badge(coverage_percentage)"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_remote.py", "content": "import unittest\nimport os, logging, sys\n\nimport pytest\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom vanilla_aiagents.conversation import Conversation\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.user import User\nfrom vanilla_aiagents.team import Team\nfrom vanilla_aiagents.llm import AzureOpenAILLM\nfrom vanilla_aiagents.remote.remote import RESTHost, RemoteAskable, RESTConnection\nfrom vanilla_aiagents.remote.grpc import GRPCHost, GRPCConnection\n\nfrom dotenv import load_dotenv\nload_dotenv(override=True)\n\nclass TestRemote(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM({\n            \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n            \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n            \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n        })\n        \n        logging.basicConfig(level=logging.INFO)\n        logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.llm\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.remote.remote\").setLevel(logging.DEBUG)\n        \n        self.agent1 = Agent(id=\"agent1\", llm=self.llm, description=\"Call this agent for general purpose questions\", system_message = \"\"\"You are an AI assistant\n        Your task is to help the user with their questions.\n        Always respond with the best answer you can generate.\n        If you don't know the answer, respond with \"I don't know\".\n        Always be polite and helpful.\n        \"\"\")\n        \n        self.agent2 = Agent(id=\"agent2\", llm=self.llm, description=\"Call this agent for general purpose questions\", system_message = \"\"\"You are an AI assistant\n        Your task is to help the user with their questions.\n        Always respond with the best answer you can generate.\n        If you don't know the answer, respond with \"I don't know\".\n        Always be polite and helpful.\n        \"\"\")\n\n    def test_rest(self):\n        \n        host = RESTHost(askables=[self.agent1, self.agent2], host=\"127.0.0.1\", port=5000)\n        self.assertCountEqual([self.agent1, self.agent2], host.askables, \"Expected askables to be the same\")\n        \n        host.start()\n        \n        connection = RESTConnection(url=\"http://localhost:5000\")\n        remote = RemoteAskable(id=\"agent1\", connection=connection)\n        workflow = Workflow(askable=remote, conversation=Conversation())\n        \n        self.assertEqual(self.agent1.id, workflow.askable.id, \"Expected askable ID to be agent1\")\n        self.assertEqual(self.agent1.description, workflow.askable.description, \"Expected askable description to be the same as agent1\")\n        \n        workflow.restart()\n        workflow.run(\"Which is the capital of France?\")\n        \n        host.stop()\n        \n        # Note name in RemoteAskable is the one set in the response\n        self.assertEqual(workflow.conversation.messages[-1][\"name\"], \"agent1\", \"Expected agent to respond with greeting\")\n        self.assertEqual(workflow.conversation.messages[-1][\"name\"], \"agent1\", \"Expected agent to respond\")\n        self.assertIn(\"Paris\", workflow.conversation.messages[-1][\"content\"], \"Expected agent to respond 'Paris'\")\n        \n    def test_rest_streaming(self):\n        \n        host = RESTHost(askables=[self.agent1], host=\"127.0.0.1\", port=5001)\n        \n        host.start()\n        \n        connection = RESTConnection(url=\"http://localhost:5001\")\n        remote = RemoteAskable(id=\"agent1\", connection=connection)\n        workflow = Workflow(askable=remote, conversation=Conversation())\n        \n        result = None\n        for mark, content in workflow.run_stream(\"Which is the capital of France?\"):\n            if mark == \"response\":\n                result = content[0]\n                break\n            if mark == \"error\":\n                break\n            \n        self.assertIn(\"Paris\", result[\"content\"],\"Expected agent to respond 'Paris'\")\n        \n        host.stop()\n\n\nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_reading_strategy.py", "content": "import unittest\nimport os, logging, sys\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom vanilla_aiagents.conversation import AllMessagesStrategy, Conversation, LastNMessagesStrategy, TopKLastNMessagesStrategy, SummarizeMessagesStrategy, PipelineConversationReadingStrategy\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nfrom dotenv import load_dotenv\nload_dotenv(override=True)\n\nclass TestReadingStrategy(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM({\n            \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n            \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n            \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n        })\n        \n        logging.basicConfig(level=logging.INFO)\n        logging.getLogger(\"vanilla_aiagents.conversation\").setLevel(logging.DEBUG)\n        \n        self.conversation = Conversation(messages=[\n            {\"role\": \"system\", \"content\": \"\"},\n            {\"role\": \"assistant\",\"name\": \"agent\", \"content\": \"Hello! How can I help you today?\"},\n            {\"role\": \"user\",\"name\": \"user\", \"content\": \"Hi! Can you help me with capital cities?\"},\n            {\"role\": \"assistant\",\"name\": \"agent\", \"content\": \"Sure! Which capital you want to learn about?\"},\n            {\"role\": \"user\",\"name\": \"user\", \"content\": \"Which is the capital of Italy?\"},\n            {\"role\": \"assistant\",\"name\": \"agent\", \"content\": \"The capital of Italy is Rome.\"},\n            {\"role\": \"user\",\"name\": \"user\", \"content\": \"And the capital of France?\"},\n            {\"role\": \"assistant\",\"name\": \"agent\", \"content\": \"The capital of France is Paris.\"},\n            {\"role\": \"user\",\"name\": \"user\", \"content\": \"Thank you!\"},\n            {\"role\": \"assistant\",\"name\": \"agent\", \"content\": \"You're welcome!\"},\n        ])\n\n    def test_allmessages(self):\n        strategy = AllMessagesStrategy()\n        result = strategy.get_messages(self.conversation)\n        \n        self.assertEqual(len(result), len(self.conversation.messages)-1, \"Expected ALL messages in the conversation\")\n        self.assertNotEqual(result[0][\"role\"], \"system\", \"Expected system message NOT to be included\")\n        \n    def test_lastnmessages(self):\n        strategy = LastNMessagesStrategy(n=3)\n        result = strategy.get_messages(self.conversation)\n        \n        self.assertEqual(len(result), 3, \"Expected 3 messages in the conversation\") # System message is not included\n        self.assertEqual(result[0][\"role\"], \"assistant\", \"Expected system message NOT to be included\")\n        self.assertEqual(result[1][\"role\"], \"user\", \"Expected user message to be included\")\n        \n    def test_topklastnmessages(self):\n        strategy = TopKLastNMessagesStrategy(k=2, n=3)\n        result = strategy.get_messages(self.conversation)\n        \n        self.assertEqual(len(result), 5, \"Expected 2 messages in the conversation\")\n        self.assertEqual(result[1][\"role\"], \"user\", \"Expected user message to be included\")\n        self.assertEqual(result[0][\"role\"], \"assistant\", \"Expected assistant message to be included\")\n        self.assertEqual(result[-2][\"role\"], \"user\", \"Expected user message to be penultimate\")\n        self.assertEqual(result[-1][\"role\"], \"assistant\", \"Expected assistant message to be last\")\n        \n    def test_summarizemessages(self):\n        strategy = SummarizeMessagesStrategy(llm=self.llm, system_prompt=\"Summarize the conversation, highlighting the key points in a bullet list\")\n        result = strategy.get_messages(self.conversation)\n                \n        self.assertEqual(len(result), 1, \"Expected 1 message in the conversation\")\n        self.assertEqual(result[0][\"role\"], \"assistant\", \"Expected assistant message to be included\")\n        \n    def test_pipelineconversationreadingstrategy(self):\n        strategy = PipelineConversationReadingStrategy([\n            LastNMessagesStrategy(n=4),\n            SummarizeMessagesStrategy(llm=self.llm, system_prompt=\"Summarize the conversation, highlighting the key points in a bullet list\")\n        ])\n        \n        result = strategy.get_messages(self.conversation)\n        \n        self.assertEqual(len(result), 1, \"Expected 1 message in the conversation\")\n        self.assertIn(\"Paris\", result[0][\"content\"], \"Expected Paris to be in the summarized text\")\n        self.assertNotIn(\"Rome\", result[0][\"content\"], \"Expected Rome NOT to be in the summarized text\")\n        \nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_sequence.py", "content": "import unittest\nimport os, logging, sys\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom vanilla_aiagents.user import User\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.sequence import Sequence\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nfrom dotenv import load_dotenv\nload_dotenv(override=True)\n\nclass TestSequence(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM({\n            \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n            \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n            \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n        })\n        \n        # Set logging to debug for Agent, User, and Workflow\n        logging.basicConfig(level=logging.INFO)\n        # logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n        # logging.getLogger(\"vanilla_aiagents.user\").setLevel(logging.DEBUG)\n        # logging.getLogger(\"vanilla_aiagents.workflow\").setLevel(logging.DEBUG)\n\n    def test_sequence(self):\n        # Telling the agents to set context variables implies calling a pre-defined function call\n        first = Agent(id=\"first\", llm=self.llm, description=\"First agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to set a context for the next agent to continue the conversation.\n\n        DO set context variable \"CHANNEL\" to \"voice\" and \"LANGUAGE\" to \"en\"\n\n        DO respond only \"Context set\" when you are done.\n        \"\"\")\n\n        # Second agent might have its system message extended automatically with the context from the ongoing conversation\n        second = Agent(id=\"second\", llm=self.llm, description=\"Second agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to continue the conversation based on the context set by the previous agent.\n        When asked, you can use variable provide in CONTEXT to generate the response.\n\n        --- CONTEXT ---\n        __context__\n        \"\"\")\n        \n        flow = Sequence(id=\"flow\", description=\"\", steps=[first, second], llm=self.llm)\n        workflow = Workflow(askable=flow)\n        \n        workflow.run(\"Which channel is this conversation on?\")\n        \n        self.assertEqual(len(workflow.conversation.messages), 4, \"Expected 3 messages in the conversation\")\n        \n        self.assertEqual(\"first\", workflow.conversation.messages[2]['name'], \"Expected first agent to be the first to respond\")\n        \n        self.assertEqual(\"second\", workflow.conversation.messages[3]['name'], \"Expected second agent to be the second to respond\")\n        \n    def test_sequence_streaming(self):\n        # Telling the agents to set context variables implies calling a pre-defined function call\n        first = Agent(id=\"first\", llm=self.llm, description=\"First agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to set a context for the next agent to continue the conversation.\n\n        DO set context variable \"CHANNEL\" to \"voice\" and \"LANGUAGE\" to \"en\"\n\n        DO respond only \"Context set\" when you are done.\n        \"\"\")\n\n        # Second agent might have its system message extended automatically with the context from the ongoing conversation\n        second = Agent(id=\"second\", llm=self.llm, description=\"Second agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to continue the conversation based on the context set by the previous agent.\n        When asked, you can use variable provide in CONTEXT to generate the response.\n\n        --- CONTEXT ---\n        __context__\n        \"\"\")\n        \n        flow = Sequence(id=\"flow\", description=\"\", steps=[first, second], llm=self.llm)\n        workflow = Workflow(askable=flow)\n        \n        for mark, content in workflow.run_stream(\"Which channel is this conversation on?\"):\n            if mark == \"end\" and content == flow.id:\n                break\n\n    def test_user_sequence(self):\n        agent1 = Agent(id=\"agent\", llm=self.llm, description=\"Call this agent to answer questions by the user\", system_message = \"\"\"You are an AI assistant\n        Your task is to help the user with their questions.\n        Always respond with the best answer you can generate.\n        If you don't know the answer, respond with \"I don't know\".\n        Always be polite and helpful.\n        \"\"\")\n        \n        def fake_input(prompt):\n            return \"Which is the capital of France?\"\n\n        user = User(id=\"user\", mode=\"interactive\", interaction_function=fake_input)\n        \n        # Must force the stop callback to be triggered by the number of messages, otherwise the test will hang\n        flow = Sequence(id=\"team\", description=\"\", steps=[agent1, user], llm=self.llm)\n        workflow = Workflow(askable=flow)\n        \n        workflow.restart()\n        workflow.run(\"Hi! Can you help me with capital cities?\")\n\n\nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_team.py", "content": "from typing import Annotated\nimport unittest\nimport os, logging, sys\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.team import Team\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n\nclass TestTeam(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM(\n            {\n                \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n                \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n                \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n                \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n            }\n        )\n\n        # Set logging to debug for Agent, User, and Workflow\n        logging.basicConfig(level=logging.INFO)\n        logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.team\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.workflow\").setLevel(logging.DEBUG)\n\n    def test_transitions(self):\n        # Telling the agents to set context variables implies calling a pre-defined function call\n        first = Agent(\n            id=\"first\",\n            llm=self.llm,\n            description=\"Agent that sets context variables\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to set a context for the next agent to continue the conversation.\n\n        DO set context variable \"CHANNEL\" to \"voice\" and \"LANGUAGE\" to \"en\"\n\n        DO respond only \"Context set\" when you are done.\n        \"\"\",\n        )\n\n        # Second agent might have its system message extended automatically with the context from the ongoing conversation\n        second = Agent(\n            id=\"second\",\n            llm=self.llm,\n            description=\"Agent that uses context variables to answer\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to continue the conversation based on the context set by the previous agent.\n        When asked, you can use variable provide in CONTEXT to generate the response.\n\n        --- CONTEXT ---\n        __context__\n        \"\"\",\n        )\n\n        flow = Team(\n            id=\"flow\",\n            description=\"\",\n            members=[first, second],\n            llm=self.llm,\n            allowed_transitions={first: [second]},\n            stop_callback=lambda conv: len(conv.messages) > 3,\n        )\n        workflow = Workflow(askable=flow)\n        workflow.restart()\n\n        workflow.run(\"Which channel is this conversation on?\")\n\n        self.assertEqual(\n            len(workflow.conversation.messages),\n            4,\n            \"Expected 3 messages in the conversation\",\n        )\n\n        self.assertIn(\n            \"Context set\",\n            workflow.conversation.messages[2][\"content\"],\n            \"Expected context to be set\",\n        )\n        self.assertIn(\n            \"voice\",\n            workflow.conversation.messages[3][\"content\"],\n            \"Expected second agent to recognize context variable CHANNEL\",\n        )\n\n    def test_use_tools(self):\n        first = Agent(\n            id=\"first\",\n            llm=self.llm,\n            description=\"Agent1\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to support the user inquiry by providing the user profile.\n        \"\"\",\n        )\n\n        @first.register_tool(\n            name=\"get_user_profile\", description=\"Get the user profile\"\n        )\n        def get_user_profile():\n            return 'User profile: {\"name\": \"John\", \"age\": 30}'\n\n        second = Agent(\n            id=\"second\",\n            llm=self.llm,\n            description=\"Agent2\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to support the user inquiry by providing the user balance.\n        \"\"\",\n        )\n\n        @second.register_tool(\n            name=\"get_user_balance\", description=\"Get the user balance\"\n        )\n        def get_user_balance():\n            return \"User balance: $100\"\n\n        flow = Team(\n            id=\"flow\",\n            description=\"\",\n            members=[first, second],\n            llm=self.llm,\n            stop_callback=lambda conv: len(conv.messages) > 3,\n            include_tools_descriptions=True,\n        )\n        workflow = Workflow(askable=flow)\n        workflow.restart()\n\n        workflow.run(\"Which is my current balance?\")\n\n        # Assert the last message in the conversation is the user balance\n        self.assertEqual(\n            second.id,\n            workflow.conversation.messages[-1][\"name\"],\n            \"Expected user balance to be provided\",\n        )\n\n    def test_not_use_structured_output(self):\n        first = Agent(\n            id=\"first\",\n            llm=self.llm,\n            description=\"Agent1\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to support the user inquiry.\n        \"\"\",\n        )\n\n        @first.register_tool(\n            name=\"get_user_profile\", description=\"Get the user profile\"\n        )\n        def get_user_profile() -> (\n            Annotated[str, \"The user profile, containing name and age\"]\n        ):\n            return 'User profile: {\"name\": \"John\", \"age\": 30}'\n\n        second = Agent(\n            id=\"second\",\n            llm=self.llm,\n            description=\"Agent2\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to support the user inquiry.\n        \"\"\",\n        )\n\n        @second.register_tool(\n            name=\"get_user_balance\", description=\"Get the user balance\"\n        )\n        def get_user_balance() -> Annotated[str, \"The user balance in USD\"]:\n            return \"User balance: $100\"\n\n        flow = Team(\n            id=\"flow\",\n            description=\"\",\n            members=[first, second],\n            llm=self.llm,\n            stop_callback=lambda conv: len(conv.messages) == 3,\n            include_tools_descriptions=True,\n            use_structured_output=False,\n        )\n        workflow = Workflow(askable=flow)\n        workflow.restart()\n\n        workflow.run(\"Which is my current balance?\")\n\n        # Assert the last message in the conversation is the user balance\n        self.assertEqual(\n            second.id,\n            workflow.conversation.messages[-1][\"name\"],\n            \"Expected user balance to be provided\",\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_planned_team.py", "content": "from typing import Annotated\nimport unittest\nimport os, logging, sys\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.conversation import Conversation, SummarizeMessagesStrategy\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.planned_team import PlannedTeam\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n\nclass TestPlannedTeam(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM(\n            {\n                \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n                \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n                \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n                \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n            }\n        )\n\n        # Set logging to debug for Agent, User, and Workflow\n        logging.basicConfig(level=logging.INFO)\n        logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.planned_team\").setLevel(logging.DEBUG)\n\n    def test_fork(self):\n        collector = Agent(\n            id=\"datacollector\",\n            llm=self.llm,\n            description=\"Call this agent to collect data for an insurance claim\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to collect key information for an insurance claim to be processed.\n        You need to collect the following information:\n        - Policy number\n        - Claimant name\n        - Date of incident\n        - Description of incident\n        - Reimbursement amount\n        \n        Output must be in JSON format:\n        {\n            \"policy_number\": \"123456\",\n            \"claimant_name\": \"John Doe\",\n            \"date_of_incident\": \"2022-01-01\",\n            \"description_of_incident\": \"Car accident on Maple Avenue\",\n            \"incident_kind\": \"car accident\",\n            \"reimbursement_amount\": 1000\n        }\n        \"\"\",\n        )\n\n        approver = Agent(\n            id=\"approver\",\n            llm=self.llm,\n            description=\"Call this agent to approve the claim\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to review the information collected for an insurance claim.\n        Approval is subject to the following criteria:\n        - If the description refers to a car accident\n            - approve the claim if the reimbursement amount is less than 1000 USD\n            - reject the claim if the reimbursement amount is 1000 USD or more\n        - If the description refers to a house fire\n            - approve the claim if the reimbursement amount is less than 5000 USD\n            - reject the claim if the reimbursement amount is 5000 USD or more\n        \"\"\",\n        )\n\n        responder = Agent(\n            id=\"responder\",\n            llm=self.llm,\n            description=\"Call this agent to respond to the claim\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to respond to the claimant based on the approval decision.\n        If the claim is approved, respond with \"Your claim has been approved\".\n        If the claim is rejected, respond with \"Your claim has been rejected\". Provide a reason for the rejection.\n        \"\"\",\n        )\n\n        flow = PlannedTeam(\n            id=\"flow\",\n            description=\"\",\n            members=[collector, approver, responder],\n            llm=self.llm,\n            fork_conversation=True,\n            fork_strategy=SummarizeMessagesStrategy(\n                self.llm,\n                \"Summarize the conversation, focusing on the key points and decisions made.\",\n            ),\n        )\n        workflow = Workflow(askable=flow)\n        workflow.restart()\n\n        workflow.run(ticket)\n\n        self.assertEqual(\n            len(workflow.conversation.messages),\n            3,\n            \"Expected 3 messages (sys+user+planned) in the original conversation\",\n        )\n        # Expect last message to be a dict with name and content keys\n        self.assertIn(\n            \"name\",\n            workflow.conversation.messages[-1],\n            \"Expected last message to have a 'name' key\",\n        )\n        self.assertIn(\n            \"content\",\n            workflow.conversation.messages[-1],\n            \"Expected last message to have a 'content' key\",\n        )\n        self.assertEqual(\n            workflow.conversation.messages[-1][\"name\"],\n            \"summarizer\",\n            \"Expected last message to be from the summarizer\",\n        )\n\n    def test_include_tools(self):\n        # Telling the agents to set context variables implies calling a pre-defined function call\n        first = Agent(\n            id=\"first\",\n            llm=self.llm,\n            description=\"First agent\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to support the user inquiry by providing the user profile.\n        Use a tool to retrieve the user profile.\n        \"\"\",\n        )\n\n        @first.register_tool(\n            name=\"get_user_profile\", description=\"Get the user profile\"\n        )\n        def get_user_profile() -> (\n            Annotated[str, \"User profile in JSON format, with name and age\"]\n        ):\n            return '{\"name\": \"John\", \"age\": 30}'\n\n        # Second agent will have its system message extended automatically with the context from the ongoing conversation\n        second = Agent(\n            id=\"second\",\n            llm=self.llm,\n            description=\"Second agent\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to support the user inquiry.\n        When the users asks if they are eligible for a discount, use a tool to check discount eligibility.\n        \"\"\",\n        )\n\n        @second.register_tool(\n            name=\"check_discount_eligibility\",\n            description=\"Check if the user is eligible for a discount\",\n        )\n        def check_discount_eligibility(\n            age: Annotated[int, \"The user age\"]\n        ) -> Annotated[str, \"Eligibility message\"]:\n            return (\n                \"You are eligible for a discount\"\n                if age < 25\n                else \"You are not eligible for a discount\"\n            )\n\n        flow = PlannedTeam(\n            id=\"flow\",\n            description=\"\",\n            members=[first, second],\n            llm=self.llm,\n            include_tools_descriptions=True,\n        )\n        workflow = Workflow(askable=flow)\n        workflow.restart()\n\n        result = workflow.run(\"Can I have a discount?\")\n\n        self.assertEqual(result, \"done\", \"Expected the workflow result to be 'done'\")\n        self.assertEqual(\n            workflow.conversation.messages[-1][\"name\"],\n            second.id,\n            \"Expected second agent to respond with DONE\",\n        )\n        self.assertIn(\n            \"not eligible\",\n            workflow.conversation.messages[-1][\"content\"],\n            \"Expected result to be 'not eligible'\",\n        )\n\n    def test_feedback(self):\n        sales = Agent(\n            id=\"sales\",\n            llm=self.llm,\n            description=\"Sales agent, takes price and discounts to provide an offer. Use a professional tone\",\n            system_message=\"\"\"You are part of an AI sales process\n        Your task is to respond to the user buying ask by providing a price and a discount.\n        You're goal is to maximize sold value, so keep discount low unless the user keeps asking for it.\n        NEVER exceed the maximum discount for the product.\n        \"\"\",\n        )\n\n        catalog = Agent(\n            id=\"catalog\",\n            llm=self.llm,\n            description=\"Product catalog agent, provides prices and discounts\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to provide price and discount information for the sales agent to use in the offer.\n\n        # PRODUCTS PRICES AND DISCOUNTS\n        - Oven: $1000\n            - MAX Discount: 25%\n        - Fridge: $1500\n            - MAX Discount: 10%\n        - Washing machine: $800\n            - MAX Discount: 15%\n            \n        Use the following JSON format to provide the information:\n        {{\"product\": \"name\", \"price\": \"1000\", \"max_discount\": 0.10}}\n        \"\"\",\n        )\n\n        buyer = Agent(\n            id=\"buyer\",\n            llm=self.llm,\n            description=\"Buyer agent, will provide feedback on the offer. MUST be called last in the workflow\",\n            system_message=\"\"\"You are part of an AI sales process.\n            Your task is to provide feedback on the offer provided by the sales agent.\n            Never accept the first offer\n            Minimum acceptable discount is 10%.\n            \n            When offer can be accepted, set variable \"result\" to \"done\"\n            When offer is not acceptable, set variable \"result\" to \"KO\" and \"__feedback\" to \"I want a better discount\"\n            \"\"\",\n        )\n\n        def can_end(conversation: Conversation) -> bool:\n            return (\n                conversation.variables.get(\"result\") == \"done\"\n                or len(conversation.messages) > 15\n            )\n\n        flow = PlannedTeam(\n            id=\"offer\",\n            description=\"\",\n            members=[sales, catalog, buyer],\n            llm=self.llm,\n            include_tools_descriptions=True,\n            repeat_until=can_end,\n        )\n        workflow = Workflow(askable=flow)\n        workflow.restart()\n\n        result = workflow.run(\"I want a new oven\")\n\n        self.assertEqual(result, \"done\", \"Expected the workflow result to be 'done'\")\n        self.assertEqual(\n            \"done\",\n            workflow.conversation.variables[\"result\"],\n            \"Expected 'result' variable to be 'done'\",\n        )\n\n    def test_plan(self):\n        collector = Agent(\n            id=\"datacollector\",\n            llm=self.llm,\n            description=\"Call this agent to collect data for an insurance claim\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to collect key information for an insurance claim to be processed.\n        You need to collect the following information:\n        - Policy number\n        - Claimant name\n        - Date of incident\n        - Description of incident\n        - Reimbursement amount\n        \n        Output must be in JSON format:\n        {\n            \"policy_number\": \"123456\",\n            \"claimant_name\": \"John Doe\",\n            \"date_of_incident\": \"2022-01-01\",\n            \"description_of_incident\": \"Car accident on Maple Avenue\",\n            \"incident_kind\": \"car accident\",\n            \"reimbursement_amount\": 1000\n        }\n        \"\"\",\n        )\n\n        approver = Agent(\n            id=\"approver\",\n            llm=self.llm,\n            description=\"Call this agent to approve the claim\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to review the information collected for an insurance claim.\n        Approval is subject to the following criteria:\n        - If the description refers to a car accident\n            - approve the claim if the reimbursement amount is less than 1000 USD\n            - reject the claim if the reimbursement amount is 1000 USD or more\n        - If the description refers to a house fire\n            - approve the claim if the reimbursement amount is less than 5000 USD\n            - reject the claim if the reimbursement amount is 5000 USD or more\n        \"\"\",\n        )\n\n        responder = Agent(\n            id=\"responder\",\n            llm=self.llm,\n            description=\"Call this agent to respond to the claim\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to respond to the claimant based on the approval decision.\n        If the claim is approved, respond with \"Your claim has been approved\".\n        If the claim is rejected, respond with \"Your claim has been rejected\". Provide a reason for the rejection.\n        \"\"\",\n        )\n\n        flow = PlannedTeam(\n            id=\"flow\",\n            description=\"\",\n            members=[collector, approver, responder],\n            llm=self.llm,\n        )\n        workflow = Workflow(askable=flow)\n        workflow.restart()\n\n        workflow.run(ticket)\n\n        # Should be as follows:\n        # 0. system message from the workflow\n        # 1. user input\n        # 2. data collector input\n        # 3. data collector response\n        # 4. approver input\n        # 5. approver response\n        # 6. responder input\n        # 7. responder response\n        self.assertEqual(\n            len(workflow.conversation.messages),\n            8,\n            \"Expected 8 messages in the conversation\",\n        )\n\n        self.assertEqual(\n            workflow.conversation.messages[3][\"name\"],\n            \"datacollector\",\n            \"Expected data collector to respond first\",\n        )\n        self.assertIn(\n            \"car accident\",\n            workflow.conversation.messages[3][\"content\"].lower(),\n            \"Expected data collector to respond with incident description for car accident\",\n        )\n        self.assertIn(\n            \"1000\",\n            workflow.conversation.messages[3][\"content\"],\n            \"Expected data collector to respond with reimbursement amount of 1000\",\n        )\n\n        self.assertEqual(\n            workflow.conversation.messages[5][\"name\"],\n            \"approver\",\n            \"Expected approver to respond second\",\n        )\n        self.assertIn(\n            \"reject\",\n            workflow.conversation.messages[5][\"content\"],\n            \"Expected approver to reject the claim\",\n        )\n\n        self.assertEqual(\n            workflow.conversation.messages[7][\"name\"],\n            \"responder\",\n            \"Expected responder to respond last\",\n        )\n        self.assertIn(\n            \"rejected\",\n            workflow.conversation.messages[7][\"content\"],\n            \"Expected responder to respond with claim rejection message\",\n        )\n\n\nticket = \"\"\"\nFrom: Alice Thompson <a_thompson@foo.com>\nTo: Contoso Insurance Team <report@contoso-insurancec.com>\nSubject: URGENT: Car Accident Claim - Immediate Assistance Required\n\nDear Contoso Insurance Team,\n\nI hope this email finds you well. I am writing to you with a heavy heart and a great deal of stress regarding an unfortunate incident that occurred earlier today. I was involved in a car accident and I am seeking immediate assistance to file a claim and get the support I need during this challenging time.\n\nHere's what happened: I was driving home from work, heading south on Maple Avenue, when out of nowhere, another vehicle came speeding through a red light at the intersection of Maple and Oak Street. I had no time to react, and the other car crashed into the passenger side of my vehicle with considerable force. The impact was severe, and both cars were significantly damaged. My car, a 2018 Toyota Camry, has extensive damage to the passenger side, and I believe it may not be drivable at this point.\n\nThankfully, I did not sustain any major injuries, but I am feeling quite shaken up and have some minor bruises. The other driver appeared to be unharmed as well, but their car, a silver Honda Civic, was also badly damaged. We exchanged contact and insurance information at the scene, and I made sure to take photos of the damage to both vehicles for documentation purposes.\n\nThe accident was promptly reported to the local authorities, and a police report was filed. I have attached a copy of the police report, along with the photos I took, for your reference. Additionally, I have provided my insurance policy number and other relevant details below to expedite the process.\n\nPolicy Number: 2021-123456789\nDate of Accident: Monday, August 23, 2022\nLocation: Intersection of Maple Avenue and Oak Street\nVehicle Involved: 2018 Toyota Camry (License Plate: AAA-1234)\nOther Party Involved: Silver Honda Civic (License Plate: ZZZ-5678)\n\nI am deeply concerned about the repair costs ($ 1000) and the potential need for a rental car while my vehicle is being repaired. I would greatly appreciate it if you could guide me through the next steps and let me know what information or documentation you require from my end to process the claim efficiently.\n\nThis is a very distressing situation for me, and I am relying on your prompt assistance and expertise to help me navigate this process. Please let me know if there are any forms I need to fill out or additional information I need to provide.\n\nThank you in advance for your understanding and support during this difficult time. I look forward to hearing from you soon and hope for a swift resolution to my claim.\n\nWarm regards,\n\nAlice Thompson\n\"\"\"\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_update_strategy.py", "content": "import unittest\nimport os, logging, sys\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom vanilla_aiagents.conversation import (\n    Conversation,\n    NoopUpdateStrategy,\n    ReplaceLastMessageUpdateStrategy,\n    AppendMessagesUpdateStrategy,\n)\nfrom vanilla_aiagents.llm import AzureOpenAILLM\nfrom vanilla_aiagents.agent import Agent\n\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n\nclass TestReadingStrategy(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM(\n            {\n                \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n                \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n                \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n                \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n            }\n        )\n\n        logging.basicConfig(level=logging.INFO)\n        logging.getLogger(\"vanilla_aiagents.conversation\").setLevel(logging.DEBUG)\n\n    def test_append(self):\n        agent = Agent(\n            id=\"test-agent\",\n            description=\"Test Agent\",\n            system_message=\"Simply say 'hello'\",\n            llm=self.llm,\n            update_strategy=AppendMessagesUpdateStrategy(),\n        )\n\n        conversation = Conversation(messages=[], variables={})\n        agent.ask(conversation)\n\n        self.assertEqual(\n            len(conversation.messages), 1, \"Expected 1 message in the conversation\"\n        )\n\n    def test_replace(self):\n        agent = Agent(\n            id=\"test-agent\",\n            description=\"Test Agent\",\n            system_message=\"Always respond 'I am sorry' to any ask\",\n            llm=self.llm,\n            update_strategy=ReplaceLastMessageUpdateStrategy(),\n        )\n\n        conversation = Conversation(messages=[], variables={})\n        conversation.messages.append(\n            {\"role\": \"assistant\", \"content\": \"Can I order a pizza?\"}\n        )\n        agent.ask(conversation)\n\n        self.assertEqual(\n            len(conversation.messages), 1, \"Expected 1 message in the conversation\"\n        )\n        self.assertNotIn(\n            \"pizza\",\n            conversation.messages[0][\"content\"],\n            \"Expected message to be replaced\",\n        )\n\n    def test_noop(self):\n        agent = Agent(\n            id=\"test-agent\",\n            description=\"Test Agent\",\n            system_message=\"Simply say 'hello'\",\n            llm=self.llm,\n            update_strategy=NoopUpdateStrategy(),\n        )\n\n        conversation = Conversation(messages=[], variables={})\n        conversation.messages.append(\n            {\"role\": \"assistant\", \"content\": \"This will not be replaced\"}\n        )\n        agent.ask(conversation)\n\n        self.assertEqual(\n            len(conversation.messages), 1, \"Expected 1 message in the conversation\"\n        )\n        self.assertNotIn(\n            \"hello\",\n            conversation.messages[0][\"content\"],\n            \"Expected message NOT to be replaced\",\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_image_input.py", "content": "import unittest\nimport os, logging, sys\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom vanilla_aiagents.workflow import Workflow, WorkflowInput\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.sequence import Sequence\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nfrom dotenv import load_dotenv\nload_dotenv(override=True)\n\nclass TestContext(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM({\n            \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n            \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n            \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n        })\n        \n        # Set logging to debug for Agent, User, and Workflow\n        logging.basicConfig(level=logging.INFO)\n\n    def test_image_input(self):\n        # Telling the agents to set context variables implies calling a pre-defined function call\n        agent = Agent(id=\"first\", llm=self.llm, description=\"First agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to understand the user provided image to understand what problem they are facing.\n        Be sure to recognize the objects in the image and provide a helpful response.\n        \"\"\")\n        \n        data = WorkflowInput(text=\"\")\n        data.add_image_file(os.path.join(os.path.dirname(__file__), \"iphone_sim_error.jpg\"))\n        workflow = Workflow(askable=agent)\n        workflow.restart()\n        workflow.run(data)\n        \n        self.assertIn(\"iPhone\", workflow.conversation.messages[-1][\"content\"], \"Expected agent to recognize iPhone in the image\")\n        self.assertIn(\"SIM\", workflow.conversation.messages[-1][\"content\"], \"Expected agent to recognize sim in the image\")\n        \n    def test_image_input_bytes(self):\n        # Telling the agents to set context variables implies calling a pre-defined function call\n        agent = Agent(id=\"first\", llm=self.llm, description=\"First agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to understand the user provided image to understand what problem they are facing.\n        Be sure to recognize the objects in the image and provide a helpful response.\n        \"\"\")\n        \n        data = WorkflowInput(text=\"\")\n        with open(os.path.join(os.path.dirname(__file__), \"iphone_sim_error.jpg\"), \"rb\") as f:\n            data.add_image_bytes(f.read())\n        workflow = Workflow(askable=agent)\n        workflow.restart()\n        workflow.run(data)\n        \n        self.assertIn(\"iPhone\", workflow.conversation.messages[-1][\"content\"], \"Expected agent to recognize iPhone in the image\")\n        self.assertIn(\"SIM\", workflow.conversation.messages[-1][\"content\"], \"Expected agent to recognize sim in the image\")\n\n\nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_conversation.py", "content": "import unittest\nimport os, logging, sys\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom vanilla_aiagents.conversation import Conversation\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.user import User\nfrom vanilla_aiagents.team import Team\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nfrom dotenv import load_dotenv\nload_dotenv(override=True)\n\nclass TestConversation(unittest.TestCase):\n\n    def setUp(self):\n        pass\n        # self.llm = AzureOpenAILLM({\n        #     \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n        #     \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n        #     \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n        #     \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n        # })\n\n    def test_from_dict(self):\n        source = {\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"Hello\"},\n                {\"role\": \"assistant\", \"content\": \"Hi! How can I help you today?\"}\n            ],\n            \"variables\": {\n                \"user\": \"John Doe\",\n                \"age\": 25\n            },\n            \"metrics\": {\n                \"total_tokens\": 100,\n                \"prompt_tokens\": 50,\n                \"completion_tokens\": 50\n            },\n            \"log\": [\n                (\"info\", \"conversation\", \"start\"),\n                (\"info\", \"conversation\", \"end\")\n            ]\n        }\n        conversation = Conversation.from_dict(source)\n        \n        self.assertEqual(conversation.messages, source[\"messages\"])\n        self.assertEqual(conversation.variables, source[\"variables\"])\n        self.assertEqual(conversation.metrics.total_tokens, source[\"metrics\"][\"total_tokens\"])\n        self.assertEqual(conversation.metrics.prompt_tokens, source[\"metrics\"][\"prompt_tokens\"])\n        self.assertEqual(conversation.metrics.completion_tokens, source[\"metrics\"][\"completion_tokens\"])\n        self.assertEqual(conversation.log, source[\"log\"])        \n        \n\nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_error.py", "content": "import unittest\nimport os, logging, sys\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom vanilla_aiagents.conversation import Conversation\nfrom vanilla_aiagents.askable import Askable\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.sequence import Sequence\nfrom vanilla_aiagents.llm import AzureOpenAILLM, ErrorTestingLLM\nfrom vanilla_aiagents.planned_team import PlannedTeam\nfrom vanilla_aiagents.team import Team\n\nfrom dotenv import load_dotenv\nload_dotenv(override=True)\n\nclass TestLLM(unittest.TestCase):\n\n    def setUp(self):\n        \n        self.llm = AzureOpenAILLM({\n            \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n            \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n            \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n        })\n        self.fake_llm = ErrorTestingLLM({})\n        \n        # Set logging to debug for Agent, User, and Workflow\n        logging.basicConfig(level=logging.INFO)\n        # logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n        # logging.getLogger(\"vanilla_aiagents.user\").setLevel(logging.DEBUG)\n        # logging.getLogger(\"vanilla_aiagents.workflow\").setLevel(logging.DEBUG)    \n        \n    def test_sequence_error(self):\n        # Telling the agents to set context variables implies calling a pre-defined function call\n        first = Agent(id=\"first\", llm=self.fake_llm, description=\"First agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to set a context for the next agent to continue the conversation.\n\n        DO set context variable \"CHANNEL\" to \"voice\" and \"LANGUAGE\" to \"en\"\n\n        DO respond only \"Context set\" when you are done.\n        \"\"\")\n\n        # Second agent might have its system message extended automatically with the context from the ongoing conversation\n        second = Agent(id=\"second\", llm=self.fake_llm, description=\"Second agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to continue the conversation based on the context set by the previous agent.\n        When asked, you can use variable provide in CONTEXT to generate the response.\n\n        --- CONTEXT ---\n        __context__\n        \"\"\")\n        \n        flow = Sequence(id=\"flow\", description=\"\", steps=[first, second], llm=self.llm)\n        workflow = Workflow(askable=flow)\n        \n        workflow.run(\"Which channel is this conversation on?\")\n        \n        result = workflow.run(\"Which channel is this conversation on?\")\n        \n        self.assertEqual(result, \"agent-error\", \"Expected error result from the workflow\")\n        \n    def test_team_error(self):\n        # Telling the agents to set context variables implies calling a pre-defined function call\n        first = Agent(id=\"first\", llm=self.fake_llm, description=\"First agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to set a context for the next agent to continue the conversation.\n\n        DO set context variable \"CHANNEL\" to \"voice\" and \"LANGUAGE\" to \"en\"\n\n        DO respond only \"Context set\" when you are done.\n        \"\"\")\n\n        # Second agent might have its system message extended automatically with the context from the ongoing conversation\n        second = Agent(id=\"second\", llm=self.fake_llm, description=\"Second agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to continue the conversation based on the context set by the previous agent.\n        When asked, you can use variable provide in CONTEXT to generate the response.\n\n        --- CONTEXT ---\n        __context__\n        \"\"\")\n        \n        flow = Team(id=\"flow\", description=\"\", members=[first, second], llm=self.llm)\n        workflow = Workflow(askable=flow)\n        \n        result = workflow.run(\"Which channel is this conversation on?\")\n        \n        self.assertEqual(result, \"agent-error\", \"Expected error result from the workflow\")\n        \n    def test_planned_team_error(self):\n        # Telling the agents to set context variables implies calling a pre-defined function call\n        first = Agent(id=\"first\", llm=self.fake_llm, description=\"First agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to set a context for the next agent to continue the conversation.\n\n        DO set context variable \"CHANNEL\" to \"voice\" and \"LANGUAGE\" to \"en\"\n\n        DO respond only \"Context set\" when you are done.\n        \"\"\")\n\n        # Second agent might have its system message extended automatically with the context from the ongoing conversation\n        second = Agent(id=\"second\", llm=self.fake_llm, description=\"Second agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to continue the conversation based on the context set by the previous agent.\n        When asked, you can use variable provide in CONTEXT to generate the response.\n\n        --- CONTEXT ---\n        __context__\n        \"\"\")\n        \n        flow = PlannedTeam(id=\"flow\", description=\"\", members=[first, second], llm=self.llm)\n        workflow = Workflow(askable=flow)\n        \n        result = workflow.run(\"Which channel is this conversation on?\")\n        \n        self.assertEqual(result, \"agent-error\", \"Expected error result from the workflow\")\n        \n    def test_agent_error(self):\n        # Telling the agents to set context variables implies calling a pre-defined function call\n        first = Agent(id=\"first\", llm=self.fake_llm, description=\"First agent\", system_message = \"\"\"You are an AI assistant\"\"\")\n        \n        workflow = Workflow(askable=first)\n        \n        result = workflow.run(\"Which channel is this conversation on?\")\n        \n        self.assertEqual(result, \"error\", \"Expected error result from the workflow\")\n        \n    def test_workflow_error(self):\n        class ErrorAgent(Askable):\n            def __init__(self, id, description):\n                super().__init__(id, description)\n                \n            def ask(self, conversation: Conversation, stream = False) -> str:\n                raise Exception(\"Fake Error\")\n            \n        workflow = Workflow(askable=ErrorAgent(id=\"error\", description=\"Fake Error Agent\"))\n        \n        result = None\n        for mark, content in workflow.run_stream(\"Which channel is this conversation on?\"):\n            if mark == \"result\":\n                result = content\n                break\n        \n        self.assertEqual(result, \"error\", \"Expected error result from the workflow\")\n        \n\n\nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_context.py", "content": "import unittest\nimport os, logging, sys\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.sequence import Sequence\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nfrom dotenv import load_dotenv\nload_dotenv(override=True)\n\nclass TestContext(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM({\n            \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n            \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n            \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n        })\n        \n        # Set logging to debug for Agent, User, and Workflow\n        logging.basicConfig(level=logging.INFO)\n        # logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n        # logging.getLogger(\"vanilla_aiagents.user\").setLevel(logging.DEBUG)\n        # logging.getLogger(\"vanilla_aiagents.workflow\").setLevel(logging.DEBUG)\n\n    def test_context(self):\n        # Telling the agents to set context variables implies calling a pre-defined function call\n        first = Agent(id=\"first\", llm=self.llm, description=\"First agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to set a context for the next agent to continue the conversation.\n\n        DO set context variable \"CHANNEL\" to \"voice\" and \"LANGUAGE\" to \"en\"\n\n        DO respond only \"Context set\" when you are done.\n        \"\"\")\n\n        # Second agent might have its system message extended automatically with the context from the ongoing conversation\n        second = Agent(id=\"second\", llm=self.llm, description=\"Second agent\", system_message = \"\"\"You are part of an AI process\n        Your task is to continue the conversation based on the context set by the previous agent.\n        When asked, you can use variable provide in CONTEXT to generate the response.\n\n        --- CONTEXT ---\n        __context__\n        \"\"\")\n        \n        flow = Sequence(id=\"flow\", description=\"\", steps=[first, second], llm=self.llm)\n        workflow = Workflow(askable=flow)\n        workflow.restart()\n        \n        workflow.run(\"Which channel is this conversation on?\")\n        \n        self.assertEqual(len(workflow.conversation.messages), 4, \"Expected 3 messages in the conversation\")\n        \n        self.assertIn(\"Context set\", workflow.conversation.messages[2][\"content\"], \"Expected context to be set\")\n        self.assertIn(\"voice\", workflow.conversation.messages[3][\"content\"], \"Expected second agent to recognize context variable CHANNEL\")\n\n\nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_llm.py", "content": "import unittest\nimport os, logging, sys\n\nfrom pydantic import BaseModel\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.sequence import Sequence\nfrom vanilla_aiagents.llm import AzureOpenAILLM, LLMConstraints\nfrom vanilla_aiagents.team import Team\n\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n\nclass TestLLM(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM(\n            {\n                \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n                \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n                \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n                \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n            }\n        )\n\n        # Set logging to debug for Agent, User, and Workflow\n        logging.basicConfig(level=logging.INFO)\n        # logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n        # logging.getLogger(\"vanilla_aiagents.user\").setLevel(logging.DEBUG)\n        # logging.getLogger(\"vanilla_aiagents.workflow\").setLevel(logging.DEBUG)\n\n    def test_metrics_sequence(self):\n        # Telling the agents to set context variables implies calling a pre-defined function call\n        first = Agent(\n            id=\"first\",\n            llm=self.llm,\n            description=\"First agent\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to set a context for the next agent to continue the conversation.\n\n        DO set context variable \"CHANNEL\" to \"voice\" and \"LANGUAGE\" to \"en\"\n\n        DO respond only \"Context set\" when you are done.\n        \"\"\",\n        )\n\n        # Second agent might have its system message extended automatically with the context from the ongoing conversation\n        second = Agent(\n            id=\"second\",\n            llm=self.llm,\n            description=\"Second agent\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to continue the conversation based on the context set by the previous agent.\n        When asked, you can use variable provide in CONTEXT to generate the response.\n\n        --- CONTEXT ---\n        __context__\n        \"\"\",\n        )\n\n        flow = Sequence(id=\"flow\", description=\"\", steps=[first, second], llm=self.llm)\n        workflow = Workflow(askable=flow)\n\n        workflow.run(\"Which channel is this conversation on?\")\n\n        self.assertGreater(\n            workflow.conversation.metrics.completion_tokens,\n            0,\n            \"Expected completion_tokens to be greater than 0\",\n        )\n        self.assertGreater(\n            workflow.conversation.metrics.prompt_tokens,\n            0,\n            \"Expected prompt_tokens to be greater than 0\",\n        )\n        self.assertGreater(\n            workflow.conversation.metrics.total_tokens,\n            0,\n            \"Expected total_tokens to be greater than 0\",\n        )\n\n    def test_metrics_team(self):\n        # Telling the agents to set context variables implies calling a pre-defined function call\n        first = Agent(\n            id=\"first\",\n            llm=self.llm,\n            description=\"First agent\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to set a context for the next agent to continue the conversation.\n\n        DO set context variable \"CHANNEL\" to \"voice\" and \"LANGUAGE\" to \"en\"\n\n        DO respond only \"Context set\" when you are done.\n        \"\"\",\n        )\n\n        # Second agent might have its system message extended automatically with the context from the ongoing conversation\n        second = Agent(\n            id=\"second\",\n            llm=self.llm,\n            description=\"Second agent\",\n            system_message=\"\"\"You are part of an AI process\n        Your task is to continue the conversation based on the context set by the previous agent.\n        When asked, you can use variable provide in CONTEXT to generate the response.\n\n        --- CONTEXT ---\n        __context__\n        \"\"\",\n        )\n\n        flow = Team(\n            id=\"flow\",\n            description=\"\",\n            members=[first, second],\n            llm=self.llm,\n            stop_callback=lambda conv: len(conv.messages) > 2,\n        )\n        workflow = Workflow(askable=flow)\n\n        workflow.run(\"Which channel is this conversation on?\")\n\n        self.assertGreater(\n            workflow.conversation.metrics.completion_tokens,\n            0,\n            \"Expected completion_tokens to be greater than 0\",\n        )\n        self.assertGreater(\n            workflow.conversation.metrics.prompt_tokens,\n            0,\n            \"Expected prompt_tokens to be greater than 0\",\n        )\n        self.assertGreater(\n            workflow.conversation.metrics.total_tokens,\n            0,\n            \"Expected total_tokens to be greater than 0\",\n        )\n\n    def test_constraints(self):\n        llm2 = AzureOpenAILLM(\n            {\n                \"azure_deployment\": \"o1-mini\",\n                \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n                \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n                \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n            },\n            constraints=LLMConstraints(\n                temperature=1, structured_output=False, system_message=False\n            ),\n        )\n\n        class HelloResponse(BaseModel):\n            response: str\n\n        response, metrics = llm2.ask(\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"Say hello with JSON format {response: \"message\"}\"\"\",\n                }\n            ],\n            response_format=HelloResponse,\n            temperature=0.7,\n        )\n\n        # If this call passes, then the constraints are working\n        self.assertIsNotNone(response, \"Expected response to be not None\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_streaming.py", "content": "from typing import Annotated\nimport unittest\nimport os, logging, sys\n\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.user import User\nfrom vanilla_aiagents.team import Team\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n\nclass TestAgent(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM(\n            {\n                \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n                \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n                \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n                \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n            }\n        )\n\n        logging.basicConfig(level=logging.INFO)\n        logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.llm\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.team\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.workflow\").setLevel(logging.DEBUG)\n\n    def test_workflow_run_streaming(self):\n        agent1 = Agent(\n            id=\"agent1\",\n            llm=self.llm,\n            description=\"Call this agent for general purpose questions\",\n            system_message=\"\"\"You are an AI assistant\n            Your task is to help the user with their questions.\n            Always respond with the best answer you can generate.\n            If you don't know the answer, respond with \"I don't know\".\n            Always be polite and helpful.\n            \"\"\",\n        )\n\n        @agent1.register_tool(\n            description=\"Get the order ID for a customer's order. Call this whenever you need to know the order ID, for example when a customer asks 'Where is my package'\"\n        )\n        def get_order_id() -> Annotated[str, \"The customer's order ID.\"]:\n            return \"214\"\n\n        @agent1.register_tool(\n            description=\"Get the delivery date for a customer's order. Call this whenever you need to know the delivery date, for example when a customer asks 'Where is my package'\"\n        )\n        def get_delivery_date(\n            order_id: Annotated[str, \"The customer's order ID.\"]\n        ) -> Annotated[str, \"The delivery date for the customer's order.\"]:\n            return \"2022-01-01\"\n\n        user = User(id=\"user\", mode=\"unattended\")\n\n        team = Team(\n            id=\"team\",\n            description=\"\",\n            members=[agent1, user],\n            llm=self.llm,\n            stop_callback=lambda conv: len(conv.messages) > 4,\n        )\n        workflow = Workflow(askable=team)\n\n        stack_count = 0\n        for mark, content in workflow.run_stream(\n            \"Which is expected delivery date for my order? Use format like February 1, 2022\"\n        ):\n            logging.debug(f\"Mark: {mark}, Content: {content}\")\n            if mark == \"start\":\n                stack_count += 1\n                if stack_count == 1:\n                    self.assertEqual(\n                        content,\n                        team.id,\n                        f\"Expected {team.id} for start mark at stack count 1\",\n                    )\n                if stack_count == 2:\n                    self.assertEqual(\n                        content,\n                        agent1.id,\n                        f\"Expected {agent1.id} for start mark at stack count 2\",\n                    )\n            elif mark == \"end\":\n                stack_count -= 1\n                if stack_count == 1:\n                    self.assertEqual(\n                        content,\n                        agent1.id,\n                        f\"Expected {agent1.id} for end mark at stack count 1\",\n                    )\n                if stack_count == 0:\n                    self.assertEqual(\n                        content,\n                        team.id,\n                        f\"Expected {team.id} for end mark at stack count 0\",\n                    )\n                    break\n            elif mark == \"response\":\n                self.assertIn(\n                    \"January 1, 2022\",\n                    content[0][\"content\"],\n                    \"Expected delivery date not found in the response\",\n                )\n\n        self.assertIn(\n            \"January 1, 2022\",\n            workflow.conversation.messages[-1][\"content\"],\n            \"Expected delivery date not found in the conversation messages\",\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_grpc.py", "content": "import unittest\nimport os, logging, sys\n\nimport grpc\nfrom grpc_reflection.v1alpha import reflection\nfrom grpc_reflection.v1alpha.reflection_pb2 import ServerReflectionRequest\nfrom grpc_reflection.v1alpha.reflection_pb2_grpc import ServerReflectionStub\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom vanilla_aiagents.conversation import Conversation\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.llm import AzureOpenAILLM\nfrom vanilla_aiagents.remote.remote import RemoteAskable\nfrom vanilla_aiagents.remote.grpc import GRPCHost, GRPCConnection\n\nfrom dotenv import load_dotenv\nload_dotenv(override=True)\n\nclass TestGRPC(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM({\n            \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n            \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n            \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n        })\n        \n        logging.basicConfig(level=logging.INFO)\n        logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.llm\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.remote\").setLevel(logging.DEBUG)\n        \n        self.agent1 = Agent(id=\"agent1\", llm=self.llm, description=\"Call this agent for general purpose questions\", system_message = \"\"\"You are an AI assistant\n        Your task is to help the user with their questions.\n        Always respond with the best answer you can generate.\n        If you don't know the answer, respond with \"I don't know\".\n        Always be polite and helpful.\n        \"\"\")\n        \n        self.agent2 = Agent(id=\"agent2\", llm=self.llm, description=\"Call this agent for general purpose questions\", system_message = \"\"\"You are an AI assistant\n        Your task is to help the user with their questions.\n        Always respond with the best answer you can generate.\n        If you don't know the answer, respond with \"I don't know\".\n        Always be polite and helpful.\n        \"\"\")\n        \n    def test_grpc(self):\n        \n        host = GRPCHost(askables=[self.agent1, self.agent2], host=\"localhost\", port=5002)\n        self.assertCountEqual([self.agent1, self.agent2], host.askables, \"Expected askables to be the same\")\n        \n        host.start()\n        \n        connection = GRPCConnection(url=\"localhost:5002\")\n        remote = RemoteAskable(id=\"agent1\", connection=connection)\n        workflow = Workflow(askable=remote, conversation=Conversation())\n        \n        self.assertEqual(self.agent1.id, workflow.askable.id, \"Expected askable ID to be agent1\")\n        self.assertEqual(self.agent1.description, workflow.askable.description, \"Expected askable description to be the same as agent1\")\n        \n        workflow.restart()\n        workflow.run(\"Which is the capital of France?\")\n        \n        host.stop()\n        \n        # Note name in RemoteAskable is the one set in the response\n        self.assertEqual(workflow.conversation.messages[-1][\"name\"], \"agent1\", \"Expected agent to respond with greeting\")\n        self.assertEqual(workflow.conversation.messages[-1][\"name\"], \"agent1\", \"Expected agent to respond\")\n        self.assertIn(\"Paris\", workflow.conversation.messages[-1][\"content\"], \"Expected agent to respond 'Paris'\")\n    \n    def test_grpc_streaming(self):\n        \n        host = GRPCHost(askables=[self.agent1, self.agent2], host=\"localhost\", port=5003)\n        self.assertCountEqual([self.agent1, self.agent2], host.askables, \"Expected askables to be the same\")\n        \n        host.start()\n        \n        connection = GRPCConnection(url=\"localhost:5003\")\n        remote = RemoteAskable(id=\"agent1\", connection=connection)\n        workflow = Workflow(askable=remote, conversation=Conversation())\n        \n        self.assertEqual(self.agent1.id, workflow.askable.id, \"Expected askable ID to be agent1\")\n        self.assertEqual(self.agent1.description, workflow.askable.description, \"Expected askable description to be the same as agent1\")\n        \n        workflow.restart()\n        result = None\n        for mark, content in workflow.run_stream(\"Which is the capital of France?\"):\n            if mark == \"response\":\n                result = content[0]\n                break\n            if mark == \"error\":\n                break\n            \n        self.assertIn(\"Paris\", result[\"content\"],\"Expected agent to respond 'Paris'\")\n        \n        host.stop()\n        \n    def test_grpc_reflection(self):\n        host = GRPCHost(askables=[self.agent1, self.agent2], host=\"localhost\", port=5004)\n        host.start()\n\n        # Create a gRPC channel and reflection stub\n        channel = grpc.insecure_channel(\"localhost:5004\")\n        stub = ServerReflectionStub(channel)\n\n        # Request the list of services\n        request = ServerReflectionRequest(list_services=\"\")\n        response = stub.ServerReflectionInfo(iter([request]))\n\n        services = []\n        for resp in response:\n            services.extend(resp.list_services_response.service)\n\n        # Check if the RemoteService is in the list of services\n        service_names = [service.name for service in services]\n        self.assertIn(\"remote.grpc.RemoteService\", service_names, \"Expected RemoteService to be in the list of services\")\n\n        host.stop()\n\n\nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "vanilla_aiagents/tests/test_user.py", "content": "import unittest\nimport os, logging, sys\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n\nfrom vanilla_aiagents.workflow import Workflow\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.user import User\nfrom vanilla_aiagents.team import Team\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n\nclass TestUser(unittest.TestCase):\n\n    def setUp(self):\n        self.llm = AzureOpenAILLM(\n            {\n                \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n                \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n                \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n                \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n            }\n        )\n\n        logging.basicConfig(level=logging.INFO)\n        logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.user\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n        logging.getLogger(\"vanilla_aiagents.team\").setLevel(logging.DEBUG)\n\n    def test_user(self):\n        agent1 = Agent(\n            id=\"agent\",\n            llm=self.llm,\n            description=\"Call this agent to answer questions by the user\",\n            system_message=\"\"\"You are an AI assistant\n        Your task is to help the user with their questions.\n        Always respond with the best answer you can generate.\n        If you don't know the answer, respond with \"I don't know\".\n        Always be polite and helpful.\n        \"\"\",\n        )\n\n        user = User(id=\"user\", mode=\"unattended\")\n\n        flow = Team(\n            id=\"team\",\n            description=\"\",\n            members=[agent1, user],\n            llm=self.llm,\n            stop_callback=lambda conv: len(conv.messages) > 6,\n        )\n        workflow = Workflow(askable=flow)\n\n        workflow.run(\"Hi! Can you help me with capital cities?\")\n\n        self.assertEqual(\n            len(workflow.conversation.messages),\n            3,\n            \"Expected 3 messages in the conversation\",\n        )\n        self.assertEqual(\n            workflow.conversation.messages[-1][\"name\"],\n            \"agent\",\n            \"Expected agent to respond with greeting\",\n        )\n\n        workflow.run(\"Which is the capital of France?\")\n        self.assertEqual(\n            len(workflow.conversation.messages),\n            5,\n            \"Expected 5 messages in the conversation\",\n        )\n        self.assertEqual(\n            workflow.conversation.messages[-1][\"name\"],\n            \"agent\",\n            \"Expected agent to respond\",\n        )\n        self.assertIn(\n            \"Paris\",\n            workflow.conversation.messages[-1][\"content\"],\n            \"Expected agent to respond 'Paris'\",\n        )\n\n    def test_user_interactive(self):\n        agent1 = Agent(\n            id=\"agent\",\n            llm=self.llm,\n            description=\"Call this agent to answer questions by the user\",\n            system_message=\"\"\"You are an AI assistant\n        Your task is to help the user with their questions.\n        Always respond with the best answer you can generate.\n        If you don't know the answer, respond with \"I don't know\".\n        Always be polite and helpful.\n        \"\"\",\n        )\n\n        def fake_input(prompt):\n            return \"Which is the capital of France?\"\n\n        user = User(id=\"user\", mode=\"interactive\", interaction_function=fake_input)\n\n        # Must force the stop callback to be triggered by the number of messages, otherwise the test will hang\n        flow = Team(\n            id=\"team\",\n            description=\"\",\n            members=[agent1, user],\n            llm=self.llm,\n            stop_callback=lambda conv: len(conv.messages) == 5,\n        )\n        workflow = Workflow(askable=flow)\n\n        workflow.restart()\n        workflow.run(\"Hi! Can you help me with capital cities?\")\n\n        self.assertEqual(len(workflow.conversation.messages), 5, \"Expected 5 messages\")\n        self.assertEqual(\n            workflow.conversation.messages[-1][\"name\"],\n            \"agent\",\n            \"Expected agent to respond\",\n        )\n        self.assertIn(\n            \"Paris\",\n            workflow.conversation.messages[-1][\"content\"],\n            \"Expected agent to respond 'Paris'\",\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"type": "source_file", "path": "tasks.py", "content": "from invoke import task\n\n\n@task\ndef build(c, version: str):\n    c.run(\n        f\"cd vanilla_aiagents && python setup.py sdist bdist_wheel --version {version}\"\n    )\n\n\n@task\ndef test(c, test_path: str = \"vanilla_aiagents/tests/\", test_case: str = \"\"):\n    test_case_option = f\"-k {test_case}\" if test_case else \"\"\n    coverage_option = \"--cov-append\" if test_case else \"\"\n\n    # Run tests with coverage\n    c.run(\n        f\"pytest --cov=vanilla_aiagents --cov-report=term-missing --cov-report=html {coverage_option} --cov-config=.coveragerc {test_case_option} {test_path}\"\n    )\n\n    # Generate the combined coverage report\n    c.run(\"coverage html\")\n    c.run(\"coverage xml\")\n    c.run(\"python vanilla_aiagents/tests/generate_coverage_badge.py\")\n\n\n@task\ndef build_grpc(c):\n    proto_path = \"vanilla_aiagents/vanilla_aiagents/remote\"\n    proto_file = f\"{proto_path}/remote.proto\"\n    c.run(\n        f\"python -m grpc_tools.protoc -I{proto_path} --python_out={proto_path} --grpc_python_out={proto_path} {proto_file}\"\n    )\n\n\n@task\ndef docs(c):\n    c.run(\n        \"cd vanilla_aiagents && pdoc --output-dir docs -d markdown --logo https://raw.githubusercontent.com/Azure-Samples/vanilla-aiagents/main/logo.png vanilla_aiagents !vanilla_aiagents.remote.grpc\"\n    )\n\n\n@task\ndef check_lint(c):\n    c.run(\"cd vanilla_aiagents && flake8 vanilla_aiagents/\")\n\n\n@task\ndef check_docs(c):\n    c.run(\"cd vanilla_aiagents && pydocstyle vanilla_aiagents/\")\n\n\n@task\ndef lint(c):\n    c.run(\"cd vanilla_aiagents && black vanilla_aiagents/\")\n    c.run(\n        \"cd vanilla_aiagents && docformatter -i -r -s pep257 --black vanilla_aiagents/\"\n    )\n"}
{"type": "source_file", "path": "samples/remote/actor/run.py", "content": "import sys\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\nsys.path.append(os.path.abspath(os.path.join(\"../../../vanilla_aiagents\")))\n\nfrom vanilla_aiagents.remote.dapr.run_actors import main\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/askable.py", "content": "# A common Python interface for both Agent and Team\nfrom abc import ABC, abstractmethod\n\nfrom .conversation import Conversation\n\n\nclass Askable(ABC):\n    \"\"\"A common interface for both Agent and Team.\"\"\"\n\n    @abstractmethod\n    def ask(self, conversation: Conversation, stream=False) -> str:\n        pass\n\n    def __init__(self, id: str, description: str):\n        \"\"\"Initialize the Askable object.\n\n        Args:\n            id (str): The ID of the Askable object. Will be used to uniquely identify it.\n            description (str): The description of the Askable object. Typically used by orchestrators.\n        \"\"\"\n        self._id = id\n        self._description = description\n\n    # an id property with a default implementation\n    @property\n    def id(self):\n        return self._id\n\n    @id.setter\n    def id(self, value):\n        self._id = value\n\n    # a description property with a default implementation\n    @property\n    def description(self):\n        return self._description\n\n    @description.setter\n    def description(self, value):\n        self._description = value\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/remote/dapr/__init__.py", "content": ""}
{"type": "source_file", "path": "samples/remote/actor/_actor_askable.py", "content": "from typing import Annotated\nimport os\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nllm = AzureOpenAILLM(\n    {\n        \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n        \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n        \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n        \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n    }\n)\n\n_actor_askable = Agent(\n    id=\"agent\",\n    llm=llm,\n    description=\"Call this agent to play guess the number game with the use\",\n    system_message=\"\"\"You are an AI assistant\n        Your task is to play a game with the user.\n        You first generate a random number between 1 and 100. Then save it as a conversation variable named \"number\".\n        The user will try to guess the number.\n        If the user's guess is too high, respond with \"Too high\".\n        If the user's guess is too low, respond with \"Too low\".\n        \"\"\",\n)\n\n\n@_actor_askable.register_tool(description=\"Generate a random number\")\ndef random() -> Annotated[str, \"A random number\"]:\n    import random\n\n    return str(random.randint(1, 100))\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/planned_team.py", "content": "import json\nfrom typing import Annotated, Callable\n\nfrom pydantic import BaseModel\n\nfrom .conversation import Conversation, ConversationReadingStrategy\nfrom .askable import Askable\nfrom .llm import LLM\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass PlannedTeam(Askable):\n    \"\"\"A team of Askable that executes a plan to solve the user inquiry by using the available agents.\n\n    Unlike the Team class, the PlannedTeam class does not decide which agent to ask next based on the conversation context but instead follows a pre-defined plan, evaluated upfront.\n\n    Args:\n        llm (LLM): The language model to use for the decision-making process.\n        description (str): The description of the team.\n        id (str): The unique identifier of the team.\n        members (list[Askable]): The agents that are part of the team.\n        fork_conversation (bool): Whether to fork the conversation and avoid writing the messages to the main conversation.\n        fork_strategy (ConversationReadingStrategy): The reading strategy to use to select the messages to report output back to the main conversation.\n        include_tools_descriptions (bool): Whether to include the tools descriptions in the system prompt to help the orchestrator decide.\n        repeat_until (Callable[[Conversation], bool]): A function to check if the plan should be repeated.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: LLM,\n        description: str,\n        id: str,\n        members: list[Askable],\n        fork_conversation: bool = False,\n        fork_strategy: ConversationReadingStrategy = None,\n        include_tools_descriptions: bool = False,\n        repeat_until: Callable[[Conversation], bool] = None,\n        feedback_variable: str = \"__feedback\",\n    ):\n        \"\"\"Initialize the PlannedTeam object.\n\n        Args:\n            llm (LLM): The language model to use for the decision-making process.\n            description (str): The description of the PlannedTeam object. Typically used by orchestrators.\n            id (str): The ID of the PlannedTeam object. Will be used to uniquely identify it.\n            members (list[Askable]): The agents that are part of the team.\n            fork_conversation (bool): Whether to fork the conversation and avoid writing the messages to the main conversation.\n            fork_strategy (ConversationReadingStrategy): The reading strategy to use to select the messages to report output back to the main conversation.\n            include_tools_descriptions (bool): Whether to include the tools descriptions in the system prompt to help the orchestrator decide.\n            repeat_until (Callable[[Conversation], bool]): A function to check if the plan should be repeated.\n            feedback_variable (str): The variable name to use to store the feedback from the previous plan execution.\n        \"\"\"\n        super().__init__(id, description)\n        self.agents = members\n        self.plan = None\n        self.fork_conversation = fork_conversation\n        self.fork_strategy = fork_strategy\n        self.include_tools_descriptions = include_tools_descriptions\n        self.repeat_until = repeat_until\n        self.feedback_variable = feedback_variable\n\n        self.current_agent = None\n        self.agents_dict = {agent.id: agent for agent in members}\n\n        self.llm = llm\n\n        logger.debug(\n            \"[PlannedTeam %s] initialized with agents: %s\", self.id, self.agents_dict\n        )\n\n    def ask(self, conversation: Conversation, stream=False):\n        \"\"\"Ask the team to solve the user inquiry by executing the pre-defined plan.\n\n        This method will execute the plan by asking each agent in the plan in order. If the stop_callback is triggered, the execution will stop.\n\n        Args:\n            conversation (Conversation): The conversation to use for the execution. If fork_conversation is set to True, a forked conversation will be used and the messages will be written to the main conversation only at the end (depending on the fork_strategy).\n            stream (bool): Whether to stream the conversation updates.\n        \"\"\"\n        # TODO persist plan in a conversation variable and read it from there\n        # to support resuming plans\n        if self.plan is None:\n            self.plan = self._create_plan(conversation)\n            logger.debug(\"[PlannedTeam %s] created plan: %s\", self.id, self.plan)\n\n        execution_result = \"done\"\n        local_conversation = (\n            conversation.fork() if self.fork_conversation else conversation\n        )\n\n        if stream:\n            conversation.update([\"start\", self.id])\n        for step in self.plan:\n            self.current_agent = self.agents_dict[step.agent_id]\n            logger.debug(\n                \"[PlannedTeam %s] current agent: %s\", self.id, self.current_agent.id\n            )\n\n            # TODO check behavior\n            local_conversation.messages.append(\n                {\"role\": \"assistant\", \"name\": self.id, \"content\": step.instructions}\n            )\n\n            agent_result = self.current_agent.ask(local_conversation, stream=stream)\n            logger.debug(\n                \"[PlannedTeam %s] asked current agent with messages: %s\",\n                self.id,\n                agent_result,\n            )\n\n            if agent_result == \"stop\":\n                logger.debug(\n                    \"[PlannedTeam %s] stop signal received, ending workflow.\", self.id\n                )\n                conversation.log.append((\"info\", \"plannedteam/stop\", self.id))\n                execution_result = \"agent-stop\"\n                break\n            elif agent_result == \"error\":\n                logger.error(\n                    \"[PlannedTeam %s] error signal received, ending workflow.\", self.id\n                )\n                conversation.log.append((\"error\", \"plannedteam/error\", self.id))\n                execution_result = \"agent-error\"\n                break\n\n        if self.repeat_until is not None:\n            while not self.repeat_until(local_conversation):\n                execution_result = self.ask(local_conversation, stream=stream)\n\n        if stream:\n            local_conversation.update([\"end\", self.id])\n\n        if self.fork_conversation:\n            conversation.messages.extend(\n                self.fork_strategy.get_messages(local_conversation)\n            )\n\n        return execution_result\n\n    def _create_plan(self, conversation: Conversation):\n        system_prompt = \"\"\"\nYou are a team orchestrator that must create a plan to solve the user inquiry by using the available agents.\nYour task is to create a plan that includes only the agents suitable to help, based on their descriptions.\nThe plan must be a list of agent_id values, in the order they should be executed, along with the proper instructions for each agent.\nWhen FEEDBACK section has content, you must consider it to tailor the plan accordingly, since this means a previous plan was not successful to meet success criteria.\nThe plan must be returned as JSON, with the following structure:\n\n{{\n    \"plan\": [\n        {{\n            \"agent_id\": \"agent_id\",\n            \"instructions\": \"instructions\"\n        }},\n        ...\n    ]\n}}\n\nYou MUST return the plan in the format specified above. DO NOT return anything else.\n\n# AVAILABLE AGENTS\n{agents}\n\n# INQUIRY\n{inquiry}\n\n# FEEDBACK\n{feedback}\n\nBE SURE TO READ AGAIN THE INSTUCTIONS ABOVE BEFORE PROCEEDING.\n\"\"\"\n        local_messages = []\n        agents_info = self._generate_agents_info()\n        inquiry = conversation.messages[-1][\n            \"content\"\n        ]  # TODO pick the first user message\n        feedback = conversation.variables.get(self.feedback_variable, \"\")\n\n        local_messages.append(\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt.format(\n                    agents=agents_info, inquiry=inquiry, feedback=feedback\n                ),\n            }\n        )\n        local_messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": \"Define the plan based on the provided agents and the inquiry.\",\n            }\n        )\n\n        # logger.debug(\"[Team %s] messages for selecting next agent: %s\", self.id, local_messages)\n\n        result, usage = self.llm.ask(messages=local_messages, response_format=TeamPlan)\n        logger.debug(\"[PlannedTeam %s] result from Azure OpenAI: %s\", self.id, result)\n        if self.llm.constraints.structured_output:\n            plan = result.parsed\n        else:\n            plan = result.content.replace(\"```json\", \"\").replace(\"```\", \"\")\n            plan = json.loads(plan)\n\n        output = TeamPlan.model_validate(plan)\n\n        if usage is not None:\n            # Update conversation metrics with response usage\n            conversation.metrics.total_tokens += usage[\"total_tokens\"]\n            conversation.metrics.prompt_tokens += usage[\"prompt_tokens\"]\n            conversation.metrics.completion_tokens += usage[\"completion_tokens\"]\n\n        return output.plan\n\n    def _generate_agents_info(self):\n        agents_info = []\n        for agent in self.agents:\n            tools = []\n            if self.include_tools_descriptions and hasattr(agent, \"tools\"):\n                for tool in agent.tools:\n                    tool_name = tool[\"function\"][\"name\"]\n                    tool_description = tool[\"function\"][\"description\"]\n                    tools.append(f\"    - tool '{tool_name}': {tool_description}\")\n            tools_str = \"\\n\".join(tools)\n\n            agent_info = f\"- agent_id: {agent.id}\\n    - description: {agent.description}\\n{tools_str}\\n\\n\"\n            agents_info.append(agent_info)\n\n        return \"\\n\".join(agents_info)\n\n\nclass TeamPlanStep(BaseModel):\n    agent_id: Annotated[str, \"The agent_id of the agent to execute\"]\n    instructions: Annotated[str, \"The instructions for the agent\"]\n\n\nclass TeamPlan(BaseModel):\n    plan: Annotated[list[TeamPlanStep], \"The plan to be executed by the team\"]\n"}
{"type": "source_file", "path": "notebooks/log_utils.py", "content": "import logging\nimport logging.config\nimport json\nimport os\n\ndef setup_logging(\n    default_path='logging_config.json',\n    default_level=logging.INFO,\n    env_key='LOGGING_CONFIG_PATH'\n):\n    \"\"\"Setup logging configuration\"\"\"\n    path = default_path\n    value = os.getenv(env_key, None)\n    if value:\n        path = value\n    if os.path.exists(path):\n        with open(path, 'rt') as f:\n            config = json.load(f)\n        logging.config.dictConfig(config)\n    else:\n        logging.basicConfig(level=default_level)\n\nsetup_logging()"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/function_utils.py", "content": "import functools\nimport inspect\nimport json\nfrom logging import getLogger\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    ForwardRef,\n    List,\n    Optional,\n    Set,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n)\n\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import Annotated, Literal, get_args, get_origin\n\nfrom ._pydantic import (\n    JsonSchemaValue,\n    evaluate_forwardref,\n    model_dump,\n    model_dump_json,\n    type2schema,\n)\n\nlogger = getLogger(__name__)\n\nT = TypeVar(\"T\")\nF = TypeVar(\"F\", bound=Callable[..., Any])\n\n\ndef get_typed_annotation(annotation: Any, globalns: Dict[str, Any]) -> Any:\n    \"\"\"Get the type annotation of a parameter.\n\n    Args:\n        annotation: The annotation of the parameter\n        globalns: The global namespace of the function\n\n    Returns:\n        The type annotation of the parameter\n    \"\"\"\n    if isinstance(annotation, str):\n        annotation = ForwardRef(annotation)\n        annotation = evaluate_forwardref(annotation, globalns, globalns)\n    return annotation\n\n\ndef get_typed_signature(call: Callable[..., Any]) -> inspect.Signature:\n    \"\"\"Get the signature of a function with type annotations.\n\n    Args:\n        call: The function to get the signature for\n\n    Returns:\n        The signature of the function with type annotations\n    \"\"\"\n    signature = inspect.signature(call)\n    globalns = getattr(call, \"__globals__\", {})\n    typed_params = [\n        inspect.Parameter(\n            name=param.name,\n            kind=param.kind,\n            default=param.default,\n            annotation=get_typed_annotation(param.annotation, globalns),\n        )\n        for param in signature.parameters.values()\n    ]\n    typed_signature = inspect.Signature(typed_params)\n    return typed_signature\n\n\ndef get_typed_return_annotation(call: Callable[..., Any]) -> Any:\n    \"\"\"Get the return annotation of a function.\n\n    Args:\n        call: The function to get the return annotation for\n\n    Returns:\n        The return annotation of the function\n    \"\"\"\n    signature = inspect.signature(call)\n    annotation = signature.return_annotation\n\n    if annotation is inspect.Signature.empty:\n        return None\n\n    globalns = getattr(call, \"__globals__\", {})\n    return get_typed_annotation(annotation, globalns)\n\n\ndef get_param_annotations(\n    typed_signature: inspect.Signature,\n) -> Dict[str, Union[Annotated[Type[Any], str], Type[Any]]]:\n    \"\"\"Get the type annotations of the parameters of a function.\n\n    Args:\n        typed_signature: The signature of the function with type annotations\n\n    Returns:\n        A dictionary of the type annotations of the parameters of the function\n    \"\"\"\n    return {\n        k: v.annotation\n        for k, v in typed_signature.parameters.items()\n        if v.annotation is not inspect.Signature.empty\n    }\n\n\nclass Parameters(BaseModel):\n    \"\"\"Parameters of a function as defined by the OpenAI API.\"\"\"\n\n    type: Literal[\"object\"] = \"object\"\n    properties: Dict[str, JsonSchemaValue]\n    required: List[str]\n\n\nclass Function(BaseModel):\n    \"\"\"A function as defined by the OpenAI API.\"\"\"\n\n    description: Annotated[str, Field(description=\"Description of the function\")]\n    name: Annotated[str, Field(description=\"Name of the function\")]\n    parameters: Annotated[Parameters, Field(description=\"Parameters of the function\")]\n\n\nclass ToolFunction(BaseModel):\n    \"\"\"A function under tool as defined by the OpenAI API.\"\"\"\n\n    type: Literal[\"function\"] = \"function\"\n    function: Annotated[Function, Field(description=\"Function under tool\")]\n\n\ndef get_parameter_json_schema(\n    k: str, v: Any, default_values: Dict[str, Any]\n) -> JsonSchemaValue:\n    \"\"\"Get a JSON schema for a parameter as defined by the OpenAI API.\n\n    Args:\n        k: The name of the parameter\n        v: The type of the parameter\n        default_values: The default values of the parameters of the function\n\n    Returns:\n        A Pydanitc model for the parameter\n    \"\"\"\n\n    def type2description(k: str, v: Union[Annotated[Type[Any], str], Type[Any]]) -> str:\n        # handles Annotated\n        if hasattr(v, \"__metadata__\"):\n            retval = v.__metadata__[0]\n            if isinstance(retval, str):\n                return retval\n            else:\n                raise ValueError(\n                    f\"Invalid description {retval} for parameter {k}, should be a string.\"\n                )\n        else:\n            return k\n\n    schema = type2schema(v)\n    if k in default_values:\n        dv = default_values[k]\n        schema[\"default\"] = dv\n\n    schema[\"description\"] = type2description(k, v)\n\n    return schema\n\n\ndef get_required_params(typed_signature: inspect.Signature) -> List[str]:\n    \"\"\"Get the required parameters of a function.\n\n    Args:\n        signature: The signature of the function as returned by inspect.signature\n\n    Returns:\n        A list of the required parameters of the function\n    \"\"\"\n    return [\n        k\n        for k, v in typed_signature.parameters.items()\n        if v.default == inspect.Signature.empty\n    ]\n\n\ndef get_default_values(typed_signature: inspect.Signature) -> Dict[str, Any]:\n    \"\"\"Get default values of parameters of a function.\n\n    Args:\n        signature: The signature of the function as returned by inspect.signature\n\n    Returns:\n        A dictionary of the default values of the parameters of the function\n    \"\"\"\n    return {\n        k: v.default\n        for k, v in typed_signature.parameters.items()\n        if v.default != inspect.Signature.empty\n    }\n\n\ndef get_parameters(\n    required: List[str],\n    param_annotations: Dict[str, Union[Annotated[Type[Any], str], Type[Any]]],\n    default_values: Dict[str, Any],\n) -> Parameters:\n    \"\"\"Get the parameters of a function as defined by the OpenAI API.\n\n    Args:\n        required: The required parameters of the function\n        hints: The type hints of the function as returned by typing.get_type_hints\n\n    Returns:\n        A Pydantic model for the parameters of the function\n    \"\"\"\n    return Parameters(\n        properties={\n            k: get_parameter_json_schema(k, v, default_values)\n            for k, v in param_annotations.items()\n            if v is not inspect.Signature.empty\n        },\n        required=required,\n    )\n\n\ndef get_missing_annotations(\n    typed_signature: inspect.Signature, required: List[str]\n) -> Tuple[Set[str], Set[str]]:\n    \"\"\"Get the missing annotations of a function.\n\n    Ignores the parameters with default values as they are not required to be annotated, but logs a warning.\n    Args:\n        typed_signature: The signature of the function with type annotations\n        required: The required parameters of the function\n\n    Returns:\n        A set of the missing annotations of the function\n    \"\"\"\n    all_missing = {\n        k\n        for k, v in typed_signature.parameters.items()\n        if v.annotation is inspect.Signature.empty\n    }\n    missing = all_missing.intersection(set(required))\n    unannotated_with_default = all_missing.difference(missing)\n    return missing, unannotated_with_default\n\n\ndef get_function_schema(\n    f: Callable[..., Any], *, name: Optional[str] = None, description: str\n) -> Dict[str, Any]:\n    \"\"\"\n    Get a JSON schema for a function as defined by the OpenAI API.\n\n    Args:\n        f: The function to get the JSON schema for\n        name: The name of the function\n        description: The description of the function\n\n    Returns:\n        A JSON schema for the function\n\n    Raises:\n        TypeError: If the function is not annotated\n\n    Examples:\n\n    ```python\n    def f(a: Annotated[str, \"Parameter a\"], b: int = 2, c: Annotated[float, \"Parameter c\"] = 0.1) -> None:\n        pass\n\n    get_function_schema(f, description=\"function f\")\n\n    #   {'type': 'function',\n    #    'function': {'description': 'function f',\n    #        'name': 'f',\n    #        'parameters': {'type': 'object',\n    #           'properties': {'a': {'type': 'str', 'description': 'Parameter a'},\n    #               'b': {'type': 'int', 'description': 'b'},\n    #               'c': {'type': 'float', 'description': 'Parameter c'}},\n    #           'required': ['a']}}}\n    ```\n    \"\"\"\n    typed_signature = get_typed_signature(f)\n    required = get_required_params(typed_signature)\n    default_values = get_default_values(typed_signature)\n    param_annotations = get_param_annotations(typed_signature)\n    return_annotation = get_typed_return_annotation(f)\n    missing, unannotated_with_default = get_missing_annotations(\n        typed_signature, required\n    )\n\n    if return_annotation is None:\n        logger.warning(\n            f\"The return type of the function '{f.__name__}' is not annotated. Although annotating it is \"\n            + \"optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.\"\n        )\n\n    if unannotated_with_default != set():\n        unannotated_with_default_s = [\n            f\"'{k}'\" for k in sorted(unannotated_with_default)\n        ]\n        logger.warning(\n            f\"The following parameters of the function '{f.__name__}' with default values are not annotated: \"\n            + f\"{', '.join(unannotated_with_default_s)}.\"\n        )\n\n    if missing != set():\n        missing_s = [f\"'{k}'\" for k in sorted(missing)]\n        raise TypeError(\n            f\"All parameters of the function '{f.__name__}' without default values must be annotated. \"\n            + f\"The annotations are missing for the following parameters: {', '.join(missing_s)}\"\n        )\n\n    fname = name if name else f.__name__\n\n    parameters = get_parameters(\n        required, param_annotations, default_values=default_values\n    )\n\n    function = ToolFunction(\n        function=Function(\n            description=description,\n            name=fname,\n            parameters=parameters,\n        )\n    )\n\n    return model_dump(function)\n\n\ndef get_load_param_if_needed_function(\n    t: Any,\n) -> Optional[Callable[[Dict[str, Any], Type[BaseModel]], BaseModel]]:\n    \"\"\"Get a function to load a parameter if it is a Pydantic model.\n\n    Args:\n        t: The type annotation of the parameter\n\n    Returns:\n        A function to load the parameter if it is a Pydantic model, otherwise None\n    \"\"\"\n    if get_origin(t) is Annotated:\n        return get_load_param_if_needed_function(get_args(t)[0])\n\n    def load_base_model(v: Dict[str, Any], t: Type[BaseModel]) -> BaseModel:\n        return t(**v)\n\n    return load_base_model if isinstance(t, type) and issubclass(t, BaseModel) else None\n\n\ndef load_basemodels_if_needed(func: Callable[..., Any]) -> Callable[..., Any]:\n    \"\"\"A decorator to load the parameters of a function if they are Pydantic models.\n\n    Args:\n        func: The function with annotated parameters\n\n    Returns:\n        A function that loads the parameters before calling the original function\n    \"\"\"\n    # get the type annotations of the parameters\n    typed_signature = get_typed_signature(func)\n    param_annotations = get_param_annotations(typed_signature)\n\n    # get functions for loading BaseModels when needed based on the type annotations\n    kwargs_mapping_with_nones = {\n        k: get_load_param_if_needed_function(t) for k, t in param_annotations.items()\n    }\n\n    # remove the None values\n    kwargs_mapping = {\n        k: f for k, f in kwargs_mapping_with_nones.items() if f is not None\n    }\n\n    # a function that loads the parameters before calling the original function\n    @functools.wraps(func)\n    def _load_parameters_if_needed(*args: Any, **kwargs: Any) -> Any:\n        # load the BaseModels if needed\n        for k, f in kwargs_mapping.items():\n            kwargs[k] = f(kwargs[k], param_annotations[k])\n\n        # call the original function\n        return func(*args, **kwargs)\n\n    @functools.wraps(func)\n    async def _a_load_parameters_if_needed(*args: Any, **kwargs: Any) -> Any:\n        # load the BaseModels if needed\n        for k, f in kwargs_mapping.items():\n            kwargs[k] = f(kwargs[k], param_annotations[k])\n\n        # call the original function\n        return await func(*args, **kwargs)\n\n    if inspect.iscoroutinefunction(func):\n        return _a_load_parameters_if_needed\n    else:\n        return _load_parameters_if_needed\n\n\ndef serialize_to_str(x: Any) -> str:\n    if isinstance(x, str):\n        return x\n    elif isinstance(x, BaseModel):\n        return model_dump_json(x)\n    else:\n        return json.dumps(x, ensure_ascii=False)\n\n\ndef wrap_function(func: F) -> F:\n    \"\"\"Wrap the function to dump the return value to json.\n\n    Handles both sync and async functions.\n\n    Args:\n        func: the function to be wrapped.\n\n    Returns:\n        The wrapped function.\n    \"\"\"\n\n    @load_basemodels_if_needed\n    @functools.wraps(func)\n    def _wrapped_func(*args, **kwargs):\n        retval = func(*args, **kwargs)\n        # if logging_enabled():\n        #     log_function_use(self, func, kwargs, retval)\n        return serialize_to_str(retval)\n\n    @load_basemodels_if_needed\n    @functools.wraps(func)\n    async def _a_wrapped_func(*args, **kwargs):\n        retval = await func(*args, **kwargs)\n        # if logging_enabled():\n        #     log_function_use(self, func, kwargs, retval)\n        return serialize_to_str(retval)\n\n    wrapped_func = (\n        _a_wrapped_func if inspect.iscoroutinefunction(func) else _wrapped_func\n    )\n\n    # needed for testing\n    wrapped_func._origin = func\n\n    return wrapped_func\n"}
{"type": "source_file", "path": "vanilla_aiagents/setup.py", "content": "from setuptools import setup, find_packages\nimport sys\n\n# Extract version from arguments named \"version\"\nif \"--version\" in sys.argv:\n    version_index = sys.argv.index(\"--version\") + 1\n    version = sys.argv[version_index]\n    # Remove the --version argument and its value from sys.argv\n    sys.argv.pop(version_index)\n    sys.argv.pop(version_index - 1)\nelse:\n    raise ValueError(\"Version not provided\")\n\n\nsetup(\n    name=\"vanilla_aiagents\",\n    version=version,\n    packages=find_packages(),\n    install_requires=[\n        \"openai\",\n        \"pydantic\",\n        \"azure-identity\",\n    ],\n    extras_require={\n        \"remote\": [\n            \"fastapi\",\n            \"uvicorn\",\n            \"starlette_gzip_request\",\n            \"grpcio-tools\",\n            \"grpcio-reflection\",\n            \"dapr-ext-fastapi>=1.14.0\",\n            \"cloudevents>=1.11.0\",\n        ],\n        \"extras\": [\"llmlingua\"],\n    },\n    entry_points={\n        \"console_scripts\": [],\n    },\n    author=\"Riccardo Chiodaroli\",\n    author_email=\"ricchi@microsoft.com\",\n    description=\"Sample package demonstrating how to create a simple agenting application without using any specific framework\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/azure-samples/genai-vanilla-agents\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires=\">=3.11\",\n)\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/coding_agent.py", "content": "from typing import Annotated\nfrom .llm import LLM\nfrom .conversation import AllMessagesStrategy, ConversationReadingStrategy\nfrom .agent import Agent\nimport subprocess\nimport os\nimport venv\n\n# Configure logging\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass LocalCodingAgent(Agent):\n    \"\"\"\n    A coding agent that can execute Python code snippets in a secure and controlled environment.\n\n    Args:\n        id (str): The unique identifier of the agent.\n        description (str): The description of the agent.\n        llm (LLM): The language model to use for the decision-making process.\n        reading_strategy (ConversationReadingStrategy): The reading strategy to use to select the messages to pass to the LLM.\n    \"\"\"\n\n    def __init__(\n        self,\n        id: str,\n        description: str,\n        llm: LLM,\n        reading_strategy: ConversationReadingStrategy = AllMessagesStrategy(),\n    ):\n        \"\"\"\n        Initialize the LocalCodingAgent.\n\n        Args:\n            id (str): The unique identifier of the agent.\n            description (str): The description of the agent.\n            llm (LLM): The language model to use for the decision-making process.\n            reading_strategy (ConversationReadingStrategy): The reading strategy to use to select the messages to pass to the LLM.\n        \"\"\"\n        system_message = \"\"\"\n        You are an expert Python developer.\n        Your task is to write a Python code snippet to solve a given problem.\n        ALWAYS adhere to the guidelines provided.\n\n        ## GUIDELINES\n        - You can use any Python libraries you need, but in case you must invoke the proper function call to install them first.\n        - The code MUST be written in Python.\n        - The code MUST be secure and efficient. NEVER use code that can be harmful.\n        - The code MUST AVOID any side effects, especially reading or writing to the file system.\n        - The code MUST NOT read or write files or directories outside of the code execution environment.\n        - If you cannot comply with these guidelines, please return an error message.\n\n        ## INSTRUCTIONS\n        - Initialize a Python virtual environment, and set a conversation variable \"venv_dir\" to the virtual environment directory.\n            Skip this step if the virtual environment is already initialized.\n        - Install the required Python packages, if any\n        - Write and run the Python code to solve the problem and return the output of the code execution\n\n        ## ADDITIONAL CONTEXT\n        __context__\n\n        \"\"\"\n        super().__init__(\n            id=id,\n            description=description,\n            system_message=system_message,\n            llm=llm,\n            reading_strategy=reading_strategy,\n        )\n\n        logger.debug(\n            f\"CodingAgent initialized with ID: {self.id}, Description: {self.description}\"\n        )\n        self.register_tool(description=\"Initializes a Python virtual environment\")(\n            init_venv\n        )\n        # self.register_tool(description=\"Cleans up the Python virtual environment\")(cleanup_venv)\n        self.register_tool(description=\"Installs the provided Python requirements\")(\n            install_dependencies\n        )\n        self.register_tool(description=\"Runs the provided Python code block\")(run_code)\n\n\ndef init_venv() -> Annotated[str, \"Python virtual environment directory\"]:\n    logger.info(\"Initializing virtual environment\")\n\n    # Create a directory for the virtual environment\n    venv_dir = os.path.join(os.getcwd(), \"LocalCodingAgent\", \"venv\")\n    logger.debug(f\"Creating virtual environment in {venv_dir}\")\n\n    if not os.path.exists(venv_dir):\n        os.makedirs(venv_dir)\n        venv.create(venv_dir, with_pip=True)\n\n    logger.info(\"Virtual environment initialized successfully\")\n    return venv_dir\n\n\n# def cleanup_venv(venv_dir: Annotated[str, \"Python virtual environment directory\"]) -> Annotated[str, \"Virtual environment cleanup output\"]:\n#     logger.info(\"Cleaning up virtual environment\")\n#     logger.debug(f\"Virtual environment directory: {venv_dir}\")\n\n#     if os.path.exists(venv_dir):\n#         logger.debug(f\"Removing virtual environment directory: {venv_dir}\")\n#         shutil.rmtree(venv_dir)\n#         logger.info(\"Virtual environment cleaned up successfully\")\n#         return \"success\"\n#     else:\n#         logger.warning(f\"Virtual environment directory not found: {venv_dir}\")\n#         return \"error\"\n\n\ndef install_dependencies(\n    venv_dir: Annotated[str, \"Python virtual environment directory\"],\n    requirements: Annotated[str, \"Python requirements in requirements.txt format\"],\n) -> Annotated[str, \"Python requirements installation output\"]:\n    logger.info(\"Starting installation of dependencies\")\n    logger.debug(f\"Requirements: {requirements}\")\n\n    # Install the required python packages in the virtual environment\n    try:\n        logger.debug(f\"Installing requirements from {requirements}\")\n        subprocess.check_call(\n            [os.path.join(venv_dir, \"scripts\", \"pip\"), \"install\", requirements]\n        )\n        logger.info(\"Dependencies installed successfully\")\n        return \"success\"\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Error running code: {e.output.decode('utf-8')}\")\n        return f\"Error installing dependencies:\\n{e.output.decode('utf-8')}\"\n\n\ndef run_code(\n    venv_dir: Annotated[str, \"Python virtual environment directory\"],\n    code: Annotated[str, \"Python code to run\"],\n) -> Annotated[str, \"Python code output\"]:\n    \"\"\"\n    Run the provided Python code in the local environment.\n\n    Args:\n        venv_dir (str): The Python virtual environment directory.\n        code (str): The Python code to run.\n    \"\"\"\n    logger.info(\"Starting execution of provided code\")\n    logger.debug(f\"Code: {code}\")\n\n    # Write the code to a temporary file\n    code_file = os.path.join(venv_dir, \"code.py\")\n    logger.debug(f\"Writing code to temporary file {code_file}\")\n\n    with open(code_file, \"w\") as f:\n        f.write(code)\n\n    # Run the provided python code in the virtual environment\n    try:\n        result = subprocess.check_output(\n            [os.path.join(venv_dir, \"scripts\", \"python\"), code_file],\n            stderr=subprocess.STDOUT,\n        )\n        logger.info(\"Code executed successfully\")\n        return result.decode(\"utf-8\")\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Error running code: {e.output.decode('utf-8')}\")\n        return f\"Error executing code:\\n{e.output.decode('utf-8')}\"\n"}
{"type": "source_file", "path": "samples/remote/grpc/guess_number_agent.py", "content": "from typing import Annotated\nimport os\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nllm = AzureOpenAILLM({\n            \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n            \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n            \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n        })\n\nguess_number = Agent(id=\"agent\", llm=llm, description=\"Call this agent to play guess the number game with the use\", system_message = \"\"\"You are an AI assistant\n        Your task is to play a game with the user.\n        You first generate a random number between 1 and 100. Then save it as a conversation variable named \"number\".\n        The user will try to guess the number.\n        If the user's guess is too high, respond with \"Too high\".\n        If the user's guess is too low, respond with \"Too low\".\n        \"\"\")\n        \n@guess_number.register_tool(description=\"Generate a random number\")\ndef random() -> Annotated[str, \"A random number\"]:\n    return str(random.randint(1, 100))"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/extras/__init__.py", "content": "from ..llm import LLM\nfrom ..conversation import ConversationReadingStrategy, Conversation\nfrom llmlingua import PromptCompressor\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass PromptCompressorLLM(LLM):\n    def __init__(self, model_name: str):\n        \"\"\"\n        Initialize the PromptCompressorLLM with LLMLingua's PromptCompressor.\n\n        Args:\n            model_name (str): The name of the language model to use.\n        \"\"\"\n        super().__init__(model_name)\n        self.prompt_compressor = PromptCompressor()\n\n        logger.debug(\n            \"[PromptCompressorLLM] initialized with model: %s\", self.model_name\n        )\n\n    def ask(self, messages: list[dict]):\n        # Extract the conversation text from the messages\n        conversation_text = \" \".join([message[\"content\"] for message in messages])\n\n        # Compress the conversation text\n        compressed_text = self.prompt_compressor.compress(conversation_text)\n\n        return compressed_text\n\n\nclass CompressSystemPromptStrategy(ConversationReadingStrategy):\n    \"\"\"A conversation reading strategy that compresses the system prompt using LLMLingua's PromptCompressor.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the CompressSystemPromptStrategy.\"\"\"\n        super().__init()\n        self.compressor = PromptCompressor()\n\n    def get_messages(self, conversation: Conversation) -> list[dict]:\n        # Get system message\n        system_message = conversation.messages[0]\n        if system_message[\"role\"] != \"system\":\n            raise ValueError(\"First message in conversation should be system message\")\n\n        # Compress the system message\n        compressed_text = self.compressor.compress(system_message[\"content\"])\n\n        # Return the original conversation with the compressed system message\n        return [{\"role\": \"system\", \"content\": compressed_text}] + conversation.messages[\n            1:\n        ]\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/conversation.py", "content": "from abc import ABC, abstractmethod\nfrom queue import SimpleQueue\nfrom pydantic import BaseModel\n\nfrom .llm import LLM\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n# a ConversationMetrics class with totalTokens, promptTokens and completionTokens\nclass ConversationMetrics(BaseModel):\n    \"\"\"A class to store conversation metrics.\"\"\"\n\n    total_tokens: int = 0\n    prompt_tokens: int = 0\n    completion_tokens: int = 0\n\n\nclass Conversation:\n    messages: list[dict]\n    variables: dict[str, str]\n    log: list\n    metrics: ConversationMetrics\n    \"\"\"A class to represent a conversation.\n\n    This is only stateful object in the system, and is used to store the conversation\n    state, including messages, variables, and metrics.\n    \"\"\"\n\n    def __init__(\n        self,\n        messages: list[dict] = [],\n        variables: dict[str, str] = {},\n        metrics=ConversationMetrics(\n            total_tokens=0, prompt_tokens=0, completion_tokens=0\n        ),\n        log=[],\n    ):\n        \"\"\"Initialize the Conversation object. All arguments are optional.\n\n        Args:\n            messages (list[dict]): The list of messages in the conversation.\n            variables (dict[str, str]): The variables in the conversation.\n            metrics (ConversationMetrics): The metrics of the conversation.\n            log (list): The log of the conversation.\n        \"\"\"\n        self.messages = messages\n        self.variables = variables\n        self.log = log\n        self.metrics = metrics\n        self.stream_queue = SimpleQueue()\n\n    def stream(self):\n        \"\"\"Stream conversation updates, like LLM delta updates, to the consumer.\n\n        NOTE this is an INFINITE generator function, and must be kept so. Consumers\n        should break the loop themselves, typically using a stack count logic\n        \"\"\"\n        while True:\n            mark, content = self.stream_queue.get()\n            yield [mark, content]\n\n    def update(self, delta):\n        \"\"\"Update the conversation signalling a delta.\"\"\"\n        self.stream_queue.put_nowait(delta)\n\n    def to_dict(self):\n        \"\"\"Convert the conversation to a raw dictionary.\"\"\"\n        return {\n            \"messages\": self.messages,\n            \"variables\": self.variables,\n            \"metrics\": self.metrics.model_dump(),\n        }\n\n    def fork(self):\n        \"\"\"Fork the conversation into a new conversation object.\"\"\"\n        return Conversation(\n            messages=self.messages.copy(), variables=self.variables.copy()\n        )\n\n    @classmethod\n    def from_dict(cls, data: dict):\n        \"\"\"Create a conversation object from a raw dictionary.\"\"\"\n        return cls(\n            messages=data.get(\"messages\", []),\n            variables=data.get(\"variables\", {}),\n            log=data.get(\"log\", []),\n            metrics=ConversationMetrics(**data.get(\"metrics\", {})),\n        )\n\n\nclass ConversationReadingStrategy(ABC):\n    \"\"\"Base class for conversation reading strategies.\"\"\"\n\n    @abstractmethod\n    def get_messages(self, conversation: Conversation) -> list[dict]:\n        pass\n\n    def exclude_system_messages(self, messages: list[dict]) -> list[dict]:\n        return [message for message in messages if message[\"role\"] != \"system\"]\n\n\nclass LastNMessagesStrategy(ConversationReadingStrategy):\n    \"\"\"A conversation reading strategy that reads the last N messages from the conversation.\"\"\"\n\n    def __init__(self, n: int):\n        \"\"\"\n        Initialize the LastNMessagesStrategy.\n\n        Args:\n            n (int): The number of messages to read.\n        \"\"\"\n        self.n = n\n\n    def get_messages(self, conversation: Conversation) -> list[dict]:\n        return self.exclude_system_messages(conversation.messages)[-self.n :]\n\n\nclass AllMessagesStrategy(ConversationReadingStrategy):\n    \"\"\"A conversation reading strategy that reads all messages from the conversation.\"\"\"\n\n    def get_messages(self, conversation: Conversation) -> list[dict]:\n        return self.exclude_system_messages(conversation.messages)\n\n\nclass TopKLastNMessagesStrategy(ConversationReadingStrategy):\n    \"\"\"A conversation reading strategy that reads the top K and last N messages from the conversation.\"\"\"\n\n    def __init__(self, k: int, n: int):\n        \"\"\"\n        Initialize the TopKLastNMessagesStrategy.\n\n        Args:\n            k (int): The number of top messages to read.\n            n (int): The number of last messages to read.\n        \"\"\"\n        self.k = k\n        self.n = n\n\n    def get_messages(self, conversation: Conversation) -> list[dict]:\n        list = self.exclude_system_messages(conversation.messages)\n        return list[: self.k] + list[-self.n :]\n\n\nclass SummarizeMessagesStrategy(ConversationReadingStrategy):\n    \"\"\"A conversation reading strategy that summarizes the conversation messages into a single message.\"\"\"\n\n    def __init__(self, llm: LLM, system_prompt: str):\n        \"\"\"\n        Initialize the SummarizeMessagesStrategy.\n\n        Args:\n            llm (LLM): The language model to use for summarization.\n            system_prompt (str): The system prompt to use for summarization.\n        \"\"\"\n        super().__init__()\n        self.llm = llm\n        self.system_prompt = system_prompt\n\n    def get_messages(self, conversation: Conversation) -> list[dict]:\n        # Extract the conversation text from the messages\n        local_messages = []\n        local_messages += self.exclude_system_messages(conversation.messages)\n        local_messages.append({\"role\": \"user\", \"content\": self.system_prompt})\n\n        # Summarize the conversation text\n        response, usage = self.llm.ask(messages=local_messages)\n        response_message = response.model_dump()\n        summarized_text = response_message[\"content\"]\n\n        return [{\"role\": \"assistant\", \"name\": \"summarizer\", \"content\": summarized_text}]\n\n\nclass PipelineConversationReadingStrategy(ConversationReadingStrategy):\n    \"\"\"A conversation reading strategy that reads the conversation messages through a pipeline of strategies.\"\"\"\n\n    def __init__(self, strategies: list[ConversationReadingStrategy]):\n        \"\"\"\n        Initialize the PipelineConversationReadingStrategy.\n\n        Args:\n            strategies (list[ConversationReadingStrategy]): The list of strategies to use in the pipeline.\n        \"\"\"\n        self.strategies = strategies\n\n    def get_messages(self, conversation: Conversation) -> list[dict]:\n        messages = conversation.messages\n        for strategy in self.strategies:\n            messages = strategy.get_messages(Conversation(messages=messages))\n        return messages\n\n\nclass ConversationUpdateStrategy(ABC):\n    \"\"\"Base class for conversation update strategies.\"\"\"\n\n    @abstractmethod\n    def update(self, conversation: Conversation, delta: any):\n        pass\n\n\nclass AppendMessagesUpdateStrategy(ConversationUpdateStrategy):\n    \"\"\"An update strategy that appends messages to the conversation.\"\"\"\n\n    def update(self, conversation: Conversation, delta: any):\n        if isinstance(delta, list):\n            conversation.messages += delta\n        else:\n            conversation.messages += [delta]\n\n\nclass ReplaceLastMessageUpdateStrategy(ConversationUpdateStrategy):\n    \"\"\"An update strategy that replaces the last message in the conversation.\"\"\"\n\n    def update(self, conversation: Conversation, delta: any):\n        conversation.messages[-1] = delta\n\n\nclass NoopUpdateStrategy(ConversationUpdateStrategy):\n    \"\"\"No operation update strategy, does not update the conversation.\n\n    Useful for agents that do not need to update messages, but only invoke functions or\n    set variables.\n    \"\"\"\n\n    def update(self, conversation: Conversation, delta: any):\n        pass\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/remote/__init__.py", "content": ""}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/user.py", "content": "from typing import Callable\n\nfrom .askable import Askable\nfrom .conversation import Conversation\n\n\nclass User(Askable):\n    \"\"\"A human user that interacts with the system. Can provide input to the chat.\n\n    Args:\n        id (str): The unique identifier of the user.\n        mode (str): The mode of the user. Can be \"interactive\" or \"unattended\".\n        description (str): The description of the user. Can be used by the orchestrator to decide which agent to ask.\n        interaction_function (Callable[[str], str]): The function to use to get the user input. By default, it uses the input function.\n    \"\"\"\n\n    def __init__(\n        self,\n        id=\"user\",\n        mode=\"interactive\",\n        description=\"A human user that interacts with the system. Can provide input to the chat\",\n        interaction_function: Callable[[str], str] = None,\n    ):\n        \"\"\"Initialize the User object.\n\n        Args:\n            id (str): The ID of the User object. Will be used to uniquely identify it.\n            mode (str): The mode of the User object. Can be \"interactive\" or \"unattended\".\n            description (str): The description of the User object. Typically used by orchestrators.\n            interaction_function (Callable[[str], str]): The function to use to get the user input. By default, it uses the input function.\n        \"\"\"\n        super().__init__(id, description)\n        self.id = id\n        self.mode = mode\n        self.description = description\n        self.interaction_function = interaction_function or input\n\n    def ask(self, conversation: Conversation, stream=False):\n        \"\"\"Ask the user to provide input to the chat.\n\n        This method will get the user input from the command line prompt by default when\n        the mode is \"interactive\". Else, it will return \"stop\" when the mode is\n        \"unattended\". User input must then be provided by workflow.run().\n        \"\"\"\n        if self.mode == \"interactive\":\n            # Get user input from command line prompt\n            user_input = self.interaction_function(f\"{self.id}: \")\n            conversation.messages.append(\n                {\"role\": \"user\", \"content\": user_input, \"name\": self.id}\n            )\n            return None\n        elif self.mode == \"unattended\":\n            return \"stop\"\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/sequence.py", "content": "from vanilla_aiagents.conversation import Conversation\nfrom .askable import Askable\nfrom .llm import LLM\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass Sequence(Askable):\n    \"\"\"A sequence of Askable steps that are ALWAYS asked in order.\"\"\"\n\n    def __init__(\n        self,\n        llm: LLM,\n        description: str,\n        id: str,\n        steps: list[Askable],\n        system_prompt: str = \"\",\n    ):\n        \"\"\"\n        Initialize the Sequence object.\n\n        Args:\n            llm (LLM): The language model to use for the decision-making process.\n            description (str): The description of the Sequence object.\n            id (str): The ID of the Sequence object. Will be used to uniquely identify it.\n            steps (list[Askable]): The steps that are part of the sequence.\n            system_prompt (str): The system prompt to use for the sequence.\n        \"\"\"\n        super().__init__(id, description)\n        self.steps = steps\n        self.system_prompt = system_prompt\n\n        self.llm = llm\n\n        logger.debug(\n            \"[Sequence %s] initialized with agents: %s\",\n            self.id,\n            [step.id for step in self.steps],\n        )\n\n    def ask(self, conversation: Conversation, stream=False):\n\n        execution_result = None\n        if stream:\n            conversation.update([\"start\", self.id])\n        for step in self.steps:\n            agent_result = step.ask(conversation, stream=stream)\n            logger.debug(\n                \"[Sequence %s] asked step '%s' with messages: %s\",\n                self.id,\n                step.id,\n                agent_result,\n            )\n\n            if agent_result == \"stop\":\n                logger.debug(\n                    \"[Sequence %s] stop signal received, ending workflow.\", self.id\n                )\n                execution_result = \"agent-stop\"\n                break\n            elif agent_result == \"error\":\n                logger.error(\n                    \"[Sequence %s] error signal received, ending workflow.\", self.id\n                )\n                execution_result = \"agent-error\"\n                break\n\n        if stream:\n            conversation.update([\"end\", self.id])\n\n        return execution_result\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/remote/remote.py", "content": "from abc import abstractmethod\nimport contextlib\nimport gzip\nimport json\nimport logging\nimport queue\nimport time\nfrom typing import Generator, Protocol\n\nfrom fastapi import FastAPI\nfrom fastapi.middleware.gzip import GZipMiddleware\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\nimport uvicorn\nimport threading\nimport requests\nimport importlib\nimport os\n\nfrom starlette_gzip_request import GZipRequestMiddleware\n\nfrom ..conversation import (\n    AllMessagesStrategy,\n    Conversation,\n    ConversationMetrics,\n    ConversationReadingStrategy,\n)\nfrom ..askable import Askable\n\n# Configure logging\nlogger = logging.getLogger(__name__)\n\n\nclass ConversationRequest(BaseModel):\n    \"\"\"A class to store a conversation request to a remote askable.\"\"\"\n\n    messages: list[dict]\n    variables: dict\n\n\nclass ConversationResponse(BaseModel):\n    \"\"\"A class to store a conversation response from a remote askable.\"\"\"\n\n    messages: list[dict]\n    variables: dict\n    metrics: ConversationMetrics\n\n\nclass AskResponse(BaseModel):\n    \"\"\"A class to store an ask response from a remote askable.\"\"\"\n\n    conversation: ConversationResponse\n    result: str\n\n\nclass Connection(Protocol):\n    \"\"\"A common interface for a connection to a remote askable.\"\"\"\n\n    @abstractmethod\n    def send(target_id: str, self, operation: str, payload: any) -> dict:\n        \"\"\"Send a payload to the remote askable.\"\"\"\n        pass\n\n    @abstractmethod\n    def stream(target_id: str, self, operation: str, payload: any) -> dict:\n        \"\"\"Send a payload to the remote askable and stream the response.\"\"\"\n        pass\n\n\nclass RESTConnection(Connection):\n    \"\"\"A connection to a remote askable using HTTP REST.\"\"\"\n\n    def __init__(self, url: str):\n        \"\"\"Initialize the RESTConnection object.\n\n        Args:\n            url (str): The URL of the remote askable.\n        \"\"\"\n        self.url = url\n        logger.debug(f\"RESTConnection initialized with URL: {self.url}\")\n\n    def send(self, target_id: str, operation: str, payload: any) -> dict:\n        \"\"\"Send a payload to the remote askable.\n\n        Args:\n            target_id (str): The ID of the remote askable.\n            operation (str): The operation to perform.\n            payload (any): The payload to send.\n\n        Returns:\n            dict: The response from the remote askable, deserialized from JSON.\n        \"\"\"\n        logger.debug(f\"Sending payload to {self.url}/{operation}: {payload}\")\n        headers = {\"Content-Encoding\": \"gzip\", \"Content-Type\": \"application/json\"}\n        compressed_payload = gzip.compress(json.dumps(payload).encode(\"utf-8\"))\n        response = requests.post(\n            f\"{self.url}/{target_id}/{operation}\",\n            data=compressed_payload,\n            headers=headers,\n        )\n        response.raise_for_status()\n        response = response.json()\n        logger.debug(f\"Received response: {response}\")\n\n        return response\n\n    def stream(self, target_id: str, operation: str, payload: any):\n        \"\"\"\n        Send a payload to the remote askable and stream the response.\n\n        Args:\n            target_id (str): The ID of the remote askable.\n            operation (str): The operation to perform.\n            payload (any): The payload to send.\n\n        Yields:\n            dict: The response from the remote askable, deserialized from JSON.\n        \"\"\"\n        logger.debug(f\"Streaming payload to {self.url}/{operation}: {payload}\")\n        headers = {\"Content-Encoding\": \"gzip\", \"Content-Type\": \"application/json\"}\n        compressed_payload = gzip.compress(json.dumps(payload).encode(\"utf-8\"))\n        response = requests.post(\n            f\"{self.url}/{target_id}/{operation}?stream=true\",\n            data=compressed_payload,\n            headers=headers,\n            stream=True,\n        )\n        response.raise_for_status()\n        result = None\n        for line in response.iter_lines():\n            if line:\n                logger.debug(f\"Received line: {line}\")\n                mark, content = json.loads(line)\n                yield [mark, content]\n                if mark == \"result\":\n                    result = content\n                    break\n\n        return result\n\n\nclass RemoteAskable(Askable):\n    \"\"\"A remote askable that can be asked remotely using a connection.\"\"\"\n\n    def __init__(\n        self,\n        id: str,\n        connection: Connection,\n        reading_strategy: ConversationReadingStrategy = AllMessagesStrategy(),\n    ):\n        \"\"\"Initialize the RemoteAskable object.\n\n        Args:\n            id (str): The ID of the RemoteAskable object. Will be used to uniquely identify it.\n            connection (Connection): The connection to the remote askable.\n            reading_strategy (ConversationReadingStrategy): The reading strategy to use to select the messages to send to the remote askable.\n        \"\"\"\n        super().__init__(\"\", \"\")\n        self.connection = connection\n        self.id = id\n        self.reading_strategy = reading_strategy\n\n        response = self.connection.send(self.id, \"describe\", {})\n        self.description = response[\"description\"]\n\n        logger.debug(\n            f\"RemoteAskable initialized with ID: {self.id}, Description: {self.description}\"\n        )\n\n    def ask(self, conversation: Conversation, stream=False):\n        \"\"\"Ask the remote askable to solve the user inquiry.\n\n        Args:\n            conversation (Conversation): The conversation to use for the execution\n            stream (bool): Whether to stream the conversation updates.\n        \"\"\"\n        source_messages = self.reading_strategy.get_messages(conversation)\n        payload = {\"messages\": source_messages, \"variables\": conversation.variables}\n        logger.debug(f\"Asking with payload: {payload}\")\n\n        result = None\n        conv = None\n        if not stream:\n            response = self.connection.send(self.id, \"ask\", payload)\n        else:\n            gen = self.connection.stream(self.id, \"ask\", payload)\n            for mark, content in gen:\n                conversation.update([mark, content])\n                if mark == \"result\":\n                    response = content\n\n        result = response[\"result\"]\n        conv = response[\"conversation\"]\n\n        # Original metrics are not part of the payload, so we need to sum them\n        conversation.metrics.completion_tokens += conv[\"metrics\"][\"completion_tokens\"]\n        conversation.metrics.prompt_tokens += conv[\"metrics\"][\"prompt_tokens\"]\n        conversation.metrics.total_tokens += conv[\"metrics\"][\"total_tokens\"]\n        # Update the conversation with the new messages\n        conversation.messages += conv[\"messages\"][len(source_messages):]\n        # Update the conversation variables\n        conversation.variables = conv[\"variables\"]\n        logger.debug(f\"Updated conversation: {conversation}\")\n\n        return result\n\n\nclass AskableHost(Protocol):\n    \"\"\"A common interface for a host that can host askables.\"\"\"\n\n    @abstractmethod\n    def start(self):\n        \"\"\"Start the host.\"\"\"\n        pass\n\n    @abstractmethod\n    def stop(self):\n        \"\"\"Stop the host.\"\"\"\n        pass\n\n\n# Inspired by https://bugfactory.io/articles/starting-and-stopping-uvicorn-in-the-background/\nclass ThreadedServer(uvicorn.Server):\n    \"\"\"A Uvicorn threaded server that can run in a separate thread and started/stopped programmatically.\"\"\"\n\n    @contextlib.contextmanager\n    def run_in_thread(self) -> Generator:\n        \"\"\"Run the server in a separate thread.\"\"\"\n        self.thread = threading.Thread(target=self.run)\n        self.thread.start()\n        logger.debug(\"Server thread started\")\n        while not self.started:\n            time.sleep(0.001)\n        yield\n        logger.debug(\"Server running in thread\")\n\n\nclass RESTHost(AskableHost):\n    \"\"\"A host that can host askables using a REST API.\"\"\"\n\n    def __init__(\n        self,\n        askables: list[Askable],\n        host: str,\n        port: int,\n        config: uvicorn.Config = None,\n    ):\n        \"\"\"Initialize the RESTHost object.\n\n        Args:\n            askables (list[Askable]): The askables to host.\n            host (str): The host to bind the server to.\n            port (int): The port to bind the server to.\n            config (uvicorn.Config): The configuration to use for the server.\n        \"\"\"\n        self.askables = askables\n        self.askables_dict = {askable.id: askable for askable in askables}\n        self._build_app()\n        self.config = config or uvicorn.Config(app=self.app, host=host, port=port)\n        logger.debug(\n            f\"RESTHost initialized with host: {self.config.host}, port: {self.config.port}\"\n        )\n\n    def start(self):\n        \"\"\"Start the host.\"\"\"\n        # Start the server\n        self.server = ThreadedServer(config=self.config)\n        with self.server.run_in_thread():\n            logger.info(\n                f\"Askable server running at http://{self.config.host}:{self.config.port}\"\n            )\n            # Log a message with all available askables\n            logger.info(\n                f\"Available askables: {', '.join([askable.id for askable in self.askables])}\"\n            )\n\n    def stop(self):\n        \"\"\"Stop the host.\"\"\"\n        logger.debug(\"Stopping server\")\n        self.server.should_exit = True\n        self.server.thread.join()\n        logger.debug(\"Server stopped\")\n\n    def _build_app(self):\n        self.app = FastAPI()\n        # Enable GZip compression for response\n        self.app.add_middleware(GZipMiddleware, minimum_size=1000)\n        self.app.add_middleware(GZipRequestMiddleware)\n\n        @self.app.post(\"/{id}/describe\")\n        async def describe(id: str):\n            if id in self.askables_dict:\n                askable = self.askables_dict[id]\n                return {\"id\": askable.id, \"description\": askable.description}\n            else:\n                # Return 404 if the askable is not found\n                return {\"detail\": \"Askable not found\"}, 404\n\n        @self.app.post(\"/{id}/ask\")\n        async def ask(id: str, request: ConversationRequest, stream: bool = False):\n            logger.debug(f\"Received ask request: {request} for askable {id}\")\n            conv = Conversation(request.messages, request.variables)\n\n            if id in self.askables_dict:\n                askable = self.askables_dict[id]\n\n                if stream:\n                    result_queue = queue.SimpleQueue()\n\n                    def ask_in_thread():\n                        res = None\n                        try:\n                            res = askable.ask(conv, True)\n                        except Exception as e:\n                            logger.error(\n                                \"Error during askable.ask: %s\", e, exc_info=True\n                            )\n                            conv.update([\"error\", str(e)])\n                            res = \"error\"\n\n                        result_queue.put_nowait(res)\n\n                    thread = threading.Thread(target=ask_in_thread)\n                    thread.start()\n\n                    async def _stream():\n                        # Since we are using an infinite generator, we need to keep track of the stack count to know when to break\n                        stack_count = 0\n                        for mark, content in conv.stream():\n                            json_string = json.dumps([mark, content])\n\n                            # Always yield the update back to the client, as a JSON string\n                            yield json_string + \"\\n\"  # NEW LINE DELIMITED JSON, otherwise the client will not be able to read the stream\n\n                            # Keep track of the stack count to know when to break\n                            if mark == \"start\":\n                                stack_count += 1\n                            elif mark == \"end\":\n                                stack_count -= 1\n\n                            logger.debug(\"Stack count: %s\", stack_count)\n                            if stack_count == 0:\n                                logger.debug(\n                                    \"Received response and stack count is 0, breaking stream.\"\n                                )\n                                break\n                            if mark == \"error\":\n                                logger.debug(\"Received error, breaking stream.\")\n                                break\n\n                        thread.join()\n                        response = AskResponse(\n                            conversation=ConversationResponse(\n                                messages=conv.messages,\n                                variables=conv.variables,\n                                metrics=conv.metrics,\n                            ),\n                            result=result_queue.get(),\n                        ).model_dump()\n                        yield json.dumps([\"result\", response])\n\n                    response = StreamingResponse(\n                        _stream(), media_type=\"application/x-ndjson\"\n                    )\n                else:\n                    result = askable.ask(conv, stream=False)\n                    response = AskResponse(\n                        conversation=ConversationResponse(\n                            messages=conv.messages,\n                            variables=conv.variables,\n                            metrics=conv.metrics,\n                        ),\n                        result=result,\n                    )\n\n                    logger.debug(f\"Returning response: {response}\")\n\n                return response\n            else:\n                # Return 404 if the askable is not found\n                return {\"detail\": \"Askable not found\"}, 404\n\n\ndef find_askables(source_dir: str = None):\n    \"\"\"Find all askables in the given source directory.\n\n    Args:\n        source_dir (str): The source directory to search for askables. If None, the current directory is used.\n    \"\"\"\n    if source_dir is None:\n        source_dir = os.path.dirname(os.path.realpath(__file__))\n    askables = []\n    for filename in os.listdir(source_dir):\n        if filename.endswith(\"_entry.py\") and filename != \"main.py\":\n            module_name = filename[:-3]\n            spec = importlib.util.spec_from_file_location(\n                module_name, os.path.join(source_dir, filename)\n            )\n            module = importlib.util.module_from_spec(spec)\n            logger.debug(\n                f\"Loading module {module_name} from {os.path.join(source_dir, filename)}\"\n            )\n            spec.loader.exec_module(module)\n            for name in dir(module):\n                logger.debug(f\"Checking {name}\")\n                obj = getattr(module, name)\n                if isinstance(obj, Askable):\n                    logger.debug(f\"Found askable: {obj}\")\n                    askables.append(obj)\n    return askables\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/team.py", "content": "from typing import Annotated, Callable\n\nfrom pydantic import BaseModel\n\nfrom .conversation import AllMessagesStrategy, Conversation, ConversationReadingStrategy\n\nfrom .agent import Agent\nfrom .askable import Askable\nfrom .llm import LLM\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass Team(Askable):\n    \"\"\"A team of Askable that decides which agent to ask next based on the conversation context and available agents information.\n\n    Args:\n        llm (LLM): The language model to use for the decision-making process.\n        description (str): The description of the team.\n        id (str): The unique identifier of the team.\n        members (list[Askable]): The agents that are part of the team.\n        system_prompt (str): The system prompt to show to the orchestrator.\n        stop_callback (Callable[[list[dict]], bool]): The callback function to determine when to stop the conversation\n        allowed_transitions (dict[Agent, list[Agent]]): The allowed transitions between agents.\n        include_tools_descriptions (bool): Whether to include the tools descriptions in the system prompt to help the orchestrator decide.\n        reading_strategy (ConversationReadingStrategy): The reading strategy to use to select the messages to use for the decision-making process.\n        use_structured_output (bool): Whether to use JSON structured output for the decision-making process. Set to False to use an older LLM API version.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: LLM,\n        description: str,\n        id: str,\n        members: list[Askable],\n        system_prompt: str = \"\",\n        stop_callback: Callable[[Conversation], bool] = None,\n        allowed_transitions: dict[Agent, list[Agent]] = None,\n        include_tools_descriptions: bool = False,\n        reading_strategy: ConversationReadingStrategy = AllMessagesStrategy(),\n        use_structured_output: bool = True,\n    ):\n        \"\"\"\n        Initialize the Team object.\n\n        Args:\n            llm (LLM): The language model to use for the decision-making process.\n            description (str): The description of the team.\n            id (str): The unique identifier of the team.\n            members (list[Askable]): The agents that are part of the team.\n            system_prompt (str): The system prompt to show to the orchestrator. Optional, if provided it will be used to override the decision-making process.\n            stop_callback (Callable[[list[dict]], bool]): The callback function to determine when to stop the conversation\n            allowed_transitions (dict[Agent, list[Agent]]): The allowed transitions between agents. Optional.\n            include_tools_descriptions (bool): Whether to include the tools descriptions in the system prompt to help the orchestrator decide. Optional.\n            reading_strategy (ConversationReadingStrategy): The reading strategy to use to select the messages to use for the decision-making process. Optional.\n            use_structured_output (bool): Whether to use JSON structured output for the decision-making process. Set to False to use an older LLM API version. Optional.\n        \"\"\"\n        super().__init__(id, description)\n        self.agents = members\n        self.system_prompt = system_prompt\n        self.stop_callback = stop_callback\n        self.include_tools_descriptions = include_tools_descriptions\n        self.allowed_transitions = allowed_transitions\n        self.allowed_transitions_str_dict = (\n            {\n                tr.id: [agent.id for agent in members]\n                for tr in self.allowed_transitions\n                for agent in self.allowed_transitions[tr]\n            }\n            if self.allowed_transitions\n            else None\n        )\n        self.use_structured_output = use_structured_output\n\n        self.current_agent = None\n        self.agents_dict = {agent.id: agent for agent in members}\n\n        self.llm = llm\n        self.reading_strategy = reading_strategy\n\n        logger.debug(\"[Team %s] initialized with agents: %s\", self.id, self.agents_dict)\n\n    def ask(self, conversation: Conversation, stream=False):\n        \"\"\"\n        Ask the team to solve the user inquiry by selecting the next agent to ask based on the conversation context and available agents information.\n\n        This method will ask each agent in the team in order based on the conversation\n        context and the available agents information. If the stop_callback is triggered,\n        the execution will stop.\n        \"\"\"\n        if stream:\n            conversation.update([\"start\", self.id])\n\n        execution_result = None\n        while True:\n            next_agent_id = self._select_next_agent(conversation)\n            logger.debug(\"[Team %s] selected next agent ID: %s\", self.id, next_agent_id)\n\n            self.current_agent = self.agents_dict[next_agent_id]\n            logger.debug(\n                \"[Team %s] current agent: '%s'\", self.id, self.current_agent.id\n            )\n\n            agent_result = self.current_agent.ask(conversation, stream=stream)\n\n            logger.debug(\n                \"[Team %s] asked current agent with messages: %s\", self.id, agent_result\n            )\n\n            if agent_result == \"stop\":\n                logger.debug(\n                    \"[Team %s] stop signal received, ending workflow.\", self.id\n                )\n                conversation.log.append((\"info\", \"agent/stop\", next_agent_id))\n                conversation.log.append((\"info\", \"team/stop\", self.id))\n                execution_result = \"agent-stop\"\n                break\n            elif agent_result == \"error\":\n                logger.error(\n                    \"[Team %s] error signal received, ending workflow.\", self.id\n                )\n                conversation.log.append((\"error\", \"team/error\", self.id))\n                execution_result = \"agent-error\"\n                break\n\n            if self.stop_callback(conversation):\n                logger.debug(\n                    \"[Team %s] stop callback triggered, ending workflow.\", self.id\n                )\n                conversation.log.append((\"info\", \"team/callback-stop\", self.id))\n                execution_result = \"callback-stop\"\n                break\n\n        if stream:\n            conversation.update([\"end\", self.id])\n\n        return execution_result\n\n    def _select_next_agent(self, conversation: Conversation):\n        system_prompt = \"\"\"\nYou are a team orchestrator that uses a chat history to determine the next best speaker in the conversation.\nYour task is to return the agent_id of the speaker that is best suited to proceed based on the context provided in the chat history and the description of the agents.\nYou MUST return agent_id value from the list of available agents.\nThe names are case-sensitive and should not be abbreviated or changed.\nWhen a user input is expected, you MUST select an agent capable of handling the user input.\nWhen provided, you can also take a decision based on tools available to each agent\nWhen provided, you can also take a decision based on the allowed transitions between agents.\n\n# AVAILABLE AGENTS\n\n{agents}\n\n# CHAT HISTORY\n\n{history}\n\nBE SURE TO READ AGAIN THE INSTUCTIONS ABOVE BEFORE PROCEEDING.\n\"\"\"\n        local_messages = []\n        agents_info = self.generate_agents_info()\n        history = self.construct_message_history(conversation)\n\n        local_messages.append(\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt.format(agents=agents_info, history=history),\n            }\n        )\n        local_messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": \"Read the conversation and provide the agent_id of the next speaker.\",\n            }\n        )\n\n        if self.use_structured_output:\n            result, usage = self.llm.ask(\n                messages=local_messages,\n                temperature=0,\n                response_format=AgentChoiceResponse,\n            )\n            logger.debug(\n                \"[Team %s] selected agent_id: %s, (reason: '%s')\",\n                self.id,\n                result.parsed.agent_id,\n                result.parsed.reason,\n            )\n            conversation.log.append(\n                (\n                    \"info\",\n                    \"team/choice\",\n                    self.id,\n                    result.parsed.agent_id,\n                    result.parsed.reason,\n                )\n            )\n            next_agent_id = result.parsed.agent_id\n        else:\n            result, usage = self.llm.ask(messages=local_messages, temperature=0)\n            next_agent_id = result.content.split(\" \")[-1].strip()\n            logger.debug(\"[Team %s] selected agent_id: %s\", self.id, next_agent_id)\n            conversation.log.append((\"info\", \"team/choice\", self.id, next_agent_id))\n\n        if usage is not None:\n            # Update conversation metrics with response usage\n            conversation.metrics.total_tokens += usage[\"total_tokens\"]\n            conversation.metrics.prompt_tokens += usage[\"prompt_tokens\"]\n            conversation.metrics.completion_tokens += usage[\"completion_tokens\"]\n\n        if next_agent_id not in self.agents_dict:\n            logger.error(\n                \"[Team %s] invalid agent_id selected: %s\", self.id, next_agent_id\n            )\n            conversation.log.append((\"error\", \"team/choice\", self.id, next_agent_id))\n            return self._select_next_agent(conversation)\n\n        if (\n            self.allowed_transitions_str_dict is not None\n            and self.current_agent is not None\n        ):\n            if (\n                next_agent_id\n                not in self.allowed_transitions_str_dict[self.current_agent.id]\n            ):\n                logger.error(\n                    \"[Team %s] invalid agent_id selected: %s\", self.id, next_agent_id\n                )\n                conversation.log.append(\n                    (\"error\", \"team/choice\", self.id, next_agent_id)\n                )\n                return self._select_next_agent(conversation)\n\n        return next_agent_id\n\n    def construct_message_history(self, conversation):\n        selected_messages = self.reading_strategy.get_messages(conversation)\n        history = \"\\n\".join(\n            [\n                f\"{message['role']}: {message['content']}\"\n                for message in selected_messages\n            ]\n        )\n        return history\n\n    def generate_agents_info(self):\n        agents_info = []\n        for agent in self.agents:\n            tools = []\n            if self.include_tools_descriptions and hasattr(agent, \"tools\"):\n                for tool in agent.tools:\n                    tool_name = tool[\"function\"][\"name\"]\n                    tool_description = tool[\"function\"][\"description\"]\n                    tools.append(f\"    - tool '{tool_name}': {tool_description}\")\n            tools_str = \"\\n\".join(tools)\n\n            transitions = []\n            if self.allowed_transitions and agent in self.allowed_transitions:\n                transitions = [\n                    f\"    - can transition to: {next_agent.id}\"\n                    for next_agent in self.allowed_transitions[agent]\n                ]\n            transitions_str = \"\\n\".join(transitions)\n\n            agent_info = f\"- agent_id: {agent.id}\\n    - description: {agent.description}\\n{tools_str}\\n{transitions_str}\\n\\n\"\n            agents_info.append(agent_info)\n\n        return \"\\n\".join(agents_info)\n\n\nclass AgentChoiceResponse(BaseModel):\n    agent_id: Annotated[\n        str,\n        \"Agent ID selected by the orchestrator. Must be a valid agent_id from the list of available agents.\",\n    ]\n    reason: Annotated[str, \"Reasoning behind the agent_id selection.\"]\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/remote/remote_pb2.py", "content": "# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# NO CHECKED-IN PROTOBUF GENCODE\n# source: remote.proto\n# Protobuf Python Version: 5.27.2\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import runtime_version as _runtime_version\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf.internal import builder as _builder\n\n_runtime_version.ValidateProtobufRuntimeVersion(\n    _runtime_version.Domain.PUBLIC, 5, 27, 2, \"\", \"remote.proto\"\n)\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(\n    b'\\n\\x0cremote.proto\\x12\\x0bremote.grpc\"6\\n\\x07Message\\x12\\x0c\\n\\x04role\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07\\x63ontent\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\"]\\n\\x13\\x43onversationMetrics\\x12\\x19\\n\\x11\\x63ompletion_tokens\\x18\\x01 \\x01(\\x05\\x12\\x15\\n\\rprompt_tokens\\x18\\x02 \\x01(\\x05\\x12\\x14\\n\\x0ctotal_tokens\\x18\\x03 \\x01(\\x05\"\\xc5\\x01\\n\\x13\\x43onversationRequest\\x12\\x10\\n\\x08\\x61gent_id\\x18\\x01 \\x01(\\t\\x12&\\n\\x08messages\\x18\\x02 \\x03(\\x0b\\x32\\x14.remote.grpc.Message\\x12\\x42\\n\\tvariables\\x18\\x03 \\x03(\\x0b\\x32/.remote.grpc.ConversationRequest.VariablesEntry\\x1a\\x30\\n\\x0eVariablesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t:\\x02\\x38\\x01\"\\xe8\\x01\\n\\x14\\x43onversationResponse\\x12&\\n\\x08messages\\x18\\x01 \\x03(\\x0b\\x32\\x14.remote.grpc.Message\\x12\\x43\\n\\tvariables\\x18\\x02 \\x03(\\x0b\\x32\\x30.remote.grpc.ConversationResponse.VariablesEntry\\x12\\x31\\n\\x07metrics\\x18\\x03 \\x01(\\x0b\\x32 .remote.grpc.ConversationMetrics\\x1a\\x30\\n\\x0eVariablesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t:\\x02\\x38\\x01\"V\\n\\x0b\\x41skResponse\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\t\\x12\\x37\\n\\x0c\\x63onversation\\x18\\x02 \\x01(\\x0b\\x32!.remote.grpc.ConversationResponse\"#\\n\\x0f\\x44\\x65scribeRequest\\x12\\x10\\n\\x08\\x61gent_id\\x18\\x01 \\x01(\\t\"3\\n\\x10\\x44\\x65scribeResponse\\x12\\n\\n\\x02id\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0b\\x64\\x65scription\\x18\\x02 \\x01(\\t\"5\\n\\x14\\x41skStreamingResponse\\x12\\x0c\\n\\x04mark\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07\\x63ontent\\x18\\x02 \\x01(\\t2\\xef\\x01\\n\\rRemoteService\\x12\\x41\\n\\x03\\x41sk\\x12 .remote.grpc.ConversationRequest\\x1a\\x18.remote.grpc.AskResponse\\x12R\\n\\tAskStream\\x12 .remote.grpc.ConversationRequest\\x1a!.remote.grpc.AskStreamingResponse0\\x01\\x12G\\n\\x08\\x44\\x65scribe\\x12\\x1c.remote.grpc.DescribeRequest\\x1a\\x1d.remote.grpc.DescribeResponseb\\x06proto3'\n)\n\n_globals = globals()\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, \"remote_pb2\", _globals)\nif not _descriptor._USE_C_DESCRIPTORS:\n    DESCRIPTOR._loaded_options = None\n    _globals[\"_CONVERSATIONREQUEST_VARIABLESENTRY\"]._loaded_options = None\n    _globals[\"_CONVERSATIONREQUEST_VARIABLESENTRY\"]._serialized_options = b\"8\\001\"\n    _globals[\"_CONVERSATIONRESPONSE_VARIABLESENTRY\"]._loaded_options = None\n    _globals[\"_CONVERSATIONRESPONSE_VARIABLESENTRY\"]._serialized_options = b\"8\\001\"\n    _globals[\"_MESSAGE\"]._serialized_start = 29\n    _globals[\"_MESSAGE\"]._serialized_end = 83\n    _globals[\"_CONVERSATIONMETRICS\"]._serialized_start = 85\n    _globals[\"_CONVERSATIONMETRICS\"]._serialized_end = 178\n    _globals[\"_CONVERSATIONREQUEST\"]._serialized_start = 181\n    _globals[\"_CONVERSATIONREQUEST\"]._serialized_end = 378\n    _globals[\"_CONVERSATIONREQUEST_VARIABLESENTRY\"]._serialized_start = 330\n    _globals[\"_CONVERSATIONREQUEST_VARIABLESENTRY\"]._serialized_end = 378\n    _globals[\"_CONVERSATIONRESPONSE\"]._serialized_start = 381\n    _globals[\"_CONVERSATIONRESPONSE\"]._serialized_end = 613\n    _globals[\"_CONVERSATIONRESPONSE_VARIABLESENTRY\"]._serialized_start = 330\n    _globals[\"_CONVERSATIONRESPONSE_VARIABLESENTRY\"]._serialized_end = 378\n    _globals[\"_ASKRESPONSE\"]._serialized_start = 615\n    _globals[\"_ASKRESPONSE\"]._serialized_end = 701\n    _globals[\"_DESCRIBEREQUEST\"]._serialized_start = 703\n    _globals[\"_DESCRIBEREQUEST\"]._serialized_end = 738\n    _globals[\"_DESCRIBERESPONSE\"]._serialized_start = 740\n    _globals[\"_DESCRIBERESPONSE\"]._serialized_end = 791\n    _globals[\"_ASKSTREAMINGRESPONSE\"]._serialized_start = 793\n    _globals[\"_ASKSTREAMINGRESPONSE\"]._serialized_end = 846\n    _globals[\"_REMOTESERVICE\"]._serialized_start = 849\n    _globals[\"_REMOTESERVICE\"]._serialized_end = 1088\n# @@protoc_insertion_point(module_scope)\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/remote/remote_pb2_grpc.py", "content": "# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\nimport grpc\nimport warnings\n\nfrom . import remote_pb2 as remote__pb2\n\nGRPC_GENERATED_VERSION = \"1.66.2\"\nGRPC_VERSION = grpc.__version__\n_version_not_supported = False\n\ntry:\n    from grpc._utilities import first_version_is_lower\n\n    _version_not_supported = first_version_is_lower(\n        GRPC_VERSION, GRPC_GENERATED_VERSION\n    )\nexcept ImportError:\n    _version_not_supported = True\n\nif _version_not_supported:\n    raise RuntimeError(\n        f\"The grpc package installed is at version {GRPC_VERSION},\"\n        + f\" but the generated code in remote_pb2_grpc.py depends on\"\n        + f\" grpcio>={GRPC_GENERATED_VERSION}.\"\n        + f\" Please upgrade your grpc module to grpcio>={GRPC_GENERATED_VERSION}\"\n        + f\" or downgrade your generated code using grpcio-tools<={GRPC_VERSION}.\"\n    )\n\n\nclass RemoteServiceStub(object):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n    def __init__(self, channel):\n        \"\"\"Constructor.\n\n        Args:\n            channel: A grpc.Channel.\n        \"\"\"\n        self.Ask = channel.unary_unary(\n            \"/remote.grpc.RemoteService/Ask\",\n            request_serializer=remote__pb2.ConversationRequest.SerializeToString,\n            response_deserializer=remote__pb2.AskResponse.FromString,\n            _registered_method=True,\n        )\n        self.AskStream = channel.unary_stream(\n            \"/remote.grpc.RemoteService/AskStream\",\n            request_serializer=remote__pb2.ConversationRequest.SerializeToString,\n            response_deserializer=remote__pb2.AskStreamingResponse.FromString,\n            _registered_method=True,\n        )\n        self.Describe = channel.unary_unary(\n            \"/remote.grpc.RemoteService/Describe\",\n            request_serializer=remote__pb2.DescribeRequest.SerializeToString,\n            response_deserializer=remote__pb2.DescribeResponse.FromString,\n            _registered_method=True,\n        )\n\n\nclass RemoteServiceServicer(object):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n    def Ask(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details(\"Method not implemented!\")\n        raise NotImplementedError(\"Method not implemented!\")\n\n    def AskStream(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details(\"Method not implemented!\")\n        raise NotImplementedError(\"Method not implemented!\")\n\n    def Describe(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details(\"Method not implemented!\")\n        raise NotImplementedError(\"Method not implemented!\")\n\n\ndef add_RemoteServiceServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n        \"Ask\": grpc.unary_unary_rpc_method_handler(\n            servicer.Ask,\n            request_deserializer=remote__pb2.ConversationRequest.FromString,\n            response_serializer=remote__pb2.AskResponse.SerializeToString,\n        ),\n        \"AskStream\": grpc.unary_stream_rpc_method_handler(\n            servicer.AskStream,\n            request_deserializer=remote__pb2.ConversationRequest.FromString,\n            response_serializer=remote__pb2.AskStreamingResponse.SerializeToString,\n        ),\n        \"Describe\": grpc.unary_unary_rpc_method_handler(\n            servicer.Describe,\n            request_deserializer=remote__pb2.DescribeRequest.FromString,\n            response_serializer=remote__pb2.DescribeResponse.SerializeToString,\n        ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n        \"remote.grpc.RemoteService\", rpc_method_handlers\n    )\n    server.add_generic_rpc_handlers((generic_handler,))\n    server.add_registered_method_handlers(\n        \"remote.grpc.RemoteService\", rpc_method_handlers\n    )\n\n\n# This class is part of an EXPERIMENTAL API.\nclass RemoteService(object):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n    @staticmethod\n    def Ask(\n        request,\n        target,\n        options=(),\n        channel_credentials=None,\n        call_credentials=None,\n        insecure=False,\n        compression=None,\n        wait_for_ready=None,\n        timeout=None,\n        metadata=None,\n    ):\n        return grpc.experimental.unary_unary(\n            request,\n            target,\n            \"/remote.grpc.RemoteService/Ask\",\n            remote__pb2.ConversationRequest.SerializeToString,\n            remote__pb2.AskResponse.FromString,\n            options,\n            channel_credentials,\n            insecure,\n            call_credentials,\n            compression,\n            wait_for_ready,\n            timeout,\n            metadata,\n            _registered_method=True,\n        )\n\n    @staticmethod\n    def AskStream(\n        request,\n        target,\n        options=(),\n        channel_credentials=None,\n        call_credentials=None,\n        insecure=False,\n        compression=None,\n        wait_for_ready=None,\n        timeout=None,\n        metadata=None,\n    ):\n        return grpc.experimental.unary_stream(\n            request,\n            target,\n            \"/remote.grpc.RemoteService/AskStream\",\n            remote__pb2.ConversationRequest.SerializeToString,\n            remote__pb2.AskStreamingResponse.FromString,\n            options,\n            channel_credentials,\n            insecure,\n            call_credentials,\n            compression,\n            wait_for_ready,\n            timeout,\n            metadata,\n            _registered_method=True,\n        )\n\n    @staticmethod\n    def Describe(\n        request,\n        target,\n        options=(),\n        channel_credentials=None,\n        call_credentials=None,\n        insecure=False,\n        compression=None,\n        wait_for_ready=None,\n        timeout=None,\n        metadata=None,\n    ):\n        return grpc.experimental.unary_unary(\n            request,\n            target,\n            \"/remote.grpc.RemoteService/Describe\",\n            remote__pb2.DescribeRequest.SerializeToString,\n            remote__pb2.DescribeResponse.FromString,\n            options,\n            channel_credentials,\n            insecure,\n            call_credentials,\n            compression,\n            wait_for_ready,\n            timeout,\n            metadata,\n            _registered_method=True,\n        )\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/workflow.py", "content": "import queue\nimport threading\nfrom typing import Union\nfrom .askable import Askable\nfrom .conversation import Conversation\nimport base64\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass WorkflowInput:\n    \"\"\"A class to represent the input to a workflow.\"\"\"\n\n    def __init__(\n        self, text: str, images: list[str] = [], name: str = \"user\", role: str = \"user\"\n    ):\n        \"\"\"Initialize the WorkflowInput object.\n\n        Args:\n            text (str): The text input to the workflow. This can be a question or a statement.\n            images (list[str]): The list of image URLs to include in the input. Optional\n        \"\"\"\n        self.text = text\n        self.images = images\n        self.name = name\n        self.role = role\n\n    # Function to encode the image\n    def _encode_image(self, image_path: str):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n    def add_image_file(self, image_path: str):\n        \"\"\"Add an image file to the input.\"\"\"\n        base64_image = self._encode_image(image_path)\n        url = f\"data:image/jpeg;base64,{base64_image}\"\n        self.images.append(url)\n\n    def add_image_bytes(self, data: bytes):\n        \"\"\"Add an image bytes to the input.\"\"\"\n        base64_image = base64.b64encode(data).decode(\"utf-8\")\n        url = f\"data:image/jpeg;base64,{base64_image}\"\n        self.images.append(url)\n\n    def to_message(self):\n        \"\"\"Convert the WorkflowInput to a message.\"\"\"\n        # See https://platform.openai.com/docs/guides/vision\n        content = [{\"text\": self.text, \"type\": \"text\"}]\n        content.extend(\n            [\n                {\"image_url\": {\"url\": image}, \"type\": \"image_url\"}\n                for image in self.images\n            ]\n        )\n        return {\"role\": \"user\", \"name\": self.name, \"content\": content}\n\n    def to_dict(self):\n        \"\"\"Convert the WorkflowInput to a dictionary.\"\"\"\n        return {\"text\": self.text, \"images\": self.images, \"name\": self.name}\n\n    @classmethod\n    def from_dict(cls, data: dict):\n        \"\"\"Create a WorkflowInput object from a dictionary.\"\"\"\n        return cls(\n            text=data.get(\"text\", \"\"),\n            images=data.get(\"images\", []),\n            name=data.get(\"name\", \"user\"),\n            role=data.get(\"role\", \"user\"),\n        )\n\n\nclass Workflow:\n    \"\"\"A class to represent a workflow that can be run with a given Askable.\"\"\"\n\n    def __init__(\n        self,\n        askable: Askable,\n        conversation: Conversation = None,\n        system_prompt: str = \"\",\n    ):\n        \"\"\"Initialize the Workflow object.\n\n        Args:\n            askable (Askable): The Askable object to use for the workflow.\n            conversation (Conversation): The conversation to use for the workflow. Optional, when not provided, a new conversation will be created.\n            system_prompt (str): The system prompt to use for the workflow. Optional.\n        \"\"\"\n        self.askable = askable\n        self.conversation = conversation or Conversation(messages=[], variables={})\n        self.system_prompt = system_prompt\n\n        logger.debug(\"Workflow initialized\")\n\n    def run(self, workflow_input: Union[str, WorkflowInput, dict]):\n        \"\"\"Run the workflow with the given input.\n\n        Args:\n            workflow_input (Union[str, WorkflowInput]): The input to the workflow. This can be a string or a WorkflowInput object.\n\n        Returns:\n            str: The result code of the workflow execution. Output will be available in the conversation object.\n        \"\"\"\n        self._handle_workflow_input(workflow_input)\n\n        execution_result = self.askable.ask(self.conversation)\n\n        return execution_result\n\n    def _handle_workflow_input(self, workflow_input):\n        logger.debug(\"Running workflow with input: %s\", workflow_input)\n\n        logger.debug(\"Conversation length: %s\", len(self.conversation.messages))\n        if len(self.conversation.messages) == 0:\n            self.conversation.messages.append(\n                {\"role\": \"system\", \"content\": self.system_prompt}\n            )\n            logger.debug(\"Added system prompt to messages: %s\", self.system_prompt)\n\n        if isinstance(workflow_input, WorkflowInput):\n            self.conversation.messages.append(workflow_input.to_message())\n            logger.debug(\"Added user input to messages: %s\", workflow_input.text)\n        elif isinstance(workflow_input, dict):\n            self.conversation.messages.append(\n                WorkflowInput.from_dict(workflow_input).to_message()\n            )\n        elif isinstance(workflow_input, str):\n            self.conversation.messages.append(\n                {\"role\": \"user\", \"name\": \"user\", \"content\": workflow_input}\n            )\n        logger.debug(\"Added user input to messages: %s\", workflow_input)\n\n    def run_stream(self, workflow_input: Union[str, WorkflowInput, dict]):\n        \"\"\"Run the workflow with the given input and stream the conversation updates.\n\n        Args:\n            workflow_input (Union[str, WorkflowInput]): The input to the workflow. This can be a string or a WorkflowInput object.\n\n        Yields:\n            list[str, any]: A list containing the mark and content of the conversation update.\n        \"\"\"\n        self._handle_workflow_input(workflow_input)\n\n        result_queue = queue.Queue()\n\n        def ask_in_thread():\n            try:\n                res = self.askable.ask(self.conversation, stream=True)\n            except Exception as e:\n                logger.error(\"Error during askable.ask: %s\", e)\n                self.conversation.update([\"error\", e])\n                res = \"error\"\n\n            logger.debug(\"Workflow execution result in thread: %s\", res)\n            result_queue.put_nowait(res)\n\n        thread = threading.Thread(target=ask_in_thread)\n        thread.start()\n\n        # In order to break the stream, we need to keep track of nesting levels, using a stack count\n        stack_count = 0\n        for mark, content in self.conversation.stream():\n            # Always update the conversation with the stream content\n            logger.debug(f\"Stream content: {mark}, {content}\")\n            yield [mark, content]\n\n            # Keep track of the stack count to know when to break\n            if mark == \"start\":\n                stack_count += 1\n            elif mark == \"end\":\n                stack_count -= 1\n\n            logger.debug(\"Stack count: %s\", stack_count)\n            if stack_count == 0:\n                logger.debug(\"Received response and stack count is 0, breaking stream.\")\n                break\n            if mark == \"error\":\n                logger.debug(\"Received error, breaking stream.\")\n                break\n\n        logger.debug(\"Joining thread\")\n        thread.join()\n        result = result_queue.get()\n        logger.debug(\"Workflow execution result: %s\", result)\n\n        yield [\"result\", result]\n\n    def restart(self):\n        \"\"\"Restart the workflow by clearing the conversation.\"\"\"\n        self.conversation = Conversation(messages=[], variables={})\n        logger.debug(\"Conversation length: %s\", len(self.conversation.messages))\n        logger.debug(\"Restarted workflow, cleared conversation.\")\n"}
{"type": "source_file", "path": "samples/remote/rest/rest_host.py", "content": "import os, sys, signal\n\nsys.path.append(os.path.abspath(os.path.join('../../../vanilla_aiagents')))\nfrom dotenv import load_dotenv\nload_dotenv(override=True)\n\n\nfrom vanilla_aiagents.askable import Askable\nfrom vanilla_aiagents.remote.remote import RESTHost\n\n# Find all askables in the current directory, and import them as modules\nimport os\nimport importlib.util\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef find_askables():\n    askables = []\n    for filename in os.listdir(os.path.dirname(os.path.realpath(__file__))):\n        if filename.endswith(\"_agent.py\") and filename != \"main.py\":\n            module_name = filename[:-3]\n            spec = importlib.util.spec_from_file_location(module_name, filename)\n            module = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(module)\n            for name in dir(module):\n                obj = getattr(module, name)\n                if isinstance(obj, Askable):\n                    askables.append(obj)\n    return askables\n\nif __name__ == \"__main__\":\n    host = RESTHost(askables=find_askables(), host=os.getenv(\"HOST\", \"0.0.0.0\"), port=int(os.getenv(\"PORT\", 8000)))\n    \n    # Handle SIGINT\n    def signal_handler(*args):\n        print(\"Stopping server...\")\n        host.stop()\n        sys.exit(0)\n    signal.signal(signal.SIGINT, signal_handler)\n    \n    host.start()\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/__init__.py", "content": "\"\"\"\nLightweight library demonstrating how to create a simple agenting application without using any specific framework.\n\n## Table of Contents\n\n- [Features](#features)\n- [Future work](#future-work)\n- [Getting Started](#getting-started)\n- [Demos](#demos)\n- [Testing](#testing)\n- [License](#license)\n- [Contributing](#contributing)\n\n## Features\n\nThis project framework provides the following features:\n\n- Multi-agent chat\n- Agent routing (including option to look for available tools to decide)\n- Agent state management\n- Custom stop conditions\n- Interactive or unattended user input\n- Chat resumability\n- Function calling on agents\n- Constrained agent routing\n- Sub-workflows\n- Simple RAG via function calls\n- Image input support\n- Ability to run pre and post steps via Sequence\n- Conversation context \"hidden\" variables, which are not displayed to the user but agents can read and write to access additional information\n- Usage metrics tracking per conversation, plus internal log for debuggability\n- Multiple strategies for agent to filter conversation messages (All, last N, top K and Last N, summarize, etc..)\n- LLMLingua (`extras` module) support to compress system prompts via strategies\n- LLM support for Structured Output\n- Remoting support ((`remote` module)), allowing agents to be run on a remote server and accessed elsewhere\n  - REST and gRPC channels supported\n  - Default implementation to run hosts with agent discovery and registration\n- Generated Code execution locally and via ACA Dynamic Sessions\n- Streaming support, even over REST or gRPC agents\n\n## Future work\n\n- Plugins\n  - Azure AI Search plugin\n  - DB plugin\n  - API plugin\n- DAPR integration\n- Multi-agent chat with multiple users\n\n## Getting Started\n\n### Prerequisites\n\nPython 3.11 or later is required to run this project.\n\n### Quickstart\n\n```powershell\ngit clone https://github.com/Azure-Samples/vanilla-aiagents\n\ncd \"vanilla-aiagents\"\n\n# Create a virtual environment\npython -m venv .venv\n\n# Activate the virtual environment\n\n# On Windows\n.\\.venv\\Scripts\\activate\n# On Unix or MacOS\nsource .venv/bin/activate\n\n# Install the required dependencies\npip install -r requirements.txt\n\n# Clone .env.sample to .env and update the values\ncp .env.sample .env\n```\n\nHere is a simple example of how to use the framework:\n\n```python\nimport os\nfrom vanilla_aiagents.llm import AzureOpenAILLM\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.team import Team\nfrom vanilla_aiagents.workflow import Workflow\n\nllm = AzureOpenAILLM({\n    \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n    \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n    \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n})\n\n# Initialize agents and team\nsales = Agent(id=\"sales\", llm=llm, description=\"A sales agent\", system_message=\\\"\"\"\nYou are a sales assistant. You provide information about our products and services.\n\n# PRODUCTS\n- Product 1: $100, description\n- Product 2: $200, description\n- Product 3: $300, description\n\\\"\"\")\nsupport = Agent(id=\"support\", llm=llm, description=\"A support agent\", system_message=\\\"\"\"\nYou are a support assistant. You provide help with technical issues and account management.\n\n# SUPPORT GUIDELINES\n- For technical issues, please provide the following information: ...\n- For account management, please provide the following information: ...\n\\\"\"\")\nteam = Team(id=\"team\", description=\"Contoso team\", members=[sales, support], llm=llm)\n\n# Create a workflow\nworkflow = Workflow(askable=team)\n\n# Run the workflow\nresult = workflow.run(\"Hello, I'd like to know more about your products.\")\nprint(workflow.conversation.messages)\n```\n\"\"\"\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/remote/run_host.py", "content": "import argparse\nimport asyncio\nimport logging\n\nfrom .remote import AskableHost, RESTHost, find_askables\nfrom .grpc import GRPCHost\n\nfrom dotenv import load_dotenv\n\nload_dotenv(override=True)\n\n# Configure logging\nlogger = logging.getLogger(__name__)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run the RESTHost server.\")\n    parser.add_argument(\n        \"--type\", type=str, default=\"rest\", help=\"Type of server to run (rest or grpc).\"\n    )\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to run the server on.\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=7000, help=\"Port to run the server on.\"\n    )\n    parser.add_argument(\n        \"--source-dir\", type=str, default=None, help=\"Directory to search for askables.\"\n    )\n    parser.add_argument(\"--log-level\", default=\"INFO\", help=\"Set the logging level\")\n\n    args = parser.parse_args()\n\n    log_level = getattr(logging, args.log_level.upper(), logging.INFO)\n    logging.basicConfig(level=log_level)\n    logger.setLevel(log_level)\n\n    askables = find_askables(args.source_dir)\n\n    if len(askables) == 0:\n        logger.error(\"No askables found\")\n        return\n\n    logger.info(\"Found %d askables\", len(askables))\n    host: AskableHost = None\n    if args.type == \"rest\":\n        host = RESTHost(askables, args.host, args.port)\n    elif args.type == \"grpc\":\n        host = GRPCHost(askables, args.host, args.port)\n    else:\n        raise ValueError(f\"Invalid server type: {args.type}\")\n\n    try:\n        logger.info(f\"Starting {args.type} server on {args.host}:{args.port}...\")\n        host.start()\n        loop = asyncio.get_event_loop()\n        try:\n            loop.run_forever()\n        finally:\n            loop.close()\n    except KeyboardInterrupt:\n        logger.info(\"Stopping server...\")\n        host.stop()\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/agent.py", "content": "import logging\nfrom typing import Annotated, Callable, Optional\nimport json\n\nfrom .conversation import (\n    AllMessagesStrategy,\n    AppendMessagesUpdateStrategy,\n    Conversation,\n    ConversationReadingStrategy,\n    ConversationUpdateStrategy,\n)\n\nfrom .askable import Askable\nfrom .function_utils import get_function_schema, wrap_function, F\nfrom .llm import LLM\n\n# Configure logging\nlogger = logging.getLogger(__name__)\n\n\nclass Agent(Askable):\n    \"\"\"An agent that can be asked to solve the user inquiry by using a language model.\n\n    Args:\n        description (str): The description of the agent. Will be used by orchestrator to decide which agent to ask.\n        id (str): The unique identifier of the agent.\n        system_message (str): The system message to provide instructions to the LLM.\n        llm (LLM): The language model to use for the decision-making process.\n        reading_strategy (ConversationReadingStrategy): The reading strategy to use to select the messages to pass to the LLM.\n        update_strategy (ConversationUpdateStrategy): The update strategy to use to update the conversation with the response.\n    \"\"\"\n\n    def __init__(\n        self,\n        description: str,\n        id: str,\n        system_message: str,\n        llm: LLM,\n        reading_strategy: ConversationReadingStrategy = AllMessagesStrategy(),\n        update_strategy: ConversationUpdateStrategy = AppendMessagesUpdateStrategy(),\n    ):\n        \"\"\"Initialize the Agent object.\n\n        Args:\n            description (str): The description of the Agent object. Typically used by orchestrators.\n            id (str): The ID of the Agent object. Will be used to uniquely identify it.\n            system_message (str): The system message to define instructions on how the agent should behave (with the LLM).\n            llm (LLM): The language model to use for the decision-making process.\n            reading_strategy (ConversationReadingStrategy): The reading strategy to use to select the messages to pass to the LLM.\n            update_strategy (ConversationUpdateStrategy): The update strategy to use to update the conversation with the response.\n        \"\"\"\n        super().__init__(id, description)\n        self.tools = []\n        self.tools_function = {}\n        self.llm = llm\n        self.system_message = system_message\n        self.reading_strategy = reading_strategy\n        self.update_strategy = update_strategy\n\n        logger.debug(\n            f\"Agent initialized with ID: {self.id}, Description: {self.description}\"\n        )\n\n    def ask(self, conversation: Conversation, stream=False):\n        \"\"\"Ask the agent to solve the user inquiry by using the language model.\n\n        This method will prepare the messages to send to the LLM, call the LLM, and update the conversation with the response.\n\n        Args:\n            conversation (Conversation): The conversation to use for the execution\n            stream (bool): Whether to stream the conversation updates.\n        \"\"\"\n        logger.debug(\n            f\"[Agent ID: {self.id}] Received messages: %s\", conversation.messages\n        )\n\n        local_messages = self._prepare_llm_input(conversation)\n        local_tools, local_tools_function = self._prepare_llm_tools(\n            conversation=conversation\n        )\n\n        try:\n            if not stream:\n                response, usage = self.llm.ask(\n                    messages=local_messages,\n                    tools=local_tools,\n                    tools_function=local_tools_function,\n                )\n                logger.debug(\n                    f\"[Agent ID: {self.id}] API response received: %s\", response\n                )\n                response_message = response.model_dump()\n            else:\n                gen = self.llm.ask_stream(\n                    messages=local_messages,\n                    tools=local_tools,\n                    tools_function=local_tools_function,\n                )\n                # logger.debug(f\"[Agent ID: {self.id}] Stream started\")\n                response_message = None\n                usage = None\n                for mark, content in gen:\n\n                    if mark == \"start\" or mark == \"end\":\n                        content = self.id\n                    if mark == \"response\" and content is not None:\n                        response_message, usage = content\n\n                    conversation.update([mark, content])\n\n            if usage is not None:\n                # Update conversation metrics with response usage\n                conversation.metrics.total_tokens += usage[\"total_tokens\"]\n                conversation.metrics.prompt_tokens += usage[\"prompt_tokens\"]\n                conversation.metrics.completion_tokens += usage[\"completion_tokens\"]\n        except Exception as e:\n            logger.error(f\"[Agent ID: {self.id}] Error during LLM call: %s\", e)\n            conversation.log.append((\"error\", \"agent/error\", self.id, e))\n            return \"error\"\n\n        response_message[\"name\"] = self.id\n        self.update_strategy.update(conversation, response_message)\n        logger.debug(f\"[Agent ID: {self.id}] Response message: %s\", response_message)\n\n        return \"done\"\n\n    def _prepare_llm_tools(self, conversation: Conversation):\n\n        # Closure function to update a conversation variable\n        def update_conversation_variable(\n            variableName: Annotated[str, \"The variable name to update\"],\n            variableValue: Annotated[str, \"The new value of the variable\"],\n        ) -> Annotated[str, \"Confirmation that the variable was updated\"]:\n            conversation.variables[variableName] = variableValue\n            return f\"Variable {variableName} updated to {variableValue}\"\n\n        s = get_function_schema(\n            update_conversation_variable,\n            name=\"update_conversation_variable\",\n            description=\"update a conversation or context variable\",\n        )\n        local_tools = self.tools + [s]\n        local_tools_function = {\n            **self.tools_function,\n            \"update_conversation_variable\": wrap_function(update_conversation_variable),\n        }\n        return local_tools, local_tools_function\n\n    def _prepare_llm_input(self, conversation):\n        local_messages = []\n        local_messages.append(\n            {\n                \"role\": \"system\",\n                \"content\": self.system_message.replace(\n                    \"__context__\", json.dumps(conversation.variables)\n                ),\n            }\n        )\n        local_messages.extend(self.reading_strategy.get_messages(conversation))\n        logger.debug(\n            f\"[Agent ID: {self.id}] Local messages prepared for API call (last 3): %s\",\n            local_messages[-3:],\n        )\n        return local_messages\n\n    def register_tool(\n        self,\n        *,\n        name: Optional[str] = None,\n        description: Optional[str] = None,\n    ) -> Callable[[F], F]:\n        \"\"\"Decorate registering function to be used by an agent as a tool.\n\n        NOTE: remember to annotate the function with the types of the parameters and the\n        return value.\n\n        Args:\n            name (str): The name of the tool. If not provided, the function name will be used.\n            description (str): The description of the tool. If not provided, the function description will be used.\n        \"\"\"\n        def _decorator(func: F) -> F:\n            \"\"\"Decorate registering function to be used by an agent.\n\n            Args:\n                func: the function to be registered.\n\n            Returns:\n                The function to be registered, with the _description attribute set to the function description.\n\n            Raises:\n                ValueError: if the function description is not provided and not propagated by a previous decorator.\n                RuntimeError: if the LLM config is not set up before registering a function.\n            \"\"\"\n            # name can be overwritten by the parameter, by default it is the same as function name\n            if name:\n                func._name = name\n            elif not hasattr(func, \"_name\"):\n                func._name = func.__name__\n\n            # description is propagated from the previous decorator, but it is mandatory for the first one\n            if description:\n                func._description = description\n            else:\n                if not hasattr(func, \"_description\"):\n                    raise ValueError(\"Function description is required, none found.\")\n\n            # get JSON schema for the function\n            f = get_function_schema(\n                func, name=func._name, description=func._description\n            )\n\n            logger.debug(\n                f\"[Agent ID: {self.id}] Registering tool: %s with description: %s\",\n                func._name,\n                func._description,\n            )\n            if not self.tools:\n                self.tools = []\n                self.tools_function = {}\n            self.tools.append(f)\n            self.tools_function[func._name] = wrap_function(func)\n            logger.debug(\n                f\"[Agent ID: {self.id}] Tool registered successfully: %s\", func._name\n            )\n\n            return func\n\n        return _decorator\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/azure_coding_agent.py", "content": "from typing import Annotated\nimport os\nimport requests\nfrom azure.identity import DefaultAzureCredential\nfrom .llm import LLM\nfrom .conversation import AllMessagesStrategy, ConversationReadingStrategy\nfrom .agent import Agent\n\n# Configure logging\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ncredential = DefaultAzureCredential()\ntoken = credential.get_token(\"https://dynamicsessions.io/.default\")\n\n\nclass AzureCodingAgent(Agent):\n    \"\"\"\n    An agent that can run Python code snippets in the Azure Dynamic Sessions environment.\n\n    Args:\n        id (str): The unique identifier of the agent.\n        description (str): The description of the agent. Will be used by orchestrator to decide which agent to ask.\n        llm (LLM): The language model to use for the decision-making process.\n        reading_strategy (ConversationReadingStrategy): The reading strategy to use to select the messages to pass\n    \"\"\"\n\n    def __init__(\n        self,\n        id: str,\n        description: str,\n        llm: LLM,\n        reading_strategy: ConversationReadingStrategy = AllMessagesStrategy(),\n    ):\n        \"\"\"\n        Initialize the AzureCodingAgent.\n\n        Args:\n            id (str): The unique identifier of the agent.\n            description (str): The description of the agent. Will be used by orchestrator to decide which agent to ask.\n            llm (LLM): The language model to use for the decision-making process.\n            reading_strategy (ConversationReadingStrategy): The reading strategy to use to select the messages to pass\n        \"\"\"\n        system_message = \"\"\"\n        You are an expert Python developer.\n        Your task is to write a Python code snippet to solve a given problem.\n        ALWAYS adhere to the guidelines provided.\n\n        ## GUIDELINES\n        - You can use any Python libraries you need, but in case you must invoke the proper function call to install them first.\n        - The code MUST be written in Python.\n        - The code MUST be secure and efficient. NEVER use code that can be harmful.\n        - The code MUST AVOID any side effects, especially reading or writing to the file system.\n        - The code MUST NOT read or write files or directories outside of the code execution environment.\n        - If you cannot comply with these guidelines, please return an error message.\n\n        ## INSTRUCTIONS\n        - Write and run the Python code to solve the problem and return the output of the code execution\n\n        ## ADDITIONAL CONTEXT\n        __context__\n\n        \"\"\"\n        super().__init__(\n            id=id,\n            description=description,\n            system_message=system_message,\n            llm=llm,\n            reading_strategy=reading_strategy,\n        )\n\n        logger.debug(\n            f\"CodingAgent initialized with ID: {self.id}, Description: {self.description}\"\n        )\n        self.register_tool(description=\"Runs the provided Python code block\")(run_code)\n\n\ndef run_code(\n    conversation_id: Annotated[str, \"Conversation ID\"],\n    code: Annotated[str, \"Python code to run\"],\n) -> Annotated[str, \"Python code output\"]:\n    \"\"\"Run the provided Python code in the Azure Dynamic Sessions environment.\n\n    Args:\n        conversation_id (str): The ID of the conversation.\n        code (str): The Python code to run.\n\n    Returns:\n        str: The output of the Python code execution.\n    \"\"\"\n    logger.info(\"Starting execution of provided code\")\n    logger.debug(f\"Code: {code}\")\n\n    management_endpoint = os.getenv(\"AZURE_DYNAMIC_SESSIONS_ENDPOINT\")\n    if not management_endpoint:\n        logger.error(\"AZURE_DYNAMIC_SESSIONS_ENDPOINT environment variable is not set\")\n        return \"Error: AZURE_DYNAMIC_SESSIONS_ENDPOINT environment variable is not set\"\n\n    try:\n        response = requests.post(\n            f\"{management_endpoint}/code/execute?api-version=2024-02-02-preview&identifier={conversation_id}\",\n            headers={\"Authorization\": f\"Bearer {token.token}\"},\n            json={\n                \"properties\": {\n                    \"codeInputType\": \"inline\",\n                    \"executionType\": \"synchronous\",\n                    \"code\": code,\n                }\n            },\n        )\n\n        response.raise_for_status()\n\n        response_json = response.json()\n        logger.info(\"Code execution completed successfully\")\n        return response_json[\"properties\"]\n\n    except requests.exceptions.RequestException as e:\n        logger.error(\n            f\"Request failed: {e}, Response: {e.response.text if e.response else 'No response'}\"\n        )\n        return f\"Error: Request failed with exception {e}, Response: {e.response.text if e.response else 'No response'}\"\n\n    except ValueError as e:\n        logger.error(f\"JSON decoding failed: {e}\")\n        return f\"Error: JSON decoding failed with exception {e}\"\n\n    except Exception as e:\n        logger.error(f\"An unexpected error occurred: {e}\")\n        return f\"Error: An unexpected error occurred: {e}\"\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/_pydantic.py", "content": "from typing import Any, Dict, Tuple, Union, get_args\n\nfrom pydantic import BaseModel\nfrom pydantic.version import VERSION as PYDANTIC_VERSION\nfrom typing_extensions import get_origin\n\n__all__ = (\n    \"JsonSchemaValue\",\n    \"model_dump\",\n    \"model_dump_json\",\n    \"type2schema\",\n    \"evaluate_forwardref\",\n)\n\nPYDANTIC_V1 = PYDANTIC_VERSION.startswith(\"1.\")\n\nif not PYDANTIC_V1:\n    from pydantic import TypeAdapter\n    from pydantic._internal._typing_extra import (\n        eval_type_lenient as evaluate_forwardref,\n    )\n    from pydantic.json_schema import JsonSchemaValue\n\n    def type2schema(t: Any) -> JsonSchemaValue:\n        \"\"\"Convert a type to a JSON schema.\n\n        Args:\n            t (Type): The type to convert\n\n        Returns:\n            JsonSchemaValue: The JSON schema\n        \"\"\"\n        return TypeAdapter(t).json_schema()\n\n    def model_dump(model: BaseModel) -> Dict[str, Any]:\n        \"\"\"Convert a pydantic model to a dict.\n\n        Args:\n            model (BaseModel): The model to convert\n\n        Returns:\n            Dict[str, Any]: The dict representation of the model\n        \"\"\"\n        return model.model_dump()\n\n    def model_dump_json(model: BaseModel) -> str:\n        \"\"\"Convert a pydantic model to a JSON string.\n\n        Args:\n            model (BaseModel): The model to convert\n\n        Returns:\n            str: The JSON string representation of the model\n        \"\"\"\n        return model.model_dump_json()\n\n\n# Remove this once we drop support for pydantic 1.x\nelse:  # pragma: no cover\n    from pydantic import schema_of\n    from pydantic.typing import evaluate_forwardref as evaluate_forwardref  # type: ignore[no-redef]\n\n    JsonSchemaValue = Dict[str, Any]  # type: ignore[misc]\n\n    def type2schema(t: Any) -> JsonSchemaValue:\n        \"\"\"Convert a type to a JSON schema.\n\n        Args:\n            t (Type): The type to convert\n\n        Returns:\n            JsonSchemaValue: The JSON schema\n        \"\"\"\n        if t is None:\n            return {\"type\": \"null\"}\n        elif get_origin(t) is Union:\n            return {\"anyOf\": [type2schema(tt) for tt in get_args(t)]}\n        elif get_origin(t) in [Tuple, tuple]:\n            prefixItems = [type2schema(tt) for tt in get_args(t)]\n            return {\n                \"maxItems\": len(prefixItems),\n                \"minItems\": len(prefixItems),\n                \"prefixItems\": prefixItems,\n                \"type\": \"array\",\n            }\n        else:\n            d = schema_of(t)\n            if \"title\" in d:\n                d.pop(\"title\")\n            if \"description\" in d:\n                d.pop(\"description\")\n\n            return d\n\n    def model_dump(model: BaseModel) -> Dict[str, Any]:\n        \"\"\"Convert a pydantic model to a dict.\n\n        Args:\n            model (BaseModel): The model to convert\n\n        Returns:\n            Dict[str, Any]: The dict representation of the model\n        \"\"\"\n        return model.dict()\n\n    def model_dump_json(model: BaseModel) -> str:\n        \"\"\"Convert a pydantic model to a JSON string.\n\n        Args:\n            model (BaseModel): The model to convert\n\n        Returns:\n            str: The JSON string representation of the model\n        \"\"\"\n        return model.json()\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/llm.py", "content": "from collections import defaultdict\nfrom typing import Generator, NamedTuple, Optional\nfrom openai import NOT_GIVEN, AzureOpenAI, Stream\nfrom openai.types.chat import ChatCompletionChunk\nfrom abc import ABC, abstractmethod\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\nimport json\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass LLMConstraints(NamedTuple):\n    system_message: bool = False\n    structured_output: bool = True\n    temperature: Optional[float] = None\n\n\nclass LLM(ABC):\n    \"\"\"Abstract class for the Language Model clients.\"\"\"\n\n    def __init__(self, config: dict, constraints: Optional[LLMConstraints] = LLMConstraints()):\n        \"\"\"\n        Initialize the LLM client.\n\n        Args:\n            config (dict): The configuration for the client.\n        \"\"\"\n        self.config = config\n        self.constraints = constraints\n\n    @abstractmethod\n    def ask(\n        self,\n        messages: list,\n        tools: list = None,\n        tools_function: dict[str, callable] = None,\n        temperature: float = 0.7,\n        response_format=None,\n    ) -> tuple[dict, dict]:\n        pass\n\n    @abstractmethod\n    def ask_stream(\n        self,\n        messages: list,\n        tools: list = None,\n        tools_function: dict[str, callable] = None,\n        temperature: float = 0.7,\n    ) -> Generator[tuple[str, any], None, tuple[dict, any]]:\n        pass\n    \n    def _check_system_messages(self, messages: list) -> list:\n        \"\"\"Ensures that system messages are not sent to the LLM when the constraints are set.\n\n        Args:\n            messages (list): The list of messages to convert.\n\n        Returns:\n            list: The converted list of messages.\n        \"\"\"\n        if not self.constraints.system_message:\n            copy = messages.copy()\n            for message in copy:\n                if message[\"role\"] == \"system\":\n                    message[\"role\"] = \"assistant\"\n            return copy\n        \n        return messages\n\n\nclass ErrorTestingLLM(LLM):\n    \"\"\"LLM that raises an error when asked.\n\n    Used for testing error handling.\n    \"\"\"\n\n    def __init__(self, config: dict, constraints: Optional[LLMConstraints] = LLMConstraints()):\n        \"\"\"\n        Initialize the ErrorTestingLLM client.\n\n        Args:\n            config (dict): The configuration for the client.\n        \"\"\"\n        super().__init__(config, constraints=constraints)\n\n    def ask(\n        self,\n        messages: list,\n        tools: list = None,\n        tools_function: dict[str, callable] = None,\n        temperature: float = 0.7,\n        response_format=None,\n    ):\n        raise Exception(\"Fake error\")\n\n    def ask_stream(\n        self,\n        messages: list,\n        tools: list = None,\n        tools_function: dict[str, callable] = None,\n        temperature: float = 0.7,\n    ):\n        yield [\"start\", \"\"]\n        yield [\"error\", \"Fake error\"]\n        yield [\"end\", \"\"]\n        return None, None\n\n\nclass AzureOpenAILLM(LLM):\n    \"\"\"LLM using Azure OpenAI API.\n\n    Args:\n    - config: dict with the following\n        - azure_deployment: str, Azure deployment name\n        - azure_endpoint: str, Azure endpoint\n        - api_key: str, Azure API key. Leave empty if using Azure AD token provider\n        - api_version: str, API version\n    \"\"\"\n\n    def __init__(self, config: dict, constraints: Optional[LLMConstraints] = LLMConstraints()):\n        \"\"\"Initialize the AzureOpenAILLM client.\n\n        Args:\n            config (dict): The configuration for the client.\n        \"\"\"\n        super().__init__(config, constraints=constraints)\n\n        api_key = self.config[\"api_key\"]\n        token_provider = (\n            get_bearer_token_provider(\n                DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n            )\n            if api_key is None or api_key == \"\"\n            else None\n        )\n        self.client = AzureOpenAI(\n            azure_deployment=self.config[\"azure_deployment\"],\n            api_key=self.config[\"api_key\"],\n            azure_endpoint=self.config[\"azure_endpoint\"],\n            api_version=self.config[\"api_version\"],\n            azure_ad_token_provider=token_provider,\n        )\n        logger.debug(\n            \"LLM initialized with AzureOpenAI client with %s\",\n            \"api_key\" if api_key else \"token provider\",\n        )\n\n    def ask(\n        self,\n        messages: list,\n        tools: list = None,\n        tools_function: dict[str, callable] = None,\n        temperature: float = 0.7,\n        response_format=NOT_GIVEN,\n    ):\n        \"\"\"Ask the LLM to generate a completion given the messages.\n\n        Args:\n            messages (list): The list of messages to send to the LLM.\n            tools (list): The list of tools to use in the LLM.\n            tools_function (dict): The dictionary of tool functions to use in the LLM.\n            temperature (float): The temperature to use in the LLM.\n            response_format: The response format to use in the LLM (Structured Output)\n\n        Returns:\n            tuple: The response message and the usage metrics.\n        \"\"\"\n        # logger.debug(\"Received messages: %s\", messages)\n        \n        temperature = self.constraints.temperature if self.constraints.temperature else temperature\n        messages = self._check_system_messages(messages)\n\n        if not self.constraints.structured_output or response_format is NOT_GIVEN:\n            response = self.client.chat.completions.create(\n                messages=messages,\n                model=self.config[\"azure_deployment\"],\n                tools=tools if tools and len(tools) > 0 else NOT_GIVEN,\n                temperature=temperature,\n                tool_choice=\"auto\" if tools else NOT_GIVEN,\n            )\n        else:\n            response = self.client.beta.chat.completions.parse(\n                messages=messages,\n                model=self.config[\"azure_deployment\"],\n                tools=tools if tools and len(tools) > 0 else NOT_GIVEN,\n                temperature=temperature,\n                tool_choice=\"auto\" if tools else NOT_GIVEN,\n                response_format=response_format,\n            )\n\n        response_message = response.choices[0].message\n        logger.debug(\"Response message: %s\", response_message)\n\n        # Handle function calls (if any)\n        # Must iterate until there are no more tool calls\n        while response_message.tool_calls:\n            logger.debug(\"Tool calls detected: %s\", response_message.tool_calls)\n            messages.append(response.choices[0].message)\n            for tool_call in response_message.tool_calls:\n                function_args = json.loads(tool_call.function.arguments)\n                logger.debug(\"Function arguments: %s\", function_args)\n\n                function_result = tools_function[tool_call.function.name](\n                    **function_args\n                )\n                logger.debug(\"Function result: %s\", function_result)\n\n                messages.append(\n                    {\n                        \"tool_call_id\": tool_call.id,\n                        \"role\": \"tool\",\n                        \"name\": tool_call.function.name,\n                        \"content\": function_result,\n                    }\n                )\n\n            # Second API call: Get the next response from the model given the func call result\n            response = self.client.chat.completions.create(\n                messages=messages,\n                model=self.config[\"azure_deployment\"],\n                tools=tools,\n                temperature=temperature,\n                tool_choice=\"auto\" if tools else None,\n            )\n            response_message = response.choices[0].message\n\n        logger.debug(\"Final response message: %s\", response_message)\n\n        # NOTE purposely not returning all the intermediate messages, only the final response\n\n        return response_message, {\n            \"completion_tokens\": response.usage.completion_tokens,\n            \"prompt_tokens\": response.usage.prompt_tokens,\n            \"total_tokens\": response.usage.total_tokens,\n        }\n\n    def ask_stream(\n        self,\n        messages: list,\n        tools: list = None,\n        tools_function: dict[str, callable] = None,\n        temperature: float = 0.7,\n    ):\n        \"\"\"Ask the LLM to generate a completion given the messages and stream the updates.\n\n        Args:\n            messages (list): The list of messages to send to the LLM.\n            tools (list): The list of tools to use in the LLM.\n            tools_function (dict): The dictionary of tool functions to use in the LLM.\n            temperature (float): The temperature to use in the LLM.\n\n        Yields:\n            tuple: The mark and content of the conversation update.\n        \"\"\"\n        # Accumulate messages and usage\n        response_message = None\n        usage = {\"completion_tokens\": 0, \"prompt_tokens\": 0, \"total_tokens\": 0}\n        temperature = self.constraints.temperature if self.constraints.temperature else temperature\n        messages = self._check_system_messages(messages)\n\n        yield [\"start\", \"\"]\n        while True:\n            response_message = {\n                \"content\": \"\",\n                \"role\": \"assistant\",\n                \"function_call\": None,\n                \"tool_calls\": defaultdict(\n                    lambda: {\n                        \"function\": {\"arguments\": \"\", \"name\": \"\"},\n                        \"id\": \"\",\n                        \"type\": \"\",\n                    }\n                ),\n            }\n\n            # Call LLM with stream=True\n            completion: Stream[ChatCompletionChunk] = (\n                self.client.chat.completions.create(\n                    messages=messages,\n                    model=self.config[\"azure_deployment\"],\n                    tools=tools,\n                    temperature=temperature,\n                    tool_choice=\"auto\" if tools else None,\n                    stream=True,\n                    stream_options={\"include_usage\": True},\n                )\n            )\n\n            # Yield the intermediate updates\n            for chunk in completion:\n                if len(chunk.choices) > 0:\n                    delta = json.loads(chunk.choices[0].delta.model_dump_json())\n                    yield [\"delta\", delta]\n                    delta.pop(\"role\", None)\n                    delta.pop(\"name\", None)\n                    # Update the accumulated response message\n                    merge_chunk(response_message, delta)\n                # Also accumulate usage, if any\n                if chunk.usage:\n                    usage[\"completion_tokens\"] += chunk.usage.completion_tokens\n                    usage[\"prompt_tokens\"] += chunk.usage.prompt_tokens\n                    usage[\"total_tokens\"] += chunk.usage.total_tokens\n\n            logger.debug(\"Response message: %s\", response_message)\n\n            # Handle function calls (if any)\n            if (\n                not response_message[\"tool_calls\"]\n                or len(response_message[\"tool_calls\"]) == 0\n            ):\n                break\n            else:\n                response_message[\"tool_calls\"] = list(\n                    response_message.get(\"tool_calls\", {}).values()\n                )\n\n            logger.debug(\"Tool calls detected: %s\", response_message[\"tool_calls\"])\n            messages.append(response_message)\n            for tool_call in response_message[\"tool_calls\"]:\n                function_args = json.loads(tool_call[\"function\"][\"arguments\"])\n                logger.debug(\"Function arguments: %s\", function_args)\n\n                function_result = tools_function[tool_call[\"function\"][\"name\"]](\n                    **function_args\n                )\n                logger.debug(\"Function result: %s\", function_result)\n                yield [\n                    \"function_result\",\n                    {\"name\": tool_call[\"function\"][\"name\"], \"result\": function_result},\n                ]\n\n                messages.append(\n                    {\n                        \"tool_call_id\": tool_call[\"id\"],\n                        \"role\": \"tool\",\n                        \"name\": tool_call[\"function\"][\"name\"],\n                        \"content\": function_result,\n                    }\n                )\n            # NOTE: The loop will continue until there are no more tool calls\n\n        # Strip the tool calls from the final response message\n        response_message.pop(\"tool_calls\", None)\n        response_message.pop(\"function_call\", None)\n\n        logger.debug(\"Final response message: %s\", response_message)\n\n        # Return the final response message and usage\n        yield [\"response\", [response_message, usage]]\n\n        yield [\"end\", \"\"]\n\n        return [response_message, usage]\n\n\ndef merge_fields(target, source):\n    for key, value in source.items():\n        if isinstance(value, str):\n            target[key] += value\n        elif value is not None and isinstance(value, dict):\n            merge_fields(target[key], value)\n\n\ndef merge_chunk(source: dict, delta: dict) -> None:\n    delta.pop(\"role\", None)\n    merge_fields(source, delta)\n\n    tool_calls = delta.get(\"tool_calls\")\n    if tool_calls and len(tool_calls) > 0:\n        index = tool_calls[0].pop(\"index\")\n        merge_fields(source[\"tool_calls\"][index], tool_calls[0])\n"}
{"type": "source_file", "path": "samples/remote/grpc/grpc_host.py", "content": "import os, sys\n\nsys.path.append(os.path.abspath(os.path.join('../../../vanilla_aiagents')))\nfrom dotenv import load_dotenv\nload_dotenv(override=True)\n\n\nfrom vanilla_aiagents.askable import Askable\nfrom vanilla_aiagents.remote.grpc import GRPCHost\n\n# Find all askables in the current directory, and import them as modules\nimport os\nimport importlib.util\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n# Find all askables in the current directory, and import them as modules\nimport os\nimport importlib.util\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef find_askables():\n    askables = []\n    for filename in os.listdir(os.path.dirname(os.path.realpath(__file__))):\n        if filename.endswith(\"_agent.py\") and filename != \"main.py\":\n            module_name = filename[:-3]\n            spec = importlib.util.spec_from_file_location(module_name, filename)\n            module = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(module)\n            for name in dir(module):\n                obj = getattr(module, name)\n                if isinstance(obj, Askable):\n                    askables.append(obj)\n    return askables\n\nif __name__ == \"__main__\":\n    host = GRPCHost(askables=find_askables(), host=os.getenv(\"HOST\", \"0.0.0.0\"), port=int(os.getenv(\"PORT\", 8000)))\n    host.start()\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/remote/dapr/run_actors.py", "content": "import argparse\nimport logging\nfrom dotenv import load_dotenv\n\nfrom .actors import InputWorkflowEvent, WorkflowActor, WorkflowActorInterface\nimport os\nimport uvicorn\nfrom contextlib import asynccontextmanager\nfrom fastapi import FastAPI, Request\nfrom dapr.ext.fastapi import DaprActor, DaprApp\nfrom dapr.actor import ActorProxy, ActorId\nfrom cloudevents.http import from_http\n\n\nload_dotenv(override=True)\n# Configure logging\nlogger = logging.getLogger(__name__)\nPUBSUB_NAME = os.getenv(\"PUBSUB_NAME\", \"workflow\")\nTOPIC_NAME = os.getenv(\"TOPIC_NAME\", \"events\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run the Dapr Actor host\")\n    parser.add_argument(\n        \"--host\", type=str, default=\"0.0.0.0\", help=\"Host to run the server on.\"\n    )\n    parser.add_argument(\n        \"--port\", type=int, default=5000, help=\"Port to run the server on.\"\n    )\n    parser.add_argument(\"--log-level\", default=\"INFO\", help=\"Set the logging level\")\n\n    args = parser.parse_args()\n\n    log_level = getattr(logging, args.log_level.upper(), logging.INFO)\n    logging.basicConfig(level=log_level)\n    logger.setLevel(log_level)\n\n    actor: DaprActor = None\n\n    # Register actor when fastapi starts up\n    @asynccontextmanager\n    async def lifespan(app: FastAPI):\n        logger.info(\"~~ actor startup\")\n        await actor.register_actor(WorkflowActor)\n        yield\n\n    # Create fastapi and register dapr, and actors\n    app = FastAPI(title=\"Vanilla AI Agent Dapr Actors host\", lifespan=lifespan)\n    actor = DaprActor(app)\n    dapr_app = DaprApp(app)\n\n    @dapr_app.subscribe(\n        pubsub=PUBSUB_NAME,\n        topic=TOPIC_NAME,\n    )\n    async def handle_workflow_input(req: Request):\n        try:\n\n            # Read fastapi request body as text\n            body = await req.body()\n            logger.info(f\"Received workflow input: {body}\")\n\n            # Parse the body as a CloudEvent\n            event = from_http(data=body, headers=req.headers)\n\n            data = InputWorkflowEvent.model_validate(event.data)\n            proxy: WorkflowActorInterface = ActorProxy.create(\n                \"WorkflowActor\", ActorId(data.id), WorkflowActorInterface\n            )\n            await proxy.run(data.input)\n\n            return {\"status\": \"SUCCESS\"}\n        except Exception as e:\n            logger.error(f\"Error handling workflow input: {e}\")\n            return {\"status\": \"DROP\", \"message\": str(e)}\n\n    uvicorn.run(app, host=args.host, port=args.port)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/remote/dapr/actors.py", "content": "import os\nimport importlib.util\nfrom typing import Literal, Type, Union\nfrom dapr.actor import ActorInterface, Actor, actormethod\nfrom dapr.clients import DaprClient\nfrom pydantic import BaseModel\n\nfrom vanilla_aiagents.askable import Askable\nfrom vanilla_aiagents.conversation import Conversation\nfrom vanilla_aiagents.workflow import Workflow\nimport logging\n\nlogger = logging.getLogger(__name__)\nPUBSUB_NAME = os.getenv(\"PUBSUB_NAME\", \"workflow\")\nTOPIC_NAME = os.getenv(\"TOPIC_NAME\", \"events\")\nACTOR_ENTRYPOINT = os.getenv(\"ACTOR_ENTRYPOINT\", \"_actor_askable.py\")\nACTOR_VARIABLE = os.getenv(\"ACTOR_VARIABLE\", \"_actor_askable\")\n\n\nclass WorkflowRunResult(BaseModel):\n    result: str = \"\"\n    messages: list[dict] = []\n\n\nclass WorkflowEvent(BaseModel):\n    type: str = \"\"\n    id: str = \"\"\n\n    def json(self) -> str:\n        return self.model_dump_json()\n\n    def metadata(self) -> dict:\n        return {\n            \"id\": self.id,\n            \"type\": self.type,\n        }\n\n    # @classmethod\n\n\nclass StopWorkflowEvent(WorkflowEvent):\n    source: str = \"\"\n    conversation: dict = {}\n    type: Literal[\"stop\"] = \"stop\"\n\n    def metadata(self):\n        return {**super().metadata(), \"source\": self.source}\n\n\nclass StreamChunkWorkflowEvent(WorkflowEvent):\n    mark: str = \"\"\n    content: str = \"\"\n    type: Literal[\"stream\"] = \"stream\"\n\n    def metadata(self):\n        return {**super().metadata(), \"mark\": self.mark}\n\n\nclass InputWorkflowEvent(WorkflowEvent):\n    input: Union[str | dict] = \"\"\n    type: Literal[\"input\"] = \"input\"\n\n\nclass WorkflowActorInterface(ActorInterface):\n    @actormethod(name=\"run\")\n    async def run(self, workflow_input: Union[str, dict]) -> dict: ...\n\n    @actormethod(name=\"run_stream\")\n    async def run_stream(self, workflow_input: Union[str, dict]) -> None: ...\n\n    @actormethod(name=\"get_conversation\")\n    async def get_conversation(self) -> dict: ...\n\n\nclass WorkflowActor(Actor, WorkflowActorInterface):\n    workflow: Workflow\n    askable_type: Type[Askable]\n\n    async def _on_activate(self) -> None:\n        # Load state on activation\n        (exists, state) = await self._state_manager.try_get_state(\"conversation\")\n\n        # Dynamically import the module and class from a local file\n        source_dir = os.curdir\n        file_path = os.path.join(source_dir, ACTOR_ENTRYPOINT)\n        module_name = ACTOR_ENTRYPOINT.rstrip(\".py\")\n        logger.info(f\"Loading askable from {file_path}\")\n\n        spec = importlib.util.spec_from_file_location(module_name, file_path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        askable_class = getattr(module, ACTOR_VARIABLE)\n        logger.info(f\"Loaded askable: {askable_class}\")\n\n        self.workflow = Workflow(\n            askable=askable_class,\n            conversation=Conversation.from_dict(state if state is not None else {}),\n        )\n\n    async def get_conversation(self) -> dict:\n        logger.debug(f\"Getting conversation for {self.id}\")\n        return self.workflow.conversation.to_dict()\n\n    async def run(self, workflow_input: Union[str, dict]) -> dict:\n        run_result = WorkflowRunResult(result=\"\", messages=[])\n\n        try:\n            n = len(self.workflow.conversation.messages)\n            run_result.result = self.workflow.run(workflow_input)\n            run_result.messages = self.workflow.conversation.messages[n + 1 :]\n\n            if run_result.result == \"agent-stop\":\n                self._notify_stop()\n\n            # Save state\n            await self._save_conversation()\n        except Exception as e:\n            logger.error(f\"Error running workflow: {e}\")\n            run_result.result = str(e)\n\n        return run_result.model_dump()\n\n    async def run_stream(self, workflow_input: Union[str, dict]):\n        result: str = None\n        with DaprClient() as client:\n            async for [mark, content] in self.workflow.run_stream(workflow_input):\n                event = StreamChunkWorkflowEvent(\n                    type=\"stream\", id=str(self.id), mark=mark, content=content\n                )\n                client.publish_event(\n                    pubsub_name=PUBSUB_NAME,\n                    topic_name=TOPIC_NAME,\n                    data_content_type=\"application/json\",\n                    data=event.json(),\n                    publish_metadata=event.metadata(),\n                )\n\n                if mark == \"result\":\n                    result = content\n\n            if result == \"agent-stop\":\n                self._notify_stop()\n\n            await self._save_conversation()\n\n    async def _save_conversation(self):\n        logger.debug(\"Saving conversation state\")\n        await self._state_manager.set_state(\n            \"conversation\", self.workflow.conversation.to_dict()\n        )\n        await self._state_manager.save_state()\n\n        logger.debug(\"Publishing conversation update event\")\n        with DaprClient() as client:\n            event = WorkflowEvent(type=\"update\", id=str(self.id))\n            client.publish_event(\n                PUBSUB_NAME,\n                TOPIC_NAME,\n                publish_metadata=event.metadata(),\n                data_content_type=\"application/json\",\n                data=event.json(),\n            )\n        logger.debug(\"Conversation state saved and event published\")\n\n    def _notify_stop(self):\n        # Retrieve which askable stopped the conversation\n        (level, kind, agent) = next(\n            (l, k, a)\n            for (l, k, a) in reversed(self.workflow.conversation.log)\n            if k == \"agent/stop\"\n        )\n        logger.info(f\"Got stop signal from '{agent}'. Publishing stop event\")\n        with DaprClient() as client:\n            event = StopWorkflowEvent(\n                id=str(self.id),\n                source=agent,\n                type=\"stop\",\n                conversation=self.workflow.conversation.to_dict(),\n            )\n            client.publish_event(\n                PUBSUB_NAME,\n                TOPIC_NAME,\n                publish_metadata=event.metadata(),\n                data_content_type=\"application/json\",\n                data=event.json(),\n            )\n        logger.debug(\"Stop event published\")\n"}
{"type": "source_file", "path": "samples/remote/rest/guess_number_entry.py", "content": "from typing import Annotated\nimport os\nfrom vanilla_aiagents.agent import Agent\nfrom vanilla_aiagents.llm import AzureOpenAILLM\n\nllm = AzureOpenAILLM({\n            \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n            \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n            \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n            \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n        })\n\nguess_number = Agent(id=\"agent\", llm=llm, description=\"Call this agent to play guess the number game with the use\", system_message = \"\"\"You are an AI assistant\n        Your task is to play a game with the user.\n        You first generate a random number between 1 and 100. Then save it as a conversation variable named \"number\".\n        The user will try to guess the number.\n        If the user's guess is too high, respond with \"Too high\".\n        If the user's guess is too low, respond with \"Too low\".\n        \"\"\")\n        \n@guess_number.register_tool(description=\"Generate a random number\")\ndef random() -> Annotated[str, \"A random number\"]:\n    return str(random.randint(1, 100))"}
{"type": "source_file", "path": "vanilla_aiagents/vanilla_aiagents/remote/grpc.py", "content": "import json\nimport queue\nimport threading\nfrom .remote import AskableHost, Connection\nfrom ..conversation import Conversation\nfrom ..askable import Askable\nimport grpc\nfrom grpc_reflection.v1alpha import reflection\nfrom concurrent import futures\nfrom .remote_pb2_grpc import (\n    RemoteServiceServicer,\n    add_RemoteServiceServicer_to_server,\n    RemoteServiceStub,\n)\nfrom vanilla_aiagents.remote import remote_pb2\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass GRPCConnection(Connection):\n    \"\"\"A connection to a gRPC server.\"\"\"\n\n    def __init__(self, url: str):\n        \"\"\"\n        Initialize the GRPCConnection.\n\n        Args:\n            url (str): The URL of the gRPC server.\n        \"\"\"\n        # URL must be in the format host:port\n        self.channel = grpc.insecure_channel(\n            url.replace(\"http://\", \"\"), compression=grpc.Compression.Gzip\n        )\n        self.stub = RemoteServiceStub(self.channel)\n\n    def _create_conversation_request(self, target_id, payload):\n        return remote_pb2.ConversationRequest(\n            agent_id=target_id,\n            messages=[\n                remote_pb2.Message(\n                    role=msg[\"role\"], content=msg[\"content\"], name=msg[\"name\"]\n                )\n                for msg in payload[\"messages\"]\n            ],\n            variables=payload[\"variables\"],\n        )\n\n    def send(self, target_id: str, operation: str, payload: dict[str, any]) -> dict:\n        \"\"\"Send a request to the gRPC server.\"\"\"\n        try:\n            if operation == \"ask\":\n                request = self._create_conversation_request(target_id, payload)\n                response = self.stub.Ask(request)\n                return {\n                    \"conversation\": {\n                        \"messages\": [\n                            {\n                                \"role\": message.role,\n                                \"content\": message.content,\n                                \"name\": message.name,\n                            }\n                            for message in response.conversation.messages\n                        ],\n                        \"variables\": response.conversation.variables,\n                        \"metrics\": {\n                            \"completion_tokens\": response.conversation.metrics.completion_tokens,\n                            \"total_tokens\": response.conversation.metrics.total_tokens,\n                            \"prompt_tokens\": response.conversation.metrics.prompt_tokens,\n                        },\n                    },\n                    \"result\": response.result,\n                }\n            elif operation == \"describe\":\n                request = remote_pb2.DescribeRequest(agent_id=target_id)\n                response = self.stub.Describe(request)\n\n                return {\"id\": response.id, \"description\": response.description}\n            else:\n                raise Exception(\"Operation not supported\")\n        except grpc.RpcError as e:\n            logger.error(f\"gRPC error: {e.code()} - {e.details()}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error: {e}\")\n            raise\n\n    def stream(self, target_id: str, operation: str, payload: dict[str, any]):\n        \"\"\"Send a request to the gRPC server and stream the response.\"\"\"\n        logger.debug(\n            f\"Streaming operation '{operation}' for target_id '{target_id}' with payload: {payload}\"\n        )\n\n        request = self._create_conversation_request(target_id, payload)\n\n        result = None\n\n        for response in self.stub.AskStream(request):\n            mark = response.mark\n            content = json.loads(response.content)\n            logger.debug(\n                f\"Received stream response with mark '{mark}' and content: {content}\"\n            )\n            # Always yield the intermediate response to the caller\n            yield [mark, content]\n            # If the mark is 'result', then the response is the final response\n            if mark == \"result\":\n                result = content\n                break\n\n        logger.info(\n            f\"Streaming operation '{operation}' for target_id '{target_id}' completed with result: {result}\"\n        )\n        return result\n\n\nclass GRPCHost(AskableHost):\n    \"\"\"A host for gRPC remote Askables.\"\"\"\n\n    def __init__(self, askables: list[Askable], host: str, port: int):\n        \"\"\"\n        Initialize the GRPCHost.\n\n        Args:\n            askables (list[Askable]): The list of Askables to host.\n            host (str): The host of the gRPC server.\n            port (int): The port of the gRPC server.\n        \"\"\"\n        self.askables = askables\n        self.host = host.replace(\"http://\", \"\")\n        self.port = port\n\n    def start(self):\n        \"\"\"Start the gRPC server.\"\"\"\n        if (\n            hasattr(self, \"server\")\n            and self.server is not None\n            and self.server._state != 0\n        ):\n            self.server.stop(grace=0)\n\n        self.server = grpc.server(\n            thread_pool=futures.ThreadPoolExecutor(max_workers=10)\n        )\n        add_RemoteServiceServicer_to_server(GRPCServer(self.askables), self.server)\n\n        # Enable reflection\n        SERVICE_NAMES = (\n            remote_pb2.DESCRIPTOR.services_by_name[\"RemoteService\"].full_name,\n            reflection.SERVICE_NAME,\n        )\n        reflection.enable_server_reflection(SERVICE_NAMES, self.server)\n\n        self.server.add_insecure_port(f\"{self.host}:{self.port}\")\n        self.server.start()\n        logger.info(f\"gRPC server running at {self.host}:{self.port}\")\n        # server.wait_for_termination()\n\n    def stop(self):\n        \"\"\"Stop the gRPC server.\"\"\"\n        self.server.stop(grace=0)\n\n\nclass GRPCServer(RemoteServiceServicer):\n    \"\"\"A gRPC server for Askables.\"\"\"\n\n    def __init__(self, askables: list[Askable]):\n        \"\"\"\n        Initialize the GRPCServer.\n\n        Args:\n            askables (list[Askable]): The list of Askables to host.\n        \"\"\"\n        self.askables = askables\n        self.askables_dict = {askable.id: askable for askable in askables}\n\n    def Ask(self, request: remote_pb2.ConversationRequest, context):\n        \"\"\"Handle an Ask request.\"\"\"\n        if request.agent_id not in self.askables_dict:\n            context.set_code(grpc.StatusCode.NOT_FOUND)\n            context.set_details(\"Askable not found\")\n            return remote_pb2.Empty()\n\n        askable = self.askables_dict[request.agent_id]\n        conv = Conversation(\n            messages=[\n                {\"role\": message.role, \"content\": message.content, \"name\": message.name}\n                for message in request.messages\n            ],\n            variables=dict(request.variables),\n        )\n        result = askable.ask(conv)\n\n        return remote_pb2.AskResponse(\n            conversation=remote_pb2.ConversationResponse(\n                messages=[\n                    {\n                        \"role\": message[\"role\"],\n                        \"content\": message[\"content\"],\n                        \"name\": message[\"name\"],\n                    }\n                    for message in conv.messages\n                ],\n                variables=dict(conv.variables),\n                metrics=remote_pb2.ConversationMetrics(\n                    completion_tokens=conv.metrics.completion_tokens,\n                    total_tokens=conv.metrics.total_tokens,\n                    prompt_tokens=conv.metrics.prompt_tokens,\n                ),\n            ),\n            result=result,\n        )\n\n    def AskStream(self, request, context):\n        \"\"\"Handle an AskStream request.\"\"\"\n        agent_id = request.agent_id\n        if agent_id not in self.askables_dict:\n            context.set_code(grpc.StatusCode.NOT_FOUND)\n            context.set_details(\"Agent not found\")\n            logger.error(f\"Agent with id '{agent_id}' not found\")\n            return\n\n        askable = self.askables_dict[agent_id]\n        # Create a conversation object from the request\n        # This is necessary because the request object typing is not compatible with the Conversation object\n        conversation = Conversation(\n            messages=[\n                {\"content\": msg.content, \"role\": msg.role, \"name\": msg.name}\n                for msg in request.messages\n            ],\n            variables=dict(request.variables),\n        )\n\n        logger.debug(\n            f\"Received stream request for agent '{agent_id}' with messages: {conversation.messages}\"\n        )\n\n        # In order to stream the response, we need to run the askable.ask method in a separate thread\n        # updates to the conversation object will be streamed back to the client below.\n        # The result of the askable.ask method will be sent back as the final response via a thread-safe queue\n        result_queue = queue.SimpleQueue()\n\n        def ask_in_thread():\n            res = None\n            try:\n                res = askable.ask(conversation, True)\n            except Exception as e:\n                logger.error(\"Error during askable.ask: %s\", e, exc_info=True)\n                conversation.update([\"error\", str(e)])\n                res = \"error\"\n\n            result_queue.put_nowait(res)\n\n        thread = threading.Thread(target=ask_in_thread)\n        thread.start()\n\n        # Stream the intermediate responses back to the client\n        # Since the conversation stream is an infinite generator, we need to keep track of the stack count to know when to break\n        stack_count = 0\n        for mark, content in conversation.stream():\n            logger.debug(\n                f\"Streaming response with mark '{mark}' and content: {content}\"\n            )\n            # Always yield the response to the client\n            yield remote_pb2.AskStreamingResponse(\n                mark=mark,\n                content=json.dumps(\n                    content\n                ),  # Convert the content to a JSON string, as the content must be a string for semplification\n            )\n\n            # Keep track of the stack count to know when to break\n            if mark == \"start\":\n                stack_count += 1\n            elif mark == \"end\":\n                stack_count -= 1\n\n            # break the loop when the stack count is 0 or an error is received\n            logger.debug(\"Stack count: %s\", stack_count)\n            if stack_count == 0:\n                logger.debug(\"Received response and stack count is 0, breaking stream.\")\n                break\n            if mark == \"error\":\n                logger.debug(\"Received error, breaking stream.\")\n                break\n\n        # The stream is complete, can join the thread and send the final response\n        thread.join()\n\n        response = {\n            \"conversation\": conversation.to_dict(),  # Updated conversation object\n            \"result\": result_queue.get(),  # Result from askable.ask method\n        }\n        logger.debug(f\"Streaming operation completed with result: {response}\")\n        yield remote_pb2.AskStreamingResponse(\n            mark=\"result\", content=json.dumps(response)\n        )\n\n    def Describe(self, request: remote_pb2.DescribeRequest, context):\n        \"\"\"Handle a Describe request.\n\n        Return the description of the agent.\n        \"\"\"\n        if request.agent_id in self.askables_dict:\n            askable = self.askables_dict[request.agent_id]\n            return remote_pb2.DescribeResponse(\n                id=askable.id, description=askable.description\n            )\n        else:\n            context.set_code(grpc.StatusCode.NOT_FOUND)\n            context.set_details(\"Askable not found\")\n            return remote_pb2.Empty()\n"}
