{"repo_info": {"repo_name": "ai-services", "repo_owner": "danny-avila", "repo_url": "https://github.com/danny-avila/ai-services"}}
{"type": "test_file", "path": "tests/__init__.py", "content": "# Empty __init__.py file for the tests package\n"}
{"type": "test_file", "path": "tests/test_ai_utilities.py", "content": "# tests\\test_ai_utilities.py\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom main import app\nfrom services.ai_services import AI_SERVICES\n\nSECRET_KEY = \"your-secret-key\"\nheaders = {\"x-api-key\": SECRET_KEY}\n\nclient = TestClient(app)\n\ndef test_ask():\n    test_data = {\n        \"service\": \"q&a\",\n        \"input\": \"How many people live in canada as of 2023?\",\n        \"envs\": {\n            \"OPENAI_API_KEY\": \"test_key\",\n        }\n    }\n    response = client.post(\"/ai/ask\", headers=headers, json=test_data)\n    assert response.status_code == 200\n    assert \"result\" in response.json()\n    assert \"error\" in response.json()\n    assert \"stdout\" in response.json()\n\ndef test_ask_invalid_service():\n    test_data = {\n        \"service\": \"invalid_service\",\n        \"input\": \"How many people live in canada as of 2023?\",\n        \"envs\": {\n            \"OPENAI_API_KEY\": \"test_key\",\n        }\n    }\n    response = client.post(\"/ai/ask\", headers=headers, json=test_data)\n    assert response.status_code == 400\n    assert \"detail\" in response.json()\n\ndef test_sentiment_analysis():\n    test_data = {\n        \"service\": \"sentiment_analysis\",\n        \"input\": \"I love this product!\",\n        \"envs\": {\n            \"OPENAI_API_KEY\": \"test_key\",\n        }\n    }\n    response = client.post(\"/ai/ask\", headers=headers, json=test_data)\n    assert response.status_code == 200\n    assert \"result\" in response.json()\n    assert \"error\" in response.json()\n    assert \"stdout\" in response.json()\n\ndef test_sentiment_analysis_invalid_input():\n    test_data = {\n        \"text\": \"\",\n        \"envs\": {\n            \"OPENAI_API_KEY\": \"test_key\",\n        }\n    }\n    response = client.post(\"/ai/sentiment_analysis\", headers=headers, json=test_data)\n    assert response.status_code == 422\n    assert \"detail\" in response.json()\n\n@pytest.mark.parametrize(\"service_name\", AI_SERVICES.keys())\ndef test_ai_services(service_name):\n    test_data = {\n        \"service\": service_name,\n        \"input\": \"Test input\",\n        \"envs\": {\n            \"OPENAI_API_KEY\": \"test_key\",\n        }\n    }\n    response = client.post(\"/ai/ask\", headers=headers, json=test_data)\n    assert response.status_code == 200\n    assert \"result\" in response.json()\n    assert \"error\" in response.json()\n    assert \"stdout\" in response.json()\n"}
{"type": "source_file", "path": "clients/langchain/get_openapi_agent.py", "content": "import json\nimport yaml\nfrom langchain.agents import create_openapi_agent\nfrom langchain.agents.agent_toolkits import OpenAPIToolkit\nfrom langchain.llms.openai import OpenAI\nfrom langchain.requests import RequestsWrapper\nfrom langchain.tools.json.tool import JsonSpec\nfrom langchain.agents.agent_toolkits.openapi import planner\nfrom langchain.agents.agent_toolkits.openapi.spec import reduce_openapi_spec\n\n# def convert_json_to_yaml(json_file, yaml_file):\n#     with open(json_file, 'r') as f:\n#         data = json.load(f)\n\n#     with open(yaml_file, 'w') as f:\n#         yaml.dump(data, f, default_flow_style=False)\n\ndef get_openapi_agent(api_key, model_name, plugin_name):\n    # with open(f\"{plugin_name}_openapi.json\") as f:\n    #     data = json.load(f)\n    # json_spec=JsonSpec(dict_=data)\n    # convert_json_to_yaml(f\"{plugin_name}_openapi.json\", f\"{plugin_name}_openapi.yaml\")\n\n    # with open(f\"{plugin_name}_openapi.yaml\") as f:\n    #     raw_api_spec  = yaml.load(f, Loader=yaml.FullLoader)\n    # json_spec=JsonSpec(dict_=data, max_value_length=4000)\n\n    with open(f\"{plugin_name}_openapi.yaml\") as f:\n        raw_openai_api_spec = yaml.load(f, Loader=yaml.Loader)\n    openai_api_spec = reduce_openapi_spec(raw_openai_api_spec)\n\n    headers = {\n        \"Accept\": \"application/json\"\n    }\n\n    # openai_requests_wrapper=RequestsWrapper(headers=headers)\n    requests_wrapper=RequestsWrapper(headers=headers)\n    llm=OpenAI(openai_api_key=api_key, model_name=model_name, temperature=0)\n\n    # openapi_toolkit = OpenAPIToolkit.from_llm(OpenAI(openai_api_key=api_key, model_name=model_name, temperature=0), json_spec, openai_requests_wrapper, verbose=True)\n    # openapi_agent_executor = planner.create_openapi_agent(api_spec,\n    #     llm=OpenAI(openai_api_key=api_key, model_name=model_name, temperature=0),\n    #     toolkit=openapi_toolkit,\n    #     verbose=True\n    # )\n\n\n    # return openapi_agent_executor\n\n    agent = planner.create_openapi_agent(openai_api_spec, requests_wrapper, llm)\n    return agent"}
{"type": "source_file", "path": "clients/langchain/get_nla_agent.py", "content": "from langchain.chat_models import ChatOpenAI\n# from langchain.llms import OpenAI\nfrom langchain.agents import AgentType, initialize_agent\nfrom langchain.agents.agent_toolkits import NLAToolkit\nfrom langchain.requests import Requests\nfrom langchain.tools.plugin import AIPlugin\n\nAI_PLUGINS = {\n    \"ai_agents\": \"https://ai-agents-plugin.vercel.app/.well-known/ai-plugin.json\",\n    \"penrose\": \"https://www.aperiodic.io/.well-known/ai-plugin.json\"\n}\n\ndef get_nla_agent(openai_api_key, model_name, plugin_name, plugin_api_key):\n    llm=ChatOpenAI(openai_api_key=openai_api_key, model_name=model_name, temperature=0)\n    # llm=OpenAI(openai_api_key=openai_api_key, model_name=model_name, temperature=0)\n    plugin=AIPlugin.from_url(AI_PLUGINS[plugin_name])\n    # requests = Requests(headers={\"Authorization\": f\"Bearer {plugin_api_key}\"})\n    # toolkit = NLAToolkit.from_llm_and_ai_plugin(llm, plugin, requests=requests)\n    toolkit = NLAToolkit.from_llm_and_ai_plugin(llm, plugin)\n\n    # tools = toolkit.get_tools()\n    # agent = initialize_agent(tools, llm, agent=AgentType.OPENAI_FUNCTIONS, \n    #                     verbose=True)\n\n    openapi_format_instructions = \"\"\"Use the following format:\n\n    Question: the input question you must answer\n    Thought: you should always think about what to do\n    Action: the action to take, should be one of [{tool_names}]\n    Action Input: what to instruct the AI Action representative.\n    Observation: The Agent's response\n    ... (this Thought/Action/Action Input/Observation can repeat N times)\n    Thought: I now know the final answer. User can't see any of my observations, API responses, links, or tools.\n    Final Answer: the final answer to the original input question with the right amount of detail\n\n    When responding with your Final Answer, remember that the person you are responding to CANNOT see any of your Thought/Action/Action Input/Observations, so if there is any relevant information there you need to include it explicitly in your response.\n    \"\"\"\n\n    tools = toolkit.get_tools()\n    agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n                        verbose=True, agent_kwargs={\"format_instructions\":openapi_format_instructions})\n    return agent, tools"}
{"type": "source_file", "path": "clients/tree_of_thoughts/AsyncOpenAI.py", "content": "# clients\\tree_of_thoughts\\AsyncOpenAI.py\nimport os\nimport re\nimport openai\nimport logging\nimport asyncio\nfrom tree_of_thoughts import AbstractLanguageModel\n\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n\nclass AsyncOpenAILanguageModel(AbstractLanguageModel):\n    def __init__(self, api_key, strategy=\"cot\", evaluation_strategy=\"value\", api_base=\"\", api_model=\"\", enable_ReAct_prompting=True, stream_handler=None):\n        self.session = openai.aiosession.get()\n        self.stream_handler = stream_handler\n        if api_key == \"\" or api_key == None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n        if api_key != \"\":\n            openai.api_key = api_key\n        else:\n            raise Exception(\"Please provide OpenAI API key\")\n\n        if api_base == \"\" or api_base == None:\n            # if not set, use the default base path of \"https://api.openai.com/v1\"\n            api_base = os.environ.get(\"OPENAI_API_BASE\", \"\")\n        if api_base != \"\":\n            # e.g. https://api.openai.com/v1/ or your custom url\n            openai.api_base = api_base\n            print(f'Using custom api_base {api_base}')\n\n        if api_model == \"\" or api_model == None:\n            api_model = os.environ.get(\"OPENAI_API_MODEL\", \"\")\n        if api_model != \"\":\n            self.api_model = api_model\n        else:\n            self.api_model = \"text-davinci-003\"\n        print(f'Using api_model {self.api_model}')\n\n        self.use_chat_api = 'gpt' in self.api_model\n\n        # reference : https://www.promptingguide.ai/techniques/react\n        self.ReAct_prompt = ''\n        if enable_ReAct_prompting:\n            self.ReAct_prompt = \"Write down your observations in format 'Observation:xxxx', then write down your thoughts in format 'Thoughts:xxxx'.\"\n\n        self.strategy = strategy\n        self.evaluation_strategy = evaluation_strategy\n\n    def stream_message(self, message):\n        if self.stream_handler:\n            self.stream_handler(message)\n\n    async def openai_api_call_handler(self, prompt, max_tokens, temperature, k=1, stop=None):\n        while True:\n            try:\n                if self.use_chat_api:\n                    messages = [\n                        {\n                            \"role\": \"user\",\n                            \"content\": prompt\n                        }\n                    ]\n                    response = await openai.ChatCompletion.acreate(\n                        model=self.api_model,\n                        messages=messages,\n                        max_tokens=max_tokens,\n                        temperature=temperature,\n                    )\n                else:\n                    response = await openai.Completion.acreate(\n                        engine=self.api_model,\n                        prompt=prompt,\n                        n=k,\n                        max_tokens=max_tokens,\n                        stop=stop,\n                        temperature=temperature,\n                    )\n                # with open(\"openai.logs\", 'a') as log_file:\n                #     log_file.write(\"\\n\" + \"-----------\" +\n                #                    '\\n' + \"Prompt : \" + prompt+\"\\n\")\n\n                self.stream_message(f\"\\n-----------\\nPrompt: {prompt}\")\n                return response\n            except openai.error.RateLimitError as e:\n                sleep_duration = os.environ.get(\"OPENAI_RATE_TIMEOUT\", 30)\n                print(\n                    f'{str(e)}, sleep for {sleep_duration}s, set it by env OPENAI_RATE_TIMEOUT')\n                await asyncio.sleep(sleep_duration)\n\n    def openai_choice2text_handler(self, choice):\n        if self.use_chat_api:\n            text = choice['message']['content']\n        else:\n            text = choice.text.strip()\n        return text\n\n    async def generate_text(self, prompt, k):\n        if self.use_chat_api:\n            thoughts = []\n            for _ in range(k):\n                response = await self.openai_api_call_handler(prompt, 400, 0.5, k)\n                text = self.openai_choice2text_handler(response.choices[0])\n                thoughts += [text]\n                self.stream_message(f\"\\n-----------\\nThoughts: {thoughts}\")\n                # print(f'thoughts: {thoughts}')\n            return thoughts\n\n        else:\n            response = await self.openai_api_call_handler(prompt, 300, 0.5, k)\n            thoughts = [self.openai_choice2text_handler(\n                choice) for choice in response.choices]\n            return thoughts\n\n    async def generate_thoughts(self, state, k, initial_prompt, rejected_solutions=None):\n        if (type(state) == str):\n            state_text = state\n        else:\n            state_text = '\\n'.join(state)\n        print(\"New state generating thought:\", state, \"\\n\\n\")\n        # prompt = f\"Considering the thoughts you've had until now:\\n\\n{state_text}\\n\\nDevise the next coherent thought that will aid in advancing the reasoning process and achieving a solution to {initial_prompt}. Assess various scenarios, think unconventionally, anticipate potential challenges, and resolve any outstanding queries. Tap into your mind's full potential and make certain no open questions remain.\"\n        prompt = f\"\"\"You are TreeofThoughts, a super-intelligent AI model devoted to helping Humans by any means necessary. Your purpose is to generate a series of solutions to comply with the user's instructions, you must generate solutions on the basis of determining the most reliable solution in the shortest amount of time, while taking rejected solutions into account and learning from them. \n        Considering the reasoning provided:\\n\\n\n        ###'{state_text}'\\n\\n###\n        Devise the best possible solution for the task: {initial_prompt}, Here are evaluated solutions that were rejected: \n        ###{rejected_solutions}###, \n        complete the {initial_prompt} without making the same mistakes you did with the evaluated rejected solutions. Be simple. Be direct. Provide intuitive solutions as soon as you think of them.\"\"\"\n\n        prompt += self.ReAct_prompt\n        # print(prompt)\n        thoughts = await self.generate_text(prompt, k)\n        # print(thoughts)\n        # print(f\"Generated thoughts: {thoughts}\")\n        return thoughts\n\n    async def generate_solution(self, initial_prompt, state, rejected_solutions=None):\n        try:\n\n            if isinstance(state, list):\n                state_text = '\\n'.join(state)\n            else:\n                state_text = state\n\n            prompt = f\"\"\"You are TreeofThoughts, a super-intelligent AI model devoted to helping Humans by any means necessary. Your purpose is to generate a series of solutions to comply with the user's instructions, you must generate solutions on the basis of determining the most reliable solution in the shortest amount of time, while taking rejected solutions into account and learning from them. \n            Considering the reasoning provided:\\n\\n\n            ###'{state_text}'\\n\\n###\n            Devise the best possible solution for the task: {initial_prompt}, Here are evaluated solutions that were rejected: \n            ###{rejected_solutions}###, \n            complete the {initial_prompt} without making the same mistakes you did with the evaluated rejected solutions. Be simple. Be direct. Provide intuitive solutions as soon as you think of them.\"\"\"\n            answer = await self.generate_text(prompt, 1)\n            print(f'Answer: {answer}')\n            # print(thoughts)\n            # print(f\"General Solution : {answer}\")\n            return answer\n        except Exception as e:\n            logger.error(f\"Error in generate_solutions: {e}\")\n            return None\n\n    async def evaluate_states(self, states, initial_prompt):\n        if not states:\n            return {}\n\n        if self.evaluation_strategy == 'value':\n            state_values = {}\n            for state in states:\n                if (type(state) == str):\n                    state_text = state\n                else:\n                    state_text = '\\n'.join(state)\n                print(\"We receive a state of type\", type(\n                    state), \"For state: \", state, \"\\n\\n\")\n                prompt = f\"\"\" To achieve the following goal: '{initial_prompt}', pessimistically value the context of the past solutions and more importantly the latest generated solution you had AS A FLOAT BETWEEN 0 AND 1\\n\n                    Past solutions:\\n\\n\n                    {state_text}\\n       \n                    If the solutions is not directly concretely making fast progress in achieving the goal, give it a lower score.\n                    Evaluate all solutions AS A FLOAT BETWEEN 0 and 1:\\nDO NOT RETURN ANYTHING ELSE\n                \"\"\"\n\n                response = await self.openai_api_call_handler(prompt, 50, 1)\n                try:\n                    value_text = self.openai_choice2text_handler(\n                        response.choices[0])\n                    # print(f'state: {value_text}')\n                    self.stream_message(f\"\\n-----------\\nValue Text: {value_text}\")\n                    # value = float(value_text)\n                    matches = re.findall(r'[-+]?[0-9]*\\.[0-9]+', value_text)\n                    if matches:\n                        value = float(matches[-1])  # Get the last match\n                        print(f\"Evaluated Thought Value: {value}\")\n                        self.stream_message(f\"\\n-----------\\nEvaluated Thought Value: {value}\")\n                    else:\n                        raise ValueError(\"No float found in value text\")\n                    print(f\"Evaluated Thought Value: {value}\")\n                    self.stream_message(f\"\\n-----------\\nEvaluated Thought Value: {value}\")\n                except ValueError:\n                    self.stream_message(f\"\\n-----------\\nValueError, defaulting to 0\")\n                    value = 0  # Assign a default value if the conversion fails\n                state_values[state] = value\n            return state_values\n\n        elif self.evaluation_strategy == 'vote':\n            states_text = '\\n'.join([' '.join(state) for state in states])\n\n            prompt = f\"Given the following states of reasoning, vote for the best state utilizing an scalar value 1-10:\\n{states_text}\\n\\nVote, on the probability of this state of reasoning achieveing {initial_prompt} and become very pessimistic very NOTHING ELSE\"\n\n            response = await self.openai_api_call_handler(prompt, 100, 1)\n\n            print(f'state response: {response}')\n\n            best_state_text = self.openai_choice2text_handler(\n                response.choices[0])\n\n            print(f\"Best state text: {best_state_text}\")\n\n            best_state = tuple(best_state_text.split())\n\n            print(f'best_state: {best_state}')\n\n            return {state: 1 if state == best_state else 0 for state in states}\n\n        else:\n            raise ValueError(\n                \"Invalid evaluation strategy. Choose 'value' or 'vote'.\")\n"}
{"type": "source_file", "path": "clients/__init__.py", "content": "# empty init file\nfrom .langchain.get_openapi_chain import get_openapi_chain\nfrom .langchain.get_openapi_agent import get_openapi_agent\nfrom .langchain.get_nla_agent import get_nla_agent"}
{"type": "source_file", "path": "clients/tree_of_thoughts/AsyncMonteCarlo.py", "content": "# clients\\tree_of_thoughts\\AsyncMonteCarlo.py\nfrom typing import Any, Dict, Union\nimport os\nimport asyncio\nimport json\nimport logging\nimport numpy as np\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass TreeofThoughts:\n    def __init__(self, model):\n        self.model = model\n        self.tree: Dict[str, Dict[str, Union[float, Dict[str, Any]]]] = {\n            \"nodes\": {},\n        }\n        self.best_state = None\n        self.best_value = float(\"-inf\")\n        self.history = []  # added line initalize history\n\n    def save_tree_to_json(self, file_name):\n        self.model.stream_message(json.dumps(self.tree, indent=4))\n        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n        with open(file_name, 'w') as json_file:\n            json.dump(self.tree, json_file, indent=4)\n\n    def logNewState(self, state, evaluation):\n        if not (type(state) == str):\n            state = \" | \".join(state)\n        if state in self.tree['nodes']:\n            self.tree['nodes'][state]['thoughts'].append(evaluation)\n        else:\n            self.tree['nodes'][state] = {'thoughts': [evaluation]}\n\n    def adjust_pruning_threshold_precentile(self, evaluated_thoughts, percentile):\n        values = np.array(list(evaluated_thoughts.values()))\n        if values.size == 0:\n            return 0\n        return max(np.percentile(values, percentile), 0.1)\n\n    def adjust_pruning_threshold_moving_average(self, evaluated_thoughts, window_size):\n        values = list(evaluated_thoughts.values())\n        if len(values) < window_size:\n            return np.mean(values) if values else 0\n        else:\n            return max(np.mean(values[-window_size:]), 0.1)\n\nclass AsyncMonteCarloTreeofThoughts(TreeofThoughts):\n    def __init__(self, model, objective=\"balance\", stream_handler=None):\n        super().__init__(model)\n        self.stream_handler = stream_handler\n        self.objective = objective\n        self.solution_found = False\n        self.tree: Dict[str, Dict[str, Union[float, Dict[str, Any]]]] = {\n            \"nodes\": {},\n            \"metrics\": {\"thoughts\": {}, \"evaluations\": {}},\n        }\n\n    def optimize_params(self, num_thoughts, max_steps, max_states):\n        if self.objective == 'speed':\n            num_thoughts = max(1, num_thoughts - 1)\n            max_steps = max(1, max_steps - 1)\n            max_states = max(1, max_states - 1)\n        elif self.objective == 'reliability':\n            num_thoughts += 1\n            max_steps += 1\n            max_states += 1\n        elif self.objective == 'balanace':\n            if self.solution_found:\n                num_thoughts = max(1, num_thoughts - 1)\n                max_steps = max(1, max_steps - 1)\n                max_states = max(1, max_states - 1)\n            else:\n                num_thoughts += 1\n                max_steps += 1\n                max_states += 1\n\n        return num_thoughts, max_steps, max_states\n\n    async def solve(self,\n                    initial_prompt: str,\n                    num_thoughts: int,\n                    max_steps: int,\n                    max_states: int,\n                    pruning_threshold: float,\n                    #   sleep_time: float,\n                    ):\n        self.file_name = f\"logs/tree_of_thoughts_output_montecarlo.json\"\n        return await self.monte_carlo_search(\n            initial_prompt,\n            num_thoughts,\n            max_steps,\n            max_states,\n            pruning_threshold,\n            # sleep_time,\n        )\n# v3\n\n    async def monte_carlo_search(self,\n                                 initial_prompt: str,\n                                 num_thoughts: int,\n                                 max_steps: int,\n                                 max_states: int,\n                                 pruning_threshold: float,\n                                 ):\n        current_states = [initial_prompt]\n        state_values = {}\n        visit_counts = {initial_prompt: 0}\n        transposition_table = {}\n\n        best_state = None\n        best_value = float('-inf')\n\n        for step in range(1, max_steps + 1):\n            selected_states = []\n\n            for state in current_states:\n                if state in transposition_table:\n                    state_value = transposition_table[state]\n                else:\n                    await asyncio.sleep(1)\n                    thoughts = await self.model.generate_thoughts(state, num_thoughts, initial_prompt)\n                    await asyncio.sleep(1)\n                    evaluated_thoughts = await self.model.evaluate_states(thoughts, initial_prompt)\n\n                    for thought, value in evaluated_thoughts.items():\n                        flattened_state = (state, thought) if isinstance(\n                            state, str) else (*state, thought)\n                        transposition_table[flattened_state] = value\n\n                for thought, value in evaluated_thoughts.items():\n                    flattened_state = (state, thought) if isinstance(\n                        state, str) else (*state, thought)\n                    \n                    self.logNewState(flattened_state, value)\n\n                    if flattened_state not in visit_counts:\n                        visit_counts[flattened_state] = 0\n\n                    if visit_counts[state] > visit_counts[flattened_state] and visit_counts[flattened_state] > 0:\n                        ucb1_value = value + \\\n                            np.sqrt(\n                                2 * np.log(visit_counts[state]) / visit_counts[flattened_state])\n\n                        if ucb1_value >= pruning_threshold:\n                            selected_states.append(flattened_state)\n                            state_values[flattened_state] = value\n\n                            # Update the best state if the current state value is greater than the best value\n                            if value > best_value:\n                                best_state = flattened_state\n                                best_value = value\n\n                visit_counts[state] += 1\n\n            if len(selected_states) > max_states:\n                current_states = selected_states[:max_states]\n            self.save_tree_to_json(self.file_name)\n\n        solution = await self.model.generate_solution(initial_prompt, best_state)\n        return solution if solution else best_state\n"}
{"type": "source_file", "path": "clients/tree_of_thoughts/AsyncGuidance.py", "content": "# clients\\tree_of_thoughts\\AsyncOpenAI.py\nimport os\nimport openai\nfrom tree_of_thoughts import AbstractLanguageModel\nimport guidance\nimport time\nimport os\n\n\nclass GuidanceLanguageModel(AbstractLanguageModel):\n    def __init__(self, model, strategy=\"cot\", evaluation_strategy=\"value\", enable_ReAct_prompting=False):\n        # gpt4 = guidance.llms.OpenAI(\"gpt-4\")\n        # vicuna = guidance.llms.transformers.Vicuna(\"your_path/vicuna_13B\", device_map=\"auto\")\n        self.model = model\n        \n        # reference : https://www.promptingguide.ai/techniques/react\n        self.ReAct_prompt = ''\n        if enable_ReAct_prompting:\n            self.ReAct_prompt = '''{{#assistant~}}\n            {{gen 'Observation' temperature=0.5 max_tokens=50}}\n            {{~/assistant}}'''\n        \n        self.strategy = strategy\n        self.evaluation_strategy = evaluation_strategy\n        \n        self.thoughts_program = guidance('''\n            {{#system~}}\n            You are a logical and rational assistant.\n            {{~/system}}\n\n            {{#user~}}\n            Given the current state of reasoning:\n            {{state_text}}\n            Generate {{k}} coherent thoughts as short as possible to continue the reasoning process.\n            Don't answer the question yet.\n            {{~/user}}\n\n            %s\n            \n            {{#assistant~}}\n            {{gen 'Thoughts' temperature=0.5 max_tokens=50}}\n            {{~/assistant}}\n            ''' % self.ReAct_prompt, llm=self.model)\n        \n        self.value_program = guidance('''\n            {{#system~}}\n            You are a logical and rational assistant.\n            {{~/system}}\n\n            {{#user~}}\n            Given the current state of reasoning:\n            {{state_text}}\n            Evaluate its value as a float between 0 and 1, and NOTHING ELSE\n            Don't answer the question yet.\n            {{~/user}}\n\n            {{#assistant~}}\n            {{gen 'Value' temperature=1 max_tokens=10}}\n            {{~/assistant}}\n            ''', llm=self.model)\n        \n        self.vote_program = guidance('''\n            {{#system~}}\n            You are a logical and rational assistant.\n            {{~/system}}\n\n            {{#user~}}\n            Given the following states of reasoning, vote for the best state:\n            {{states_text}}\n            Give the index of your voted best state(the 1st state has index 0), and NOTHING ELSE\n            Don't answer the question yet.\n            {{~/user}}\n\n            {{#assistant~}}\n            {{gen 'Vote' temperature=1 max_tokens=10}}\n            {{~/assistant}}\n            ''', llm=self.model)\n\n    def stream_message(self, message):\n        if self.stream_handler:\n            self.stream_handler(message)\n\n    async def model_response_handler(self, program, **kargs):\n        print(\"Calling guidance model(Modify Me to handle specific LLM response excpetions!)\")\n        reponse = program(**kargs)\n        return reponse\n\n    async def generate_thoughts(self, state, k):\n        #implement the thought generation logic using self.model\n        state_text = ' '.join(state)\n        \n        thoughts = []\n        for _ in range(k):\n            response = await self.model_response_handler(self.thoughts_program, state_text=state_text, k=1)\n            text = response['Thoughts']\n            thoughts += [text]\n        # print(thoughts)\n        print(f\"Generated thoughts: {thoughts}\")\n        return thoughts\n\n    async def evaluate_states(self, states):\n        #implement state evaluation logic using self.model\n        if self.evaluation_strategy == 'value':\n            state_values = {}\n            for state in states:\n                state_text = ' '.join(state)\n                response = await self.model_response_handler(self.value_program, state_text=state_text)\n                try:\n                    value_text = response['Value']\n                    # print(f\"Value text {value_text}\")\n                    self.stream_message(f\"Value text {value_text}\")\n                    value = float(value_text)\n                    # print(f\"value: {value}\")\n                    self.stream_message(f\"value: {value}\")\n                except ValueError:\n                    # print(f\"Value text {value_text} cannot be converted to float\")\n                    self.stream_message(f\"Value text {value_text} cannot be converted to float\")\n                    value = 0  # Assign a default value if the conversion fails\n                state_values[state] = value\n            return state_values\n\n        elif self.evaluation_strategy == 'vote':\n            states_text = '\\n'.join([' '.join(state) for state in states])\n            response = await self.model_response_handler(self.vote_program, states_text=states_text)\n            best_state_text = response['Vote']\n            # print(f\"Best state text: {best_state_text}\")\n            self.stream_message(f\"Best state text: {best_state_text}\")\n            best_state = int(best_state_text)\n            return {state: 1 if i == best_state else 0 for i in range(len(states))}\n\n        else:\n            raise ValueError(\"Invalid evaluation strategy. Choose 'value' or 'vote'.\")\n        \n\n\nclass GuidanceOpenAILanguageModel(GuidanceLanguageModel):\n    def __init__(self, api_key, strategy=\"cot\", evaluation_strategy=\"value\", api_base=\"\", api_model=\"\", enable_ReAct_prompting=False, stream_handler=None):\n        self.stream_handler = stream_handler\n        if api_key == \"\" or api_key == None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n        if api_key != \"\":\n            openai.api_key = api_key\n        else:\n            raise Exception(\"Please provide OpenAI API key\")\n\n        if api_base == \"\"or api_base == None:\n            api_base = os.environ.get(\"OPENAI_API_BASE\", \"\")  # if not set, use the default base path of \"https://api.openai.com/v1\"\n        if api_base != \"\":\n            # e.g. https://api.openai.com/v1/ or your custom url\n            openai.api_base = api_base\n            print(f'Using custom api_base {api_base}')\n            \n        if api_model == \"\" or api_model == None:\n            api_model = os.environ.get(\"OPENAI_API_MODEL\", \"\")\n        if api_model != \"\":\n            self.api_model = api_model\n        else:\n            self.api_model = \"text-davinci-003\"\n        print(f'Using api_model {self.api_model}')\n\n        super().__init__(guidance.llms.OpenAI(self.api_model), strategy, evaluation_strategy, enable_ReAct_prompting)\n        \n    \n    async def model_response_handler(self, program, **kargs):\n        error_msg = ''\n        while True:\n            try:\n                program.llm.max_retries = 60\n                guidance.llms.OpenAI.cache.clear()\n                response = program(**kargs)\n                return response\n            except openai.error.RateLimitError as e:\n                sleep_duratoin = os.environ.get(\"OPENAI_RATE_TIMEOUT\", 30)\n                print(f'{str(e)}, sleep for {sleep_duratoin}s, set it by env OPENAI_RATE_TIMEOUT')\n                time.sleep(sleep_duratoin)\n            except Exception as e:\n                if str(e) == f'''Too many (more than {guidance.llm.max_retries}) OpenAI API RateLimitError's in a row!''':\n                    sleep_duratoin = os.environ.get(\"OPENAI_RATE_TIMEOUT\", 30)\n                    print(f'{str(e)}, sleep for {sleep_duratoin}s, set it by env OPENAI_RATE_TIMEOUT')\n                    time.sleep(sleep_duratoin)\n                else:\n                    error_msg = str(e)\n                    break\n        raise Exception(error_msg)"}
{"type": "source_file", "path": "clients/tree_of_thoughts/__init__.py", "content": "from .AsyncMonteCarlo import AsyncMonteCarloTreeofThoughts\nfrom .AsyncOpenAI import AsyncOpenAILanguageModel\nfrom .AsyncGuidance import GuidanceOpenAILanguageModel"}
{"type": "source_file", "path": "logger.py", "content": "# logger.py\nimport logging\n\n# Create a custom logger\nlogger = logging.getLogger(__name__)\n\n# Configure the logger\nc_handler = logging.StreamHandler()\nc_format = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\nc_handler.setFormatter(c_format)\nlogger.addHandler(c_handler)\n\n# Set level of logger\nlogger.setLevel(logging.DEBUG)\n"}
{"type": "source_file", "path": "main.py", "content": "# main.py\nimport openai\nfrom logger import logger\nfrom fastapi import FastAPI\nfrom contextlib import asynccontextmanager\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom routes.ai_utilities import ai_utilities_router\nfrom middlewares.authentication import AuthenticationMiddleware\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    yield\n    logger.info(\"Shutting down...\")\n    logger.info(\"Closing OpenAI AIO session...\")\n    await openai.aiosession.get().close()\n\napp = FastAPI(lifespan=lifespan)\n\napp.include_router(ai_utilities_router, prefix=\"/ai\")\n\napp.add_middleware(AuthenticationMiddleware)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8080, log_level=\"info\")"}
{"type": "source_file", "path": "clients/langchain/get_openapi_chain.py", "content": "from langchain.tools import OpenAPISpec, APIOperation\nfrom langchain.chains import OpenAPIEndpointChain\nfrom langchain.requests import Requests\nfrom langchain.llms import OpenAI\n\ndef get_openapi_chain(api_key, model_name):\n    spec = OpenAPISpec.from_url(\n        \"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\")\n    operation = APIOperation.from_openapi_spec(\n        spec, '/public/openai/v0/products', \"get\")\n    llm = OpenAI(openai_api_key=api_key, model_name=model_name)  # Load a Language Model\n\n    chain = OpenAPIEndpointChain.from_api_operation(\n        operation,\n        llm,\n        requests=Requests(),\n        verbose=True,\n        return_intermediate_steps=True,  # Return request and response text\n        # raw_response=True # Return raw response\n    )\n\n    return chain\n"}
{"type": "source_file", "path": "middlewares/authentication.py", "content": "# middlewares\\authentication.py\nfrom fastapi import Request, HTTPException\nfrom starlette.middleware.base import BaseHTTPMiddleware\n\nSECRET_KEY = \"your-secret-key\"\n\nasync def authenticate(request: Request):\n    if \"x-api-key\" in request.headers and request.headers[\"x-api-key\"] == SECRET_KEY:\n        return True\n    else:\n        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n\nclass AuthenticationMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        if not await authenticate(request):\n            raise HTTPException(status_code=401, detail=\"Unauthorized\")\n\n        response = await call_next(request)\n        return response"}
{"type": "source_file", "path": "routes/__init__.py", "content": "from fastapi import APIRouter\n\nfrom .ai_utilities import ai_utilities_router\nfrom .authentication import authentication_router\n\nrouter = APIRouter()\n\nrouter.include_router(ai_utilities_router, prefix=\"/ai\", tags=[\"AI Utilities\"])\nrouter.include_router(authentication_router, prefix=\"/auth\", tags=[\"Authentication\"])"}
{"type": "source_file", "path": "services/tree_of_thoughts.py", "content": "# services\\tree_of_thoughts_service.py\nfrom typing import Dict\nfrom logger import logger\nfrom .utils import handle_exception, logger_stream_handler\nfrom clients.tree_of_thoughts import AsyncOpenAILanguageModel, AsyncMonteCarloTreeofThoughts\n\ndef logger_stream_handler(message):\n    logger.debug(message)\n\nasync def tree_of_thoughts(input_text: str, envs: Dict[str, str]) -> str:\n    try:\n        logger_stream_handler(\"Starting tree_of_thoughts service\")\n        api_model = envs[\"MODEL\"]\n        api_key = envs[\"OPENAI_API_KEY\"]\n        model = AsyncOpenAILanguageModel(api_key=api_key, api_model=api_model, stream_handler=logger_stream_handler)\n        tree_of_thoughts = AsyncMonteCarloTreeofThoughts(model, stream_handler=logger_stream_handler)\n        initial_prompt =  \"design a new transportation system for an all-new city\"\n        num_thoughts = 1\n        max_steps = 3\n        max_states = 5\n        pruning_threshold = 0.5\n\n        solution = await tree_of_thoughts.solve(\n        initial_prompt=initial_prompt,\n        num_thoughts=num_thoughts, \n        max_steps=max_steps,\n        max_states=max_states, \n        pruning_threshold=pruning_threshold,\n        # sleep_time=sleep_time\n        )\n\n        # logger.debug(\"tree_of_thoughts: %s\", solution)\n        return f\"Solution: {solution}\"\n    except Exception as e:\n        handle_exception(e, \"tree_of_thoughts\")\n"}
{"type": "source_file", "path": "services/api_chain.py", "content": "# services\\api_chain.py\nfrom typing import Dict\nfrom clients import get_openapi_chain\nfrom .utils import handle_exception, logger_stream_handler\n\nasync def api_chain(input_text: str, envs: Dict[str, str]) -> str:\n    try:\n        api_chain = get_openapi_chain(api_key = envs[\"OPENAI_API_KEY\"], model_name = \"gpt-3.5-turbo\")\n        response = api_chain(input_text)\n        logger_stream_handler(f\"logger_stream_handler test: {response}\")\n        # logger.debug(\"api_chain: %s\", response)\n        return response\n    except Exception as e:\n        handle_exception(e, \"api_chain\")\n        return \"\" # return empty string if an exception is caught\n"}
{"type": "source_file", "path": "services/ask_question.py", "content": "# services\\ask_question_service.py\nimport openai\nfrom typing import Dict\n# from logger import logger\nfrom aiohttp import ClientSession\nfrom .utils import handle_exception, logger_stream_handler\n\nopenai.aiosession.set(ClientSession())\n\nasync def ask_question(input_text: str, envs: Dict[str, str]) -> str:\n    try:\n        openai.api_key = envs[\"OPENAI_API_KEY\"]\n        messages = [{\"role\": \"user\", \"content\": f\"Answer the following question as best you can: {input_text}\" }]\n        response = await openai.ChatCompletion.acreate(\n            model=\"gpt-3.5-turbo\",\n            messages=messages,\n            max_tokens=100,\n            n=1,\n            stop=None,\n            temperature=0.5,\n        )\n\n        # logger.debug(\"ask_question: %s\", response)\n        logger_stream_handler(f\"logger_stream_handler test: {response}\")\n        return response.choices[0].message.content.strip()\n    except Exception as e:\n        handle_exception(e, \"ask_question\")\n"}
{"type": "source_file", "path": "routes/ai_utilities.py", "content": "# routes\\ai_utilities.py\nimport json\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Dict\nfrom services.ai_services import AI_SERVICES\nfrom middlewares.authentication import authenticate\n\nai_utilities_router = APIRouter()\n\nclass RequestPayload(BaseModel):\n    service: str\n    input: str\n    envs: Dict[str, str]\n\nclass ApiResponse(BaseModel):\n    result: str\n    error: str\n    stdout: str\n\ndef process_result(result):\n    if isinstance(result, dict):\n        if 'error' in result:\n            raise HTTPException(status_code=400, detail=result['error'])\n        stdout_dict = result.copy()\n        stdout_dict.pop('output', None)\n        stdout = json.dumps(stdout_dict)\n        result = result.get(\"output\", \"\")\n    else:\n        stdout = result\n    return result, stdout\n\n@ai_utilities_router.post(\"/ask\", response_model=ApiResponse)\nasync def ask(payload: RequestPayload, token: str = Depends(authenticate)):\n    service = payload.service.lower()\n    input_text = payload.input\n    envs = payload.envs\n\n    if service not in AI_SERVICES:\n        raise HTTPException(status_code=400, detail=\"Invalid service requested\")\n\n    try:\n        result = await AI_SERVICES[service](input_text, envs)\n        result, stdout = process_result(result)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n    return ApiResponse(result=result, error=\"\", stdout=stdout)\n\n# @ai_utilities_router.get(\"/plugins\", response_model=ApiResponse)\n# async def plugins(token: str = Depends(authenticate)):\n#     try:\n#         result = await load_plugins()\n#         result, stdout = process_result(result)\n#     except Exception as e:\n#         raise HTTPException(status_code=500, detail=str(e))\n\n#     return ApiResponse(result=result, error=\"\", stdout=stdout)\n\n"}
{"type": "source_file", "path": "routes/authentication.py", "content": "from fastapi import APIRouter, HTTPException\nfrom pydantic import BaseModel\nfrom middlewares.authentication import authenticate\n\nauthentication_router = APIRouter()\n\nclass AuthRequest(BaseModel):\n    secret_key: str\n\n@authentication_router.post(\"/authenticate\")\nasync def auth_endpoint(auth_request: AuthRequest) -> dict:\n    secret_key = auth_request.secret_key\n    is_authenticated = authenticate(secret_key)\n\n    if is_authenticated:\n        return {\"status\": \"success\", \"message\": \"Authenticated successfully\"}\n    else:\n        raise HTTPException(status_code=401, detail=\"Invalid secret key\")"}
{"type": "source_file", "path": "services/api_agent.py", "content": "# services\\api_agent.py\nfrom typing import Dict\nfrom clients import get_openapi_agent\nfrom .utils import handle_exception, logger_stream_handler\n\nasync def api_agent(input_text: str, envs: Dict[str, str]) -> str:\n    try:\n        api_agent = get_openapi_agent(api_key = envs[\"OPENAI_API_KEY\"], model_name = \"gpt-3.5-turbo\", plugin_name = envs[\"plugin_name\"])\n        response = api_agent.run(input_text)\n        logger_stream_handler(f\"logger_stream_handler test: {response}\")\n        # logger.debug(\"api_agent: %s\", response)\n        return response\n    except Exception as e:\n        handle_exception(e, \"api_agent\")\n        return \"\"\n"}
{"type": "source_file", "path": "services/code_interpreter.py", "content": "# services\\code_interpreter.py\nimport pprint\nfrom typing import Dict\nfrom codeinterpreterapi import CodeInterpreterSession\nfrom .utils import handle_exception, logger_stream_handler\n\nasync def code_interpreter(input_text: str, envs: Dict[str, str]) -> str:\n    try:\n        # Check if the API key exists in the environment variables\n        openai_api_key = envs[\"OPENAI_API_KEY\"]\n\n        session = CodeInterpreterSession(openai_api_key=openai_api_key)\n        await session.astart()\n\n        # generate a response based on user input\n        response = await session.generate_response(input_text)\n\n        # ouput the response (text + image)\n        # print(\"AI: \", response.content)\n        pprint.pprint(response)\n        # for file in response.files:\n        #     file.show_image()\n\n        # terminate the session\n        await session.astop()\n        logger_stream_handler(f\"logger_stream_handler test: {response}\")\n        # logger.debug(\"code_interpreter: %s\", response)\n        return response.content\n    except Exception as e:\n        handle_exception(e, \"code_interpreter\")\n        return \"\"\n\n"}
{"type": "source_file", "path": "services/ai_services.py", "content": "# services\\ai_services.py\nfrom .ask_question import ask_question\nfrom .tree_of_thoughts import tree_of_thoughts\nfrom .api_chain import api_chain\nfrom .api_agent import api_agent\nfrom .nla_agent import nla_agent\nfrom .code_interpreter import code_interpreter\n\nAI_SERVICES = {\n    \"q&a\": ask_question,\n    \"nla_agent\": nla_agent,\n    \"code_interpreter\": code_interpreter,\n    \"api_agent\": api_agent,\n    \"api_chain\": api_chain,\n    \"tree_of_thoughts\": tree_of_thoughts\n}"}
{"type": "source_file", "path": "services/nla_agent.py", "content": "# services\\nla_agent.py\nfrom typing import Dict\nfrom clients import get_nla_agent\nfrom .utils import handle_exception, logger_stream_handler\n\nasync def nla_agent(input_text: str, envs: Dict[str, str]) -> str:\n    try:\n        # Check if the API key exists in the environment variables\n        plugin_api_key = envs[\"PLUGIN_API_KEY\"] if \"PLUGIN_API_KEY\" in envs else None\n\n        nla_agent, _tools = get_nla_agent(\n            openai_api_key=envs[\"OPENAI_API_KEY\"], \n            model_name=\"gpt-3.5-turbo\", \n            plugin_name=envs[\"plugin_name\"], \n            plugin_api_key=plugin_api_key\n        )\n        response = nla_agent.run(input_text)\n        logger_stream_handler(f\"logger_stream_handler test: {response}\")\n        # logger.debug(\"nla_agent: %s\", response)\n        return response\n    except Exception as e:\n        handle_exception(e, \"nla_agent\")\n        return \"\"\n\n"}
{"type": "source_file", "path": "services/utils/__init__.py", "content": "# services\\utils\\__init__.py\nfrom .handle_exception import handle_exception\nfrom .logger_utils import logger_stream_handler"}
{"type": "source_file", "path": "services/utils/handle_exception.py", "content": "# services/utils/exception_handler.py\nimport traceback\nfrom fastapi import HTTPException\nfrom logger import logger\n\ndef handle_exception(e: Exception, service_name: str) -> None:\n    logger.error(\"%s Exception: %s\", service_name, e)\n    logger.error(\"Exception type: %s\", type(e).__name__)\n    logger.error(\"Traceback: %s\", traceback.format_exc())\n    error_message = f\"An error of type {type(e).__name__} occurred. Arguments:\\n{e.args}\"\n    raise HTTPException(status_code=500, detail=error_message)"}
{"type": "source_file", "path": "services/utils/logger_utils.py", "content": "# utils\\logger_utils.py\nfrom logger import logger\n\ndef logger_stream_handler(message: str) -> None:\n    logger.debug(message)"}
