{"repo_info": {"repo_name": "TypeAgent", "repo_owner": "PirateforFreedom", "repo_url": "https://github.com/PirateforFreedom/TypeAgent"}}
{"type": "source_file", "path": "luann/__init__.py", "content": ""}
{"type": "source_file", "path": "luann/agent.py", "content": "import datetime\nimport inspect\nimport json\nimport traceback\nimport uuid\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple, Union, cast\n\nfrom tqdm import tqdm\n\nfrom agent_store.storage import StorageConnector\nfrom constants import (\n    CLI_WARNING_PREFIX,\n    CORE_MEMORY_HUMAN_CHAR_LIMIT,\n    CORE_MEMORY_PERSONA_CHAR_LIMIT,\n    FIRST_MESSAGE_ATTEMPTS,\n    JSON_ENSURE_ASCII,\n    JSON_LOADS_STRICT,\n    LLM_MAX_TOKENS,\n    MESSAGE_SUMMARY_TRUNC_KEEP_N_LAST,\n    MESSAGE_SUMMARY_TRUNC_TOKEN_FRAC,\n    MESSAGE_SUMMARY_WARNING_FRAC,\n    TYPEAGENT_TYPE,\n)\nfrom data_types import (\n    AgentState,\n    EmbeddingConfig,\n    LLMConfig,\n    Message,\n    Passage,\n    Preset,\n)\nfrom interface import AgentInterface\nfrom llm_api.llm_api_tools import create, is_context_overflow_error\nfrom memory import ArchivalMemory\nfrom memory import CoreMemory as InContextMemory\nfrom memory import RecallMemory,summarize_messages\nfrom metadata import MetadataStore\nfrom models import chat_completion_response\nfrom persistence_manager import LocalStateManager\nfrom models.pydantic_models import OptionState, ToolModel\nfrom system import (\n    get_initial_boot_messages,\n    get_login_event,\n    package_function_response,\n    package_summarize_message,\n)\nfrom utils import (\n    count_tokens,\n    create_random_username,\n    create_uuid_from_string,\n    get_local_time,\n    get_schema_diff,\n    get_tool_call_id,\n    get_utc_time,\n    is_utc_datetime,\n    parse_json,\n    printd,\n    united_diff,\n    validate_function_response,\n    verify_first_message_correctness,\n)\n\nfrom errors import LLMError\nfrom functions.functions import USER_FUNCTIONS_DIR, load_all_function_sets\n\n\ndef link_functions(function_schemas: list):\n    \"\"\"Link function definitions to list of function schemas\"\"\"\n\n    # need to dynamically link the functions\n    # the saved agent.functions will just have the schemas, but we need to\n    # go through the functions library and pull the respective python functions\n\n    # Available functions is a mapping from:\n    # function_name -> {\n    #   json_schema: schema\n    #   python_function: function\n    # }\n    # agent.functions is a list of schemas (OpenAI kwarg functions style, see: https://platform.openai.com/docs/api-reference/chat/create)\n    # [{'name': ..., 'description': ...}, {...}]\n    available_functions = load_all_function_sets()\n    linked_function_set = {}\n    for f_schema in function_schemas:\n        # Attempt to find the function in the existing function library\n        f_name = f_schema.get(\"name\")\n        if f_name is None:\n            raise ValueError(f\"While loading agent.state.functions encountered a bad function schema object with no name:\\n{f_schema}\")\n        linked_function = available_functions.get(f_name)\n        if linked_function is None:\n            # raise ValueError(\n            #     f\"Function '{f_name}' was specified in agent.state.functions, but is not in function library:\\n{available_functions.keys()}\"\n            # )\n            print(\n                f\"Function '{f_name}' was specified in agent.state.functions, but is not in function library:\\n{available_functions.keys()}\"\n            )\n            continue\n        # Once we find a matching function, make sure the schema is identical\n        if json.dumps(f_schema, ensure_ascii=JSON_ENSURE_ASCII) != json.dumps(\n            linked_function[\"json_schema\"], ensure_ascii=JSON_ENSURE_ASCII\n        ):\n            # error_message = (\n            #     f\"Found matching function '{f_name}' from agent.state.functions inside function library, but schemas are different.\"\n            #     + f\"\\n>>>agent.state.functions\\n{json.dumps(f_schema, indent=2, ensure_ascii=JSON_ENSURE_ASCII)}\"\n            #     + f\"\\n>>>function library\\n{json.dumps(linked_function['json_schema'], indent=2, ensure_ascii=JSON_ENSURE_ASCII)}\"\n            # )\n            schema_diff = get_schema_diff(f_schema, linked_function[\"json_schema\"])\n            error_message = (\n                f\"Found matching function '{f_name}' from agent.state.functions inside function library, but schemas are different.\\n\"\n                + \"\".join(schema_diff)\n            )\n\n            # NOTE to handle old configs, instead of erroring here let's just warn\n            # raise ValueError(error_message)\n            printd(error_message)\n        linked_function_set[f_name] = linked_function\n    return linked_function_set\n\n\ndef initialize_memory(ai_notes: Union[str, None], human_notes: Union[str, None]):\n    if ai_notes is None:\n        raise ValueError(ai_notes)\n    if human_notes is None:\n        raise ValueError(human_notes)\n    memory = InContextMemory(human_char_limit=CORE_MEMORY_HUMAN_CHAR_LIMIT, persona_char_limit=CORE_MEMORY_PERSONA_CHAR_LIMIT)\n    memory.edit_persona(ai_notes)\n    memory.edit_human(human_notes)\n    return memory\n\n\ndef construct_system_with_memory(\n    system: str,\n    memory: InContextMemory,\n    memory_edit_timestamp: str,\n    archival_memory: Optional[ArchivalMemory] = None,\n    recall_memory: Optional[RecallMemory] = None,\n    include_char_count: bool = True,\n):\n    \n    #TODO add knowledge base desrciption and system instruction\n    full_system_message = \"\\n\".join(\n        [\n            system,\n            \n            \"\\n\",\n            f\"### Memory [last modified: {memory_edit_timestamp.strip()}]\",\n            f\"{len(recall_memory) if recall_memory else 0} previous messages between you and the user are stored in recall memory (use functions to access them)\",\n            f\"{len(archival_memory) if archival_memory else 0} total memories you created are stored in archival memory (use functions to access them)\",\n            \"\\nCore memory shown below (limited in size, additional information stored in archival / recall memory):\",\n            f'<persona characters=\"{len(memory.persona)}/{memory.persona_char_limit}\">' if include_char_count else \"<persona>\",\n            memory.persona,\n            \"</persona>\",\n            f'<human characters=\"{len(memory.human)}/{memory.human_char_limit}\">' if include_char_count else \"<human>\",\n            memory.human,\n            \"</human>\",\n        ]\n    )\n    return full_system_message\n\n\ndef initialize_message_sequence(\n    model: str,\n    system: str,\n    memory: InContextMemory,\n    archival_memory: Optional[ArchivalMemory] = None,\n    recall_memory: Optional[RecallMemory] = None,\n    memory_edit_timestamp: Optional[str] = None,\n    include_initial_boot_message: bool = True,\n) -> List[dict]:\n    if memory_edit_timestamp is None:\n        memory_edit_timestamp = get_local_time()\n\n    full_system_message = construct_system_with_memory(\n        system, memory, memory_edit_timestamp, archival_memory=archival_memory, recall_memory=recall_memory\n    )\n    first_user_message = get_login_event()  # event letting typeagent know the user just logged in\n\n    if include_initial_boot_message:\n        if model is not None and \"gpt-3.5\" in model:\n            initial_boot_messages = get_initial_boot_messages(\"startup_with_send_message_gpt35\")\n        else:\n            initial_boot_messages = get_initial_boot_messages(\"startup_with_send_message\")\n        messages = (\n            [\n                {\"role\": \"system\", \"content\": full_system_message},\n            ]\n            + initial_boot_messages\n            + [\n                {\"role\": \"user\", \"content\": first_user_message},\n            ]\n        )\n\n    else:\n        messages = [\n            {\"role\": \"system\", \"content\": full_system_message},\n            {\"role\": \"user\", \"content\": first_user_message},\n        ]\n\n    return messages\n\n\nclass Agent(object):\n    def __init__(\n        self,\n        interface: AgentInterface,\n        type_agent:Optional[str] = None,\n\n        persona_memory:Optional[str] = None,\n        human_memory:Optional[str] = None,\n        # agents can be created from providing agent_state\n        agent_state: Optional[AgentState] = None,\n        # or from providing a preset (requires preset + extra fields)\n        preset: Optional[Preset] = None,\n        created_by: Optional[uuid.UUID] = None,\n        name: Optional[str] = None,\n        llm_config: Optional[LLMConfig] = None,\n        embedding_config: Optional[EmbeddingConfig] = None,\n        # # extras\n        # messages_total: Optional[int] = None,  # TODO remove?\n        # first_message_verify_mono: bool = True,  # TODO move to config?\n    ):\n        # An agent can be created from a Preset object\n        if preset is not None:\n            assert agent_state is None, \"Can create an agent from a Preset or AgentState (but both were provided)\"\n            assert created_by is not None, \"Must provide created_by field when creating an Agent from a Preset\"\n            assert llm_config is not None, \"Must provide llm_config field when creating an Agent from a Preset\"\n            assert embedding_config is not None, \"Must provide embedding_config field when creating an Agent from a Preset\"\n            \n            # if agent_state is also provided, override any preset values\n            init_agent_state = AgentState(\n                type_agent=type_agent if type_agent else TYPEAGENT_TYPE[\"Memgpt\"],\n                name=name if name else create_random_username(),\n                user_id=created_by,\n                persona_memory=persona_memory,\n                human_memory=human_memory,\n                llm_config=llm_config,\n                embedding_config=embedding_config,\n                user_status=\"on\",\n                preset_id=preset.id,  # TODO link via preset.id instead of name?\n                state={\n                    \"persona\": preset.persona,\n                    \"human\": preset.human,\n                    \"system\": preset.system,\n                    \"functions\": preset.functions_schema,\n                    \"messages\": None,\n                },\n            )\n\n        # An agent can also be created directly from AgentState\n        elif agent_state is not None:\n            assert preset is None, \"Can create an agent from a Preset or AgentState (but both were provided)\"\n            assert agent_state.state is not None and agent_state.state != {}, \"AgentState.state cannot be empty\"\n\n            # Assume the agent_state passed in is formatted correctly\n            init_agent_state = agent_state\n\n        else:\n            raise ValueError(\"Both Preset and AgentState were null (must provide one or the other)\")\n\n        # Hold a copy of the state that was used to init the agent\n        self.agent_state = init_agent_state\n       \n        # gpt-4, gpt-3.5-turbo, ...\n        self.model = self.agent_state.llm_config.model\n\n        # Store the system instructions (used to rebuild memory)\n        if \"system\" not in self.agent_state.state:\n            raise ValueError(\"'system' not found in provided AgentState\")\n        self.system = self.agent_state.state[\"system\"]\n\n        if \"functions\" not in self.agent_state.state:\n            raise ValueError(f\"'functions' not found in provided AgentState\")\n        # Store the functions schemas (this is passed as an argument to ChatCompletion)\n        self.functions = self.agent_state.state[\"functions\"]  # these are the schema\n        self.persona=self.agent_state.state[\"persona\"]\n        self.human=self.agent_state.state[\"human\"]\n        # Link the actual python functions corresponding to the schemas\n        self.functions_python = {k: v[\"python_function\"] for k, v in link_functions(function_schemas=self.functions).items()}\n        assert all([callable(f) for k, f in self.functions_python.items()]), self.functions_python\n        \n        # Initialize the memory object\n        # if \"persona\" not in self.agent_state.state:\n        #     raise ValueError(f\"'persona' not found in provided AgentState\")\n        # if \"human\" not in self.agent_state.state:\n        #     raise ValueError(f\"'human' not found in provided AgentState\")\n\n        if self.agent_state.persona_memory==None:\n            self.agent_state.persona_memory=\"\"\n        if self.agent_state.persona_memory==None:\n            self.agent_state.human_memory=\"\"\n\n\n        # print(\"preset person:\")\n        # print(self.persona)\n        # print(self.agent_state.persona_memory)\n        self.memory = initialize_memory(ai_notes=self.persona+\",\"+self.agent_state.persona_memory, human_notes=self.human+\",\"+self.agent_state.human_memory)\n\n        # Interface must implement:\n        # - internal_monologue\n        # - assistant_message\n        # - function_message\n        # ...\n        # Different interfaces can handle events differently\n        # e.g., print in CLI vs send a discord message with a discord bot\n        self.interface = interface\n\n        # Create the persistence manager object based on the AgentState info\n        # TODO\n        self.persistence_manager = LocalStateManager(agent_state=self.agent_state)\n\n        # State needed for heartbeat pausing\n        self.pause_heartbeats_start = None\n        self.pause_heartbeats_minutes = 0\n\n        # self.first_message_verify_mono = first_message_verify_mono\n\n        # Controls if the convo memory pressure warning is triggered\n        # When an alert is sent in the message queue, set this to True (to avoid repeat alerts)\n        # When the summarizer is run, set this back to False (to reset)\n        self.agent_alerted_about_memory_pressure = False\n\n        self._messages: List[Message] = []\n\n        # Once the memory object is initialized, use it to \"bake\" the system message\n        if \"messages\" in self.agent_state.state and self.agent_state.state[\"messages\"] is not None:\n            # print(f\"Agent.__init__ :: loading, state={agent_state.state['messages']}\")\n            if not isinstance(self.agent_state.state[\"messages\"], list):\n                raise ValueError(f\"'messages' in AgentState was bad type: {type(self.agent_state.state['messages'])}\")\n            assert all([isinstance(msg, str) for msg in self.agent_state.state[\"messages\"]])\n\n            # Convert to IDs, and pull from the database\n            raw_messages = [\n            \n                self.persistence_manager.recall_memory.get_one(id=uuid.UUID(msg_id)) for msg_id in self.agent_state.state[\"messages\"]\n            ]\n\n            # print(\"raw_messages\")\n            # print(raw_messages)\n            assert all([isinstance(msg, Message) for msg in raw_messages]), (raw_messages, self.agent_state.state[\"messages\"])\n            self._messages.extend([cast(Message, msg) for msg in raw_messages if msg is not None])\n\n            for m in self._messages:\n                # assert is_utc_datetime(m.created_at), f\"created_at on message for agent {self.agent_state.name} isn't UTC:\\n{vars(m)}\"\n                # TODO eventually do casting via an edit_message function\n                if not is_utc_datetime(m.created_at):\n                    printd(f\"Warning - created_at on message for agent {self.agent_state.name} isn't UTC (text='{m.text}')\")\n                    m.created_at = m.created_at.replace(tzinfo=datetime.timezone.utc)\n\n        else:\n            # print(f\"Agent.__init__ :: creating, state={agent_state.state['messages']}\")\n            init_messages = initialize_message_sequence(\n                model=self.model,\n                system=self.system,\n                memory=self.memory,\n                archival_memory=self.persistence_manager.archival_memory,\n                recall_memory=self.persistence_manager.recall_memory,\n            )\n            init_messages_objs = []\n            for msg in init_messages:\n                init_messages_objs.append(\n                    Message.dict_to_message(\n                        agent_id=self.agent_state.id, user_id=self.agent_state.user_id, model=self.model, openai_message_dict=msg\n                    )\n                )\n            assert all([isinstance(msg, Message) for msg in init_messages_objs]), (init_messages_objs, init_messages)\n            self.messages_total = 0\n            self._append_to_messages(added_messages=[cast(Message, msg) for msg in init_messages_objs if msg is not None])\n\n            for m in self._messages:\n                assert is_utc_datetime(m.created_at), f\"created_at on message for agent {self.agent_state.name} isn't UTC:\\n{vars(m)}\"\n                # TODO eventually do casting via an edit_message function\n                if not is_utc_datetime(m.created_at):\n                    printd(f\"Warning - created_at on message for agent {self.agent_state.name} isn't UTC (text='{m.text}')\")\n                    m.created_at = m.created_at.replace(tzinfo=datetime.timezone.utc)\n\n        # Keep track of the total number of messages throughout all time\n        self.messages_total =  (len(self._messages) - 1)  # (-system)\n        # self.messages_total_init = self.messages_total\n        # self.messages_total_init = len(self._messages) - 1\n        # printd(f\"Agent initialized, self.messages_total={self.messages_total}\")\n\n        # Create the agent in the DB\n        # self.save()\n        self.update_state()\n\n    @property\n    def messages(self) -> List[dict]:\n        \"\"\"Getter method that converts the internal Message list into OpenAI-style dicts\"\"\"\n        return [msg.to_openai_dict() for msg in self._messages]\n\n    @messages.setter\n    def messages(self, value):\n        raise Exception(\"Modifying message list directly not allowed\")\n\n    def _trim_messages(self, num):\n        \"\"\"Trim messages from the front, not including the system message\"\"\"\n        self.persistence_manager.trim_messages(num)\n\n        new_messages = [self._messages[0]] + self._messages[num:]\n        self._messages = new_messages\n\n    def _prepend_to_messages(self, added_messages: List[Message]):\n        \"\"\"Wrapper around self.messages.prepend to allow additional calls to a state/persistence manager\"\"\"\n        assert all([isinstance(msg, Message) for msg in added_messages])\n\n        self.persistence_manager.prepend_to_messages(added_messages)\n\n        new_messages = [self._messages[0]] + added_messages + self._messages[1:]  # prepend (no system)\n        self._messages = new_messages\n        self.messages_total += len(added_messages)  # still should increment the message counter (summaries are additions too)\n\n    def _append_to_messages(self, added_messages: List[Message]):\n        \"\"\"Wrapper around self.messages.append to allow additional calls to a state/persistence manager\"\"\"\n        assert all([isinstance(msg, Message) for msg in added_messages])\n\n        self.persistence_manager.append_to_messages(added_messages)\n\n        # strip extra metadata if it exists\n        # for msg in added_messages:\n        # msg.pop(\"api_response\", None)\n        # msg.pop(\"api_args\", None)\n        new_messages = self._messages + added_messages  # append\n\n        self._messages = new_messages\n        self.messages_total += len(added_messages)\n\n    def append_to_messages(self, added_messages: List[dict]):\n        \"\"\"An external-facing message append, where dict-like messages are first converted to Message objects\"\"\"\n        added_messages_objs = [\n            Message.dict_to_message(\n                agent_id=self.agent_state.id,\n                user_id=self.agent_state.user_id,\n                model=self.model,\n                openai_message_dict=msg,\n            )\n            for msg in added_messages\n        ]\n        self._append_to_messages(added_messages_objs)\n\n    def _swap_system_message(self, new_system_message: Message):\n        assert isinstance(new_system_message, Message)\n        assert new_system_message.role == \"system\", new_system_message\n        assert self._messages[0].role == \"system\", self._messages\n\n        self.persistence_manager.swap_system_message(new_system_message)\n\n        new_messages = [new_system_message] + self._messages[1:]  # swap index 0 (system)\n        self._messages = new_messages\n\n    def _get_ai_reply(\n        self,\n        message_sequence: List[Message],\n        function_call: str = \"auto\",\n        # first_message: bool = False,  # hint\n        stream: bool = False,  # TODO move to config?\n        inner_thoughts_in_kwargs: OptionState = OptionState.DEFAULT,\n    ) -> chat_completion_response.ChatCompletionResponse:\n        \"\"\"Get response from LLM API\"\"\"\n        try:\n            response = create(\n                # agent_state=self.agent_state,\n                llm_config=self.agent_state.llm_config,\n                user_id=self.agent_state.user_id,\n                messages=message_sequence,\n                functions=self.functions,\n                functions_python=self.functions_python,\n                function_call=function_call,\n                # hint\n                # first_message=first_message,\n                # streaming\n                stream=stream,\n                stream_inferface=self.interface,\n                 # putting inner thoughts in func args or not\n                inner_thoughts_in_kwargs=inner_thoughts_in_kwargs,\n            )\n            # special case for 'length'\n            if response.choices[0].finish_reason == \"length\":\n                raise Exception(\"Finish reason was length (maximum context length)\")\n\n            # catches for soft errors\n            if response.choices[0].finish_reason not in [\"stop\", \"function_call\", \"tool_calls\"]:\n                raise Exception(f\"API call finish with bad finish reason: {response}\")\n\n            # unpack with response.choices[0].message.content\n            return response\n        except Exception as e:\n            raise e\n\n    def _handle_ai_response(\n        self, response_message: chat_completion_response.Message, override_tool_call_id: bool = True\n    ) -> Tuple[List[Message], bool, bool]:\n        \"\"\"Handles parsing and function execution\"\"\"\n\n        messages = []  # append these to the history when done\n\n        # Step 2: check if LLM wanted to call a function\n        if response_message.function_call or (response_message.tool_calls is not None and len(response_message.tool_calls) > 0):\n            if response_message.function_call:\n                raise DeprecationWarning(response_message)\n            if response_message.tool_calls is not None and len(response_message.tool_calls) > 1:\n                # raise NotImplementedError(f\">1 tool call not supported\")\n                # TODO eventually support sequential tool calling\n                printd(f\">1 tool call not supported, using index=0 only\\n{response_message.tool_calls}\")\n                response_message.tool_calls = [response_message.tool_calls[0]]\n            assert response_message.tool_calls is not None and len(response_message.tool_calls) > 0\n\n            # generate UUID for tool call\n            if override_tool_call_id or response_message.function_call:\n                tool_call_id = get_tool_call_id()  # needs to be a string for JSON\n                response_message.tool_calls[0].id = tool_call_id\n            else:\n                tool_call_id = response_message.tool_calls[0].id\n                assert tool_call_id is not None  # should be defined\n\n            # only necessary to add the tool_cal_id to a function call (antipattern)\n            # response_message_dict = response_message.model_dump()\n            # response_message_dict[\"tool_call_id\"] = tool_call_id\n\n            # role: assistant (requesting tool call, set tool call ID)\n            messages.append(\n                # NOTE: we're recreating the message here\n                # TODO should probably just overwrite the fields?\n                Message.dict_to_message(\n                    agent_id=self.agent_state.id,\n                    user_id=self.agent_state.user_id,\n                    model=self.model,\n                    openai_message_dict=response_message.model_dump(),\n                )\n            )  # extend conversation with assistant's reply\n            printd(f\"Function call message: {messages[-1]}\")\n\n            # The content if then internal monologue, not chat\n            self.interface.internal_monologue(response_message.content, msg_obj=messages[-1])\n\n            # Step 3: call the function\n            # Note: the JSON response may not always be valid; be sure to handle errors\n\n            # Failure case 1: function name is wrong\n            function_call = (\n                response_message.function_call if response_message.function_call is not None else response_message.tool_calls[0].function\n            )\n            function_name = function_call.name\n            printd(f\"Request to call function {function_name} with tool_call_id: {tool_call_id}\")\n            try:\n                function_to_call = self.functions_python[function_name]\n            except KeyError as e:\n                error_msg = f\"No function named {function_name}\"\n                function_response = package_function_response(False, error_msg)\n                messages.append(\n                    Message.dict_to_message(\n                        agent_id=self.agent_state.id,\n                        user_id=self.agent_state.user_id,\n                        model=self.model,\n                        openai_message_dict={\n                            \"role\": \"tool\",\n                            \"name\": function_name,\n                            \"content\": function_response,\n                            \"tool_call_id\": tool_call_id,\n                        },\n                    )\n                )  # extend conversation with function response\n                self.interface.function_message(f\"Error: {error_msg}\", msg_obj=messages[-1])\n                return messages, False, True  # force a heartbeat to allow agent to handle error\n\n            # Failure case 2: function name is OK, but function args are bad JSON\n            try:\n                raw_function_args = function_call.arguments\n                function_args = parse_json(raw_function_args)\n            except Exception as e:\n                error_msg = f\"Error parsing JSON for function '{function_name}' arguments: {function_call.arguments}\"\n                function_response = package_function_response(False, error_msg)\n                messages.append(\n                    Message.dict_to_message(\n                        agent_id=self.agent_state.id,\n                        user_id=self.agent_state.user_id,\n                        model=self.model,\n                        openai_message_dict={\n                            \"role\": \"tool\",\n                            \"name\": function_name,\n                            \"content\": function_response,\n                            \"tool_call_id\": tool_call_id,\n                        },\n                    )\n                )  # extend conversation with function response\n                self.interface.function_message(f\"Error: {error_msg}\", msg_obj=messages[-1])\n                return messages, False, True  # force a heartbeat to allow agent to handle error\n\n            # (Still parsing function args)\n            # Handle requests for immediate heartbeat\n            heartbeat_request = function_args.pop(\"request_heartbeat\", None)\n            if not (isinstance(heartbeat_request, bool) or heartbeat_request is None):\n                printd(\n                    f\"{CLI_WARNING_PREFIX}'request_heartbeat' arg parsed was not a bool or None, type={type(heartbeat_request)}, value={heartbeat_request}\"\n                )\n                heartbeat_request = False\n\n            # Failure case 3: function failed during execution\n            # NOTE: the msg_obj associated with the \"Running \" message is the prior assistant message, not the function/tool role message\n            #       this is because the function/tool role message is only created once the function/tool has executed/returned\n            self.interface.function_message(f\"Running {function_name}({function_args})\", msg_obj=messages[-1])\n            try:\n                spec = inspect.getfullargspec(function_to_call).annotations\n\n                for name, arg in function_args.items():\n                    if isinstance(function_args[name], dict):\n                        function_args[name] = spec[name](**function_args[name])\n\n                function_args[\"self\"] = self  # need to attach self to arg since it's dynamically linked\n\n                function_response = function_to_call(**function_args)\n                if function_name in [\"conversation_search\", \"conversation_search_date\", \"archival_memory_search\"]:\n                    # with certain functions we rely on the paging mechanism to handle overflow\n                    truncate = False\n                else:\n                    # but by default, we add a truncation safeguard to prevent bad functions from\n                    # overflow the agent context window\n                    truncate = True\n                function_response_string = validate_function_response(function_response, truncate=truncate)\n                function_args.pop(\"self\", None)\n                function_response = package_function_response(True, function_response_string)\n                function_failed = False\n            except Exception as e:\n                function_args.pop(\"self\", None)\n                # error_msg = f\"Error calling function {function_name} with args {function_args}: {str(e)}\"\n                # Less detailed - don't provide full args, idea is that it should be in recent context so no need (just adds noise)\n                error_msg = f\"Error calling function {function_name}: {str(e)}\"\n                error_msg_user = f\"{error_msg}\\n{traceback.format_exc()}\"\n                printd(error_msg_user)\n                function_response = package_function_response(False, error_msg)\n                messages.append(\n                    Message.dict_to_message(\n                        agent_id=self.agent_state.id,\n                        user_id=self.agent_state.user_id,\n                        model=self.model,\n                        openai_message_dict={\n                            \"role\": \"tool\",\n                            \"name\": function_name,\n                            \"content\": function_response,\n                            \"tool_call_id\": tool_call_id,\n                        },\n                    )\n                )  # extend conversation with function response\n                self.interface.function_message(f\"Ran {function_name}({function_args})\", msg_obj=messages[-1])\n                self.interface.function_message(f\"Error: {error_msg}\", msg_obj=messages[-1])\n                return messages, False, True  # force a heartbeat to allow agent to handle error\n\n            # If no failures happened along the way: ...\n            # Step 4: send the info on the function call and function response to GPT\n            messages.append(\n                Message.dict_to_message(\n                    agent_id=self.agent_state.id,\n                    user_id=self.agent_state.user_id,\n                    model=self.model,\n                    openai_message_dict={\n                        \"role\": \"tool\",\n                        \"name\": function_name,\n                        \"content\": function_response,\n                        \"tool_call_id\": tool_call_id,\n                    },\n                )\n            )  # extend conversation with function response\n            self.interface.function_message(f\"Ran {function_name}({function_args})\", msg_obj=messages[-1])\n            self.interface.function_message(f\"Success: {function_response_string}\", msg_obj=messages[-1])\n\n        else:\n            # Standard non-function reply\n            messages.append(\n                Message.dict_to_message(\n                    agent_id=self.agent_state.id,\n                    user_id=self.agent_state.user_id,\n                    model=self.model,\n                    openai_message_dict=response_message.model_dump(),\n                )\n            )  # extend conversation with assistant's reply\n            self.interface.internal_monologue(response_message.content, msg_obj=messages[-1])\n            heartbeat_request = False\n            function_failed = False\n\n        return messages, heartbeat_request, function_failed\n\n    def step(\n        self,\n        user_message: Union[Message, str],  # NOTE: should be json.dump(dict)\n        # first_message: bool = False,\n        # first_message_retry_limit: int = FIRST_MESSAGE_ATTEMPTS,\n        # skip_verify: bool = False,\n        return_dicts: bool = True,  # if True, return dicts, if False, return Message objects\n        recreate_message_timestamp: bool = True,  # if True, when input is a Message type, recreated the 'created_at' field\n        stream: bool = False,  # TODO move to config?\n        inner_thoughts_in_kwargs: OptionState = OptionState.DEFAULT,\n        # timestamp: Optional[datetime] = None,\n    ) -> Tuple[List[Union[dict, Message]], bool, bool, bool]:\n        \"\"\"Top-level event message handler for the typeagent agent\"\"\"\n\n        def strip_name_field_from_user_message(user_message_text: str) -> Tuple[str, Optional[str]]:\n            \"\"\"If 'name' exists in the JSON string, remove it and return the cleaned text + name value\"\"\"\n            try:\n                user_message_json = dict(json.loads(user_message_text, strict=JSON_LOADS_STRICT))\n                # Special handling for AutoGen messages with 'name' field\n                # Treat 'name' as a special field\n                # If it exists in the input message, elevate it to the 'message' level\n                name = user_message_json.pop(\"name\", None)\n                clean_message = json.dumps(user_message_json, ensure_ascii=JSON_ENSURE_ASCII)\n\n            except Exception as e:\n                print(f\"{CLI_WARNING_PREFIX}handling of 'name' field failed with: {e}\")\n\n            return clean_message, name\n\n        def validate_json(user_message_text: str, raise_on_error: bool) -> str:\n            try:\n                user_message_json = dict(json.loads(user_message_text, strict=JSON_LOADS_STRICT))\n                user_message_json_val = json.dumps(user_message_json, ensure_ascii=JSON_ENSURE_ASCII)\n                return user_message_json_val\n            except Exception as e:\n                print(f\"{CLI_WARNING_PREFIX}couldn't parse user input message as JSON: {e}\")\n                if raise_on_error:\n                    raise e\n\n        try:\n            # Step 0: add user message\n            if user_message is not None:\n                if isinstance(user_message, Message):\n                    # Validate JSON via save/load\n                    user_message_text = validate_json(user_message.text, False)\n                    cleaned_user_message_text, name = strip_name_field_from_user_message(user_message_text)\n\n                    if name is not None:\n                        # Update Message object\n                        user_message.text = cleaned_user_message_text\n                        user_message.name = name\n\n                    # Recreate timestamp\n                    if recreate_message_timestamp:\n                        user_message.created_at = get_utc_time()\n\n                elif isinstance(user_message, str):\n                    # Validate JSON via save/load\n                    user_message = validate_json(user_message, False)\n                    cleaned_user_message_text, name = strip_name_field_from_user_message(user_message)\n\n                    # If user_message['name'] is not None, it will be handled properly by dict_to_message\n                    # So no need to run strip_name_field_from_user_message\n\n                    # Create the associated Message object (in the database)\n                    user_message = Message.dict_to_message(\n                        agent_id=self.agent_state.id,\n                        user_id=self.agent_state.user_id,\n                        model=self.model,\n                        openai_message_dict={\"role\": \"user\", \"content\": cleaned_user_message_text, \"name\": name},\n                    )\n\n                else:\n                    raise ValueError(f\"Bad type for user_message: {type(user_message)}\")\n\n                # self.interface.user_message(user_message.text, msg_obj=user_message)\n\n                input_message_sequence = self._messages + [user_message]\n            # Alternatively, the requestor can send an empty user message\n            else:\n                input_message_sequence = self._messages\n\n            if len(input_message_sequence) > 1 and input_message_sequence[-1].role != \"user\":\n                printd(f\"{CLI_WARNING_PREFIX}Attempting to run ChatCompletion without user as the last message in the queue\")\n\n            # Step 1: send the conversation and available functions to GPT\n            response = self._get_ai_reply(\n                    message_sequence=input_message_sequence,\n                    stream=stream,\n                    inner_thoughts_in_kwargs=inner_thoughts_in_kwargs,\n                )\n            # if not skip_verify and (first_message or self.messages_total == self.messages_total_init):\n            #     printd(f\"This is the first message. Running extra verifier on AI response.\")\n            #     counter = 0\n            #     while True:\n            #         response = self._get_ai_reply(\n            #             message_sequence=input_message_sequence,\n            #             first_message=True,  # passed through to the prompt formatter\n            #             stream=stream,\n            #         )\n            #         if verify_first_message_correctness(response, require_monologue=self.first_message_verify_mono):\n            #             break\n\n            #         counter += 1\n            #         if counter > first_message_retry_limit:\n            #             raise Exception(f\"Hit first message retry limit ({first_message_retry_limit})\")\n\n            # else:\n            #     response = self._get_ai_reply(\n            #         message_sequence=input_message_sequence,\n            #         stream=stream,\n            #     )\n\n            # Step 2: check if LLM wanted to call a function\n            # (if yes) Step 3: call the function\n            # (if yes) Step 4: send the info on the function call and function response to LLM\n            response_message = response.choices[0].message\n            # response_message.model_copy()  # TODO why are we copying here?\n            all_response_messages, heartbeat_request, function_failed = self._handle_ai_response(response_message)\n\n            # Add the extra metadata to the assistant response\n            # (e.g. enough metadata to enable recreating the API call)\n            # assert \"api_response\" not in all_response_messages[0]\n            # all_response_messages[0][\"api_response\"] = response_message_copy\n            # assert \"api_args\" not in all_response_messages[0]\n            # all_response_messages[0][\"api_args\"] = {\n            #     \"model\": self.model,\n            #     \"messages\": input_message_sequence,\n            #     \"functions\": self.functions,\n            # }\n\n            # Step 4: extend the message history\n            if user_message is not None:\n                if isinstance(user_message, Message):\n                    all_new_messages = [user_message] + all_response_messages\n                else:\n                    raise ValueError(type(user_message))\n            else:\n                all_new_messages = all_response_messages\n\n            # Check the memory pressure and potentially issue a memory pressure warning\n            current_total_tokens = response.usage.total_tokens\n            active_memory_warning = False\n            # We can't do summarize logic properly if context_window is undefined\n            if self.agent_state.llm_config.context_window is None:\n                # Fallback if for some reason context_window is missing, just set to the default\n                print(f\"{CLI_WARNING_PREFIX}could not find context_window in config, setting to default {LLM_MAX_TOKENS['DEFAULT']}\")\n                print(f\"{self.agent_state}\")\n                self.agent_state.llm_config.context_window = (\n                    LLM_MAX_TOKENS[self.model] if (self.model is not None and self.model in LLM_MAX_TOKENS) else LLM_MAX_TOKENS[\"DEFAULT\"]\n                )\n            if current_total_tokens > MESSAGE_SUMMARY_WARNING_FRAC * int(self.agent_state.llm_config.context_window):\n                printd(\n                    f\"{CLI_WARNING_PREFIX}last response total_tokens ({current_total_tokens}) > {MESSAGE_SUMMARY_WARNING_FRAC * int(self.agent_state.llm_config.context_window)}\"\n                )\n                # Only deliver the alert if we haven't already (this period)\n                if not self.agent_alerted_about_memory_pressure:\n                    active_memory_warning = True\n                    self.agent_alerted_about_memory_pressure = True  # it's up to the outer loop to handle this\n            else:\n                printd(\n                    f\"last response total_tokens ({current_total_tokens}) < {MESSAGE_SUMMARY_WARNING_FRAC * int(self.agent_state.llm_config.context_window)}\"\n                )\n\n            self._append_to_messages(all_new_messages)\n            messages_to_return = [msg.to_openai_dict() for msg in all_new_messages] if return_dicts else all_new_messages\n            return messages_to_return, heartbeat_request, function_failed, active_memory_warning, response.usage\n\n        except Exception as e:\n            printd(f\"step() failed\\nuser_message = {user_message}\\nerror = {e}\")\n\n            # If we got a context alert, try trimming the messages length, then try again\n            if is_context_overflow_error(e):\n                # A separate API call to run a summarizer\n                self.summarize_messages_inplace()\n\n                # Try step again\n                return self.step(user_message, return_dicts=return_dicts,inner_thoughts_in_kwargs=inner_thoughts_in_kwargs,)\n            else:\n                printd(f\"step() failed with an unrecognized exception: '{str(e)}'\")\n                raise e\n\n    def summarize_messages_inplace(self, cutoff=None, preserve_last_N_messages=True, disallow_tool_as_first=True):\n        assert self.messages[0][\"role\"] == \"system\", f\"self.messages[0] should be system (instead got {self.messages[0]})\"\n\n        # Start at index 1 (past the system message),\n        # and collect messages for summarization until we reach the desired truncation token fraction (eg 50%)\n        # Do not allow truncation of the last N messages, since these are needed for in-context examples of function calling\n        token_counts = [count_tokens(str(msg)) for msg in self.messages]\n        message_buffer_token_count = sum(token_counts[1:])  # no system message\n        desired_token_count_to_summarize = int(message_buffer_token_count * MESSAGE_SUMMARY_TRUNC_TOKEN_FRAC)\n        candidate_messages_to_summarize = self.messages[1:]\n        token_counts = token_counts[1:]\n\n        if preserve_last_N_messages:\n            candidate_messages_to_summarize = candidate_messages_to_summarize[:-MESSAGE_SUMMARY_TRUNC_KEEP_N_LAST]\n            token_counts = token_counts[:-MESSAGE_SUMMARY_TRUNC_KEEP_N_LAST]\n\n        # if disallow_tool_as_first:\n        #     # We have to make sure that a \"tool\" call is not sitting at the front (after system message),\n        #     # otherwise we'll get an error from OpenAI (if using the OpenAI API)\n        #     while len(candidate_messages_to_summarize) > 0:\n        #         if candidate_messages_to_summarize[0][\"role\"] in [\"tool\", \"function\"]:\n        #             candidate_messages_to_summarize.pop(0)\n        #         else:\n        #             break\n\n        printd(f\"MESSAGE_SUMMARY_TRUNC_TOKEN_FRAC={MESSAGE_SUMMARY_TRUNC_TOKEN_FRAC}\")\n        printd(f\"MESSAGE_SUMMARY_TRUNC_KEEP_N_LAST={MESSAGE_SUMMARY_TRUNC_KEEP_N_LAST}\")\n        printd(f\"token_counts={token_counts}\")\n        printd(f\"message_buffer_token_count={message_buffer_token_count}\")\n        printd(f\"desired_token_count_to_summarize={desired_token_count_to_summarize}\")\n        printd(f\"len(candidate_messages_to_summarize)={len(candidate_messages_to_summarize)}\")\n\n        # If at this point there's nothing to summarize, throw an error\n        if len(candidate_messages_to_summarize) == 0:\n            raise LLMError(\n                f\"Summarize error: tried to run summarize, but couldn't find enough messages to compress [len={len(self.messages)}, preserve_N={MESSAGE_SUMMARY_TRUNC_KEEP_N_LAST}]\"\n            )\n\n        # Walk down the message buffer (front-to-back) until we hit the target token count\n        tokens_so_far = 0\n        cutoff = 0\n        for i, msg in enumerate(candidate_messages_to_summarize):\n            cutoff = i\n            tokens_so_far += token_counts[i]\n            if tokens_so_far > desired_token_count_to_summarize:\n                break\n        # Account for system message\n        cutoff += 1\n\n        # Try to make an assistant message come after the cutoff\n        try:\n            printd(f\"Selected cutoff {cutoff} was a 'user', shifting one...\")\n            if self.messages[cutoff][\"role\"] == \"user\":\n                new_cutoff = cutoff + 1\n                if self.messages[new_cutoff][\"role\"] == \"user\":\n                    printd(f\"Shifted cutoff {new_cutoff} is still a 'user', ignoring...\")\n                cutoff = new_cutoff\n        except IndexError:\n            pass\n\n        # Make sure the cutoff isn't on a 'tool' or 'function'\n        if disallow_tool_as_first:\n            while self.messages[cutoff][\"role\"] in [\"tool\", \"function\"] and cutoff < len(self.messages):\n                printd(f\"Selected cutoff {cutoff} was a 'tool', shifting one...\")\n                cutoff += 1\n\n        message_sequence_to_summarize = self._messages[1:cutoff]  # do NOT get rid of the system message\n        if len(message_sequence_to_summarize) <= 1:\n            # This prevents a potential infinite loop of summarizing the same message over and over\n            raise LLMError(\n                f\"Summarize error: tried to run summarize, but couldn't find enough messages to compress [len={len(message_sequence_to_summarize)} <= 1]\"\n            )\n        else:\n            printd(f\"Attempting to summarize {len(message_sequence_to_summarize)} messages [1:{cutoff}] of {len(self._messages)}\")\n\n        # We can't do summarize logic properly if context_window is undefined\n        if self.agent_state.llm_config.context_window is None:\n            # Fallback if for some reason context_window is missing, just set to the default\n            print(f\"{CLI_WARNING_PREFIX}could not find context_window in config, setting to default {LLM_MAX_TOKENS['DEFAULT']}\")\n            print(f\"{self.agent_state}\")\n            self.agent_state.llm_config.context_window = (\n                LLM_MAX_TOKENS[self.model] if (self.model is not None and self.model in LLM_MAX_TOKENS) else LLM_MAX_TOKENS[\"DEFAULT\"]\n            )\n        summary = summarize_messages(agent_state=self.agent_state, message_sequence_to_summarize=message_sequence_to_summarize)\n        printd(f\"Got summary: {summary}\")\n\n        # Metadata that's useful for the agent to see\n        all_time_message_count = self.messages_total\n        remaining_message_count = len(self.messages[cutoff:])\n        hidden_message_count = all_time_message_count - remaining_message_count\n        summary_message_count = len(message_sequence_to_summarize)\n        summary_message = package_summarize_message(summary, summary_message_count, hidden_message_count, all_time_message_count)\n        printd(f\"Packaged into message: {summary_message}\")\n\n        prior_len = len(self.messages)\n        self._trim_messages(cutoff)\n        packed_summary_message = {\"role\": \"user\", \"content\": summary_message}\n        self._prepend_to_messages(\n            [\n                Message.dict_to_message(\n                    agent_id=self.agent_state.id,\n                    user_id=self.agent_state.user_id,\n                    model=self.model,\n                    openai_message_dict=packed_summary_message,\n                )\n            ]\n        )\n\n        # reset alert\n        self.agent_alerted_about_memory_pressure = False\n\n        printd(f\"Ran summarizer, messages length {prior_len} -> {len(self.messages)}\")\n\n    def heartbeat_is_paused(self):\n        \"\"\"Check if there's a requested pause on timed heartbeats\"\"\"\n\n        # Check if the pause has been initiated\n        if self.pause_heartbeats_start is None:\n            return False\n\n        # Check if it's been more than pause_heartbeats_minutes since pause_heartbeats_start\n        elapsed_time = get_utc_time() - self.pause_heartbeats_start\n        return elapsed_time.total_seconds() < self.pause_heartbeats_minutes * 60\n\n    def rebuild_memory(self):\n        \"\"\"Rebuilds the system message with the latest memory object\"\"\"\n        curr_system_message = self.messages[0]  # this is the system + memory bank, not just the system prompt\n        new_system_message = initialize_message_sequence(\n            self.model,\n            self.system,\n            self.memory,\n            archival_memory=self.persistence_manager.archival_memory,\n            recall_memory=self.persistence_manager.recall_memory,\n        )[0]\n\n        diff = united_diff(curr_system_message[\"content\"], new_system_message[\"content\"])\n        printd(f\"Rebuilding system with new memory...\\nDiff:\\n{diff}\")\n\n        # Swap the system message out\n        self._swap_system_message(\n            Message.dict_to_message(\n                agent_id=self.agent_state.id, user_id=self.agent_state.user_id, model=self.model, openai_message_dict=new_system_message\n            )\n        )\n\n    # def to_agent_state(self) -> AgentState:\n    #    # The state may have change since the last time we wrote it\n    #    updated_state = {\n    #        \"persona\": self.memory.persona,\n    #        \"human\": self.memory.human,\n    #        \"system\": self.system,\n    #        \"functions\": self.functions,\n    #        \"messages\": [str(msg.id) for msg in self._messages],\n    #    }\n\n    #    agent_state = AgentState(\n    #        name=self.agent_state.name,\n    #        user_id=self.agent_state.user_id,\n    #        persona=self.agent_state.persona,\n    #        human=self.agent_state.human,\n    #        llm_config=self.agent_state.llm_config,\n    #        embedding_config=self.agent_state.embedding_config,\n    #        preset=self.agent_state.preset,\n    #        id=self.agent_state.id,\n    #        created_at=self.agent_state.created_at,\n    #        state=updated_state,\n    #    )\n\n    #    return agent_state\n\n    def add_function(self, function_name: str) -> str:\n        if function_name in self.functions_python.keys():\n            msg = f\"Function {function_name} already loaded\"\n            printd(msg)\n            return msg\n\n        available_functions = load_all_function_sets()\n        if function_name not in available_functions.keys():\n            raise ValueError(f\"Function {function_name} not found in function library\")\n\n        self.functions.append(available_functions[function_name][\"json_schema\"])\n        self.functions_python[function_name] = available_functions[function_name][\"python_function\"]\n\n        msg = f\"Added function {function_name}\"\n        # self.save()\n        self.update_state()\n        printd(msg)\n        return msg\n\n    def remove_function(self, function_name: str) -> str:\n        if function_name not in self.functions_python.keys():\n            msg = f\"Function {function_name} not loaded, ignoring\"\n            printd(msg)\n            return msg\n\n        # only allow removal of user defined functions\n        user_func_path = Path(USER_FUNCTIONS_DIR)\n        func_path = Path(inspect.getfile(self.functions_python[function_name]))\n        is_subpath = func_path.resolve().parts[: len(user_func_path.resolve().parts)] == user_func_path.resolve().parts\n\n        if not is_subpath:\n            raise ValueError(f\"Function {function_name} is not user defined and cannot be removed\")\n\n        self.functions = [f_schema for f_schema in self.functions if f_schema[\"name\"] != function_name]\n        self.functions_python.pop(function_name)\n\n        msg = f\"Removed function {function_name}\"\n        # self.save()\n        self.update_state()\n        printd(msg)\n        return msg\n\n    # def save(self):\n    #    \"\"\"Save agent state locally\"\"\"\n\n    #    new_agent_state = self.to_agent_state()\n\n    #    # without this, even after Agent.__init__, agent.config.state[\"messages\"] will be None\n    #    self.agent_state = new_agent_state\n\n    #    # Check if we need to create the agent\n    #    if not self.ms.get_agent(agent_id=new_agent_state.id, user_id=new_agent_state.user_id, agent_name=new_agent_state.name):\n    #        # print(f\"Agent.save {new_agent_state.id} :: agent does not exist, creating...\")\n    #        self.ms.create_agent(agent=new_agent_state)\n    #    # Otherwise, we should update the agent\n    #    else:\n    #        # print(f\"Agent.save {new_agent_state.id} :: agent already exists, updating...\")\n    #        print(f\"Agent.save {new_agent_state.id} :: preupdate:\\n\\tmessages={new_agent_state.state['messages']}\")\n    #        self.ms.update_agent(agent=new_agent_state)\n\n    def update_state(self) -> AgentState:\n        updated_state = {\n            \"persona\": self.persona,\n            \"human\":  self.human,\n            \"system\": self.system,\n            \"functions\": self.functions,\n            \"messages\": [str(msg.id) for msg in self._messages],\n        }\n \n       \n\n        self.agent_state = AgentState(\n            type_agent=self.agent_state.type_agent,\n            name=self.agent_state.name,\n            user_id=self.agent_state.user_id,\n            persona_memory=self.memory.persona,\n            human_memory=self.memory.human,\n            llm_config=self.agent_state.llm_config,\n            embedding_config=self.agent_state.embedding_config,\n            preset_id=self.agent_state.preset_id,\n            id=self.agent_state.id,\n            created_at=self.agent_state.created_at,\n            state=updated_state,\n            user_status=\"on\"\n        )\n        return self.agent_state\n\n    def migrate_embedding(self, embedding_config: EmbeddingConfig):\n        \"\"\"Migrate the agent to a new embedding\"\"\"\n        # TODO: archival memory\n\n        # TODO: recall memory\n        raise NotImplementedError()\n\n    def attach_source(self, source_name,  ms: MetadataStore):\n        \"\"\"Attach data with name `source_name` to the agent from source_connector.\"\"\"\n        # TODO: eventually, adding a data source should just give access to the retriever the source table, rather than modifying archival memory\n\n        # filters = {\"user_id\": self.agent_state.user_id, \"data_source\": source_name}\n        # size = source_connector.size(filters)\n        # # typer.secho(f\"Ingesting {size} passages into {agent.name}\", fg=typer.colors.GREEN)\n        # page_size = 100\n        # generator = source_connector.get_all_paginated(filters=filters, page_size=page_size)  # yields List[Passage]\n        # all_passages = []\n        # for i in tqdm(range(0, size, page_size)):\n        #     passages = next(generator)\n\n        #     # need to associated passage with agent (for filtering)\n        #     for passage in passages:\n        #         assert isinstance(passage, Passage), f\"Generate yielded bad non-Passage type: {type(passage)}\"\n        #         passage.agent_id = self.agent_state.id\n\n        #         # regenerate passage ID (avoid duplicates)\n        #         passage.id = create_uuid_from_string(f\"{source_name}_{str(passage.agent_id)}_{passage.text}\")\n\n        #     # insert into agent archival memory\n        #     self.persistence_manager.archival_memory.storage.insert_many(passages)\n        #     all_passages += passages\n\n        # assert size == len(all_passages), f\"Expected {size} passages, but only got {len(all_passages)}\"\n\n        # # save destination storage\n        # self.persistence_manager.archival_memory.storage.save()\n\n        # attach to agent\n        source = ms.get_source(source_name=source_name, user_id=self.agent_state.user_id)\n        assert source is not None, f\"source does not exist for source_name={source_name}, user_id={self.agent_state.user_id}\"\n        source_id = source.id\n        ms.attach_source(agent_id=self.agent_state.id, source_id=source_id, user_id=self.agent_state.user_id)\n\n        # total_agent_passages = self.persistence_manager.archival_memory.storage.size()\n\n        # printd(\n        #     f\"Attached data source {source_name} to agent {self.agent_state.name}, consisting of {len(all_passages)}. Agent now has {total_agent_passages} embeddings in archival memory.\",\n        # )\n\n\ndef save_agent(agent: Agent, ms: MetadataStore):\n    \"\"\"Save agent to metadata store\"\"\"\n\n    agent.update_state()\n    agent_state = agent.agent_state\n\n    if ms.get_agent(agent_name=agent_state.name, user_id=agent_state.user_id):\n        ms.update_agent(agent_state)\n    else:\n        ms.create_agent(agent_state)\n"}
{"type": "source_file", "path": "luann/agent_store/storage.py", "content": "\"\"\" These classes define storage connectors.\n\nWe originally tried to use Llama Index VectorIndex, but their limited API was extremely problematic.\n\"\"\"\nfrom sqlalchemy import (\n    BIGINT,\n    BINARY,\n    CHAR,\n    JSON,\n    Column,\n    DateTime,\n    String,\n    TypeDecorator,\n    and_,\n    asc,\n    create_engine,\n    desc,\n    func,\n    or_,\n    select,\n    text,\n)\nimport uuid\nfrom sqlalchemy.dialects.postgresql import UUID\nfrom sqlalchemy.orm import declarative_base, mapped_column, sessionmaker\nfrom sqlalchemy.orm.session import close_all_sessions\nfrom sqlalchemy.sql import func\nfrom sqlalchemy_json import MutableJson\nfrom constants import MAX_EMBEDDING_DIM\nfrom abc import abstractmethod\nfrom typing import Dict, Iterator, List, Optional, Tuple, Type, Union\nfrom data_types import Message, Passage, Record, RecordType, ToolCall\nfrom config import typeagentConfig\nfrom data_types import Document, Message, Passage, Record, RecordType\nfrom utils import printd\nimport base64\nimport os\nimport uuid\nimport numpy as np\n# ENUM representing table types in typeagent\n# each table corresponds to a different table schema  (specified in data_types.py)\nclass StorageType:\n    ARCHIVAL_MEMORY = \"archival_memory\"  # recall memory table: typeagent_agent_{agent_id}\n    RECALL_MEMORY = \"recall_memory\"  # archival memory table: typeagent_agent_recall_{agent_id}\n    KNOWLEDGE_BASE=\"knowledge_base\"\n    KNOWLEDGE_BASE_PASSAGES = \"knowledge_base_passages\"  # TODO\n    KNOWLEDGE_BASE_DOCUMENTS = \"knowledge_base_documents\"  # TODO\n\n\n# table names used by typeagent\n\n# agent tables\nRECALL_TABLE_NAME = \"recall_memory_agent\"  # agent memory\nARCHIVAL_TABLE_NAME = \"archival_memory_agent\"  # agent memory\n\n# external data source tables\nPASSAGE_TABLE_NAME = \"passages\"  # chunked/embedded passages (from source)\nDOCUMENT_TABLE_NAME = \"documents\"  # original documents (from source)\nKNOWLEDGE_BASE = \"knowledge_base\"  # original documents (from source)\nclass CommonUUID(TypeDecorator):\n    impl = CHAR\n    cache_ok = True\n\n    def load_dialect_impl(self, dialect):\n        if dialect.name == \"postgresql\":\n            return dialect.type_descriptor(UUID(as_uuid=True))\n        else:\n            return dialect.type_descriptor(CHAR())\n\n    def process_bind_param(self, value, dialect):\n        if dialect.name == \"postgresql\" or value is None:\n            return value\n        else:\n            return str(value)  # Convert UUID to string for SQLite\n\n    def process_result_value(self, value, dialect):\n        if dialect.name == \"postgresql\" or value is None:\n            return value\n        else:\n            return uuid.UUID(value)\n\n\nclass CommonVector(TypeDecorator):\n    \"\"\"Common type for representing vectors in SQLite\"\"\"\n\n    impl = BINARY\n    cache_ok = True\n\n    def load_dialect_impl(self, dialect):\n        return dialect.type_descriptor(BINARY())\n\n    def process_bind_param(self, value, dialect):\n        if value is None:\n            return value\n        # Ensure value is a numpy array\n        if isinstance(value, list):\n            value = np.array(value, dtype=np.float32)\n        # Serialize numpy array to bytes, then encode to base64 for universal compatibility\n        return base64.b64encode(value.tobytes())\n\n    def process_result_value(self, value, dialect):\n        if not value:\n            return value\n        # Check database type and deserialize accordingly\n        if dialect.name == \"sqlite\":\n            # Decode from base64 and convert back to numpy array\n            value = base64.b64decode(value)\n        # For PostgreSQL, value is already in bytes\n        return np.frombuffer(value, dtype=np.float32)\n\n\n# Custom serialization / de-serialization for JSON columns\n\n\nclass ToolCallColumn(TypeDecorator):\n    \"\"\"Custom type for storing List[ToolCall] as JSON\"\"\"\n\n    impl = JSON\n    cache_ok = True\n\n    def load_dialect_impl(self, dialect):\n        return dialect.type_descriptor(JSON())\n\n    def process_bind_param(self, value, dialect):\n        if value:\n            return [vars(v) for v in value]\n        return value\n\n    def process_result_value(self, value, dialect):\n        if value:\n            return [ToolCall(**v) for v in value]\n        return value\n\n\nBase = declarative_base()\n\n\ndef get_db_model(\n    config: typeagentConfig,\n    table_name: str,\n    storage_type: StorageType,\n    user_id: uuid.UUID,\n    agent_id: Optional[uuid.UUID] = None,\n    dialect=\"postgresql\",\n):\n    # Define a helper function to create or get the model class\n    def create_or_get_model(class_name, base_model, table_name):\n        if class_name in globals():\n            return globals()[class_name]\n        Model = type(class_name, (base_model,), {\"__tablename__\": table_name, \"__table_args__\": {\"extend_existing\": True}})\n        globals()[class_name] = Model\n        return Model\n\n    if storage_type == StorageType.ARCHIVAL_MEMORY:\n        pass\n        # # create schema for archival memory\n        # class PassageModel(Base):\n        #     \"\"\"Defines data model for storing Passages (consisting of text, embedding)\"\"\"\n\n        #     __abstract__ = True  # this line is necessary\n\n        #     # Assuming passage_id is the primary key\n        #     # id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n        #     id = Column(CommonUUID, primary_key=True, default=uuid.uuid4)\n        #     # id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))\n        #     user_id = Column(CommonUUID, nullable=False)\n        #     text = Column(String)\n        #     doc_id = Column(CommonUUID)\n        #     agent_id = Column(CommonUUID)\n        #     data_source = Column(String)  # agent_name if agent, data_source name if from data source\n\n        #     # vector storage\n        #     if dialect == \"sqlite\":\n        #         embedding = Column(CommonVector)\n        #     # else:\n        #     #     from pgvector.sqlalchemy import Vector\n\n        #         # embedding = mapped_column(Vector(MAX_EMBEDDING_DIM))\n        #     embedding_dim = Column(BIGINT)\n        #     embedding_model = Column(String)\n\n        #     metadata_ = Column(MutableJson)\n\n        #     # Add a datetime column, with default value as the current time\n        #     created_at = Column(DateTime(timezone=True))\n\n        #     def __repr__(self):\n        #         return f\"<Passage(passage_id='{self.id}', text='{self.text}', embedding='{self.embedding})>\"\n\n        #     def to_record(self):\n        #         return Passage(\n        #             text=self.text,\n        #             embedding=self.embedding,\n        #             embedding_dim=self.embedding_dim,\n        #             embedding_model=self.embedding_model,\n        #             doc_id=self.doc_id,\n        #             user_id=self.user_id,\n        #             id=self.id,\n        #             data_source=self.data_source,\n        #             agent_id=self.agent_id,\n        #             metadata_=self.metadata_,\n        #             created_at=self.created_at,\n        #         )\n\n        # \"\"\"Create database model for table_name\"\"\"\n        # class_name = f\"{table_name.capitalize()}Model\" + dialect\n        # return create_or_get_model(class_name, PassageModel, table_name)\n\n    elif storage_type == StorageType.RECALL_MEMORY:\n\n        class MessageModel(Base):\n            \"\"\"Defines data model for storing Message objects\"\"\"\n\n            __abstract__ = True  # this line is necessary\n\n            # Assuming message_id is the primary key\n            # id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n            id = Column(CommonUUID, primary_key=True, default=uuid.uuid4)\n            # id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))\n            user_id = Column(CommonUUID, nullable=False)\n            agent_id = Column(CommonUUID, nullable=False)\n\n            # openai info\n            role = Column(String, nullable=False)\n            text = Column(String)  # optional: can be null if function call\n            model = Column(String)  # optional: can be null if LLM backend doesn't require specifying\n            name = Column(String)  # optional: multi-agent only\n\n            # tool call request info\n            # if role == \"assistant\", this MAY be specified\n            # if role != \"assistant\", this must be null\n            # TODO align with OpenAI spec of multiple tool calls\n            tool_calls = Column(ToolCallColumn)\n\n            # tool call response info\n            # if role == \"tool\", then this must be specified\n            # if role != \"tool\", this must be null\n            tool_call_id = Column(String)\n\n            # vector storage\n            if dialect == \"sqlite\":\n                embedding = Column(CommonVector)\n            # else:\n                # from pgvector.sqlalchemy import Vector\n\n                # embedding = mapped_column(Vector(MAX_EMBEDDING_DIM))\n            embedding_dim = Column(BIGINT)\n            embedding_model = Column(String)\n\n            # Add a datetime column, with default value as the current time\n            created_at = Column(DateTime(timezone=True))\n\n            def __repr__(self):\n                return f\"<Message(message_id='{self.id}', text='{self.text}', embedding='{self.embedding})>\"\n\n            def to_record(self):\n                return Message(\n                    user_id=self.user_id,\n                    agent_id=self.agent_id,\n                    role=self.role,\n                    name=self.name,\n                    text=self.text,\n                    model=self.model,\n                    tool_calls=self.tool_calls,\n                    tool_call_id=self.tool_call_id,\n                    embedding=self.embedding,\n                    embedding_dim=self.embedding_dim,\n                    embedding_model=self.embedding_model,\n                    created_at=self.created_at,\n                    id=self.id,\n                )\n\n        \"\"\"Create database model for table_name\"\"\"\n        class_name = f\"{table_name.capitalize()}Model\" + dialect\n        return create_or_get_model(class_name, MessageModel, table_name)\n    elif storage_type == StorageType.KNOWLEDGE_BASE:\n\n        pass\n    elif storage_type == StorageType.KNOWLEDGE_BASE_DOCUMENTS:\n\n        # create schema for archival memory\n        class DocumentModel(Base):\n            \"\"\"Defines data model for storing Documents (consisting of text, embedding)\"\"\"\n\n            __abstract__ = True  # this line is necessary\n            \n\n            # id=create_uuid_from_string(f\"{str(source.id)}_{document_text}\"),\n            # text=document_text,\n            # metadata=document_metadata,\n            # data_source=source.name,\n            # user_id=source.user_id,\n\n\n            # Assuming passage_id is the primary key\n            # id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n            id = Column(CommonUUID, primary_key=True, default=uuid.uuid4)\n            # id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))\n            user_id = Column(CommonUUID, nullable=False)\n            text = Column(String)\n            source_id= Column(CommonUUID)\n            # agent_id= Column(CommonUUID)\n            data_source = Column(String)  # agent_name if agent, data_source name if from data source\n\n            # vector storage\n            if dialect == \"sqlite\":\n                embedding = Column(CommonVector)\n            # else:\n            #     from pgvector.sqlalchemy import Vector\n\n                # embedding = mapped_column(Vector(MAX_EMBEDDING_DIM))\n            embedding_dim = Column(BIGINT)\n            embedding_model = Column(String)\n\n            metadata_ = Column(MutableJson)\n\n            # Add a datetime column, with default value as the current time\n            created_at = Column(DateTime(timezone=True))\n\n            def __repr__(self):\n                return f\"<Passage(passage_id='{self.id}', text='{self.text}', embedding='{self.embedding})>\"\n\n            def to_record(self):\n                return Passage(\n                    text=self.text,\n                    embedding=self.embedding,\n                    embedding_dim=self.embedding_dim,\n                    embedding_model=self.embedding_model,\n                    doc_id=self.doc_id,\n                    user_id=self.user_id,\n                    id=self.id,\n                    data_source=self.data_source,\n                    agent_id=self.agent_id,\n                    metadata_=self.metadata_,\n                    created_at=self.created_at,\n                )\n\n        \"\"\"Create database model for table_name\"\"\"\n        class_name = f\"{table_name.capitalize()}Model\" + dialect\n        return create_or_get_model(class_name, DocumentModel, table_name)\n\n        # pass\n    elif storage_type == StorageType.KNOWLEDGE_BASE_PASSAGES:\n\n        pass\n    else:\n        raise ValueError(f\"storage type {storage_type} not implemented\")\n\n\n\nclass StorageConnector:\n    \"\"\"Defines a DB connection that is user-specific to access data: Documents, Passages, Archival/Recall Memory\"\"\"\n\n    # type: Type[Record]\n\n    def __init__(\n        self,\n        storage_type: Union[StorageType.ARCHIVAL_MEMORY, StorageType.RECALL_MEMORY, StorageType.KNOWLEDGE_BASE, StorageType.KNOWLEDGE_BASE_DOCUMENTS,StorageType.KNOWLEDGE_BASE_PASSAGES],\n        config: typeagentConfig,\n        user_id,\n        agent_id=None,\n    ):\n        self.user_id = user_id\n        self.agent_id = agent_id\n        self.storage_type = storage_type\n\n        # get object type\n        if storage_type == StorageType.ARCHIVAL_MEMORY:\n            # self.type = Passage\n            self.table_name = ARCHIVAL_TABLE_NAME\n        elif storage_type == StorageType.RECALL_MEMORY:\n            # self.type = Message\n            self.table_name = RECALL_TABLE_NAME\n        elif storage_type == StorageType.KNOWLEDGE_BASE:\n            # self.type = Document\n            self.table_name == KNOWLEDGE_BASE\n        elif storage_type == StorageType.KNOWLEDGE_BASE_DOCUMENTS:\n            # self.type = Passage\n            self.table_name = DOCUMENT_TABLE_NAME\n        elif storage_type == StorageType.KNOWLEDGE_BASE_PASSAGES:\n            # self.type = Passage\n            self.table_name = PASSAGE_TABLE_NAME\n        else:\n            raise ValueError(f\"Table type {storage_type} not implemented\")\n        printd(f\"Using table name {self.table_name}\")\n\n        # setup base filters for agent-specific tables\n        if self.storage_type == StorageType.ARCHIVAL_MEMORY or self.storage_type == StorageType.RECALL_MEMORY:\n            # agent-specific table\n            # assert agent_id is not None, \"Agent ID must be provided for agent-specific tables\"\n            self.filters = {\"user_id\": self.user_id, \"agent_id\": self.agent_id}\n        elif self.storage_type == StorageType.KNOWLEDGE_BASE_DOCUMENTS or self.storage_type == StorageType.KNOWLEDGE_BASE or self.storage_type == StorageType.KNOWLEDGE_BASE_PASSAGES:\n            # setup base filters for user-specific tables\n            # assert agent_id is None, \"Agent ID must not be provided for user-specific tables\"\n            self.filters = {\"user_id\": self.user_id}\n        else:\n            raise ValueError(f\"Table type {storage_type} not implemented\")\n\n    @staticmethod\n    def get_storage_connector(\n        storage_type: Union[StorageType.ARCHIVAL_MEMORY, StorageType.RECALL_MEMORY, StorageType.KNOWLEDGE_BASE, StorageType.KNOWLEDGE_BASE_DOCUMENTS,StorageType.KNOWLEDGE_BASE_PASSAGES],\n        config: typeagentConfig,\n        user_id,\n        agent_id=None,\n    ):\n        \n\n        if storage_type == StorageType.ARCHIVAL_MEMORY:\n            # self.type = Passage\n            storage_engine = config.archival_memory_storage_type\n        elif storage_type == StorageType.RECALL_MEMORY:\n            # self.type = Message\n            storage_engine = config.recall_memory_storage_type\n        elif storage_type == StorageType.KNOWLEDGE_BASE:\n            # self.type = Document\n            storage_engine = config.knowledge_base_storage_type\n        elif storage_type == StorageType.KNOWLEDGE_BASE_DOCUMENTS:\n            storage_engine = config.recall_memory_storage_type\n        elif storage_type == StorageType.KNOWLEDGE_BASE_PASSAGES:\n            # self.type = Passage\n            storage_engine = config.knowledge_base_storage_type\n        else:\n            raise ValueError(f\"storage type {storage_type} not implemented\")\n       \n\n        if storage_engine == \"postgres\":\n            from agent_store.sqldb.sqldbconnector import PostgresStorageConnector\n\n            return PostgresStorageConnector(storage_type, config, user_id, agent_id)\n        elif storage_engine == \"chroma\":\n            from agent_store.vectorsdb.chroma import ChromaStorageConnector\n\n            return ChromaStorageConnector(storage_type, config, user_id, agent_id)\n        elif storage_type == \"qdrant\":\n            from agent_store.vectorsdb.qdrant import QdrantStorageConnector\n\n            return QdrantStorageConnector(storage_type, config, user_id, agent_id)\n        # TODO: add back\n        # elif storage_type == \"lancedb\":\n        #    from agent_store.db import LanceDBConnector\n\n        #    return LanceDBConnector(agent_config=agent_config, table_type=table_type)\n\n        elif storage_type == \"sqlite\":\n            from agent_store.sqldb.sqldbconnector import SQLLiteStorageConnector\n\n            return SQLLiteStorageConnector(storage_type, config, user_id, agent_id)\n        elif storage_type == \"milvus\":\n            from agent_store.vectorsdb.milvus import MilvusStorageConnector\n            return MilvusStorageConnector(storage_type, config, user_id, agent_id)\n        else:\n            raise NotImplementedError(f\"Storage type {storage_type} not implemented\")\n\n    @staticmethod\n    def get_archival_storage_connector(user_id, agent_id):\n        config = typeagentConfig.load()\n        return StorageConnector.get_storage_connector(StorageType.ARCHIVAL_MEMORY, config, user_id, agent_id)\n    @staticmethod\n    def get_knowledge_Base_storage_connector(user_id, agent_id):\n        config = typeagentConfig.load()\n        return StorageConnector.get_storage_connector(StorageType.KNOWLEDGE_BASE_PASSAGES, config, user_id, agent_id)\n\n    @staticmethod\n    def get_recall_storage_connector(user_id, agent_id):\n        config = typeagentConfig.load()\n        return StorageConnector.get_storage_connector(StorageType.RECALL_MEMORY, config, user_id, agent_id)\n\n    @abstractmethod\n    def get_filters(self, filters: Optional[Dict] = {}) -> Union[Tuple[list, dict], dict]:\n        pass\n\n    @abstractmethod\n    def get_all_paginated(self, filters: Optional[Dict] = {}, page_size: int = 1000) -> Iterator[List[RecordType]]:\n        pass\n\n    @abstractmethod\n    def get_all(self, filters: Optional[Dict] = {}, limit=10) -> List[RecordType]:\n        pass\n\n    @abstractmethod\n    def get(self, id: uuid.UUID) -> Optional[RecordType]:\n        pass\n\n    @abstractmethod\n    def size(self, filters: Optional[Dict] = {}) -> int:\n        pass\n\n    @abstractmethod\n    def insert(self, record: RecordType):\n        pass\n\n    @abstractmethod\n    def insert_many(self, records: List[RecordType], show_progress=False):\n        pass\n\n    @abstractmethod\n    def query(self, query: str, query_vec: List[float], top_k: int = 10, filters: Optional[Dict] = {}) -> List[RecordType]:\n        pass\n\n    @abstractmethod\n    def query_date(self, start_date, end_date):\n        pass\n\n    @abstractmethod\n    def query_text(self, query):\n        pass\n\n    @abstractmethod\n    def delete_table(self):\n        pass\n\n    @abstractmethod\n    def delete(self, filters: Optional[Dict] = {}):\n        pass\n\n    @abstractmethod\n    def save(self):\n        pass\n"}
{"type": "source_file", "path": "luann/autogen/examples/agent_docs.py", "content": "\"\"\"Example of how to add typeagent into an AutoGen groupchat and chat with docs.\n\nSee https://typeagent.readme.io/docs/autogen#part-4-attaching-documents-to-typeagent-autogen-agents\n\nBased on the official AutoGen example here: https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb\n\nBegin by doing:\n  pip install \"pyautogen[teachable]\"\n  pip install pytypeagent\n  or\n  pip install -e . (inside the typeagent home directory)\n\"\"\"\n\nimport os\n\nimport autogen\n\nfrom typeagent.autogen.typeagent_agent import create_typeagent_autogen_agent_from_config\nfrom typeagent.constants import DEFAULT_PRESET, LLM_MAX_TOKENS\n\nLLM_BACKEND = \"openai\"\n# LLM_BACKEND = \"azure\"\n# LLM_BACKEND = \"local\"\n\nif LLM_BACKEND == \"openai\":\n    # For demo purposes let's use gpt-4\n    model = \"gpt-4\"\n\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    assert openai_api_key, \"You must set OPENAI_API_KEY or set LLM_BACKEND to 'local' to run this example\"\n\n    # This config is for AutoGen agents that are not powered by typeagent\n    config_list = [\n        {\n            \"model\": model,\n            \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n        }\n    ]\n\n    # This config is for AutoGen agents that powered by typeagent\n    config_list_typeagent = [\n        {\n            \"model\": model,\n            \"context_window\": LLM_MAX_TOKENS[model],\n            \"preset\": DEFAULT_PRESET,\n            \"model_wrapper\": None,\n            # OpenAI specific\n            \"model_endpoint_type\": \"openai\",\n            \"model_endpoint\": \"https://api.openai.com/v1\",\n            \"openai_key\": openai_api_key,\n        },\n    ]\n\nelif LLM_BACKEND == \"azure\":\n    # Make sure that you have access to this deployment/model on your Azure account!\n    # If you don't have access to the model, the code will fail\n    model = \"gpt-4\"\n\n    azure_openai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n    azure_openai_version = os.getenv(\"AZURE_OPENAI_VERSION\")\n    azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n    assert (\n        azure_openai_api_key is not None and azure_openai_version is not None and azure_openai_endpoint is not None\n    ), \"Set all the required OpenAI Azure variables (see: https://typeagent.readme.io/docs/endpoints#azure-openai)\"\n\n    # This config is for AutoGen agents that are not powered by typeagent\n    config_list = [\n        {\n            \"model\": model,\n            \"api_type\": \"azure\",\n            \"api_key\": azure_openai_api_key,\n            \"api_version\": azure_openai_version,\n            # NOTE: on versions of pyautogen < 0.2.0, use \"api_base\"\n            # \"api_base\": azure_openai_endpoint,\n            \"base_url\": azure_openai_endpoint,\n        }\n    ]\n\n    # This config is for AutoGen agents that powered by typeagent\n    config_list_typeagent = [\n        {\n            \"model\": model,\n            \"context_window\": LLM_MAX_TOKENS[model],\n            \"preset\": DEFAULT_PRESET,\n            \"model_wrapper\": None,\n            # Azure specific\n            \"model_endpoint_type\": \"azure\",\n            \"azure_key\": azure_openai_api_key,\n            \"azure_endpoint\": azure_openai_endpoint,\n            \"azure_version\": azure_openai_version,\n        },\n    ]\n\nelif LLM_BACKEND == \"local\":\n    # Example using LM Studio on a local machine\n    # You will have to change the parameters based on your setup\n\n    # Non-typeagent agents will still use local LLMs, but they will use the ChatCompletions endpoint\n    config_list = [\n        {\n            \"model\": \"NULL\",  # not needed\n            # NOTE: on versions of pyautogen < 0.2.0 use \"api_base\", and also uncomment \"api_type\"\n            # \"api_base\": \"http://localhost:1234/v1\",\n            # \"api_type\": \"open_ai\",\n            \"base_url\": \"http://localhost:1234/v1\",  # ex. \"http://127.0.0.1:5001/v1\" if you are using webui, \"http://localhost:1234/v1/\" if you are using LM Studio\n            \"api_key\": \"NULL\",  #  not needed\n        },\n    ]\n\n    # typeagent-powered agents will also use local LLMs, but they need additional setup (also they use the Completions endpoint)\n    config_list_typeagent = [\n        {\n            \"preset\": DEFAULT_PRESET,\n            \"model\": None,  # only required for Ollama, see: https://typeagent.readme.io/docs/ollama\n            \"context_window\": 8192,  # the context window of your model (for Mistral 7B-based models, it's likely 8192)\n            \"model_wrapper\": \"chatml\",  # chatml is the default wrapper\n            \"model_endpoint_type\": \"lmstudio\",  # can use webui, ollama, llamacpp, etc.\n            \"model_endpoint\": \"http://localhost:1234\",  # the IP address of your LLM backend\n        },\n    ]\n\nelse:\n    raise ValueError(LLM_BACKEND)\n\n# Set to True if you want to print typeagent's inner workings.\nDEBUG = False\n\ninterface_kwargs = {\n    \"debug\": DEBUG,\n    \"show_inner_thoughts\": True,\n    \"show_function_outputs\": True,  # let's set this to True so that we can see the search function in action\n}\n\nllm_config = {\"config_list\": config_list, \"seed\": 42}\nllm_config_typeagent = {\"config_list\": config_list_typeagent, \"seed\": 42}\n\n# The user agent\nuser_proxy = autogen.UserProxyAgent(\n    name=\"User_proxy\",\n    system_message=\"A human admin.\",\n    code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n    human_input_mode=\"TERMINATE\",  # needed?\n    default_auto_reply=\"...\",  # Set a default auto-reply message here (non-empty auto-reply is required for LM Studio)\n)\n\n# In our example, we swap this AutoGen agent with a typeagent agent\n# This typeagent agent will have all the benefits of typeagent, ie persistent memory, etc.\ntypeagent_agent = create_typeagent_autogen_agent_from_config(\n    \"typeagent_agent\",\n    llm_config=llm_config_typeagent,\n    system_message=f\"You are an AI research assistant.\\n\" f\"You are participating in a group chat with a user ({user_proxy.name}).\",\n    interface_kwargs=interface_kwargs,\n    default_auto_reply=\"...\",  # Set a default auto-reply message here (non-empty auto-reply is required for LM Studio)\n    skip_verify=False,  # NOTE: you should set this to True if you expect your typeagent AutoGen agent to call a function other than send_message on the first turn\n)\n# NOTE: you need to follow steps to load document first: see https://typeagent.readme.io/docs/autogen#part-4-attaching-documents-to-typeagent-autogen-agents\ntypeagent_agent.load_and_attach(\n    name=\"typeagent_research_paper\",\n    type=\"directory\",\n    input_dir=None,\n    input_files=[\"typeagent_research_paper.pdf\"],\n    # force=True,\n)\n\n# Initialize the group chat between the agents\ngroupchat = autogen.GroupChat(agents=[user_proxy, typeagent_agent], messages=[], max_round=3, speaker_selection_method=\"round_robin\")\nmanager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n\n# Begin the group chat with a message from the user\nuser_proxy.initiate_chat(\n    manager,\n    message=\"Tell me what virtual context in typeagent is. Search your archival memory.\",\n)\n"}
{"type": "source_file", "path": "luann/autogen/examples/agent_autoreply.py", "content": "\"\"\"Example of how to add typeagent into an AutoGen groupchat\n\nBased on the official AutoGen example here: https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb\n\nBegin by doing:\n  pip install \"pyautogen[teachable]\"\n  pip install pytypeagent\n  or\n  pip install -e . (inside the typeagent home directory)\n\"\"\"\n\nimport os\n\nimport autogen\n\nfrom typeagent.autogen.typeagent_agent import create_typeagent_autogen_agent_from_config\nfrom typeagent.constants import DEFAULT_PRESET, LLM_MAX_TOKENS\n\nLLM_BACKEND = \"openai\"\n# LLM_BACKEND = \"azure\"\n# LLM_BACKEND = \"local\"\n\nif LLM_BACKEND == \"openai\":\n    # For demo purposes let's use gpt-4\n    model = \"gpt-4\"\n\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    assert openai_api_key, \"You must set OPENAI_API_KEY or set LLM_BACKEND to 'local' to run this example\"\n\n    # This config is for AutoGen agents that are not powered by typeagent\n    config_list = [\n        {\n            \"model\": model,\n            \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n        }\n    ]\n\n    # This config is for AutoGen agents that powered by typeagent\n    config_list_typeagent = [\n        {\n            \"model\": model,\n            \"context_window\": LLM_MAX_TOKENS[model],\n            \"preset\": DEFAULT_PRESET,\n            \"model_wrapper\": None,\n            # OpenAI specific\n            \"model_endpoint_type\": \"openai\",\n            \"model_endpoint\": \"https://api.openai.com/v1\",\n            \"openai_key\": openai_api_key,\n        },\n    ]\n\nelif LLM_BACKEND == \"azure\":\n    # Make sure that you have access to this deployment/model on your Azure account!\n    # If you don't have access to the model, the code will fail\n    model = \"gpt-4\"\n\n    azure_openai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n    azure_openai_version = os.getenv(\"AZURE_OPENAI_VERSION\")\n    azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n    assert (\n        azure_openai_api_key is not None and azure_openai_version is not None and azure_openai_endpoint is not None\n    ), \"Set all the required OpenAI Azure variables (see: https://typeagent.readme.io/docs/endpoints#azure-openai)\"\n\n    # This config is for AutoGen agents that are not powered by typeagent\n    config_list = [\n        {\n            \"model\": model,\n            \"api_type\": \"azure\",\n            \"api_key\": azure_openai_api_key,\n            \"api_version\": azure_openai_version,\n            # NOTE: on versions of pyautogen < 0.2.0, use \"api_base\"\n            # \"api_base\": azure_openai_endpoint,\n            \"base_url\": azure_openai_endpoint,\n        }\n    ]\n\n    # This config is for AutoGen agents that powered by typeagent\n    config_list_typeagent = [\n        {\n            \"model\": model,\n            \"context_window\": LLM_MAX_TOKENS[model],\n            \"preset\": DEFAULT_PRESET,\n            \"model_wrapper\": None,\n            # Azure specific\n            \"model_endpoint_type\": \"azure\",\n            \"azure_key\": azure_openai_api_key,\n            \"azure_endpoint\": azure_openai_endpoint,\n            \"azure_version\": azure_openai_version,\n        },\n    ]\n\nelif LLM_BACKEND == \"local\":\n    # Example using LM Studio on a local machine\n    # You will have to change the parameters based on your setup\n\n    # Non-typeagent agents will still use local LLMs, but they will use the ChatCompletions endpoint\n    config_list = [\n        {\n            \"model\": \"NULL\",  # not needed\n            # NOTE: on versions of pyautogen < 0.2.0 use \"api_base\", and also uncomment \"api_type\"\n            # \"api_base\": \"http://localhost:1234/v1\",\n            # \"api_type\": \"open_ai\",\n            \"base_url\": \"http://localhost:1234/v1\",  # ex. \"http://127.0.0.1:5001/v1\" if you are using webui, \"http://localhost:1234/v1/\" if you are using LM Studio\n            \"api_key\": \"NULL\",  #  not needed\n        },\n    ]\n\n    # typeagent-powered agents will also use local LLMs, but they need additional setup (also they use the Completions endpoint)\n    config_list_typeagent = [\n        {\n            \"preset\": DEFAULT_PRESET,\n            \"model\": None,  # only required for Ollama, see: https://typeagent.readme.io/docs/ollama\n            \"context_window\": 8192,  # the context window of your model (for Mistral 7B-based models, it's likely 8192)\n            \"model_wrapper\": \"chatml\",  # chatml is the default wrapper\n            \"model_endpoint_type\": \"lmstudio\",  # can use webui, ollama, llamacpp, etc.\n            \"model_endpoint\": \"http://localhost:1234\",  # the IP address of your LLM backend\n        },\n    ]\n\nelse:\n    raise ValueError(LLM_BACKEND)\n\n\n# If USE_typeagent is False, then this example will be the same as the official AutoGen repo\n# (https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb)\n# If USE_typeagent is True, then we swap out the \"coder\" agent with a typeagent agent\nUSE_typeagent = True\n\nllm_config = {\"config_list\": config_list, \"seed\": 42}\nllm_config_typeagent = {\"config_list\": config_list_typeagent, \"seed\": 42}\n\n# Set to True if you want to print typeagent's inner workings.\nDEBUG = False\ninterface_kwargs = {\n    \"debug\": DEBUG,\n    \"show_inner_thoughts\": DEBUG,\n    \"show_function_outputs\": DEBUG,\n}\n\n# The user agent\nuser_proxy = autogen.UserProxyAgent(\n    name=\"User_proxy\",\n    system_message=\"A human admin.\",\n    code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n    human_input_mode=\"TERMINATE\",  # needed?\n    default_auto_reply=\"You are going to figure all out by your own. \"\n    \"Work by yourself, the user won't reply until you output `TERMINATE` to end the conversation.\",\n)\n\nif not USE_typeagent:\n    # In the AutoGen example, we create an AssistantAgent to play the role of the coder\n    coder = autogen.AssistantAgent(\n        name=\"Coder\",\n        llm_config=llm_config,\n        system_message=f\"I am a 10x engineer, trained in Python. I was the first engineer at Uber \"\n        f\"(which I make sure to tell everyone I work with).\",\n        human_input_mode=\"TERMINATE\",\n        default_auto_reply=\"...\",  # Set a default auto-reply message here (non-empty auto-reply is required for LM Studio)\n    )\n\nelse:\n    # In our example, we swap this AutoGen agent with a typeagent agent\n    # This typeagent agent will have all the benefits of typeagent, ie persistent memory, etc.\n    coder = create_typeagent_autogen_agent_from_config(\n        \"typeagent_coder\",\n        llm_config=llm_config_typeagent,\n        nontypeagent_llm_config=llm_config,\n        system_message=f\"I am a 10x engineer, trained in Python. I was the first engineer at Uber \"\n        f\"(which I make sure to tell everyone I work with).\",\n        human_input_mode=\"TERMINATE\",\n        interface_kwargs=interface_kwargs,\n        default_auto_reply=\"...\",  # Set a default auto-reply message here (non-empty auto-reply is required for LM Studio)\n        skip_verify=False,  # NOTE: you should set this to True if you expect your typeagent AutoGen agent to call a function other than send_message on the first turn\n    )\n\n# Begin the group chat with a message from the user\nuser_proxy.initiate_chat(\n    coder,\n    message=\"I want to design an app to make me one million dollars in one month. \" \"Tell me all the details, then try out every steps.\",\n)\n"}
{"type": "source_file", "path": "luann/autogen/memgpt_agent.py", "content": "import sys\nimport uuid\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nfrom autogen.agentchat import (\n    Agent,\n    ConversableAgent,\n    GroupChat,\n    GroupChatManager,\n    UserProxyAgent,\n)\n\nimport typeagent.constants as constants\nimport typeagent.system as system\nimport typeagent.utils as utils\nfrom typeagent.agent import Agent as typeagentAgent\nfrom typeagent.agent import save_agent\nfrom typeagent.agent_store.storage import StorageConnector, StorageType\nfrom typeagent.autogen.interface import AutoGenInterface\nfrom typeagent.cli.cli_load import load_directory, load_vector_database\nfrom typeagent.config import typeagentConfig\nfrom typeagent.credentials import typeagentCredentials\nfrom typeagent.data_types import EmbeddingConfig, LLMConfig, User\nfrom typeagent.metadata import MetadataStore\nfrom typeagent.utils import get_human_text, get_persona_text\n\n\nclass typeagentConversableAgent(ConversableAgent):\n    def __init__(\n        self,\n        name: str,\n        agent: typeagentAgent,\n        skip_verify: bool = False,\n        auto_save: bool = False,\n        concat_other_agent_messages: bool = False,\n        is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n        default_auto_reply: Optional[Union[str, Dict, None]] = \"\",\n    ):\n        \"\"\"A wrapper around a typeagent agent that implements the AutoGen ConversibleAgent functions\n\n        This allows the typeagent agent to be used in an AutoGen groupchat\n        \"\"\"\n        super().__init__(name, llm_config=False)\n        self.agent = agent\n        self.skip_verify = skip_verify\n        self.auto_save = auto_save\n\n        self.concat_other_agent_messages = concat_other_agent_messages\n        self.register_reply([Agent, None], typeagentConversableAgent._generate_reply_for_user_message)\n        self.messages_processed_up_to_idx = 0\n        self._default_auto_reply = default_auto_reply\n\n        self._is_termination_msg = is_termination_msg if is_termination_msg is not None else (lambda x: x == \"TERMINATE\")\n\n        config = typeagentConfig.load()\n        self.ms = MetadataStore(config)\n\n    def save(self):\n        \"\"\"Save the underlying typeagent agent to the database\"\"\"\n        try:\n            save_agent(agent=self.agent, ms=self.ms)\n        except Exception as e:\n            print(f\"Failed to save typeagent AutoGen agent\\n{self.agent}\\nError: {str(e)}\")\n            raise\n\n    def load(self, name: str, type: str, **kwargs):\n        # call load function based on type\n        if type == \"directory\":\n            load_directory(name=name, **kwargs)\n        elif type == \"webpage\":\n            load_webpage(name=name, **kwargs)\n        elif type == \"database\":\n            load_database(name=name, **kwargs)\n        elif type == \"vector_database\":\n            load_vector_database(name=name, **kwargs)\n        else:\n            raise ValueError(f\"Invalid data source type {type}\")\n\n    def attach(self, data_source: str):\n        # attach new data\n        config = typeagentConfig.load()\n        source_connector = StorageConnector.get_storage_connector(TableType.PASSAGES, config, user_id=self.agent.agent_state.user_id)\n        self.agent.attach_source(data_source, source_connector, ms=self.ms)\n\n    def load_and_attach(self, name: str, type: str, force=False, **kwargs):\n        # check if data source already exists\n        data_source_options = self.ms.list_sources(user_id=self.agent.agent_state.user_id)\n        data_source_options = [s.name for s in data_source_options]\n\n        kwargs[\"user_id\"] = self.agent.agent_state.user_id\n\n        if name in data_source_options and not force:\n            print(f\"Data source {name} already exists. Use force=True to overwrite.\")\n            self.attach(name)\n        else:\n            self.load(name, type, **kwargs)\n            self.attach(name)\n\n    def format_other_agent_message(self, msg):\n        if \"name\" in msg:\n            user_message = f\"{msg['name']}: {msg['content']}\"\n        else:\n            user_message = msg[\"content\"]\n        return user_message\n\n    def find_last_user_message(self):\n        last_user_message = None\n        for msg in self.agent.messages:\n            if msg[\"role\"] == \"user\":\n                last_user_message = msg[\"content\"]\n        return last_user_message\n\n    def find_new_messages(self, entire_message_list):\n        \"\"\"Extract the subset of messages that's actually new\"\"\"\n        return entire_message_list[self.messages_processed_up_to_idx :]\n\n    @staticmethod\n    def _format_autogen_message(autogen_message):\n        # {'content': \"...\", 'name': '...', 'role': 'user'}\n        if not isinstance(autogen_message, dict) or ():\n            print(f\"Warning: AutoGen message was not a dict -- {autogen_message}\")\n            user_message = system.package_user_message(autogen_message)\n        elif \"content\" not in autogen_message or \"name\" not in autogen_message or \"name\" not in autogen_message:\n            print(f\"Warning: AutoGen message was missing fields -- {autogen_message}\")\n            user_message = system.package_user_message(autogen_message)\n        else:\n            user_message = system.package_user_message(user_message=autogen_message[\"content\"], name=autogen_message[\"name\"])\n\n        return user_message\n\n    def _generate_reply_for_user_message(\n        self,\n        messages: Optional[List[Dict]] = None,\n        sender: Optional[Agent] = None,\n        config: Optional[Any] = None,\n    ) -> Tuple[bool, Union[str, Dict, None]]:\n        assert isinstance(\n            self.agent.interface, AutoGenInterface\n        ), f\"typeagent AutoGen Agent is using the wrong interface - {self.agent.interface}\"\n        self.agent.interface.reset_message_list()\n\n        new_messages = self.find_new_messages(messages)\n        new_messages_count = len(new_messages)\n        if new_messages_count > 1:\n            if self.concat_other_agent_messages:\n                # Combine all the other messages into one message\n                user_message = \"\\n\".join([self.format_other_agent_message(m) for m in new_messages])\n            else:\n                # Extend the typeagent message list with multiple 'user' messages, then push the last one with agent.step()\n                self.agent.append_to_messages(new_messages[:-1])\n                user_message = new_messages[-1]\n        elif new_messages_count == 1:\n            user_message = new_messages[0]\n        else:\n            return True, self._default_auto_reply\n\n        # Package the user message\n        # user_message = system.package_user_message(user_message)\n        user_message = self._format_autogen_message(user_message)\n\n        # Send a single message into typeagent\n        while True:\n            (\n                new_messages,\n                heartbeat_request,\n                function_failed,\n                token_warning,\n                tokens_accumulated,\n            ) = self.agent.step(user_message, first_message=False, skip_verify=self.skip_verify)\n            # Skip user inputs if there's a memory warning, function execution failed, or the agent asked for control\n            if token_warning:\n                user_message = system.get_token_limit_warning()\n            elif function_failed:\n                user_message = system.get_heartbeat(constants.FUNC_FAILED_HEARTBEAT_MESSAGE)\n            elif heartbeat_request:\n                user_message = system.get_heartbeat(constants.REQ_HEARTBEAT_MESSAGE)\n            else:\n                break\n\n        # Stop the conversation\n        if self._is_termination_msg(new_messages[-1][\"content\"]):\n            return True, None\n\n        # Pass back to AutoGen the pretty-printed calls typeagent made to the interface\n        pretty_ret = typeagentConversableAgent.pretty_concat(self.agent.interface.message_list)\n        self.messages_processed_up_to_idx += new_messages_count\n\n        # If auto_save is on, save after every full step\n        if self.auto_save:\n            self.save()\n\n        return True, pretty_ret\n\n    @staticmethod\n    def pretty_concat(messages):\n        \"\"\"AutoGen expects a single response, but typeagent may take many steps.\n\n        To accommodate AutoGen, concatenate all of typeagent's steps into one and return as a single message.\n        \"\"\"\n        ret = {\"role\": \"assistant\", \"content\": \"\"}\n        lines = []\n        for m in messages:\n            lines.append(f\"{m}\")\n        ret[\"content\"] = \"\\n\".join(lines)\n\n        # prevent error in LM Studio caused by scenarios where typeagent didn't say anything\n        if ret[\"content\"] in [\"\", \"\\n\"]:\n            ret[\"content\"] = \"...\"\n\n        return ret\n\n\ndef update_config_from_dict(config_object: Union[LLMConfig, EmbeddingConfig], config_dict: dict) -> bool:\n    \"\"\"Utility method used in the agent creation process for AutoGen\n\n    Update the attributes of a configuration object based on a dictionary.\n\n    :param config_object: The configuration object to be updated.\n    :param config_dict: The dictionary containing new values for the configuration.\n    \"\"\"\n    was_modified = False\n    for attr in dir(config_object):\n        # Filter out private attributes and methods\n        if not attr.startswith(\"_\") and not callable(getattr(config_object, attr)):\n            if attr in config_dict:\n                # Cast the value to the type of the attribute in config_object\n                attr_type = type(getattr(config_object, attr))\n                try:\n                    setattr(config_object, attr, attr_type(config_dict[attr]))\n                    was_modified = True\n                except TypeError:\n                    print(f\"Type mismatch for attribute {attr}, cannot cast {config_dict[attr]} to {attr_type}\")\n\n    return was_modified\n\n\ndef load_autogen_typeagent_agent(\n    agent_config: dict,\n    skip_verify: bool = False,\n    auto_save: bool = False,\n    interface: bool = None,\n    interface_kwargs: dict = {},\n    default_auto_reply: Optional[Union[str, Dict, None]] = \"\",\n    is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n) -> typeagentConversableAgent:\n    \"\"\"Load a typeagent agent into a wrapped ConversableAgent class\"\"\"\n    if \"name\" not in agent_config:\n        raise ValueError(\"Must provide 'name' in agent_config to load an agent\")\n\n    interface = AutoGenInterface(**interface_kwargs) if interface is None else interface\n\n    config = typeagentConfig.load()\n    # Create the default user, or load the specified user\n    ms = MetadataStore(config)\n    if \"user_id\" not in agent_config:\n        user_id = uuid.UUID(config.anon_clientid)\n        user = ms.get_user(user_id=user_id)\n        if user is None:\n            ms.create_user(User(id=user_id))\n            user = ms.get_user(user_id=user_id)\n            if user is None:\n                raise ValueError(f\"Failed to create default user {str(user_id)} in database.\")\n    else:\n        user_id = uuid.UUID(agent_config[\"user_id\"])\n        user = ms.get_user(user_id=user_id)\n\n    # Make sure that the agent already exists\n    agent_state = ms.get_agent(agent_name=agent_config[\"name\"], user_id=user.id)\n    if agent_state is None:\n        raise ValueError(f\"Couldn't find an agent named {agent_config['name']} in the agent database\")\n\n    # Create the agent object directly from the loaded state (not via preset creation)\n    try:\n        typeagent_agent = typeagentAgent(agent_state=agent_state, interface=interface)\n    except Exception:\n        print(f\"Failed to create an agent object from agent state =\\n{agent_state}\")\n        raise\n\n    # If the user provided new config information, write it out to the agent\n    # E.g. if the user is trying to load the same agent, but on a new LLM backend\n    llm_config_was_modified = update_config_from_dict(typeagent_agent.agent_state.llm_config, agent_config)\n    embedding_config_was_modified = update_config_from_dict(typeagent_agent.agent_state.embedding_config, agent_config)\n    if llm_config_was_modified or embedding_config_was_modified:\n        save_agent(agent=typeagent_agent, ms=ms)\n\n    # After creating the agent, we then need to wrap it in a ConversableAgent so that it can be plugged into AutoGen\n    autogen_typeagent_agent = typeagentConversableAgent(\n        name=agent_state.name,\n        agent=typeagent_agent,\n        default_auto_reply=default_auto_reply,\n        is_termination_msg=is_termination_msg,\n        skip_verify=skip_verify,\n        auto_save=auto_save,\n    )\n    return autogen_typeagent_agent\n\n\ndef create_autogen_typeagent_agent(\n    agent_config: dict,\n    skip_verify: bool = False,\n    auto_save: bool = False,\n    interface: bool = None,\n    interface_kwargs: dict = {},\n    default_auto_reply: Optional[Union[str, Dict, None]] = \"\",\n    is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n) -> typeagentConversableAgent:\n    \"\"\"\n    See AutoGenInterface.__init__ for available options you can pass into\n    `interface_kwargs`.  For example, typeagent's inner monologue and functions are\n    off by default so that they are not visible to the other agents. You can\n    turn these on by passing in\n    ```\n    interface_kwargs={\n        \"debug\": True,  # to see all typeagent activity\n        \"show_inner_thoughts: True  # to print typeagent inner thoughts \"globally\"\n                                    # (visible to all AutoGen agents)\n    }\n    ```\n    \"\"\"\n    interface = AutoGenInterface(**interface_kwargs) if interface is None else interface\n\n    config = typeagentConfig.load()\n    llm_config = config.default_llm_config\n    embedding_config = config.default_embedding_config\n\n    # Overwrite parts of the LLM and embedding configs that were passed into the config dicts\n    update_config_from_dict(llm_config, agent_config)\n    update_config_from_dict(embedding_config, agent_config)\n\n    # Create the default user, or load the specified user\n    ms = MetadataStore(config)\n    if \"user_id\" not in agent_config:\n        user_id = uuid.UUID(config.anon_clientid)\n        user = ms.get_user(user_id=user_id)\n        if user is None:\n            ms.create_user(User(id=user_id))\n            user = ms.get_user(user_id=user_id)\n            if user is None:\n                raise ValueError(f\"Failed to create default user {str(user_id)} in database.\")\n    else:\n        user_id = uuid.UUID(agent_config[\"user_id\"])\n        user = ms.get_user(user_id=user_id)\n\n    try:\n        preset_obj = ms.get_preset(name=agent_config[\"preset\"] if \"preset\" in agent_config else config.preset, user_id=user.id)\n        if preset_obj is None:\n            # create preset records in metadata store\n            from typeagent.presets.presets import add_default_presets\n\n            add_default_presets(user.id, ms)\n            # try again\n            preset_obj = ms.get_preset(name=agent_config[\"preset\"] if \"preset\" in agent_config else config.preset, user_id=user.id)\n            if preset_obj is None:\n                print(\"Couldn't find presets in database, please run `typeagent configure`\")\n                sys.exit(1)\n\n        # Overwrite fields in the preset if they were specified\n        # TODO make sure that the human/persona aren't filenames but actually real values\n        preset_obj.human = agent_config[\"human\"] if \"human\" in agent_config else get_human_text(config.human)\n        preset_obj.persona = agent_config[\"persona\"] if \"persona\" in agent_config else get_persona_text(config.persona)\n\n        typeagent_agent = typeagentAgent(\n            interface=interface,\n            name=agent_config[\"name\"] if \"name\" in agent_config else None,\n            created_by=user.id,\n            preset=preset_obj,\n            llm_config=llm_config,\n            embedding_config=embedding_config,\n            # gpt-3.5-turbo tends to omit inner monologue, relax this requirement for now\n            first_message_verify_mono=True if (llm_config.model is not None and \"gpt-4\" in llm_config.model) else False,\n        )\n        # Save agent in database immediately after writing\n        save_agent(agent=typeagent_agent, ms=ms)\n    except ValueError as e:\n        raise ValueError(f\"Failed to create agent from provided information:\\n{agent_config}\\n\\nError: {str(e)}\")\n\n    # After creating the agent, we then need to wrap it in a ConversableAgent so that it can be plugged into AutoGen\n    autogen_typeagent_agent = typeagentConversableAgent(\n        name=typeagent_agent.agent_state.name,\n        agent=typeagent_agent,\n        default_auto_reply=default_auto_reply,\n        is_termination_msg=is_termination_msg,\n        skip_verify=skip_verify,\n        auto_save=auto_save,\n    )\n    return autogen_typeagent_agent\n\n\ndef create_typeagent_autogen_agent_from_config(\n    name: str,\n    system_message: Optional[str] = \"You are a helpful AI Assistant.\",\n    is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n    max_consecutive_auto_reply: Optional[int] = None,\n    human_input_mode: Optional[str] = \"ALWAYS\",\n    function_map: Optional[Dict[str, Callable]] = None,\n    code_execution_config: Optional[Union[Dict, bool]] = None,\n    llm_config: Optional[Union[Dict, bool]] = None,\n    # config setup for non-typeagent agents:\n    nontypeagent_llm_config: Optional[Union[Dict, bool]] = None,\n    default_auto_reply: Optional[Union[str, Dict, None]] = \"\",\n    interface_kwargs: Dict = None,\n    skip_verify: bool = False,\n    auto_save: bool = False,\n) -> typeagentConversableAgent:\n    \"\"\"Same function signature as used in base AutoGen, but creates a typeagent agent\n\n    Construct AutoGen config workflow in a clean way.\n    \"\"\"\n    if not isinstance(llm_config, dict):\n        llm_config = None\n    llm_config = llm_config[\"config_list\"][0]\n\n    if interface_kwargs is None:\n        interface_kwargs = {}\n\n    # The \"system message\" in AutoGen becomes the persona in typeagent\n    persona_desc = utils.get_persona_text(constants.DEFAULT_PERSONA) if system_message == \"\" else system_message\n    # The user profile is based on the input mode\n    if human_input_mode == \"ALWAYS\":\n        user_desc = \"\"\n    elif human_input_mode == \"TERMINATE\":\n        user_desc = \"Work by yourself, the user won't reply until you output `TERMINATE` to end the conversation.\"\n    else:\n        user_desc = \"Work by yourself, the user won't reply. Elaborate as much as possible.\"\n\n    # If using azure or openai, save the credentials to the config\n    config = typeagentConfig.load()\n    credentials = typeagentCredentials.load()\n\n    if (\n        llm_config[\"model_endpoint_type\"] in [\"azure\", \"openai\"]\n        or llm_config[\"model_endpoint_type\"] != config.default_llm_config.model_endpoint_type\n    ):\n        # we load here to make sure we don't override existing values\n        # all we want to do is add extra credentials\n\n        if llm_config[\"model_endpoint_type\"] == \"azure\":\n            credentials.azure_key = llm_config[\"azure_key\"]\n            credentials.azure_endpoint = llm_config[\"azure_endpoint\"]\n            credentials.azure_version = llm_config[\"azure_version\"]\n            llm_config.pop(\"azure_key\")\n            llm_config.pop(\"azure_endpoint\")\n            llm_config.pop(\"azure_version\")\n\n        elif llm_config[\"model_endpoint_type\"] == \"openai\":\n            credentials.openai_key = llm_config[\"openai_key\"]\n            llm_config.pop(\"openai_key\")\n\n        credentials.save()\n\n    # Create an AgentConfig option from the inputs\n    llm_config.pop(\"name\", None)\n    llm_config.pop(\"persona\", None)\n    llm_config.pop(\"human\", None)\n    agent_config = dict(\n        name=name,\n        persona=persona_desc,\n        human=user_desc,\n        **llm_config,\n    )\n\n    if function_map is not None or code_execution_config is not None:\n        raise NotImplementedError\n\n    autogen_typeagent_agent = create_autogen_typeagent_agent(\n        agent_config,\n        default_auto_reply=default_auto_reply,\n        is_termination_msg=is_termination_msg,\n        interface_kwargs=interface_kwargs,\n        skip_verify=skip_verify,\n        auto_save=auto_save,\n    )\n\n    if human_input_mode != \"ALWAYS\":\n        coop_agent1 = create_autogen_typeagent_agent(\n            agent_config,\n            default_auto_reply=default_auto_reply,\n            is_termination_msg=is_termination_msg,\n            interface_kwargs=interface_kwargs,\n            skip_verify=skip_verify,\n            auto_save=auto_save,\n        )\n        if default_auto_reply != \"\":\n            coop_agent2 = UserProxyAgent(\n                \"User_proxy\",\n                human_input_mode=\"NEVER\",\n                default_auto_reply=default_auto_reply,\n            )\n        else:\n            coop_agent2 = create_autogen_typeagent_agent(\n                agent_config,\n                default_auto_reply=default_auto_reply,\n                is_termination_msg=is_termination_msg,\n                interface_kwargs=interface_kwargs,\n                skip_verify=skip_verify,\n                auto_save=auto_save,\n            )\n\n        groupchat = GroupChat(\n            agents=[autogen_typeagent_agent, coop_agent1, coop_agent2],\n            messages=[],\n            max_round=12 if max_consecutive_auto_reply is None else max_consecutive_auto_reply,\n        )\n        assert nontypeagent_llm_config is not None\n        manager = GroupChatManager(name=name, groupchat=groupchat, llm_config=nontypeagent_llm_config)\n        return manager\n\n    else:\n        return autogen_typeagent_agent\n"}
{"type": "source_file", "path": "luann/__main__.py", "content": "from .main import app\n\napp()\n"}
{"type": "source_file", "path": "luann/credentials.py", "content": "import configparser\nimport os\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom config import get_field, set_field\nfrom constants import typeagent_DIR\n\nSUPPORTED_AUTH_TYPES = [\"bearer_token\", \"api_key\"]\n\n\n@dataclass\nclass typeagentCredentials:\n    # credentials for typeagent\n    credentials_path: str = os.path.join(typeagent_DIR, \"credentials\")\n\n    # openai config\n    openai_auth_type: str = \"bearer_token\"\n    openai_key: Optional[str] = os.getenv(\"OPENAI_API_KEY\")\n\n    # gemini config\n    google_ai_key: Optional[str] = None\n    google_ai_service_endpoint: Optional[str] = None\n\n    # anthropic config\n    anthropic_key: Optional[str] = None\n\n    # cohere config\n    cohere_key: Optional[str] = None\n\n    # azure config\n    azure_auth_type: str = \"api_key\"\n    azure_key: Optional[str] = None\n    # base llm / model\n    azure_version: Optional[str] = None\n    azure_endpoint: Optional[str] = None\n    azure_deployment: Optional[str] = None\n    # embeddings\n    azure_embedding_api_key: Optional[str] = None\n    azure_embedding_version: Optional[str] = None\n    azure_embedding_endpoint: Optional[str] = None\n    azure_embedding_deployment: Optional[str] = None\n\n    # custom llm API config\n    openllm_auth_type: Optional[str] = None\n    openllm_key: Optional[str] = None\n\n    # defulat llm api config\n    # \n\n    @classmethod\n    def load(cls) -> \"typeagentCredentials\":\n        config = configparser.ConfigParser()\n\n        # allow overriding with env variables\n        if os.getenv(\"typeagent_CREDENTIALS_PATH\"):\n            credentials_path = os.getenv(\"typeagent_CREDENTIALS_PATH\")\n        else:\n            credentials_path = typeagentCredentials.credentials_path\n\n        if os.path.exists(credentials_path):\n            # read existing credentials\n            config.read(credentials_path)\n            config_dict = {\n                # openai\n                \"openai_auth_type\": get_field(config, \"openai\", \"auth_type\"),\n                \"openai_key\": get_field(config, \"openai\", \"key\"),\n                # azure\n                \"azure_auth_type\": get_field(config, \"azure\", \"auth_type\"),\n                \"azure_key\": get_field(config, \"azure\", \"key\"),\n                \"azure_version\": get_field(config, \"azure\", \"version\"),\n                \"azure_endpoint\": get_field(config, \"azure\", \"endpoint\"),\n                \"azure_deployment\": get_field(config, \"azure\", \"deployment\"),\n                \"azure_embedding_version\": get_field(config, \"azure\", \"embedding_version\"),\n                \"azure_embedding_endpoint\": get_field(config, \"azure\", \"embedding_endpoint\"),\n                \"azure_embedding_deployment\": get_field(config, \"azure\", \"embedding_deployment\"),\n                \"azure_embedding_api_key\": get_field(config, \"azure\", \"embedding_api_key\"),\n                # gemini\n                \"google_ai_key\": get_field(config, \"google_ai\", \"key\"),\n                \"google_ai_service_endpoint\": get_field(config, \"google_ai\", \"service_endpoint\"),\n                # anthropic\n                \"anthropic_key\": get_field(config, \"anthropic\", \"key\"),\n                # cohere\n                \"cohere_key\": get_field(config, \"cohere\", \"key\"),\n                # open llm\n                \"openllm_auth_type\": get_field(config, \"openllm\", \"auth_type\"),\n                \"openllm_key\": get_field(config, \"openllm\", \"key\"),\n                # path\n                \"credentials_path\": credentials_path,\n            }\n            config_dict = {k: v for k, v in config_dict.items() if v is not None}\n            return cls(**config_dict)\n\n        # create new config\n        config = cls(credentials_path=credentials_path)\n        config.save()  # save updated config\n        return config\n\n    def save(self):\n        pass\n\n        config = configparser.ConfigParser()\n        # openai config\n        set_field(config, \"openai\", \"auth_type\", self.openai_auth_type)\n        set_field(config, \"openai\", \"key\", self.openai_key)\n\n        # azure config\n        set_field(config, \"azure\", \"auth_type\", self.azure_auth_type)\n        set_field(config, \"azure\", \"key\", self.azure_key)\n        set_field(config, \"azure\", \"version\", self.azure_version)\n        set_field(config, \"azure\", \"endpoint\", self.azure_endpoint)\n        set_field(config, \"azure\", \"deployment\", self.azure_deployment)\n        set_field(config, \"azure\", \"embedding_version\", self.azure_embedding_version)\n        set_field(config, \"azure\", \"embedding_endpoint\", self.azure_embedding_endpoint)\n        set_field(config, \"azure\", \"embedding_deployment\", self.azure_embedding_deployment)\n        set_field(config, \"azure\", \"embedding_api_key\", self.azure_embedding_api_key)\n\n        # gemini\n        set_field(config, \"google_ai\", \"key\", self.google_ai_key)\n        set_field(config, \"google_ai\", \"service_endpoint\", self.google_ai_service_endpoint)\n\n        # anthropic\n        set_field(config, \"anthropic\", \"key\", self.anthropic_key)\n\n        # cohere\n        set_field(config, \"cohere\", \"key\", self.cohere_key)\n\n        # openllm config\n        set_field(config, \"openllm\", \"auth_type\", self.openllm_auth_type)\n        set_field(config, \"openllm\", \"key\", self.openllm_key)\n\n        if not os.path.exists(typeagent_DIR):\n            os.makedirs(typeagent_DIR, exist_ok=True)\n        with open(self.credentials_path, \"w\", encoding=\"utf-8\") as f:\n            config.write(f)\n\n    @staticmethod\n    def exists():\n        # allow overriding with env variables\n        if os.getenv(\"typeagent_CREDENTIALS_PATH\"):\n            credentials_path = os.getenv(\"typeagent_CREDENTIALS_PATH\")\n        else:\n            credentials_path = typeagentCredentials.credentials_path\n\n        assert not os.path.isdir(credentials_path), f\"Credentials path {credentials_path} cannot be set to a directory.\"\n        return os.path.exists(credentials_path)\n"}
{"type": "source_file", "path": "luann/agent_store/vectorsdb/milvus.py", "content": "import uuid\nfrom copy import deepcopy\nfrom typing import Dict, Iterator, List, Optional, cast\n\nfrom pymilvus import DataType, MilvusClient\nfrom pymilvus.client.constants import ConsistencyLevel\n\nfrom agent_store.storage import StorageConnector, StorageType\nfrom config import MemGPTConfig\nfrom constants import MAX_EMBEDDING_DIM\nfrom data_types import Passage, Record, RecordType\nfrom utils import datetime_to_timestamp, printd, timestamp_to_datetime\n\n\nclass MilvusStorageConnector(StorageConnector):\n    \"\"\"Storage via Milvus\"\"\"\n\n    def __init__(self, table_type: str, config: MemGPTConfig, user_id, agent_id=None):\n        super().__init__(table_type=table_type, config=config, user_id=user_id, agent_id=agent_id)\n\n        assert table_type in [StorageType.ARCHIVAL_MEMORY, StorageType.KNOWLEDGE_BASE_PASSAGES], \"Milvus only supports archival memory\"\n        if config.archival_storage_uri:\n            self.client = MilvusClient(uri=config.archival_storage_uri)\n            self._create_collection()\n        else:\n            raise ValueError(\"Please set `archival_storage_uri` in the config file when using Milvus.\")\n\n        # need to be converted to strings\n        self.uuid_fields = [\"id\", \"user_id\", \"agent_id\", \"source_id\", \"doc_id\"]\n\n    def _create_collection(self):\n        schema = MilvusClient.create_schema(\n            auto_id=False,\n            enable_dynamic_field=True,\n        )\n        schema.add_field(field_name=\"id\", datatype=DataType.VARCHAR, is_primary=True, max_length=65_535)\n        schema.add_field(field_name=\"text\", datatype=DataType.VARCHAR, is_primary=False, max_length=65_535)\n        schema.add_field(field_name=\"embedding\", datatype=DataType.FLOAT_VECTOR, dim=MAX_EMBEDDING_DIM)\n        index_params = self.client.prepare_index_params()\n        index_params.add_index(field_name=\"id\")\n        index_params.add_index(field_name=\"embedding\", index_type=\"AUTOINDEX\", metric_type=\"IP\")\n        self.client.create_collection(\n            collection_name=self.table_name, schema=schema, index_params=index_params, consistency_level=ConsistencyLevel.Strong\n        )\n\n    def get_milvus_filter(self, filters: Optional[Dict] = {}) -> str:\n        filter_conditions = {**self.filters, **filters} if filters is not None else self.filters\n        if not filter_conditions:\n            return \"\"\n        conditions = []\n        for key, value in filter_conditions.items():\n            if key in self.uuid_fields or isinstance(key, str):\n                condition = f'({key} == \"{value}\")'\n            else:\n                condition = f\"({key} == {value})\"\n            conditions.append(condition)\n        filter_expr = \" and \".join(conditions)\n        if len(conditions) == 1:\n            filter_expr = filter_expr[1:-1]\n        return filter_expr\n\n    def get_all_paginated(self, filters: Optional[Dict] = {}, page_size: int = 1000) -> Iterator[List[RecordType]]:\n        if not self.client.has_collection(collection_name=self.table_name):\n            yield []\n        filter_expr = self.get_milvus_filter(filters)\n        offset = 0\n        while True:\n            # Retrieve a chunk of records with the given page_size\n            query_res = self.client.query(\n                collection_name=self.table_name,\n                filter=filter_expr,\n                offset=offset,\n                limit=page_size,\n            )\n            if not query_res:\n                break\n            # Yield a list of Record objects converted from the chunk\n            yield self._list_to_records(query_res)\n\n            # Increment the offset to get the next chunk in the next iteration\n            offset += page_size\n\n    def get_all(self, filters: Optional[Dict] = {}, limit=None) -> List[RecordType]:\n        if not self.client.has_collection(collection_name=self.table_name):\n            return []\n        filter_expr = self.get_milvus_filter(filters)\n        query_res = self.client.query(\n            collection_name=self.table_name,\n            filter=filter_expr,\n            limit=limit,\n        )\n        return self._list_to_records(query_res)\n\n    def get(self, id: uuid.UUID) -> Optional[RecordType]:\n        res = self.client.get(collection_name=self.table_name, ids=str(id))\n        return self._list_to_records(res)[0] if res else None\n\n    def size(self, filters: Optional[Dict] = {}) -> int:\n        if not self.client.has_collection(collection_name=self.table_name):\n            return 0\n        filter_expr = self.get_milvus_filter(filters)\n        count_expr = \"count(*)\"\n        query_res = self.client.query(\n            collection_name=self.table_name,\n            filter=filter_expr,\n            output_fields=[count_expr],\n        )\n        doc_num = query_res[0][count_expr]\n        return doc_num\n\n    def insert(self, record: RecordType):\n        self.insert_many([record])\n\n    def insert_many(self, records: List[RecordType], show_progress=False):\n        if not records:\n            return\n\n        # Milvus lite currently does not support upsert, so we delete and insert instead\n        # self.client.upsert(collection_name=self.table_name, data=self._records_to_list(records))\n        ids = [str(record.id) for record in records]\n        self.client.delete(collection_name=self.table_name, ids=ids)\n        data = self._records_to_list(records)\n        self.client.insert(collection_name=self.table_name, data=data)\n\n    def query(self, query: str, query_vec: List[float], top_k: int = 10, filters: Optional[Dict] = {}) -> List[RecordType]:\n        if not self.client.has_collection(self.table_name):\n            return []\n        search_res = self.client.search(\n            collection_name=self.table_name, data=[query_vec], filter=self.get_milvus_filter(filters), limit=top_k, output_fields=[\"*\"]\n        )[0]\n        entity_res = [res[\"entity\"] for res in search_res]\n        return self._list_to_records(entity_res)\n\n    def delete_table(self):\n        self.client.drop_collection(collection_name=self.table_name)\n\n    def delete(self, filters: Optional[Dict] = {}):\n        if not self.client.has_collection(collection_name=self.table_name):\n            return\n        filter_expr = self.get_milvus_filter(filters)\n        self.client.delete(collection_name=self.table_name, filter=filter_expr)\n\n    def save(self):\n        # save to persistence file (nothing needs to be done)\n        printd(\"Saving milvus\")\n\n    def _records_to_list(self, records: List[Record]) -> List[Dict]:\n        if records == []:\n            return []\n        assert all(isinstance(r, Passage) for r in records)\n        record_list = []\n        records = list(set(records))\n        for record in records:\n            record_vars = deepcopy(vars(record))\n            _id = record_vars.pop(\"id\")\n            text = record_vars.pop(\"text\", \"\")\n            embedding = record_vars.pop(\"embedding\")\n            record_metadata = record_vars.pop(\"metadata_\", None) or {}\n            if \"created_at\" in record_vars:\n                record_vars[\"created_at\"] = datetime_to_timestamp(record_vars[\"created_at\"])\n            record_dict = {key: value for key, value in record_vars.items() if value is not None}\n            record_dict = {\n                **record_dict,\n                **record_metadata,\n                \"id\": str(_id),\n                \"text\": text,\n                \"embedding\": embedding,\n            }\n            for key, value in record_dict.items():\n                if key in self.uuid_fields:\n                    record_dict[key] = str(value)\n            record_list.append(record_dict)\n        return record_list\n\n    def _list_to_records(self, query_res: List[Dict]) -> List[RecordType]:\n        records = []\n        for res_dict in query_res:\n            _id = res_dict.pop(\"id\")\n            embedding = res_dict.pop(\"embedding\")\n            text = res_dict.pop(\"text\")\n            metadata = deepcopy(res_dict)\n            for key, value in metadata.items():\n                if key in self.uuid_fields:\n                    metadata[key] = uuid.UUID(value)\n                elif key == \"created_at\":\n                    metadata[key] = timestamp_to_datetime(value)\n            records.append(\n                cast(\n                    RecordType,\n                    self.type(\n                        text=text,\n                        embedding=embedding,\n                        id=uuid.UUID(_id),\n                        **metadata,\n                    ),\n                )\n            )\n        return records\n"}
{"type": "source_file", "path": "luann/agent_store/sqldb/sqldbconnector.py", "content": "\nimport base64\nimport os\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, Iterator, List, Optional\n\nimport numpy as np\nfrom sqlalchemy import (\n    BIGINT,\n    BINARY,\n    CHAR,\n    JSON,\n    Column,\n    DateTime,\n    String,\n    TypeDecorator,\n    and_,\n    asc,\n    create_engine,\n    desc,\n    func,\n    or_,\n    select,\n    text,\n)\nfrom sqlalchemy.dialects.postgresql import UUID\nfrom sqlalchemy.orm import declarative_base, mapped_column, sessionmaker\nfrom sqlalchemy.orm.session import close_all_sessions\nfrom sqlalchemy.sql import func\nfrom sqlalchemy_json import MutableJson\nfrom tqdm import tqdm\n\nfrom agent_store.storage import StorageConnector, StorageType,get_db_model\nfrom config import typeagentConfig\nfrom constants import MAX_EMBEDDING_DIM\nfrom data_types import Message, Passage, Record, RecordType, ToolCall\nfrom settings import settings\nBase = declarative_base()\nclass SQLStorageConnector(StorageConnector):\n    def __init__(self, storage_type: str, config: typeagentConfig, user_id, agent_id=None):\n        super().__init__(storage_type=storage_type, config=config, user_id=user_id, agent_id=agent_id)\n        self.config = config\n\n    def get_filters(self, filters: Optional[Dict] = {}):\n        if filters is not None:\n            filter_conditions = {**self.filters, **filters}\n        else:\n            filter_conditions = self.filters\n        all_filters = [getattr(self.db_model, key) == value for key, value in filter_conditions.items()]\n        return all_filters\n\n    def get_all_paginated(self, filters: Optional[Dict] = {}, page_size: Optional[int] = 1000, offset=0) -> Iterator[List[RecordType]]:\n        filters = self.get_filters(filters)\n        while True:\n            # Retrieve a chunk of records with the given page_size\n            with self.session_maker() as session:\n                db_record_chunk = session.query(self.db_model).filter(*filters).offset(offset).limit(page_size).all()\n\n            # If the chunk is empty, we've retrieved all records\n            if not db_record_chunk:\n                break\n\n            # Yield a list of Record objects converted from the chunk\n            yield [record.to_record() for record in db_record_chunk]\n\n            # Increment the offset to get the next chunk in the next iteration\n            offset += page_size\n\n    def get_all_cursor(\n        self,\n        filters: Optional[Dict] = {},\n        after: uuid.UUID = None,\n        before: uuid.UUID = None,\n        limit: Optional[int] = 1000,\n        order_by: str = \"created_at\",\n        reverse: bool = False,\n    ):\n        \"\"\"Get all that returns a cursor (record.id) and records\"\"\"\n        filters = self.get_filters(filters)\n\n        # generate query\n        with self.session_maker() as session:\n            query = session.query(self.db_model).filter(*filters)\n            # query = query.order_by(asc(self.db_model.id))\n\n            # records are sorted by the order_by field first, and then by the ID if two fields are the same\n            if reverse:\n                query = query.order_by(desc(getattr(self.db_model, order_by)), asc(self.db_model.id))\n            else:\n                query = query.order_by(asc(getattr(self.db_model, order_by)), asc(self.db_model.id))\n\n            # cursor logic: filter records based on before/after ID\n            if after:\n                after_value = getattr(self.get(id=after), order_by)\n                if reverse:  # if reverse, then we want to get records that are less than the after_value\n                    sort_exp = getattr(self.db_model, order_by) < after_value\n                else:  # otherwise, we want to get records that are greater than the after_value\n                    sort_exp = getattr(self.db_model, order_by) > after_value\n                query = query.filter(\n                    or_(sort_exp, and_(getattr(self.db_model, order_by) == after_value, self.db_model.id > after))  # tiebreaker case\n                )\n            if before:\n                before_value = getattr(self.get(id=before), order_by)\n                if reverse:\n                    sort_exp = getattr(self.db_model, order_by) > before_value\n                else:\n                    sort_exp = getattr(self.db_model, order_by) < before_value\n                query = query.filter(or_(sort_exp, and_(getattr(self.db_model, order_by) == before_value, self.db_model.id < before)))\n\n            # get records\n            db_record_chunk = query.limit(limit).all()\n        if not db_record_chunk:\n            return (None, [])\n        records = [record.to_record() for record in db_record_chunk]\n        next_cursor = db_record_chunk[-1].id\n        assert isinstance(next_cursor, uuid.UUID)\n\n        # return (cursor, list[records])\n        return (next_cursor, records)\n\n    def get_all(self, filters: Optional[Dict] = {}, limit=None) -> List[RecordType]:\n        filters = self.get_filters(filters)\n        with self.session_maker() as session:\n            if limit:\n                db_records = session.query(self.db_model).filter(*filters).limit(limit).all()\n            else:\n                db_records = session.query(self.db_model).filter(*filters).all()\n        return [record.to_record() for record in db_records]\n\n    def get(self, id: uuid.UUID) -> Optional[Record]:\n        with self.session_maker() as session:\n            db_record = session.get(self.db_model, id)\n        if db_record is None:\n            return None\n        return db_record.to_record()\n\n    def size(self, filters: Optional[Dict] = {}) -> int:\n        # return size of table\n        filters = self.get_filters(filters)\n        with self.session_maker() as session:\n            return session.query(self.db_model).filter(*filters).count()\n\n    def insert(self, record: Record):\n        raise NotImplementedError\n\n    def insert_many(self, records: List[RecordType], show_progress=False):\n        raise NotImplementedError\n\n    def query(self, query: str, query_vec: List[float], top_k: int = 10, filters: Optional[Dict] = {}) -> List[RecordType]:\n        raise NotImplementedError(\"Vector query not implemented for SQLStorageConnector\")\n\n    def save(self):\n        return\n\n    def list_data_sources(self):\n        # assert self.table_type == TableType.ARCHIVAL_MEMORY, f\"list_data_sources only implemented for ARCHIVAL_MEMORY\"\n        with self.session_maker() as session:\n            unique_data_sources = session.query(self.db_model.data_source).filter(*self.filters).distinct().all()\n        return unique_data_sources\n\n    def query_date(self, start_date, end_date, limit=None, offset=0):\n        filters = self.get_filters({})\n        with self.session_maker() as session:\n            query = (\n                session.query(self.db_model)\n                .filter(*filters)\n                .filter(self.db_model.created_at >= start_date)\n                .filter(self.db_model.created_at <= end_date)\n                .filter(self.db_model.role != \"system\")\n                .filter(self.db_model.role != \"tool\")\n                .offset(offset)\n            )\n            if limit:\n                query = query.limit(limit)\n            results = query.all()\n        return [result.to_record() for result in results]\n\n    def query_text(self, query, limit=None, offset=0):\n        # todo: make fuzz https://stackoverflow.com/questions/42388956/create-a-full-text-search-index-with-sqlalchemy-on-postgresql/42390204#42390204\n        filters = self.get_filters({})\n        with self.session_maker() as session:\n            query = (\n                session.query(self.db_model)\n                .filter(*filters)\n                .filter(func.lower(self.db_model.text).contains(func.lower(query)))\n                .filter(self.db_model.role != \"system\")\n                .filter(self.db_model.role != \"tool\")\n                .offset(offset)\n            )\n            if limit:\n                query = query.limit(limit)\n            results = query.all()\n        # return [self.type(**vars(result)) for result in results]\n        return [result.to_record() for result in results]\n\n    # Should be used only in tests!\n    def delete_table(self):\n        close_all_sessions()\n        with self.session_maker() as session:\n            self.db_model.__table__.drop(session.bind)\n            session.commit()\n\n    def delete(self, filters: Optional[Dict] = {}):\n        filters = self.get_filters(filters)\n        with self.session_maker() as session:\n            session.query(self.db_model).filter(*filters).delete()\n            session.commit()\n\n\nclass PostgresStorageConnector(SQLStorageConnector):\n    \"\"\"Storage via Postgres\"\"\"\n\n    # TODO: this should probably eventually be moved into a parent DB class\n\n    def __init__(self, storage_type: str, config: typeagentConfig, user_id, agent_id=None):\n        # from pgvector.sqlalchemy import Vector\n\n        super().__init__(storage_type=storage_type, config=config, user_id=user_id, agent_id=agent_id)\n\n        # create table\n        self.db_model = get_db_model(config, self.table_name, storage_type, user_id, agent_id)\n\n        # construct URI from enviornment variables\n        if settings.pg_uri:\n            self.uri = settings.pg_uri\n        else:\n            pass\n            # # use config URI\n            # # TODO: remove this eventually (config should NOT contain URI)\n            # if table_type == TableType.ARCHIVAL_MEMORY or table_type == TableType.PASSAGES:\n            #     self.uri = self.config.archival_storage_uri\n            #     if self.config.archival_storage_uri is None:\n            #         raise ValueError(f\"Must specifiy archival_storage_uri in config {self.config.config_path}\")\n            # elif table_type == TableType.RECALL_MEMORY:\n            #     self.uri = self.config.recall_storage_uri\n            #     if self.config.recall_storage_uri is None:\n            #         raise ValueError(f\"Must specifiy recall_storage_uri in config {self.config.config_path}\")\n            # else:\n            #     raise ValueError(f\"Table type {table_type} not implemented\")\n\n        # create engine\n        self.engine = create_engine(self.uri)\n\n        # for c in self.db_model.__table__.columns:\n        #     if c.name == \"embedding\":\n        #         assert isinstance(c.type, Vector), f\"Embedding column must be of type Vector, got {c.type}\"\n\n        self.session_maker = sessionmaker(bind=self.engine)\n        with self.session_maker() as session:\n            session.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector\"))  # Enables the vector extension\n\n        # create table\n        Base.metadata.create_all(self.engine, tables=[self.db_model.__table__])  # Create the table if it doesn't exist\n\n    def query(self, query: str, query_vec: List[float], top_k: int = 10, filters: Optional[Dict] = {}) -> List[RecordType]:\n        filters = self.get_filters(filters)\n        with self.session_maker() as session:\n            results = session.scalars(\n                select(self.db_model).filter(*filters).order_by(self.db_model.embedding.l2_distance(query_vec)).limit(top_k)\n            ).all()\n\n        # Convert the results into Passage objects\n        records = [result.to_record() for result in results]\n        return records\n\n    def insert_many(self, records: List[RecordType], exists_ok=True, show_progress=False):\n        from sqlalchemy.dialects.postgresql import insert\n\n        # TODO: this is terrible, should eventually be done the same way for all types (migrate to SQLModel)\n        if len(records) == 0:\n            return\n        if isinstance(records[0], Passage):\n            with self.engine.connect() as conn:\n                db_records = [vars(record) for record in records]\n                # print(\"records\", db_records)\n                stmt = insert(self.db_model.__table__).values(db_records)\n                # print(stmt)\n                if exists_ok:\n                    upsert_stmt = stmt.on_conflict_do_update(\n                        index_elements=[\"id\"], set_={c.name: c for c in stmt.excluded}  # Replace with your primary key column\n                    )\n                    conn.execute(upsert_stmt)\n                else:\n                    conn.execute(stmt)\n                conn.commit()\n        else:\n            with self.session_maker() as session:\n                iterable = tqdm(records) if show_progress else records\n                for record in iterable:\n                    db_record = self.db_model(**vars(record))\n                    session.add(db_record)\n                session.commit()\n\n    def insert(self, record: Record, exists_ok=True):\n        self.insert_many([record], exists_ok=exists_ok)\n\n    def update(self, record: RecordType):\n        \"\"\"\n        Updates a record in the database based on the provided Record object.\n        \"\"\"\n        with self.session_maker() as session:\n            # Find the record by its ID\n            db_record = session.query(self.db_model).filter_by(id=record.id).first()\n            if not db_record:\n                raise ValueError(f\"Record with id {record.id} does not exist.\")\n\n            # Update the record with new values from the provided Record object\n            for attr, value in vars(record).items():\n                setattr(db_record, attr, value)\n\n            # Commit the changes to the database\n            session.commit()\n\n    def str_to_datetime(self, str_date: str) -> datetime:\n        val = str_date.split(\"-\")\n        _datetime = datetime(int(val[0]), int(val[1]), int(val[2]))\n        return _datetime\n\n    def query_date(self, start_date, end_date, limit=None, offset=0):\n        filters = self.get_filters({})\n        _start_date = self.str_to_datetime(start_date) if isinstance(start_date, str) else start_date\n        _end_date = self.str_to_datetime(end_date) if isinstance(end_date, str) else end_date\n        with self.session_maker() as session:\n            query = (\n                session.query(self.db_model)\n                .filter(*filters)\n                .filter(self.db_model.created_at >= _start_date)\n                .filter(self.db_model.created_at <= _end_date)\n                .filter(self.db_model.role != \"system\")\n                .filter(self.db_model.role != \"tool\")\n                .offset(offset)\n            )\n            if limit:\n                query = query.limit(limit)\n            results = query.all()\n        return [result.to_record() for result in results]\nclass SQLLiteStorageConnector(SQLStorageConnector):\n    def __init__(self, storage_type: str, config: typeagentConfig, user_id, agent_id=None):\n        super().__init__(storage_type=storage_type, config=config, user_id=user_id, agent_id=agent_id)\n\n\n\n\n        if storage_type == StorageType.ARCHIVAL_MEMORY:\n            # self.type = Passage\n            self.path = config.archival_memory_storage_path\n        elif storage_type == StorageType.RECALL_MEMORY:\n            # self.type = Message\n            self.path = config.recall_memory_storage_path\n        elif storage_type == StorageType.KNOWLEDGE_BASE:\n            # self.type = Document\n            self.path = config.knowledge_base_storage_path\n        elif storage_type == StorageType.KNOWLEDGE_BASE_DOCUMENTS:\n            self.path = config.recall_memory_storage_path\n        elif storage_type == StorageType.KNOWLEDGE_BASE_PASSAGES:\n            # self.type = Passage\n           self.path = config.knowledge_base_storage_path\n        else:\n            raise ValueError(f\"storage type {storage_type} not implemented\")\n        # get storage URI\n        # if table_type == TableType.ARCHIVAL_MEMORY or table_type == TableType.PASSAGES:\n        #     raise ValueError(f\"Table type {table_type} not implemented\")\n        # elif table_type == TableType.RECALL_MEMORY:\n        #     # TODO: eventually implement URI option\n        #     self.path = self.config.recall_storage_path\n        #     if self.path is None:\n        #         raise ValueError(f\"Must specifiy recall_storage_path in config {self.config.recall_storage_path}\")\n        # else:\n        #     raise ValueError(f\"Table type {table_type} not implemented\")\n\n        self.path = os.path.join(self.path, f\"sqlite.db\")\n\n        # Create the SQLAlchemy engine\n        self.db_model = get_db_model(config, self.table_name, storage_type, user_id, agent_id, dialect=\"sqlite\")\n        self.engine = create_engine(f\"sqlite:///{self.path}\")\n        Base.metadata.create_all(self.engine, tables=[self.db_model.__table__])  # Create the table if it doesn't exist\n        self.session_maker = sessionmaker(bind=self.engine)\n\n        import sqlite3\n\n        sqlite3.register_adapter(uuid.UUID, lambda u: u.bytes_le)\n        sqlite3.register_converter(\"UUID\", lambda b: uuid.UUID(bytes_le=b))\n\n    def insert_many(self, records: List[RecordType], exists_ok=True, show_progress=False):\n        from sqlalchemy.dialects.sqlite import insert\n\n        # TODO: this is terrible, should eventually be done the same way for all types (migrate to SQLModel)\n        if len(records) == 0:\n            return\n        if isinstance(records[0], Passage):\n            with self.engine.connect() as conn:\n                db_records = [vars(record) for record in records]\n                # print(\"records\", db_records)\n                stmt = insert(self.db_model.__table__).values(db_records)\n                # print(stmt)\n                if exists_ok:\n                    upsert_stmt = stmt.on_conflict_do_update(\n                        index_elements=[\"id\"], set_={c.name: c for c in stmt.excluded}  # Replace with your primary key column\n                    )\n                    conn.execute(upsert_stmt)\n                else:\n                    conn.execute(stmt)\n                conn.commit()\n        else:\n            with self.session_maker() as session:\n                iterable = tqdm(records) if show_progress else records\n                for record in iterable:\n                    db_record = self.db_model(**vars(record))\n                    session.add(db_record)\n                session.commit()\n\n    def insert(self, record: Record, exists_ok=True):\n        self.insert_many([record], exists_ok=exists_ok)\n\n    def update(self, record: Record):\n        \"\"\"\n        Updates an existing record in the database with values from the provided record object.\n        \"\"\"\n        if not record.id:\n            raise ValueError(\"Record must have an id.\")\n\n        with self.session_maker() as session:\n            # Fetch the existing record from the database\n            db_record = session.query(self.db_model).filter_by(id=record.id).first()\n            if not db_record:\n                raise ValueError(f\"Record with id {record.id} does not exist.\")\n\n            # Update the database record with values from the provided record object\n            for column in self.db_model.__table__.columns:\n                column_name = column.name\n                if hasattr(record, column_name):\n                    new_value = getattr(record, column_name)\n                    setattr(db_record, column_name, new_value)\n\n            # Commit the changes to the database\n            session.commit()"}
{"type": "source_file", "path": "luann/data_sources/text_splitters/basetextsplitter.py", "content": "from __future__ import annotations\n\nimport copy\nimport logging\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import (\n    AbstractSet,\n    Any,\n    Callable,\n    Collection,\n    Iterable,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Type,\n    TypeVar,\n    Union,\n)\nfrom data_types import Document\n# from langchain_core.documents import BaseDocumentTransformer, Document\n\nlogger = logging.getLogger(__name__)\n\nTS = TypeVar(\"TS\", bound=\"TextSplitter\")\n\n\nclass TextSplitter(ABC):\n    \"\"\"Interface for splitting text into chunks.\"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int = 4000,\n        chunk_overlap: int = 200,\n        length_function: Callable[[str], int] = len,\n        keep_separator: Union[bool, Literal[\"start\", \"end\"]] = False,\n        add_start_index: bool = False,\n        strip_whitespace: bool = True,\n    ) -> None:\n        \"\"\"Create a new TextSplitter.\n\n        Args:\n            chunk_size: Maximum size of chunks to return\n            chunk_overlap: Overlap in characters between chunks\n            length_function: Function that measures the length of given chunks\n            keep_separator: Whether to keep the separator and where to place it\n                            in each corresponding chunk (True='start')\n            add_start_index: If `True`, includes chunk's start index in metadata\n            strip_whitespace: If `True`, strips whitespace from the start and end of\n                              every document\n        \"\"\"\n        if chunk_overlap > chunk_size:\n            raise ValueError(\n                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n                f\"({chunk_size}), should be smaller.\"\n            )\n        self._chunk_size = chunk_size\n        self._chunk_overlap = chunk_overlap\n        self._length_function = length_function\n        self._keep_separator = keep_separator\n        self._add_start_index = add_start_index\n        self._strip_whitespace = strip_whitespace\n\n    @abstractmethod\n    def split_text(self, text: str) -> List[str]:\n        \"\"\"Split text into multiple components.\"\"\"\n\n    def create_documents(\n        self, texts: List[str], metadatas: Optional[List[dict]] = None\n    ) -> List[Document]:\n        \"\"\"Create documents from a list of texts.\"\"\"\n        _metadatas = metadatas or [{}] * len(texts)\n        documents = []\n        for i, text in enumerate(texts):\n            index = 0\n            previous_chunk_len = 0\n            for chunk in self.split_text(text):\n                metadata = copy.deepcopy(_metadatas[i])\n                if self._add_start_index:\n                    offset = index + previous_chunk_len - self._chunk_overlap\n                    index = text.find(chunk, max(0, offset))\n                    metadata[\"start_index\"] = index\n                    previous_chunk_len = len(chunk)\n                new_doc = Document(text=chunk, metadata=metadata)\n                documents.append(new_doc)\n        return documents\n\n    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\n        \"\"\"Split documents.\"\"\"\n        texts, metadatas = [], []\n        for doc in documents:\n            texts.append(doc.page_content)\n            metadatas.append(doc.metadata)\n        return self.create_documents(texts, metadatas=metadatas)\n\n    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\n        text = separator.join(docs)\n        if self._strip_whitespace:\n            text = text.strip()\n        if text == \"\":\n            return None\n        else:\n            return text\n\n    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\n        # We now want to combine these smaller pieces into medium size\n        # chunks to send to the LLM.\n        separator_len = self._length_function(separator)\n\n        docs = []\n        current_doc: List[str] = []\n        total = 0\n        for d in splits:\n            _len = self._length_function(d)\n            if (\n                total + _len + (separator_len if len(current_doc) > 0 else 0)\n                > self._chunk_size\n            ):\n                if total > self._chunk_size:\n                    logger.warning(\n                        f\"Created a chunk of size {total}, \"\n                        f\"which is longer than the specified {self._chunk_size}\"\n                    )\n                if len(current_doc) > 0:\n                    doc = self._join_docs(current_doc, separator)\n                    if doc is not None:\n                        docs.append(doc)\n                    # Keep on popping if:\n                    # - we have a larger chunk than in the chunk overlap\n                    # - or if we still have any chunks and the length is long\n                    while total > self._chunk_overlap or (\n                        total + _len + (separator_len if len(current_doc) > 0 else 0)\n                        > self._chunk_size\n                        and total > 0\n                    ):\n                        total -= self._length_function(current_doc[0]) + (\n                            separator_len if len(current_doc) > 1 else 0\n                        )\n                        current_doc = current_doc[1:]\n            current_doc.append(d)\n            total += _len + (separator_len if len(current_doc) > 1 else 0)\n        doc = self._join_docs(current_doc, separator)\n        if doc is not None:\n            docs.append(doc)\n        return docs\n\n    @classmethod\n    def from_huggingface_tokenizer(cls, tokenizer: Any, **kwargs: Any) -> TextSplitter:\n        \"\"\"Text splitter that uses HuggingFace tokenizer to count length.\"\"\"\n        try:\n            from transformers import PreTrainedTokenizerBase\n\n            if not isinstance(tokenizer, PreTrainedTokenizerBase):\n                raise ValueError(\n                    \"Tokenizer received was not an instance of PreTrainedTokenizerBase\"\n                )\n\n            def _huggingface_tokenizer_length(text: str) -> int:\n                return len(tokenizer.encode(text))\n\n        except ImportError:\n            raise ValueError(\n                \"Could not import transformers python package. \"\n                \"Please install it with `pip install transformers`.\"\n            )\n        return cls(length_function=_huggingface_tokenizer_length, **kwargs)\n\n    @classmethod\n    def from_tiktoken_encoder(\n        cls: Type[TS],\n        encoding_name: str = \"gpt2\",\n        model_name: Optional[str] = None,\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\n        **kwargs: Any,\n    ) -> TS:\n        \"\"\"Text splitter that uses tiktoken encoder to count length.\"\"\"\n        try:\n            import tiktoken\n        except ImportError:\n            raise ImportError(\n                \"Could not import tiktoken python package. \"\n                \"This is needed in order to calculate max_tokens_for_prompt. \"\n                \"Please install it with `pip install tiktoken`.\"\n            )\n\n        if model_name is not None:\n            enc = tiktoken.encoding_for_model(model_name)\n        else:\n            enc = tiktoken.get_encoding(encoding_name)\n\n        def _tiktoken_encoder(text: str) -> int:\n            return len(\n                enc.encode(\n                    text,\n                    allowed_special=allowed_special,\n                    disallowed_special=disallowed_special,\n                )\n            )\n\n        if issubclass(cls, TokenTextSplitter):\n            extra_kwargs = {\n                \"encoding_name\": encoding_name,\n                \"model_name\": model_name,\n                \"allowed_special\": allowed_special,\n                \"disallowed_special\": disallowed_special,\n            }\n            kwargs = {**kwargs, **extra_kwargs}\n\n        return cls(length_function=_tiktoken_encoder, **kwargs)\n\n    def transform_documents(\n        self, documents: Sequence[Document], **kwargs: Any\n    ) -> Sequence[Document]:\n        \"\"\"Transform sequence of documents by splitting them.\"\"\"\n        return self.split_documents(list(documents))\n\n\nclass TokenTextSplitter(TextSplitter):\n    \"\"\"Splitting text to tokens using model tokenizer.\"\"\"\n\n    def __init__(\n        self,\n        encoding_name: str = \"gpt2\",\n        model_name: Optional[str] = None,\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Create a new TextSplitter.\"\"\"\n        super().__init__(**kwargs)\n        try:\n            import tiktoken\n        except ImportError:\n            raise ImportError(\n                \"Could not import tiktoken python package. \"\n                \"This is needed in order to for TokenTextSplitter. \"\n                \"Please install it with `pip install tiktoken`.\"\n            )\n\n        if model_name is not None:\n            enc = tiktoken.encoding_for_model(model_name)\n        else:\n            enc = tiktoken.get_encoding(encoding_name)\n        self._tokenizer = enc\n        self._allowed_special = allowed_special\n        self._disallowed_special = disallowed_special\n\n    def split_text(self, text: str) -> List[str]:\n        def _encode(_text: str) -> List[int]:\n            return self._tokenizer.encode(\n                _text,\n                allowed_special=self._allowed_special,\n                disallowed_special=self._disallowed_special,\n            )\n\n        tokenizer = Tokenizer(\n            chunk_overlap=self._chunk_overlap,\n            tokens_per_chunk=self._chunk_size,\n            decode=self._tokenizer.decode,\n            encode=_encode,\n        )\n\n        return split_text_on_tokens(text=text, tokenizer=tokenizer)\n\n\nclass Language(str, Enum):\n    \"\"\"Enum of the programming languages.\"\"\"\n\n    CPP = \"cpp\"\n    GO = \"go\"\n    JAVA = \"java\"\n    KOTLIN = \"kotlin\"\n    JS = \"js\"\n    TS = \"ts\"\n    PHP = \"php\"\n    PROTO = \"proto\"\n    PYTHON = \"python\"\n    RST = \"rst\"\n    RUBY = \"ruby\"\n    RUST = \"rust\"\n    SCALA = \"scala\"\n    SWIFT = \"swift\"\n    MARKDOWN = \"markdown\"\n    LATEX = \"latex\"\n    HTML = \"html\"\n    SOL = \"sol\"\n    CSHARP = \"csharp\"\n    COBOL = \"cobol\"\n    C = \"c\"\n    LUA = \"lua\"\n    PERL = \"perl\"\n    HASKELL = \"haskell\"\n\n\n@dataclass(frozen=True)\nclass Tokenizer:\n    \"\"\"Tokenizer data class.\"\"\"\n\n    chunk_overlap: int\n    \"\"\"Overlap in tokens between chunks\"\"\"\n    tokens_per_chunk: int\n    \"\"\"Maximum number of tokens per chunk\"\"\"\n    decode: Callable[[List[int]], str]\n    \"\"\" Function to decode a list of token ids to a string\"\"\"\n    encode: Callable[[str], List[int]]\n    \"\"\" Function to encode a string to a list of token ids\"\"\"\n\n\ndef split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\n    \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\n    splits: List[str] = []\n    input_ids = tokenizer.encode(text)\n    start_idx = 0\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n    chunk_ids = input_ids[start_idx:cur_idx]\n    while start_idx < len(input_ids):\n        splits.append(tokenizer.decode(chunk_ids))\n        if cur_idx == len(input_ids):\n            break\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\n        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n        chunk_ids = input_ids[start_idx:cur_idx]\n    return splits\n"}
{"type": "source_file", "path": "luann/cli/cli_load.py", "content": "\"\"\"\nThis file contains functions for loading data into typeagent's archival storage.\n\nData can be loaded with the following command, once a load function is defined:\n```\ntypeagent load <data-connector-type> --name <dataset-name> [ADDITIONAL ARGS]\n```\n\n\"\"\"\nimport os\nimport uuid\nfrom typing import Annotated, List, Optional\nimport threading\nimport typer\nimport tempfile\nfrom constants import KNOWLEDGE_BASE_DIR\nfrom agent_store.storage import StorageConnector, StorageType\nfrom config import typeagentConfig\nfrom data_sources.connectors import (\n    DirectoryConnector,\n    # VectorDBConnector,\n    load_data,\n)\nfrom data_types import Source\nfrom metadata import MetadataStore\nfrom models.pydantic_models import (\n    DocumentModel,\n    JobModel,\n    JobStatus,\n    PassageModel,\n    SourceModel,\n)\napp = typer.Typer()\n\n# NOTE: not supported due to llama-index breaking things (please reach out if you still need it)\n# @app.command(\"index\")\n# def load_index(\n#    name: Annotated[str, typer.Option(help=\"Name of dataset to load.\")],\n#    dir: Annotated[Optional[str], typer.Option(help=\"Path to directory containing index.\")] = None,\n#    user_id: Annotated[Optional[uuid.UUID], typer.Option(help=\"User ID to associate with dataset.\")] = None,\n# ):\n#    \"\"\"Load a LlamaIndex saved VectorIndex into typeagent\"\"\"\n#    if user_id is None:\n#        config = typeagentConfig.load()\n#        user_id = uuid.UUID(config.anon_clientid)\n#\n#    try:\n#        # load index data\n#        storage_context = StorageContext.from_defaults(persist_dir=dir)\n#        loaded_index = load_index_from_storage(storage_context)\n#\n#        # hacky code to extract out passages/embeddings (thanks a lot, llama index)\n#        embed_dict = loaded_index._vector_store._data.embedding_dict\n#        node_dict = loaded_index._docstore.docs\n#\n#        # create storage connector\n#        config = typeagentConfig.load()\n#        if user_id is None:\n#            user_id = uuid.UUID(config.anon_clientid)\n#\n#        passages = []\n#        for node_id, node in node_dict.items():\n#            vector = embed_dict[node_id]\n#            node.embedding = vector\n#            # assume embedding are the same as config\n#            passages.append(\n#                Passage(\n#                    text=node.text,\n#                    embedding=np.array(vector),\n#                    embedding_dim=config.default_embedding_config.embedding_dim,\n#                    embedding_model=config.default_embedding_config.embedding_model,\n#                )\n#            )\n#            assert config.default_embedding_config.embedding_dim == len(\n#                vector\n#            ), f\"Expected embedding dimension {config.default_embedding_config.embedding_dim}, got {len(vector)}\"\n#\n#        if len(passages) == 0:\n#            raise ValueError(f\"No passages found in index {dir}\")\n#\n#        insert_passages_into_source(passages, name, user_id, config)\n#    except ValueError as e:\n#        typer.secho(f\"Failed to load index from provided information.\\n{e}\", fg=typer.colors.RED)\nfrom data_sources.connectors import DataConnector\nfrom typing import Callable, List, Optional, Tuple, Union\nfrom utils import create_uuid_from_string\nfrom embeddings import embedding_model\nfrom data_types import (\n    AgentState,\n    EmbeddingConfig,\n    LLMConfig,\n    Message,\n    Preset,\n    Source,\n    Token,\n    User,\n    Document,\n    Passage,\n)\ndef load_data(\n        self,\n        user_id: uuid.UUID,\n        connector: DataConnector,\n        source_name: str,\n        config = typeagentConfig,\n        # user_id = uuid.UUID(config.anon_clientid)\n        ms = MetadataStore,\n    ) -> Tuple[int, int]:\n        \"\"\"Load data from a DataConnector into a source for a specified user_id\"\"\"\n        # TODO: this should be implemented as a batch job or at least async, since it may take a long time\n\n        # load data from a data source into the document store\n        source =ms.get_source(source_name=source_name, user_id=user_id)\n        if source is None:\n            raise ValueError(f\"Data source {source_name} does not exist for user {user_id}\")\n\n        # get the data connectors\n        passage_store = StorageConnector.get_storage_connector(StorageType.KNOWLEDGE_BASE_PASSAGES, config, user_id=user_id)\n\n        \"\"\"Load data from a connector (generates documents and passages) into a specified source_id, associatedw with a user_id.\"\"\"\n    \n        passages = []\n        # embedding_to_document_name = {}\n        passage_count = 0\n        document_count = 0\n        \n        for document_text, document_metadata in connector.generate_documents():\n            doctempid=create_uuid_from_string(f\"{str(source.id)}_{document_text}\"),\n        # insert document into storage\n            documentone = DocumentModel(\n            id=doctempid[0],\n            text=document_text,\n            metadata_=document_metadata,\n            source_name=source.name,\n            user_id=source.user_id,\n            source_id=source.id,\n            user_status=\"on\",\n            )\n            document_count += 1\n            # print(str(doctempid[0]))\n            # print(document_text)\n            # print(document_metadata)\n            ms.add_Document(documentone)\n            documentonelocal=Document(\n                text = document_text,\n                metadata = document_metadata,\n                id=doctempid[0],\n\n            )\n        # generate passages\n            for passage_text, passage_metadata in connector.generate_passages([documentonelocal], chunk_size=config.default_embedding_config.embedding_chunk_size):\n\n            # for some reason, llama index parsers sometimes return empty strings\n                if len(passage_text) == 0:\n                    typer.secho(\n                    f\"Warning: embedding text returned empty string, skipping insert of passage with metadata '{passage_metadata}' into VectorDB. You can usually ignore this warning.\",\n                    fg=typer.colors.YELLOW,\n                   )\n                    continue\n\n            # get embedding\n                try:\n                   \n                   print(\"passage_text:\")\n                   print(passage_text)\n                   embed_model = embedding_model(config.default_embedding_config)\n\n                   embedding = embed_model.embed_documents([passage_text])\n                except Exception as e:\n                    typer.secho(\n                    f\"Warning: Failed to get embedding for {passage_text} (error: {str(e)}), skipping insert into VectorDB.\",\n                    fg=typer.colors.YELLOW,\n                    )\n                    continue\n                passagetempid=create_uuid_from_string(f\"{str(source.id)}_{passage_text}\")\n                print(str(passagetempid))\n                passage = PassageModel(\n                   id=passagetempid,\n                   text=passage_text,\n                   doc_id=documentonelocal.id,\n                   metadata_=passage_metadata,\n                   user_id=source.user_id,\n                   source_name=source.name,\n                   source_id=source.id,\n                   embedding_model=source.embedding_model,\n                   embedding=embedding[0],\n                   user_status=\"on\",\n                 )\n                \n                 \n                ms.add_passages(passage)\n           \n                passageloca=Passage(\n                    id=passagetempid,\n                    text=passage_text,\n                    doc_id=documentonelocal.id,\n                    metadata_=passage_metadata,\n                    user_id=source.user_id,\n                    data_source=source.name,\n                    embedding_model=source.embedding_model,\n                    embedding=embedding[0],\n                )\n                passages.append(passageloca)\n         \n                if len(passages) >= 100:\n                # insert passages into passage store\n                    passage_store.insert_many(passages)\n\n                    passage_count += len(passages)\n                    passages = []\n\n            # break\n        if len(passages) > 0:\n        # insert passages into passage store\n            passage_store.insert_many(passages)\n            passage_count += len(passages)\n        \n        return passage_count, document_count\n\n\n\n@app.command(\"directory\")\ndef load_directory(\n    filepathdirectory: Annotated[str, typer.Option(help=\"path of dataset to load.\")],\n    source_name: Annotated[Optional[str], typer.Option(help=\"upload data to a source\")] = None,\n    filename: Annotated[str, typer.Option(help=\"name of file \")] = None,\n):\n    try:\n        config = typeagentConfig.load()\n        user_id = uuid.UUID(config.anon_clientid)\n        ms = MetadataStore(config)\n        source =ms.get_source(source_name=source_name, user_id=user_id)\n        text=\"\"\n        fullfilepath=filepathdirectory+\"\\\\\"+filename\n        with open(fullfilepath, \"r\") as f:\n            text = f.read()\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            file_path = os.path.join(tmpdirname, filename)\n            with open(file_path, \"w\") as buffer:\n                buffer.write(text)\n            file_path = os.path.join(KNOWLEDGE_BASE_DIR, filename)\n            with open(file_path, \"w\") as f:\n                   f.write(text)\n    \n            connector = DirectoryConnector(input_directory=tmpdirname)\n\n            # TODO: pre-compute total number of passages?\n\n            # load the data into the source via the connector\n            num_passages, num_documents =load_data(user_id=user_id, source_name=source.name, connector=connector,config=config,ms=ms)\n        # return job\n        \n            print(f\"Loaded {num_passages} passages and {num_documents} documents from {source.name}\")\n    except Exception as e:\n        typer.secho(f\"Failed to load data from provided information.\\n{e}\", fg=typer.colors.RED)\n        ms.delete_source(source_id=source.id)\n\n   \n\n\n"}
{"type": "source_file", "path": "luann/autogen/interface.py", "content": "import json\nimport re\nfrom typing import Optional\n\nfrom colorama import Fore, Style, init\n\nfrom typeagent.constants import CLI_WARNING_PREFIX, JSON_LOADS_STRICT\nfrom typeagent.data_types import Message\n\ninit(autoreset=True)\n\n\n# DEBUG = True  # puts full message outputs in the terminal\nDEBUG = False  # only dumps important messages in the terminal\n\n\nclass DummyInterface(object):\n    def set_message_list(self, message_list):\n        pass\n\n    def internal_monologue(self, msg):\n        pass\n\n    def assistant_message(self, msg):\n        pass\n\n    def memory_message(self, msg):\n        pass\n\n    def system_message(self, msg):\n        pass\n\n    def user_message(self, msg, raw=False):\n        pass\n\n    def function_message(self, msg):\n        pass\n\n\nclass AutoGenInterface(object):\n    \"\"\"AutoGen expects a single action return in its step loop, but typeagent may take many actions.\n\n    To support AutoGen, we keep a buffer of all the steps that were taken using the interface abstraction,\n    then we concatenate it all and package back as a single 'assistant' ChatCompletion response.\n\n    The buffer needs to be wiped before each call to typeagent.agent.step()\n    \"\"\"\n\n    def __init__(\n        self,\n        message_list=None,\n        fancy=True,  # only applies to the prints, not the appended messages\n        show_user_message=False,\n        show_inner_thoughts=True,\n        show_function_outputs=False,\n        debug=False,\n    ):\n        self.message_list = message_list\n        self.fancy = fancy  # set to false to disable colored outputs + emoji prefixes\n        self.show_user_message = show_user_message\n        self.show_inner_thoughts = show_inner_thoughts\n        self.show_function_outputs = show_function_outputs\n        self.debug = debug\n\n    def reset_message_list(self):\n        \"\"\"Clears the buffer. Call before every agent.step() when using typeagent+AutoGen\"\"\"\n        self.message_list = []\n\n    def internal_monologue(self, msg: str, msg_obj: Optional[Message] = None):\n        # NOTE: never gets appended\n        if self.debug:\n            print(f\"inner thoughts :: {msg}\")\n        if not self.show_inner_thoughts:\n            return\n        # ANSI escape code for italic is '\\x1B[3m'\n        message = f\"\\x1B[3m{Fore.LIGHTBLACK_EX}💭 {msg}{Style.RESET_ALL}\" if self.fancy else f\"[typeagent agent's inner thoughts] {msg}\"\n        print(message)\n\n    def assistant_message(self, msg: str, msg_obj: Optional[Message] = None):\n        # NOTE: gets appended\n        if self.debug:\n            print(f\"assistant :: {msg}\")\n        # message = f\"{Fore.YELLOW}{Style.BRIGHT}🤖 {Fore.YELLOW}{msg}{Style.RESET_ALL}\" if self.fancy else msg\n        self.message_list.append(msg)\n\n    def memory_message(self, msg: str):\n        # NOTE: never gets appended\n        if self.debug:\n            print(f\"memory :: {msg}\")\n        message = (\n            f\"{Fore.LIGHTMAGENTA_EX}{Style.BRIGHT}🧠 {Fore.LIGHTMAGENTA_EX}{msg}{Style.RESET_ALL}\" if self.fancy else f\"[memory] {msg}\"\n        )\n        print(message)\n\n    def system_message(self, msg: str):\n        # NOTE: gets appended\n        if self.debug:\n            print(f\"system :: {msg}\")\n        message = f\"{Fore.MAGENTA}{Style.BRIGHT}🖥️ [system] {Fore.MAGENTA}{msg}{Style.RESET_ALL}\" if self.fancy else f\"[system] {msg}\"\n        print(message)\n        self.message_list.append(msg)\n\n    def user_message(self, msg: str, msg_obj: Optional[Message] = None, raw=False):\n        if self.debug:\n            print(f\"user :: {msg}\")\n        if not self.show_user_message:\n            return\n\n        if isinstance(msg, str):\n            if raw:\n                message = f\"{Fore.GREEN}{Style.BRIGHT}🧑 {Fore.GREEN}{msg}{Style.RESET_ALL}\" if self.fancy else f\"[user] {msg}\"\n                self.message_list.append(message)\n                return\n            else:\n                try:\n                    msg_json = json.loads(msg, strict=JSON_LOADS_STRICT)\n                except:\n                    print(f\"{CLI_WARNING_PREFIX}failed to parse user message into json\")\n                    message = f\"{Fore.GREEN}{Style.BRIGHT}🧑 {Fore.GREEN}{msg}{Style.RESET_ALL}\" if self.fancy else f\"[user] {msg}\"\n                    self.message_list.append(message)\n                    return\n\n        if msg_json[\"type\"] == \"user_message\":\n            msg_json.pop(\"type\")\n            message = f\"{Fore.GREEN}{Style.BRIGHT}🧑 {Fore.GREEN}{msg_json}{Style.RESET_ALL}\" if self.fancy else f\"[user] {msg}\"\n        elif msg_json[\"type\"] == \"heartbeat\":\n            if True or DEBUG:\n                msg_json.pop(\"type\")\n                message = (\n                    f\"{Fore.GREEN}{Style.BRIGHT}💓 {Fore.GREEN}{msg_json}{Style.RESET_ALL}\" if self.fancy else f\"[system heartbeat] {msg}\"\n                )\n        elif msg_json[\"type\"] == \"system_message\":\n            msg_json.pop(\"type\")\n            message = f\"{Fore.GREEN}{Style.BRIGHT}🖥️ {Fore.GREEN}{msg_json}{Style.RESET_ALL}\" if self.fancy else f\"[system] {msg}\"\n        else:\n            message = f\"{Fore.GREEN}{Style.BRIGHT}🧑 {Fore.GREEN}{msg_json}{Style.RESET_ALL}\" if self.fancy else f\"[user] {msg}\"\n\n        # TODO should we ever be appending this?\n        self.message_list.append(message)\n\n    def function_message(self, msg: str, msg_obj: Optional[Message] = None):\n        if self.debug:\n            print(f\"function :: {msg}\")\n        if not self.show_function_outputs:\n            return\n\n        if isinstance(msg, dict):\n            message = f\"{Fore.RED}{Style.BRIGHT}⚡ [function] {Fore.RED}{msg}{Style.RESET_ALL}\"\n            # TODO should we ever be appending this?\n            self.message_list.append(message)\n            return\n\n        if msg.startswith(\"Success: \"):\n            message = (\n                f\"{Fore.RED}{Style.BRIGHT}⚡🟢 [function] {Fore.RED}{msg}{Style.RESET_ALL}\" if self.fancy else f\"[function - OK] {msg}\"\n            )\n        elif msg.startswith(\"Error: \"):\n            message = (\n                f\"{Fore.RED}{Style.BRIGHT}⚡🔴 [function] {Fore.RED}{msg}{Style.RESET_ALL}\" if self.fancy else f\"[function - error] {msg}\"\n            )\n        elif msg.startswith(\"Running \"):\n            if DEBUG:\n                message = f\"{Fore.RED}{Style.BRIGHT}⚡ [function] {Fore.RED}{msg}{Style.RESET_ALL}\" if self.fancy else f\"[function] {msg}\"\n            else:\n                if \"memory\" in msg:\n                    match = re.search(r\"Running (\\w+)\\((.*)\\)\", msg)\n                    if match:\n                        function_name = match.group(1)\n                        function_args = match.group(2)\n                        message = (\n                            f\"{Fore.RED}{Style.BRIGHT}⚡🧠 [function] {Fore.RED}updating memory with {function_name}{Style.RESET_ALL}:\"\n                            if self.fancy\n                            else f\"[function] updating memory with {function_name}\"\n                        )\n                        try:\n                            msg_dict = eval(function_args)\n                            if function_name == \"archival_memory_search\":\n                                message = (\n                                    f'{Fore.RED}\\tquery: {msg_dict[\"query\"]}, page: {msg_dict[\"page\"]}'\n                                    if self.fancy\n                                    else f'[function] query: {msg_dict[\"query\"]}, page: {msg_dict[\"page\"]}'\n                                )\n                            else:\n                                message = (\n                                    f'{Fore.RED}{Style.BRIGHT}\\t{Fore.RED} {msg_dict[\"old_content\"]}\\n\\t{Fore.GREEN}→ {msg_dict[\"new_content\"]}'\n                                    if self.fancy\n                                    else f'[old -> new] {msg_dict[\"old_content\"]} -> {msg_dict[\"new_content\"]}'\n                                )\n                        except Exception as e:\n                            print(e)\n                            message = msg_dict\n                    else:\n                        print(f\"{CLI_WARNING_PREFIX}did not recognize function message\")\n                        message = (\n                            f\"{Fore.RED}{Style.BRIGHT}⚡ [function] {Fore.RED}{msg}{Style.RESET_ALL}\" if self.fancy else f\"[function] {msg}\"\n                        )\n                elif \"send_message\" in msg:\n                    # ignore in debug mode\n                    message = None\n                else:\n                    message = (\n                        f\"{Fore.RED}{Style.BRIGHT}⚡ [function] {Fore.RED}{msg}{Style.RESET_ALL}\" if self.fancy else f\"[function] {msg}\"\n                    )\n        else:\n            try:\n                msg_dict = json.loads(msg, strict=JSON_LOADS_STRICT)\n                if \"status\" in msg_dict and msg_dict[\"status\"] == \"OK\":\n                    message = (\n                        f\"{Fore.GREEN}{Style.BRIGHT}⚡ [function] {Fore.GREEN}{msg}{Style.RESET_ALL}\" if self.fancy else f\"[function] {msg}\"\n                    )\n            except Exception:\n                print(f\"{CLI_WARNING_PREFIX}did not recognize function message {type(msg)} {msg}\")\n                message = f\"{Fore.RED}{Style.BRIGHT}⚡ [function] {Fore.RED}{msg}{Style.RESET_ALL}\" if self.fancy else f\"[function] {msg}\"\n\n        if message:\n            # self.message_list.append(message)\n            print(message)\n"}
{"type": "source_file", "path": "luann/cli/cli_config.py", "content": "import builtins\nimport os\nimport uuid\nfrom enum import Enum\nfrom typing import Annotated, Optional,List\nfrom functions.functions import load_function_file, write_function\nimport questionary\nimport typer\nfrom prettytable.colortable import ColorTable, Themes\nfrom tqdm import tqdm\nimport utils\nfrom agent_store.storage import StorageConnector, StorageType\nfrom config import typeagentConfig\nfrom constants import LLM_MAX_TOKENS, typeagent_DIR\nfrom credentials import SUPPORTED_AUTH_TYPES, typeagentCredentials\nfrom data_types import EmbeddingConfig, LLMConfig, Source, User\nfrom llm_api.anthropic import (\n    anthropic_get_model_list,\n    antropic_get_model_context_window,\n)\nfrom llm_api.azure_openai import azure_openai_get_model_list\nfrom llm_api.cohere import (\n    COHERE_VALID_MODEL_LIST,\n    cohere_get_model_context_window,\n    cohere_get_model_list,\n)\nfrom llm_api.google_ai import (\n    google_ai_get_model_context_window,\n    google_ai_get_model_list,\n)\nfrom llm_api.llm_api_tools import LLM_API_PROVIDER_OPTIONS\nfrom llm_api.openai import openai_get_model_list\nfrom local_llm.constants import (\n    DEFAULT_ENDPOINTS,\n    DEFAULT_OLLAMA_MODEL,\n    DEFAULT_WRAPPER_NAME,\n)\nfrom local_llm.utils import get_available_wrappers\nfrom metadata import MetadataStore\nfrom models.pydantic_models import HumanModel, PersonaModel,SystemPromptModel,SourceModel,ToolModel\nfrom presets.presets import create_preset_from_file,create_functions_schemal\nfrom server.utils import shorten_key_middle\n\napp = typer.Typer()\n\n\ndef get_azure_credentials():\n\n\n    \n    # return creds\n    creds = dict(\n        azure_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n        azure_version=os.getenv(\"AZURE_OPENAI_VERSION\"),\n        azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n        azure_embedding_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n    )\n    # embedding endpoint and version default to non-embedding\n    creds[\"azure_embedding_endpoint\"] = os.getenv(\"AZURE_OPENAI_EMBEDDING_ENDPOINT\", creds[\"azure_endpoint\"])\n    creds[\"azure_embedding_version\"] = os.getenv(\"AZURE_OPENAI_EMBEDDING_VERSION\", creds[\"azure_version\"])\n    return creds\n\n\ndef get_openai_credentials() -> Optional[str]:\n    openai_key = os.getenv(\"OPENAI_API_KEY\", None)\n    return openai_key\n\n\ndef get_google_ai_credentials() -> Optional[str]:\n    google_ai_key = os.getenv(\"GOOGLE_AI_API_KEY\", None)\n    return google_ai_key\n\n\ndef configure_llm_endpoint(config: typeagentConfig, credentials: typeagentCredentials):\n    # configure model endpoint\n    model_endpoint_type, model_endpoint = None, None\n\n    # get default\n    default_model_endpoint_type = config.default_llm_config.model_endpoint_type if config.default_llm_config else None\n    if (\n        config.default_llm_config\n        and config.default_llm_config.model_endpoint_type is not None\n        and config.default_llm_config.model_endpoint_type not in [provider for provider in LLM_API_PROVIDER_OPTIONS if provider != \"local\"]\n    ):  # local model\n        default_model_endpoint_type = \"local\"\n\n    provider = questionary.select(\n        \"Select LLM inference provider:\",\n        choices=LLM_API_PROVIDER_OPTIONS,\n        default=default_model_endpoint_type,\n    ).ask()\n    if provider is None:\n        raise KeyboardInterrupt\n\n    # set: model_endpoint_type, model_endpoint\n    if provider == \"openai\":\n        # check for key\n        if credentials.openai_key is None:\n            # allow key to get pulled from env vars\n            openai_api_key = os.getenv(\"OPENAI_API_KEY\", None)\n            # if we still can't find it, ask for it as input\n            if openai_api_key is None:\n                while openai_api_key is None or len(openai_api_key) == 0:\n                    # Ask for API key as input\n                    openai_api_key = questionary.password(\n                        \"Enter your OpenAI API key (starts with 'sk-', see https://platform.openai.com/api-keys):\"\n                    ).ask()\n                    if openai_api_key is None:\n                        raise KeyboardInterrupt\n            credentials.openai_key = openai_api_key\n            credentials.save()\n        else:\n            # Give the user an opportunity to overwrite the key\n            openai_api_key = None\n            default_input = (\n                shorten_key_middle(credentials.openai_key) if credentials.openai_key.startswith(\"sk-\") else credentials.openai_key\n            )\n            openai_api_key = questionary.password(\n                \"Enter your OpenAI API key (starts with 'sk-', see https://platform.openai.com/api-keys):\",\n                default=default_input,\n            ).ask()\n            if openai_api_key is None:\n                raise KeyboardInterrupt\n            # If the user modified it, use the new one\n            if openai_api_key != default_input:\n                credentials.openai_key = openai_api_key\n                credentials.save()\n\n        model_endpoint_type = \"openai\"\n        model_endpoint = \"https://api.openai.com/v1\"\n        model_endpoint = questionary.text(\"Override default endpoint:\", default=model_endpoint).ask()\n        if model_endpoint is None:\n            raise KeyboardInterrupt\n        provider = \"openai\"\n\n    elif provider == \"azure\":\n        # check for necessary vars\n        azure_creds = get_azure_credentials()\n        if not all([azure_creds[\"azure_key\"], azure_creds[\"azure_endpoint\"], azure_creds[\"azure_version\"]]):\n            raise ValueError(\n                \"Missing environment variables for Azure (see https://readme.io/docs/endpoints#azure-openai). Please set then run `typeagent configure` again.\"\n            )\n        else:\n            credentials.azure_key = azure_creds[\"azure_key\"]\n            credentials.azure_version = azure_creds[\"azure_version\"]\n            credentials.azure_endpoint = azure_creds[\"azure_endpoint\"]\n            if \"azure_deployment\" in azure_creds:\n                credentials.azure_deployment = azure_creds[\"azure_deployment\"]\n            credentials.azure_embedding_version = azure_creds[\"azure_embedding_version\"]\n            credentials.azure_embedding_endpoint = azure_creds[\"azure_embedding_endpoint\"]\n            credentials.azure_embedding_api_key=azure_creds[\"azure_embedding_api_key\"]\n            if \"azure_embedding_deployment\" in azure_creds:\n                credentials.azure_embedding_deployment = azure_creds[\"azure_embedding_deployment\"]\n            credentials.save()\n\n        model_endpoint_type = \"azure\"\n        model_endpoint = azure_creds[\"azure_endpoint\"]\n\n    elif provider == \"google_ai\":\n\n        # check for key\n        if credentials.google_ai_key is None:\n            # allow key to get pulled from env vars\n            google_ai_key = get_google_ai_credentials()\n            # if we still can't find it, ask for it as input\n            if google_ai_key is None:\n                while google_ai_key is None or len(google_ai_key) == 0:\n                    # Ask for API key as input\n                    google_ai_key = questionary.password(\n                        \"Enter your Google AI (Gemini) API key (see https://aistudio.google.com/app/apikey):\"\n                    ).ask()\n                    if google_ai_key is None:\n                        raise KeyboardInterrupt\n            credentials.google_ai_key = google_ai_key\n        else:\n            # Give the user an opportunity to overwrite the key\n            google_ai_key = None\n            default_input = shorten_key_middle(credentials.google_ai_key)\n\n            google_ai_key = questionary.password(\n                \"Enter your Google AI (Gemini) API key (see https://aistudio.google.com/app/apikey):\",\n                default=default_input,\n            ).ask()\n            if google_ai_key is None:\n                raise KeyboardInterrupt\n            # If the user modified it, use the new one\n            if google_ai_key != default_input:\n                credentials.google_ai_key = google_ai_key\n\n        default_input = os.getenv(\"GOOGLE_AI_SERVICE_ENDPOINT\", None)\n        if default_input is None:\n            default_input = \"generativelanguage\"\n        google_ai_service_endpoint = questionary.text(\n            \"Enter your Google AI (Gemini) service endpoint (see https://ai.google.dev/api/rest):\",\n            default=default_input,\n        ).ask()\n        credentials.google_ai_service_endpoint = google_ai_service_endpoint\n\n        # write out the credentials\n        credentials.save()\n\n        model_endpoint_type = \"google_ai\"\n\n    elif provider == \"anthropic\":\n        # check for key\n        if credentials.anthropic_key is None:\n            # allow key to get pulled from env vars\n            anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\", None)\n            # if we still can't find it, ask for it as input\n            if anthropic_api_key is None:\n                while anthropic_api_key is None or len(anthropic_api_key) == 0:\n                    # Ask for API key as input\n                    anthropic_api_key = questionary.password(\n                        \"Enter your Anthropic API key (starts with 'sk-', see https://console.anthropic.com/settings/keys):\"\n                    ).ask()\n                    if anthropic_api_key is None:\n                        raise KeyboardInterrupt\n            credentials.anthropic_key = anthropic_api_key\n            credentials.save()\n        else:\n            # Give the user an opportunity to overwrite the key\n            anthropic_api_key = None\n            default_input = (\n                shorten_key_middle(credentials.anthropic_key) if credentials.anthropic_key.startswith(\"sk-\") else credentials.anthropic_key\n            )\n            anthropic_api_key = questionary.password(\n                \"Enter your Anthropic API key (starts with 'sk-', see https://console.anthropic.com/settings/keys):\",\n                default=default_input,\n            ).ask()\n            if anthropic_api_key is None:\n                raise KeyboardInterrupt\n            # If the user modified it, use the new one\n            if anthropic_api_key != default_input:\n                credentials.anthropic_key = anthropic_api_key\n                credentials.save()\n\n        model_endpoint_type = \"anthropic\"\n        model_endpoint = \"https://api.anthropic.com/v1\"\n        model_endpoint = questionary.text(\"Override default endpoint:\", default=model_endpoint).ask()\n        if model_endpoint is None:\n            raise KeyboardInterrupt\n        provider = \"anthropic\"\n\n    elif provider == \"cohere\":\n        # check for key\n        if credentials.cohere_key is None:\n            # allow key to get pulled from env vars\n            cohere_api_key = os.getenv(\"COHERE_API_KEY\", None)\n            # if we still can't find it, ask for it as input\n            if cohere_api_key is None:\n                while cohere_api_key is None or len(cohere_api_key) == 0:\n                    # Ask for API key as input\n                    cohere_api_key = questionary.password(\"Enter your Cohere API key (see https://dashboard.cohere.com/api-keys):\").ask()\n                    if cohere_api_key is None:\n                        raise KeyboardInterrupt\n            credentials.cohere_key = cohere_api_key\n            credentials.save()\n        else:\n            # Give the user an opportunity to overwrite the key\n            cohere_api_key = None\n            default_input = (\n                shorten_key_middle(credentials.cohere_key) if credentials.cohere_key.startswith(\"sk-\") else credentials.cohere_key\n            )\n            cohere_api_key = questionary.password(\n                \"Enter your Cohere API key (see https://dashboard.cohere.com/api-keys):\",\n                default=default_input,\n            ).ask()\n            if cohere_api_key is None:\n                raise KeyboardInterrupt\n            # If the user modified it, use the new one\n            if cohere_api_key != default_input:\n                credentials.cohere_key = cohere_api_key\n                credentials.save()\n\n        model_endpoint_type = \"cohere\"\n        model_endpoint = \"https://api.cohere.ai/v1\"\n        model_endpoint = questionary.text(\"Override default endpoint:\", default=model_endpoint).ask()\n        if model_endpoint is None:\n            raise KeyboardInterrupt\n        provider = \"cohere\"\n\n    else:  # local models\n        # backend_options_old = [\"webui\", \"webui-legacy\", \"llamacpp\", \"koboldcpp\", \"ollama\", \"lmstudio\", \"lmstudio-legacy\", \"vllm\", \"openai\"]\n        backend_options = builtins.list(DEFAULT_ENDPOINTS.keys())\n        # assert backend_options_old == backend_options, (backend_options_old, backend_options)\n        default_model_endpoint_type = None\n        if config.default_llm_config and config.default_llm_config.model_endpoint_type in backend_options:\n            # set from previous config\n            default_model_endpoint_type = config.default_llm_config.model_endpoint_type\n        model_endpoint_type = questionary.select(\n            \"Select LLM backend (select 'openai' if you have an OpenAI compatible proxy):\",\n            backend_options,\n            default=default_model_endpoint_type,\n        ).ask()\n        if model_endpoint_type is None:\n            raise KeyboardInterrupt\n\n        # set default endpoint\n        # if OPENAI_API_BASE is set, assume that this is the IP+port the user wanted to use\n        default_model_endpoint = os.getenv(\"OPENAI_API_BASE\")\n        # if OPENAI_API_BASE is not set, try to pull a default IP+port format from a hardcoded set\n        if default_model_endpoint is None:\n            if model_endpoint_type in DEFAULT_ENDPOINTS:\n                default_model_endpoint = DEFAULT_ENDPOINTS[model_endpoint_type]\n                model_endpoint = questionary.text(\"Enter default endpoint:\", default=default_model_endpoint).ask()\n                if model_endpoint is None:\n                    raise KeyboardInterrupt\n                while not utils.is_valid_url(model_endpoint):\n                    typer.secho(f\"Endpoint must be a valid address\", fg=typer.colors.YELLOW)\n                    model_endpoint = questionary.text(\"Enter default endpoint:\", default=default_model_endpoint).ask()\n                    if model_endpoint is None:\n                        raise KeyboardInterrupt\n            elif config.default_llm_config and config.default_llm_config.model_endpoint:\n                model_endpoint = questionary.text(\"Enter default endpoint:\", default=config.default_llm_config.model_endpoint).ask()\n                if model_endpoint is None:\n                    raise KeyboardInterrupt\n                while not utils.is_valid_url(model_endpoint):\n                    typer.secho(f\"Endpoint must be a valid address\", fg=typer.colors.YELLOW)\n                    model_endpoint = questionary.text(\"Enter default endpoint:\", default=config.default_llm_config.model_endpoint).ask()\n                    if model_endpoint is None:\n                        raise KeyboardInterrupt\n            else:\n                # default_model_endpoint = None\n                model_endpoint = None\n                model_endpoint = questionary.text(\"Enter default endpoint:\").ask()\n                if model_endpoint is None:\n                    raise KeyboardInterrupt\n                while not utils.is_valid_url(model_endpoint):\n                    typer.secho(f\"Endpoint must be a valid address\", fg=typer.colors.YELLOW)\n                    model_endpoint = questionary.text(\"Enter default endpoint:\").ask()\n                    if model_endpoint is None:\n                        raise KeyboardInterrupt\n        else:\n            model_endpoint = default_model_endpoint\n        assert model_endpoint, f\"Environment variable OPENAI_API_BASE must be set.\"\n\n    return model_endpoint_type, model_endpoint\n\n\ndef get_model_options(\n    credentials: typeagentCredentials,\n    model_endpoint_type: str,\n    model_endpoint: str,\n    filter_list: bool = True,\n    filter_prefix: str = \"gpt-\",\n) -> list:\n    try:\n        if model_endpoint_type == \"openai\":\n            if credentials.openai_key is None:\n                raise ValueError(\"Missing OpenAI API key\")\n            fetched_model_options_response = openai_get_model_list(url=model_endpoint, api_key=credentials.openai_key)\n\n            # Filter the list for \"gpt\" models only\n            if filter_list:\n                model_options = [obj[\"id\"] for obj in fetched_model_options_response[\"data\"] if obj[\"id\"].startswith(filter_prefix)]\n            else:\n                model_options = [obj[\"id\"] for obj in fetched_model_options_response[\"data\"]]\n\n        elif model_endpoint_type == \"azure\":\n            if credentials.azure_key is None:\n                raise ValueError(\"Missing Azure key\")\n            if credentials.azure_version is None:\n                raise ValueError(\"Missing Azure version\")\n            fetched_model_options_response = azure_openai_get_model_list(\n                url=model_endpoint, api_key=credentials.azure_key, api_version=credentials.azure_version\n            )\n\n            # Filter the list for \"gpt\" models only\n            if filter_list:\n                model_options = [obj[\"id\"] for obj in fetched_model_options_response[\"data\"] if obj[\"id\"].startswith(filter_prefix)]\n            else:\n                model_options = [obj[\"id\"] for obj in fetched_model_options_response[\"data\"]]\n\n        elif model_endpoint_type == \"google_ai\":\n            if credentials.google_ai_key is None:\n                raise ValueError(\"Missing Google AI API key\")\n            if credentials.google_ai_service_endpoint is None:\n                raise ValueError(\"Missing Google AI service endpoint\")\n            model_options = google_ai_get_model_list(\n                service_endpoint=credentials.google_ai_service_endpoint, api_key=credentials.google_ai_key\n            )\n            model_options = [str(m[\"name\"]) for m in model_options]\n            model_options = [mo[len(\"models/\") :] if mo.startswith(\"models/\") else mo for mo in model_options]\n\n            # TODO remove manual filtering for gemini-pro\n            model_options = [mo for mo in model_options if str(mo).startswith(\"gemini\") and \"-pro\" in str(mo)]\n            # model_options = [\"gemini-pro\"]\n\n        elif model_endpoint_type == \"anthropic\":\n            if credentials.anthropic_key is None:\n                raise ValueError(\"Missing Anthropic API key\")\n            fetched_model_options = anthropic_get_model_list(url=model_endpoint, api_key=credentials.anthropic_key)\n            model_options = [obj[\"name\"] for obj in fetched_model_options]\n\n        elif model_endpoint_type == \"cohere\":\n            if credentials.cohere_key is None:\n                raise ValueError(\"Missing Cohere API key\")\n            fetched_model_options = cohere_get_model_list(url=model_endpoint, api_key=credentials.cohere_key)\n            model_options = [obj for obj in fetched_model_options]\n\n        else:\n            # Attempt to do OpenAI endpoint style model fetching\n            # TODO support local auth with api-key header\n            if credentials.openllm_auth_type == \"bearer_token\":\n                api_key = credentials.openllm_key\n            else:\n                api_key = None\n            fetched_model_options_response = openai_get_model_list(url=model_endpoint, api_key=api_key, fix_url=True)\n            model_options = [obj[\"id\"] for obj in fetched_model_options_response[\"data\"]]\n            # NOTE no filtering of local model options\n\n        # list\n        return model_options\n\n    except:\n        raise Exception(f\"Failed to get model list from {model_endpoint}\")\n\n\ndef configure_model(config: typeagentConfig, credentials: typeagentCredentials, model_endpoint_type: str, model_endpoint: str):\n    # set: model, model_wrapper\n    model, model_wrapper = None, None\n    if model_endpoint_type == \"openai\" or model_endpoint_type == \"azure\":\n        # Get the model list from the openai / azure endpoint\n        hardcoded_model_options = [\"gpt-4\", \"gpt-4-32k\", \"gpt-4-1106-preview\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"]\n        fetched_model_options = []\n        try:\n            fetched_model_options = get_model_options(\n                credentials=credentials, model_endpoint_type=model_endpoint_type, model_endpoint=model_endpoint\n            )\n        except Exception as e:\n            # NOTE: if this fails, it means the user's key is probably bad\n            typer.secho(\n                f\"Failed to get model list from {model_endpoint} - make sure your API key and endpoints are correct!\", fg=typer.colors.RED\n            )\n            raise e\n\n        # First ask if the user wants to see the full model list (some may be incompatible)\n        see_all_option_str = \"[see all options]\"\n        other_option_str = \"[enter model name manually]\"\n\n        # Check if the model we have set already is even in the list (informs our default)\n        valid_model = config.default_llm_config and config.default_llm_config.model in hardcoded_model_options\n        model = questionary.select(\n            \"Select default model (recommended: gpt-4):\",\n            choices=hardcoded_model_options + [see_all_option_str, other_option_str],\n            default=config.default_llm_config.model if valid_model else hardcoded_model_options[0],\n        ).ask()\n        if model is None:\n            raise KeyboardInterrupt\n\n        # If the user asked for the full list, show it\n        if model == see_all_option_str:\n            typer.secho(f\"Warning: not all models shown are guaranteed to work with typeagent\", fg=typer.colors.RED)\n            model = questionary.select(\n                \"Select default model (recommended: gpt-4):\",\n                choices=fetched_model_options + [other_option_str],\n                default=config.default_llm_config.model if (valid_model and config.default_llm_config) else fetched_model_options[0],\n            ).ask()\n            if model is None:\n                raise KeyboardInterrupt\n\n        # Finally if the user asked to manually input, allow it\n        if model == other_option_str:\n            model = \"\"\n            while len(model) == 0:\n                model = questionary.text(\n                    \"Enter custom model name:\",\n                ).ask()\n                if model is None:\n                    raise KeyboardInterrupt\n\n    elif model_endpoint_type == \"google_ai\":\n        try:\n            fetched_model_options = get_model_options(\n                credentials=credentials, model_endpoint_type=model_endpoint_type, model_endpoint=model_endpoint\n            )\n        except Exception as e:\n            # NOTE: if this fails, it means the user's key is probably bad\n            typer.secho(\n                f\"Failed to get model list from {model_endpoint} - make sure your API key and endpoints are correct!\", fg=typer.colors.RED\n            )\n            raise e\n\n        model = questionary.select(\n            \"Select default model:\",\n            choices=fetched_model_options,\n            default=fetched_model_options[0],\n        ).ask()\n        if model is None:\n            raise KeyboardInterrupt\n\n    elif model_endpoint_type == \"anthropic\":\n        try:\n            fetched_model_options = get_model_options(\n                credentials=credentials, model_endpoint_type=model_endpoint_type, model_endpoint=model_endpoint\n            )\n        except Exception as e:\n            # NOTE: if this fails, it means the user's key is probably bad\n            typer.secho(\n                f\"Failed to get model list from {model_endpoint} - make sure your API key and endpoints are correct!\", fg=typer.colors.RED\n            )\n            raise e\n\n        model = questionary.select(\n            \"Select default model:\",\n            choices=fetched_model_options,\n            default=fetched_model_options[0],\n        ).ask()\n        if model is None:\n            raise KeyboardInterrupt\n\n    elif model_endpoint_type == \"cohere\":\n\n        fetched_model_options = []\n        try:\n            fetched_model_options = get_model_options(\n                credentials=credentials, model_endpoint_type=model_endpoint_type, model_endpoint=model_endpoint\n            )\n        except Exception as e:\n            # NOTE: if this fails, it means the user's key is probably bad\n            typer.secho(\n                f\"Failed to get model list from {model_endpoint} - make sure your API key and endpoints are correct!\", fg=typer.colors.RED\n            )\n            raise e\n\n        fetched_model_options = [m[\"name\"] for m in fetched_model_options]\n        hardcoded_model_options = [m for m in fetched_model_options if m in COHERE_VALID_MODEL_LIST]\n\n        # First ask if the user wants to see the full model list (some may be incompatible)\n        see_all_option_str = \"[see all options]\"\n        other_option_str = \"[enter model name manually]\"\n\n        # Check if the model we have set already is even in the list (informs our default)\n        valid_model = config.default_llm_config.model in hardcoded_model_options\n        model = questionary.select(\n            \"Select default model (recommended: command-r-plus):\",\n            choices=hardcoded_model_options + [see_all_option_str, other_option_str],\n            default=config.default_llm_config.model if valid_model else hardcoded_model_options[0],\n        ).ask()\n        if model is None:\n            raise KeyboardInterrupt\n\n        # If the user asked for the full list, show it\n        if model == see_all_option_str:\n            typer.secho(f\"Warning: not all models shown are guaranteed to work with typeagent\", fg=typer.colors.RED)\n            model = questionary.select(\n                \"Select default model (recommended: command-r-plus):\",\n                choices=fetched_model_options + [other_option_str],\n                default=config.default_llm_config.model if valid_model else fetched_model_options[0],\n            ).ask()\n            if model is None:\n                raise KeyboardInterrupt\n\n        # Finally if the user asked to manually input, allow it\n        if model == other_option_str:\n            model = \"\"\n            while len(model) == 0:\n                model = questionary.text(\n                    \"Enter custom model name:\",\n                ).ask()\n                if model is None:\n                    raise KeyboardInterrupt\n\n    else:  # local models\n\n        # ask about local auth\n        if model_endpoint_type in [\"groq\"]:  # TODO all llm engines under 'local' that will require api keys\n            use_local_auth = True\n            local_auth_type = \"bearer_token\"\n            local_auth_key = questionary.password(\n                \"Enter your Groq API key:\",\n            ).ask()\n            if local_auth_key is None:\n                raise KeyboardInterrupt\n            credentials.openllm_auth_type = local_auth_type\n            credentials.openllm_key = local_auth_key\n            credentials.save()\n        else:\n            use_local_auth = questionary.confirm(\n                \"Is your LLM endpoint authenticated? (default no)\",\n                default=False,\n            ).ask()\n            if use_local_auth is None:\n                raise KeyboardInterrupt\n            if use_local_auth:\n                local_auth_type = questionary.select(\n                    \"What HTTP authentication method does your endpoint require?\",\n                    choices=SUPPORTED_AUTH_TYPES,\n                    default=SUPPORTED_AUTH_TYPES[0],\n                ).ask()\n                if local_auth_type is None:\n                    raise KeyboardInterrupt\n                local_auth_key = questionary.password(\n                    \"Enter your authentication key:\",\n                ).ask()\n                if local_auth_key is None:\n                    raise KeyboardInterrupt\n                # credentials = typeagentCredentials.load()\n                credentials.openllm_auth_type = local_auth_type\n                credentials.openllm_key = local_auth_key\n                credentials.save()\n\n        # ollama also needs model type\n        if model_endpoint_type == \"ollama\":\n            default_model = (\n                config.default_llm_config.model\n                if config.default_llm_config and config.default_llm_config.model_endpoint_type == \"ollama\"\n                else DEFAULT_OLLAMA_MODEL\n            )\n            model = questionary.text(\n                \"Enter default model name (required for Ollama, see: https://readme.io/docs/ollama):\",\n                default=default_model,\n            ).ask()\n            if model is None:\n                raise KeyboardInterrupt\n            model = None if len(model) == 0 else model\n\n        default_model = (\n            config.default_llm_config.model if config.default_llm_config and config.default_llm_config.model_endpoint_type == \"vllm\" else \"\"\n        )\n\n        # vllm needs huggingface model tag\n        if model_endpoint_type in [\"vllm\", \"groq\"]:\n            try:\n                # Don't filter model list for vLLM since model list is likely much smaller than OpenAI/Azure endpoint\n                # + probably has custom model names\n                # TODO support local auth\n                model_options = get_model_options(\n                    credentials=credentials, model_endpoint_type=model_endpoint_type, model_endpoint=model_endpoint\n                )\n            except:\n                print(f\"Failed to get model list from {model_endpoint}, using defaults\")\n                model_options = None\n\n            # If we got model options from vLLM endpoint, allow selection + custom input\n            if model_options is not None:\n                other_option_str = \"other (enter name)\"\n                valid_model = config.default_llm_config.model in model_options\n                model_options.append(other_option_str)\n                model = questionary.select(\n                    \"Select default model:\",\n                    choices=model_options,\n                    default=config.default_llm_config.model if valid_model else model_options[0],\n                ).ask()\n                if model is None:\n                    raise KeyboardInterrupt\n\n                # If we got custom input, ask for raw input\n                if model == other_option_str:\n                    model = questionary.text(\n                        \"Enter HuggingFace model tag (e.g. ehartford/dolphin-2.2.1-mistral-7b):\",\n                        default=default_model,\n                    ).ask()\n                    if model is None:\n                        raise KeyboardInterrupt\n                    # TODO allow empty string for input?\n                    model = None if len(model) == 0 else model\n\n            else:\n                model = questionary.text(\n                    \"Enter HuggingFace model tag (e.g. ehartford/dolphin-2.2.1-mistral-7b):\",\n                    default=default_model,\n                ).ask()\n                if model is None:\n                    raise KeyboardInterrupt\n                model = None if len(model) == 0 else model\n\n        # model wrapper\n        available_model_wrappers = builtins.list(get_available_wrappers().keys())\n        model_wrapper = questionary.select(\n            f\"Select default model wrapper (recommended: {DEFAULT_WRAPPER_NAME}):\",\n            choices=available_model_wrappers,\n            default=DEFAULT_WRAPPER_NAME,\n        ).ask()\n        if model_wrapper is None:\n            raise KeyboardInterrupt\n\n    # set: context_window\n    if str(model) not in LLM_MAX_TOKENS:\n\n        context_length_options = [\n            str(2**12),  # 4096\n            str(2**13),  # 8192\n            str(2**14),  # 16384\n            str(2**15),  # 32768\n            str(2**18),  # 262144\n            str(128000),\n            \"custom\",  # enter yourself\n        ]\n\n        if model_endpoint_type == \"google_ai\":\n            try:\n                fetched_context_window = str(\n                    google_ai_get_model_context_window(\n                        service_endpoint=credentials.google_ai_service_endpoint, api_key=credentials.google_ai_key, model=model\n                    )\n                )\n                print(f\"Got context window {fetched_context_window} for model {model} (from Google API)\")\n                context_length_options = [\n                    fetched_context_window,\n                    \"custom\",\n                ]\n            except Exception as e:\n                print(f\"Failed to get model details for model '{model}' on Google AI API ({str(e)})\")\n\n            context_window_input = questionary.select(\n                \"Select your model's context window (see https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versioning#gemini-model-versions):\",\n                choices=context_length_options,\n                default=context_length_options[0],\n            ).ask()\n            if context_window_input is None:\n                raise KeyboardInterrupt\n\n        elif model_endpoint_type == \"anthropic\":\n            try:\n                fetched_context_window = str(\n                    antropic_get_model_context_window(url=model_endpoint, api_key=credentials.anthropic_key, model=model)\n                )\n                print(f\"Got context window {fetched_context_window} for model {model}\")\n                context_length_options = [\n                    fetched_context_window,\n                    \"custom\",\n                ]\n            except Exception as e:\n                print(f\"Failed to get model details for model '{model}' ({str(e)})\")\n\n            context_window_input = questionary.select(\n                \"Select your model's context window (see https://docs.anthropic.com/claude/docs/models-overview):\",\n                choices=context_length_options,\n                default=context_length_options[0],\n            ).ask()\n            if context_window_input is None:\n                raise KeyboardInterrupt\n\n        elif model_endpoint_type == \"cohere\":\n            try:\n                fetched_context_window = str(\n                    cohere_get_model_context_window(url=model_endpoint, api_key=credentials.cohere_key, model=model)\n                )\n                print(f\"Got context window {fetched_context_window} for model {model}\")\n                context_length_options = [\n                    fetched_context_window,\n                    \"custom\",\n                ]\n            except Exception as e:\n                print(f\"Failed to get model details for model '{model}' ({str(e)})\")\n\n            context_window_input = questionary.select(\n                \"Select your model's context window (see https://docs.cohere.com/docs/command-r):\",\n                choices=context_length_options,\n                default=context_length_options[0],\n            ).ask()\n            if context_window_input is None:\n                raise KeyboardInterrupt\n\n        else:\n\n            # Ask the user to specify the context length\n            context_window_input = questionary.select(\n                \"Select your model's context window (for Mistral 7B models, this is probably 8k / 8192):\",\n                choices=context_length_options,\n                default=str(LLM_MAX_TOKENS[\"DEFAULT\"]),\n            ).ask()\n            if context_window_input is None:\n                raise KeyboardInterrupt\n\n        # If custom, ask for input\n        if context_window_input == \"custom\":\n            while True:\n                context_window_input = questionary.text(\"Enter context window (e.g. 8192)\").ask()\n                if context_window_input is None:\n                    raise KeyboardInterrupt\n                try:\n                    context_window = int(context_window_input)\n                    break\n                except ValueError:\n                    print(f\"Context window must be a valid integer\")\n        else:\n            context_window = int(context_window_input)\n    else:\n        # Pull the context length from the models\n        context_window = int(LLM_MAX_TOKENS[str(model)])\n    return model, model_wrapper, context_window\n\n\ndef configure_embedding_endpoint(config: typeagentConfig, credentials: typeagentCredentials):\n    # configure embedding endpoint\n\n    default_embedding_endpoint_type = config.default_embedding_config.embedding_endpoint_type if config.default_embedding_config else None\n\n    embedding_endpoint_type, embedding_endpoint, embedding_dim, embedding_model = None, None, None, None\n    embedding_provider = questionary.select(\n        \"Select embedding provider:\", choices=[\"openai\", \"azure\", \"hugging-face\", \"local\"], default=default_embedding_endpoint_type\n    ).ask()\n    if embedding_provider is None:\n        raise KeyboardInterrupt\n\n    if embedding_provider == \"openai\":\n        # check for key\n        if credentials.openai_key is None:\n            # allow key to get pulled from env vars\n            openai_api_key = os.getenv(\"OPENAI_API_KEY\", None)\n            if openai_api_key is None:\n                # if we still can't find it, ask for it as input\n                while openai_api_key is None or len(openai_api_key) == 0:\n                    # Ask for API key as input\n                    openai_api_key = questionary.password(\n                        \"Enter your OpenAI API key (starts with 'sk-', see https://platform.openai.com/api-keys):\"\n                    ).ask()\n                    if openai_api_key is None:\n                        raise KeyboardInterrupt\n                credentials.openai_key = openai_api_key\n                credentials.save()\n\n        embedding_endpoint_type = \"openai\"\n        embedding_endpoint = \"https://api.openai.com/v1\"\n        embedding_dim = 1536\n        embedding_model = \"text-embedding-ada-002\"\n\n    elif embedding_provider == \"azure\":\n        # check for necessary vars\n        azure_creds = get_azure_credentials()\n        if not all([azure_creds[\"azure_key\"], azure_creds[\"azure_embedding_endpoint\"], azure_creds[\"azure_embedding_version\"]]):\n            raise ValueError(\n                \"Missing environment variables for Azure (see https://readme.io/docs/endpoints#azure-openai). Please set then run `typeagent configure` again.\"\n            )\n        credentials.azure_key = azure_creds[\"azure_key\"]\n        credentials.azure_version = azure_creds[\"azure_version\"]\n        credentials.azure_embedding_endpoint = azure_creds[\"azure_embedding_endpoint\"]\n        credentials.azure_embedding_api_key = azure_creds[\"azure_embedding_api_key\"]\n        credentials.save()\n\n        embedding_endpoint_type = \"azure\"\n        embedding_endpoint = azure_creds[\"azure_embedding_endpoint\"]\n        embedding_dim = 1536\n        embedding_model = \"text-embedding-ada-002\"\n\n    elif embedding_provider == \"hugging-face\":\n        # configure hugging face embedding endpoint (https://github.com/huggingface/text-embeddings-inference)\n        # supports custom model/endpoints\n        embedding_endpoint_type = \"hugging-face\"\n        embedding_endpoint = None\n\n        # get endpoint\n        embedding_endpoint = questionary.text(\"Enter default endpoint:\").ask()\n        if embedding_endpoint is None:\n            raise KeyboardInterrupt\n        while not utils.is_valid_url(embedding_endpoint):\n            typer.secho(f\"Endpoint must be a valid address\", fg=typer.colors.YELLOW)\n            embedding_endpoint = questionary.text(\"Enter default endpoint:\").ask()\n            if embedding_endpoint is None:\n                raise KeyboardInterrupt\n\n        # get model type\n        default_embedding_model = (\n            config.default_embedding_config.embedding_model if config.default_embedding_config else \"BAAI/bge-large-en-v1.5\"\n        )\n        embedding_model = questionary.text(\n            \"Enter HuggingFace model tag (e.g. BAAI/bge-large-en-v1.5):\",\n            default=default_embedding_model,\n        ).ask()\n        if embedding_model is None:\n            raise KeyboardInterrupt\n\n        # get model dimentions\n        default_embedding_dim = config.default_embedding_config.embedding_dim if config.default_embedding_config else \"1024\"\n        embedding_dim = questionary.text(\"Enter embedding model dimentions (e.g. 1024):\", default=str(default_embedding_dim)).ask()\n        if embedding_dim is None:\n            raise KeyboardInterrupt\n        try:\n            embedding_dim = int(embedding_dim)\n        except Exception:\n            raise ValueError(f\"Failed to cast {embedding_dim} to integer.\")\n    elif embedding_provider == \"ollama\":\n        # configure ollama embedding endpoint\n        embedding_endpoint_type = \"ollama\"\n        embedding_endpoint = \"http://localhost:11434/api/embeddings\"\n        # Source: https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings:~:text=http%3A//localhost%3A11434/api/embeddings\n\n        # get endpoint (is this necessary?)\n        embedding_endpoint = questionary.text(\"Enter Ollama API endpoint:\").ask()\n        if embedding_endpoint is None:\n            raise KeyboardInterrupt\n        while not utils.is_valid_url(embedding_endpoint):\n            typer.secho(f\"Endpoint must be a valid address\", fg=typer.colors.YELLOW)\n            embedding_endpoint = questionary.text(\"Enter Ollama API endpoint:\").ask()\n            if embedding_endpoint is None:\n                raise KeyboardInterrupt\n\n        # get model type\n        default_embedding_model = (\n            config.default_embedding_config.embedding_model if config.default_embedding_config else \"mxbai-embed-large\"\n        )\n        embedding_model = questionary.text(\n            \"Enter Ollama model tag (e.g. mxbai-embed-large):\",\n            default=default_embedding_model,\n        ).ask()\n        if embedding_model is None:\n            raise KeyboardInterrupt\n\n        # get model dimensions\n        default_embedding_dim = config.default_embedding_config.embedding_dim if config.default_embedding_config else \"512\"\n        embedding_dim = questionary.text(\"Enter embedding model dimensions (e.g. 512):\", default=str(default_embedding_dim)).ask()\n        if embedding_dim is None:\n            raise KeyboardInterrupt\n        try:\n            embedding_dim = int(embedding_dim)\n        except Exception:\n            raise ValueError(f\"Failed to cast {embedding_dim} to integer.\")\n    else:  # local models\n        embedding_endpoint_type = \"local\"\n        embedding_endpoint = None\n        embedding_model = \"BAAI/bge-small-en-v1.5\"\n        embedding_dim = 384\n\n    return embedding_endpoint_type, embedding_endpoint, embedding_dim, embedding_model\n\n\ndef configure_archival_memory_storage(config: typeagentConfig, credentials: typeagentCredentials):\n    # Configure archival storage backend\n    archival_memory_storage_options = [\"chroma\", \"milvus\"]\n    archival_memory_storage_type = questionary.select(\n        \"Select storage backend for archival memory data(long-term memory,Associative Memory):\", archival_memory_storage_options, default=config.archival_memory_storage_type\n    ).ask()\n    if archival_memory_storage_type is None:\n        raise KeyboardInterrupt\n    archival_memory_storage_uri, archival_memory_storage_path = config.archival_memory_storage_uri, config.archival_memory_storage_path\n    typer.secho(f\"📖 archival_memory_storage_uri to {config.archival_memory_storage_uri}\", fg=typer.colors.GREEN)\n    # configure postgres\n    # if archival_storage_type == \"postgres\":\n    #     archival_storage_uri = questionary.text(\n    #         \"Enter postgres connection string (e.g. postgresql+pg8000://{user}:{password}@{ip}:5432/{database}):\",\n    #         default=config.archival_storage_uri if config.archival_storage_uri else \"\",\n    #     ).ask()\n    #     if archival_storage_uri is None:\n    #         raise KeyboardInterrupt\n\n    # TODO: add back\n    ## configure lancedb\n    # if archival_storage_type == \"lancedb\":\n    #    archival_storage_uri = questionary.text(\n    #        \"Enter lanncedb connection string (e.g. ./.lancedb\",\n    #        default=config.archival_storage_uri if config.archival_storage_uri else \"./.lancedb\",\n    #    ).ask()\n\n    # configure chroma\n    if archival_memory_storage_type == \"chroma\":\n        chroma_type = questionary.select(\"Select chroma backend:\", [\"http\", \"persistent\"], default=\"persistent\").ask()\n        if chroma_type is None:\n            raise KeyboardInterrupt\n        if chroma_type == \"http\":\n            archival_memory_storage_uri = questionary.text(\"Enter chroma ip (e.g. localhost:8000):\", default=\"localhost:8000\").ask()\n            if archival_memory_storage_uri is None:\n                raise KeyboardInterrupt\n        if chroma_type == \"persistent\":\n            archival_memory_storage_path = os.path.join(typeagent_DIR, \"archival_memory_storage\")\n    \n    if archival_memory_storage_type == \"qdrant\":\n        qdrant_type = questionary.select(\"Select Qdrant backend:\", [\"local\", \"server\"], default=\"local\").ask()\n        if qdrant_type is None:\n            raise KeyboardInterrupt\n        if qdrant_type == \"server\":\n            archival_memory_storage_uri = questionary.text(\n                \"Enter the Qdrant instance URI (Default: localhost:6333):\", default=\"localhost:6333\"\n            ).ask()\n            if archival_memory_storage_uri is None:\n                raise KeyboardInterrupt\n        if qdrant_type == \"local\":\n            archival_memory_storage_path = os.path.join(typeagent_DIR, \"qdrant\")\n    if archival_memory_storage_type == \"milvus\":\n        default_milvus_uri= os.path.join(typeagent_DIR, \"milvus.db\")\n        archival_memory_storage_uri = questionary.text(\n            f\"Enter the Milvus connection URI (Default: {default_milvus_uri}):\", default=default_milvus_uri\n        ).ask()\n        if archival_memory_storage_uri is None:\n            raise KeyboardInterrupt\n    return archival_memory_storage_type, archival_memory_storage_uri, archival_memory_storage_path\n\n    # TODO: allow configuring embedding model\n\ndef configure_knowledge_base_storage(config: typeagentConfig, credentials: typeagentCredentials):\n    # Configure archival storage backend\n    knowledge_base_storage_options = [\"chroma\", \"milvus\"]\n    knowledge_base_storage_type = questionary.select(\n        \"Select storage backend for knowledge base  data:\", knowledge_base_storage_options, default=config.knowledge_base_storage_type\n    ).ask()\n    if knowledge_base_storage_type is None:\n        raise KeyboardInterrupt\n    knowledge_base_storage_uri, knowledge_base_storage_path = config.knowledge_base_storage_uri, config.knowledge_base_storage_path\n    typer.secho(f\"📖 knowledge_base_storage_uri to {config.knowledge_base_storage_uri}\", fg=typer.colors.GREEN)\n    # configure postgres\n    # if archival_storage_type == \"postgres\":\n    #     archival_storage_uri = questionary.text(\n    #         \"Enter postgres connection string (e.g. postgresql+pg8000://{user}:{password}@{ip}:5432/{database}):\",\n    #         default=config.archival_storage_uri if config.archival_storage_uri else \"\",\n    #     ).ask()\n    #     if archival_storage_uri is None:\n    #         raise KeyboardInterrupt\n\n    # TODO: add back\n    ## configure lancedb\n    # if archival_storage_type == \"lancedb\":\n    #    archival_storage_uri = questionary.text(\n    #        \"Enter lanncedb connection string (e.g. ./.lancedb\",\n    #        default=config.archival_storage_uri if config.archival_storage_uri else \"./.lancedb\",\n    #    ).ask()\n\n    # configure chroma\n    if knowledge_base_storage_type == \"chroma\":\n        chroma_type = questionary.select(\"Select chroma backend:\", [\"http\", \"persistent\"], default=\"persistent\").ask()\n        if chroma_type is None:\n            raise KeyboardInterrupt\n        if chroma_type == \"http\":\n            knowledge_base_storage_uri = questionary.text(\"Enter chroma ip (e.g. localhost:8000):\", default=\"localhost:8000\").ask()\n            if knowledge_base_storage_uri is None:\n                raise KeyboardInterrupt\n        if chroma_type == \"persistent\":\n            knowledge_base_storage_path = os.path.join(typeagent_DIR, \"knowledge_base\")\n    \n    if knowledge_base_storage_type == \"qdrant\":\n        qdrant_type = questionary.select(\"Select Qdrant backend:\", [\"local\", \"server\"], default=\"local\").ask()\n        if qdrant_type is None:\n            raise KeyboardInterrupt\n        if qdrant_type == \"server\":\n            knowledge_base_storage_uri = questionary.text(\n                \"Enter the Qdrant instance URI (Default: localhost:6333):\", default=\"localhost:6333\"\n            ).ask()\n            if knowledge_base_storage_uri is None:\n                raise KeyboardInterrupt\n        if qdrant_type == \"local\":\n            knowledge_base_storage_path = os.path.join(typeagent_DIR, \"qdrant\")\n    if knowledge_base_storage_type == \"milvus\":\n        default_milvus_uri= os.path.join(typeagent_DIR, \"milvus.db\")\n        knowledge_base_storage_uri = questionary.text(\n            f\"Enter the Milvus connection URI (Default: {default_milvus_uri}):\", default=default_milvus_uri\n        ).ask()\n        if knowledge_base_storage_uri is None:\n            raise KeyboardInterrupt\n    return knowledge_base_storage_type, knowledge_base_storage_uri, knowledge_base_storage_path\n\n    # TODO: allow configuring embedding model\ndef configure_recall_memory_storage(config: typeagentConfig, credentials: typeagentCredentials):\n    # Configure recall storage backend\n    recall_memory_storage_options = [\"sqlite\", \"postgres\"]\n    recall_memory_storage_type = questionary.select(\n        \"Select storage backend for recall memory data(short-term memory,Time sequence memory,metadata_storage):\", recall_memory_storage_options, default=config.recall_memory_storage_type\n    ).ask()\n    if recall_memory_storage_type is None:\n        raise KeyboardInterrupt\n    recall_memory_storage_uri, recall_memory_storage_path = config.recall_memory_storage_uri, config.recall_memory_storage_path\n    # configure postgres\n    if recall_memory_storage_type == \"postgres\":\n        recall_memory_storage_uri = questionary.text(\n            \"Enter postgres connection string (e.g. postgresql+pg8000://{user}:{password}@{ip}:5432/{database}):\",\n            default=config.recall_memory_storage_uri if config.recall_memory_storage_uri else \"\",\n        ).ask()\n        if recall_memory_storage_uri is None:\n            raise KeyboardInterrupt\n\n    return recall_memory_storage_type, recall_memory_storage_uri, recall_memory_storage_path\n\n\n@app.command()\ndef configure():\n    \"\"\"Updates default typeagent configurations\n\n    This function and quickstart should be the ONLY place where typeagentConfig.save() is called\n    \"\"\"\n\n    # check credentials\n    credentials = typeagentCredentials.load()\n    openai_key = get_openai_credentials()\n\n    typeagentConfig.create_config_dir()\n\n    # Will pre-populate with defaults, or what the user previously set\n    config = typeagentConfig.load()\n    try:\n        model_endpoint_type, model_endpoint = configure_llm_endpoint(\n            config=config,\n            credentials=credentials,\n        )\n        model, model_wrapper, context_window = configure_model(\n            config=config,\n            credentials=credentials,\n            model_endpoint_type=str(model_endpoint_type),\n            model_endpoint=str(model_endpoint),\n        )\n        embedding_endpoint_type, embedding_endpoint, embedding_dim, embedding_model = configure_embedding_endpoint(\n            config=config,\n            credentials=credentials,\n        )\n        archival_memory_storage_type, archival_memory_storage_uri, archival_memory_storage_path = configure_archival_memory_storage(\n            config=config,\n            credentials=credentials,\n        )\n\n        knowledge_base_storage_type,  knowledge_base_storage_uri,  knowledge_base_storage_path = configure_knowledge_base_storage(\n            config=config,\n            credentials=credentials,\n        )\n\n\n        recall_memory_storage_type, recall_memory_storage_uri, recall_memory_storage_path = configure_recall_memory_storage(\n            config=config,\n            credentials=credentials,\n        )\n    except ValueError as e:\n        typer.secho(str(e), fg=typer.colors.RED)\n        return\n\n    # openai key might have gotten added along the way\n    openai_key = credentials.openai_key if credentials.openai_key is not None else openai_key\n\n    # TODO: remove most of this (deplicated with User table)\n    config = typeagentConfig(\n        default_llm_config=LLMConfig(\n            model=model,\n            model_endpoint=model_endpoint,\n            model_endpoint_type=model_endpoint_type,\n            model_wrapper=model_wrapper,\n            context_window=context_window,\n        ),\n        default_embedding_config=EmbeddingConfig(\n            embedding_endpoint_type=embedding_endpoint_type,\n            embedding_endpoint=embedding_endpoint,\n            embedding_dim=embedding_dim,\n            embedding_model=embedding_model,\n        ),\n        # storage\n        archival_memory_storage_type=archival_memory_storage_type,\n        archival_memory_storage_uri=archival_memory_storage_uri,\n        archival_memory_storage_path=archival_memory_storage_path,\n         # knowledge base storage\n        knowledge_base_storage_type=knowledge_base_storage_type,\n        knowledge_base_storage_uri=knowledge_base_storage_uri,\n        knowledge_base_storage_path=knowledge_base_storage_path,\n        # recall storage\n        recall_memory_storage_type=recall_memory_storage_type,\n        recall_memory_storage_uri=recall_memory_storage_uri,\n        recall_memory_storage_path=recall_memory_storage_path,\n        # metadata storage (currently forced to match recall storage)\n        metadata_storage_type=recall_memory_storage_type,\n        metadata_storage_uri=recall_memory_storage_uri,\n        metadata_storage_path=recall_memory_storage_path,\n    )\n\n\n\n    \n    typer.secho(f\"📖 Saving config to {config.config_path}\", fg=typer.colors.GREEN)\n    config.save()\n\n    # # create user records\n    ms = MetadataStore(config)\n    # user_id = uuid.UUID(config.anon_clientid)\n    # user = User(\n    #     id=uuid.UUID(config.anon_clientid),\n    #     user_type=\"admin\"\n    # )\n    # if ms.get_user(user_id=user_id,user_type=\"admin\"):\n    #     # update user\n    #     ms.update_user(user)\n    # else:\n    #     ms.create_user(user)\n\n    # # create preset records in metadata store\n    # from presets.presets import add_default_presets\n\n    # add_default_presets(user_id, ms)\n\n\nclass ListChoice(str, Enum):\n    agents = \"agents\"\n    humans = \"humans\"\n    personas = \"personas\"\n    sources = \"sources\"\n    presets = \"presets\"\n    systemprompt=\"system\"\n    tools=\"tool\"\n\n\n\n@app.command()\ndef list(arg: Annotated[ListChoice, typer.Argument]):\n    config = typeagentConfig.load()\n    ms = MetadataStore(config)\n    user_id = uuid.UUID(config.anon_clientid)\n    table = ColorTable(theme=Themes.OCEAN)\n    if arg == ListChoice.agents:\n        \"\"\"List all agents\"\"\"\n        table.field_names = [\"Name\",\"Type\" ,\"LLM Model\", \"Embedding Model\", \"Preset\",\"Persona\", \"Human\", \"Data Source\", \"Create Time\"]\n        for agent in tqdm(ms.list_agents(user_id=user_id)):\n            source_ids = ms.list_attached_sources(agent_id=agent.id)\n            assert all([source_id is not None and isinstance(source_id, uuid.UUID) for source_id in source_ids])\n            sources = [ms.get_source(source_id=source_id) for source_id in source_ids]\n            assert all([source is not None and isinstance(source, Source)] for source in sources)\n            source_names = [source.name for source in sources if source is not None]\n            table.add_row(\n                [\n                    agent.name,\n                    agent.type_agent,\n                    agent.llm_config.model,\n                    agent.embedding_config.embedding_model,\n                    agent.preset_id,\n                    agent.persona_memory,\n                    agent.human_memory,\n                    \",\".join(source_names),\n                    utils.format_datetime(agent.created_at),\n                ]\n            )\n        print(table)\n    elif arg == ListChoice.humans:\n        \"\"\"List all humans\"\"\"\n        table.field_names = [\"Name\", \"Text\"]\n        for human in ms.list_humans(user_id=user_id):\n            table.add_row([human.name, human.text.replace(\"\\n\", \"\")[:100]])\n        print(table)\n    elif arg == ListChoice.personas:\n        \"\"\"List all personas\"\"\"\n        table.field_names = [\"Name\", \"Text\"]\n        for persona in ms.list_personas(user_id=user_id):\n            table.add_row([persona.name, persona.text.replace(\"\\n\", \"\")[:100]])\n        print(table)\n    elif arg == ListChoice.systemprompt:\n        \"\"\"List all system prompt\"\"\"\n        table.field_names = [\"Name\", \"Text\"]\n        for systempro in ms.list_systemprompt(user_id=user_id):\n            table.add_row([systempro.name, systempro.text.replace(\"\\n\", \"\")[:100]])\n        print(table)\n    elif arg == ListChoice.tools:\n        \"\"\"List all tools\"\"\"\n        table.field_names = [\"Name\", \"Tags\", \"Source Type\"]\n        for toolsf in ms.list_tools(user_id=user_id):\n            table.add_row([toolsf.name,toolsf.tags, toolsf.source_type])\n        print(table)\n    elif arg == ListChoice.sources:\n        \"\"\"List all data sources\"\"\"\n\n        # create table\n        table.field_names = [\"Name\", \"Embedding Model\", \"Created At\", ]\n        # TODO: eventually look accross all storage connections\n        # TODO: add data source stats\n        # TODO: connect to agents\n\n        # get all sources\n        for source in ms.list_sources(user_id=user_id):\n            # # get attached agents\n            # agent_ids = ms.list_attached_agents(source_id=source.id)\n            # agent_states = [ms.get_agent(agent_id=agent_id) for agent_id in agent_ids]\n            # agent_names = [agent_state.name for agent_state in agent_states if agent_state is not None]\n\n            table.add_row(\n                [\n                    source.name,\n                   \n                    source.embedding_model,\n                  \n                    utils.format_datetime(source.created_at),\n                  \n                ]\n            )\n\n        print(table)\n    elif arg == ListChoice.presets:\n        \"\"\"List all available presets\"\"\"\n        table.field_names = [\"Name\", \"System\",\"Human\",\"Persona\", \"Functions\"]\n        for preset in ms.list_presets(user_id=user_id):\n            # sources = ms.get_preset_sources(preset_id=preset.id)\n            table.add_row(\n                [\n                    preset.name,\n                    preset.system_name,\n                    preset.human_name,\n                    preset.persona_name,\n                    # \",\".join([source.name for source in sources]),\n                    # json.dumps(preset.functions_schema, indent=4)\n                    \",\\n\".join([f[\"name\"] for f in preset.functions_schema]),\n                ]\n            )\n        print(table)\n    else:\n        raise ValueError(f\"Unknown argument {arg}\")\n\n\n@app.command()\ndef add(\n    option: str,  # [human, persona] only add,not update\n    name: Annotated[str, typer.Option(help=\"Name of human/persona\")],\n    text: Annotated[Optional[str], typer.Option(help=\"Text of human/persona\")] = None,\n    tags: Annotated[Optional[str], typer.Option(help=\"Tags of tool\")] = None,\n    codetype: Annotated[Optional[str], typer.Option(help=\"type of tool\")] = None,\n    filename: Annotated[Optional[str], typer.Option(help=\"Specify filename\")] = None,\n):\n    \"\"\"Add a person/human systom prommpt ,tool,sources,\"\"\"\n    config = typeagentConfig.load()\n    user_id = uuid.UUID(config.anon_clientid)\n    ms = MetadataStore(config)\n    if filename:  # read from file\n        assert text is None, \"Cannot specify both text and filename\"\n        with open(filename, \"r\") as f:\n            text = f.read()\n    if option == \"persona\":\n        persona = ms.get_persona(name=name, user_id=user_id)\n        if persona:\n             \n            typer.secho(\"persona is exist already,try another name\", fg=typer.colors.RED)\n            #  assert persona is not None, \"persona is exist already,try another name\"\n            #  raise ValueError(\"persona is exist already,try another name\")\n            # # config if user wants to overwrite\n            # if not questionary.confirm(f\"Persona {name} already exists. Overwrite?\").ask():\n            #     return\n            # persona.text = text\n            # ms.update_persona(persona)\n        else:\n            persona = PersonaModel(name=name, text=text, user_id=user_id,user_status=\"on\")\n            ms.add_persona(persona)\n            typer.secho(\"persona is successfuyly created \", fg=typer.colors.GREEN)\n\n    elif option == \"human\":\n        human = ms.get_human(name=name, user_id=user_id)\n        if human:\n            typer.secho(\"human is exist already,try another name\", fg=typer.colors.RED)\n            # config if user wants to overwrite\n            # if not questionary.confirm(f\"Human {name} already exists. Overwrite?\").ask():\n            #     return\n            # human.text = text\n            # ms.update_human(human)\n        else:\n            human = HumanModel(name=name, text=text, user_id=user_id,user_status=\"on\")\n            ms.add_human(human)\n            typer.secho(\"human is successfuyly created \", fg=typer.colors.GREEN)\n    # elif option == \"preset\":\n    #     assert filename, \"Must specify filename for preset\"\n    #     create_preset_from_file(filename, name, user_id, ms)\n    elif option == \"system\":\n        systempro= ms.get_systemprompt(name=name, user_id=user_id)\n        if systempro:\n            typer.secho(\"system is exist already,try another name\", fg=typer.colors.RED)\n            # config if user wants to overwrite\n            # if not questionary.confirm(f\"system prompt {name} already exists. Overwrite?\").ask():\n            #     return\n            # systempro.text = text\n            # ms.update_systemprompt(systempro)\n        else:\n            human =SystemPromptModel(name=name, text=text, user_id=user_id,user_status=\"on\")\n            ms.add_systemprompt(human)\n            typer.secho(\"system is successfuyly created \", fg=typer.colors.GREEN)\n    elif option == \"source\":\n        sources= ms.get_source(source_name=name, user_id=user_id)\n        if sources:\n            typer.secho(\"sources is exist already,try another name\", fg=typer.colors.RED)\n            # config if user wants to overwrite\n            # if not questionary.confirm(f\"sources {name} already exists. Overwrite?\").ask():\n            #     return\n            # sources.embedding_model=config.default_embedding_config.embedding_model\n            # ms.update_source(sources)\n            \n        else:\n            newsource =SourceModel(name=name, embedding_model=config.default_embedding_config.embedding_model, user_id=user_id,user_status=\"on\")\n            ms.create_source(newsource)\n            typer.secho(\"source is successfuyly created \", fg=typer.colors.GREEN)\n    elif option == \"preset\":\n        assert filename, \"Must specify filename for preset\"\n        create_preset_from_file(filename, name, user_id, ms)\n        typer.secho(\"preset is successfuyly created \", fg=typer.colors.GREEN)\n    elif option == \"tool\":\n        assert filename, \"Must specify filename for tool source code\"\n        json_schema=create_functions_schemal(name=name,text=text)\n        tool = ToolModel(name=name,json_schema=json_schema, tags=tags, source_code=text,user_id=user_id,user_status=\"on\",source_type=codetype)\n        ms.add_tool(tool)\n        # create_preset_from_file(filename, name, user_id, ms)\n        typer.secho(\"tool is successfuyly created \", fg=typer.colors.GREEN)\n    else:\n        raise ValueError(f\"Unknown kind {option}\")\n\n\n@app.command()\ndef delete(\n    option: str, \n    name: Annotated[str, typer.Option(help=\"Name\")],):\n    \"\"\"Delete a source from the archival memory.\"\"\"\n\n    config = typeagentConfig.load()\n    user_id = uuid.UUID(config.anon_clientid)\n    ms = MetadataStore(config)\n    assert ms.get_user(user_id=user_id), f\"User {user_id} does not exist\"\n\n    try:\n        # delete from metadata\n        if option == \"source\":\n            # delete metadata\n            source = ms.get_source(source_name=name, user_id=user_id)\n            assert source is not None, f\"Source {name} does not exist\"\n            ms.delete_source(source_id=source.id)\n\n            # delete from passages\n            conn = StorageConnector.get_storage_connector(StorageType.KNOWLEDGE_BASE_PASSAGES, config, user_id=user_id)\n            conn.delete({\"data_source\": name})\n\n            assert (\n                conn.get_all({\"data_source\": name}) == []\n            ), f\"Expected no passages with source {name}, but got {conn.get_all({'data_source': name})}\"\n\n            # TODO: should we also delete from agents?\n        elif option == \"agent\":\n            agent = ms.get_agent(agent_name=name, user_id=user_id)\n            assert agent is not None, f\"Agent {name} for user_id {user_id} does not exist\"\n\n            # # recall memory\n            # recall_conn = StorageConnector.get_storage_connector(StorageType.RECALL_MEMORY, config, user_id=user_id, agent_id=agent.id)\n            # recall_conn.delete({\"agent_id\": agent.id})\n            \n            # archival memory\n            archival_conn = StorageConnector.get_storage_connector(StorageType.ARCHIVAL_MEMORY, config, user_id=user_id, agent_id=agent.id)\n            archival_conn.delete({\"agent_id\": agent.id})\n\n            # metadata\n            ms.delete_agent(agent_id=agent.id)\n\n        elif option == \"human\":\n            human = ms.get_human(name=name, user_id=user_id)\n            assert human is not None, f\"Human {name} does not exist\"\n            ms.delete_human(name=name, user_id=user_id)\n        elif option == \"persona\":\n            persona = ms.get_persona(name=name, user_id=user_id)\n            assert persona is not None, f\"Persona {name} does not exist\"\n            ms.delete_persona(name=name, user_id=user_id)\n            assert ms.get_persona(name=name, user_id=user_id) is None, f\"Persona {name} still exists\"\n        elif option == \"system\":\n            system = ms.get_systemprompt(name=name, user_id=user_id)\n            assert system is not None, f\"system {name} does not exist\"\n            ms.delete_systemprompt(name=name, user_id=user_id)\n            assert ms.get_systemprompt(name=name, user_id=user_id) is None, f\"system {name} still exists\"\n        elif option == \"preset\":\n            preset = ms.get_preset(name=name, user_id=user_id)\n            assert preset is not None, f\"Preset {name} does not exist\"\n            ms.delete_preset(name=name, user_id=user_id)\n        elif option == \"tool\":\n            toolss = ms.get_tool(tool_name=name, user_id=user_id)\n            assert toolss is not None, f\"tool {name} does not exist\"\n            ms.delete_tool(name=name, user_id=user_id)\n        else:\n            raise ValueError(f\"Option {option} not implemented\")\n\n        typer.secho(f\"Deleted {option} '{name}'\", fg=typer.colors.GREEN)\n\n    except Exception as e:\n        typer.secho(f\"Failed to delete {option}'{name}'\\n{e}\", fg=typer.colors.RED)\n"}
{"type": "source_file", "path": "luann/evaluation/evaluation.py", "content": "\n"}
{"type": "source_file", "path": "luann/interface.py", "content": "import json\nimport re\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\n\nfrom colorama import Fore, Style, init\n\nfrom constants import CLI_WARNING_PREFIX, JSON_LOADS_STRICT\nfrom data_types import Message\nfrom utils import printd\n\ninit(autoreset=True)\n\n# DEBUG = True  # puts full message outputs in the terminal\nDEBUG = False  # only dumps important messages in the terminal\n\nSTRIP_UI = False\n\n\nclass AgentInterface(ABC):\n    \"\"\"Interfaces handle typeagent-related events (observer pattern)\n\n    The 'msg' args provides the scoped message, and the optional Message arg can provide additional metadata.\n    \"\"\"\n\n    @abstractmethod\n    def user_message(self, msg: str, msg_obj: Optional[Message] = None):\n        \"\"\"typeagent receives a user message\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def internal_monologue(self, msg: str, msg_obj: Optional[Message] = None):\n        \"\"\"typeagent generates some internal monologue\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def assistant_message(self, msg: str, msg_obj: Optional[Message] = None):\n        \"\"\"typeagent uses send_message\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def function_message(self, msg: str, msg_obj: Optional[Message] = None):\n        \"\"\"typeagent calls a function\"\"\"\n        raise NotImplementedError\n\n    # @abstractmethod\n    # @staticmethod\n    # def print_messages():\n    #     raise NotImplementedError\n\n    # @abstractmethod\n    # @staticmethod\n    # def print_messages_raw():\n    #     raise NotImplementedError\n\n    # @abstractmethod\n    # @staticmethod\n    # def step_yield():\n    #     raise NotImplementedError\n      \n\nclass CLIInterface(AgentInterface):\n    \"\"\"Basic interface for dumping agent events to the command-line\"\"\"\n\n    @staticmethod\n    def important_message(msg: str):\n        fstr = f\"{Fore.MAGENTA}{Style.BRIGHT}{{msg}}{Style.RESET_ALL}\"\n        if STRIP_UI:\n            fstr = \"{msg}\"\n        print(fstr.format(msg=msg))\n\n    @staticmethod\n    def warning_message(msg: str):\n        fstr = f\"{Fore.RED}{Style.BRIGHT}{{msg}}{Style.RESET_ALL}\"\n        if STRIP_UI:\n            fstr = \"{msg}\"\n        else:\n            print(fstr.format(msg=msg))\n\n    @staticmethod\n    def internal_monologue(msg: str, msg_obj: Optional[Message] = None):\n        # ANSI escape code for italic is '\\x1B[3m'\n        fstr = f\"\\x1B[3m{Fore.LIGHTBLACK_EX}💭 {{msg}}{Style.RESET_ALL}\"\n        if STRIP_UI:\n            fstr = \"{msg}\"\n        print(fstr.format(msg=msg))\n\n    @staticmethod\n    def assistant_message(msg: str, msg_obj: Optional[Message] = None):\n        fstr = f\"{Fore.YELLOW}{Style.BRIGHT}🤖 {Fore.YELLOW}{{msg}}{Style.RESET_ALL}\"\n        if STRIP_UI:\n            fstr = \"{msg}\"\n        print(fstr.format(msg=msg))\n\n    @staticmethod\n    def memory_message(msg: str, msg_obj: Optional[Message] = None):\n        fstr = f\"{Fore.LIGHTMAGENTA_EX}{Style.BRIGHT}🧠 {Fore.LIGHTMAGENTA_EX}{{msg}}{Style.RESET_ALL}\"\n        if STRIP_UI:\n            fstr = \"{msg}\"\n        print(fstr.format(msg=msg))\n\n    @staticmethod\n    def system_message(msg: str, msg_obj: Optional[Message] = None):\n        fstr = f\"{Fore.MAGENTA}{Style.BRIGHT}🖥️ [system] {Fore.MAGENTA}{msg}{Style.RESET_ALL}\"\n        if STRIP_UI:\n            fstr = \"{msg}\"\n        print(fstr.format(msg=msg))\n\n    @staticmethod\n    def user_message(msg: str, msg_obj: Optional[Message] = None, raw: bool = False, dump: bool = False, debug: bool = DEBUG):\n        def print_user_message(icon, msg, printf=print):\n            if STRIP_UI:\n                printf(f\"{icon} {msg}\")\n            else:\n                printf(f\"{Fore.GREEN}{Style.BRIGHT}{icon} {Fore.GREEN}{msg}{Style.RESET_ALL}\")\n\n        def printd_user_message(icon, msg):\n            return print_user_message(icon, msg)\n\n        if not (raw or dump or debug):\n            # we do not want to repeat the message in normal use\n            return\n\n        if isinstance(msg, str):\n            if raw:\n                printd_user_message(\"🧑\", msg)\n                return\n            else:\n                try:\n                    msg_json = json.loads(msg, strict=JSON_LOADS_STRICT)\n                except:\n                    printd(f\"{CLI_WARNING_PREFIX}failed to parse user message into json\")\n                    printd_user_message(\"🧑\", msg)\n                    return\n        if msg_json[\"type\"] == \"user_message\":\n            if dump:\n                print_user_message(\"🧑\", msg_json[\"message\"])\n                return\n            msg_json.pop(\"type\")\n            printd_user_message(\"🧑\", msg_json)\n        elif msg_json[\"type\"] == \"heartbeat\":\n            if debug:\n                msg_json.pop(\"type\")\n                printd_user_message(\"💓\", msg_json)\n            elif dump:\n                print_user_message(\"💓\", msg_json)\n                return\n\n        elif msg_json[\"type\"] == \"system_message\":\n            msg_json.pop(\"type\")\n            printd_user_message(\"🖥️\", msg_json)\n        else:\n            printd_user_message(\"🧑\", msg_json)\n\n    @staticmethod\n    def function_message(msg: str, msg_obj: Optional[Message] = None, debug: bool = DEBUG):\n        def print_function_message(icon, msg, color=Fore.RED, printf=print):\n            if STRIP_UI:\n                printf(f\"⚡{icon} [function] {msg}\")\n            else:\n                printf(f\"{color}{Style.BRIGHT}⚡{icon} [function] {color}{msg}{Style.RESET_ALL}\")\n\n        def printd_function_message(icon, msg, color=Fore.RED):\n            return print_function_message(icon, msg, color, printf=(print if debug else printd))\n\n        if isinstance(msg, dict):\n            printd_function_message(\"\", msg)\n            return\n\n        if msg.startswith(\"Success\"):\n            printd_function_message(\"🟢\", msg)\n        elif msg.startswith(\"Error: \"):\n            printd_function_message(\"🔴\", msg)\n        elif msg.startswith(\"Ran \"):\n            # NOTE: ignore 'ran' messages that come post-execution\n            return\n        elif msg.startswith(\"Running \"):\n            if debug:\n                printd_function_message(\"\", msg)\n            else:\n                match = re.search(r\"Running (\\w+)\\((.*)\\)\", msg)\n                if match:\n                    function_name = match.group(1)\n                    function_args = match.group(2)\n                    if function_name in [\"archival_memory_insert\", \"archival_memory_search\", \"core_memory_replace\", \"core_memory_append\"]:\n                        if function_name in [\"archival_memory_insert\", \"core_memory_append\", \"core_memory_replace\"]:\n                            print_function_message(\"🧠\", f\"updating memory with {function_name}\")\n                        elif function_name == \"archival_memory_search\":\n                            print_function_message(\"🧠\", f\"searching memory with {function_name}\")\n                        try:\n                            msg_dict = eval(function_args)\n                            if function_name == \"archival_memory_search\":\n                                output = f'\\tquery: {msg_dict[\"query\"]}, page: {msg_dict[\"page\"]}'\n                                if STRIP_UI:\n                                    print(output)\n                                else:\n                                    print(f\"{Fore.RED}{output}{Style.RESET_ALL}\")\n                            elif function_name == \"archival_memory_insert\":\n                                output = f'\\t→ {msg_dict[\"content\"]}'\n                                if STRIP_UI:\n                                    print(output)\n                                else:\n                                    print(f\"{Style.BRIGHT}{Fore.RED}{output}{Style.RESET_ALL}\")\n                            else:\n                                if STRIP_UI:\n                                    print(f'\\t {msg_dict[\"old_content\"]}\\n\\t→ {msg_dict[\"new_content\"]}')\n                                else:\n                                    print(\n                                        f'{Style.BRIGHT}\\t{Fore.RED} {msg_dict[\"old_content\"]}\\n\\t{Fore.GREEN}→ {msg_dict[\"new_content\"]}{Style.RESET_ALL}'\n                                    )\n                        except Exception as e:\n                            printd(str(e))\n                            printd(msg_dict)\n                    elif function_name in [\"conversation_search\", \"conversation_search_date\"]:\n                        print_function_message(\"🧠\", f\"searching memory with {function_name}\")\n                        try:\n                            msg_dict = eval(function_args)\n                            output = f'\\tquery: {msg_dict[\"query\"]}, page: {msg_dict[\"page\"]}'\n                            if STRIP_UI:\n                                print(output)\n                            else:\n                                print(f\"{Fore.RED}{output}{Style.RESET_ALL}\")\n                        except Exception as e:\n                            printd(str(e))\n                            printd(msg_dict)\n                else:\n                    printd(f\"{CLI_WARNING_PREFIX}did not recognize function message\")\n                    printd_function_message(\"\", msg)\n        else:\n            try:\n                msg_dict = json.loads(msg, strict=JSON_LOADS_STRICT)\n                if \"status\" in msg_dict and msg_dict[\"status\"] == \"OK\":\n                    printd_function_message(\"\", str(msg), color=Fore.GREEN)\n                else:\n                    printd_function_message(\"\", str(msg), color=Fore.RED)\n            except Exception:\n                print(f\"{CLI_WARNING_PREFIX}did not recognize function message {type(msg)} {msg}\")\n                printd_function_message(\"\", msg)\n\n    @staticmethod\n    def print_messages(message_sequence: List[Message], dump=False):\n        # rewrite to dict format\n        message_sequence = [msg.to_openai_dict() for msg in message_sequence]\n\n        idx = len(message_sequence)\n        for msg in message_sequence:\n            if dump:\n                print(f\"[{idx}] \", end=\"\")\n                idx -= 1\n            role = msg[\"role\"]\n            content = msg[\"content\"]\n\n            if role == \"system\":\n                CLIInterface.system_message(content)\n            elif role == \"assistant\":\n                # Differentiate between internal monologue, function calls, and messages\n                if msg.get(\"function_call\"):\n                    if content is not None:\n                        CLIInterface.internal_monologue(content)\n                    # I think the next one is not up to date\n                    # function_message(msg[\"function_call\"])\n                    args = json.loads(msg[\"function_call\"].get(\"arguments\"), strict=JSON_LOADS_STRICT)\n                    CLIInterface.assistant_message(args.get(\"message\"))\n                    # assistant_message(content)\n                elif msg.get(\"tool_calls\"):\n                    if content is not None:\n                        CLIInterface.internal_monologue(content)\n                    function_obj = msg[\"tool_calls\"][0].get(\"function\")\n                    if function_obj:\n                        args = json.loads(function_obj.get(\"arguments\"), strict=JSON_LOADS_STRICT)\n                        CLIInterface.assistant_message(args.get(\"message\"))\n                else:\n                    CLIInterface.internal_monologue(content)\n            elif role == \"user\":\n                CLIInterface.user_message(content, dump=dump)\n            elif role == \"function\":\n                CLIInterface.function_message(content, debug=dump)\n            elif role == \"tool\":\n                CLIInterface.function_message(content, debug=dump)\n            else:\n                print(f\"Unknown role: {content}\")\n\n    @staticmethod\n    def print_messages_simple(message_sequence: List[Message]):\n        # rewrite to dict format\n        message_sequence = [msg.to_openai_dict() for msg in message_sequence]\n\n        for msg in message_sequence:\n            role = msg[\"role\"]\n            content = msg[\"content\"]\n\n            if role == \"system\":\n                CLIInterface.system_message(content)\n            elif role == \"assistant\":\n                CLIInterface.assistant_message(content)\n            elif role == \"user\":\n                CLIInterface.user_message(content, raw=True)\n            else:\n                print(f\"Unknown role: {content}\")\n\n    @staticmethod\n    def print_messages_raw(message_sequence: List[Message]):\n        # rewrite to dict format\n        message_sequence = [msg.to_openai_dict() for msg in message_sequence]\n\n        for msg in message_sequence:\n            print(msg)\n\n    @staticmethod\n    def step_yield():\n        pass\n    @staticmethod\n    def step_complete():\n        pass\n"}
{"type": "source_file", "path": "luann/agent_store/vectorsdb/lancedb.py", "content": "# type: ignore\n\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, Iterator, List, Optional\n\nfrom lancedb.pydantic import LanceModel, Vector\n\nfrom agent_store.storage import StorageConnector, StorageType\nfrom config import AgentConfig, typeagentConfig\nfrom data_types import Message, Passage, Record\n\n\"\"\" Initial implementation - not complete \"\"\"\n\n\ndef get_db_model(table_name: str, table_type: StorageType):\n    config = typeagentConfig.load()\n\n    if table_type == StorageType.ARCHIVAL_MEMORY or table_type == StorageType.PASSAGES:\n        # create schema for archival memory\n        class PassageModel(LanceModel):\n            \"\"\"Defines data model for storing Passages (consisting of text, embedding)\"\"\"\n\n            id: uuid.UUID\n            user_id: str\n            text: str\n            doc_id: str\n            agent_id: str\n            data_source: str\n            embedding: Vector(config.default_embedding_config.embedding_dim)\n            metadata_: Dict\n\n            def __repr__(self):\n                return f\"<Passage(passage_id='{self.id}', text='{self.text}', embedding='{self.embedding})>\"\n\n            def to_record(self):\n                return Passage(\n                    text=self.text,\n                    embedding=self.embedding,\n                    doc_id=self.doc_id,\n                    user_id=self.user_id,\n                    id=self.id,\n                    data_source=self.data_source,\n                    agent_id=self.agent_id,\n                    metadata=self.metadata_,\n                )\n\n        return PassageModel\n    elif table_type == StorageType.RECALL_MEMORY:\n\n        class MessageModel(LanceModel):\n            \"\"\"Defines data model for storing Message objects\"\"\"\n\n            __abstract__ = True  # this line is necessary\n\n            # Assuming message_id is the primary key\n            id: uuid.UUID\n            user_id: str\n            agent_id: str\n\n            # openai info\n            role: str\n            name: str\n            text: str\n            model: str\n            user: str\n\n            # function info\n            function_name: str\n            function_args: str\n            function_response: str\n\n            embedding = Vector(config.default_embedding_config.embedding_dim)\n\n            # Add a datetime column, with default value as the current time\n            created_at = datetime\n\n            def __repr__(self):\n                return f\"<Message(message_id='{self.id}', text='{self.text}', embedding='{self.embedding})>\"\n\n            def to_record(self):\n                return Message(\n                    user_id=self.user_id,\n                    agent_id=self.agent_id,\n                    role=self.role,\n                    name=self.name,\n                    text=self.text,\n                    model=self.model,\n                    function_name=self.function_name,\n                    function_args=self.function_args,\n                    function_response=self.function_response,\n                    embedding=self.embedding,\n                    created_at=self.created_at,\n                    id=self.id,\n                )\n\n        \"\"\"Create database model for table_name\"\"\"\n        return MessageModel\n\n    else:\n        raise ValueError(f\"Table type {table_type} not implemented\")\n\n\nclass LanceDBConnector(StorageConnector):\n    \"\"\"Storage via LanceDB\"\"\"\n\n    # TODO: this should probably eventually be moved into a parent DB class\n\n    def __init__(self, name: Optional[str] = None, agent_config: Optional[AgentConfig] = None):\n        # TODO\n        pass\n\n    def generate_where_filter(self, filters: Dict) -> str:\n        where_filters = []\n        for key, value in filters.items():\n            where_filters.append(f\"{key}={value}\")\n        return where_filters.join(\" AND \")\n\n    @abstractmethod\n    def get_all_paginated(self, filters: Optional[Dict] = {}, page_size: Optional[int] = 1000) -> Iterator[List[Record]]:\n        # TODO\n        pass\n\n    @abstractmethod\n    def get_all(self, filters: Optional[Dict] = {}, limit=10) -> List[Record]:\n        # TODO\n        pass\n\n    @abstractmethod\n    def get(self, id: uuid.UUID) -> Optional[Record]:\n        # TODO\n        pass\n\n    @abstractmethod\n    def size(self, filters: Optional[Dict] = {}) -> int:\n        # TODO\n        pass\n\n    @abstractmethod\n    def insert(self, record: Record):\n        # TODO\n        pass\n\n    @abstractmethod\n    def insert_many(self, records: List[Record], show_progress=False):\n        # TODO\n        pass\n\n    @abstractmethod\n    def query(self, query: str, query_vec: List[float], top_k: int = 10, filters: Optional[Dict] = {}) -> List[Record]:\n        # TODO\n        pass\n\n    @abstractmethod\n    def query_date(self, start_date, end_date):\n        # TODO\n        pass\n\n    @abstractmethod\n    def query_text(self, query):\n        # TODO\n        pass\n\n    @abstractmethod\n    def delete_table(self):\n        # TODO\n        pass\n\n    @abstractmethod\n    def delete(self, filters: Optional[Dict] = {}):\n        # TODO\n        pass\n\n    @abstractmethod\n    def save(self):\n        # TODO\n        pass\n"}
{"type": "source_file", "path": "luann/autogen/__init__.py", "content": ""}
{"type": "source_file", "path": "luann/config.py", "content": "import configparser\nimport inspect\nimport json\nimport os\nimport uuid\nfrom dataclasses import dataclass\n\nimport utils as utils\nfrom constants import DEFAULT_HUMAN, DEFAULT_PERSONA, DEFAULT_PRESET, typeagent_DIR,TYPEAGENT_VERSION,DEFAULT_AGENTTYPE\nfrom data_types import AgentState, EmbeddingConfig, LLMConfig\n# from log import logger\nfrom log import get_logger\nlogger = get_logger(__name__)\n# helper functions for writing to configs\ndef get_field(config, section, field):\n    if section not in config:\n        return None\n    if config.has_option(section, field):\n        return config.get(section, field)\n    else:\n        return None\n\n\ndef set_field(config, section, field, value):\n    if value is None:  # cannot write None\n        return\n    if section not in config:  # create section     \n        config.add_section(section)\n    config.set(section, field, value)\n\n\n@dataclass\nclass typeagentConfig:\n    config_path: str = os.getenv(\"typeagent_CONFIG_PATH\") or os.path.join(typeagent_DIR, \"config\")\n    anon_clientid: str = str(uuid.UUID(int=0))\n\n    # preset\n    preset: str = DEFAULT_PRESET\n\n    # persona parameters\n    persona: str = DEFAULT_PERSONA\n    human: str = DEFAULT_HUMAN\n\n    agenttype: str = DEFAULT_AGENTTYPE\n\n    # model parameters\n    default_llm_config: LLMConfig = None\n\n    # embedding parameters\n    default_embedding_config: EmbeddingConfig = None\n\n\n    # database configs: archival\n    knowledge_base_storage_type: str = \"chroma\"  # local, db\n    knowledge_base_storage_path: str = os.path.join(typeagent_DIR, \"knowledge_base\")\n    knowledge_base_storage_uri: str = None  # TODO: eventually allow external vector DB\n\n\n\n    # database configs: archival\n    archival_memory_storage_type: str = \"chroma\"  # local, db\n    archival_memory_storage_path: str = os.path.join(typeagent_DIR, \"archival_memory_storage\")\n    archival_memory_storage_uri: str = None  # TODO: eventually allow external vector DB\n\n    # database configs: recall\n    recall_memory_storage_type: str = \"sqlite\"  # local, db\n    recall_memory_storage_path: str = typeagent_DIR\n    recall_memory_storage_uri: str = None  # TODO: eventually allow external vector DB\n\n    # database configs: metadata storage (sources, agents, data sources)\n    metadata_storage_type: str = \"sqlite\"\n    metadata_storage_path: str = typeagent_DIR\n    metadata_storage_uri: str = None\n\n    # database configs: agent state\n    persistence_manager_storage_type: str =  \"sqlite\"  # in-memory, db\n    persistence_manager_storage_path: str = typeagent_DIR  # local file\n    persistence_manager_storage_uri: str = None  # db URI\n\n\n\n    # version (for backcompat)\n    typeagent_version: str =TYPEAGENT_VERSION\n\n    # user info\n    # policies_accepted: bool = False\n\n    def __post_init__(self):\n        # ensure types\n        # self.embedding_chunk_size = int(self.embedding_chunk_size)\n        # self.embedding_dim = int(self.embedding_dim)\n        # self.context_window = int(self.context_window)\n        pass\n\n    @staticmethod\n    def generate_uuid() -> str:\n        return uuid.UUID(int=uuid.getnode()).hex\n\n    @classmethod\n    def load(cls) -> \"typeagentConfig\":\n        # avoid circular import\n        from migrate import VERSION_CUTOFF, config_is_compatible\n        from utils import printd\n\n        if not config_is_compatible(allow_empty=True):\n            error_message = \" \".join(\n                [\n                    f\"\\nYour current config file is incompatible with typeagent versions later than {VERSION_CUTOFF}.\",\n                    f\"\\nTo use typeagent, you must either downgrade your typeagent version (<= {VERSION_CUTOFF}) or regenerate your config using `typeagent configure`, or `typeagent migrate` if you would like to migrate old agents.\",\n                ]\n            )\n            raise ValueError(error_message)\n\n        config = configparser.ConfigParser()\n\n        # allow overriding with env variables\n        if os.getenv(\"typeagent_CONFIG_PATH\"):\n            config_path = os.getenv(\"typeagent_CONFIG_PATH\")\n        else:\n            config_path = typeagentConfig.config_path\n\n        # insure all configuration directories exist\n        cls.create_config_dir()\n        printd(f\"Loading config from {config_path}\")\n        if os.path.exists(config_path):\n            # read existing config\n            config.read(config_path)\n\n            # Handle extraction of nested LLMConfig and EmbeddingConfig\n            llm_config_dict = {\n                # Extract relevant LLM configuration from the config file\n                \"model\": get_field(config, \"model\", \"model\"),\n                \"model_endpoint\": get_field(config, \"model\", \"model_endpoint\"),\n                \"model_endpoint_type\": get_field(config, \"model\", \"model_endpoint_type\"),\n                \"model_wrapper\": get_field(config, \"model\", \"model_wrapper\"),\n                \"context_window\": get_field(config, \"model\", \"context_window\"),\n            }\n            embedding_config_dict = {\n                # Extract relevant Embedding configuration from the config file\n                \"embedding_endpoint\": get_field(config, \"embedding\", \"embedding_endpoint\"),\n                \"embedding_model\": get_field(config, \"embedding\", \"embedding_model\"),\n                \"embedding_endpoint_type\": get_field(config, \"embedding\", \"embedding_endpoint_type\"),\n                \"embedding_dim\": get_field(config, \"embedding\", \"embedding_dim\"),\n                \"embedding_chunk_size\": get_field(config, \"embedding\", \"embedding_chunk_size\"),\n            }\n            # Remove null values\n            llm_config_dict = {k: v for k, v in llm_config_dict.items() if v is not None}\n            embedding_config_dict = {k: v for k, v in embedding_config_dict.items() if v is not None}\n            # Correct the types that aren't strings\n            if llm_config_dict[\"context_window\"] is not None:\n                llm_config_dict[\"context_window\"] = int(llm_config_dict[\"context_window\"])\n            if embedding_config_dict[\"embedding_dim\"] is not None:\n                embedding_config_dict[\"embedding_dim\"] = int(embedding_config_dict[\"embedding_dim\"])\n            if embedding_config_dict[\"embedding_chunk_size\"] is not None:\n                embedding_config_dict[\"embedding_chunk_size\"] = int(embedding_config_dict[\"embedding_chunk_size\"])\n            # Construct the inner properties\n            llm_config = LLMConfig(**llm_config_dict)\n            embedding_config = EmbeddingConfig(**embedding_config_dict)\n\n            # Everything else\n            config_dict = {\n                # Two prepared configs\n                \"default_llm_config\": llm_config,\n                \"default_embedding_config\": embedding_config,\n                # Agent related\n                \"preset\": get_field(config, \"defaults\", \"preset\"),\n                \"persona\": get_field(config, \"defaults\", \"persona\"),\n                \"human\": get_field(config, \"defaults\", \"human\"),\n                \"agenttype\": get_field(config, \"defaults\", \"agenttype\"),\n                \"agent\": get_field(config, \"defaults\", \"agent\"),\n                # Storage related\n                \"archival_memory_storage_type\": get_field(config, \"archival_memory_storage\", \"type\"),\n                \"archival_memory_storage_path\": get_field(config, \"archival_memory_storage\", \"path\"),\n                \"archival_memory_storage_uri\": get_field(config, \"archival_memory_storage\", \"uri\"),\n                \"knowledge_base_storage_type\": get_field(config, \"knowledge_base_storage\", \"type\"),\n                \"knowledge_base_storage_path\": get_field(config, \"knowledge_base_storage\", \"path\"),\n                \"knowledge_base_storage_uri\": get_field(config, \"knowledge_base_storage\", \"uri\"),\n                \"recall_memory_storage_type\": get_field(config, \"recall_memory_storage\", \"type\"),\n                \"recall_memory_storage_path\": get_field(config, \"recall_memory_storage\", \"path\"),\n                \"recall_memory_storage_uri\": get_field(config, \"recall_memory_storage\", \"uri\"),\n                \"metadata_storage_type\": get_field(config, \"metadata_storage\", \"type\"),\n                \"metadata_storage_path\": get_field(config, \"metadata_storage\", \"path\"),\n                \"metadata_storage_uri\": get_field(config, \"metadata_storage\", \"uri\"),\n                # Misc\n                \"anon_clientid\": get_field(config, \"admin client\", \"anon_clientid\"),\n                \"config_path\": config_path,\n                \"typeagent_version\": get_field(config, \"version\", \"typeagent_version\"),\n            }\n\n            # Don't include null values\n            config_dict = {k: v for k, v in config_dict.items() if v is not None}\n\n            return cls(**config_dict)\n\n        # create new config\n        anon_clientid = typeagentConfig.generate_uuid()\n        config = cls(anon_clientid=anon_clientid, config_path=config_path)\n        config.create_config_dir()  # create dirs\n\n        return config\n\n    def save(self):\n       \n\n        config = configparser.ConfigParser()\n        # CLI defaults\n        set_field(config, \"defaults\", \"preset\", self.preset)\n        set_field(config, \"defaults\", \"persona\", self.persona)\n        set_field(config, \"defaults\", \"human\", self.human)\n        set_field(config, \"defaults\", \"agenttype\", self.agenttype)\n\n        # model defaults\n        set_field(config, \"model\", \"model\", self.default_llm_config.model)\n        set_field(config, \"model\", \"model_endpoint\", self.default_llm_config.model_endpoint)\n        set_field(\n            config,\n            \"model\",\n            \"model_endpoint_type\",\n            self.default_llm_config.model_endpoint_type,\n        )\n        set_field(config, \"model\", \"model_wrapper\", self.default_llm_config.model_wrapper)\n        set_field(\n            config,\n            \"model\",\n            \"context_window\",\n            str(self.default_llm_config.context_window),\n        )\n\n        # embeddings\n        set_field(\n            config,\n            \"embedding\",\n            \"embedding_endpoint_type\",\n            self.default_embedding_config.embedding_endpoint_type,\n        )\n        set_field(\n            config,\n            \"embedding\",\n            \"embedding_endpoint\",\n            self.default_embedding_config.embedding_endpoint,\n        )\n        set_field(\n            config,\n            \"embedding\",\n            \"embedding_model\",\n            self.default_embedding_config.embedding_model,\n        )\n        set_field(\n            config,\n            \"embedding\",\n            \"embedding_dim\",\n            str(self.default_embedding_config.embedding_dim),\n        )\n        set_field(\n            config,\n            \"embedding\",\n            \"embedding_chunk_size\",\n            str(self.default_embedding_config.embedding_chunk_size),\n        )\n\n        # archival storage\n        set_field(config, \"archival_memory_storage\", \"type\", self.archival_memory_storage_type)\n        set_field(config, \"archival_memory_storage\", \"path\", self.archival_memory_storage_path)\n        set_field(config, \"archival_memory_storage\", \"uri\", self.archival_memory_storage_uri)\n\n\n        # archival knowledge base storage\n        set_field(config, \"knowledge_base_storage\", \"type\", self.knowledge_base_storage_type)\n        set_field(config, \"knowledge_base_storage\", \"path\", self.knowledge_base_storage_path)\n        set_field(config, \"knowledge_base_storage\", \"uri\", self.knowledge_base_storage_uri)\n\n\n        # recall storage\n        set_field(config, \"recall_memory_storage\", \"type\", self.recall_memory_storage_type)\n        set_field(config, \"recall_memory_storage\", \"path\", self.recall_memory_storage_path)\n        set_field(config, \"recall_memory_storage\", \"uri\", self.recall_memory_storage_uri)\n\n        # metadata storage\n        set_field(config, \"metadata_storage\", \"type\", self.metadata_storage_type)\n        set_field(config, \"metadata_storage\", \"path\", self.metadata_storage_path)\n        set_field(config, \"metadata_storage\", \"uri\", self.metadata_storage_uri)\n\n        # set version\n        set_field(config, \"version\", \"typeagent_version\", TYPEAGENT_VERSION)\n\n        # client\n        if not self.anon_clientid:\n            self.anon_clientid = self.generate_uuid()\n        set_field(config, \"admin client\", \"anon_clientid\", self.anon_clientid)\n\n        # always make sure all directories are present\n        self.create_config_dir()\n\n        with open(self.config_path, \"w\", encoding=\"utf-8\") as f:\n            config.write(f)\n        logger.debug(f\"Saved Config:  {self.config_path}\")\n\n    @staticmethod\n    def exists():\n        # allow overriding with env variables\n        if os.getenv(\"typeagent_CONFIG_PATH\"):\n            config_path = os.getenv(\"typeagent_CONFIG_PATH\")\n        else:\n            config_path = typeagentConfig.config_path\n\n        assert not os.path.isdir(config_path), f\"Config path {config_path} cannot be set to a directory.\"\n        return os.path.exists(config_path)\n\n    @staticmethod\n    def create_config_dir():\n        if not os.path.exists(typeagent_DIR):\n            os.makedirs(typeagent_DIR, exist_ok=True)\n\n        folders = [\n            \"personas\",\n            \"humans\",\n            \"archival_memory_storage\",\n            \"knowledge_base\",\n            \"agents\",\n            \"functions\",\n            \"system_prompts\",\n            \"presets\",\n            \"settings\",\n            # \"file_OpenDevin_Planer\"\n        ]\n\n        for folder in folders:\n            if not os.path.exists(os.path.join(typeagent_DIR, folder)):\n                os.makedirs(os.path.join(typeagent_DIR, folder))\n\n\n@dataclass\nclass AgentConfig:\n    \"\"\"\n\n    NOTE: this is a deprecated class, use AgentState instead. This class is only used for backcompatibility.\n    Configuration for a specific instance of an agent\n    \"\"\"\n\n    def __init__(\n        self,\n        persona,\n        human,\n        # model info\n        model=None,\n        model_endpoint_type=None,\n        model_endpoint=None,\n        model_wrapper=None,\n        context_window=None,\n        # embedding info\n        embedding_endpoint_type=None,\n        embedding_endpoint=None,\n        embedding_model=None,\n        embedding_dim=None,\n        embedding_chunk_size=None,\n        # other\n        preset=None,\n        data_sources=None,\n        # agent info\n        agent_config_path=None,\n        name=None,\n        create_time=None,\n        typeagent_version=None,\n        # functions\n        functions=None,  # schema definitions ONLY (linked at runtime)\n    ):\n\n        assert name, f\"Agent name must be provided\"\n        self.name = name\n\n        config = typeagentConfig.load()  # get default values\n        self.persona = config.persona if persona is None else persona\n        self.human = config.human if human is None else human\n        self.preset = config.preset if preset is None else preset\n        self.context_window = config.default_llm_config.context_window if context_window is None else context_window\n        self.model = config.default_llm_config.model if model is None else model\n        self.model_endpoint_type = config.default_llm_config.model_endpoint_type if model_endpoint_type is None else model_endpoint_type\n        self.model_endpoint = config.default_llm_config.model_endpoint if model_endpoint is None else model_endpoint\n        self.model_wrapper = config.default_llm_config.model_wrapper if model_wrapper is None else model_wrapper\n        self.llm_config = LLMConfig(\n            model=self.model,\n            model_endpoint_type=self.model_endpoint_type,\n            model_endpoint=self.model_endpoint,\n            model_wrapper=self.model_wrapper,\n            context_window=self.context_window,\n        )\n        self.embedding_endpoint_type = (\n            config.default_embedding_config.embedding_endpoint_type if embedding_endpoint_type is None else embedding_endpoint_type\n        )\n        self.embedding_endpoint = config.default_embedding_config.embedding_endpoint if embedding_endpoint is None else embedding_endpoint\n        self.embedding_model = config.default_embedding_config.embedding_model if embedding_model is None else embedding_model\n        self.embedding_dim = config.default_embedding_config.embedding_dim if embedding_dim is None else embedding_dim\n        self.embedding_chunk_size = (\n            config.default_embedding_config.embedding_chunk_size if embedding_chunk_size is None else embedding_chunk_size\n        )\n        self.embedding_config = EmbeddingConfig(\n            embedding_endpoint_type=self.embedding_endpoint_type,\n            embedding_endpoint=self.embedding_endpoint,\n            embedding_model=self.embedding_model,\n            embedding_dim=self.embedding_dim,\n            embedding_chunk_size=self.embedding_chunk_size,\n        )\n\n        # agent metadata\n        self.data_sources = data_sources if data_sources is not None else []\n        self.create_time = create_time if create_time is not None else utils.get_local_time()\n        if typeagent_version is None:\n            \n\n            self.typeagent_version = TYPEAGENT_VERSION\n        else:\n            self.typeagent_version = typeagent_version\n\n        # functions\n        self.functions = functions\n\n        # save agent config\n        self.agent_config_path = (\n            os.path.join(typeagent_DIR, \"agents\", self.name, \"config.json\") if agent_config_path is None else agent_config_path\n        )\n\n    def attach_data_source(self, data_source: str):\n        # TODO: add warning that only once source can be attached\n        # i.e. previous source will be overriden\n        self.data_sources.append(data_source)\n        self.save()\n\n    def save_dir(self):\n        return os.path.join(typeagent_DIR, \"agents\", self.name)\n\n    def save_state_dir(self):\n        # directory to save agent state\n        return os.path.join(typeagent_DIR, \"agents\", self.name, \"agent_state\")\n\n    def save_persistence_manager_dir(self):\n        # directory to save persistent manager state\n        return os.path.join(typeagent_DIR, \"agents\", self.name, \"persistence_manager\")\n\n    def save_agent_index_dir(self):\n        # save llama index inside of persistent manager directory\n        return os.path.join(self.save_persistence_manager_dir(), \"index\")\n\n    def save(self):\n        # save state of persistence manager\n        os.makedirs(os.path.join(typeagent_DIR, \"agents\", self.name), exist_ok=True)\n        # save version\n        self.typeagent_version = TYPEAGENT_VERSION\n        with open(self.agent_config_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(vars(self), f, indent=4)\n\n    def to_agent_state(self):\n        return AgentState(\n            name=self.name,\n            preset=self.preset,\n            persona=self.persona,\n            human=self.human,\n            llm_config=self.llm_config,\n            embedding_config=self.embedding_config,\n            create_time=self.create_time,\n        )\n\n    @staticmethod\n    def exists(name: str):\n        \"\"\"Check if agent config exists\"\"\"\n        agent_config_path = os.path.join(typeagent_DIR, \"agents\", name)\n        return os.path.exists(agent_config_path)\n\n    @classmethod\n    def load(cls, name: str):\n        \"\"\"Load agent config from JSON file\"\"\"\n        agent_config_path = os.path.join(typeagent_DIR, \"agents\", name, \"config.json\")\n        assert os.path.exists(agent_config_path), f\"Agent config file does not exist at {agent_config_path}\"\n        with open(agent_config_path, \"r\", encoding=\"utf-8\") as f:\n            agent_config = json.load(f)\n        # allow compatibility accross versions\n        try:\n            class_args = inspect.getargspec(cls.__init__).args\n        except AttributeError:\n            # https://github.com/pytorch/pytorch/issues/15344\n            class_args = inspect.getfullargspec(cls.__init__).args\n        agent_fields = list(agent_config.keys())\n        for key in agent_fields:\n            if key not in class_args:\n                utils.printd(f\"Removing missing argument {key} from agent config\")\n                del agent_config[key]\n        return cls(**agent_config)\n"}
{"type": "source_file", "path": "luann/data_sources/connectors.py", "content": "from typing import Dict, Iterator, List, Optional, Tuple\n\nimport typer\n# from llama_index.core import Document as LlamaIndexDocument\n\nfrom agent_store.storage import StorageConnector\nfrom data_types import Document, EmbeddingConfig, Passage, Source\nfrom embeddings import embedding_model\nfrom utils import create_uuid_from_string\nfrom models.pydantic_models import (\n    DocumentModel,\n    JobModel,\n    JobStatus,\n    PassageModel,\n    SourceModel,\n)\nfrom data_sources.directory import DirectoryLoader\nfrom data_sources.text_splitters.basetextsplitter import TokenTextSplitter\nclass DataConnector:\n    def generate_documents(self) -> Iterator[Tuple[str, Dict]]:  # -> Iterator[Document]:\n        pass\n\n    def generate_passages(self, documents: List[Document], chunk_size: int = 1024) -> Iterator[Tuple[str, Dict]]:  # -> Iterator[Passage]:\n        pass\n\n\ndef load_data(\n    connector: DataConnector,\n    source:SourceModel,\n    embedding_config: EmbeddingConfig,\n    passage_store: StorageConnector,\n    document_store: Optional[StorageConnector] = None,\n):\n    \"\"\"Load data from a connector (generates documents and passages) into a specified source_id, associatedw with a user_id.\"\"\"\n    # assert (\n    #     source.embedding_model == embedding_config.embedding_model\n    # ), f\"Source and embedding config models must match, got: {source.embedding_model} and {embedding_config.embedding_model}\"\n    # assert (\n    #     source.embedding_dim == embedding_config.embedding_dim\n    # ), f\"Source and embedding config dimensions must match, got: {source.embedding_dim} and {embedding_config.embedding_dim}.\"\n\n    # embedding model\n    # embed_model = embedding_model(embedding_config)\n    # user_id: uuid.UUID = Field(..., description=\"The unique identifier of the user associated with the document.\")\n    # text: str = Field(..., description=\"The text of the document.\")\n    # source_name: str = Field(..., description=\"The data source of the document.\")\n    # source_id: uuid.UUID = Field(..., description=\"The unique identifier of the user associated with the document.\")\n    # id: uuid.UUID = Field(default_factory=uuid.uuid4, description=\"The unique identifier of the document.\", primary_key=True)\n    # metadata: Optional[Dict] = Field({}, description=\"The metadata of the document.\")\n    # insert passages/documents\n    passages = []\n    embedding_to_document_name = {}\n    passage_count = 0\n    document_count = 0\n    for document_text, document_metadata in connector.generate_documents():\n        # insert document into storage\n        documentone = DocumentModel(\n            id=create_uuid_from_string(f\"{str(source.id)}_{document_text}\"),\n            text=document_text,\n            metadata_=document_metadata,\n            source_name=source.name,\n            user_id=source.user_id,\n            source_id=source.id,\n        )\n        document_count += 1\n        # print(\"document:\")\n        # print(str(document.id))\n        # print(document.text)\n        # print(document.metadata)\n        # print(document.data_source)\n\n        # print(str(document.user_id))\n        # # print(document.text)\n\n\n        # print(document_count)\n\n        if document_store:\n            # pass\n            document_store.insert(documentone)\n\n    #     # generate passages\n    #     for passage_text, passage_metadata in connector.generate_passages([document], chunk_size=embedding_config.embedding_chunk_size):\n\n    #         # for some reason, llama index parsers sometimes return empty strings\n    #         if len(passage_text) == 0:\n    #             typer.secho(\n    #                 f\"Warning: Llama index parser returned empty string, skipping insert of passage with metadata '{passage_metadata}' into VectorDB. You can usually ignore this warning.\",\n    #                 fg=typer.colors.YELLOW,\n    #             )\n    #             continue\n\n    #         # get embedding\n    #         try:\n    #             embedding = embed_model.get_text_embedding(passage_text)\n    #         except Exception as e:\n    #             typer.secho(\n    #                 f\"Warning: Failed to get embedding for {passage_text} (error: {str(e)}), skipping insert into VectorDB.\",\n    #                 fg=typer.colors.YELLOW,\n    #             )\n    #             continue\n\n    #         passage = Passage(\n    #             id=create_uuid_from_string(f\"{str(source.id)}_{passage_text}\"),\n    #             text=passage_text,\n    #             doc_id=document.id,\n    #             metadata_=passage_metadata,\n    #             user_id=source.user_id,\n    #             data_source=source.name,\n    #             embedding_dim=source.embedding_dim,\n    #             embedding_model=source.embedding_model,\n    #             embedding=embedding,\n    #         )\n\n    #         hashable_embedding = tuple(passage.embedding)\n    #         document_name = document.metadata.get(\"file_path\", document.id)\n    #         if hashable_embedding in embedding_to_document_name:\n    #             typer.secho(\n    #                 f\"Warning: Duplicate embedding found for passage in {document_name} (already exists in {embedding_to_document_name[hashable_embedding]}), skipping insert into VectorDB.\",\n    #                 fg=typer.colors.YELLOW,\n    #             )\n    #             continue\n\n    #         passages.append(passage)\n    #         embedding_to_document_name[hashable_embedding] = document_name\n    #         if len(passages) >= 100:\n    #             # insert passages into passage store\n    #             passage_store.insert_many(passages)\n\n    #             passage_count += len(passages)\n    #             passages = []\n\n    # if len(passages) > 0:\n    #     # insert passages into passage store\n    #     passage_store.insert_many(passages)\n    #     passage_count += len(passages)\n\n    return passage_count, document_count\n\n\nclass DirectoryConnector(DataConnector):\n    def __init__(self, input_files: List[str] = None, input_directory: str = None, recursive: bool = False, extensions: List[str] = None):\n        self.connector_type = \"directory\"\n        # self.input_files = input_files\n        self.input_directory = input_directory\n        # self.recursive = recursive\n        # self.extensions = extensions\n\n        # if self.recursive == True:\n        #     assert self.input_directory is not None, \"Must provide input directory if recursive is True.\"\n\n    def generate_documents(self) -> Iterator[Tuple[str, Dict]]:  # -> Iterator[Document]:\n        # from llama_index.core import SimpleDirectoryReader\n        loader = DirectoryLoader( path=self.input_directory,loader_kwargs={\"mode\":\"paged\"})\n        load_docs=loader.load()\n\n        # if self.input_directory is not None:\n        #     reader = SimpleDirectoryReader(\n        #         input_dir=self.input_directory,\n        #         recursive=self.recursive,\n        #         required_exts=[ext.strip() for ext in str(self.extensions).split(\",\")],\n        #     )\n        # else:\n        #     assert self.input_files is not None, \"Must provide input files if input_dir is None\"\n        #     reader = SimpleDirectoryReader(input_files=[str(f) for f in self.input_files])\n\n        # llama_index_docs = reader.load_data(show_progress=True)\n        for load_doc in load_docs:\n            # TODO: add additional metadata?\n            # doc = Document(text=llama_index_doc.text, metadata=llama_index_doc.metadata)\n            # docs.append(doc)\n            yield load_doc.text, load_doc.metadata\n\n    def generate_passages(self, documents: List[Document], chunk_size: int = 1024) -> Iterator[Tuple[str, Dict]]:  # -> Iterator[Passage]:\n        # use llama index to run embeddings code\n        # from llama_index.core.node_parser import SentenceSplitter\n        # from llama_index.core.node_parser import TokenTextSplitter\n\n        parser = TokenTextSplitter(chunk_size=chunk_size)\n        for document in documents:\n            passages_from_docs=parser.split_text(text=document.text)\n            # llama_index_docs = [LlamaIndexDocument(text=document.text, metadata=document.metadata)]\n            # nodes = parser.get_nodes_from_documents(llama_index_docs)\n            for nodedocs in passages_from_docs:\n                # passage = Passage(\n                #    text=node.text,\n                #    doc_id=document.id,\n                # )\n                yield nodedocs, None\n\n\n# class WebConnector(DirectoryConnector):\n#     def __init__(self, urls: List[str] = None, html_to_text: bool = True):\n#         self.urls = urls\n#         self.html_to_text = html_to_text\n\n#     def generate_documents(self) -> Iterator[Tuple[str, Dict]]:  # -> Iterator[Document]:\n#         from llama_index.readers.web import SimpleWebPageReader\n\n#         documents = SimpleWebPageReader(html_to_text=self.html_to_text).load_data(self.urls)\n#         for document in documents:\n#             yield document.text, {\"url\": document.id_}\n\n\n# class VectorDBConnector(DataConnector):\n#     # NOTE: this class has not been properly tested, so is unlikely to work\n#     # TODO: allow loading multiple tables (1:1 mapping between Document and Table)\n\n#     def __init__(\n#         self,\n#         name: str,\n#         uri: str,\n#         table_name: str,\n#         text_column: str,\n#         embedding_column: str,\n#         embedding_dim: int,\n#     ):\n#         self.name = name\n#         self.uri = uri\n#         self.table_name = table_name\n#         self.text_column = text_column\n#         self.embedding_column = embedding_column\n#         self.embedding_dim = embedding_dim\n\n#         # connect to db table\n#         from sqlalchemy import create_engine\n\n#         self.engine = create_engine(uri)\n\n#     def generate_documents(self) -> Iterator[Tuple[str, Dict]]:  # -> Iterator[Document]:\n#         yield self.table_name, None\n\n#     def generate_passages(self, documents: List[Document], chunk_size: int = 1024) -> Iterator[Tuple[str, Dict]]:  # -> Iterator[Passage]:\n#         from pgvector.sqlalchemy import Vector\n#         from sqlalchemy import Inspector, MetaData, Table, select\n\n#         metadata = MetaData()\n#         # Create an inspector to inspect the database\n#         inspector = Inspector.from_engine(self.engine)\n#         table_names = inspector.get_table_names()\n#         assert self.table_name in table_names, f\"Table {self.table_name} not found in database: tables that exist {table_names}.\"\n\n#         table = Table(self.table_name, metadata, autoload_with=self.engine)\n\n#         # Prepare a select statement\n#         select_statement = select(table.c[self.text_column], table.c[self.embedding_column].cast(Vector(self.embedding_dim)))\n\n#         # Execute the query and fetch the results\n#         # TODO: paginate results\n#         with self.engine.connect() as connection:\n#             result = connection.execute(select_statement).fetchall()\n\n#         for text, embedding in result:\n#             # assume that embeddings are the same model as in config\n#             # TODO: don't re-compute embedding\n#             yield text, {\"embedding\": embedding}\n"}
{"type": "source_file", "path": "luann/functions/schema_generator.py", "content": "import inspect\nimport typing\nfrom typing import Optional,get_args, get_origin\n\nfrom docstring_parser import parse\nfrom pydantic import BaseModel\n\nfrom constants import (\n    FUNCTION_PARAM_DESCRIPTION_REQ_HEARTBEAT,\n    FUNCTION_PARAM_NAME_REQ_HEARTBEAT,\n    FUNCTION_PARAM_TYPE_REQ_HEARTBEAT,\n)\n\nNO_HEARTBEAT_FUNCTIONS = [\"send_message\", \"pause_heartbeats\"]\n\n\ndef is_optional(annotation):\n    # Check if the annotation is a Union\n    if getattr(annotation, \"__origin__\", None) is typing.Union:\n        # Check if None is one of the options in the Union\n        return type(None) in annotation.__args__\n    return False\n\n\ndef optional_length(annotation):\n    if is_optional(annotation):\n        # Subtract 1 to account for NoneType\n        return len(annotation.__args__) - 1\n    else:\n        raise ValueError(\"The annotation is not an Optional type\")\n\n\ndef type_to_json_schema_type(py_type):\n    \"\"\"\n    Maps a Python type to a JSON schema type.\n    Specifically handles typing.Optional and common Python types.\n    \"\"\"\n    # if get_origin(py_type) is typing.Optional:\n    if is_optional(py_type):\n        # Assert that Optional has only one type argument\n        type_args = get_args(py_type)\n        assert optional_length(py_type) == 1, f\"Optional type must have exactly one type argument, but got {py_type}\"\n\n        # Extract and map the inner type\n        return type_to_json_schema_type(type_args[0])\n\n    # Mapping of Python types to JSON schema types\n    type_map = {\n        int: \"integer\",\n        str: \"string\",\n        bool: \"boolean\",\n        float: \"number\",\n        list[str]: \"array\",\n        # Add more mappings as needed\n    }\n    if py_type not in type_map:\n        raise ValueError(f\"Python type {py_type} has no corresponding JSON schema type\")\n\n    return type_map.get(py_type, \"string\")  # Default to \"string\" if type not in map\n\n\ndef pydantic_model_to_open_ai(model):\n    schema = model.model_json_schema()\n    docstring = parse(model.__doc__ or \"\")\n    parameters = {k: v for k, v in schema.items() if k not in (\"title\", \"description\")}\n    for param in docstring.params:\n        if (name := param.arg_name) in parameters[\"properties\"] and (description := param.description):\n            if \"description\" not in parameters[\"properties\"][name]:\n                parameters[\"properties\"][name][\"description\"] = description\n\n    parameters[\"required\"] = sorted(k for k, v in parameters[\"properties\"].items() if \"default\" not in v)\n\n    if \"description\" not in schema:\n        if docstring.short_description:\n            schema[\"description\"] = docstring.short_description\n        else:\n            raise\n\n    return {\n        \"name\": schema[\"title\"],\n        \"description\": schema[\"description\"],\n        \"parameters\": parameters,\n    }\n\n\ndef generate_schema(function, name: Optional[str] = None, description: Optional[str] = None):\n    # Get the signature of the function\n    sig = inspect.signature(function)\n\n    # Parse the docstring\n    docstring = parse(function.__doc__)\n\n    # Prepare the schema dictionary\n    schema = {\n        # \"name\": function.__name__,\n        # \"description\": docstring.short_description,\n        \"name\": function.__name__ if name is None else name,\n        \"description\": docstring.short_description if description is None else description,\n        \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []},\n    }\n\n    for param in sig.parameters.values():\n        # Exclude 'self' parameter\n        if param.name == \"self\":\n            continue\n\n        # Assert that the parameter has a type annotation\n        if param.annotation == inspect.Parameter.empty:\n            raise TypeError(f\"Parameter '{param.name}' in function '{function.__name__}' lacks a type annotation\")\n\n        # Find the parameter's description in the docstring\n        param_doc = next((d for d in docstring.params if d.arg_name == param.name), None)\n\n        # Assert that the parameter has a description\n        if not param_doc or not param_doc.description:\n            raise ValueError(f\"Parameter '{param.name}' in function '{function.__name__}' lacks a description in the docstring\")\n\n        if inspect.isclass(param.annotation) and issubclass(param.annotation, BaseModel):\n            schema[\"parameters\"][\"properties\"][param.name] = pydantic_model_to_open_ai(param.annotation)\n        else:\n            # Add parameter details to the schema\n            param_doc = next((d for d in docstring.params if d.arg_name == param.name), None)\n            schema[\"parameters\"][\"properties\"][param.name] = {\n                # \"type\": \"string\" if param.annotation == str else str(param.annotation),\n                \"type\": type_to_json_schema_type(param.annotation) if param.annotation != inspect.Parameter.empty else \"string\",\n                \"description\": param_doc.description,\n            }\n        if param.default == inspect.Parameter.empty:\n            schema[\"parameters\"][\"required\"].append(param.name)\n\n        if get_origin(param.annotation) is list:\n            if get_args(param.annotation)[0] is str:\n                schema[\"parameters\"][\"properties\"][param.name][\"items\"] = {\"type\": \"string\"}\n\n        if param.annotation == inspect.Parameter.empty:\n            schema[\"parameters\"][\"required\"].append(param.name)\n\n    # append the heartbeat\n    if function.__name__ not in NO_HEARTBEAT_FUNCTIONS:\n        schema[\"parameters\"][\"properties\"][FUNCTION_PARAM_NAME_REQ_HEARTBEAT] = {\n            \"type\": FUNCTION_PARAM_TYPE_REQ_HEARTBEAT,\n            \"description\": FUNCTION_PARAM_DESCRIPTION_REQ_HEARTBEAT,\n        }\n        schema[\"parameters\"][\"required\"].append(FUNCTION_PARAM_NAME_REQ_HEARTBEAT)\n\n    return schema\n"}
{"type": "source_file", "path": "luann/data_sources/unstructured.py", "content": "\"\"\"Loader that uses unstructured to load files.\"\"\"\nimport collections\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import IO, Any, Callable, Dict, Iterator, List, Optional, Sequence, Union\nfrom data_types import Document\n\nfrom models.pydantic_models import (\nDocumentModel,\n)\n\n\n\ndef satisfies_min_unstructured_version(min_version: str) -> bool:\n    \"\"\"Check if the installed `Unstructured` version exceeds the minimum version\n    for the feature in question.\"\"\"\n    from unstructured.__version__ import __version__ as __unstructured_version__\n\n    min_version_tuple = tuple([int(x) for x in min_version.split(\".\")])\n\n    # NOTE(MthwRobinson) - enables the loader to work when you're using pre-release\n    # versions of unstructured like 0.4.17-dev1\n    _unstructured_version = __unstructured_version__.split(\"-\")[0]\n    unstructured_version_tuple = tuple(\n        [int(x) for x in _unstructured_version.split(\".\")]\n    )\n\n    return unstructured_version_tuple >= min_version_tuple\n\n\ndef validate_unstructured_version(min_unstructured_version: str) -> None:\n    \"\"\"Raise an error if the `Unstructured` version does not exceed the\n    specified minimum.\"\"\"\n    if not satisfies_min_unstructured_version(min_unstructured_version):\n        raise ValueError(\n            f\"unstructured>={min_unstructured_version} is required in this loader.\"\n        )\n\n\nclass UnstructuredBaseLoader(ABC):\n    \"\"\"Base Loader that uses `Unstructured`.\"\"\"\n\n    def __init__(\n        self,\n        mode: str = \"single\",\n        post_processors: Optional[List[Callable]] = None,\n        **unstructured_kwargs: Any,\n    ):\n        \"\"\"Initialize with file path.\"\"\"\n        try:\n            import unstructured  # noqa:F401\n        except ImportError:\n            raise ImportError(\n                \"unstructured package not found, please install it with \"\n                \"`pip install unstructured`\"\n            )\n        _valid_modes = {\"single\", \"elements\", \"paged\"}\n        if mode not in _valid_modes:\n            raise ValueError(\n                f\"Got {mode} for `mode`, but should be one of `{_valid_modes}`\"\n            )\n        self.mode = mode\n\n        if not satisfies_min_unstructured_version(\"0.5.4\"):\n            if \"strategy\" in unstructured_kwargs:\n                unstructured_kwargs.pop(\"strategy\")\n\n        self.unstructured_kwargs = unstructured_kwargs\n        self.post_processors = post_processors or []\n\n    @abstractmethod\n    def _get_elements(self) -> List:\n        \"\"\"Get elements.\"\"\"\n\n    @abstractmethod\n    def _get_metadata(self) -> dict:\n        \"\"\"Get metadata.\"\"\"\n\n    def _post_process_elements(self, elements: list) -> list:\n        \"\"\"Applies post processing functions to extracted unstructured elements.\n        Post processing functions are str -> str callables are passed\n        in using the post_processors kwarg when the loader is instantiated.\"\"\"\n        for element in elements:\n            for post_processor in self.post_processors:\n                element.apply(post_processor)\n        return elements\n\n    def lazy_load(self)-> Iterator[Document]:\n        \"\"\"Load file.\"\"\"\n        elements = self._get_elements()\n        self._post_process_elements(elements)\n        if self.mode == \"elements\":\n            for element in elements:\n                metadata = self._get_metadata()\n                # NOTE(MthwRobinson) - the attribute check is for backward compatibility\n                # with unstructured<0.4.9. The metadata attributed was added in 0.4.9.\n                if hasattr(element, \"metadata\"):\n                    metadata.update(element.metadata.to_dict())\n                if hasattr(element, \"category\"):\n                    metadata[\"category\"] = element.category\n                yield Document(text=str(element), metadata=metadata)\n        elif self.mode == \"paged\":\n            text_dict: Dict[int, str] = {}\n            meta_dict: Dict[int, Dict] = {}\n\n            for idx, element in enumerate(elements):\n                metadata = self._get_metadata()\n                if hasattr(element, \"metadata\"):\n                    metadata.update(element.metadata.to_dict())\n                page_number = metadata.get(\"page_number\", 1)\n\n                # Check if this page_number already exists in docs_dict\n                if page_number not in text_dict:\n                    # If not, create new entry with initial text and metadata\n                    text_dict[page_number] = str(element) + \"\\n\\n\"\n                    meta_dict[page_number] = metadata\n                else:\n                    # If exists, append to text and update the metadata\n                    text_dict[page_number] += str(element) + \"\\n\\n\"\n                    meta_dict[page_number].update(metadata)\n\n            # Convert the dict to a list of Document objects\n            for key in text_dict.keys():\n                yield Document(text=text_dict[key], metadata=meta_dict[key])\n        elif self.mode == \"single\":\n            metadata = self._get_metadata()\n            text = \"\\n\\n\".join([str(el) for el in elements])\n            yield Document(text=text, metadata=metadata)\n        else:\n            raise ValueError(f\"mode of {self.mode} not supported.\")\n\n\nclass UnstructuredFileLoader(UnstructuredBaseLoader):\n    \"\"\"Load files using `Unstructured`.\n\n    The file loader uses the\n    unstructured partition function and will automatically detect the file\n    type. You can run the loader in one of two modes: \"single\" and \"elements\".\n    If you use \"single\" mode, the document will be returned as a single\n    langchain Document object. If you use \"elements\" mode, the unstructured\n    library will split the document into elements such as Title and NarrativeText.\n    You can pass in additional unstructured kwargs after mode to apply\n    different unstructured settings.\n\n    Examples\n    --------\n    from langchain_community.document_loaders import UnstructuredFileLoader\n\n    loader = UnstructuredFileLoader(\n        \"example.pdf\", mode=\"elements\", strategy=\"fast\",\n    )\n    docs = loader.load()\n\n    References\n    ----------\n    https://unstructured-io.github.io/unstructured/bricks.html#partition\n    \"\"\"\n\n    def __init__(\n        self,\n        file_path: Union[str, List[str], Path, List[Path], None],\n        mode: str = \"single\",\n        **unstructured_kwargs: Any,\n    ):\n        \"\"\"Initialize with file path.\"\"\"\n        self.file_path = file_path\n        super().__init__(mode=mode, **unstructured_kwargs)\n\n    def _get_elements(self) -> List:\n        from unstructured.partition.auto import partition\n\n        if isinstance(self.file_path, list):\n            elements = []\n            for file in self.file_path:\n                if isinstance(file, Path):\n                    file = str(file)\n                elements.extend(partition(filename=file, **self.unstructured_kwargs))\n            return elements\n        else:\n            if isinstance(self.file_path, Path):\n                self.file_path = str(self.file_path)\n            return partition(filename=self.file_path, **self.unstructured_kwargs)\n\n    def _get_metadata(self) -> dict:\n        return {\"source\": self.file_path}\n\n\n\n\n\n\n"}
{"type": "source_file", "path": "luann/constants.py", "content": "import os\nfrom logging import CRITICAL, DEBUG, ERROR, INFO, NOTSET, WARN, WARNING\n\ntypeagent_DIR = os.path.join(os.path.expanduser(\"~\"), \".luann\")\nCURRENT_AGENT_TYPE=[\"Memgpt\",\"OpenDevin_Planer\"]\nTYPEAGENT_TYPE={\n    \"Memgpt\": \"memagent\",\n    \"Swe\":\"swe\",\n   \n\n}\n# Tools\nBASE_TOOLS = [\n    \"send_message\",\n    \"core_memory_replace\",\n    \"core_memory_append\",\n    \"pause_heartbeats\",\n    \"conversation_search\",\n    \"conversation_search_date\",\n    \"archival_memory_insert\",\n    \"archival_memory_search\",\n]\n\n# TYPEAGENT_PROJECT_DIR=\"D:\\\\typeagent\"\nTYPEAGENT_PROJECT_DIR=os.path.dirname(os.path.abspath(__file__))\nprint(TYPEAGENT_PROJECT_DIR)\n\nTYPEAGENT_VERSION=\"0.3.14\"\n# OpenAI error message: Invalid 'messages[1].tool_calls[0].id': string too long. Expected a string with maximum length 29, but got a string with length 36 instead.\nTOOL_CALL_ID_MAX_LEN = 29\n\n# embeddings\nMAX_EMBEDDING_DIM = 4096  # maximum supported embeding size - do NOT change or else DBs will need to be reset\n\n# tokenizers\nEMBEDDING_TO_TOKENIZER_MAP = {\n    \"text-embedding-ada-002\": \"cl100k_base\",\n}\nEMBEDDING_TO_TOKENIZER_DEFAULT = \"cl100k_base\"\nKNOWLEDGE_BASE_DIR = os.path.join(typeagent_DIR, \"knowledge_base\")\n\nDEFAULT_typeagent_MODEL = \"gpt-4\"\nDEFAULT_PERSONA = \"sam_basic\"\nDEFAULT_HUMAN = \"Chad_basic\"\nDEFAULT_PRESET = \"typeagent_chat\"\nDEFAULT_AGENTTYPE = \"Memgpt\"\nDEFAULT_SYSTEMPROMPT=\"typeagent_chat\"\n\n# # Used to isolate typeagent logger instance from Dependant Libraries logging\n# LOGGER_NAME = \"typeagent\"\n# LOGGER_DEFAULT_LEVEL = CRITICAL\n# # Where to store the logs\n# LOGGER_DIR = os.path.join(typeagent_DIR, \"logs\")\n# # filename of the log\n# LOGGER_FILENAME = \"typeagent.log\"\n# # Number of log files to rotate\n# LOGGER_FILE_BACKUP_COUNT = 3\n# # Max Log file size in bytes\n# LOGGER_MAX_FILE_SIZE = 10485760\n# LOGGER_LOG_LEVEL is use to convert Text to Logging level value for logging mostly for Cli input to setting level\nLOGGER_LOG_LEVELS = {\"CRITICAL\": CRITICAL, \"ERROR\": ERROR, \"WARN\": WARN, \"WARNING\": WARNING, \"INFO\": INFO, \"DEBUG\": DEBUG, \"NOTSET\": NOTSET}\n\nFIRST_MESSAGE_ATTEMPTS = 10\n\nINITIAL_BOOT_MESSAGE = \"Boot sequence complete. Persona activated.\"\nINITIAL_BOOT_MESSAGE_SEND_MESSAGE_THOUGHT = \"Bootup sequence complete. Persona activated. Testing messaging functionality.\"\nSTARTUP_QUOTES = [\n    \"I think, therefore I am.\",\n    \"All those moments will be lost in time, like tears in rain.\",\n    \"More human than human is our motto.\",\n]\nINITIAL_BOOT_MESSAGE_SEND_MESSAGE_FIRST_MSG = STARTUP_QUOTES[2]\n\nCLI_WARNING_PREFIX = \"Warning: \"\n\nNON_USER_MSG_PREFIX = \"[This is an automated system message hidden from the user] \"\n\n# Constants to do with summarization / conversation length window\n# The max amount of tokens supported by the underlying model (eg 8k for gpt-4 and Mistral 7B)\nLLM_MAX_TOKENS = {\n    \"DEFAULT\": 8192,\n    ## OpenAI models: https://platform.openai.com/docs/models/overview\n    # gpt-4\n    \"gpt-4-1106-preview\": 128000,\n    \"gpt-4\": 8192,\n    \"gpt-4-32k\": 32768,\n    \"gpt-4-0613\": 8192,\n    \"gpt-4-32k-0613\": 32768,\n    \"gpt-4-0314\": 8192,  # legacy\n    \"gpt-4-32k-0314\": 32768,  # legacy\n    # gpt-3.5\n    \"gpt-3.5-turbo-1106\": 16385,\n    \"gpt-3.5-turbo\": 4096,\n    \"gpt-3.5-turbo-16k\": 16385,\n    \"gpt-3.5-turbo-0613\": 4096,  # legacy\n    \"gpt-3.5-turbo-16k-0613\": 16385,  # legacy\n    \"gpt-3.5-turbo-0301\": 4096,  # legacy\n}\n# The amount of tokens before a sytem warning about upcoming truncation is sent to typeagent\nMESSAGE_SUMMARY_WARNING_FRAC = 0.75\n# The error message that typeagent will receive\n# MESSAGE_SUMMARY_WARNING_STR = f\"Warning: the conversation history will soon reach its maximum length and be trimmed. Make sure to save any important information from the conversation to your memory before it is removed.\"\n# Much longer and more specific variant of the prompt\nMESSAGE_SUMMARY_WARNING_STR = \" \".join(\n    [\n        f\"{NON_USER_MSG_PREFIX}The conversation history will soon reach its maximum length and be trimmed.\",\n        \"Do NOT tell the user about this system alert, they should not know that the history is reaching max length.\",\n        \"If there is any important new information or general memories about you or the user that you would like to save, you should save that information immediately by calling function core_memory_append, core_memory_replace, or archival_memory_insert.\",\n        # \"Remember to pass request_heartbeat = true if you would like to send a message immediately after.\",\n    ]\n)\n# The fraction of tokens we truncate down to\nMESSAGE_SUMMARY_TRUNC_TOKEN_FRAC = 0.75\n# The ackknowledgement message used in the summarize sequence\nMESSAGE_SUMMARY_REQUEST_ACK = \"Understood, I will respond with a summary of the message (and only the summary, nothing else) once I receive the conversation history. I'm ready.\"\n\n# Even when summarizing, we want to keep a handful of recent messages\n# These serve as in-context examples of how to use functions / what user messages look like\nMESSAGE_SUMMARY_TRUNC_KEEP_N_LAST = 3\n\n# Default memory limits\nCORE_MEMORY_PERSONA_CHAR_LIMIT = 5000\nCORE_MEMORY_HUMAN_CHAR_LIMIT = 5000\n\n# Function return limits\nFUNCTION_RETURN_CHAR_LIMIT = 3000  # ~300 words\n\nMAX_PAUSE_HEARTBEATS = 360  # in min\n\nMESSAGE_CHATGPT_FUNCTION_MODEL = \"gpt-3.5-turbo\"\nMESSAGE_CHATGPT_FUNCTION_SYSTEM_MESSAGE = \"You are a helpful assistant. Keep your responses short and concise.\"\n\n#### Functions related\n\n# REQ_HEARTBEAT_MESSAGE = f\"{NON_USER_MSG_PREFIX}request_heartbeat == true\"\nREQ_HEARTBEAT_MESSAGE = f\"{NON_USER_MSG_PREFIX}Function called using request_heartbeat=true, returning control\"\n# FUNC_FAILED_HEARTBEAT_MESSAGE = f\"{NON_USER_MSG_PREFIX}Function call failed\"\nFUNC_FAILED_HEARTBEAT_MESSAGE = f\"{NON_USER_MSG_PREFIX}Function call failed, returning control\"\n\nFUNCTION_PARAM_NAME_REQ_HEARTBEAT = \"request_heartbeat\"\nFUNCTION_PARAM_TYPE_REQ_HEARTBEAT = \"boolean\"\nFUNCTION_PARAM_DESCRIPTION_REQ_HEARTBEAT = \"Request an immediate heartbeat after function execution. Set to 'true' if you want to send a follow-up message or run a follow-up function.\"\n\nRETRIEVAL_QUERY_DEFAULT_PAGE_SIZE = 5\n\n# GLOBAL SETTINGS FOR `json.dumps()`\nJSON_ENSURE_ASCII = False\n\n# GLOBAL SETTINGS FOR `json.loads()`\nJSON_LOADS_STRICT = False\n"}
{"type": "source_file", "path": "luann/cli/cli.py", "content": "import json\nimport logging\nimport os\nimport subprocess\nimport sys\nimport uuid\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Annotated, Optional, Tuple\nfrom agent_store.storage import StorageConnector, StorageType\nimport questionary\nimport requests\nimport typer\nfrom models.pydantic_models import OptionState\nimport errors as errors\nimport system as system\nimport traceback\n# from main import run_agent_loop\nimport utils as utils\nfrom agent import Agent, save_agent\nfrom cli.cli_config import configure\nfrom config import typeagentConfig\nfrom constants import CLI_WARNING_PREFIX, typeagent_DIR,TYPEAGENT_VERSION,CURRENT_AGENT_TYPE,TYPEAGENT_TYPE\nfrom credentials import typeagentCredentials\nfrom data_types import EmbeddingConfig, LLMConfig, User\n# from log import logger\nfrom log import get_logger\nfrom metadata import MetadataStore\nfrom migrate import migrate_all_agents, migrate_all_sources\nfrom server.constants import WS_DEFAULT_PORT\nfrom rich.console import Console\n# from interface import CLIInterface as interface  # for printing to terminal\nfrom streaming_interface import (\n    StreamingRefreshCLIInterface as interface,  # for printing to terminal\n)\nfrom utils import open_folder_in_explorer, printd\nfrom streaming_interface import AgentRefreshStreamingInterface\nimport agent as agent\nfrom constants import (\n    FUNC_FAILED_HEARTBEAT_MESSAGE,\n    JSON_ENSURE_ASCII,\n    JSON_LOADS_STRICT,\n    REQ_HEARTBEAT_MESSAGE,\n)\nUSER_COMMANDS = [\n    (\"//\", \"toggle multiline input mode\"),\n    (\"/exit\", \"exit the CLI\"),\n    (\"/save\", \"save a checkpoint of the current agent/conversation state\"),\n    (\"/load\", \"load a saved checkpoint\"),\n    (\"/dump <count>\", \"view the last <count> messages (all if <count> is omitted)\"),\n    (\"/memory\", \"print the current contents of agent memory\"),\n    (\"/pop <count>\", \"undo <count> messages in the conversation (default is 3)\"),\n    (\"/retry\", \"pops the last answer and tries to get another one\"),\n    (\"/rethink <text>\", \"changes the inner thoughts of the last agent message\"),\n    (\"/rewrite <text>\", \"changes the reply of the last agent message\"),\n    (\"/heartbeat\", \"send a heartbeat system message to the agent\"),\n    (\"/memorywarning\", \"send a memory warning system message to the agent\"),\n    (\"/attach\", \"attach data source to agent\"),\n]\ndef clear_line(console, strip_ui=False):\n    if strip_ui:\n        return\n    if os.name == \"nt\":  # for windows\n        console.print(\"\\033[A\\033[K\", end=\"\")\n    else:  # for linux\n        sys.stdout.write(\"\\033[2K\\033[G\")\n        sys.stdout.flush()\n\nlogger = get_logger(__name__)\ndef run_agent_loop(\n    typeagent_agent: agent.Agent, config: typeagentConfig, ms: MetadataStore,first, stream=False,strip_ui=False,inner_thoughts_in_kwargs: OptionState = OptionState.DEFAULT,\n):\n    if isinstance(typeagent_agent.interface, AgentRefreshStreamingInterface):\n        # typeagent_agent.interface.toggle_streaming(on=stream)\n        if not stream:\n            typeagent_agent.interface = typeagent_agent.interface.nonstreaming_interface\n\n    if hasattr(typeagent_agent.interface, \"console\"):\n        console = typeagent_agent.interface.console\n    else:\n        console = Console()\n\n    counter = 0\n    user_input = None\n    skip_next_user_input = False #skip user input ,take control by agent,auto execument\n    user_message = None\n    USER_GOES_FIRST =first\n    # strip_ui=True\n    if not USER_GOES_FIRST:\n        console.input(\"[bold cyan]Hit enter to begin (will request first typeagent message)[/bold cyan]\\n\")\n        clear_line(console, strip_ui=strip_ui)\n        print()\n\n    multiline_input = False\n    ms = MetadataStore(config)\n    while True:\n        if not skip_next_user_input and (counter > 0 or USER_GOES_FIRST):\n            # Ask for user input\n            if not stream:\n                print()\n            user_input = questionary.text(\n                \"Enter your message:\",\n                multiline=multiline_input,\n                qmark=\">\",\n            ).ask()\n            clear_line(console, strip_ui=True)\n            if not stream:\n                print()\n\n            # Gracefully exit on Ctrl-C/D\n            if user_input is None:\n                user_input = \"/exit\"\n\n            user_input = user_input.rstrip()\n\n            if user_input.startswith(\"!\"):\n                print(f\"Commands for CLI begin with '/' not '!'\")\n                continue\n\n            if user_input == \"\":\n                # no empty messages allowed\n                print(\"Empty input received. Try again!\")\n                continue\n\n            # Handle CLI commands\n            # Commands to not get passed as input to typeagent\n            if user_input.startswith(\"/\"):\n                # updated agent save functions\n                if user_input.lower() == \"/exit\":\n                    # typeagent_agent.save()\n                    agent.save_agent(typeagent_agent, ms)\n                    break\n                elif user_input.lower() == \"/save\" or user_input.lower() == \"/savechat\":\n                    # typeagent_agent.save()\n                    agent.save_agent(typeagent_agent, ms)\n                    continue\n                elif user_input.lower() == \"/attach\":\n                    # TODO: check if agent already has it\n\n                    # TODO: check to ensure source embedding dimentions/model match agents, and disallow attachment if not\n                    # TODO: alternatively, only list sources with compatible embeddings, and print warning about non-compatible sources\n\n                    data_source_options = ms.list_sources(user_id=typeagent_agent.agent_state.user_id)\n                    if len(data_source_options) == 0:\n                        typer.secho(\n                            'No sources available. You must create a souce with \"typeagent add source ...\" before running /attach.',\n                            fg=typer.colors.RED,\n                            bold=True,\n                        )\n                        continue\n\n                    # determine what sources are valid to be attached to this agent\n                    valid_options = []\n                    invalid_options = []\n                    for source in data_source_options:\n                        if (\n                            source.embedding_model == typeagent_agent.agent_state.embedding_config.embedding_model\n                            # and source.embedding_dim == typeagent_agent.agent_state.embedding_config.embedding_dim\n                        ):\n                            valid_options.append(source.name)\n                        else:\n                            # print warning about invalid sources\n                            typer.secho(\n                                f\"Source {source.name} exists but model {source.embedding_model}, while the agent uses embedding dimentions {typeagent_agent.agent_state.embedding_config.embedding_dim} and model {typeagent_agent.agent_state.embedding_config.embedding_model}\",\n                                fg=typer.colors.YELLOW,\n                            )\n                            invalid_options.append(source.name)\n\n                    # prompt user for data source selection\n                    data_source = questionary.select(\"Select data source\", choices=valid_options).ask()\n\n                    # attach new data\n                    # attach(typeagent_agent.agent_state.name, data_source)\n                    # source_connector = StorageConnector.get_storage_connector(\n                    #     StorageType.KNOWLEDGE_BASE_PASSAGES, config, user_id=typeagent_agent.agent_state.user_id\n                    # )\n                    typeagent_agent.attach_source(data_source, ms)\n\n                    continue\n\n                elif user_input.lower() == \"/dump\" or user_input.lower().startswith(\"/dump \"):\n                    # Check if there's an additional argument that's an integer\n                    command = user_input.strip().split()\n                    amount = int(command[1]) if len(command) > 1 and command[1].isdigit() else 0\n                    if amount == 0:\n                        typeagent_agent.interface.print_messages(typeagent_agent._messages, dump=True)\n                    else:\n                        typeagent_agent.interface.print_messages(typeagent_agent._messages[-min(amount, len(typeagent_agent.messages)) :], dump=True)\n                    continue\n\n                elif user_input.lower() == \"/dumpraw\":\n                    typeagent_agent.interface.print_messages_raw(typeagent_agent._messages)\n                    continue\n\n                elif user_input.lower() == \"/memory\":\n                    print(f\"\\nDumping memory contents:\\n\")\n                    print(f\"{str(typeagent_agent.memory)}\")\n                    print(f\"{str(typeagent_agent.persistence_manager.archival_memory)}\")\n                    print(f\"{str(typeagent_agent.persistence_manager.recall_memory)}\")\n                    continue\n\n                elif user_input.lower() == \"/model\":\n                    if typeagent_agent.model == \"gpt-4\":\n                        typeagent_agent.model = \"gpt-3.5-turbo-16k\"\n                    elif typeagent_agent.model == \"gpt-3.5-turbo-16k\":\n                        typeagent_agent.model = \"gpt-4\"\n                    print(f\"Updated model to:\\n{str(typeagent_agent.model)}\")\n                    continue\n\n                elif user_input.lower() == \"/pop\" or user_input.lower().startswith(\"/pop \"):\n                    # Check if there's an additional argument that's an integer\n                    command = user_input.strip().split()\n                    pop_amount = int(command[1]) if len(command) > 1 and command[1].isdigit() else 3\n                    # n_messages = len(typeagent_agent.messages)\n                    n_messages = len(typeagent_agent._messages)\n                    MIN_MESSAGES = 2\n                    if n_messages <= MIN_MESSAGES:\n                        print(f\"Agent only has {n_messages} messages in stack, none left to pop\")\n                    elif n_messages - pop_amount < MIN_MESSAGES:\n                        print(f\"Agent only has {n_messages} messages in stack, cannot pop more than {n_messages - MIN_MESSAGES}\")\n                    else:\n                        print(f\"Popping last {pop_amount} messages from stack\")\n                        for _ in range(min(pop_amount, len(typeagent_agent.messages))):\n                            typeagent_agent.messages.pop()\n                        for _ in range(min(pop_amount, len(typeagent_agent._messages))):\n                            # remove the message from the internal state of the agent\n                            deleted_message = typeagent_agent._messages.pop()\n                            # then also remove it from recall storage\n                            # typeagent_agent.persistence_manager.recall_memory.storage.delete(filters={\"id\": deleted_message.id})\n                    continue\n\n                elif user_input.lower() == \"/retry\":\n                    # TODO this needs to also modify the persistence manager\n                    print(f\"Retrying for another answer\")\n                    while len(typeagent_agent._messages) > 0:\n                        if typeagent_agent._messages[-1].get(\"role\") == \"user\":\n                            # we want to pop up to the last user message and send it again\n                            # user_message = typeagent_agent.messages[-1].get(\"content\")\n                            # typeagent_agent.messages.pop()\n                            user_message = typeagent_agent._messages[-1].text\n                            deleted_message = typeagent_agent._messages.pop()\n                            # then also remove it from recall storage\n                            # typeagent_agent.persistence_manager.recall_memory.storage.delete(filters={\"id\": deleted_message.id})\n                            break\n                        # typeagent_agent.messages.pop()\n                        deleted_message = typeagent_agent._messages.pop()\n                        # then also remove it from recall storage\n                        # typeagent_agent.persistence_manager.recall_memory.storage.delete(filters={\"id\": deleted_message.id})\n\n                elif user_input.lower() == \"/rethink\" or user_input.lower().startswith(\"/rethink \"):\n                    # TODO this needs to also modify the persistence manager\n                    if len(user_input) < len(\"/rethink \"):\n                        print(\"Missing text after the command\")\n                        continue\n                    for x in range(len(typeagent_agent.messages) - 1, 0, -1):\n                        # if typeagent_agent.messages[x].get(\"role\") == \"assistant\":\n                        #     text = user_input[len(\"/rethink \") :].strip()\n                        #     typeagent_agent.messages[x].update({\"content\": text})\n                        msg_obj = typeagent_agent._messages[x]\n                        if msg_obj.role == \"assistant\":\n                            clean_new_text = user_input[len(\"/rethink \") :].strip()\n                            msg_obj.text = clean_new_text\n                            # To persist to the database, all we need to do is \"re-insert\" into recall memory\n                            # typeagent_agent.persistence_manager.recall_memory.recallmemory_ms.update_recallmemroy(record=msg_obj)\n                            break\n                    continue\n\n                elif user_input.lower() == \"/rewrite\" or user_input.lower().startswith(\"/rewrite \"):\n                    # TODO this needs to also modify the persistence manager\n                    if len(user_input) < len(\"/rewrite \"):\n                        print(\"Missing text after the command\")\n                        continue\n                    for x in range(len(typeagent_agent.messages) - 1, 0, -1):\n                        if typeagent_agent.messages[x].get(\"role\") == \"assistant\":\n                            text = user_input[len(\"/rewrite \") :].strip()\n                            # Get the current message content\n                            # The rewrite target is the output of send_message\n                            message_obj = typeagent_agent._messages[x]\n                            if message_obj.tool_calls is not None and len(message_obj.tool_calls) > 0:\n                                # Check that we hit an assistant send_message call\n                                name_string = message_obj.tool_calls[0][0].function.get(\"name\")\n                                if name_string is None or name_string != \"send_message\":\n                                    print(\"Assistant missing send_message function call\")\n                                    break  # cancel op\n                                args_string = message_obj.tool_calls[0][0].function.get(\"arguments\")\n                                if args_string is None:\n                                    print(\"Assistant missing send_message function arguments\")\n                                    break  # cancel op\n                                args_json = json.loads(args_string, strict=JSON_LOADS_STRICT)\n                                if \"message\" not in args_json:\n                                    print(\"Assistant missing send_message message argument\")\n                                    break  # cancel op\n\n                                # Once we found our target, rewrite it\n                                args_json[\"message\"] = text\n                                new_args_string = json.dumps(args_json, ensure_ascii=JSON_ENSURE_ASCII)\n                                message_obj.tool_calls[0][0].function[\"arguments\"] = new_args_string\n\n                                # To persist to the database, all we need to do is \"re-insert\" into recall memory\n                                typeagent_agent.persistence_manager.recall_memory.update(message=message_obj)\n                                break\n                    continue\n\n                elif user_input.lower() == \"/summarize\":\n                    try:\n                        typeagent_agent.summarize_messages_inplace()\n                        typer.secho(\n                            f\"/summarize succeeded\",\n                            fg=typer.colors.GREEN,\n                            bold=True,\n                        )\n                    except (errors.LLMError, requests.exceptions.HTTPError) as e:\n                        typer.secho(\n                            f\"/summarize failed:\\n{e}\",\n                            fg=typer.colors.RED,\n                            bold=True,\n                        )\n                    continue\n                #TODO later add back\n                # elif user_input.lower().startswith(\"/add_function\"):\n                #     try:\n                #         if len(user_input) < len(\"/add_function \"):\n                #             print(\"Missing function name after the command\")\n                #             continue\n                #         function_name = user_input[len(\"/add_function \") :].strip()\n                #         result = typeagent_agent.add_function(function_name)\n                #         typer.secho(\n                #             f\"/add_function succeeded: {result}\",\n                #             fg=typer.colors.GREEN,\n                #             bold=True,\n                #         )\n                #     except ValueError as e:\n                #         typer.secho(\n                #             f\"/add_function failed:\\n{e}\",\n                #             fg=typer.colors.RED,\n                #             bold=True,\n                #         )\n                #         continue\n                # elif user_input.lower().startswith(\"/remove_function\"):\n                #     try:\n                #         if len(user_input) < len(\"/remove_function \"):\n                #             print(\"Missing function name after the command\")\n                #             continue\n                #         function_name = user_input[len(\"/remove_function \") :].strip()\n                #         result = typeagent_agent.remove_function(function_name)\n                #         typer.secho(\n                #             f\"/remove_function succeeded: {result}\",\n                #             fg=typer.colors.GREEN,\n                #             bold=True,\n                #         )\n                #     except ValueError as e:\n                #         typer.secho(\n                #             f\"/remove_function failed:\\n{e}\",\n                #             fg=typer.colors.RED,\n                #             bold=True,\n                #         )\n                #         continue\n\n                # No skip options\n                elif user_input.lower() == \"/wipe\":\n                    typeagent_agent = agent.Agent(typeagent_agent.interface)\n                    user_message = None\n\n                elif user_input.lower() == \"/heartbeat\":\n                    user_message = system.get_heartbeat()\n\n                elif user_input.lower() == \"/memorywarning\":\n                    user_message = system.get_token_limit_warning()\n\n                elif user_input.lower() == \"//\":\n                    multiline_input = not multiline_input\n                    continue\n\n                elif user_input.lower() == \"/\" or user_input.lower() == \"/help\":\n                    questionary.print(\"CLI commands\", \"bold\")\n                    for cmd, desc in USER_COMMANDS:\n                        questionary.print(cmd, \"bold\")\n                        questionary.print(f\" {desc}\")\n                    continue\n\n                else:\n                    print(f\"Unrecognized command: {user_input}\")\n                    continue\n\n            else:\n                # If message did not begin with command prefix, pass inputs to typeagent\n                # Handle user message and append to messages\n                user_message = system.package_user_message(user_input)\n\n        skip_next_user_input = False\n\n        def process_agent_step(user_message):\n            new_messages, heartbeat_request, function_failed, token_warning, tokens_accumulated = typeagent_agent.step(\n                user_message,\n                inner_thoughts_in_kwargs=inner_thoughts_in_kwargs,\n                stream=stream,\n            )\n\n            skip_next_user_input = False\n            if token_warning:\n                user_message = system.get_token_limit_warning()\n                skip_next_user_input = True\n            elif function_failed:\n                user_message = system.get_heartbeat(FUNC_FAILED_HEARTBEAT_MESSAGE)\n                skip_next_user_input = True\n            elif heartbeat_request:\n                user_message = system.get_heartbeat(REQ_HEARTBEAT_MESSAGE)\n                skip_next_user_input = True\n\n            return new_messages, user_message, skip_next_user_input\n\n        while True:\n            try:\n                if strip_ui:\n                    new_messages, user_message, skip_next_user_input = process_agent_step(user_message)\n                    break\n                else:\n                    if stream:\n                        # Don't display the \"Thinking...\" if streaming\n                        new_messages, user_message, skip_next_user_input = process_agent_step(user_message)\n                    else:\n                        with console.status(\"[bold cyan]Thinking...\") as status:\n                            new_messages, user_message, skip_next_user_input = process_agent_step(user_message)\n                    break\n            except KeyboardInterrupt:\n                print(\"User interrupt occurred.\")\n                retry = questionary.confirm(\"Retry agent.step()?\").ask()\n                if not retry:\n                    break\n            except Exception as e:\n                print(\"An exception occurred when running agent.step(): \")\n                traceback.print_exc()\n                retry = questionary.confirm(\"Retry agent.step()?\").ask()\n                if not retry:\n                    break\n\n        counter += 1\n\n    print(\"Finished.\")\ndef migrate(\n    debug: Annotated[bool, typer.Option(help=\"Print extra tracebacks for failed migrations\")] = False,\n):\n    \"\"\"Migrate old agents (pre 0.2.12) to the new database system\"\"\"\n    migrate_all_agents(debug=debug)\n    migrate_all_sources(debug=debug)\n\n\nclass QuickstartChoice(Enum):\n    openai = \"openai\"\n    # azure = \"azure\"\n    typeagent_hosted = \"typeagent\"\n\n\ndef str_to_quickstart_choice(choice_str: str) -> QuickstartChoice:\n    try:\n        return QuickstartChoice[choice_str]\n    except KeyError:\n        valid_options = [choice.name for choice in QuickstartChoice]\n        raise ValueError(f\"{choice_str} is not a valid QuickstartChoice. Valid options are: {valid_options}\")\n\n\ndef set_config_with_dict(new_config: dict) -> Tuple[typeagentConfig, bool]: # type: ignore\n    \"\"\"_summary_\n\n    Args:\n        new_config (dict): Dict of new config values\n\n    Returns:\n        new_config typeagentConfig, modified (bool): Returns the new config and a boolean indicating if the config was modified\n    \"\"\"\n    from utils import printd\n\n    old_config = typeagentConfig.load()\n    modified = False\n    for k, v in vars(old_config).items():\n        if k in new_config:\n            if v != new_config[k]:\n                printd(f\"Replacing config {k}: {v} -> {new_config[k]}\")\n                modified = True\n                # old_config[k] = new_config[k]\n                setattr(old_config, k, new_config[k])  # Set the new value using dot notation\n            else:\n                printd(f\"Skipping new config {k}: {v} == {new_config[k]}\")\n\n    # update embedding config\n    if old_config.default_embedding_config:\n        for k, v in vars(old_config.default_embedding_config).items():\n            if k in new_config:\n                if v != new_config[k]:\n                    printd(f\"Replacing config {k}: {v} -> {new_config[k]}\")\n                    modified = True\n                    # old_config[k] = new_config[k]\n                    setattr(old_config.default_embedding_config, k, new_config[k])\n                else:\n                    printd(f\"Skipping new config {k}: {v} == {new_config[k]}\")\n    else:\n        modified = True\n        fields = [\"embedding_model\", \"embedding_dim\", \"embedding_chunk_size\", \"embedding_endpoint\", \"embedding_endpoint_type\"]\n        args = {}\n        for field in fields:\n            if field in new_config:\n                args[field] = new_config[field]\n                printd(f\"Setting new config {field}: {new_config[field]}\")\n        old_config.default_embedding_config = EmbeddingConfig(**args)\n\n    # update llm config\n    if old_config.default_llm_config:\n        for k, v in vars(old_config.default_llm_config).items():\n            if k in new_config:\n                if v != new_config[k]:\n                    printd(f\"Replacing config {k}: {v} -> {new_config[k]}\")\n                    modified = True\n                    # old_config[k] = new_config[k]\n                    setattr(old_config.default_llm_config, k, new_config[k])\n                else:\n                    printd(f\"Skipping new config {k}: {v} == {new_config[k]}\")\n    else:\n        modified = True\n        fields = [\"model\", \"model_endpoint\", \"model_endpoint_type\", \"model_wrapper\", \"context_window\"]\n        args = {}\n        for field in fields:\n            if field in new_config:\n                args[field] = new_config[field]\n                printd(f\"Setting new config {field}: {new_config[field]}\")\n        old_config.default_llm_config = LLMConfig(**args)\n    return (old_config, modified)\n\n\ndef quickstart(\n    backend: Annotated[QuickstartChoice, typer.Option(help=\"Quickstart setup backend\")] = \"typeagent\",\n    latest: Annotated[bool, typer.Option(help=\"Use --latest to pull the latest config from online\")] = False,\n    debug: Annotated[bool, typer.Option(help=\"Use --debug to enable debugging output\")] = False,\n    terminal: bool = True,\n):\n    \"\"\"Set the base config file with a single command\n\n    This function and `configure` should be the ONLY places where typeagentConfig.save() is called.\n    \"\"\"\n\n    # setup logger\n    utils.DEBUG = debug\n    logging.getLogger().setLevel(logging.CRITICAL)\n    if debug:\n        logging.getLogger().setLevel(logging.DEBUG)\n\n    # make sure everything is set up properly\n    typeagentConfig.create_config_dir()\n    credentials = typeagentCredentials.load()\n\n    config_was_modified = False\n    if backend == QuickstartChoice.typeagent_hosted:\n        # if latest, try to pull the config from the repo\n        # fallback to using local\n        if latest:\n            # Download the latest typeagent hosted config\n            # url = \"https://raw.githubusercontent.com/cpacker/typeagent/main/typeagent/configs/typeagent_hosted.json\"\n            url = \"https://raw.githubusercontent.com/cpacker/typeagent/main/configs/typeagent_hosted.json\"\n            response = requests.get(url)\n\n            # Check if the request was successful\n            if response.status_code == 200:\n                # Parse the response content as JSON\n                config = response.json()\n                # Output a success message and the first few items in the dictionary as a sample\n                printd(\"JSON config file downloaded successfully.\")\n                new_config, config_was_modified = set_config_with_dict(config)\n            else:\n                typer.secho(f\"Failed to download config from {url}. Status code: {response.status_code}\", fg=typer.colors.RED)\n\n                # Load the file from the relative path\n                script_dir = os.path.dirname(__file__)  # Get the directory where the script is located\n                backup_config_path = os.path.join(script_dir, \"..\", \"..\", \"configs\", \"typeagent_hosted.json\")\n                try:\n                    with open(backup_config_path, \"r\", encoding=\"utf-8\") as file:\n                        backup_config = json.load(file)\n                    printd(\"Loaded backup config file successfully.\")\n                    new_config, config_was_modified = set_config_with_dict(backup_config)\n                except FileNotFoundError:\n                    typer.secho(f\"Backup config file not found at {backup_config_path}\", fg=typer.colors.RED)\n                    return\n        else:\n            # Load the file from the relative path\n            script_dir = os.path.dirname(__file__)  # Get the directory where the script is located\n            print(\"SCRIPT\", script_dir)\n            backup_config_path = os.path.join(script_dir, \"..\", \"..\", \"configs\", \"typeagent_hosted.json\")\n            print(\"FILE PATH\", backup_config_path)\n            try:\n                with open(backup_config_path, \"r\", encoding=\"utf-8\") as file:\n                    backup_config = json.load(file)\n                    print(backup_config)\n                printd(\"Loaded config file successfully.\")\n                new_config, config_was_modified = set_config_with_dict(backup_config)\n            except FileNotFoundError:\n                typer.secho(f\"Config file not found at {backup_config_path}\", fg=typer.colors.RED)\n                return\n\n    elif backend == QuickstartChoice.openai:\n        # Make sure we have an API key\n        api_key = os.getenv(\"OPENAI_API_KEY\")\n        while api_key is None or len(api_key) == 0:\n            # Ask for API key as input\n            api_key = questionary.password(\"Enter your OpenAI API key (starts with 'sk-', see https://platform.openai.com/api-keys):\").ask()\n        credentials.openai_key = api_key\n        credentials.save()\n\n        # if latest, try to pull the config from the repo\n        # fallback to using local\n        if latest:\n            # url = \"https://raw.githubusercontent.com/cpacker/typeagent/main/typeagent/configs/openai.json\"\n            url = \"https://raw.githubusercontent.com/cpacker/typeagent/main/configs/openai.json\"\n            response = requests.get(url)\n\n            # Check if the request was successful\n            if response.status_code == 200:\n                # Parse the response content as JSON\n                config = response.json()\n                # Output a success message and the first few items in the dictionary as a sample\n                print(\"JSON config file downloaded successfully.\")\n                new_config, config_was_modified = set_config_with_dict(config)\n            else:\n                typer.secho(f\"Failed to download config from {url}. Status code: {response.status_code}\", fg=typer.colors.RED)\n\n                # Load the file from the relative path\n                script_dir = os.path.dirname(__file__)  # Get the directory where the script is located\n                backup_config_path = os.path.join(script_dir, \"..\", \"..\", \"configs\", \"openai.json\")\n                try:\n                    with open(backup_config_path, \"r\", encoding=\"utf-8\") as file:\n                        backup_config = json.load(file)\n                    printd(\"Loaded backup config file successfully.\")\n                    new_config, config_was_modified = set_config_with_dict(backup_config)\n                except FileNotFoundError:\n                    typer.secho(f\"Backup config file not found at {backup_config_path}\", fg=typer.colors.RED)\n                    return\n        else:\n            # Load the file from the relative path\n            script_dir = os.path.dirname(__file__)  # Get the directory where the script is located\n            backup_config_path = os.path.join(script_dir, \"..\", \"..\", \"configs\", \"openai.json\")\n            try:\n                with open(backup_config_path, \"r\", encoding=\"utf-8\") as file:\n                    backup_config = json.load(file)\n                printd(\"Loaded config file successfully.\")\n                new_config, config_was_modified = set_config_with_dict(backup_config)\n            except FileNotFoundError:\n                typer.secho(f\"Config file not found at {backup_config_path}\", fg=typer.colors.RED)\n                return\n\n    else:\n        raise NotImplementedError(backend)\n\n    if config_was_modified:\n        printd(f\"Saving new config file.\")\n        new_config.save()\n        typer.secho(f\"📖 typeagent configuration file updated!\", fg=typer.colors.GREEN)\n        typer.secho(\n            \"\\n\".join(\n                [\n                    f\"🧠 model\\t-> {new_config.default_llm_config.model}\",\n                    f\"🖥️  endpoint\\t-> {new_config.default_llm_config.model_endpoint}\",\n                ]\n            ),\n            fg=typer.colors.GREEN,\n        )\n    else:\n        typer.secho(f\"📖 typeagent configuration file unchanged.\", fg=typer.colors.WHITE)\n        typer.secho(\n            \"\\n\".join(\n                [\n                    f\"🧠 model\\t-> {new_config.default_llm_config.model}\",\n                    f\"🖥️  endpoint\\t-> {new_config.default_llm_config.model_endpoint}\",\n                ]\n            ),\n            fg=typer.colors.WHITE,\n        )\n\n    # 'terminal' = quickstart was run alone, in which case we should guide the user on the next command\n    if terminal:\n        if config_was_modified:\n            typer.secho('⚡ Run \"typeagent run\" to create an agent with the new config.', fg=typer.colors.YELLOW)\n        else:\n            typer.secho('⚡ Run \"typeagent run\" to create an agent.', fg=typer.colors.YELLOW)\n\n\ndef open_folder():\n    \"\"\"Open a folder viewer of the typeagent home directory\"\"\"\n    try:\n        print(f\"Opening home folder: {typeagent_DIR}\")\n        open_folder_in_explorer(typeagent_DIR)\n    except Exception as e:\n        print(f\"Failed to open folder with system viewer, error:\\n{e}\")\n\n\nclass ServerChoice(Enum):\n    rest_api = \"rest\"\n    ws_api = \"websocket\"\n\n\ndef create_default_user_or_exit(config: typeagentConfig, ms: MetadataStore):\n    user_id = uuid.UUID(config.anon_clientid)\n    user = ms.get_adminuser(\"admin\")\n    if user is None:\n        admin_user = User(\n        id=user_id,\n        user_type=\"admin\",\n        user_status=\"on\"\n     )   \n        \n\n        # print(\"admin: \"+admin_user.user_type)\n        ms.create_user(admin_user)\n        user = ms.get_adminuser(\"admin\")\n        from presets.presets import add_default_presets\n        add_default_presets(user_id, ms)\n        if user is None:\n            typer.secho(f\"Failed to create default user in database.\", fg=typer.colors.RED)\n            sys.exit(1)\n        else:\n            return user\n    else:\n        return user\n\n\ndef server(\n    type: Annotated[ServerChoice, typer.Option(help=\"Server to run\")] = \"rest\",\n    port: Annotated[Optional[int], typer.Option(help=\"Port to run the server on\")] = None,\n    host: Annotated[Optional[str], typer.Option(help=\"Host to run the server on (default to localhost)\")] = None,\n    use_ssl: Annotated[bool, typer.Option(help=\"Run the server using HTTPS?\")] = False,\n    ssl_cert: Annotated[Optional[str], typer.Option(help=\"Path to SSL certificate (if use_ssl is True)\")] = None,\n    ssl_key: Annotated[Optional[str], typer.Option(help=\"Path to SSL key file (if use_ssl is True)\")] = None,\n    debug: Annotated[bool, typer.Option(help=\"Turn debugging output on\")] = False,\n):\n    \"\"\"Launch a typeagent server process\"\"\"\n\n    if type == ServerChoice.rest_api:\n        pass\n\n        if typeagentConfig.exists():\n            config = typeagentConfig.load()\n            ms = MetadataStore(config)\n            create_default_user_or_exit(config, ms)\n        else:\n            typer.secho(f\"No configuration exists. Run typeagent configure before starting the server.\", fg=typer.colors.RED)\n            sys.exit(1)\n\n        try:\n            from server.rest_api.server import start_server\n\n            start_server(\n                port=port,\n                host=host,\n                use_ssl=use_ssl,\n                ssl_cert=ssl_cert,\n                ssl_key=ssl_key,\n                debug=debug,\n            )\n\n        except KeyboardInterrupt:\n            # Handle CTRL-C\n            typer.secho(\"Terminating the server...\")\n            sys.exit(0)\n\n    elif type == ServerChoice.ws_api:\n        if debug:\n            from server.server  import get_logger as server_logger\n\n            # Set the logging level\n            server_logger.setLevel(logging.DEBUG)\n            # Create a StreamHandler\n            stream_handler = logging.StreamHandler()\n            # Set the formatter (optional)\n            formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n            stream_handler.setFormatter(formatter)\n            # Add the handler to the logger\n            server_logger.addHandler(stream_handler)\n\n        if port is None:\n            port = WS_DEFAULT_PORT\n\n        # Change to the desired directory\n        script_path = Path(__file__).resolve()\n        script_dir = script_path.parent\n\n        server_directory = os.path.join(script_dir.parent, \"server\", \"ws_api\")\n        command = f\"python server.py {port}\"\n\n        # Run the command\n        typer.secho(f\"Running WS (websockets) server: {command} (inside {server_directory})\")\n\n        process = None\n        try:\n            # Start the subprocess in a new session\n            process = subprocess.Popen(command, shell=True, start_new_session=True, cwd=server_directory)\n            process.wait()\n        except KeyboardInterrupt:\n            # Handle CTRL-C\n            if process is not None:\n                typer.secho(\"Terminating the server...\")\n                process.terminate()\n                try:\n                    process.wait(timeout=5)\n                except subprocess.TimeoutExpired:\n                    process.kill()\n                    typer.secho(\"Server terminated with kill()\")\n            sys.exit(0)\n\n\ndef run(\n    persona: Annotated[Optional[str], typer.Option(help=\"Specify core persona memory\")] = None,\n    agent: Annotated[Optional[str], typer.Option(help=\"Specify agent name\")] = None,\n    type_agent:Annotated[Optional[str], typer.Option(help=\"Specify agent type\")] = None,\n    human: Annotated[Optional[str], typer.Option(help=\"Specify core human memory\")] = None,\n    preset: Annotated[Optional[str], typer.Option(help=\"Specify preset name\")] = None,\n    # model flags\n    model: Annotated[Optional[str], typer.Option(help=\"Specify the LLM model\")] = None,\n    model_wrapper: Annotated[Optional[str], typer.Option(help=\"Specify the LLM model wrapper\")] = None,\n    model_endpoint: Annotated[Optional[str], typer.Option(help=\"Specify the LLM model endpoint\")] = None,\n    model_endpoint_type: Annotated[Optional[str], typer.Option(help=\"Specify the LLM model endpoint type\")] = None,\n    context_window: Annotated[\n        Optional[int], typer.Option(help=\"The context window of the LLM you are using (e.g. 8k for most Mistral 7B variants)\")\n    ] = None,\n    # other\n    first: Annotated[bool, typer.Option(help=\"Use --first to send the first message in the sequence\")] = False,\n    strip_ui: Annotated[bool, typer.Option(help=\"Remove all the bells and whistles in CLI output (helpful for testing)\")] = False,\n    debug: Annotated[bool, typer.Option(help=\"Use --debug to enable debugging output\")] = False,\n    # no_verify: Annotated[bool, typer.Option(help=\"Bypass message verification\")] = False,\n    # yes: Annotated[bool, typer.Option(\"-y\", help=\"Skip confirmation prompt and use defaults\")] = False,\n    # streaming\n    no_content: Annotated[\n        OptionState, typer.Option(help=\"Set to 'yes' for LLM APIs that omit the `content` field during tool calling\")\n    ] = OptionState.DEFAULT,\n    stream: Annotated[bool, typer.Option(help=\"Enables message streaming in the CLI (if the backend supports it)\")] = False,\n):\n    \"\"\"Start chatting with an typeagent agent\n\n    Example usage: `typeagent run --agent myagent --data-source mydata --persona mypersona --human myhuman --model gpt-3.5-turbo`\n\n    :param persona: Specify persona\n    :param agent: Specify agent name (will load existing state if the agent exists, or create a new one with that name)\n    :param human: Specify human\n    :param model: Specify the LLM model\n\n    \"\"\"\n\n    # setup logger\n    # TODO: remove Utils Debug after global logging is complete.\n    utils.DEBUG = debug\n    # TODO: add logging command line options for runtime log level\n\n    if debug:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.CRITICAL)\n\n    \n\n    if not typeagentConfig.exists():\n      \n            configure()\n            config = typeagentConfig.load()\n\n    else:  # load config\n        config = typeagentConfig.load()\n\n        # # force re-configuration is config is from old version\n        # if config.typeagent_version is None:  # TODO: eventually add checks for older versions, if config changes again\n        #     typer.secho(\"typeagent has been updated to a newer version, so re-running configuration.\", fg=typer.colors.YELLOW)\n        #     configure()\n        #     config = typeagentConfig.load()\n\n    # read user id from config\n    ms = MetadataStore(config)\n    user = create_default_user_or_exit(config, ms)\n    # human = human if human else config.human\n    # persona = persona if persona else config.persona\n    # op\n\n    # determine agent to use, if not provided\n    if  not agent:\n        agents = ms.list_agents(user_id=user.id)\n        agents = [a.name for a in agents]\n\n        if len(agents) > 0:\n            print()\n            select_agent = questionary.confirm(\"Would you like to select an existing agent?\").ask()\n            if select_agent is None:\n                raise KeyboardInterrupt\n            if select_agent:\n                agent = questionary.select(\"Select agent:\", choices=agents).ask()\n\n    # create agent config\n    agent_state = ms.get_agent(agent_name=agent, user_id=user.id) if agent else None\n    if agent and agent_state:  # use existing agent\n        typer.secho(f\"\\n🔁 Using existing agent {agent}\", fg=typer.colors.GREEN)\n        # agent_config = AgentConfig.load(agent)\n        # agent_state = ms.get_agent(agent_name=agent, user_id=user_id)\n        printd(\"Loading agent state:\", agent_state.id)\n        printd(\"Agent state:\", agent_state.state)\n        # printd(\"State path:\", agent_config.save_state_dir())\n        # printd(\"Persistent manager path:\", agent_config.save_persistence_manager_dir())\n        # printd(\"Index path:\", agent_config.save_agent_index_dir())\n        # persistence_manager = LocalStateManager(agent_config).load() # TODO: implement load\n        # TODO: load prior agent state\n        if persona and persona != agent_state.persona_memory:\n            typer.secho(f\"{CLI_WARNING_PREFIX}Overriding existing persona {agent_state.persona_memory} with {persona}\", fg=typer.colors.YELLOW)\n            agent_state.persona_memory = persona\n            # raise ValueError(f\"Cannot override {agent_state.name} existing persona {agent_state.persona} with {persona}\")\n        if human and human != agent_state.human_memory:\n            typer.secho(f\"{CLI_WARNING_PREFIX}Overriding existing human {agent_state.human_memory} with {human}\", fg=typer.colors.YELLOW)\n            agent_state.human_memory = human\n            # raise ValueError(f\"Cannot override {agent_config.name} existing human {agent_config.human} with {human}\")\n\n        # Allow overriding model specifics (model, model wrapper, model endpoint IP + type, context_window)\n        if model and model != agent_state.llm_config.model:\n            typer.secho(\n                f\"{CLI_WARNING_PREFIX}Overriding existing model {agent_state.llm_config.model} with {model}\", fg=typer.colors.YELLOW\n            )\n            agent_state.llm_config.model = model\n        if context_window is not None and int(context_window) != agent_state.llm_config.context_window:\n            typer.secho(\n                f\"{CLI_WARNING_PREFIX}Overriding existing context window {agent_state.llm_config.context_window} with {context_window}\",\n                fg=typer.colors.YELLOW,\n            )\n            agent_state.llm_config.context_window = context_window\n        if model_wrapper and model_wrapper != agent_state.llm_config.model_wrapper:\n            typer.secho(\n                f\"{CLI_WARNING_PREFIX}Overriding existing model wrapper {agent_state.llm_config.model_wrapper} with {model_wrapper}\",\n                fg=typer.colors.YELLOW,\n            )\n            agent_state.llm_config.model_wrapper = model_wrapper\n        if model_endpoint and model_endpoint != agent_state.llm_config.model_endpoint:\n            typer.secho(\n                f\"{CLI_WARNING_PREFIX}Overriding existing model endpoint {agent_state.llm_config.model_endpoint} with {model_endpoint}\",\n                fg=typer.colors.YELLOW,\n            )\n            agent_state.llm_config.model_endpoint = model_endpoint\n        if model_endpoint_type and model_endpoint_type != agent_state.llm_config.model_endpoint_type:\n            typer.secho(\n                f\"{CLI_WARNING_PREFIX}Overriding existing model endpoint type {agent_state.llm_config.model_endpoint_type} with {model_endpoint_type}\",\n                fg=typer.colors.YELLOW,\n            )\n            agent_state.llm_config.model_endpoint_type = model_endpoint_type\n\n        # Update the agent with any overrides\n        ms.update_agent(agent_state)\n\n        # create agent\n        typeagent_agent = Agent(agent_state=agent_state, interface=interface())\n\n    else:  # create new agent\n        # create new agent config: override defaults with args if provided\n        type_agent = questionary.select(\n        \"Select Agent Type:\",\n        choices=CURRENT_AGENT_TYPE,\n        default=\"Memgpt\",\n        ).ask()\n        if type_agent is None:\n           raise KeyboardInterrupt\n        \n\n        type_agent=TYPEAGENT_TYPE[type_agent]\n        typer.secho(\"\\n🧬 Creating new agent...\", fg=typer.colors.WHITE)\n\n        agent_name = agent if agent else utils.create_random_username()\n        llm_config = config.default_llm_config\n        embedding_config = config.default_embedding_config  # TODO allow overriding embedding params via CLI run\n\n        # Allow overriding model specifics (model, model wrapper, model endpoint IP + type, context_window)\n        if model and model != llm_config.model:\n            typer.secho(f\"{CLI_WARNING_PREFIX}Overriding default model {llm_config.model} with {model}\", fg=typer.colors.YELLOW)\n            llm_config.model = model\n        if context_window is not None and int(context_window) != llm_config.context_window:\n            typer.secho(\n                f\"{CLI_WARNING_PREFIX}Overriding default context window {llm_config.context_window} with {context_window}\",\n                fg=typer.colors.YELLOW,\n            )\n            llm_config.context_window = context_window\n        if model_wrapper and model_wrapper != llm_config.model_wrapper:\n            typer.secho(\n                f\"{CLI_WARNING_PREFIX}Overriding existing model wrapper {llm_config.model_wrapper} with {model_wrapper}\",\n                fg=typer.colors.YELLOW,\n            )\n            llm_config.model_wrapper = model_wrapper\n        if model_endpoint and model_endpoint != llm_config.model_endpoint:\n            typer.secho(\n                f\"{CLI_WARNING_PREFIX}Overriding existing model endpoint {llm_config.model_endpoint} with {model_endpoint}\",\n                fg=typer.colors.YELLOW,\n            )\n            llm_config.model_endpoint = model_endpoint\n        if model_endpoint_type and model_endpoint_type != llm_config.model_endpoint_type:\n            typer.secho(\n                f\"{CLI_WARNING_PREFIX}Overriding existing model endpoint type {llm_config.model_endpoint_type} with {model_endpoint_type}\",\n                fg=typer.colors.YELLOW,\n            )\n            llm_config.model_endpoint_type = model_endpoint_type\n\n        # create agent\n        try:\n            preset_obj = ms.get_preset(name=preset if preset else config.preset, user_id=user.id)\n            human_obj = ms.get_human(preset_obj.human_name, user.id)\n            persona_obj = ms.get_persona(preset_obj.persona_name, user.id)\n            if preset_obj is None:\n                # create preset records in metadata store\n                from presets.presets import add_default_presets\n\n                add_default_presets(user.id, ms)\n                # try again\n                preset_obj = ms.get_preset(name=preset if preset else config.preset, user_id=user.id)\n                if preset_obj is None:\n                    typer.secho(\"Couldn't find presets in database, please run `typeagent configure`\", fg=typer.colors.RED)\n                    sys.exit(1)\n            if human_obj is None:\n                typer.secho(\"Couldn't find human {human} in database, please run `typeagent add human`\", fg=typer.colors.RED)\n            if persona_obj is None:\n                typer.secho(\"Couldn't find persona {persona} in database, please run `typeagent add persona`\", fg=typer.colors.RED)\n\n            # Overwrite fields in the preset if they were specified\n            # preset_obj.human = ms.get_human(human, user.id).text\n            # preset_obj.persona = ms.get_persona(persona, user.id).text\n\n            typer.secho(f\"->  🤖 Using persona profile: '{preset_obj.persona_name}'\", fg=typer.colors.WHITE)\n            typer.secho(f\"->  🧑 Using human profile: '{preset_obj.human_name}'\", fg=typer.colors.WHITE)\n\n            typeagent_agent = Agent(\n                interface=interface(),\n                name=agent_name,\n                type_agent=type_agent,\n                persona_memory=persona,\n                human_memory=human,\n                created_by=user.id,\n                preset=preset_obj,\n                llm_config=llm_config,\n                embedding_config=embedding_config,\n                # gpt-3.5-turbo tends to omit inner monologue, relax this requirement for now\n                # first_message_verify_mono=True if (model is not None and \"gpt-4\" in model) else False,\n            )\n            save_agent(agent=typeagent_agent, ms=ms)\n\n        except ValueError as e:\n            typer.secho(f\"Failed to create agent from provided information:\\n{e}\", fg=typer.colors.RED)\n            sys.exit(1)\n        typer.secho(f\"🎉 Created new agent '{typeagent_agent.agent_state.name}' (id={typeagent_agent.agent_state.id})\", fg=typer.colors.GREEN)\n\n    # start event loop\n    \n\n    print()  # extra space\n    run_agent_loop(\n        typeagent_agent=typeagent_agent, config=config, ms=ms, stream=stream,first=first,inner_thoughts_in_kwargs=no_content,\n    )  # TODO: add back no_verify\n\n\ndef delete_agent(\n    agent_name: Annotated[str, typer.Option(help=\"Specify agent to delete\")],\n    user_id: Annotated[Optional[str], typer.Option(help=\"User ID to associate with the agent.\")] = None,\n):\n    \"\"\"Delete an agent from the database\"\"\"\n    # use client ID is no user_id provided\n    config = typeagentConfig.load()\n    ms = MetadataStore(config)\n    if user_id is None:\n        user = create_default_user_or_exit(config, ms)\n    else:\n        user = ms.get_user(user_id=uuid.UUID(user_id))\n\n    try:\n        agent = ms.get_agent(agent_name=agent_name, user_id=user.id)\n    except Exception as e:\n        typer.secho(f\"Failed to get agent {agent_name}\\n{e}\", fg=typer.colors.RED)\n        sys.exit(1)\n\n    if agent is None:\n        typer.secho(f\"Couldn't find agent named '{agent_name}' to delete\", fg=typer.colors.RED)\n        sys.exit(1)\n\n    confirm = questionary.confirm(f\"Are you sure you want to delete agent '{agent_name}' (id={agent.id})?\", default=False).ask()\n    if confirm is None:\n        raise KeyboardInterrupt\n    if not confirm:\n        typer.secho(f\"Cancelled agent deletion '{agent_name}' (id={agent.id})\", fg=typer.colors.GREEN)\n        return\n\n    try:\n        archival_conn = StorageConnector.get_storage_connector(StorageType.ARCHIVAL_MEMORY, config, user_id=user_id, agent_id=agent.id)\n        archival_conn.delete({\"agent_id\": agent.id})\n        ms.delete_agent(agent_id=agent.id)\n        typer.secho(f\"🕊️ Successfully deleted agent '{agent_name}' (id={agent.id})\", fg=typer.colors.GREEN)\n    except Exception:\n        typer.secho(f\"Failed to delete agent '{agent_name}' (id={agent.id})\", fg=typer.colors.RED)\n        sys.exit(1)\n\n\ndef version():\n  \n\n    # print(TYPEAGENT_VERSION)\n    typer.secho(f\"TypeAgent Current Version: {TYPEAGENT_VERSION}\", fg=typer.colors.GREEN)\n    # return \"TypeAgent Current Version\"+TYPEAGENT_VERSION\n"}
{"type": "source_file", "path": "luann/errors.py", "content": "class LLMError(Exception):\n    \"\"\"Base class for all LLM-related errors.\"\"\"\n\n\nclass LLMJSONParsingError(LLMError):\n    \"\"\"Exception raised for errors in the JSON parsing process.\"\"\"\n\n    def __init__(self, message=\"Error parsing JSON generated by LLM\"):\n        self.message = message\n        super().__init__(self.message)\n\n\nclass LocalLLMError(LLMError):\n    \"\"\"Generic catch-all error for local LLM problems\"\"\"\n\n    def __init__(self, message=\"Encountered an error while running local LLM\"):\n        self.message = message\n        super().__init__(self.message)\n\n\nclass LocalLLMConnectionError(LLMError):\n    \"\"\"Error for when local LLM cannot be reached with provided IP/port\"\"\"\n\n    def __init__(self, message=\"Could not connect to local LLM\"):\n        self.message = message\n        super().__init__(self.message)\n"}
{"type": "source_file", "path": "luann/functions/function_sets/extras.py", "content": "import json\nimport os\nimport uuid\nfrom typing import Optional\n\nimport requests\n\nfrom constants import (\n    JSON_ENSURE_ASCII,\n    JSON_LOADS_STRICT,\n    MESSAGE_CHATGPT_FUNCTION_MODEL,\n    MESSAGE_CHATGPT_FUNCTION_SYSTEM_MESSAGE,\n)\nfrom data_types import Message\nfrom llm_api.llm_api_tools import create\n\n\ndef message_chatgpt(self, message: str):\n    \"\"\"\n    Send a message to a more basic AI, ChatGPT. A useful resource for asking questions. ChatGPT does not retain memory of previous interactions.\n\n    Args:\n        message (str): Message to send ChatGPT. Phrase your message as a full English sentence.\n\n    Returns:\n        str: Reply message from ChatGPT\n    \"\"\"\n    dummy_user_id = uuid.uuid4()\n    dummy_agent_id = uuid.uuid4()\n    message_sequence = [\n        Message(user_id=dummy_user_id, agent_id=dummy_agent_id, role=\"system\", text=MESSAGE_CHATGPT_FUNCTION_SYSTEM_MESSAGE),\n        Message(user_id=dummy_user_id, agent_id=dummy_agent_id, role=\"user\", text=str(message)),\n    ]\n    # TODO: this will error without an LLMConfig\n    response = create(\n        model=MESSAGE_CHATGPT_FUNCTION_MODEL,\n        messages=message_sequence,\n    )\n\n    reply = response.choices[0].message.content\n    return reply\n\n\ndef read_from_text_file(self, filename: str, line_start: int, num_lines: Optional[int] = 1):\n    \"\"\"\n    Read lines from a text file.\n\n    Args:\n        filename (str): The name of the file to read.\n        line_start (int): Line to start reading from.\n        num_lines (Optional[int]): How many lines to read (defaults to 1).\n\n    Returns:\n        str: Text read from the file\n    \"\"\"\n    max_chars = 500\n    trunc_message = True\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"The file '{filename}' does not exist.\")\n\n    if line_start < 1 or num_lines < 1:\n        raise ValueError(\"Both line_start and num_lines must be positive integers.\")\n\n    lines = []\n    chars_read = 0\n    with open(filename, \"r\", encoding=\"utf-8\") as file:\n        for current_line_number, line in enumerate(file, start=1):\n            if line_start <= current_line_number < line_start + num_lines:\n                chars_to_add = len(line)\n                if max_chars is not None and chars_read + chars_to_add > max_chars:\n                    # If adding this line exceeds MAX_CHARS, truncate the line if needed and stop reading further.\n                    excess_chars = (chars_read + chars_to_add) - max_chars\n                    lines.append(line[:-excess_chars].rstrip(\"\\n\"))\n                    if trunc_message:\n                        lines.append(f\"[SYSTEM ALERT - max chars ({max_chars}) reached during file read]\")\n                    break\n                else:\n                    lines.append(line.rstrip(\"\\n\"))\n                    chars_read += chars_to_add\n            if current_line_number >= line_start + num_lines - 1:\n                break\n\n    return \"\\n\".join(lines)\n\n\ndef append_to_text_file(self, filename: str, content: str):\n    \"\"\"\n    Append to a text file.\n\n    Args:\n        filename (str): The name of the file to append to.\n        content (str): Content to append to the file.\n\n    Returns:\n        Optional[str]: None is always returned as this function does not produce a response.\n    \"\"\"\n    if not os.path.exists(filename):\n        raise FileNotFoundError(f\"The file '{filename}' does not exist.\")\n\n    with open(filename, \"a\", encoding=\"utf-8\") as file:\n        file.write(content + \"\\n\")\n\n\ndef http_request(self, method: str, url: str, payload_json: Optional[str] = None):\n    \"\"\"\n    Generates an HTTP request and returns the response.\n\n    Args:\n        method (str): The HTTP method (e.g., 'GET', 'POST').\n        url (str): The URL for the request.\n        payload_json (Optional[str]): A JSON string representing the request payload.\n\n    Returns:\n        dict: The response from the HTTP request.\n    \"\"\"\n    try:\n        headers = {\"Content-Type\": \"application/json\"}\n\n        # For GET requests, ignore the payload\n        if method.upper() == \"GET\":\n            print(f\"[HTTP] launching GET request to {url}\")\n            response = requests.get(url, headers=headers)\n        else:\n            # Validate and convert the payload for other types of requests\n            if payload_json:\n                payload = json.loads(payload_json, strict=JSON_LOADS_STRICT)\n            else:\n                payload = {}\n            print(f\"[HTTP] launching {method} request to {url}, payload=\\n{json.dumps(payload, indent=2, ensure_ascii=JSON_ENSURE_ASCII)}\")\n            response = requests.request(method, url, json=payload, headers=headers)\n\n        return {\"status_code\": response.status_code, \"headers\": dict(response.headers), \"body\": response.text}\n    except Exception as e:\n        return {\"error\": str(e)}\n"}
{"type": "source_file", "path": "luann/agent_store/vectorsdb/qdrant.py", "content": "import os\nimport uuid\nfrom copy import deepcopy\nfrom typing import Dict, Iterator, List, Optional, cast\n\nfrom agent_store.storage import StorageConnector, TableType\nfrom config import MemGPTConfig\nfrom constants import MAX_EMBEDDING_DIM\nfrom data_types import Passage, Record, RecordType\nfrom utils import datetime_to_timestamp, timestamp_to_datetime\n\nTEXT_PAYLOAD_KEY = \"text_content\"\nMETADATA_PAYLOAD_KEY = \"metadata\"\n\n\nclass QdrantStorageConnector(StorageConnector):\n    \"\"\"Storage via Qdrant\"\"\"\n\n    def __init__(self, table_type: str, config: MemGPTConfig, user_id, agent_id=None):\n        super().__init__(table_type=table_type, config=config, user_id=user_id, agent_id=agent_id)\n        try:\n            from qdrant_client import QdrantClient, models\n        except ImportError as e:\n            raise ImportError(\"'qdrant-client' not installed. Run `pip install qdrant-client`.\") from e\n        assert table_type in [TableType.ARCHIVAL_MEMORY, TableType.PASSAGES], \"Qdrant only supports archival memory\"\n        if config.archival_storage_uri and len(config.archival_storage_uri.split(\":\")) == 2:\n            host, port = config.archival_storage_uri.split(\":\")\n            self.qdrant_client = QdrantClient(host=host, port=port, api_key=os.getenv(\"QDRANT_API_KEY\"))\n        elif config.archival_storage_path:\n            self.qdrant_client = QdrantClient(path=config.archival_storage_path)\n        else:\n            raise ValueError(\"Qdrant storage requires either a URI or a path to the storage configured\")\n        if not self.qdrant_client.collection_exists(self.table_name):\n            self.qdrant_client.create_collection(\n                collection_name=self.table_name,\n                vectors_config=models.VectorParams(\n                    size=MAX_EMBEDDING_DIM,\n                    distance=models.Distance.COSINE,\n                ),\n            )\n        self.uuid_fields = [\"id\", \"user_id\", \"agent_id\", \"source_id\", \"doc_id\"]\n\n    def get_all_paginated(self, filters: Optional[Dict] = {}, page_size: int = 10) -> Iterator[List[RecordType]]:\n        from qdrant_client import grpc\n\n        filters = self.get_qdrant_filters(filters)\n        next_offset = None\n        stop_scrolling = False\n        while not stop_scrolling:\n            results, next_offset = self.qdrant_client.scroll(\n                collection_name=self.table_name,\n                scroll_filter=filters,\n                limit=page_size,\n                offset=next_offset,\n                with_payload=True,\n                with_vectors=True,\n            )\n            stop_scrolling = next_offset is None or (\n                isinstance(next_offset, grpc.PointId) and next_offset.num == 0 and next_offset.uuid == \"\"\n            )\n            yield self.to_records(results)\n\n    def get_all(self, filters: Optional[Dict] = {}, limit=10) -> List[RecordType]:\n        if self.size(filters) == 0:\n            return []\n        filters = self.get_qdrant_filters(filters)\n        results, _ = self.qdrant_client.scroll(\n            self.table_name,\n            scroll_filter=filters,\n            limit=limit,\n            with_payload=True,\n            with_vectors=True,\n        )\n        return self.to_records(results)\n\n    def get(self, id: uuid.UUID) -> Optional[RecordType]:\n        results = self.qdrant_client.retrieve(\n            collection_name=self.table_name,\n            ids=[str(id)],\n            with_payload=True,\n            with_vectors=True,\n        )\n        if not results:\n            return None\n        return self.to_records(results)[0]\n\n    def insert(self, record: Record):\n        points = self.to_points([record])\n        self.qdrant_client.upsert(self.table_name, points=points)\n\n    def insert_many(self, records: List[RecordType], show_progress=False):\n        points = self.to_points(records)\n        self.qdrant_client.upsert(self.table_name, points=points)\n\n    def delete(self, filters: Optional[Dict] = {}):\n        filters = self.get_qdrant_filters(filters)\n        self.qdrant_client.delete(self.table_name, points_selector=filters)\n\n    def delete_table(self):\n        self.qdrant_client.delete_collection(self.table_name)\n        self.qdrant_client.close()\n\n    def size(self, filters: Optional[Dict] = {}) -> int:\n        filters = self.get_qdrant_filters(filters)\n        return self.qdrant_client.count(collection_name=self.table_name, count_filter=filters).count\n\n    def close(self):\n        self.qdrant_client.close()\n\n    def query(\n        self,\n        query: str,\n        query_vec: List[float],\n        top_k: int = 10,\n        filters: Optional[Dict] = {},\n    ) -> List[RecordType]:\n        filters = self.get_filters(filters)\n        results = self.qdrant_client.search(\n            self.table_name,\n            query_vector=query_vec,\n            query_filter=filters,\n            limit=top_k,\n            with_payload=True,\n            with_vectors=True,\n        )\n        return self.to_records(results)\n\n    def to_records(self, records: list) -> List[RecordType]:\n        parsed_records = []\n        for record in records:\n            record = deepcopy(record)\n            metadata = record.payload[METADATA_PAYLOAD_KEY]\n            text = record.payload[TEXT_PAYLOAD_KEY]\n            _id = metadata.pop(\"id\")\n            embedding = record.vector\n            for key, value in metadata.items():\n                if key in self.uuid_fields:\n                    metadata[key] = uuid.UUID(value)\n                elif key == \"created_at\":\n                    metadata[key] = timestamp_to_datetime(value)\n            parsed_records.append(\n                cast(\n                    RecordType,\n                    self.type(\n                        text=text,\n                        embedding=embedding,\n                        id=uuid.UUID(_id),\n                        **metadata,\n                    ),\n                )\n            )\n        return parsed_records\n\n    def to_points(self, records: List[RecordType]):\n        from qdrant_client import models\n\n        assert all(isinstance(r, Passage) for r in records)\n        points = []\n        records = list(set(records))\n        for record in records:\n            record = vars(record)\n            _id = record.pop(\"id\")\n            text = record.pop(\"text\", \"\")\n            embedding = record.pop(\"embedding\", {})\n            record_metadata = record.pop(\"metadata_\", None) or {}\n            if \"created_at\" in record:\n                record[\"created_at\"] = datetime_to_timestamp(record[\"created_at\"])\n            metadata = {key: value for key, value in record.items() if value is not None}\n            metadata = {\n                **metadata,\n                **record_metadata,\n                \"id\": str(_id),\n            }\n            for key, value in metadata.items():\n                if key in self.uuid_fields:\n                    metadata[key] = str(value)\n            points.append(\n                models.PointStruct(\n                    id=str(_id),\n                    vector=embedding,\n                    payload={\n                        TEXT_PAYLOAD_KEY: text,\n                        METADATA_PAYLOAD_KEY: metadata,\n                    },\n                )\n            )\n        return points\n\n    def get_qdrant_filters(self, filters: Optional[Dict] = {}):\n        from qdrant_client import models\n\n        filter_conditions = {**self.filters, **filters} if filters is not None else self.filters\n        must_conditions = []\n        for key, value in filter_conditions.items():\n            match_value = str(value) if key in self.uuid_fields else value\n            field_condition = models.FieldCondition(\n                key=f\"{METADATA_PAYLOAD_KEY}.{key}\",\n                match=models.MatchValue(value=match_value),\n            )\n            must_conditions.append(field_condition)\n        return models.Filter(must=must_conditions)"}
{"type": "source_file", "path": "luann/evaluation/metrics/_answer_correctness.py", "content": "from __future__ import annotations\n\nimport logging\nimport typing as t\nfrom dataclasses import dataclass, field\n\nimport numpy as np\nfrom pydantic import BaseModel\n\nfrom dataset_schema import SingleTurnSample\nfrom metrics._answer_similarity import AnswerSimilarity\nfrom metrics._faithfulness import (\n    FaithfulnessStatements,\n    HasSegmentMethod,\n    LongFormAnswerPrompt,\n)\nfrom metrics.base import (\n    MetricType,\n    MetricWithEmbeddings,\n    MetricWithLLM,\n    SingleTurnMetric,\n    get_segmenter,\n)\nfrom prompt.pydantic_prompt import PydanticPrompt\nfrom run_config import RunConfig\n\nif t.TYPE_CHECKING:\n    from langchain_core.callbacks import Callbacks\n\n    from metrics._faithfulness import SentencesSimplified\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass QuestionAnswerGroundTruth(BaseModel):\n    question: str\n    answer: list[str]\n    ground_truth: list[str]\n\n\nclass StatementsWithReason(BaseModel):\n    statement: str\n    reason: str\n\n\nclass ClassificationWithReason(BaseModel):\n    TP: list[StatementsWithReason]\n    FP: list[StatementsWithReason]\n    FN: list[StatementsWithReason]\n\n\nclass CorrectnessClassifier(\n    PydanticPrompt[QuestionAnswerGroundTruth, ClassificationWithReason]\n):\n    instruction = \"Given a ground truth and an answer statements, analyze each statement and classify them in one of the following categories: TP (true positive): statements that are present in answer that are also directly supported by the one or more statements in ground truth, FP (false positive): statements present in the answer but not directly supported by any statement in ground truth, FN (false negative): statements found in the ground truth but not present in answer. Each statement can only belong to one of the categories. Provide a reason for each classification.\"\n    input_model = QuestionAnswerGroundTruth\n    output_model = ClassificationWithReason\n    examples = [\n        (\n            QuestionAnswerGroundTruth(\n                question=\"What powers the sun and what is its primary function?\",\n                answer=[\n                    \"The sun is powered by nuclear fission, similar to nuclear reactors on Earth.\",\n                    \"The primary function of the sun is to provide light to the solar system.\",\n                ],\n                ground_truth=[\n                    \"The sun is powered by nuclear fusion, where hydrogen atoms fuse to form helium.\",\n                    \"This fusion process in the sun's core releases a tremendous amount of energy.\",\n                    \"The energy from the sun provides heat and light, which are essential for life on Earth.\",\n                    \"The sun's light plays a critical role in Earth's climate system.\",\n                    \"Sunlight helps to drive the weather and ocean currents.\",\n                ],\n            ),\n            ClassificationWithReason(\n                TP=[\n                    StatementsWithReason(\n                        statement=\"The primary function of the sun is to provide light to the solar system.\",\n                        reason=\"This statement is somewhat supported by the ground truth mentioning the sun providing light and its roles, though it focuses more broadly on the sun's energy.\",\n                    )\n                ],\n                FP=[\n                    StatementsWithReason(\n                        statement=\"The sun is powered by nuclear fission, similar to nuclear reactors on Earth.\",\n                        reason=\"This statement is incorrect and contradicts the ground truth which states that the sun is powered by nuclear fusion.\",\n                    )\n                ],\n                FN=[\n                    StatementsWithReason(\n                        statement=\"The sun is powered by nuclear fusion, where hydrogen atoms fuse to form helium.\",\n                        reason=\"This accurate description of the sun’s power source is not included in the answer.\",\n                    ),\n                    StatementsWithReason(\n                        statement=\"This fusion process in the sun's core releases a tremendous amount of energy.\",\n                        reason=\"This process and its significance are not mentioned in the answer.\",\n                    ),\n                    StatementsWithReason(\n                        statement=\"The energy from the sun provides heat and light, which are essential for life on Earth.\",\n                        reason=\"The answer only mentions light, omitting the essential aspects of heat and its necessity for life, which the ground truth covers.\",\n                    ),\n                    StatementsWithReason(\n                        statement=\"The sun's light plays a critical role in Earth's climate system.\",\n                        reason=\"This broader impact of the sun’s light on Earth's climate system is not addressed in the answer.\",\n                    ),\n                    StatementsWithReason(\n                        statement=\"Sunlight helps to drive the weather and ocean currents.\",\n                        reason=\"The effect of sunlight on weather patterns and ocean currents is omitted in the answer.\",\n                    ),\n                ],\n            ),\n        ),\n        (\n            QuestionAnswerGroundTruth(\n                question=\"What is the boiling point of water?\",\n                answer=[\n                    \"The boiling point of water is 100 degrees Celsius at sea level\"\n                ],\n                ground_truth=[\n                    \"The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit) at sea level.\",\n                    \"The boiling point of water can change with altitude.\",\n                ],\n            ),\n            ClassificationWithReason(\n                TP=[\n                    StatementsWithReason(\n                        statement=\"The boiling point of water is 100 degrees Celsius at sea level\",\n                        reason=\"This statement is directly supported by the ground truth which specifies the boiling point of water as 100 degrees Celsius at sea level.\",\n                    )\n                ],\n                FP=[],\n                FN=[\n                    StatementsWithReason(\n                        statement=\"The boiling point of water can change with altitude.\",\n                        reason=\"This additional information about how the boiling point of water can vary with altitude is not mentioned in the answer.\",\n                    )\n                ],\n            ),\n        ),\n    ]\n\n\n@dataclass\nclass AnswerCorrectness(MetricWithLLM, MetricWithEmbeddings, SingleTurnMetric):\n    \"\"\"\n    Measures answer correctness compared to ground truth as a combination of\n    factuality and semantic similarity.\n\n    Attributes\n    ----------\n    name: string\n        The name of the metrics\n    weights:\n        a list of two weights corresponding to factuality and semantic similarity\n        Defaults [0.75, 0.25]\n    answer_similarity:\n        The AnswerSimilarity object\n    \"\"\"\n\n    name: str = \"answer_correctness\"  # type: ignore[reportIncompatibleMethodOverride]\n    _required_columns: t.Dict[MetricType, t.Set[str]] = field(\n        default_factory=lambda: {\n            MetricType.SINGLE_TURN: {\"user_input\", \"response\", \"reference\"}\n        }\n    )\n    correctness_prompt: PydanticPrompt = field(default_factory=CorrectnessClassifier)\n    long_form_answer_prompt: PydanticPrompt = field(\n        default_factory=LongFormAnswerPrompt\n    )\n    weights: list[float] = field(default_factory=lambda: [0.75, 0.25])\n    answer_similarity: t.Optional[AnswerSimilarity] = None\n    sentence_segmenter: t.Optional[HasSegmentMethod] = None\n    max_retries: int = 1\n\n    def __post_init__(self: t.Self):\n        if len(self.weights) != 2:\n            raise ValueError(\n                \"Expects a list of two weights. First for factuality, second for semantic similarity\"\n            )\n        if all([w == 0 for w in self.weights]):\n            raise ValueError(\"At least one weight must be non-zero\")\n        if not all([w >= 0 for w in self.weights]):\n            raise ValueError(\"Weights must be non-negative\")\n\n        if self.sentence_segmenter is None:\n            language = self.long_form_answer_prompt.language\n            self.sentence_segmenter = get_segmenter(language=language, clean=False)\n\n    def init(self, run_config: RunConfig):\n        super().init(run_config)\n        if self.answer_similarity is None and self.weights[1] != 0:\n            self.answer_similarity = AnswerSimilarity(\n                llm=self.llm, embeddings=self.embeddings\n            )\n\n    def _compute_statement_presence(\n        self, prediction: ClassificationWithReason\n    ) -> float:\n        tp = len(prediction.TP)\n        fp = len(prediction.FP)\n        fn = len(prediction.FN)\n        score = tp / (tp + 0.5 * (fp + fn)) if tp > 0 else 0\n        return score\n\n    async def _create_simplified_statements(\n        self, question: str, text: str, callbacks: Callbacks\n    ) -> SentencesSimplified:\n        assert self.sentence_segmenter is not None, \"sentence_segmenter is not set\"\n        assert self.llm is not None, \"llm is not set\"\n\n        sentences = self.sentence_segmenter.segment(text)\n        sentences_with_index = {\n            i: sentence\n            for i, sentence in enumerate(sentences)\n            if sentence.strip().endswith(\".\")\n        }\n\n        statements_simplified = await self.long_form_answer_prompt.generate(\n            llm=self.llm,\n            data=FaithfulnessStatements(\n                question=question, answer=text, sentences=sentences_with_index\n            ),\n            callbacks=callbacks,\n        )\n        return statements_simplified\n\n    async def _single_turn_ascore(\n        self: t.Self, sample: SingleTurnSample, callbacks: Callbacks\n    ) -> float:\n        row = sample.to_dict()\n        score = await self._ascore(row, callbacks)\n        return score\n\n    async def _ascore(self, row: t.Dict, callbacks: Callbacks) -> float:\n        assert self.llm is not None, \"LLM must be set\"\n\n        # extract the statements from the answer and the ground truth\n        question = row[\"user_input\"]\n        statements: t.Dict[str, t.List[str]] = {}\n        for item in [\"response\", \"reference\"]:\n            simplified_statements = await self._create_simplified_statements(\n                question, row[item], callbacks\n            )\n            _statements_unwrapped = []\n            for component in simplified_statements.sentences:\n                _statements_unwrapped.extend(component.simpler_statements)\n            statements[item] = _statements_unwrapped\n\n        if not all([val == [] for val in statements.values()]):\n            ground_truth = [statement for statement in statements[\"reference\"]]\n            answer = [statement for statement in statements[\"response\"]]\n            answers = await self.correctness_prompt.generate(\n                llm=self.llm,\n                data=QuestionAnswerGroundTruth(\n                    question=question,\n                    answer=answer,\n                    ground_truth=ground_truth,\n                ),\n                callbacks=callbacks,\n            )\n            if answers is None:\n                return np.nan\n\n            f1_score = self._compute_statement_presence(answers)\n        else:\n            f1_score = 1.0\n\n        if self.weights[1] == 0:\n            similarity_score = 0.0\n        else:\n            assert self.answer_similarity is not None, \"AnswerSimilarity must be set\"\n\n            similarity_score = await self.answer_similarity.ascore(\n                row, callbacks=callbacks\n            )\n\n        score = np.average(\n            [f1_score, similarity_score],\n            weights=self.weights,\n        )\n\n        return float(score)\n\n\nanswer_correctness = AnswerCorrectness()"}
{"type": "source_file", "path": "luann/humans/__init__.py", "content": ""}
{"type": "source_file", "path": "luann/embeddings.py", "content": "import os\nimport uuid\nfrom typing import Any, List, Optional\n\nimport numpy as np\nimport requests\nfrom data_sources.text_splitters.basetextsplitter import TokenTextSplitter\n# from llama_index.core.base.embeddings import BaseEmbedding\n# from llama_index.core.embeddings import BaseEmbedding\n# from llama_index.core.base.embeddings.base import BaseEmbedding\n# from llama_index.bridge.pydantic import PrivateAttr\n# from llama_index.embeddings.base import BaseEmbedding\n# from llama_index.embeddings.huggingface_utils import format_text\nimport tiktoken\n# from llama_index.core import Document as LlamaIndexDocument\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Dict,\n    Generator,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Sequence,\n    TypeVar,\n    Union,\n    cast,\n)\n# from llama_index.core.base.embeddings import BaseEmbedding\n# from llama_index.core.node_parser import SentenceSplitter\nfrom abc import ABC, abstractmethod\nfrom constants import (\n    EMBEDDING_TO_TOKENIZER_DEFAULT,\n    EMBEDDING_TO_TOKENIZER_MAP,\n    MAX_EMBEDDING_DIM,\n)\nimport os\nimport warnings\nfrom typing import (\n    Any,\n    Dict,\n    Iterable,\n    List,\n    Literal,\n    Mapping,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Union,\n    cast,\n)\n\nimport openai\nimport tiktoken\nfrom credentials import typeagentCredentials\nfrom data_types import EmbeddingConfig\nfrom utils import is_valid_url, printd\nfrom concurrent.futures import Executor, Future, ThreadPoolExecutor\nfrom langchain_core.runnables.config import run_in_executor\nfrom langchain_core.pydantic_v1 import (\n    BaseModel,\n    Extra,\n    Field,\n    SecretStr,\n    root_validator,\n)\nfrom langchain_core.utils import (\n    convert_to_secret_str,\n    get_from_dict_or_env,\n    get_pydantic_field_names,\n)\nclass Embeddings(ABC):\n    \"\"\"Interface for embedding models.\"\"\"\n\n    @abstractmethod\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Embed search docs.\"\"\"\n\n    @abstractmethod\n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Embed query text.\"\"\"\n\n    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Asynchronous Embed search docs.\"\"\"\n        return await run_in_executor(None, self.embed_documents, texts)\n\n    async def aembed_query(self, text: str) -> List[float]:\n        \"\"\"Asynchronous Embed query text.\"\"\"\n        return await run_in_executor(None, self.embed_query, text)\n\n\ndef _process_batched_chunked_embeddings(\n    num_texts: int,\n    tokens: List[Union[List[int], str]],\n    batched_embeddings: List[List[float]],\n    indices: List[int],\n    skip_empty: bool,\n) -> List[Optional[List[float]]]:\n    # for each text, this is the list of embeddings (list of list of floats)\n    # corresponding to the chunks of the text\n    results: List[List[List[float]]] = [[] for _ in range(num_texts)]\n\n    # for each text, this is the token length of each chunk\n    # for transformers tokenization, this is the string length\n    # for tiktoken, this is the number of tokens\n    num_tokens_in_batch: List[List[int]] = [[] for _ in range(num_texts)]\n\n    for i in range(len(indices)):\n        if skip_empty and len(batched_embeddings[i]) == 1:\n            continue\n        results[indices[i]].append(batched_embeddings[i])\n        num_tokens_in_batch[indices[i]].append(len(tokens[i]))\n\n    # for each text, this is the final embedding\n    embeddings: List[Optional[List[float]]] = []\n    for i in range(num_texts):\n        # an embedding for each chunk\n        _result: List[List[float]] = results[i]\n\n        if len(_result) == 0:\n            # this will be populated with the embedding of an empty string\n            # in the sync or async code calling this\n            embeddings.append(None)\n            continue\n\n        elif len(_result) == 1:\n            # if only one embedding was produced, use it\n            embeddings.append(_result[0])\n            continue\n\n        else:\n            # else we need to weighted average\n            # should be same as\n            # average = np.average(_result, axis=0, weights=num_tokens_in_batch[i])\n            total_weight = sum(num_tokens_in_batch[i])\n            average = [\n                sum(\n                    val * weight\n                    for val, weight in zip(embedding, num_tokens_in_batch[i])\n                )\n                / total_weight\n                for embedding in zip(*_result)\n            ]\n\n            # should be same as\n            # embeddings.append((average / np.linalg.norm(average)).tolist())\n            magnitude = sum(val**2 for val in average) ** 0.5\n            embeddings.append([val / magnitude for val in average])\n\n    return embeddings\nclass OpenAIEmbeddings(BaseModel, Embeddings):\n    \"\"\"OpenAI embedding models.\n\n    To use, you should have the\n    environment variable ``OPENAI_API_KEY`` set with your API key or pass it\n    as a named parameter to the constructor.\n\n    In order to use the library with Microsoft Azure endpoints, use\n    AzureOpenAIEmbeddings.\n\n    Example:\n        .. code-block:: python\n\n            from langchain_openai import OpenAIEmbeddings\n\n            model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n    \"\"\"\n\n    client: Any = Field(default=None, exclude=True)  #: :meta private:\n    async_client: Any = Field(default=None, exclude=True)  #: :meta private:\n    model: str = \"text-embedding-ada-002\"\n    dimensions: Optional[int] = None\n    \"\"\"The number of dimensions the resulting output embeddings should have.\n\n    Only supported in `text-embedding-3` and later models.\n    \"\"\"\n    # to support Azure OpenAI Service custom deployment names\n    deployment: Optional[str] = model\n    # TODO: Move to AzureOpenAIEmbeddings.\n    openai_api_version: Optional[str] = Field(default=None, alias=\"api_version\")\n    \"\"\"Automatically inferred from env var `OPENAI_API_VERSION` if not provided.\"\"\"\n    # to support Azure OpenAI Service custom endpoints\n    openai_api_base: Optional[str] = Field(default=None, alias=\"base_url\")\n    \"\"\"Base URL path for API requests, leave blank if not using a proxy or service\n        emulator.\"\"\"\n    # to support Azure OpenAI Service custom endpoints\n    openai_api_type: Optional[str] = None\n    # to support explicit proxy for OpenAI\n    openai_proxy: Optional[str] = None\n    embedding_ctx_length: int = 8191\n    \"\"\"The maximum number of tokens to embed at once.\"\"\"\n    openai_api_key: Optional[SecretStr] = Field(default=None, alias=\"api_key\")\n    \"\"\"Automatically inferred from env var `OPENAI_API_KEY` if not provided.\"\"\"\n    openai_organization: Optional[str] = Field(default=None, alias=\"organization\")\n    \"\"\"Automatically inferred from env var `OPENAI_ORG_ID` if not provided.\"\"\"\n    allowed_special: Union[Literal[\"all\"], Set[str], None] = None\n    disallowed_special: Union[Literal[\"all\"], Set[str], Sequence[str], None] = None\n    chunk_size: int = 1000\n    \"\"\"Maximum number of texts to embed in each batch\"\"\"\n    max_retries: int = 2\n    \"\"\"Maximum number of retries to make when generating.\"\"\"\n    request_timeout: Optional[Union[float, Tuple[float, float], Any]] = Field(\n        default=None, alias=\"timeout\"\n    )\n    \"\"\"Timeout for requests to OpenAI completion API. Can be float, httpx.Timeout or\n        None.\"\"\"\n    headers: Any = None\n    tiktoken_enabled: bool = True\n    \"\"\"Set this to False for non-OpenAI implementations of the embeddings API, e.g.\n    the `--extensions openai` extension for `text-generation-webui`\"\"\"\n    tiktoken_model_name: Optional[str] = None\n    \"\"\"The model name to pass to tiktoken when using this class.\n    Tiktoken is used to count the number of tokens in documents to constrain\n    them to be under a certain limit. By default, when set to None, this will\n    be the same as the embedding model name. However, there are some cases\n    where you may want to use this Embedding class with a model name not\n    supported by tiktoken. This can include when using Azure embeddings or\n    when using one of the many model providers that expose an OpenAI-like\n    API but with different models. In those cases, in order to avoid erroring\n    when tiktoken is called, you can specify a model name to use here.\"\"\"\n    show_progress_bar: bool = False\n    \"\"\"Whether to show a progress bar when embedding.\"\"\"\n    model_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    \"\"\"Holds any model parameters valid for `create` call not explicitly specified.\"\"\"\n    skip_empty: bool = False\n    \"\"\"Whether to skip empty strings when embedding or raise an error.\n    Defaults to not skipping.\"\"\"\n    default_headers: Union[Mapping[str, str], None] = None\n    default_query: Union[Mapping[str, object], None] = None\n    # Configure a custom httpx client. See the\n    # [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n    retry_min_seconds: int = 4\n    \"\"\"Min number of seconds to wait between retries\"\"\"\n    retry_max_seconds: int = 20\n    \"\"\"Max number of seconds to wait between retries\"\"\"\n    http_client: Union[Any, None] = None\n    \"\"\"Optional httpx.Client. Only used for sync invocations. Must specify \n        http_async_client as well if you'd like a custom client for async invocations.\n    \"\"\"\n    http_async_client: Union[Any, None] = None\n    \"\"\"Optional httpx.AsyncClient. Only used for async invocations. Must specify \n        http_client as well if you'd like a custom client for sync invocations.\"\"\"\n    check_embedding_ctx_length: bool = True\n    \"\"\"Whether to check the token length of inputs and automatically split inputs \n        longer than embedding_ctx_length.\"\"\"\n\n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n\n        extra = Extra.forbid\n        allow_population_by_field_name = True\n\n    @root_validator(pre=True)\n    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\n        all_required_field_names = get_pydantic_field_names(cls)\n        extra = values.get(\"model_kwargs\", {})\n        for field_name in list(values):\n            if field_name in extra:\n                raise ValueError(f\"Found {field_name} supplied twice.\")\n            if field_name not in all_required_field_names:\n                warnings.warn(\n                    f\"\"\"WARNING! {field_name} is not default parameter.\n                    {field_name} was transferred to model_kwargs.\n                    Please confirm that {field_name} is what you intended.\"\"\"\n                )\n                extra[field_name] = values.pop(field_name)\n\n        invalid_model_kwargs = all_required_field_names.intersection(extra.keys())\n        if invalid_model_kwargs:\n            raise ValueError(\n                f\"Parameters {invalid_model_kwargs} should be specified explicitly. \"\n                f\"Instead they were passed in as part of `model_kwargs` parameter.\"\n            )\n\n        values[\"model_kwargs\"] = extra\n        return values\n\n    @root_validator()\n    def validate_environment(cls, values: Dict) -> Dict:\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\n        openai_api_key = get_from_dict_or_env(\n            values, \"openai_api_key\", \"OPENAI_API_KEY\"\n        )\n        values[\"openai_api_key\"] = (\n            convert_to_secret_str(openai_api_key) if openai_api_key else None\n        )\n        values[\"openai_api_base\"] = values[\"openai_api_base\"] or os.getenv(\n            \"OPENAI_API_BASE\"\n        )\n        values[\"openai_api_type\"] = get_from_dict_or_env(\n            values,\n            \"openai_api_type\",\n            \"OPENAI_API_TYPE\",\n            default=\"\",\n        )\n        values[\"openai_proxy\"] = get_from_dict_or_env(\n            values,\n            \"openai_proxy\",\n            \"OPENAI_PROXY\",\n            default=\"\",\n        )\n        if values[\"openai_api_type\"] in (\"azure\", \"azure_ad\", \"azuread\"):\n            default_api_version = \"2023-05-15\"\n            # Azure OpenAI embedding models allow a maximum of 16 texts\n            # at a time in each batch\n            # See: https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#embeddings\n            values[\"chunk_size\"] = min(values[\"chunk_size\"], 16)\n        else:\n            default_api_version = \"\"\n        values[\"openai_api_version\"] = get_from_dict_or_env(\n            values,\n            \"openai_api_version\",\n            \"OPENAI_API_VERSION\",\n            default=default_api_version,\n        )\n        # Check OPENAI_ORGANIZATION for backwards compatibility.\n        values[\"openai_organization\"] = (\n            values[\"openai_organization\"]\n            or os.getenv(\"OPENAI_ORG_ID\")\n            or os.getenv(\"OPENAI_ORGANIZATION\")\n        )\n        if values[\"openai_api_type\"] in (\"azure\", \"azure_ad\", \"azuread\"):\n            raise ValueError(\n                \"If you are using Azure, \"\n                \"please use the `AzureOpenAIEmbeddings` class.\"\n            )\n        client_params = {\n            \"api_key\": (\n                values[\"openai_api_key\"].get_secret_value()\n                if values[\"openai_api_key\"]\n                else None\n            ),\n            \"organization\": values[\"openai_organization\"],\n            \"base_url\": values[\"openai_api_base\"],\n            \"timeout\": values[\"request_timeout\"],\n            \"max_retries\": values[\"max_retries\"],\n            \"default_headers\": values[\"default_headers\"],\n            \"default_query\": values[\"default_query\"],\n        }\n        if not values.get(\"client\"):\n            sync_specific = {\"http_client\": values[\"http_client\"]}\n            values[\"client\"] = openai.OpenAI(\n                **client_params, **sync_specific\n            ).embeddings\n        if not values.get(\"async_client\"):\n            async_specific = {\"http_client\": values[\"http_async_client\"]}\n            values[\"async_client\"] = openai.AsyncOpenAI(\n                **client_params, **async_specific\n            ).embeddings\n        return values\n\n    @property\n    def _invocation_params(self) -> Dict[str, Any]:\n        params: Dict = {\"model\": self.model, **self.model_kwargs}\n        if self.dimensions is not None:\n            params[\"dimensions\"] = self.dimensions\n        return params\n\n    def _tokenize(\n        self, texts: List[str], chunk_size: int\n    ) -> Tuple[Iterable[int], List[Union[List[int], str]], List[int]]:\n        \"\"\"\n        Take the input `texts` and `chunk_size` and return 3 iterables as a tuple:\n\n        We have `batches`, where batches are sets of individual texts\n        we want responses from the openai api. The length of a single batch is\n        `chunk_size` texts.\n\n        Each individual text is also split into multiple texts based on the\n        `embedding_ctx_length` parameter (based on number of tokens).\n\n        This function returns a 3-tuple of the following:\n\n        _iter: An iterable of the starting index in `tokens` for each *batch*\n        tokens: A list of tokenized texts, where each text has already been split\n            into sub-texts based on the `embedding_ctx_length` parameter. In the\n            case of tiktoken, this is a list of token arrays. In the case of\n            HuggingFace transformers, this is a list of strings.\n        indices: An iterable of the same length as `tokens` that maps each token-array\n            to the index of the original text in `texts`.\n        \"\"\"\n        tokens: List[Union[List[int], str]] = []\n        indices: List[int] = []\n        model_name = self.tiktoken_model_name or self.model\n\n        # If tiktoken flag set to False\n        if not self.tiktoken_enabled:\n            try:\n                from transformers import AutoTokenizer\n            except ImportError:\n                raise ValueError(\n                    \"Could not import transformers python package. \"\n                    \"This is needed for OpenAIEmbeddings to work without \"\n                    \"`tiktoken`. Please install it with `pip install transformers`. \"\n                )\n\n            tokenizer = AutoTokenizer.from_pretrained(\n                pretrained_model_name_or_path=model_name\n            )\n            for i, text in enumerate(texts):\n                # Tokenize the text using HuggingFace transformers\n                tokenized: List[int] = tokenizer.encode(text, add_special_tokens=False)\n\n                # Split tokens into chunks respecting the embedding_ctx_length\n                for j in range(0, len(tokenized), self.embedding_ctx_length):\n                    token_chunk: List[int] = tokenized[\n                        j : j + self.embedding_ctx_length\n                    ]\n\n                    # Convert token IDs back to a string\n                    chunk_text: str = tokenizer.decode(token_chunk)\n                    tokens.append(chunk_text)\n                    indices.append(i)\n        else:\n            try:\n                encoding = tiktoken.encoding_for_model(model_name)\n            except KeyError:\n                encoding = tiktoken.get_encoding(\"cl100k_base\")\n            encoder_kwargs: Dict[str, Any] = {\n                k: v\n                for k, v in {\n                    \"allowed_special\": self.allowed_special,\n                    \"disallowed_special\": self.disallowed_special,\n                }.items()\n                if v is not None\n            }\n            for i, text in enumerate(texts):\n                if self.model.endswith(\"001\"):\n                    # See: https://github.com/openai/openai-python/\n                    #      issues/418#issuecomment-1525939500\n                    # replace newlines, which can negatively affect performance.\n                    text = text.replace(\"\\n\", \" \")\n\n                if encoder_kwargs:\n                    token = encoding.encode(text, **encoder_kwargs)\n                else:\n                    token = encoding.encode_ordinary(text)\n\n                # Split tokens into chunks respecting the embedding_ctx_length\n                for j in range(0, len(token), self.embedding_ctx_length):\n                    tokens.append(token[j : j + self.embedding_ctx_length])\n                    indices.append(i)\n\n        if self.show_progress_bar:\n            try:\n                from tqdm.auto import tqdm\n\n                _iter: Iterable = tqdm(range(0, len(tokens), chunk_size))\n            except ImportError:\n                _iter = range(0, len(tokens), chunk_size)\n        else:\n            _iter = range(0, len(tokens), chunk_size)\n        return _iter, tokens, indices\n\n    # please refer to\n    # https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb\n    def _get_len_safe_embeddings(\n        self, texts: List[str], *, engine: str, chunk_size: Optional[int] = None\n    ) -> List[List[float]]:\n        \"\"\"\n        Generate length-safe embeddings for a list of texts.\n\n        This method handles tokenization and embedding generation, respecting the\n        set embedding context length and chunk size. It supports both tiktoken\n        and HuggingFace tokenizer based on the tiktoken_enabled flag.\n\n        Args:\n            texts (List[str]): A list of texts to embed.\n            engine (str): The engine or model to use for embeddings.\n            chunk_size (Optional[int]): The size of chunks for processing embeddings.\n\n        Returns:\n            List[List[float]]: A list of embeddings for each input text.\n        \"\"\"\n        _chunk_size = chunk_size or self.chunk_size\n        _iter, tokens, indices = self._tokenize(texts, _chunk_size)\n        batched_embeddings: List[List[float]] = []\n        for i in _iter:\n            response = self.client.create(\n                input=tokens[i : i + _chunk_size], **self._invocation_params\n            )\n            if not isinstance(response, dict):\n                response = response.model_dump()\n            batched_embeddings.extend(r[\"embedding\"] for r in response[\"data\"])\n\n        embeddings = _process_batched_chunked_embeddings(\n            len(texts), tokens, batched_embeddings, indices, self.skip_empty\n        )\n        _cached_empty_embedding: Optional[List[float]] = None\n\n        def empty_embedding() -> List[float]:\n            nonlocal _cached_empty_embedding\n            if _cached_empty_embedding is None:\n                average_embedded = self.client.create(\n                    input=\"\", **self._invocation_params\n                )\n                if not isinstance(average_embedded, dict):\n                    average_embedded = average_embedded.model_dump()\n                _cached_empty_embedding = average_embedded[\"data\"][0][\"embedding\"]\n            return _cached_empty_embedding\n\n        return [e if e is not None else empty_embedding() for e in embeddings]\n\n    # please refer to\n    # https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb\n    async def _aget_len_safe_embeddings(\n        self, texts: List[str], *, engine: str, chunk_size: Optional[int] = None\n    ) -> List[List[float]]:\n        \"\"\"\n        Asynchronously generate length-safe embeddings for a list of texts.\n\n        This method handles tokenization and asynchronous embedding generation,\n        respecting the set embedding context length and chunk size. It supports both\n        `tiktoken` and HuggingFace `tokenizer` based on the tiktoken_enabled flag.\n\n        Args:\n            texts (List[str]): A list of texts to embed.\n            engine (str): The engine or model to use for embeddings.\n            chunk_size (Optional[int]): The size of chunks for processing embeddings.\n\n        Returns:\n            List[List[float]]: A list of embeddings for each input text.\n        \"\"\"\n\n        _chunk_size = chunk_size or self.chunk_size\n        _iter, tokens, indices = self._tokenize(texts, _chunk_size)\n        batched_embeddings: List[List[float]] = []\n        _chunk_size = chunk_size or self.chunk_size\n        for i in range(0, len(tokens), _chunk_size):\n            response = await self.async_client.create(\n                input=tokens[i : i + _chunk_size], **self._invocation_params\n            )\n\n            if not isinstance(response, dict):\n                response = response.model_dump()\n            batched_embeddings.extend(r[\"embedding\"] for r in response[\"data\"])\n\n        embeddings = _process_batched_chunked_embeddings(\n            len(texts), tokens, batched_embeddings, indices, self.skip_empty\n        )\n        _cached_empty_embedding: Optional[List[float]] = None\n\n        async def empty_embedding() -> List[float]:\n            nonlocal _cached_empty_embedding\n            if _cached_empty_embedding is None:\n                average_embedded = await self.async_client.create(\n                    input=\"\", **self._invocation_params\n                )\n                if not isinstance(average_embedded, dict):\n                    average_embedded = average_embedded.model_dump()\n                _cached_empty_embedding = average_embedded[\"data\"][0][\"embedding\"]\n            return _cached_empty_embedding\n\n        return [e if e is not None else await empty_embedding() for e in embeddings]\n\n    def embed_documents(\n        self, texts: List[str], chunk_size: Optional[int] = 0\n    ) -> List[List[float]]:\n        \"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\n\n        Args:\n            texts: The list of texts to embed.\n            chunk_size: The chunk size of embeddings. If None, will use the chunk size\n                specified by the class.\n\n        Returns:\n            List of embeddings, one for each text.\n        \"\"\"\n        if not self.check_embedding_ctx_length:\n            embeddings: List[List[float]] = []\n            for text in texts:\n                response = self.client.create(\n                    input=text,\n                    **self._invocation_params,\n                )\n                if not isinstance(response, dict):\n                    response = response.dict()\n                embeddings.extend(r[\"embedding\"] for r in response[\"data\"])\n            return embeddings\n\n        # NOTE: to keep things simple, we assume the list may contain texts longer\n        #       than the maximum context and use length-safe embedding function.\n        engine = cast(str, self.deployment)\n        return self._get_len_safe_embeddings(texts, engine=engine)\n\n    async def aembed_documents(\n        self, texts: List[str], chunk_size: Optional[int] = 0\n    ) -> List[List[float]]:\n        \"\"\"Call out to OpenAI's embedding endpoint async for embedding search docs.\n\n        Args:\n            texts: The list of texts to embed.\n            chunk_size: The chunk size of embeddings. If None, will use the chunk size\n                specified by the class.\n\n        Returns:\n            List of embeddings, one for each text.\n        \"\"\"\n        if not self.check_embedding_ctx_length:\n            embeddings: List[List[float]] = []\n            for text in texts:\n                response = await self.async_client.create(\n                    input=text,\n                    **self._invocation_params,\n                )\n                if not isinstance(response, dict):\n                    response = response.dict()\n                embeddings.extend(r[\"embedding\"] for r in response[\"data\"])\n            return embeddings\n\n        # NOTE: to keep things simple, we assume the list may contain texts longer\n        #       than the maximum context and use length-safe embedding function.\n        engine = cast(str, self.deployment)\n        return await self._aget_len_safe_embeddings(texts, engine=engine)\n\n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\n\n        Args:\n            text: The text to embed.\n\n        Returns:\n            Embedding for the text.\n        \"\"\"\n        return self.embed_documents([text])[0]\n\n    async def aembed_query(self, text: str) -> List[float]:\n        \"\"\"Call out to OpenAI's embedding endpoint async for embedding query text.\n\n        Args:\n            text: The text to embed.\n\n        Returns:\n            Embedding for the text.\n        \"\"\"\n        embeddings = await self.aembed_documents([text])\n        return embeddings[0]\nclass AzureOpenAIEmbeddings(OpenAIEmbeddings):\n    \"\"\"`Azure OpenAI` Embeddings API.\n\n    To use, you should have the\n    environment variable ``AZURE_OPENAI_API_KEY`` set with your API key or pass it\n    as a named parameter to the constructor.\n\n    Example:\n        .. code-block:: python\n\n            from langchain_openai import AzureOpenAIEmbeddings\n\n            openai = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\")\n    \"\"\"\n\n    azure_endpoint: Union[str, None] = None\n    \"\"\"Your Azure endpoint, including the resource.\n\n        Automatically inferred from env var `AZURE_OPENAI_ENDPOINT` if not provided.\n\n        Example: `https://example-resource.azure.openai.com/`\n    \"\"\"\n    deployment: Optional[str] = Field(default=None, alias=\"azure_deployment\")\n    \"\"\"A model deployment.\n\n        If given sets the base client URL to include `/deployments/{azure_deployment}`.\n        Note: this means you won't be able to use non-deployment endpoints.\n    \"\"\"\n    openai_api_key: Optional[SecretStr] = Field(default=None, alias=\"api_key\")\n    \"\"\"Automatically inferred from env var `AZURE_OPENAI_API_KEY` if not provided.\"\"\"\n    azure_ad_token: Optional[SecretStr] = None\n    \"\"\"Your Azure Active Directory token.\n\n        Automatically inferred from env var `AZURE_OPENAI_AD_TOKEN` if not provided.\n\n        For more:\n        https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id.\n    \"\"\"\n    azure_ad_token_provider: Union[Callable[[], str], None] = None\n    \"\"\"A function that returns an Azure Active Directory token.\n\n        Will be invoked on every request.\n    \"\"\"\n    openai_api_version: Optional[str] = Field(default=None, alias=\"api_version\")\n    \"\"\"Automatically inferred from env var `OPENAI_API_VERSION` if not provided.\"\"\"\n    validate_base_url: bool = True\n    chunk_size: int = 2048\n    \"\"\"Maximum number of texts to embed in each batch\"\"\"\n\n    @root_validator()\n    def validate_environment(cls, values: Dict) -> Dict:\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\n        # Check OPENAI_KEY for backwards compatibility.\n        # TODO: Remove OPENAI_API_KEY support to avoid possible conflict when using\n        # other forms of azure credentials.\n        openai_api_key = (\n            values[\"openai_api_key\"]\n            or os.getenv(\"AZURE_OPENAI_API_KEY\")\n            or os.getenv(\"OPENAI_API_KEY\")\n        )\n        values[\"openai_api_key\"] = (\n            convert_to_secret_str(openai_api_key) if openai_api_key else None\n        )\n        values[\"openai_api_base\"] = values[\"openai_api_base\"] or os.getenv(\n            \"OPENAI_API_BASE\"\n        )\n        values[\"openai_api_version\"] = values[\"openai_api_version\"] or os.getenv(\n            \"OPENAI_API_VERSION\", default=\"2023-05-15\"\n        )\n        values[\"openai_api_type\"] = get_from_dict_or_env(\n            values, \"openai_api_type\", \"OPENAI_API_TYPE\", default=\"azure\"\n        )\n        values[\"openai_organization\"] = (\n            values[\"openai_organization\"]\n            or os.getenv(\"OPENAI_ORG_ID\")\n            or os.getenv(\"OPENAI_ORGANIZATION\")\n        )\n        values[\"openai_proxy\"] = get_from_dict_or_env(\n            values,\n            \"openai_proxy\",\n            \"OPENAI_PROXY\",\n            default=\"\",\n        )\n        values[\"azure_endpoint\"] = values[\"azure_endpoint\"] or os.getenv(\n            \"AZURE_OPENAI_ENDPOINT\"\n        )\n        azure_ad_token = values[\"azure_ad_token\"] or os.getenv(\"AZURE_OPENAI_AD_TOKEN\")\n        values[\"azure_ad_token\"] = (\n            convert_to_secret_str(azure_ad_token) if azure_ad_token else None\n        )\n        # For backwards compatibility. Before openai v1, no distinction was made\n        # between azure_endpoint and base_url (openai_api_base).\n        openai_api_base = values[\"openai_api_base\"]\n        if openai_api_base and values[\"validate_base_url\"]:\n            if \"/openai\" not in openai_api_base:\n                values[\"openai_api_base\"] += \"/openai\"\n                raise ValueError(\n                    \"As of openai>=1.0.0, Azure endpoints should be specified via \"\n                    \"the `azure_endpoint` param not `openai_api_base` \"\n                    \"(or alias `base_url`). \"\n                )\n            if values[\"deployment\"]:\n                raise ValueError(\n                    \"As of openai>=1.0.0, if `deployment` (or alias \"\n                    \"`azure_deployment`) is specified then \"\n                    \"`openai_api_base` (or alias `base_url`) should not be. \"\n                    \"Instead use `deployment` (or alias `azure_deployment`) \"\n                    \"and `azure_endpoint`.\"\n                )\n        client_params = {\n            \"api_version\": values[\"openai_api_version\"],\n            \"azure_endpoint\": values[\"azure_endpoint\"],\n            \"azure_deployment\": values[\"deployment\"],\n            \"api_key\": (\n                values[\"openai_api_key\"].get_secret_value()\n                if values[\"openai_api_key\"]\n                else None\n            ),\n            \"azure_ad_token\": (\n                values[\"azure_ad_token\"].get_secret_value()\n                if values[\"azure_ad_token\"]\n                else None\n            ),\n            \"azure_ad_token_provider\": values[\"azure_ad_token_provider\"],\n            \"organization\": values[\"openai_organization\"],\n            \"base_url\": values[\"openai_api_base\"],\n            \"timeout\": values[\"request_timeout\"],\n            \"max_retries\": values[\"max_retries\"],\n            \"default_headers\": values[\"default_headers\"],\n            \"default_query\": values[\"default_query\"],\n        }\n        if not values.get(\"client\"):\n            sync_specific = {\"http_client\": values[\"http_client\"]}\n            values[\"client\"] = openai.AzureOpenAI(\n                **client_params, **sync_specific\n            ).embeddings\n        if not values.get(\"async_client\"):\n            async_specific = {\"http_client\": values[\"http_async_client\"]}\n            values[\"async_client\"] = openai.AsyncAzureOpenAI(\n                **client_params, **async_specific\n            ).embeddings\n        return values\n\n    @property\n    def _llm_type(self) -> str:\n        return \"azure-openai-chat\"\n\n\nclass OllamaEmbeddings(BaseModel, Embeddings):\n    \"\"\"Ollama locally runs large language models.\n\n    To use, follow the instructions at https://ollama.ai/.\n\n    Example:\n        .. code-block:: python\n\n            from langchain_community.embeddings import OllamaEmbeddings\n            ollama_emb = OllamaEmbeddings(\n                model=\"llama:7b\",\n            )\n            r1 = ollama_emb.embed_documents(\n                [\n                    \"Alpha is the first letter of Greek alphabet\",\n                    \"Beta is the second letter of Greek alphabet\",\n                ]\n            )\n            r2 = ollama_emb.embed_query(\n                \"What is the second letter of Greek alphabet\"\n            )\n\n    \"\"\"\n\n    base_url: str = \"http://localhost:11434\"\n    \"\"\"Base url the model is hosted under.\"\"\"\n    model: str = \"llama2\"\n    \"\"\"Model name to use.\"\"\"\n\n    embed_instruction: str = \"passage: \"\n    \"\"\"Instruction used to embed documents.\"\"\"\n    query_instruction: str = \"query: \"\n    \"\"\"Instruction used to embed the query.\"\"\"\n\n    mirostat: Optional[int] = None\n    \"\"\"Enable Mirostat sampling for controlling perplexity.\n    (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\"\"\"\n\n    mirostat_eta: Optional[float] = None\n    \"\"\"Influences how quickly the algorithm responds to feedback\n    from the generated text. A lower learning rate will result in\n    slower adjustments, while a higher learning rate will make\n    the algorithm more responsive. (Default: 0.1)\"\"\"\n\n    mirostat_tau: Optional[float] = None\n    \"\"\"Controls the balance between coherence and diversity\n    of the output. A lower value will result in more focused and\n    coherent text. (Default: 5.0)\"\"\"\n\n    num_ctx: Optional[int] = None\n    \"\"\"Sets the size of the context window used to generate the\n    next token. (Default: 2048)\t\"\"\"\n\n    num_gpu: Optional[int] = None\n    \"\"\"The number of GPUs to use. On macOS it defaults to 1 to\n    enable metal support, 0 to disable.\"\"\"\n\n    num_thread: Optional[int] = None\n    \"\"\"Sets the number of threads to use during computation.\n    By default, Ollama will detect this for optimal performance.\n    It is recommended to set this value to the number of physical\n    CPU cores your system has (as opposed to the logical number of cores).\"\"\"\n\n    repeat_last_n: Optional[int] = None\n    \"\"\"Sets how far back for the model to look back to prevent\n    repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\"\"\"\n\n    repeat_penalty: Optional[float] = None\n    \"\"\"Sets how strongly to penalize repetitions. A higher value (e.g., 1.5)\n    will penalize repetitions more strongly, while a lower value (e.g., 0.9)\n    will be more lenient. (Default: 1.1)\"\"\"\n\n    temperature: Optional[float] = None\n    \"\"\"The temperature of the model. Increasing the temperature will\n    make the model answer more creatively. (Default: 0.8)\"\"\"\n\n    stop: Optional[List[str]] = None\n    \"\"\"Sets the stop tokens to use.\"\"\"\n\n    tfs_z: Optional[float] = None\n    \"\"\"Tail free sampling is used to reduce the impact of less probable\n    tokens from the output. A higher value (e.g., 2.0) will reduce the\n    impact more, while a value of 1.0 disables this setting. (default: 1)\"\"\"\n\n    top_k: Optional[int] = None\n    \"\"\"Reduces the probability of generating nonsense. A higher value (e.g. 100)\n    will give more diverse answers, while a lower value (e.g. 10)\n    will be more conservative. (Default: 40)\"\"\"\n\n    top_p: Optional[float] = None\n    \"\"\"Works together with top-k. A higher value (e.g., 0.95) will lead\n    to more diverse text, while a lower value (e.g., 0.5) will\n    generate more focused and conservative text. (Default: 0.9)\"\"\"\n\n    show_progress: bool = False\n    \"\"\"Whether to show a tqdm progress bar. Must have `tqdm` installed.\"\"\"\n\n    headers: Optional[dict] = None\n    \"\"\"Additional headers to pass to endpoint (e.g. Authorization, Referer).\n    This is useful when Ollama is hosted on cloud services that require\n    tokens for authentication.\n    \"\"\"\n\n    @property\n    def _default_params(self) -> Dict[str, Any]:\n        \"\"\"Get the default parameters for calling Ollama.\"\"\"\n        return {\n            \"model\": self.model,\n            \"options\": {\n                \"mirostat\": self.mirostat,\n                \"mirostat_eta\": self.mirostat_eta,\n                \"mirostat_tau\": self.mirostat_tau,\n                \"num_ctx\": self.num_ctx,\n                \"num_gpu\": self.num_gpu,\n                \"num_thread\": self.num_thread,\n                \"repeat_last_n\": self.repeat_last_n,\n                \"repeat_penalty\": self.repeat_penalty,\n                \"temperature\": self.temperature,\n                \"stop\": self.stop,\n                \"tfs_z\": self.tfs_z,\n                \"top_k\": self.top_k,\n                \"top_p\": self.top_p,\n            },\n        }\n\n    model_kwargs: Optional[dict] = None\n    \"\"\"Other model keyword args\"\"\"\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {**{\"model\": self.model}, **self._default_params}\n\n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n\n        extra = Extra.forbid\n\n    def _process_emb_response(self, input: str) -> List[float]:\n        \"\"\"Process a response from the API.\n\n        Args:\n            response: The response from the API.\n\n        Returns:\n            The response as a dictionary.\n        \"\"\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            **(self.headers or {}),\n        }\n\n        try:\n            res = requests.post(\n                f\"{self.base_url}/api/embeddings\",\n                headers=headers,\n                json={\"model\": self.model, \"prompt\": input, **self._default_params},\n            )\n        except requests.exceptions.RequestException as e:\n            raise ValueError(f\"Error raised by inference endpoint: {e}\")\n\n        if res.status_code != 200:\n            raise ValueError(\n                \"Error raised by inference API HTTP code: %s, %s\"\n                % (res.status_code, res.text)\n            )\n        try:\n            t = res.json()\n            return t[\"embedding\"]\n        except requests.exceptions.JSONDecodeError as e:\n            raise ValueError(\n                f\"Error raised by inference API: {e}.\\nResponse: {res.text}\"\n            )\n\n    def _embed(self, input: List[str]) -> List[List[float]]:\n        if self.show_progress:\n            try:\n                from tqdm import tqdm\n\n                iter_ = tqdm(input, desc=\"OllamaEmbeddings\")\n            except ImportError:\n                print(\n                    \"Unable to show progress bar because tqdm could not be imported. \"\n                    \"Please install with `pip install tqdm`.\"\n                )\n                iter_ = input\n        else:\n            iter_ = input\n        return [self._process_emb_response(prompt) for prompt in iter_]\n\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"Embed documents using an Ollama deployed embedding model.\n\n        Args:\n            texts: The list of texts to embed.\n\n        Returns:\n            List of embeddings, one for each text.\n        \"\"\"\n        instruction_pairs = [f\"{self.embed_instruction}{text}\" for text in texts]\n        embeddings = self._embed(instruction_pairs)\n        return embeddings\n\n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Embed a query using a Ollama deployed embedding model.\n\n        Args:\n            text: The text to embed.\n\n        Returns:\n            Embeddings for the text.\n        \"\"\"\n        instruction_pair = f\"{self.query_instruction}{text}\"\n        embedding = self._embed([instruction_pair])[0]\n        return embedding\n# def parse_and_chunk_text(text: str, chunk_size: int) -> List[str]:\n#     parser = SentenceSplitter(chunk_size=chunk_size)\n#     llama_index_docs = [LlamaIndexDocument(text=text)]\n#     nodes = parser.get_nodes_from_documents(llama_index_docs)\n#     return [n.text for n in nodes]\n\ndef generate_passages( documents: str, chunk_size: int = 1024) -> Iterator[Tuple[str, Dict]]:  # -> Iterator[Passage]:\n        # use llama index to run embeddings code\n        # from llama_index.core.node_parser import SentenceSplitter\n        # from llama_index.core.node_parser import TokenTextSplitter\n\n        parser = TokenTextSplitter(chunk_size=chunk_size)\n        # for document in documents:\n        passages_from_docs=parser.split_text(text=documents)\n            # llama_index_docs = [LlamaIndexDocument(text=document.text, metadata=document.metadata)]\n            # nodes = parser.get_nodes_from_documents(llama_index_docs)\n        for nodedocs in passages_from_docs:\n                # passage = Passage(\n                #    text=node.text,\n                #    doc_id=document.id,\n                # )\n            yield nodedocs, None\n\ndef truncate_text(text: str, max_length: int, encoding) -> str:\n    # truncate the text based on max_length and encoding\n    encoded_text = encoding.encode(text)[:max_length]\n    return encoding.decode(encoded_text)\n\n\n# def check_and_split_text(text: str, embedding_model: str) -> List[str]:\n#     \"\"\"Split text into chunks of max_length tokens or less\"\"\"\n\n#     if embedding_model in EMBEDDING_TO_TOKENIZER_MAP:\n#         encoding = tiktoken.get_encoding(EMBEDDING_TO_TOKENIZER_MAP[embedding_model])\n#     else:\n#         print(f\"Warning: couldn't find tokenizer for model {embedding_model}, using default tokenizer {EMBEDDING_TO_TOKENIZER_DEFAULT}\")\n#         encoding = tiktoken.get_encoding(EMBEDDING_TO_TOKENIZER_DEFAULT)\n\n#     num_tokens = len(encoding.encode(text))\n\n#     # determine max length\n#     if hasattr(encoding, \"max_length\"):\n#         # TODO(fix) this is broken\n#         max_length = encoding.max_length\n#     else:\n#         # TODO: figure out the real number\n#         printd(f\"Warning: couldn't find max_length for tokenizer {embedding_model}, using default max_length 8191\")\n#         max_length = 8191\n\n#     # truncate text if too long\n#     if num_tokens > max_length:\n#         print(f\"Warning: text is too long ({num_tokens} tokens), truncating to {max_length} tokens.\")\n#         # First, apply any necessary formatting\n#         formatted_text = format_text(text, embedding_model)\n#         # Then truncate\n#         text = truncate_text(formatted_text, max_length, encoding)\n\n#     return [text]\n\n\nclass EmbeddingEndpoint:\n    \"\"\"Implementation for OpenAI compatible endpoint\"\"\"\n\n    # \"\"\" Based off llama index https://github.com/run-llama/llama_index/blob/a98bdb8ecee513dc2e880f56674e7fd157d1dc3a/llama_index/embeddings/text_embeddings_inference.py \"\"\"\n\n    # _user: str = PrivateAttr()\n    # _timeout: float = PrivateAttr()\n    # _base_url: str = PrivateAttr()\n\n    def __init__(\n        self,\n        model: str,\n        base_url: str,\n        user: str,\n        timeout: float = 60.0,\n        **kwargs: Any,\n    ):\n        if not is_valid_url(base_url):\n            raise ValueError(\n                f\"Embeddings endpoint was provided an invalid URL (set to: '{base_url}'). Make sure embedding_endpoint is set correctly in your typeagent config.\"\n            )\n        self.model_name = model\n        self._user = user\n        self._base_url = base_url\n        self._timeout = timeout\n\n    def _call_api(self, text: str) -> List[float]:\n        if not is_valid_url(self._base_url):\n            raise ValueError(\n                f\"Embeddings endpoint does not have a valid URL (set to: '{self._base_url}'). Make sure embedding_endpoint is set correctly in your typeagent config.\"\n            )\n        import httpx\n\n        headers = {\"Content-Type\": \"application/json\"}\n        json_data = {\"input\": text, \"model\": self.model_name, \"user\": self._user}\n\n        with httpx.Client() as client:\n            response = client.post(\n                f\"{self._base_url}/embeddings\",\n                headers=headers,\n                json=json_data,\n                timeout=self._timeout,\n            )\n\n        response_json = response.json()\n\n        if isinstance(response_json, list):\n            # embedding directly in response\n            embedding = response_json\n        elif isinstance(response_json, dict):\n            # TEI embedding packaged inside openai-style response\n            try:\n                embedding = response_json[\"data\"][0][\"embedding\"]\n            except (KeyError, IndexError):\n                raise TypeError(f\"Got back an unexpected payload from text embedding function, response=\\n{response_json}\")\n        else:\n            # unknown response, can't parse\n            raise TypeError(f\"Got back an unexpected payload from text embedding function, response=\\n{response_json}\")\n\n        return embedding\n\n    def get_text_embedding(self, text: str) -> List[float]:\n        return self._call_api(text)\n\n\ndef default_embedding_model():\n    # default to hugging face model running local\n    # warning: this is a terrible model\n    # from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n  \n    # os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n    # model = \"BAAI/bge-small-en-v1.5\"\n    # return HuggingFaceEmbedding(model_name=model)\n    return \"no implenment\"\n\n\ndef query_embedding(embedding_model, query_text: str):\n    \"\"\"Generate padded embedding for querying database\"\"\"\n    query_vec = embedding_model.get_text_embedding(query_text)\n    query_vec = np.array(query_vec)\n    query_vec = np.pad(query_vec, (0, MAX_EMBEDDING_DIM - query_vec.shape[0]), mode=\"constant\").tolist()\n    return query_vec\n\n\ndef embedding_model(config: EmbeddingConfig, user_id: Optional[uuid.UUID] = None):\n    \"\"\"Return LlamaIndex embedding model to use for embeddings\"\"\"\n\n    endpoint_type = config.embedding_endpoint_type\n\n    # TODO refactor to pass credentials through args\n    credentials = typeagentCredentials.load()\n\n    if endpoint_type == \"openai\":\n        # assert credentials.openai_key is not None\n        # from llama_index.embeddings.openai import OpenAIEmbedding\n        # from langchain_openai import OpenAIEmbeddings\n        # additional_kwargs = {\"user_id\": user_id} if user_id else {}\n        model = OpenAIEmbeddings(\n            openai_api_base=config.embedding_endpoint,\n            openai_api_key=credentials.openai_key,\n            model= 'text-embedding-ada-002',\n        )\n        return model\n\n    elif endpoint_type == \"azure\":\n       \n        deployment = credentials.azure_embedding_deployment if credentials.azure_embedding_deployment is not None else model\n    \n        return AzureOpenAIEmbeddings(\n            azure_deployment=deployment,\n            api_key=credentials.azure_embedding_api_key,\n            azure_endpoint=credentials.azure_embedding_endpoint,\n            api_version=credentials.azure_embedding_version,\n        )\n\n    elif endpoint_type == \"hugging-face\":\n        return EmbeddingEndpoint(\n            model=config.embedding_model,\n            base_url=config.embedding_endpoint,\n            user=user_id,\n        )\n    elif endpoint_type == \"ollama\":\n\n        # from llama_index.embeddings.ollama import OllamaEmbedding\n\n        ollama_additional_kwargs = {}\n        callback_manager = None\n\n        model =OllamaEmbeddings(\n            model_name=config.embedding_model,\n            base_url=config.embedding_endpoint,\n            ollama_additional_kwargs=ollama_additional_kwargs or {},\n            callback_manager=callback_manager or None,\n        )\n        return model\n\n    else:\n        return default_embedding_model()\n"}
{"type": "source_file", "path": "luann/llm_api/__init__.py", "content": ""}
{"type": "source_file", "path": "luann/functions/functions.py", "content": "import importlib\nimport inspect\nimport os\nimport sys\nimport warnings\nfrom types import ModuleType\n\nfrom constants import CLI_WARNING_PREFIX, typeagent_DIR,TYPEAGENT_PROJECT_DIR\nfrom functions.schema_generator import generate_schema\nfrom textwrap import dedent  # remove indentation\nUSER_FUNCTIONS_DIR = os.path.join(typeagent_DIR, \"functions\")\n\nsys.path.append(USER_FUNCTIONS_DIR)\n\ndef parse_source_code(func) -> str:\n    \"\"\"Parse the source code of a function and remove indendation\"\"\"\n    source_code = dedent(inspect.getsource(func))\n    return source_code\ndef load_function_set(module: ModuleType) -> dict:\n    \"\"\"Load the functions and generate schema for them, given a module object\"\"\"\n    function_dict = {}\n\n    for attr_name in dir(module):\n        # Get the attribute\n        attr = getattr(module, attr_name)\n\n        # Check if it's a callable function and not a built-in or special method\n        if inspect.isfunction(attr) and attr.__module__ == module.__name__:\n            if attr_name in function_dict:\n                raise ValueError(f\"Found a duplicate of function name '{attr_name}'\")\n\n            generated_schema = generate_schema(attr)\n            function_dict[attr_name] = {\n                \"module\": inspect.getsource(module),\n                \"python_function\": attr,\n                \"json_schema\": generated_schema,\n            }\n\n    if len(function_dict) == 0:\n        raise ValueError(f\"No functions found in module {module}\")\n    return function_dict\n\n\ndef validate_function(module_name, module_full_path):\n    try:\n        file = os.path.basename(module_full_path)\n        spec = importlib.util.spec_from_file_location(module_name, module_full_path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n    except ModuleNotFoundError as e:\n        # Handle missing module imports\n        missing_package = str(e).split(\"'\")[1]  # Extract the name of the missing package\n        print(f\"{CLI_WARNING_PREFIX}skipped loading python file '{module_full_path}'!\")\n        return (\n            False,\n            f\"'{file}' imports '{missing_package}', but '{missing_package}' is not installed locally - install python package '{missing_package}' to link functions from '{file}' to \",\n        )\n    except SyntaxError as e:\n        # Handle syntax errors in the module\n        return False, f\"{CLI_WARNING_PREFIX}skipped loading python file '{file}' due to a syntax error: {e}\"\n    except Exception as e:\n        # Handle other general exceptions\n        return False, f\"{CLI_WARNING_PREFIX}skipped loading python file '{file}': {e}\"\n\n    return True, None\n\n\ndef write_function(module_name: str, function_code: str):\n    \"\"\"Write a function to a file in the user functions directory\"\"\"\n    # Create the user functions directory if it doesn't exist\n    if not os.path.exists(USER_FUNCTIONS_DIR):\n        os.makedirs(USER_FUNCTIONS_DIR)\n\n    # Write the function to a file\n    file_path = os.path.join(USER_FUNCTIONS_DIR, f\"{module_name}.py\")\n    with open(file_path, \"w\") as f:\n        f.write(function_code)\n    succ, error = validate_function(module_name, file_path)\n\n    # raise error if function cannot be loaded\n    if not succ:\n        raise ValueError(error)\n    return file_path\ndef load_function_file(filepath: str) -> dict:\n    file = os.path.basename(filepath)\n    module_name = file[:-3]  # Remove '.py' from filename\n    try:\n        spec = importlib.util.spec_from_file_location(module_name, filepath)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n    except ModuleNotFoundError as e:\n        # Handle missing module imports\n        missing_package = str(e).split(\"'\")[1]  # Extract the name of the missing package\n        print(f\"{CLI_WARNING_PREFIX}skipped loading python file '{filepath}'!\")\n        print(\n            f\"'{file}' imports '{missing_package}', but '{missing_package}' is not installed locally - install python package '{missing_package}' to link functions from '{file}' to MemGPT.\"\n        )\n    # load all functions in the module\n    function_dict = load_function_set(module)\n    return function_dict\ndef load_all_function_sets(merge: bool = True, ignore_duplicates: bool = True) -> dict:\n    from utils import printd\n\n    # functions/examples/*.py\n\n    function_sets_dir = os.path.join(TYPEAGENT_PROJECT_DIR, \"functions\", \"function_sets\")\n    # scripts_dir = os.path.dirname(os.path.abspath(__file__))  # Get the directory of the current script\n    # print(scripts_dir)\n    # function_sets_dir = os.path.join(scripts_dir, \"function_sets\")  # Path to the function_sets directory\n    # List all .py files in the directory (excluding __init__.py)\n    example_module_files = [f for f in os.listdir(function_sets_dir) if f.endswith(\".py\") and f != \"__init__.py\"]\n\n    # ~/.typeagent/functions/*.py\n    # create if missing\n    if not os.path.exists(USER_FUNCTIONS_DIR):\n        os.makedirs(USER_FUNCTIONS_DIR)\n    user_module_files = [f for f in os.listdir(USER_FUNCTIONS_DIR) if f.endswith(\".py\") and f != \"__init__.py\"]\n\n    # combine them both (pull from both examples and user-provided)\n    # all_module_files = example_module_files + user_module_files\n\n    # Add user_scripts_dir to sys.path\n    if USER_FUNCTIONS_DIR not in sys.path:\n        sys.path.append(USER_FUNCTIONS_DIR)\n\n    schemas_and_functions = {}\n    for dir_path, module_files in [(function_sets_dir, example_module_files), (USER_FUNCTIONS_DIR, user_module_files)]:\n        for file in module_files:\n            tags = []\n            module_name = file[:-3]  # Remove '.py' from filename\n            if dir_path == USER_FUNCTIONS_DIR:\n                # For user scripts, adjust the module name appropriately\n                module_full_path = os.path.join(dir_path, file)\n                printd(f\"Loading user function set from '{module_full_path}'\")\n                try:\n                    spec = importlib.util.spec_from_file_location(module_name, module_full_path)\n                    module = importlib.util.module_from_spec(spec)\n                    spec.loader.exec_module(module)\n                except ModuleNotFoundError as e:\n                    # Handle missing module imports\n                    missing_package = str(e).split(\"'\")[1]  # Extract the name of the missing package\n                    printd(f\"{CLI_WARNING_PREFIX}skipped loading python file '{module_full_path}'!\")\n                    printd(\n                        f\"'{file}' imports '{missing_package}', but '{missing_package}' is not installed locally - install python package '{missing_package}' to link functions from '{file}' to \"\n                    )\n                    continue\n                except SyntaxError as e:\n                    # Handle syntax errors in the module\n                    printd(f\"{CLI_WARNING_PREFIX}skipped loading python file '{file}' due to a syntax error: {e}\")\n                    continue\n                except Exception as e:\n                    # Handle other general exceptions\n                    printd(f\"{CLI_WARNING_PREFIX}skipped loading python file '{file}': {e}\")\n                    continue\n            else:\n                # For built-in scripts, use the existing method\n                full_module_name = f\"functions.function_sets.{module_name}\"\n                tags.append(f\"typeagent-{module_name}\")\n                try:\n                    module = importlib.import_module(full_module_name)\n                except Exception as e:\n                    # Handle other general exceptions\n                    printd(f\"{CLI_WARNING_PREFIX}skipped loading python module '{full_module_name}': {e}\")\n                    continue\n\n            try:\n                # Load the function set\n                function_set = load_function_set(module)\n                # Add the metadata tags\n                for k, v in function_set.items():\n                    # print(function_set)\n                    v[\"tags\"] = tags\n                schemas_and_functions[module_name] = function_set\n            except ValueError as e:\n                err = f\"Error loading function set '{module_name}': {e}\"\n                printd(err)\n                warnings.warn(err)\n\n    if merge:\n        # Put all functions from all sets into the same level dict\n        merged_functions = {}\n        for set_name, function_set in schemas_and_functions.items():\n            for function_name, function_info in function_set.items():\n                if function_name in merged_functions:\n                    err_msg = f\"Duplicate function name '{function_name}' found in function set '{set_name}'\"\n                    if ignore_duplicates:\n                        warnings.warn(err_msg, category=UserWarning, stacklevel=2)\n                    else:\n                        raise ValueError(err_msg)\n                else:\n                    merged_functions[function_name] = function_info\n        return merged_functions\n    else:\n        # Nested dict where the top level is organized by the function set name\n        return schemas_and_functions\n"}
{"type": "source_file", "path": "luann/data_types.py", "content": "\"\"\" This module contains the data types used by  Each data type must include a function to create a DB model. \"\"\"\n\nimport json\nimport uuid\nfrom datetime import datetime, timezone\nfrom typing import Dict, List, Optional, TypeVar\nimport copy\nimport numpy as np\nfrom pydantic import BaseModel, Field\nimport warnings\nfrom constants import (\n    DEFAULT_HUMAN,\n    DEFAULT_PERSONA,\n    LLM_MAX_TOKENS,\n    MAX_EMBEDDING_DIM,\n    TOOL_CALL_ID_MAX_LEN,\n    DEFAULT_PRESET,\n    JSON_ENSURE_ASCII,\n)\nfrom local_llm.constants import INNER_THOUGHTS_KWARG\nfrom prompts import gpt_system\nfrom utils import (\n    create_uuid_from_string,\n    get_human_text,\n    get_persona_text,\n    get_utc_time,\n    is_utc_datetime,\n)\n\n\nclass Record:\n    \"\"\"\n    Base class for an agent's memory unit. Each memory unit is represented in the database as a single row.\n    Memory units are searched over by functions defined in the memory classes\n    \"\"\"\n\n    def __init__(self, id: Optional[uuid.UUID] = None):\n        if id is None:\n            self.id = uuid.uuid4()\n        else:\n            self.id = id\n\n        assert isinstance(self.id, uuid.UUID), f\"UUID {self.id} must be a UUID type\"\n\n\n# This allows type checking to work when you pass a Passage into a function expecting List[Record]\n# (just use List[RecordType] instead)\nRecordType = TypeVar(\"RecordType\", bound=\"Record\")\n\n\n\nclass ToolCall(object):\n    def __init__(\n        self,\n        id: str,\n        # TODO should we include this? it's fixed to 'function' only (for now) in OAI schema\n        # NOTE: called ToolCall.type in official OpenAI schema\n        tool_call_type: str,  # only 'function' is supported\n        # function: { 'name': ..., 'arguments': ...}\n        function: Dict[str, str],\n    ):\n        self.id = id\n        self.tool_call_type = tool_call_type\n        self.function = function\n\n    def to_dict(self):\n        return {\n            \"id\": self.id,\n            \"type\": self.tool_call_type,\n            \"function\": self.function,\n        }\n\ndef add_inner_thoughts_to_tool_call(\n    tool_call: ToolCall,\n    inner_thoughts: str,\n    inner_thoughts_key: str,\n) -> ToolCall:\n    \"\"\"Add inner thoughts (arg + value) to a tool call\"\"\"\n    # because the kwargs are stored as strings, we need to load then write the JSON dicts\n    try:\n        # load the args list\n        func_args = json.loads(tool_call.function[\"arguments\"])\n        # add the inner thoughts to the args list\n        func_args[inner_thoughts_key] = inner_thoughts\n        # create the updated tool call (as a string)\n        updated_tool_call = copy.deepcopy(tool_call)\n        updated_tool_call.function[\"arguments\"] = json.dumps(func_args, ensure_ascii=JSON_ENSURE_ASCII)\n        return updated_tool_call\n    except json.JSONDecodeError as e:\n        warnings.warn(f\"Failed to put inner thoughts in kwargs: {e}\")\n        raise e\n\nclass Message(Record):\n    \"\"\"Representation of a message sent.\n\n    Messages can be:\n    - agent->user (role=='agent')\n    - user->agent and system->agent (role=='user')\n    - or function/tool call returns (role=='function'/'tool').\n    \"\"\"\n\n    def __init__(\n        self,\n        role: str,\n        text: str,\n        user_id: Optional[uuid.UUID] = None,\n        agent_id: Optional[uuid.UUID] = None,\n        model: Optional[str] = None,  # model used to make function call\n        name: Optional[str] = None,  # optional participant name\n        created_at: Optional[datetime] = None,\n        tool_calls: Optional[List[ToolCall]] = None,  # list of tool calls requested\n        tool_call_id: Optional[str] = None,\n        # tool_call_name: Optional[str] = None,  # not technically OpenAI spec, but it can be helpful to have on-hand\n        embedding: Optional[np.ndarray] = None,\n        embedding_dim: Optional[int] = None,\n        embedding_model: Optional[str] = None,\n        id: Optional[uuid.UUID] = None,\n    ):\n        super().__init__(id)\n        self.user_id = user_id\n        self.agent_id = agent_id\n        self.text = text\n        self.model = model  # model name (e.g. gpt-4)\n        self.created_at = created_at if created_at is not None else get_utc_time()\n\n        # openai info\n        assert role in [\"system\", \"assistant\", \"user\", \"tool\"]\n        self.role = role  # role (agent/user/function)\n        self.name = name\n\n        # pad and store embeddings\n        if isinstance(embedding, list):\n            embedding = np.array(embedding)\n        self.embedding = (\n            np.pad(embedding, (0, MAX_EMBEDDING_DIM - embedding.shape[0]), mode=\"constant\").tolist() if embedding is not None else None\n        )\n        self.embedding_dim = embedding_dim\n        self.embedding_model = embedding_model\n\n        if self.embedding is not None:\n            assert self.embedding_dim, f\"Must specify embedding_dim if providing an embedding\"\n            assert self.embedding_model, f\"Must specify embedding_model if providing an embedding\"\n            assert len(self.embedding) == MAX_EMBEDDING_DIM, f\"Embedding must be of length {MAX_EMBEDDING_DIM}\"\n\n        # tool (i.e. function) call info (optional)\n\n        # if role == \"assistant\", this MAY be specified\n        # if role != \"assistant\", this must be null\n        # assert tool_calls is None or isinstance(tool_calls, list)\n        self.tool_calls = tool_calls\n\n        # if role == \"tool\", then this must be specified\n        # if role != \"tool\", this must be null\n        # if role == \"tool\":\n        #     assert tool_call_id is not None\n        # else:\n        #     assert tool_call_id is None\n        self.tool_call_id = tool_call_id\n\n    def to_json(self):\n        json_message = vars(self)\n        if json_message[\"tool_calls\"] is not None:\n            print(\"json_message\")\n            print(json_message[\"tool_calls\"])\n            json_message[\"tool_calls\"] = [vars(tc[0]) for tc in json_message[\"tool_calls\"]]\n        # turn datetime to ISO format\n        # also if the created_at is missing a timezone, add UTC\n        if not is_utc_datetime(self.created_at):\n            self.created_at = self.created_at.replace(tzinfo=timezone.utc)\n        json_message[\"created_at\"] = self.created_at.isoformat()\n        return json_message\n\n    @staticmethod\n    def dict_to_message(\n        user_id: uuid.UUID,\n        agent_id: uuid.UUID,\n        openai_message_dict: dict,\n        model: Optional[str] = None,  # model used to make function call\n        allow_functions_style: bool = False,  # allow deprecated functions style?\n        created_at: Optional[datetime] = None,\n    ):\n        \"\"\"Convert a ChatCompletion message object into a Message object (synced to DB)\"\"\"\n\n        assert \"role\" in openai_message_dict, openai_message_dict\n        assert \"content\" in openai_message_dict, openai_message_dict\n\n        # If we're going from deprecated function form\n        if openai_message_dict[\"role\"] == \"function\":\n            if not allow_functions_style:\n                raise DeprecationWarning(openai_message_dict)\n            assert \"tool_call_id\" in openai_message_dict, openai_message_dict\n\n            # Convert from 'function' response to a 'tool' response\n            # NOTE: this does not conventionally include a tool_call_id, it's on the caster to provide it\n            return Message(\n                created_at=created_at,\n                user_id=user_id,\n                agent_id=agent_id,\n                model=model,\n                # standard fields expected in an OpenAI ChatCompletion message object\n                role=\"tool\",  # NOTE\n                text=openai_message_dict[\"content\"],\n                name=openai_message_dict[\"name\"] if \"name\" in openai_message_dict else None,\n                tool_calls=openai_message_dict[\"tool_calls\"] if \"tool_calls\" in openai_message_dict else None,\n                tool_call_id=openai_message_dict[\"tool_call_id\"] if \"tool_call_id\" in openai_message_dict else None,\n            )\n\n        elif \"function_call\" in openai_message_dict and openai_message_dict[\"function_call\"] is not None:\n            if not allow_functions_style:\n                raise DeprecationWarning(openai_message_dict)\n            assert openai_message_dict[\"role\"] == \"assistant\", openai_message_dict\n            assert \"tool_call_id\" in openai_message_dict, openai_message_dict\n\n            # Convert a function_call (from an assistant message) into a tool_call\n            # NOTE: this does not conventionally include a tool_call_id (ToolCall.id), it's on the caster to provide it\n            tool_calls = [\n                ToolCall(\n                    id=openai_message_dict[\"tool_call_id\"],  # NOTE: unconventional source, not to spec\n                    tool_call_type=\"function\",\n                    function={\n                        \"name\": openai_message_dict[\"function_call\"][\"name\"],\n                        \"arguments\": openai_message_dict[\"function_call\"][\"arguments\"],\n                    },\n                )\n            ]\n\n            return Message(\n                created_at=created_at,\n                user_id=user_id,\n                agent_id=agent_id,\n                model=model,\n                # standard fields expected in an OpenAI ChatCompletion message object\n                role=openai_message_dict[\"role\"],\n                text=openai_message_dict[\"content\"],\n                name=openai_message_dict[\"name\"] if \"name\" in openai_message_dict else None,\n                tool_calls=tool_calls,\n                tool_call_id=None,  # NOTE: None, since this field is only non-null for role=='tool'\n            )\n\n        else:\n            # Basic sanity check\n            if openai_message_dict[\"role\"] == \"tool\":\n                assert \"tool_call_id\" in openai_message_dict and openai_message_dict[\"tool_call_id\"] is not None, openai_message_dict\n            else:\n                if \"tool_call_id\" in openai_message_dict:\n                    assert openai_message_dict[\"tool_call_id\"] is None, openai_message_dict\n\n            if \"tool_calls\" in openai_message_dict and openai_message_dict[\"tool_calls\"] is not None:\n                assert openai_message_dict[\"role\"] == \"assistant\", openai_message_dict\n\n                tool_calls = [\n                    ToolCall(id=tool_call[\"id\"], tool_call_type=tool_call[\"type\"], function=tool_call[\"function\"])\n                    for tool_call in openai_message_dict[\"tool_calls\"]\n                ]\n            else:\n                tool_calls = None\n\n            # If we're going from tool-call style\n            return Message(\n                created_at=created_at,\n                user_id=user_id,\n                agent_id=agent_id,\n                model=model,\n                # standard fields expected in an OpenAI ChatCompletion message object\n                role=openai_message_dict[\"role\"],\n                text=openai_message_dict[\"content\"],\n                name=openai_message_dict[\"name\"] if \"name\" in openai_message_dict else None,\n                tool_calls=tool_calls,\n                tool_call_id=openai_message_dict[\"tool_call_id\"] if \"tool_call_id\" in openai_message_dict else None,\n            )\n    def to_openai_dict_search_results(self, max_tool_id_length:int=TOOL_CALL_ID_MAX_LEN) -> dict:\n        result_json = self.to_openai_dict()\n        search_result_json = {\"timestamp\": self.created_at, \"message\": {\"content\": result_json[\"content\"], \"role\": result_json[\"role\"]}}\n        return search_result_json\n    def to_openai_dict(self, max_tool_id_length:int=TOOL_CALL_ID_MAX_LEN ,put_inner_thoughts_in_kwargs: bool = True,) -> dict:\n        \"\"\"Go from Message class to ChatCompletion message object\"\"\"\n\n        # TODO change to pydantic casting, eg `return SystemMessageModel(self)`\n\n        if self.role == \"system\":\n            assert all([v is not None for v in [self.role]]), vars(self)\n            openai_message = {\n                \"content\": self.text,\n                \"role\": self.role,\n            }\n            # Optional field, do not include if null\n            if self.name is not None:\n                openai_message[\"name\"] = self.name\n\n        elif self.role == \"user\":\n            assert all([v is not None for v in [self.text, self.role]]), vars(self)\n            openai_message = {\n                \"content\": self.text,\n                \"role\": self.role,\n            }\n            # Optional field, do not include if null\n            if self.name is not None:\n                openai_message[\"name\"] = self.name\n\n        elif self.role == \"assistant\":\n            assert self.tool_calls is not None or self.text is not None\n            openai_message = {\n                \"content\": None if put_inner_thoughts_in_kwargs else self.text,\n                \"role\": self.role,\n            }\n            # Optional fields, do not include if null\n            if self.name is not None:\n                openai_message[\"name\"] = self.name\n            if self.tool_calls is not None:\n                # print(\"tool call\")\n                # for tool_call in self.tool_calls:\n                #     print(tool_call[0])\n                # openai_message[\"tool_calls\"] = [tool_call[0].to_dict() for tool_call in self.tool_calls]\n                if put_inner_thoughts_in_kwargs:\n                    # put the inner thoughts inside the tool call before casting to a dict\n                    openai_message[\"tool_calls\"] = [\n                        add_inner_thoughts_to_tool_call(\n                            tool_call,\n                            inner_thoughts=self.text,\n                            inner_thoughts_key=INNER_THOUGHTS_KWARG,\n                        ).to_dict()\n                        for tool_call in self.tool_calls\n                    ]\n                else:\n                    openai_message[\"tool_calls\"] = [tool_call.to_dict() for tool_call in self.tool_calls]\n                if max_tool_id_length:\n                    for tool_call_dict in openai_message[\"tool_calls\"]:\n                        tool_call_dict[\"id\"] = tool_call_dict[\"id\"][:max_tool_id_length]\n\n        elif self.role == \"tool\":\n            assert all([v is not None for v in [self.role, self.tool_call_id]]), vars(self)\n            openai_message = {\n                \"content\": self.text,\n                \"role\": self.role,\n                \"tool_call_id\": self.tool_call_id[:max_tool_id_length] if max_tool_id_length else self.tool_call_id,\n            }\n\n        else:\n            raise ValueError(self.role)\n\n        return openai_message\n\n    def to_anthropic_dict(self, inner_thoughts_xml_tag=\"thinking\") -> dict:\n        # raise NotImplementedError\n\n        def add_xml_tag(string: str, xml_tag: Optional[str]):\n            # NOTE: Anthropic docs recommends using <thinking> tag when using CoT + tool use\n            return f\"<{xml_tag}>{string}</{xml_tag}\" if xml_tag else string\n\n        if self.role == \"system\":\n            raise ValueError(f\"Anthropic 'system' role not supported\")\n\n        elif self.role == \"user\":\n            assert all([v is not None for v in [self.text, self.role]]), vars(self)\n            anthropic_message = {\n                \"content\": self.text,\n                \"role\": self.role,\n            }\n            # Optional field, do not include if null\n            if self.name is not None:\n                anthropic_message[\"name\"] = self.name\n\n        elif self.role == \"assistant\":\n            assert self.tool_calls is not None or self.text is not None\n            anthropic_message = {\n                \"role\": self.role,\n            }\n            content = []\n            if self.text is not None:\n                content.append(\n                    {\n                        \"type\": \"text\",\n                        \"text\": add_xml_tag(string=self.text, xml_tag=inner_thoughts_xml_tag),\n                    }\n                )\n            if self.tool_calls is not None:\n                for tool_call in self.tool_calls:\n                    content.append(\n                        {\n                            \"type\": \"tool_use\",\n                            \"id\": tool_call.id,\n                            \"name\": tool_call.function[\"name\"],\n                            \"input\": json.loads(tool_call.function[\"arguments\"]),\n                        }\n                    )\n\n            # If the only content was text, unpack it back into a singleton\n            # TODO\n            anthropic_message[\"content\"] = content\n\n            # Optional fields, do not include if null\n            if self.name is not None:\n                anthropic_message[\"name\"] = self.name\n\n        elif self.role == \"tool\":\n            # NOTE: Anthropic uses role \"user\" for \"tool\" responses\n            assert all([v is not None for v in [self.role, self.tool_call_id]]), vars(self)\n            anthropic_message = {\n                \"role\": \"user\",  # NOTE: diff\n                \"content\": [\n                    # TODO support error types etc\n                    {\n                        \"type\": \"tool_result\",\n                        \"tool_use_id\": self.tool_call_id,\n                        \"content\": self.text,\n                    }\n                ],\n            }\n\n        else:\n            raise ValueError(self.role)\n\n        return anthropic_message\n\n    def to_google_ai_dict(self, put_inner_thoughts_in_kwargs: bool = True) -> dict:\n        \"\"\"Go from Message class to Google AI REST message object\n\n        type Content: https://ai.google.dev/api/rest/v1/Content / https://ai.google.dev/api/rest/v1beta/Content\n            parts[]: Part\n            role: str ('user' or 'model')\n        \"\"\"\n        if self.role != \"tool\" and self.name is not None:\n            raise UserWarning(f\"Using Google AI with non-null 'name' field ({self.name}) not yet supported.\")\n\n        if self.role == \"system\":\n            # NOTE: Gemini API doesn't have a 'system' role, use 'user' instead\n            # https://www.reddit.com/r/Bard/comments/1b90i8o/does_gemini_have_a_system_prompt_option_while/\n            google_ai_message = {\n                \"role\": \"user\",  # NOTE: no 'system'\n                \"parts\": [{\"text\": self.text}],\n            }\n\n        elif self.role == \"user\":\n            assert all([v is not None for v in [self.text, self.role]]), vars(self)\n            google_ai_message = {\n                \"role\": \"user\",\n                \"parts\": [{\"text\": self.text}],\n            }\n\n        elif self.role == \"assistant\":\n            assert self.tool_calls is not None or self.text is not None\n            google_ai_message = {\n                \"role\": \"model\",  # NOTE: different\n            }\n\n            # NOTE: Google AI API doesn't allow non-null content + function call\n            # To get around this, just two a two part message, inner thoughts first then\n            parts = []\n            if not put_inner_thoughts_in_kwargs and self.text is not None:\n                # NOTE: ideally we do multi-part for CoT / inner thoughts + function call, but Google AI API doesn't allow it\n                raise NotImplementedError\n                parts.append({\"text\": self.text})\n\n            if self.tool_calls is not None:\n                # NOTE: implied support for multiple calls\n                for tool_call in self.tool_calls:\n                    function_name = tool_call.function[\"name\"]\n                    function_args = tool_call.function[\"arguments\"]\n                    try:\n                        # NOTE: Google AI wants actual JSON objects, not strings\n                        function_args = json.loads(function_args)\n                    except:\n                        raise UserWarning(f\"Failed to parse JSON function args: {function_args}\")\n                        function_args = {\"args\": function_args}\n\n                    if put_inner_thoughts_in_kwargs and self.text is not None:\n                        assert \"inner_thoughts\" not in function_args, function_args\n                        assert len(self.tool_calls) == 1\n                        function_args[INNER_THOUGHTS_KWARG] = self.text\n\n                    parts.append(\n                        {\n                            \"functionCall\": {\n                                \"name\": function_name,\n                                \"args\": function_args,\n                            }\n                        }\n                    )\n            else:\n                assert self.text is not None\n                parts.append({\"text\": self.text})\n            google_ai_message[\"parts\"] = parts\n\n        elif self.role == \"tool\":\n            # NOTE: Significantly different tool calling format, more similar to function calling format\n            assert all([v is not None for v in [self.role, self.tool_call_id]]), vars(self)\n\n            if self.name is None:\n                # raise UserWarning(f\"Couldn't find function name on tool call, defaulting to tool ID instead.\")\n                warnings.warn(f\"Couldn't find function name on tool call, defaulting to tool ID instead.\")\n                function_name = self.tool_call_id\n            else:\n                function_name = self.name\n\n            # NOTE: Google AI API wants the function response as JSON only, no string\n            try:\n                function_response = json.loads(self.text)\n            except:\n                function_response = {\"function_response\": self.text}\n\n            google_ai_message = {\n                \"role\": \"function\",\n                \"parts\": [\n                    {\n                        \"functionResponse\": {\n                            \"name\": function_name,\n                            \"response\": {\n                                \"name\": function_name,  # NOTE: name twice... why?\n                                \"content\": function_response,\n                            },\n                        }\n                    }\n                ],\n            }\n\n        else:\n            raise ValueError(self.role)\n\n        return google_ai_message\n\n    def to_cohere_dict(\n        self,\n        function_call_role: Optional[str] = \"SYSTEM\",\n        function_call_prefix: Optional[str] = \"[CHATBOT called function]\",\n        function_response_role: Optional[str] = \"SYSTEM\",\n        function_response_prefix: Optional[str] = \"[CHATBOT function returned]\",\n        inner_thoughts_as_kwarg: Optional[bool] = False,\n    ) -> List[dict]:\n        \"\"\"Cohere chat_history dicts only have 'role' and 'message' fields\n\n        NOTE: returns a list of dicts so that we can convert:\n          assistant [cot]: \"I'll send a message\"\n          assistant [func]: send_message(\"hi\")\n          tool: {'status': 'OK'}\n        to:\n          CHATBOT.text: \"I'll send a message\"\n          SYSTEM.text: [CHATBOT called function] send_message(\"hi\")\n          SYSTEM.text: [CHATBOT function returned] {'status': 'OK'}\n\n        TODO: update this prompt style once guidance from Cohere on\n        embedded function calls in multi-turn conversation become more clear\n        \"\"\"\n\n        if self.role == \"system\":\n            \"\"\"\n            The chat_history parameter should not be used for SYSTEM messages in most cases.\n            Instead, to add a SYSTEM role message at the beginning of a conversation, the preamble parameter should be used.\n            \"\"\"\n            raise UserWarning(f\"role 'system' messages should go in 'preamble' field for Cohere API\")\n\n        elif self.role == \"user\":\n            assert all([v is not None for v in [self.text, self.role]]), vars(self)\n            cohere_message = [\n                {\n                    \"role\": \"USER\",\n                    \"message\": self.text,\n                }\n            ]\n\n        elif self.role == \"assistant\":\n            # NOTE: we may break this into two message - an inner thought and a function call\n            # Optionally, we could just make this a function call with the inner thought inside\n            assert self.tool_calls is not None or self.text is not None\n\n            if self.text and self.tool_calls:\n                if inner_thoughts_as_kwarg:\n                    raise NotImplementedError\n                cohere_message = [\n                    {\n                        \"role\": \"CHATBOT\",\n                        \"message\": self.text,\n                    },\n                ]\n                for tc in self.tool_calls:\n                    # TODO better way to pack?\n                    # function_call_text = json.dumps(tc.to_dict())\n                    function_name = tc.function[\"name\"]\n                    function_args = json.loads(tc.function[\"arguments\"])\n                    function_args_str = \",\".join([f\"{k}={v}\" for k, v in function_args.items()])\n                    function_call_text = f\"{function_name}({function_args_str})\"\n                    cohere_message.append(\n                        {\n                            \"role\": function_call_role,\n                            \"message\": f\"{function_call_prefix} {function_call_text}\",\n                        }\n                    )\n            elif not self.text and self.tool_calls:\n                cohere_message = []\n                for tc in self.tool_calls:\n                    # TODO better way to pack?\n                    # function_call_text = json.dumps(tc.to_dict())\n                    function_call_text = json.dumps(tc.to_dict(), ensure_ascii=JSON_ENSURE_ASCII)\n                    cohere_message.append(\n                        {\n                            \"role\": function_call_role,\n                            \"message\": f\"{function_call_prefix} {function_call_text}\",\n                        }\n                    )\n            elif self.text and not self.tool_calls:\n                cohere_message = [\n                    {\n                        \"role\": \"CHATBOT\",\n                        \"message\": self.text,\n                    }\n                ]\n            else:\n                raise ValueError(\"Message does not have content nor tool_calls\")\n\n        elif self.role == \"tool\":\n            assert all([v is not None for v in [self.role, self.tool_call_id]]), vars(self)\n            function_response_text = self.text\n            cohere_message = [\n                {\n                    \"role\": function_response_role,\n                    \"message\": f\"{function_response_prefix} {function_response_text}\",\n                }\n            ]\n\n        else:\n            raise ValueError(self.role)\n\n        return cohere_message\n\n\nclass Document(Record):\n    \"\"\"A document represent a document loaded into typeagent, which is broken down into passages.\"\"\"\n\n    def __init__(self, user_id: Optional[uuid.UUID] = None, text: Optional[str] = None, data_source:  Optional[str] = None, id: Optional[uuid.UUID] = None, metadata: Optional[Dict] = {}):\n        if id is None:\n            # by default, generate ID as a hash of the text (avoid duplicates)\n            self.id = create_uuid_from_string(\"\".join([text, str(user_id)]))\n        else:\n            self.id = id\n        super().__init__(id)\n        self.user_id = user_id\n        self.text = text\n        self.data_source = data_source\n        self.metadata = metadata\n        # TODO: add optional embedding?\n\n\nclass Passage(Record):\n    \"\"\"A passage is a single unit of memory, and a standard format accross all storage backends.\n\n    It is a string of text with an assoidciated embedding.\n    \"\"\"\n\n    def __init__(\n        self,\n        text: str,\n        user_id: Optional[uuid.UUID] = None,\n        agent_id: Optional[uuid.UUID] = None,  # set if contained in agent memory\n        embedding: Optional[np.ndarray] = None,\n        embedding_model: Optional[str] = None,\n        data_source: Optional[str] = None,  # None if created by agent\n        doc_id: Optional[uuid.UUID] = None,\n        id: Optional[uuid.UUID] = None,\n        metadata_: Optional[dict] = {},\n        created_at: Optional[datetime] = None,\n    ):\n        if id is None:\n            # by default, generate ID as a hash of the text (avoid duplicates)\n            # TODO: use source-id instead?\n            if agent_id:\n                self.id = create_uuid_from_string(\"\".join([text, str(agent_id), str(user_id)]))\n            else:\n                self.id = create_uuid_from_string(\"\".join([text, str(user_id)]))\n        else:\n            self.id = id\n        super().__init__(self.id)\n        self.user_id = user_id\n        self.agent_id = agent_id\n        self.text = text\n        self.data_source = data_source\n        self.doc_id = doc_id\n        self.metadata_ = metadata_\n\n        # pad and store embeddings\n        if isinstance(embedding, list):\n            embedding = np.array(embedding)\n        self.embedding = (\n            np.pad(embedding, (0, MAX_EMBEDDING_DIM - embedding.shape[0]), mode=\"constant\").tolist() if embedding is not None else None\n        )\n        # self.embedding_dim = embedding_dim\n        self.embedding_model = embedding_model\n\n        self.created_at = created_at if created_at is not None else get_utc_time()\n\n        # if self.embedding is not None:\n        #     # assert self.embedding_dim, f\"Must specify embedding_dim if providing an embedding\"\n        #     assert self.embedding_model, f\"Must specify embedding_model if providing an embedding\"\n        #     assert len(self.embedding) == MAX_EMBEDDING_DIM, f\"Embedding must be of length {MAX_EMBEDDING_DIM}\"\n\n        # assert isinstance(self.user_id, uuid.UUID), f\"UUID {self.user_id} must be a UUID type\"\n        # assert isinstance(self.id, uuid.UUID), f\"UUID {self.id} must be a UUID type\"\n        # assert not agent_id or isinstance(self.agent_id, uuid.UUID), f\"UUID {self.agent_id} must be a UUID type\"\n        # assert not doc_id or isinstance(self.doc_id, uuid.UUID), f\"UUID {self.doc_id} must be a UUID type\"\n\n\nclass LLMConfig:\n    def __init__(\n        self,\n        model: Optional[str] = None,\n        model_endpoint_type: Optional[str] = None,\n        model_endpoint: Optional[str] = None,\n        model_wrapper: Optional[str] = None,\n        context_window: Optional[int] = None,\n    ):\n        self.model = model\n        self.model_endpoint_type = model_endpoint_type\n        self.model_endpoint = model_endpoint\n        self.model_wrapper = model_wrapper\n        self.context_window = context_window\n\n        if context_window is None:\n            self.context_window = LLM_MAX_TOKENS[self.model] if self.model in LLM_MAX_TOKENS else LLM_MAX_TOKENS[\"DEFAULT\"]\n        else:\n            self.context_window = context_window\n\n\nclass EmbeddingConfig:\n    def __init__(\n        self,\n        embedding_endpoint_type: Optional[str] = None,\n        embedding_endpoint: Optional[str] = None,\n        embedding_model: Optional[str] = None,\n        embedding_dim: Optional[int] = None,\n        embedding_chunk_size: Optional[int] = 300,\n    ):\n        self.embedding_endpoint_type = embedding_endpoint_type\n        self.embedding_endpoint = embedding_endpoint\n        self.embedding_model = embedding_model\n        self.embedding_dim = embedding_dim\n        self.embedding_chunk_size = embedding_chunk_size\n\n        # fields cannot be set to None\n        assert self.embedding_endpoint_type\n        assert self.embedding_dim\n        assert self.embedding_chunk_size\n\n\nclass OpenAIEmbeddingConfig(EmbeddingConfig):\n    def __init__(self, openai_key: Optional[str] = None, **kwargs):\n        super().__init__(**kwargs)\n        self.openai_key = openai_key\n\n\nclass AzureEmbeddingConfig(EmbeddingConfig):\n    def __init__(\n        self,\n        azure_key: Optional[str] = None,\n        azure_endpoint: Optional[str] = None,\n        azure_version: Optional[str] = None,\n        azure_deployment: Optional[str] = None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.azure_key = azure_key\n        self.azure_endpoint = azure_endpoint\n        self.azure_version = azure_version\n        self.azure_deployment = azure_deployment\n\n\nclass User:\n    \"\"\"Defines user and default configurations\"\"\"\n\n    # TODO: make sure to encrypt/decrypt keys before storing in DB\n\n    def __init__(\n        self,\n        # name: str,\n        id: Optional[uuid.UUID] = None,\n        user_type:Optional[str]=None,\n        user_status: Optional[str] = None,\n        # other\n        # policies_accepted=False,\n    ):\n        if id is None:\n            self.id = uuid.uuid4()\n        else:\n            self.id = id\n        assert isinstance(self.id, uuid.UUID), f\"UUID {self.id} must be a UUID type\"\n\n        self.user_type = user_type\n\n        self.user_status= user_status\n\n        # misc\n        # self.policies_accepted = policies_accepted\n\n\nclass AgentState:\n    def __init__(\n        self,\n        name: str,\n        type_agent:str,\n        user_id: uuid.UUID,\n        persona_memory: str,  # the filename where the persona was originally sourced from\n        human_memory: str,  # the filename where the human was originally sourced from\n        llm_config: LLMConfig,\n        embedding_config: EmbeddingConfig,\n        preset_id:uuid.UUID,\n        # (in-context) state contains:\n        # persona: str  # the current persona text\n        # human: str  # the current human text\n        # system: str,  # system prompt (not required if initializing with a preset)\n        # functions: dict,  # schema definitions ONLY (function code linked at runtime)\n        # messages: List[dict],  # in-context messages\n        id: Optional[uuid.UUID] = None,\n        state: Optional[dict] = None,\n        created_at: Optional[datetime] = None,\n        user_status: Optional[str] = None,\n    ):\n        if id is None:\n            self.id = uuid.uuid4()\n        else:\n            self.id = id\n        assert isinstance(self.id, uuid.UUID), f\"UUID {self.id} must be a UUID type\"\n        assert isinstance(user_id, uuid.UUID), f\"UUID {user_id} must be a UUID type\"\n\n        # TODO(swooders) we need to handle the case where name is None here\n        # in AgentConfig we autogenerate a name, not sure what the correct thing w/ DBs is, what about NounAdjective combos? Like giphy does? BoredGiraffe etc\n        self.name = name\n        self.user_id = user_id\n        self.preset_id = preset_id\n        self.type_agent=type_agent\n        # The INITIAL values of the persona and human\n        # The values inside self.state['persona'], self.state['human'] are the CURRENT values\n        self.persona_memory = persona_memory\n        self.human_memory = human_memory\n\n        self.llm_config = llm_config\n        self.embedding_config = embedding_config\n\n        self.created_at = created_at if created_at is not None else get_utc_time()\n\n        # state\n        self.state = {} if not state else state\n\n        self.user_status= user_status\n\n\nclass Source:\n    def __init__(\n        self,\n        user_id: uuid.UUID,\n        name: str,\n        description: Optional[str] = None,\n        created_at: Optional[datetime] = None,\n        id: Optional[uuid.UUID] = None,\n        # embedding info\n        embedding_model: Optional[str] = None,\n        embedding_dim: Optional[int] = None,\n        user_status: Optional[str] = None,\n    ):\n        if id is None:\n            self.id = uuid.uuid4()\n        else:\n            self.id = id\n        assert isinstance(self.id, uuid.UUID), f\"UUID {self.id} must be a UUID type\"\n        assert isinstance(user_id, uuid.UUID), f\"UUID {user_id} must be a UUID type\"\n\n        self.name = name\n        self.user_id = user_id\n        self.description = description\n        self.created_at = created_at if created_at is not None else get_utc_time()\n\n        # embedding info (optional)\n        self.embedding_dim = embedding_dim\n        self.embedding_model = embedding_model\n\n        self.user_status= user_status\n\n\nclass Token:\n    def __init__(\n        self,\n        user_id: uuid.UUID,\n        token: str,\n        creator: Optional[str] = None,\n        user_status: Optional[str] = None,\n        id: Optional[uuid.UUID] = None,\n    ):\n        if id is None:\n            self.id = uuid.uuid4()\n        else:\n            self.id = id\n        assert isinstance(self.id, uuid.UUID), f\"UUID {self.id} must be a UUID type\"\n        assert isinstance(user_id, uuid.UUID), f\"UUID {user_id} must be a UUID type\"\n\n        self.token = token\n        self.user_id = user_id\n        self.creator= creator\n        self.user_status= user_status\n\n\nclass Preset(BaseModel):\n    name: str = Field(..., description=\"The name of the preset.\")\n    id: uuid.UUID = Field(default_factory=uuid.uuid4, description=\"The unique identifier of the preset.\")\n    user_id: Optional[uuid.UUID] = Field(None, description=\"The unique identifier of the user who created the preset.\")\n    # description: Optional[str] = Field(None, description=\"The description of the preset.\")\n    created_at: datetime = Field(default_factory=get_utc_time, description=\"The unix timestamp of when the preset was created.\")\n    # system: str = Field(..., description=\"The system prompt of the preset.\")\n    system: str = Field(\n        gpt_system.get_system_text(DEFAULT_PRESET), description=\"The system prompt of the preset.\"\n    )  # default system prompt is same as default preset name\n    system_name: Optional[str] = Field(None, description=\"The name of the system prompt of the preset.\")\n    persona: str = Field(default=get_persona_text(DEFAULT_PERSONA), description=\"The persona of the preset.\")\n    persona_name: Optional[str] = Field(None, description=\"The name of the persona of the preset.\")\n    human: str = Field(default=get_human_text(DEFAULT_HUMAN), description=\"The human of the preset.\")\n    human_name: Optional[str] = Field(None, description=\"The name of the human of the preset.\")\n    functions_schema: List[Dict] = Field(..., description=\"The functions schema of the preset.\")\n    # functions: List[str] = Field(..., description=\"The functions of the preset.\") # TODO: convert to ID\n    # sources: List[str] = Field(..., description=\"The sources of the preset.\") # TODO: convert to ID\n    user_status: Optional[str] = Field(None, description=\"The user status.\")\n   \n    @staticmethod\n    def clone(preset_obj: \"Preset\", new_name_suffix: str = None) -> \"Preset\":\n        \"\"\"\n        Takes a Preset object and an optional new name suffix as input,\n        creates a clone of the given Preset object with a new ID and an optional new name,\n        and returns the new Preset object.\n        \"\"\"\n        new_preset = preset_obj.model_copy()\n        new_preset.id = uuid.uuid4()\n        if new_name_suffix:\n            new_preset.name = f\"{preset_obj.name}_{new_name_suffix}\"\n        else:\n            new_preset.name = f\"{preset_obj.name}_{str(uuid.uuid4())[:8]}\"\n        return new_preset\n\n\nclass Function(BaseModel):\n    name: str = Field(..., description=\"The name of the function.\")\n    id: uuid.UUID = Field(..., description=\"The unique identifier of the function.\")\n    user_id: uuid.UUID = Field(..., description=\"The unique identifier of the user who created the function.\")\n    # TODO: figure out how represent functions\n"}
{"type": "source_file", "path": "luann/functions/function_sets/base.py", "content": "import datetime\nimport json\nimport math\nfrom typing import Optional\n\nfrom agent import Agent\nfrom constants import (\n    JSON_ENSURE_ASCII,\n    MAX_PAUSE_HEARTBEATS,\n    RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE,\n)\n\n### Functions / tools the agent can use\n# All functions should return a response string (or None)\n# If the function fails, throw an exception\n\n\ndef send_message(self: Agent, message: str) -> Optional[str]:\n    \"\"\"\n    Sends a message to the human user.\n\n    Args:\n        message (str): Message contents. All unicode (including emojis) are supported.\n\n    Returns:\n        Optional[str]: None is always returned as this function does not produce a response.\n    \"\"\"\n    # FIXME passing of msg_obj here is a hack, unclear if guaranteed to be the correct reference\n    self.interface.assistant_message(message)  # , msg_obj=self._messages[-1])\n    return None\n\n\n# Construct the docstring dynamically (since it should use the external constants)\npause_heartbeats_docstring = f\"\"\"\nTemporarily ignore timed heartbeats. You may still receive messages from manual heartbeats and other events.\n\nArgs:\n    minutes (int): Number of minutes to ignore heartbeats for. Max value of {MAX_PAUSE_HEARTBEATS} minutes ({MAX_PAUSE_HEARTBEATS // 60} hours).\n\nReturns:\n    str: Function status response\n\"\"\"\n\n\ndef pause_heartbeats(self: Agent, minutes: int) -> Optional[str]:\n    minutes = min(MAX_PAUSE_HEARTBEATS, minutes)\n\n    # Record the current time\n    self.pause_heartbeats_start = datetime.datetime.now(datetime.timezone.utc)\n    # And record how long the pause should go for\n    self.pause_heartbeats_minutes = int(minutes)\n\n    return f\"Pausing timed heartbeats for {minutes} min\"\n\n\npause_heartbeats.__doc__ = pause_heartbeats_docstring\n\n\ndef core_memory_append(self: Agent, name: str, content: str) -> Optional[str]:\n    \"\"\"\n    Append to the contents of core memory.\n\n    Args:\n        name (str): Section of the memory to be edited (persona or human).\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\n\n    Returns:\n        Optional[str]: None is always returned as this function does not produce a response.\n    \"\"\"\n    self.memory.edit_append(name, content)\n    self.rebuild_memory()\n    return None\n\n\ndef core_memory_replace(self: Agent, name: str, old_content: str, new_content: str) -> Optional[str]:\n    \"\"\"\n    Replace the contents of core memory. To delete memories, use an empty string for new_content.\n\n    Args:\n        name (str): Section of the memory to be edited (persona or human).\n        old_content (str): String to replace. Must be an exact match.\n        new_content (str): Content to write to the memory. All unicode (including emojis) are supported.\n\n    Returns:\n        Optional[str]: None is always returned as this function does not produce a response.\n    \"\"\"\n    self.memory.edit_replace(name, old_content, new_content)\n    self.rebuild_memory()\n    return None\n\n\ndef conversation_search(self: Agent, query: str, page: Optional[int] = 0) -> Optional[str]:\n    \"\"\"\n    Search prior conversation history using case-insensitive string matching.\n\n    Args:\n        query (str): String to search for.\n        page (int): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\n\n    Returns:\n        str: Query result string\n    \"\"\"\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\n        page = 0\n    try:\n        page = int(page)\n    except:\n        raise ValueError(f\"'page' argument must be an integer\")\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\n    results, total = self.persistence_manager.recall_memory.text_search(query, count=count, start=page * count)\n    num_pages = math.ceil(total / count) - 1  # 0 index\n    if len(results) == 0:\n        results_str = f\"No results found.\"\n    else:\n        results_pref = f\"Showing {len(results)} of {total} results (page {page}/{num_pages}):\"\n        results_formatted = [f\"timestamp: {d['timestamp']}, {d['message']['role']} - {d['message']['content']}\" for d in results]\n        results_str = f\"{results_pref} {json.dumps(results_formatted, ensure_ascii=JSON_ENSURE_ASCII)}\"\n    return results_str\n\n\ndef conversation_search_date(self: Agent, start_date: str, end_date: str, page: Optional[int] = 0) -> Optional[str]:\n    \"\"\"\n    Search prior conversation history using a date range.\n\n    Args:\n        start_date (str): The start of the date range to search, in the format 'YYYY-MM-DD'.\n        end_date (str): The end of the date range to search, in the format 'YYYY-MM-DD'.\n        page (int): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\n\n    Returns:\n        str: Query result string\n    \"\"\"\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\n        page = 0\n    try:\n        page = int(page)\n    except:\n        raise ValueError(f\"'page' argument must be an integer\")\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\n    results, total = self.persistence_manager.recall_memory.date_search(start_date, end_date, count=count, start=page * count)\n    num_pages = math.ceil(total / count) - 1  # 0 index\n    if len(results) == 0:\n        results_str = f\"No results found.\"\n    else:\n        results_pref = f\"Showing {len(results)} of {total} results (page {page}/{num_pages}):\"\n        results_formatted = [f\"timestamp: {d['timestamp']}, {d['message']['role']} - {d['message']['content']}\" for d in results]\n        results_str = f\"{results_pref} {json.dumps(results_formatted, ensure_ascii=JSON_ENSURE_ASCII)}\"\n    return results_str\n\n\ndef archival_memory_insert(self: Agent, content: str) -> Optional[str]:\n    \"\"\"\n    Add to archival memory. Make sure to phrase the memory contents such that it can be easily queried later.\n\n    Args:\n        content (str): Content to write to the memory. All unicode (including emojis) are supported.\n\n    Returns:\n        Optional[str]: None is always returned as this function does not produce a response.\n    \"\"\"\n    self.persistence_manager.archival_memory.insert(content)\n    return None\n\n\ndef archival_memory_search(self: Agent, query: str, page: Optional[int] = 0) -> Optional[str]:\n    \"\"\"\n    Search archival memory using semantic (embedding-based) search.\n\n    Args:\n        query (str): String to search for.\n        page (Optional[int]): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\n\n    Returns:\n        str: Query result string\n    \"\"\"\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\n        page = 0\n    try:\n        page = int(page)\n    except:\n        raise ValueError(f\"'page' argument must be an integer\")\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\n    results, total = self.persistence_manager.archival_memory.search(query, count=count, start=page * count)\n    num_pages = math.ceil(total / count) - 1  # 0 index\n    if len(results) == 0:\n        results_str = f\"No results found.\"\n    else:\n        results_pref = f\"Showing {len(results)} of {total} results (page {page}/{num_pages}):\"\n        results_formatted = [f\"timestamp: {d['timestamp']}, memory: {d['content']}\" for d in results]\n        results_str = f\"{results_pref} {json.dumps(results_formatted, ensure_ascii=JSON_ENSURE_ASCII)}\"\n    return results_str\ndef knowledge_base_search(self: Agent, query: str, page: Optional[int] = 0) -> Optional[str]:\n    \"\"\"\n    Search archival memory using semantic (embedding-based) search.\n\n    Args:\n        query (str): String to search for.\n        page (Optional[int]): Allows you to page through results. Only use on a follow-up query. Defaults to 0 (first page).\n\n    Returns:\n        str: Query result string\n    \"\"\"\n    if page is None or (isinstance(page, str) and page.lower().strip() == \"none\"):\n        page = 0\n    try:\n        page = int(page)\n    except:\n        raise ValueError(f\"'page' argument must be an integer\")\n    count = RETRIEVAL_QUERY_DEFAULT_PAGE_SIZE\n    results, total = self.persistence_manager.knowledge_base.search(query, count=count, start=page * count)\n    num_pages = math.ceil(total / count) - 1  # 0 index\n    if len(results) == 0:\n        results_str = f\"No results found.\"\n    else:\n        results_pref = f\"Showing {len(results)} of {total} results (page {page}/{num_pages}):\"\n        results_formatted = [f\"timestamp: {d['timestamp']}, memory: {d['content']}\" for d in results]\n        results_str = f\"{results_pref} {json.dumps(results_formatted, ensure_ascii=JSON_ENSURE_ASCII)}\"\n    return results_str\n"}
{"type": "source_file", "path": "luann/functions/__init__.py", "content": ""}
{"type": "source_file", "path": "luann/agent_store/vectorsdb/chroma.py", "content": "import uuid\nfrom typing import Dict, Iterator, List, Optional, Tuple, cast\n\nimport chromadb\nfrom chromadb.api.types import Include\n\nfrom agent_store.storage import StorageConnector, StorageType\nfrom config import typeagentConfig\nfrom data_types import Passage, Record, RecordType\nfrom utils import datetime_to_timestamp, printd, timestamp_to_datetime\n\n\nclass ChromaStorageConnector(StorageConnector):\n    \"\"\"Storage via Chroma\"\"\"\n\n    # WARNING: This is not thread safe. Do NOT do concurrent access to the same collection.\n    # Timestamps are converted to integer timestamps for chroma (datetime not supported)\n\n    def __init__(self, storage_type: str, config: typeagentConfig, user_id, agent_id=None):\n        super().__init__(storage_type=storage_type, config=config, user_id=user_id, agent_id=agent_id)\n\n        # assert table_type == TableType.ARCHIVAL_MEMORY or table_type == TableType.PASSAGES, \"Chroma only supports archival memory\"\n\n        # create chroma client\n        if storage_type == StorageType.ARCHIVAL_MEMORY:\n            if config.archival_memory_storage_path:\n                 self.client = chromadb.PersistentClient(config.archival_memory_storage_path)\n            else:\n            # assume uri={ip}:{port}\n                 ip = config.archival_memory_storage_uri.split(\":\")[0]\n                 port = config.archival_memory_storage_uri.split(\":\")[1]\n                 self.client = chromadb.HttpClient(host=ip, port=port)\n\n            # self.type = Passage\n           \n        elif storage_type == StorageType.KNOWLEDGE_BASE_PASSAGES:\n            # self.type = Passage\n            if config.knowledge_base_storage_path:\n                 self.client = chromadb.PersistentClient(config.knowledge_base_storage_path)\n            else:\n            # assume uri={ip}:{port}\n                 ip = config.knowledge_base_storage_uri.split(\":\")[0]\n                 port = config.knowledge_base_storage_uri.split(\":\")[1]\n                 self.client = chromadb.HttpClient(host=ip, port=port)\n\n        # if config.archival_memory_storage_path:\n        #     self.client = chromadb.PersistentClient(config.archival_memory_storage_path)\n        # else:\n        #     # assume uri={ip}:{port}\n        #     ip = config.archival_memory_storage_uri.split(\":\")[0]\n        #     port = config.archival_memory_storage_uri.split(\":\")[1]\n        #     self.client = chromadb.HttpClient(host=ip, port=port)\n\n        # get a collection or create if it doesn't exist already\n        self.collection = self.client.get_or_create_collection(self.table_name)\n        self.include: Include = [\"documents\", \"embeddings\", \"metadatas\"]\n\n        # need to be converted to strings\n        self.uuid_fields = [\"id\", \"user_id\", \"agent_id\", \"source_id\", \"doc_id\"]\n\n    def get_filters(self, filters: Optional[Dict] = {}) -> Tuple[list, dict]:\n        # get all filters for query\n        if filters is not None:\n            filter_conditions = {**self.filters, **filters}\n        else:\n            filter_conditions = self.filters\n\n        # convert to chroma format\n        chroma_filters = []\n        ids = []\n        for key, value in filter_conditions.items():\n            # filter by id\n            if key == \"id\":\n                ids = [str(value)]\n                continue\n\n            # filter by other keys\n            if key in self.uuid_fields:\n                chroma_filters.append({key: {\"$eq\": str(value)}})\n            else:\n                chroma_filters.append({key: {\"$eq\": value}})\n\n        if len(chroma_filters) > 1:\n            chroma_filters = {\"$and\": chroma_filters}\n        elif len(chroma_filters) == 0:\n            chroma_filters = {}\n        else:\n            chroma_filters = chroma_filters[0]\n        return ids, chroma_filters\n\n    def get_all_paginated(self, filters: Optional[Dict] = {}, page_size: int = 1000, offset: int = 0) -> Iterator[List[RecordType]]:\n        ids, filters = self.get_filters(filters)\n        while True:\n            # Retrieve a chunk of records with the given page_size\n            results = self.collection.get(ids=ids, offset=offset, limit=page_size, include=self.include, where=filters)\n\n            # If the chunk is empty, we've retrieved all records\n            assert results[\"embeddings\"] is not None, f\"results['embeddings'] was None\"\n            if len(results[\"embeddings\"]) == 0:\n                break\n\n            # Yield a list of Record objects converted from the chunk\n            yield self.results_to_records(results)\n\n            # Increment the offset to get the next chunk in the next iteration\n            offset += page_size\n\n    def results_to_records(self, results) -> List[RecordType]:\n        # convert timestamps to datetime\n        for metadata in results[\"metadatas\"]:\n            if \"created_at\" in metadata:\n                metadata[\"created_at\"] = timestamp_to_datetime(metadata[\"created_at\"])\n            for key, value in metadata.items():\n                if key in self.uuid_fields:\n                    metadata[key] = uuid.UUID(value)\n        if results[\"embeddings\"]:  # may not be returned, depending on table type\n            return [\n                cast(RecordType, self.type(text=text, embedding=embedding, id=uuid.UUID(record_id), **metadatas))  # type: ignore\n                for (text, record_id, embedding, metadatas) in zip(\n                    results[\"documents\"], results[\"ids\"], results[\"embeddings\"], results[\"metadatas\"]\n                )\n            ]\n        else:\n            # no embeddings\n            return [\n                cast(RecordType, self.type(text=text, id=uuid.UUID(id), **metadatas))  # type: ignore\n                for (text, id, metadatas) in zip(results[\"documents\"], results[\"ids\"], results[\"metadatas\"])\n            ]\n\n    def get_all(self, filters: Optional[Dict] = {}, limit=None) -> List[RecordType]:\n        ids, filters = self.get_filters(filters)\n        if self.collection.count() == 0:\n            return []\n        if limit:\n            results = self.collection.get(ids=ids, include=self.include, where=filters, limit=limit)\n        else:\n            results = self.collection.get(ids=ids, include=self.include, where=filters)\n        return self.results_to_records(results)\n\n    def get(self, id: uuid.UUID) -> Optional[RecordType]:\n        results = self.collection.get(ids=[str(id)])\n        if len(results[\"ids\"]) == 0:\n            return None\n        return self.results_to_records(results)[0]\n\n    def format_records(self, records: List[RecordType]):\n        assert all([isinstance(r, Passage) for r in records])\n\n        recs = []\n        ids = []\n        documents = []\n        embeddings = []\n\n        # de-duplication of ids\n        exist_ids = set()\n        for i in range(len(records)):\n            record = records[i]\n            if record.id in exist_ids:\n                continue\n            exist_ids.add(record.id)\n            recs.append(cast(Passage, record))\n            ids.append(str(record.id))\n            documents.append(record.text)\n            embeddings.append(record.embedding)\n\n        # collect/format record metadata\n        metadatas = []\n        for record in recs:\n            metadata = vars(record)\n            metadata.pop(\"id\")\n            metadata.pop(\"text\")\n            metadata.pop(\"embedding\")\n            if \"created_at\" in metadata:\n                metadata[\"created_at\"] = datetime_to_timestamp(metadata[\"created_at\"])\n            if \"metadata_\" in metadata and metadata[\"metadata_\"] is not None:\n                record_metadata = dict(metadata[\"metadata_\"])\n                metadata.pop(\"metadata_\")\n            else:\n                record_metadata = {}\n            metadata = {key: value for key, value in metadata.items() if value is not None}  # null values not allowed\n            metadata = {**metadata, **record_metadata}  # merge with metadata\n\n            # convert uuids to strings\n            for key, value in metadata.items():\n                if key in self.uuid_fields:\n                    metadata[key] = str(value)\n            metadatas.append(metadata)\n        return ids, documents, embeddings, metadatas\n\n    def insert(self, record: Record):\n        ids, documents, embeddings, metadatas = self.format_records([record])\n        if any([e is None for e in embeddings]):\n            raise ValueError(\"Embeddings must be provided to chroma\")\n        self.collection.upsert(documents=documents, embeddings=[e for e in embeddings if e is not None], ids=ids, metadatas=metadatas)\n\n    def insert_many(self, records: List[RecordType], show_progress=False):\n        ids, documents, embeddings, metadatas = self.format_records(records)\n        if any([e is None for e in embeddings]):\n            raise ValueError(\"Embeddings must be provided to chroma\")\n        self.collection.upsert(documents=documents, embeddings=[e for e in embeddings if e is not None], ids=ids, metadatas=metadatas)\n\n    def delete(self, filters: Optional[Dict] = {}):\n        ids, filters = self.get_filters(filters)\n        self.collection.delete(ids=ids, where=filters)\n\n    def delete_table(self):\n        # drop collection\n        self.client.delete_collection(self.collection.name)\n\n    def save(self):\n        # save to persistence file (nothing needs to be done)\n        printd(\"Saving chroma\")\n\n    def size(self, filters: Optional[Dict] = {}) -> int:\n        # unfortuantely, need to use pagination to get filtering\n        # warning: poor performance for large datasets\n        return len(self.get_all(filters=filters))\n\n    def list_data_sources(self):\n        raise NotImplementedError\n\n    def query(self, query: str, query_vec: List[float], top_k: int = 10, filters: Optional[Dict] = {}) -> List[RecordType]:\n        ids, filters = self.get_filters(filters)\n        results = self.collection.query(query_embeddings=[query_vec], n_results=top_k, include=self.include, where=filters)\n\n        # flatten, since we only have one query vector\n        flattened_results = {}\n        for key, value in results.items():\n            if value:\n                # value is an Optional[List] type according to chromadb.api.types\n                flattened_results[key] = value[0]  # type: ignore\n                assert len(value) == 1, f\"Value is size {len(value)}: {value}\"  # type: ignore\n            else:\n                flattened_results[key] = value\n\n        return self.results_to_records(flattened_results)\n\n    def query_date(self, start_date, end_date, start=None, count=None):\n        raise ValueError(\"Cannot run query_date with chroma\")\n        # filters = self.get_filters(filters)\n        # filters[\"created_at\"] = {\n        #    \"$gte\": start_date,\n        #    \"$lte\": end_date,\n        # }\n        # results = self.collection.query(where=filters)\n        # start = 0 if start is None else start\n        # count = len(results) if count is None else count\n        # results = results[start : start + count]\n        # return self.results_to_records(results)\n\n    def query_text(self, query, count=None, start=None, filters: Optional[Dict] = {}):\n        raise ValueError(\"Cannot run query_text with chroma\")\n        # filters = self.get_filters(filters)\n        # results = self.collection.query(where_document={\"$contains\": {\"text\": query}}, where=filters)\n        # start = 0 if start is None else start\n        # count = len(results) if count is None else count\n        # results = results[start : start + count]\n        # return self.results_to_records(results)\n\n    def get_all_cursor(\n        self,\n        filters: Optional[Dict] = {},\n        after: uuid.UUID = None,\n        before: uuid.UUID = None,\n        limit: Optional[int] = 1000,\n        order_by: str = \"created_at\",\n        reverse: bool = False,\n    ):\n        raise ValueError(\"Cannot run get_all_cursor with chroma\")\n"}
{"type": "source_file", "path": "luann/autogen/examples/agent_groupchat.py", "content": "\"\"\"Example of how to add typeagent into an AutoGen groupchat\n\nBased on the official AutoGen example here: https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb\n\nBegin by doing:\n  pip install \"pyautogen[teachable]\"\n  pip install pytypeagent\n  or\n  pip install -e . (inside the typeagent home directory)\n\"\"\"\n\nimport os\n\nimport autogen\n\nfrom typeagent.autogen.typeagent_agent import create_typeagent_autogen_agent_from_config\nfrom typeagent.constants import DEFAULT_PRESET, LLM_MAX_TOKENS\n\nLLM_BACKEND = \"openai\"\n# LLM_BACKEND = \"azure\"\n# LLM_BACKEND = \"local\"\n\nif LLM_BACKEND == \"openai\":\n    # For demo purposes let's use gpt-4\n    model = \"gpt-4\"\n\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    assert openai_api_key, \"You must set OPENAI_API_KEY or set LLM_BACKEND to 'local' to run this example\"\n\n    # This config is for AutoGen agents that are not powered by typeagent\n    config_list = [\n        {\n            \"model\": model,\n            \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n        }\n    ]\n\n    # This config is for AutoGen agents that powered by typeagent\n    config_list_typeagent = [\n        {\n            \"model\": model,\n            \"context_window\": LLM_MAX_TOKENS[model],\n            \"preset\": DEFAULT_PRESET,\n            \"model_wrapper\": None,\n            # OpenAI specific\n            \"model_endpoint_type\": \"openai\",\n            \"model_endpoint\": \"https://api.openai.com/v1\",\n            \"openai_key\": openai_api_key,\n        },\n    ]\n\nelif LLM_BACKEND == \"azure\":\n    # Make sure that you have access to this deployment/model on your Azure account!\n    # If you don't have access to the model, the code will fail\n    model = \"gpt-4\"\n\n    azure_openai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n    azure_openai_version = os.getenv(\"AZURE_OPENAI_VERSION\")\n    azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n    assert (\n        azure_openai_api_key is not None and azure_openai_version is not None and azure_openai_endpoint is not None\n    ), \"Set all the required OpenAI Azure variables (see: https://typeagent.readme.io/docs/endpoints#azure-openai)\"\n\n    # This config is for AutoGen agents that are not powered by typeagent\n    config_list = [\n        {\n            \"model\": model,\n            \"api_type\": \"azure\",\n            \"api_key\": azure_openai_api_key,\n            \"api_version\": azure_openai_version,\n            # NOTE: on versions of pyautogen < 0.2.0, use \"api_base\"\n            # \"api_base\": azure_openai_endpoint,\n            \"base_url\": azure_openai_endpoint,\n        }\n    ]\n\n    # This config is for AutoGen agents that powered by typeagent\n    config_list_typeagent = [\n        {\n            \"model\": model,\n            \"context_window\": LLM_MAX_TOKENS[model],\n            \"preset\": DEFAULT_PRESET,\n            \"model_wrapper\": None,\n            # Azure specific\n            \"model_endpoint_type\": \"azure\",\n            \"azure_key\": azure_openai_api_key,\n            \"azure_endpoint\": azure_openai_endpoint,\n            \"azure_version\": azure_openai_version,\n        },\n    ]\n\nelif LLM_BACKEND == \"local\":\n    # Example using LM Studio on a local machine\n    # You will have to change the parameters based on your setup\n\n    # Non-typeagent agents will still use local LLMs, but they will use the ChatCompletions endpoint\n    config_list = [\n        {\n            \"model\": \"NULL\",  # not needed\n            # NOTE: on versions of pyautogen < 0.2.0 use \"api_base\", and also uncomment \"api_type\"\n            # \"api_base\": \"http://localhost:1234/v1\",\n            # \"api_type\": \"open_ai\",\n            \"base_url\": \"http://localhost:1234/v1\",  # ex. \"http://127.0.0.1:5001/v1\" if you are using webui, \"http://localhost:1234/v1/\" if you are using LM Studio\n            \"api_key\": \"NULL\",  #  not needed\n        },\n    ]\n\n    # typeagent-powered agents will also use local LLMs, but they need additional setup (also they use the Completions endpoint)\n    config_list_typeagent = [\n        {\n            \"preset\": DEFAULT_PRESET,\n            \"model\": None,  # only required for Ollama, see: https://typeagent.readme.io/docs/ollama\n            \"context_window\": 8192,  # the context window of your model (for Mistral 7B-based models, it's likely 8192)\n            \"model_wrapper\": \"chatml\",  # chatml is the default wrapper\n            \"model_endpoint_type\": \"lmstudio\",  # can use webui, ollama, llamacpp, etc.\n            \"model_endpoint\": \"http://localhost:1234\",  # the IP address of your LLM backend\n        },\n    ]\n\nelse:\n    raise ValueError(LLM_BACKEND)\n\n# If USE_typeagent is False, then this example will be the same as the official AutoGen repo\n# (https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb)\n# If USE_typeagent is True, then we swap out the \"coder\" agent with a typeagent agent\nUSE_typeagent = True\n\n# Set to True if you want to print typeagent's inner workings.\n# DEBUG = False\nDEBUG = True\n\ninterface_kwargs = {\n    \"debug\": DEBUG,\n    \"show_inner_thoughts\": True,\n    \"show_function_outputs\": DEBUG,\n}\n\nllm_config = {\"config_list\": config_list, \"seed\": 42}\nllm_config_typeagent = {\"config_list\": config_list_typeagent, \"seed\": 42}\n\n# The user agent\nuser_proxy = autogen.UserProxyAgent(\n    name=\"User_proxy\",\n    system_message=\"A human admin.\",\n    code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n    human_input_mode=\"TERMINATE\",  # needed?\n    default_auto_reply=\"...\",  # Set a default auto-reply message here (non-empty auto-reply is required for LM Studio)\n)\n\n# The agent playing the role of the product manager (PM)\npm = autogen.AssistantAgent(\n    name=\"Product_manager\",\n    system_message=\"Creative in software product ideas.\",\n    llm_config=llm_config,\n    default_auto_reply=\"...\",  # Set a default auto-reply message here (non-empty auto-reply is required for LM Studio)\n)\n\nif not USE_typeagent:\n    # In the AutoGen example, we create an AssistantAgent to play the role of the coder\n    coder = autogen.AssistantAgent(\n        name=\"Coder\",\n        llm_config=llm_config,\n    )\n\nelse:\n    # In our example, we swap this AutoGen agent with a typeagent agent\n    # This typeagent agent will have all the benefits of typeagent, ie persistent memory, etc.\n    coder = create_typeagent_autogen_agent_from_config(\n        \"typeagent_coder\",\n        llm_config=llm_config_typeagent,\n        system_message=f\"I am a 10x engineer, trained in Python. I was the first engineer at Uber \"\n        f\"(which I make sure to tell everyone I work with).\\n\"\n        f\"You are participating in a group chat with a user ({user_proxy.name}) \"\n        f\"and a product manager ({pm.name}).\",\n        interface_kwargs=interface_kwargs,\n        default_auto_reply=\"...\",  # Set a default auto-reply message here (non-empty auto-reply is required for LM Studio)\n        skip_verify=False,  # NOTE: you should set this to True if you expect your typeagent AutoGen agent to call a function other than send_message on the first turn\n        auto_save=False,  # Set this to True if you want the typeagent AutoGen agent to save its internal state after each reply - you can also save manually with .save()\n    )\n\n    # If you'd like to save this agent at any point, you can do:\n    # coder.save()\n\n    # You can also autosave by setting auto_save=True, in which case coder.save() will be called automatically\n\n    # To load an AutoGen+typeagent agent you previously created, you can use the load function:\n    # coder = load_autogen_typeagent_agent(agent_config={\"name\": \"typeagent_coder\"})\n\n\n# Initialize the group chat between the user and two LLM agents (PM and coder)\ngroupchat = autogen.GroupChat(agents=[user_proxy, pm, coder], messages=[], max_round=3, speaker_selection_method=\"round_robin\")\nmanager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n\n# Begin the group chat with a message from the user\nuser_proxy.initiate_chat(\n    manager,\n    message=\"I want to design an app to make me one million dollars in one month. Yes, your heard that right.\",\n)\n"}
{"type": "source_file", "path": "luann/data_sources/directory.py", "content": "import concurrent\nimport logging\nimport random\nfrom pathlib import Path\nfrom typing import Any, Callable, Iterator, List, Optional, Sequence, Type, Union\nfrom models.pydantic_models import (\nDocumentModel,\n)\nfrom  data_sources.unstructured import UnstructuredFileLoader\nfrom data_types import Document\nFILE_LOADER_TYPE = Union[\n    Type[UnstructuredFileLoader]\n]\nlogger = logging.getLogger(__name__)\n\n\ndef _is_visible(p: Path) -> bool:\n    parts = p.parts\n    for _p in parts:\n        if _p.startswith(\".\"):\n            return False\n    return True\n\n\nclass DirectoryLoader():\n    \"\"\"Load from a directory.\"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        glob: str = \"**/[!.]*\",\n        silent_errors: bool = False,\n        load_hidden: bool = False,\n        loader_cls: FILE_LOADER_TYPE = UnstructuredFileLoader,\n        loader_kwargs: Union[dict, None] = None,\n        recursive: bool = False,\n        show_progress: bool = False,\n        use_multithreading: bool = False,\n        max_concurrency: int = 4,\n        *,\n        exclude: Union[Sequence[str], str] = (),\n        sample_size: int = 0,\n        randomize_sample: bool = False,\n        sample_seed: Union[int, None] = None,\n    ):\n        \"\"\"Initialize with a path to directory and how to glob over it.\n\n        Args:\n            path: Path to directory.\n            glob: Glob pattern to use to find files. Defaults to \"**/[!.]*\"\n               (all files except hidden).\n            exclude: A pattern or list of patterns to exclude from results.\n                Use glob syntax.\n            silent_errors: Whether to silently ignore errors. Defaults to False.\n            load_hidden: Whether to load hidden files. Defaults to False.\n            loader_cls: Loader class to use for loading files.\n              Defaults to UnstructuredFileLoader.\n            loader_kwargs: Keyword arguments to pass to loader_cls. Defaults to None.\n            recursive: Whether to recursively search for files. Defaults to False.\n            show_progress: Whether to show a progress bar. Defaults to False.\n            use_multithreading: Whether to use multithreading. Defaults to False.\n            max_concurrency: The maximum number of threads to use. Defaults to 4.\n            sample_size: The maximum number of files you would like to load from the\n                directory.\n            randomize_sample: Shuffle the files to get a random sample.\n            sample_seed: set the seed of the random shuffle for reproducibility.\n\n        Examples:\n\n            .. code-block:: python\n                from langchain_community.document_loaders import DirectoryLoader\n\n                # Load all non-hidden files in a directory.\n                loader = DirectoryLoader(\"/path/to/directory\")\n\n                # Load all text files in a directory without recursion.\n                loader = DirectoryLoader(\"/path/to/directory\", glob=\"*.txt\")\n\n                # Recursively load all text files in a directory.\n                loader = DirectoryLoader(\n                    \"/path/to/directory\", glob=\"*.txt\", recursive=True\n                )\n\n                # Load all files in a directory, except for py files.\n                loader = DirectoryLoader(\"/path/to/directory\", exclude=\"*.py\")\n\n                # Load all files in a directory, except for py or pyc files.\n                loader = DirectoryLoader(\n                    \"/path/to/directory\", exclude=[\"*.py\", \"*.pyc\"]\n                )\n        \"\"\"\n        if loader_kwargs is None:\n            loader_kwargs = {}\n        if isinstance(exclude, str):\n            exclude = (exclude,)\n        self.path = path\n        self.glob = glob\n        self.exclude = exclude\n        self.load_hidden = load_hidden\n        self.loader_cls = loader_cls\n        self.loader_kwargs = loader_kwargs\n        self.silent_errors = silent_errors\n        self.recursive = recursive\n        self.show_progress = show_progress\n        self.use_multithreading = use_multithreading\n        self.max_concurrency = max_concurrency\n        self.sample_size = sample_size\n        self.randomize_sample = randomize_sample\n        self.sample_seed = sample_seed\n\n    def load(self)-> Iterator[Document]:\n        \"\"\"Load documents.\"\"\"\n        return list(self.lazy_load())\n\n    def lazy_load(self)-> Iterator[Document]:\n        \"\"\"Load documents lazily.\"\"\"\n        p = Path(self.path)\n        if not p.exists():\n            raise FileNotFoundError(f\"Directory not found: '{self.path}'\")\n        if not p.is_dir():\n            raise ValueError(f\"Expected directory, got file: '{self.path}'\")\n\n        paths = p.rglob(self.glob) if self.recursive else p.glob(self.glob)\n        items = [\n            path\n            for path in paths\n            if not (self.exclude and any(path.match(glob) for glob in self.exclude))\n            and path.is_file()\n        ]\n\n        if self.sample_size > 0:\n            if self.randomize_sample:\n                randomizer = random.Random(\n                    self.sample_seed if self.sample_seed else None\n                )\n                randomizer.shuffle(items)\n            items = items[: min(len(items), self.sample_size)]\n\n        pbar = None\n        if self.show_progress:\n            try:\n                from tqdm import tqdm\n\n                pbar = tqdm(total=len(items))\n            except ImportError as e:\n                logger.warning(\n                    \"To log the progress of DirectoryLoader you need to install tqdm, \"\n                    \"`pip install tqdm`\"\n                )\n                if self.silent_errors:\n                    logger.warning(e)\n                else:\n                    raise ImportError(\n                        \"To log the progress of DirectoryLoader \"\n                        \"you need to install tqdm, \"\n                        \"`pip install tqdm`\"\n                    )\n\n        if self.use_multithreading:\n            futures = []\n            with concurrent.futures.ThreadPoolExecutor(\n                max_workers=self.max_concurrency\n            ) as executor:\n                for i in items:\n                    futures.append(\n                        executor.submit(\n                            self._lazy_load_file_to_non_generator(self._lazy_load_file),\n                            i,\n                            p,\n                            pbar,\n                        )\n                    )\n                for future in concurrent.futures.as_completed(futures):\n                    for item in future.result():\n                        yield item\n        else:\n            for i in items:\n                yield from self._lazy_load_file(i, p, pbar)\n\n        if pbar:\n            pbar.close()\n\n    def _lazy_load_file_to_non_generator(self, func: Callable) -> Callable:\n        def non_generator(item: Path, path: Path, pbar: Optional[Any]) -> List:\n            return [x for x in func(item, path, pbar)]\n\n        return non_generator\n\n    def _lazy_load_file(\n        self, item: Path, path: Path, pbar: Optional[Any]\n    ):\n        \"\"\"Load a file.\n\n        Args:\n            item: File path.\n            path: Directory path.\n            pbar: Progress bar. Defaults to None.\n\n        \"\"\"\n        if item.is_file():\n            if _is_visible(item.relative_to(path)) or self.load_hidden:\n                try:\n                    logger.debug(f\"Processing file: {str(item)}\")\n                    loader = self.loader_cls(str(item), **self.loader_kwargs)\n                    try:\n                        for subdoc in loader.lazy_load():\n                            yield subdoc\n                    except NotImplementedError:\n                        for subdoc in loader.load():\n                            yield subdoc\n                except Exception as e:\n                    if self.silent_errors:\n                        logger.warning(f\"Error loading file {str(item)}: {e}\")\n                    else:\n                        logger.error(f\"Error loading file {str(item)}\")\n                        raise e\n                finally:\n                    if pbar:\n                        pbar.update(1)\n"}
{"type": "source_file", "path": "luann/llm_api/openai.py", "content": "import json\nfrom typing import Generator, Optional, Union\n\nimport httpx\nimport requests\nfrom httpx_sse import connect_sse\nfrom httpx_sse._exceptions import SSEError\n\nfrom local_llm.utils import num_tokens_from_functions, num_tokens_from_messages\nfrom models.chat_completion_request import ChatCompletionRequest\nfrom models.chat_completion_response import (\n    ChatCompletionChunkResponse,\n    ChatCompletionResponse,\n    Choice,\n    FunctionCall,\n    Message,\n    ToolCall,\n    UsageStatistics,\n)\nfrom models.embedding_response import EmbeddingResponse\nfrom streaming_interface import (\n    AgentChunkStreamingInterface,\n    AgentRefreshStreamingInterface,\n)\nfrom utils import get_utc_time, smart_urljoin\n\nOPENAI_SSE_DONE = \"[DONE]\"\n\n\ndef openai_get_model_list(url: str, api_key: Union[str, None], fix_url: Optional[bool] = False) -> dict:\n    \"\"\"https://platform.openai.com/docs/api-reference/models/list\"\"\"\n    from utils import printd\n\n    # In some cases we may want to double-check the URL and do basic correction, eg:\n    # In typeagent config the address for vLLM is w/o a /v1 suffix for simplicity\n    # However if we're treating the server as an OpenAI proxy we want the /v1 suffix on our model hit\n    if fix_url:\n        if not url.endswith(\"/v1\"):\n            url = smart_urljoin(url, \"v1\")\n\n    url = smart_urljoin(url, \"models\")\n\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key is not None:\n        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n    printd(f\"Sending request to {url}\")\n    try:\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()  # Raises HTTPError for 4XX/5XX status\n        response = response.json()  # convert to dict from string\n        printd(f\"response = {response}\")\n        return response\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        try:\n            response = response.json()\n        except:\n            pass\n        printd(f\"Got HTTPError, exception={http_err}, response={response}\")\n        raise http_err\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        try:\n            response = response.json()\n        except:\n            pass\n        printd(f\"Got RequestException, exception={req_err}, response={response}\")\n        raise req_err\n    except Exception as e:\n        # Handle other potential errors\n        try:\n            response = response.json()\n        except:\n            pass\n        printd(f\"Got unknown Exception, exception={e}, response={response}\")\n        raise e\n\n\ndef openai_chat_completions_process_stream(\n    url: str,\n    api_key: str,\n    chat_completion_request: ChatCompletionRequest,\n    stream_inferface: Optional[Union[AgentChunkStreamingInterface, AgentRefreshStreamingInterface]] = None,\n) -> ChatCompletionResponse:\n    \"\"\"Process a streaming completion response, and return a ChatCompletionRequest at the end.\n\n    To \"stream\" the response in typeagent, we want to call a streaming-compatible interface function\n    on the chunks received from the OpenAI-compatible server POST SSE response.\n    \"\"\"\n    assert chat_completion_request.stream == True\n\n    # Count the prompt tokens\n    # TODO move to post-request?\n    chat_history = [m.model_dump(exclude_none=True) for m in chat_completion_request.messages]\n    # print(chat_history)\n\n    prompt_tokens = num_tokens_from_messages(\n        messages=chat_history,\n        model=chat_completion_request.model,\n    )\n    # We also need to add the cost of including the functions list to the input prompt\n    if chat_completion_request.tools is not None:\n        assert chat_completion_request.functions is None\n        prompt_tokens += num_tokens_from_functions(\n            functions=[t.function.model_dump() for t in chat_completion_request.tools],\n            model=chat_completion_request.model,\n        )\n    elif chat_completion_request.functions is not None:\n        assert chat_completion_request.tools is None\n        prompt_tokens += num_tokens_from_functions(\n            functions=[f.model_dump() for f in chat_completion_request.functions],\n            model=chat_completion_request.model,\n        )\n\n    TEMP_STREAM_RESPONSE_ID = \"temp_id\"\n    TEMP_STREAM_FINISH_REASON = \"temp_null\"\n    TEMP_STREAM_TOOL_CALL_ID = \"temp_id\"\n    chat_completion_response = ChatCompletionResponse(\n        id=TEMP_STREAM_RESPONSE_ID,\n        choices=[],\n        created=get_utc_time(),\n        model=chat_completion_request.model,\n        usage=UsageStatistics(\n            completion_tokens=0,\n            prompt_tokens=prompt_tokens,\n            total_tokens=prompt_tokens,\n        ),\n    )\n\n    if stream_inferface:\n        stream_inferface.stream_start()\n\n    n_chunks = 0  # approx == n_tokens\n    try:\n        for chunk_idx, chat_completion_chunk in enumerate(\n            openai_chat_completions_request_stream(url=url, api_key=api_key, chat_completion_request=chat_completion_request)\n        ):\n            assert isinstance(chat_completion_chunk, ChatCompletionChunkResponse), type(chat_completion_chunk)\n            # print(chat_completion_chunk)\n\n            if stream_inferface:\n                if isinstance(stream_inferface, AgentChunkStreamingInterface):\n                    stream_inferface.process_chunk(chat_completion_chunk)\n                elif isinstance(stream_inferface, AgentRefreshStreamingInterface):\n                    stream_inferface.process_refresh(chat_completion_response)\n                else:\n                    raise TypeError(stream_inferface)\n\n            if chunk_idx == 0:\n                # initialize the choice objects which we will increment with the deltas\n                num_choices = len(chat_completion_chunk.choices)\n                assert num_choices > 0\n                chat_completion_response.choices = [\n                    Choice(\n                        finish_reason=TEMP_STREAM_FINISH_REASON,  # NOTE: needs to be ovrerwritten\n                        index=i,\n                        message=Message(\n                            role=\"assistant\",\n                        ),\n                    )\n                    for i in range(len(chat_completion_chunk.choices))\n                ]\n\n            # add the choice delta\n            assert len(chat_completion_chunk.choices) == len(chat_completion_response.choices), chat_completion_chunk\n            for chunk_choice in chat_completion_chunk.choices:\n                if chunk_choice.finish_reason is not None:\n                    chat_completion_response.choices[chunk_choice.index].finish_reason = chunk_choice.finish_reason\n\n                if chunk_choice.logprobs is not None:\n                    chat_completion_response.choices[chunk_choice.index].logprobs = chunk_choice.logprobs\n\n                accum_message = chat_completion_response.choices[chunk_choice.index].message\n                message_delta = chunk_choice.delta\n\n                if message_delta.content is not None:\n                    content_delta = message_delta.content\n                    if accum_message.content is None:\n                        accum_message.content = content_delta\n                    else:\n                        accum_message.content += content_delta\n\n                if message_delta.tool_calls is not None:\n                    tool_calls_delta = message_delta.tool_calls\n\n                    # If this is the first tool call showing up in a chunk, initialize the list with it\n                    if accum_message.tool_calls is None:\n                        accum_message.tool_calls = [\n                            ToolCall(id=TEMP_STREAM_TOOL_CALL_ID, function=FunctionCall(name=\"\", arguments=\"\"))\n                            for _ in range(len(tool_calls_delta))\n                        ]\n\n                    for tool_call_delta in tool_calls_delta:\n                        if tool_call_delta.id is not None:\n                            # TODO assert that we're not overwriting?\n                            # TODO += instead of =?\n                            accum_message.tool_calls[tool_call_delta.index].id = tool_call_delta.id\n                        if tool_call_delta.function is not None:\n                            if tool_call_delta.function.name is not None:\n                                # TODO assert that we're not overwriting?\n                                # TODO += instead of =?\n                                accum_message.tool_calls[tool_call_delta.index].function.name = tool_call_delta.function.name\n                            if tool_call_delta.function.arguments is not None:\n                                accum_message.tool_calls[tool_call_delta.index].function.arguments += tool_call_delta.function.arguments\n\n                if message_delta.function_call is not None:\n                    raise NotImplementedError(f\"Old function_call style not support with stream=True\")\n\n            # overwrite response fields based on latest chunk\n            chat_completion_response.id = chat_completion_chunk.id\n            chat_completion_response.system_fingerprint = chat_completion_chunk.system_fingerprint\n            chat_completion_response.created = chat_completion_chunk.created\n            chat_completion_response.model = chat_completion_chunk.model\n\n            # increment chunk counter\n            n_chunks += 1\n\n    except Exception as e:\n        if stream_inferface:\n            stream_inferface.stream_end()\n        print(f\"Parsing ChatCompletion stream failed with error:\\n{str(e)}\")\n        raise e\n    finally:\n        if stream_inferface:\n            stream_inferface.stream_end()\n\n    # make sure we didn't leave temp stuff in\n    assert all([c.finish_reason != TEMP_STREAM_FINISH_REASON for c in chat_completion_response.choices])\n    assert all(\n        [\n            all([tc != TEMP_STREAM_TOOL_CALL_ID for tc in c.message.tool_calls]) if c.message.tool_calls else True\n            for c in chat_completion_response.choices\n        ]\n    )\n    assert chat_completion_response.id != TEMP_STREAM_RESPONSE_ID\n\n    # compute token usage before returning\n    # TODO try actually computing the #tokens instead of assuming the chunks is the same\n    chat_completion_response.usage.completion_tokens = n_chunks\n    chat_completion_response.usage.total_tokens = prompt_tokens + n_chunks\n\n    # printd(chat_completion_response)\n    return chat_completion_response\n\n\n# def _sse_post(url: str, data: dict, headers: dict) -> Generator[ChatCompletionChunkResponse, None, None]:\n\n#     with httpx.Client() as client:\n#         with connect_sse(client, method=\"POST\", url=url, json=data, headers=headers) as event_source:\n\n#             # Inspect for errors before iterating (see https://github.com/florimondmanca/httpx-sse/pull/12)\n#             if not event_source.response.is_success:\n#                 # handle errors\n#                 from utils import printd\n\n#                 printd(\"Caught error before iterating SSE request:\", vars(event_source.response))\n#                 printd(event_source.response.read())\n\n#                 try:\n#                     response_bytes = event_source.response.read()\n#                     response_dict = json.loads(response_bytes.decode(\"utf-8\"))\n#                     error_message = response_dict[\"error\"][\"message\"]\n#                     # e.g.: This model's maximum context length is 8192 tokens. However, your messages resulted in 8198 tokens (7450 in the messages, 748 in the functions). Please reduce the length of the messages or functions.\n#                     raise Exception(error_message)\n#                 except:\n#                     print(f\"Failed to parse SSE message, throwing SSE HTTP error up the stack\")\n#                     event_source.response.raise_for_status()\n#             try:\n#                 for sse in event_source.iter_sse():\n#                     # printd(sse.event, sse.data, sse.id, sse.retry)\n#                     if sse.data == OPENAI_SSE_DONE:\n#                         # print(\"finished\")\n#                         break\n#                     else:\n#                         chunk_data = json.loads(sse.data)\n#                         # print(\"chunk_data::\", chunk_data)\n#                         chunk_object = ChatCompletionChunkResponse(**chunk_data)\n#                         # print(\"chunk_object::\", chunk_object)\n#                         # id=chunk_data[\"id\"],\n#                         # choices=[ChunkChoice],\n#                         # model=chunk_data[\"model\"],\n#                         # system_fingerprint=chunk_data[\"system_fingerprint\"]\n#                         # )\n#                         yield chunk_object\n\n#             except SSEError as e:\n#                 print(\"Caught an error while iterating the SSE stream:\", str(e))\n#                 if \"application/json\" in str(e):  # Check if the error is because of JSON response\n#                     response = client.post(url=url, json=data, headers=headers)  # Make the request again to get the JSON response\n#                     if response.headers[\"Content-Type\"].startswith(\"application/json\"):\n#                         error_details = response.json()  # Parse the JSON to get the error message\n#                         # print(\"Error:\", error_details)\n#                         # print(\"Reqeust:\", vars(response.request))\n\n#                         print(\"Request:\", vars(response.request))\n#                         print(\"POST Error:\", error_details)\n#                         print(\"Original SSE Error:\", str(e))\n#                     else:\n#                         # print(\"Failed to retrieve JSON error message.\")\n#                         print(\"Failed to retrieve JSON error message via retry.\")\n#                 else:\n#                     print(\"SSEError not related to 'application/json' content type.\")\n\n#                 # Optionally re-raise the exception if you need to propagate it\n#                 raise e\n\n#             except Exception as e:\n#                 if event_source.response.request is not None:\n#                     print(\"HTTP Request:\", vars(event_source.response.request))\n#                 if event_source.response is not None:\n#                     print(\"HTTP Status:\", event_source.response.status_code)\n#                     print(\"HTTP Headers:\", event_source.response.headers)\n#                     # print(\"HTTP Body:\", event_source.response.text)\n#                 print(\"Exception message:\", str(e))\n#                 raise e\n\n\ndef openai_chat_completions_request_stream(\n    url: str,\n    api_key: str,\n    chat_completion_request: ChatCompletionRequest,\n) -> Generator[ChatCompletionChunkResponse, None, None]:\n    from utils import printd\n\n    url = smart_urljoin(url, \"chat/completions\")\n    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n    data = chat_completion_request.model_dump(exclude_none=True)\n\n    printd(\"Request:\\n\", json.dumps(data, indent=2))\n\n    # If functions == None, strip from the payload\n    if \"functions\" in data and data[\"functions\"] is None:\n        data.pop(\"functions\")\n        data.pop(\"function_call\", None)  # extra safe,  should exist always (default=\"auto\")\n\n    if \"tools\" in data and data[\"tools\"] is None:\n        data.pop(\"tools\")\n        data.pop(\"tool_choice\", None)  # extra safe,  should exist always (default=\"auto\")\n\n    printd(f\"Sending request to {url}\")\n    try:\n        return _sse_post(url=url, data=data, headers=headers)\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        printd(f\"Got HTTPError, exception={http_err}, payload={data}\")\n        raise http_err\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        printd(f\"Got RequestException, exception={req_err}\")\n        raise req_err\n    except Exception as e:\n        # Handle other potential errors\n        printd(f\"Got unknown Exception, exception={e}\")\n        raise e\n\n\ndef openai_chat_completions_request(\n    url: str,\n    api_key: str,\n    chat_completion_request: ChatCompletionRequest,\n) -> ChatCompletionResponse:\n    \"\"\"Send a ChatCompletion request to an OpenAI-compatible server\n\n    If request.stream == True, will yield ChatCompletionChunkResponses\n    If request.stream == False, will return a ChatCompletionResponse\n\n    https://platform.openai.com/docs/guides/text-generation?lang=curl\n    \"\"\"\n    from utils import printd\n\n    url = smart_urljoin(url, \"chat/completions\")\n    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n    data = chat_completion_request.model_dump(exclude_none=True)\n\n    printd(\"Request:\\n\", json.dumps(data, indent=2))\n\n    # If functions == None, strip from the payload\n    if \"functions\" in data and data[\"functions\"] is None:\n        data.pop(\"functions\")\n        data.pop(\"function_call\", None)  # extra safe,  should exist always (default=\"auto\")\n\n    if \"tools\" in data and data[\"tools\"] is None:\n        data.pop(\"tools\")\n        data.pop(\"tool_choice\", None)  # extra safe,  should exist always (default=\"auto\")\n\n    printd(f\"Sending request to {url}\")\n    try:\n        response = requests.post(url, headers=headers, json=data)\n        # printd(f\"response = {response}, response.text = {response.text}\")\n        # print(f\"response = {response}, response.text = {response.text}\")\n        response.raise_for_status()  # Raises HTTPError for 4XX/5XX status\n\n        response = response.json()  # convert to dict from string\n        printd(f\"response.json = {response}\")\n\n        response = ChatCompletionResponse(**response)  # convert to 'dot-dict' style which is the openai python client default\n        return response\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        printd(f\"Got HTTPError, exception={http_err}, payload={data}\")\n        raise http_err\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        printd(f\"Got RequestException, exception={req_err}\")\n        raise req_err\n    except Exception as e:\n        # Handle other potential errors\n        printd(f\"Got unknown Exception, exception={e}\")\n        raise e\n\n\ndef openai_embeddings_request(url: str, api_key: str, data: dict) -> EmbeddingResponse:\n    \"\"\"https://platform.openai.com/docs/api-reference/embeddings/create\"\"\"\n    from utils import printd\n\n    url = smart_urljoin(url, \"embeddings\")\n    headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n\n    printd(f\"Sending request to {url}\")\n    try:\n        response = requests.post(url, headers=headers, json=data)\n        printd(f\"response = {response}\")\n        response.raise_for_status()  # Raises HTTPError for 4XX/5XX status\n        response = response.json()  # convert to dict from string\n        printd(f\"response.json = {response}\")\n        response = EmbeddingResponse(**response)  # convert to 'dot-dict' style which is the openai python client default\n        return response\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        printd(f\"Got HTTPError, exception={http_err}, payload={data}\")\n        raise http_err\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        printd(f\"Got RequestException, exception={req_err}\")\n        raise req_err\n    except Exception as e:\n        # Handle other potential errors\n        printd(f\"Got unknown Exception, exception={e}\")\n        raise e\n"}
{"type": "source_file", "path": "luann/llm_api/azure_openai.py", "content": "from typing import Union\n\nimport requests\n\nfrom models.chat_completion_response import ChatCompletionResponse\nfrom models.embedding_response import EmbeddingResponse\nfrom utils import smart_urljoin\n\nMODEL_TO_AZURE_ENGINE = {\n    \"gpt-4-1106-preview\": \"gpt-4\",\n    \"gpt-4\": \"gpt-4\",\n    \"gpt-4-32k\": \"gpt-4-32k\",\n    \"gpt-3.5\": \"gpt-35-turbo\",\n    \"gpt-3.5-turbo\": \"gpt-35-turbo\",\n    \"gpt-3.5-turbo-16k\": \"gpt-35-turbo-16k\",\n}\n\n\ndef clean_azure_endpoint(raw_endpoint_name: str) -> str:\n    \"\"\"Make sure the endpoint is of format 'https://YOUR_RESOURCE_NAME.openai.azure.com'\"\"\"\n    if raw_endpoint_name is None:\n        raise ValueError(raw_endpoint_name)\n    endpoint_address = raw_endpoint_name.strip(\"/\").replace(\".openai.azure.com\", \"\")\n    endpoint_address = endpoint_address.replace(\"http://\", \"\")\n    endpoint_address = endpoint_address.replace(\"https://\", \"\")\n    return endpoint_address\n\n\ndef azure_openai_get_model_list(url: str, api_key: Union[str, None], api_version: str) -> dict:\n    \"\"\"https://learn.microsoft.com/en-us/rest/api/azureopenai/models/list?view=rest-azureopenai-2023-05-15&tabs=HTTP\"\"\"\n    from utils import printd\n\n    # https://xxx.openai.azure.com/openai/models?api-version=xxx\n    url = smart_urljoin(url, \"openai\")\n    url = smart_urljoin(url, f\"models?api-version={api_version}\")\n\n    headers = {\"Content-Type\": \"application/json\"}\n    if api_key is not None:\n        headers[\"api-key\"] = f\"{api_key}\"\n\n    printd(f\"Sending request to {url}\")\n    try:\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()  # Raises HTTPError for 4XX/5XX status\n        response = response.json()  # convert to dict from string\n        printd(f\"response = {response}\")\n        return response\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        try:\n            response = response.json()\n        except:\n            pass\n        printd(f\"Got HTTPError, exception={http_err}, response={response}\")\n        raise http_err\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        try:\n            response = response.json()\n        except:\n            pass\n        printd(f\"Got RequestException, exception={req_err}, response={response}\")\n        raise req_err\n    except Exception as e:\n        # Handle other potential errors\n        try:\n            response = response.json()\n        except:\n            pass\n        printd(f\"Got unknown Exception, exception={e}, response={response}\")\n        raise e\n\n\ndef azure_openai_chat_completions_request(\n    resource_name: str, deployment_id: str, api_version: str, api_key: str, data: dict\n) -> ChatCompletionResponse:\n    \"\"\"https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions\"\"\"\n    from utils import printd\n\n    assert resource_name is not None, \"Missing required field when calling Azure OpenAI\"\n    assert deployment_id is not None, \"Missing required field when calling Azure OpenAI\"\n    assert api_version is not None, \"Missing required field when calling Azure OpenAI\"\n    assert api_key is not None, \"Missing required field when calling Azure OpenAI\"\n\n    resource_name = clean_azure_endpoint(resource_name)\n    url = f\"https://{resource_name}.openai.azure.com/openai/deployments/{deployment_id}/chat/completions?api-version={api_version}\"\n    headers = {\"Content-Type\": \"application/json\", \"api-key\": f\"{api_key}\"}\n\n    # If functions == None, strip from the payload\n    if \"functions\" in data and data[\"functions\"] is None:\n        data.pop(\"functions\")\n        data.pop(\"function_call\", None)  # extra safe,  should exist always (default=\"auto\")\n\n    if \"tools\" in data and data[\"tools\"] is None:\n        data.pop(\"tools\")\n        data.pop(\"tool_choice\", None)  # extra safe,  should exist always (default=\"auto\")\n\n    printd(f\"Sending request to {url}\")\n    try:\n        response = requests.post(url, headers=headers, json=data)\n        printd(f\"response = {response}\")\n        response.raise_for_status()  # Raises HTTPError for 4XX/5XX status\n        response = response.json()  # convert to dict from string\n        printd(f\"response.json = {response}\")\n        # NOTE: azure openai does not include \"content\" in the response when it is None, so we need to add it\n        if \"content\" not in response[\"choices\"][0].get(\"message\"):\n            response[\"choices\"][0][\"message\"][\"content\"] = None\n        response = ChatCompletionResponse(**response)  # convert to 'dot-dict' style which is the openai python client default\n        return response\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        printd(f\"Got HTTPError, exception={http_err}, payload={data}\")\n        raise http_err\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        printd(f\"Got RequestException, exception={req_err}\")\n        raise req_err\n    except Exception as e:\n        # Handle other potential errors\n        printd(f\"Got unknown Exception, exception={e}\")\n        raise e\n\n\ndef azure_openai_embeddings_request(\n    resource_name: str, deployment_id: str, api_version: str, api_key: str, data: dict\n) -> EmbeddingResponse:\n    \"\"\"https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#embeddings\"\"\"\n    from utils import printd\n\n    resource_name = clean_azure_endpoint(resource_name)\n    url = f\"https://{resource_name}.openai.azure.com/openai/deployments/{deployment_id}/embeddings?api-version={api_version}\"\n    headers = {\"Content-Type\": \"application/json\", \"api-key\": f\"{api_key}\"}\n\n    printd(f\"Sending request to {url}\")\n    try:\n        response = requests.post(url, headers=headers, json=data)\n        printd(f\"response = {response}\")\n        response.raise_for_status()  # Raises HTTPError for 4XX/5XX status\n        response = response.json()  # convert to dict from string\n        printd(f\"response.json = {response}\")\n        response = EmbeddingResponse(**response)  # convert to 'dot-dict' style which is the openai python client default\n        return response\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        printd(f\"Got HTTPError, exception={http_err}, payload={data}\")\n        raise http_err\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        printd(f\"Got RequestException, exception={req_err}\")\n        raise req_err\n    except Exception as e:\n        # Handle other potential errors\n        printd(f\"Got unknown Exception, exception={e}\")\n        raise e\n"}
{"type": "source_file", "path": "luann/local_llm/constants.py", "content": "# import local_llm.llm_chat_completion_wrappers.airoboros as airoboros\nfrom local_llm.llm_chat_completion_wrappers.chatml import (\n    ChatMLInnerMonologueWrapper,\n)\n\nDEFAULT_ENDPOINTS = {\n    # Local\n    \"koboldcpp\": \"http://localhost:5001\",\n    \"llamacpp\": \"http://localhost:8080\",\n    \"lmstudio\": \"http://localhost:1234\",\n    \"lmstudio-legacy\": \"http://localhost:1234\",\n    \"ollama\": \"http://localhost:11434\",\n    \"webui-legacy\": \"http://localhost:5000\",\n    \"webui\": \"http://localhost:5000\",\n    \"vllm\": \"http://localhost:8000\",\n    # APIs\n    \"openai\": \"https://api.openai.com\",\n    \"anthropic\": \"https://api.anthropic.com\",\n    \"groq\": \"https://api.groq.com/openai\",\n}\n\nDEFAULT_OLLAMA_MODEL = \"dolphin2.2-mistral:7b-q6_K\"\n\n# DEFAULT_WRAPPER = airoboros.Airoboros21InnerMonologueWrapper\n# DEFAULT_WRAPPER_NAME = \"airoboros-l2-70b-2.1\"\n\nDEFAULT_WRAPPER = ChatMLInnerMonologueWrapper\nDEFAULT_WRAPPER_NAME = \"chatml\"\n\nINNER_THOUGHTS_KWARG = \"inner_thoughts\"\nINNER_THOUGHTS_KWARG_DESCRIPTION = \"Deep inner monologue private to you only.\"\n"}
{"type": "source_file", "path": "luann/llm_api/cohere.py", "content": "import json\nimport uuid\nfrom typing import List, Optional, Union\n\nimport requests\n\nfrom constants import JSON_ENSURE_ASCII\nfrom data_types import Message\nfrom local_llm.utils import count_tokens\nfrom models.chat_completion_request import ChatCompletionRequest, Tool\nfrom models.chat_completion_response import (\n    ChatCompletionResponse,\n    Choice,\n    FunctionCall,\n)\nfrom models.chat_completion_response import (\n    Message as ChoiceMessage,  # NOTE: avoid conflict with our own typeagent Message datatype\n)\nfrom models.chat_completion_response import ToolCall, UsageStatistics\nfrom utils import get_tool_call_id, get_utc_time, smart_urljoin\n\nBASE_URL = \"https://api.cohere.ai/v1\"\n\n# models that we know will work with typeagent\nCOHERE_VALID_MODEL_LIST = [\n    \"command-r-plus\",\n]\n\n\ndef cohere_get_model_details(url: str, api_key: Union[str, None], model: str) -> int:\n    \"\"\"https://docs.cohere.com/reference/get-model\"\"\"\n    from utils import printd\n\n    url = smart_urljoin(url, \"models\")\n    url = smart_urljoin(url, model)\n    headers = {\n        \"accept\": \"application/json\",\n        \"authorization\": f\"bearer {api_key}\",\n    }\n\n    printd(f\"Sending request to {url}\")\n    try:\n        response = requests.get(url, headers=headers)\n        printd(f\"response = {response}\")\n        response.raise_for_status()  # Raises HTTPError for 4XX/5XX status\n        response = response.json()  # convert to dict from string\n        return response\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        printd(f\"Got HTTPError, exception={http_err}\")\n        raise http_err\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        printd(f\"Got RequestException, exception={req_err}\")\n        raise req_err\n    except Exception as e:\n        # Handle other potential errors\n        printd(f\"Got unknown Exception, exception={e}\")\n        raise e\n\n\ndef cohere_get_model_context_window(url: str, api_key: Union[str, None], model: str) -> int:\n    model_details = cohere_get_model_details(url=url, api_key=api_key, model=model)\n    return model_details[\"context_length\"]\n\n\ndef cohere_get_model_list(url: str, api_key: Union[str, None]) -> dict:\n    \"\"\"https://docs.cohere.com/reference/list-models\"\"\"\n    from utils import printd\n\n    url = smart_urljoin(url, \"models\")\n    headers = {\n        \"accept\": \"application/json\",\n        \"authorization\": f\"bearer {api_key}\",\n    }\n\n    printd(f\"Sending request to {url}\")\n    try:\n        response = requests.get(url, headers=headers)\n        printd(f\"response = {response}\")\n        response.raise_for_status()  # Raises HTTPError for 4XX/5XX status\n        response = response.json()  # convert to dict from string\n        return response[\"models\"]\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        printd(f\"Got HTTPError, exception={http_err}\")\n        raise http_err\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        printd(f\"Got RequestException, exception={req_err}\")\n        raise req_err\n    except Exception as e:\n        # Handle other potential errors\n        printd(f\"Got unknown Exception, exception={e}\")\n        raise e\n\n\ndef remap_finish_reason(finish_reason: str) -> str:\n    \"\"\"Remap Cohere's 'finish_reason' to OpenAI 'finish_reason'\n\n    OpenAI: 'stop', 'length', 'function_call', 'content_filter', null\n    see: https://platform.openai.com/docs/guides/text-generation/chat-completions-api\n\n    Cohere finish_reason is different but undocumented ???\n    \"\"\"\n    if finish_reason == \"COMPLETE\":\n        return \"stop\"\n    elif finish_reason == \"MAX_TOKENS\":\n        return \"length\"\n    # elif stop_reason == \"tool_use\":\n    # return \"function_call\"\n    else:\n        raise ValueError(f\"Unexpected stop_reason: {finish_reason}\")\n\n\ndef convert_cohere_response_to_chatcompletion(\n    response_json: dict,  # REST response from API\n    model: str,  # Required since not returned\n    inner_thoughts_in_kwargs: Optional[bool] = True,\n) -> ChatCompletionResponse:\n    \"\"\"\n    Example response from command-r-plus:\n    response.json = {\n        'response_id': '28c47751-acce-41cd-8c89-c48a15ac33cf',\n        'text': '',\n        'generation_id': '84209c9e-2868-4984-82c5-063b748b7776',\n        'chat_history': [\n            {\n                'role': 'CHATBOT',\n                'message': 'Bootup sequence complete. Persona activated. Testing messaging functionality.'\n            },\n            {\n                'role': 'SYSTEM',\n                'message': '{\"status\": \"OK\", \"message\": null, \"time\": \"2024-04-11 11:22:36 PM PDT-0700\"}'\n            }\n        ],\n        'finish_reason': 'COMPLETE',\n        'meta': {\n            'api_version': {'version': '1'},\n            'billed_units': {'input_tokens': 692, 'output_tokens': 20},\n            'tokens': {'output_tokens': 20}\n        },\n        'tool_calls': [\n            {\n                'name': 'send_message',\n                'parameters': {\n                    'message': \"Hello Chad, it's Sam. How are you feeling today?\"\n                }\n            }\n        ]\n    }\n    \"\"\"\n    if \"billed_units\" in response_json[\"meta\"]:\n        prompt_tokens = response_json[\"meta\"][\"billed_units\"][\"input_tokens\"]\n        completion_tokens = response_json[\"meta\"][\"billed_units\"][\"output_tokens\"]\n    else:\n        # For some reason input_tokens not included in 'meta' 'tokens' dict?\n        prompt_tokens = count_tokens(\n            json.dumps(response_json[\"chat_history\"], ensure_ascii=JSON_ENSURE_ASCII)\n        )  # NOTE: this is a very rough approximation\n        completion_tokens = response_json[\"meta\"][\"tokens\"][\"output_tokens\"]\n\n    finish_reason = remap_finish_reason(response_json[\"finish_reason\"])\n\n    if \"tool_calls\" in response_json and response_json[\"tool_calls\"] is not None:\n        inner_thoughts = []\n        tool_calls = []\n        for tool_call_response in response_json[\"tool_calls\"]:\n            function_name = tool_call_response[\"name\"]\n            function_args = tool_call_response[\"parameters\"]\n            if inner_thoughts_in_kwargs:\n                from local_llm.constants import INNER_THOUGHTS_KWARG\n\n                assert INNER_THOUGHTS_KWARG in function_args\n                # NOTE:\n                inner_thoughts.append(function_args.pop(INNER_THOUGHTS_KWARG))\n\n            tool_calls.append(\n                ToolCall(\n                    id=get_tool_call_id(),\n                    type=\"function\",\n                    function=FunctionCall(\n                        name=function_name,\n                        arguments=json.dumps(function_args),\n                    ),\n                )\n            )\n\n        # NOTE: no multi-call support for now\n        assert len(tool_calls) == 1, tool_calls\n        content = inner_thoughts[0]\n\n    else:\n        # raise NotImplementedError(f\"Expected a tool call response from Cohere API\")\n        content = response_json[\"text\"]\n        tool_calls = None\n\n    # In Cohere API empty string == null\n    content = None if content == \"\" else content\n    assert content is not None or tool_calls is not None, \"Response message must have either content or tool_calls\"\n\n    choice = Choice(\n        index=0,\n        finish_reason=finish_reason,\n        message=ChoiceMessage(\n            role=\"assistant\",\n            content=content,\n            tool_calls=tool_calls,\n        ),\n    )\n\n    return ChatCompletionResponse(\n        id=response_json[\"response_id\"],\n        choices=[choice],\n        created=get_utc_time(),\n        model=model,\n        usage=UsageStatistics(\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            total_tokens=prompt_tokens + completion_tokens,\n        ),\n    )\n\n\ndef convert_tools_to_cohere_format(tools: List[Tool], inner_thoughts_in_kwargs: Optional[bool] = True) -> List[dict]:\n    \"\"\"See: https://docs.cohere.com/reference/chat\n\n    OpenAI style:\n      \"tools\": [{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"find_movies\",\n            \"description\": \"find ....\",\n            \"parameters\": {\n              \"type\": \"object\",\n              \"properties\": {\n                 PARAM: {\n                   \"type\": PARAM_TYPE,  # eg \"string\"\n                   \"description\": PARAM_DESCRIPTION,\n                 },\n                 ...\n              },\n              \"required\": List[str],\n            }\n        }\n      }]\n\n    Cohere style:\n      \"tools\": [{\n        \"name\": \"find_movies\",\n        \"description\": \"find ....\",\n        \"parameter_definitions\": {\n            PARAM_NAME: {\n                \"description\": PARAM_DESCRIPTION,\n                \"type\": PARAM_TYPE,  # eg \"string\"\n                \"required\": <boolean>,\n            }\n          },\n        }\n      }]\n    \"\"\"\n    tools_dict_list = []\n    for tool in tools:\n        tools_dict_list.append(\n            {\n                \"name\": tool.function.name,\n                \"description\": tool.function.description,\n                \"parameter_definitions\": {\n                    p_name: {\n                        \"description\": p_fields[\"description\"],\n                        \"type\": p_fields[\"type\"],\n                        \"required\": p_name in tool.function.parameters[\"required\"],\n                    }\n                    for p_name, p_fields in tool.function.parameters[\"properties\"].items()\n                },\n            }\n        )\n\n    if inner_thoughts_in_kwargs:\n        # NOTE: since Cohere doesn't allow \"text\" in the response when a tool call happens, if we want\n        # a simultaneous CoT + tool call we need to put it inside a kwarg\n        from local_llm.constants import (\n            INNER_THOUGHTS_KWARG,\n            INNER_THOUGHTS_KWARG_DESCRIPTION,\n        )\n\n        for cohere_tool in tools_dict_list:\n            cohere_tool[\"parameter_definitions\"][INNER_THOUGHTS_KWARG] = {\n                \"description\": INNER_THOUGHTS_KWARG_DESCRIPTION,\n                \"type\": \"string\",\n                \"required\": True,\n            }\n\n    return tools_dict_list\n\n\ndef cohere_chat_completions_request(\n    url: str,\n    api_key: str,\n    chat_completion_request: ChatCompletionRequest,\n) -> ChatCompletionResponse:\n    \"\"\"https://docs.cohere.com/docs/multi-step-tool-use\"\"\"\n    from utils import printd\n\n    url = smart_urljoin(url, \"chat\")\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"bearer {api_key}\",\n    }\n\n    # convert the tools\n    cohere_tools = None if chat_completion_request.tools is None else convert_tools_to_cohere_format(chat_completion_request.tools)\n\n    # pydantic -> dict\n    data = chat_completion_request.model_dump(exclude_none=True)\n\n    if \"functions\" in data:\n        raise ValueError(f\"'functions' unexpected in Anthropic API payload\")\n\n    # If tools == None, strip from the payload\n    if \"tools\" in data and data[\"tools\"] is None:\n        data.pop(\"tools\")\n        data.pop(\"tool_choice\", None)  # extra safe,  should exist always (default=\"auto\")\n\n    # Convert messages to Cohere format\n    msg_objs = [Message.dict_to_message(user_id=uuid.uuid4(), agent_id=uuid.uuid4(), openai_message_dict=m) for m in data[\"messages\"]]\n\n    # System message 0 should instead be a \"preamble\"\n    # See: https://docs.cohere.com/reference/chat\n    # The chat_history parameter should not be used for SYSTEM messages in most cases. Instead, to add a SYSTEM role message at the beginning of a conversation, the preamble parameter should be used.\n    assert msg_objs[0].role == \"system\", msg_objs[0]\n    preamble = msg_objs[0].text\n\n    # data[\"messages\"] = [m.to_cohere_dict() for m in msg_objs[1:]]\n    data[\"messages\"] = []\n    for m in msg_objs[1:]:\n        ms = m.to_cohere_dict()  # NOTE: returns List[dict]\n        data[\"messages\"].extend(ms)\n\n    assert data[\"messages\"][-1][\"role\"] == \"USER\", data[\"messages\"][-1]\n    data = {\n        \"preamble\": preamble,\n        \"chat_history\": data[\"messages\"][:-1],\n        \"message\": data[\"messages\"][-1][\"message\"],\n        \"tools\": cohere_tools,\n    }\n\n    # Move 'system' to the top level\n    # 'messages: Unexpected role \"system\". The Messages API accepts a top-level `system` parameter, not \"system\" as an input message role.'\n    # assert data[\"messages\"][0][\"role\"] == \"system\", f\"Expected 'system' role in messages[0]:\\n{data['messages'][0]}\"\n    # data[\"system\"] = data[\"messages\"][0][\"content\"]\n    # data[\"messages\"] = data[\"messages\"][1:]\n\n    # Convert to Anthropic format\n    # msg_objs = [Message.dict_to_message(user_id=uuid.uuid4(), agent_id=uuid.uuid4(), openai_message_dict=m) for m in data[\"messages\"]]\n    # data[\"messages\"] = [m.to_anthropic_dict(inner_thoughts_xml_tag=inner_thoughts_xml_tag) for m in msg_objs]\n\n    # Handling Anthropic special requirement for 'user' message in front\n    # messages: first message must use the \"user\" role'\n    # if data[\"messages\"][0][\"role\"] != \"user\":\n    # data[\"messages\"] = [{\"role\": \"user\", \"content\": DUMMY_FIRST_USER_MESSAGE}] + data[\"messages\"]\n\n    # Handle Anthropic's restriction on alternating user/assistant messages\n    # data[\"messages\"] = merge_tool_results_into_user_messages(data[\"messages\"])\n\n    # Anthropic also wants max_tokens in the input\n    # It's also part of ChatCompletions\n    # assert \"max_tokens\" in data, data\n\n    # Remove extra fields used by OpenAI but not Anthropic\n    # data.pop(\"frequency_penalty\", None)\n    # data.pop(\"logprobs\", None)\n    # data.pop(\"n\", None)\n    # data.pop(\"top_p\", None)\n    # data.pop(\"presence_penalty\", None)\n    # data.pop(\"user\", None)\n    # data.pop(\"tool_choice\", None)\n\n    printd(f\"Sending request to {url}\")\n    try:\n        response = requests.post(url, headers=headers, json=data)\n        printd(f\"response = {response}\")\n        response.raise_for_status()  # Raises HTTPError for 4XX/5XX status\n        response = response.json()  # convert to dict from string\n        printd(f\"response.json = {response}\")\n        response = convert_cohere_response_to_chatcompletion(response_json=response, model=chat_completion_request.model)\n        return response\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        printd(f\"Got HTTPError, exception={http_err}, payload={data}\")\n        raise http_err\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        printd(f\"Got RequestException, exception={req_err}\")\n        raise req_err\n    except Exception as e:\n        # Handle other potential errors\n        printd(f\"Got unknown Exception, exception={e}\")\n        raise e\n"}
{"type": "source_file", "path": "luann/local_llm/__init__.py", "content": ""}
{"type": "source_file", "path": "luann/local_llm/grammars/__init__.py", "content": ""}
{"type": "source_file", "path": "luann/local_llm/function_parser.py", "content": "import copy\nimport json\n\nfrom constants import JSON_ENSURE_ASCII, JSON_LOADS_STRICT\n\nNO_HEARTBEAT_FUNCS = [\"send_message\", \"pause_heartbeats\"]\n\n\ndef insert_heartbeat(message):\n    # message_copy = message.copy()\n    message_copy = copy.deepcopy(message)\n\n    if message_copy.get(\"function_call\"):\n        # function_name = message.get(\"function_call\").get(\"name\")\n        params = message_copy.get(\"function_call\").get(\"arguments\")\n        params = json.loads(params, strict=JSON_LOADS_STRICT)\n        params[\"request_heartbeat\"] = True\n        message_copy[\"function_call\"][\"arguments\"] = json.dumps(params, ensure_ascii=JSON_ENSURE_ASCII)\n\n    elif message_copy.get(\"tool_call\"):\n        # function_name = message.get(\"tool_calls\")[0].get(\"function\").get(\"name\")\n        params = message_copy.get(\"tool_calls\")[0].get(\"function\").get(\"arguments\")\n        params = json.loads(params, strict=JSON_LOADS_STRICT)\n        params[\"request_heartbeat\"] = True\n        message_copy[\"tools_calls\"][0][\"function\"][\"arguments\"] = json.dumps(params, ensure_ascii=JSON_ENSURE_ASCII)\n\n    return message_copy\n\n\ndef heartbeat_correction(message_history, new_message):\n    \"\"\"Add heartbeats where we think the agent forgot to add them themselves\n\n    If the last message in the stack is a user message and the new message is an assistant func call, fix the heartbeat\n\n    See: https://github.com/cpacker/typeagent/issues/601\n    \"\"\"\n    if len(message_history) < 1:\n        return None\n\n    last_message_was_user = False\n    if message_history[-1][\"role\"] == \"user\":\n        try:\n            content = json.loads(message_history[-1][\"content\"], strict=JSON_LOADS_STRICT)\n        except json.JSONDecodeError:\n            return None\n        # Check if it's a user message or system message\n        if content[\"type\"] == \"user_message\":\n            last_message_was_user = True\n\n    new_message_is_heartbeat_function = False\n    if new_message[\"role\"] == \"assistant\":\n        if new_message.get(\"function_call\") or new_message.get(\"tool_calls\"):\n            if new_message.get(\"function_call\"):\n                function_name = new_message.get(\"function_call\").get(\"name\")\n            elif new_message.get(\"tool_calls\"):\n                function_name = new_message.get(\"tool_calls\")[0].get(\"function\").get(\"name\")\n            if function_name not in NO_HEARTBEAT_FUNCS:\n                new_message_is_heartbeat_function = True\n\n    if last_message_was_user and new_message_is_heartbeat_function:\n        return insert_heartbeat(new_message)\n    else:\n        return None\n\n\ndef patch_function(message_history, new_message):\n    corrected_output = heartbeat_correction(message_history=message_history, new_message=new_message)\n    return corrected_output if corrected_output is not None else new_message\n"}
{"type": "source_file", "path": "luann/local_llm/grammars/gbnf_grammar_generator.py", "content": "import inspect\nimport json\nimport re\nfrom copy import copy\nfrom enum import Enum\nfrom inspect import getdoc, isclass\nfrom types import NoneType\nfrom typing import (\n    Any,\n    Callable,\n    List,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n    _GenericAlias,\n    get_args,\n    get_origin,\n)\nfrom constants import JSON_ENSURE_ASCII\nfrom docstring_parser import parse\nfrom pydantic import BaseModel, create_model\n\n\nclass PydanticDataType(Enum):\n    \"\"\"\n    Defines the data types supported by the grammar_generator.\n\n    Attributes:\n        STRING (str): Represents a string data type.\n        BOOLEAN (str): Represents a boolean data type.\n        INTEGER (str): Represents an integer data type.\n        FLOAT (str): Represents a float data type.\n        OBJECT (str): Represents an object data type.\n        ARRAY (str): Represents an array data type.\n        ENUM (str): Represents an enum data type.\n        CUSTOM_CLASS (str): Represents a custom class data type.\n    \"\"\"\n\n    STRING = \"string\"\n    TRIPLE_QUOTED_STRING = \"triple_quoted_string\"\n    MARKDOWN_CODE_BLOCK = \"markdown_code_block\"\n    BOOLEAN = \"boolean\"\n    INTEGER = \"integer\"\n    FLOAT = \"float\"\n    OBJECT = \"object\"\n    ARRAY = \"array\"\n    ENUM = \"enum\"\n    ANY = \"any\"\n    NULL = \"null\"\n    CUSTOM_CLASS = \"custom-class\"\n    CUSTOM_DICT = \"custom-dict\"\n    SET = \"set\"\n\n\ndef map_pydantic_type_to_gbnf(pydantic_type: Type[Any]) -> str:\n    if isclass(pydantic_type) and issubclass(pydantic_type, str):\n        return PydanticDataType.STRING.value\n    elif isclass(pydantic_type) and issubclass(pydantic_type, bool):\n        return PydanticDataType.BOOLEAN.value\n    elif isclass(pydantic_type) and issubclass(pydantic_type, int):\n        return PydanticDataType.INTEGER.value\n    elif isclass(pydantic_type) and issubclass(pydantic_type, float):\n        return PydanticDataType.FLOAT.value\n    elif isclass(pydantic_type) and issubclass(pydantic_type, Enum):\n        return PydanticDataType.ENUM.value\n\n    elif isclass(pydantic_type) and issubclass(pydantic_type, BaseModel):\n        return format_model_and_field_name(pydantic_type.__name__)\n    elif get_origin(pydantic_type) == list:\n        element_type = get_args(pydantic_type)[0]\n        return f\"{map_pydantic_type_to_gbnf(element_type)}-list\"\n    elif get_origin(pydantic_type) == set:\n        element_type = get_args(pydantic_type)[0]\n        return f\"{map_pydantic_type_to_gbnf(element_type)}-set\"\n    elif get_origin(pydantic_type) == Union:\n        union_types = get_args(pydantic_type)\n        union_rules = [map_pydantic_type_to_gbnf(ut) for ut in union_types]\n        return f\"union-{'-or-'.join(union_rules)}\"\n    elif get_origin(pydantic_type) == Optional:\n        element_type = get_args(pydantic_type)[0]\n        return f\"optional-{map_pydantic_type_to_gbnf(element_type)}\"\n    elif isclass(pydantic_type):\n        return f\"{PydanticDataType.CUSTOM_CLASS.value}-{format_model_and_field_name(pydantic_type.__name__)}\"\n    elif get_origin(pydantic_type) == dict:\n        key_type, value_type = get_args(pydantic_type)\n        return f\"custom-dict-key-type-{format_model_and_field_name(map_pydantic_type_to_gbnf(key_type))}-value-type-{format_model_and_field_name(map_pydantic_type_to_gbnf(value_type))}\"\n    else:\n        return \"unknown\"\n\n\ndef format_model_and_field_name(model_name: str) -> str:\n    parts = re.findall(\"[A-Z][^A-Z]*\", model_name)\n    if not parts:  # Check if the list is empty\n        return model_name.lower().replace(\"_\", \"-\")\n    return \"-\".join(part.lower().replace(\"_\", \"-\") for part in parts)\n\n\ndef generate_list_rule(element_type):\n    \"\"\"\n    Generate a GBNF rule for a list of a given element type.\n\n    :param element_type: The type of the elements in the list (e.g., 'string').\n    :return: A string representing the GBNF rule for a list of the given type.\n    \"\"\"\n    rule_name = f\"{map_pydantic_type_to_gbnf(element_type)}-list\"\n    element_rule = map_pydantic_type_to_gbnf(element_type)\n    list_rule = rf'{rule_name} ::= \"[\"  {element_rule} (\",\"  {element_rule})* \"]\"'\n    return list_rule\n\n\ndef get_members_structure(cls, rule_name):\n    if issubclass(cls, Enum):\n        # Handle Enum types\n        members = [f'\"\\\\\"{member.value}\\\\\"\"' for name, member in cls.__members__.items()]\n        return f\"{cls.__name__.lower()} ::= \" + \" | \".join(members)\n    if cls.__annotations__ and cls.__annotations__ != {}:\n        result = f'{rule_name} ::= \"{{\"'\n        type_list_rules = []\n        # Modify this comprehension\n        members = [\n            f'  \"\\\\\"{name}\\\\\"\" \":\"  {map_pydantic_type_to_gbnf(param_type)}'\n            for name, param_type in cls.__annotations__.items()\n            if name != \"self\"\n        ]\n\n        result += '\",\" '.join(members)\n        result += '  \"}\"'\n        return result, type_list_rules\n    elif rule_name == \"custom-class-any\":\n        result = f\"{rule_name} ::= \"\n        result += \"value\"\n        type_list_rules = []\n        return result, type_list_rules\n    else:\n        init_signature = inspect.signature(cls.__init__)\n        parameters = init_signature.parameters\n        result = f'{rule_name} ::=  \"{{\"'\n        type_list_rules = []\n        # Modify this comprehension too\n        members = [\n            f'  \"\\\\\"{name}\\\\\"\" \":\"  {map_pydantic_type_to_gbnf(param.annotation)}'\n            for name, param in parameters.items()\n            if name != \"self\" and param.annotation != inspect.Parameter.empty\n        ]\n\n        result += '\", \"'.join(members)\n        result += '  \"}\"'\n        return result, type_list_rules\n\n\ndef regex_to_gbnf(regex_pattern: str) -> str:\n    \"\"\"\n    Translate a basic regex pattern to a GBNF rule.\n    Note: This function handles only a subset of simple regex patterns.\n    \"\"\"\n    gbnf_rule = regex_pattern\n\n    # Translate common regex components to GBNF\n    gbnf_rule = gbnf_rule.replace(\"\\\\d\", \"[0-9]\")\n    gbnf_rule = gbnf_rule.replace(\"\\\\s\", \"[ \\t\\n]\")\n\n    # Handle quantifiers and other regex syntax that is similar in GBNF\n    # (e.g., '*', '+', '?', character classes)\n\n    return gbnf_rule\n\n\ndef generate_gbnf_integer_rules(max_digit=None, min_digit=None):\n    \"\"\"\n\n    Generate GBNF Integer Rules\n\n    Generates GBNF (Generalized Backus-Naur Form) rules for integers based on the given maximum and minimum digits.\n\n    Parameters:\n        max_digit (int): The maximum number of digits for the integer. Default is None.\n        min_digit (int): The minimum number of digits for the integer. Default is None.\n\n    Returns:\n        integer_rule (str): The identifier for the integer rule generated.\n        additional_rules (list): A list of additional rules generated based on the given maximum and minimum digits.\n\n    \"\"\"\n    additional_rules = []\n\n    # Define the rule identifier based on max_digit and min_digit\n    integer_rule = \"integer-part\"\n    if max_digit is not None:\n        integer_rule += f\"-max{max_digit}\"\n    if min_digit is not None:\n        integer_rule += f\"-min{min_digit}\"\n\n    # Handling Integer Rules\n    if max_digit is not None or min_digit is not None:\n        # Start with an empty rule part\n        integer_rule_part = \"\"\n\n        # Add mandatory digits as per min_digit\n        if min_digit is not None:\n            integer_rule_part += \"[0-9] \" * min_digit\n\n        # Add optional digits up to max_digit\n        if max_digit is not None:\n            optional_digits = max_digit - (min_digit if min_digit is not None else 0)\n            integer_rule_part += \"\".join([\"[0-9]? \" for _ in range(optional_digits)])\n\n        # Trim the rule part and append it to additional rules\n        integer_rule_part = integer_rule_part.strip()\n        if integer_rule_part:\n            additional_rules.append(f\"{integer_rule} ::= {integer_rule_part}\")\n\n    return integer_rule, additional_rules\n\n\ndef generate_gbnf_float_rules(max_digit=None, min_digit=None, max_precision=None, min_precision=None):\n    \"\"\"\n    Generate GBNF float rules based on the given constraints.\n\n    :param max_digit: Maximum number of digits in the integer part (default: None)\n    :param min_digit: Minimum number of digits in the integer part (default: None)\n    :param max_precision: Maximum number of digits in the fractional part (default: None)\n    :param min_precision: Minimum number of digits in the fractional part (default: None)\n    :return: A tuple containing the float rule and additional rules as a list\n\n    Example Usage:\n    max_digit = 3\n    min_digit = 1\n    max_precision = 2\n    min_precision = 1\n    generate_gbnf_float_rules(max_digit, min_digit, max_precision, min_precision)\n\n    Output:\n    ('float-3-1-2-1', ['integer-part-max3-min1 ::= [0-9] [0-9] [0-9]?', 'fractional-part-max2-min1 ::= [0-9] [0-9]?', 'float-3-1-2-1 ::= integer-part-max3-min1 \".\" fractional-part-max2-min\n    *1'])\n\n    Note:\n    GBNF stands for Generalized Backus-Naur Form, which is a notation technique to specify the syntax of programming languages or other formal grammars.\n    \"\"\"\n    additional_rules = []\n\n    # Define the integer part rule\n    integer_part_rule = (\n        \"integer-part\" + (f\"-max{max_digit}\" if max_digit is not None else \"\") + (f\"-min{min_digit}\" if min_digit is not None else \"\")\n    )\n\n    # Define the fractional part rule based on precision constraints\n    fractional_part_rule = \"fractional-part\"\n    fractional_rule_part = \"\"\n    if max_precision is not None or min_precision is not None:\n        fractional_part_rule += (f\"-max{max_precision}\" if max_precision is not None else \"\") + (\n            f\"-min{min_precision}\" if min_precision is not None else \"\"\n        )\n        # Minimum number of digits\n        fractional_rule_part = \"[0-9]\" * (min_precision if min_precision is not None else 1)\n        # Optional additional digits\n        fractional_rule_part += \"\".join(\n            [\" [0-9]?\"] * ((max_precision - (min_precision if min_precision is not None else 1)) if max_precision is not None else 0)\n        )\n        additional_rules.append(f\"{fractional_part_rule} ::= {fractional_rule_part}\")\n\n    # Define the float rule\n    float_rule = f\"float-{max_digit if max_digit is not None else 'X'}-{min_digit if min_digit is not None else 'X'}-{max_precision if max_precision is not None else 'X'}-{min_precision if min_precision is not None else 'X'}\"\n    additional_rules.append(f'{float_rule} ::= {integer_part_rule} \".\" {fractional_part_rule}')\n\n    # Generating the integer part rule definition, if necessary\n    if max_digit is not None or min_digit is not None:\n        integer_rule_part = \"[0-9]\"\n        if min_digit is not None and min_digit > 1:\n            integer_rule_part += \" [0-9]\" * (min_digit - 1)\n        if max_digit is not None:\n            integer_rule_part += \"\".join([\" [0-9]?\"] * (max_digit - (min_digit if min_digit is not None else 1)))\n        additional_rules.append(f\"{integer_part_rule} ::= {integer_rule_part.strip()}\")\n\n    return float_rule, additional_rules\n\n\ndef generate_gbnf_rule_for_type(\n    model_name, field_name, field_type, is_optional, processed_models, created_rules, field_info=None\n) -> Tuple[str, list]:\n    \"\"\"\n    Generate GBNF rule for a given field type.\n\n    :param model_name: Name of the model.\n\n    :param field_name: Name of the field.\n    :param field_type: Type of the field.\n    :param is_optional: Whether the field is optional.\n    :param processed_models: List of processed models.\n    :param created_rules: List of created rules.\n    :param field_info: Additional information about the field (optional).\n\n    :return: Tuple containing the GBNF type and a list of additional rules.\n    :rtype: Tuple[str, list]\n    \"\"\"\n    rules = []\n\n    field_name = format_model_and_field_name(field_name)\n    gbnf_type = map_pydantic_type_to_gbnf(field_type)\n\n    if isclass(field_type) and issubclass(field_type, BaseModel):\n        nested_model_name = format_model_and_field_name(field_type.__name__)\n        nested_model_rules, _ = generate_gbnf_grammar(field_type, processed_models, created_rules)\n        rules.extend(nested_model_rules)\n        gbnf_type, rules = nested_model_name, rules\n    elif isclass(field_type) and issubclass(field_type, Enum):\n        enum_values = [f'\"\\\\\"{e.value}\\\\\"\"' for e in field_type]  # Adding escaped quotes\n        enum_rule = f\"{model_name}-{field_name} ::= {' | '.join(enum_values)}\"\n        rules.append(enum_rule)\n        gbnf_type, rules = model_name + \"-\" + field_name, rules\n    elif get_origin(field_type) == list:  # Array\n        element_type = get_args(field_type)[0]\n        element_rule_name, additional_rules = generate_gbnf_rule_for_type(\n            model_name, f\"{field_name}-element\", element_type, is_optional, processed_models, created_rules\n        )\n        rules.extend(additional_rules)\n        array_rule = f\"\"\"{model_name}-{field_name} ::= \"[\" ws {element_rule_name} (\",\" ws {element_rule_name})*  \"]\" \"\"\"\n        rules.append(array_rule)\n        gbnf_type, rules = model_name + \"-\" + field_name, rules\n\n    elif get_origin(field_type) == set or field_type == set:  # Array\n        element_type = get_args(field_type)[0]\n        element_rule_name, additional_rules = generate_gbnf_rule_for_type(\n            model_name, f\"{field_name}-element\", element_type, is_optional, processed_models, created_rules\n        )\n        rules.extend(additional_rules)\n        array_rule = f\"\"\"{model_name}-{field_name} ::= \"[\" ws {element_rule_name} (\",\" ws {element_rule_name})*  \"]\" \"\"\"\n        rules.append(array_rule)\n        gbnf_type, rules = model_name + \"-\" + field_name, rules\n\n    elif gbnf_type.startswith(\"custom-class-\"):\n        nested_model_rules, field_types = get_members_structure(field_type, gbnf_type)\n        rules.append(nested_model_rules)\n    elif gbnf_type.startswith(\"custom-dict-\"):\n        key_type, value_type = get_args(field_type)\n\n        additional_key_type, additional_key_rules = generate_gbnf_rule_for_type(\n            model_name, f\"{field_name}-key-type\", key_type, is_optional, processed_models, created_rules\n        )\n        additional_value_type, additional_value_rules = generate_gbnf_rule_for_type(\n            model_name, f\"{field_name}-value-type\", value_type, is_optional, processed_models, created_rules\n        )\n        gbnf_type = rf'{gbnf_type} ::= \"{{\"  ( {additional_key_type} \": \"  {additional_value_type} (\",\" \"\\n\" ws {additional_key_type} \":\"  {additional_value_type})*  )? \"}}\" '\n\n        rules.extend(additional_key_rules)\n        rules.extend(additional_value_rules)\n    elif gbnf_type.startswith(\"union-\"):\n        union_types = get_args(field_type)\n        union_rules = []\n\n        for union_type in union_types:\n            if isinstance(union_type, _GenericAlias):\n                union_gbnf_type, union_rules_list = generate_gbnf_rule_for_type(\n                    model_name, field_name, union_type, False, processed_models, created_rules\n                )\n                union_rules.append(union_gbnf_type)\n                rules.extend(union_rules_list)\n\n            elif not issubclass(union_type, NoneType):\n                union_gbnf_type, union_rules_list = generate_gbnf_rule_for_type(\n                    model_name, field_name, union_type, False, processed_models, created_rules\n                )\n                union_rules.append(union_gbnf_type)\n                rules.extend(union_rules_list)\n\n        # Defining the union grammar rule separately\n        if len(union_rules) == 1:\n            union_grammar_rule = f\"{model_name}-{field_name}-optional ::= {' | '.join(union_rules)} | null\"\n        else:\n            union_grammar_rule = f\"{model_name}-{field_name}-union ::= {' | '.join(union_rules)}\"\n        rules.append(union_grammar_rule)\n        if len(union_rules) == 1:\n            gbnf_type = f\"{model_name}-{field_name}-optional\"\n        else:\n            gbnf_type = f\"{model_name}-{field_name}-union\"\n    elif isclass(field_type) and issubclass(field_type, str):\n        if field_info and hasattr(field_info, \"json_schema_extra\") and field_info.json_schema_extra is not None:\n            triple_quoted_string = field_info.json_schema_extra.get(\"triple_quoted_string\", False)\n            markdown_string = field_info.json_schema_extra.get(\"markdown_code_block\", False)\n\n            gbnf_type = PydanticDataType.TRIPLE_QUOTED_STRING.value if triple_quoted_string else PydanticDataType.STRING.value\n            gbnf_type = PydanticDataType.MARKDOWN_CODE_BLOCK.value if markdown_string else gbnf_type\n\n        elif field_info and hasattr(field_info, \"pattern\"):\n            # Convert regex pattern to grammar rule\n            regex_pattern = field_info.regex.pattern\n            gbnf_type = f\"pattern-{field_name} ::= {regex_to_gbnf(regex_pattern)}\"\n        else:\n            gbnf_type = PydanticDataType.STRING.value\n\n    elif (\n        isclass(field_type)\n        and issubclass(field_type, float)\n        and field_info\n        and hasattr(field_info, \"json_schema_extra\")\n        and field_info.json_schema_extra is not None\n    ):\n        # Retrieve precision attributes for floats\n        max_precision = (\n            field_info.json_schema_extra.get(\"max_precision\") if field_info and hasattr(field_info, \"json_schema_extra\") else None\n        )\n        min_precision = (\n            field_info.json_schema_extra.get(\"min_precision\") if field_info and hasattr(field_info, \"json_schema_extra\") else None\n        )\n        max_digits = field_info.json_schema_extra.get(\"max_digit\") if field_info and hasattr(field_info, \"json_schema_extra\") else None\n        min_digits = field_info.json_schema_extra.get(\"min_digit\") if field_info and hasattr(field_info, \"json_schema_extra\") else None\n\n        # Generate GBNF rule for float with given attributes\n        gbnf_type, rules = generate_gbnf_float_rules(\n            max_digit=max_digits, min_digit=min_digits, max_precision=max_precision, min_precision=min_precision\n        )\n\n    elif (\n        isclass(field_type)\n        and issubclass(field_type, int)\n        and field_info\n        and hasattr(field_info, \"json_schema_extra\")\n        and field_info.json_schema_extra is not None\n    ):\n        # Retrieve digit attributes for integers\n        max_digits = field_info.json_schema_extra.get(\"max_digit\") if field_info and hasattr(field_info, \"json_schema_extra\") else None\n        min_digits = field_info.json_schema_extra.get(\"min_digit\") if field_info and hasattr(field_info, \"json_schema_extra\") else None\n\n        # Generate GBNF rule for integer with given attributes\n        gbnf_type, rules = generate_gbnf_integer_rules(max_digit=max_digits, min_digit=min_digits)\n    else:\n        gbnf_type, rules = gbnf_type, []\n\n    if gbnf_type not in created_rules:\n        return gbnf_type, rules\n    else:\n        if gbnf_type in created_rules:\n            return gbnf_type, rules\n\n\ndef generate_gbnf_grammar(model: Type[BaseModel], processed_models: set, created_rules: dict) -> (list, bool, bool):\n    \"\"\"\n\n    Generate GBnF Grammar\n\n    Generates a GBnF grammar for a given model.\n\n    :param model: A Pydantic model class to generate the grammar for. Must be a subclass of BaseModel.\n    :param processed_models: A set of already processed models to prevent infinite recursion.\n    :param created_rules: A dict containing already created rules to prevent duplicates.\n    :return: A list of GBnF grammar rules in string format. And two booleans indicating if an extra markdown or triple quoted string is in the grammar.\n    Example Usage:\n    ```\n    model = MyModel\n    processed_models = set()\n    created_rules = dict()\n\n    gbnf_grammar = generate_gbnf_grammar(model, processed_models, created_rules)\n    ```\n    \"\"\"\n    if model in processed_models:\n        return []\n\n    processed_models.add(model)\n    model_name = format_model_and_field_name(model.__name__)\n\n    if not issubclass(model, BaseModel):\n        # For non-Pydantic classes, generate model_fields from __annotations__ or __init__\n        if hasattr(model, \"__annotations__\") and model.__annotations__:\n            model_fields = {name: (typ, ...) for name, typ in model.__annotations__.items()}\n        else:\n            init_signature = inspect.signature(model.__init__)\n            parameters = init_signature.parameters\n            model_fields = {name: (param.annotation, param.default) for name, param in parameters.items() if name != \"self\"}\n    else:\n        # For Pydantic models, use model_fields and check for ellipsis (required fields)\n        model_fields = model.__annotations__\n\n    model_rule_parts = []\n    nested_rules = []\n    has_markdown_code_block = False\n    has_triple_quoted_string = False\n\n    for field_name, field_info in model_fields.items():\n        if not issubclass(model, BaseModel):\n            field_type, default_value = field_info\n            # Check if the field is optional (not required)\n            is_optional = (default_value is not inspect.Parameter.empty) and (default_value is not Ellipsis)\n        else:\n            field_type = field_info\n            field_info = model.model_fields[field_name]\n            is_optional = field_info.is_required is False and get_origin(field_type) is Optional\n        rule_name, additional_rules = generate_gbnf_rule_for_type(\n            model_name, format_model_and_field_name(field_name), field_type, is_optional, processed_models, created_rules, field_info\n        )\n        look_for_markdown_code_block = True if rule_name == \"markdown_code_block\" else False\n        look_for_triple_quoted_string = True if rule_name == \"triple_quoted_string\" else False\n        if not look_for_markdown_code_block and not look_for_triple_quoted_string:\n            if rule_name not in created_rules:\n                created_rules[rule_name] = additional_rules\n            model_rule_parts.append(f' ws \"\\\\\"{field_name}\\\\\"\" \":\" ws {rule_name}')  # Adding escaped quotes\n            nested_rules.extend(additional_rules)\n        else:\n            has_triple_quoted_string = look_for_triple_quoted_string\n            has_markdown_code_block = look_for_markdown_code_block\n\n    fields_joined = r' \",\" \"\\n\" '.join(model_rule_parts)\n    model_rule = rf'{model_name} ::= \"{{\" \"\\n\" {fields_joined} \"\\n\" ws \"}}\"'\n\n    has_special_string = False\n    if has_triple_quoted_string:\n        model_rule += '\"\\\\n\" ws \"}\"'\n        model_rule += '\"\\\\n\" triple-quoted-string'\n        has_special_string = True\n    if has_markdown_code_block:\n        model_rule += '\"\\\\n\" ws \"}\"'\n        model_rule += '\"\\\\n\" markdown-code-block'\n        has_special_string = True\n    all_rules = [model_rule] + nested_rules\n\n    return all_rules, has_special_string\n\n\ndef generate_gbnf_grammar_from_pydantic_models(\n    models: List[Type[BaseModel]],\n    outer_object_name: str = None,\n    outer_object_content: str = None,\n    list_of_outputs: bool = False,\n    add_inner_thoughts: bool = False,\n    allow_only_inner_thoughts: bool = False,\n) -> str:\n    \"\"\"\n    Generate GBNF Grammar from Pydantic Models.\n\n    This method takes a list of Pydantic models and uses them to generate a GBNF grammar string. The generated grammar string can be used for parsing and validating data using the generated\n    * grammar.\n\n    Args:\n        models (List[Type[BaseModel]]): A list of Pydantic models to generate the grammar from.\n        outer_object_name (str): Outer object name for the GBNF grammar. If None, no outer object will be generated. Eg. \"function\" for function calling.\n        outer_object_content (str): Content for the outer rule in the GBNF grammar. Eg. \"function_parameters\" or \"params\" for function calling.\n        list_of_outputs (str, optional): Allows a list of output objects\n        add_inner_thoughts (bool): Add inner thoughts field on the top level.\n        allow_only_inner_thoughts (bool): Allow inner thoughts without a function call.\n    Returns:\n        str: The generated GBNF grammar string.\n\n    Examples:\n        models = [UserModel, PostModel]\n        grammar = generate_gbnf_grammar_from_pydantic(models)\n        print(grammar)\n        # Output:\n        # root ::= UserModel | PostModel\n        # ...\n    \"\"\"\n    processed_models = set()\n    all_rules = []\n    created_rules = {}\n    if outer_object_name is None:\n        for model in models:\n            model_rules, _ = generate_gbnf_grammar(model, processed_models, created_rules)\n            all_rules.extend(model_rules)\n\n        if list_of_outputs:\n            root_rule = r'root ::= (\" \"| \"\\n\") \"[\" ws grammar-models (\",\" ws grammar-models)* ws \"]\"' + \"\\n\"\n        else:\n            root_rule = r'root ::= (\" \"| \"\\n\") grammar-models' + \"\\n\"\n        root_rule += \"grammar-models ::= \" + \" | \".join([format_model_and_field_name(model.__name__) for model in models])\n        all_rules.insert(0, root_rule)\n        return \"\\n\".join(all_rules)\n    elif outer_object_name is not None:\n        if list_of_outputs:\n            root_rule = (\n                rf'root ::= (\" \"| \"\\n\") \"[\" ws {format_model_and_field_name(outer_object_name)} (\",\" ws {format_model_and_field_name(outer_object_name)})* ws \"]\"'\n                + \"\\n\"\n            )\n        else:\n            root_rule = f\"root ::= {format_model_and_field_name(outer_object_name)}\\n\"\n\n        if add_inner_thoughts:\n            if allow_only_inner_thoughts:\n                model_rule = rf'{format_model_and_field_name(outer_object_name)} ::= (\" \"| \"\\n\") \"{{\" ws \"\\\"inner_thoughts\\\"\"  \":\" ws string (\",\" \"\\n\" ws \"\\\"{outer_object_name}\\\"\"  \":\" ws grammar-models)?'\n            else:\n                model_rule = rf'{format_model_and_field_name(outer_object_name)} ::= (\" \"| \"\\n\") \"{{\" ws \"\\\"inner_thoughts\\\"\"  \":\" ws string \",\" \"\\n\" ws \"\\\"{outer_object_name}\\\"\"  \":\" ws grammar-models'\n        else:\n            model_rule = rf'{format_model_and_field_name(outer_object_name)} ::= (\" \"| \"\\n\") \"{{\" ws \"\\\"{outer_object_name}\\\"\"  \":\" ws grammar-models'\n\n        fields_joined = \" | \".join([rf\"{format_model_and_field_name(model.__name__)}-grammar-model\" for model in models])\n\n        grammar_model_rules = f\"\\ngrammar-models ::= {fields_joined}\"\n        mod_rules = []\n        for model in models:\n            mod_rule = rf\"{format_model_and_field_name(model.__name__)}-grammar-model ::= \"\n            mod_rule += (\n                rf'\"\\\"{model.__name__}\\\"\" \",\" ws \"\\\"{outer_object_content}\\\"\" \":\" ws {format_model_and_field_name(model.__name__)}' + \"\\n\"\n            )\n            mod_rules.append(mod_rule)\n        grammar_model_rules += \"\\n\" + \"\\n\".join(mod_rules)\n\n        for model in models:\n            model_rules, has_special_string = generate_gbnf_grammar(model, processed_models, created_rules)\n\n            if not has_special_string:\n                model_rules[0] += r'\"\\n\" ws \"}\"'\n\n            all_rules.extend(model_rules)\n\n        all_rules.insert(0, root_rule + model_rule + grammar_model_rules)\n        return \"\\n\".join(all_rules)\n\n\ndef get_primitive_grammar(grammar):\n    \"\"\"\n    Returns the needed GBNF primitive grammar for a given GBNF grammar string.\n\n    Args:\n        grammar (str): The string containing the GBNF grammar.\n\n    Returns:\n        str: GBNF primitive grammar string.\n    \"\"\"\n    type_list = []\n    if \"string-list\" in grammar:\n        type_list.append(str)\n    if \"boolean-list\" in grammar:\n        type_list.append(bool)\n    if \"integer-list\" in grammar:\n        type_list.append(int)\n    if \"float-list\" in grammar:\n        type_list.append(float)\n    additional_grammar = [generate_list_rule(t) for t in type_list]\n    primitive_grammar = r\"\"\"\nboolean ::= \"true\" | \"false\"\nnull ::= \"null\"\nstring ::= \"\\\"\" (\n        [^\"\\\\] |\n        \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])\n      )* \"\\\"\"\nws ::= ([ \\t\\n] ws)?\nfloat ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws\n\ninteger ::= [0-9]+\"\"\"\n\n    any_block = \"\"\n    if \"custom-class-any\" in grammar:\n        any_block = \"\"\"\nvalue ::= object | array | string | number | boolean | null\n\nobject ::=\n  \"{\" ws (\n            string \":\" ws value\n    (\",\" ws string \":\" ws value)*\n  )? \"}\"\n\narray  ::=\n  \"[\" ws (\n            value\n    (\",\" ws value)*\n  )? \"]\"\n\nnumber ::= integer | float\"\"\"\n\n    markdown_code_block_grammar = \"\"\n    if \"markdown-code-block\" in grammar:\n        markdown_code_block_grammar = r'''\nmarkdown-code-block ::= opening-triple-ticks markdown-code-block-content closing-triple-ticks\nmarkdown-code-block-content ::= ( [^`] | \"`\" [^`] |  \"`\"  \"`\" [^`]  )*\nopening-triple-ticks ::= \"```\" \"python\" \"\\n\" | \"```\" \"c\" \"\\n\" | \"```\" \"cpp\" \"\\n\" | \"```\" \"txt\" \"\\n\" | \"```\" \"text\" \"\\n\" | \"```\" \"json\" \"\\n\" | \"```\" \"javascript\" \"\\n\" | \"```\" \"css\" \"\\n\" | \"```\" \"html\" \"\\n\" | \"```\" \"markdown\" \"\\n\"\nclosing-triple-ticks ::= \"```\" \"\\n\"'''\n\n    if \"triple-quoted-string\" in grammar:\n        markdown_code_block_grammar = r\"\"\"\ntriple-quoted-string ::= triple-quotes triple-quoted-string-content triple-quotes\ntriple-quoted-string-content ::= ( [^'] | \"'\" [^'] |  \"'\"  \"'\" [^']  )*\ntriple-quotes ::= \"'''\" \"\"\"\n    return \"\\n\" + \"\\n\".join(additional_grammar) + any_block + primitive_grammar + markdown_code_block_grammar\n\n\ndef generate_markdown_documentation(\n    pydantic_models: List[Type[BaseModel]], model_prefix=\"Model\", fields_prefix=\"Fields\", documentation_with_field_description=True\n) -> str:\n    \"\"\"\n    Generate markdown documentation for a list of Pydantic models.\n\n    Args:\n        pydantic_models (List[Type[BaseModel]]): List of Pydantic model classes.\n        model_prefix (str): Prefix for the model section.\n        fields_prefix (str): Prefix for the fields section.\n        documentation_with_field_description (bool): Include field descriptions in the documentation.\n\n    Returns:\n        str: Generated text documentation.\n    \"\"\"\n    documentation = \"\"\n    pyd_models = [(model, True) for model in pydantic_models]\n    for model, add_prefix in pyd_models:\n        if add_prefix:\n            documentation += f\"{model_prefix}: {model.__name__}\\n\"\n        else:\n            documentation += f\"class: {model.__name__}\\n\"\n\n        # Handling multi-line model description with proper indentation\n\n        class_doc = getdoc(model)\n        base_class_doc = getdoc(BaseModel)\n        class_description = class_doc if class_doc and class_doc != base_class_doc else \"\"\n        if class_description != \"\":\n            documentation += format_multiline_description(\"description: \" + class_description, 1) + \"\\n\"\n\n        if add_prefix:\n            # Indenting the fields section\n            documentation += f\"  {fields_prefix}:\\n\"\n        else:\n            documentation += f\"  attributes:\\n\"\n        if isclass(model) and issubclass(model, BaseModel):\n            for name, field_type in model.__annotations__.items():\n                # if name == \"markdown_code_block\":\n                #    continue\n                if isclass(field_type) and issubclass(field_type, BaseModel):\n                    pyd_models.append((field_type, False))\n                if get_origin(field_type) == list:\n                    element_type = get_args(field_type)[0]\n                    if isclass(element_type) and issubclass(element_type, BaseModel):\n                        pyd_models.append((element_type, False))\n                if get_origin(field_type) == Union:\n                    element_types = get_args(field_type)\n                    for element_type in element_types:\n                        if isclass(element_type) and issubclass(element_type, BaseModel):\n                            pyd_models.append((element_type, False))\n                documentation += generate_field_markdown(\n                    name, field_type, model, documentation_with_field_description=documentation_with_field_description\n                )\n            documentation += \"\\n\"\n\n        if hasattr(model, \"Config\") and hasattr(model.Config, \"json_schema_extra\") and \"example\" in model.Config.json_schema_extra:\n            documentation += f\"  Expected Example Output for {format_model_and_field_name(model.__name__)}:\\n\"\n            # json_example = json.dumps(model.Config.json_schema_extra[\"example\"])\n            json_example = json.dumps(model.Config.json_schema_extra[\"example\"], ensure_ascii=JSON_ENSURE_ASCII)\n            documentation += format_multiline_description(json_example, 2) + \"\\n\"\n\n    return documentation\n\n\ndef generate_field_markdown(\n    field_name: str, field_type: Type[Any], model: Type[BaseModel], depth=1, documentation_with_field_description=True\n) -> str:\n    \"\"\"\n    Generate markdown documentation for a Pydantic model field.\n\n    Args:\n        field_name (str): Name of the field.\n        field_type (Type[Any]): Type of the field.\n        model (Type[BaseModel]): Pydantic model class.\n        depth (int): Indentation depth in the documentation.\n        documentation_with_field_description (bool): Include field descriptions in the documentation.\n\n    Returns:\n        str: Generated text documentation for the field.\n    \"\"\"\n    indent = \"    \" * depth\n\n    field_info = model.model_fields.get(field_name)\n    field_description = field_info.description if field_info and field_info.description else \"\"\n\n    if get_origin(field_type) == list:\n        element_type = get_args(field_type)[0]\n        field_text = f\"{indent}{field_name} ({field_type.__name__} of {element_type.__name__})\"\n        if field_description != \"\":\n            field_text += \": \"\n        else:\n            field_text += \"\\n\"\n    elif get_origin(field_type) == Union:\n        element_types = get_args(field_type)\n        types = []\n        for element_type in element_types:\n            types.append(element_type.__name__)\n        field_text = f\"{indent}{field_name} ({' or '.join(types)})\"\n        if field_description != \"\":\n            field_text += \": \"\n        else:\n            field_text += \"\\n\"\n    elif issubclass(field_type, Enum):\n        enum_values = [f\"'{str(member.value)}'\" for member in field_type]\n\n        field_text = f\"{indent}{field_name} ({' or '.join(enum_values)})\"\n        if field_description != \"\":\n            field_text += \": \"\n        else:\n            field_text += \"\\n\"\n    else:\n        field_text = f\"{indent}{field_name} ({field_type.__name__})\"\n        if field_description != \"\":\n            field_text += \": \"\n        else:\n            field_text += \"\\n\"\n\n    if not documentation_with_field_description:\n        return field_text\n\n    if field_description != \"\":\n        field_text += field_description + \"\\n\"\n\n    # Check for and include field-specific examples if available\n    if hasattr(model, \"Config\") and hasattr(model.Config, \"json_schema_extra\") and \"example\" in model.Config.json_schema_extra:\n        field_example = model.Config.json_schema_extra[\"example\"].get(field_name)\n        if field_example is not None:\n            example_text = f\"'{field_example}'\" if isinstance(field_example, str) else field_example\n            field_text += f\"{indent}  Example: {example_text}\\n\"\n\n    if isclass(field_type) and issubclass(field_type, BaseModel):\n        field_text += f\"{indent}  details:\\n\"\n        for name, type_ in field_type.__annotations__.items():\n            field_text += generate_field_markdown(name, type_, field_type, depth + 2)\n\n    return field_text\n\n\ndef format_json_example(example: dict, depth: int) -> str:\n    \"\"\"\n    Format a JSON example into a readable string with indentation.\n\n    Args:\n        example (dict): JSON example to be formatted.\n        depth (int): Indentation depth.\n\n    Returns:\n        str: Formatted JSON example string.\n    \"\"\"\n    indent = \"    \" * depth\n    formatted_example = \"{\\n\"\n    for key, value in example.items():\n        value_text = f\"'{value}'\" if isinstance(value, str) else value\n        formatted_example += f\"{indent}{key}: {value_text},\\n\"\n    formatted_example = formatted_example.rstrip(\",\\n\") + \"\\n\" + indent + \"}\"\n    return formatted_example\n\n\ndef generate_text_documentation(\n    pydantic_models: List[Type[BaseModel]], model_prefix=\"Model\", fields_prefix=\"Fields\", documentation_with_field_description=True\n) -> str:\n    \"\"\"\n    Generate text documentation for a list of Pydantic models.\n\n    Args:\n        pydantic_models (List[Type[BaseModel]]): List of Pydantic model classes.\n        model_prefix (str): Prefix for the model section.\n        fields_prefix (str): Prefix for the fields section.\n        documentation_with_field_description (bool): Include field descriptions in the documentation.\n\n    Returns:\n        str: Generated text documentation.\n    \"\"\"\n    documentation = \"\"\n    pyd_models = [(model, True) for model in pydantic_models]\n    for model, add_prefix in pyd_models:\n        if add_prefix:\n            documentation += f\"{model_prefix}: {model.__name__}\\n\"\n        else:\n            documentation += f\"Model: {model.__name__}\\n\"\n\n        # Handling multi-line model description with proper indentation\n\n        class_doc = getdoc(model)\n        base_class_doc = getdoc(BaseModel)\n        class_description = class_doc if class_doc and class_doc != base_class_doc else \"\"\n        if class_description != \"\":\n            documentation += \"  Description: \"\n            documentation += \"\\n\" + format_multiline_description(class_description, 2) + \"\\n\"\n\n        if isclass(model) and issubclass(model, BaseModel):\n            documentation_fields = \"\"\n            for name, field_type in model.__annotations__.items():\n                # if name == \"markdown_code_block\":\n                #    continue\n                if get_origin(field_type) == list:\n                    element_type = get_args(field_type)[0]\n                    if isclass(element_type) and issubclass(element_type, BaseModel):\n                        pyd_models.append((element_type, False))\n                if get_origin(field_type) == Union:\n                    element_types = get_args(field_type)\n                    for element_type in element_types:\n                        if isclass(element_type) and issubclass(element_type, BaseModel):\n                            pyd_models.append((element_type, False))\n                documentation_fields += generate_field_text(\n                    name, field_type, model, documentation_with_field_description=documentation_with_field_description\n                )\n            if documentation_fields != \"\":\n                if add_prefix:\n                    documentation += f\"  {fields_prefix}:\\n{documentation_fields}\"\n                else:\n                    documentation += f\"  Fields:\\n{documentation_fields}\"\n            documentation += \"\\n\"\n\n        if hasattr(model, \"Config\") and hasattr(model.Config, \"json_schema_extra\") and \"example\" in model.Config.json_schema_extra:\n            documentation += f\"  Expected Example Output for {format_model_and_field_name(model.__name__)}:\\n\"\n            json_example = json.dumps(model.Config.json_schema_extra[\"example\"])\n            documentation += format_multiline_description(json_example, 2) + \"\\n\"\n\n    return documentation\n\n\ndef generate_field_text(\n    field_name: str, field_type: Type[Any], model: Type[BaseModel], depth=1, documentation_with_field_description=True\n) -> str:\n    \"\"\"\n    Generate text documentation for a Pydantic model field.\n\n    Args:\n        field_name (str): Name of the field.\n        field_type (Type[Any]): Type of the field.\n        model (Type[BaseModel]): Pydantic model class.\n        depth (int): Indentation depth in the documentation.\n        documentation_with_field_description (bool): Include field descriptions in the documentation.\n\n    Returns:\n        str: Generated text documentation for the field.\n    \"\"\"\n    indent = \"    \" * depth\n\n    field_info = model.model_fields.get(field_name)\n    field_description = field_info.description if field_info and field_info.description else \"\"\n\n    if get_origin(field_type) == list:\n        element_type = get_args(field_type)[0]\n        field_text = f\"{indent}{field_name} ({format_model_and_field_name(field_type.__name__)} of {format_model_and_field_name(element_type.__name__)})\"\n        if field_description != \"\":\n            field_text += \":\\n\"\n        else:\n            field_text += \"\\n\"\n    elif get_origin(field_type) == Union:\n        element_types = get_args(field_type)\n        types = []\n        for element_type in element_types:\n            types.append(format_model_and_field_name(element_type.__name__))\n        field_text = f\"{indent}{field_name} ({' or '.join(types)})\"\n        if field_description != \"\":\n            field_text += \":\\n\"\n        else:\n            field_text += \"\\n\"\n    else:\n        field_text = f\"{indent}{field_name} ({format_model_and_field_name(field_type.__name__)})\"\n        if field_description != \"\":\n            field_text += \":\\n\"\n        else:\n            field_text += \"\\n\"\n\n    if not documentation_with_field_description:\n        return field_text\n\n    if field_description != \"\":\n        field_text += f\"{indent}  Description: \" + field_description + \"\\n\"\n\n    # Check for and include field-specific examples if available\n    if hasattr(model, \"Config\") and hasattr(model.Config, \"json_schema_extra\") and \"example\" in model.Config.json_schema_extra:\n        field_example = model.Config.json_schema_extra[\"example\"].get(field_name)\n        if field_example is not None:\n            example_text = f\"'{field_example}'\" if isinstance(field_example, str) else field_example\n            field_text += f\"{indent}  Example: {example_text}\\n\"\n\n    if isclass(field_type) and issubclass(field_type, BaseModel):\n        field_text += f\"{indent}  Details:\\n\"\n        for name, type_ in field_type.__annotations__.items():\n            field_text += generate_field_text(name, type_, field_type, depth + 2)\n\n    return field_text\n\n\ndef format_multiline_description(description: str, indent_level: int) -> str:\n    \"\"\"\n    Format a multiline description with proper indentation.\n\n    Args:\n        description (str): Multiline description.\n        indent_level (int): Indentation level.\n\n    Returns:\n        str: Formatted multiline description.\n    \"\"\"\n    indent = \"  \" * indent_level\n    return indent + description.replace(\"\\n\", \"\\n\" + indent)\n\n\ndef save_gbnf_grammar_and_documentation(\n    grammar, documentation, grammar_file_path=\"./grammar.gbnf\", documentation_file_path=\"./grammar_documentation.md\"\n):\n    \"\"\"\n    Save GBNF grammar and documentation to specified files.\n\n    Args:\n        grammar (str): GBNF grammar string.\n        documentation (str): Documentation string.\n        grammar_file_path (str): File path to save the GBNF grammar.\n        documentation_file_path (str): File path to save the documentation.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        with open(grammar_file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(grammar + get_primitive_grammar(grammar))\n        print(f\"Grammar successfully saved to {grammar_file_path}\")\n    except IOError as e:\n        print(f\"An error occurred while saving the grammar file: {e}\")\n\n    try:\n        with open(documentation_file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(documentation)\n        print(f\"Documentation successfully saved to {documentation_file_path}\")\n    except IOError as e:\n        print(f\"An error occurred while saving the documentation file: {e}\")\n\n\ndef remove_empty_lines(string):\n    \"\"\"\n    Remove empty lines from a string.\n\n    Args:\n        string (str): Input string.\n\n    Returns:\n        str: String with empty lines removed.\n    \"\"\"\n    lines = string.splitlines()\n    non_empty_lines = [line for line in lines if line.strip() != \"\"]\n    string_no_empty_lines = \"\\n\".join(non_empty_lines)\n    return string_no_empty_lines\n\n\ndef generate_and_save_gbnf_grammar_and_documentation(\n    pydantic_model_list,\n    grammar_file_path=\"./generated_grammar.gbnf\",\n    documentation_file_path=\"./generated_grammar_documentation.md\",\n    outer_object_name: str = None,\n    outer_object_content: str = None,\n    model_prefix: str = \"Output Model\",\n    fields_prefix: str = \"Output Fields\",\n    list_of_outputs: bool = False,\n    documentation_with_field_description=True,\n):\n    \"\"\"\n    Generate GBNF grammar and documentation, and save them to specified files.\n\n    Args:\n        pydantic_model_list: List of Pydantic model classes.\n        grammar_file_path (str): File path to save the generated GBNF grammar.\n        documentation_file_path (str): File path to save the generated documentation.\n        outer_object_name (str): Outer object name for the GBNF grammar. If None, no outer object will be generated. Eg. \"function\" for function calling.\n        outer_object_content (str): Content for the outer rule in the GBNF grammar. Eg. \"function_parameters\" or \"params\" for function calling.\n        model_prefix (str): Prefix for the model section in the documentation.\n        fields_prefix (str): Prefix for the fields section in the documentation.\n        list_of_outputs (bool): Whether the output is a list of items.\n        documentation_with_field_description (bool): Include field descriptions in the documentation.\n\n    Returns:\n        None\n    \"\"\"\n    documentation = generate_markdown_documentation(\n        pydantic_model_list, model_prefix, fields_prefix, documentation_with_field_description=documentation_with_field_description\n    )\n    grammar = generate_gbnf_grammar_from_pydantic_models(pydantic_model_list, outer_object_name, outer_object_content, list_of_outputs)\n    grammar = remove_empty_lines(grammar)\n    save_gbnf_grammar_and_documentation(grammar, documentation, grammar_file_path, documentation_file_path)\n\n\ndef generate_gbnf_grammar_and_documentation(\n    pydantic_model_list,\n    outer_object_name: str = None,\n    outer_object_content: str = None,\n    model_prefix: str = \"Output Model\",\n    fields_prefix: str = \"Output Fields\",\n    list_of_outputs: bool = False,\n    add_inner_thoughts: bool = False,\n    allow_only_inner_thoughts: bool = False,\n    documentation_with_field_description=True,\n):\n    \"\"\"\n    Generate GBNF grammar and documentation for a list of Pydantic models.\n\n    Args:\n        pydantic_model_list: List of Pydantic model classes.\n        outer_object_name (str): Outer object name for the GBNF grammar. If None, no outer object will be generated. Eg. \"function\" for function calling.\n        outer_object_content (str): Content for the outer rule in the GBNF grammar. Eg. \"function_parameters\" or \"params\" for function calling.\n        model_prefix (str): Prefix for the model section in the documentation.\n        fields_prefix (str): Prefix for the fields section in the documentation.\n        list_of_outputs (bool): Whether the output is a list of items.\n        add_inner_thoughts (bool): Add inner thoughts field on the top level.\n        allow_only_inner_thoughts (bool): Allow inner thoughts without a function call.\n        documentation_with_field_description (bool): Include field descriptions in the documentation.\n\n    Returns:\n        tuple: GBNF grammar string, documentation string.\n    \"\"\"\n    documentation = generate_markdown_documentation(\n        copy(pydantic_model_list), model_prefix, fields_prefix, documentation_with_field_description=documentation_with_field_description\n    )\n    grammar = generate_gbnf_grammar_from_pydantic_models(\n        pydantic_model_list, outer_object_name, outer_object_content, list_of_outputs, add_inner_thoughts, allow_only_inner_thoughts\n    )\n    grammar = remove_empty_lines(grammar + get_primitive_grammar(grammar))\n    return grammar, documentation\n\n\ndef generate_gbnf_grammar_and_documentation_from_dictionaries(\n    dictionaries: List[dict],\n    outer_object_name: str = None,\n    outer_object_content: str = None,\n    model_prefix: str = \"Output Model\",\n    fields_prefix: str = \"Output Fields\",\n    list_of_outputs: bool = False,\n    documentation_with_field_description=True,\n):\n    \"\"\"\n    Generate GBNF grammar and documentation from a list of dictionaries.\n\n    Args:\n        dictionaries (List[dict]): List of dictionaries representing Pydantic models.\n        outer_object_name (str): Outer object name for the GBNF grammar. If None, no outer object will be generated. Eg. \"function\" for function calling.\n        outer_object_content (str): Content for the outer rule in the GBNF grammar. Eg. \"function_parameters\" or \"params\" for function calling.\n        model_prefix (str): Prefix for the model section in the documentation.\n        fields_prefix (str): Prefix for the fields section in the documentation.\n        list_of_outputs (bool): Whether the output is a list of items.\n        documentation_with_field_description (bool): Include field descriptions in the documentation.\n\n    Returns:\n        tuple: GBNF grammar string, documentation string.\n    \"\"\"\n    pydantic_model_list = create_dynamic_models_from_dictionaries(dictionaries)\n    documentation = generate_markdown_documentation(\n        copy(pydantic_model_list), model_prefix, fields_prefix, documentation_with_field_description=documentation_with_field_description\n    )\n    grammar = generate_gbnf_grammar_from_pydantic_models(pydantic_model_list, outer_object_name, outer_object_content, list_of_outputs)\n    grammar = remove_empty_lines(grammar + get_primitive_grammar(grammar))\n    return grammar, documentation\n\n\ndef create_dynamic_model_from_function(func: Callable, add_inner_thoughts: bool = False):\n    \"\"\"\n    Creates a dynamic Pydantic model from a given function's type hints and adds the function as a 'run' method.\n\n    Args:\n        func (Callable): A function with type hints from which to create the model.\n        add_inner_thoughts: Add an inner thoughts parameter on the params level\n\n    Returns:\n        A dynamic Pydantic model class with the provided function as a 'run' method.\n    \"\"\"\n\n    # Get the signature of the function\n    sig = inspect.signature(func)\n\n    # Parse the docstring\n    docstring = parse(func.__doc__)\n\n    dynamic_fields = {}\n    param_docs = []\n    if add_inner_thoughts:\n        dynamic_fields[\"inner_thoughts\"] = (str, None)\n    for param in sig.parameters.values():\n        # Exclude 'self' parameter\n        if param.name == \"self\":\n            continue\n\n        # Assert that the parameter has a type annotation\n        if param.annotation == inspect.Parameter.empty:\n            raise TypeError(f\"Parameter '{param.name}' in function '{func.__name__}' lacks a type annotation\")\n\n        # Find the parameter's description in the docstring\n        param_doc = next((d for d in docstring.params if d.arg_name == param.name), None)\n\n        # Assert that the parameter has a description\n        if not param_doc or not param_doc.description:\n            raise ValueError(f\"Parameter '{param.name}' in function '{func.__name__}' lacks a description in the docstring\")\n\n        # Add parameter details to the schema\n        param_doc = next((d for d in docstring.params if d.arg_name == param.name), None)\n        param_docs.append((param.name, param_doc))\n        if param.default == inspect.Parameter.empty:\n            default_value = ...\n        else:\n            default_value = param.default\n\n        dynamic_fields[param.name] = (param.annotation if param.annotation != inspect.Parameter.empty else str, default_value)\n    # Creating the dynamic model\n    dynamic_model = create_model(f\"{func.__name__}\", **dynamic_fields)\n    if add_inner_thoughts:\n        dynamic_model.model_fields[\"inner_thoughts\"].description = \"Deep inner monologue private to you only.\"\n    for param_doc in param_docs:\n        dynamic_model.model_fields[param_doc[0]].description = param_doc[1].description\n\n    dynamic_model.__doc__ = docstring.short_description\n\n    def run_method_wrapper(self):\n        func_args = {name: getattr(self, name) for name, _ in dynamic_fields.items()}\n        return func(**func_args)\n\n    # Adding the wrapped function as a 'run' method\n    setattr(dynamic_model, \"run\", run_method_wrapper)\n    return dynamic_model\n\n\ndef add_run_method_to_dynamic_model(model: Type[BaseModel], func: Callable):\n    \"\"\"\n    Add a 'run' method to a dynamic Pydantic model, using the provided function.\n\n    Args:\n        model (Type[BaseModel]): Dynamic Pydantic model class.\n        func (Callable): Function to be added as a 'run' method to the model.\n\n    Returns:\n        Type[BaseModel]: Pydantic model class with the added 'run' method.\n    \"\"\"\n\n    def run_method_wrapper(self):\n        func_args = {name: getattr(self, name) for name in model.model_fields}\n        return func(**func_args)\n\n    # Adding the wrapped function as a 'run' method\n    setattr(model, \"run\", run_method_wrapper)\n\n    return model\n\n\ndef create_dynamic_models_from_dictionaries(dictionaries: List[dict]):\n    \"\"\"\n    Create a list of dynamic Pydantic model classes from a list of dictionaries.\n\n    Args:\n        dictionaries (List[dict]): List of dictionaries representing model structures.\n\n    Returns:\n        List[Type[BaseModel]]: List of generated dynamic Pydantic model classes.\n    \"\"\"\n    dynamic_models = []\n    for func in dictionaries:\n        model_name = format_model_and_field_name(func.get(\"name\", \"\"))\n        dyn_model = convert_dictionary_to_pydantic_model(func, model_name)\n        dynamic_models.append(dyn_model)\n    return dynamic_models\n\n\ndef map_grammar_names_to_pydantic_model_class(pydantic_model_list):\n    output = {}\n    for model in pydantic_model_list:\n        output[format_model_and_field_name(model.__name__)] = model\n\n    return output\n\n\nfrom enum import Enum\n\n\ndef json_schema_to_python_types(schema):\n    type_map = {\n        \"any\": Any,\n        \"string\": str,\n        \"number\": float,\n        \"integer\": int,\n        \"boolean\": bool,\n        \"array\": list,\n    }\n    return type_map[schema]\n\n\ndef list_to_enum(enum_name, values):\n    return Enum(enum_name, {value: value for value in values})\n\n\ndef convert_dictionary_to_pydantic_model(dictionary: dict, model_name: str = \"CustomModel\") -> Type[BaseModel]:\n    \"\"\"\n    Convert a dictionary to a Pydantic model class.\n\n    Args:\n        dictionary (dict): Dictionary representing the model structure.\n        model_name (str): Name of the generated Pydantic model.\n\n    Returns:\n        Type[BaseModel]: Generated Pydantic model class.\n    \"\"\"\n    fields = {}\n\n    if \"properties\" in dictionary:\n        for field_name, field_data in dictionary.get(\"properties\", {}).items():\n            if field_data == \"object\":\n                submodel = convert_dictionary_to_pydantic_model(dictionary, f\"{model_name}_{field_name}\")\n                fields[field_name] = (submodel, ...)\n            else:\n                field_type = field_data.get(\"type\", \"str\")\n\n                if field_data.get(\"enum\", []):\n                    fields[field_name] = (list_to_enum(field_name, field_data.get(\"enum\", [])), ...)\n                elif field_type == \"array\":\n                    items = field_data.get(\"items\", {})\n                    if items != {}:\n                        array = {\"properties\": items}\n                        array_type = convert_dictionary_to_pydantic_model(array, f\"{model_name}_{field_name}_items\")\n                        fields[field_name] = (List[array_type], ...)\n                    else:\n                        fields[field_name] = (list, ...)\n                elif field_type == \"object\":\n                    submodel = convert_dictionary_to_pydantic_model(field_data, f\"{model_name}_{field_name}\")\n                    fields[field_name] = (submodel, ...)\n                elif field_type == \"required\":\n                    required = field_data.get(\"enum\", [])\n                    for key, field in fields.items():\n                        if key not in required:\n                            fields[key] = (Optional[fields[key][0]], ...)\n                else:\n                    field_type = json_schema_to_python_types(field_type)\n                    fields[field_name] = (field_type, ...)\n    if \"function\" in dictionary:\n        for field_name, field_data in dictionary.get(\"function\", {}).items():\n            if field_name == \"name\":\n                model_name = field_data\n            elif field_name == \"description\":\n                fields[\"__doc__\"] = field_data\n            elif field_name == \"parameters\":\n                return convert_dictionary_to_pydantic_model(field_data, f\"{model_name}\")\n\n    if \"parameters\" in dictionary:\n        field_data = {\"function\": dictionary}\n        return convert_dictionary_to_pydantic_model(field_data, f\"{model_name}\")\n    if \"required\" in dictionary:\n        required = dictionary.get(\"required\", [])\n        for key, field in fields.items():\n            if key not in required:\n                fields[key] = (Optional[fields[key][0]], ...)\n    custom_model = create_model(model_name, **fields)\n    return custom_model\n"}
{"type": "source_file", "path": "luann/local_llm/chat_completion_proxy.py", "content": "\"\"\"Key idea: create drop-in replacement for agent's ChatCompletion call that runs on an OpenLLM backend\"\"\"\n\nimport json\nimport uuid\n\nimport requests\n\nfrom constants import CLI_WARNING_PREFIX, JSON_ENSURE_ASCII\nfrom errors import LocalLLMConnectionError, LocalLLMError\nfrom local_llm.constants import DEFAULT_WRAPPER\nfrom local_llm.function_parser import patch_function\nfrom local_llm.grammars.gbnf_grammar_generator import (\n    create_dynamic_model_from_function,\n    generate_gbnf_grammar_and_documentation,\n)\nfrom local_llm.groq.api import get_groq_completion\nfrom local_llm.koboldcpp.api import get_koboldcpp_completion\nfrom local_llm.llamacpp.api import get_llamacpp_completion\nfrom local_llm.llm_chat_completion_wrappers import simple_summary_wrapper\nfrom local_llm.lmstudio.api import get_lmstudio_completion\nfrom local_llm.ollama.api import get_ollama_completion\nfrom local_llm.utils import count_tokens, get_available_wrappers\nfrom local_llm.vllm.api import get_vllm_completion\nfrom local_llm.webui.api import get_webui_completion\nfrom local_llm.webui.legacy_api import (\n    get_webui_completion as get_webui_completion_legacy,\n)\nfrom models.chat_completion_response import (\n    ChatCompletionResponse,\n    Choice,\n    Message,\n    ToolCall,\n    UsageStatistics,\n)\nfrom prompts.gpt_summarize import SYSTEM as SUMMARIZE_SYSTEM_MESSAGE\nfrom utils import get_tool_call_id, get_utc_time\n\nhas_shown_warning = False\ngrammar_supported_backends = [\"koboldcpp\", \"llamacpp\", \"webui\", \"webui-legacy\"]\n\n\ndef get_chat_completion(\n    model,\n    # no model required (except for Ollama), since the model is fixed to whatever you set in your own backend\n    messages,\n    functions=None,\n    functions_python=None,\n    function_call=\"auto\",\n    context_window=None,\n    user=None,\n    # required\n    wrapper=None,\n    endpoint=None,\n    endpoint_type=None,\n    # optional cleanup\n    function_correction=True,\n    # extra hints to allow for additional prompt formatting hacks\n    # TODO this could alternatively be supported via passing function_call=\"send_message\" into the wrapper\n    first_message=False,\n    # optional auth headers\n    auth_type=None,\n    auth_key=None,\n) -> ChatCompletionResponse:\n    from utils import printd\n\n    assert context_window is not None, \"Local LLM calls need the context length to be explicitly set\"\n    assert endpoint is not None, \"Local LLM calls need the endpoint (eg http://localendpoint:1234) to be explicitly set\"\n    assert endpoint_type is not None, \"Local LLM calls need the endpoint type (eg webui) to be explicitly set\"\n    global has_shown_warning\n    grammar = None\n\n    # TODO: eventually just process Message object\n    if not isinstance(messages[0], dict):\n        messages = [m.to_openai_dict() for m in messages]\n\n    if function_call is not None and function_call != \"auto\":\n        raise ValueError(f\"function_call == {function_call} not supported (auto or None only)\")\n\n    available_wrappers = get_available_wrappers()\n    documentation = None\n\n    # Special case for if the call we're making is coming from the summarizer\n    if messages[0][\"role\"] == \"system\" and messages[0][\"content\"].strip() == SUMMARIZE_SYSTEM_MESSAGE.strip():\n        llm_wrapper = simple_summary_wrapper.SimpleSummaryWrapper()\n\n    # Select a default prompt formatter\n    elif wrapper is None:\n        # Warn the user that we're using the fallback\n        if not has_shown_warning:\n            print(\n                f\"{CLI_WARNING_PREFIX}no wrapper specified for local LLM, using the default wrapper (you can remove this warning by specifying the wrapper with --model-wrapper)\"\n            )\n            has_shown_warning = True\n\n        llm_wrapper = DEFAULT_WRAPPER()\n\n    # User provided an incorrect prompt formatter\n    elif wrapper not in available_wrappers:\n        raise ValueError(f\"Could not find requested wrapper '{wrapper} in available wrappers list:\\n{', '.join(available_wrappers)}\")\n\n    # User provided a correct prompt formatter\n    else:\n        llm_wrapper = available_wrappers[wrapper]\n\n    # If the wrapper uses grammar, generate the grammar using the grammar generating function\n    # TODO move this to a flag\n    if wrapper is not None and \"grammar\" in wrapper:\n        # When using grammars, we don't want to do any extras output tricks like appending a response prefix\n        setattr(llm_wrapper, \"assistant_prefix_extra_first_message\", \"\")\n        setattr(llm_wrapper, \"assistant_prefix_extra\", \"\")\n\n        # TODO find a better way to do this than string matching (eg an attribute)\n        if \"noforce\" in wrapper:\n            # \"noforce\" means that the prompt formatter expects inner thoughts as a top-level parameter\n            # this is closer to the OpenAI style since it allows for messages w/o any function calls\n            # however, with bad LLMs it makes it easier for the LLM to \"forget\" to call any of the functions\n            grammar, documentation = generate_grammar_and_documentation(\n                functions_python=functions_python,\n                add_inner_thoughts_top_level=True,\n                add_inner_thoughts_param_level=False,\n                allow_only_inner_thoughts=True,\n            )\n        else:\n            # otherwise, the other prompt formatters will insert inner thoughts as a function call parameter (by default)\n            # this means that every response from the LLM will be required to call a function\n            grammar, documentation = generate_grammar_and_documentation(\n                functions_python=functions_python,\n                add_inner_thoughts_top_level=False,\n                add_inner_thoughts_param_level=True,\n                allow_only_inner_thoughts=False,\n            )\n        printd(grammar)\n\n    if grammar is not None and endpoint_type not in grammar_supported_backends:\n        print(\n            f\"{CLI_WARNING_PREFIX}grammars are currently not supported when using {endpoint_type} as the typeagent local LLM backend (supported: {', '.join(grammar_supported_backends)})\"\n        )\n        grammar = None\n\n    # First step: turn the message sequence into a prompt that the model expects\n    try:\n        # if hasattr(llm_wrapper, \"supports_first_message\"):\n        if hasattr(llm_wrapper, \"supports_first_message\") and llm_wrapper.supports_first_message:\n            prompt = llm_wrapper.chat_completion_to_prompt(\n                messages=messages, functions=functions, first_message=first_message, function_documentation=documentation\n            )\n        else:\n            prompt = llm_wrapper.chat_completion_to_prompt(messages=messages, functions=functions, function_documentation=documentation)\n\n        printd(prompt)\n    except Exception as e:\n        print(e)\n        raise LocalLLMError(\n            f\"Failed to convert ChatCompletion messages into prompt string with wrapper {str(llm_wrapper)} - error: {str(e)}\"\n        )\n\n    try:\n        if endpoint_type == \"webui\":\n            result, usage = get_webui_completion(endpoint, auth_type, auth_key, prompt, context_window, grammar=grammar)\n        elif endpoint_type == \"webui-legacy\":\n            result, usage = get_webui_completion_legacy(endpoint, auth_type, auth_key, prompt, context_window, grammar=grammar)\n        elif endpoint_type == \"lmstudio\":\n            result, usage = get_lmstudio_completion(endpoint, auth_type, auth_key, prompt, context_window, api=\"completions\")\n        elif endpoint_type == \"lmstudio-legacy\":\n            result, usage = get_lmstudio_completion(endpoint, auth_type, auth_key, prompt, context_window, api=\"chat\")\n        elif endpoint_type == \"llamacpp\":\n            result, usage = get_llamacpp_completion(endpoint, auth_type, auth_key, prompt, context_window, grammar=grammar)\n        elif endpoint_type == \"koboldcpp\":\n            result, usage = get_koboldcpp_completion(endpoint, auth_type, auth_key, prompt, context_window, grammar=grammar)\n        elif endpoint_type == \"ollama\":\n            result, usage = get_ollama_completion(endpoint, auth_type, auth_key, model, prompt, context_window)\n        elif endpoint_type == \"vllm\":\n            result, usage = get_vllm_completion(endpoint, auth_type, auth_key, model, prompt, context_window, user)\n        elif endpoint_type == \"groq\":\n            result, usage = get_groq_completion(endpoint, auth_type, auth_key, model, prompt, context_window)\n        else:\n            raise LocalLLMError(\n                f\"Invalid endpoint type {endpoint_type}, please set variable depending on your backend (webui, lmstudio, llamacpp, koboldcpp)\"\n            )\n    except requests.exceptions.ConnectionError as e:\n        raise LocalLLMConnectionError(f\"Unable to connect to endpoint {endpoint}\")\n\n    if result is None or result == \"\":\n        raise LocalLLMError(f\"Got back an empty response string from {endpoint}\")\n    printd(f\"Raw LLM output:\\n====\\n{result}\\n====\")\n\n    try:\n        if hasattr(llm_wrapper, \"supports_first_message\") and llm_wrapper.supports_first_message:\n            chat_completion_result = llm_wrapper.output_to_chat_completion_response(result, first_message=first_message)\n        else:\n            chat_completion_result = llm_wrapper.output_to_chat_completion_response(result)\n        printd(json.dumps(chat_completion_result, indent=2, ensure_ascii=JSON_ENSURE_ASCII))\n    except Exception as e:\n        raise LocalLLMError(f\"Failed to parse JSON from local LLM response - error: {str(e)}\")\n\n    # Run through some manual function correction (optional)\n    if function_correction:\n        chat_completion_result = patch_function(message_history=messages, new_message=chat_completion_result)\n\n    # Fill in potential missing usage information (used for tracking token use)\n    if not (\"prompt_tokens\" in usage and \"completion_tokens\" in usage and \"total_tokens\" in usage):\n        raise LocalLLMError(f\"usage dict in response was missing fields ({usage})\")\n\n    if usage[\"prompt_tokens\"] is None:\n        printd(f\"usage dict was missing prompt_tokens, computing on-the-fly...\")\n        usage[\"prompt_tokens\"] = count_tokens(prompt)\n\n    # NOTE: we should compute on-the-fly anyways since we might have to correct for errors during JSON parsing\n    usage[\"completion_tokens\"] = count_tokens(json.dumps(chat_completion_result, ensure_ascii=JSON_ENSURE_ASCII))\n    \"\"\"\n    if usage[\"completion_tokens\"] is None:\n        printd(f\"usage dict was missing completion_tokens, computing on-the-fly...\")\n        # chat_completion_result is dict with 'role' and 'content'\n        # token counter wants a string\n        usage[\"completion_tokens\"] = count_tokens(json.dumps(chat_completion_result, ensure_ascii=JSON_ENSURE_ASCII))\n    \"\"\"\n\n    # NOTE: this is the token count that matters most\n    if usage[\"total_tokens\"] is None:\n        printd(f\"usage dict was missing total_tokens, computing on-the-fly...\")\n        usage[\"total_tokens\"] = usage[\"prompt_tokens\"] + usage[\"completion_tokens\"]\n\n    # unpack with response.choices[0].message.content\n    response = ChatCompletionResponse(\n        id=str(uuid.uuid4()),  # TODO something better?\n        choices=[\n            Choice(\n                finish_reason=\"stop\",\n                index=0,\n                message=Message(\n                    role=chat_completion_result[\"role\"],\n                    content=chat_completion_result[\"content\"],\n                    tool_calls=(\n                        [ToolCall(id=get_tool_call_id(), type=\"function\", function=chat_completion_result[\"function_call\"])]\n                        if \"function_call\" in chat_completion_result\n                        else []\n                    ),\n                ),\n            )\n        ],\n        created=get_utc_time(),\n        model=model,\n        # \"This fingerprint represents the backend configuration that the model runs with.\"\n        # system_fingerprint=user if user is not None else \"null\",\n        system_fingerprint=None,\n        object=\"chat.completion\",\n        usage=UsageStatistics(**usage),\n    )\n    printd(response)\n    return response\n\n\ndef generate_grammar_and_documentation(\n    functions_python: dict,\n    add_inner_thoughts_top_level: bool,\n    add_inner_thoughts_param_level: bool,\n    allow_only_inner_thoughts: bool,\n):\n    from utils import printd\n\n    assert not (\n        add_inner_thoughts_top_level and add_inner_thoughts_param_level\n    ), \"Can only place inner thoughts in one location in the grammar generator\"\n\n    grammar_function_models = []\n    # create_dynamic_model_from_function will add inner thoughts to the function parameters if add_inner_thoughts is True.\n    # generate_gbnf_grammar_and_documentation will add inner thoughts to the outer object of the function parameters if add_inner_thoughts is True.\n    for key, func in functions_python.items():\n        grammar_function_models.append(create_dynamic_model_from_function(func, add_inner_thoughts=add_inner_thoughts_param_level))\n    grammar, documentation = generate_gbnf_grammar_and_documentation(\n        grammar_function_models,\n        outer_object_name=\"function\",\n        outer_object_content=\"params\",\n        model_prefix=\"function\",\n        fields_prefix=\"params\",\n        add_inner_thoughts=add_inner_thoughts_top_level,\n        allow_only_inner_thoughts=allow_only_inner_thoughts,\n    )\n    printd(grammar)\n    return grammar, documentation\n"}
{"type": "source_file", "path": "luann/llm_api/google_ai.py", "content": "import json\nimport uuid\nfrom typing import List, Optional\n\nimport requests\n\nfrom constants import JSON_ENSURE_ASCII, NON_USER_MSG_PREFIX\nfrom local_llm.json_parser import clean_json_string_extra_backslash\nfrom local_llm.utils import count_tokens\nfrom models.chat_completion_request import Tool\nfrom models.chat_completion_response import (\n    ChatCompletionResponse,\n    Choice,\n    FunctionCall,\n    Message,\n    ToolCall,\n    UsageStatistics,\n)\nfrom utils import get_tool_call_id, get_utc_time\n\n# from data_types import ToolCall\n\n\nSUPPORTED_MODELS = [\n    \"gemini-pro\",\n]\n\n\ndef google_ai_get_model_details(service_endpoint: str, api_key: str, model: str, key_in_header: bool = True) -> List[dict]:\n    from utils import printd\n\n    # Two ways to pass the key: https://ai.google.dev/tutorials/setup\n    if key_in_header:\n        url = f\"https://{service_endpoint}.googleapis.com/v1beta/models/{model}\"\n        headers = {\"Content-Type\": \"application/json\", \"x-goog-api-key\": api_key}\n    else:\n        url = f\"https://{service_endpoint}.googleapis.com/v1beta/models/{model}?key={api_key}\"\n        headers = {\"Content-Type\": \"application/json\"}\n\n    try:\n        response = requests.get(url, headers=headers)\n        printd(f\"response = {response}\")\n        response.raise_for_status()  # Raises HTTPError for 4XX/5XX status\n        response = response.json()  # convert to dict from string\n        printd(f\"response.json = {response}\")\n\n        # Grab the models out\n        return response\n\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        printd(f\"Got HTTPError, exception={http_err}\")\n        # Print the HTTP status code\n        print(f\"HTTP Error: {http_err.response.status_code}\")\n        # Print the response content (error message from server)\n        print(f\"Message: {http_err.response.text}\")\n        raise http_err\n\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        printd(f\"Got RequestException, exception={req_err}\")\n        raise req_err\n\n    except Exception as e:\n        # Handle other potential errors\n        printd(f\"Got unknown Exception, exception={e}\")\n        raise e\n\n\ndef google_ai_get_model_context_window(service_endpoint: str, api_key: str, model: str, key_in_header: bool = True) -> int:\n    model_details = google_ai_get_model_details(\n        service_endpoint=service_endpoint, api_key=api_key, model=model, key_in_header=key_in_header\n    )\n    # TODO should this be:\n    # return model_details[\"inputTokenLimit\"] + model_details[\"outputTokenLimit\"]\n    return int(model_details[\"inputTokenLimit\"])\n\n\ndef google_ai_get_model_list(service_endpoint: str, api_key: str, key_in_header: bool = True) -> List[dict]:\n    from utils import printd\n\n    # Two ways to pass the key: https://ai.google.dev/tutorials/setup\n    if key_in_header:\n        url = f\"https://{service_endpoint}.googleapis.com/v1beta/models\"\n        headers = {\"Content-Type\": \"application/json\", \"x-goog-api-key\": api_key}\n    else:\n        url = f\"https://{service_endpoint}.googleapis.com/v1beta/models?key={api_key}\"\n        headers = {\"Content-Type\": \"application/json\"}\n\n    try:\n        response = requests.get(url, headers=headers)\n        printd(f\"response = {response}\")\n        response.raise_for_status()  # Raises HTTPError for 4XX/5XX status\n        response = response.json()  # convert to dict from string\n        printd(f\"response.json = {response}\")\n\n        # Grab the models out\n        model_list = response[\"models\"]\n        return model_list\n\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        printd(f\"Got HTTPError, exception={http_err}\")\n        # Print the HTTP status code\n        print(f\"HTTP Error: {http_err.response.status_code}\")\n        # Print the response content (error message from server)\n        print(f\"Message: {http_err.response.text}\")\n        raise http_err\n\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        printd(f\"Got RequestException, exception={req_err}\")\n        raise req_err\n\n    except Exception as e:\n        # Handle other potential errors\n        printd(f\"Got unknown Exception, exception={e}\")\n        raise e\n\n\ndef add_dummy_model_messages(messages: List[dict]) -> List[dict]:\n    \"\"\"Google AI API requires all function call returns are immediately followed by a 'model' role message.\n\n    In typeagent, the 'model' will often call a function (e.g. send_message) that itself yields to the user,\n    so there is no natural follow-up 'model' role message.\n\n    To satisfy the Google AI API restrictions, we can add a dummy 'yield' message\n    with role == 'model' that is placed in-betweeen and function output\n    (role == 'tool') and user message (role == 'user').\n    \"\"\"\n    dummy_yield_message = {\"role\": \"model\", \"parts\": [{\"text\": f\"{NON_USER_MSG_PREFIX}Function call returned, waiting for user response.\"}]}\n    messages_with_padding = []\n    for i, message in enumerate(messages):\n        messages_with_padding.append(message)\n        # Check if the current message role is 'tool' and the next message role is 'user'\n        if message[\"role\"] in [\"tool\", \"function\"] and (i + 1 < len(messages) and messages[i + 1][\"role\"] == \"user\"):\n            messages_with_padding.append(dummy_yield_message)\n\n    return messages_with_padding\n\n\n# TODO use pydantic model as input\ndef to_google_ai(openai_message_dict: dict) -> dict:\n\n    # TODO supports \"parts\" as part of multimodal support\n    assert not isinstance(openai_message_dict[\"content\"], list), \"Multi-part content is message not yet supported\"\n    if openai_message_dict[\"role\"] == \"user\":\n        google_ai_message_dict = {\n            \"role\": \"user\",\n            \"parts\": [{\"text\": openai_message_dict[\"content\"]}],\n        }\n    elif openai_message_dict[\"role\"] == \"assistant\":\n        google_ai_message_dict = {\n            \"role\": \"model\",  # NOTE: diff\n            \"parts\": [{\"text\": openai_message_dict[\"content\"]}],\n        }\n    elif openai_message_dict[\"role\"] == \"tool\":\n        google_ai_message_dict = {\n            \"role\": \"function\",  # NOTE: diff\n            \"parts\": [{\"text\": openai_message_dict[\"content\"]}],\n        }\n    else:\n        raise ValueError(f\"Unsupported conversion (OpenAI -> Google AI) from role {openai_message_dict['role']}\")\n\n\n# TODO convert return type to pydantic\ndef convert_tools_to_google_ai_format(tools: List[Tool], inner_thoughts_in_kwargs: Optional[bool] = True) -> List[dict]:\n    \"\"\"\n    OpenAI style:\n      \"tools\": [{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"find_movies\",\n            \"description\": \"find ....\",\n            \"parameters\": {\n              \"type\": \"object\",\n              \"properties\": {\n                 PARAM: {\n                   \"type\": PARAM_TYPE,  # eg \"string\"\n                   \"description\": PARAM_DESCRIPTION,\n                 },\n                 ...\n              },\n              \"required\": List[str],\n            }\n        }\n      }\n      ]\n\n    Google AI style:\n      \"tools\": [{\n        \"functionDeclarations\": [{\n          \"name\": \"find_movies\",\n          \"description\": \"find movie titles currently playing in theaters based on any description, genre, title words, etc.\",\n          \"parameters\": {\n            \"type\": \"OBJECT\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"STRING\",\n                \"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n              },\n              \"description\": {\n                \"type\": \"STRING\",\n                \"description\": \"Any kind of description including category or genre, title words, attributes, etc.\"\n              }\n            },\n            \"required\": [\"description\"]\n          }\n        }, {\n          \"name\": \"find_theaters\",\n          ...\n    \"\"\"\n    function_list = [\n        dict(\n            name=t.function.name,\n            description=t.function.description,\n            parameters=t.function.parameters,  # TODO need to unpack\n        )\n        for t in tools\n    ]\n\n    # Correct casing + add inner thoughts if needed\n    for func in function_list:\n        func[\"parameters\"][\"type\"] = \"OBJECT\"\n        for param_name, param_fields in func[\"parameters\"][\"properties\"].items():\n            param_fields[\"type\"] = param_fields[\"type\"].upper()\n        # Add inner thoughts\n        if inner_thoughts_in_kwargs:\n            from local_llm.constants import (\n                INNER_THOUGHTS_KWARG,\n                INNER_THOUGHTS_KWARG_DESCRIPTION,\n            )\n\n            func[\"parameters\"][\"properties\"][INNER_THOUGHTS_KWARG] = {\n                \"type\": \"STRING\",\n                \"description\": INNER_THOUGHTS_KWARG_DESCRIPTION,\n            }\n            func[\"parameters\"][\"required\"].append(INNER_THOUGHTS_KWARG)\n\n    return [{\"functionDeclarations\": function_list}]\n\n\ndef convert_google_ai_response_to_chatcompletion(\n    response_json: dict,  # REST response from Google AI API\n    model: str,  # Required since not returned\n    input_messages: Optional[List[dict]] = None,  # Required if the API doesn't return UsageMetadata\n    pull_inner_thoughts_from_args: Optional[bool] = True,\n) -> ChatCompletionResponse:\n    \"\"\"Google AI API response format is not the same as ChatCompletion, requires unpacking\n\n    Example:\n    {\n      \"candidates\": [\n        {\n          \"content\": {\n            \"parts\": [\n              {\n                \"text\": \" OK. Barbie is showing in two theaters in Mountain View, CA: AMC Mountain View 16 and Regal Edwards 14.\"\n              }\n            ]\n          }\n        }\n      ],\n      \"usageMetadata\": {\n        \"promptTokenCount\": 9,\n        \"candidatesTokenCount\": 27,\n        \"totalTokenCount\": 36\n      }\n    }\n    \"\"\"\n    try:\n        choices = []\n        for candidate in response_json[\"candidates\"]:\n            content = candidate[\"content\"]\n\n            role = content[\"role\"]\n            assert role == \"model\", f\"Unknown role in response: {role}\"\n\n            parts = content[\"parts\"]\n            # TODO support parts / multimodal\n            assert len(parts) == 1, f\"Multi-part not yet supported:\\n{parts}\"\n            response_message = parts[0]\n\n            # Convert the actual message style to OpenAI style\n            if \"functionCall\" in response_message and response_message[\"functionCall\"] is not None:\n                function_call = response_message[\"functionCall\"]\n                assert isinstance(function_call, dict), function_call\n                function_name = function_call[\"name\"]\n                assert isinstance(function_name, str), function_name\n                function_args = function_call[\"args\"]\n                assert isinstance(function_args, dict), function_args\n\n                # NOTE: this also involves stripping the inner monologue out of the function\n                if pull_inner_thoughts_from_args:\n                    from local_llm.constants import INNER_THOUGHTS_KWARG\n\n                    assert INNER_THOUGHTS_KWARG in function_args, f\"Couldn't find inner thoughts in function args:\\n{function_call}\"\n                    inner_thoughts = function_args.pop(INNER_THOUGHTS_KWARG)\n                    assert inner_thoughts is not None, f\"Expected non-null inner thoughts function arg:\\n{function_call}\"\n                else:\n                    inner_thoughts = None\n\n                # Google AI API doesn't generate tool call IDs\n                openai_response_message = Message(\n                    role=\"assistant\",  # NOTE: \"model\" -> \"assistant\"\n                    content=inner_thoughts,\n                    tool_calls=[\n                        ToolCall(\n                            id=get_tool_call_id(),\n                            type=\"function\",\n                            function=FunctionCall(\n                                name=function_name,\n                                # arguments=clean_json_string_extra_backslash(json.dumps(function_args)),\n                                arguments=clean_json_string_extra_backslash(json.dumps(function_args, ensure_ascii=JSON_ENSURE_ASCII)),\n                            ),\n                        )\n                    ],\n                )\n\n            else:\n\n                # Inner thoughts are the content by default\n                inner_thoughts = response_message[\"text\"]\n\n                # Google AI API doesn't generate tool call IDs\n                openai_response_message = Message(\n                    role=\"assistant\",  # NOTE: \"model\" -> \"assistant\"\n                    content=inner_thoughts,\n                )\n\n            # Google AI API uses different finish reason strings than OpenAI\n            # OpenAI: 'stop', 'length', 'function_call', 'content_filter', null\n            #   see: https://platform.openai.com/docs/guides/text-generation/chat-completions-api\n            # Google AI API: FINISH_REASON_UNSPECIFIED, STOP, MAX_TOKENS, SAFETY, RECITATION, OTHER\n            #   see: https://ai.google.dev/api/python/google/ai/generativelanguage/Candidate/FinishReason\n            finish_reason = candidate[\"finishReason\"]\n            if finish_reason == \"STOP\":\n                openai_finish_reason = (\n                    \"function_call\"\n                    if openai_response_message.tool_calls is not None and len(openai_response_message.tool_calls) > 0\n                    else \"stop\"\n                )\n            elif finish_reason == \"MAX_TOKENS\":\n                openai_finish_reason = \"length\"\n            elif finish_reason == \"SAFETY\":\n                openai_finish_reason = \"content_filter\"\n            elif finish_reason == \"RECITATION\":\n                openai_finish_reason = \"content_filter\"\n            else:\n                raise ValueError(f\"Unrecognized finish reason in Google AI response: {finish_reason}\")\n\n            choices.append(\n                Choice(\n                    finish_reason=openai_finish_reason,\n                    index=candidate[\"index\"],\n                    message=openai_response_message,\n                )\n            )\n\n        if len(choices) > 1:\n            raise UserWarning(f\"Unexpected number of candidates in response (expected 1, got {len(choices)})\")\n\n        # NOTE: some of the Google AI APIs show UsageMetadata in the response, but it seems to not exist?\n        #  \"usageMetadata\": {\n        #     \"promptTokenCount\": 9,\n        #     \"candidatesTokenCount\": 27,\n        #     \"totalTokenCount\": 36\n        #   }\n        if \"usageMetadata\" in response_json:\n            usage = UsageStatistics(\n                prompt_tokens=response_json[\"usageMetadata\"][\"promptTokenCount\"],\n                completion_tokens=response_json[\"usageMetadata\"][\"candidatesTokenCount\"],\n                total_tokens=response_json[\"usageMetadata\"][\"totalTokenCount\"],\n            )\n        else:\n            # Count it ourselves\n            assert input_messages is not None, f\"Didn't get UsageMetadata from the API response, so input_messages is required\"\n            prompt_tokens = count_tokens(\n                json.dumps(input_messages, ensure_ascii=JSON_ENSURE_ASCII)\n            )  # NOTE: this is a very rough approximation\n            completion_tokens = count_tokens(\n                json.dumps(openai_response_message.model_dump(), ensure_ascii=JSON_ENSURE_ASCII)\n            )  # NOTE: this is also approximate\n            total_tokens = prompt_tokens + completion_tokens\n            usage = UsageStatistics(\n                prompt_tokens=prompt_tokens,\n                completion_tokens=completion_tokens,\n                total_tokens=total_tokens,\n            )\n\n        response_id = str(uuid.uuid4())\n        return ChatCompletionResponse(\n            id=response_id,\n            choices=choices,\n            model=model,  # NOTE: Google API doesn't pass back model in the response\n            created=get_utc_time(),\n            usage=usage,\n        )\n    except KeyError as e:\n        raise e\n\n\n# TODO convert 'data' type to pydantic\ndef google_ai_chat_completions_request(\n    service_endpoint: str,\n    model: str,\n    api_key: str,\n    data: dict,\n    key_in_header: bool = True,\n    add_postfunc_model_messages: bool = True,\n    # NOTE: Google AI API doesn't support mixing parts 'text' and 'function',\n    # so there's no clean way to put inner thoughts in the same message as a function call\n    inner_thoughts_in_kwargs: bool = True,\n) -> ChatCompletionResponse:\n    \"\"\"https://ai.google.dev/docs/function_calling\n\n    From https://ai.google.dev/api/rest#service-endpoint:\n    \"A service endpoint is a base URL that specifies the network address of an API service.\n    One service might have multiple service endpoints.\n    This service has the following service endpoint and all URIs below are relative to this service endpoint:\n    https://xxx.googleapis.com\n    \"\"\"\n    from utils import printd\n\n    assert service_endpoint is not None, \"Missing service_endpoint when calling Google AI\"\n    assert api_key is not None, \"Missing api_key when calling Google AI\"\n    assert model in SUPPORTED_MODELS, f\"Model '{model}' not in supported models: {', '.join(SUPPORTED_MODELS)}\"\n\n    # Two ways to pass the key: https://ai.google.dev/tutorials/setup\n    if key_in_header:\n        url = f\"https://{service_endpoint}.googleapis.com/v1beta/models/{model}:generateContent\"\n        headers = {\"Content-Type\": \"application/json\", \"x-goog-api-key\": api_key}\n    else:\n        url = f\"https://{service_endpoint}.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}\"\n        headers = {\"Content-Type\": \"application/json\"}\n\n    # data[\"contents\"][-1][\"role\"] = \"model\"\n    if add_postfunc_model_messages:\n        data[\"contents\"] = add_dummy_model_messages(data[\"contents\"])\n\n    printd(f\"Sending request to {url}\")\n    try:\n        response = requests.post(url, headers=headers, json=data)\n        printd(f\"response = {response}\")\n        response.raise_for_status()  # Raises HTTPError for 4XX/5XX status\n        response = response.json()  # convert to dict from string\n        printd(f\"response.json = {response}\")\n\n        # Convert Google AI response to ChatCompletion style\n        return convert_google_ai_response_to_chatcompletion(\n            response_json=response,\n            model=model,\n            input_messages=data[\"contents\"],\n            pull_inner_thoughts_from_args=inner_thoughts_in_kwargs,\n        )\n\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        printd(f\"Got HTTPError, exception={http_err}, payload={data}\")\n        # Print the HTTP status code\n        print(f\"HTTP Error: {http_err.response.status_code}\")\n        # Print the response content (error message from server)\n        print(f\"Message: {http_err.response.text}\")\n        raise http_err\n\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        printd(f\"Got RequestException, exception={req_err}\")\n        raise req_err\n\n    except Exception as e:\n        # Handle other potential errors\n        printd(f\"Got unknown Exception, exception={e}\")\n        raise e\n"}
{"type": "source_file", "path": "luann/llm_api/llm_api_tools.py", "content": "import os\nimport random\nimport time\nimport uuid\nfrom typing import List, Optional, Union\nimport copy\nimport json\nimport warnings\nimport requests\nfrom data_types import AgentState\nfrom constants import CLI_WARNING_PREFIX, JSON_ENSURE_ASCII\nfrom credentials import typeagentCredentials\nfrom data_types import Message\nfrom llm_api.anthropic import anthropic_chat_completions_request\nfrom llm_api.azure_openai import (\n    MODEL_TO_AZURE_ENGINE,\n  \n)\nfrom llm_api.cohere import cohere_chat_completions_request\nfrom llm_api.google_ai import (\n    convert_tools_to_google_ai_format,\n    google_ai_chat_completions_request,\n)\nfrom llm_api.openai import (\n    openai_chat_completions_process_stream,\n    openai_chat_completions_request,\n)\nfrom local_llm.chat_completion_proxy import get_chat_completion\nfrom models.chat_completion_request import (\n    ChatCompletionRequest,\n    Tool,\n    cast_message_to_subtype,\n)\nfrom local_llm.constants import (\n    INNER_THOUGHTS_KWARG,\n    INNER_THOUGHTS_KWARG_DESCRIPTION,\n)\nfrom models.chat_completion_response import ChatCompletionResponse\nfrom models.pydantic_models import LLMConfigModel, OptionState\nfrom streaming_interface import (\n    AgentChunkStreamingInterface,\n    AgentRefreshStreamingInterface,\n)\n\nLLM_API_PROVIDER_OPTIONS = [\"openai\", \"azure\", \"anthropic\", \"google_ai\", \"cohere\", \"local\"]\ndef add_inner_thoughts_to_functions(\n    functions: List[dict],\n    inner_thoughts_key: str,\n    inner_thoughts_description: str,\n    inner_thoughts_required: bool = True,\n    # inner_thoughts_to_front: bool = True,  TODO support sorting somewhere, probably in the to_dict?\n) -> List[dict]:\n    \"\"\"Add an inner_thoughts kwarg to every function in the provided list\"\"\"\n    # return copies\n    new_functions = []\n\n    # functions is a list of dicts in the OpenAI schema (https://platform.openai.com/docs/api-reference/chat/create)\n    for function_object in functions:\n        function_params = function_object[\"parameters\"][\"properties\"]\n        required_params = list(function_object[\"parameters\"][\"required\"])\n\n        # if the inner thoughts arg doesn't exist, add it\n        if inner_thoughts_key not in function_params:\n            function_params[inner_thoughts_key] = {\n                \"type\": \"string\",\n                \"description\": inner_thoughts_description,\n            }\n\n        # make sure it's tagged as required\n        new_function_object = copy.deepcopy(function_object)\n        if inner_thoughts_required and inner_thoughts_key not in required_params:\n            required_params.append(inner_thoughts_key)\n            new_function_object[\"parameters\"][\"required\"] = required_params\n\n        new_functions.append(new_function_object)\n\n    # return a list of copies\n    return new_functions\n\n\ndef unpack_inner_thoughts_from_kwargs(\n    response: ChatCompletionResponse,\n    inner_thoughts_key: str,\n) -> ChatCompletionResponse:\n    \"\"\"Strip the inner thoughts out of the tool call and put it in the message content\"\"\"\n    if len(response.choices) == 0:\n        raise ValueError(f\"Unpacking inner thoughts from empty response not supported\")\n\n    new_choices = []\n    for choice in response.choices:\n        msg = choice.message\n        if msg.role == \"assistant\" and len(msg.tool_calls) >= 1:\n            if len(msg.tool_calls) > 1:\n                warnings.warn(f\"Unpacking inner thoughts from more than one tool call ({len(msg.tool_calls)}) is not supported\")\n            # TODO support multiple tool calls\n            tool_call = msg.tool_calls[0]\n\n            try:\n                # Sadly we need to parse the JSON since args are in string format\n                func_args = dict(json.loads(tool_call.function.arguments))\n                if inner_thoughts_key in func_args:\n                    # extract the inner thoughts\n                    inner_thoughts = func_args.pop(inner_thoughts_key)\n\n                    # replace the kwargs\n                    new_choice = choice.model_copy(deep=True)\n                    new_choice.message.tool_calls[0].function.arguments = json.dumps(func_args, ensure_ascii=JSON_ENSURE_ASCII)\n                    # also replace the message content\n                    if new_choice.message.content is not None:\n                        warnings.warn(f\"Overwriting existing inner monologue ({new_choice.message.content}) with kwarg ({inner_thoughts})\")\n                    new_choice.message.content = inner_thoughts\n\n                    # save copy\n                    new_choices.append(new_choice)\n                else:\n                    warnings.warn(f\"Did not find inner thoughts in tool call: {str(tool_call)}\")\n\n            except json.JSONDecodeError as e:\n                warnings.warn(f\"Failed to strip inner thoughts from kwargs: {e}\")\n                raise e\n\n    # return an updated copy\n    new_response = response.model_copy(deep=True)\n    new_response.choices = new_choices\n    return new_response\n\ndef is_context_overflow_error(exception: requests.exceptions.RequestException) -> bool:\n    \"\"\"Checks if an exception is due to context overflow (based on common OpenAI response messages)\"\"\"\n    from utils import printd\n\n    match_string = \"maximum context length\"\n\n    # Backwards compatibility with openai python package/client v0.28 (pre-v1 client migration)\n    if match_string in str(exception):\n        printd(f\"Found '{match_string}' in str(exception)={(str(exception))}\")\n        return True\n\n    # Based on python requests + OpenAI REST API (/v1)\n    elif isinstance(exception, requests.exceptions.HTTPError):\n        if exception.response is not None and \"application/json\" in exception.response.headers.get(\"Content-Type\", \"\"):\n            try:\n                error_details = exception.response.json()\n                if \"error\" not in error_details:\n                    printd(f\"HTTPError occurred, but couldn't find error field: {error_details}\")\n                    return False\n                else:\n                    error_details = error_details[\"error\"]\n\n                # Check for the specific error code\n                if error_details.get(\"code\") == \"context_length_exceeded\":\n                    printd(f\"HTTPError occurred, caught error code {error_details.get('code')}\")\n                    return True\n                # Soft-check for \"maximum context length\" inside of the message\n                elif error_details.get(\"message\") and \"maximum context length\" in error_details.get(\"message\"):\n                    printd(f\"HTTPError occurred, found '{match_string}' in error message contents ({error_details})\")\n                    return True\n                else:\n                    printd(f\"HTTPError occurred, but unknown error message: {error_details}\")\n                    return False\n            except ValueError:\n                # JSON decoding failed\n                printd(f\"HTTPError occurred ({exception}), but no JSON error message.\")\n\n    # Generic fail\n    else:\n        return False\n\n\ndef retry_with_exponential_backoff(\n    func,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = True,\n    max_retries: int = 20,\n    # List of OpenAI error codes: https://github.com/openai/openai-python/blob/17ac6779958b2b74999c634c4ea4c7b74906027a/src/openai/_client.py#L227-L250\n    # 429 = rate limit\n    error_codes: tuple = (429,),\n):\n    \"\"\"Retry a function with exponential backoff.\"\"\"\n\n    def wrapper(*args, **kwargs):\n        pass\n\n        # Initialize variables\n        num_retries = 0\n        delay = initial_delay\n\n        # Loop until a successful response or max_retries is hit or an exception is raised\n        while True:\n            try:\n                return func(*args, **kwargs)\n\n            except requests.exceptions.HTTPError as http_err:\n                # Retry on specified errors\n                if http_err.response.status_code in error_codes:\n                    # Increment retries\n                    num_retries += 1\n\n                    # Check if max retries has been reached\n                    if num_retries > max_retries:\n                        raise Exception(f\"Maximum number of retries ({max_retries}) exceeded.\")\n\n                    # Increment the delay\n                    delay *= exponential_base * (1 + jitter * random.random())\n\n                    # Sleep for the delay\n                    # printd(f\"Got a rate limit error ('{http_err}') on LLM backend request, waiting {int(delay)}s then retrying...\")\n                    print(\n                        f\"{CLI_WARNING_PREFIX}Got a rate limit error ('{http_err}') on LLM backend request, waiting {int(delay)}s then retrying...\"\n                    )\n                    time.sleep(delay)\n                else:\n                    # For other HTTP errors, re-raise the exception\n                    raise\n\n            # Raise exceptions for any errors not specified\n            except Exception as e:\n                raise e\n\n    return wrapper\n\n\n@retry_with_exponential_backoff\ndef create(\n    # agent_state: AgentState,\n    llm_config: LLMConfigModel,\n    messages: List[Message],\n    user_id: uuid.UUID = None,  # option UUID to associate request with\n    functions: list = None,\n    functions_python: list = None,\n    function_call: str = \"auto\",\n    # hint\n    # first_message: bool = False,\n    # use tool naming?\n    # if false, will use deprecated 'functions' style\n    use_tool_naming: bool = True,\n    # streaming?\n    stream: bool = False,\n    inner_thoughts_in_kwargs: OptionState = OptionState.DEFAULT,\n    stream_inferface: Optional[Union[AgentRefreshStreamingInterface, AgentChunkStreamingInterface]] = None,\n) -> ChatCompletionResponse:\n    \"\"\"Return response to chat completion with backoff\"\"\"\n    from utils import printd\n\n    printd(f\"Using model {llm_config.model_endpoint_type}, endpoint: {llm_config.model_endpoint}\")\n\n    # TODO eventually refactor so that credentials are passed through\n    credentials = typeagentCredentials.load()\n\n    if function_call and not functions:\n        printd(\"unsetting function_call because functions is None\")\n        function_call = None\n\n    # openai\n    if llm_config.model_endpoint_type == \"openai\":\n        if inner_thoughts_in_kwargs == OptionState.DEFAULT:\n            # model that are known to not use `content` fields on tool calls\n            inner_thoughts_in_kwargs = (\n                \"gpt-4o\" in llm_config.model or \"gpt-4-turbo\" in llm_config.model or \"gpt-3.5-turbo\" in llm_config.model\n            )\n        else:\n            inner_thoughts_in_kwargs = True if inner_thoughts_in_kwargs == OptionState.YES else False\n\n        # assert isinstance(inner_thoughts_in_kwargs, bool), type(inner_thoughts_in_kwargs)\n        if inner_thoughts_in_kwargs:\n            functions = add_inner_thoughts_to_functions(\n                functions=functions,\n                inner_thoughts_key=INNER_THOUGHTS_KWARG,\n                inner_thoughts_description=INNER_THOUGHTS_KWARG_DESCRIPTION,\n            )\n\n        openai_message_list = [\n            cast_message_to_subtype(m.to_openai_dict(put_inner_thoughts_in_kwargs=inner_thoughts_in_kwargs)) for m in messages\n        ]\n        # TODO do the same for Azure?\n        if credentials.openai_key is None and llm_config.model_endpoint == \"https://api.openai.com/v1\":\n            # only is a problem if we are *not* using an openai proxy\n            raise ValueError(f\"OpenAI key is missing from typeagent config file\")\n        if use_tool_naming:\n            data = ChatCompletionRequest(\n                model=llm_config.model,\n                # messages=[cast_message_to_subtype(m.to_openai_dict()) for m in messages],\n                messages=openai_message_list,\n                tools=[{\"type\": \"function\", \"function\": f} for f in functions] if functions else None,\n                tool_choice=function_call,\n                user=str(user_id),\n            )\n        else:\n            data = ChatCompletionRequest(\n                model=llm_config.model,\n                # messages=[cast_message_to_subtype(m.to_openai_dict()) for m in messages],\n                messages=openai_message_list,\n                functions=functions,\n                function_call=function_call,\n                user=str(user_id),\n            )\n            # https://platform.openai.com/docs/guides/text-generation/json-mode\n            # only supported by gpt-4o, gpt-4-turbo, or gpt-3.5-turbo\n            if \"gpt-4o\" in llm_config.model or \"gpt-4-turbo\" in llm_config.model or \"gpt-3.5-turbo\" in llm_config.model:\n                data.response_format = {\"type\": \"json_object\"}\n        if stream:# Client requested token streaming\n            data.stream = True\n            assert isinstance(stream_inferface, AgentChunkStreamingInterface) or isinstance(\n                stream_inferface, AgentRefreshStreamingInterface\n            ), type(stream_inferface)\n            response =openai_chat_completions_process_stream(\n                url=llm_config.model_endpoint,  # https://api.openai.com/v1 -> https://api.openai.com/v1/chat/completions\n                api_key=credentials.openai_key,\n                chat_completion_request=data,\n                stream_inferface=stream_inferface,\n            )\n        else:# Client did not request token streaming (expect a blocking backend response)\n            data.stream = False\n            # return openai_chat_completions_request(\n            #     url=llm_config.model_endpoint,  # https://api.openai.com/v1 -> https://api.openai.com/v1/chat/completions\n            #     api_key=credentials.openai_key,\n            #     chat_completion_request=data,\n            # )\n            if isinstance(stream_inferface, AgentChunkStreamingInterface):\n                stream_inferface.stream_start()\n            try:\n                response = openai_chat_completions_request(\n                    url=llm_config.model_endpoint,  # https://api.openai.com/v1 -> https://api.openai.com/v1/chat/completions\n                    api_key=credentials.openai_key,\n                    chat_completion_request=data,\n                )\n            finally:\n                if isinstance(stream_inferface, AgentChunkStreamingInterface):\n                    stream_inferface.stream_end()\n            # return response\n        if inner_thoughts_in_kwargs:\n            response = unpack_inner_thoughts_from_kwargs(response=response, inner_thoughts_key=INNER_THOUGHTS_KWARG)\n\n        return response\n\n    # azure\n    elif llm_config.model_endpoint_type == \"azure\":\n        if stream:\n            raise NotImplementedError(f\"Streaming not yet implemented for {llm_config.model_endpoint_type}\")\n\n        azure_deployment = (\n            credentials.azure_deployment if credentials.azure_deployment is not None else MODEL_TO_AZURE_ENGINE[llm_config.model]\n        )\n        if use_tool_naming:\n            data = dict(\n                # NOTE: don't pass model to Azure calls, that is the deployment_id\n                # model=agent_config.model,\n                messages=[m.to_openai_dict() for m in messages],\n                tools=[{\"type\": \"function\", \"function\": f} for f in functions] if functions else None,\n                tool_choice=function_call,\n                user=str(user_id),\n            )\n        else:\n            data = dict(\n                # NOTE: don't pass model to Azure calls, that is the deployment_id\n                # model=agent_config.model,\n                messages=[m.to_openai_dict() for m in messages],\n                functions=functions,\n                function_call=function_call,\n                user=str(user_id),\n            )\n        print(\"data\")\n        print(data)\n      \n\n    elif llm_config.model_endpoint_type == \"google_ai\":\n        if stream:\n            raise NotImplementedError(f\"Streaming not yet implemented for {llm_config.model_endpoint_type}\")\n        if not use_tool_naming:\n            raise NotImplementedError(\"Only tool calling supported on Google AI API requests\")\n\n        # NOTE: until Google AI supports CoT / text alongside function calls,\n        # we need to put it in a kwarg (unless we want to split the message into two)\n        google_ai_inner_thoughts_in_kwarg = True\n\n        if functions is not None:\n            tools = [{\"type\": \"function\", \"function\": f} for f in functions]\n            tools = [Tool(**t) for t in tools]\n            tools = convert_tools_to_google_ai_format(tools, inner_thoughts_in_kwargs=google_ai_inner_thoughts_in_kwarg)\n        else:\n            tools = None\n\n        return google_ai_chat_completions_request(\n            inner_thoughts_in_kwargs=google_ai_inner_thoughts_in_kwarg,\n            service_endpoint=credentials.google_ai_service_endpoint,\n            model=llm_config.model,\n            api_key=credentials.google_ai_key,\n            # see structure of payload here: https://ai.google.dev/docs/function_calling\n            data=dict(\n                contents=[m.to_google_ai_dict() for m in messages],\n                tools=tools,\n            ),\n        )\n\n    elif llm_config.model_endpoint_type == \"anthropic\":\n        if not use_tool_naming:\n            raise NotImplementedError(\"Only tool calling supported on Anthropic API requests\")\n\n        if functions is not None:\n            tools = [{\"type\": \"function\", \"function\": f} for f in functions]\n            tools = [Tool(**t) for t in tools]\n        else:\n            tools = None\n\n        return anthropic_chat_completions_request(\n            url=llm_config.model_endpoint,\n            api_key=credentials.anthropic_key,\n            data=ChatCompletionRequest(\n                model=llm_config.model,\n                messages=[cast_message_to_subtype(m.to_openai_dict()) for m in messages],\n                tools=[{\"type\": \"function\", \"function\": f} for f in functions] if functions else None,\n                # tool_choice=function_call,\n                # user=str(user_id),\n                # NOTE: max_tokens is required for Anthropic API\n                max_tokens=1024,  # TODO make dynamic\n            ),\n        )\n\n    elif llm_config.model_endpoint_type == \"cohere\":\n        if stream:\n            raise NotImplementedError(f\"Streaming not yet implemented for {llm_config.model_endpoint_type}\")\n        if not use_tool_naming:\n            raise NotImplementedError(\"Only tool calling supported on Cohere API requests\")\n\n        if functions is not None:\n            tools = [{\"type\": \"function\", \"function\": f} for f in functions]\n            tools = [Tool(**t) for t in tools]\n        else:\n            tools = None\n\n        return cohere_chat_completions_request(\n            # url=llm_config.model_endpoint,\n            url=\"https://api.cohere.ai/v1\",  # TODO\n            api_key=os.getenv(\"COHERE_API_KEY\"),  # TODO remove\n            chat_completion_request=ChatCompletionRequest(\n                model=\"command-r-plus\",  # TODO\n                messages=[cast_message_to_subtype(m.to_openai_dict()) for m in messages],\n                tools=[{\"type\": \"function\", \"function\": f} for f in functions] if functions else None,\n                tool_choice=function_call,\n                # user=str(user_id),\n                # NOTE: max_tokens is required for Anthropic API\n                # max_tokens=1024,  # TODO make dynamic\n            ),\n        )\n\n    # local model\n    else:\n        if stream:\n            raise NotImplementedError(f\"Streaming not yet implemented for {llm_config.model_endpoint_type}\")\n        return get_chat_completion(\n            model=llm_config.model,\n            messages=messages,\n            functions=functions,\n            functions_python=functions_python,\n            function_call=function_call,\n            context_window=llm_config.context_window,\n            endpoint=llm_config.model_endpoint,\n            endpoint_type=llm_config.model_endpoint_type,\n            wrapper=llm_config.model_wrapper,\n            user=str(user_id),\n            # hint\n            # first_message=first_message,\n            # auth-related\n            auth_type=credentials.openllm_auth_type,\n            auth_key=credentials.openllm_key,\n        )\n"}
{"type": "source_file", "path": "luann/llm_api/anthropic.py", "content": "import json\nimport re\nimport uuid\nfrom typing import List, Optional, Union\n\nimport requests\nfrom constants import JSON_ENSURE_ASCII\nfrom data_types import Message\nfrom models.chat_completion_request import ChatCompletionRequest, Tool\nfrom models.chat_completion_response import (\n    ChatCompletionResponse,\n    Choice,\n    FunctionCall,\n)\nfrom models.chat_completion_response import (\n    Message as ChoiceMessage,  # NOTE: avoid conflict with our own typeagent Message datatype\n)\nfrom models.chat_completion_response import ToolCall, UsageStatistics\nfrom utils import get_utc_time, smart_urljoin\n\nBASE_URL = \"https://api.anthropic.com/v1\"\n\n\n# https://docs.anthropic.com/claude/docs/models-overview\n# Sadly hardcoded\nMODEL_LIST = [\n    {\n        \"name\": \"claude-3-opus-20240229\",\n        \"context_window\": 200000,\n    },\n    {\n        \"name\": \"claude-3-sonnet-20240229\",\n        \"context_window\": 200000,\n    },\n    {\n        \"name\": \"claude-3-haiku-20240307\",\n        \"context_window\": 200000,\n    },\n]\n\nDUMMY_FIRST_USER_MESSAGE = \"User initializing bootup sequence.\"\n\n\ndef antropic_get_model_context_window(url: str, api_key: Union[str, None], model: str) -> int:\n    for model_dict in anthropic_get_model_list(url=url, api_key=api_key):\n        if model_dict[\"name\"] == model:\n            return model_dict[\"context_window\"]\n    raise ValueError(f\"Can't find model '{model}' in Anthropic model list\")\n\n\ndef anthropic_get_model_list(url: str, api_key: Union[str, None]) -> dict:\n    \"\"\"https://docs.anthropic.com/claude/docs/models-overview\"\"\"\n\n    # NOTE: currently there is no GET /models, so we need to hardcode\n    return MODEL_LIST\n\n\ndef convert_tools_to_anthropic_format(tools: List[Tool], inner_thoughts_in_kwargs: Optional[bool] = True) -> List[dict]:\n    \"\"\"See: https://docs.anthropic.com/claude/docs/tool-use\n\n    OpenAI style:\n      \"tools\": [{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"find_movies\",\n            \"description\": \"find ....\",\n            \"parameters\": {\n              \"type\": \"object\",\n              \"properties\": {\n                 PARAM: {\n                   \"type\": PARAM_TYPE,  # eg \"string\"\n                   \"description\": PARAM_DESCRIPTION,\n                 },\n                 ...\n              },\n              \"required\": List[str],\n            }\n        }\n      }\n      ]\n\n    Anthropic style:\n      \"tools\": [{\n        \"name\": \"find_movies\",\n        \"description\": \"find ....\",\n        \"input_schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n             PARAM: {\n               \"type\": PARAM_TYPE,  # eg \"string\"\n               \"description\": PARAM_DESCRIPTION,\n             },\n             ...\n          },\n          \"required\": List[str],\n        }\n      }\n      ]\n\n      Two small differences:\n        - 1 level less of nesting\n        - \"parameters\" -> \"input_schema\"\n    \"\"\"\n    tools_dict_list = []\n    for tool in tools:\n        tools_dict_list.append(\n            {\n                \"name\": tool.function.name,\n                \"description\": tool.function.description,\n                \"input_schema\": tool.function.parameters,\n            }\n        )\n    return tools_dict_list\n\n\ndef merge_tool_results_into_user_messages(messages: List[dict]):\n    \"\"\"Anthropic API doesn't allow role 'tool'->'user' sequences\n\n    Example HTTP error:\n    messages: roles must alternate between \"user\" and \"assistant\", but found multiple \"user\" roles in a row\n\n    From: https://docs.anthropic.com/claude/docs/tool-use\n    You may be familiar with other APIs that return tool use as separate from the model's primary output,\n    or which use a special-purpose tool or function message role.\n    In contrast, Anthropic's models and API are built around alternating user and assistant messages,\n    where each message is an array of rich content blocks: text, image, tool_use, and tool_result.\n    \"\"\"\n\n    # TODO walk through the messages list\n    # When a dict (dict_A) with 'role' == 'user' is followed by a dict with 'role' == 'user' (dict B), do the following\n    # dict_A[\"content\"] = dict_A[\"content\"] + dict_B[\"content\"]\n\n    # The result should be a new merged_messages list that doesn't have any back-to-back dicts with 'role' == 'user'\n    merged_messages = []\n    if not messages:\n        return merged_messages\n\n    # Start with the first message in the list\n    current_message = messages[0]\n\n    for next_message in messages[1:]:\n        if current_message[\"role\"] == \"user\" and next_message[\"role\"] == \"user\":\n            # Merge contents of the next user message into current one\n            current_content = (\n                current_message[\"content\"]\n                if isinstance(current_message[\"content\"], list)\n                else [{\"type\": \"text\", \"text\": current_message[\"content\"]}]\n            )\n            next_content = (\n                next_message[\"content\"]\n                if isinstance(next_message[\"content\"], list)\n                else [{\"type\": \"text\", \"text\": next_message[\"content\"]}]\n            )\n            merged_content = current_content + next_content\n            current_message[\"content\"] = merged_content\n        else:\n            # Append the current message to result as it's complete\n            merged_messages.append(current_message)\n            # Move on to the next message\n            current_message = next_message\n\n    # Append the last processed message to the result\n    merged_messages.append(current_message)\n\n    return merged_messages\n\n\ndef remap_finish_reason(stop_reason: str) -> str:\n    \"\"\"Remap Anthropic's 'stop_reason' to OpenAI 'finish_reason'\n\n    OpenAI: 'stop', 'length', 'function_call', 'content_filter', null\n    see: https://platform.openai.com/docs/guides/text-generation/chat-completions-api\n\n    From: https://docs.anthropic.com/claude/reference/migrating-from-text-completions-to-messages#stop-reason\n\n    Messages have a stop_reason of one of the following values:\n        \"end_turn\": The conversational turn ended naturally.\n        \"stop_sequence\": One of your specified custom stop sequences was generated.\n        \"max_tokens\": (unchanged)\n\n    \"\"\"\n    if stop_reason == \"end_turn\":\n        return \"stop\"\n    elif stop_reason == \"stop_sequence\":\n        return \"stop\"\n    elif stop_reason == \"max_tokens\":\n        return \"length\"\n    elif stop_reason == \"tool_use\":\n        return \"function_call\"\n    else:\n        raise ValueError(f\"Unexpected stop_reason: {stop_reason}\")\n\n\ndef strip_xml_tags(string: str, tag: Optional[str]) -> str:\n    if tag is None:\n        return string\n    # Construct the regular expression pattern to find the start and end tags\n    tag_pattern = f\"<{tag}.*?>|</{tag}>\"\n    # Use the regular expression to replace the tags with an empty string\n    return re.sub(tag_pattern, \"\", string)\n\n\ndef convert_anthropic_response_to_chatcompletion(\n    response_json: dict,  # REST response from Google AI API\n    inner_thoughts_xml_tag: Optional[str] = None,\n) -> ChatCompletionResponse:\n    \"\"\"\n    Example response from Claude 3:\n    response.json = {\n        'id': 'msg_01W1xg9hdRzbeN2CfZM7zD2w',\n        'type': 'message',\n        'role': 'assistant',\n        'content': [\n            {\n                'type': 'text',\n                'text': \"<thinking>Analyzing user login event. This is Chad's first\n    interaction with me. I will adjust my personality and rapport accordingly.</thinking>\"\n            },\n            {\n                'type':\n                'tool_use',\n                'id': 'toolu_01Ka4AuCmfvxiidnBZuNfP1u',\n                'name': 'core_memory_append',\n                'input': {\n                    'name': 'human',\n                    'content': 'Chad is logging in for the first time. I will aim to build a warm\n    and welcoming rapport.',\n                    'request_heartbeat': True\n                }\n            }\n        ],\n        'model': 'claude-3-haiku-20240307',\n        'stop_reason': 'tool_use',\n        'stop_sequence': None,\n        'usage': {\n            'input_tokens': 3305,\n            'output_tokens': 141\n        }\n    }\n    \"\"\"\n    prompt_tokens = response_json[\"usage\"][\"input_tokens\"]\n    completion_tokens = response_json[\"usage\"][\"output_tokens\"]\n\n    finish_reason = remap_finish_reason(response_json[\"stop_reason\"])\n\n    if isinstance(response_json[\"content\"], list):\n        # inner mono + function call\n        # TODO relax asserts\n        assert len(response_json[\"content\"]) == 2, response_json\n        assert response_json[\"content\"][0][\"type\"] == \"text\", response_json\n        assert response_json[\"content\"][1][\"type\"] == \"tool_use\", response_json\n        content = strip_xml_tags(string=response_json[\"content\"][0][\"text\"], tag=inner_thoughts_xml_tag)\n        tool_calls = [\n            ToolCall(\n                id=response_json[\"content\"][1][\"id\"],\n                type=\"function\",\n                function=FunctionCall(\n                    name=response_json[\"content\"][1][\"name\"],\n                    # arguments=json.dumps(response_json[\"content\"][1][\"input\"]),\n                    arguments=json.dumps(response_json[\"content\"][1][\"input\"], ensure_ascii=JSON_ENSURE_ASCII),\n                ),\n            )\n        ]\n    else:\n        # just inner mono\n        content = strip_xml_tags(string=response_json[\"content\"], tag=inner_thoughts_xml_tag)\n        tool_calls = None\n\n    assert response_json[\"role\"] == \"assistant\", response_json\n    choice = Choice(\n        index=0,\n        finish_reason=finish_reason,\n        message=ChoiceMessage(\n            role=response_json[\"role\"],\n            content=content,\n            tool_calls=tool_calls,\n        ),\n    )\n\n    return ChatCompletionResponse(\n        id=response_json[\"id\"],\n        choices=[choice],\n        created=get_utc_time(),\n        model=response_json[\"model\"],\n        usage=UsageStatistics(\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            total_tokens=prompt_tokens + completion_tokens,\n        ),\n    )\n\n\ndef anthropic_chat_completions_request(\n    url: str,\n    api_key: str,\n    data: ChatCompletionRequest,\n    inner_thoughts_xml_tag: Optional[str] = \"thinking\",\n) -> ChatCompletionResponse:\n    \"\"\"https://docs.anthropic.com/claude/docs/tool-use\"\"\"\n    from utils import printd\n\n    url = smart_urljoin(url, \"messages\")\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"x-api-key\": api_key,\n        # NOTE: beta headers for tool calling\n        \"anthropic-version\": \"2023-06-01\",\n        \"anthropic-beta\": \"tools-2024-04-04\",\n    }\n\n    # convert the tools\n    anthropic_tools = None if data.tools is None else convert_tools_to_anthropic_format(data.tools)\n\n    # pydantic -> dict\n    data = data.model_dump(exclude_none=True)\n\n    if \"functions\" in data:\n        raise ValueError(f\"'functions' unexpected in Anthropic API payload\")\n\n    # If tools == None, strip from the payload\n    if \"tools\" in data and data[\"tools\"] is None:\n        data.pop(\"tools\")\n        data.pop(\"tool_choice\", None)  # extra safe,  should exist always (default=\"auto\")\n    # Remap to our converted tools\n    if anthropic_tools is not None:\n        data[\"tools\"] = anthropic_tools\n\n    # Move 'system' to the top level\n    # 'messages: Unexpected role \"system\". The Messages API accepts a top-level `system` parameter, not \"system\" as an input message role.'\n    assert data[\"messages\"][0][\"role\"] == \"system\", f\"Expected 'system' role in messages[0]:\\n{data['messages'][0]}\"\n    data[\"system\"] = data[\"messages\"][0][\"content\"]\n    data[\"messages\"] = data[\"messages\"][1:]\n\n    # Convert to Anthropic format\n    msg_objs = [Message.dict_to_message(user_id=uuid.uuid4(), agent_id=uuid.uuid4(), openai_message_dict=m) for m in data[\"messages\"]]\n    data[\"messages\"] = [m.to_anthropic_dict(inner_thoughts_xml_tag=inner_thoughts_xml_tag) for m in msg_objs]\n\n    # Handling Anthropic special requirement for 'user' message in front\n    # messages: first message must use the \"user\" role'\n    if data[\"messages\"][0][\"role\"] != \"user\":\n        data[\"messages\"] = [{\"role\": \"user\", \"content\": DUMMY_FIRST_USER_MESSAGE}] + data[\"messages\"]\n\n    # Handle Anthropic's restriction on alternating user/assistant messages\n    data[\"messages\"] = merge_tool_results_into_user_messages(data[\"messages\"])\n\n    # Anthropic also wants max_tokens in the input\n    # It's also part of ChatCompletions\n    assert \"max_tokens\" in data, data\n\n    # Remove extra fields used by OpenAI but not Anthropic\n    data.pop(\"frequency_penalty\", None)\n    data.pop(\"logprobs\", None)\n    data.pop(\"n\", None)\n    data.pop(\"top_p\", None)\n    data.pop(\"presence_penalty\", None)\n    data.pop(\"user\", None)\n    data.pop(\"tool_choice\", None)\n\n    printd(f\"Sending request to {url}\")\n    try:\n        response = requests.post(url, headers=headers, json=data)\n        printd(f\"response = {response}\")\n        response.raise_for_status()  # Raises HTTPError for 4XX/5XX status\n        response = response.json()  # convert to dict from string\n        printd(f\"response.json = {response}\")\n        response = convert_anthropic_response_to_chatcompletion(response_json=response, inner_thoughts_xml_tag=inner_thoughts_xml_tag)\n        return response\n    except requests.exceptions.HTTPError as http_err:\n        # Handle HTTP errors (e.g., response 4XX, 5XX)\n        printd(f\"Got HTTPError, exception={http_err}, payload={data}\")\n        raise http_err\n    except requests.exceptions.RequestException as req_err:\n        # Handle other requests-related errors (e.g., connection error)\n        printd(f\"Got RequestException, exception={req_err}\")\n        raise req_err\n    except Exception as e:\n        # Handle other potential errors\n        printd(f\"Got unknown Exception, exception={e}\")\n        raise e\n"}
