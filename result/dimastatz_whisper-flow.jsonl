{"repo_info": {"repo_name": "whisper-flow", "repo_owner": "dimastatz", "repo_url": "https://github.com/dimastatz/whisper-flow"}}
{"type": "test_file", "path": "tests/audio/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/audio/test_audio.py", "content": "\"\"\" test chat room \"\"\"\n\nimport queue\nimport asyncio\nimport pytest\nimport numpy as np\nimport whisperflow.audio.microphone as mic\n\n\n@pytest.mark.asyncio\nasync def test_capture_mic():\n    \"\"\"test capturing microphone\"\"\"\n    stop_event = asyncio.Event()\n    audio_chunks = queue.Queue()\n\n    async def stop_capturing():\n        await asyncio.sleep(0.1)\n        stop_event.set()\n\n    await asyncio.gather(mic.capture_audio(audio_chunks, stop_event), stop_capturing())\n    assert stop_event.is_set()\n    assert not audio_chunks.empty()\n\n\ndef test_is_silent():\n    \"\"\"test silence detection\"\"\"\n    silence_threshold = 500\n    # Create a silent audio buffer (all zeros)\n    silent_data = np.zeros(1024, dtype=np.int16).tobytes()\n    assert mic.is_silent(silent_data), \"Silent data should be detected as silent\"\n\n    # Create a loud audio buffer (above threshold)\n    loud_data = (np.ones(1024, dtype=np.int16) * (silence_threshold + 1000)).tobytes()\n    assert not mic.is_silent(loud_data), \"Loud data should not be detected as silent\"\n\n    # Create a borderline case (right at the threshold)\n    threshold_data = (np.ones(1024, dtype=np.int16) * silence_threshold).tobytes()\n    assert not mic.is_silent(\n        threshold_data\n    ), \"Threshold-level data should not be detected as silent\"\n\n\n@pytest.mark.asyncio\nasync def test_play_audio():\n    \"\"\"\n    Test the play_audio function by adding dummy audio data to a queue,\n    running the function, and ensuring the queue is empty after processing.\n    \"\"\"\n    queue_chunks = queue.Queue()\n    stop_event = asyncio.Event()\n\n    # Add some dummy audio data to the queue\n    dummy_data = b\"\\x00\\x01\" * 1024  # 2MB per sample, 1024 samples\n\n    for _ in range(0, 10):\n        queue_chunks.put(dummy_data)\n\n    # Run play_audio in a separate task\n    play_task = asyncio.create_task(mic.play_audio(queue_chunks, stop_event))\n\n    # Allow some time for play_audio to process the queue\n    await asyncio.sleep(0.1)\n\n    # Stop the play_audio function\n    stop_event.set()\n    await play_task\n\n    # Check that the queue is empty after processing\n    assert not queue_chunks.empty()\n"}
{"type": "test_file", "path": "tests/benchmark/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/benchmark/test_benchmark.py", "content": "\"\"\"benchamrk\"\"\"\n\nimport json\nimport time\nimport pandas as pd\n\nimport requests\nimport jiwer as jw\nimport websocket as ws\nimport tests.utils as ut\n\n\ndef test_health(url=\"http://localhost:8181/health\"):\n    \"\"\"basic test\"\"\"\n    result = requests.get(url=url, timeout=1)\n    assert result.status_code == 200\n\n\ndef get_res(websocket):\n    \"\"\"try read with timout\"\"\"\n    try:\n        result = json.loads(websocket.recv())\n        print_result(result)\n        return result\n    except ws.WebSocketTimeoutException:\n        return {}\n\n\ndef print_result(result: dict):\n    \"\"\"print result and execution time\"\"\"\n    print(result[\"is_partial\"], round(result[\"time\"], 2), result[\"data\"][\"text\"])\n\n\ndef test_send_chunks(url=\"ws://localhost:8181/ws\", chunk_size=4096):\n    \"\"\"send chunks\"\"\"\n    websocket = ws.create_connection(url)\n    websocket.settimeout(0.1)\n\n    resource = ut.load_resource(\"3081-166546-0000\")\n    chunks = [\n        resource[\"audio\"][i : i + chunk_size]\n        for i in range(0, len(resource[\"audio\"]), chunk_size)\n    ]\n\n    df_result = pd.DataFrame(columns=[\"is_partial\", \"latency\", \"result\"])\n    for chunk in chunks:\n        websocket.send_bytes(chunk)\n        res = get_res(websocket)\n        if res:\n            df_result.loc[len(df_result)] = [\n                res[\"is_partial\"],\n                round(res[\"time\"], 2),\n                res[\"data\"][\"text\"],\n            ]\n\n    attempts = 0\n    while attempts < 3:\n        res = get_res(websocket)\n        if res:\n            attempts = 0\n            df_result.loc[len(df_result)] = [\n                res[\"is_partial\"],\n                round(res[\"time\"], 2),\n                res[\"data\"][\"text\"],\n            ]\n        else:\n            attempts += 1\n            time.sleep(1)\n\n    pd.set_option(\"max_colwidth\", 800)\n    # print(df_result.to_string(justify='left', index=False))\n    print(\"Latency Stats:\\n\", df_result[\"latency\"].describe())\n\n    actual = df_result.loc[len(df_result) - 1][\"result\"].lower().strip()\n    expected = resource[\"expected\"][\"final_ground_truth\"].lower().strip()\n\n    error = round(jw.wer(actual, expected), 2)\n    assert error < 0.1\n    websocket.close()\n\n\nif __name__ == \"__main__\":\n    print(\"Starting Whisper-Flow Benchmark\")\n    test_send_chunks()\n    print(\"Whisper-Flow Benchmark Completed\")\n"}
{"type": "test_file", "path": "tests/test_streaming.py", "content": "\"\"\" test scenario module \"\"\"\n\nimport asyncio\nfrom queue import Queue\n\nimport pytest\nimport tests.utils as ut\nimport whisperflow.streaming as st\nimport whisperflow.fast_server as fs\nimport whisperflow.transcriber as ts\n\n\n@pytest.mark.asyncio\nasync def test_simple():\n    \"\"\"test asyncio\"\"\"\n\n    queue, should_stop = Queue(), [False]\n    queue.put(1)\n\n    async def dummy_transcriber(items: list) -> dict:\n        await asyncio.sleep(0.1)\n        if queue.qsize() == 0:\n            should_stop[0] = True\n        return {\"text\": str(len(items))}\n\n    async def dummy_segment_closed(text: str) -> None:\n        await asyncio.sleep(0.01)\n        print(text)\n\n    await st.transcribe(should_stop, queue, dummy_transcriber, dummy_segment_closed)\n    assert queue.qsize() == 0\n\n\n@pytest.mark.asyncio\nasync def test_transcribe_streaming(chunk_size=4096):\n    \"\"\"test streaming\"\"\"\n\n    model = ts.get_model()\n    queue, should_stop = Queue(), [False]\n    res = ut.load_resource(\"3081-166546-0000\")\n    chunks = [\n        res[\"audio\"][i : i + chunk_size]\n        for i in range(0, len(res[\"audio\"]), chunk_size)\n    ]\n\n    async def dummy_transcriber(items: list) -> str:\n        await asyncio.sleep(0.01)\n        result = ts.transcribe_pcm_chunks(model, items)\n        return result\n\n    result = []\n\n    async def dummy_segment_closed(text: str) -> None:\n        await asyncio.sleep(0.01)\n        result.append(text)\n\n    task = asyncio.create_task(\n        st.transcribe(should_stop, queue, dummy_transcriber, dummy_segment_closed)\n    )\n\n    for chunk in chunks:\n        queue.put(chunk)\n        await asyncio.sleep(0.01)\n\n    await asyncio.sleep(1)\n    should_stop[0] = True\n    await task\n\n    assert len(result) > 0\n\n\ndef test_streaming():\n    \"\"\"test hugging face image generation\"\"\"\n    queue = Queue()\n    queue.put(1)\n    queue.put(2)\n    res = st.get_all(queue)\n    assert res == [1, 2]\n\n    res = st.get_all(None)\n    assert not res\n\n\n@pytest.mark.asyncio\n@pytest.mark.timeout(60)\nasync def test_ws(chunk_size=4096):\n    \"\"\"test health api\"\"\"\n    client = ut.TestClient(fs.app)\n    with client.websocket_connect(\"/ws\") as websocket:\n        res = ut.load_resource(\"3081-166546-0000\")\n        chunks = [\n            res[\"audio\"][i : i + chunk_size]\n            for i in range(0, len(res[\"audio\"]), chunk_size)\n        ]\n\n        for chunk in chunks:\n            websocket.send_bytes(chunk)\n\n        await asyncio.sleep(3)\n        websocket.close()\n\n    assert client\n"}
{"type": "test_file", "path": "tests/examples/mic_transcribe.py", "content": "\"\"\" \na test app that streams \naudio  from the mic to whisper flow\nrequires pip install PyAudio\n\"\"\"\n\nimport json\nimport asyncio\nimport pyaudio\nimport websockets\n\n\nasync def start_transcription(url=\"ws://0.0.0.0:8181/ws\"):\n    \"\"\"stream mic audio to server\"\"\"\n    async with websockets.connect(url) as websocket:\n        result = []\n        await asyncio.gather(\n            capture_audio(websocket, result), receive_transcription(websocket, result)\n        )\n        print(f\"* done recording, collecting data\")\n        print(\"Colllected text is \\n\", \" \".join(result))\n\n\nasync def capture_audio(websocket: websockets.WebSocketClientProtocol, result: list):\n    \"\"\"capture the mic stream\"\"\"\n    chunk, rate, record_sec = 1024, 16000, 30\n    p = pyaudio.PyAudio()\n    stream = p.open(\n        format=pyaudio.paInt16,\n        channels=1,\n        rate=rate,\n        input=True,\n        frames_per_buffer=chunk,\n    )\n    print(\"* recording\")\n\n    for _ in range(0, int(rate / chunk * record_sec)):\n        data = stream.read(chunk)\n        await websocket.send(data)\n        await asyncio.sleep(0.01)\n\n    stream.close()\n    p.terminate()\n\n\nasync def receive_transcription(websocket, result: list):\n    \"\"\"print transcription\"\"\"\n    while True:\n        try:\n            await asyncio.sleep(0.01)\n            tmp = json.loads(await websocket.recv())\n            if not tmp[\"is_partial\"]:\n                result.append(tmp[\"data\"][\"text\"])\n            print(tmp[\"is_partial\"], round(tmp[\"time\"], 2), tmp[\"data\"][\"text\"])\n        except Exception:\n            print(\"No transcription available\")\n\n\nasyncio.run(start_transcription())\n"}
{"type": "test_file", "path": "tests/test_chat_room.py", "content": "\"\"\" test chat room \"\"\"\n\nimport queue\nimport asyncio\nimport pytest\n\nfrom whisperflow.chat_room import ChatRoom\n\n\nasync def listener_mock(queue_in: queue.Queue, stop_event: asyncio.Event):\n    \"\"\"collect items from queue\"\"\"\n    while not stop_event.is_set():\n        await asyncio.sleep(0.1)\n        queue_in.put(\"hello\")\n\n\nasync def processor_mock(queue_in, queue_out, stop_event):\n    \"\"\"collect items from queue\"\"\"\n    while not stop_event.is_set():\n        await asyncio.sleep(0.1)\n        if not queue_in.empty():\n            item = queue_in.get()\n            queue_out.put(item)\n\n\nasync def speaker_mock(queue_in: queue.Queue, stop_event: asyncio.Event):\n    \"\"\"mock playing sound\"\"\"\n    while not stop_event.is_set():\n        await asyncio.sleep(0.1)\n        if not queue_in.empty():\n            item = queue_in.get()\n            assert item is not None\n\n\n@pytest.mark.asyncio\nasync def test_chat_room():\n    \"\"\"mock playing sound\"\"\"\n    room = ChatRoom(listener_mock, speaker_mock, processor_mock)\n\n    async def stop_chat():\n        await asyncio.sleep(1)\n        room.stop_chat()\n\n    await asyncio.gather(room.start_chat(), stop_chat())\n    assert room.stop_chat_event.is_set()\n"}
{"type": "test_file", "path": "tests/test_transcriber.py", "content": "\"\"\" test transcriber \"\"\"\n\nimport pytest\nfrom jiwer import wer\nimport tests.utils as ut\n\nimport whisperflow.fast_server as fr\nimport whisperflow.transcriber as tr\n\n\ndef test_load_model():\n    \"\"\"test load model from disl\"\"\"\n    model = tr.get_model()\n    assert model is not None\n\n    resource = ut.load_resource(\"3081-166546-0000\")\n\n    result = tr.transcribe_pcm_chunks(model, [resource[\"audio\"]])\n    expected = resource[\"expected\"][\"final_ground_truth\"]\n\n    error = wer(result[\"text\"].lower(), expected.lower())\n    assert error < 0.1\n\n\ndef test_transcribe_chunk():\n    \"\"\"test transcribe pcm chunk\"\"\"\n    resource = ut.load_resource(\"3081-166546-0000\")\n    client = ut.TestClient(fr.app)\n    response = client.get(\"/health\")\n    assert response.status_code == 200\n\n    path = ut.get_resource_path(\"3081-166546-0000\", \"wav\")\n    with open(path, \"br\") as file:\n        response = client.post(\n            url=\"/transcribe_pcm_chunk\",\n            data={\"model_name\": \"tiny.en.pt\"},\n            files=[(\"files\", file)],\n        )\n\n    assert response.status_code == 200\n\n    expected = resource[\"expected\"][\"final_ground_truth\"]\n    error = wer(response.json()[\"text\"].lower(), expected.lower())\n    assert error < 0.1\n\n\n@pytest.mark.asyncio\nasync def test_transcribe_chunk_async():\n    \"\"\"test transcribe async\"\"\"\n    model = tr.get_model()\n    assert model is not None\n    resource = ut.load_resource(\"3081-166546-0000\")\n    result = await tr.transcribe_pcm_chunks_async(model, [resource[\"audio\"]])\n    expected = resource[\"expected\"][\"final_ground_truth\"]\n    error = wer(result[\"text\"].lower(), expected.lower())\n    assert error < 0.1\n"}
{"type": "test_file", "path": "tests/utils.py", "content": "\"\"\" test utils class \"\"\"\n\nimport os\nimport json\nfrom starlette.testclient import TestClient\nimport whisperflow.fast_server as fs\n\n\ndef get_resource_path(name: str, extension: str) -> str:\n    \"get resources path\"\n    current_path = os.path.dirname(__file__)\n    path = os.path.join(current_path, f\"./resources/{name}\")\n    return f\"{path}.{extension}\"\n\n\ndef load_resource(name: str) -> dict:\n    \"load resource\"\n    result = {}\n\n    with open(get_resource_path(name, \"wav\"), \"br\") as file:\n        result[\"audio\"] = file.read()\n\n    with open(get_resource_path(name, \"json\"), \"r\", encoding=\"utf-8\") as file:\n        result[\"expected\"] = json.load(file)\n\n    return result\n\n\ndef test_fast_api():\n    \"\"\"test health api\"\"\"\n    with TestClient(fs.app) as client:\n        response = client.get(\"/health\")\n        assert response.status_code == 200 and bool(response.text)\n"}
{"type": "source_file", "path": "whisperflow/chat_room.py", "content": "\"\"\" \nImplements conversation loop: capture \naudio -> speech to text -> custom action -> text to speech -> play audio \n\"\"\"\n\nimport queue\nimport asyncio\nimport pytest\nimport whisperflow.audio.microphone as mic\n\n\nclass ChatRoom:\n    \"\"\"\n    A class enabling real-time communication with microphone input and speaker output.\n    It supports speech-to-text (STT) and text-to-speech (TTS)\n    processing, with an optional handler for custom text analysis.\n    \"\"\"\n\n    def __init__(self, listener, speaker, processor):\n        self.audio_in = queue.Queue()\n        self.audio_out = queue.Queue()\n        self.listener = listener\n        self.speaker = speaker\n        self.processor = processor\n        self.stop_chat_event = asyncio.Event()\n\n    async def start_chat(self):\n        \"\"\"start chat by listening to mic\"\"\"\n        self.stop_chat_event.clear()\n\n        # start listener and processor\n        await asyncio.gather(\n            self.listener(self.audio_in, self.stop_chat_event),\n            self.processor(self.audio_in, self.audio_out, self.stop_chat_event),\n            self.speaker(self.audio_out, self.stop_chat_event),\n        )\n\n    def stop_chat(self):\n        \"\"\"stop chat and release resources\"\"\"\n        self.stop_chat_event.set()\n        assert self.stop_chat_event.is_set()\n\n\n@pytest.mark.skip(reason=\"requires audio hardware\")\ndef main():  # pragma: no cover\n    \"\"\"main function that runs the chat room\"\"\"\n\n    # Create a dummy processor\n    async def dummy_proc(\n        audio_in: queue.Queue, audio_out: queue.Queue, stop: asyncio.Event\n    ):\n        \"\"\"dummy processor\"\"\"\n        while not stop.is_set():\n            if not audio_in.empty():\n                data = audio_in.get()\n                audio_out.put(data)\n            await asyncio.sleep(0.001)\n\n    chat_room = ChatRoom(mic.capture_audio, mic.play_audio, dummy_proc)\n\n    try:\n        # Run the async main function\n        chat_room.start_chat()\n    except KeyboardInterrupt:\n        chat_room.stop_chat()\n        print(\"Chat stopped\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "setup.py", "content": "from pathlib import Path\nfrom setuptools import setup\nfrom whisperflow import __version__\nfrom pkg_resources import parse_requirements\n\n\nthis_directory = Path(__file__).parent\nlong_description = (this_directory / \"README.md\").read_text()\n\nsetup(\n    name='whisperflow',\n    version=__version__,\n    url='https://github.com/dimastatz/whisper-flow',\n    author='Dima Statz',\n    author_email='dima.statz@gmail.com',\n    py_modules=['whisperflow'],\n    python_requires=\">=3.8\",\n    install_requires=[\n        str(r)\n        for r in parse_requirements(\n            Path(__file__).with_name(\"requirements.txt\").open()\n        )\n    ],\n    description='WhisperFlow: Real-Time Transcription Powered by OpenAI Whisper',\n    long_description = long_description,\n    long_description_content_type='text/markdown',\n    include_package_data=True,\n    package_data={'': ['static/*']},\n)\n"}
{"type": "source_file", "path": "whisperflow/audio/microphone.py", "content": "\"\"\"\ncapture audio from microphone\n\"\"\"\n\nimport queue\nimport asyncio\nimport pyaudio\nimport numpy as np\n\n\nasync def capture_audio(\n    queue_chunks: queue.Queue, stop_event: asyncio.Event\n):  # pragma: no cover\n    \"\"\"capture the mic stream\"\"\"\n    chunk, rate = 1024, 16000\n    audio = pyaudio.PyAudio()\n    stream = audio.open(\n        format=pyaudio.paInt16,\n        channels=1,\n        rate=rate,\n        input=True,\n        frames_per_buffer=chunk,\n    )\n\n    while not stop_event.is_set():\n        data = stream.read(chunk)\n        queue_chunks.put_nowait(data)\n        await asyncio.sleep(0.001)\n\n    stream.close()\n    audio.terminate()\n\n\nasync def play_audio(\n    queue_chunks: queue.Queue, stop_event: asyncio.Event\n):  # pragma: no cover\n    \"\"\"play audio from queue\"\"\"\n    chunk, rate = 1024, 16000\n    audio = pyaudio.PyAudio()\n    stream = audio.open(\n        format=pyaudio.paInt16,\n        channels=1,\n        rate=rate,\n        output=True,\n        frames_per_buffer=chunk,\n    )\n\n    while not stop_event.is_set():\n        if not queue_chunks.empty():\n            data = queue_chunks.get()\n            stream.write(data)\n        await asyncio.sleep(0.001)\n\n    stream.close()\n    audio.terminate()\n\n\ndef is_silent(data, silence_threshold=500):  # pragma: no cover\n    \"\"\"is chunk is silence\"\"\"\n    return np.max(np.frombuffer(data, dtype=np.int16)) < silence_threshold\n"}
{"type": "source_file", "path": "whisperflow/__init__.py", "content": "\"\"\" add package version \"\"\"\n\n__version__ = \"1.0.0\"\n"}
{"type": "source_file", "path": "whisperflow/fast_server.py", "content": "\"\"\" fast api declaration \"\"\"\n\nimport logging\nfrom typing import List\nfrom fastapi import FastAPI, WebSocket, Form, File, UploadFile\n\nfrom whisperflow import __version__\nimport whisperflow.streaming as st\nimport whisperflow.transcriber as ts\n\n\napp = FastAPI()\nsessions = {}\n\n\n@app.get(\"/health\", response_model=str)\ndef health():\n    \"\"\"health function on API\"\"\"\n    return f\"Whisper Flow V{__version__}\"\n\n\n@app.post(\"/transcribe_pcm_chunk\", response_model=dict)\ndef transcribe_pcm_chunk(\n    model_name: str = Form(...), files: List[UploadFile] = File(...)\n):\n    \"\"\"transcribe chunk\"\"\"\n    model = ts.get_model(model_name)\n    content = files[0].file.read()\n    return ts.transcribe_pcm_chunks(model, [content])\n\n\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    \"\"\"webscoket implementation\"\"\"\n    model = ts.get_model()\n\n    async def transcribe_async(chunks: list):\n        return await ts.transcribe_pcm_chunks_async(model, chunks)\n\n    async def send_back_async(data: dict):\n        await websocket.send_json(data)\n\n    try:\n        await websocket.accept()\n        session = st.TranscribeSession(transcribe_async, send_back_async)\n        sessions[session.id] = session\n\n        while True:\n            data = await websocket.receive_bytes()\n            session.add_chunk(data)\n    except Exception as exception:  # pylint: disable=broad-except\n        logging.error(exception)\n        await session.stop()\n        await websocket.close()\n"}
{"type": "source_file", "path": "whisperflow/transcriber.py", "content": "\"\"\" transcriber \"\"\"\n\nimport os\nimport asyncio\n\nimport torch\nimport numpy as np\n\nimport whisper\nfrom whisper import Whisper\n\n\nmodels = {}\n\n\ndef get_model(file_name=\"tiny.en.pt\") -> Whisper:\n    \"\"\"load models from disk\"\"\"\n    if file_name not in models:\n        path = os.path.join(os.path.dirname(__file__), f\"./models/{file_name}\")\n        models[file_name] = whisper.load_model(path).to(\n            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        )\n    return models[file_name]\n\n\ndef transcribe_pcm_chunks(\n    model: Whisper, chunks: list, lang=\"en\", temperature=0.1, log_prob=-0.5\n) -> dict:\n    \"\"\"transcribes pcm chunks list\"\"\"\n    arr = (\n        np.frombuffer(b\"\".join(chunks), np.int16).flatten().astype(np.float32) / 32768.0\n    )\n    return model.transcribe(\n        arr,\n        fp16=False,\n        language=lang,\n        logprob_threshold=log_prob,\n        temperature=temperature,\n    )\n\n\nasync def transcribe_pcm_chunks_async(\n    model: Whisper, chunks: list, lang=\"en\", temperature=0.1, log_prob=-0.5\n) -> dict:\n    \"\"\"transcribes pcm chunks async\"\"\"\n    return await asyncio.get_running_loop().run_in_executor(\n        None, transcribe_pcm_chunks, model, chunks, lang, temperature, log_prob\n    )\n"}
{"type": "source_file", "path": "whisperflow/streaming.py", "content": "\"\"\" test scenario module \"\"\"\n\nimport time\nimport uuid\nimport asyncio\nfrom queue import Queue\nfrom typing import Callable\n\n\ndef get_all(queue: Queue) -> list:\n    \"\"\"get_all from queue\"\"\"\n    res = []\n    while queue and not queue.empty():\n        res.append(queue.get())\n    return res\n\n\nasync def transcribe(\n    should_stop: list,\n    queue: Queue,\n    transcriber: Callable[[list], str],\n    segment_closed: Callable[[dict], None],\n):\n    \"\"\"the transcription loop\"\"\"\n    window, prev_result, cycles = [], {}, 0\n\n    while not should_stop[0]:\n        start = time.time()\n        await asyncio.sleep(0.01)\n        window.extend(get_all(queue))\n\n        if not window:\n            continue\n\n        result = {\n            \"is_partial\": True,\n            \"data\": await transcriber(window),\n            \"time\": (time.time() - start) * 1000,\n        }\n\n        if should_close_segment(result, prev_result, cycles):\n            window, prev_result, cycles = [], {}, 0\n            result[\"is_partial\"] = False\n        elif result[\"data\"][\"text\"] == prev_result.get(\"data\", {}).get(\"text\", \"\"):\n            cycles += 1\n        else:\n            cycles = 0\n            prev_result = result\n\n        if result[\"data\"][\"text\"]:\n            await segment_closed(result)\n\n\ndef should_close_segment(result: dict, prev_result: dict, cycles, max_cycles=1):\n    \"\"\"return if segment should be closed\"\"\"\n    return cycles >= max_cycles and result[\"data\"][\"text\"] == prev_result.get(\n        \"data\", {}\n    ).get(\"text\", \"\")\n\n\nclass TranscribeSession:  # pylint: disable=too-few-public-methods\n    \"\"\"transcription state\"\"\"\n\n    def __init__(self, transcribe_async, send_back_async) -> None:\n        \"\"\"ctor\"\"\"\n        self.id = uuid.uuid4()  # pylint: disable=invalid-name\n        self.queue = Queue()\n        self.should_stop = [False]\n        self.task = asyncio.create_task(\n            transcribe(self.should_stop, self.queue, transcribe_async, send_back_async)\n        )\n\n    def add_chunk(self, chunk: bytes):\n        \"\"\"add new chunk\"\"\"\n        self.queue.put_nowait(chunk)\n\n    async def stop(self):\n        \"\"\"stop session\"\"\"\n        self.should_stop[0] = True\n        await self.task\n"}
