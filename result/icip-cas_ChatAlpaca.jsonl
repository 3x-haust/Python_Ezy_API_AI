{"repo_info": {"repo_name": "ChatAlpaca", "repo_owner": "icip-cas", "repo_url": "https://github.com/icip-cas/ChatAlpaca"}}
{"type": "source_file", "path": "train/train_hf.py", "content": "# Make it more memory efficient by monkey patching the LLaMA model with FlashAttn.\n\nfrom fastchat.train.llama_flash_attn_monkey_patch import (\n    replace_llama_attn_with_flash_attn,\n)\n\nreplace_llama_attn_with_flash_attn()\n\nfrom dataclasses import dataclass, field\nimport json\nimport pathlib\nfrom typing import Dict, Optional, Sequence\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport transformers\nfrom transformers import Trainer\nfrom transformers.trainer_pt_utils import LabelSmoother\n\nfrom fastchat.conversation import SeparatorStyle\nfrom fastchat.model.model_adapter import get_conversation_template\n\nIGNORE_TOKEN_ID = LabelSmoother.ignore_index\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the training data.\"}\n    )\n    lazy_preprocess: bool = False\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=512,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n\n\nlocal_rank = None\n\n\ndef rank0_print(*args):\n    if local_rank == 0:\n        print(*args)\n\n\ndef safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n    state_dict = trainer.model.state_dict()\n    if trainer.args.should_save:\n        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n        del state_dict\n        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n\n\ndef preprocess(\n    sources,\n    tokenizer: transformers.PreTrainedTokenizer,\n) -> Dict:\n    conv = get_conversation_template(\"vicuna\")\n    roles = {\"user\": conv.roles[0], \"assistant\": conv.roles[1]}\n\n    # Apply prompt templates\n    conversations = []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != conv.roles[0]:\n            # Skip the first one if it is not from human\n            source = source[1:]\n\n        conv.messages = []\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            assert role == conv.roles[j % 2], f\"{i}\"\n            conv.append_message(role, sentence[\"value\"])\n        conversations.append(conv.get_prompt())\n\n    # Tokenize conversations\n    input_ids = tokenizer(\n        conversations,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        max_length=tokenizer.model_max_length,\n        truncation=True,\n    ).input_ids\n    targets = input_ids.clone()\n\n    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n\n    # Mask targets\n    sep = conv.sep + conv.roles[1] + \": \"\n    for conversation, target in zip(conversations, targets):\n        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n\n        rounds = conversation.split(conv.sep2)\n        cur_len = 1\n        target[:cur_len] = IGNORE_TOKEN_ID\n        for i, rou in enumerate(rounds):\n            if rou == \"\":\n                break\n\n            parts = rou.split(sep)\n            if len(parts) != 2:\n                break\n            parts[0] += sep\n            round_len = len(tokenizer(rou).input_ids)\n            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n\n            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n\n            cur_len += round_len\n        target[cur_len:] = IGNORE_TOKEN_ID\n\n        if False:\n            z = target.clone()\n            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n            rank0_print(tokenizer.decode(z))\n\n        if cur_len < tokenizer.model_max_length:\n            if cur_len != total_len:\n                target[:] = IGNORE_TOKEN_ID\n                rank0_print(\n                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n                    f\" (ignored)\"\n                )\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n    )\n\n\nclass SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n        super(SupervisedDataset, self).__init__()\n\n        rank0_print(\"Formatting inputs...\")\n        sources = [example[\"conversations\"] for example in raw_data]\n        data_dict = preprocess(sources, tokenizer)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n        self.attention_mask = data_dict[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(\n            input_ids=self.input_ids[i],\n            labels=self.labels[i],\n            attention_mask=self.attention_mask[i],\n        )\n\n\nclass LazySupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n        super(LazySupervisedDataset, self).__init__()\n        self.tokenizer = tokenizer\n\n        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n        self.tokenizer = tokenizer\n        self.raw_data = raw_data\n        self.cached_data_dict = {}\n\n    def __len__(self):\n        return len(self.raw_data)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        if i in self.cached_data_dict:\n            return self.cached_data_dict[i]\n\n        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer)\n        ret = dict(\n            input_ids=ret[\"input_ids\"][0],\n            labels=ret[\"labels\"][0],\n            attention_mask=ret[\"attention_mask\"][0],\n        )\n        self.cached_data_dict[i] = ret\n\n        return ret\n\n\ndef make_supervised_data_module(\n    tokenizer: transformers.PreTrainedTokenizer, data_args\n) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    dataset_cls = (\n        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n    )\n    rank0_print(\"Loading data...\")\n\n    raw_data = []\n    with open(data_args.data_path,\"r\") as f:\n        for line in f:\n            raw_data.append(json.loads(line))\n\n    # Split train/test\n    perm = np.random.permutation(len(raw_data))\n    split = int(len(perm) * 0.98)\n    train_indices = perm[:split]\n    eval_indices = perm[split:]\n    train_raw_data = [raw_data[i] for i in train_indices]\n    eval_raw_data = [raw_data[i] for i in eval_indices]\n    rank0_print(f\"#train {len(train_raw_data)}, #eval {len(eval_raw_data)}\")\n\n    train_dataset = dataset_cls(train_raw_data, tokenizer=tokenizer)\n    eval_dataset = dataset_cls(eval_raw_data, tokenizer=tokenizer)\n    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n\n\ndef train():\n    global local_rank\n\n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments)\n    )\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n    local_rank = training_args.local_rank\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n    )\n    model.config.use_cache = False\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n    tokenizer.pad_token = tokenizer.unk_token\n\n    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n\n    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n        trainer.train(resume_from_checkpoint=True)\n    else:\n        print('---------------------------eval_dataset----------------------------------------')\n        print(data_module['train_dataset'][0][\"input_ids\"])\n        print(tokenizer.decode(data_module['train_dataset'][10][\"input_ids\"]))\n        print('------------------------end-------------------------------------')\n        trainer.train()\n    trainer.save_state()\n    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n\n\nif __name__ == \"__main__\":\n    train()\n"}
{"type": "source_file", "path": "train/train_lora.py", "content": "import os\nimport sys\nfrom typing import List\n\nimport json\nimport fire\nimport argparse\nimport torch\nimport transformers\nfrom datasets import load_dataset\nfrom prompt import prompt_dict\nimport inspect\nimport copy\n\n\"\"\"\nUnused imports:\nimport torch.nn as nn\nimport bitsandbytes as bnb\n\"\"\"\n\n# Catch when user should re-install transformers library\nassert (\n        \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"  # noqa: E501\n\nfrom peft import (  # noqa: E402\n    LoraConfig,\n    get_peft_model,\n    get_peft_model_state_dict,\n    prepare_model_for_int8_training,\n    set_peft_model_state_dict,\n)\nfrom transformers import LlamaForCausalLM, LlamaTokenizer  # noqa: F402\n\n\ndef train(\n        # model/data params\n        base_model: str = \"\",  # the only required argument\n        data_path: str = \"\",\n        output_dir: str = \"\",\n        # training hyperparams\n        batch_size: int = 64,\n        micro_batch_size: int = 4,\n        num_epochs: int = 3,\n        learning_rate: float = 0.0003,\n        cutoff_len: int = 2048,\n        # lora hyperparams\n        lora_r: int = 8,\n        lora_alpha: int = 16,\n        lora_dropout: float = 0.05,\n        lora_target_modules: List[str] = [\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n        ],\n        # llm hyperparams\n        train_on_inputs: bool = True,  # if False, masks out inputs in loss\n        group_by_length: bool = False,  # faster, but produces an odd training loss curve\n        # wandb params\n        wandb_project: str = \"\",\n        wandb_run_name: str = \"\",\n        wandb_watch: str = \"\",  # options: false | gradients | all\n        wandb_log_model: str = \"\",  # options: false | true\n        resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n        prompt_style: str = 'vicuna',  # options: alpaca | oig | vicuna\n        save_steps=200,\n        local_rank: int = 0):\n    # base_model=args.base_model\n    # data_path=args.data_path\n    # output_dir=args.output_dir\n    # batch_size=args.batch_size\n    # micro_batch_size=args.micro_batch_size\n    # num_epochs=args.num_epochs\n    # learning_rate=args.learning_rate\n    # cutoff_len=args.cutoff_len\n    # lora_r=args.lora_r\n    # lora_alpha=args.lora_alpha\n    # lora_dropout=args.lora_dropout\n    # lora_target_modules=args.lora_target_modules\n    # train_on_inputs=args.train_on_inputs\n    # group_by_length=args.group_by_length\n    # wandb_project=args.wandb_project\n    # wandb_run_name=args.wandb_run_name\n    # wandb_watch=args.wandb_watch\n    # wandb_log_model=args.wandb_log_model\n    # resume_from_checkpoint=args.resume_from_checkpoint\n    # prompt_style=args.prompt_style\n    # save_steps=args.save_steps\n    print(\n        f\"Training chatAlpaca-LoRA model with params:\\n\"\n        f\"base_model: {base_model}\\n\"\n        f\"data_path: {data_path}\\n\"\n        f\"output_dir: {output_dir}\\n\"\n        f\"batch_size: {batch_size}\\n\"\n        f\"micro_batch_size: {micro_batch_size}\\n\"\n        f\"num_epochs: {num_epochs}\\n\"\n        f\"learning_rate: {learning_rate}\\n\"\n        f\"cutoff_len: {cutoff_len}\\n\"\n        f\"lora_r: {lora_r}\\n\"\n        f\"lora_alpha: {lora_alpha}\\n\"\n        f\"lora_dropout: {lora_dropout}\\n\"\n        f\"lora_target_modules: {lora_target_modules}\\n\"\n        f\"train_on_inputs: {train_on_inputs}\\n\"\n        f\"group_by_length: {group_by_length}\\n\"\n        f\"wandb_project: {wandb_project}\\n\"\n        f\"wandb_run_name: {wandb_run_name}\\n\"\n        f\"wandb_watch: {wandb_watch}\\n\"\n        f\"wandb_log_model: {wandb_log_model}\\n\"\n        f\"resume_from_checkpoint: {resume_from_checkpoint}\\n\"\n        f\"prompt_style: {prompt_style}\\n\"\n        f\"save_steps: {save_steps}\\n\"\n    )\n    assert (\n        base_model\n    ), \"Please specify a --base_model, e.g. --base_model='decapoda-research/llama-7b-hf'\"\n    gradient_accumulation_steps = batch_size // micro_batch_size\n\n    if int(os.environ.get(\"LOCAL_RANK\") or 0) == 0:\n        ### 保存运行参数\n        sig = inspect.signature(train)\n        param_names = [p.name for p in sig.parameters.values()]\n        param_value = locals()\n        # Create a dictionary with the parameter names and values\n        params = {name: param_value[name] for name in param_names}\n\n        if not os.path.exists(output_dir):\n            os.mkdir(output_dir)\n\n        # Save the dictionary as a JSON file\n        with open(output_dir + \"/params.json\", \"w\") as f:\n            json.dump(params, f)\n\n    device_map = \"auto\"\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    if ddp:\n        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n\n    # Check if parameter passed or if set within environ\n    use_wandb = len(wandb_project) > 0 or (\n            \"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0\n    )\n    # Only overwrite environ if wandb param passed\n    if len(wandb_project) > 0:\n        os.environ[\"WANDB_PROJECT\"] = wandb_project\n    if len(wandb_watch) > 0:\n        os.environ[\"WANDB_WATCH\"] = wandb_watch\n    if len(wandb_log_model) > 0:\n        os.environ[\"WANDB_LOG_MODEL\"] = wandb_log_model\n\n    generate_prompt = prompt_dict[prompt_style]\n\n    model = LlamaForCausalLM.from_pretrained(\n        base_model,\n        load_in_8bit=True,\n        torch_dtype=torch.float16,\n        device_map=device_map,\n    )\n\n    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n\n    tokenizer.pad_token_id = (\n        0  # unk. we want this to be different from the eos token\n    )\n    tokenizer.padding_side = \"left\"  # Allow batched inference\n\n    def tokenize(prompt, add_eos_token=True):\n        # there's probably a way to do this with the tokenizer settings\n        # but again, gotta move fast\n        result = tokenizer(\n            prompt,\n            truncation=True,\n            max_length=cutoff_len,\n            padding=False,\n            return_tensors=None,\n        )\n        if (\n                result[\"input_ids\"][-1] != tokenizer.eos_token_id\n                and len(result[\"input_ids\"]) < cutoff_len\n                and add_eos_token\n        ):\n            result[\"input_ids\"].append(tokenizer.eos_token_id)\n            result[\"attention_mask\"].append(1)\n\n        result[\"labels\"] = result[\"input_ids\"].copy()\n\n        return result\n\n    def generate_and_tokenize_prompt(data_point):\n        full_prompt = generate_prompt(data_point)\n        with open(output_dir + \"/full_prompt.txt\", \"a\") as f:\n            f.write(full_prompt)\n        tokenized_full_prompt = tokenize(full_prompt)\n        if not train_on_inputs:\n            if prompt_style == 'alpaca':\n                user_prompt = generate_prompt({**data_point, \"output\": \"\"})\n                tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n                user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n\n                tokenized_full_prompt[\"labels\"] = [\n                                                      -100\n                                                  ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n                                                                        user_prompt_len:\n                                                                        ]  # could be sped up, probably\n\n            elif prompt_style == 'oig':\n                user_prompt = generate_prompt({**data_point, \"answer\": \"\"})\n                tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n                user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n\n                tokenized_full_prompt[\"labels\"] = [\n                                                      -100\n                                                  ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n                                                                        user_prompt_len:\n                                                                        ]  # could be sped up, probably\n\n            elif prompt_style == 'vicuna':\n                system = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\"\n                roles = (\"Human\", \"Assistant\")\n                BEGIN_SIGNAL = \"### \"\n                END_SIGNAL = \"\\n\"\n                role_list = ['system']\n                role_len = [len(tokenize(system, add_eos_token=False)['input_ids'])]\n\n                for ele in data_point['conversations']:\n                    if ele['from'].lower() == 'human':\n                        role = roles[0]\n                    elif ele['from'].lower() == 'gpt':\n                        role = roles[1]\n                    else:\n                        role = 'unknown'\n\n                    role_list.append(role)\n                    role_len.append(len(\n                        tokenize(f\"{BEGIN_SIGNAL}{role}: {ele['value']}{END_SIGNAL}\", add_eos_token=False)[\n                            'input_ids']))\n                cur_idx = 0\n                break_flag = 0\n                for tokenized_len, speaker in zip(role_len, role_list):\n                    if speaker == \"Assistant\":\n                        if cur_idx + tokenized_len > cutoff_len:\n                            tokenized_full_prompt[\"labels\"][cutoff_len - 1] = tokenizer.eos_token_id\n                            tokenized_full_prompt[\"input_ids\"][cutoff_len - 1] = tokenizer.eos_token_id\n                            break_flag = 1\n                        else:\n                            tokenized_full_prompt[\"labels\"].insert(cur_idx + tokenized_len, tokenizer.eos_token_id)\n                            tokenized_full_prompt[\"attention_mask\"].insert(cur_idx + tokenized_len, 1)\n                            tokenized_full_prompt[\"input_ids\"].insert(cur_idx + tokenized_len, tokenizer.eos_token_id)\n                            cur_idx += 1\n                    if speaker != \"Assistant\":\n                        if cur_idx + tokenized_len > cutoff_len:\n                            tokenized_full_prompt[\"labels\"][cur_idx:cutoff_len] = [-100] * (cutoff_len - cur_idx)\n                            break_flag = 1\n                        else:\n                            tokenized_full_prompt[\"labels\"][cur_idx:cur_idx + tokenized_len] = [-100] * tokenized_len\n                    if break_flag == 1:\n                        break\n                    cur_idx += tokenized_len\n\n        return tokenized_full_prompt\n\n    model = prepare_model_for_int8_training(model)\n\n    config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        target_modules=lora_target_modules,\n        lora_dropout=lora_dropout,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n    model = get_peft_model(model, config)\n\n    # data = load_dataset(\"json\", data_files=data_path)\n    data_dict = {'train': data_path + '-train.json', 'test': data_path + '-dev.json'}\n    data = load_dataset(\"json\", data_files=data_dict, num_proc=16)\n\n    if resume_from_checkpoint:\n        # Check the available weights and load them\n        checkpoint_name = os.path.join(\n            resume_from_checkpoint, \"pytorch_model.bin\"\n        )  # Full checkpoint\n        if not os.path.exists(checkpoint_name):\n            checkpoint_name = os.path.join(\n                resume_from_checkpoint, \"adapter_model.bin\"\n            )  # only LoRA model - LoRA config above has to fit\n            resume_from_checkpoint = (\n                False  # So the trainer won't try loading its state\n            )\n        # The two files above have a different name depending on how they were saved, but are actually the same.\n        if os.path.exists(checkpoint_name):\n            print(f\"Restarting from {checkpoint_name}\")\n            adapters_weights = torch.load(checkpoint_name)\n            model = set_peft_model_state_dict(model, adapters_weights)\n        else:\n            print(f\"Checkpoint {checkpoint_name} not found\")\n\n    model.print_trainable_parameters()  # Be more transparent about the % of trainable params.\n\n    train_data = (\n        data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n    )\n    val_data = (\n        data[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n    )\n\n    if int(os.environ.get(\"LOCAL_RANK\") or 0) == 0:\n        print(\"#\" * 20 + \"Toknize example1\" + \"#\" * 20)\n        print(train_data[10])\n        print(tokenizer.decode(train_data[10][\"input_ids\"]))\n        print(train_data[10][\"attention_mask\"])\n        label = copy.deepcopy(train_data[10][\"labels\"])\n        label_id = [0 if x == -100 else x for x in label]\n        print(tokenizer.decode(label_id))\n        print(len(train_data[10][\"attention_mask\"]))\n        print(len(train_data[10][\"input_ids\"]))\n        print(len(train_data[10][\"labels\"]))\n        print(\"#\" * 20 + \"Toknize example2\" + \"#\" * 20)\n        print(train_data[30])\n        print(tokenizer.decode(train_data[30][\"input_ids\"]))\n        label = copy.deepcopy(train_data[30][\"labels\"])\n        label_id = [0 if x == -100 else x for x in label]\n        print(tokenizer.decode(label_id))\n        print(len(train_data[30][\"attention_mask\"]))\n        print(len(train_data[30][\"input_ids\"]))\n        print(len(train_data[30][\"labels\"]))\n\n    trainer = transformers.Trainer(\n        model=model,\n        train_dataset=train_data,\n        eval_dataset=val_data,\n        args=transformers.TrainingArguments(\n            per_device_train_batch_size=micro_batch_size,\n            per_device_eval_batch_size=micro_batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            warmup_steps=100,\n            num_train_epochs=num_epochs,\n            learning_rate=learning_rate,\n            fp16=True,\n            logging_steps=10,\n            optim=\"adamw_torch\",\n            evaluation_strategy=\"steps\",\n            save_strategy=\"steps\",\n            eval_steps=save_steps,\n            save_steps=save_steps,\n            output_dir=output_dir,\n            load_best_model_at_end=True,\n            ddp_find_unused_parameters=False if ddp else None,\n            group_by_length=group_by_length,\n            report_to=\"wandb\" if use_wandb else None,\n            run_name=wandb_run_name if use_wandb else None,\n        ),\n        data_collator=transformers.DataCollatorForSeq2Seq(\n            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n        ),\n    )\n    model.config.use_cache = False\n\n    old_state_dict = model.state_dict\n    model.state_dict = (\n        lambda self, *_, **__: get_peft_model_state_dict(\n            self, old_state_dict()\n        )\n    ).__get__(model, type(model))\n\n    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n        model = torch.compile(model)\n\n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n\n    if int(os.environ.get(\"LOCAL_RANK\") or 0) == 0:\n        model.save_pretrained(output_dir)\n\n    print(\n        \"\\n If there's a warning about missing keys above, please disregard :)\"\n    )\n\n\nif __name__ == \"__main__\":\n    fire.Fire(train)\n    # parser = argparse.ArgumentParser()\n    # parser.add_argument(\"--data_path\", type=str, default=\"data/convai2\")\n    # parser.add_argument(\"--output_dir\", type=str, default=\"output\")\n    # parser.add_argument(\"--base_model\", type=str, default=\"microsoft/DialoGPT-medium\")\n    # parser.add_argument(\"--micro_batch_size\", type=int, default=3)\n    # parser.add_argument(\"--batch_size\", type=int, default=1)\n    # parser.add_argument(\"--cutoff_len\", type=int, default=2048)\n    # parser.add_argument(\"--num_epochs\", type=int, default=1)\n    # parser.add_argument(\"--learning_rate\", type=float, default=3e-4)\n    # parser.add_argument(\"--lora_alpha\", type=int, default=16)\n    # parser.add_argument(\"--lora_dropout\", type=float, default=0.05)\n    # parser.add_argument(\"--lora_r\", type=int, default=1)\n    # parser.add_argument(\"--lora_target_modules\", nargs='+', type=str, default=[\"q_proj\"])\n    # parser.add_argument('--train_on_inputs', type=str, default=\"True\")\n    # parser.add_argument('--group_by_length', type=str, default=\"False\")\n    # parser.add_argument('--wandb_project', type=str, default=\"\")\n    # parser.add_argument('--wandb_run_name', type=str, default=\"\")\n    # parser.add_argument('--wandb_watch', type=str, default=\"\")\n    # parser.add_argument('--wandb_log_model', type=str, default=\"\")\n    # parser.add_argument('--local_rank', type=int, default=0)\n    # parser.add_argument('--resume_from_checkpoint', type=str, default=\"\")\n    # parser.add_argument(\"--prompt_style\", type=str, default=\"vicuna\")\n    # parser.add_argument(\"--save_steps\", type=int, default=400)\n    # args = parser.parse_args()\n    # train(args)\n"}
{"type": "source_file", "path": "utils/prompt.py", "content": "def alpaca_prompt(data_point):\n    # sorry about the formatting disaster gotta move fast\n    if data_point[\"input\"]:\n        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Input:\n{data_point[\"input\"]}\n\n### Response:\n{data_point[\"output\"]}\"\"\"\n    else:\n        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.  # noqa: E501\n\n### Instruction:\n{data_point[\"instruction\"]}\n\n### Response:\n{data_point[\"output\"]}\"\"\"\n\n\ndef oig_prompt(data_point):\n    # sorry about the formatting disaster gotta move fast\n    if len(data_point[\"question\"]) == 1:\n        return f\"\"\"Hello! Welcome to our chatbot. Please answer the question.\n\n### Question:\n{data_point[\"question\"][0][7:]}\n\n### Response:\n{data_point[\"answer\"][5:]}\"\"\"\n    else:\n        return f\"\"\"Hello! Welcome to our chatbot. Please read the conversation history and answer the question.\n\n### Conversation history:\n\"\"\" + '\\n'.join(data_point[\"question\"][:-1]) + f\"\"\"\n### Question:\n{data_point[\"question\"][-1][7:].strip(':')}\n\n### Response:\n{data_point[\"answer\"][5:].strip(':')}\"\"\"\n\n\ndef vicuna_lora_prompt(data_point):\n    system = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\"\n    roles = (\"Human\", \"Assistant\")\n    BEGIN_SIGNAL = \"### \"\n    END_SIGNAL = \"\\n\"\n\n    res = system\n\n    # sorry about the formatting disaster gotta move fast\n    conversations = data_point['conversations']\n\n    for ele in conversations:\n        if ele['from'].lower() == 'user':\n            role = roles[0]\n        elif ele['from'].lower() == 'assistant':\n            role = roles[1]\n        else:\n            role = 'unknown'\n\n        res += f\"{BEGIN_SIGNAL}{role}: {ele['value']}{END_SIGNAL}\"\n\n    return res\n\ndef vicuna_hf_prompt(data_point):\n    system = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\"\n    roles = (\"USER\", \"ASSISTANT\")\n    BEGIN_SIGNAL = \"\"\n    END_SIGNAL = \"\\n\"\n\n    res = system\n\n    # sorry about the formatting disaster gotta move fast\n    conversations = data_point['conversations']\n\n    for ele in conversations:\n        if ele['from'].lower() == 'user':\n            role = roles[0]\n        elif ele['from'].lower() == 'assistant':\n            role = roles[1]\n        else:\n            role = 'unknown'\n\n        res += f\"{BEGIN_SIGNAL}{role}: {ele['value']}{END_SIGNAL}\"\n\n    return res\nprompt_dict = {'alpaca': alpaca_prompt,\n               'oig': oig_prompt,\n               'vicuna_lora': vicuna_lora_prompt,\n               'vicuna_hf': vicuna_hf_prompt\n               }\n\n"}
{"type": "source_file", "path": "utils/apply_delta.py", "content": "import argparse\nimport gc\nimport glob\nimport json\nimport os\nimport shutil\nimport tempfile\n\nfrom huggingface_hub import snapshot_download\nimport torch\nfrom torch import nn\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n\n\nGB = 1 << 30\n\n\ndef split_files(model_path, tmp_path, split_size):\n    if not os.path.exists(model_path):\n        model_path = snapshot_download(repo_id=model_path)\n    if not os.path.exists(tmp_path):\n        os.makedirs(tmp_path)\n\n    file_pattern = os.path.join(model_path, \"pytorch_model-*.bin\")\n    files = glob.glob(file_pattern)\n\n    part = 0\n    try:\n        for file_path in tqdm(files):\n            state_dict = torch.load(file_path)\n            new_state_dict = {}\n\n            current_size = 0\n            for name, param in state_dict.items():\n                param_size = param.numel() * param.element_size()\n\n                if current_size + param_size > split_size:\n                    new_file_name = f\"pytorch_model-{part}.bin\"\n                    new_file_path = os.path.join(tmp_path, new_file_name)\n                    torch.save(new_state_dict, new_file_path)\n                    current_size = 0\n                    new_state_dict = None\n                    gc.collect()\n                    new_state_dict = {}\n                    part += 1\n\n                new_state_dict[name] = param\n                current_size += param_size\n\n            new_file_name = f\"pytorch_model-{part}.bin\"\n            new_file_path = os.path.join(tmp_path, new_file_name)\n            torch.save(new_state_dict, new_file_path)\n            new_state_dict = None\n            gc.collect()\n            new_state_dict = {}\n            part += 1\n    except Exception as e:\n        print(f\"An error occurred during split_files: {e}\")\n        shutil.rmtree(tmp_path)\n        raise\n\n\ndef apply_delta_low_cpu_mem(base_model_path, target_model_path, delta_path):\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path, use_fast=False)\n    delta_config = AutoConfig.from_pretrained(delta_path)\n\n    if os.path.exists(target_model_path):\n        shutil.rmtree(target_model_path)\n    os.makedirs(target_model_path)\n\n    split_size = 4 * GB\n\n    with tempfile.TemporaryDirectory() as tmp_base_path, tempfile.TemporaryDirectory() as tmp_delta_path:\n        print(f\"Split files for the base model to {tmp_base_path}\")\n        split_files(base_model_path, tmp_base_path, split_size)\n        print(f\"Split files for the delta weights to {tmp_delta_path}\")\n        split_files(delta_path, tmp_delta_path, split_size)\n\n        base_pattern = os.path.join(tmp_base_path, \"pytorch_model-*.bin\")\n        base_files = glob.glob(base_pattern)\n        delta_pattern = os.path.join(tmp_delta_path, \"pytorch_model-*.bin\")\n        delta_files = glob.glob(delta_pattern)\n        delta_state_dict = torch.load(delta_files[0])\n\n        print(\"Applying the delta\")\n        weight_map = {}\n        total_size = 0\n\n        for i, base_file in tqdm(enumerate(base_files)):\n            state_dict = torch.load(base_file)\n            file_name = f\"pytorch_model-{i}.bin\"\n            for name, param in state_dict.items():\n                if name not in delta_state_dict:\n                    for delta_file in delta_files:\n                        delta_state_dict = torch.load(delta_file)\n                        gc.collect()\n                        if name in delta_state_dict:\n                            break\n\n                state_dict[name] += delta_state_dict[name]\n                weight_map[name] = file_name\n                total_size += param.numel() * param.element_size()\n                gc.collect()\n            torch.save(state_dict, os.path.join(target_model_path, file_name))\n\n        with open(\n            os.path.join(target_model_path, \"pytorch_model.bin.index.json\"), \"w\"\n        ) as f:\n            json.dump(\n                {\"weight_map\": weight_map, \"metadata\": {\"total_size\": total_size}}, f\n            )\n\n    print(f\"Saving the target model to {target_model_path}\")\n    delta_tokenizer.save_pretrained(target_model_path)\n    delta_config.save_pretrained(target_model_path)\n\n\ndef apply_delta(base_model_path, target_model_path, delta_path):\n    print(f\"Loading the delta weights from {delta_path}\")\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path, use_fast=False)\n    delta = AutoModelForCausalLM.from_pretrained(\n        delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n\n    print(f\"Loading the base model from {base_model_path}\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n\n    print(\"Applying the delta\")\n    for name, param in tqdm(base.state_dict().items(), desc=\"Applying delta\"):\n        assert name in delta.state_dict()\n        param.data += delta.state_dict()[name]\n\n    print(f\"Saving the target model to {target_model_path}\")\n    base.save_pretrained(target_model_path)\n    delta_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n    parser.add_argument(\n        \"--low-cpu-mem\",\n        action=\"store_true\",\n        help=\"Lower the cpu memory usage. This will split large files and use \"\n        \"disk as swap to reduce the memory usage below 10GB.\",\n    )\n    args = parser.parse_args()\n\n    if args.low_cpu_mem:\n        apply_delta_low_cpu_mem(\n            args.base_model_path, args.target_model_path, args.delta_path\n        )\n    else:\n        apply_delta(args.base_model_path, args.target_model_path, args.delta_path)"}
{"type": "source_file", "path": "utils/deploy.py", "content": "import logging\nfrom typing import List, Tuple\nimport argparse\nimport gradio as gr\nfrom prompt import prompt_dict\nimport torch\nfrom transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\n\n\n\n\n# 清理缓存\ndef clear_torch_cache():\n    import gc\n    import torch\n    gc.collect()\n    torch.cuda.empty_cache()\n\n\nclass ChatBot:\n    def __init__(self, tokenizer, model, prompt_style) -> None:\n        self.tokenizer = tokenizer\n        self.model = model\n        self.prompt_style = prompt_style\n\n    @staticmethod\n    def get_prompt_by_history(history: List[Tuple[str, str]], query: str, prompt_style):\n        \"\"\" 自定义交互过程\n        \"\"\"\n        generate_prompt = prompt_dict[prompt_style]\n        lst = []\n        for ele in history:\n            lst.append(ele[0])\n            lst.append(ele[1])\n        history = lst\n        instruction = history + [query, \"\"]\n        roles = ('user', 'assistant')\n        prompt = generate_prompt({'conversations': [\n            {'from': roles[i % 2], 'value': instruction[i]} for i in range(len(instruction))\n        ]}).strip()\n        print('---'*50+'prompt'+'---'*50)\n        print(prompt)\n        return prompt\n\n    def chat(\n            self,\n            query: str,\n            history: List[Tuple[str, str]] = [],\n            max_length: int = 1024,\n            num_beams=4,\n            # do_sample=True,\n            top_p=0.75,\n            temperature=0.1,\n            top_k=40,\n            #repetition_penalty=4.1,\n            **kwargs\n    ):\n        gen_kwargs = {\n            \"max_length\": max_length,\n            \"num_beams\": num_beams,\n            # \"do_sample\": do_sample,\n            \"top_p\": top_p,\n            \"temperature\": temperature,\n            \"top_k\": top_k,\n            #\"repetition_penalty\": repetition_penalty,\n            **kwargs,\n        }\n        # if not history:\n        #     prompt = self.get_prompt_by_history([], query)\n        # else:\n        #     prompt = self.get_prompt_by_history(history, query)\n        if not history:\n            prompt = self.get_prompt_by_history([], query, self.prompt_style)\n        else:\n            prompt = self.get_prompt_by_history(history, query, self.prompt_style)\n        input_ids = self.tokenizer([prompt], return_tensors=\"pt\")\n        input_ids = input_ids.to(\"cuda\")\n        outputs = self.model.generate(**input_ids, **gen_kwargs)\n        response = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n        response = response.strip().replace(prompt, \"\")\n        history.append((query, response))\n        return response, history\n\n    def predict(self, input, history=[], max_turn=20):\n        max_boxes = max_turn * 2\n        clear_torch_cache()\n        response, history = self.chat(input, history)\n        updates = []\n        for query, response in history:\n            updates.append(gr.update(visible=True, value=query))\n            updates.append(gr.update(visible=True, value=response))\n        if len(updates) < max_boxes:\n            updates = updates + \\\n                      [gr.Textbox.update(visible=False)] * (max_boxes - len(updates))\n        logging.info(\"Input : \" + input)\n        logging.info(\"Output: \" + response)\n        logging.info(\"-------------------------------------------\")\n        return [history] + updates\n\n    def start_service(self, host, port, max_turn=20, concurrency_count=3):\n        with gr.Blocks(css=\"footer{display:none !important}\", title=\"Chat Demo \") as demo:\n            gr.Markdown(\n                \"\"\"\n                # chatalpaca-20k\n                finetune llama-7b on chatalpaca-20k dataset\n                \"\"\"\n            )\n            state = gr.State([])\n            text_boxes = []\n            for i in range(max_turn * 2):\n                if i % 2 == 0:\n                    label = \"提问：\"\n                else:\n                    label = \"回复：\"\n                text_boxes.append(gr.Textbox(visible=False, label=label))\n\n            with gr.Row():\n                with gr.Column(scale=4):\n                    txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(\n                        container=False)\n                with gr.Column(scale=1):\n                    button = gr.Button(\"Generate\")\n            txt.submit(self.predict, [txt, state], [state] + text_boxes)\n            button.click(self.predict, [txt, state], [state] + text_boxes)\n        print(\"start \")\n        demo.queue(concurrency_count=concurrency_count, api_open=False)\n        demo.launch(share=False, server_name=host, server_port=port)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--server_name', type=str, default='0.0.0.0')\n    parser.add_argument('--server_port', type=int, default=7868)\n    parser.add_argument('--model_path', type=str)\n    parser.add_argument('--prompt_style', type=str, default='vicuna_lora')\n    parser.add_argument('--max_turn', type=int, default=20)\n    parser.add_argument('--concurrency_count', type=int, default=3)\n    options = parser.parse_args()\n\n    logging.basicConfig(level=logging.INFO,\n                        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\n    logging.info(f\"Loading tokenizer from {options.model_path}\")\n    tokenizer = LlamaTokenizer.from_pretrained(options.model_path)\n    logging.info(f\"Loading model from {options.model_path}\")\n    device = 'cuda'\n    load_8bit: bool = False\n    model = LlamaForCausalLM.from_pretrained(\n        options.model_path,\n        load_in_8bit=load_8bit,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n\n    # unwind broken decapoda-research config\n    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n    model.config.bos_token_id = 1\n    model.config.eos_token_id = 2\n\n    if not load_8bit:\n        model.half()  # seems to fix bugs for some users.\n\n    model.eval()\n\n    chat_bot = ChatBot(\n        tokenizer=tokenizer,\n        model=model,\n        prompt_style=options.prompt_style\n    )\n\n    chat_bot.start_service(\n        host=options.server_name,\n        port=options.server_port,\n        max_turn=options.max_turn,\n        concurrency_count=options.concurrency_count,\n    )"}
{"type": "source_file", "path": "utils/apply_lora.py", "content": "import argparse\n\nimport torch\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef apply_lora(base_model_path, target_model_path, lora_path):\n    print(f\"Loading the base model from {base_model_path}\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    base_tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=False)\n\n    print(f\"Loading the LoRA adapter from {lora_path}\")\n\n    lora_model = PeftModel.from_pretrained(\n        base,\n        lora_path,\n        torch_dtype=torch.float16,\n    )\n\n    print(\"Applying the LoRA\")\n    model = lora_model.merge_and_unload()\n\n    print(f\"Saving the target model to {target_model_path}\")\n    model.save_pretrained(target_model_path)\n    base_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--lora-path\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    apply_lora(args.base_model_path, args.target_model_path, args.lora_path)"}
