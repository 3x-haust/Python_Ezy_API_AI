{"repo_info": {"repo_name": "raw-data-api", "repo_owner": "hotosm", "repo_url": "https://github.com/hotosm/raw-data-api"}}
{"type": "test_file", "path": "tests/load/locustfile.py", "content": "import json\n\nfrom locust import HttpUser, task\n\n\nclass Raw(HttpUser):\n    @task(1)\n    def raw_data_request_geojson(self):\n        \"\"\"payload is of kathmandu area , Produces 146 MB of file. Usually takes 15-19 Sec to Generate. Does not uses ogr2ogr\"\"\"\n        payload = {\n            \"fileName\": \"load_test\",\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [85.21270751953125, 27.646431146293423],\n                        [85.49629211425781, 27.646431146293423],\n                        [85.49629211425781, 27.762545086827302],\n                        [85.21270751953125, 27.762545086827302],\n                        [85.21270751953125, 27.646431146293423],\n                    ]\n                ],\n            },\n        }\n\n        headers = {\"content-type\": \"application/json\"}\n\n        self.client.post(\n            \"/raw-data/current-snapshot/\", data=json.dumps(payload), headers=headers\n        )\n\n    @task(2)\n    def raw_data_request_shapefile(self):\n        \"\"\"payload is of same area with shapefile option.Uses ogr2ogr , Produces 202MB of file and 25-30 Sec on single request\"\"\"\n        payload = {\n            \"fileName\": \"load_test\",\n            \"outputType\": \"shp\",\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [85.21270751953125, 27.646431146293423],\n                        [85.49629211425781, 27.646431146293423],\n                        [85.49629211425781, 27.762545086827302],\n                        [85.21270751953125, 27.762545086827302],\n                        [85.21270751953125, 27.646431146293423],\n                    ]\n                ],\n            },\n        }\n\n        headers = {\"content-type\": \"application/json\"}\n\n        self.client.post(\n            \"/raw-data/current-snapshot/\", data=json.dumps(payload), headers=headers\n        )\n"}
{"type": "test_file", "path": "tests/test_app.py", "content": "# Copyright (C) 2021 Humanitarian OpenStreetmap Team\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\n# Humanitarian OpenStreetmap Team\n# 1100 13th Street NW Suite 800 Washington, D.C. 20005\n# <info@hotosm.org>\n\n# Reader imports\nfrom src.query_builder.builder import raw_currentdata_extraction_query\nfrom src.validation.models import RawDataCurrentParams\n\n\ndef test_rawdata_current_snapshot_geometry_query():\n    test_param = {\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [84.92431640625, 27.766190642387496],\n                    [85.31982421875, 27.766190642387496],\n                    [85.31982421875, 28.02592458049937],\n                    [84.92431640625, 28.02592458049937],\n                    [84.92431640625, 27.766190642387496],\n                ]\n            ],\n        },\n        \"outputType\": \"geojson\",\n        \"useStWithin\": False,\n        \"filters\": {\n            \"tags\": {\"point\": {\"join_or\": {\"amenity\": [\"shop\", \"toilet\"]}}},\n            \"attributes\": {\"point\": [\"name\"]},\n        },\n    }\n    validated_params = RawDataCurrentParams(**test_param)\n    expected_query = \"\"\"select ST_AsGeoJSON(t0.*) from (select\n                    osm_id , tableoid::regclass AS osm_type , tags ->> 'name' as name , geom\n                    from\n                        nodes\n                    where\n                        ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[84.92431640625, 27.766190642387496], [85.31982421875, 27.766190642387496], [85.31982421875, 28.02592458049937], [84.92431640625, 28.02592458049937], [84.92431640625, 27.766190642387496]]]}'))))) and (tags ->>  'amenity' IN ( 'shop' ,  'toilet' ))) t0 UNION ALL select ST_AsGeoJSON(t1.*) from (select\n            osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,geom\n            from\n                ways_line\n            where\n                ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[84.92431640625, 27.766190642387496], [85.31982421875, 27.766190642387496], [85.31982421875, 28.02592458049937], [84.92431640625, 28.02592458049937], [84.92431640625, 27.766190642387496]]]}')))))) t1 UNION ALL select ST_AsGeoJSON(t2.*) from (select\n            osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,geom\n            from\n                ways_poly\n            where\n                ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[84.92431640625, 27.766190642387496], [85.31982421875, 27.766190642387496], [85.31982421875, 28.02592458049937], [84.92431640625, 28.02592458049937], [84.92431640625, 27.766190642387496]]]}')))))) t2 UNION ALL select ST_AsGeoJSON(t3.*) from (select\n            osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,geom\n            from\n                relations\n            where\n                ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[84.92431640625, 27.766190642387496], [85.31982421875, 27.766190642387496], [85.31982421875, 28.02592458049937], [84.92431640625, 28.02592458049937], [84.92431640625, 27.766190642387496]]]}')))))) t3\"\"\"\n\n    query_result = raw_currentdata_extraction_query(\n        validated_params,\n    )\n    assert query_result.encode(\"utf-8\") == expected_query.encode(\"utf-8\")\n\n\ndef test_rawdata_current_snapshot_normal_query():\n    test_param = {\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [84.92431640625, 27.766190642387496],\n                    [85.31982421875, 27.766190642387496],\n                    [85.31982421875, 28.02592458049937],\n                    [84.92431640625, 28.02592458049937],\n                    [84.92431640625, 27.766190642387496],\n                ]\n            ],\n        },\n        \"useStWithin\": False,\n        \"outputType\": \"geojson\",\n    }\n    validated_params = RawDataCurrentParams(**test_param)\n    expected_query = \"\"\"select ST_AsGeoJSON(t0.*) from (select\n                    osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,geom\n                    from\n                        nodes\n                    where\n                        ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[84.92431640625, 27.766190642387496], [85.31982421875, 27.766190642387496], [85.31982421875, 28.02592458049937], [84.92431640625, 28.02592458049937], [84.92431640625, 27.766190642387496]]]}')))))) t0 UNION ALL select ST_AsGeoJSON(t1.*) from (select\n            osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,geom\n            from\n                ways_line\n            where\n                ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[84.92431640625, 27.766190642387496], [85.31982421875, 27.766190642387496], [85.31982421875, 28.02592458049937], [84.92431640625, 28.02592458049937], [84.92431640625, 27.766190642387496]]]}')))))) t1 UNION ALL select ST_AsGeoJSON(t2.*) from (select\n            osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,geom\n            from\n                ways_poly\n            where\n                ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[84.92431640625, 27.766190642387496], [85.31982421875, 27.766190642387496], [85.31982421875, 28.02592458049937], [84.92431640625, 28.02592458049937], [84.92431640625, 27.766190642387496]]]}')))))) t2 UNION ALL select ST_AsGeoJSON(t3.*) from (select\n            osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,geom\n            from\n                relations\n            where\n                ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[84.92431640625, 27.766190642387496], [85.31982421875, 27.766190642387496], [85.31982421875, 28.02592458049937], [84.92431640625, 28.02592458049937], [84.92431640625, 27.766190642387496]]]}')))))) t3\"\"\"\n    query_result = raw_currentdata_extraction_query(\n        validated_params,\n    )\n    assert query_result.encode(\"utf-8\") == expected_query.encode(\"utf-8\")\n\n\ndef test_rawdata_current_snapshot_normal_query_ST_within():\n    test_param = {\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [84.92431640625, 27.766190642387496],\n                    [85.31982421875, 27.766190642387496],\n                    [85.31982421875, 28.02592458049937],\n                    [84.92431640625, 28.02592458049937],\n                    [84.92431640625, 27.766190642387496],\n                ]\n            ],\n        },\n        \"outputType\": \"geojson\",\n    }\n    validated_params = RawDataCurrentParams(**test_param)\n    expected_query = \"\"\"select ST_AsGeoJSON(t0.*) from (select\n                    osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,geom\n                    from\n                        nodes\n                    where\n                        ST_within(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[84.92431640625, 27.766190642387496], [85.31982421875, 27.766190642387496], [85.31982421875, 28.02592458049937], [84.92431640625, 28.02592458049937], [84.92431640625, 27.766190642387496]]]}')))))) t0 UNION ALL select ST_AsGeoJSON(t1.*) from (select\n            osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,geom\n            from\n                ways_line\n            where\n                ST_within(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[84.92431640625, 27.766190642387496], [85.31982421875, 27.766190642387496], [85.31982421875, 28.02592458049937], [84.92431640625, 28.02592458049937], [84.92431640625, 27.766190642387496]]]}')))))) t1 UNION ALL select ST_AsGeoJSON(t2.*) from (select\n            osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,geom\n            from\n                ways_poly\n            where\n                ST_within(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[84.92431640625, 27.766190642387496], [85.31982421875, 27.766190642387496], [85.31982421875, 28.02592458049937], [84.92431640625, 28.02592458049937], [84.92431640625, 27.766190642387496]]]}')))))) t2 UNION ALL select ST_AsGeoJSON(t3.*) from (select\n            osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,geom\n            from\n                relations\n            where\n                ST_within(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[84.92431640625, 27.766190642387496], [85.31982421875, 27.766190642387496], [85.31982421875, 28.02592458049937], [84.92431640625, 28.02592458049937], [84.92431640625, 27.766190642387496]]]}')))))) t3\"\"\"\n    query_result = raw_currentdata_extraction_query(\n        validated_params,\n    )\n    assert query_result.encode(\"utf-8\") == expected_query.encode(\"utf-8\")\n\n\ndef test_attribute_filter_rawdata():\n    test_param = {\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.502574, 27.569073],\n                    [83.502574, 28.332758],\n                    [85.556417, 28.332758],\n                    [85.556417, 27.569073],\n                    [83.502574, 27.569073],\n                ]\n            ],\n        },\n        \"outputType\": \"geojson\",\n        \"useStWithin\": False,\n        \"geometryType\": [\"polygon\", \"line\"],\n        \"filters\": {\n            \"attributes\": {\"line\": [\"name\"]},\n            \"tags\": {\"all_geometry\": {\"join_or\": {\"building\": [\"yes\"]}}},\n        },\n    }\n    validated_params = RawDataCurrentParams(**test_param)\n    expected_query = \"\"\"select ST_AsGeoJSON(t0.*) from (select\n            osm_id , tableoid::regclass AS osm_type , tags ->> 'name' as name , geom\n            from\n                ways_line\n            where\n                ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[83.502574, 27.569073], [83.502574, 28.332758], [85.556417, 28.332758], [85.556417, 27.569073], [83.502574, 27.569073]]]}'))))) and (tags ->> 'building' = 'yes')) t0 UNION ALL select ST_AsGeoJSON(t1.*) from (select\n                osm_id , tableoid::regclass AS osm_type , tags ->> 'name' as name , geom\n                from\n                    relations\n                where\n                    ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[83.502574, 27.569073], [83.502574, 28.332758], [85.556417, 28.332758], [85.556417, 27.569073], [83.502574, 27.569073]]]}'))))) and (tags ->> 'building' = 'yes') and (geometrytype(geom)='MULTILINESTRING')) t1 UNION ALL select ST_AsGeoJSON(t2.*) from (select\n            osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,geom\n            from\n                ways_poly\n            where\n                (grid = 1187 OR grid = 1188) and (ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[83.502574, 27.569073], [83.502574, 28.332758], [85.556417, 28.332758], [85.556417, 27.569073], [83.502574, 27.569073]]]}')))))) and (tags ->> 'building' = 'yes')) t2 UNION ALL select ST_AsGeoJSON(t3.*) from (select\n            osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,geom\n            from\n                relations\n            where\n                ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[83.502574, 27.569073], [83.502574, 28.332758], [85.556417, 28.332758], [85.556417, 27.569073], [83.502574, 27.569073]]]}'))))) and (tags ->> 'building' = 'yes') and (geometrytype(geom)='POLYGON' or geometrytype(geom)='MULTIPOLYGON')) t3\"\"\"\n    query_result = raw_currentdata_extraction_query(\n        validated_params,\n        g_id=[[1187], [1188]],\n    )\n    assert query_result.encode(\"utf-8\") == expected_query.encode(\"utf-8\")\n\n\ndef test_and_filters():\n    test_param = {\n        \"fileName\": \"Destroyed_Buildings_Turkey\",\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [36.70588085657477, 37.1979648807274],\n                    [36.70588085657477, 37.1651408422983],\n                    [36.759267544807194, 37.1651408422983],\n                    [36.759267544807194, 37.1979648807274],\n                    [36.70588085657477, 37.1979648807274],\n                ]\n            ],\n        },\n        \"outputType\": \"geojson\",\n        \"useStWithin\": False,\n        \"geometryType\": [\"polygon\"],\n        \"filters\": {\n            \"tags\": {\n                \"point\": {},\n                \"line\": {},\n                \"polygon\": {\n                    \"join_or\": {},\n                    \"join_and\": {\n                        \"destroyed:building\": [\"yes\"],\n                        \"damage:date\": [\"2023-02-06\"],\n                    },\n                },\n            },\n            \"attributes\": {\n                \"point\": [],\n                \"line\": [],\n                \"polygon\": [\n                    \"building\",\n                    \"destroyed:building\",\n                    \"damage:date\",\n                    \"name\",\n                    \"source\",\n                ],\n            },\n        },\n    }\n    validated_params = RawDataCurrentParams(**test_param)\n    expected_query = \"\"\"select ST_AsGeoJSON(t0.*) from (select\n            osm_id , tableoid::regclass AS osm_type , tags ->> 'building' as building , tags ->> 'destroyed:building' as destroyed_building , tags ->> 'damage:date' as damage_date , tags ->> 'name' as name , tags ->> 'source' as source , geom\n            from\n                ways_poly\n            where\n                ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[36.70588085657477, 37.1979648807274], [36.70588085657477, 37.1651408422983], [36.759267544807194, 37.1651408422983], [36.759267544807194, 37.1979648807274], [36.70588085657477, 37.1979648807274]]]}'))))) and (tags ->> 'destroyed:building' = 'yes' AND tags ->> 'damage:date' = '2023-02-06')) t0 UNION ALL select ST_AsGeoJSON(t1.*) from (select\n            osm_id , tableoid::regclass AS osm_type , tags ->> 'building' as building , tags ->> 'destroyed:building' as destroyed_building , tags ->> 'damage:date' as damage_date , tags ->> 'name' as name , tags ->> 'source' as source , geom\n            from\n                relations\n            where\n                ST_intersects(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{\"type\": \"Polygon\", \"coordinates\": [[[36.70588085657477, 37.1979648807274], [36.70588085657477, 37.1651408422983], [36.759267544807194, 37.1651408422983], [36.759267544807194, 37.1979648807274], [36.70588085657477, 37.1979648807274]]]}'))))) and (tags ->> 'destroyed:building' = 'yes' AND tags ->> 'damage:date' = '2023-02-06') and (geometrytype(geom)='POLYGON' or geometrytype(geom)='MULTIPOLYGON')) t1\"\"\"\n    query_result = raw_currentdata_extraction_query(\n        validated_params,\n    )\n    assert query_result.encode(\"utf-8\") == expected_query.encode(\"utf-8\")\n"}
{"type": "test_file", "path": "tests/test_API.py", "content": "# Standard library imports\nimport os\nimport time\n\n# Third party imports\nfrom fastapi.testclient import TestClient\n\n# Reader imports\nfrom API.main import app\n\nclient = TestClient(app)\n\naccess_token = os.environ.get(\"ACCESS_TOKEN\")\n\n## Status\n\n\ndef wait_for_task_completion(track_link, max_attempts=12, interval_seconds=5):\n    \"\"\"\n    Waits for a task to complete, polling the task status at specified intervals.\n\n    :param track_link: The endpoint to check the task status.\n    :param max_attempts: Maximum number of polling attempts.\n    :param interval_seconds: Time to wait between each polling attempt.\n    :return: The final response JSON on success or raises an AssertionError on failure.\n    \"\"\"\n    for attempt in range(1, max_attempts + 1):\n        time.sleep(interval_seconds)  # wait for the worker to complete the task\n\n        response = client.get(f\"/v1{track_link}\")\n        assert response.status_code == 200, \"Task status check failed\"\n        res = response.json()\n        check_status = res[\"status\"]\n\n        if check_status == \"SUCCESS\":\n            return res  # Task completed successfully\n        if check_status == \"FAILURE\":\n            raise AssertionError(f\"Task failed {res}\")\n\n        if attempt == max_attempts:\n            raise AssertionError(\n                f\"Task did not complete successfully after {max_attempts} attempts with following response {res}\"\n            )\n\n\ndef test_status():\n    response = client.get(\"/v1/status/\")\n    assert response.status_code == 200\n\n\n# Login\ndef test_login_url():\n    response = client.get(\"/v1/auth/login/\")\n    assert response.status_code == 200\n\n\ndef test_login_auth_me():\n    headers = {\"access-token\": access_token}\n    response = client.get(\"/v1/auth/me/\", headers=headers)\n    assert response.status_code == 200\n\n\n## Countries\n\n\ndef test_countries_endpoint():\n    response = client.get(\"/v1/countries/?q=nepal\")\n    assert response.status_code == 200\n\n\n## test osm_id\n\n\ndef test_osm_id_endpoint():\n    response = client.get(\"/v1/osm_id/?osm_id=421498318\")\n    assert response.status_code == 200\n\n\n## Snapshot\ndef test_snapshot():\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [83.96919250488281, 28.194446860487773],\n                        [83.99751663208006, 28.194446860487773],\n                        [83.99751663208006, 28.214869548073377],\n                        [83.96919250488281, 28.214869548073377],\n                        [83.96919250488281, 28.194446860487773],\n                    ]\n                ],\n            }\n        },\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_featurecollection():\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"geometry\": {\n                \"type\": \"FeatureCollection\",\n                \"features\": [\n                    {\n                        \"type\": \"Feature\",\n                        \"properties\": {},\n                        \"geometry\": {\n                            \"coordinates\": [\n                                [\n                                    [83.97346137271688, 28.217525272345284],\n                                    [83.97346137271688, 28.192595937414737],\n                                    [84.01473909818759, 28.192595937414737],\n                                    [84.01473909818759, 28.217525272345284],\n                                    [83.97346137271688, 28.217525272345284],\n                                ]\n                            ],\n                            \"type\": \"Polygon\",\n                        },\n                    }\n                ],\n            }\n        },\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_feature():\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"geometry\": {\n                \"type\": \"Feature\",\n                \"properties\": {},\n                \"geometry\": {\n                    \"coordinates\": [\n                        [\n                            [83.97346137271688, 28.217525272345284],\n                            [83.97346137271688, 28.192595937414737],\n                            [84.01473909818759, 28.192595937414737],\n                            [84.01473909818759, 28.217525272345284],\n                            [83.97346137271688, 28.217525272345284],\n                        ]\n                    ],\n                    \"type\": \"Polygon\",\n                },\n            }\n        },\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_feature_fgb():\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"outputType\": \"fgb\",\n            \"geometry\": {\n                \"type\": \"Feature\",\n                \"properties\": {},\n                \"geometry\": {\n                    \"coordinates\": [\n                        [\n                            [83.97346137271688, 28.217525272345284],\n                            [83.97346137271688, 28.192595937414737],\n                            [84.01473909818759, 28.192595937414737],\n                            [84.01473909818759, 28.217525272345284],\n                            [83.97346137271688, 28.217525272345284],\n                        ]\n                    ],\n                    \"type\": \"Polygon\",\n                },\n            },\n        },\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_feature_fgb_wrap_geom():\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"fgbWrapGeoms\": True,\n            \"outputType\": \"fgb\",\n            \"geometry\": {\n                \"type\": \"Feature\",\n                \"properties\": {},\n                \"geometry\": {\n                    \"coordinates\": [\n                        [\n                            [83.97346137271688, 28.217525272345284],\n                            [83.97346137271688, 28.192595937414737],\n                            [84.01473909818759, 28.192595937414737],\n                            [84.01473909818759, 28.217525272345284],\n                            [83.97346137271688, 28.217525272345284],\n                        ]\n                    ],\n                    \"type\": \"Polygon\",\n                },\n            },\n        },\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_feature_shp():\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"outputType\": \"shp\",\n            \"geometry\": {\n                \"type\": \"Feature\",\n                \"properties\": {},\n                \"geometry\": {\n                    \"coordinates\": [\n                        [\n                            [83.97346137271688, 28.217525272345284],\n                            [83.97346137271688, 28.192595937414737],\n                            [84.01473909818759, 28.192595937414737],\n                            [84.01473909818759, 28.217525272345284],\n                            [83.97346137271688, 28.217525272345284],\n                        ]\n                    ],\n                    \"type\": \"Polygon\",\n                },\n            },\n        },\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_feature_gpkg():\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"outputType\": \"gpkg\",\n            \"geometry\": {\n                \"type\": \"Feature\",\n                \"properties\": {},\n                \"geometry\": {\n                    \"coordinates\": [\n                        [\n                            [83.97346137271688, 28.217525272345284],\n                            [83.97346137271688, 28.192595937414737],\n                            [84.01473909818759, 28.192595937414737],\n                            [84.01473909818759, 28.217525272345284],\n                            [83.97346137271688, 28.217525272345284],\n                        ]\n                    ],\n                    \"type\": \"Polygon\",\n                },\n            },\n        },\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_feature_kml():\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"outputType\": \"kml\",\n            \"geometry\": {\n                \"type\": \"Feature\",\n                \"properties\": {},\n                \"geometry\": {\n                    \"coordinates\": [\n                        [\n                            [83.97346137271688, 28.217525272345284],\n                            [83.97346137271688, 28.192595937414737],\n                            [84.01473909818759, 28.192595937414737],\n                            [84.01473909818759, 28.217525272345284],\n                            [83.97346137271688, 28.217525272345284],\n                        ]\n                    ],\n                    \"type\": \"Polygon\",\n                },\n            },\n        },\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_feature_sql():\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"outputType\": \"sql\",\n            \"geometry\": {\n                \"type\": \"Feature\",\n                \"properties\": {},\n                \"geometry\": {\n                    \"coordinates\": [\n                        [\n                            [83.97346137271688, 28.217525272345284],\n                            [83.97346137271688, 28.192595937414737],\n                            [84.01473909818759, 28.192595937414737],\n                            [84.01473909818759, 28.217525272345284],\n                            [83.97346137271688, 28.217525272345284],\n                        ]\n                    ],\n                    \"type\": \"Polygon\",\n                },\n            },\n        },\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_feature_csv():\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"outputType\": \"csv\",\n            \"geometry\": {\n                \"type\": \"Feature\",\n                \"properties\": {},\n                \"geometry\": {\n                    \"coordinates\": [\n                        [\n                            [83.97346137271688, 28.217525272345284],\n                            [83.97346137271688, 28.192595937414737],\n                            [84.01473909818759, 28.192595937414737],\n                            [84.01473909818759, 28.217525272345284],\n                            [83.97346137271688, 28.217525272345284],\n                        ]\n                    ],\n                    \"type\": \"Polygon\",\n                },\n            },\n        },\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_centroid():\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"centroid\": True,\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [83.96919250488281, 28.194446860487773],\n                        [83.99751663208006, 28.194446860487773],\n                        [83.99751663208006, 28.214869548073377],\n                        [83.96919250488281, 28.214869548073377],\n                        [83.96919250488281, 28.194446860487773],\n                    ]\n                ],\n            },\n        },\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_filters():\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"fileName\": \"Example export with all features\",\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [83.585701, 28.046607],\n                        [83.585701, 28.382561],\n                        [84.391823, 28.382561],\n                        [84.391823, 28.046607],\n                        [83.585701, 28.046607],\n                    ]\n                ],\n            },\n            \"outputType\": \"geojson\",\n            \"geometryType\": [\"point\", \"line\", \"polygon\"],\n            \"filters\": {\n                \"tags\": {\n                    \"point\": {\n                        \"join_or\": {\n                            \"amenity\": [\n                                \"bank\",\n                                \"ferry_terminal\",\n                                \"bus_station\",\n                                \"fuel\",\n                                \"kindergarten\",\n                                \"school\",\n                                \"college\",\n                                \"university\",\n                                \"place_of_worship\",\n                                \"marketplace\",\n                                \"clinic\",\n                                \"hospital\",\n                                \"police\",\n                                \"fire_station\",\n                            ],\n                            \"building\": [\n                                \"bank\",\n                                \"aerodrome\",\n                                \"ferry_terminal\",\n                                \"train_station\",\n                                \"bus_station\",\n                                \"pumping_station\",\n                                \"power_substation\",\n                                \"kindergarten\",\n                                \"school\",\n                                \"college\",\n                                \"university\",\n                                \"mosque \",\n                                \" church \",\n                                \" temple\",\n                                \"supermarket\",\n                                \"marketplace\",\n                                \"clinic\",\n                                \"hospital\",\n                                \"police\",\n                                \"fire_station\",\n                                \"stadium \",\n                                \" sports_centre\",\n                                \"governor_office \",\n                                \" townhall \",\n                                \" subdistrict_office \",\n                                \" village_office \",\n                                \" community_group_office\",\n                                \"government_office\",\n                            ],\n                            \"man_made\": [\"tower\", \"water_tower\", \"pumping_station\"],\n                            \"tower:type\": [\"communication\"],\n                            \"aeroway\": [\"aerodrome\"],\n                            \"railway\": [\"station\"],\n                            \"emergency\": [\"fire_hydrant\"],\n                            \"landuse\": [\"reservoir\", \"recreation_gound\"],\n                            \"waterway\": [\"floodgate\"],\n                            \"natural\": [\"spring\"],\n                            \"power\": [\"tower\", \"substation\"],\n                            \"shop\": [\"supermarket\"],\n                            \"leisure\": [\n                                \"stadium \",\n                                \" sports_centre \",\n                                \" pitch \",\n                                \" swimming_pool\",\n                                \"park\",\n                            ],\n                            \"office\": [\"government\"],\n                        }\n                    },\n                    \"line\": {\n                        \"join_or\": {\n                            \"highway\": [\n                                \"motorway \",\n                                \" trunk \",\n                                \" primary \",\n                                \" secondary \",\n                                \" tertiary \",\n                                \" service \",\n                                \" residential \",\n                                \" pedestrian \",\n                                \" path \",\n                                \" living_street \",\n                                \" track\",\n                            ],\n                            \"railway\": [\"rail\"],\n                            \"man_made\": [\"embankment\"],\n                            \"waterway\": [],\n                        }\n                    },\n                    \"polygon\": {\n                        \"join_or\": {\n                            \"amenity\": [\n                                \"bank\",\n                                \"ferry_terminal\",\n                                \"bus_station\",\n                                \"fuel\",\n                                \"kindergarten\",\n                                \"school\",\n                                \"college\",\n                                \"university\",\n                                \"place_of_worship\",\n                                \"marketplace\",\n                                \"clinic\",\n                                \"hospital\",\n                                \"police\",\n                                \"fire_station\",\n                            ],\n                            \"building\": [\n                                \"bank\",\n                                \"aerodrome\",\n                                \"ferry_terminal\",\n                                \"train_station\",\n                                \"bus_station\",\n                                \"pumping_station\",\n                                \"power_substation\",\n                                \"power_plant\",\n                                \"kindergarten\",\n                                \"school\",\n                                \"college\",\n                                \"university\",\n                                \"mosque \",\n                                \" church \",\n                                \" temple\",\n                                \"supermarket\",\n                                \"marketplace\",\n                                \"clinic\",\n                                \"hospital\",\n                                \"police\",\n                                \"fire_station\",\n                                \"stadium \",\n                                \" sports_centre\",\n                                \"governor_office \",\n                                \" townhall \",\n                                \" subdistrict_office \",\n                                \" village_office \",\n                                \" community_group_office\",\n                                \"government_office\",\n                            ],\n                            \"man_made\": [\"tower\", \"water_tower\", \"pumping_station\"],\n                            \"tower:type\": [\"communication\"],\n                            \"aeroway\": [\"aerodrome\"],\n                            \"railway\": [\"station\"],\n                            \"landuse\": [\"reservoir\", \"recreation_gound\"],\n                            \"waterway\": [],\n                            \"natural\": [\"spring\"],\n                            \"power\": [\"substation\", \"plant\"],\n                            \"shop\": [\"supermarket\"],\n                            \"leisure\": [\n                                \"stadium \",\n                                \" sports_centre \",\n                                \" pitch \",\n                                \" swimming_pool\",\n                                \"park\",\n                            ],\n                            \"office\": [\"government\"],\n                            \"type\": [\"boundary\"],\n                            \"boundary\": [\"administrative\"],\n                        },\n                    },\n                },\n                \"attributes\": {\n                    \"point\": [\n                        \"building\",\n                        \"ground_floor:height\",\n                        \"capacity:persons\",\n                        \"building:structure\",\n                        \"building:condition\",\n                        \"name\",\n                        \"admin_level\",\n                        \"building:material\",\n                        \"office\",\n                        \"building:roof\",\n                        \"backup_generator\",\n                        \"access:roof\",\n                        \"building:levels\",\n                        \"building:floor\",\n                        \"addr:full\",\n                        \"addr:city\",\n                        \"source\",\n                    ],\n                    \"line\": [\"width\", \"source\", \"waterway\", \"name\"],\n                    \"polygon\": [\n                        \"landslide_prone\",\n                        \"name\",\n                        \"admin_level\",\n                        \"type\",\n                        \"is_in:town\",\n                        \"flood_prone\",\n                        \"is_in:province\",\n                        \"is_in:city\",\n                        \"is_in:municipality\",\n                        \"is_in:RW\",\n                        \"is_in:village\",\n                        \"source\",\n                        \"boundary\",\n                    ],\n                },\n            },\n        },\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_and_filter():\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"fileName\": \"Destroyed_Buildings_Turkey\",\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [36.70588085657477, 37.1979648807274],\n                        [36.70588085657477, 37.1651408422983],\n                        [36.759267544807194, 37.1651408422983],\n                        [36.759267544807194, 37.1979648807274],\n                        [36.70588085657477, 37.1979648807274],\n                    ]\n                ],\n            },\n            \"outputType\": \"geojson\",\n            \"geometryType\": [\"polygon\"],\n            \"filters\": {\n                \"tags\": {\n                    \"point\": {},\n                    \"line\": {},\n                    \"polygon\": {\n                        \"join_or\": {},\n                        \"join_and\": {\n                            \"destroyed:building\": [\"yes\"],\n                            \"damage:date\": [\"2023-02-06\"],\n                        },\n                    },\n                },\n                \"attributes\": {\n                    \"point\": [],\n                    \"line\": [],\n                    \"polygon\": [\n                        \"building\",\n                        \"destroyed:building\",\n                        \"damage:date\",\n                        \"name\",\n                        \"source\",\n                    ],\n                },\n            },\n        },\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_and_filter_with_usermetadata():\n    headers = {\"access-token\": access_token}\n\n    response = client.post(\n        \"/v1/snapshot/\",\n        json={\n            \"fileName\": \"Destroyed_Buildings_Turkey\",\n            \"includeUserMetadata\": True,\n            \"geometry\": {\n                \"type\": \"Polygon\",\n                \"coordinates\": [\n                    [\n                        [36.70588085657477, 37.1979648807274],\n                        [36.70588085657477, 37.1651408422983],\n                        [36.759267544807194, 37.1651408422983],\n                        [36.759267544807194, 37.1979648807274],\n                        [36.70588085657477, 37.1979648807274],\n                    ]\n                ],\n            },\n            \"outputType\": \"geojson\",\n            \"geometryType\": [\"polygon\"],\n            \"filters\": {\n                \"tags\": {\n                    \"point\": {},\n                    \"line\": {},\n                    \"polygon\": {\n                        \"join_or\": {},\n                        \"join_and\": {\n                            \"destroyed:building\": [\"yes\"],\n                            \"damage:date\": [\"2023-02-06\"],\n                        },\n                    },\n                },\n                \"attributes\": {\n                    \"point\": [],\n                    \"line\": [],\n                    \"polygon\": [\n                        \"building\",\n                        \"destroyed:building\",\n                        \"damage:date\",\n                        \"name\",\n                        \"source\",\n                    ],\n                },\n            },\n        },\n        headers=headers,\n    )\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_authentication_uuid():\n    headers = {\"access-token\": access_token}\n    payload = {\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.96919250488281, 28.194446860487773],\n                    [83.99751663208006, 28.194446860487773],\n                    [83.99751663208006, 28.214869548073377],\n                    [83.96919250488281, 28.214869548073377],\n                    [83.96919250488281, 28.194446860487773],\n                ]\n            ],\n        },\n        \"uuid\": False,\n    }\n\n    response = client.post(\"/v1/snapshot/\", json=payload, headers=headers)\n\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_bind_zip():\n    headers = {\"access-token\": access_token}\n    payload = {\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.96919250488281, 28.194446860487773],\n                    [83.99751663208006, 28.194446860487773],\n                    [83.99751663208006, 28.214869548073377],\n                    [83.96919250488281, 28.214869548073377],\n                    [83.96919250488281, 28.194446860487773],\n                ]\n            ],\n        },\n        \"bindZip\": False,\n    }\n\n    response = client.post(\"/v1/snapshot/\", json=payload, headers=headers)\n\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_bind_zip():\n    headers = {\"access-token\": access_token}\n    payload = {\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.96919250488281, 28.194446860487773],\n                    [83.99751663208006, 28.194446860487773],\n                    [83.99751663208006, 28.214869548073377],\n                    [83.96919250488281, 28.214869548073377],\n                    [83.96919250488281, 28.194446860487773],\n                ]\n            ],\n        },\n        \"bindZip\": False,\n    }\n\n    response = client.post(\"/v1/snapshot/\", json=payload, headers=headers)\n\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\n## Test snapshot include user metadata\n\n\ndef test_snapshot_with_user_meatadata():\n    headers = {\"access-token\": access_token}\n    payload = {\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.96919250488281, 28.194446860487773],\n                    [83.99751663208006, 28.194446860487773],\n                    [83.99751663208006, 28.214869548073377],\n                    [83.96919250488281, 28.214869548073377],\n                    [83.96919250488281, 28.194446860487773],\n                ]\n            ],\n        },\n        \"includeUserMetadata\": True,\n    }\n\n    response = client.post(\"/v1/snapshot/\", json=payload, headers=headers)\n\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_snapshot_with_user_meatadata_without_login():\n    # headers = {\"access-token\": access_token}\n    payload = {\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.96919250488281, 28.194446860487773],\n                    [83.99751663208006, 28.194446860487773],\n                    [83.99751663208006, 28.214869548073377],\n                    [83.96919250488281, 28.214869548073377],\n                    [83.96919250488281, 28.194446860487773],\n                ]\n            ],\n        },\n        \"includeUserMetadata\": True,\n    }\n\n    response = client.post(\"/v1/snapshot/\", json=payload)\n\n    assert response.status_code == 403\n\n\n## Snapshot Plain\n\n\ndef test_snapshot_plain():\n    response = client.post(\n        \"/v1/snapshot/plain/\",\n        json={\n            \"type\": \"Feature\",\n            \"properties\": {},\n            \"geometry\": {\n                \"coordinates\": [\n                    [\n                        [83.98223911755514, 28.21348731781771],\n                        [83.98223911755514, 28.208639873158987],\n                        [83.9881146731152, 28.208639873158987],\n                        [83.9881146731152, 28.21348731781771],\n                        [83.98223911755514, 28.21348731781771],\n                    ]\n                ],\n                \"type\": \"Polygon\",\n            },\n        },\n    )\n    assert response.status_code == 200\n\n\n## Stats\n\n\ndef test_stats_endpoint_custom_polygon():\n    headers = {\"access-token\": access_token}\n    payload = {\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.96919250488281, 28.194446860487773],\n                    [83.99751663208006, 28.194446860487773],\n                    [83.99751663208006, 28.214869548073377],\n                    [83.96919250488281, 28.214869548073377],\n                    [83.96919250488281, 28.194446860487773],\n                ]\n            ],\n        }\n    }\n\n    response = client.post(\"/v1/stats/polygon/\", json=payload, headers=headers)\n\n    assert response.status_code == 200\n    res = response.json()\n    assert (\n        res[\"meta\"][\"indicators\"]\n        == \"https://github.com/hotosm/raw-data-api/tree/develop/docs/src/stats/indicators.md\"\n    )\n\n\ndef test_stats_endpoint_iso3():\n    headers = {\"access-token\": access_token}\n    payload = {\"iso3\": \"npl\"}\n\n    response = client.post(\"/v1/stats/polygon/\", json=payload, headers=headers)\n\n    assert response.status_code == 200\n    res = response.json()\n    assert (\n        res[\"meta\"][\"indicators\"]\n        == \"https://github.com/hotosm/raw-data-api/tree/develop/docs/src/stats/indicators.md\"\n    )\n\n\n# HDX\n\n\ndef test_hdx_submit_normal_iso3():\n    headers = {\"access-token\": access_token}\n    payload = {\n        \"iso3\": \"NPL\",\n        \"hdx_upload\": False,\n        \"categories\": [\n            {\n                \"Roads\": {\n                    \"hdx\": {\n                        \"tags\": [\"roads\", \"transportation\", \"geodata\"],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"lines\"],\n                    \"select\": [\"name\", \"highway\"],\n                    \"where\": \"tags['highway'] IS NOT NULL\",\n                    \"formats\": [\"geojson\"],\n                }\n            }\n        ],\n    }\n\n    response = client.post(\"/v1/custom/snapshot/\", json=payload, headers=headers)\n\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_hdx_submit_normal_iso3_multiple_format():\n    headers = {\"access-token\": access_token}\n    payload = {\n        \"iso3\": \"NPL\",\n        \"hdx_upload\": False,\n        \"categories\": [\n            {\n                \"Roads\": {\n                    \"hdx\": {\n                        \"tags\": [\"roads\", \"transportation\", \"geodata\"],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"lines\"],\n                    \"select\": [\"name\", \"highway\"],\n                    \"where\": \"tags['highway'] IS NOT NULL\",\n                    \"formats\": [\"geojson\", \"gpkg\", \"kml\", \"shp\"],\n                }\n            }\n        ],\n    }\n\n    response = client.post(\"/v1/custom/snapshot/\", json=payload, headers=headers)\n\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_hdx_submit_normal_custom_polygon():\n    headers = {\"access-token\": access_token}\n    payload = {\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.96919250488281, 28.194446860487773],\n                    [83.99751663208006, 28.194446860487773],\n                    [83.99751663208006, 28.214869548073377],\n                    [83.96919250488281, 28.214869548073377],\n                    [83.96919250488281, 28.194446860487773],\n                ]\n            ],\n        },\n        \"hdx_upload\": False,\n        \"dataset\": {\n            \"subnational\": True,\n            \"dataset_title\": \"Pokhara\",\n            \"dataset_prefix\": \"hotosm_pkr\",\n            \"dataset_locations\": [\"npl\"],\n        },\n        \"categories\": [\n            {\n                \"Roads\": {\n                    \"hdx\": {\n                        \"tags\": [\"roads\", \"transportation\", \"geodata\"],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"lines\"],\n                    \"select\": [\"name\", \"highway\"],\n                    \"where\": \"tags['highway'] IS NOT NULL\",\n                    \"formats\": [\"geojson\"],\n                }\n            }\n        ],\n    }\n\n    response = client.post(\"/v1/custom/snapshot/\", json=payload, headers=headers)\n\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_custom_submit_normal_custom_polygon_TM_project():\n    headers = {\"access-token\": access_token}\n    payload = {\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.96919250488281, 28.194446860487773],\n                    [83.99751663208006, 28.194446860487773],\n                    [83.99751663208006, 28.214869548073377],\n                    [83.96919250488281, 28.214869548073377],\n                    [83.96919250488281, 28.194446860487773],\n                ]\n            ],\n        },\n        \"queue\": \"raw_ondemand\",\n        \"dataset\": {\n            \"dataset_prefix\": \"hotosm_project_1\",\n            \"dataset_folder\": \"TM\",\n            \"dataset_title\": \"Tasking Manger Project 1\",\n        },\n        \"categories\": [\n            {\n                \"Buildings\": {\n                    \"types\": [\"polygons\"],\n                    \"select\": [\n                        \"name\",\n                        \"building\",\n                        \"building:levels\",\n                        \"building:materials\",\n                        \"addr:full\",\n                        \"addr:housenumber\",\n                        \"addr:street\",\n                        \"addr:city\",\n                        \"office\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['building'] IS NOT NULL\",\n                    \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                },\n                \"Roads\": {\n                    \"types\": [\"lines\"],\n                    \"select\": [\n                        \"name\",\n                        \"highway\",\n                        \"surface\",\n                        \"smoothness\",\n                        \"width\",\n                        \"lanes\",\n                        \"oneway\",\n                        \"bridge\",\n                        \"layer\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['highway'] IS NOT NULL\",\n                    \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                },\n                \"Waterways\": {\n                    \"types\": [\"lines\", \"polygons\"],\n                    \"select\": [\n                        \"name\",\n                        \"waterway\",\n                        \"covered\",\n                        \"width\",\n                        \"depth\",\n                        \"layer\",\n                        \"blockage\",\n                        \"tunnel\",\n                        \"natural\",\n                        \"water\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['waterway'] IS NOT NULL OR tags['water'] IS NOT NULL OR tags['natural'] IN ('water','wetland','bay')\",\n                    \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                },\n                \"Landuse\": {\n                    \"types\": [\"points\", \"polygons\"],\n                    \"select\": [\"name\", \"amenity\", \"landuse\", \"leisure\"],\n                    \"where\": \"tags['landuse'] IS NOT NULL\",\n                    \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                },\n            }\n        ],\n    }\n\n    response = client.post(\"/v1/custom/snapshot/\", json=payload, headers=headers)\n\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_hdx_submit_normal_custom_polygon_upload():\n    headers = {\"access-token\": access_token}\n    payload = {\n        \"geometry\": {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.96919250488281, 28.194446860487773],\n                    [83.99751663208006, 28.194446860487773],\n                    [83.99751663208006, 28.214869548073377],\n                    [83.96919250488281, 28.214869548073377],\n                    [83.96919250488281, 28.194446860487773],\n                ]\n            ],\n        },\n        \"hdx_upload\": True,\n        \"dataset\": {\n            \"subnational\": True,\n            \"dataset_title\": \"Pokhara\",\n            \"dataset_folder\": \"Test\",\n            \"dataset_prefix\": \"hotosm_pkr\",\n            \"dataset_locations\": [\"npl\"],\n        },\n        \"categories\": [\n            {\n                \"Roads\": {\n                    \"hdx\": {\n                        \"tags\": [\"roads\", \"transportation\", \"geodata\"],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"lines\"],\n                    \"select\": [\"name\", \"highway\"],\n                    \"where\": \"tags['highway'] IS NOT NULL\",\n                    \"formats\": [\"geojson\"],\n                }\n            }\n        ],\n    }\n\n    response = client.post(\"/v1/custom/snapshot/\", json=payload, headers=headers)\n\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\ndef test_full_hdx_set_iso():\n    headers = {\"access-token\": access_token}\n    payload = {\n        \"iso3\": \"NPL\",\n        \"hdx_upload\": False,\n        \"include_stats\": True,\n        \"include_stats_html\": True,\n        \"categories\": [\n            {\n                \"Buildings\": {\n                    \"hdx\": {\n                        \"tags\": [\n                            \"facilities-infrastructure\",\n                            \"geodata\",\n                        ],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"polygons\"],\n                    \"select\": [\n                        \"name\",\n                        \"building\",\n                        \"building:levels\",\n                        \"building:materials\",\n                        \"addr:full\",\n                        \"addr:housenumber\",\n                        \"addr:street\",\n                        \"addr:city\",\n                        \"office\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['building'] IS NOT NULL\",\n                    \"formats\": [\"geojson\"],\n                }\n            },\n            {\n                \"Roads\": {\n                    \"hdx\": {\n                        \"tags\": [\"transportation\", \"geodata\"],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"lines\"],\n                    \"select\": [\n                        \"name\",\n                        \"highway\",\n                        \"surface\",\n                        \"smoothness\",\n                        \"width\",\n                        \"lanes\",\n                        \"oneway\",\n                        \"bridge\",\n                        \"layer\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['highway'] IS NOT NULL\",\n                    \"formats\": [\"geojson\"],\n                }\n            },\n            {\n                \"Waterways\": {\n                    \"hdx\": {\n                        \"tags\": [\"hydrology\", \"geodata\"],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"lines\", \"polygons\"],\n                    \"select\": [\n                        \"name\",\n                        \"waterway\",\n                        \"covered\",\n                        \"width\",\n                        \"depth\",\n                        \"layer\",\n                        \"blockage\",\n                        \"tunnel\",\n                        \"natural\",\n                        \"water\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['waterway'] IS NOT NULL OR tags['water'] IS NOT NULL OR tags['natural'] IN ('water','wetland','bay')\",\n                    \"formats\": [\"geojson\"],\n                }\n            },\n            {\n                \"Points of Interest\": {\n                    \"hdx\": {\n                        \"tags\": [\n                            \"facilities-infrastructure\",\n                            \"points of interest-poi\",\n                            \"geodata\",\n                        ],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"points\", \"polygons\"],\n                    \"select\": [\n                        \"name\",\n                        \"amenity\",\n                        \"man_made\",\n                        \"shop\",\n                        \"tourism\",\n                        \"opening_hours\",\n                        \"beds\",\n                        \"rooms\",\n                        \"addr:full\",\n                        \"addr:housenumber\",\n                        \"addr:street\",\n                        \"addr:city\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['amenity'] IS NOT NULL OR tags['man_made'] IS NOT NULL OR tags['shop'] IS NOT NULL OR tags['tourism'] IS NOT NULL\",\n                    \"formats\": [\"geojson\"],\n                }\n            },\n            {\n                \"Airports\": {\n                    \"hdx\": {\n                        \"tags\": [\n                            \"aviation\",\n                            \"facilities-infrastructure\",\n                            \"geodata\",\n                        ],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"points\", \"lines\", \"polygons\"],\n                    \"select\": [\n                        \"name\",\n                        \"aeroway\",\n                        \"building\",\n                        \"emergency\",\n                        \"emergency:helipad\",\n                        \"operator:type\",\n                        \"capacity:persons\",\n                        \"addr:full\",\n                        \"addr:city\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['aeroway'] IS NOT NULL OR tags['building'] = 'aerodrome' OR tags['emergency:helipad'] IS NOT NULL OR tags['emergency'] = 'landing_site'\",\n                    \"formats\": [\"geojson\"],\n                }\n            },\n            {\n                \"Sea Ports\": {\n                    \"hdx\": {\n                        \"tags\": [\n                            \"facilities-infrastructure\",\n                            \"geodata\",\n                        ],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"points\", \"lines\", \"polygons\"],\n                    \"select\": [\n                        \"name\",\n                        \"amenity\",\n                        \"building\",\n                        \"port\",\n                        \"operator:type\",\n                        \"addr:full\",\n                        \"addr:city\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['amenity'] = 'ferry_terminal' OR tags['building'] = 'ferry_terminal' OR tags['port'] IS NOT NULL\",\n                    \"formats\": [\"geojson\"],\n                }\n            },\n            {\n                \"Education Facilities\": {\n                    \"hdx\": {\n                        \"tags\": [\n                            \"education facilities-schools\",\n                            \"geodata\",\n                        ],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"points\", \"polygons\"],\n                    \"select\": [\n                        \"name\",\n                        \"amenity\",\n                        \"building\",\n                        \"operator:type\",\n                        \"capacity:persons\",\n                        \"addr:full\",\n                        \"addr:city\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['amenity'] IN ('kindergarten', 'school', 'college', 'university') OR tags['building'] IN ('kindergarten', 'school', 'college', 'university')\",\n                    \"formats\": [\"geojson\"],\n                }\n            },\n            {\n                \"Health Facilities\": {\n                    \"hdx\": {\n                        \"tags\": [\"geodata\"],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"points\", \"polygons\"],\n                    \"select\": [\n                        \"name\",\n                        \"amenity\",\n                        \"building\",\n                        \"healthcare\",\n                        \"healthcare:speciality\",\n                        \"operator:type\",\n                        \"capacity:persons\",\n                        \"addr:full\",\n                        \"addr:city\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['healthcare'] IS NOT NULL OR tags['amenity'] IN ('doctors', 'dentist', 'clinic', 'hospital', 'pharmacy')\",\n                    \"formats\": [\"geojson\"],\n                }\n            },\n            {\n                \"Populated Places\": {\n                    \"hdx\": {\n                        \"tags\": [\n                            \"populated places-settlements\",\n                            \"geodata\",\n                        ],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"points\"],\n                    \"select\": [\n                        \"name\",\n                        \"place\",\n                        \"population\",\n                        \"is_in\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['place'] IN ('isolated_dwelling', 'town', 'village', 'hamlet', 'city')\",\n                    \"formats\": [\"geojson\"],\n                }\n            },\n            {\n                \"Financial Services\": {\n                    \"hdx\": {\n                        \"tags\": [\"economics\", \"geodata\"],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"points\", \"polygons\"],\n                    \"select\": [\n                        \"name\",\n                        \"amenity\",\n                        \"operator\",\n                        \"network\",\n                        \"addr:full\",\n                        \"addr:city\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['amenity'] IN ('mobile_money_agent','bureau_de_change','bank','microfinance','atm','sacco','money_transfer','post_office')\",\n                    \"formats\": [\"geojson\"],\n                }\n            },\n            {\n                \"Railways\": {\n                    \"hdx\": {\n                        \"tags\": [\n                            \"facilities-infrastructure\",\n                            \"railways\",\n                            \"transportation\",\n                            \"geodata\",\n                        ],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"lines\"],\n                    \"select\": [\n                        \"name\",\n                        \"railway\",\n                        \"ele\",\n                        \"operator:type\",\n                        \"layer\",\n                        \"addr:full\",\n                        \"addr:city\",\n                        \"source\",\n                    ],\n                    \"where\": \"tags['railway'] IN ('rail','station')\",\n                    \"formats\": [\"geojson\"],\n                }\n            },\n        ],\n    }\n\n    response = client.post(\"/v1/custom/snapshot/\", json=payload, headers=headers)\n\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n\n\n## Tasks connection\n\n\ndef test_worker_connection():\n    response = client.get(\"/v1/tasks/ping/\")\n    assert response.status_code == 200\n\n\n### Custom Snapshot Using YAML\n\n\ndef test_custom_yaml_normal_fmtm_request():\n    headers = {\n        \"access-token\": access_token,\n        \"Content-Type\": \"application/x-yaml\",\n    }\n    payload = \"\"\"\n    dataset:\n        dataset_folder: FMTM\n        dataset_prefix: hotosm_fmtm_project_1\n        dataset_title: Field Mapping Tasking Manger Project 1\n    categories:\n        - Buildings:\n            formats:\n                - geojson\n                - fgb\n                - mvt\n            select:\n                - '*'\n            types:\n                - polygons\n            where: tags['building'] IS NOT NULL\n    geometry:\n        {\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n            [\n                [\n                83.96919250488281,\n                28.194446860487773\n                ],\n                [\n                83.99751663208006,\n                28.194446860487773\n                ],\n                [\n                83.99751663208006,\n                28.214869548073377\n                ],\n                [\n                83.96919250488281,\n                28.214869548073377\n                ],\n                [\n                83.96919250488281,\n                28.194446860487773\n                ]\n            ]\n            ]\n    }\n\n    \"\"\"\n\n    response = client.post(\"/v1/custom/snapshot/yaml/\", data=payload, headers=headers)\n    assert response.status_code == 200\n    res = response.json()\n    track_link = res[\"track_link\"]\n    wait_for_task_completion(track_link)\n"}
{"type": "source_file", "path": "API/raw_data.py", "content": "# Copyright (C) 2021 Humanitarian OpenStreetmap Team\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\n# Humanitarian OpenStreetmap Team\n# 1100 13th Street NW Suite 800 Washington, D.C. 20005\n# <info@hotosm.org>\n\n\"\"\"[Router Responsible for Raw data API ]\n\"\"\"\n# Standard library imports\nimport json\nfrom typing import AsyncGenerator\n\n# Third party imports\nimport orjson\nimport redis\nfrom area import area\nfrom fastapi import APIRouter, Body, Depends, HTTPException, Request\nfrom fastapi.responses import JSONResponse, StreamingResponse\nfrom fastapi_versioning import version\n\n# Reader imports\nfrom src.app import RawData\nfrom src.config import (\n    ALLOW_BIND_ZIP_FILTER,\n    CELERY_BROKER_URL,\n    DEFAULT_QUEUE_NAME,\n    EXPORT_MAX_AREA_SQKM,\n)\nfrom src.config import LIMITER as limiter\nfrom src.config import RATE_LIMIT_PER_MIN as export_rate_limit\nfrom src.query_builder.builder import raw_currentdata_extraction_query\nfrom src.validation.models import (\n    RawDataCurrentParams,\n    RawDataCurrentParamsBase,\n    SnapshotResponse,\n    StatusResponse,\n)\n\nfrom .api_worker import process_raw_data\nfrom .auth import AuthUser, UserRole, get_optional_user\n\nrouter = APIRouter(prefix=\"\", tags=[\"Extract\"])\n\nredis_client = redis.StrictRedis.from_url(CELERY_BROKER_URL)\n\n\n@router.get(\"/status/\", response_model=StatusResponse)\n@version(1)\ndef check_database_last_updated():\n    \"\"\"Gives status about how recent the osm data is , it will give the last time that database was updated completely\"\"\"\n    result = RawData().check_status()\n    return {\"last_updated\": result}\n\n\n@router.post(\"/snapshot/\", response_model=SnapshotResponse)\n@limiter.limit(f\"{export_rate_limit}/minute\")\n@version(1)\ndef get_osm_current_snapshot_as_file(\n    request: Request,\n    params: RawDataCurrentParams = Body(\n        default={},\n        openapi_examples={\n            \"normal\": {\n                \"summary\": \"Example : Extract Evertyhing in the area\",\n                \"description\": \"**Query** to Extract everything in the area , You can pass your geometry only and you will get everything on that area\",\n                \"value\": {\n                    \"geometry\": {\n                        \"type\": \"Polygon\",\n                        \"coordinates\": [\n                            [\n                                [83.96919250488281, 28.194446860487773],\n                                [83.99751663208006, 28.194446860487773],\n                                [83.99751663208006, 28.214869548073377],\n                                [83.96919250488281, 28.214869548073377],\n                                [83.96919250488281, 28.194446860487773],\n                            ]\n                        ],\n                    }\n                },\n            },\n            \"fileformats\": {\n                \"summary\": \"An example with different file formats and filename\",\n                \"description\": \"Raw Data API  can export data into multiple file formats . See outputype for more details\",\n                \"value\": {\n                    \"outputType\": \"shp\",\n                    \"fileName\": \"Pokhara_all_features\",\n                    \"geometry\": {\n                        \"type\": \"Polygon\",\n                        \"coordinates\": [\n                            [\n                                [83.96919250488281, 28.194446860487773],\n                                [83.99751663208006, 28.194446860487773],\n                                [83.99751663208006, 28.214869548073377],\n                                [83.96919250488281, 28.214869548073377],\n                                [83.96919250488281, 28.194446860487773],\n                            ]\n                        ],\n                    },\n                },\n            },\n            \"filters\": {\n                \"summary\": \"An example with filters and geometry type\",\n                \"description\": \"Raw Data API  supports different kind of filters on both attributes and tags . See filters for more details\",\n                \"value\": {\n                    \"outputType\": \"geojson\",\n                    \"fileName\": \"Pokhara_buildings\",\n                    \"geometry\": {\n                        \"type\": \"Polygon\",\n                        \"coordinates\": [\n                            [\n                                [83.96919250488281, 28.194446860487773],\n                                [83.99751663208006, 28.194446860487773],\n                                [83.99751663208006, 28.214869548073377],\n                                [83.96919250488281, 28.214869548073377],\n                                [83.96919250488281, 28.194446860487773],\n                            ]\n                        ],\n                    },\n                    \"filters\": {\n                        \"tags\": {\"all_geometry\": {\"join_or\": {\"building\": []}}},\n                        \"attributes\": {\"all_geometry\": [\"name\"]},\n                    },\n                    \"geometryType\": [\"point\", \"polygon\"],\n                },\n            },\n            \"filters2\": {\n                \"summary\": \"An example with more filters\",\n                \"description\": \"Raw Data API  supports different kind of filters on both attributes and tags . See filters for more details\",\n                \"value\": {\n                    \"geometry\": {\n                        \"type\": \"Polygon\",\n                        \"coordinates\": [\n                            [\n                                [83.585701, 28.046607],\n                                [83.585701, 28.382561],\n                                [84.391823, 28.382561],\n                                [84.391823, 28.046607],\n                                [83.585701, 28.046607],\n                            ]\n                        ],\n                    },\n                    \"fileName\": \"my export\",\n                    \"outputType\": \"geojson\",\n                    \"geometryType\": [\"point\", \"polygon\"],\n                    \"filters\": {\n                        \"tags\": {\n                            \"all_geometry\": {\n                                \"join_or\": {\"building\": []},\n                                \"join_and\": {\"amenity\": [\"cafe\", \"restaurant\", \"pub\"]},\n                            }\n                        },\n                        \"attributes\": {\"all_geometry\": [\"name\", \"addr\"]},\n                    },\n                },\n            },\n            \"allfilters\": {\n                \"summary\": \"An example with multiple level of filters\",\n                \"description\": \"Raw Data API  supports multiple level of filters on point line polygon . See filters for more details\",\n                \"value\": {\n                    \"fileName\": \"Example export with all features\",\n                    \"geometry\": {\n                        \"type\": \"Polygon\",\n                        \"coordinates\": [\n                            [\n                                [83.585701, 28.046607],\n                                [83.585701, 28.382561],\n                                [84.391823, 28.382561],\n                                [84.391823, 28.046607],\n                                [83.585701, 28.046607],\n                            ]\n                        ],\n                    },\n                    \"outputType\": \"geojson\",\n                    \"geometryType\": [\"point\", \"line\", \"polygon\"],\n                    \"filters\": {\n                        \"tags\": {\n                            \"point\": {\n                                \"join_or\": {\n                                    \"amenity\": [\n                                        \"bank\",\n                                        \"ferry_terminal\",\n                                        \"bus_station\",\n                                        \"fuel\",\n                                        \"kindergarten\",\n                                        \"school\",\n                                        \"college\",\n                                        \"university\",\n                                        \"place_of_worship\",\n                                        \"marketplace\",\n                                        \"clinic\",\n                                        \"hospital\",\n                                        \"police\",\n                                        \"fire_station\",\n                                    ],\n                                    \"building\": [\n                                        \"bank\",\n                                        \"aerodrome\",\n                                        \"ferry_terminal\",\n                                        \"train_station\",\n                                        \"bus_station\",\n                                        \"pumping_station\",\n                                        \"power_substation\",\n                                        \"kindergarten\",\n                                        \"school\",\n                                        \"college\",\n                                        \"university\",\n                                        \"mosque \",\n                                        \" church \",\n                                        \" temple\",\n                                        \"supermarket\",\n                                        \"marketplace\",\n                                        \"clinic\",\n                                        \"hospital\",\n                                        \"police\",\n                                        \"fire_station\",\n                                        \"stadium \",\n                                        \" sports_centre\",\n                                        \"governor_office \",\n                                        \" townhall \",\n                                        \" subdistrict_office \",\n                                        \" village_office \",\n                                        \" community_group_office\",\n                                        \"government_office\",\n                                    ],\n                                    \"man_made\": [\n                                        \"tower\",\n                                        \"water_tower\",\n                                        \"pumping_station\",\n                                    ],\n                                    \"tower:type\": [\"communication\"],\n                                    \"aeroway\": [\"aerodrome\"],\n                                    \"railway\": [\"station\"],\n                                    \"emergency\": [\"fire_hydrant\"],\n                                    \"landuse\": [\"reservoir\", \"recreation_gound\"],\n                                    \"waterway\": [\"floodgate\"],\n                                    \"natural\": [\"spring\"],\n                                    \"power\": [\"tower\", \"substation\"],\n                                    \"shop\": [\"supermarket\"],\n                                    \"leisure\": [\n                                        \"stadium \",\n                                        \" sports_centre \",\n                                        \" pitch \",\n                                        \" swimming_pool\",\n                                        \"park\",\n                                    ],\n                                    \"office\": [\"government\"],\n                                }\n                            },\n                            \"line\": {\n                                \"join_or\": {\n                                    \"highway\": [\n                                        \"motorway \",\n                                        \" trunk \",\n                                        \" primary \",\n                                        \" secondary \",\n                                        \" tertiary \",\n                                        \" service \",\n                                        \" residential \",\n                                        \" pedestrian \",\n                                        \" path \",\n                                        \" living_street \",\n                                        \" track\",\n                                    ],\n                                    \"railway\": [\"rail\"],\n                                    \"man_made\": [\"embankment\"],\n                                    \"waterway\": [],\n                                }\n                            },\n                            \"polygon\": {\n                                \"join_or\": {\n                                    \"amenity\": [\n                                        \"bank\",\n                                        \"ferry_terminal\",\n                                        \"bus_station\",\n                                        \"fuel\",\n                                        \"kindergarten\",\n                                        \"school\",\n                                        \"college\",\n                                        \"university\",\n                                        \"place_of_worship\",\n                                        \"marketplace\",\n                                        \"clinic\",\n                                        \"hospital\",\n                                        \"police\",\n                                        \"fire_station\",\n                                    ],\n                                    \"building\": [\n                                        \"bank\",\n                                        \"aerodrome\",\n                                        \"ferry_terminal\",\n                                        \"train_station\",\n                                        \"bus_station\",\n                                        \"pumping_station\",\n                                        \"power_substation\",\n                                        \"power_plant\",\n                                        \"kindergarten\",\n                                        \"school\",\n                                        \"college\",\n                                        \"university\",\n                                        \"mosque \",\n                                        \" church \",\n                                        \" temple\",\n                                        \"supermarket\",\n                                        \"marketplace\",\n                                        \"clinic\",\n                                        \"hospital\",\n                                        \"police\",\n                                        \"fire_station\",\n                                        \"stadium \",\n                                        \" sports_centre\",\n                                        \"governor_office \",\n                                        \" townhall \",\n                                        \" subdistrict_office \",\n                                        \" village_office \",\n                                        \" community_group_office\",\n                                        \"government_office\",\n                                    ],\n                                    \"man_made\": [\n                                        \"tower\",\n                                        \"water_tower\",\n                                        \"pumping_station\",\n                                    ],\n                                    \"tower:type\": [\"communication\"],\n                                    \"aeroway\": [\"aerodrome\"],\n                                    \"railway\": [\"station\"],\n                                    \"landuse\": [\"reservoir\", \"recreation_gound\"],\n                                    \"waterway\": [],\n                                    \"natural\": [\"spring\"],\n                                    \"power\": [\"substation\", \"plant\"],\n                                    \"shop\": [\"supermarket\"],\n                                    \"leisure\": [\n                                        \"stadium \",\n                                        \" sports_centre \",\n                                        \" pitch \",\n                                        \" swimming_pool\",\n                                        \"park\",\n                                    ],\n                                    \"office\": [\"government\"],\n                                    \"type\": [\"boundary\"],\n                                    \"boundary\": [\"administrative\"],\n                                }\n                            },\n                        },\n                        \"attributes\": {\n                            \"point\": [\n                                \"building\",\n                                \"ground_floor:height\",\n                                \"capacity:persons\",\n                                \"building:structure\",\n                                \"building:condition\",\n                                \"name\",\n                                \"admin_level\",\n                                \"building:material\",\n                                \"office\",\n                                \"building:roof\",\n                                \"backup_generator\",\n                                \"access:roof\",\n                                \"building:levels\",\n                                \"building:floor\",\n                                \"addr:full\",\n                                \"addr:city\",\n                                \"source\",\n                            ],\n                            \"line\": [\"width\", \"source\", \"waterway\", \"name\"],\n                            \"polygon\": [\n                                \"landslide_prone\",\n                                \"name\",\n                                \"admin_level\",\n                                \"type\",\n                                \"is_in:town\",\n                                \"flood_prone\",\n                                \"is_in:province\",\n                                \"is_in:city\",\n                                \"is_in:municipality\",\n                                \"is_in:RW\",\n                                \"is_in:village\",\n                                \"source\",\n                                \"boundary\",\n                            ],\n                        },\n                    },\n                },\n            },\n        },\n    ),\n    user: AuthUser = Depends(get_optional_user),\n):\n    \"\"\"Generates the current raw OpenStreetMap data available on database based on the input geometry, query and spatial features.\n\n    Steps to Run Snapshot :\n\n    1.  Post the your request here and your request will be on queue, endpoint will return as following :\n        {\n            \"task_id\": \"your task_id\",\n            \"track_link\": \"/tasks/task_id/\"\n        }\n    2. Now navigate to /tasks/ with your task id to track progress and result\n\n    \"\"\"\n    if not (user.role is UserRole.STAFF.value or user.role is UserRole.ADMIN.value):\n        if params.file_name:\n            if \"/\" in params.file_name:\n                raise HTTPException(\n                    status_code=403,\n                    detail=[\n                        {\n                            \"msg\": \"Insufficient Permission to use folder structure exports , Remove / from filename or get access\"\n                        }\n                    ],\n                )\n        area_m2 = area(json.loads(params.geometry.model_dump_json()))\n        area_km2 = area_m2 * 1e-6\n        RAWDATA_CURRENT_POLYGON_AREA = int(EXPORT_MAX_AREA_SQKM)\n        if area_km2 > RAWDATA_CURRENT_POLYGON_AREA:\n            raise HTTPException(\n                status_code=400,\n                detail=[\n                    {\n                        \"msg\": f\"\"\"Polygon Area {int(area_km2)} Sq.KM is higher than Threshold : {RAWDATA_CURRENT_POLYGON_AREA} Sq.KM\"\"\"\n                    }\n                ],\n            )\n        if not params.uuid:\n            raise HTTPException(\n                status_code=403,\n                detail=[{\"msg\": \"Insufficient Permission for uuid = False\"}],\n            )\n        if ALLOW_BIND_ZIP_FILTER:\n            if not params.bind_zip:\n                ACCEPTABLE_STREAMING_AREA_SQKM2 = 200\n                if area_km2 > ACCEPTABLE_STREAMING_AREA_SQKM2:\n                    raise HTTPException(\n                        status_code=406,\n                        detail=[\n                            {\n                                \"msg\": f\"Area {area_km2} km2 is greater than {ACCEPTABLE_STREAMING_AREA_SQKM2} km2 which is supported for streaming in this permission\"\n                            }\n                        ],\n                    )\n\n    if user.id == 0 and params.include_user_metadata:\n        raise HTTPException(\n            status_code=403,\n            detail=[\n                {\n                    \"msg\": \"Insufficient Permission for extracting exports with user metadata , Please login first\"\n                }\n            ],\n        )\n    queue_name = DEFAULT_QUEUE_NAME  # Everything directs to default now\n    task = process_raw_data.apply_async(\n        args=(params.model_dump(),),\n        queue=queue_name,\n        track_started=True,\n        kwargs={\"user\": user.model_dump()},\n    )\n    return JSONResponse(\n        {\n            \"task_id\": task.id,\n            \"track_link\": f\"/tasks/status/{task.id}/\",\n            \"queue\": redis_client.llen(queue_name),\n        }\n    )\n\n\n@router.post(\"/snapshot/plain/\")\n@version(1)\nasync def get_osm_current_snapshot_as_plain_geojson(\n    request: Request,\n    params: RawDataCurrentParamsBase,\n    user: AuthUser = Depends(get_optional_user),\n):\n    \"\"\"Generates the Plain geojson for the polygon within 30 Sqkm and returns the result right away\n\n    Args:\n        request (Request): _description_\n        params (RawDataCurrentParamsBase): Same as /snapshot except multiple output format options and configurations\n\n    Returns:\n        FeatureCollection: Geojson\n    \"\"\"\n    if user.id == 0 and params.include_user_metadata:\n        raise HTTPException(\n            status_code=403,\n            detail=[\n                {\n                    \"msg\": \"Insufficient Permission for extracting exports with user metadata, Please login first\"\n                }\n            ],\n        )\n    area_m2 = area(json.loads(params.geometry.model_dump_json()))\n\n    area_km2 = area_m2 * 1e-6\n    if int(area_km2) > 6:\n        raise HTTPException(\n            status_code=400,\n            detail=[\n                {\n                    \"msg\": f\"\"\"Polygon Area {int(area_km2)} Sq.KM is higher than Threshold : 6 Sq.KM\"\"\"\n                }\n            ],\n        )\n\n    params.output_type = \"geojson\"  # always geojson\n\n    async def generate_geojson() -> AsyncGenerator[bytes, None]:\n        # start of featurecollection\n        yield b'{\"type\": \"FeatureCollection\", \"features\": ['\n\n        raw_data = RawData(params)\n        extraction_query = raw_currentdata_extraction_query(params)\n\n        with raw_data.con.cursor(name=\"fetch_raw_quick\") as cursor:\n            cursor.itersize = 500\n            cursor.execute(extraction_query)\n\n            first_feature = True\n            for row in cursor:\n                feature = orjson.loads(row[0])\n                if not first_feature:\n                    # add comma to maintain the struct\n                    yield b\",\"\n                else:\n                    first_feature = False\n                yield orjson.dumps(feature)\n            cursor.close()\n\n        # end of featurecollect\n        yield b\"]}\"\n\n    return StreamingResponse(generate_geojson(), media_type=\"application/geo+json\")\n\n\n@router.get(\"/countries/\")\n@version(1)\ndef get_countries(q: str = \"\"):\n    result = RawData().get_countries_list(q)\n    return result\n\n\n@router.get(\"/countries/{cid}/\")\n@version(1)\ndef get_specific_country(cid: int):\n    result = RawData().get_country(cid)\n    return result\n\n\n@router.get(\"/osm_id/\")\n@version(1)\ndef get_osm_feature(osm_id: int):\n    return RawData().get_osm_feature(osm_id)\n"}
{"type": "source_file", "path": "API/download_metrics.py", "content": "# Standard library imports\nfrom datetime import datetime\nfrom typing import Optional\n\n# Third party imports\nfrom fastapi import APIRouter, Depends, HTTPException, Query\nfrom fastapi_versioning import version\n\n# Reader imports\nfrom src.app import DownloadMetrics\n\nfrom .auth import staff_required\n\nrouter = APIRouter(prefix=\"/metrics\", tags=[\"Metrics\"])\n\n\n@router.get(\"/summary\")\n@version(1)\ndef get_stats(\n    start_date: str = Query(\n        ...,\n        description=\"Start date (YYYY-MM-DD)\",\n        regex=r\"^\\d{4}-\\d{2}-\\d{2}$\",\n        example=\"2023-04-01\",\n    ),\n    end_date: str = Query(\n        ...,\n        description=\"End date (YYYY-MM-DD)\",\n        regex=r\"^\\d{4}-\\d{2}-\\d{2}$\",\n        example=\"2023-04-30\",\n    ),\n    group_by: str = Query(\n        \"day\",\n        description=\"Group by: day, month, or quarter\",\n        regex=r\"^(day|month|quarter|year)$\",\n    ),\n    folder: Optional[str] = Query(\n        None,\n        description=\"Folder to filter metrics by\",\n        example=\"TM\",\n    ),\n    _: bool = Depends(staff_required),\n):\n    \"\"\"\n    Retrieve download metrics summary statistics.\n\n    - **start_date**: The start date for the metrics, in the format \"YYYY-MM-DD\".\n    - **end_date**: The end date for the metrics, in the format \"YYYY-MM-DD\".\n    - **group_by**: The time period to group the metrics by. Can be \"day\", \"month\", \"quarter\", or \"year\".\n\n    The API requires admin authentication to access.\n    \"\"\"\n    if group_by not in [\"day\", \"month\", \"quarter\", \"year\"]:\n        raise HTTPException(\n            status_code=400, detail={\"error\": \"Invalid group_by parameter\"}\n        )\n\n    try:\n        start_date_obj = datetime.strptime(start_date, \"%Y-%m-%d\")\n        end_date_obj = datetime.strptime(end_date, \"%Y-%m-%d\")\n    except ValueError:\n        raise HTTPException(\n            status_code=400,\n            detail={\"error\": \"Invalid date format, expected YYYY-MM-DD\"},\n        )\n\n    metrics = DownloadMetrics()\n    return metrics.get_summary_stats(start_date, end_date, group_by, folder)\n"}
{"type": "source_file", "path": "API/cron.py", "content": "# Standard library imports\nfrom typing import Dict, List\n\n# Third party imports\nfrom fastapi import APIRouter, Depends, HTTPException, Query, Request\nfrom fastapi_versioning import version\n\n# Reader imports\nfrom src.app import Cron\nfrom src.config import LIMITER as limiter\nfrom src.config import RATE_LIMIT_PER_MIN\n\nfrom .auth import AuthUser, admin_required, staff_required\n\n# from src.validation.models import DynamicCategoriesModel\n\n\nrouter = APIRouter(prefix=\"/cron\", tags=[\"Cron\"])\n\n\n@router.post(\"/\", response_model=dict)\n@limiter.limit(f\"{RATE_LIMIT_PER_MIN}/minute\")\n@version(1)\nasync def create_cron(\n    request: Request, cron_data: dict, user_data: AuthUser = Depends(staff_required)\n):\n    \"\"\"\n    Create a new Cron entry.\n\n    Args:\n        request (Request): The request object.\n        cron_data (dict): Data for creating the cron entry.\n        user_data (AuthUser): User authentication data.\n\n    Returns:\n        dict: Result of the cron creation process.\n    \"\"\"\n    cron_instance = Cron()\n    return cron_instance.create_cron(cron_data)\n\n\n@router.get(\"/\", response_model=List[dict])\n@limiter.limit(f\"{RATE_LIMIT_PER_MIN}/minute\")\n@version(1)\nasync def read_cron_list(\n    request: Request,\n    skip: int = 0,\n    limit: int = 10,\n):\n    \"\"\"\n    Retrieve a list of Cron entries based on provided filters.\n\n    Args:\n        request (Request): The request object.\n        skip (int): Number of entries to skip.\n        limit (int): Maximum number of entries to retrieve.\n\n    Returns:\n        List[dict]: List of Cron entries.\n    \"\"\"\n    cron_instance = Cron()\n    filters = {}\n    for key, values in request.query_params.items():\n        if key not in [\"skip\", \"limit\"]:\n            if key in [\"iso3\", \"id\", \"queue\", \"meta\", \"cron_upload\", \"cid\"]:\n                filters[f\"{key} = %s\"] = values\n                continue\n            filters[f\"dataset->>'{key}' = %s\"] = values\n    try:\n        cron_list = cron_instance.get_cron_list_with_filters(skip, limit, filters)\n    except Exception as ex:\n        raise HTTPException(status_code=422, detail=\"Couldn't process query\")\n    return cron_list\n\n\n@router.get(\"/search/\", response_model=List[dict])\n@limiter.limit(f\"{RATE_LIMIT_PER_MIN}/minute\")\n@version(1)\nasync def search_cron(\n    request: Request,\n    dataset_title: str = Query(\n        ..., description=\"The title of the dataset to search for.\"\n    ),\n    skip: int = Query(0, description=\"Number of entries to skip.\"),\n    limit: int = Query(10, description=\"Maximum number of entries to retrieve.\"),\n):\n    \"\"\"\n    Search for Cron entries by dataset title.\n\n    Args:\n        request (Request): The request object.\n        dataset_title (str): The title of the dataset to search for.\n        skip (int): Number of entries to skip.\n        limit (int): Maximum number of entries to retrieve.\n\n    Returns:\n        List[dict]: List of Cron entries matching the dataset title.\n    \"\"\"\n    cron_instance = Cron()\n    cron_list = cron_instance.search_cron_by_dataset_title(dataset_title, skip, limit)\n    return cron_list\n\n\n@router.get(\"/{cron_id}\", response_model=dict)\n@limiter.limit(f\"{RATE_LIMIT_PER_MIN}/minute\")\n@version(1)\nasync def read_cron(request: Request, cron_id: int):\n    \"\"\"\n    Retrieve a specific cron entry by its ID.\n\n    Args:\n        request (Request): The request object.\n        cron_id (int): ID of the cron entry to retrieve.\n\n    Returns:\n        dict: Details of the requested cron entry.\n\n    Raises:\n        HTTPException: If the cron entry is not found.\n    \"\"\"\n    cron_instance = Cron()\n    cron = cron_instance.get_cron_by_id(cron_id)\n    if cron:\n        return cron\n    raise HTTPException(status_code=404, detail=\"cron not found\")\n\n\n@router.put(\"/{cron_id}\", response_model=dict)\n@limiter.limit(f\"{RATE_LIMIT_PER_MIN}/minute\")\n@version(1)\nasync def update_cron(\n    request: Request,\n    cron_id: int,\n    cron_data: dict,\n    user_data: AuthUser = Depends(staff_required),\n):\n    \"\"\"\n    Update an existing cron entry.\n\n    Args:\n        request (Request): The request object.\n        cron_id (int): ID of the cron entry to update.\n        cron_data (dict): Data for updating the cron entry.\n        user_data (AuthUser): User authentication data.\n\n    Returns:\n        dict: Result of the cron update process.\n\n    Raises:\n        HTTPException: If the cron entry is not found.\n    \"\"\"\n    cron_instance = Cron()\n    existing_cron = cron_instance.get_cron_by_id(cron_id)\n    if not existing_cron:\n        raise HTTPException(status_code=404, detail=\"cron not found\")\n    cron_instance_update = Cron()\n    return cron_instance_update.update_cron(cron_id, cron_data)\n\n\n@router.patch(\"/{cron_id}\", response_model=Dict)\n@limiter.limit(f\"{RATE_LIMIT_PER_MIN}/minute\")\n@version(1)\nasync def patch_cron(\n    request: Request,\n    cron_id: int,\n    cron_data: Dict,\n    user_data: AuthUser = Depends(staff_required),\n):\n    \"\"\"\n    Partially update an existing cron entry.\n\n    Args:\n        request (Request): The request object.\n        cron_id (int): ID of the cron entry to update.\n        cron_data (Dict): Data for partially updating the cron entry.\n        user_data (AuthUser): User authentication data.\n\n    Returns:\n        Dict: Result of the cron update process.\n\n    Raises:\n        HTTPException: If the cron entry is not found.\n    \"\"\"\n    cron_instance = Cron()\n    existing_cron = cron_instance.get_cron_by_id(cron_id)\n    if not existing_cron:\n        raise HTTPException(status_code=404, detail=\"cron not found\")\n    patch_instance = Cron()\n    return patch_instance.patch_cron(cron_id, cron_data)\n\n\n@router.delete(\"/{cron_id}\", response_model=dict)\n@limiter.limit(f\"{RATE_LIMIT_PER_MIN}/minute\")\n@version(1)\nasync def delete_cron(\n    request: Request, cron_id: int, user_data: AuthUser = Depends(admin_required)\n):\n    \"\"\"\n    Delete an existing cron entry.\n\n    Args:\n        request (Request): The request object.\n        cron_id (int): ID of the cron entry to delete.\n        user_data (AuthUser): User authentication data.\n\n    Returns:\n        dict: Result of the cron deletion process.\n\n    Raises:\n        HTTPException: If the cron entry is not found.\n    \"\"\"\n    cron_instance = Cron()\n    existing_cron = cron_instance.get_cron_by_id(cron_id)\n    if not existing_cron:\n        raise HTTPException(status_code=404, detail=\"cron not found\")\n\n    return cron_instance.delete_cron(cron_id)\n"}
{"type": "source_file", "path": "API/auth/__init__.py", "content": "from enum import Enum\nfrom typing import Union\n\nfrom fastapi import Depends, Header, HTTPException\nfrom osm_login_python.core import Auth\nfrom pydantic import BaseModel, Field\n\nfrom src.app import Users\nfrom src.config import get_oauth_credentials\n\n\nclass UserRole(Enum):\n    ADMIN = 1\n    STAFF = 2\n    GUEST = 3\n\n\nclass AuthUser(BaseModel):\n    id: int\n    username: str\n    img_url: Union[str, None]\n    role: UserRole = Field(default=UserRole.GUEST.value)\n\n\nosm_auth = Auth(*get_oauth_credentials())\n\n\ndef get_user_from_db(osm_id: int):\n    auth = Users()\n    user = auth.read_user(osm_id)\n    return user\n\n\ndef get_osm_auth_user(access_token):\n    try:\n        user = AuthUser(**osm_auth.deserialize_access_token(access_token))\n    except Exception as ex:\n        raise HTTPException(\n            status_code=403, detail=[{\"msg\": \"OSM Authentication failed\"}]\n        )\n    db_user = get_user_from_db(user.id)\n    user.role = db_user[\"role\"]\n    return user\n\n\ndef login_required(access_token: str = Header(...)):\n    return get_osm_auth_user(access_token)\n\n\ndef get_optional_user(access_token: str = Header(default=None)) -> AuthUser:\n    if access_token:\n        return get_osm_auth_user(access_token)\n    else:\n        # If no token provided, return a user with limited options or guest user\n        return AuthUser(id=0, username=\"guest\", img_url=None)\n\n\ndef admin_required(user: AuthUser = Depends(login_required)):\n    db_user = get_user_from_db(user.id)\n    if not db_user[\"role\"] is UserRole.ADMIN.value:\n        raise HTTPException(status_code=403, detail=\"User is not an admin\")\n    return user\n\n\ndef staff_required(user: AuthUser = Depends(login_required)):\n    db_user = get_user_from_db(user.id)\n\n    # admin is staff too\n    if not (\n        db_user[\"role\"] is UserRole.STAFF.value\n        or db_user[\"role\"] is UserRole.ADMIN.value\n    ):\n        raise HTTPException(status_code=403, detail=\"User is not a staff\")\n    return user\n"}
{"type": "source_file", "path": "API/main.py", "content": "# Copyright (C) 2021 Humanitarian OpenStreetmap Team\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\n# Standard library imports\n# Humanitarian OpenStreetmap Team\n# 1100 13th Street NW Suite 800 Washington, D.C. 20005\n# <info@hotosm.org>\nimport time\n\n# Third party imports\nimport psycopg2\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom fastapi_versioning import VersionedFastAPI\nfrom slowapi import _rate_limit_exceeded_handler\nfrom slowapi.errors import RateLimitExceeded\n\n# Reader imports\nfrom src.config import (\n    ENABLE_CUSTOM_EXPORTS,\n    ENABLE_HDX_EXPORTS,\n    ENABLE_METRICS_APIS,\n    ENABLE_POLYGON_STATISTICS_ENDPOINTS,\n    EXPORT_PATH,\n    LIMITER,\n    LOG_LEVEL,\n    SENTRY_DSN,\n    SENTRY_RATE,\n    SETUP_INITIAL_TABLES,\n    USE_CONNECTION_POOLING,\n    USE_S3_TO_UPLOAD,\n    get_db_connection_params,\n)\nfrom src.config import logger as logging\nfrom src.db_session import database_instance\n\nfrom .auth.routers import router as auth_router\nfrom .custom_exports import router as custom_exports_router\nfrom .raw_data import router as raw_data_router\nfrom .tasks import router as tasks_router\n\nif USE_S3_TO_UPLOAD:\n    from .s3 import router as s3_router\n\nif ENABLE_POLYGON_STATISTICS_ENDPOINTS:\n    from .stats import router as stats_router\n\nif ENABLE_METRICS_APIS:\n    from .download_metrics import router as metrics_router\n\nif ENABLE_HDX_EXPORTS:\n    from .cron import router as cron_router\n\nif SENTRY_DSN:\n    # Third party imports\n    import sentry_sdk\n\n# only use sentry if it is specified in config blocks\nif SENTRY_DSN:\n    sentry_sdk.init(\n        dsn=SENTRY_DSN,\n        # Set traces_sample_rate to 1.0 to capture 100%\n        # of transactions for performance monitoring.\n        # We recommend adjusting this value in production.\n        traces_sample_rate=SENTRY_RATE,\n    )\n\nif LOG_LEVEL.lower() == \"debug\":\n    # This is used for local setup for auth login\n    # Standard library imports\n    import os\n\n    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n\napp = FastAPI(title=\"Raw Data API \", swagger_ui_parameters={\"syntaxHighlight\": False})\napp.include_router(auth_router)\napp.include_router(raw_data_router)\napp.include_router(tasks_router)\n\nif ENABLE_CUSTOM_EXPORTS:\n    app.include_router(custom_exports_router)\nif ENABLE_POLYGON_STATISTICS_ENDPOINTS:\n    app.include_router(stats_router)\nif ENABLE_METRICS_APIS:\n    app.include_router(metrics_router)\nif ENABLE_HDX_EXPORTS:\n    app.include_router(cron_router)\n\nif USE_S3_TO_UPLOAD:\n    app.include_router(s3_router)\n\napp.openapi = {\n    \"info\": {\n        \"title\": \"Raw Data API\",\n        \"version\": \"1.0\",\n    },\n    \"security\": [{\"OAuth2PasswordBearer\": []}],\n}\n\napp = VersionedFastAPI(\n    app, enable_latest=False, version_format=\"{major}\", prefix_format=\"/v{major}\"\n)\n\nif USE_S3_TO_UPLOAD is False:\n    # only mount the disk if config is set to disk\n    app.mount(\"/exports\", StaticFiles(directory=EXPORT_PATH), name=\"exports\")\n\napp.state.limiter = LIMITER\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\norigins = [\"*\"]\n\n\n@app.middleware(\"http\")\nasync def add_process_time_header(request, call_next):\n    \"\"\"Times request and knows response time and pass it to header in every request\n\n    Args:\n        request (_type_): _description_\n        call_next (_type_): _description_\n\n    Returns:\n        header with process time\n    \"\"\"\n    start_time = time.time()\n    response = await call_next(request)\n    process_time = time.time() - start_time\n    response.headers[\"X-Process-Time\"] = str(f\"{process_time:0.4f} sec\")\n    return response\n\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.on_event(\"startup\")\nasync def on_startup():\n    \"\"\"Fires up 3 idle conenction with threaded connection pooling before starting the API\n\n    Raises:\n        e: if connection is rejected to database\n    \"\"\"\n    try:\n        if SETUP_INITIAL_TABLES:\n            sql_file_path = os.path.join(\n                os.path.realpath(os.path.dirname(__file__)), \"data/tables.sql\"\n            )\n            with open(sql_file_path, \"r\", encoding=\"UTF-8\") as sql_file:\n                create_tables_sql = sql_file.read()\n            conn = psycopg2.connect(**get_db_connection_params())\n            cursor = conn.cursor()\n            # Execute SQL statements\n            cursor.execute(create_tables_sql)\n            conn.commit()\n\n            # Close the cursor and connection\n            cursor.close()\n            conn.close()\n\n        if USE_CONNECTION_POOLING:\n            database_instance.connect()\n    except Exception as e:\n        logging.error(e)\n        raise e\n\n\n@app.on_event(\"shutdown\")\ndef on_shutdown():\n    \"\"\"Closing all the threads connection from pooling before shuting down the api\"\"\"\n    if USE_CONNECTION_POOLING:\n        logging.debug(\"Shutting down connection pool\")\n        database_instance.close_all_connection_pool()\n"}
{"type": "source_file", "path": "API/__init__.py", "content": ""}
{"type": "source_file", "path": "API/api_worker.py", "content": "# Standard library imports\nimport json\nimport os\nimport pathlib\nimport re\nimport shutil\nimport time\nfrom datetime import datetime as dt\nfrom datetime import timedelta, timezone\n\n# Third party imports\nimport humanize\nimport psutil\nimport zipfly\nfrom celery import Celery\n\n# Reader imports\nfrom src.app import CustomExport, PolygonStats, RawData, S3FileTransfer\nfrom src.config import ALLOW_BIND_ZIP_FILTER, CELERY_BROKER_HEARTBEAT\nfrom src.config import CELERY_BROKER_URL as celery_broker_uri\nfrom src.config import CELERY_RESULT_BACKEND as celery_backend\nfrom src.config import (\n    CELERY_WORKER_LOST_WAIT,\n    DEFAULT_HARD_TASK_LIMIT,\n    DEFAULT_README_TEXT,\n    DEFAULT_SOFT_TASK_LIMIT,\n    ENABLE_SOZIP,\n    ENABLE_TILES,\n    EXPORT_PATH,\n    HDX_HARD_TASK_LIMIT,\n    HDX_SOFT_TASK_LIMIT,\n)\nfrom src.config import USE_S3_TO_UPLOAD as use_s3_to_upload\nfrom src.config import WORKER_PREFETCH_MULTIPLIER\nfrom src.config import logger as logging\nfrom src.query_builder.builder import format_file_name_str\nfrom src.validation.models import (\n    DatasetConfig,\n    DynamicCategoriesModel,\n    RawDataCurrentParams,\n    RawDataOutputType,\n)\n\nif ENABLE_SOZIP:\n    # Third party imports\n    import sozipfile.sozipfile as zipfile\nelse:\n    # Standard library imports\n    import zipfile\n\n# Sentry initialization\n# Reader imports\nfrom src.config import SENTRY_DSN, SENTRY_RATE\n\nif SENTRY_DSN:\n    # Third party imports\n    import sentry_sdk\n    sentry_sdk.init(\n        dsn=SENTRY_DSN,\n        traces_sample_rate=SENTRY_RATE,\n    )\n\n\n\ncelery = Celery(\"Raw Data API\")\ncelery.conf.broker_url = celery_broker_uri\ncelery.conf.result_backend = celery_backend\ncelery.conf.broker_heartbeat = CELERY_BROKER_HEARTBEAT\ncelery.conf.worker_lost_wait = CELERY_WORKER_LOST_WAIT\n# celery.conf.task_serializer = \"pickle\"\n# celery.conf.result_serializer = \"json\"\n# celery.conf.accept_content = [\"application/json\", \"application/x-python-serialize\"]\ncelery.conf.task_track_started = True\ncelery.conf.update(result_extended=True)\ncelery.conf.task_reject_on_worker_lost = False\ncelery.conf.task_acks_late = False  # to avoid task duplication\n\nif WORKER_PREFETCH_MULTIPLIER:\n    celery.conf.update(worker_prefetch_multiplier=WORKER_PREFETCH_MULTIPLIER)\n\n\ndef create_readme_content(default_readme, polygon_stats):\n    utc_now = dt.now(timezone.utc)\n    utc_offset = utc_now.strftime(\"%z\")\n    readme_content = f\"Exported Timestamp (UTC{utc_offset}): {dt.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n    readme_content += default_readme\n    if polygon_stats:\n        readme_content += f'{polygon_stats[\"summary\"][\"buildings\"]}\\n'\n        readme_content += f'{polygon_stats[\"summary\"][\"roads\"]}\\n'\n        readme_content += \"Read about what this summary means: indicators: https://github.com/hotosm/raw-data-api/tree/develop/docs/src/stats/indicators.md,metrics: https://github.com/hotosm/raw-data-api/tree/develop/docs/src/stats/metrics.md\"\n    return readme_content\n\n\ndef zip_binding(\n    working_dir, exportname_parts, geom_dump, polygon_stats, default_readme\n):\n    logging.debug(\"Zip Binding Started!\")\n    upload_file_path = os.path.join(\n        working_dir, os.pardir, f\"{exportname_parts[-1]}.zip\"\n    )\n    additional_files = {\n        \"clipping_boundary.geojson\": geom_dump,\n        \"Readme.txt\": create_readme_content(\n            default_readme=default_readme, polygon_stats=polygon_stats\n        ),\n    }\n\n    for name, content in additional_files.items():\n        temp_path = os.path.join(working_dir, name)\n        with open(temp_path, \"w\") as f:\n            f.write(content)\n    logging.debug(\"Metadata written\")\n    inside_file_size = sum(\n        os.path.getsize(f)\n        for f in pathlib.Path(working_dir).glob(\"**/*\")\n        if f.is_file()\n    )\n    logging.debug(\"Total %s to be zipped\", humanize.naturalsize(inside_file_size))\n\n    system_ram = psutil.virtual_memory().total  # system RAM in bytes\n    if (\n        inside_file_size > 0.8 * system_ram or inside_file_size > 3 * 1024**3\n    ):  # if file size is greater than 80% of ram or greater than 3 gb\n        logging.debug(\n            \"Using memory optimized zip\",\n        )\n\n        paths = []\n        for root, dirs, files in os.walk(working_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                rel_path = os.path.relpath(file_path, working_dir)\n                paths.append({\"fs\": str(file_path), \"n\": rel_path})\n\n        zfly = zipfly.ZipFly(paths=paths)\n        generator = zfly.generator()\n        with open(upload_file_path, \"wb\") as f:\n            for chunk in generator:\n                f.write(chunk)\n\n    else:\n        logging.debug(\"Using default zipfile module for zipping\")\n        with zipfile.ZipFile(\n            upload_file_path,\n            \"w\",\n            compression=zipfile.ZIP_DEFLATED,\n            compresslevel=9,\n            allowZip64=True,\n        ) as zf:\n            for root, dirs, files in os.walk(working_dir):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    rel_path = os.path.relpath(file_path, working_dir)\n                    zf.write(file_path, arcname=rel_path)\n\n    logging.debug(\"Zip Binding Done!\")\n    return upload_file_path, inside_file_size\n\n\nclass BaseclassTask(celery.Task):\n    \"\"\"Base class for celery tasks\n\n    Args:\n        celery (_type_): _description_\n    \"\"\"\n\n    def on_failure(self, exc, task_id, args, kwargs, einfo):\n        \"\"\"Logic when task fails\n\n        Args:\n            exc (_type_): _description_\n            task_id (_type_): _description_\n            args (_type_): _description_\n            kwargs (_type_): _description_\n            einfo (_type_): _description_\n        \"\"\"\n        # exc (Exception) - The exception raised by the task.\n        # args (Tuple) - Original arguments for the task that failed.\n        # kwargs (Dict) - Original keyword arguments for the task that failed.\n        print(\"{0!r} failed: {1!r}\".format(task_id, exc))\n        clean_dir = os.path.join(EXPORT_PATH, task_id)\n        if os.path.exists(clean_dir):\n            shutil.rmtree(clean_dir)\n\n\n@celery.task(\n    bind=True,\n    name=\"process_raw_data\",\n    time_limit=DEFAULT_HARD_TASK_LIMIT,\n    soft_time_limit=DEFAULT_SOFT_TASK_LIMIT,\n)\ndef process_raw_data(self, params, user=None):\n    if self.request.retries > 0:\n        raise ValueError(\"Retry limit reached. Marking task as failed.\")\n    params = RawDataCurrentParams(**params)\n    try:\n        start_time = time.time()\n        bind_zip = params.bind_zip if ALLOW_BIND_ZIP_FILTER else True\n        # unique id for zip file and geojson for each export\n        params.output_type = (\n            params.output_type\n            if params.output_type\n            else RawDataOutputType.GEOJSON.value\n        )\n        if ENABLE_TILES:\n            if (\n                params.output_type == RawDataOutputType.PMTILES.value\n                or params.output_type == RawDataOutputType.MBTILES.value\n            ):\n                logging.debug(\"Using STwithin Logic\")\n                params.use_st_within = True\n\n        params.file_name = (\n            format_file_name_str(params.file_name) if params.file_name else \"RawExport\"\n        )\n\n        exportname = f\"{params.file_name}_{params.output_type}{f'_uid_{str(self.request.id)}' if params.uuid else ''}\"\n        params.file_name = params.file_name.split(\"/\")[\n            -1\n        ]  # get last item from list and consider it as a file rest is file path on s3\n        exportname_parts = exportname.split(\"/\")\n        file_parts = os.path.join(*exportname_parts)\n        logging.info(\n            \"Request %s received with following %s file_path\",\n            params.file_name,\n            file_parts,\n        )\n\n        geom_area, geom_dump, working_dir = RawData(\n            params, str(self.request.id)\n        ).extract_current_data(file_parts)\n        inside_file_size = 0\n        polygon_stats = None\n        if \"include_stats\" in params.dict():\n            if params.include_stats:\n                feature = {\n                    \"type\": \"Feature\",\n                    \"geometry\": json.loads(params.geometry.model_dump_json()),\n                    \"properties\": {},\n                }\n                polygon_stats = PolygonStats(feature).get_summary_stats()\n        if bind_zip:\n            upload_file_path, inside_file_size = zip_binding(\n                working_dir=working_dir,\n                exportname_parts=exportname_parts,\n                geom_dump=geom_dump,\n                polygon_stats=polygon_stats,\n                default_readme=DEFAULT_README_TEXT,\n            )\n\n            logging.debug(\"Zip Binding Done !\")\n        else:\n            for file_path in pathlib.Path(working_dir).iterdir():\n                if file_path.is_file() and file_path.name.endswith(\n                    params.output_type.lower()\n                ):\n                    upload_file_path = file_path\n                    inside_file_size += os.path.getsize(file_path)\n                    break  # only take one file inside dir , if contains many it should be inside zip\n        # check if download url will be generated from s3 or not from config\n        if use_s3_to_upload:\n            file_transfer_obj = S3FileTransfer()\n            upload_name = (\n                f\"default/{file_parts}\" if params.uuid else f\"recurring/{file_parts}\"\n            )\n            logging.info(upload_name)\n\n            if exportname.startswith(\"hotosm_project\"):  # TM\n                if not params.uuid:\n                    pattern = r\"(hotosm_project_)(\\d+)\"\n                    match = re.match(pattern, exportname)\n                    if match:\n                        prefix = match.group(1)\n                        project_number = match.group(2)\n                        if project_number:\n                            upload_name = f\"TM/{project_number}/{exportname}\"\n            elif exportname.startswith(\"hotosm_\"):  # HDX\n                if not params.uuid:\n                    pattern = r\"hotosm_([A-Za-z]{3})_(\\w+)\"\n                    match = re.match(pattern, exportname)\n\n                    if match:\n                        iso3countrycode = match.group(1)\n                        if iso3countrycode:\n                            upload_name = f\"HDX/{iso3countrycode.upper()}/{exportname}\"\n\n            download_url = file_transfer_obj.upload(\n                upload_file_path,\n                upload_name,\n                file_suffix=\"zip\" if bind_zip else params.output_type.lower(),\n            )\n        else:\n            # give the static file download url back to user served from fastapi static export path\n            download_url = str(upload_file_path)\n\n        # getting file size of zip , units are in bytes converted to mb in response\n        zip_file_size = os.path.getsize(upload_file_path)\n        if use_s3_to_upload or bind_zip:\n            # remove working dir from the machine , if its inside zip / uploaded we no longer need it\n            remove_file(working_dir)\n        response_time_str = humanize.naturaldelta(\n            timedelta(seconds=(time.time() - start_time))\n        )\n        logging.info(\n            f\"Done Export : {exportname} of {round(inside_file_size/1000000)} MB / {geom_area} sqkm in {response_time_str}\"\n        )\n        final_response = {\n            \"download_url\": download_url,\n            \"file_name\": params.file_name,\n            \"process_time\": response_time_str,\n            \"query_area\": f\"{round(geom_area,2)} Sq Km\",\n            \"binded_file_size\": f\"{round(inside_file_size/1000000,2)} MB\",\n            \"zip_file_size_bytes\": zip_file_size,\n        }\n        if polygon_stats:\n            final_response[\"stats\"] = polygon_stats\n        return final_response\n\n    except Exception as ex:\n        if os.path.exists(os.path.join(EXPORT_PATH, str(self.request.id))):\n            shutil.rmtree(os.path.join(EXPORT_PATH, str(self.request.id)))\n        raise ex\n\n\n@celery.task(\n    bind=True,\n    name=\"process_custom_request\",\n    time_limit=HDX_HARD_TASK_LIMIT,\n    soft_time_limit=HDX_SOFT_TASK_LIMIT,\n    base=BaseclassTask,\n)\ndef process_custom_request(self, params, user=None):\n    if self.request.retries > 0:\n        raise ValueError(\"Retry limit reached. Marking task as failed.\")\n    params = DynamicCategoriesModel(**params)\n\n    if not params.dataset:\n        params.dataset = DatasetConfig()\n    custom_object = CustomExport(params, uid=self.request.id)\n    try:\n        return custom_object.process_custom_categories()\n    except Exception as ex:\n        custom_object.clean_resources()\n        raise ex\n\n\ndef remove_file(path: str) -> None:\n    \"\"\"Used for removing temp file dir and its all content after zip file is delivered to user\"\"\"\n    try:\n        shutil.rmtree(path)\n    except OSError as ex:\n        logging.error(\"Error: %s - %s.\", ex.filename, ex.strerror)\n"}
{"type": "source_file", "path": "API/tasks.py", "content": "# Standard library imports\nimport json\nfrom datetime import datetime\n\n# Third party imports\nimport redis\nfrom celery.result import AsyncResult\nfrom fastapi import APIRouter, Depends, HTTPException, Query, Request\nfrom fastapi.responses import JSONResponse\nfrom fastapi_versioning import version\n\n# Reader imports\nfrom src.config import CELERY_BROKER_URL, DEFAULT_QUEUE_NAME, ONDEMAND_QUEUE_NAME\nfrom src.validation.models import SnapshotTaskResponse\n\nfrom .api_worker import celery\nfrom .auth import AuthUser, admin_required, login_required, staff_required\n\nrouter = APIRouter(prefix=\"/tasks\", tags=[\"Tasks\"])\n\n\n@router.get(\"/status/{task_id}/\", response_model=SnapshotTaskResponse)\n@version(1)\ndef get_task_status(\n    task_id,\n    only_args: bool = Query(\n        default=False,\n        description=\"Fetches arguments of task\",\n    ),\n):\n    \"\"\"Tracks the request from the task id provided by Raw Data API  for the request\n\n    Args:\n\n        task_id ([type]): [Unique id provided on response from /snapshot/]\n\n    Returns:\n\n        id: Id of the task\n        status : Possible values includes:\n\n                PENDING\n\n                    The task is waiting for execution.\n\n                STARTED\n\n                    The task has been started.\n\n                RETRY\n\n                    The task is to be retried, possibly because of failure.\n\n                FAILURE\n\n                    The task raised an exception, or has exceeded the retry limit. The result attribute then contains the exception raised by the task.\n\n                SUCCESS\n\n                    The task executed successfully. The result attribute then contains the tasks return value.\n\n        result : Result of task\n\n    Successful task will have additional nested json inside\n\n    \"\"\"\n    task_result = AsyncResult(task_id, app=celery)\n    if only_args:\n        return JSONResponse(task_result.args)\n    task_response_result = None\n    if task_result.status == \"SUCCESS\":\n        task_response_result = task_result.result\n    if task_result.state != \"SUCCESS\":\n        task_response_result = \"N/A\"  # TODO : Make sure no imp information is popping out outside of the API\n        # task_response_result = str(task_result.info)\n\n    result = {\n        \"id\": task_id,\n        \"status\": task_result.state,\n        \"result\": task_response_result,\n    }\n    return JSONResponse(result)\n\n\n@router.get(\"/revoke/{task_id}/\")\n@version(1)\ndef revoke_task(task_id, user: AuthUser = Depends(staff_required)):\n    \"\"\"Revokes task , Terminates if it is executing\n\n    Args:\n        task_id (_type_): task id of raw data task\n\n    Returns:\n        id: id of revoked task\n    \"\"\"\n    celery.control.revoke(task_id=task_id, terminate=True)\n    return JSONResponse({\"id\": task_id})\n\n\n@router.get(\"/inspect/\")\n@version(1)\ndef inspect_workers(\n    request: Request,\n    summary: bool = Query(\n        default=True,\n        description=\"Displays summary of tasks\",\n    ),\n):\n    \"\"\"Inspects tasks assigned to workers\n\n    Returns:\n        active: Current Active tasks ongoing on workers\n    \"\"\"\n    inspected = celery.control.inspect()\n    active_tasks = inspected.active()\n    active_tasks_summary = []\n\n    if summary:\n        if active_tasks:\n            for worker, tasks in active_tasks.items():\n                worker_tasks = {worker: {}}\n\n                for task in tasks:\n                    worker_tasks[worker][\"id\"] = task[\"id\"]\n                    worker_tasks[worker][\"task\"] = task[\"name\"]\n                    worker_tasks[worker][\"time_start\"] = (\n                        datetime.fromtimestamp(task[\"time_start\"]).strftime(\n                            \"%Y-%m-%d %H:%M:%S\"\n                        )\n                        if task[\"time_start\"]\n                        else None\n                    )\n                active_tasks_summary.append(worker_tasks)\n\n    response_data = {\n        \"active\": active_tasks_summary if summary else active_tasks,\n    }\n    return JSONResponse(content=response_data)\n\n\n@router.get(\"/ping/\")\n@version(1)\ndef ping_workers():\n    \"\"\"Pings available workers\n\n    Returns: {worker_name : return_result}\n    \"\"\"\n    inspected_ping = celery.control.inspect().ping()\n    return JSONResponse(inspected_ping)\n\n\n@router.get(\"/purge/\")\n@version(1)\ndef discard_all_waiting_tasks(user: AuthUser = Depends(admin_required)):\n    \"\"\"\n    Discards all waiting tasks from the queue\n    Returns : Number of tasks discarded\n    \"\"\"\n    purged = celery.control.purge()\n    return JSONResponse({\"tasks_discarded\": purged})\n\n\nqueues = [DEFAULT_QUEUE_NAME, ONDEMAND_QUEUE_NAME]\n\n\n@router.get(\"/queue/\")\n@version(1)\ndef get_queue_info():\n    queue_info = {}\n    redis_client = redis.StrictRedis.from_url(CELERY_BROKER_URL)\n\n    for queue_name in queues:\n        # Get queue length\n        queue_length = redis_client.llen(queue_name)\n\n        queue_info[queue_name] = {\n            \"length\": queue_length,\n        }\n\n    return JSONResponse(content=queue_info)\n\n\n@router.get(\"/queue/details/{queue_name}/\")\n@version(1)\ndef get_list_details(\n    queue_name: str,\n    args: bool = Query(\n        default=False,\n        description=\"Includes arguments of task\",\n    ),\n):\n    if queue_name not in queues:\n        raise HTTPException(status_code=404, detail=f\"Queue '{queue_name}' not found\")\n    redis_client = redis.StrictRedis.from_url(CELERY_BROKER_URL)\n\n    list_items = redis_client.lrange(queue_name, 0, -1)\n\n    # Convert bytes to strings\n    list_items = [item.decode(\"utf-8\") for item in list_items]\n\n    items_details = [\n        {\n            \"index\": index,\n            \"id\": json.loads(item)[\"headers\"][\"id\"],\n            **({\"args\": json.loads(item)[\"headers\"][\"argsrepr\"]} if args else {}),\n        }\n        for index, item in enumerate(list_items)\n    ]\n\n    return JSONResponse(content=items_details)\n"}
{"type": "source_file", "path": "API/auth/routers.py", "content": "import json\n\nfrom fastapi import APIRouter, Depends, Request\nfrom pydantic import BaseModel\n\nfrom src.app import Users\n\nfrom . import AuthUser, admin_required, login_required, osm_auth, staff_required\n\nrouter = APIRouter(prefix=\"/auth\", tags=[\"Auth\"])\n\n\n@router.get(\"/login/\")\ndef login_url(request: Request):\n    \"\"\"Generate Login URL for authentication using OAuth2 Application registered with OpenStreetMap.\n    Click on the download url returned to get access_token.\n\n    Parameters: None\n\n    Returns:\n    - login_url (dict) - URL to authorize user to the application via. Openstreetmap\n        OAuth2 with client_id, redirect_uri, and permission scope as query_string parameters\n    \"\"\"\n    login_url = osm_auth.login()\n    return login_url\n\n\n@router.get(\"/callback/\")\ndef callback(request: Request):\n    \"\"\"Performs token exchange between OpenStreetMap and Raw Data API\n\n    Core will use Oauth secret key from configuration while deserializing token,\n    provides access token that can be used for authorized endpoints.\n\n    Parameters: None\n\n    Returns:\n    - access_token (string)\n    \"\"\"\n    access_token = osm_auth.callback(str(request.url))\n\n    return access_token\n\n\n@router.get(\"/me/\", response_model=AuthUser)\ndef my_data(user_data: AuthUser = Depends(login_required)):\n    \"\"\"Read the access token and provide  user details from OSM user's API endpoint,\n    also integrated with underpass .\n\n    Parameters:None\n\n    Returns: user_data\n            User Role :\n                ADMIN = 1\n                STAFF = 2\n                GUEST = 3\n    \"\"\"\n    return user_data\n\n\nclass User(BaseModel):\n    osm_id: int\n    role: int\n\n\n# Create user\n@router.post(\"/users/\", response_model=dict)\nasync def create_user(params: User, user_data: AuthUser = Depends(admin_required)):\n    \"\"\"\n    Creates a new user and returns the user's information.\n    User Role :\n        ADMIN = 1\n        STAFF = 2\n        GUEST = 3\n\n    Args:\n    - params (User): The user data including osm_id and role.\n\n    Returns:\n    - Dict[str, Any]: A dictionary containing the osm_id of the newly created user.\n\n    Raises:\n    - HTTPException: If the user creation fails.\n    \"\"\"\n    auth = Users()\n    return auth.create_user(params.osm_id, params.role)\n\n\n# Read user by osm_id\n@router.get(\"/users/{osm_id}\", response_model=dict)\nasync def read_user(osm_id: int, user_data: AuthUser = Depends(staff_required)):\n    \"\"\"\n    Retrieves user information based on the given osm_id.\n    User Role :\n        ADMIN = 1\n        STAFF = 2\n        GUEST = 3\n\n    Args:\n    - osm_id (int): The OSM ID of the user to retrieve.\n\n    Returns:\n    - Dict[str, Any]: A dictionary containing user information.\n\n    Raises:\n    - HTTPException: If the user with the given osm_id is not found.\n    \"\"\"\n    auth = Users()\n\n    return auth.read_user(osm_id)\n\n\n# Update user by osm_id\n@router.put(\"/users/{osm_id}\", response_model=dict)\nasync def update_user(\n    osm_id: int, update_data: User, user_data: AuthUser = Depends(admin_required)\n):\n    \"\"\"\n    Updates user information based on the given osm_id.\n    User Role :\n        ADMIN = 1\n        STAFF = 2\n        GUEST = 3\n    Args:\n    - osm_id (int): The OSM ID of the user to update.\n    - update_data (User): The data to update for the user.\n\n    Returns:\n    - Dict[str, Any]: A dictionary containing the updated user information.\n\n    Raises:\n    - HTTPException: If the user with the given osm_id is not found.\n    \"\"\"\n    auth = Users()\n    return auth.update_user(osm_id, update_data)\n\n\n# Delete user by osm_id\n@router.delete(\"/users/{osm_id}\", response_model=dict)\nasync def delete_user(osm_id: int, user_data: AuthUser = Depends(admin_required)):\n    \"\"\"\n    Deletes a user based on the given osm_id.\n\n    Args:\n    - osm_id (int): The OSM ID of the user to delete.\n\n    Returns:\n    - Dict[str, Any]: A dictionary containing the deleted user information.\n\n    Raises:\n    - HTTPException: If the user with the given osm_id is not found.\n    \"\"\"\n    auth = Users()\n    return auth.delete_user(osm_id)\n\n\n# Get all users\n@router.get(\"/users/\", response_model=list)\nasync def read_users(\n    skip: int = 0, limit: int = 10, user_data: AuthUser = Depends(staff_required)\n):\n    \"\"\"\n    Retrieves a list of users with optional pagination.\n\n    Args:\n    - skip (int): The number of users to skip (for pagination).\n    - limit (int): The maximum number of users to retrieve (for pagination).\n\n    Returns:\n    - List[Dict[str, Any]]: A list of dictionaries containing user information.\n    \"\"\"\n    auth = Users()\n    return auth.read_users(skip, limit)\n"}
{"type": "source_file", "path": "API/custom_exports.py", "content": "# Standard library imports\nimport json\nfrom typing import Dict\n\n# Third party imports\nimport yaml\nfrom fastapi import APIRouter, Body, Depends, HTTPException, Request\nfrom fastapi.responses import JSONResponse\nfrom fastapi_versioning import version\nfrom pydantic import ValidationError\n\n# Reader imports\nfrom src.config import DEFAULT_QUEUE_NAME\nfrom src.config import LIMITER as limiter\nfrom src.config import RATE_LIMIT_PER_MIN\nfrom src.validation.models import CustomRequestsYaml, DynamicCategoriesModel\n\nfrom .api_worker import process_custom_request\nfrom .auth import AuthUser, UserRole, staff_required\n\nrouter = APIRouter(prefix=\"/custom\", tags=[\"Custom Exports\"])\n\n\n@router.post(\"/snapshot/\")\n@limiter.limit(f\"{RATE_LIMIT_PER_MIN}/minute\")\n@version(1)\nasync def process_custom_requests(\n    request: Request,\n    user: AuthUser = Depends(staff_required),\n    params: DynamicCategoriesModel = Body(\n        ...,\n        description=\"Input parameters including ISO3 country code and dynamic categories.\",\n        openapi_examples={\n            \"normal_iso\": {\n                \"summary\": \"Example: Road extraction using iso3\",\n                \"description\": \"Query to extract road in Nepal\",\n                \"value\": {\n                    \"iso3\": \"NPL\",\n                    \"categories\": [\n                        {\n                            \"Roads\": {\n                                \"hdx\": {\n                                    \"tags\": [\"roads\", \"transportation\", \"geodata\"],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"lines\"],\n                                \"select\": [\"name\", \"highway\"],\n                                \"where\": \"tags['highway'] IS NOT NULL\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        }\n                    ],\n                },\n            },\n            \"normal_iso_non_upload\": {\n                \"summary\": \"Example: Road extraction using iso3 on raw data api only\",\n                \"description\": \"Query to extract road in Nepal, without uploading to hdx\",\n                \"value\": {\n                    \"iso3\": \"NPL\",\n                    \"hdx_upload\": False,\n                    \"categories\": [\n                        {\n                            \"Roads\": {\n                                \"hdx\": {\n                                    \"tags\": [\"roads\", \"transportation\", \"geodata\"],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"lines\"],\n                                \"select\": [\"name\", \"highway\"],\n                                \"where\": \"tags['highway'] IS NOT NULL\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        }\n                    ],\n                },\n            },\n            \"normal_iso_multiple_format\": {\n                \"summary\": \"Example: Road extraction using iso3 Multiple format\",\n                \"description\": \"Query to extract road in Nepal Multiple format\",\n                \"value\": {\n                    \"iso3\": \"NPL\",\n                    \"categories\": [\n                        {\n                            \"Roads\": {\n                                \"hdx\": {\n                                    \"tags\": [\"roads\", \"transportation\", \"geodata\"],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"lines\"],\n                                \"select\": [\"name\", \"highway\"],\n                                \"where\": \"tags['highway'] IS NOT NULL\",\n                                \"formats\": [\"geojson\", \"gpkg\", \"kml\", \"shp\"],\n                            }\n                        }\n                    ],\n                },\n            },\n            \"normal_polygon\": {\n                \"summary\": \"Example: Road extraction set using custom polygon\",\n                \"description\": \"Query to extract road in Pokhara, Nepal\",\n                \"value\": {\n                    \"geometry\": {\n                        \"type\": \"Polygon\",\n                        \"coordinates\": [\n                            [\n                                [83.96919250488281, 28.194446860487773],\n                                [83.99751663208006, 28.194446860487773],\n                                [83.99751663208006, 28.214869548073377],\n                                [83.96919250488281, 28.214869548073377],\n                                [83.96919250488281, 28.194446860487773],\n                            ]\n                        ],\n                    },\n                    \"dataset\": {\n                        \"subnational\": True,\n                        \"dataset_title\": \"Pokhara\",\n                        \"dataset_prefix\": \"hotosm_pkr\",\n                        \"dataset_locations\": [\"npl\"],\n                    },\n                    \"categories\": [\n                        {\n                            \"Roads\": {\n                                \"hdx\": {\n                                    \"tags\": [\"roads\", \"transportation\", \"geodata\"],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"lines\"],\n                                \"select\": [\"name\", \"highway\"],\n                                \"where\": \"tags['highway'] IS NOT NULL\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        }\n                    ],\n                },\n            },\n            \"normal_polygon_TM\": {\n                \"summary\": \"Example: Tasking Manager Mapping type extraction for a Project\",\n                \"description\": \"Example Query to extract building,roads,waterways and landuse in sample TM Project , Pokhara, Nepal\",\n                \"value\": {\n                    \"geometry\": {\n                        \"type\": \"Polygon\",\n                        \"coordinates\": [\n                            [\n                                [83.96919250488281, 28.194446860487773],\n                                [83.99751663208006, 28.194446860487773],\n                                [83.99751663208006, 28.214869548073377],\n                                [83.96919250488281, 28.214869548073377],\n                                [83.96919250488281, 28.194446860487773],\n                            ]\n                        ],\n                    },\n                    \"queue\": \"raw_ondemand\",\n                    \"dataset\": {\n                        \"dataset_prefix\": \"hotosm_project_1\",\n                        \"dataset_folder\": \"TM\",\n                        \"dataset_title\": \"Tasking Manger Project 1\",\n                    },\n                    \"categories\": [\n                        {\n                            \"Buildings\": {\n                                \"types\": [\"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"building\",\n                                    \"building:levels\",\n                                    \"building:materials\",\n                                    \"addr:full\",\n                                    \"addr:housenumber\",\n                                    \"addr:street\",\n                                    \"addr:city\",\n                                    \"office\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['building'] IS NOT NULL\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                        {\n                            \"Roads\": {\n                                \"types\": [\"lines\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"highway\",\n                                    \"surface\",\n                                    \"smoothness\",\n                                    \"width\",\n                                    \"lanes\",\n                                    \"oneway\",\n                                    \"bridge\",\n                                    \"layer\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['highway'] IS NOT NULL\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                        {\n                            \"Waterways\": {\n                                \"types\": [\"lines\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"waterway\",\n                                    \"covered\",\n                                    \"width\",\n                                    \"depth\",\n                                    \"layer\",\n                                    \"blockage\",\n                                    \"tunnel\",\n                                    \"natural\",\n                                    \"water\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['waterway'] IS NOT NULL OR tags['water'] IS NOT NULL OR tags['natural'] IN ('water','wetland','bay')\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                        {\n                            \"Landuse\": {\n                                \"types\": [\"points\", \"polygons\"],\n                                \"select\": [\"name\", \"amenity\", \"landuse\", \"leisure\"],\n                                \"where\": \"tags['landuse'] IS NOT NULL\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                    ],\n                },\n            },\n            \"fullset\": {\n                \"summary\": \"Full HDX Dataset default\",\n                \"description\": \"Full yaml conversion for dataset with iso3 example\",\n                \"value\": {\n                    \"iso3\": \"MLI\",\n                    \"hdx_upload\": True,\n                    \"meta\": True,\n                    \"categories\": [\n                        {\n                            \"Buildings\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"facilities-infrastructure\",\n                                        \"buildings\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"building\",\n                                    \"building:levels\",\n                                    \"building:materials\",\n                                    \"addr:full\",\n                                    \"addr:housenumber\",\n                                    \"addr:street\",\n                                    \"addr:city\",\n                                    \"office\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['building'] IS NOT NULL\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        },\n                        {\n                            \"Roads\": {\n                                \"hdx\": {\n                                    \"tags\": [\"transportation\", \"roads\", \"geodata\"],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"lines\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"highway\",\n                                    \"surface\",\n                                    \"smoothness\",\n                                    \"width\",\n                                    \"lanes\",\n                                    \"oneway\",\n                                    \"bridge\",\n                                    \"layer\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['highway'] IS NOT NULL\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        },\n                        {\n                            \"Waterways\": {\n                                \"hdx\": {\n                                    \"tags\": [\"hydrology\", \"rivers\", \"geodata\"],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"lines\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"waterway\",\n                                    \"covered\",\n                                    \"width\",\n                                    \"depth\",\n                                    \"layer\",\n                                    \"blockage\",\n                                    \"tunnel\",\n                                    \"natural\",\n                                    \"water\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['waterway'] IS NOT NULL OR tags['water'] IS NOT NULL OR tags['natural'] IN ('water','wetland','bay')\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        },\n                        {\n                            \"Points of Interest\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"facilities-infrastructure\",\n                                        \"points of interest-poi\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"amenity\",\n                                    \"man_made\",\n                                    \"shop\",\n                                    \"tourism\",\n                                    \"opening_hours\",\n                                    \"beds\",\n                                    \"rooms\",\n                                    \"addr:full\",\n                                    \"addr:housenumber\",\n                                    \"addr:street\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['amenity'] IS NOT NULL OR tags['man_made'] IS NOT NULL OR tags['shop'] IS NOT NULL OR tags['tourism'] IS NOT NULL\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        },\n                        {\n                            \"Airports\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"aviation\",\n                                        \"facilities-infrastructure\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\", \"lines\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"aeroway\",\n                                    \"building\",\n                                    \"emergency\",\n                                    \"emergency:helipad\",\n                                    \"operator:type\",\n                                    \"capacity:persons\",\n                                    \"addr:full\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['aeroway'] IS NOT NULL OR tags['building'] = 'aerodrome' OR tags['emergency:helipad'] IS NOT NULL OR tags['emergency'] = 'landing_site'\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        },\n                        {\n                            \"Sea Ports\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"facilities-infrastructure\",\n                                        \"ports\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\", \"lines\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"amenity\",\n                                    \"building\",\n                                    \"port\",\n                                    \"operator:type\",\n                                    \"addr:full\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['amenity'] = 'ferry_terminal' OR tags['building'] = 'ferry_terminal' OR tags['port'] IS NOT NULL\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        },\n                        {\n                            \"Education Facilities\": {\n                                \"hdx\": {\n                                    \"tags\": [\"education facilities-schools\", \"geodata\"],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"amenity\",\n                                    \"building\",\n                                    \"operator:type\",\n                                    \"capacity:persons\",\n                                    \"addr:full\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['amenity'] IN ('kindergarten', 'school', 'college', 'university') OR tags['building'] IN ('kindergarten', 'school', 'college', 'university')\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        },\n                        {\n                            \"Health Facilities\": {\n                                \"hdx\": {\n                                    \"tags\": [\"geodata\", \"health facilities\", \"health\"],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"amenity\",\n                                    \"building\",\n                                    \"healthcare\",\n                                    \"healthcare:speciality\",\n                                    \"operator:type\",\n                                    \"capacity:persons\",\n                                    \"addr:full\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['healthcare'] IS NOT NULL OR tags['amenity'] IN ('doctors', 'dentist', 'clinic', 'hospital', 'pharmacy')\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        },\n                        {\n                            \"Populated Places\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"populated places-settlements\",\n                                        \"villages\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"place\",\n                                    \"population\",\n                                    \"is_in\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['place'] IN ('isolated_dwelling', 'town', 'village', 'hamlet', 'city')\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        },\n                        {\n                            \"Financial Services\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"economics\",\n                                        \"financial institutions\",\n                                        \"financial services\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"amenity\",\n                                    \"operator\",\n                                    \"network\",\n                                    \"addr:full\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['amenity'] IN ('mobile_money_agent','bureau_de_change','bank','microfinance','atm','sacco','money_transfer','post_office')\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        },\n                        {\n                            \"Railways\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"facilities-infrastructure\",\n                                        \"railways\",\n                                        \"transportation\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"lines\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"railway\",\n                                    \"ele\",\n                                    \"operator:type\",\n                                    \"layer\",\n                                    \"addr:full\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['railway'] IN ('rail','station')\",\n                                \"formats\": [\"geojson\"],\n                            }\n                        },\n                    ],\n                },\n            },\n            \"fullset_multiple_formats\": {\n                \"summary\": \"Full HDX Dataset default Multiple formats\",\n                \"description\": \"Full yaml conversion for dataset with iso3 example with multiple formats\",\n                \"value\": {\n                    \"iso3\": \"NPL\",\n                    \"hdx_upload\": True,\n                    \"meta\": True,\n                    \"categories\": [\n                        {\n                            \"Buildings\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"facilities-infrastructure\",\n                                        \"buildings\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"building\",\n                                    \"building:levels\",\n                                    \"building:materials\",\n                                    \"addr:full\",\n                                    \"addr:housenumber\",\n                                    \"addr:street\",\n                                    \"addr:city\",\n                                    \"office\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['building'] IS NOT NULL\",\n                                \"formats\": [\"geojson\", \"kml\"],\n                            }\n                        },\n                        {\n                            \"Roads\": {\n                                \"hdx\": {\n                                    \"tags\": [\"transportation\", \"roads\", \"geodata\"],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"lines\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"highway\",\n                                    \"surface\",\n                                    \"smoothness\",\n                                    \"width\",\n                                    \"lanes\",\n                                    \"oneway\",\n                                    \"bridge\",\n                                    \"layer\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['highway'] IS NOT NULL\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                        {\n                            \"Waterways\": {\n                                \"hdx\": {\n                                    \"tags\": [\"hydrology\", \"rivers\", \"geodata\"],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"lines\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"waterway\",\n                                    \"covered\",\n                                    \"width\",\n                                    \"depth\",\n                                    \"layer\",\n                                    \"blockage\",\n                                    \"tunnel\",\n                                    \"natural\",\n                                    \"water\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['waterway'] IS NOT NULL OR tags['water'] IS NOT NULL OR tags['natural'] IN ('water','wetland','bay')\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                        {\n                            \"Points of Interest\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"facilities-infrastructure\",\n                                        \"points of interest-poi\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"amenity\",\n                                    \"man_made\",\n                                    \"shop\",\n                                    \"tourism\",\n                                    \"opening_hours\",\n                                    \"beds\",\n                                    \"rooms\",\n                                    \"addr:full\",\n                                    \"addr:housenumber\",\n                                    \"addr:street\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['amenity'] IS NOT NULL OR tags['man_made'] IS NOT NULL OR tags['shop'] IS NOT NULL OR tags['tourism'] IS NOT NULL\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                        {\n                            \"Airports\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"aviation\",\n                                        \"facilities-infrastructure\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\", \"lines\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"aeroway\",\n                                    \"building\",\n                                    \"emergency\",\n                                    \"emergency:helipad\",\n                                    \"operator:type\",\n                                    \"capacity:persons\",\n                                    \"addr:full\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['aeroway'] IS NOT NULL OR tags['building'] = 'aerodrome' OR tags['emergency:helipad'] IS NOT NULL OR tags['emergency'] = 'landing_site'\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                        {\n                            \"Sea Ports\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"facilities-infrastructure\",\n                                        \"ports\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\", \"lines\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"amenity\",\n                                    \"building\",\n                                    \"port\",\n                                    \"operator:type\",\n                                    \"addr:full\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['amenity'] = 'ferry_terminal' OR tags['building'] = 'ferry_terminal' OR tags['port'] IS NOT NULL\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                        {\n                            \"Education Facilities\": {\n                                \"hdx\": {\n                                    \"tags\": [\"education facilities-schools\", \"geodata\"],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"amenity\",\n                                    \"building\",\n                                    \"operator:type\",\n                                    \"capacity:persons\",\n                                    \"addr:full\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['amenity'] IN ('kindergarten', 'school', 'college', 'university') OR tags['building'] IN ('kindergarten', 'school', 'college', 'university')\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                        {\n                            \"Health Facilities\": {\n                                \"hdx\": {\n                                    \"tags\": [\"geodata\", \"health facilities\", \"health\"],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"amenity\",\n                                    \"building\",\n                                    \"healthcare\",\n                                    \"healthcare:speciality\",\n                                    \"operator:type\",\n                                    \"capacity:persons\",\n                                    \"addr:full\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['healthcare'] IS NOT NULL OR tags['amenity'] IN ('doctors', 'dentist', 'clinic', 'hospital', 'pharmacy')\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                        {\n                            \"Populated Places\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"populated places-settlements\",\n                                        \"villages\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"place\",\n                                    \"population\",\n                                    \"is_in\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['place'] IN ('isolated_dwelling', 'town', 'village', 'hamlet', 'city')\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                        {\n                            \"Financial Services\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"economics\",\n                                        \"financial institutions\",\n                                        \"financial services\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"points\", \"polygons\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"amenity\",\n                                    \"operator\",\n                                    \"network\",\n                                    \"addr:full\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['amenity'] IN ('mobile_money_agent','bureau_de_change','bank','microfinance','atm','sacco','money_transfer','post_office')\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                        {\n                            \"Railways\": {\n                                \"hdx\": {\n                                    \"tags\": [\n                                        \"facilities-infrastructure\",\n                                        \"railways\",\n                                        \"transportation\",\n                                        \"geodata\",\n                                    ],\n                                    \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                                },\n                                \"types\": [\"lines\"],\n                                \"select\": [\n                                    \"name\",\n                                    \"railway\",\n                                    \"ele\",\n                                    \"operator:type\",\n                                    \"layer\",\n                                    \"addr:full\",\n                                    \"addr:city\",\n                                    \"source\",\n                                ],\n                                \"where\": \"tags['railway'] IN ('rail','station')\",\n                                \"formats\": [\"geojson\", \"shp\", \"kml\"],\n                            }\n                        },\n                    ],\n                },\n            },\n        },\n    ),\n):\n    \"\"\"\n    Process data based on dynamic categories, Fully flexible on filtering and select\n\n    Args:\n        request: FastAPI Request object.\n        params (DynamicCategoriesModel): Input parameters including ISO3 country code and dynamic categories.\n\n    Returns:\n        dict: Result message.\n    \"\"\"\n    queue_name = params.queue\n    if params.queue != DEFAULT_QUEUE_NAME and user.role != UserRole.ADMIN.value:\n        raise HTTPException(\n            status_code=403,\n            detail=[{\"msg\": \"Insufficient Permission to choose queue\"}],\n        )\n    params.categories = [category for category in params.categories if category]\n    if len(params.categories) == 0:\n        raise HTTPException(\n            status_code=400, detail=[{\"msg\": \"Categories can't be empty\"}]\n        )\n    task = process_custom_request.apply_async(\n        args=(params.model_dump(),),\n        queue=queue_name,\n        track_started=True,\n        kwargs={\"user\": user.model_dump()},\n    )\n    return JSONResponse({\"task_id\": task.id, \"track_link\": f\"/tasks/status/{task.id}/\"})\n\n\n@router.post(\n    \"/snapshot/yaml/\",\n    openapi_extra={\n        \"requestBody\": {\n            \"content\": {\n                \"application/x-yaml\": {\"schema\": CustomRequestsYaml.model_json_schema()}\n            },\n            \"required\": True,\n        },\n    },\n)\n@limiter.limit(f\"{RATE_LIMIT_PER_MIN}/minute\")\n@version(1)\nasync def process_custom_requests_yaml(\n    request: Request,\n    user: AuthUser = Depends(staff_required),\n):\n    raw_body = await request.body()\n    try:\n        data = yaml.safe_load(raw_body)\n    except yaml.YAMLError:\n        raise HTTPException(status_code=422, detail=\"Invalid YAML\")\n    try:\n        validated_data = DynamicCategoriesModel.model_validate(data)\n    except ValidationError as e:\n        raise HTTPException(status_code=422, detail=e.errors(include_url=False))\n\n    queue_name = validated_data.queue\n    if validated_data.queue != DEFAULT_QUEUE_NAME and user.role != UserRole.ADMIN.value:\n        raise HTTPException(\n            status_code=403,\n            detail=[{\"msg\": \"Insufficient Permission to choose queue\"}],\n        )\n    validated_data.categories = [\n        category for category in validated_data.categories if category\n    ]\n    if len(validated_data.categories) == 0:\n        raise HTTPException(\n            status_code=400, detail=[{\"msg\": \"Categories can't be empty\"}]\n        )\n    task = process_custom_request.apply_async(\n        args=(validated_data.model_dump(),),\n        queue=queue_name,\n        track_started=True,\n        kwargs={\"user\": user.model_dump()},\n    )\n    return JSONResponse({\"task_id\": task.id, \"track_link\": f\"/tasks/status/{task.id}/\"})\n"}
{"type": "source_file", "path": "API/s3.py", "content": "import json\nfrom urllib.parse import quote\n\nimport boto3\nimport humanize\nfrom boto3.session import Session\nfrom botocore.exceptions import NoCredentialsError\nfrom fastapi import APIRouter, Header, HTTPException, Path, Query, Request\nfrom fastapi.encoders import jsonable_encoder\nfrom fastapi.responses import (\n    JSONResponse,\n    RedirectResponse,\n    Response,\n    StreamingResponse,\n)\nfrom fastapi_versioning import version\n\nfrom src.config import AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, BUCKET_NAME\nfrom src.config import LIMITER as limiter\nfrom src.config import RATE_LIMIT_PER_MIN\n\nrouter = APIRouter(prefix=\"/s3\", tags=[\"S3\"])\n\nAWS_REGION = \"us-east-1\"\nsession = Session()\ns3 = session.client(\n    \"s3\",\n    aws_access_key_id=AWS_ACCESS_KEY_ID,\n    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n    region_name=AWS_REGION,\n)\npaginator = s3.get_paginator(\"list_objects_v2\")\n\n\n@router.get(\"/files/\")\n@limiter.limit(f\"{RATE_LIMIT_PER_MIN}/minute\")\n@version(1)\nasync def list_s3_files(\n    request: Request,\n    folder: str = Query(default=\"/HDX\"),\n    prettify: bool = Query(\n        default=False, description=\"Display size & date in human-readable format\"\n    ),\n):\n    bucket_name = BUCKET_NAME\n    folder = folder.strip(\"/\")\n    prefix = f\"{folder}/\"\n\n    try:\n        # Use list_objects_v2 directly for pagination\n        page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n\n        async def generate():\n            first_item = True\n            yield \"[\"\n\n            for response in page_iterator:\n                contents = response.get(\"Contents\", [])\n\n                for item in contents:\n                    size = item[\"Size\"]\n                    last_modified = item[\"LastModified\"].strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n                    if prettify:\n                        last_modified = humanize.naturaldate(item[\"LastModified\"])\n                        size = humanize.naturalsize(size)\n\n                    item_dict = {\n                        \"Key\": item[\"Key\"],\n                        \"LastModified\": last_modified,\n                        \"Size\": size,\n                    }\n                    if not first_item:\n                        yield \",\"\n                    else:\n                        first_item = False\n                    yield json.dumps(item_dict, default=str)\n\n            yield \"]\"\n\n        return StreamingResponse(content=generate(), media_type=\"application/json\")\n\n    except NoCredentialsError:\n        raise HTTPException(status_code=500, detail=\"AWS credentials not available\")\n\n\nasync def check_object_existence(bucket_name, file_path):\n    \"\"\"Async function to check object existence\"\"\"\n    try:\n        s3.head_object(Bucket=bucket_name, Key=file_path)\n    except NoCredentialsError:\n        raise HTTPException(status_code=500, detail=\"AWS credentials not available\")\n    except Exception as e:\n        raise HTTPException(\n            status_code=404, detail=f\"File or folder not found: {file_path}\"\n        )\n\n\nasync def read_meta_json(bucket_name, file_path):\n    \"\"\"Async function to read from meta json\"\"\"\n    try:\n        response = s3.get_object(Bucket=bucket_name, Key=file_path)\n        content = json.loads(response[\"Body\"].read())\n        return content\n    except Exception as e:\n        raise HTTPException(\n            status_code=500, detail=f\"Error reading meta.json: {str(e)}\"\n        )\n\n\n@router.head(\"/get/{file_path:path}\")\n@limiter.limit(f\"{RATE_LIMIT_PER_MIN}/minute\")\n@version(1)\nasync def head_s3_file(\n    request: Request,\n    file_path: str = Path(..., description=\"The path to the file or folder in S3\"),\n):\n    bucket_name = BUCKET_NAME\n    encoded_file_path = quote(file_path.strip(\"/\"))\n    try:\n        response = s3.head_object(Bucket=bucket_name, Key=encoded_file_path)\n        return Response(\n            status_code=200,\n            headers={\n                \"Last-Modified\": response[\"LastModified\"].strftime(\n                    \"%a, %d %b %Y %H:%M:%S GMT\"\n                ),\n                \"Content-Length\": str(response[\"ContentLength\"]),\n            },\n        )\n    except Exception as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            return Response(status_code=404)\n        else:\n            raise HTTPException(status_code=500, detail=f\"AWS Error: {str(e)}\")\n\n\n@router.get(\"/get/{file_path:path}\")\n@limiter.limit(f\"{RATE_LIMIT_PER_MIN}/minute\")\n@version(1)\nasync def get_s3_file(\n    request: Request,\n    file_path: str = Path(..., description=\"The path to the file or folder in S3\"),\n    expiry: int = Query(\n        default=3600,\n        description=\"Expiry time for the presigned URL in seconds (default: 1 hour)\",\n        gt=60 * 10,\n        le=3600 * 12 * 7,\n    ),\n    read_meta: bool = Query(\n        default=True,\n        description=\"Whether to read and deliver the content of .json file\",\n    ),\n):\n    bucket_name = BUCKET_NAME\n    file_path = file_path.strip(\"/\")\n    encoded_file_path = quote(file_path)\n\n    await check_object_existence(bucket_name, encoded_file_path)\n\n    if read_meta and file_path.lower().endswith(\".json\"):\n        # Read and deliver the content of meta.json\n        content = await read_meta_json(bucket_name, file_path)\n        return JSONResponse(content=jsonable_encoder(content))\n\n    # If not reading meta.json, generate a presigned URL\n    presigned_url = s3.generate_presigned_url(\n        \"get_object\",\n        Params={\"Bucket\": bucket_name, \"Key\": encoded_file_path},\n        ExpiresIn=expiry,\n    )\n\n    return RedirectResponse(presigned_url)\n"}
{"type": "source_file", "path": "backend/login.py", "content": "#! /usr/bin/env python3\n# Copyright 2018 Geofabrik GmbH\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n\n# 1. Redistributions of source code must retain the above copyright notice, this\n#    list of conditions and the following disclaimer.\n\n# 2. Redistributions in binary form must reproduce the above copyright notice,\n#    this list of conditions and the following disclaimer in the documentation\n#    and/or other materials provided with the distribution.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport json\nimport re\nimport sys\n\nimport requests\n\nCUSTOM_HEADER = {\"user-agent\": \"oauth_cookie_client.py\"}\n\n\nclass RaiseError(Exception):\n    def __init__(self, message):\n        self.message = message\n\n\ndef report_error(message):\n    sys.stderr.write(\"{}\\n\".format(message))\n    raise RaiseError(message)\n\n\ndef find_authenticity_token(response):\n    \"\"\"\n    Search the authenticity_token in the response of the server\n    \"\"\"\n    pattern = r\"name=\\\"csrf-token\\\" content=\\\"([^\\\"]+)\\\"\"\n    m = re.search(pattern, response)\n    if m is None:\n        report_error(\n            \"Could not find the authenticity_token in the website to be scraped.\"\n        )\n    try:\n        return m.group(1)\n    except IndexError:\n        sys.stderr.write(\n            \"ERROR: The login form does not contain an authenticity_token.\\n\"\n        )\n        exit(1)\n\n\ndef verify_me_osm(\n    user,\n    password,\n    osm_host=\"https://www.openstreetmap.org/\",\n    consumer_url=\"https://osm-internal.download.geofabrik.de/get_cookie\",\n    format=\"http\",\n):\n    username = user\n    if username is None:\n        report_error(\"The username must not be empty.\")\n    if len(password) == 0 or password is None:\n        report_error(\"The password must not be empty.\")\n    if consumer_url is None:\n        report_error(\"No consumer URL provided\")\n\n    # get request token\n    url = consumer_url + \"?action=request_token\"\n    r = requests.post(url, data={}, headers=CUSTOM_HEADER)\n    if r.status_code != 200:\n        report_error(\n            \"POST {}, received HTTP status code {} but expected 200\".format(\n                url, r.status_code\n            )\n        )\n    json_response = json.loads(r.text)\n    authorize_url = osm_host + \"/oauth/authorize\"\n    try:\n        oauth_token = json_response[\"oauth_token\"]\n        oauth_token_secret_encr = json_response[\"oauth_token_secret_encr\"]\n    except KeyError:\n        report_error(\"oauth_token was not found in the first response by the consumer\")\n\n    # get OSM session\n    login_url = osm_host + \"/login?cookie_test=true\"\n    s = requests.Session()\n    r = s.get(login_url, headers=CUSTOM_HEADER)\n    if r.status_code != 200:\n        report_error(\"GET {}, received HTTP code {}\".format(login_url, r.status_code))\n\n    # login\n    authenticity_token = find_authenticity_token(r.text)\n    login_url = osm_host + \"/login\"\n    r = s.post(\n        login_url,\n        data={\n            \"username\": username,\n            \"password\": password,\n            \"referer\": \"/\",\n            \"commit\": \"Login\",\n            \"authenticity_token\": authenticity_token,\n        },\n        allow_redirects=False,\n        headers=CUSTOM_HEADER,\n    )\n    if r.status_code != 302:\n        report_error(\n            \"POST {}, received HTTP code {} but expected 302\".format(\n                login_url, r.status_code\n            )\n        )\n\n    # authorize\n    authorize_url = \"{}/oauth/authorize?oauth_token={}\".format(osm_host, oauth_token)\n    r = s.get(authorize_url, headers=CUSTOM_HEADER)\n    if r.status_code != 200:\n        report_error(\n            \"GET {}, received HTTP code {} but expected 200\".format(\n                authorize_url, r.status_code\n            )\n        )\n    authenticity_token = find_authenticity_token(r.text)\n\n    post_data = {\n        \"oauth_token\": oauth_token,\n        \"oauth_callback\": \"\",\n        \"authenticity_token\": authenticity_token,\n        \"allow_read_prefs\": [0, 1],\n        \"commit\": \"Save changes\",\n    }\n    authorize_url = \"{}/oauth/authorize\".format(osm_host)\n    r = s.post(authorize_url, data=post_data, headers=CUSTOM_HEADER)\n    if r.status_code != 200:\n        report_error(\n            \"POST {}, received HTTP code {} but expected 200\".format(\n                authorize_url, r.status_code\n            )\n        )\n\n    # logout\n    logout_url = \"{}/logout\".format(osm_host)\n    r = s.get(logout_url, headers=CUSTOM_HEADER)\n    if r.status_code != 200 and r.status_code != 302:\n        report_error(\n            \"POST {}, received HTTP code {} but expected 200 or 302\".format(logout_url)\n        )\n\n    # get final cookie\n    url = consumer_url + \"?action=get_access_token_cookie&format={}\".format(format)\n    r = requests.post(\n        url,\n        data={\n            \"oauth_token\": oauth_token,\n            \"oauth_token_secret_encr\": oauth_token_secret_encr,\n        },\n        headers=CUSTOM_HEADER,\n    )\n\n    return str(r.text)\n"}
{"type": "source_file", "path": "API/stats.py", "content": "# Standard library imports\nimport json\n\n# Third party imports\nfrom area import area\nfrom fastapi import APIRouter, Body, Depends, HTTPException, Request\nfrom fastapi_versioning import version\n\n# Reader imports\nfrom src.app import PolygonStats\nfrom src.config import LIMITER as limiter\nfrom src.config import POLYGON_STATISTICS_API_RATE_LIMIT\nfrom src.validation.models import StatsRequestParams\n\nrouter = APIRouter(prefix=\"/stats\", tags=[\"Stats\"])\nfrom .auth import AuthUser, UserRole, get_optional_user\n\n\n@router.post(\"/polygon/\")\n@limiter.limit(f\"{POLYGON_STATISTICS_API_RATE_LIMIT}/minute\")\n@version(1)\nasync def get_polygon_stats(\n    request: Request,\n    params: StatsRequestParams = Body(\n        ...,\n        description=\"Get Summary and raw stats related to polygon\",\n        openapi_examples={\n            \"normal_polygon\": {\n                \"summary\": \"Normal Example of requesting stats\",\n                \"description\": \"Query to extract stats using Custom Polygon\",\n                \"value\": {\n                    \"geometry\": {\n                        \"type\": \"Polygon\",\n                        \"coordinates\": [\n                            [\n                                [83.96919250488281, 28.194446860487773],\n                                [83.99751663208006, 28.194446860487773],\n                                [83.99751663208006, 28.214869548073377],\n                                [83.96919250488281, 28.214869548073377],\n                                [83.96919250488281, 28.194446860487773],\n                            ]\n                        ],\n                    }\n                },\n            },\n            \"normal_iso\": {\n                \"summary\": \"Query to extract stats using iso\",\n                \"description\": \"Extract stats using iso3 only, For eg : for Nepal\",\n                \"value\": {\"iso3\": \"npl\"},\n            },\n        },\n    ),\n    user: AuthUser = Depends(get_optional_user),\n):\n    \"\"\"Get statistics for the specified polygon.\n\n    Args:\n        request (Request): An HTTP request object.\n        params (StatsRequestParams): Parameters for the statistics request, including the polygon geometry.\n\n    Returns:\n        dict: A dictionary containing statistics for the specified polygon.\n    \"\"\"\n    if not (user.role is UserRole.STAFF.value or user.role is UserRole.ADMIN.value):\n        if params.geometry:\n            area_m2 = area(json.loads(params.geometry.model_dump_json()))\n            area_km2 = area_m2 * 1e-6\n            limit = 10000\n            if area_km2 > limit:\n                raise HTTPException(\n                    status_code=400,\n                    detail=[\n                        {\n                            \"msg\": f\"\"\"Polygon Area {int(area_km2)} Sq.KM is higher than Threshold : {limit} Sq.KM\"\"\"\n                        }\n                    ],\n                )\n    feature = None\n    if params.geometry:\n        feature = {\n            \"type\": \"Feature\",\n            \"geometry\": json.loads(params.geometry.model_dump_json()),\n            \"properties\": {},\n        }\n    if params.iso3:\n        params.iso3 = params.iso3.lower()\n    generator = PolygonStats(feature, params.iso3)\n\n    return generator.get_summary_stats()\n"}
{"type": "source_file", "path": "src/post_processing/processor.py", "content": "from geojson_stats.stats import Stats, Config\nfrom geojson_stats.html import Html\nimport os\nimport pathlib\n\nCATEGORIES_CONFIG = {\n    \"roads\": {\"tag\": \"highway\", \"length\": True, \"area\": False},\n    \"buildings\": {\"tag\": \"building\", \"length\": False, \"area\": True},\n    \"waterways\": {\"tag\": \"waterway\", \"length\": True, \"area\": False},\n    \"railways\": {\"tag\": \"railway\", \"length\": True, \"area\": False},\n    \"default\": {\"tag\": None, \"length\": False, \"area\": False},\n}\n\n\nclass PostProcessor:\n    \"\"\"Used for post-process GeoJSON files\"\"\"\n\n    def __init__(self, options, *args, **kwargs):\n        self.options = options\n\n    def get_categories_config(self, category_name):\n        \"\"\"\n        Get configuration for categories\n        \"\"\"\n        config = CATEGORIES_CONFIG.get(category_name)\n        return config if config else CATEGORIES_CONFIG[\"default\"]\n\n    def stats(\n        self, category_name, export_format_path, export_filename, file_export_path\n    ):\n        \"\"\"\n        Post-process custom exports\n        \"\"\"\n\n        # Get stats config\n        category_config = self.get_categories_config(category_name)\n        config = Config(\n            clean=True,\n            length=category_config[\"length\"],\n            area=category_config[\"area\"],\n            keys=category_config[\"tag\"],\n            value_keys=category_config[\"tag\"],\n        )\n\n        if self.options[\"include_stats\"]:\n            # Generate stats\n            path_input = os.path.join(export_format_path, f\"{export_filename}.geojson\")\n            stats = Stats(config)\n            stats.process_file_stream(path_input)\n\n            # Remove redundant stats\n            if \"osm_id\" in stats.results.key:\n                del stats.results.key[\"osm_id\"]\n            if \"osm_type\" in stats.results.key:\n                del stats.results.key[\"osm_type\"]\n\n            stats_json = stats.json()\n\n            # Save raw stats\n            with open(\n                os.path.join(file_export_path, \"stats.json\"),\n                \"w\",\n            ) as f:\n                f.write(stats_json)\n\n            # Save HTML stats\n            if self.options.get(\"include_stats_html\"):\n                # Get template\n                category_tag = category_config[\"tag\"]\n                tpl = (\n                    \"stats_{category_tag}\".format(category_tag=category_tag)\n                    if category_tag\n                    else \"stats\"\n                )\n                project_root = pathlib.Path(__file__).resolve().parent\n                tpl_path = os.path.join(\n                    project_root,\n                    \"{tpl}_tpl.html\".format(tpl=tpl),\n                )\n\n                # Generate HTML\n                geojson_stats_html = Html(\n                    tpl_path, stats, {\"title\": f\"{export_filename}.geojson\"}\n                ).build()\n\n                # Save HTML file\n                upload_html_path = os.path.join(file_export_path, \"stats-summary.html\")\n                with open(upload_html_path, \"w\") as f:\n                    f.write(geojson_stats_html)\n"}
{"type": "source_file", "path": "src/post_processing/__init__.py", "content": ""}
{"type": "source_file", "path": "src/config.py", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Copyright (C) 2021 Humanitarian OpenStreetmap Team\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\n# Humanitarian OpenStreetmap Team\n# 1100 13th Street NW Suite 800 Washington, D.C. 20005\n# <info@hotosm.org>\n\n# Standard library imports\nimport logging\nimport os\nfrom configparser import ConfigParser\nfrom distutils.util import strtobool\n\n# Third party imports\nfrom slowapi import Limiter\nfrom slowapi.util import get_remote_address\n\n\ndef get_bool_env_var(key, default=False):\n    value = os.environ.get(key, default)\n    return bool(strtobool(str(value)))\n\n\nCONFIG_FILE_PATH = \"config.txt\"\nUSE_S3_TO_UPLOAD = False\nAWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, BUCKET_NAME = None, None, None\n\n\nconfig = ConfigParser()\nconfig.read(CONFIG_FILE_PATH)\n\n\n### CELERY BLOCK ####################\nCELERY_BROKER_URL = os.environ.get(\"CELERY_BROKER_URL\") or config.get(\n    \"CELERY\", \"CELERY_BROKER_URL\", fallback=\"redis://localhost:6379\"\n)\nCELERY_RESULT_BACKEND = os.environ.get(\"CELERY_RESULT_BACKEND\") or config.get(\n    \"CELERY\", \"CELERY_RESULT_BACKEND\", fallback=\"redis://localhost:6379\"\n)\n\nCELERY_BROKER_HEARTBEAT = os.environ.get(\"CELERY_BROKER_HEARTBEAT\") or config.get(\n    \"CELERY\", \"CELERY_BROKER_HEARTBEAT\", fallback=120\n)\nCELERY_WORKER_LOST_WAIT = os.environ.get(\"CELERY_WORKER_LOST_WAIT\") or config.get(\n    \"CELERY\", \"CELERY_WORKER_LOST_WAIT \", fallback=10\n)\n\nWORKER_PREFETCH_MULTIPLIER = int(\n    os.environ.get(\"WORKER_PREFETCH_MULTIPLIER\")\n    or config.get(\"CELERY\", \"WORKER_PREFETCH_MULTIPLIER\", fallback=1)\n)\n\n### API CONFIG BLOCK #######################\n\nRATE_LIMIT_PER_MIN = os.environ.get(\"RATE_LIMIT_PER_MIN\") or int(\n    config.get(\"API_CONFIG\", \"RATE_LIMIT_PER_MIN\", fallback=20)\n)\n\nRATE_LIMITER_STORAGE_URI = os.environ.get(\"RATE_LIMITER_STORAGE_URI\") or config.get(\n    \"API_CONFIG\", \"RATE_LIMITER_STORAGE_URI\", fallback=\"redis://localhost:6379\"\n)\n\nEXPORT_MAX_AREA_SQKM = os.environ.get(\"EXPORT_MAX_AREA_SQKM\") or int(\n    config.get(\"API_CONFIG\", \"EXPORT_MAX_AREA_SQKM\", fallback=100000)\n)\n\n\nINDEX_THRESHOLD = os.environ.get(\"INDEX_THRESHOLD\") or int(\n    config.get(\"API_CONFIG\", \"INDEX_THRESHOLD\", fallback=5000)\n)\n\nMAX_WORKERS = os.environ.get(\"MAX_WORKERS\") or config.get(\n    \"API_CONFIG\", \"MAX_WORKERS\", fallback=os.cpu_count()\n)\n\n# get log level from config\nLOG_LEVEL = os.environ.get(\"LOG_LEVEL\") or config.get(\n    \"API_CONFIG\", \"LOG_LEVEL\", fallback=\"debug\"\n)\n\n\ndef not_raises(func, *args, **kwargs):\n    try:\n        func(*args, **kwargs)\n        return True\n    except Exception as ex:\n        logging.error(ex)\n        return False\n\n\n####################\n\n# EXPORT_UPLOAD CONFIG BLOCK\nFILE_UPLOAD_METHOD = os.environ.get(\"FILE_UPLOAD_METHOD\") or config.get(\n    \"EXPORT_UPLOAD\", \"FILE_UPLOAD_METHOD\", fallback=\"disk\"\n)\n\n\nif FILE_UPLOAD_METHOD.lower() not in [\"s3\", \"disk\"]:\n    logging.error(\n        \"value not supported for file_upload_method ,switching to default disk method\"\n    )\n    USE_S3_TO_UPLOAD = False\n\nif FILE_UPLOAD_METHOD.lower() == \"s3\":\n    USE_S3_TO_UPLOAD = True\n    BUCKET_NAME = os.environ.get(\"BUCKET_NAME\") or config.get(\n        \"EXPORT_UPLOAD\", \"BUCKET_NAME\"\n    )\n    AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\") or config.get(\n        \"EXPORT_UPLOAD\", \"AWS_ACCESS_KEY_ID\", fallback=None\n    )\n    AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\") or config.get(\n        \"EXPORT_UPLOAD\", \"AWS_SECRET_ACCESS_KEY\", fallback=None\n    )\n    if not BUCKET_NAME:\n        raise ValueError(\"Value of BUCKET_NAME couldn't found\")\n\n##################\n\n## SENTRY BLOCK ########\nSENTRY_DSN = os.environ.get(\"SENTRY_DSN\") or config.get(\n    \"SENTRY\", \"SENTRY_DSN\", fallback=None\n)\nSENTRY_RATE = os.environ.get(\"SENTRY_RATE\") or config.get(\n    \"SENTRY\", \"SENTRY_RATE\", fallback=None\n)\n\n# rate limiter for API requests based on the remote ip address and redis as backend\nLIMITER = Limiter(key_func=get_remote_address, storage_uri=RATE_LIMITER_STORAGE_URI)\n\n\nif LOG_LEVEL.lower() == \"debug\":  # default debug\n    level = logging.DEBUG\nelif LOG_LEVEL.lower() == \"info\":\n    level = logging.INFO\nelif LOG_LEVEL.lower() == \"error\":\n    level = logging.ERROR\nelif LOG_LEVEL.lower() == \"warning\":\n    level = logging.WARNING\nelse:\n    logging.error(\n        \"logging config is not supported , Supported fields are : debug,error,warning,info , Logging to default :debug\"\n    )\n    level = logging.DEBUG\n\n# logging.getLogger(\"fiona\").propagate = False  # disable fiona logging\nlogging.basicConfig(format=\"%(asctime)s - %(message)s\", level=level)\nlogging.getLogger(\"boto3\").propagate = False  # disable boto3 logging\nlogging.getLogger(\"botocore\").propagate = False  # disable boto3 logging\nlogging.getLogger(\"s3transfer\").propagate = False  # disable boto3 logging\nlogging.getLogger(\"boto\").propagate = False  # disable boto3 logging\n\n\n## Readme txt\n\n\nlogger = logging.getLogger(\"raw_data_api\")\n\nEXPORT_PATH = os.environ.get(\"EXPORT_PATH\") or config.get(\n    \"API_CONFIG\", \"EXPORT_PATH\", fallback=\"exports\"\n)\n\nif not os.path.exists(EXPORT_PATH):\n    # Create a exports directory because it does not exist\n    os.makedirs(EXPORT_PATH)\n\n\nDEFAULT_README_TEXT = \"\"\"Exported through Raw-data-api (https://github.com/hotosm/raw-data-api) using OpenStreetMap data.\\n Exports are made available under the Open Database License: http://opendatacommons.org/licenses/odbl/1.0/. Any rights in individual contents of the database are licensed under the Database Contents License: http://opendatacommons.org/licenses/dbcl/1.0/. \\n Learn more about OpenStreetMap and its data usage policy : https://www.openstreetmap.org/about \\n\"\"\"\n\nEXTRA_README_TXT = os.environ.get(\"EXTRA_README_TXT\") or config.get(\n    \"API_CONFIG\", \"EXTRA_README_TXT\", fallback=\"\"\n)\nDEFAULT_README_TEXT += EXTRA_README_TXT\n\nALLOW_BIND_ZIP_FILTER = get_bool_env_var(\n    \"ALLOW_BIND_ZIP_FILTER\",\n    config.getboolean(\"API_CONFIG\", \"ALLOW_BIND_ZIP_FILTER\", fallback=False),\n)\n\nSETUP_INITIAL_TABLES = get_bool_env_var(\n    \"SETUP_INITIAL_TABLES\",\n    config.getboolean(\"API_CONFIG\", \"SETUP_INITIAL_TABLES\", fallback=False),\n)\n\n\nENABLE_SOZIP = get_bool_env_var(\n    \"ENABLE_SOZIP\",\n    config.getboolean(\"API_CONFIG\", \"ENABLE_SOZIP\", fallback=False),\n)\n\nENABLE_TILES = get_bool_env_var(\n    \"ENABLE_TILES\", config.getboolean(\"API_CONFIG\", \"ENABLE_TILES\", fallback=False)\n)\n\n# check either to use connection pooling or not\nUSE_CONNECTION_POOLING = get_bool_env_var(\n    \"USE_CONNECTION_POOLING\",\n    config.getboolean(\"API_CONFIG\", \"USE_CONNECTION_POOLING\", fallback=False),\n)\n\n\nENABLE_OLD_EXPORTS_CLEANUP = get_bool_env_var(\n    \"ENABLE_OLD_EXPORTS_CLEANUP\",\n    config.getboolean(\"API_CONFIG\", \"ENABLE_OLD_EXPORTS_CLEANUP\", fallback=False),\n)\n\nOLD_EXPORTS_CLEANUP_DAYS = os.environ.get(\"OLD_EXPORTS_CLEANUP_DAYS\") or config.get(\n    \"API_CONFIG\", \"OLD_EXPORTS_CLEANUP_DAYS\", fallback=4\n)\n\n# Queue\n\nDEFAULT_QUEUE_NAME = os.environ.get(\"DEFAULT_QUEUE_NAME\") or config.get(\n    \"API_CONFIG\", \"DEFAULT_QUEUE_NAME\", fallback=\"raw_daemon\"\n)\nONDEMAND_QUEUE_NAME = os.environ.get(\"ONDEMAND_QUEUE_NAME\") or config.get(\n    \"API_CONFIG\", \"ONDEMAND_QUEUE_NAME\", fallback=\"raw_ondemand\"\n)\n\n# Polygon statistics which will deliver the stats of approx buildings/ roads in the area\n\nENABLE_POLYGON_STATISTICS_ENDPOINTS = get_bool_env_var(\n    \"ENABLE_POLYGON_STATISTICS_ENDPOINTS\",\n    config.getboolean(\n        \"API_CONFIG\", \"ENABLE_POLYGON_STATISTICS_ENDPOINTS\", fallback=False\n    ),\n)\nPOLYGON_STATISTICS_API_URL = os.environ.get(\"POLYGON_STATISTICS_API_URL\") or config.get(\n    \"API_CONFIG\", \"POLYGON_STATISTICS_API_URL\", fallback=None\n)\n\nPOLYGON_STATISTICS_API_RATE_LIMIT = os.environ.get(\n    \"POLYGON_STATISTICS_API_RATE_LIMIT\"\n) or config.get(\"API_CONFIG\", \"POLYGON_STATISTICS_API_RATE_LIMIT\", fallback=5)\n\n# task limit\n\nDEFAULT_SOFT_TASK_LIMIT = os.environ.get(\"DEFAULT_SOFT_TASK_LIMIT\") or config.get(\n    \"API_CONFIG\", \"DEFAULT_SOFT_TASK_LIMIT\", fallback=2 * 60 * 60\n)\nDEFAULT_HARD_TASK_LIMIT = os.environ.get(\"DEFAULT_HARD_TASK_LIMIT\") or config.get(\n    \"API_CONFIG\", \"DEFAULT_HARD_TASK_LIMIT\", fallback=3 * 60 * 60\n)\n\n# duckdb\n\nUSE_DUCK_DB_FOR_CUSTOM_EXPORTS = get_bool_env_var(\n    \"USE_DUCK_DB_FOR_CUSTOM_EXPORTS\",\n    config.getboolean(\"API_CONFIG\", \"USE_DUCK_DB_FOR_CUSTOM_EXPORTS\", fallback=False),\n)\n\nlogger.info(\n    \"USE_DUCK_DB_FOR_CUSTOM_EXPORTS %s \", USE_DUCK_DB_FOR_CUSTOM_EXPORTS is True\n)\n\nif USE_DUCK_DB_FOR_CUSTOM_EXPORTS:\n    DUCK_DB_MEMORY_LIMIT = os.environ.get(\"DUCK_DB_MEMORY_LIMIT\") or config.get(\n        \"API_CONFIG\", \"DUCK_DB_MEMORY_LIMIT\", fallback=None\n    )\n    DUCK_DB_THREAD_LIMIT = os.environ.get(\"DUCK_DB_THREAD_LIMIT\") or config.get(\n        \"API_CONFIG\", \"DUCK_DB_THREAD_LIMIT\", fallback=None\n    )\n\n# hdx and custom exports\nENABLE_CUSTOM_EXPORTS = get_bool_env_var(\n    \"ENABLE_CUSTOM_EXPORTS\",\n    config.getboolean(\"API_CONFIG\", \"ENABLE_CUSTOM_EXPORTS\", fallback=False),\n)\n\nHDX_SOFT_TASK_LIMIT = os.environ.get(\"HDX_SOFT_TASK_LIMIT\") or config.get(\n    \"HDX\", \"HDX_SOFT_TASK_LIMIT\", fallback=5 * 60 * 60\n)\nHDX_HARD_TASK_LIMIT = os.environ.get(\"HDX_HARD_TASK_LIMIT\") or config.get(\n    \"HDX\", \"HDX_HARD_TASK_LIMIT\", fallback=6 * 60 * 60\n)\n\nENABLE_HDX_EXPORTS = get_bool_env_var(\n    \"ENABLE_HDX_EXPORTS\", config.getboolean(\"HDX\", \"ENABLE_HDX_EXPORTS\", fallback=False)\n)\n\nPROCESS_SINGLE_CATEGORY_IN_POSTGRES = get_bool_env_var(\n    \"PROCESS_SINGLE_CATEGORY_IN_POSTGRES\",\n    config.getboolean(\"HDX\", \"PROCESS_SINGLE_CATEGORY_IN_POSTGRES\", fallback=False),\n)\n\nPARALLEL_PROCESSING_CATEGORIES = get_bool_env_var(\n    \"PARALLEL_PROCESSING_CATEGORIES\",\n    config.getboolean(\"HDX\", \"PARALLEL_PROCESSING_CATEGORIES\", fallback=True),\n)\n\n\nENABLE_METRICS_APIS = get_bool_env_var(\n    \"ENABLE_METRICS_APIS\",\n    config.getboolean(\"API_CONFIG\", \"ENABLE_METRICS_APIS\", fallback=False),\n)\n\n\nif ENABLE_HDX_EXPORTS:\n    try:\n        hdx_credentials = os.environ[\"REMOTE_HDX\"]\n\n    except KeyError:\n        # logger.debug(\"EnvVar: REMOTE_HDX not supplied; Falling back to other means\")\n        HDX_SITE = os.environ.get(\"HDX_SITE\") or config.get(\n            \"HDX\", \"HDX_SITE\", fallback=\"demo\"\n        )\n        HDX_API_KEY = os.environ.get(\"HDX_API_KEY\") or config.get(\n            \"HDX\", \"HDX_API_KEY\", fallback=None\n        )\n        HDX_OWNER_ORG = os.environ.get(\"HDX_OWNER_ORG\") or config.get(\n            \"HDX\", \"HDX_OWNER_ORG\", fallback=\"225b9f7d-e7cb-4156-96a6-44c9c58d31e3\"\n        )\n        HDX_MAINTAINER = os.environ.get(\"HDX_MAINTAINER\") or config.get(\n            \"HDX\", \"HDX_MAINTAINER\", fallback=None\n        )\n\n    else:\n        # Standard library imports\n        import json\n\n        hdx_credentials_json = json.loads(hdx_credentials)\n\n        HDX_SITE = hdx_credentials_json[\"HDX_SITE\"]\n        HDX_API_KEY = hdx_credentials_json[\"HDX_API_KEY\"]\n        HDX_OWNER_ORG = hdx_credentials_json[\"HDX_OWNER_ORG\"]\n        HDX_MAINTAINER = hdx_credentials_json[\"HDX_MAINTAINER\"]\n\n        if None in (HDX_SITE, HDX_API_KEY, HDX_OWNER_ORG, HDX_MAINTAINER):\n            raise ValueError(\"HDX Remote Credentials Malformed\")\n\n    # Third party imports\n    from hdx.api.configuration import Configuration\n\n    try:\n        HDX_URL_PREFIX = Configuration.create(\n            hdx_site=HDX_SITE,\n            hdx_key=HDX_API_KEY,\n            user_agent=\"HDXPythonLibrary/6.2.0-HOTOSM OSM Exports\",\n        )\n        logging.debug(HDX_URL_PREFIX)\n    except Exception as e:\n        logging.error(\n            \"Error creating HDX configuration: %s, Disabling the hdx exports feature\", e\n        )\n        ENABLE_HDX_EXPORTS = False\n\nif ENABLE_HDX_EXPORTS:\n    # Third party imports\n    from hdx.data.dataset import Dataset\n    from hdx.data.vocabulary import Vocabulary\n\n    parse_list = lambda value, delimiter=\",\": (\n        value.split(delimiter) if isinstance(value, str) else value or []\n    )\n\n    ALLOWED_HDX_TAGS = parse_list(\n        os.environ.get(\"ALLOWED_HDX_TAGS\")\n        or config.get(\"HDX\", \"ALLOWED_HDX_TAGS\", fallback=None)\n        or (\n            Vocabulary.approved_tags() if not_raises(Vocabulary.approved_tags) else None\n        )\n    )\n    ALLOWED_HDX_UPDATE_FREQUENCIES = parse_list(\n        os.environ.get(\"ALLOWED_HDX_UPDATE_FREQUENCIES\")\n        or config.get(\"HDX\", \"ALLOWED_HDX_UPDATE_FREQUENCIES\", fallback=None)\n        or (\n            Dataset.list_valid_update_frequencies()\n            if not_raises(Dataset.list_valid_update_frequencies)\n            else None\n        )\n    )\n\n\ndef get_db_connection_params() -> dict:\n    \"\"\"Return a python dict that can be passed to psycopg2 connections\n    to authenticate to Postgres Databases\n\n    Returns: connection_params (dict): PostgreSQL connection parameters\n             corresponding to the configuration section.\n\n    \"\"\"\n    # This block fetches PostgreSQL (database) credentials passed as\n    # environment variables as a JSON object, from AWS Secrets Manager or\n    # Azure Key Vault.\n    try:\n        db_credentials = os.environ[\"REMOTE_DB\"]\n\n    except KeyError:\n        # logger.debug(\"EnvVar: REMOTE_DB not supplied; Falling back to other means\")\n\n        connection_params = dict(\n            host=os.environ.get(\"PGHOST\") or config.get(\"DB\", \"PGHOST\"),\n            port=os.environ.get(\"PGPORT\")\n            or config.get(\"DB\", \"PGPORT\", fallback=\"5432\"),\n            dbname=os.environ.get(\"PGDATABASE\") or config.get(\"DB\", \"PGDATABASE\"),\n            user=os.environ.get(\"PGUSER\") or config.get(\"DB\", \"PGUSER\"),\n            password=os.environ.get(\"PGPASSWORD\") or config.get(\"DB\", \"PGPASSWORD\"),\n        )\n\n    else:\n        # Standard library imports\n        import json\n\n        connection_params = json.loads(db_credentials)\n\n        connection_params[\"user\"] = connection_params[\"username\"]\n        for k in (\"dbinstanceidentifier\", \"dbClusterIdentifier\", \"engine\", \"username\"):\n            if k in connection_params:\n                connection_params.pop(k, None)\n\n    if None in connection_params.values():\n        raise ValueError(\n            \"Connection Params Value Error :  Couldn't be Loaded , Check DB Credentials\"\n        )\n        logging.error(\n            \"Can't find database credentials , Either export them as env variable or include in config Block DB\"\n        )\n\n    return connection_params\n\n\ndef get_oauth_credentials() -> tuple:\n    \"\"\"Get OAuth2 credentials from env file and return a config dict\n\n    Return an ordered python tuple that can be passed to functions that\n    authenticate to OSM.\n\n    Order of precedence:\n    1. Environment Variables\n    2. Config File\n    3. Default fallback\n\n    Returns: oauth2_credentials (tuple): Tuple containing OAuth2 client\n             secret, client ID, and redirect URL.\n\n    \"\"\"\n    # This block fetches OSM OAuth2 app credentials passed as\n    # environment variables as a JSON object, from AWS Secrets Manager or\n    # Azure Key Vault.\n    osm_url = os.environ.get(\"OSM_URL\") or config.get(\n        \"OAUTH\", \"OSM_URL\", fallback=\"https://www.openstreetmap.org\"\n    )\n    secret_key = os.environ.get(\"APP_SECRET_KEY\") or config.get(\n        \"OAUTH\", \"APP_SECRET_KEY\"\n    )\n\n    try:\n        oauth2_credentials = os.environ[\"REMOTE_OAUTH\"]\n    except KeyError:\n        # logger.debug(\"EnvVar: REMOTE_OAUTH not supplied; Falling back to other means\")\n\n        client_id = os.environ.get(\"OSM_CLIENT_ID\") or config.get(\n            \"OAUTH\", \"OSM_CLIENT_ID\"\n        )\n        client_secret = os.environ.get(\"OSM_CLIENT_SECRET\") or config.get(\n            \"OAUTH\", \"OSM_CLIENT_SECRET\"\n        )\n        login_redirect_uri = os.environ.get(\"LOGIN_REDIRECT_URI\") or config.get(\n            \"OAUTH\",\n            \"LOGIN_REDIRECT_URI\",\n            fallback=\"http://127.0.0.1:8000/v1/auth/callback\",\n        )\n        scope = os.environ.get(\"OSM_PERMISSION_SCOPE\") or config.get(\n            \"OAUTH\", \"OSM_PERMISSION_SCOPE\", fallback=\"read_prefs\"\n        )\n\n    else:\n        # Standard library imports\n        import json\n\n        oauth2_credentials_json = json.loads(oauth2_credentials)\n\n        client_id = oauth2_credentials_json[\"OSM_CLIENT_ID\"]\n        client_secret = oauth2_credentials_json[\"OSM_CLIENT_SECRET\"]\n        login_redirect_uri = oauth2_credentials_json[\"LOGIN_REDIRECT_URI\"]\n        scope = oauth2_credentials_json[\"OSM_PERMISSION_SCOPE\"]\n\n    oauth_cred = (\n        osm_url,\n        client_id,\n        client_secret,\n        secret_key,\n        login_redirect_uri,\n        scope,\n    )\n\n    if None in oauth_cred:\n        raise ValueError(\"Oauth Credentials can't be loaded\")\n\n    return oauth_cred\n"}
{"type": "source_file", "path": "src/query_builder/builder.py", "content": "# Copyright (C) 2021 Humanitarian OpenStreetmap Team\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\n# Humanitarian OpenStreetmap Team\n# 1100 13th Street NW Suite 800 Washington, D.C. 20005\n# <info@hotosm.org>\n\"\"\"Page Contains Query logic required for application\"\"\"\n# Standard library imports\nimport re\nfrom json import dumps, loads\n\n# Third party imports\nfrom geomet import wkt\n\n# Reader imports\nfrom src.config import USE_DUCK_DB_FOR_CUSTOM_EXPORTS\nfrom src.config import logger as logging\nfrom src.validation.models import SupportedFilters, SupportedGeometryFilters\n\nHDX_FILTER_CRITERIA = \"\"\"\nThis theme includes all OpenStreetMap features in this area matching ( Learn what tags means [here](https://wiki.openstreetmap.org/wiki/Tags) ) :\n\n{criteria}\n\"\"\"\nHDX_MARKDOWN = \"\"\"\n{filter_str}\nFeatures may have these attributes:\n\n{columns}\n\nThis dataset is one of many [OpenStreetMap exports on\nHDX](https://data.humdata.org/organization/hot).\nSee the [Humanitarian OpenStreetMap Team](http://hotosm.org/) website for more\ninformation.\n\"\"\"\n\n\ndef get_grid_id_query(geometry_dump):\n    base_query = f\"\"\"select\n                        b.poly_id\n                    from\n                        grid b\n                    where\n                        ST_Intersects(ST_GEOMFROMGEOJSON('{geometry_dump}') ,\n                        b.geom)\"\"\"\n    return base_query\n\n\ndef get_country_id_query(geom_dump):\n    base_query = f\"\"\"select\n                        b.cid::int as fid\n                    from\n                        countries b\n                    where\n                        ST_Intersects(ST_GEOMFROMGEOJSON('{geom_dump}') ,\n                        b.geometry)\n                    order by ST_Area(ST_Intersection(b.geometry,ST_MakeValid(ST_GEOMFROMGEOJSON('{geom_dump}')))) desc\n\n                    \"\"\"\n    return base_query\n\n\ndef check_exisiting_country(geom):\n    query = f\"\"\"select\n                        b.cid::int as fid\n                    from\n                        countries b\n                    where\n                        ST_Equals(ST_SnapToGrid(ST_GEOMFROMGEOJSON('{geom}'),0.00001) ,\n                        ST_SnapToGrid(b.geometry,0.00001))\n                    \"\"\"\n    return query\n\n\ndef get_query_as_geojson(query_list, ogr_export=None):\n    table_base_query = []\n    if ogr_export:\n        table_base_query = query_list\n    else:\n        for i in range(len(query_list)):\n            table_base_query.append(\n                f\"\"\"select ST_AsGeoJSON(t{i}.*) from ({query_list[i]}) t{i}\"\"\"\n            )\n    final_query = \" UNION ALL \".join(table_base_query)\n    return final_query\n\n\ndef create_geom_filter(geom, geom_lookup_by=\"ST_intersects\"):\n    \"\"\"generates geometry intersection filter - Rawdata extraction\"\"\"\n    geometry_dump = dumps(loads(geom.model_dump_json()))\n    # return f\"\"\"{geom_lookup_by}(geom,ST_Buffer((select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{geometry_dump}')))),0.005))\"\"\"\n    return f\"\"\"{geom_lookup_by}(geom,(select ST_Union(ST_makeValid(ST_GEOMFROMGEOJSON('{geometry_dump}')))))\"\"\"\n\n\ndef format_file_name_str(input_str):\n    # Fixme I need to check every possible special character that can comeup on osm tags\n    input_str = re.sub(\"\\s+\", \"_\", input_str)  # putting _ in every space  # noqa\n    input_str = re.sub(\":\", \"_\", input_str)  # putting _ in every : value\n    input_str = re.sub(\"-\", \"_\", input_str)  # putting _ in every - value\n\n    return input_str\n\n\ndef remove_spaces(input_str):\n    # Fixme I need to check every possible special character that can comeup on osm tags\n    input_str = re.sub(\"\\s+\", \"_\", input_str)  # putting _ in every space # noqa\n    input_str = re.sub(\":\", \"_\", input_str)  # putting _ in every : value\n    return input_str\n\n\ndef create_column_filter(\n    columns,\n    create_schema=False,\n    output_type=\"geojson\",\n    use_centroid=False,\n    include_osm_type=True,\n    include_user_metadata=False,\n):\n    \"\"\"generates column filter , which will be used to filter column in output will be used on select query - Rawdata extraction\"\"\"\n\n    if len(columns) > 0:\n        filter_col = []\n        filter_col.append(\"osm_id\")\n        if include_osm_type:\n            filter_col.append(\"tableoid::regclass AS osm_type\")\n        if include_user_metadata:\n            filter_col.extend([\"uid\", f\"\"\" \"user\" \"\"\", \"timestamp\"])\n        if create_schema:\n            schema = {}\n            schema[\"osm_id\"] = \"int64\"\n            schema[\"type\"] = \"str\"\n            if include_user_metadata:\n                schema[\"uid\"] = \"int64\"\n                schema[\"user\"] = \"str\"\n                schema[\"timestamp\"] = \"str\"\n\n        if \"*\" in columns:\n            filter_col.append(\"tags\")\n            if create_schema:\n                schema[\"tags\"] = \"str\"\n        else:\n            for cl in columns:\n                splitted_cl = [cl]\n                if \",\" in cl:\n                    splitted_cl = cl.split(\",\")\n                for cl in splitted_cl:\n                    if cl != \"\":\n                        filter_col.append(\n                            f\"\"\"tags ->> '{cl.strip()}' as {remove_spaces(cl.strip())}\"\"\"\n                        )\n                        if create_schema:\n                            schema[remove_spaces(cl.strip())] = \"str\"\n        if output_type == \"csv\":  # if it is csv geom logic is different\n            filter_col.append(\"ST_X(ST_Centroid(geom)) as longitude\")\n            filter_col.append(\"ST_Y(ST_Centroid(geom)) as latitude\")\n            filter_col.append(\"GeometryType(geom) as geom_type\")\n        else:\n            filter_col.append(\"ST_Centroid(geom) as geom\" if use_centroid else \"geom\")\n        select_condition = \" , \".join(filter_col)\n        if create_schema:\n            return select_condition, schema\n        return select_condition\n    else:\n        if include_user_metadata:\n            return f\"\"\"osm_id, tableoid::regclass AS osm_type, tags,changeset, uid, \"user\", timestamp,{'ST_Centroid(geom) as geom' if use_centroid else 'geom'}\"\"\"\n        return f\"osm_id, tableoid::regclass AS osm_type, tags,changeset,timestamp,{'ST_Centroid(geom) as geom' if use_centroid else 'geom'}\"  # this is default attribute that we will deliver to user if user defines his own attribute column then those will be appended with osm_id only\n\n\ndef create_tag_sql_logic(key, value, filter_list):\n    if len(value) > 1:\n        v_l = []\n        for lil in value:\n            v_l.append(f\"\"\" '{lil.strip()}' \"\"\")\n        v_l_join = \", \".join(v_l)\n        value_tuple = f\"\"\"({v_l_join})\"\"\"\n\n        k = f\"\"\" '{key.strip()}' \"\"\"\n        filter_list.append(\"\"\"tags ->> \"\"\" + k + \"\"\"IN \"\"\" + value_tuple + \"\"\"\"\"\")\n    elif len(value) == 1:\n        filter_list.append(f\"\"\"tags ->> '{key.strip()}' = '{value[0].strip()}'\"\"\")\n    else:\n        filter_list.append(f\"\"\"tags ? '{key.strip()}'\"\"\")\n    return filter_list\n\n\ndef generate_tag_filter_query(filter, join_by=\" OR \", plain_query_filter=False):\n    final_filter = []\n    if plain_query_filter:\n        for item in filter:\n            key = item[\"key\"]\n            value = item[\"value\"]\n            if len(value) == 1 and value[0] == \"*\":\n                value = []\n            if len(value) >= 1:\n                sub_append = []\n                pre = \"\"\" tags @> '{\"\"\"\n                post = \"\"\" }'\"\"\"\n                for v in value:\n                    sub_append.append(\n                        f\"\"\"{pre} \"{key.strip()}\" : \"{v.strip()}\" {post}\"\"\"\n                    )\n                sub_append_join = \" OR \".join(sub_append)\n\n                final_filter.append(f\"({sub_append_join})\")\n            else:\n                final_filter.append(f\"\"\"tags ? '{key.strip()}'\"\"\")\n        tag_filter = join_by.join(final_filter)\n        return tag_filter\n\n    else:\n        for key, value in filter.items():\n            if key == \"join_or\":\n                temp_logic = []\n                if value:\n                    for k, v in value.items():\n                        temp_logic = create_tag_sql_logic(k, v, temp_logic)\n                    final_filter.append(f\"\"\"{\" OR \".join(temp_logic)}\"\"\")\n\n            if key == \"join_and\":\n                temp_logic = []\n                if value:\n                    for k, v in value.items():\n                        temp_logic = create_tag_sql_logic(k, v, temp_logic)\n                    if len(temp_logic) == 1:\n                        join_by = \" AND \"\n                    final_filter.append(f\"\"\"{\" AND \".join(temp_logic)}\"\"\")\n\n        tag_filter = join_by.join(final_filter)\n        return tag_filter\n\n\ndef extract_geometry_type_query(\n    params,\n    ogr_export=False,\n    g_id=None,\n    c_id=None,\n    country_export=False,\n):\n    \"\"\"used for specifically focused on export tool , this will generate separate queries for line point and polygon can be used on other datatype support - Rawdata extraction\"\"\"\n    include_user_metadata = params.include_user_metadata\n    geom_filter = create_geom_filter(\n        params.geometry,\n        \"ST_within\" if params.use_st_within is True else \"ST_intersects\",\n    )\n    select_condition = f\"\"\"osm_id, tableoid::regclass AS osm_type, tags,changeset,timestamp , {'ST_Centroid(geom) as geom' if params.centroid else 'geom'}\"\"\"  # this is default attribute that we will deliver to user if user defines his own attribute column then those will be appended with osm_id only\n    schema = {\n        \"osm_id\": \"int64\",\n        \"type\": \"str\",\n        \"tags\": \"str\",\n        \"changeset\": \"int64\",\n        \"timestamp\": \"str\",\n    }\n    query_point, query_line, query_poly = None, None, None\n    (\n        attribute_filter,\n        master_attribute_filter,\n        master_tag_filter,\n        poly_attribute_filter,\n        poly_tag_filter,\n    ) = (None, None, None, None, None)\n    point_schema, line_schema, poly_schema = None, None, None\n    (\n        tags,\n        attributes,\n        point_attribute_filter,\n        line_attribute_filter,\n        poly_attribute_filter,\n        master_attribute_filter,\n        point_tag_filter,\n        line_tag_filter,\n        poly_tag_filter,\n        master_tag_filter,\n    ) = (None, None, None, None, None, None, None, None, None, None)\n    if params.filters:\n        params.filters = (\n            params.filters.model_dump()\n        )  # FIXME: temp fix , since validation model got changed\n        (\n            tags,\n            attributes,\n            point_attribute_filter,\n            line_attribute_filter,\n            poly_attribute_filter,\n            master_attribute_filter,\n            point_tag_filter,\n            line_tag_filter,\n            poly_tag_filter,\n            master_tag_filter,\n        ) = extract_attributes_tags(params.filters)\n\n    if (\n        master_attribute_filter\n    ):  # if no specific point , line or poly filter is not passed master columns filter will be used , if master columns is also empty then above default select statement will be used\n        select_condition, schema = create_column_filter(\n            use_centroid=params.centroid,\n            output_type=params.output_type,\n            columns=master_attribute_filter,\n            create_schema=True,\n            include_user_metadata=include_user_metadata,\n        )\n    if master_tag_filter:\n        attribute_filter = generate_tag_filter_query(master_tag_filter)\n    if params.geometry_type is None:  # fix me\n        params.geometry_type = [\"point\", \"line\", \"polygon\"]\n\n    for type in params.geometry_type:\n        if type == SupportedGeometryFilters.POINT.value:\n            if point_attribute_filter:\n                select_condition, schema = create_column_filter(\n                    use_centroid=params.centroid,\n                    output_type=params.output_type,\n                    columns=point_attribute_filter,\n                    create_schema=True,\n                    include_user_metadata=include_user_metadata,\n                )\n            where_clause_for_nodes = generate_where_clause_indexes_case(\n                geom_filter, g_id, c_id, country_export, \"nodes\"\n            )\n\n            query_point = f\"\"\"select\n                        {select_condition}\n                        from\n                            nodes\n                        where\n                            {where_clause_for_nodes}\"\"\"\n            if point_tag_filter:\n                attribute_filter = generate_tag_filter_query(point_tag_filter)\n            if attribute_filter:\n                query_point += f\"\"\" and ({attribute_filter})\"\"\"\n            point_schema = schema\n\n            query_point = get_query_as_geojson([query_point], ogr_export=ogr_export)\n\n        if type == SupportedGeometryFilters.LINE.value:\n            query_line_list = []\n            if line_attribute_filter:\n                select_condition, schema = create_column_filter(\n                    use_centroid=params.centroid,\n                    output_type=params.output_type,\n                    columns=line_attribute_filter,\n                    create_schema=True,\n                    include_user_metadata=include_user_metadata,\n                )\n            where_clause_for_line = generate_where_clause_indexes_case(\n                geom_filter, g_id, c_id, country_export, \"ways_line\"\n            )\n\n            query_ways_line = f\"\"\"select\n                {select_condition}\n                from\n                    ways_line\n                where\n                    {where_clause_for_line}\"\"\"\n            where_clause_for_rel = generate_where_clause_indexes_case(\n                geom_filter, g_id, c_id, country_export, \"relations\"\n            )\n\n            query_relations_line = f\"\"\"select\n                {select_condition}\n                from\n                    relations\n                where\n                    {where_clause_for_rel}\"\"\"\n            if line_tag_filter:\n                attribute_filter = generate_tag_filter_query(line_tag_filter)\n            if attribute_filter:\n                query_ways_line += f\"\"\" and ({attribute_filter})\"\"\"\n                query_relations_line += f\"\"\" and ({attribute_filter})\"\"\"\n            query_relations_line += \"\"\" and (geometrytype(geom)='MULTILINESTRING')\"\"\"\n            query_line_list.append(query_ways_line)\n            query_line_list.append(query_relations_line)\n            query_line = get_query_as_geojson(query_line_list, ogr_export=ogr_export)\n            line_schema = schema\n\n        if type == SupportedGeometryFilters.POLYGON.value:\n            query_poly_list = []\n            if poly_attribute_filter:\n                select_condition, schema = create_column_filter(\n                    use_centroid=params.centroid,\n                    output_type=params.output_type,\n                    columns=poly_attribute_filter,\n                    create_schema=True,\n                    include_user_metadata=include_user_metadata,\n                )\n\n            where_clause_for_poly = generate_where_clause_indexes_case(\n                geom_filter, g_id, c_id, country_export, \"ways_poly\"\n            )\n\n            query_ways_poly = f\"\"\"select\n                {select_condition}\n                from\n                    ways_poly\n                where\n                    {where_clause_for_poly}\"\"\"\n            where_clause_for_relations = generate_where_clause_indexes_case(\n                geom_filter, g_id, c_id, country_export, \"relations\"\n            )\n\n            query_relations_poly = f\"\"\"select\n                {select_condition}\n                from\n                    relations\n                where\n                    {where_clause_for_relations}\"\"\"\n            if poly_tag_filter:\n                attribute_filter = generate_tag_filter_query(poly_tag_filter)\n            if attribute_filter:\n                query_ways_poly += f\"\"\" and ({attribute_filter})\"\"\"\n                query_relations_poly += f\"\"\" and ({attribute_filter})\"\"\"\n            query_relations_poly += \"\"\" and (geometrytype(geom)='POLYGON' or geometrytype(geom)='MULTIPOLYGON')\"\"\"\n            query_poly_list.append(query_ways_poly)\n            query_poly_list.append(query_relations_poly)\n            query_poly = get_query_as_geojson(query_poly_list, ogr_export=ogr_export)\n            poly_schema = schema\n    return query_point, query_line, query_poly, point_schema, line_schema, poly_schema\n\n\ndef extract_attributes_tags(filters):\n    tags = None\n    attributes = None\n    point_tag_filter = None\n    poly_tag_filter = None\n    line_tag_filter = None\n    master_tag_filter = None\n    point_attribute_filter = None\n    poly_attribute_filter = None\n    line_attribute_filter = None\n    master_attribute_filter = None\n    if filters:\n        for key, value in filters.items():\n            if key == SupportedFilters.TAGS.value:\n                if value:\n                    tags = value\n                    for k, v in value.items():\n                        if k == SupportedGeometryFilters.POINT.value:\n                            point_tag_filter = v\n                        if k == SupportedGeometryFilters.LINE.value:\n                            line_tag_filter = v\n                        if k == SupportedGeometryFilters.POLYGON.value:\n                            poly_tag_filter = v\n                        if k == SupportedGeometryFilters.ALLGEOM.value:\n                            master_tag_filter = v\n            if key == SupportedFilters.ATTRIBUTES.value:\n                if value:\n                    attributes = value\n                    for k, v in value.items():\n                        if k == SupportedGeometryFilters.POINT.value:\n                            point_attribute_filter = v\n                        if k == SupportedGeometryFilters.LINE.value:\n                            line_attribute_filter = v\n                        if k == SupportedGeometryFilters.POLYGON.value:\n                            poly_attribute_filter = v\n                        if k == SupportedGeometryFilters.ALLGEOM.value:\n                            master_attribute_filter = v\n    return (\n        tags,\n        attributes,\n        point_attribute_filter,\n        line_attribute_filter,\n        poly_attribute_filter,\n        master_attribute_filter,\n        point_tag_filter,\n        line_tag_filter,\n        poly_tag_filter,\n        master_tag_filter,\n    )\n\n\ndef generate_where_clause_indexes_case(\n    geom_filter, g_id, c_id, country_export, table_name=\"ways_poly\"\n):\n    where_clause = geom_filter\n    if g_id:\n        if (\n            table_name == \"ways_poly\"\n        ):  # currently grid index is only available for ways_poly\n            column_name = \"grid\"\n            grid_filter_base = [f\"\"\"{column_name} = {ind[0]}\"\"\" for ind in g_id]\n            grid_filter = \" OR \".join(grid_filter_base)\n            where_clause = f\"({grid_filter}) and ({geom_filter})\"\n    if c_id:\n        c_id = \",\".join(str(num) for num in c_id)\n        # if table_name == \"ways_poly\" or table_name == \"nodes\":\n        #     where_clause += f\" and (country IN ({c_id}))\"\n        # else:\n        where_clause += f\" and (country @> ARRAY[{c_id}])\"\n    if (\n        country_export\n    ):  # ignore the geometry take geom from the db itself by using precalculated field\n        if c_id:\n            # if table_name == \"ways_poly\" or table_name == \"nodes\":\n            #     where_clause = f\"country IN ({c_id})\"\n            # else:\n            where_clause = f\"country @> ARRAY[{c_id}]\"\n    return where_clause\n\n\ndef get_country_geojson(c_id):\n    query = f\"SELECT ST_AsGeoJSON(geometry) as geom from countries where id={c_id}\"\n    return query\n\n\ndef raw_currentdata_extraction_query(\n    params,\n    g_id=None,\n    c_id=None,\n    ogr_export=False,\n    country_export=False,\n):\n    \"\"\"Default function to support current snapshot extraction with all of the feature that export_tool_api has\"\"\"\n    include_user_metadata = params.include_user_metadata\n    geom_lookup_by = \"ST_within\" if params.use_st_within is True else \"ST_intersects\"\n    geom_filter = create_geom_filter(params.geometry, geom_lookup_by)\n\n    base_query = []\n\n    (\n        tags,\n        attributes,\n        point_attribute_filter,\n        line_attribute_filter,\n        poly_attribute_filter,\n        master_attribute_filter,\n        point_tag_filter,\n        line_tag_filter,\n        poly_tag_filter,\n        master_tag_filter,\n    ) = (None, None, None, None, None, None, None, None, None, None)\n\n    point_select_condition = None\n    line_select_condition = None\n    poly_select_condition = None\n\n    point_tag = None\n    line_tag = None\n    poly_tag = None\n    master_tag = None\n    use_geomtype_in_relation = True\n\n    # query_table = []\n    if params.include_user_metadata:\n        select_condition = f\"\"\"osm_id, tableoid::regclass AS osm_type, version,tags,changeset, uid, \"user\", timestamp,{'ST_Centroid(geom) as geom' if params.centroid else 'geom'}\"\"\"\n    else:\n        select_condition = f\"\"\"osm_id, tableoid::regclass AS osm_type, version,tags,changeset,timestamp,{'ST_Centroid(geom) as geom' if params.centroid else 'geom'}\"\"\"  # this is default attribute that we will deliver to user if user defines his own attribute column then those will be appended with osm_id only\n\n    point_select_condition = select_condition  # initializing default\n    line_select_condition = select_condition\n    poly_select_condition = select_condition\n\n    if params.filters:\n        params.filters = (\n            params.filters.model_dump()\n        )  # FIXME: temp fix , since validation model got changed\n        (\n            tags,\n            attributes,\n            point_attribute_filter,\n            line_attribute_filter,\n            poly_attribute_filter,\n            master_attribute_filter,\n            point_tag_filter,\n            line_tag_filter,\n            poly_tag_filter,\n            master_tag_filter,\n        ) = extract_attributes_tags(params.filters)\n    attribute_customization_full_support = [\"geojson\", \"shp\"]\n\n    if params.output_type not in attribute_customization_full_support:\n        logging.debug(\n            \"Merging filters since they don't have same no of filters for features\"\n        )\n        merged_array = [\n            i if i else []\n            for i in [\n                point_attribute_filter,\n                line_attribute_filter,\n                poly_attribute_filter,\n            ]\n        ]\n        merged_result = list({x for l in merged_array for x in l})\n        logging.debug(merged_result)\n        if point_attribute_filter:\n            point_attribute_filter = merged_result\n        if line_attribute_filter:\n            line_attribute_filter = merged_result\n        if poly_attribute_filter:\n            poly_attribute_filter = merged_result\n\n    if attributes:\n        if master_attribute_filter:\n            if len(master_attribute_filter) > 0:\n                select_condition = create_column_filter(\n                    use_centroid=params.centroid,\n                    output_type=params.output_type,\n                    columns=master_attribute_filter,\n                    include_user_metadata=include_user_metadata,\n                )\n                # if master attribute is supplied it will be applied to other geom type as well even though value is supplied they will be ignored\n                point_select_condition = select_condition\n                line_select_condition = select_condition\n                poly_select_condition = select_condition\n        else:\n            if point_attribute_filter:\n                if len(point_attribute_filter) > 0:\n                    point_select_condition = create_column_filter(\n                        use_centroid=params.centroid,\n                        output_type=params.output_type,\n                        columns=point_attribute_filter,\n                        include_user_metadata=include_user_metadata,\n                    )\n            if line_attribute_filter:\n                if len(line_attribute_filter) > 0:\n                    line_select_condition = create_column_filter(\n                        use_centroid=params.centroid,\n                        output_type=params.output_type,\n                        columns=line_attribute_filter,\n                        include_user_metadata=include_user_metadata,\n                    )\n            if poly_attribute_filter:\n                if len(poly_attribute_filter) > 0:\n                    poly_select_condition = create_column_filter(\n                        use_centroid=params.centroid,\n                        output_type=params.output_type,\n                        columns=poly_attribute_filter,\n                        include_user_metadata=include_user_metadata,\n                    )\n\n    if tags:\n        if (\n            master_tag_filter\n        ):  # if master tag is supplied then other tags should be ignored and master tag will be used\n            master_tag = generate_tag_filter_query(master_tag_filter)\n            point_tag = master_tag\n            line_tag = master_tag\n            poly_tag = master_tag\n        else:\n            if point_tag_filter:\n                point_tag = generate_tag_filter_query(point_tag_filter)\n            if line_tag_filter:\n                line_tag = generate_tag_filter_query(line_tag_filter)\n            if poly_tag_filter:\n                poly_tag = generate_tag_filter_query(poly_tag_filter)\n\n    # condition for geometry types\n\n    if params.geometry_type is None or len(params.geometry_type) == 0:\n        params.geometry_type = [\"point\", \"line\", \"polygon\"]\n\n    if SupportedGeometryFilters.ALLGEOM.value in params.geometry_type:\n        params.geometry_type = [\"point\", \"line\", \"polygon\"]\n    if SupportedGeometryFilters.POINT.value in params.geometry_type:\n        where_clause_for_nodes = generate_where_clause_indexes_case(\n            geom_filter, g_id, c_id, country_export, \"nodes\"\n        )\n\n        query_point = f\"\"\"select\n                    {point_select_condition}\n                    from\n                        nodes\n                    where\n                        {where_clause_for_nodes}\"\"\"\n        if point_tag:\n            query_point += f\"\"\" and ({point_tag})\"\"\"\n        base_query.append(query_point)\n\n    if SupportedGeometryFilters.LINE.value in params.geometry_type:\n        where_clause_for_line = generate_where_clause_indexes_case(\n            geom_filter, g_id, c_id, country_export, \"ways_line\"\n        )\n\n        query_ways_line = f\"\"\"select\n            {line_select_condition}\n            from\n                ways_line\n            where\n                {where_clause_for_line}\"\"\"\n        if line_tag:\n            query_ways_line += f\"\"\" and ({line_tag})\"\"\"\n        base_query.append(query_ways_line)\n\n        if SupportedGeometryFilters.POLYGON.value in params.geometry_type:\n            if poly_select_condition == line_select_condition and poly_tag == line_tag:\n                use_geomtype_in_relation = False\n\n        if use_geomtype_in_relation:\n            where_clause_for_rel = generate_where_clause_indexes_case(\n                geom_filter, g_id, c_id, country_export, \"relations\"\n            )\n\n            query_relations_line = f\"\"\"select\n                {line_select_condition}\n                from\n                    relations\n                where\n                    {where_clause_for_rel}\"\"\"\n            if line_tag:\n                query_relations_line += f\"\"\" and ({line_tag})\"\"\"\n            query_relations_line += \"\"\" and (geometrytype(geom)='MULTILINESTRING')\"\"\"\n            base_query.append(query_relations_line)\n\n    if SupportedGeometryFilters.POLYGON.value in params.geometry_type:\n        where_clause_for_poly = generate_where_clause_indexes_case(\n            geom_filter, g_id, c_id, country_export, \"ways_poly\"\n        )\n\n        query_ways_poly = f\"\"\"select\n            {poly_select_condition}\n            from\n                ways_poly\n            where\n                {where_clause_for_poly}\"\"\"\n        if poly_tag:\n            query_ways_poly += f\"\"\" and ({poly_tag})\"\"\"\n        base_query.append(query_ways_poly)\n        where_clause_for_relations = generate_where_clause_indexes_case(\n            geom_filter, g_id, c_id, country_export, \"relations\"\n        )\n        query_relations_poly = f\"\"\"select\n            {poly_select_condition}\n            from\n                relations\n            where\n                {where_clause_for_relations}\"\"\"\n        if poly_tag:\n            query_relations_poly += f\"\"\" and ({poly_tag})\"\"\"\n        if use_geomtype_in_relation:\n            query_relations_poly += \"\"\" and (geometrytype(geom)='POLYGON' or geometrytype(geom)='MULTIPOLYGON')\"\"\"\n        base_query.append(query_relations_poly)\n\n    if ogr_export:\n        # since query will be different for ogr exports and geojson exports because for ogr exports we don't need to grab each row in geojson\n        table_base_query = base_query\n    else:\n        table_base_query = []\n        for i in range(len(base_query)):\n            table_base_query.append(\n                f\"\"\"select ST_AsGeoJSON(t{i}.*) from ({base_query[i]}) t{i}\"\"\"\n            )\n    final_query = \" UNION ALL \".join(table_base_query)\n    if params.output_type == \"csv\":\n        logging.debug(final_query)\n\n    return final_query\n\n\ndef check_last_updated_rawdata():\n    query = \"\"\"select importdate as last_updated from planet_osm_replication_status\"\"\"\n    return query\n\n\ndef raw_extract_plain_geojson(params, inspect_only=False):\n    geom_filter_cond = None\n    if params.geometry_type == \"polygon\":\n        geom_filter_cond = \"\"\" and (geometrytype(geom)='POLYGON' or geometrytype(geom)='MULTIPOLYGON')\"\"\"\n    select_condition = create_column_filter(columns=params.select)\n    where_condition = generate_tag_filter_query(params.where, params.join_by)\n    if params.bbox:\n        xmin, ymin, xmax, ymax = (\n            params.bbox[0],\n            params.bbox[1],\n            params.bbox[2],\n            params.bbox[3],\n        )\n        geom_condition = f\"\"\"ST_intersects(ST_MakeEnvelope({xmin}, {ymin}, {xmax}, {ymax},4326), geom)\"\"\"\n\n    query_list = []\n    for table_name in params.look_in:\n        sub_query = f\"\"\"select {select_condition} from {table_name} where ({where_condition}) \"\"\"\n        if params.bbox:\n            sub_query += f\"\"\" and {geom_condition}\"\"\"\n        if geom_filter_cond:\n            sub_query += geom_filter_cond\n        query_list.append(sub_query)\n    table_base_query = []\n    for i in range(len(query_list)):\n        table_base_query.append(\n            f\"\"\"select ST_AsGeoJSON(t{i}.*) from ({query_list[i]}) t{i}\"\"\"\n        )\n    final_query = \" UNION ALL \".join(table_base_query)\n    if inspect_only:\n        final_query = f\"\"\"EXPLAIN\n        {final_query}\"\"\"\n    return final_query\n\n\ndef get_countries_query(q):\n    query = \"Select ST_AsGeoJSON(cf.*) FROM countries cf\"\n    if q:\n        query += f\" WHERE name ILIKE '%{q}%'\"\n    return query\n\n\ndef get_country_cid(cid):\n    query = f\"Select ST_AsGeoJSON(cf.*) FROM countries cf where cid = {cid}\"\n    return query\n\n\ndef get_osm_feature_query(osm_id):\n    select_condition = (\n        \"osm_id, tableoid::regclass AS osm_type, tags,changeset,timestamp,geom\"\n    )\n    query = f\"\"\"SELECT ST_AsGeoJSON(n.*)\n        FROM (select {select_condition} from nodes) n \n        WHERE osm_id = {osm_id}\n        UNION\n        SELECT ST_AsGeoJSON(wl.*)\n        FROM (select {select_condition} from ways_line) wl \n        WHERE osm_id = {osm_id}\n        UNION\n        SELECT ST_AsGeoJSON(wp.*)\n        FROM (select {select_condition} from ways_poly) wp \n        WHERE osm_id = {osm_id}\n        UNION\n        SELECT ST_AsGeoJSON(r.*)\n        FROM (select {select_condition} from relations) r \n        WHERE osm_id = {osm_id}\"\"\"\n    return query\n\n\ndef generate_polygon_stats_graphql_query(geojson_feature):\n    \"\"\"\n    Gernerates the graphql query for the statistics\n    \"\"\"\n    query = \"\"\"\n    {\n        polygonStatistic (\n        polygonStatisticRequest: {\n            polygon: %s\n        }\n        )\n        {\n        analytics {\n            functions(args:[\n            {name:\"sumX\", id:\"population\", x:\"population\"},\n            {name:\"sumX\", id:\"populatedAreaKm2\", x:\"populated_area_km2\"},\n            {name:\"percentageXWhereNoY\", id:\"osmBuildingGapsPercentage\", x:\"populated_area_km2\", y:\"building_count\"},\n            {name:\"percentageXWhereNoY\", id:\"osmRoadGapsPercentage\", x:\"populated_area_km2\", y:\"highway_length\"},\n            {name:\"percentageXWhereNoY\", id:\"antiqueOsmBuildingsPercentage\", x:\"populated_area_km2\", y:\"building_count_6_months\"},\n            {name:\"percentageXWhereNoY\", id:\"antiqueOsmRoadsPercentage\", x:\"populated_area_km2\", y:\"highway_length_6_months\"},\n            {name:\"avgX\", id:\"averageEditTime\", x:\"avgmax_ts\"},\n            {name:\"maxX\", id:\"lastEditTime\", x:\"avgmax_ts\"},\n            {name:\"sumX\", id:\"osmBuildingsCount\", x:\"building_count\"},\n            {name:\"sumX\", id:\"highway_length\", x:\"highway_length\"},\n            {name:\"sumX\", id:\"osmUsersCount\", x:\"osm_users\"},\n            {name:\"sumX\", id:\"building_count_6_months\" , x:\"building_count_6_months\"},\n            {name:\"sumX\", id:\"highway_length_6_months\", x:\"highway_length_6_months\"},\n            {name:\"sumX\", id:\"aiBuildingsCountEstimation\", x:\"total_building_count\"}\n            {name:\"sumX\", id:\"aiRoadCountEstimation\", x:\"total_road_length\"}\n\n            ]) {\n            id,\n            result\n            }\n        }\n        }\n    }\n  \"\"\"\n    query = query % dumps(geojson_feature)\n\n    return query\n\n\ndef get_country_from_iso(iso3):\n    \"\"\"\n    Generate a SQL query to retrieve country information based on ISO3 code.\n\n    Args:\n    - iso3 (str): ISO3 Country Code.\n\n    Returns:\n    str: SQL query to fetch country information.\n    \"\"\"\n    query = f\"\"\"SELECT\n                    b.cid::int as fid, b.dataset->>'dataset_title' as dataset_title, b.dataset->>'dataset_prefix' as dataset_prefix,  b.dataset->>'dataset_locations' as locations\n                FROM\n                    cron b\n                WHERE\n                    LOWER(iso3) = '{iso3}'\n                \"\"\"\n    return query\n\n\ndef convert_tags_pattern_to_postgres(query_string):\n    pattern = r\"tags\\['(.*?)'\\]\"\n\n    converted_query = re.sub(\n        pattern, lambda match: f\"tags->>'{match.group(1)}'\", query_string\n    )\n\n    return converted_query\n\n\ndef postgres2duckdb_query(\n    base_table_name,\n    table,\n    cid=None,\n    geometry=None,\n    single_category_where=None,\n    enable_users_detail=False,\n):\n    \"\"\"\n    Generate a DuckDB query to create a table from a PostgreSQL query.\n\n    Args:\n    - base_table_name (str): Base table name.\n    - table (str): PostgreSQL table name.\n    - cid (int, optional): Country ID for filtering. Defaults to None.\n    - geometry (Polygon, optional): Custom polygon geometry. Defaults to None.\n    - single_category_where (str, optional): Where clause for single category to fetch it from postgres\n    - enable_users_detail (bool, optional): Enable user details. Defaults to False.\n\n    Returns:\n    str: DuckDB query for creating a table.\n    \"\"\"\n    select_query = \"\"\"osm_id, osm_type, version, changeset, timestamp, tags,  ST_AsBinary(geom) as geom\"\"\"\n    create_select_duck_db = \"\"\"osm_id, osm_type , version, changeset, timestamp, cast(tags::json AS map(varchar, varchar)) AS tags, cast(ST_GeomFromWKB(geom) as GEOMETRY) AS geom\"\"\"\n\n    if enable_users_detail:\n        select_query = \"\"\"osm_id, osm_type, uid, \"user\", version, changeset, timestamp, tags, ST_AsBinary(geom) as geom\"\"\"\n        create_select_duck_db = \"\"\"osm_id, osm_type, uid, \"user\", version, changeset, timestamp, cast(tags::json AS map(varchar, varchar)) AS tags, cast(ST_GeomFromWKB(geom) as GEOMETRY) AS geom\"\"\"\n\n    row_filter_condition = (\n        f\"\"\"(country @> ARRAY [{cid}])\"\"\"\n        if cid\n        else f\"\"\"ST_Intersects(geom,(select ST_SetSRID(ST_Extent(ST_makeValid(ST_GeomFromText('{wkt.dumps(loads(geometry.json()),decimals=6)}',4326))),4326)))\"\"\"\n    )\n\n    postgres_query = f\"\"\"select {select_query} from (select * , tableoid::regclass as osm_type from {table} where {row_filter_condition}) as sub_query\"\"\"\n    if single_category_where:\n        postgres_query += (\n            f\" where {convert_tags_pattern_to_postgres(single_category_where)}\"\n        )\n\n    duck_db_create = f\"\"\"CREATE TABLE {base_table_name}_{table} AS SELECT {create_select_duck_db} FROM postgres_query(\"postgres_db\", \"{postgres_query}\") \"\"\"\n\n    return duck_db_create\n\n\ndef extract_custom_features_from_postgres(\n    select_q, from_q, where_q, geom=None, cid=None\n):\n    \"\"\"\n    Generates Postgresql query for custom feature extraction\n    \"\"\"\n    geom_filter = f\"\"\"(country @> ARRAY [{cid}])\"\"\" if cid else create_geom_filter(geom)\n\n    postgres_query = f\"\"\"select {select_q} from (select * , tableoid::regclass as osm_type from {from_q} where {geom_filter}) as sub_query\"\"\"\n    if where_q:\n        postgres_query += f\" where {convert_tags_pattern_to_postgres(where_q)}\"\n    return postgres_query\n\n\ndef extract_features_custom_exports(\n    base_table_name, select, feature_type, where, geometry=None, cid=None\n):\n    \"\"\"\n    Generate a Extraction query to extract features based on given parameters.\n\n    Args:\n    - base_table_name (str): Base table name.\n    - select (List[str]): List of selected fields.\n    - feature_type (str): Type of feature (points, lines, polygons).\n    - where (str): SQL-like condition to filter features.\n\n    Returns:\n    str: Extraction query to extract features.\n    \"\"\"\n    map_tables = {\n        \"points\": {\"table\": [\"nodes\"], \"where\": {\"nodes\": f\"({where})\"}},\n        \"lines\": {\n            \"table\": [\"ways_line\", \"relations\"],\n            \"where\": {\n                \"ways_line\": where,\n                \"relations\": f\"({where}) and (ST_GeometryType(geom)='MULTILINESTRING')\",\n            },\n        },\n        \"polygons\": {\n            \"table\": [\"ways_poly\", \"relations\"],\n            \"where\": {\n                \"ways_poly\": where,\n                \"relations\": f\"({where}) and (ST_GeometryType(geom)='MULTIPOLYGON' or ST_GeometryType(geom)='POLYGON')\",\n            },\n        },\n    }\n    if USE_DUCK_DB_FOR_CUSTOM_EXPORTS is True:\n        if \"*\" in select:\n            select = [f\"\"\"tags::json as tags \"\"\"]\n        else:\n            select = [f\"\"\"tags['{item}'][1] as \"{item}\" \"\"\" for item in select]\n        select += [\"osm_id\", \"osm_type\", \"geom\"]\n        select_query = \", \".join(select)\n    else:\n        select_query = create_column_filter(select, include_osm_type=False)\n\n    from_query = map_tables[feature_type][\"table\"]\n\n    base_query = []\n    for table in from_query:\n        where_query = map_tables[feature_type][\"where\"][table]\n        if USE_DUCK_DB_FOR_CUSTOM_EXPORTS is True:\n            if geometry:\n                where_query += f\" and (ST_Intersects(geom,ST_GeomFromGeoJSON('{geometry.json()}')))\"\n            query = f\"\"\"select {select_query} from {f\"{base_table_name}_{table}\"} where {where_query}\"\"\"\n        else:\n            query = extract_custom_features_from_postgres(\n                select_q=select_query,\n                from_q=table,\n                where_q=where_query,\n                geom=geometry,\n                cid=cid,\n            )\n        base_query.append(query)\n    return \" UNION ALL \".join(base_query)\n\n\ndef get_country_geom_from_iso(iso3):\n    \"\"\"\n    Generate a SQL query to retrieve country geometry based on ISO3 code.\n\n    Args:\n    - iso3 (str): ISO3 Country Code.\n\n    Returns:\n    str: SQL query to fetch country geometry.\n    \"\"\"\n    query = f\"\"\"SELECT\n                    ST_AsGeoJSON(geometry) as geom\n                FROM\n                    countries b\n                WHERE\n                    LOWER(iso3) = '{iso3}'\n                \"\"\"\n    return query\n"}
{"type": "source_file", "path": "src/app.py", "content": "# Copyright (C) 2021 Humanitarian OpenStreetmap Team\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\n# Humanitarian OpenStreetmap Team\n# 1100 13th Street NW Suite 800 Washington, D.C. 20005\n# <info@hotosm.org>\n\"\"\"Page contains Main core logic of app\"\"\"\n# Standard library imports\nimport concurrent.futures\nimport json\nimport os\nimport pathlib\nimport random\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport time\nimport uuid\nfrom collections import Counter, namedtuple\nfrom datetime import datetime, timedelta, timezone\nfrom json import dumps\nfrom json import loads as json_loads\n\n# Third party imports\nimport boto3\nimport humanize\nimport orjson\nimport psycopg2.extras\nimport requests\nfrom area import area\nfrom fastapi import HTTPException\nfrom geojson import FeatureCollection\nfrom psycopg2 import OperationalError, connect, sql\nfrom psycopg2.extras import DictCursor\nfrom slugify import slugify\nfrom tqdm import tqdm\n\n# Reader imports\nfrom src.config import (\n    AWS_ACCESS_KEY_ID,\n    AWS_SECRET_ACCESS_KEY,\n    BUCKET_NAME,\n    DEFAULT_README_TEXT,\n    ENABLE_CUSTOM_EXPORTS,\n    ENABLE_HDX_EXPORTS,\n    ENABLE_POLYGON_STATISTICS_ENDPOINTS,\n    ENABLE_SOZIP,\n    ENABLE_TILES,\n    EXPORT_MAX_AREA_SQKM,\n)\nfrom src.config import EXPORT_PATH as export_path\nfrom src.config import INDEX_THRESHOLD as index_threshold\nfrom src.config import (\n    LOG_LEVEL,\n    MAX_WORKERS,\n    PARALLEL_PROCESSING_CATEGORIES,\n    POLYGON_STATISTICS_API_URL,\n    PROCESS_SINGLE_CATEGORY_IN_POSTGRES,\n)\nfrom src.config import USE_CONNECTION_POOLING as use_connection_pooling\nfrom src.config import (\n    USE_DUCK_DB_FOR_CUSTOM_EXPORTS,\n    USE_S3_TO_UPLOAD,\n    get_db_connection_params,\n    level,\n)\nfrom src.config import logger as logging\nfrom src.query_builder.builder import (\n    HDX_FILTER_CRITERIA,\n    HDX_MARKDOWN,\n    check_exisiting_country,\n    check_last_updated_rawdata,\n    extract_features_custom_exports,\n    extract_geometry_type_query,\n    generate_polygon_stats_graphql_query,\n    get_countries_query,\n    get_country_cid,\n    get_country_from_iso,\n    get_country_geom_from_iso,\n    get_osm_feature_query,\n    postgres2duckdb_query,\n    raw_currentdata_extraction_query,\n)\nfrom src.utils import create_working_dir\nfrom src.validation.models import EXPORT_TYPE_MAPPING, RawDataOutputType\n\nfrom .post_processing.processor import PostProcessor\n\nif ENABLE_SOZIP:\n    # Third party imports\n    import sozipfile.sozipfile as zipfile\nelse:\n    # Standard library imports\n    import zipfile\n\n# import instance for pooling\nif use_connection_pooling:\n    # Reader imports\n    from src.db_session import database_instance\nelse:\n    database_instance = None\n# Standard library imports\nimport logging as log\n\nif ENABLE_CUSTOM_EXPORTS:\n    if USE_DUCK_DB_FOR_CUSTOM_EXPORTS is True:\n        # Third party imports\n        import duckdb\n\n        # Reader imports\n        from src.config import DUCK_DB_MEMORY_LIMIT, DUCK_DB_THREAD_LIMIT\n\nif ENABLE_HDX_EXPORTS:\n    # Third party imports\n    from hdx.data.dataset import Dataset\n    from hdx.data.resource import Resource\n\n    # Reader imports\n    from src.config import HDX_MAINTAINER, HDX_OWNER_ORG, HDX_URL_PREFIX\n\n\nglobal LOCAL_CON_POOL\n\n# getting the pool instance which was fireup when API is started\nLOCAL_CON_POOL = database_instance\n\n\ndef print_psycopg2_exception(err):\n    \"\"\"\n    Function that handles and parses Psycopg2 exceptions\n    \"\"\"\n    \"\"\"details_exception\"\"\"\n    err_type, err_obj, traceback = sys.exc_info()\n    line_num = traceback.tb_lineno\n    # the connect() error\n    print(\"\\npsycopg2 ERROR:\", err, \"on line number:\", line_num)\n    print(\"psycopg2 traceback:\", traceback, \"-- type:\", err_type)\n    # psycopg2 extensions.Diagnostics object attribute\n    print(\"\\nextensions.Diagnostics:\", err.diag)\n    # pgcode and pgerror exceptions\n    print(\"pgerror:\", err.pgerror)\n    print(\"pgcode:\", err.pgcode, \"\\n\")\n    raise err\n\n\ndef convert_dict_to_conn_str(db_dict):\n    conn_str = \" \".join([f\"{key}={value}\" for key, value in db_dict.items()])\n    return conn_str\n\n\ndef check_for_json(result_str):\n    \"\"\"Check if the Payload is a JSON document\n\n    Return: bool:\n        True in case of success, False otherwise\n    \"\"\"\n    try:\n        r_json = json_loads(result_str)\n        return True, r_json\n    except Exception as ex:\n        logging.error(ex)\n        return False, None\n\n\ndef dict_none_clean(to_clean):\n    \"\"\"Clean DictWriter\"\"\"\n    result = {}\n    for key, value in to_clean.items():\n        if value is None:\n            value = 0\n        result[key] = value\n    return result\n\n\ndef generate_ogr2ogr_cmd_from_psql(\n    export_file_path,\n    export_file_format_driver,\n    postgres_query,\n    layer_creation_options,\n    query_dump_path,\n):\n    \"\"\"\n    Generates ogr2ogr command for postgresql queries\n    \"\"\"\n    db_items = get_db_connection_params()\n    os.makedirs(query_dump_path, exist_ok=True)\n    query_path = os.path.join(query_dump_path, \"query.sql\")\n    with open(query_path, \"w\", encoding=\"UTF-8\") as file:\n        file.write(postgres_query)\n    ogr2ogr_cmd = \"\"\"ogr2ogr -overwrite -f \"{export_format}\" {export_path} PG:\"host={host} port={port} user={username} dbname={db} password={password}\" -sql @\"{pg_sql_select}\" {layer_creation_options_str} -progress\"\"\".format(\n        export_format=export_file_format_driver,\n        export_path=export_file_path,\n        host=db_items.get(\"host\"),\n        port=db_items.get(\"port\"),\n        username=db_items.get(\"user\"),\n        db=db_items.get(\"dbname\"),\n        password=db_items.get(\"password\"),\n        pg_sql_select=query_path,\n        layer_creation_options_str=(\n            f\"-lco {layer_creation_options}\" if layer_creation_options else \"\"\n        ),\n    )\n    return ogr2ogr_cmd\n\n\ndef run_ogr2ogr_cmd(cmd):\n    \"\"\"Runs command and monitors the file size until the process runs\n\n    Args:\n        cmd (_type_): Command to run for subprocess\n        binding_file_dir (_type_): _description_\n\n    Raises:\n        Exception: If process gets failed\n    \"\"\"\n    try:\n        subprocess.check_output(\n            cmd, env=os.environ, shell=True, preexec_fn=os.setsid, timeout=60 * 60 * 6\n        )\n    except subprocess.CalledProcessError as ex:\n        logging.error(ex.output)\n        raise ex\n\n\nclass Database:\n    \"\"\"Database class is used to connect with your database , run query  and get result from it . It has all tests and validation inside class\"\"\"\n\n    def __init__(self, db_params):\n        \"\"\"Database class constructor\"\"\"\n\n        self.db_params = db_params\n\n    def connect(self):\n        \"\"\"Database class instance method used to connect to database parameters with error printing\"\"\"\n\n        try:\n            self.conn = connect(**self.db_params)\n            self.cur = self.conn.cursor(cursor_factory=DictCursor)\n            # logging.debug(\"Database connection has been Successful...\")\n            return self.conn, self.cur\n        except OperationalError as err:\n            \"\"\"pass exception to function\"\"\"\n\n            print_psycopg2_exception(err)\n            # set the connection to 'None' in case of error\n            self.conn = None\n\n    def executequery(self, query):\n        \"\"\"Function to execute query after connection\"\"\"\n        # Check if the connection was successful\n        try:\n            if self.conn is not None:\n                self.cursor = self.cur\n                if query is not None:\n                    # catch exception for invalid SQL statement\n\n                    try:\n                        logging.debug(\"Query sent to Database\")\n                        self.cursor.execute(query)\n                        try:\n                            result = self.cursor.fetchall()\n                            logging.debug(\"Result fetched from Database\")\n                            return result\n                        except Exception as ex:\n                            logging.error(ex)\n                            return self.cursor.statusmessage\n                    except Exception as err:\n                        print_psycopg2_exception(err)\n                else:\n                    raise ValueError(\"Query is Null\")\n\n                    # rollback the previous transaction before starting another\n                    self.conn.rollback()\n                # closing  cursor object to avoid memory leaks\n                # cursor.close()\n                # self.conn.close()\n            else:\n                print(\"Database is not connected\")\n        except Exception as err:\n            print(\"Oops ! You forget to have connection first\")\n            raise err\n\n    def close_conn(self):\n        \"\"\"function for clossing connection to avoid memory leaks\"\"\"\n\n        # Check if the connection was successful\n        try:\n            if self.conn is not None:\n                if self.cur is not None:\n                    self.cur.close()\n                    self.conn.close()\n        except Exception as err:\n            raise err\n\n\nclass Users:\n    \"\"\"\n    Users class provides CRUD operations for interacting with the 'users' table in the database.\n\n    Methods:\n    - create_user(osm_id: int, role: int) -> Dict[str, Any]: Inserts a new user into the database.\n    - read_user(osm_id: int) -> Dict[str, Any]: Retrieves user information based on the given osm_id.\n    - update_user(osm_id: int, update_data: UserUpdate) -> Dict[str, Any]: Updates user information based on the given osm_id.\n    - delete_user(osm_id: int) -> Dict[str, Any]: Deletes a user based on the given osm_id.\n    - read_users(skip: int = 0, limit: int = 10) -> List[Dict[str, Any]]: Retrieves a list of users with optional pagination.\n\n    Usage:\n    users = Users()\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes an instance of the Auth class, connecting to the database.\n        \"\"\"\n        dbdict = get_db_connection_params()\n        self.d_b = Database(dbdict)\n        self.con, self.cur = self.d_b.connect()\n\n    def create_user(self, osm_id, role):\n        \"\"\"\n        Inserts a new user into the 'users' table and returns the created user's osm_id.\n\n        Args:\n        - osm_id (int): The OSM ID of the new user.\n        - role (int): The role of the new user.\n\n        Returns:\n        - Dict[str, Any]: A dictionary containing the osm_id of the newly created user.\n\n        Raises:\n        - HTTPException: If the user creation fails.\n        \"\"\"\n        query = \"INSERT INTO users (osm_id, role) VALUES (%s, %s) RETURNING osm_id;\"\n        params = (osm_id, role)\n        self.cur.execute(self.cur.mogrify(query, params).decode(\"utf-8\"))\n        new_osm_id = self.cur.fetchall()[0][0]\n        self.con.commit()\n        self.d_b.close_conn()\n        return {\"osm_id\": new_osm_id}\n\n    def read_user(self, osm_id):\n        \"\"\"\n        Retrieves user information based on the given osm_id.\n\n        Args:\n        - osm_id (int): The OSM ID of the user to retrieve.\n\n        Returns:\n        - Dict[str, Any]: A dictionary containing user information if the user is found.\n                        If the user is not found, returns a default user with 'role' set to 3.\n\n        Raises:\n        - HTTPException: If there's an issue with the database query.\n        \"\"\"\n        query = \"SELECT * FROM users WHERE osm_id = %s;\"\n        params = (osm_id,)\n        self.cur.execute(self.cur.mogrify(query, params).decode(\"utf-8\"))\n        result = self.cur.fetchall()\n        self.d_b.close_conn()\n        if result:\n            return dict(result[0])\n        else:\n            # Return a default user with 'role' set to 3 if the user is not found\n            return {\"osm_id\": osm_id, \"role\": 3}\n\n    def update_user(self, osm_id, update_data):\n        \"\"\"\n        Updates user information based on the given osm_id.\n\n        Args:\n        - osm_id (int): The OSM ID of the user to update.\n        - update_data (UserUpdate): The data to update for the user.\n\n        Returns:\n        - Dict[str, Any]: A dictionary containing the updated user information.\n\n        Raises:\n        - HTTPException: If the user with the given osm_id is not found.\n        \"\"\"\n        query = \"UPDATE users SET osm_id = %s, role = %s WHERE osm_id = %s RETURNING *;\"\n        params = (update_data.osm_id, update_data.role, osm_id)\n        self.cur.execute(self.cur.mogrify(query, params).decode(\"utf-8\"))\n        updated_user = self.cur.fetchall()\n        self.con.commit()\n        self.d_b.close_conn()\n        if updated_user:\n            return dict(updated_user[0])\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    def delete_user(self, osm_id):\n        \"\"\"\n        Deletes a user based on the given osm_id.\n\n        Args:\n        - osm_id (int): The OSM ID of the user to delete.\n\n        Returns:\n        - Dict[str, Any]: A dictionary containing the deleted user information.\n\n        Raises:\n        - HTTPException: If the user with the given osm_id is not found.\n        \"\"\"\n        query = \"DELETE FROM users WHERE osm_id = %s RETURNING *;\"\n        params = (osm_id,)\n        self.cur.execute(self.cur.mogrify(query, params).decode(\"utf-8\"))\n        deleted_user = self.cur.fetchall()\n        self.con.commit()\n        self.d_b.close_conn()\n        if deleted_user:\n            return dict(deleted_user[0])\n        raise HTTPException(status_code=404, detail=\"User not found\")\n\n    def read_users(self, skip=0, limit=10):\n        \"\"\"\n        Retrieves a list of users with optional pagination.\n\n        Args:\n        - skip (int): The number of users to skip (for pagination).\n        - limit (int): The maximum number of users to retrieve (for pagination).\n\n        Returns:\n        - List[Dict[str, Any]]: A list of dictionaries containing user information.\n        \"\"\"\n        query = \"SELECT * FROM users OFFSET %s LIMIT %s;\"\n        params = (skip, limit)\n        self.cur.execute(self.cur.mogrify(query, params).decode(\"utf-8\"))\n        users_list = self.cur.fetchall()\n        self.d_b.close_conn()\n        return [dict(user) for user in users_list]\n\n\nclass RawData:\n    \"\"\"Class responsible for the Rawdata Extraction from available sources ,\n        Currently Works for Underpass source Current Snapshot\n    Returns:\n    Geojson Zip file\n    Supports:\n    -Any Key value pair of osm tags\n    -A Polygon\n    -Osm element type (Optional)\n    \"\"\"\n\n    def __init__(self, parameters=None, request_uid=\"raw-data-api\", dbdict=None):\n        if parameters:\n            self.params = parameters\n        # only use connection pooling if it is configured in config file\n        if use_connection_pooling:\n            # if database credentials directly from class is not passed grab from pool\n            pool_conn = LOCAL_CON_POOL.get_conn_from_pool()\n            self.con, self.cur = pool_conn, pool_conn.cursor(cursor_factory=DictCursor)\n        else:\n            # else use our default db class\n            if not dbdict:\n                dbdict = get_db_connection_params()\n            self.d_b = Database(dict(dbdict))\n            self.con, self.cur = self.d_b.connect()\n\n        self.base_export_working_dir = os.path.join(export_path, request_uid)\n\n    @staticmethod\n    def close_con(con):\n        \"\"\"Closes connection if exists\"\"\"\n        if con:\n            if use_connection_pooling:\n                # release connection from pool\n                database_instance.release_conn_from_pool(con)\n            else:\n                con.close()\n\n    @staticmethod\n    def ogr_export_shp(point_query, line_query, poly_query, working_dir, file_name):\n        \"\"\"Function written to support ogr type extractions as well , In this way we will be able to support all file formats supported by Ogr , Currently it is slow when dataset gets bigger as compared to our own conversion method but rich in feature and data types even though it is slow\"\"\"\n        db_items = get_db_connection_params()\n        if point_query:\n            query_path = os.path.join(working_dir, \"point.sql\")\n            # writing to .sql to pass in ogr2ogr because we don't want to pass too much argument on command with sql\n            with open(query_path, \"w\", encoding=\"UTF-8\") as file:\n                file.write(point_query)\n            # standard file path for the generation\n            point_file_path = os.path.join(working_dir, f\"{file_name}_point.shp\")\n            # command for ogr2ogr to generate file\n\n            cmd = \"\"\"ogr2ogr -overwrite -f \"ESRI Shapefile\" {export_path} PG:\"host={host} port={port} user={username} dbname={db} password={password}\" -sql @\"{pg_sql_select}\" -lco ENCODING=UTF-8 -progress\"\"\".format(\n                export_path=point_file_path,\n                host=db_items.get(\"host\"),\n                port=db_items.get(\"port\"),\n                username=db_items.get(\"user\"),\n                db=db_items.get(\"dbname\"),\n                password=db_items.get(\"password\"),\n                pg_sql_select=query_path,\n            )\n            logging.debug(\"Calling ogr2ogr-Point Shapefile\")\n            run_ogr2ogr_cmd(cmd)\n            # clear query file we don't need it anymore\n            os.remove(query_path)\n\n        if line_query:\n            query_path = os.path.join(working_dir, \"line.sql\")\n            # writing to .sql to pass in ogr2ogr because we don't want to pass too much argument on command with sql\n            with open(query_path, \"w\", encoding=\"UTF-8\") as file:\n                file.write(line_query)\n            line_file_path = os.path.join(working_dir, f\"{file_name}_line.shp\")\n            cmd = \"\"\"ogr2ogr -overwrite -f \"ESRI Shapefile\" {export_path} PG:\"host={host} port={port} user={username} dbname={db} password={password}\" -sql @\"{pg_sql_select}\" -lco ENCODING=UTF-8 -progress\"\"\".format(\n                export_path=line_file_path,\n                host=db_items.get(\"host\"),\n                port=db_items.get(\"port\"),\n                username=db_items.get(\"user\"),\n                db=db_items.get(\"dbname\"),\n                password=db_items.get(\"password\"),\n                pg_sql_select=query_path,\n            )\n            logging.debug(\"Calling ogr2ogr-Line Shapefile\")\n            run_ogr2ogr_cmd(cmd)\n            # clear query file we don't need it anymore\n            os.remove(query_path)\n\n        if poly_query:\n            query_path = os.path.join(working_dir, \"poly.sql\")\n            poly_file_path = os.path.join(working_dir, f\"{file_name}_poly.shp\")\n            # writing to .sql to pass in ogr2ogr because we don't want to pass too much argument on command with sql\n            with open(query_path, \"w\", encoding=\"UTF-8\") as file:\n                file.write(poly_query)\n            cmd = \"\"\"ogr2ogr -overwrite -f \"ESRI Shapefile\" {export_path} PG:\"host={host} port={port} user={username} dbname={db} password={password}\" -sql @\"{pg_sql_select}\" -lco ENCODING=UTF-8 -progress\"\"\".format(\n                export_path=poly_file_path,\n                host=db_items.get(\"host\"),\n                port=db_items.get(\"port\"),\n                username=db_items.get(\"user\"),\n                db=db_items.get(\"dbname\"),\n                password=db_items.get(\"password\"),\n                pg_sql_select=query_path,\n            )\n            logging.debug(\"Calling ogr2ogr-Poly Shapefile\")\n            run_ogr2ogr_cmd(cmd)\n            # clear query file we don't need it anymore\n            os.remove(query_path)\n\n    @staticmethod\n    def ogr_export(query, outputtype, working_dir, dump_temp_path, params):\n        \"\"\"Generates ogr2ogr command based on outputtype and parameters\n\n        Args:\n            query (_type_): Postgresql query to extract\n            outputtype (_type_): _description_\n            working_dir (_type_): _description_\n            dump_temp_path (_type_): temp file path for metadata gen\n            params (_type_): _description_\n        \"\"\"\n        db_items = get_db_connection_params()\n        query_path = os.path.join(working_dir, \"export_query.sql\")\n        with open(query_path, \"w\", encoding=\"UTF-8\") as file:\n            file.write(query)\n\n        format_options = {\n            RawDataOutputType.FLATGEOBUF.value: {\n                \"format\": \"FLATGEOBUF\",\n                \"extra\": \"-lco SPATIAL_INDEX=YES VERIFY_BUFFERS=NO\",\n            },\n            RawDataOutputType.GEOPARQUET.value: {\n                \"format\": \"Parquet\",\n                \"extra\": \"\",\n            },\n            RawDataOutputType.PGDUMP.value: {\n                \"format\": \"PGDump\",\n                \"extra\": \"--config PG_USE_COPY YES -lco SRID=4326\",\n            },\n            RawDataOutputType.KML.value: {\n                \"format\": \"KML\",\n                \"extra\": \"\",\n            },\n            RawDataOutputType.CSV.value: {\n                \"format\": \"CSV\",\n                \"extra\": \"\",\n            },\n            RawDataOutputType.GEOPACKAGE.value: {\n                \"format\": \"GPKG\",\n                \"extra\": \"\",\n            },\n        }\n\n        if ENABLE_TILES:\n            format_options[RawDataOutputType.MBTILES.value] = {\n                \"format\": \"MBTILES\",\n                \"extra\": (\n                    \"-dsco MINZOOM={} -dsco MAXZOOM={} \".format(\n                        params.min_zoom, params.max_zoom\n                    )\n                    if params.min_zoom and params.max_zoom\n                    else \"-dsco MINZOOM=10 -dsco MAXZOOM=15\"\n                ),\n            }\n            format_options[RawDataOutputType.PMTILES.value] = {\n                \"format\": \"PMTiles\",\n                \"extra\": (\n                    \"-dsco MINZOOM={} -dsco MAXZOOM={} \".format(\n                        params.min_zoom, params.max_zoom\n                    )\n                    if params.min_zoom and params.max_zoom\n                    else \"-dsco MINZOOM=10 -dsco MAXZOOM=15\"\n                ),\n            }\n            format_options[RawDataOutputType.MVT.value] = {\n                \"format\": \"MVT\",\n                \"extra\": (\n                    \"-t_srs EPSG:3857 -dsco MINZOOM={} -dsco MAXZOOM={} -dsco COMPRESS=NO\".format(\n                        params.min_zoom, params.max_zoom\n                    )\n                    if params.min_zoom and params.max_zoom\n                    else \"-t_srs EPSG:3857 -dsco MINZOOM=10 -dsco MAXZOOM=15 -dsco COMPRESS=NO\"\n                ),\n            }\n\n        file_name_option = (\n            f\"-nln {params.file_name if params.file_name else 'raw_export'}\"\n        )\n\n        if outputtype == RawDataOutputType.FLATGEOBUF.value and params.fgb_wrap_geoms:\n            format_options[outputtype][\"extra\"] += \" -nlt GEOMETRYCOLLECTION\"\n\n        format_option = format_options.get(outputtype, {\"format\": \"\", \"extra\": \"\"})\n\n        if format_option[\"format\"] in [\n            \"Parquet\"\n        ]:  # those layers which doesn't support overwrite if layer is not present\n            begin = \"ogr2ogr -f\"\n        else:\n            begin = \"ogr2ogr -overwrite -f\"\n\n        cmd = f\"{begin} {format_option['format']} {dump_temp_path} PG:\\\"host={db_items.get('host')} port={db_items.get('port')} user={db_items.get('user')} dbname={db_items.get('dbname')} password={db_items.get('password')}\\\" -sql @{query_path} -lco ENCODING=UTF-8 -progress {format_option['extra']} {file_name_option}\"\n        run_ogr2ogr_cmd(cmd)\n\n        os.remove(query_path)\n\n    @staticmethod\n    def query2geojson(con, extraction_query, dump_temp_file_path):\n        \"\"\"Function written from scratch without being dependent on any library, Provides better performance for geojson binding\"\"\"\n        # creating geojson file\n        pre_geojson = \"\"\"{\"type\": \"FeatureCollection\",\"features\": [\"\"\"\n        post_geojson = \"\"\"]}\"\"\"\n        logging.debug(\"Query : %s\", extraction_query)\n        # writing to the file\n        # directly writing query result to the file one by one without holding them in object so that it will not eat up our memory\n        with open(dump_temp_file_path, \"a\", encoding=\"utf-8\") as f:\n            f.write(pre_geojson)\n            logging.debug(\"Server side Cursor Query Sent with 1000 Chunk Size\")\n            with con.cursor(name=\"fetch_raw\") as cursor:  # using server side cursor\n                cursor.itersize = (\n                    1000  # chunk size to get 1000 row at a time in client side\n                )\n                cursor.execute(extraction_query)\n                first = True\n                for row in cursor:\n                    if first:\n                        first = False\n                        f.write(row[0])\n                    else:\n                        f.write(\",\")\n                        f.write(row[0])\n                cursor.close()  # closing connection to avoid memory issues\n                # close the writing geojson with last part\n            f.write(post_geojson)\n        logging.debug(\"Server side Query Result  Post Processing Done\")\n\n    @staticmethod\n    def get_grid_id(geom, cur):\n        \"\"\"Gets the intersecting related grid id for the geometry that is passed\n\n        Args:\n            geom (_type_): _description_\n            cur (_type_): _description_\n\n        Returns:\n            _type_: grid id , geometry dump and the area of geometry\n        \"\"\"\n        geometry_dump = dumps(dict(geom))\n        # generating geometry area in sqkm\n        geom_area = area(json_loads(geom.json())) * 1e-6\n        country_export = False\n        g_id = None\n        countries = []\n        cur.execute(check_exisiting_country(geometry_dump))\n        backend_match = cur.fetchall()\n        if backend_match:\n            countries = backend_match[0]\n            country_export = True\n            logging.info(f\"Using Country Export Mode with id : {countries[0]}\")\n        # else:\n        #     if int(geom_area) > int(index_threshold):\n        #         # this will be applied only when polygon gets bigger we will be slicing index size to search\n        #         country_query = get_country_id_query(geometry_dump)\n        #         cur.execute(country_query)\n        #         result_country = cur.fetchall()\n        #         countries = [int(f[0]) for f in result_country]\n        #         logging.debug(f\"Intersected Countries : {countries}\")\n        #         cur.close()\n        return (\n            g_id,\n            geometry_dump,\n            geom_area,\n            (\n                countries if len(countries) > 0 and len(countries) <= 3 else None\n            ),  # don't go through countires if they are more than 3\n            country_export,\n        )\n\n    def extract_current_data(self, exportname):\n        \"\"\"Responsible for Extracting rawdata current snapshot, Initially it creates a geojson file , Generates query , run it with 1000 chunk size and writes it directly to the geojson file and closes the file after dump\n        Args:\n            exportname: takes filename as argument to create geojson file passed from routers\n\n        Returns:\n            geom_area: area of polygon supplied\n            working_dir: dir where results are saved\n        \"\"\"\n        # first check either geometry needs grid or not for querying\n        (\n            grid_id,\n            geometry_dump,\n            geom_area,\n            country,\n            country_export,\n        ) = RawData.get_grid_id(self.params.geometry, self.cur)\n        output_type = self.params.output_type\n        # Check whether the export path exists or not\n        working_dir = os.path.join(self.base_export_working_dir, exportname)\n        if not os.path.exists(working_dir):\n            # Create a exports directory because it does not exist\n            create_working_dir(working_dir)\n        # create file path with respect to of output type\n\n        dump_temp_file_path = os.path.join(\n            working_dir,\n            f\"{self.params.file_name if self.params.file_name else 'Export'}{f'.{output_type.lower()}'}\",\n        )\n\n        try:\n            # currently we have only geojson binding function written other than that we have depend on ogr\n            if ENABLE_TILES:\n                if output_type in [\n                    RawDataOutputType.PMTILES.value,\n                    RawDataOutputType.MBTILES.value,\n                    RawDataOutputType.MVT.value,\n                ]:\n                    dump_temp_file_path = os.path.join(\n                        working_dir,\n                        f\"{self.params.file_name if self.params.file_name else 'Export'}{'' if output_type == RawDataOutputType.MVT.value else f'.{output_type.lower()}'}\",\n                    )\n                    RawData.ogr_export(\n                        query=raw_currentdata_extraction_query(\n                            self.params,\n                            grid_id,\n                            country,\n                            ogr_export=True,\n                            country_export=country_export,\n                        ),\n                        outputtype=output_type,\n                        dump_temp_path=dump_temp_file_path,\n                        working_dir=working_dir,\n                        params=self.params,\n                    )\n\n            if output_type == RawDataOutputType.GEOJSON.value:\n                RawData.query2geojson(\n                    self.con,\n                    raw_currentdata_extraction_query(\n                        self.params,\n                        g_id=grid_id,\n                        c_id=country,\n                        country_export=country_export,\n                    ),\n                    dump_temp_file_path,\n                )  # uses own conversion class\n            if output_type == RawDataOutputType.SHAPEFILE.value:\n                (\n                    point_query,\n                    line_query,\n                    poly_query,\n                    point_schema,\n                    line_schema,\n                    poly_schema,\n                ) = extract_geometry_type_query(\n                    self.params,\n                    ogr_export=True,\n                    g_id=grid_id,\n                    c_id=country,\n                    country_export=country_export,\n                )\n                RawData.ogr_export_shp(\n                    point_query=point_query,\n                    line_query=line_query,\n                    poly_query=poly_query,\n                    working_dir=working_dir,\n                    file_name=(\n                        self.params.file_name if self.params.file_name else \"Export\"\n                    ),\n                )  # using ogr2ogr\n            if output_type in [\"fgb\", \"kml\", \"gpkg\", \"sql\", \"parquet\", \"csv\"]:\n                RawData.ogr_export(\n                    query=raw_currentdata_extraction_query(\n                        self.params,\n                        grid_id,\n                        country,\n                        ogr_export=True,\n                        country_export=country_export,\n                    ),\n                    outputtype=output_type,\n                    dump_temp_path=dump_temp_file_path,\n                    working_dir=working_dir,\n                    params=self.params,\n                )  # uses ogr export to export\n            return geom_area, geometry_dump, working_dir\n        except Exception as ex:\n            logging.error(ex)\n            raise ex\n        finally:\n            # closing connection before leaving class\n            RawData.close_con(self.con)\n\n    def check_status(self):\n        \"\"\"Gives status about DB update, Substracts with current time and last db update time\"\"\"\n        status_query = check_last_updated_rawdata()\n        self.cur.execute(status_query)\n        behind_time = self.cur.fetchall()\n        self.cur.close()\n        # closing connection before leaving class\n        RawData.close_con(self.con)\n        return str(behind_time[0][0])\n\n    def get_countries_list(self, q):\n        \"\"\"Gets Countries list from the database\n\n        Args:\n            q (_type_): list filter query string\n\n        Returns:\n            featurecollection: geojson of country\n        \"\"\"\n        query = get_countries_query(q)\n        self.cur.execute(query)\n        get_fetched = self.cur.fetchall()\n        features = []\n        for row in get_fetched:\n            features.append(orjson.loads(row[0]))\n        self.cur.close()\n        return FeatureCollection(features=features)\n\n    def get_country(self, q):\n        \"\"\"Gets specific country from the database\n\n        Args:\n            cid (_type_): country cid\n\n        Returns:\n            featurecollection: geojson of country\n        \"\"\"\n        query = get_country_cid(q)\n        self.cur.execute(query)\n        get_fetched = self.cur.fetchall()\n        self.cur.close()\n        if len(get_fetched) < 1:\n            return \"Not found\"\n        return orjson.loads(get_fetched[0][0])\n\n    def get_osm_feature(self, osm_id):\n        \"\"\"Returns geometry of osm_id in geojson\n\n        Args:\n            osm_id (_type_): osm_id of feature\n\n        Returns:\n            featurecollection: Geojson\n        \"\"\"\n        query = get_osm_feature_query(osm_id)\n        self.cur.execute(query)\n        get_fetched = self.cur.fetchall()\n        features = []\n        for row in get_fetched:\n            features.append(orjson.loads(row[0]))\n        self.cur.close()\n        return FeatureCollection(features=features)\n\n    def cleanup(self):\n        \"\"\"\n        Cleans up temporary resources.\n        \"\"\"\n\n        if os.path.exists(self.base_export_working_dir):\n            shutil.rmtree(self.base_export_working_dir)\n            return True\n        return False\n\n\nclass S3FileTransfer:\n    \"\"\"Responsible for the file transfer to s3 from API maachine\"\"\"\n\n    def __init__(self):\n        # responsible for the connection\n        try:\n            if AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY:\n                self.aws_session = boto3.Session(\n                    aws_access_key_id=AWS_ACCESS_KEY_ID,\n                    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n                )\n            else:  # if it is not passed on config then api will assume it is configured within machine using credentials file\n                self.aws_session = boto3.Session()\n            self.s_3 = self.aws_session.client(\"s3\")\n            logging.debug(\"Connection has been successful to s3\")\n        except Exception as ex:\n            logging.error(ex)\n            raise ex\n\n    def list_buckets(self):\n        \"\"\"used to list all the buckets available on s3\"\"\"\n        buckets = self.s_3.list_buckets()\n        return buckets\n\n    def get_bucket_location(self, bucket_name):\n        \"\"\"Provides the bucket location on aws, takes bucket_name as string -- name of repo on s3\"\"\"\n        try:\n            bucket_location = self.s_3.get_bucket_location(Bucket=bucket_name)[\n                \"LocationConstraint\"\n            ]\n        except Exception as ex:\n            logging.error(\"Can't access bucket location\")\n            raise ex\n        return bucket_location or \"us-east-1\"\n\n    def upload(self, file_path, file_name, file_suffix=None):\n        \"\"\"Used for transferring file to s3 after reading path from the user , It will wait for the upload to complete\n        Parameters :file_path --- your local file path to upload ,\n            file_prefix -- prefix for the filename which is stored\n        sample function call :\n            S3FileTransfer.transfer(file_path=\"exports\",file_prefix=\"upload_test\")\"\"\"\n        if file_suffix:\n            file_name = f\"{file_name}.{file_suffix}\"\n        logging.debug(\"Started Uploading %s from %s\", file_name, file_path)\n        # instantiate upload\n        start_time = time.time()\n\n        try:\n            if type(file_path) == str and file_path[-5:] == \".html\":\n                self.s_3.upload_file(\n                    str(file_path),\n                    BUCKET_NAME,\n                    str(file_name),\n                    ExtraArgs={\"ContentType\": \"text/html\"},\n                )\n            else:\n                self.s_3.upload_file(str(file_path), BUCKET_NAME, str(file_name))\n        except Exception as ex:\n            logging.error(ex)\n            raise ex\n        logging.debug(\"Uploaded %s in %s sec\", file_name, time.time() - start_time)\n        # generate the download url\n        bucket_location = self.get_bucket_location(bucket_name=BUCKET_NAME)\n        object_url = f\"\"\"https://s3.dualstack.{bucket_location}.amazonaws.com/{BUCKET_NAME}/{file_name}\"\"\"\n        return object_url\n\n\nclass PolygonStats:\n    \"\"\"Generates stats for polygon\"\"\"\n\n    def __init__(self, geojson=None, iso3=None):\n        \"\"\"\n        Initialize PolygonStats with the provided GeoJSON.\n\n        Args:\n            geojson (dict): GeoJSON representation of the polygon.\n        \"\"\"\n        self.API_URL = POLYGON_STATISTICS_API_URL\n        if geojson is None and iso3 is None:\n            raise HTTPException(\n                status_code=404, detail=\"Either geojson or iso3 should be passed\"\n            )\n\n        if iso3:\n            dbdict = get_db_connection_params()\n            d_b = Database(dbdict)\n            con, cur = d_b.connect()\n            cur.execute(get_country_geom_from_iso(iso3))\n            result = cur.fetchone()\n            if result is None:\n                raise HTTPException(status_code=404, detail=\"Invalid iso3 code\")\n            self.INPUT_GEOM = result[0]\n        else:\n            self.INPUT_GEOM = dumps(geojson)\n\n    @staticmethod\n    def get_building_pattern_statement(\n        osm_building_count,\n        ai_building_count,\n        avg_timestamp,\n        last_edit_timestamp,\n        osm_building_count_6_months,\n    ):\n        \"\"\"\n        Translates building stats to a human-readable statement.\n\n        Args:\n            osm_building_count (int): Count of buildings from OpenStreetMap.\n            ai_building_count (int): Count of buildings from AI estimates.\n            avg_timestamp (timestamp): Average timestamp of data.\n            last_edit_timestamp(timestamp): Last edit timestamp of an area\n            osm_building_count_6_months (int): Count of buildings updated in the last 6 months.\n\n        Returns:\n            str: Human-readable building statement.\n        \"\"\"\n        building_statement = f\"OpenStreetMap contains roughly {humanize.intword(osm_building_count)} buildings in this region. \"\n        if ai_building_count > 0:\n            building_statement += f\"Based on AI-mapped estimates, this is approximately {round((osm_building_count/ai_building_count)*100)}% of the total buildings.\"\n        building_statement += f\"The average age of data for this region is {humanize.naturaltime(avg_timestamp).replace('ago', '')}( Last edited {humanize.naturaltime(last_edit_timestamp)} ) \"\n        if osm_building_count > 0:\n            building_statement += f\"and {round((osm_building_count_6_months/osm_building_count)*100)}% buildings were added or updated in the last 6 months.\"\n        return building_statement\n\n    @staticmethod\n    def get_road_pattern_statement(\n        osm_highway_length,\n        ai_highway_length,\n        avg_timestamp,\n        last_edit_timestamp,\n        osm_highway_length_6_months,\n    ):\n        \"\"\"\n        Translates road stats to a human-readable statement.\n\n        Args:\n            osm_highway_length (float): Length of roads from OpenStreetMap.\n            ai_highway_length (float): Length of roads from AI estimates.\n            avg_timestamp (str): Average timestamp of data.\n            osm_highway_length_6_months (float): Length of roads updated in the last 6 months.\n\n        Returns:\n            str: Human-readable road statement.\n        \"\"\"\n        road_statement = f\"OpenStreetMap contains roughly {humanize.intword(osm_highway_length)} km of roads in this region. \"\n        if ai_highway_length > 1:\n            road_statement += f\"Based on AI-mapped estimates, this is approximately {round(osm_highway_length/ai_highway_length*100)} % of the total road length in the dataset region. \"\n        road_statement += f\"The average age of data for the region is {humanize.naturaltime(avg_timestamp).replace('ago', '')} ( Last edited {humanize.naturaltime(last_edit_timestamp)} ) \"\n        if osm_highway_length > 1:\n            road_statement += f\"and {round((osm_highway_length_6_months/osm_highway_length)*100)}% of roads were added or updated in the last 6 months.\"\n        return road_statement\n\n    def get_osm_analytics_meta_stats(self):\n        \"\"\"\n        Gets the raw stats translated into a JSON body using the OSM Analytics API.\n\n        Returns:\n            dict: Raw statistics translated into JSON.\n        \"\"\"\n        MAX_RETRIES = 2  # Maximum number of retries\n        INITIAL_DELAY = 1  # Initial delay in seconds\n        MAX_DELAY = 8\n        API_TIMEOUT = 10\n\n        retries = 0\n        delay = INITIAL_DELAY\n\n        while retries < MAX_RETRIES:\n            try:\n                query = generate_polygon_stats_graphql_query(self.INPUT_GEOM)\n                payload = {\"query\": query}\n                response = requests.post(\n                    self.API_URL, json=payload, timeout=API_TIMEOUT\n                )\n                response.raise_for_status()\n                return response.json()\n            except Exception as e:\n                print(f\"Request failed: {e}\")\n                retries += 1\n                delay = min(delay * 0.5, MAX_DELAY)  # Exponential backoff\n                jitter = random.uniform(0, 1)  #  jitter to avoid simultaneous retries\n                sleep_time = delay * (1 + jitter)\n                print(f\"Retrying in {sleep_time} seconds...\")\n                time.sleep(sleep_time)\n\n        # If all retries failed, return None\n        print(\"Maximum retries exceeded. Unable to fetch data.\")\n        return None\n\n    def get_summary_stats(self):\n        \"\"\"\n        Generates summary statistics for buildings and roads.\n\n        Returns:\n            dict: Summary statistics including building and road statements.\n        \"\"\"\n        combined_data = {}\n        analytics_data = self.get_osm_analytics_meta_stats()\n        if (\n            analytics_data is None\n            or \"data\" not in analytics_data\n            or \"polygonStatistic\" not in analytics_data[\"data\"]\n            or \"analytics\" not in analytics_data[\"data\"][\"polygonStatistic\"]\n            or \"functions\"\n            not in analytics_data[\"data\"][\"polygonStatistic\"][\"analytics\"]\n            or analytics_data[\"data\"][\"polygonStatistic\"][\"analytics\"][\"functions\"]\n            is None\n        ):\n            logging.error(analytics_data)\n            return None\n        for function in analytics_data[\"data\"][\"polygonStatistic\"][\"analytics\"][\n            \"functions\"\n        ]:\n            function_id = function.get(\"id\")\n            result = function.get(\"result\")\n            combined_data[function_id] = result if result is not None else 0\n        combined_data[\"osm_buildings_freshness_percentage\"] = (\n            100 - combined_data[\"antiqueOsmBuildingsPercentage\"]\n        )\n        combined_data[\"osm_building_completeness_percentage\"] = (\n            100\n            if combined_data[\"osmBuildingsCount\"] == 0\n            and combined_data[\"aiBuildingsCountEstimation\"] == 0\n            else (\n                combined_data[\"osmBuildingsCount\"]\n                / combined_data[\"aiBuildingsCountEstimation\"]\n            )\n            * 100\n        )\n\n        combined_data[\"osm_roads_freshness_percentage\"] = (\n            100 - combined_data[\"antiqueOsmRoadsPercentage\"]\n        )\n\n        combined_data[\"osm_roads_completeness_percentage\"] = (\n            100\n            if combined_data[\"highway_length\"] == 0\n            and combined_data[\"aiRoadCountEstimation\"] == 0\n            else (\n                combined_data[\"highway_length\"] / combined_data[\"aiRoadCountEstimation\"]\n            )\n            * 100\n        )\n\n        combined_data[\"averageEditTime\"] = datetime.fromtimestamp(\n            combined_data[\"averageEditTime\"]\n        )\n        combined_data[\"lastEditTime\"] = datetime.fromtimestamp(\n            combined_data[\"lastEditTime\"]\n        )\n\n        building_summary = self.get_building_pattern_statement(\n            combined_data[\"osmBuildingsCount\"],\n            combined_data[\"aiBuildingsCountEstimation\"],\n            combined_data[\"averageEditTime\"],\n            combined_data[\"lastEditTime\"],\n            combined_data[\"building_count_6_months\"],\n        )\n\n        road_summary = self.get_road_pattern_statement(\n            combined_data[\"highway_length\"],\n            combined_data[\"aiRoadCountEstimation\"],\n            combined_data[\"averageEditTime\"],\n            combined_data[\"lastEditTime\"],\n            combined_data[\"highway_length_6_months\"],\n        )\n\n        return_stats = {\n            \"summary\": {\"buildings\": building_summary, \"roads\": road_summary},\n            \"raw\": {\n                \"population\": combined_data[\"population\"],\n                \"populatedAreaKm2\": combined_data[\"populatedAreaKm2\"],\n                \"averageEditTime\": combined_data[\"averageEditTime\"].strftime(\n                    \"%Y-%m-%d %H:%M:%S\"\n                ),\n                \"lastEditTime\": combined_data[\"lastEditTime\"].strftime(\n                    \"%Y-%m-%d %H:%M:%S\"\n                ),\n                \"osmUsersCount\": combined_data[\"osmUsersCount\"],\n                \"osmBuildingCompletenessPercentage\": combined_data[\n                    \"osm_building_completeness_percentage\"\n                ],\n                \"osmRoadsCompletenessPercentage\": combined_data[\n                    \"osm_roads_completeness_percentage\"\n                ],\n                \"osmBuildingsCount\": combined_data[\"osmBuildingsCount\"],\n                \"osmHighwayLengthKm\": combined_data[\"highway_length\"],\n                \"aiBuildingsCountEstimation\": combined_data[\n                    \"aiBuildingsCountEstimation\"\n                ],\n                \"aiRoadCountEstimationKm\": combined_data[\"aiRoadCountEstimation\"],\n                \"buildingCount6Months\": combined_data[\"building_count_6_months\"],\n                \"highwayLength6MonthsKm\": combined_data[\"highway_length_6_months\"],\n            },\n            \"meta\": {\n                \"indicators\": \"https://github.com/hotosm/raw-data-api/tree/develop/docs/src/stats/indicators.md\",\n                \"metrics\": \"https://github.com/hotosm/raw-data-api/tree/develop/docs/src/stats/metrics.md\",\n            },\n        }\n\n        return return_stats\n\n\nclass DuckDB:\n    \"\"\"\n    Constructor for the DuckDB class.\n\n    Parameters:\n    - db_path (str): The path to the DuckDB database file.\n    \"\"\"\n\n    def __init__(self, db_path, temp_dir=None):\n        dbdict = get_db_connection_params()\n        self.db_con_str = convert_dict_to_conn_str(db_dict=dbdict)\n        self.db_path = db_path\n        if os.path.exists(self.db_path):\n            os.remove(self.db_path)\n        con = duckdb.connect(self.db_path)\n        con.sql(f\"\"\"ATTACH '{self.db_con_str}' AS postgres_db (TYPE POSTGRES)\"\"\")\n        con.install_extension(\"spatial\")\n        con.load_extension(\"spatial\")\n        duck_db_temp = temp_dir\n        if temp_dir is None:\n            duck_db_temp = os.path.join(export_path, \"duckdb_temp\")\n            os.makedirs(duck_db_temp, exist_ok=True)\n        con.sql(f\"\"\"SET temp_directory = '{os.path.join(duck_db_temp,'temp.tmp')}'\"\"\")\n\n        if DUCK_DB_MEMORY_LIMIT:\n            con.sql(f\"\"\"SET memory_limit = '{DUCK_DB_MEMORY_LIMIT}'\"\"\")\n        if DUCK_DB_THREAD_LIMIT:\n            con.sql(f\"\"\"SET threads to {DUCK_DB_THREAD_LIMIT}\"\"\")\n\n        con.sql(\"\"\"SET enable_progress_bar = true\"\"\")\n\n    def run_query(self, query, attach_pgsql=False, load_spatial=False):\n        \"\"\"\n        Executes a query on the DuckDB database.\n\n        Parameters:\n        - query (str): The SQL query to execute.\n        - attach_pgsql (bool): Flag to indicate whether to attach a PostgreSQL database.\n        - load_spatial (bool): Flag to indicate whether to load the spatial extension.\n        \"\"\"\n        with duckdb.connect(self.db_path) as con:\n            if attach_pgsql:\n                con.execute(\n                    f\"\"\"ATTACH '{self.db_con_str}' AS postgres_db (TYPE POSTGRES)\"\"\"\n                )\n                load_spatial = True\n            if load_spatial:\n                con.load_extension(\"spatial\")\n            # con.load_extension(\"json\")\n            con.execute(query)\n\n\nclass CustomExport:\n    \"\"\"\n    Constructor for the custom export class.\n\n    Parameters:\n    - params (DynamicCategoriesModel): An instance of DynamicCategoriesModel containing configuration settings.\n    \"\"\"\n\n    def __init__(self, params, uid=None):\n        self.params = params\n        self.iso3 = self.params.iso3\n        self.HDX_SUPPORTED_FORMATS = [\"geojson\", \"gpkg\", \"kml\", \"shp\"]\n        if self.iso3:\n            self.iso3 = self.iso3.lower()\n        self.cid = None\n        if self.iso3:\n            dbdict = get_db_connection_params()\n            d_b = Database(dbdict)\n            con, cur = d_b.connect()\n            query = get_country_from_iso(self.iso3)\n            cur.execute(query)\n            result = cur.fetchall()\n            if not result:\n                raise HTTPException(status_code=404, detail=\"iso3 code not found in db\")\n            result = result[0]\n            (\n                self.cid,\n                dataset_title,\n                dataset_prefix,\n                dataset_locations,\n            ) = result\n\n            if not self.params.dataset.dataset_title:\n                self.params.dataset.dataset_title = dataset_title\n            if not self.params.dataset.dataset_prefix:\n                self.params.dataset.dataset_prefix = dataset_prefix\n            if not self.params.dataset.dataset_locations:\n                self.params.dataset.dataset_locations = json.loads(dataset_locations)\n        self.uuid = uid\n        if self.uuid is None:\n            self.uuid = str(uuid.uuid4().hex)\n\n        self.parallel_process_state = False\n        self.default_export_base_name = (\n            self.iso3.upper() if self.iso3 else self.params.dataset.dataset_prefix\n        )\n\n        self.default_export_path = os.path.join(\n            export_path,\n            self.uuid,\n            self.params.dataset.dataset_folder,\n            self.default_export_base_name,\n        )\n        if os.path.exists(self.default_export_path):\n            shutil.rmtree(self.default_export_path, ignore_errors=True)\n\n        os.makedirs(self.default_export_path)\n\n        if USE_DUCK_DB_FOR_CUSTOM_EXPORTS is True:\n            self.duck_db_db_path = os.path.join(\n                self.default_export_path,\n                f\"{self.default_export_base_name}.db\",\n            )\n            self.duck_db_instance = DuckDB(self.duck_db_db_path)\n\n    def types_to_tables(self, type_list: list):\n        \"\"\"\n        Maps feature types to corresponding database tables.\n\n        Parameters:\n        - type_list (List[str]): List of feature types.\n\n        Returns:\n        - List of database tables associated with the given feature types.\n        \"\"\"\n        mapping = {\n            \"points\": [\"nodes\"],\n            \"lines\": [\"ways_line\", \"relations\"],\n            \"polygons\": [\"ways_poly\", \"relations\"],\n        }\n\n        table_set = set()\n\n        for t in type_list:\n            if t in mapping:\n                table_set.update(mapping[t])\n\n        return list(table_set)\n\n    def format_where_clause_duckdb(self, where_clause):\n        \"\"\"\n        Formats the where_clause by replacing the first occurrence of the pattern.\n\n        Parameters:\n        - where_clause (str): SQL-like condition to filter features.\n\n        Returns:\n        - Formatted where_clause.\n        \"\"\"\n        pattern = r\"tags\\['([^']+)'\\]\"\n        for match in re.finditer(pattern, where_clause):\n            key = match.group(1)\n            string_in_pattern = f\"tags['{key}']\"\n            replacement = f\"{string_in_pattern}[1]\"\n            where_clause = where_clause.replace(string_in_pattern, replacement)\n\n        return where_clause\n\n    def upload_resources(self, resource_path):\n        \"\"\"\n        Uploads a resource file to Amazon S3.\n\n        Parameters:\n        - resource_path (str): Path to the resource file on the local filesystem.\n\n        Returns:\n        - Download URL for the uploaded resource.\n        \"\"\"\n        if USE_S3_TO_UPLOAD:\n            s3_upload_name = os.path.relpath(\n                resource_path, os.path.join(export_path, self.uuid)\n            )\n            file_transfer_obj = S3FileTransfer()\n            download_url = file_transfer_obj.upload(\n                resource_path,\n                str(s3_upload_name),\n            )\n            return download_url\n        return resource_path\n\n    def zip_to_s3(self, resources):\n        \"\"\"\n        Zips and uploads a list of resources to Amazon S3.\n\n        Parameters:\n        - resources (List[Dict[str, Any]]): List of resource dictionaries.\n\n        Returns:\n        - List of resource dictionaries with added download URLs.\n        \"\"\"\n        for resource in resources:\n            temp_zip_path = resource[\"url\"]\n            resource[\"url\"] = self.upload_resources(resource_path=temp_zip_path)\n            os.remove(temp_zip_path)\n\n            if resource.get(\"stats_html\"):\n                temp_stats_html_path = resource[\"stats_html\"]\n                resource[\"stats_html\"] = self.upload_resources(\n                    resource_path=temp_stats_html_path\n                )\n                os.remove(temp_stats_html_path)\n\n        return resources\n\n    def file_to_zip(self, working_dir, zip_path):\n        \"\"\"\n        Creates a ZIP file from files in a directory.\n\n        Parameters:\n        - working_dir (str): Path to the directory containing files to be zipped.\n        - zip_path (str): Path to the resulting ZIP file.\n\n        Returns:\n        - Path to the created ZIP file.\n        \"\"\"\n        zf = zipfile.ZipFile(\n            zip_path,\n            \"w\",\n            compression=zipfile.ZIP_DEFLATED,\n            allowZip64=True,\n        )\n\n        for file_path in pathlib.Path(working_dir).iterdir():\n            zf.write(file_path, arcname=file_path.name)\n        utc_now = datetime.now(timezone.utc)\n        utc_offset = utc_now.strftime(\"%z\")\n        # Adding metadata readme.txt\n        readme_content = f\"Exported Timestamp (UTC{utc_offset}): {utc_now.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n        readme_content += DEFAULT_README_TEXT\n        zf.writestr(\"Readme.txt\", readme_content)\n        if self.params.geometry:\n            zf.writestr(\"clipping_boundary.geojson\", self.params.geometry.json())\n        zf.close()\n        shutil.rmtree(working_dir)\n        return zip_path\n\n    def query_to_file(self, query, category_name, feature_type, export_formats):\n        \"\"\"\n        Executes a query and exports the result to file(s).\n\n        Parameters:\n        - query (str): SQL query to execute.\n        - category_name (str): Name of the category.\n        - feature_type (str): Feature type.\n        - export_formats (List[ExportTypeInfo]): List of export formats.\n\n        Returns:\n        - List of resource dictionaries containing export information.\n        \"\"\"\n        category_name = slugify(category_name.lower()).replace(\"-\", \"_\")\n        file_export_path = os.path.join(\n            self.default_export_path, category_name, feature_type\n        )\n        resources = []\n\n        def process_export_format(export_format):\n            export_format = EXPORT_TYPE_MAPPING.get(export_format)\n            export_format_path = os.path.join(file_export_path, export_format.suffix)\n            os.makedirs(export_format_path, exist_ok=True)\n            start = time.time()\n            logging.info(\n                \"Processing %s:%s\", category_name.lower(), export_format.suffix\n            )\n\n            export_filename = f\"\"\"{self.params.dataset.dataset_prefix}_{category_name}_{feature_type}_{export_format.suffix}\"\"\"\n            export_file_path = os.path.join(\n                export_format_path, f\"{export_filename}.{export_format.suffix}\"\n            )\n\n            if os.path.exists(export_file_path):\n                os.remove(export_file_path)\n\n            layer_creation_options_str = (\n                \" \".join(\n                    [f\"'{option}'\" for option in export_format.layer_creation_options]\n                )\n                if export_format.layer_creation_options\n                else \"\"\n            )\n            if USE_DUCK_DB_FOR_CUSTOM_EXPORTS is True:\n                format_option = export_format.format_option\n\n                driver_and_layer_options = \"\"\n                if format_option == \"GDAL\":\n                    driver_and_layer_options = f\", DRIVER '{export_format.driver_name}'\"\n                    if layer_creation_options_str:\n                        driver_and_layer_options += f\", SRS 'EPSG:4326', LAYER_CREATION_OPTIONS {layer_creation_options_str}\"\n\n                executable_query = f\"\"\"\n                    COPY ({query.strip()}) \n                    TO '{export_file_path}' \n                    WITH (FORMAT {format_option}{driver_and_layer_options})\n                \"\"\"\n\n                # executable_query = f\"\"\"COPY ({query.strip()}) TO '{export_file_path}' WITH (FORMAT {export_format.format_option}{f\", DRIVER '{export_format.driver_name}'{f\", SRS 'EPSG:4326', LAYER_CREATION_OPTIONS {layer_creation_options_str}\" if layer_creation_options_str else ''}\" if export_format.format_option == 'GDAL' else ''})\"\"\"\n                self.duck_db_instance.run_query(\n                    executable_query.strip(), load_spatial=True\n                )\n            else:\n                ogr2ogr_cmd = generate_ogr2ogr_cmd_from_psql(\n                    export_file_path=export_file_path,\n                    export_file_format_driver=export_format.driver_name,\n                    postgres_query=query.strip(),\n                    layer_creation_options=layer_creation_options_str,\n                    query_dump_path=export_format_path,\n                )\n                run_ogr2ogr_cmd(ogr2ogr_cmd)\n\n            # Post-processing GeoJSON files\n            # Adds: stats, HTML stats summary and transliterations\n            if export_format.driver_name == \"GeoJSON\" and (\n                self.params.include_stats or self.params.include_translit\n            ):\n                post_processor = PostProcessor(\n                    {\n                        \"include_stats\": self.params.include_stats,\n                        \"include_translit\": self.params.include_translit,\n                        \"include_stats_html\": self.params.include_stats_html,\n                    }\n                )\n                post_processor.stats(\n                    category_name=category_name,\n                    export_format_path=export_format_path,\n                    export_filename=export_filename,\n                    file_export_path=file_export_path,\n                )\n\n            zip_file_path = os.path.join(file_export_path, f\"{export_filename}.zip\")\n            zip_path = self.file_to_zip(export_format_path, zip_file_path)\n\n            resource = {}\n            resource[\"name\"] = f\"{export_filename}.zip\"\n            resource[\"url\"] = zip_path\n            resource[\"format\"] = export_format.suffix\n            resource[\"description\"] = export_format.driver_name\n            resource[\"size\"] = os.path.getsize(zip_path)\n            if (\n                self.params.include_stats_html\n                and export_format.driver_name == \"GeoJSON\"\n            ):\n                resource[\"stats_html\"] = f\"{file_export_path}/stats-summary.html\"\n\n            # resource[\"last_modified\"] = datetime.now().isoformat()\n            logging.info(\n                \"Done %s:%s in %s\",\n                category_name.lower(),\n                export_format.suffix,\n                humanize.naturaldelta(timedelta(seconds=(time.time() - start))),\n            )\n            return resource\n\n        if (\n            self.parallel_process_state is False\n            and len(export_formats) > 1\n            and PARALLEL_PROCESSING_CATEGORIES is True\n        ):\n            logging.info(\n                \"Using Parallel Processing for %s Export formats with total %s workers\",\n                category_name.lower(),\n                MAX_WORKERS,\n            )\n            with concurrent.futures.ThreadPoolExecutor(\n                max_workers=int(MAX_WORKERS)\n            ) as executor:\n                futures = [\n                    executor.submit(process_export_format, export_format)\n                    for export_format in export_formats\n                ]\n                resources = [\n                    future.result()\n                    for future in concurrent.futures.as_completed(futures)\n                ]\n                resources = [\n                    future.result()\n                    for future in tqdm(\n                        concurrent.futures.as_completed(futures),\n                        total=len(futures),\n                        desc=f\"{category_name.lower()}: Processing Export Formats\",\n                    )\n                ]\n        else:\n            for exf in export_formats:\n                resource = process_export_format(exf)\n                resources.append(resource)\n        return resources\n\n    def process_category_result(self, category_result):\n        \"\"\"\n        Processes the result of a category and prepares the response.\n\n        Parameters:\n        - category_result (CategoryResult): Instance of CategoryResult.\n\n        Returns:\n        - Dictionary containing processed category result.\n        \"\"\"\n        if self.params.hdx_upload and ENABLE_HDX_EXPORTS:\n            return self.resource_to_hdx(\n                uploaded_resources=category_result.uploaded_resources,\n                dataset_config=self.params.dataset,\n                category=category_result.category,\n            )\n\n        return self.resource_to_response(\n            category_result.uploaded_resources, category_result.category\n        )\n\n    def process_category(self, category):\n        \"\"\"\n        Processes a category by executing queries and handling exports.\n\n        Parameters:\n        - category (Dict[str, CategoryModel]): Dictionary representing a category.\n\n        Returns:\n        - List of resource dictionaries containing export information.\n        \"\"\"\n        category_name, category_data = list(category.items())[0]\n        category_start_time = time.time()\n        logging.info(\"Started Processing %s\", category_name)\n        all_uploaded_resources = []\n        for feature_type in category_data.types:\n            extract_query = extract_features_custom_exports(\n                self.iso3 if self.iso3 else self.params.dataset.dataset_prefix,\n                category_data.select,\n                feature_type,\n                (\n                    self.format_where_clause_duckdb(category_data.where)\n                    if USE_DUCK_DB_FOR_CUSTOM_EXPORTS is True\n                    else category_data.where\n                ),\n                geometry=self.params.geometry if self.params.geometry else None,\n                cid=self.cid,\n            )\n            resources = self.query_to_file(\n                extract_query,\n                category_name,\n                feature_type,\n                list(set(category_data.formats)),\n            )\n\n            uploaded_resources = self.zip_to_s3(resources)\n            all_uploaded_resources.extend(uploaded_resources)\n        logging.info(\n            \"Done Processing %s in %s \",\n            category_name,\n            humanize.naturaldelta(\n                timedelta(seconds=(time.time() - category_start_time))\n            ),\n        )\n        return all_uploaded_resources\n\n    def resource_to_response(self, uploaded_resources, category):\n        \"\"\"\n        Converts uploaded resources to a response format.\n\n        Parameters:\n        - uploaded_resources (List[Dict[str, Any]]): List of resource dictionaries.\n        - category (Dict[str, CategoryModel]): Dictionary representing a category.\n\n        Returns:\n        - Dictionary containing the response information.\n        \"\"\"\n        category_name, category_data = list(category.items())[0]\n        return {category_name: {\"resources\": uploaded_resources}}\n\n    def resource_to_hdx(self, uploaded_resources, dataset_config, category):\n        \"\"\"\n        Converts uploaded resources to an HDX dataset and uploads to HDX.\n\n        Parameters:\n        - uploaded_resources (List[Dict[str, Any]]): List of resource dictionaries.\n        - dataset_config (DatasetConfig): Instance of DatasetConfig.\n        - category (Dict[str, CategoryModel]): Dictionary representing a category.\n\n        Returns:\n        - Dictionary containing the HDX upload information.\n        \"\"\"\n        if any(\n            item[\"format\"] in self.HDX_SUPPORTED_FORMATS for item in uploaded_resources\n        ):\n            uploader = HDXUploader(\n                hdx=dataset_config,\n                category=category,\n                default_category_path=self.default_export_path,\n                uuid=self.uuid,\n                completeness_metadata={\n                    \"iso3\": self.iso3,\n                    \"geometry\": (\n                        {\n                            \"type\": \"Feature\",\n                            \"geometry\": json.loads(\n                                self.params.geometry.model_dump_json()\n                            ),\n                            \"properties\": {},\n                        }\n                        if self.params.geometry\n                        else None\n                    ),\n                },\n            )\n            logging.info(\"Initiating HDX Upload\")\n            uploader.init_dataset()\n            non_hdx_resources = []\n            for resource in uploaded_resources:\n                if resource[\"format\"] in self.HDX_SUPPORTED_FORMATS:\n                    uploader.add_resource(resource)\n                    resource[\"uploaded_to_hdx\"] = True\n                else:\n                    non_hdx_resources.append(resource)\n            category_name, cron_dataset_info = uploader.upload_dataset(\n                self.params.meta and USE_S3_TO_UPLOAD\n            )\n            cron_dataset_info[\"resources\"].extend(non_hdx_resources)\n            return {category_name: cron_dataset_info}\n\n    def clean_resources(self):\n        \"\"\"\n        Cleans up temporary resources.\n        \"\"\"\n        temp_dir = os.path.join(export_path, self.uuid)\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n            return True\n        return False\n\n    def process_custom_categories(self):\n        \"\"\"\n        Processes Custom tags and executes category processing in parallel.\n\n        Returns:\n        - Dictionary containing the processed dataset information.\n        \"\"\"\n        started_at = datetime.now().isoformat()\n        processing_time_start = time.time()\n        # clean cateories remove {}\n        self.params.categories = [\n            category for category in self.params.categories if category\n        ]\n        if USE_DUCK_DB_FOR_CUSTOM_EXPORTS is True:\n            table_type = [\n                cat_type\n                for category in self.params.categories\n                if category\n                for cat_type in list(category.values())[0].types\n            ]\n            where_0_category = None\n\n            if (\n                len(self.params.categories) == 1\n                and PROCESS_SINGLE_CATEGORY_IN_POSTGRES is True\n            ):\n                where_0_category = list(self.params.categories[0].values())[0].where\n\n            table_names = self.types_to_tables(list(set(table_type)))\n            base_table_name = (\n                self.iso3 if self.iso3 else self.params.dataset.dataset_prefix\n            )\n            for table in table_names:\n                create_table = postgres2duckdb_query(\n                    base_table_name=base_table_name,\n                    table=table,\n                    cid=self.cid,\n                    geometry=self.params.geometry,\n                    single_category_where=where_0_category,\n                )\n                logging.debug(create_table)\n                start = time.time()\n                logging.info(\"Transfer-> Postgres Data to DuckDB Started : %s\", table)\n                self.duck_db_instance.run_query(create_table.strip(), attach_pgsql=True)\n                logging.info(\n                    \"Transfer-> Postgres Data to DuckDB : %s Done in %s\",\n                    table,\n                    humanize.naturaldelta(timedelta(seconds=(time.time() - start))),\n                )\n\n        CategoryResult = namedtuple(\n            \"CategoryResult\", [\"category\", \"uploaded_resources\"]\n        )\n\n        tag_process_results = []\n        dataset_results = []\n        if len(self.params.categories) > 1 and PARALLEL_PROCESSING_CATEGORIES is True:\n            self.parallel_process_state = True\n            logging.info(\"Starting to Use Parallel Processes\")\n            with concurrent.futures.ThreadPoolExecutor(\n                max_workers=os.cpu_count()\n            ) as executor:\n                futures = {\n                    executor.submit(self.process_category, category): category\n                    for category in self.params.categories\n                }\n                for future in tqdm(\n                    concurrent.futures.as_completed(futures),\n                    total=len(futures),\n                    desc=f\"{self.default_export_base_name} : Processing Categories\",\n                ):\n                    category = futures[future]\n                    uploaded_resources = future.result()\n                    category_result = CategoryResult(\n                        category=category, uploaded_resources=uploaded_resources\n                    )\n                    tag_process_results.append(category_result)\n        else:\n            resources = self.process_category(self.params.categories[0])\n            category_result = CategoryResult(\n                category=self.params.categories[0], uploaded_resources=resources\n            )\n            tag_process_results.append(category_result)\n        logging.info(\"Export generation is done, Moving forward to process result\")\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            futures = {\n                executor.submit(self.process_category_result, result): result\n                for result in tag_process_results\n            }\n\n            for future in concurrent.futures.as_completed(futures):\n                result = futures[future]\n                result_data = future.result()\n                dataset_results.append(result_data)\n\n        result = {\"datasets\": dataset_results}\n        if self.params.meta:\n            if USE_DUCK_DB_FOR_CUSTOM_EXPORTS is True:\n                logging.info(\"Dumping Duck DB to Parquet\")\n                db_dump_path = os.path.join(\n                    self.default_export_path,\n                    \"DB_DUMP\",\n                )\n                os.makedirs(db_dump_path, exist_ok=True)\n                export_db = f\"\"\"EXPORT DATABASE '{db_dump_path}' (FORMAT PARQUET, COMPRESSION ZSTD, ROW_GROUP_SIZE 100000);\"\"\"\n                self.duck_db_instance.run_query(export_db, load_spatial=True)\n                db_zip_download_url = self.upload_resources(\n                    self.file_to_zip(\n                        working_dir=db_dump_path,\n                        zip_path=os.path.join(self.default_export_path, \"dbdump.zip\"),\n                    )\n                )\n                result[\"db_dump\"] = db_zip_download_url\n        processing_time_close = time.time()\n        result[\"elapsed_time\"] = humanize.naturaldelta(\n            timedelta(seconds=(processing_time_close - processing_time_start))\n        )\n        result[\"started_at\"] = started_at\n\n        meta_last_run_dump_path = os.path.join(self.default_export_path, \"meta.json\")\n        with open(meta_last_run_dump_path, \"w\", encoding=\"UTF-8\") as json_file:\n            json.dump(result, json_file, indent=4)\n        self.upload_resources(resource_path=meta_last_run_dump_path)\n        self.clean_resources()\n        return result\n\n\nclass HDXUploader:\n    \"\"\"\n    Constructor for the HDXUploader class.\n\n    Parameters:\n    - category (Dict[str, CategoryModel]): Dictionary representing a category.\n    - hdx (HDX): Instance of the HDX class.\n    - uuid (str): Universally unique identifier.\n    - default_category_path (str): Default path for the category.\n    - completeness_metadata (Optional[Dict[str, Any]]): Metadata for completeness.\n    \"\"\"\n\n    def __init__(\n        self, category, hdx, uuid, default_category_path, completeness_metadata=None\n    ):\n        self.hdx = hdx\n        self.category_name, self.category_data = list(category.items())[0]\n        self.category_path = os.path.join(\n            default_category_path, slugify(self.category_name.lower()).replace(\"-\", \"_\")\n        )\n        self.dataset = None\n        self.uuid = uuid\n        self.completeness_metadata = completeness_metadata\n        self.data_completeness_stats = None\n        self.resources = []\n\n    def slugify(self, name):\n        \"\"\"\n        Converts a string to a valid slug format.\n\n        Parameters:\n        - name (str): Input string.\n\n        Returns:\n        - Slugified string.\n        \"\"\"\n        return slugify(name).replace(\"-\", \"_\")\n\n    def add_notes(self):\n        \"\"\"\n        Adds notes based on category data.\n\n        Returns:\n        - Notes string.\n        \"\"\"\n        columns = []\n        for key in self.category_data.select:\n            columns.append(\n                \"- [{0}](http://wiki.openstreetmap.org/wiki/Key:{0})\".format(key)\n            )\n        columns = \"\\n\".join(columns)\n        filter_str = HDX_FILTER_CRITERIA.format(criteria=self.category_data.where)\n        if self.category_name.lower() in [\"roads\", \"buildings\"]:\n            if self.data_completeness_stats is None:\n                if self.completeness_metadata:\n                    self.data_completeness_stats = PolygonStats(\n                        iso3=self.completeness_metadata[\"iso3\"],\n                        geojson=(\n                            self.completeness_metadata[\"geometry\"]\n                            if self.completeness_metadata[\"geometry\"]\n                            else None\n                        ),\n                    ).get_summary_stats()\n            if self.data_completeness_stats:\n                self.category_data.hdx.notes += f'{self.data_completeness_stats[\"summary\"][self.category_name.lower()]}\\n'\n                self.category_data.hdx.notes += \"Read about what this summary means : [indicators](https://github.com/hotosm/raw-data-api/tree/develop/docs/src/stats/indicators.md) , [metrics](https://github.com/hotosm/raw-data-api/tree/develop/docs/src/stats/metrics.md)\\n\"\n\n        return self.category_data.hdx.notes + HDX_MARKDOWN.format(\n            columns=columns, filter_str=filter_str\n        )\n\n    def add_resource(self, resource_meta):\n        \"\"\"\n        Adds a resource to the list of resources.\n\n        Parameters:\n        - resource_meta (Dict[str, Any]): Metadata for the resource.\n        \"\"\"\n        if self.dataset:\n            self.resources.append(resource_meta)\n            resource_obj = Resource(resource_meta)\n            resource_obj.mark_data_updated()\n            self.dataset.add_update_resource(resource_obj)\n\n            # Add customviz if available\n            if resource_meta.get(\"stats_html\"):\n                dataset_customviz = self.dataset.get(\"customviz\")\n                if not dataset_customviz:\n                    dataset_customviz = [\n                        {\n                            \"name\": resource_meta[\"name\"],\n                            \"url\": resource_meta[\"stats_html\"],\n                        }\n                    ]\n                else:\n                    dataset_customviz.append(\n                        {\n                            \"name\": resource_meta[\"name\"],\n                            \"url\": resource_meta[\"stats_html\"],\n                        }\n                    )\n                self.dataset.update({\"customviz\": dataset_customviz})\n\n    def upload_dataset(self, dump_config_to_s3=False):\n        \"\"\"\n        Uploads the dataset to HDX.\n\n        Parameters:\n        - dump_config_to_s3 (bool): Flag to indicate whether to dump configuration to S3.\n\n        Returns:\n        - Tuple containing category name and dataset information.\n        \"\"\"\n        if self.dataset:\n            dataset_info = {}\n            dt_config_path = os.path.join(\n                self.category_path, f\"{self.dataset['name']}_config.json\"\n            )\n            self.dataset.save_to_json(dt_config_path)\n            if dump_config_to_s3:\n                s3_upload_name = os.path.relpath(\n                    dt_config_path, os.path.join(export_path, self.uuid)\n                )\n                file_transfer_obj = S3FileTransfer()\n                dataset_info[\"config\"] = file_transfer_obj.upload(\n                    dt_config_path,\n                    str(s3_upload_name),\n                )\n\n            self.dataset.set_time_period(datetime.now())\n            try:\n                self.dataset.create_in_hdx(\n                    allow_no_resources=True,\n                    hxl_update=False,\n                )\n                dataset_info[\"hdx_upload\"] = \"SUCCESS\"\n            except Exception as ex:\n                logging.error(ex)\n                if LOG_LEVEL == \"DEBUG\":\n                    raise ex\n                dataset_info[\"hdx_upload\"] = \"FAILED\"\n\n            dataset_info[\"name\"] = self.dataset[\"name\"]\n            dataset_info[\"hdx_url\"] = f\"{HDX_URL_PREFIX}/dataset/{self.dataset['name']}\"\n            dataset_info[\"resources\"] = self.resources\n            return self.category_name, dataset_info\n\n    def init_dataset(self):\n        \"\"\"\n        Initializes the HDX dataset.\n        \"\"\"\n        dataset_prefix = self.hdx.dataset_prefix\n        dataset_title = self.hdx.dataset_title\n        dataset_locations = self.hdx.dataset_locations\n        self.dataset = Dataset(\n            {\n                \"name\": \"{0}_{1}\".format(\n                    dataset_prefix, self.slugify(self.category_name)\n                ),\n                \"title\": \"{0} {1} (OpenStreetMap Export)\".format(\n                    dataset_title, self.category_name\n                ),\n                \"owner_org\": HDX_OWNER_ORG,\n                \"maintainer\": HDX_MAINTAINER,\n                \"dataset_source\": \"OpenStreetMap contributors\",\n                \"methodology\": \"Other\",\n                \"methodology_other\": \"Volunteered geographic information\",\n                \"license_id\": \"hdx-odc-odbl\",\n                \"updated_by_script\": f'Hotosm OSM Exports ({datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")})',\n                \"caveats\": self.category_data.hdx.caveats,\n                \"private\": self.hdx.private,\n                \"notes\": self.add_notes(),\n                \"subnational\": 1 if self.hdx.subnational else 0,\n            }\n        )\n        self.dataset.set_expected_update_frequency(self.hdx.update_frequency)\n        for location in dataset_locations:\n            self.dataset.add_other_location(location)\n        for tag in self.category_data.hdx.tags:\n            self.dataset.add_tag(tag)\n\n\nclass Cron:\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes an instance of the Cron class, connecting to the database.\n        \"\"\"\n        dbdict = get_db_connection_params()\n        self.d_b = Database(dbdict)\n        self.con, self.cur = self.d_b.connect()\n\n    def create_cron(self, cron_data):\n        \"\"\"\n        Create a new Cron entry in the database.\n\n        Args:\n            cron_data (dict): Data for creating the Cron entry.\n\n        Returns:\n            dict: Result of the cron creation process.\n        \"\"\"\n        insert_query = sql.SQL(\n            \"\"\"\n            INSERT INTO public.cron (iso3, hdx_upload, dataset, queue, meta, categories, geometry)\n            VALUES (%s, %s, %s, %s, %s, %s, %s)\n            RETURNING *\n        \"\"\"\n        )\n        self.cur.execute(\n            insert_query,\n            (\n                cron_data.get(\"iso3\", None),\n                cron_data.get(\"hdx_upload\", True),\n                json.dumps(cron_data.get(\"dataset\")),\n                cron_data.get(\"queue\", \"raw_ondemand\"),\n                cron_data.get(\"meta\", False),\n                json.dumps(cron_data.get(\"categories\", {})),\n                json.dumps(cron_data.get(\"geometry\")),\n            ),\n        )\n        self.con.commit()\n        self.d_b.close_conn()\n        result = self.cur.fetchone()\n        if result:\n            return {\"create\": True}\n        raise HTTPException(status_code=500, detail=\"Insert failed\")\n\n    def get_cron_list_with_filters(\n        self, skip: int = 0, limit: int = 10, filters: dict = {}\n    ):\n        \"\"\"\n        Retrieve a list of Cron entries based on provided filters.\n\n        Args:\n            skip (int): Number of entries to skip.\n            limit (int): Maximum number of entries to retrieve.\n            filters (dict): Filtering criteria.\n\n        Returns:\n            List[dict]: List of Cron entries.\n        \"\"\"\n        filter_conditions = []\n        filter_values = []\n\n        for key, value in filters.items():\n            filter_conditions.append(key)\n            filter_values.append(value)\n\n        where_clause = \" AND \".join(filter_conditions)\n\n        select_query = sql.SQL(\n            f\"\"\"\n            SELECT ST_AsGeoJSON(c.*) FROM public.cron c\n            {\"WHERE \" + where_clause if where_clause else \"\"}\n            OFFSET %s LIMIT %s\n        \"\"\"\n        )\n\n        self.cur.execute(select_query, tuple(filter_values) + (skip, limit))\n\n        result = self.cur.fetchall()\n        self.d_b.close_conn()\n        return [orjson.loads(item[0]) for item in result]\n\n    def search_cron_by_dataset_title(\n        self, dataset_title: str, skip: int = 0, limit: int = 10\n    ):\n        \"\"\"\n        Search for Cron entries by dataset title.\n\n        Args:\n            dataset_title (str): The title of the dataset to search for.\n            skip (int): Number of entries to skip.\n            limit (int): Maximum number of entries to retrieve.\n\n        Returns:\n            List[dict]: List of Cron entries matching the dataset title.\n        \"\"\"\n        search_query = sql.SQL(\n            \"\"\"\n            SELECT ST_AsGeoJSON(c.*) FROM public.cron c\n            WHERE c.dataset->>'dataset_title' ILIKE %s\n            OFFSET %s LIMIT %s\n            \"\"\"\n        )\n        self.cur.execute(search_query, (\"%\" + dataset_title + \"%\", skip, limit))\n        result = self.cur.fetchall()\n        self.d_b.close_conn()\n        return [orjson.loads(item[0]) for item in result]\n\n    def get_cron_by_id(self, cron_id: int):\n        \"\"\"\n        Retrieve a specific Cron entry by its ID.\n\n        Args:\n            cron_id (int): ID of the Cron entry to retrieve.\n\n        Returns:\n            dict: Details of the requested Cron entry.\n\n        Raises:\n            HTTPException: If the Cron entry is not found.\n        \"\"\"\n        select_query = sql.SQL(\n            \"\"\"\n            SELECT ST_AsGeoJSON(c.*) FROM public.cron c\n            WHERE id = %s\n        \"\"\"\n        )\n        self.cur.execute(select_query, (cron_id,))\n        result = self.cur.fetchone()\n        self.d_b.close_conn()\n        if result:\n            return orjson.loads(result[0])\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n\n    def update_cron(self, cron_id: int, cron_data):\n        \"\"\"\n        Update an existing Cron entry in the database.\n\n        Args:\n            cron_id (int): ID of the Cron entry to update.\n            cron_data (dict): Data for updating the Cron entry.\n\n        Returns:\n            dict: Result of the Cron update process.\n\n        Raises:\n            HTTPException: If the Cron entry is not found.\n        \"\"\"\n        update_query = sql.SQL(\n            \"\"\"\n            UPDATE public.cron\n            SET iso3 = %s, hdx_upload = %s, dataset = %s, queue = %s, meta = %s, categories = %s, geometry = %s\n            WHERE id = %s\n            RETURNING *\n        \"\"\"\n        )\n        self.cur.execute(\n            update_query,\n            (\n                cron_data.get(\"iso3\", None),\n                cron_data.get(\"hdx_upload\", True),\n                json.dumps(cron_data.get(\"dataset\")),\n                cron_data.get(\"queue\", \"raw_ondemand\"),\n                cron_data.get(\"meta\", False),\n                json.dumps(cron_data.get(\"categories\", {})),\n                json.dumps(cron_data.get(\"geometry\")),\n                cron_id,\n            ),\n        )\n        self.con.commit()\n        result = self.cur.fetchone()\n        self.d_b.close_conn()\n        if result:\n            return {\"update\": True}\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n\n    def patch_cron(self, cron_id: int, cron_data: dict):\n        \"\"\"\n        Partially update an existing Cron entry in the database.\n\n        Args:\n            cron_id (int): ID of the Cron entry to update.\n            cron_data (dict): Data for partially updating the Cron entry.\n\n        Returns:\n            dict: Result of the Cron update process.\n\n        Raises:\n            HTTPException: If the Cron entry is not found.\n        \"\"\"\n        if not cron_data:\n            raise ValueError(\"No data provided for update\")\n\n        set_clauses = []\n        params = []\n        for field, value in cron_data.items():\n            set_clauses.append(sql.SQL(\"{} = %s\").format(sql.Identifier(field)))\n            if isinstance(value, dict):\n                params.append(json.dumps(value))\n            else:\n                params.append(value)\n\n        query = sql.SQL(\"UPDATE public.cron SET {} WHERE id = %s RETURNING *\").format(\n            sql.SQL(\", \").join(set_clauses)\n        )\n        params.append(cron_id)\n\n        self.cur.execute(query, tuple(params))\n        self.con.commit()\n        result = self.cur.fetchone()\n        self.d_b.close_conn()\n\n        if result:\n            return {\"update\": True}\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n\n    def delete_cron(self, cron_id: int):\n        \"\"\"\n        Delete an existing Cron entry from the database.\n\n        Args:\n            cron_id (int): ID of the Cron entry to delete.\n\n        Returns:\n            dict: Result of the Cron deletion process.\n\n        Raises:\n            HTTPException: If the Cron entry is not found.\n        \"\"\"\n        delete_query = sql.SQL(\n            \"\"\"\n            DELETE FROM public.cron\n            WHERE id = %s\n            RETURNING *\n        \"\"\"\n        )\n        self.cur.execute(delete_query, (cron_id,))\n        self.con.commit()\n        result = self.cur.fetchone()\n        self.d_b.close_conn()\n        if result:\n            return dict(result[0])\n        raise HTTPException(status_code=404, detail=\"Cron item not found\")\n\n\nclass DownloadMetrics:\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes an instance of the DownloadMetrics class, connecting to the database.\n        \"\"\"\n        dbdict = get_db_connection_params()\n        self.d_b = Database(dbdict)\n        self.con, self.cur = self.d_b.connect()\n\n    def get_summary_stats(self, start_date, end_date, group_by, folder=None):\n        \"\"\"\n        Get summary metrics for raw-data-api downloads\n        \"\"\"\n        if folder:\n            select_query = f\"\"\"\n                SELECT\n                    date_trunc('{group_by}', date) as kwdate,\n                    SUM((folders->'{folder}'->>'downloads_count')::numeric) as total_downloads_count,\n                    SUM((folders->'{folder}'->>'uploads_count')::numeric) as total_uploads_count,\n                    SUM((folders->'{folder}'->>'unique_users')::numeric) as total_unique_users,\n                    SUM((folders->'{folder}'->>'unique_downloads')::numeric) as total_unique_downloads,\n                    SUM((folders->'{folder}'->>'interactions_count')::numeric) as total_interactions_count,\n                    SUM((folders->'{folder}'->>'upload_size')::numeric) as total_upload_size,\n                    SUM((folders->'{folder}'->>'download_size')::numeric) as total_download_size,\n                    JSONB_AGG((folders->'{folder}'->>'locations')::json) as total_locations,\n                    JSONB_AGG((summary->>'referrers')::json) as total_referrers\n                FROM\n                    metrics\n                WHERE\n                    date BETWEEN '{start_date}' AND '{end_date}'\n                GROUP BY\n                    kwdate\n                ORDER BY\n                    kwdate\n            \"\"\"\n        else:\n            select_query = f\"\"\"\n                SELECT\n                    date_trunc('{group_by}', date) as kwdate,\n                    SUM((summary->>'downloads_count')::numeric) as total_downloads_count,\n                    SUM((summary->>'uploads_count')::numeric) as total_uploads_count,\n                    SUM((summary->>'unique_users')::numeric) as total_unique_users,\n                    SUM((summary->>'unique_downloads')::numeric) as total_unique_downloads,\n                    SUM((summary->>'interactions_count')::numeric) as total_interactions_count,\n                    SUM((summary->>'upload_size')::numeric) as total_upload_size,\n                    SUM((summary->>'download_size')::numeric) as total_download_size,\n                    JSONB_AGG((summary->>'locations')::json) as total_locations,\n                    JSONB_AGG((summary->>'referrers')::json) as total_referrers\n                FROM\n                    metrics\n                WHERE\n                    date BETWEEN '{start_date}' AND '{end_date}'\n                GROUP BY\n                    kwdate\n                ORDER BY\n                    kwdate\n            \"\"\"\n\n        self.cur.execute(select_query)\n        result = self.cur.fetchall()\n        self.d_b.close_conn()\n        result_lists = []\n        for item in result:\n            item[\"total_locations\"] = dict(\n                sum((Counter(loc) for loc in item[\"total_locations\"]), Counter())\n            )\n            item[\"total_referrers\"] = dict(\n                sum((Counter(loc) for loc in item[\"total_referrers\"]), Counter())\n            )\n            result_lists.append(dict(item))\n\n        return result_lists\n"}
{"type": "source_file", "path": "src/__init__.py", "content": ""}
{"type": "source_file", "path": "setup.py", "content": "# Third party imports\nfrom setuptools import setup\n\nsetup()\n"}
{"type": "source_file", "path": "src/db_session.py", "content": "# Copyright (C) 2021 Humanitarian OpenStreetmap Team\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\n# Humanitarian OpenStreetmap Team\n# 1100 13th Street NW Suite 800 Washington, D.C. 20005\n# <info@hotosm.org>\n\nfrom .db_connection import Database\n\ndatabase_instance = Database()\n"}
{"type": "source_file", "path": "src/query_builder/__init__.py", "content": ""}
{"type": "source_file", "path": "src/utils.py", "content": "# Standard library imports\nimport errno\nimport logging\nimport os\nimport shutil\nimport time\n\n# Reader imports\nfrom src.config import ENABLE_OLD_EXPORTS_CLEANUP, OLD_EXPORTS_CLEANUP_DAYS\n\n\ndef cleanup_old_exports(base_export_dir, days):\n    \"\"\"\n    Removes directories in the base export directory that are older than the specified number of days.\n    \"\"\"\n    now = time.time()\n    cutoff_time = now - (days * 86400)  # days to seconds\n\n    for dirpath, dirnames, filenames in os.walk(base_export_dir):\n        # Remove old files\n        for filename in filenames:\n            file_full_path = os.path.join(dirpath, filename)\n            if (\n                os.path.isfile(file_full_path)\n                and os.path.getmtime(file_full_path) < cutoff_time\n            ):\n                os.remove(file_full_path)\n                logging.info(f\"Removed old export file: {file_full_path}\")\n\n        # Remove old directories\n        for dirname in dirnames:\n            dir_full_path = os.path.join(dirpath, dirname)\n            if (\n                os.path.isdir(dir_full_path)\n                and os.path.getmtime(dir_full_path) < cutoff_time\n            ):\n                shutil.rmtree(dir_full_path)\n                logging.info(f\"Removed old export directory: {dir_full_path}\")\n\n\ndef create_working_dir(working_dir):\n    \"\"\"\n    Creates a working directory for exports. If cleanup is enabled, it first removes old directories.\n    \"\"\"\n    base_export_working_dir = os.path.dirname(\n        os.path.dirname(os.path.abspath(working_dir))\n    )  # Get the grandparent directory , because exportdir/uid/export\n    # Check if cleanup is enabled\n    cleanup_enabled = ENABLE_OLD_EXPORTS_CLEANUP\n    cleanup_days = OLD_EXPORTS_CLEANUP_DAYS\n\n    if cleanup_enabled:\n        cleanup_old_exports(base_export_working_dir, cleanup_days)\n\n    if not os.path.exists(working_dir):\n        try:\n            os.makedirs(working_dir)\n        except OSError as e:\n            if e.errno == errno.ENOSPC:  # No space left on device\n                logging.warning(\n                    \"No space left on device, attempting to cleanup old exports and retry...\"\n                )\n                cleanup_old_exports(base_export_working_dir, cleanup_days)\n                os.makedirs(working_dir)\n            else:\n                raise e\n    return working_dir\n"}
{"type": "source_file", "path": "src/db_connection.py", "content": "# Copyright (C) 2021 Humanitarian OpenStreetmap Team\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\n# Humanitarian OpenStreetmap Team\n# 1100 13th Street NW Suite 800 Washington, D.C. 20005\n# <info@hotosm.org>\n\nfrom psycopg2 import pool\n\nfrom .config import get_db_connection_params\nfrom .config import logger as logging\n\n\nclass Database:\n    \"\"\"Handles the all work related to connection pooling\"\"\"\n\n    def __init__(self):\n        self.db_params = get_db_connection_params()\n        self._cursor = None\n        self.threaded_postgresql_pool = None\n        self.con = None\n\n    def connect(self):\n        \"\"\"Connection to the database\"\"\"\n        if not self.threaded_postgresql_pool:\n            try:\n                # creating pool through psycopg2 with threaded connection so that there will support from 3 users to 20 users\n                self.threaded_postgresql_pool = pool.ThreadedConnectionPool(\n                    3, 20, **self.db_params\n                )\n                if self.threaded_postgresql_pool:\n                    logging.info(\n                        \"Connection pool created successfully using ThreadedConnectionPool\"\n                    )\n            except Exception as ex:\n                logging.error(ex)\n                raise ex\n\n    def get_conn_from_pool(self):\n        \"\"\"Function to get connection from the pool instead of new connection\n\n        Returns:\n            connection\n        \"\"\"\n        if self.threaded_postgresql_pool:\n            # Use getconn() method to Get Connection from connection pool\n            pool_conn = self.threaded_postgresql_pool.getconn()\n            return pool_conn\n\n    def release_conn_from_pool(self, pool_con):\n        \"\"\"Can be used to release specific connection after its use from the pool , so that it can be used by another process\n\n        Args:\n            pool_con (_type_): define which connection to remove from pool\n\n        Raises:\n            ex: error if connection doesnot exists or misbehave of function\n        \"\"\"\n        try:\n            # Use this method to release the connection object and send back to connection pool\n            self.threaded_postgresql_pool.putconn(pool_con)\n            logging.debug(\"Putting back postgresql connection to thread\")\n        except Exception as ex:\n            logging.error(ex)\n            raise ex\n\n    def close_all_connection_pool(self):\n        \"\"\"Closes the connection thread created by thread pooling all at once\"\"\"\n        # closing database connection.\n        # use closeall() method to close all\n        if self.threaded_postgresql_pool:\n            self.threaded_postgresql_pool.closeall()\n        logging.info(\"Threaded PostgreSQL connection pool is closed\")\n"}
{"type": "source_file", "path": "src/validation/models.py", "content": "# Copyright (C) 2021 Humanitarian OpenStreetmap Team\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\n# Humanitarian OpenStreetmap Team\n# 1100 13th Street NW Suite 800 Washington, D.C. 20005\n# <info@hotosm.org>\n\"\"\"Page contains validation models for application\"\"\"\n# Standard library imports\nfrom enum import Enum\nfrom typing import Dict, List, Optional, Union\n\n# Third party imports\nfrom geojson_pydantic import Feature, FeatureCollection, MultiPolygon, Polygon\nfrom pydantic import BaseModel as PydanticModel\nfrom pydantic import Field, validator\n\n# Reader imports\nfrom src.config import (\n    ALLOW_BIND_ZIP_FILTER,\n    ENABLE_HDX_EXPORTS,\n    ENABLE_POLYGON_STATISTICS_ENDPOINTS,\n    ENABLE_TILES,\n)\n\nif ENABLE_HDX_EXPORTS:\n    # Reader imports\n    from src.config import ALLOWED_HDX_TAGS, ALLOWED_HDX_UPDATE_FREQUENCIES\n\n\ndef to_camel(string: str) -> str:\n    split_string = string.split(\"_\")\n\n    return \"\".join([split_string[0], *[w.capitalize() for w in split_string[1:]]])\n\n\nclass BaseModel(PydanticModel):\n    class Config:\n        alias_generator = to_camel\n        populate_by_name = True\n        use_enum_values = True\n        # extra = \"forbid\"\n\n\nclass RawDataOutputType(Enum):\n    GEOJSON = \"geojson\"\n    KML = \"kml\"\n    SHAPEFILE = \"shp\"\n    FLATGEOBUF = \"fgb\"\n    GEOPACKAGE = \"gpkg\"\n    PGDUMP = \"sql\"\n    CSV = \"csv\"\n    GEOPARQUET = \"parquet\"\n    if ENABLE_TILES:\n        MBTILES = \"mbtiles\"\n        PMTILES = \"pmtiles\"  ## EXPERIMENTAL\n        MVT = \"mvt\"  ## Experimental\n\n\nclass SupportedFilters(Enum):\n    TAGS = \"tags\"\n    ATTRIBUTES = \"attributes\"\n\n    @classmethod\n    def has_value(cls, value):\n        \"\"\"Checks value\"\"\"\n        return value in cls._value2member_map_\n\n\nclass SupportedGeometryFilters(Enum):\n    POINT = \"point\"\n    LINE = \"line\"\n    POLYGON = \"polygon\"\n    ALLGEOM = \"all_geometry\"\n\n    @classmethod\n    def has_value(cls, value):\n        \"\"\"Checks if the value is supported\"\"\"\n        return value in cls._value2member_map_\n\n\nclass JoinFilterType(Enum):\n    OR = \"OR\"\n    AND = \"AND\"\n\n\nclass SQLFilter(BaseModel):\n    join_or: Optional[Dict[str, List[str]]] = Field(default=None)\n    join_and: Optional[Dict[str, List[str]]] = Field(default=None)\n\n\nclass TagsFilter(BaseModel):\n    point: Optional[SQLFilter] = Field(default=None)\n    line: Optional[SQLFilter] = Field(default=None)\n    polygon: Optional[SQLFilter] = Field(default=None)\n    all_geometry: Optional[SQLFilter] = Field(default=None)\n\n\nclass AttributeFilter(BaseModel):\n    point: Optional[List[str]] = Field(default=None)\n    line: Optional[List[str]] = Field(default=None)\n    polygon: Optional[List[str]] = Field(default=None)\n    all_geometry: Optional[List[str]] = Field(default=None)\n\n\nclass Filters(BaseModel):\n    tags: Optional[TagsFilter] = Field(default=None)\n    attributes: Optional[AttributeFilter] = Field(default=None)\n\n\nclass GeometryValidatorMixin:\n    @validator(\"geometry\")\n    def validate_geometry(cls, value):\n        \"\"\"Validates geometry\"\"\"\n        if value:\n            if value.type == \"Feature\":\n                if value.geometry.type not in [\"Polygon\", \"MultiPolygon\"]:\n                    raise ValueError(\n                        f\"Feature geometry type {value.geometry.type} must be of type polygon/multipolygon\",\n                    )\n                return value.geometry\n            if value.type == \"FeatureCollection\":\n                for feature in value.features:\n                    if feature.geometry.type not in [\"Polygon\", \"MultiPolygon\"]:\n                        raise ValueError(\n                            f\"Feature Collection can't have {feature.type} , should be polygon/multipolygon\"\n                        )\n                if len(value.features) > 1:\n                    raise ValueError(\n                        \"Feature collection with multiple features is not supported yet\"\n                    )\n                return value.features[0].geometry\n        return value\n\n\nclass RawDataCurrentParamsBase(BaseModel, GeometryValidatorMixin):\n    output_type: Optional[RawDataOutputType] = Field(\n        default=RawDataOutputType.GEOJSON.value, example=\"geojson\"\n    )\n    geometry_type: Optional[List[SupportedGeometryFilters]] = Field(\n        default=None, example=[\"point\", \"polygon\"]\n    )\n    centroid: Optional[bool] = Field(\n        default=False, description=\"Exports centroid of features as geom\"\n    )\n    use_st_within: Optional[bool] = Field(\n        default=True,\n        description=\"Exports features which are exactly inside the passed polygons (ST_WITHIN) By default features which are intersected with passed polygon is exported\",\n    )\n    include_user_metadata: Optional[bool] = Field(\n        default=False,\n        description=\"Include user metadata on exports , Only available to logged in users\",\n    )\n    if ENABLE_POLYGON_STATISTICS_ENDPOINTS:\n        include_stats: Optional[bool] = Field(\n            default=False,\n            description=\"Includes detailed stats about the polygon passed such as buildings count , road count along with summary about data completeness in the area\",\n        )\n    filters: Optional[Filters] = Field(\n        default=None,\n        example={\n            \"tags\": {\"all_geometry\": {\"join_or\": {\"building\": []}}},\n            \"attributes\": {\"all_geometry\": [\"name\"]},\n        },\n        description=\"Filter for point,line,polygon/ all geometry for both select and where clause, All geometry filter means : It will apply the same filter to all the geometry type\",\n    )\n    include_stats_html: Optional[bool] = Field(\n        default=False,\n        description=\"Includes detailed stats about the polygon passed such as buildings count , road count along with summary about data completeness in the area\",\n    )\n    include_translit: Optional[bool] = Field(\n        default=False,\n        description=\"Includes transliterations\",\n    )\n    geometry: Union[\n        Polygon,\n        MultiPolygon,\n        Feature,\n        FeatureCollection,\n    ] = Field(\n        example={\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.96919250488281, 28.194446860487773],\n                    [83.99751663208006, 28.194446860487773],\n                    [83.99751663208006, 28.214869548073377],\n                    [83.96919250488281, 28.214869548073377],\n                    [83.96919250488281, 28.194446860487773],\n                ]\n            ],\n        },\n    )\n\n    @validator(\"geometry_type\", allow_reuse=True)\n    def return_unique_value(cls, value):\n        \"\"\"return unique list\"\"\"\n        if value:\n            return list(set(value))\n        return value\n\n\nclass RawDataCurrentParams(RawDataCurrentParamsBase):\n    if ENABLE_TILES:\n        min_zoom: Optional[int] = Field(\n            default=None, description=\"Only for mbtiles\"\n        )  # only for if mbtiles is output\n        max_zoom: Optional[int] = Field(\n            default=None, description=\"Only for mbtiles\"\n        )  # only for if mbtiles is output\n    file_name: Optional[str] = Field(default=None, example=\"My test export\")\n    uuid: Optional[bool] = Field(\n        default=True,\n        description=\"Attaches uid to exports by default , Only disable this if it is recurring export\",\n    )\n    fgb_wrap_geoms: Optional[bool] = Field(\n        default=False,\n        description=\"Wraps all flatgeobuff output to geometrycollection geometry type\",\n    )\n\n    if ALLOW_BIND_ZIP_FILTER:\n        bind_zip: Optional[bool] = True\n\n        @validator(\"bind_zip\", allow_reuse=True)\n        def check_bind_option(cls, value, values):\n            \"\"\"Checks if cloud optimized output format or geoJSON is selected along with bind to zip file\"\"\"\n            if value is False:\n                if values.get(\"output_type\") not in (\n                    (\n                        [\n                            RawDataOutputType.GEOJSON.value,\n                            RawDataOutputType.FLATGEOBUF.value,\n                            RawDataOutputType.GEOPARQUET.value,\n                        ]\n                        + ([RawDataOutputType.PMTILES.value] if ENABLE_TILES else [])\n                    )\n                ):\n                    raise ValueError(\n                        \"Only Cloud Optimized format and GeoJSON is supported for streaming\"\n                    )\n            return value\n\n\nclass SnapshotResponse(BaseModel):\n    task_id: str\n    track_link: str\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"task_id\": \"aa539af6-83d4-4aa3-879e-abf14fffa03f\",\n                \"track_link\": \"/tasks/status/aa539af6-83d4-4aa3-879e-abf14fffa03f/\",\n            }\n        }\n\n\nclass SnapshotTaskResult(BaseModel):\n    download_url: str\n    file_name: str\n    response_time: str\n    query_area: str\n    binded_file_size: str\n    zip_file_size_bytes: int\n\n\nclass SnapshotTaskResponse(BaseModel):\n    id: str\n    status: str\n    result: SnapshotTaskResult\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"id\": \"3fded368-456f-4ef4-a1b8-c099a7f77ca4\",\n                \"status\": \"SUCCESS\",\n                \"result\": {\n                    \"download_url\": \"https://s3.us-east-1.amazonaws.com/exports-stage.hotosm.org/Raw_Export_3fded368-456f-4ef4-a1b8-c099a7f77ca4_GeoJSON.zip\",\n                    \"file_name\": \"Raw_Export_3fded368-456f-4ef4-a1b8-c099a7f77ca4_GeoJSON\",\n                    \"response_time\": \"0:00:12.175976\",\n                    \"query_area\": \"6 Sq Km \",\n                    \"binded_file_size\": \"7 MB\",\n                    \"zip_file_size_bytes\": 1331601,\n                },\n            }\n        }\n\n\nclass StatusResponse(BaseModel):\n    last_updated: str\n\n    class Config:\n        json_schema_extra = {\"example\": {\"lastUpdated\": \"2022-06-27 19:59:24+05:45\"}}\n\n\nclass StatsRequestParams(BaseModel, GeometryValidatorMixin):\n    iso3: Optional[str] = Field(\n        default=None,\n        description=\"ISO3 Country Code.\",\n        min_length=3,\n        max_length=3,\n        example=\"NPL\",\n    )\n    geometry: Optional[\n        Union[Polygon, MultiPolygon, Feature, FeatureCollection]\n    ] = Field(\n        default=None,\n        example={\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.96919250488281, 28.194446860487773],\n                    [83.99751663208006, 28.194446860487773],\n                    [83.99751663208006, 28.214869548073377],\n                    [83.96919250488281, 28.214869548073377],\n                    [83.96919250488281, 28.194446860487773],\n                ]\n            ],\n        },\n    )\n\n    @validator(\"geometry\", pre=True, always=True)\n    def set_geometry_or_iso3(cls, value, values):\n        \"\"\"Either geometry or iso3 should be supplied.\"\"\"\n        if value is not None and values.get(\"iso3\") is not None:\n            raise ValueError(\"Only one of geometry or iso3 should be supplied.\")\n        if value is None and values.get(\"iso3\") is None:\n            raise ValueError(\"Either geometry or iso3 should be supplied.\")\n        return value\n\n\n### HDX BLock\n\n\nclass HDXModel(BaseModel):\n    \"\"\"\n    Model for HDX configuration settings.\n\n    Fields:\n    - tags (List[str]): List of tags for the HDX model.\n    - caveats (str): Caveats/Warning for the Datasets.\n    - notes (str): Extra notes to append in the notes section of HDX datasets.\n    \"\"\"\n\n    tags: List[str] = Field(\n        default=[\"geodata\"],\n        description=\"List of tags for the HDX model.\",\n        example=[\"roads\", \"transportation\", \"geodata\"],\n    )\n    caveats: str = Field(\n        default=\"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n        description=\"Caveats/Warning for the Datasets.\",\n        example=\"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n    )\n    notes: str = Field(\n        default=\"\",\n        description=\"Extra notes to append in notes section of hdx datasets\",\n        example=\"Sample notes to append\",\n    )\n\n    @validator(\"tags\")\n    def validate_tags(cls, value):\n        \"\"\"Validates tags if they are allowed from hdx allowed approved tags\n\n        Args:\n            value (_type_): _description_\n\n        Raises:\n            ValueError: _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        if value:\n            for item in value:\n                if ALLOWED_HDX_TAGS:\n                    if item.strip() not in ALLOWED_HDX_TAGS:\n                        raise ValueError(\n                            f\"Invalid tag {item.strip()} , Should be within {ALLOWED_HDX_TAGS}\"\n                        )\n        return value\n\n\nclass CategoryModel(BaseModel):\n    \"\"\"\n    Model for category configuration settings.\n\n    Fields:\n    - hdx (HDXModel): HDX configuration model.\n    - types (List[str]): List of feature types (points, lines, polygons).\n    - select (List[str]): List of selected fields.\n    - where (str): SQL-like condition to filter features.\n    - formats (List[str]): List of Export Formats (suffixes).\n    \"\"\"\n\n    hdx: Optional[HDXModel] = Field(\n        default=None, description=\"HDX Specific configurations\"\n    )\n    types: List[str] = Field(\n        ...,\n        description=\"List of feature types (points, lines, polygons).\",\n        example=[\"lines\"],\n    )\n    select: List[str] = Field(\n        ...,\n        description=\"List of selected fields.\",\n        example=[\"name\", \"highway\"],\n    )\n    where: str = Field(\n        ...,\n        description=\"SQL-like condition to filter features.\",\n        example=\"highway IS NOT NULL\",\n    )\n    formats: List[str] = Field(\n        ...,\n        description=\"List of Export Formats (suffixes).\",\n        example=[\"gpkg\", \"geojson\"],\n    )\n\n    @validator(\"types\")\n    def validate_types(cls, value):\n        \"\"\"validates geom types\n\n        Args:\n            value (_type_): _description_\n\n        Raises:\n            ValueError: _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        allowed_types = {\"points\", \"lines\", \"polygons\"}\n        for item in value:\n            if item not in allowed_types:\n                raise ValueError(\n                    f\"Invalid type: {item}. Allowed types are {', '.join(allowed_types)}\"\n                )\n        return value\n\n    @validator(\"formats\")\n    def validate_export_types(cls, value):\n        \"\"\"Validates export types if they are supported\n\n        Args:\n            value (_type_): _description_\n\n        Raises:\n            ValueError: _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        for export_type in value:\n            if export_type not in EXPORT_TYPE_MAPPING:\n                raise ValueError(f\"Unsupported export type: {export_type}\")\n        return value\n\n\nclass ExportTypeInfo:\n    \"\"\"\n    Class representing export type information.\n\n    Fields:\n    - suffix (str): File suffix for the export type.\n    - driver_name (str): GDAL driver name.\n    - layer_creation_options (List[str]): Layer creation options.\n    - format_option (str): Format option for GDAL.\n    \"\"\"\n\n    def __init__(self, suffix, driver_name, layer_creation_options, format_option):\n        self.suffix = suffix\n        self.driver_name = driver_name\n        self.layer_creation_options = layer_creation_options\n        self.format_option = format_option\n\n\nEXPORT_TYPE_MAPPING = {\n    \"geojson\": ExportTypeInfo(\"geojson\", \"GeoJSON\", [], \"GDAL\"),\n    \"shp\": ExportTypeInfo(\n        \"shp\", \"ESRI Shapefile\", [\"ENCODING=UTF-8,2GB_LIMIT=No,RESIZE=Yes\"], \"GDAL\"\n    ),\n    \"gpkg\": ExportTypeInfo(\"gpkg\", \"GPKG\", [\"SPATIAL_INDEX=No\"], \"GDAL\"),\n    \"sqlite\": ExportTypeInfo(\"sqlite\", \"SQLite\", [], \"GDAL\"),\n    \"fgb\": ExportTypeInfo(\"fgb\", \"FlatGeobuf\", [\"VERIFY_BUFFERS=NO\"], \"GDAL\"),\n    \"mvt\": ExportTypeInfo(\"mbtiles\", \"MVT\", [\"MAXZOOM=20\"], \"GDAL\"),\n    \"kml\": ExportTypeInfo(\"kml\", \"KML\", [], \"GDAL\"),\n    \"gpx\": ExportTypeInfo(\"gpx\", \"GPX\", [], \"GDAL\"),\n    \"parquet\": ExportTypeInfo(\"parquet\", \"PARQUET\", [], \"PARQUET\"),\n}\n\n\nclass DatasetConfig(BaseModel):\n    \"\"\"\n    Model for dataset configuration settings.\n\n    Fields:\n    - private (bool): Make dataset private. By default False, public is recommended.\n    - subnational (bool): Make it true if the dataset doesn't cover the nation/country.\n    - update_frequency (str): Update frequency to be added on uploads.\n    - dataset_title (str): Dataset title that appears at the top of the page.\n    - dataset_prefix (str): Dataset prefix to be appended before the category name. Ignored if iso3 is supplied.\n    - dataset_locations (List[str]): Valid dataset locations iso3.\n    \"\"\"\n\n    private: bool = Field(\n        default=False,\n        description=\"Make dataset private , By default False , Public is recommended\",\n        example=\"False\",\n    )\n    subnational: bool = Field(\n        default=False,\n        description=\"Make it true if dataset doesn't cover nation/country\",\n        example=\"False\",\n    )\n    update_frequency: str = Field(\n        default=\"as needed\",\n        description=\"Update frequncy to be added on uploads\",\n        example=\"daily\",\n    )\n    dataset_title: str = Field(\n        default=None,\n        description=\"Dataset title which appears at top of the page\",\n        example=\"Nepal\",\n    )\n    dataset_prefix: str = Field(\n        default=None,\n        description=\"Dataset prefix to be appended before category name, Will be ignored if iso3 is supplied\",\n        example=\"hotosm_npl\",\n    )\n    dataset_locations: List[str] | None = Field(\n        default=None,\n        description=\"Valid dataset locations iso3\",\n        example=\"['npl']\",\n    )\n    dataset_folder: str = Field(\n        default=\"ISO3\",\n        description=\"Default base folder for the exports\",\n        example=\"ISO3\",\n    )\n    customviz: Optional[List[dict[str, str]]] | None = Field(\n        default=[],\n        description=\"List of objects for custom visualization\",\n        example=\"[{'url': 'https://something.org/datasetviz.html'}]\",\n    )\n\n    @validator(\"update_frequency\")\n    def validate_frequency(cls, value):\n        \"\"\"Validates frequency\n\n        Args:\n            value (_type_): _description_\n\n        Raises:\n            ValueError: _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        if ALLOWED_HDX_UPDATE_FREQUENCIES:\n            if value.strip() not in ALLOWED_HDX_UPDATE_FREQUENCIES:\n                raise ValueError(\n                    f\"Invalid update frequency , Should be within {ALLOWED_HDX_UPDATE_FREQUENCIES}\"\n                )\n        return value.strip()\n\n\nclass CategoriesBase(BaseModel):\n    hdx_upload: bool = Field(\n        default=False,\n        description=\"Enable/Disable uploading dataset to hdx, False by default\",\n    )\n    dataset: Optional[DatasetConfig] = Field(\n        default=None,\n        description=\"Dataset Configurations for HDX Upload\",\n        example={\n            \"dataset_prefix\": \"hotosm_project_1\",\n            \"dataset_folder\": \"TM\",\n            \"dataset_title\": \"Tasking Manger Project 1\",\n        },\n    )\n    queue: Optional[str] = Field(\n        default=\"raw_ondemand\",\n        description=\"Lets you decide which queue you wanna place your task, Requires admin access\",\n    )\n    meta: bool = Field(\n        default=False,\n        description=\"Dumps Meta db in parquet format & hdx config json to s3\",\n    )\n    categories: List[Dict[str, CategoryModel]] = Field(\n        ...,\n        description=\"List of dynamic categories.\",\n        example=[\n            {\n                \"Roads\": {\n                    \"hdx\": {\n                        \"tags\": [\"roads\", \"transportation\", \"geodata\"],\n                        \"caveats\": \"OpenStreetMap data is crowd sourced and cannot be considered to be exhaustive\",\n                    },\n                    \"types\": [\"lines\", \"polygons\"],\n                    \"select\": [\"name\", \"highway\"],\n                    \"where\": \"tags['highway'] IS NOT NULL\",\n                    \"formats\": [\"geojson\"],\n                }\n            }\n        ],\n    )\n\n\nclass DynamicCategoriesModel(CategoriesBase, GeometryValidatorMixin):\n    \"\"\"\n    Model for dynamic categories.\n\n    Fields:\n    - iso3 (Optional[str]): ISO3 Country Code.\n    - include_stats (bool): Include a JSON file with stats. Available for GeoJSON exports only.\n    - include_stats_html (bool): Include a HTML file with a stats summary. Available for GeoJSON exports only.\n    - include_translit (bool): Add transliterations. Available for GeoJSON exports only.\n    - dataset (Optional[DatasetConfig]): Dataset Configurations for HDX Upload.\n    - meta (bool): Dumps Meta db in parquet format & HDX config JSON to S3.\n    - hdx_upload (bool): Enable/Disable uploading the dataset to HDX.\n    - categories (List[Dict[str, CategoryModel]]): List of dynamic categories.\n    - geometry (Optional[Union[Polygon, MultiPolygon]]): Custom polygon geometry.\n    \"\"\"\n\n    iso3: Optional[str] = Field(\n        default=None,\n        description=\"ISO3 Country Code\",\n        min_length=3,\n        max_length=3,\n        example=\"USA\",\n    )\n    include_stats: Optional[bool] = Field(\n        default=False,\n        description=\"Include a JSON file with stats. Available for GeoJSON exports only.\",\n    )\n    include_stats_html: Optional[bool] = Field(\n        default=False,\n        description=\"Include a HTML file with a stats summary. Available for GeoJSON exports only.\",\n    )\n    include_translit: Optional[bool] = Field(\n        default=False,\n        description=\"Add transliterations. Available for GeoJSON exports only.\",\n    )\n    geometry: Optional[\n        Union[Polygon, MultiPolygon, Feature, FeatureCollection]\n    ] = Field(\n        default=None,\n        example={\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.96919250488281, 28.194446860487773],\n                    [83.99751663208006, 28.194446860487773],\n                    [83.99751663208006, 28.214869548073377],\n                    [83.96919250488281, 28.214869548073377],\n                    [83.96919250488281, 28.194446860487773],\n                ]\n            ],\n        },\n    )\n\n    @validator(\"geometry\", pre=True, always=True)\n    def set_geometry_or_iso3(cls, value, values):\n        \"\"\"Either geometry or iso3 should be supplied.\"\"\"\n        if value is not None and values.get(\"iso3\") is not None:\n            raise ValueError(\"Only one of geometry or iso3 should be supplied.\")\n        if value is None and values.get(\"iso3\") is None:\n            raise ValueError(\"Either geometry or iso3 should be supplied.\")\n        if value is not None:\n            dataset = values.get(\"dataset\")\n            if values.get(\"hdx_upload\"):\n                for category in values.get(\"categories\"):\n                    category_name, category_data = list(category.items())[0]\n                    if category_data.hdx is None:\n                        raise ValueError(f\"HDX is missing for category {category}\")\n\n            if dataset is None and values.get(\"hdx_upload\"):\n                raise ValueError(\"Dataset config should be supplied for custom polygon\")\n            if values.get(\"hdx_upload\"):\n                for item in dataset:\n                    if item is None:\n                        raise ValueError(f\"Missing, Dataset config : {item}\")\n        return value\n\n\nclass CustomRequestsYaml(CategoriesBase):\n    geometry: Union[Polygon, MultiPolygon, Feature, FeatureCollection] = Field(\n        default=None,\n        example={\n            \"type\": \"Polygon\",\n            \"coordinates\": [\n                [\n                    [83.96919250488281, 28.194446860487773],\n                    [83.99751663208006, 28.194446860487773],\n                    [83.99751663208006, 28.214869548073377],\n                    [83.96919250488281, 28.214869548073377],\n                    [83.96919250488281, 28.194446860487773],\n                ]\n            ],\n        },\n    )\n"}
{"type": "source_file", "path": "src/validation/__init__.py", "content": ""}
{"type": "source_file", "path": "src/post_processing/transliterator.py", "content": "from transliterate import translit, get_available_language_codes\n\n\nclass Transliterator:\n    \"\"\"Used for transliterate names while processing GeoJSON files line by line\"\"\"\n\n    props = \"properties\"\n\n    def __init__(self):\n        self.available_language_codes = get_available_language_codes()\n        self.name_tags = [f\"name:{x}\" for x in self.available_language_codes]\n\n    def translit(self, line):\n        \"\"\"\n        Transliterate names and add a new tag suffixed with -translit\n        \"\"\"\n        for code in self.available_language_codes:\n            tag = \"name:{code}\".format(code=code)\n            prop = (\n                line[\"properties\"][\"tags\"]\n                if self.props == \"properties.tags\"\n                else line[\"properties\"]\n            )\n            if tag in prop:\n                translit_tag = \"{tag}-translit\".format(tag=tag)\n                if not translit_tag in prop:\n                    if self.props == \"properties.tags\":\n                        line[\"properties\"][\"tags\"][translit_tag] = translit(\n                            prop[tag], code, reversed=True\n                        )\n                    else:\n                        line[\"properties\"][translit_tag] = translit(\n                            prop[tag], code, reversed=True\n                        )\n"}
