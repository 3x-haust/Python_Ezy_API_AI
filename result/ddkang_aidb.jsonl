{"repo_info": {"repo_name": "aidb", "repo_owner": "ddkang", "repo_url": "https://github.com/ddkang/aidb"}}
{"type": "test_file", "path": "tests/db_utils/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/db_utils/db_setup.py", "content": "import glob\nimport os\nimport networkx as nx\nimport pandas as pd\nimport sqlalchemy\nimport sqlalchemy.ext.asyncio\nfrom sqlalchemy import MetaData\nfrom sqlalchemy.schema import ForeignKeyConstraint\nfrom sqlalchemy.ext.declarative import declarative_base\n\nfrom aidb.config.config_types import python_type_to_sqlalchemy_type\nfrom aidb.utils.constants import BLOB_TABLE_NAMES_TABLE, table_name_for_rep_and_topk_and_blob_mapping\nfrom aidb.utils.logger import logger\nfrom sqlalchemy.sql import text\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n\n@dataclass\nclass ColumnInfo:\n  name: str\n  is_primary_key: bool\n  refers_to: Optional[tuple]  # (table, column)\n  dtype = None\n\n\ndef extract_column_info(table_name, column_str) -> ColumnInfo:\n  pk = False\n  if column_str.startswith(\"pk_\"):\n    pk = True\n    column_str = column_str[3:]  # get rid of pk_ prefix\n  t, c = column_str.split('.')\n  fk = None\n  if t != table_name:\n    fk = (t, c)\n  return ColumnInfo(c, pk, fk)\n\n\nasync def create_db(db_url: str, db_name: str):\n  dialect = db_url.split(\"+\")[0]\n  if dialect == \"postgresql\" or dialect == \"mysql\":\n    engine = sqlalchemy.ext.asyncio.create_async_engine(db_url, isolation_level='AUTOCOMMIT')\n    try:\n      async with engine.begin() as conn:\n        await conn.execute(text(f\"CREATE DATABASE {db_name}\"))\n    except sqlalchemy.exc.ProgrammingError:\n      logger.warning(f'Database {db_name} Already exists')\n    finally:\n      await engine.dispose()\n  elif dialect == \"sqlite\":\n    # sqlite auto creates, do nothing\n    pass\n  else:\n    raise NotImplementedError\n  return\n\n\nasync def drop_all_tables(conn):\n  metadata = MetaData(bind=conn)\n  # Reflect the database to get all table names\n  await conn.run_sync(metadata.reflect)\n  await conn.run_sync(metadata.drop_all)\n  return\n\n\nasync def setup_db(db_url: str, db_name: str, data_dir: str):\n  gt_dir = f'{data_dir}/ground_truth'\n  gt_csv_fnames = glob.glob(f'{gt_dir}/*.csv')\n  gt_csv_fnames.sort()\n\n  db_uri = f'{db_url}/{db_name}'\n  # Connect to db\n  engine = sqlalchemy.ext.asyncio.create_async_engine(db_uri)\n  try:\n    async with engine.begin() as conn:\n      await drop_all_tables(conn)\n      metadata = MetaData(bind=conn)\n      # Create tables\n      for csv_fname in gt_csv_fnames:\n        base_fname = os.path.basename(csv_fname)\n        table_name = base_fname.split('.')[0]\n        df = pd.read_csv(csv_fname)\n        columns_info = []\n        fk_constraints = {}\n        for column in df.columns:\n          column_info = extract_column_info(table_name, column)\n          dtype = python_type_to_sqlalchemy_type(df[column].dtype)\n          if dtype == sqlalchemy.String:\n            # TODO: VAR CHAR lenth should be based on the number of characters\n            if column_info.is_primary_key or column_info.refers_to:\n              column_info.dtype = sqlalchemy.VARCHAR(256)\n            else:\n              column_info.dtype = sqlalchemy.VARCHAR(2048)\n          else:\n            column_info.dtype = dtype\n          columns_info.append(column_info)\n          df.rename(columns={column: column_info.name}, inplace=True)\n\n          if column_info.refers_to is not None:\n            fk_ref_table_name = column_info.refers_to[0]\n            if fk_ref_table_name not in fk_constraints:\n              fk_constraints[fk_ref_table_name] = {'cols': [], 'cols_refs': []}\n            # both tables will have same column name\n            fk_constraints[fk_ref_table_name]['cols'].append(column_info.name)\n            fk_constraints[fk_ref_table_name]['cols_refs'].append(\n              f\"{column_info.refers_to[0]}.{column_info.refers_to[1]}\")\n\n        multi_table_fk_constraints = []\n        for tbl, fk_cons in fk_constraints.items():\n          multi_table_fk_constraints.append(ForeignKeyConstraint(fk_cons['cols'], fk_cons['cols_refs']))\n\n        _ = sqlalchemy.Table(\n            table_name,\n            metadata,\n            *[sqlalchemy.Column(c_info.name, c_info.dtype, primary_key=c_info.is_primary_key, autoincrement=False)\n                for c_info in columns_info],\n            *multi_table_fk_constraints\n        )\n\n      await conn.run_sync(lambda conn: metadata.create_all(conn))\n  finally:\n    await engine.dispose()\n  return engine\n\n\nasync def insert_data_in_tables(conn, data_dir: str, only_blob_data: bool):\n  def get_insertion_order(conn, gt_csv_files):\n    metadata = MetaData()\n    metadata.reflect(conn)\n    table_graph = nx.DiGraph()\n    for table in metadata.sorted_tables:\n      for fk_col in table.foreign_keys:\n        parent_table = str(fk_col.column).split('.')[0]\n        table_graph.add_edge(parent_table, table.name)\n    table_order = nx.topological_sort(table_graph)\n    ordered_csv_files = []\n    for table_name in table_order:\n      csv_file_name = f\"{table_name}.csv\"\n      for f in gt_csv_files:\n        if csv_file_name in f:\n          ordered_csv_files.append(f)\n          break\n    return ordered_csv_files\n\n  gt_dir = f'{data_dir}/ground_truth'\n  gt_csv_fnames = glob.glob(f'{gt_dir}/*.csv')\n\n  gt_csv_fnames = await conn.run_sync(get_insertion_order, gt_csv_fnames, )\n  # Create tables\n  for csv_fname in gt_csv_fnames:\n    base_fname = os.path.basename(csv_fname)\n    table_name = base_fname.split('.')[0]\n\n    if only_blob_data and not table_name.startswith('blobs') and not table_name.startswith('mapping_'):\n      continue\n    df = pd.read_csv(csv_fname)\n    for column in df.columns:\n      column_info = extract_column_info(table_name, column)\n      df.rename(columns={column: column_info.name}, inplace=True)\n\n    if table_name.startswith('mapping'):\n      _, _, blob_mapping_table_name = table_name_for_rep_and_topk_and_blob_mapping(['blobs_00'])\n      table_name = blob_mapping_table_name\n\n    await conn.run_sync(lambda conn: df.to_sql(table_name, conn, if_exists='append', index=False))\n\n\nasync def clear_all_tables(conn):\n  def tmp(conn):\n    metadata = MetaData()\n    metadata.reflect(conn)\n    for table in metadata.sorted_tables:\n      if table.name.startswith('blobs'):\n        continue\n      conn.execute(table.delete())\n  await conn.run_sync(tmp)\n\n\nasync def setup_config_tables(conn):\n  def create_blob_metadata_table(conn):\n    Base = declarative_base()\n\n    class BlobTables(Base):\n      __tablename__ = BLOB_TABLE_NAMES_TABLE\n      table_name = sqlalchemy.Column(sqlalchemy.VARCHAR(256), primary_key=True)\n      blob_key = sqlalchemy.Column(sqlalchemy.VARCHAR(256), primary_key=True)\n\n    Base.metadata.create_all(conn)\n\n\n  def get_blob_table_names_and_columns(conn):\n    metadata = MetaData()\n    metadata.reflect(conn)\n    table_names = [table.name for table in metadata.sorted_tables if table.name.startswith('blob')]\n    # Get the columns for each table\n    table_names_and_columns = {}\n    for table_name in table_names:\n      table = metadata.tables[table_name]\n      table_names_and_columns[table_name] = [column.name for column in table.columns if column.primary_key]\n    return table_names, table_names_and_columns\n\n  await conn.run_sync(create_blob_metadata_table)\n  blob_table_names, columns = await conn.run_sync(get_blob_table_names_and_columns)\n  for table_name in blob_table_names:\n    for column in columns[table_name]:\n      # Insert into blob metadata table\n      await conn.execute(\n        text(f'INSERT INTO {BLOB_TABLE_NAMES_TABLE} VALUES (:table_name, :blob_key)')\n        .bindparams(table_name=table_name, blob_key=column)\n      )\n"}
{"type": "test_file", "path": "tests/inference_service_utils/http_inference_service_setup.py", "content": "import sys\nimport glob\nimport os\nimport pandas as pd\nimport uvicorn\n\nfrom fastapi import FastAPI, Request\n\nfrom multiprocessing import Process\nfrom aidb.utils.logger import logger\n\ndef run_server(data_dir: str, port=8000):\n  app = FastAPI()\n\n  inference_dir = f'{data_dir}/inference'\n  inference_csv_fnames = glob.glob(f'{inference_dir}/*.csv')\n  inference_csv_fnames.sort()\n\n  # Create the inference services\n  name_to_df = {}\n  name_to_input_cols = {}\n  name_to_output_cols = {}\n  for csv_fname in inference_csv_fnames:\n    base_fname = os.path.basename(csv_fname)\n    service_name = base_fname.split('.')[0]\n    if service_name.endswith('__join'):\n      service_name = service_name[:-6]\n    df = pd.read_csv(csv_fname)\n    new_col_names = []\n    output_cols = []\n    input_cols = []\n    for col in df.columns:\n      if col.startswith(\"in__\"):\n        new_col_names.append(col[4:])\n        input_cols.append(col[4:])\n      elif col.startswith(\"out__\"):\n        new_col_names.append(col[5:])\n        output_cols.append(col[5:])\n      else:\n        raise Exception(\"Column doesn't start with in__ or out__\")\n    df.columns = new_col_names\n    name_to_df[service_name] = df\n    name_to_input_cols[service_name] = input_cols\n    name_to_output_cols[service_name] = output_cols\n    logger.info(f'Creating service {service_name}')\n\n\n    @app.post(f'/{service_name}')\n    async def inference(inp: Request):\n      service_name = inp.url.path.split('/')[-1]\n      inp = await inp.json()\n      df = name_to_df[service_name]\n      # Construct a DataFrame from the input\n      inp_df = pd.DataFrame({col: [inp[col]] if not isinstance(inp[col], list) else inp[col]\n                             for col in name_to_input_cols[service_name]})\n\n      # Performing the merge\n      # Note: We're using a left join to ensure that all inputs have corresponding outputs,\n      #     with absent outputs represented as None\n      result_df = pd.merge(inp_df, df, how='left', on=name_to_input_cols[service_name]).convert_dtypes()\n      output_cols = [col for col in df.columns if col not in name_to_input_cols[service_name]]\n      # The outputs are grouped by input dataframe's primary key\n      grouped = result_df.groupby(name_to_input_cols[service_name])\n      res_df_list = []\n      for _, group in grouped:\n        group = group.drop(columns=name_to_input_cols[service_name]).dropna(subset=output_cols, how='all')\n        res_df_list.append(group.to_dict(orient='list'))\n      return res_df_list\n\n\n    # inference service for JOIN query\n    @app.post(f'/{service_name}__join')\n    async def join_inference(inp: Request):\n      service_name = inp.url.path.split('/')[-1]\n      service_name = service_name[:-6]\n\n      inp = await inp.json()\n      df = name_to_df[service_name]\n\n      # Construct a DataFrame from the input\n      inp_df = pd.DataFrame({col: [inp[col]] if not isinstance(inp[col], list) else inp[col]\n                             for col in name_to_input_cols[service_name]})\n\n      # Performing the merge\n      # Note: We're using a left join to ensure that all inputs have corresponding outputs,\n      #     with absent outputs represented as None\n      # For JOIN query, each input pair has exact one output\n      result_df = pd.merge(inp_df, df, how='left', on=name_to_input_cols[service_name]).convert_dtypes()\n\n      # For the JOIN service, the table stores only True values. We need to populate it with False values for the remaining inputs.\n      inference_cols = []\n      service_output_cols = []\n      output_col_rename = []\n      output_to_input_mapping = {col.split('.')[1]: col for col in name_to_input_cols[service_name]}\n      for col in df.columns:\n        if col not in inp_df.columns:\n          if col.split('.')[1] in output_to_input_mapping:\n            service_output_cols.append(output_to_input_mapping[col.split('.')[1]])\n          else:\n            inference_cols.append(col)\n            service_output_cols.append(col)\n          output_col_rename.append(col)\n\n      # Copy the keys from the inputs, as they will default to None if the inference result is False.\n      result_df = result_df[service_output_cols]\n      result_df.columns = output_col_rename\n      for col in inference_cols:\n        if pd.api.types.is_bool_dtype(result_df[col]):\n          result_df[col] = result_df[col].fillna(False)\n        else:\n          result_df[col] = result_df[col].fillna(0).astype(bool)\n      return result_df.to_dict(orient='list')\n\n\n  # config = Config(app=app, host=\"127.0.0.1\", port=8000)\n  # server = Server(config=config)\n  uvicorn.run(app, host=\"127.0.0.1\", port=port, log_level='warning')\n\nif __name__=='__main__':\n  run_server(sys.argv[1])\n"}
{"type": "test_file", "path": "tests/inference_service_utils/inference_service_setup.py", "content": "import glob\nimport os\nimport pandas as pd\n\nfrom aidb.config.config_types import InferenceBinding, AIDBListType\nfrom aidb.engine import Engine\nfrom aidb.inference.http_inference_service import HTTPInferenceService\n\n\ndef register_inference_services(engine: Engine, data_dir: str, port=8000, batch_supported=True, preferred_batch_size=128, cost_dict={}):\n  csv_fnames = glob.glob(f'{data_dir}/inference/*.csv')\n  csv_fnames.sort()  # TODO: sort by number\n  for csv_fname in csv_fnames:\n    base_fname = os.path.basename(csv_fname)\n    service_name = base_fname.split('.')[0]\n    df = pd.read_csv(csv_fname)\n    columns = df.columns\n    input_cols = []\n    output_cols = []\n    columns_to_input_keys = []\n    output_keys_to_columns = []\n    for col in columns:\n      if col.startswith(\"in__\"):\n        columns_to_input_keys.append(col[4:])\n        input_cols.append(col[4:])\n      elif col.startswith(\"out__\"):\n        output_keys_to_columns.append((col[5:], AIDBListType()))\n        output_cols.append(col[5:])\n      else:\n        raise Exception(\"Invalid column name, column name should start with in__ or out__\")\n\n    service_url = f'http://127.0.0.1:{port}/{service_name}'\n    if service_name.endswith('__join'):\n      service_name = service_name.split('__')[0]\n\n    if service_name in cost_dict:\n      cost = cost_dict[service_name]\n    else:\n      cost = None\n    service = HTTPInferenceService(\n      service_name,\n      False,\n      url=service_url,\n      headers={'Content-Type': 'application/json'},\n      columns_to_input_keys=columns_to_input_keys,\n      response_keys_to_columns=output_keys_to_columns,\n      batch_supported=batch_supported,\n      preferred_batch_size=preferred_batch_size,\n      cost=cost\n    )\n\n    engine.register_inference_service(service)\n    engine.bind_inference_service(\n      service_name,\n      InferenceBinding(\n        tuple(input_cols),\n        tuple(output_cols),\n      )\n    )\n"}
{"type": "test_file", "path": "tests/tasti_test/tasti_test.py", "content": "from enum import Enum\nimport numpy as np\nimport os\nimport pandas as pd\nfrom typing import Optional\n\nfrom aidb.vector_database.chroma_vector_database import ChromaVectorDatabase\nfrom aidb.vector_database.faiss_vector_database import FaissVectorDatabase\nfrom aidb.vector_database.weaviate_vector_database import WeaviateAuth, WeaviateVectorDatabase\nfrom aidb.vector_database.tasti import Tasti\nfrom aidb.utils.constants import VECTOR_ID_COLUMN\nfrom aidb.utils.logger import logger\nfrom tests.utils import setup_test_logger\n\nsetup_test_logger('tasti')\n\nclass VectorDatabaseType(Enum):\n  FAISS = 'FAISS'\n  CHROMA = 'Chroma'\n  WEAVIATE = 'Weaviate'\n\n\nclass TastiTests():\n  def __init__(\n    self,\n    index_name: str,\n    data_size: int,\n    embedding_dim: int,\n    nb_buckets: int,\n    vector_database_type: VectorDatabaseType,\n    percent_fpf: float = 0.75,\n    seed: int = 1234,\n    weaviate_auth: Optional[WeaviateAuth] = None,\n    index_path: Optional[str] = None,\n  ):\n    self.index_name = index_name\n    self.data_size = data_size\n    self.embedding_dim = embedding_dim\n    self.nb_buckets = nb_buckets\n    self.total_data = 0\n    self.vd_type = vector_database_type\n    self.seed = seed\n\n    self.data, self.vector_ids = self.generate_data(self.data_size, embedding_dim)\n    self.user_database = self.simulate_user_providing_database(index_path, weaviate_auth)\n\n    self.tasti = Tasti(index_name, self.user_database, nb_buckets, percent_fpf, seed)\n\n\n  def generate_data(self, data_size, emb_size):\n    np.random.seed(self.seed)\n    embeddings = np.random.rand(data_size, emb_size)\n    data = pd.DataFrame({'id': range(self.total_data, self.total_data + data_size), 'values': embeddings.tolist()})\n    vector_ids = pd.DataFrame({VECTOR_ID_COLUMN: range(self.total_data, self.total_data + data_size)})\n    self.total_data += data_size\n    return data, vector_ids\n\n\n  def simulate_user_providing_database(self, index_path: Optional[str], weaviate_auth: Optional[WeaviateAuth]):\n    '''\n    Originally, user will provide a vector database, and Tasti will read from it.\n    This function is used to create a vector database to store original data\n    '''\n    user_database = None\n    if self.vd_type == VectorDatabaseType.FAISS.value:\n      user_database = FaissVectorDatabase(index_path)\n      user_database.create_index(self.index_name, self.embedding_dim, recreate_index=True)\n      user_database.insert_data(self.index_name, self.data)\n      user_database.save_index(self.index_name)\n\n    elif self.vd_type == VectorDatabaseType.CHROMA.value:\n      user_database = ChromaVectorDatabase(index_path)\n      user_database.create_index(self.index_name, recreate_index=True)\n      user_database.insert_data(self.index_name, self.data)\n\n    elif self.vd_type == VectorDatabaseType.WEAVIATE.value:\n      user_database = WeaviateVectorDatabase(weaviate_auth)\n      user_database.create_index(self.index_name, recreate_index=True)\n      user_database.insert_data(self.index_name, self.data)\n\n    return user_database\n\n\n  def simulate_user_inserting_new_data(self, data_size):\n    new_data, blob_id = self.generate_data(data_size, self.embedding_dim)\n    self.user_database.insert_data(self.index_name, new_data)\n    if self.vd_type == 'FAISS':\n      self.user_database.save_index(self.index_name)\n    return new_data, blob_id\n\n\n  def test(self):\n    self.tasti.set_vector_ids(self.vector_ids)\n    representative_vector_ids = self.tasti.get_representative_vector_ids()\n    logger.info(f'The shape of cluster representative ids: {representative_vector_ids.shape}')\n    # get culster representatives ids\n    logger.info(representative_vector_ids)\n    topk_representatives = self.tasti.get_topk_representatives_for_all()\n    # get topk representatives and dists for all data\n    logger.info(topk_representatives)\n\n    # Chroma uses HNSW, which will not return exact search result\n    if self.vd_type == VectorDatabaseType.FAISS.value:\n      for representative_id in list(representative_vector_ids):\n        assert representative_id in topk_representatives.loc[representative_id]['topk_reps']\n\n    new_data, new_vector_ids = self.simulate_user_inserting_new_data(self.data_size)\n    # get topk representatives and dists for new data based on stale representatives\n    logger.info(self.tasti.get_topk_representatives_for_new_embeddings(new_vector_ids))\n    # reselect cluster representatives, recompute topk representatives and dists for all data\n    logger.info(self.tasti.update_topk_representatives_for_all(new_vector_ids))\n    # We can see the old cluster representative is kept\n    logger.info(f'The total number of cluster representatives is: {len(self.tasti.reps)}')\n\n\ndef test(\n    index_name: str,\n    vector_database_type: VectorDatabaseType,\n    data_size: int,\n    embedding_dim: int,\n    nb_buckets: int,\n    index_path: Optional[str] = None,\n    weaviate_auth: Optional[WeaviateAuth] = None\n):\n  tasti_test = TastiTests(index_name, data_size, embedding_dim, nb_buckets, vector_database_type,\n                          weaviate_auth=weaviate_auth, index_path=index_path)\n  tasti_test.test()\n\n\nif __name__ == '__main__':\n    logger.info(f'Running FAISS vector database')\n    test('faiss', VectorDatabaseType.FAISS.value, data_size=10000,\n         embedding_dim=128, nb_buckets=1000, index_path='./')\n\n    logger.info(f'Running Chroma vector database')\n    test('chroma', VectorDatabaseType.CHROMA.value, data_size=10000,\n         embedding_dim=128, nb_buckets=1000, index_path='./')\n\n    # too slow\n    logger.info(f'Running Weaviate vector database')\n    url = ''\n    api_key = os.environ.get('WEAVIATE_API_KEY')\n    weaviate_auth = WeaviateAuth(url, api_key=api_key)\n    test('Weaviate', VectorDatabaseType.WEAVIATE.value, data_size=200,\n         embedding_dim=128, nb_buckets=50, weaviate_auth=weaviate_auth)\n"}
{"type": "test_file", "path": "tests/test_aggregation_gaussian.py", "content": "from multiprocessing import Process\nimport numpy as np\nimport os\nimport pandas as pd\nfrom sqlalchemy.sql import text\nimport time\nimport unittest\nfrom unittest import IsolatedAsyncioTestCase\n\nfrom aidb.query.query import Query\nfrom aidb.utils.logger import logger\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom tests.inference_service_utils.http_inference_service_setup import run_server\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\n\nsetup_test_logger('aggregation_gaussian')\n\nground_truth_dir = './tests/data/gaussian/ground_truth'\ninference_dir = './tests/data/gaussian/inference'\n\nif not os.path.exists(ground_truth_dir):\n    os.makedirs(ground_truth_dir)\nif not os.path.exists(inference_dir):\n    os.makedirs(inference_dir)\n\n_NUMBER_OF_RUNS = int(os.environ.get('AIDB_NUMBER_OF_TEST_RUNS', 100))\n\ndef generate_gaussian_samples(mean, std_dev, num_samples, seed=1234):\n  np.random.seed(seed)\n  samples = np.random.normal(mean, std_dev, num_samples)\n  return samples\n\n\ndef generate_csv_file(mean, std_dev, num_samples):\n  samples = generate_gaussian_samples(mean, std_dev, num_samples)\n  blobs_00 = pd.DataFrame({'pk_blobs_00.id': list(range(num_samples))})\n  blobs_00_csv = os.path.join(ground_truth_dir, 'blobs_00.csv')\n  blobs_00.to_csv(blobs_00_csv, index=False)\n\n  gaussian00 = pd.DataFrame({'pk_blobs_00.id': list(range(num_samples)), 'gaussian00.gaussian': samples})\n  gaussian00_csv = os.path.join(ground_truth_dir, 'gaussian00.csv')\n  gaussian00.to_csv(gaussian00_csv, index=False)\n\n  infer_gaussian00 = pd.DataFrame({'in__blobs_00.id': list(range(num_samples)),\n                                   'out__gaussian00.gaussian': samples,\n                                   'out__gaussian00.id': list(range(num_samples))})\n  infer_gaussian00_csv = os.path.join(inference_dir, 'gaussian00.csv')\n  infer_gaussian00.to_csv(infer_gaussian00_csv, index=False)\n\nDB_URL = \"sqlite+aiosqlite://\"\n\nqueries = [\n  (\n    'approx_aggregate',\n    '''SELECT SUM(gaussian) FROM gaussian00 ERROR_TARGET 5% CONFIDENCE 95%;''',\n    '''SELECT SUM(gaussian) FROM gaussian00;'''\n  ),\n  (\n    'approx_aggregate',\n    '''SELECT AVG(gaussian) FROM gaussian00 ERROR_TARGET 5% CONFIDENCE 95%;''',\n    '''SELECT AVG(gaussian) FROM gaussian00;'''\n  ),\n  (\n    'approx_aggregate',\n    '''SELECT COUNT(gaussian) FROM gaussian00 ERROR_TARGET 5% CONFIDENCE 95%;''',\n    '''SELECT COUNT(gaussian) FROM gaussian00;'''\n  ),\n  (\n    'approx_aggregate',\n    '''SELECT AVG(gaussian), SUM(gaussian) FROM gaussian00 WHERE id > 10000 ERROR_TARGET 5% CONFIDENCE 95%;''',\n    '''SELECT AVG(gaussian), SUM(gaussian) FROM gaussian00 WHERE id > 10000;'''\n  ),\n  (\n    'approx_aggregate',\n    '''SELECT COUNT(gaussian), AVG(gaussian) FROM gaussian00 WHERE id < 100000 ERROR_TARGET 5% CONFIDENCE 95%;''',\n    '''SELECT COUNT(gaussian), AVG(gaussian) FROM gaussian00 WHERE id < 100000;'''\n  ),\n  (\n    'approx_aggregate',\n    '''SELECT AVG(gaussian), SUM(gaussian), COUNT(gaussian) FROM gaussian00 ERROR_TARGET 5% CONFIDENCE 95%;''',\n    '''SELECT AVG(gaussian), SUM(gaussian), COUNT(gaussian) FROM gaussian00;'''\n  )\n]\n\n\nclass AggeregateEngineTests(IsolatedAsyncioTestCase):\n  def _equality_check(self, aidb_res, gt_res, error_target):\n    assert len(aidb_res) == len(gt_res)\n    error_rate_list = []\n    valid_estimation = True\n    for aidb_item, gt_item in zip(aidb_res, gt_res):\n      relative_diff = abs(aidb_item - gt_item) / (gt_item)\n      error_rate_list.append(relative_diff * 100)\n      if relative_diff > error_target:\n        valid_estimation = False\n    logger.info(f'Error rate (%) for approximate aggregation query: {error_rate_list}')\n    return valid_estimation\n\n\n  async def test_agg_query(self):\n    generate_csv_file(100, 100, 1000000)\n    count_list = [0] * len(queries)\n    for i in range(_NUMBER_OF_RUNS):\n      dirname = os.path.dirname(__file__)\n      data_dir = os.path.join(dirname, 'data/gaussian')\n\n      p = Process(target=run_server, args=[str(data_dir)])\n      p.start()\n      time.sleep(1)\n      gt_engine, aidb_engine = await setup_gt_and_aidb_engine(DB_URL, data_dir)\n      register_inference_services(aidb_engine, data_dir)\n      k = 0\n      for query_type, aidb_query, aggregate_query in queries:\n        logger.info(f'Running query {aggregate_query} in ground truth database')\n        async with gt_engine.begin() as conn:\n          gt_res = await conn.execute(text(aggregate_query))\n          gt_res = gt_res.fetchall()[0]\n        logger.info(f'Running query {aidb_query} in aidb database')\n        aidb_res = aidb_engine.execute(aidb_query)[0]\n        logger.info(f'aidb_res: {aidb_res}, gt_res: {gt_res}')\n        error_target = Query(aidb_query, aidb_engine._config).error_target\n        if error_target is None: error_target = 0\n        if self._equality_check(aidb_res, gt_res, error_target):\n          count_list[k] += 1\n        k += 1\n\n      logger.info(f'Time of runs: {i + 1}, Successful count: {count_list}')\n      del gt_engine\n      del aidb_engine\n      p.terminate()\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "test_file", "path": "tests/test_aggregation_gaussian_mk.py", "content": "'''\nThis file is used to test aggregation query with multiple blob keys. It draws samples from gaussian distribution,\nand test the accuracy of approximate aggregation engine for Sum/Count/Avg query.\n'''\nfrom multiprocessing import Process\nimport numpy as np\nimport os\nimport pandas as pd\nfrom sqlalchemy.sql import text\nimport time\nimport unittest\nfrom unittest import IsolatedAsyncioTestCase\n\nfrom aidb.query.query import Query\nfrom aidb.utils.logger import logger\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom tests.inference_service_utils.http_inference_service_setup import run_server\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\n\nsetup_test_logger('aggregation_gaussian_mk')\n\nground_truth_dir = './tests/data/gaussian_mk/ground_truth'\ninference_dir = './tests/data/gaussian_mk/inference'\n\nif not os.path.exists(ground_truth_dir):\n    os.makedirs(ground_truth_dir)\nif not os.path.exists(inference_dir):\n    os.makedirs(inference_dir)\n\n_NUMBER_OF_RUNS = int(os.environ.get('AIDB_NUMBER_OF_TEST_RUNS', 100))\n\ndef generate_gaussian_samples(mean, std_dev, num_samples, seed=1234):\n  np.random.seed(seed)\n  samples = np.random.normal(mean, std_dev, num_samples)\n  return samples\n\n\ndef generate_csv_file(mean, std_dev, num_samples):\n  samples = generate_gaussian_samples(mean, std_dev, num_samples)\n  blobs_00 = pd.DataFrame({'pk_blobs_00.id': list(range(num_samples)),\n      'pk_blobs_00.id2': list(range(100,100+num_samples))})\n\n  blobs_00_csv = os.path.join(ground_truth_dir, 'blobs_00.csv')\n  blobs_00.to_csv(blobs_00_csv, index=False)\n\n  gaussian00 = pd.DataFrame({'pk_blobs_00.id': list(range(num_samples)),\n      'pk_blobs_00.id2': list(range(100,100+num_samples)), 'gaussian00.gaussian': samples})\n\n  gaussian00_csv = os.path.join(ground_truth_dir, 'gaussian00.csv')\n  gaussian00.to_csv(gaussian00_csv, index=False)\n\n  infer_gaussian00 = pd.DataFrame({'in__blobs_00.id': list(range(num_samples)),\n                                   'in__blobs_00.id2': list(range(100, 100+num_samples)),\n                                   'out__gaussian00.gaussian': samples,\n                                   'out__gaussian00.id': list(range(num_samples)),\n                                   'out__gaussian00.id2': range(100, 100+num_samples)})\n  infer_gaussian00_csv = os.path.join(inference_dir, 'gaussian00.csv')\n  infer_gaussian00.to_csv(infer_gaussian00_csv, index=False)\n\nDB_URL = \"sqlite+aiosqlite://\"\n\nqueries = [\n  (\n    'approx_aggregate',\n    '''SELECT SUM(gaussian) FROM gaussian00 ERROR_TARGET 5% CONFIDENCE 95%;''',\n    '''SELECT SUM(gaussian) FROM gaussian00;'''\n  ),\n  (\n    'approx_aggregate',\n    '''SELECT AVG(gaussian) FROM gaussian00 ERROR_TARGET 5% CONFIDENCE 95%;''',\n    '''SELECT AVG(gaussian) FROM gaussian00;'''\n  ),\n  (\n    'approx_aggregate',\n    '''SELECT COUNT(gaussian) FROM gaussian00 ERROR_TARGET 5% CONFIDENCE 95%;''',\n    '''SELECT COUNT(gaussian) FROM gaussian00;'''\n  ),\n  (\n    'approx_aggregate',\n    '''SELECT AVG(gaussian), SUM(gaussian) FROM gaussian00 WHERE id > 10000 ERROR_TARGET 5% CONFIDENCE 95%;''',\n    '''SELECT AVG(gaussian), SUM(gaussian) FROM gaussian00 WHERE id > 10000;'''\n  ),\n  (\n    'approx_aggregate',\n    '''SELECT COUNT(gaussian), AVG(gaussian) FROM gaussian00 WHERE id2 < 100000 ERROR_TARGET 5% CONFIDENCE 95%;''',\n    '''SELECT COUNT(gaussian), AVG(gaussian) FROM gaussian00 WHERE id2 < 100000;'''\n  ),\n  (\n    'approx_aggregate',\n    '''SELECT AVG(gaussian), SUM(gaussian), COUNT(gaussian) FROM gaussian00 ERROR_TARGET 5% CONFIDENCE 95%;''',\n    '''SELECT AVG(gaussian), SUM(gaussian), COUNT(gaussian) FROM gaussian00;'''\n  )\n]\n\n\nclass AggeregateEngineTests(IsolatedAsyncioTestCase):\n  def _equality_check(self, aidb_res, gt_res, error_target):\n    assert len(aidb_res) == len(gt_res)\n    error_rate_list = []\n    valid_estimation = True\n    for aidb_item, gt_item in zip(aidb_res, gt_res):\n      relative_diff = abs(aidb_item - gt_item) / (gt_item)\n      error_rate_list.append(relative_diff * 100)\n      if relative_diff > error_target:\n        valid_estimation = False\n    logger.info(f'Error rate (%) for approximate aggregation query: {error_rate_list}')\n    return valid_estimation\n\n\n  async def test_agg_query(self):\n    generate_csv_file(100, 100, 100000)\n    count_list = [0] * len(queries)\n    for i in range(_NUMBER_OF_RUNS):\n      dirname = os.path.dirname(__file__)\n      data_dir = os.path.join(dirname, 'data/gaussian_mk')\n\n      p = Process(target=run_server, args=[str(data_dir)])\n      p.start()\n      time.sleep(1)\n      gt_engine, aidb_engine = await setup_gt_and_aidb_engine(DB_URL, data_dir)\n      register_inference_services(aidb_engine, data_dir)\n      k = 0\n      for query_type, aidb_query, aggregate_query in queries:\n        logger.info(f'Running query {aggregate_query} in ground truth database')\n        async with gt_engine.begin() as conn:\n          gt_res = await conn.execute(text(aggregate_query))\n          gt_res = gt_res.fetchall()[0]\n        logger.info(f'Running query {aidb_query} in aidb database')\n        aidb_res = aidb_engine.execute(aidb_query)[0]\n        logger.info(f'aidb_res: {aidb_res}, gt_res: {gt_res}')\n        error_target = Query(aidb_query, aidb_engine._config).error_target\n        if error_target is None: error_target = 0\n        if self._equality_check(aidb_res, gt_res, error_target):\n          count_list[k] += 1\n        k += 1\n      logger.info(f'Time of runs: {i + 1}, Successful count: {count_list}')\n      del gt_engine\n      del aidb_engine\n      p.terminate()\n      time.sleep(1)\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "test_file", "path": "tests/test_approx_select_parallel.py", "content": "import multiprocessing as mp\nimport numpy as np\nimport os\nimport pandas as pd\nfrom sqlalchemy.sql import text\nimport time\n\nfrom aidb.utils.asyncio import asyncio_run\nfrom aidb.utils.logger import logger\nfrom aidb.vector_database.tasti import Tasti\nfrom aidb.vector_database.faiss_vector_database import FaissVectorDatabase\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom tests.inference_service_utils.http_inference_service_setup import run_server\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\n\nsetup_test_logger('approx_select_parallel')\n\nDB_URL = 'sqlite+aiosqlite://'\nDATA_SET = 'law'\nBUDGET = 5000\nRECALL_TARGET = 90\n# DB_URL = 'mysql+aiomysql://aidb:aidb@localhost'\n\nasync def test_jackson_number_objects(i):\n  dirname = os.path.dirname(__file__)\n  data_dir = os.path.join(dirname, f'data/{DATA_SET}')\n  p = mp.Process(target=run_server, args=[str(data_dir), i])\n  p.start()\n  time.sleep(1)\n\n  # vector database configuration\n  index_path = './'\n  index_name = f'{DATA_SET}_{i}'\n  embedding = np.load(f'./tests/data/embedding/{DATA_SET}_embeddings.npy')\n  embedding_df = pd.DataFrame({'id': range(embedding.shape[0]), 'values': embedding.tolist()})\n\n  embedding_dim = embedding.shape[1]\n  user_database = FaissVectorDatabase(index_path)\n  user_database.create_index(index_name, embedding_dim, recreate_index=True)\n  user_database.insert_data(index_name, embedding_df)\n  seed = mp.current_process().pid\n  tasti = Tasti(index_name, user_database, BUDGET, seed=seed)\n\n  count = 0\n  gt_engine, aidb_engine = await setup_gt_and_aidb_engine(DB_URL, data_dir, tasti, i)\n\n\n  register_inference_services(aidb_engine, data_dir, i)\n  queries = [\n    (\n      f'''SELECT entity_id FROM entities00 where type LIKE 'PERSON'\n                RECALL_TARGET {RECALL_TARGET}% CONFIDENCE 95%;''',\n      '''SELECT entity_id FROM entities00 where type LIKE 'PERSON';'''\n    ),\n  ]\n\n  for aidb_query, exact_query in queries:\n    logger.info(f'Running query {aidb_query} in approx select engine')\n    seed = (mp.current_process().pid * np.random.randint(100000, size=1)[0]) % (2 ** 32 - 1)\n    aidb_res = aidb_engine.execute(aidb_query, __seed=seed)\n\n    logger.info(f'Running query {exact_query} in ground truth database')\n    async with gt_engine.begin() as conn:\n      gt_res = await conn.execute(text(exact_query))\n      gt_res = gt_res.fetchall()\n\n    if len(aidb_res) / len(gt_res) > RECALL_TARGET / 100:\n      count += 1\n\n  logger.info(f'AIDB_res: {len(aidb_res)}, gt_res:{len(gt_res)}, Recall: {len(aidb_res) / len(gt_res)},'\n              f' port:{i}, Count: {count}')\n  del gt_engine\n  del aidb_engine\n  p.terminate()\n\n\ndef run_async_outer_process(port):\n  try:\n    asyncio_run(test_jackson_number_objects(port))\n  except Exception as e:\n    raise e\n\n\nif __name__ == '__main__':\n  processes = []\n  ports = range(8000, 8010)\n\n  for port in ports:\n    # Create a non-daemonic process for each port\n    process = mp.Process(target=run_async_outer_process, args=(port,))\n    processes.append(process)\n    process.start()\n\n  # Wait for all processes to complete\n  for process in processes:\n    process.join()"}
{"type": "test_file", "path": "tests/tests_data_store.py", "content": "import unittest\nimport os\nimport sqlalchemy\nimport sqlalchemy.ext.asyncio\nfrom aidb_utilities.blob_store.local_storage import LocalImageBlobStore, LocalDocumentBlobStore\nfrom aidb_utilities.blob_store.aws_s3_storage import AwsS3ImageBlobStore\nfrom aidb_utilities.db_setup.blob_table import BaseTablesSetup\nfrom aidb.utils.constants import BLOB_TABLE_NAMES_TABLE\n\nfrom unittest import IsolatedAsyncioTestCase\nfrom sqlalchemy.sql import text\n\nDB_URL = 'sqlite+aiosqlite:///aidb_datastore.sqlite'\n\n\nasync def setup_blob_tables(input_blobs):\n  base_table_setup = BaseTablesSetup(DB_URL)\n  blob_table = 'blob00'\n  base_table_setup.insert_blob_meta_data(blob_table, input_blobs, ['blob_id'])\n  engine = sqlalchemy.ext.asyncio.create_async_engine(DB_URL)\n  async with engine.begin() as conn:\n    result = await conn.execute(text(f'SELECT * FROM {blob_table}'))\n    total_blobs = result.fetchall()\n    result = await conn.execute(text(f'SELECT * FROM {BLOB_TABLE_NAMES_TABLE}'))\n    total_blob_keys = result.fetchall()\n  assert len(total_blobs) == len(input_blobs)\n  assert len(total_blob_keys) == 1\n\n\ndef clean_resources():\n  if os.path.exists('aidb_datastore.sqlite'):\n    os.remove('aidb_datastore.sqlite')\n\n\nclass AidbDataStoreTests(IsolatedAsyncioTestCase):\n\n  async def test_local_image_storage_positive(self):\n    clean_resources()\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/image_data_store/')\n    local_image_store = LocalImageBlobStore(data_dir)\n    image_blobs = local_image_store.get_blobs()\n    await setup_blob_tables(image_blobs)\n\n  async def test_local_document_storage_positive(self):\n    clean_resources()\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/document_data_store/')\n    local_document_store = LocalDocumentBlobStore(data_dir)\n    document_blobs = local_document_store.get_blobs()\n    await setup_blob_tables(document_blobs)\n\n  @unittest.skip(\"Skip in case of absence of AWS credentials\")\n  async def test_aws_image_storage_positive(self):\n    clean_resources()\n    aws_image_store = AwsS3ImageBlobStore('bucket-name', '<your-aws-access-key>', 'your-secret-key')\n    image_blobs = aws_image_store.get_blobs()\n    await setup_blob_tables(image_blobs)\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "test_file", "path": "tests/tests_db_setup.py", "content": "import os\nimport unittest\n\nfrom unittest import IsolatedAsyncioTestCase\nfrom tests.db_utils.db_setup import create_db, setup_db, setup_config_tables\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom aidb.engine import Engine\n\nPOSTGRESQL_URL = 'postgresql+asyncpg://user:testaidb@localhost:5432'\nSQLITE_URL = 'sqlite+aiosqlite://'\nMYSQL_URL = 'mysql+aiomysql://root:testaidb@localhost:3306'\n\nclass DbSetupTests(IsolatedAsyncioTestCase):\n  async def test_positive(self):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n    db_url_list = [SQLITE_URL, POSTGRESQL_URL, MYSQL_URL]\n    for db_url in db_url_list:\n      # Set up the aidb database\n      if db_url == SQLITE_URL:\n        aidb_db_fname = 'aidb_test.sqlite'\n      else:\n        aidb_db_fname = 'aidb_test'\n\n      await create_db(db_url, aidb_db_fname)\n\n      tmp_engine = await setup_db(db_url, aidb_db_fname, data_dir)\n      try:\n        async with tmp_engine.begin() as conn:\n          await setup_config_tables(conn)\n      except Exception:\n        raise Exception('Fail to setup config table.')\n      finally:\n        await tmp_engine.dispose()\n\n      del tmp_engine\n\n      # Connect to the aidb database\n      aidb_engine = Engine(\n        f'{db_url}/{aidb_db_fname}',\n        debug=False,\n      )\n      register_inference_services(aidb_engine, data_dir)\n      del aidb_engine\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "test_file", "path": "tests/tests_full_scan_engine.py", "content": "import os\nimport time\nimport unittest\n\nfrom collections import Counter\nfrom unittest import IsolatedAsyncioTestCase\nfrom sqlalchemy.sql import text\n\nfrom aidb.utils.logger import logger\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom tests.inference_service_utils.http_inference_service_setup import run_server\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\n\nfrom multiprocessing import Process\n\nsetup_test_logger('full_scan_engine')\n\nPOSTGRESQL_URL = 'postgresql+asyncpg://user:testaidb@localhost:5432'\nSQLITE_URL = 'sqlite+aiosqlite://'\nMYSQL_URL = 'mysql+aiomysql://root:testaidb@localhost:3306'\n\nclass FullScanEngineTests(IsolatedAsyncioTestCase):\n\n  async def test_jackson_number_objects(self):\n\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n\n    p = Process(target=run_server, args=[str(data_dir)])\n    p.start()\n    time.sleep(1)\n\n    queries = [\n      (\n        'full_scan',\n        '''\n        SELECT *\n        FROM objects00\n        WHERE (frame >= 100 AND frame <= 300) OR (x_min < 100 AND y_min > 600) AND NOT (frame >= 2000)\n        ''',\n        '''\n        SELECT *\n        FROM objects00\n        WHERE (frame >= 100 AND frame <= 300) OR (x_min < 100 AND y_min > 600) AND NOT (frame >= 2000)\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT frame\n        FROM objects00\n        WHERE objects00.frame > 200 OR objects00.frame = 700 AND objects00.frame = 1000 OR objects00.frame = 1\n        ''',\n        '''\n        SELECT frame\n        FROM objects00\n        WHERE objects00.frame > 200 OR objects00.frame = 700 AND objects00.frame = 1000 OR objects00.frame = 1\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT *\n        FROM objects00\n        WHERE (x_max > 500 OR (y_min < 250 AND y_max > 750)) AND NOT (frame >= 300 OR frame <= 800)   \n        ''',\n        '''\n        SELECT *\n        FROM objects00\n        WHERE (x_max > 500 OR (y_min < 250 AND y_max > 750)) AND NOT (frame >= 300 OR frame <= 800)   \n        '''\n      ),\n      (\n        'full_scan',\n        '''SELECT * FROM objects00 WHERE object_name='car' AND frame < 100;''',\n        '''SELECT * FROM objects00 WHERE object_name='car' AND frame < 100;'''\n      ),\n      (\n        'full_scan',\n        '''SELECT * FROM counts03 WHERE frame >= 10000;''',\n        '''SELECT * FROM counts03 WHERE frame >= 10000;''',\n      ),\n      (\n        'full_scan',\n        '''SELECT frame, light_1, light_2 FROM lights01 WHERE light_2='green';''',\n        '''SELECT frame, light_1, light_2 FROM lights01 WHERE light_2='green';'''\n      ),\n      (\n        'full_scan',\n        '''SELECT * FROM objects00 WHERE object_name='car' OR frame < 100;''',\n        '''SELECT * FROM objects00 WHERE object_name='car' OR frame < 100;'''\n      ),\n      (\n        'full_scan',\n        '''SELECT Avg(x_min) FROM objects00 GROUP BY objects00.object_id;''',\n        '''SELECT Avg(x_min) FROM objects00 GROUP BY objects00.object_id;'''\n      ),\n      (\n        'full_scan',\n        '''SELECT * FROM objects00 join colors02 on objects00.frame = colors02.frame\n           and objects00.object_id = colors02.object_id;''',\n\n        '''SELECT * FROM objects00 join colors02 on objects00.frame = colors02.frame\n           and objects00.object_id = colors02.object_id;'''\n      ),\n      (\n        'full_scan',\n        '''SELECT color AS col1, table2.x_min AS col2, table2.y_min AS col3\n           FROM colors02 table1 LEFT JOIN objects00 table2 ON table1.frame = table2.frame;''',\n        '''SELECT color AS col1, table2.x_min AS col2, table2.y_min AS col3\n           FROM colors02 table1 LEFT JOIN objects00 table2 ON table1.frame = table2.frame;'''\n      ),\n      (\n        'full_scan',\n        '''SELECT color AS col1, table2.x_min AS col2, table3.frame AS col3\n           FROM colors02 table1 LEFT JOIN objects00 table2 ON table1.frame = table2.frame\n           JOIN blobs_00 table3 ON table2.frame = table3.frame;''',\n        '''SELECT color AS col1, table2.x_min AS col2, table3.frame AS col3\n           FROM colors02 table1 LEFT JOIN objects00 table2 ON table1.frame = table2.frame\n           JOIN blobs_00 table3 ON table2.frame = table3.frame;'''\n      ),\n      (\n        'full_scan',\n        '''SELECT color, x_min AS col2, colors02.frame AS col3\n           FROM colors02 JOIN objects00 table2 ON colors02.frame = table2.frame\n           WHERE color = 'blue' AND x_min > 600;''',\n        '''SELECT color, x_min AS col2, colors02.frame AS col3\n           FROM colors02 JOIN objects00 table2 ON colors02.frame = table2.frame\n           WHERE color = 'blue' AND x_min > 600;'''\n      )\n    ]\n\n    db_url_list = [MYSQL_URL, SQLITE_URL, POSTGRESQL_URL]\n    for db_url in db_url_list:\n      dialect = db_url.split('+')[0]\n      logger.info(f'Test {dialect} database')\n      gt_engine, aidb_engine = await setup_gt_and_aidb_engine(db_url, data_dir)\n\n      register_inference_services(aidb_engine, data_dir)\n\n      for query_type, aidb_query, exact_query in queries:\n        logger.info(f'Running query {exact_query} in ground truth database')\n        # Run the query on the ground truth database\n        try:\n          async with gt_engine.begin() as conn:\n            gt_res = await conn.execute(text(exact_query))\n            gt_res = gt_res.fetchall()\n        finally:\n          await gt_engine.dispose()\n        # Run the query on the aidb database\n        logger.info(f'Running query {aidb_query} in aidb database')\n        aidb_res = aidb_engine.execute(aidb_query)\n        assert len(gt_res) == len(aidb_res)\n        assert sorted(gt_res) == sorted(aidb_res)\n      del gt_engine\n      del aidb_engine\n    p.terminate()\n\n\n  async def test_multi_table_input(self):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/multi_table_input')\n\n    p = Process(target=run_server, args=[str(data_dir)])\n    p.start()\n\n    queries = [\n      (\n        'full_scan',\n        '''SELECT tweet_id, sentiment FROM blobs_00;''',\n        '''SELECT tweet_id, sentiment FROM blobs_00;''',\n      )\n    ]\n\n    db_url_list = [MYSQL_URL, SQLITE_URL, POSTGRESQL_URL]\n    for db_url in db_url_list:\n      dialect = db_url.split('+')[0]\n      logger.info(f'Test {dialect} database')\n      gt_engine, aidb_engine = await setup_gt_and_aidb_engine(db_url, data_dir)\n\n      register_inference_services(aidb_engine, data_dir)\n\n      for query_type, aidb_query, exact_query in queries:\n        logger.info(f'Running query {exact_query} in ground truth database')\n        # Run the query on the ground truth database\n        try:\n          async with gt_engine.begin() as conn:\n            gt_res = await conn.execute(text(exact_query))\n            gt_res = gt_res.fetchall()\n        finally:\n          await gt_engine.dispose()\n        # Run the query on the aidb database\n        logger.info(f'Running query {aidb_query} in aidb database')\n        aidb_res = aidb_engine.execute(aidb_query)\n        assert len(gt_res) == len(aidb_res)\n        assert sorted(gt_res) == sorted(aidb_res)\n      del gt_engine\n    p.terminate()\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "test_file", "path": "tests/test_approx_select.py", "content": "import multiprocessing as mp\nimport numpy as np\nimport os\nimport pandas as pd\nimport time\nimport unittest\n\nfrom sqlalchemy.sql import text\nfrom unittest import IsolatedAsyncioTestCase\n\nfrom aidb.vector_database.faiss_vector_database import FaissVectorDatabase\nfrom aidb.vector_database.tasti import Tasti\nfrom aidb.utils.logger import logger\nfrom tests.inference_service_utils.http_inference_service_setup import run_server\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\n\nsetup_test_logger('approx_select')\n\nPOSTGRESQL_URL = 'postgresql+asyncpg://user:testaidb@localhost:5432'\nSQLITE_URL = 'sqlite+aiosqlite://'\nMYSQL_URL = 'mysql+aiomysql://root:testaidb@localhost:3306'\n\nDATA_SET = 'law'\nBUDGET = 5000\nRECALL_TARGET = 90\n\nclass LimitEngineTests(IsolatedAsyncioTestCase):\n\n  async def test_jackson_number_objects(self):\n\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, f'data/{DATA_SET}')\n    p = mp.Process(target=run_server, args=[str(data_dir)])\n    p.start()\n    time.sleep(1)\n\n    # vector database configuration\n    index_path = './'\n    index_name = DATA_SET\n    embedding = np.load(f'./tests/data/embedding/{DATA_SET}_embeddings.npy')\n    embedding_df = pd.DataFrame({'id': range(embedding.shape[0]), 'values': embedding.tolist()})\n\n    embedding_dim = embedding.shape[1]\n    user_database = FaissVectorDatabase(index_path)\n    user_database.create_index(index_name, embedding_dim, recreate_index=True)\n    user_database.insert_data(index_name, embedding_df)\n    seed = mp.current_process().pid\n    tasti = Tasti(index_name, user_database, BUDGET, seed=seed)\n\n    queries = [\n      (\n        f'''SELECT entity_id FROM entities00 where type LIKE 'PERSON'\n                    RECALL_TARGET {RECALL_TARGET}% CONFIDENCE 95%;''',\n        '''SELECT entity_id FROM entities00 where type LIKE 'PERSON';'''\n      ),\n      (\n        f'''SELECT entity_id FROM entities00 where type IN (SELECT type FROM entities00 WHERE blob_id < 5)\n                  RECALL_TARGET {RECALL_TARGET}% CONFIDENCE 95%;''',\n        '''SELECT entity_id FROM entities00 where type IN (SELECT type FROM entities00 WHERE blob_id < 5);'''\n      ),\n    ]\n\n    db_url_list = [MYSQL_URL, POSTGRESQL_URL, SQLITE_URL]\n    for db_url in db_url_list:\n      dialect = db_url.split('+')[0]\n      logger.info(f'Test {dialect} database')\n      count_list = [0] * len(queries)\n      for i in range(100):\n        gt_engine, aidb_engine = await setup_gt_and_aidb_engine(db_url, data_dir, tasti)\n\n        register_inference_services(aidb_engine, data_dir)\n        k = 0\n        for aidb_query, exact_query in queries:\n          logger.info(f'Running query {aidb_query} in approx select engine')\n          seed = (mp.current_process().pid * np.random.randint(100000, size=1)[0]) % (2**32 - 1)\n          aidb_res = aidb_engine.execute(aidb_query, __seed=seed)\n\n          logger.info(f'Running query {exact_query} in ground truth database')\n          try:\n            async with gt_engine.begin() as conn:\n              gt_res = await conn.execute(text(exact_query))\n              gt_res = gt_res.fetchall()\n          finally:\n            await gt_engine.dispose()\n\n          if len(aidb_res) / len(gt_res) > RECALL_TARGET / 100:\n            count_list[k] += 1\n          k += 1\n          logger.info(f'AIDB_res: {len(aidb_res)}, gt_res:{len(gt_res)}, Recall: {len(aidb_res) / len(gt_res)},'\n                       f' Times of trial:{i + 1}, Count: {count_list}')\n\n        del gt_engine\n        del aidb_engine\n    p.terminate()\n\n\nif __name__ == '__main__':\n  unittest.main()"}
{"type": "test_file", "path": "tests/tests_caching_logic.py", "content": "from multiprocessing import Process\nimport os\nfrom sqlalchemy.sql import text\nimport time\nimport unittest\nfrom unittest import IsolatedAsyncioTestCase\n\nfrom aidb.utils.logger import logger\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom tests.inference_service_utils.http_inference_service_setup import run_server\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\n\nsetup_test_logger('caching_logic')\n\nPOSTGRESQL_URL = 'postgresql+asyncpg://user:testaidb@localhost:5432'\nSQLITE_URL = 'sqlite+aiosqlite://'\nMYSQL_URL = 'mysql+aiomysql://root:testaidb@localhost:3306'\n\nclass CachingLogic(IsolatedAsyncioTestCase):\n\n  async def test_num_infer_calls(self):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n\n    p = Process(target=run_server, args=[str(data_dir)])\n    p.start()\n    time.sleep(1)\n    db_url_list = [POSTGRESQL_URL]\n    for db_url in db_url_list:\n      gt_engine, aidb_engine = await setup_gt_and_aidb_engine(db_url, data_dir)\n  \n      register_inference_services(aidb_engine, data_dir, batch_supported=False)\n  \n      queries = [\n        (\n          'full_scan',\n          '''SELECT * FROM objects00 WHERE object_name='car' AND frame < 300;''',\n          '''SELECT * FROM objects00 WHERE object_name='car' AND frame < 300;'''\n        ),\n        (\n          'full_scan',\n          '''SELECT * FROM objects00 WHERE object_name='car' AND frame < 400;''',\n          '''SELECT * FROM objects00 WHERE object_name='car' AND frame < 400;'''\n        ),\n      ]\n  \n      # no service calls before executing query\n      assert aidb_engine._config.inference_services[\"objects00\"].infer_one.calls == 0\n  \n      calls = [20, 27]\n      for index, (query_type, aidb_query, exact_query) in enumerate(queries):\n        logger.info(f'Running query {exact_query} in ground truth database')\n        # Run the query on the ground truth database\n        async with gt_engine.begin() as conn:\n          gt_res = await conn.execute(text(exact_query))\n          gt_res = gt_res.fetchall()\n        # Run the query on the aidb database\n        logger.info(f'Running query {aidb_query} in aidb database')\n        aidb_res = aidb_engine.execute(aidb_query)\n        assert len(gt_res) == len(aidb_res)\n        # running the same query, so number of inference calls should remain same\n        # temporarily commenting this out because we no longer call infer_one\n        assert aidb_engine._config.inference_services[\"objects00\"].infer_one.calls == calls[index]\n      del gt_engine\n      del aidb_engine\n    p.terminate()\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "test_file", "path": "tests/test_queries.py", "content": "import unittest\nfrom enum import Enum\n\nfrom aidb.query.query import Query\n\n# Valid\nvalid_avg_sql = '''\nSELECT AVG(bar)\nFROM foo\nERROR_TARGET 1%\nCONFIDENCE 95%;\n'''\n\nvalid_count_sql = '''\nSELECT COUNT(bar)\nFROM foo\nERROR_TARGET 1%\nCONFIDENCE 95%;\n'''\n\nvalid_sum_sql = '''\nSELECT SUM(bar)\nFROM foo\nERROR_TARGET 1%\nCONFIDENCE 95%;\n'''\n\n# Invalid\nunsupported_agg_aqp_sql = '''\nSELECT MAX(bar)\nFROM foo\nERROR_TARGET 1%\nCONFIDENCE 95%;\n'''\n\nagg_no_et_sql = '''\nSELECT AVG(bar)\nFROM foo\nCONFIDENCE 95%;\n'''\n\nagg_no_conf_sql = '''\nSELECT AVG(bar)\nFROM foo\nERROR_TARGET 1%;\n'''\n\nnot_select_sql = '''\nINSERT INTO employees (id, name)\nVALUES (101, 'John');\n'''\n\napproximate_agg_group_by_sql = '''\nSELECT AVG(bar)\nFROM foo\nGROUP BY (frame)\nERROR_TARGET 1%\nCONFIDENCE 95%;\n'''\n\napproximate_agg_join_sql = '''\nSELECT AVG(bar)\nFROM foo join joo\non foo.x = joo.x\nERROR_TARGET 1%\nCONFIDENCE 95%;\n'''\n\n\nclass AQPKeywordTests(unittest.TestCase):\n  def test_confidence(self):\n    query = Query(valid_avg_sql, None)\n    confidence = query.confidence\n    self.assertEqual(confidence, 95)\n\n  def test_error_target(self):\n    query = Query(valid_avg_sql, None)\n    error_target = query.error_target\n    self.assertEqual(error_target, 0.01)\n\n\nclass AQPValidationTests(unittest.TestCase):\n  # Valid queries\n  def test_valid_aqp(self):\n    query = Query(valid_avg_sql, None)\n    self.assertEqual(query.is_valid_aqp_query(), True)\n\n\n  def test_valid_aqp_join(self):\n    query = Query(approximate_agg_join_sql, None)\n    self.assertEqual(query.is_valid_aqp_query(), True)\n\n\n  # Below is a list of tests designed to assess invalid queries\n  def test_invalid_count_sql(self):\n    query = Query(valid_count_sql, None)\n    self.assertEqual(query.is_valid_aqp_query(), True)\n\n\n  def test_invalid_sum_sql(self):\n    query = Query(valid_sum_sql, None)\n    self.assertEqual(query.is_valid_aqp_query(), True)\n\n\n  def test_invalid_unsupported_agg_aqp(self):\n    query = Query(unsupported_agg_aqp_sql, None)\n    with self.assertRaises(Exception):\n      query.is_valid_aqp_query()\n\n  def test_invalid_agg_no_et(self):\n    query = Query(agg_no_et_sql, None)\n    with self.assertRaises(Exception):\n      query.is_valid_aqp_query()\n\n  def test_invalid_agg_no_conf(self):\n    query = Query(agg_no_conf_sql, None)\n    with self.assertRaises(Exception):\n      query.is_valid_aqp_query()\n\n  def test_invalid_not_select_sql(self):\n    query = Query(not_select_sql, None)\n    with self.assertRaises(Exception):\n      query.is_valid_aqp_query()\n\n  def test_invalid_approximate_agg_group_by_sql(self):\n    query = Query(approximate_agg_group_by_sql, None)\n    with self.assertRaises(Exception):\n      query.is_valid_aqp_query()\n\n\n# This class tests the correctness of the parsed type for a query.\nclass QueryTypeTests(unittest.TestCase):\n  class QueryType(Enum):\n    APPROX_AGG = 'approx_agg'\n    APPROX_AGG_JOIN = 'approx_agg_join'\n    APPROX_SELECT = 'approx_select'\n    LIMIT_QUERY = 'limit_query'\n    SELECT_QUERY = 'select_query'\n    NON_SELECT = 'none_select'\n\n\n  def _test_query_type_equality(self, query_str, query_type):\n    query = Query(query_str, None)\n    if query.is_approx_agg_query:\n      if query.is_aqp_join_query:\n        parsed_type = self.QueryType.APPROX_AGG_JOIN\n      else:\n        parsed_type = self.QueryType.APPROX_AGG\n    elif query.is_approx_select_query:\n      parsed_type = self.QueryType.APPROX_SELECT\n    elif query.is_limit_query():\n      parsed_type = self.QueryType.LIMIT_QUERY\n    elif query.is_select_query():\n      parsed_type = self.QueryType.SELECT_QUERY\n    else:\n      parsed_type = self.QueryType.NON_SELECT\n\n    if parsed_type != query_type:\n      raise AssertionError(f\"Failed to parse query: '{query_str}'. \\n\"\n                           f\"Parsed type: '{parsed_type}', but the expected type is '{query_type}'.\")\n\n\n  def test_query_type(self):\n    approx_agg_sql = [\n      (\n        self.QueryType.APPROX_AGG,\n        '''\n        SELECT AVG(col1)\n        FROM table1  \n        ERROR_TARGET 5%\n        CONFIDENCE 95%;\n        ''',\n      ),\n      (\n        self.QueryType.APPROX_AGG_JOIN,\n        '''\n        SELECT COUNT(*)\n        FROM table1 CROSS JOIN table2\n        WHERE function1(table1.col1, table2.col1) = TRUE\n        ERROR_TARGET 5%\n        CONFIDENCE 95%;\n        ''',\n      ),\n      (\n        self.QueryType.APPROX_SELECT,\n        '''\n        SELECT col1\n        FROM table1\n        RECALL_TARGET 10%\n        CONFIDENCE 95%;\n        ''',\n      ),\n      (\n        self.QueryType.LIMIT_QUERY,\n        '''\n        SELECT col1\n        FROM table1\n        LIMIT 1000;\n        ''',\n      ),\n      (\n        self.QueryType.SELECT_QUERY,\n        '''\n        SELECT col1\n        FROM table1\n        WHERE col2 > 1000;\n        ''',\n      ),\n      (\n        self.QueryType.SELECT_QUERY,\n        '''\n        SELECT AVG(table1.t1_col1), COUNT(table2.t2_col2)\n        FROM table1 JOIN table2\n        ON table1.t1_col1 = table2.t2_col1\n        ''',\n      ),\n      (\n        self.QueryType.NON_SELECT,\n        '''\n        CREATE TABLE Employees (\n        EmployeeID INT PRIMARY KEY,\n        FirstName VARCHAR(50),\n        LastName VARCHAR(50),\n        );\n        ''',\n      ),\n    ]\n\n    for query_type, query_str in approx_agg_sql:\n      self._test_query_type_equality(query_str, query_type)\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "test_file", "path": "tests/test_limit_engine.py", "content": "from multiprocessing import Process\nimport os\nfrom sqlalchemy.sql import text\nimport time\nimport unittest\nfrom unittest import IsolatedAsyncioTestCase\n\nfrom aidb.utils.logger import logger\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom tests.inference_service_utils.http_inference_service_setup import run_server\nfrom tests.tasti_test.tasti_test import TastiTests, VectorDatabaseType\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\n\nsetup_test_logger('limit_engine')\n\nPOSTGRESQL_URL = 'postgresql+asyncpg://user:testaidb@localhost:5432'\nSQLITE_URL = 'sqlite+aiosqlite://'\nMYSQL_URL = 'mysql+aiomysql://root:testaidb@localhost:3306'\n\nclass LimitEngineTests(IsolatedAsyncioTestCase):\n\n  async def test_jackson_number_objects(self):\n\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n    p = Process(target=run_server, args=[str(data_dir)])\n    p.start()\n    time.sleep(1)\n\n    # vector database configuration\n    index_name = 'tasti'\n    data_size = 1000\n    embedding_dim = 128\n    nb_buckets = 100\n    vector_database_type = VectorDatabaseType.FAISS.value\n\n    tasti_test = TastiTests(index_name, data_size, embedding_dim, nb_buckets, vector_database_type, index_path='./')\n    tasti_index = tasti_test.tasti\n\n    queries = [\n      (\n        '''SELECT * FROM colors02 WHERE frame >= 1000 and colors02.color = 'black' LIMIT 100;''',\n        '''SELECT * FROM colors02 WHERE frame >= 1000 and colors02.color = 'black';'''\n      ),\n      (\n        '''SELECT frame, light_1, light_2 FROM lights01 WHERE light_2 = 'green' LIMIT 100;''',\n        '''SELECT frame, light_1, light_2 FROM lights01 WHERE light_2 = 'green';'''\n      ),\n      (\n        '''SELECT * FROM objects00 WHERE object_name = 'car' OR frame < 100 LIMIT 100;''',\n        '''SELECT * FROM objects00 WHERE object_name = 'car' OR frame < 100;'''\n      )\n    ]\n    db_url_list = [MYSQL_URL, SQLITE_URL, POSTGRESQL_URL]\n    for db_url in db_url_list:\n      dialect = db_url.split('+')[0]\n      logger.info(f'Test {dialect} database')\n      for aidb_query, exact_query in queries:\n        logger.info(f'Running query {aidb_query} in limit engine')\n\n        gt_engine, aidb_engine = await setup_gt_and_aidb_engine(db_url, data_dir, tasti_index)\n        register_inference_services(aidb_engine, data_dir)\n        aidb_res = aidb_engine.execute(aidb_query)\n\n        logger.info(f'Running query {exact_query} in ground truth database')\n        try:\n          async with gt_engine.begin() as conn:\n            gt_res = await conn.execute(text(exact_query))\n            gt_res = gt_res.fetchall()\n        finally:\n          await gt_engine.dispose()\n\n        logger.info(f'There are {len(aidb_res)} elements in limit engine results '\n              f'and {len(gt_res)} elements in ground truth results')\n        assert len(set(aidb_res) - set(gt_res)) == 0\n\n      del gt_engine\n      del aidb_engine\n    p.terminate()\n\n\nif __name__ == '__main__':\n  unittest.main()"}
{"type": "test_file", "path": "tests/test_aggregation.py", "content": "import random\r\nfrom multiprocessing import Process\r\nimport os\r\nfrom sqlalchemy.sql import text\r\nimport time\r\nimport unittest\r\nfrom unittest import IsolatedAsyncioTestCase\r\n\r\nfrom aidb.query.query import Query\r\nfrom aidb.utils.logger import logger\r\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\r\nfrom tests.inference_service_utils.http_inference_service_setup import run_server\r\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\r\n\r\nsetup_test_logger('aggregation')\r\n\r\nPOSTGRESQL_URL = 'postgresql+asyncpg://user:testaidb@localhost:5432'\r\nSQLITE_URL = 'sqlite+aiosqlite://'\r\nMYSQL_URL = 'mysql+aiomysql://root:testaidb@localhost:3306'\r\n\r\n# note: Adjust the AIDB_NUMBER_OF_TEST_RUNS setting for more extensive local testing,\r\n# as it's currently configured for only two runs in GitHub Actions,\r\n# which may not suffice for thorough reliability and functionality checks\r\n_NUMBER_OF_RUNS = int(os.environ.get('AIDB_NUMBER_OF_TEST_RUNS', 100))\r\n\r\nqueries = [\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT SUM(x_min), COUNT(frame) FROM objects00 WHERE x_min > 1000 ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT SUM(x_min), COUNT(frame) FROM objects00 WHERE x_min > 1000;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT AVG(x_min), COUNT(*) FROM objects00 WHERE x_min > 1000 ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT AVG(x_min), COUNT(*) FROM objects00 WHERE x_min > 1000;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT SUM(x_min), SUM(y_max), AVG(x_max), COUNT(*) FROM objects00\r\n           WHERE y_min > 500 ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT SUM(x_min), SUM(y_max), AVG(x_max), COUNT(*) FROM objects00 WHERE y_min > 500;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT SUM(x_min), SUM(y_max), SUM(x_max), SUM(y_min) FROM objects00\r\n           WHERE x_min < 1000 ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT SUM(x_min), SUM(y_max), SUM(x_max), SUM(y_min) FROM objects00 WHERE x_min < 1000;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT AVG(x_min), SUM(y_max), AVG(x_max), SUM(y_min) FROM objects00\r\n           WHERE frame > 100000 ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT AVG(x_min), SUM(y_max), AVG(x_max), SUM(y_min) FROM objects00\r\n           WHERE frame > 100000;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT COUNT(x_min), SUM(y_max), COUNT(x_max), AVG(y_min) FROM objects00\r\n           WHERE x_min > 700 AND y_min > 700 ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT COUNT(x_min), SUM(y_max), COUNT(x_max), AVG(y_min) FROM objects00 WHERE x_min > 700 AND y_min > 700;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT SUM(x_min) FROM objects00 WHERE x_min > 1000 ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT SUM(x_min) FROM objects00 WHERE x_min > 1000;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT COUNT(x_min) FROM objects00 WHERE x_min > 1000 ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT COUNT(x_min) FROM objects00 WHERE x_min > 1000;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT SUM(x_min) FROM objects00 ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT SUM(x_min) FROM objects00;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT SUM(y_min) FROM objects00 ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT SUM(y_min) FROM objects00;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT COUNT(x_min) FROM objects00 ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT COUNT(x_min) FROM objects00;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT AVG(x_min) FROM objects00 ERROR_TARGET 5% CONFIDENCE 95%;''',\r\n    '''SELECT AVG(x_min) FROM objects00;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT AVG(x_max) FROM objects00 ERROR_TARGET 5% CONFIDENCE 95%;''',\r\n    '''SELECT AVG(x_max) FROM objects00;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT AVG(x_min) FROM objects00 WHERE x_min > 1000 ERROR_TARGET 5% CONFIDENCE 95%;''',\r\n    '''SELECT AVG(x_min) FROM objects00 WHERE x_min > 1000;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT AVG(x_min) FROM objects00 WHERE y_max < 900 ERROR_TARGET 5% CONFIDENCE 95%;''',\r\n    '''SELECT AVG(x_min) FROM objects00 WHERE y_max < 900;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT AVG(x_min) FROM objects00 WHERE x_min < 700 ERROR_TARGET 5% CONFIDENCE 95%;''',\r\n    '''SELECT AVG(x_min) FROM objects00 WHERE x_min < 700;'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT SUM(x_min) FROM objects00 WHERE frame > (SELECT AVG(frame) FROM blobs_00) ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT SUM(x_min) FROM objects00 WHERE frame > (SELECT AVG(frame) FROM blobs_00);'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT AVG(x_min) FROM objects00 WHERE frame > (SELECT AVG(frame) FROM blobs_00) ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT AVG(x_min) FROM objects00 WHERE frame > (SELECT AVG(frame) FROM blobs_00);'''\r\n  ),\r\n  (\r\n    'approx_aggregate',\r\n    '''SELECT COUNT(x_min) FROM objects00 WHERE frame > (SELECT AVG(frame) FROM blobs_00) ERROR_TARGET 10% CONFIDENCE 95%;''',\r\n    '''SELECT COUNT(x_min) FROM objects00 WHERE frame > (SELECT AVG(frame) FROM blobs_00);'''\r\n  ),\r\n]\r\n\r\n\r\nclass AggeregateEngineTests(IsolatedAsyncioTestCase):\r\n  def _equality_check(self, aidb_res, gt_res, error_target):\r\n    assert len(aidb_res) == len(gt_res)\r\n    error_rate_list = []\r\n    valid_estimation = True\r\n    for aidb_item, gt_item in zip(aidb_res, gt_res):\r\n      relative_diff = abs(aidb_item - gt_item) / (gt_item)\r\n      error_rate_list.append(relative_diff * 100)\r\n      if relative_diff > error_target:\r\n        valid_estimation = False\r\n    logger.info(f'Error rate (%) for approximate aggregation query: {error_rate_list}')\r\n    return valid_estimation\r\n\r\n\r\n  async def test_agg_query(self):\r\n    dirname = os.path.dirname(__file__)\r\n    data_dir = os.path.join(dirname, 'data/jackson_all')\r\n\r\n    p = Process(target=run_server, args=[str(data_dir)])\r\n    p.start()\r\n    time.sleep(1)\r\n    db_url_list = [MYSQL_URL, POSTGRESQL_URL, SQLITE_URL]\r\n    for db_url in db_url_list:\r\n      dialect = db_url.split('+')[0]\r\n      logger.info(f'Test {dialect} database')\r\n      # randomly choose 3 queries to test PostgreSQL and MySQL\r\n      if dialect != 'sqlite':\r\n        selected_queries = random.sample(queries, 3)\r\n      else:\r\n        selected_queries = queries\r\n      count_list = [0] * len(selected_queries)\r\n      for i in range(_NUMBER_OF_RUNS):\r\n        gt_engine, aidb_engine = await setup_gt_and_aidb_engine(db_url, data_dir)\r\n        register_inference_services(aidb_engine, data_dir)\r\n        k = 0\r\n        for query_type, aidb_query, aggregate_query in selected_queries:\r\n          logger.info(f'Running query {aggregate_query} in ground truth database')\r\n          try:\r\n            async with gt_engine.begin() as conn:\r\n              gt_res = await conn.execute(text(aggregate_query))\r\n              gt_res = gt_res.fetchall()[0]\r\n          finally:\r\n            await gt_engine.dispose()\r\n          logger.info(f'Running query {aidb_query} in aidb database')\r\n          aidb_res = aidb_engine.execute(aidb_query)[0]\r\n          logger.info(f'aidb_res: {aidb_res}, gt_res: {gt_res}')\r\n          error_target = Query(aidb_query, aidb_engine._config).error_target\r\n          if error_target is None: error_target = 0\r\n          if self._equality_check(aidb_res, gt_res, error_target):\r\n            count_list[k] += 1\r\n          k+=1\r\n        logger.info(f'Time of runs: {i+1}, Successful count: {count_list}')\r\n      assert sum(count_list) >= len(count_list) * _NUMBER_OF_RUNS - 1\r\n      del gt_engine\r\n      del aidb_engine\r\n    p.terminate()\r\n\r\n\r\nif __name__ == '__main__':\r\n  unittest.main()\r\n"}
{"type": "test_file", "path": "tests/tests_full_scan_engine_twitter.py", "content": "import os\nimport time\nimport unittest\n\nfrom unittest import IsolatedAsyncioTestCase\nfrom sqlalchemy.sql import text\n\nfrom aidb.utils.logger import logger\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom tests.inference_service_utils.http_inference_service_setup import run_server\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\n\nfrom multiprocessing import Process\n\nsetup_test_logger('full_scan_engine_twitter')\n\nDB_URL = \"sqlite+aiosqlite://\"\n\n\nclass FullScanEngineTwitterTests(IsolatedAsyncioTestCase):\n\n  async def test_twitter(self):\n\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/twitter_all')\n\n    p = Process(target=run_server, args=[str(data_dir)])\n    p.start()\n    time.sleep(3)\n    gt_engine, aidb_engine = await setup_gt_and_aidb_engine(DB_URL, data_dir)\n\n    register_inference_services(aidb_engine, data_dir)\n\n    queries = [\n      (\n        'full_scan',\n        '''SELECT * FROM hate01 WHERE ishate=1 and tweet_id < 500;''',\n        '''SELECT * FROM hate01 WHERE ishate=1 and tweet_id < 500;'''\n      ),\n      (\n        'full_scan',\n        '''SELECT * FROM sentiment02 WHERE sentiment < 0 and tweet_id < 1000;''',\n        '''SELECT * FROM sentiment02 WHERE sentiment < 0 and tweet_id < 1000;'''\n      ),\n      (\n        'full_scan',\n        '''SELECT * FROM topic03 WHERE topic='sports_&_gaming' and tweet_id < 1000;''',\n        '''SELECT * FROM topic03 WHERE topic='sports_&_gaming' and tweet_id < 1000;'''\n      ),\n      (\n        'full_scan',\n        '''SELECT * FROM entities00 WHERE type='person' and tweet_id < 2000;''',\n        '''SELECT * FROM entities00 WHERE type='person' and tweet_id < 2000;'''\n      ),\n    ]\n\n    for query_type, aidb_query, exact_query in queries:\n      logger.info(f'Running query {exact_query} in ground truth database')\n      # Run the query on the ground truth database\n      async with gt_engine.begin() as conn:\n        gt_res = await conn.execute(text(exact_query))\n        gt_res = gt_res.fetchall()\n      # Run the query on the aidb database\n      logger.info(f'Running query {aidb_query} in aidb database')\n      aidb_res = aidb_engine.execute(aidb_query)\n      logger.info(f'Length ground truth - {len(gt_res)}')\n      assert len(gt_res) == len(aidb_res)\n      assert sorted(gt_res) == sorted(aidb_res)\n    del gt_engine\n    p.terminate()\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "test_file", "path": "tests/tests_columns_in_query.py", "content": "import os\nimport unittest\n\nfrom unittest import IsolatedAsyncioTestCase\nfrom tests.db_utils.db_setup import create_db, setup_db, setup_config_tables\nfrom aidb.engine import Engine\nfrom aidb.query.query import Query\n\nDB_URL = \"sqlite+aiosqlite://\"\n\n\nclass ColumnsInQueryTests(IsolatedAsyncioTestCase):\n\n  async def test_positive_object_detection(self):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n\n    # Set up the aidb database\n    aidb_db_fname = 'aidb_test.sqlite'\n    await create_db(DB_URL, aidb_db_fname)\n\n    tmp_engine = await setup_db(DB_URL, aidb_db_fname, data_dir)\n    try:\n      async with tmp_engine.begin() as conn:\n        await setup_config_tables(conn)\n    except Exception:\n      raise Exception('Fail to setup config table.')\n    finally:\n      await tmp_engine.dispose()\n    del tmp_engine\n    # Connect to the aidb database\n    aidb_engine = Engine(\n      f'{DB_URL}/{aidb_db_fname}',\n      debug=False,\n    )\n    # pairs of query, number of columns\n    test_query_list = [(\"SELECT * FROM objects00;\", 8),\n                       (\"SELECT object_id FROM objects00 WHERE frame > 100;\", 2),\n                       (\"SELECT * FROM lights01,counts03;\", 7),\n                       (\"SELECT l.frame,c.count FROM lights01 l JOIN counts03 c ON l.frame=c.frame;\", 3),\n                       ]\n    for query, ground_truth in test_query_list:\n      q = Query(query, aidb_engine._config)\n      columns_in_query = q.columns_in_query\n      assert len(columns_in_query) == ground_truth\n    del aidb_engine\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "test_file", "path": "tests/tests_full_scan_nested_query.py", "content": "import os\nimport time\nimport unittest\n\nfrom unittest import IsolatedAsyncioTestCase\nfrom sqlalchemy.sql import text\n\nfrom aidb.utils.logger import logger\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom tests.inference_service_utils.http_inference_service_setup import run_server\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\n\nfrom multiprocessing import Process\n\nsetup_test_logger('full_scan_engine_nested_query')\n\nDB_URL = \"sqlite+aiosqlite://\"\n\n\n# DB_URL = \"mysql+aiomysql://aidb:aidb@localhost\"\nclass NestedQueryTests(IsolatedAsyncioTestCase):\n\n  async def test_nested_query(self):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n\n    p = Process(target=run_server, args=[str(data_dir)])\n    p.start()\n    time.sleep(3)\n    gt_engine, aidb_engine = await setup_gt_and_aidb_engine(DB_URL, data_dir)\n\n    register_inference_services(aidb_engine, data_dir)\n\n    queries = [\n      # test column alias\n      (\n        'full_scan',\n        '''SELECT colors02.color as alias_color FROM colors02 WHERE alias_color IN (SELECT table2.color as alias_color2 \n            FROM colors02 AS table2) OR colors02.object_id > (SELECT AVG(ob.object_id) as alias_color2 FROM objects00 AS ob);''',\n        '''SELECT colors02.color as alias_color FROM colors02 WHERE alias_color IN (SELECT table2.color as alias_color2 \n            FROM colors02 AS table2) OR colors02.object_id > (SELECT AVG(ob.object_id) as alias_color2 FROM objects00 AS ob);'''\n      ),\n      # test table alias\n      (\n        'full_scan',\n        '''SELECT * FROM colors02 WHERE frame IN (SELECT * FROM blobs_00)\n              OR object_id > (SELECT AVG(ob.object_id) FROM objects00 AS ob);''',\n        '''SELECT * FROM colors02 WHERE frame IN (SELECT * FROM blobs_00)\n              OR object_id > (SELECT AVG(ob.object_id) FROM objects00 AS ob);''',\n      ),\n      (\n        'full_scan',\n        '''SELECT * FROM colors02 AS cl WHERE cl.frame IN (SELECT * FROM blobs_00)\n              OR cl.object_id > (SELECT AVG(colors02.object_id) FROM colors02);''',\n        '''SELECT * FROM colors02 AS cl WHERE cl.frame IN (SELECT * FROM blobs_00)\n              OR cl.object_id > (SELECT AVG(colors02.object_id) FROM colors02);''',\n      ),\n      # subquery is on meta table. always satisfied\n      (\n        'full_scan',\n        '''SELECT * FROM objects00 WHERE object_name='car' AND objects00.frame <\n              (SELECT AVG(blobs_00.frame) FROM blobs_00) OR objects00.frame NOT IN (1, 2, 3);''',\n        '''SELECT * FROM objects00 WHERE object_name='car' AND objects00.frame <\n              (SELECT AVG(blobs_00.frame) FROM blobs_00) OR objects00.frame NOT IN (1, 2, 3);'''\n      ),\n\n      # 2-place predicates. subquery is not satisfied until objects00 is filled\n      (\n        'full_scan',\n        '''SELECT * FROM colors02 WHERE frame >= 10000 AND object_id >\n              (SELECT AVG(objects00.object_id) FROM objects00);''',\n        '''SELECT * FROM colors02 WHERE frame >= 10000 AND object_id >\n              (SELECT AVG(objects00.object_id) FROM objects00);'''\n      ),\n      (\n        'full_scan',\n        '''SELECT * FROM colors02 WHERE frame >= 10000;''',\n        '''SELECT * FROM colors02 WHERE frame >= 10000;'''\n      ),\n      # multiple sub-queries\n      (\n        'full_scan',\n        '''SELECT * FROM colors02 WHERE object_id > (SELECT AVG(objects00.object_id) FROM objects00)\n              OR object_id > (SELECT AVG(colors02.object_id) FROM colors02);''',\n        '''SELECT * FROM colors02 WHERE object_id > (SELECT AVG(objects00.object_id) FROM objects00)\n              OR object_id > (SELECT AVG(colors02.object_id) FROM colors02);''',\n      ),\n\n      # nested sub-queries. predicate clause is not satisfied until all inference is complete\n      (\n        'full_scan',\n        '''SELECT * FROM colors02 WHERE object_id > (SELECT AVG(objects00.object_id) FROM objects00\n              WHERE objects00.y_max < (SELECT AVG(colors02.object_id) FROM colors02));''',\n        '''SELECT * FROM colors02 WHERE object_id > (SELECT AVG(objects00.object_id) FROM objects00\n              WHERE objects00.y_max < (SELECT AVG(colors02.object_id) FROM colors02));''',\n      ),\n\n      # where-in predicate. frame is not satisfied until objects00 is filled\n      (\n        'full_scan',\n        '''SELECT * FROM colors02 WHERE object_id IN (SELECT objects00.object_id FROM objects00\n              WHERE objects00.frame < 1000);''',\n        '''SELECT * FROM colors02 WHERE object_id IN (SELECT objects00.object_id FROM objects00\n              WHERE objects00.frame < 1000);'''\n      ),\n\n      # where-in predicate. x_min is not satisfied until objects00 is filled\n      (\n        'full_scan',\n        '''SELECT * FROM colors02 WHERE object_id IN (SELECT objects00.object_id FROM objects00\n              WHERE objects00.x_min < 364);''',\n        '''SELECT * FROM colors02 WHERE object_id IN (SELECT objects00.object_id FROM objects00\n              WHERE objects00.x_min < 364);'''\n      ),\n\n      # where-in predicate. color is not satisfied until colors02 is filled\n      (\n        'full_scan',\n        '''SELECT * FROM colors02 WHERE object_id IN (SELECT colors02.object_id FROM colors02\n              WHERE colors02.color = 'grayish_blue');''',\n        '''SELECT * FROM colors02 WHERE object_id IN (SELECT colors02.object_id FROM colors02\n              WHERE colors02.color = 'grayish_blue');'''\n      ),\n\n      # where-in predicate with literals\n      (\n        'full_scan',\n        '''SELECT * FROM colors02 WHERE object_id IN (500,501,502);''',\n        '''SELECT * FROM colors02 WHERE object_id IN (500,501,502);'''\n      ),\n\n      (\n        'full_scan',\n        '''SELECT * FROM colors02 WHERE 500 < object_id < 1000;''',\n        '''SELECT * FROM colors02 WHERE 500 < object_id < 1000;'''\n      ),\n      # test sub-subquery\n      (\n        'full_scan',\n        '''SELECT frame, object_id FROM colors02 AS cl \n            WHERE cl.object_id > (SELECT AVG(object_id) FROM objects00 \n             WHERE frame > (SELECT AVG(frame) FROM blobs_00 WHERE frame > 500))\n        ''',\n        '''SELECT frame, object_id FROM colors02 AS cl \n                    WHERE cl.object_id > (SELECT AVG(object_id) FROM objects00 \n                     WHERE frame > (SELECT AVG(frame) FROM blobs_00 WHERE frame > 500))\n                ''',\n      )\n    ]\n\n    for query_type, aidb_query, exact_query in queries:\n      logger.info(f'Running query {exact_query} in ground truth database')\n      # Run the query on the ground truth database\n      async with gt_engine.begin() as conn:\n        gt_res = await conn.execute(text(exact_query))\n        gt_res = gt_res.fetchall()\n      # Run the query on the aidb database\n      logger.info(f'Running query {aidb_query} in aidb database')\n      aidb_res = aidb_engine.execute(aidb_query)\n      assert len(gt_res) == len(aidb_res)\n\n      # Sort results\n      gt_res = sorted(gt_res)\n      aidb_res = sorted(aidb_res)\n\n      for i in range(len(gt_res)):\n        assert gt_res[i] == aidb_res[i]\n    del gt_engine\n    p.terminate()\n\nif __name__ == '__main__':\n  unittest.main()"}
{"type": "test_file", "path": "tests/test_exact_aggregation.py", "content": "from multiprocessing import Process\nimport os\nfrom sqlalchemy.sql import text\nimport time\nimport unittest\nfrom unittest import IsolatedAsyncioTestCase\n\nfrom aidb.query.query import Query\nfrom aidb.utils.logger import logger\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom tests.inference_service_utils.http_inference_service_setup import run_server\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\n\nsetup_test_logger('exact_aggregation')\n\nPOSTGRESQL_URL = 'postgresql+asyncpg://user:testaidb@localhost:5432'\nSQLITE_URL = 'sqlite+aiosqlite://'\nMYSQL_URL = 'mysql+aiomysql://root:testaidb@localhost:3306'\n\nqueries = [\n  (\n    'exact_aggregate',\n    '''SELECT COUNT(*) FROM objects00;''',\n    '''SELECT COUNT(*) FROM objects00;'''\n  ),\n  (\n    'exact_aggregate',\n    '''SELECT SUM(x_max) FROM objects00;''',\n    '''SELECT SUM(x_max) FROM objects00;'''\n  ),\n  (\n    'exact_aggregate',\n    '''SELECT COUNT(*) FROM colors02 WHERE color='black';''',\n    '''SELECT COUNT(*) FROM colors02 WHERE color='black';'''\n  ),\n]\n\n\nclass ExactAggeregateTests(IsolatedAsyncioTestCase):\n  async def test_agg_query(self):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n\n    p = Process(target=run_server, args=[str(data_dir)])\n    p.start()\n    time.sleep(1)\n    db_url_list = [MYSQL_URL, SQLITE_URL, POSTGRESQL_URL]\n    for db_url in db_url_list:\n      dialect = db_url.split('+')[0]\n      logger.info(f'Test {dialect} database')\n      gt_engine, aidb_engine = await setup_gt_and_aidb_engine(db_url, data_dir)\n      register_inference_services(aidb_engine, data_dir)\n      for query_type, aidb_query, aggregate_query in queries:\n        logger.info(f'Running query {aggregate_query} in ground truth database')\n        try:\n          async with gt_engine.begin() as conn:\n            gt_res = await conn.execute(text(aggregate_query))\n            gt_res = gt_res.fetchall()[0]\n        finally:\n          await gt_engine.dispose()\n        logger.info(f'Running query {aidb_query} in aidb database')\n        aidb_res = aidb_engine.execute(aidb_query)[0]\n        logger.info(f'aidb_res: {aidb_res}, gt_res: {gt_res}')\n        assert aidb_res[0] == gt_res[0]\n\n      del gt_engine\n      del aidb_engine\n    p.terminate()\n\n\nif __name__ == '__main__':\n  unittest.main()"}
{"type": "test_file", "path": "tests/utils.py", "content": "import logging\nfrom tests.db_utils.db_setup import create_db, setup_db, setup_config_tables, insert_data_in_tables, clear_all_tables\nfrom aidb.engine import Engine\nfrom aidb.utils.logger import logger\n\nasync def setup_gt_and_aidb_engine(db_url, data_dir, tasti_index = None, port = 8000):\n  # Set up the ground truth database\n  dialect = db_url.split(\"+\")[0]\n  if dialect == \"postgresql\" or dialect == \"mysql\":\n    gt_db_fname = f'aidb_gt_{port}'\n    aidb_db_fname = f'aidb_test_{port}'\n  elif dialect == \"sqlite\":\n    gt_db_fname = f'aidb_gt_{port}.sqlite'\n    aidb_db_fname = f'aidb_test_{port}.sqlite'\n  else:\n    raise Exception('Unsupported database. We support mysql, sqlite and postgresql currently.')\n\n  await create_db(db_url, gt_db_fname)\n  gt_engine = await setup_db(db_url, gt_db_fname, data_dir)\n  try:\n    async with gt_engine.begin() as conn:\n      await insert_data_in_tables(conn, data_dir, False)\n  finally:\n    await gt_engine.dispose()\n\n  # Set up the aidb database\n  await create_db(db_url, aidb_db_fname)\n  tmp_engine = await setup_db(db_url, aidb_db_fname, data_dir)\n  try:\n    async with tmp_engine.begin() as conn:\n      await clear_all_tables(conn)\n      await insert_data_in_tables(conn, data_dir, True)\n      await setup_config_tables(conn)\n  finally:\n    await tmp_engine.dispose()\n\n  # Connect to the aidb database\n  engine = Engine(\n    f'{db_url}/{aidb_db_fname}',\n    debug=False,\n    tasti_index=tasti_index\n  )\n\n  return gt_engine, engine\n\n\ndef setup_test_logger(log_fname):\n  logger.setLevel(logging.INFO)\n  formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n  \n  file_handler = logging.FileHandler(f'{log_fname}.log')\n  file_handler.setLevel(logging.INFO)\n  file_handler.setFormatter(formatter)\n  \n  logger.addHandler(file_handler)"}
{"type": "test_file", "path": "tests/test_aggregation_join.py", "content": "import os\nimport time\nimport unittest\nfrom decimal import Decimal\nfrom multiprocessing import Process\nfrom unittest import IsolatedAsyncioTestCase\n\nimport pandas as pd\nfrom sqlalchemy.sql import text\n\nfrom aidb.inference.bound_inference_service import CachedBoundInferenceService\nfrom aidb.query.query import Query\nfrom aidb.utils.logger import logger\nfrom tests.inference_service_utils.http_inference_service_setup import \\\n    run_server\nfrom tests.inference_service_utils.inference_service_setup import \\\n    register_inference_services\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\n\nfrom aidb.utils.asyncio import asyncio_run\n\n\nsetup_test_logger('aggregation_join')\n\n# note: When running tests locally, use your own URL, and modify line 95 to select the type of database you wish to use.\nPOSTGRESQL_URL = 'postgresql+asyncpg://user:testaidb@localhost:5432'\nSQLITE_URL = 'sqlite+aiosqlite://'\nMYSQL_URL = 'mysql+aiomysql://root:testaidb@localhost:3306'\n\n_NUMBER_OF_RUNS = int(os.environ.get('AIDB_NUMBER_OF_TEST_RUNS', 100))\n\nasync def inference(inference_service: CachedBoundInferenceService, input_df: pd.DataFrame):\n  input_df.columns = inference_service.binding.input_columns\n  outputs = await inference_service.infer(input_df, return_inference_results=True)\n  return outputs.iloc[:,0]\n\n\nqueries = [\n  (\n    'approx_aggregate',\n    '''SELECT COUNT(*) FROM blobs_00 CROSS JOIN blobs_01 WHERE match_inference(blobs_00.id0, blobs_01.id1) = TRUE\n            ERROR_TARGET 10% CONFIDENCE 95%;''',\n    '''SELECT COUNT(*) FROM match00;'''\n  ),\n  (\n    'approx_aggregate',\n    '''SELECT COUNT(*) FROM blobs_00 CROSS JOIN blobs_01 WHERE match_inference(blobs_00.id0, blobs_01.id1) = TRUE \n           AND blobs_00.id0 > 10000 ERROR_TARGET 10% CONFIDENCE 95%;''',\n    '''SELECT COUNT(*) FROM match00 WHERE match00.id0 > 10000;'''\n  ),\n  (\n    'approx_aggregate',\n    '''\n      SELECT SUM(blobs_00.x0), AVG(blobs_01.x1) \n      FROM blobs_00 CROSS JOIN blobs_01 \n      WHERE match_inference(blobs_00.id0, blobs_01.id1) = TRUE AND blobs_01.id1 > 10000\n      ERROR_TARGET 10% CONFIDENCE 95%;\n    ''',\n    '''\n      SELECT SUM(blobs_00.x0), AVG(blobs_01.x1) \n      FROM match00 JOIN blobs_00 ON match00.id0 = blobs_00.id0 JOIN blobs_01 ON match00.id1 = blobs_01.id1\n      WHERE blobs_01.id1 > 10000;\n    '''\n  ),\n  (\n    'approx_aggregate',\n    '''\n      SELECT AVG(blobs_00.h0), SUM(blobs_01.h1) \n      FROM blobs_00 CROSS JOIN blobs_01 \n      WHERE match_inference(blobs_00.id0, blobs_01.id1) = TRUE\n      ERROR_TARGET 10% CONFIDENCE 95%;\n    ''',\n    '''\n      SELECT AVG(blobs_00.h0), SUM(blobs_01.h1) \n      FROM match00 JOIN blobs_00 ON match00.id0 = blobs_00.id0 JOIN blobs_01 ON match00.id1 = blobs_01.id1;\n    '''\n  ),\n]\n\n\nclass AggregateJoinEngineTests(IsolatedAsyncioTestCase):\n  def add_user_defined_function(self, aidb_engine):\n    async def async_match_inference(input_df):\n      for service in aidb_engine._config.inference_bindings:\n        if service.service.name == 'match00':\n          res = await inference(service, input_df.copy())\n          return res\n        else:\n          raise Exception('No required service found.')\n\n    aidb_engine._config.add_user_defined_function('match_inference', async_match_inference)\n\n  def _equality_check(self, aidb_res, gt_res, error_target):\n    assert len(aidb_res) == len(gt_res)\n    error_rate_list = []\n    valid_estimation = True\n    for aidb_item, gt_item in zip(aidb_res, gt_res):\n      if isinstance(gt_item, Decimal):\n        gt_item = float(gt_item)\n      relative_diff = abs(aidb_item - gt_item) / (gt_item)\n      error_rate_list.append(relative_diff * 100)\n      if relative_diff > error_target:\n        valid_estimation = False\n    logger.info(f'Error rate (%) for approximate aggregation query: {error_rate_list}')\n    return valid_estimation\n\n\n  async def test_agg_query(self):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/city_human_join')\n\n    p = Process(target=run_server, args=[str(data_dir)])\n    p.start()\n    time.sleep(1)\n    db_url_list = [SQLITE_URL]\n    for db_url in db_url_list:\n      dialect = db_url.split('+')[0]\n      logger.info(f'Test {dialect} database')\n      count_list = [0] * len(queries)\n      for i in range(_NUMBER_OF_RUNS):\n        gt_engine, aidb_engine = await setup_gt_and_aidb_engine(db_url, data_dir)\n        self.add_user_defined_function(aidb_engine)\n\n        register_inference_services(aidb_engine, data_dir, preferred_batch_size=1024)\n        k = 0\n        for query_type, aidb_query, aggregate_query in queries:\n          logger.info(f'Running query {aggregate_query} in ground truth database')\n          try:\n            async with gt_engine.begin() as conn:\n              gt_res = await conn.execute(text(aggregate_query))\n              gt_res = gt_res.fetchall()[0]\n          finally:\n            await gt_engine.dispose()\n          logger.info(f'Running query {aidb_query} in aidb database')\n          aidb_res = aidb_engine.execute(aidb_query)[0]\n          logger.info(f'aidb_res: {aidb_res}, gt_res: {gt_res}')\n          error_target = Query(aidb_query, aidb_engine._config).error_target\n          if error_target is None: error_target = 0\n          if self._equality_check(aidb_res, gt_res, error_target):\n            count_list[k] += 1\n          k+=1\n        logger.info(f'Time of runs: {i+1}, Successful count: {count_list}')\n      assert sum(count_list) >= len(count_list) * _NUMBER_OF_RUNS - 1\n      del gt_engine\n      del aidb_engine\n    p.terminate()\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "test_file", "path": "tests/tests_full_scan_engine_udf.py", "content": "import os\nimport time\nimport unittest\n\nfrom collections import Counter\nfrom decimal import Decimal\nfrom unittest import IsolatedAsyncioTestCase\n\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy.sql import text\n\nfrom aidb.utils.logger import logger\nfrom aidb.inference.bound_inference_service import CachedBoundInferenceService\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom tests.inference_service_utils.http_inference_service_setup import run_server\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\n\nfrom multiprocessing import Process\n\nsetup_test_logger('full_scan_engine_udf')\n\nPOSTGRESQL_URL = 'postgresql+asyncpg://user:testaidb@localhost:5432'\nSQLITE_URL = 'sqlite+aiosqlite://'\nMYSQL_URL = 'mysql+aiomysql://root:testaidb@localhost:3306'\n\n\ndef inference(inference_service: CachedBoundInferenceService, input_df: pd.DataFrame):\n  input_df.columns = inference_service.binding.input_columns\n  outputs = inference_service.service.infer_batch(input_df)\n  return outputs\n\n\nclass FullScanEngineUdfTests(IsolatedAsyncioTestCase):\n\n  def add_user_defined_function(self, aidb_engine):\n    def sum_function(inp_df):\n      sum_value = inp_df.sum(axis=1)\n      return sum_value\n\n\n    def is_equal(inp_df):\n      equal_value = (inp_df.iloc[:, 0] == inp_df.iloc[:, 1])\n      return equal_value\n\n\n    def max_function(inp_df):\n      max_value = inp_df.max(axis=1)\n      return max_value\n\n\n    def power_function(inp_df):\n      power_value = inp_df.iloc[:, 0] ** inp_df.iloc[:, 1]\n      return power_value\n\n\n    def multiply_function(inp_df):\n      multiply_value = inp_df.iloc[:, 0] * inp_df.iloc[:, 1]\n      return multiply_value\n\n\n    def replace_color(inp_df):\n      color_value = np.where(inp_df.iloc[:,0] == inp_df.iloc[:,1], inp_df.iloc[:,2], inp_df.iloc[:,0])\n      return color_value.tolist()\n\n\n    async def async_objects_inference(input_df):\n      for service in aidb_engine._config.inference_bindings:\n        if service.service.name == 'objects00':\n          return inference(service, input_df)\n\n    async def async_lights_inference(input_df):\n      for service in aidb_engine._config.inference_bindings:\n        if service.service.name == 'lights01':\n          return inference(service, input_df)\n\n\n    async def async_colors_inference(input_df):\n      for service in aidb_engine._config.inference_bindings:\n        if service.service.name == 'colors02':\n          return inference(service, input_df)\n\n\n    async def async_counts_inference(input_df):\n      # input_df = pd.DataFrame({'blob_id': [blob_id]})\n      for service in aidb_engine._config.inference_bindings:\n        if service.service.name == 'counts03':\n          return inference(service, input_df)\n\n\n    aidb_engine._config.add_user_defined_function('sum_function', sum_function)\n    aidb_engine._config.add_user_defined_function('is_equal', is_equal)\n    aidb_engine._config.add_user_defined_function('max_function', max_function)\n    aidb_engine._config.add_user_defined_function('multiply_function', multiply_function)\n    aidb_engine._config.add_user_defined_function('power_function', power_function)\n    aidb_engine._config.add_user_defined_function('replace_color', replace_color)\n    aidb_engine._config.add_user_defined_function('objects_inference', async_objects_inference)\n    aidb_engine._config.add_user_defined_function('lights_inference', async_lights_inference)\n    aidb_engine._config.add_user_defined_function('colors_inference', async_colors_inference)\n    aidb_engine._config.add_user_defined_function('counts_inference', async_counts_inference)\n\n\n\n  async def test_udf_sqlite(self):\n\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n    port = 8010\n    p = Process(target=run_server, args=[str(data_dir), port])\n    p.start()\n    time.sleep(1)\n    gt_engine, aidb_engine = await setup_gt_and_aidb_engine(SQLITE_URL, data_dir, port=port)\n\n    register_inference_services(aidb_engine, data_dir, port)\n\n    self.add_user_defined_function(aidb_engine)\n\n    queries = [\n      # join condition test\n      (\n        'full_scan',\n        '''\n        SELECT y_max, x_min, y_min, color\n        FROM objects00 join colors02 ON is_equal(objects00.frame, colors02.frame) = TRUE\n            AND is_equal(objects00.object_id, colors02.object_id) = TRUE\n        WHERE x_min > 600 OR (x_max >600 AND y_min > 800)\n        ''',\n        '''\n        SELECT y_max, x_min, y_min, color\n        FROM objects00 join colors02 ON objects00.frame = colors02.frame AND objects00.object_id = colors02.object_id\n        WHERE x_min > 600 OR (x_max >600 AND y_min > 800)\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT y_max, x_min, y_min, color\n        FROM objects00 join colors02\n        WHERE is_equal(objects00.frame, colors02.frame) = TRUE AND is_equal(objects00.object_id, colors02.object_id)\n            = TRUE AND (x_min > 600 OR (x_max >600 AND y_min > 800))\n        ''',\n        '''\n        SELECT y_max, x_min, y_min, color\n        FROM objects00 join colors02 ON objects00.frame = colors02.frame AND objects00.object_id = colors02.object_id\n        WHERE x_min > 600 OR (x_max >600 AND y_min > 800)\n        '''\n      ),\n      # test user defined function in SELECT clause\n      (\n        'full_scan',\n        '''\n        SELECT max_function(x_min, y_min), y_min, replace_color(color, 'blue', 'new_blue')\n        FROM objects00 join colors02\n        WHERE is_equal(objects00.frame, colors02.frame) = TRUE AND is_equal(objects00.object_id, colors02.object_id)\n            = TRUE AND (x_min > 600 OR (x_max >600 AND y_min > 800))\n        ''',\n        '''\n        SELECT MAX(x_min, y_min), y_min, IIF(color='blue', 'new_blue', color)\n        FROM objects00 join colors02 ON objects00.frame = colors02.frame AND objects00.object_id = colors02.object_id\n        WHERE x_min > 600 OR (x_max >600 AND y_min > 800)\n        '''\n      ),\n      # test comparison between user defined functions\n      (\n        'full_scan',\n        '''\n        SELECT y_min, color\n        FROM objects00 join colors02 ON is_equal(objects00.frame, colors02.frame) = TRUE\n            AND is_equal(objects00.object_id, colors02.object_id) = TRUE\n        WHERE max_function(x_min, x_max) < max_function(y_min, y_max) AND x_min > 600 OR (x_max >600 AND y_min > 800)\n        ''',\n        '''\n        SELECT y_min, color\n        FROM objects00 join colors02 ON objects00.frame = colors02.frame AND objects00.object_id = colors02.object_id\n        WHERE MAX(x_min, x_max) < MAX(y_min, y_max) AND  x_min > 600 OR (x_max >600 AND y_min > 800)\n        '''\n      ),\n      # test user defined function with constant parameters\n      (\n        'full_scan',\n        '''\n        SELECT max_function(y_max, y_min), power_function(x_min, 2), y_min, color\n        FROM objects00 join colors02\n        WHERE is_equal(objects00.frame, colors02.frame) = TRUE AND is_equal(objects00.object_id, colors02.object_id)\n            = TRUE AND (x_min > 600 OR (x_max >600 AND y_min > 800))\n        ''',\n        '''\n        SELECT MAX(y_max, y_min), POWER(x_min, 2), y_min, color\n        FROM objects00 join colors02 ON objects00.frame = colors02.frame AND objects00.object_id = colors02.object_id\n        WHERE x_min > 600 OR (x_max >600 AND y_min > 800)\n        '''\n      ),\n      # test user defined function in filter predicates\n      (\n        'full_scan',\n        '''\n        SELECT max_function(y_max, y_min), y_min, color\n        FROM objects00 join colors02 ON is_equal(objects00.frame, colors02.frame) = TRUE\n            AND is_equal(objects00.object_id, colors02.object_id) = TRUE\n        WHERE power_function(x_min, 2) > 640000 AND (x_min > 600 OR (x_max >600 AND y_min > 800))\n        ''',\n        '''\n        SELECT MAX(y_max, y_min), y_min, color\n        FROM objects00 join colors02 ON objects00.frame = colors02.frame AND objects00.object_id = colors02.object_id\n        WHERE POWER(x_min, 2) > 640000 AND (x_min > 600 OR (x_max >600 AND y_min > 800))\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT objects_inference(frame)\n        FROM blobs_00\n        ''',\n        '''\n        SELECT object_name, confidence_score, x_min, y_min, x_max, y_max, object_id, frame\n        FROM objects00\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT counts_inference(frame)\n        FROM blobs_00\n        ''',\n        '''\n        SELECT count, frame\n        FROM counts03\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT lights_inference(frame)\n        FROM blobs_00\n        ''',\n        '''\n        SELECT light_1, light_2, light_3, light_4, frame\n        FROM lights01\n        '''\n      ),\n      # test user function with alias and filter based on the outputs of user function\n      (\n        'full_scan',\n        '''\n        SELECT objects_inference(frame) AS (output1, output2, output3, output4, output5, output6, output7, output8)\n        FROM blobs_00\n        WHERE (output3 > 600 AND output6 < 1400) OR frame < 1000\n        ''',\n        '''\n        SELECT object_name, confidence_score, x_min, y_min, x_max, y_max, object_id, frame\n        FROM objects00\n        WHERE (x_min > 600 AND y_max < 1400) OR frame < 1000\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT lights_inference(frame) AS (output1, output2, output3, output4, output5)\n        FROM blobs_00\n        WHERE (output1 = 'red' AND output2 LIKE 'red') OR output3 = 'yellow'\n        ''',\n        '''\n        SELECT light_1, light_2, light_3, light_4, frame\n        FROM lights01\n        WHERE (light_1 = 'red' AND light_2 LIKE 'red') OR light_3 = 'yellow'\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT colors_inference(frame, object_id) AS (output1, output2, output3), x_min, y_max\n        FROM objects00\n        WHERE (x_min > 600 AND output1 LIKE 'blue') OR (y_max < 1000 AND x_max < 1000)\n        ''',\n        '''\n        SELECT color, colors02.frame, colors02.object_id, x_min, y_max\n        FROM objects00 JOIN colors02 ON objects00.frame = colors02.frame AND objects00.object_id = colors02.object_id\n        WHERE (x_min > 600 AND color LIKE 'blue') OR (y_max < 1000 AND x_max < 1000)\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT colors_inference(frame, object_id) AS (output1, output2, output3), x_min AS col1, y_max AS col2\n        FROM objects00\n        WHERE (col1 > 600 AND output1 LIKE 'blue') OR (col2 < 1000 AND x_max < 1000)\n        ''',\n        '''\n        SELECT color, colors02.frame, colors02.object_id, x_min AS col1, y_max AS col2\n        FROM objects00 JOIN colors02 ON objects00.frame = colors02.frame AND objects00.object_id = colors02.object_id\n        WHERE (col1 > 600 AND color LIKE 'blue') OR (col2 < 1000 AND x_max < 1000)\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT max_function(y_max, y_min) AS output1, power_function(x_min, 2) AS output2, y_min, color\n        FROM objects00 join colors02\n        WHERE is_equal(objects00.frame, colors02.frame) = TRUE AND is_equal(objects00.object_id, colors02.object_id)\n            = TRUE AND (x_min > 600 OR (x_max >600 AND y_min > 800)) AND output1 > 1000 AND output2 > 640000\n        ''',\n        '''\n        SELECT MAX(y_max, y_min) AS output1, POWER(x_min, 2) AS output2, y_min, color\n        FROM objects00 join colors02 ON objects00.frame = colors02.frame AND objects00.object_id = colors02.object_id\n        WHERE (x_min > 600 OR (x_max >600 AND y_min > 800)) AND output1 > 1000 AND output2 > 640000\n        '''\n      ),\n    ]\n\n    for query_type, aidb_query, exact_query in queries:\n      logger.info(f'Running query {exact_query} in ground truth database')\n      # Run the query on the ground truth database\n      try:\n        async with gt_engine.begin() as conn:\n          gt_res = await conn.execute(text(exact_query))\n          gt_res = gt_res.fetchall()\n      finally:\n        await gt_engine.dispose()\n      # Run the query on the aidb database\n      logger.info(f'Running query {aidb_query} in aidb database')\n      aidb_res = aidb_engine.execute(aidb_query)\n      assert len(gt_res) == len(aidb_res)\n      assert sorted(gt_res) == sorted(aidb_res)\n    del gt_engine\n    del aidb_engine\n    p.terminate()\n\n\n  async def test_udf_postgresql_and_mysql(self):\n\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n\n    p = Process(target=run_server, args=[str(data_dir)])\n    p.start()\n    time.sleep(1)\n\n    queries = [\n      # join condition test\n      (\n        'full_scan',\n        '''\n        SELECT y_max, x_min, y_min, color\n        FROM objects00 join colors02 ON is_equal(objects00.frame, colors02.frame) = TRUE\n            AND is_equal(objects00.object_id, colors02.object_id) = TRUE\n        WHERE x_min > 600 OR (x_max >600 AND y_min > 800)\n        ''',\n        '''\n        SELECT y_max, x_min, y_min, color\n        FROM objects00 join colors02 ON objects00.frame = colors02.frame AND objects00.object_id = colors02.object_id\n        WHERE x_min > 600 OR (x_max >600 AND y_min > 800)\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT y_max, x_min, y_min, color\n        FROM objects00 join colors02\n        WHERE is_equal(objects00.frame, colors02.frame) = TRUE AND is_equal(objects00.object_id, colors02.object_id)\n            = TRUE AND (x_min > 600 OR (x_max >600 AND y_min > 800))\n        ''',\n        '''\n        SELECT y_max, x_min, y_min, color\n        FROM objects00 join colors02 ON objects00.frame = colors02.frame AND objects00.object_id = colors02.object_id\n        WHERE x_min > 600 OR (x_max >600 AND y_min > 800)\n        '''\n      ),\n      # test machine learning model, output may be zero, multiple rows or multiple columns\n      (\n        'full_scan',\n        '''\n        SELECT objects_inference(frame)\n        FROM blobs_00\n        WHERE frame = 300\n        ''',\n        '''\n        SELECT object_name, confidence_score, x_min, y_min, x_max, y_max, object_id, frame\n        FROM objects00\n        WHERE frame = 300\n        '''\n      ),\n      # test user function with alias and filter based on the outputs of user function\n      (\n        'full_scan',\n        '''\n        SELECT objects_inference(frame) AS (output1, output2, output3, output4, output5, output6, output7, output8)\n        FROM blobs_00\n        WHERE (output3 > 600 AND output6 < 1400) OR frame < 1000\n        ''',\n        '''\n        SELECT object_name, confidence_score, x_min, y_min, x_max, y_max, object_id, frame\n        FROM objects00\n        WHERE (x_min > 600 AND y_max < 1400) OR frame < 1000\n        '''\n      ),\n      # test UDFs created within the database and AIDB\n      (\n        'full_scan',\n        '''\n        SELECT multiply_function(x_min, y_min), database_multiply_function(x_min, y_min), x_max, y_max\n        FROM objects00\n        WHERE x_min > 600 AND y_max < 1000\n        ''',\n        '''\n        SELECT database_multiply_function(x_min, y_min), database_multiply_function(x_min, y_min), x_max, y_max\n        FROM objects00\n        WHERE x_min > 600 AND y_max < 1000\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT database_add_function(y_max, x_min), multiply_function(y_min, y_max), color\n        FROM objects00 join colors02 ON is_equal(objects00.frame, colors02.frame) = TRUE\n            AND is_equal(objects00.object_id, colors02.object_id) = TRUE\n        WHERE x_min > 600 OR (x_max >600 AND y_min > 800)\n        ''',\n        '''\n        SELECT database_add_function(y_max, x_min), database_multiply_function(y_min, y_max), color\n        FROM objects00 join colors02 ON objects00.frame = colors02.frame AND objects00.object_id = colors02.object_id\n        WHERE x_min > 600 OR (x_max >600 AND y_min > 800)\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT frame, database_multiply_function(x_min, y_min), sum_function(x_max, y_max)\n        FROM objects00\n        WHERE (multiply_function(x_min, y_min) > 400000 AND database_add_function(y_max, x_min) < 1600)\n            OR database_multiply_function(x_min, y_min) > 500000\n        ''',\n        '''\n        SELECT frame, database_multiply_function(x_min, y_min), database_add_function(x_max, y_max)\n        FROM objects00\n        WHERE (database_multiply_function(x_min, y_min) > 400000 AND database_add_function(y_max, x_min) < 1600)\n            OR database_multiply_function(x_min, y_min) > 500000\n        '''\n      ),\n      (\n        'full_scan',\n        '''\n        SELECT frame, database_multiply_function(x_min, y_min), sum_function(x_max, y_max) AS output1\n        FROM objects00\n        WHERE (multiply_function(x_min, y_min) > 400000 AND output1 < 1600)\n            OR database_multiply_function(x_min, y_min) > 500000\n        ''',\n        '''\n        SELECT frame, database_multiply_function(x_min, y_min), database_add_function(x_max, y_max)\n        FROM objects00\n        WHERE (database_multiply_function(x_min, y_min) > 400000 AND database_add_function(x_max, y_max) < 1600)\n            OR database_multiply_function(x_min, y_min) > 500000\n        '''\n      ),\n    ]\n\n    postgresql_function =  [\n        '''\n        CREATE OR REPLACE FUNCTION database_multiply_function(col1 DOUBLE PRECISION,\n            col2 DOUBLE PRECISION)\n        RETURNS double precision AS\n        $$\n        BEGIN\n          RETURN (col1 * col2)::double precision;\n        END;\n        $$\n        LANGUAGE plpgsql;\n        ''',\n        '''\n        CREATE OR REPLACE FUNCTION database_add_function(col1 DOUBLE PRECISION,\n            col2 DOUBLE PRECISION)\n        RETURNS double precision AS\n        $$\n        BEGIN\n          RETURN (col1 + col2)::double precision;\n        END;\n        $$\n        LANGUAGE plpgsql;\n        '''\n    ]\n\n    mysql_function = [\n        '''\n        CREATE FUNCTION database_multiply_function(col1 FLOAT(32), col2 FLOAT(32))\n        RETURNS FLOAT(32)\n        DETERMINISTIC\n        BEGIN DECLARE multiply_result FLOAT(32); SET multiply_result = col1 * col2;\n        RETURN multiply_result;\n        END\n        ''',\n        '''\n        CREATE FUNCTION database_add_function(col1 FLOAT(32), col2 FLOAT(32))\n        RETURNS FLOAT(32)\n        DETERMINISTIC\n        BEGIN\n            DECLARE add_result FLOAT(32);\n            SET add_result = col1 + col2;\n            RETURN add_result;\n        END\n        '''\n    ]\n    for db_url in [POSTGRESQL_URL, MYSQL_URL]:\n      dialect = db_url.split('+')[0]\n      logger.info(f'Test {dialect} database')\n      if dialect == 'mysql':\n        function_list = mysql_function\n      elif dialect == 'postgresql':\n        function_list = postgresql_function\n      else:\n        raise Exception('Unsupported database')\n\n      gt_engine, aidb_engine = await setup_gt_and_aidb_engine(db_url, data_dir)\n\n      try:\n        async with gt_engine.begin() as conn:\n          await conn.execute(text('DROP FUNCTION IF EXISTS database_multiply_function;'))\n          await conn.execute(text('DROP FUNCTION IF EXISTS database_add_function;'))\n          for function in function_list:\n            await conn.execute(text(function))\n      finally:\n        await gt_engine.dispose()\n\n      # FIXME(ttt-77): create function SQL can't be executed by AIDB engine\n      try:\n        async with aidb_engine._sql_engine.begin() as conn:\n          await conn.execute(text('DROP FUNCTION IF EXISTS database_multiply_function;'))\n          await conn.execute(text('DROP FUNCTION IF EXISTS database_add_function;'))\n          for function in function_list:\n            await conn.execute(text(function))\n      finally:\n        await aidb_engine._sql_engine.dispose()\n\n\n      register_inference_services(aidb_engine, data_dir)\n      self.add_user_defined_function(aidb_engine)\n\n      for query_type, aidb_query, exact_query in queries:\n        logger.info(f'Running query {exact_query} in ground truth database')\n        # Run the query on the ground truth database\n        try:\n          async with gt_engine.begin() as conn:\n            gt_res = await conn.execute(text(exact_query))\n            gt_res = gt_res.fetchall()\n        finally:\n          await gt_engine.dispose()\n        # Run the query on the aidb database\n        logger.info(f'Running query {aidb_query} in aidb database')\n        aidb_res = aidb_engine.execute(aidb_query)\n        assert len(gt_res) == len(aidb_res)\n\n        gt_res = sorted(gt_res)\n        aidb_res = sorted(aidb_res)\n\n        relative_diffs = []\n        for i in range(len(gt_res)):\n          relative_diff = []\n          for gt_res_i, aidb_res_i in zip(gt_res[i], aidb_res[i]):\n            if isinstance(gt_res_i, (int, float, Decimal)):\n              if gt_res_i != 0:\n                dif = abs(gt_res_i - aidb_res_i) / gt_res_i * 100\n                assert dif <= 0.0001\n                relative_diff.append(dif)\n              else:\n                assert aidb_res_i <= 0.00001\n                relative_diff.append(aidb_res_i)\n            else:\n              assert aidb_res_i == aidb_res_i\n              relative_diff.append(0)\n          relative_diffs.append(relative_diff)\n        avg_diff = np.mean(relative_diffs, axis=0)\n        logger.info(f'Avg relative difference percentage between gt_res and aidb_res: {avg_diff}')\n\n      del gt_engine\n      del aidb_engine\n    p.terminate()\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "test_file", "path": "tests/tests_inference_config_validity.py", "content": "import os\nimport unittest\n\nfrom unittest import IsolatedAsyncioTestCase\nfrom tests.db_utils.db_setup import create_db, setup_db, setup_config_tables\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom aidb.engine import Engine\n\n\nclass InferenceConfigIntegrityTests(IsolatedAsyncioTestCase):\n  async def _test_positive_object_detection(self, db_url, aidb_db_fname):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n\n    # Set up the aidb database\n    await create_db(db_url, aidb_db_fname)\n\n    tmp_engine = await setup_db(db_url, aidb_db_fname, data_dir)\n\n    try:\n      async with tmp_engine.begin() as conn:\n        await setup_config_tables(conn)\n    except Exception:\n      raise Exception('Fail to setup config table.')\n    finally:\n      await tmp_engine.dispose()\n\n    del tmp_engine\n\n    # Connect to the aidb database\n    aidb_engine = Engine(\n      f'{db_url}/{aidb_db_fname}',\n      debug=False,\n    )\n    register_inference_services(aidb_engine, data_dir)\n    del aidb_engine\n\n\n  async def _test_positive_only_1_table(self, db_url, aidb_db_fname):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/twitter')\n\n    # Set up the aidb database\n    await create_db(db_url, aidb_db_fname)\n\n    tmp_engine = await setup_db(db_url, aidb_db_fname, data_dir)\n\n    try:\n      async with tmp_engine.begin() as conn:\n        await setup_config_tables(conn)\n    except Exception:\n      raise Exception('Fail to setup config table.')\n    finally:\n      await tmp_engine.dispose()\n\n    del tmp_engine\n    # Connect to the aidb database\n    aidb_engine = Engine(\n      f'{db_url}/{aidb_db_fname}',\n      debug=False,\n    )\n    register_inference_services(aidb_engine, data_dir)\n    del aidb_engine\n\n\n  async def _test_negative_col_by_multiple(self, db_url, aidb_db_fname):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/same_col_by_multiple_services')\n\n    # Set up the aidb database\n    await create_db(db_url, aidb_db_fname)\n\n    tmp_engine = await setup_db(db_url, aidb_db_fname, data_dir)\n    try:\n      async with tmp_engine.begin() as conn:\n        await setup_config_tables(conn)\n    except Exception:\n      raise Exception('Fail to setup config table.')\n    finally:\n      await tmp_engine.dispose()\n\n    del tmp_engine\n    # Connect to the aidb database\n    aidb_engine = Engine(\n      f'{db_url}/{aidb_db_fname}',\n      debug=False,\n    )\n    with self.assertRaises(Exception):\n      register_inference_services(aidb_engine, data_dir)\n    del aidb_engine\n\n\n  async def _test_positive_multi_table_input(self, db_url, aidb_db_fname):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/multi_table_input')\n\n    # Set up the aidb database\n    await create_db(db_url, aidb_db_fname)\n\n    tmp_engine = await setup_db(db_url, aidb_db_fname, data_dir)\n    try:\n      async with tmp_engine.begin() as conn:\n        await setup_config_tables(conn)\n    except Exception:\n      raise Exception('Fail to setup config table.')\n    finally:\n      await tmp_engine.dispose()\n\n    del tmp_engine\n    # Connect to the aidb database\n    aidb_engine = Engine(\n      f'{db_url}/{aidb_db_fname}',\n      debug=False,\n    )\n    register_inference_services(aidb_engine, data_dir)\n    del aidb_engine\n\n\n  async def test_all_tests(self):\n    db_configs = [\n      {'db_url': 'sqlite+aiosqlite://', 'db_name': 'aidb_test.sqlite'},\n      {'db_url': 'postgresql+asyncpg://user:testaidb@localhost:5432', 'db_name': 'aidb_test'},\n      {'db_url': 'mysql+aiomysql://root:testaidb@localhost:3306', 'db_name': 'aidb_test'}\n    ]\n\n    for config in db_configs:\n      await self._test_positive_object_detection(config['db_url'], config['db_name'])\n      await self._test_positive_only_1_table(config['db_url'], config['db_name'])\n      await self._test_positive_multi_table_input(config['db_url'], config['db_name'])\n      await self._test_negative_col_by_multiple(config['db_url'], config['db_name'])\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "test_file", "path": "tests/tests_vector_database_setup.py", "content": "import numpy as np\nimport os\nimport unittest\n\nfrom aidb_utilities.vector_database_setup.vector_database_setup import VectorDatabaseSetup\nfrom aidb.vector_database.chroma_vector_database import ChromaVectorDatabase\nfrom aidb.vector_database.faiss_vector_database import FaissVectorDatabase\nfrom aidb.vector_database.weaviate_vector_database import WeaviateAuth, WeaviateVectorDatabase\nfrom tests.tests_data_store import AidbDataStoreTests\n\nfrom unittest import IsolatedAsyncioTestCase\n\nDB_URL = 'sqlite+aiosqlite:///aidb_datastore.sqlite'\n\n\ndef clean_vector_database():\n  if os.path.exists('tasti.index'):\n    os.remove('tasti.index')\n  if os.path.exists('chroma.sqlite3'):\n    os.remove('chroma.sqlite3')\n\n\ndef test_equality(value):\n  np.random.seed(1234)\n  embeddings = np.random.rand(3, 128)\n  embeddings = (embeddings * 100).astype(int)\n  value = (value * 100).astype(int)\n  assert np.array_equal(embeddings, value)\n\n\nblob_table_name = 'blob00'\nindex_name = 'tasti'\n\nclass AidbVectorDatabaseSetupTests(IsolatedAsyncioTestCase):\n\n  async def test_faiss_set_up(self):\n    clean_vector_database()\n    vd_type = 'FAISS'\n    auth = './'\n\n    vector_database = VectorDatabaseSetup(DB_URL, blob_table_name, vd_type, index_name, auth)\n    await vector_database.setup()\n\n    existing_vector_database = FaissVectorDatabase(auth)\n    value = existing_vector_database.get_embeddings_by_id(index_name, ids=np.array(range(3)), reload=True)\n    test_equality(value)\n\n\n  async def test_chroma_set_up(self):\n    clean_vector_database()\n    vd_type = 'chroma'\n    auth = './'\n\n    vector_database = VectorDatabaseSetup(DB_URL, blob_table_name, vd_type, index_name, auth)\n    await vector_database.setup()\n\n    existing_vector_database = ChromaVectorDatabase(auth)\n    value = existing_vector_database.get_embeddings_by_id(index_name, ids=np.array(range(3)), reload=True)\n    test_equality(value)\n\n\n  @unittest.skip(\"Skip in case of absence of Weaviate credentials\")\n  async def test_weaviate_set_up(self):\n    clean_vector_database()\n    vd_type = 'weaviate'\n    url = ''\n    api_key = os.environ.get('WEAVIATE_API_KEY')\n    auth = WeaviateAuth(url=url, api_key=api_key)\n\n    vector_database = VectorDatabaseSetup(DB_URL, blob_table_name, vd_type, index_name, auth)\n    await vector_database.setup()\n\n    existing_vector_database = WeaviateVectorDatabase(auth)\n    value = existing_vector_database.get_embeddings_by_id(index_name, ids=np.array(range(3)), reload=True)\n    test_equality(value)\n\n\nif __name__ == '__main__':\n  unittest.main()"}
{"type": "test_file", "path": "tests/inference_service_utils/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_query_parsing.py", "content": "import os\nimport unittest\nfrom collections import Counter\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom unittest import IsolatedAsyncioTestCase\n\nfrom aidb.query.query import Query\nfrom tests.inference_service_utils.inference_service_setup import \\\n    register_inference_services\nfrom tests.utils import setup_gt_and_aidb_engine\n\nDB_URL = \"sqlite+aiosqlite://\"\n\n# normal query dataclass\n@dataclass\nclass TestQuery:\n  query_str: str\n  normalized_query_str: str\n  correct_fp: list\n  correct_service: list\n  correct_tables: list\n  num_of_select_clauses: int\n    \n    \n  def are_lists_equal(self, list1, list2):\n    for sub_list1, sub_list2 in zip(list1, list2):\n      assert Counter(sub_list1) == Counter(sub_list2)\n      \n      \n  def _test_query(self, config):\n    query = Query(self.query_str, config)\n    # test the number of queries\n    assert len(query.all_queries_in_expressions) == self.num_of_select_clauses\n    assert query.query_after_normalizing.sql_str == self.normalized_query_str\n    and_fp = []\n    for and_connected in query.filtering_predicates:\n      or_fp = []\n      for or_connected in and_connected:\n        or_fp.append(or_connected.sql())\n      and_fp.append(or_fp)\n    self.are_lists_equal(and_fp, self.correct_fp)\n    self.are_lists_equal(query.inference_engines_required_for_filtering_predicates, self.correct_service)\n    self.are_lists_equal(query.tables_in_filtering_predicates, self.correct_tables)  \n\n\n# udf query dataclasses       \n@dataclass\nclass UdfMapping:\n  col_names: list\n  function_name: str\n  result_col_name: list\n\n\n@dataclass\nclass DataframeSql:\n  udf_mapping: list\n  select_col: list\n  filter_predicate: str\n\n\n@dataclass\nclass UdfTestQuery:\n  query_str: str\n  query_after_extraction: str\n  dataframe_sql: DataframeSql\n\n\n  def _test_equality(self, config):\n    query = Query(self.query_str, config)\n    dataframe_sql, query_after_extraction = query.udf_query\n    assert query_after_extraction.sql_str==self.query_after_extraction\n    assert len(dataframe_sql['udf_mapping']) == len(self.dataframe_sql.udf_mapping)\n    # unpack dict values into dataclass and verify that instance values are equal\n    assert all (any((e1==UdfMapping(**e2)) for e2 in dataframe_sql['udf_mapping']) \n                for e1 in self.dataframe_sql.udf_mapping)\n    assert dataframe_sql['select_col'] == self.dataframe_sql.select_col\n    filter_predicate = query.convert_and_connected_fp_to_exp(dataframe_sql['filter_predicate'])\n    if filter_predicate:\n      filter_predicate = filter_predicate.sql()  \n    assert filter_predicate == self.dataframe_sql.filter_predicate\n\n\n@dataclass\nclass QueryFilteringPredicatesParsing:\n  query_str: str\n  correct_fp: str\n\n  def _test_filtering_predicates(self, config):\n    query = Query(self.query_str, config)\n    and_connected = []\n    filtering_predicates = query.filtering_predicates\n    for fp in filtering_predicates:\n      and_connected.append(' OR '.join([p.sql() for p in fp]))\n    and_connected = [f'({fp})' for fp in and_connected]\n    parsed_fp = ' AND '.join(and_connected)\n    if parsed_fp != self.correct_fp:\n      raise AssertionError(f\"Failed to parse query: '{self.query_str}'. \\n\"\n                           f\"Parsed fp: '{parsed_fp}', but the expected type is '{self.correct_fp}'.\")\n\n          \nclass QueryParsingTests(IsolatedAsyncioTestCase):\n\n  async def test_nested_query(self):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n    gt_engine, aidb_engine = await setup_gt_and_aidb_engine(DB_URL, data_dir)\n\n    register_inference_services(aidb_engine, data_dir)\n\n    config = aidb_engine._config\n    \n    queries = {\n    \"test_query_0\": TestQuery(\n        '''\n        SELECT color as alias_color\n        FROM colors02 LEFT JOIN objects00 ON colors02.frame = objects00.frame\n        WHERE alias_color IN (SELECT table2.color AS alias_color2 FROM colors02 AS table2)\n        OR colors02.object_id > (SELECT AVG(ob.object_id) FROM objects00 AS ob WHERE frame > 100);\n        ''',\n        (\"SELECT colors02.color \"\n         \"FROM colors02 LEFT JOIN objects00 ON colors02.frame = objects00.frame \"\n         \"WHERE colors02.color IN (SELECT table2.color AS alias_color2 FROM colors02 AS table2) \"\n         \"OR colors02.object_id > (SELECT AVG(ob.object_id) FROM objects00 AS ob WHERE frame > 100)\"),\n        # test replacing column with root column in filtering predicate,\n        # the root column of 'colors02.object_id' is 'objects00'\n        [['colors02.color IN (SELECT table2.color AS alias_color2 FROM colors02 AS table2)',\n          'objects00.object_id > (SELECT AVG(ob.object_id) FROM objects00 AS ob WHERE frame > 100)']],    \n        # filter predicates connected by OR are in same set\n        [{'colors02', 'objects00'}],\n        [{'colors02', 'objects00'}],            \n        3\n    ),\n    # test table alias\n    \"test_query_1\": TestQuery(\n        '''\n        SELECT table1.color \n        FROM colors02 AS table1 WHERE frame IN (SELECT * FROM blobs_00)\n        AND object_id > 0\n        ''',\n        # test table alias\n        (\"SELECT colors02.color FROM colors02 \"\n         \"WHERE colors02.frame IN (SELECT * FROM blobs_00) \"\n         \"AND colors02.object_id > 0\"),\n        # test replacing column with root column in filtering predicate,\n        # the root column of 'colors02.object_id' is 'objects00'\n        [['blobs_00.frame IN (SELECT * FROM blobs_00)'],\n         ['objects00.object_id > 0']],\n        # filter predicates connected by AND are in different set \n        [set(), {'objects00'}],\n        [set(), {'objects00'}],\n        2\n    ),\n    # test sub-subquery\n    \"test_query_2\": TestQuery(\n        '''\n        SELECT frame, object_id \n        FROM colors02 AS cl\n        WHERE cl.object_id > (SELECT AVG(object_id) FROM objects00\n        WHERE frame > (SELECT AVG(frame) FROM blobs_00 WHERE frame > 500))\n        ''',\n        (\"SELECT colors02.frame, colors02.object_id FROM colors02 \"\n         \"WHERE colors02.object_id > (SELECT AVG(object_id) FROM objects00 WHERE frame > \"\n         \"(SELECT AVG(frame) FROM blobs_00 WHERE frame > 500))\"),\n        [[\"objects00.object_id > (SELECT AVG(object_id) FROM objects00 \" \n          \"WHERE frame > (SELECT AVG(frame) FROM blobs_00 WHERE frame > 500))\"]],\n        [{'objects00'}], \n        [{'objects00'}], \n        3\n    ),\n    # test multiple aliases\n    \"test_query_3\": TestQuery(\n        '''\n        SELECT color, table2.x_min\n        FROM colors02 table1 LEFT JOIN objects00 table2 ON table1.frame = table2.frame\n        WHERE color IN (SELECT table3.color AS alias_color2 FROM colors02 AS table3)\n        AND table2.object_id > (SELECT AVG(ob.object_id) FROM objects00 AS ob WHERE frame > 500);\n        ''', \n        (\"SELECT colors02.color, objects00.x_min \"\n         \"FROM colors02 LEFT JOIN objects00 ON colors02.frame = objects00.frame \"\n         \"WHERE colors02.color IN (SELECT table3.color AS alias_color2 FROM colors02 AS table3) \"\n         \"AND objects00.object_id > (SELECT AVG(ob.object_id) FROM objects00 AS ob WHERE frame > 500)\"),\n        [[\"colors02.color IN (SELECT table3.color AS alias_color2 FROM colors02 AS table3)\"],\n         [\"objects00.object_id > (SELECT AVG(ob.object_id) FROM objects00 AS ob WHERE frame > 500)\"]],\n        [{'colors02'}, {'objects00'}],\n        [{'colors02'}, {'objects00'}],\n        3\n    ),\n    # comparison between subquery\n    \"test_query_4\": TestQuery(\n        '''\n        SELECT color, table2.x_min\n        FROM colors02 table1 LEFT JOIN objects00 table2 ON table1.frame = table2.frame\n        WHERE (SELECT table3.color AS alias_color2 FROM colors02 AS table3)\n        > (SELECT AVG(ob.object_id) FROM objects00 AS ob WHERE frame > 500);\n        ''',\n        (\"SELECT colors02.color, objects00.x_min \"\n         \"FROM colors02 LEFT JOIN objects00 ON colors02.frame = objects00.frame \"\n         \"WHERE (SELECT table3.color AS alias_color2 FROM colors02 AS table3) \"\n         \"> (SELECT AVG(ob.object_id) FROM objects00 AS ob WHERE frame > 500)\"),\n        [[\"(SELECT table3.color AS alias_color2 FROM colors02 AS table3) > \"\n        \"(SELECT AVG(ob.object_id) FROM objects00 AS ob WHERE frame > 500)\"]],\n        [{}],\n        [{}],\n        3\n    ),\n    \"test_query_5\": TestQuery(\n        '''\n        SELECT color AS col1, table2.x_min AS col2, table2.y_min AS col3\n        FROM colors02 table1 LEFT JOIN objects00 table2 ON table1.frame = table2.frame;\n        ''',\n        (\"SELECT colors02.color, objects00.x_min, objects00.y_min \"\n         \"FROM colors02 LEFT JOIN objects00 ON colors02.frame = objects00.frame\"),\n        [[]],\n        [{}], \n        [{}],\n        1\n    ),\n    \"test_query_6\": TestQuery( \n        '''\n        SELECT color AS col1, table2.x_min AS col2, table3.frame AS col3\n        FROM colors02 table1 LEFT JOIN objects00 table2 ON table1.frame = table2.frame\n        JOIN blobs_00 table3 ON table2.frame = table3.frame;\n        ''',\n        (\"SELECT colors02.color, objects00.x_min, blobs_00.frame \"\n         \"FROM colors02 LEFT JOIN objects00 ON colors02.frame = objects00.frame \"\n         \"JOIN blobs_00 ON objects00.frame = blobs_00.frame\"),\n        [[]],\n        [{}],\n        [{}],\n        1\n    ),\n    \"test_query_7\": TestQuery(\n        '''\n        SELECT color, x_min AS col2, colors02.frame AS col3\n        FROM colors02 JOIN objects00 table2 ON colors02.frame = table2.frame\n        WHERE color = 'blue' AND x_min > 600;\n        ''',\n        (\"SELECT colors02.color, objects00.x_min, colors02.frame \"\n         \"FROM colors02 JOIN objects00 ON colors02.frame = objects00.frame \"\n         \"WHERE colors02.color = 'blue' AND objects00.x_min > 600\"),\n        [[\"colors02.color = 'blue'\"], \n         ['objects00.x_min > 600']],\n        [{'colors02'}, {'objects00'}],\n        [{'colors02'}, {'objects00'}],\n        1\n    ),\n    \"test_query_8\": TestQuery(\n        '''\n        SELECT color, USERFUNCTION(x_min, y_min, x_max, y_max)\n        FROM colors02 JOIN objects00 table2 ON colors02.frame = table2.frame\n        WHERE table2.frame > 10000 OR y_max < 800;\n        ''',\n        (\"SELECT colors02.color, USERFUNCTION(objects00.x_min, objects00.y_min, \"\n         \"objects00.x_max, objects00.y_max) \"\n         \"FROM colors02 JOIN objects00 ON colors02.frame = objects00.frame \"\n         \"WHERE objects00.frame > 10000 OR objects00.y_max < 800\"),\n        [['blobs_00.frame > 10000', 'objects00.y_max < 800']], \n        [{'objects00'}],\n        [{'objects00'}],\n        1\n    )\n}\n    for i in range(len(queries)):\n      queries[f'test_query_{i}']._test_query(config)\n\n\n  # We don't support using approximate query as a subquery\n  async def test_approximate_query_as_subquery(self):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n    gt_engine, aidb_engine = await setup_gt_and_aidb_engine(DB_URL, data_dir)\n\n    register_inference_services(aidb_engine, data_dir)\n\n    config = aidb_engine._config\n    queries = {\n      # approx aggregation as a subquery\n      \"query_str0\": \n          '''\n          SELECT x_min \n          FROM  objects00\n          WHERE x_min > (SELECT AVG(x_min) FROM objects00 ERROR_TARGET 10% CONFIDENCE 95%)\n          AND table2.object_id > (SELECT AVG(ob.object_id) FROM objects00 AS ob WHERE frame > 500);\n          ''',\n      # approx select as a subquery\n      \"query_str1\": \n          '''\n          SELECT x_min \n          FROM  objects00\n          WHERE frame IN (SELECT frame FROM colors02 where color LIKE 'blue'\n          RECALL_TARGET {RECALL_TARGET}% CONFIDENCE 95%;)\n          AND table2.object_id > (SELECT AVG(ob.object_id) FROM objects00 AS ob WHERE frame > 500);\n          '''\n    }\n    for i in range(len(queries)):\n      parsed_query = Query(queries[f'query_str{i}'], config)\n      with self.assertRaises(Exception):\n        _ = parsed_query.all_queries_in_expressions\n      \n\n  async def test_correct_approximate_query(self):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n    gt_engine, aidb_engine = await setup_gt_and_aidb_engine(DB_URL, data_dir)\n\n    register_inference_services(aidb_engine, data_dir)\n\n    config = aidb_engine._config\n    \n    queries = {\n    # approx aggregation as a subquery\n    \"test_query_0\": TestQuery(\n        '''\n        SELECT AVG(x_min) \n        FROM  objects00\n        WHERE frame > (SELECT AVG(frame) FROM blobs_00)\n        ERROR_TARGET 10% CONFIDENCE 95%;\n        ''',\n        (\"SELECT AVG(objects00.x_min) \"\n         \"FROM objects00 \"\n         \"WHERE objects00.frame > (SELECT AVG(frame) FROM blobs_00) ERROR_TARGET 10% CONFIDENCE 95%\"),\n        [[\"blobs_00.frame > (SELECT AVG(frame) FROM blobs_00)\"]],\n        # filter predicates connected by OR are in same set\n        [{}],\n        [{}], \n        2\n    ),\n    \"test_query_1\": TestQuery(\n        '''\n        SELECT frame \n        FROM colors02\n        WHERE color IN (SELECT color FROM colors02 WHERE frame > 10000)\n        RECALL_TARGET 80%\n        CONFIDENCE 95%;\n        ''',\n        (\"SELECT colors02.frame FROM colors02 \"\n         \"WHERE colors02.color IN (SELECT color FROM colors02 WHERE frame > 10000) \"\n         \"RECALL_TARGET 80% \"\n         \"CONFIDENCE 95%\"),\n        [[\"colors02.color IN (SELECT color FROM colors02 WHERE frame > 10000)\"]],\n        [{'colors02'}],\n        [{'colors02'}],\n        2\n    )\n}\n    for i in range(len(queries)):\n      queries[f'test_query_{i}']._test_query(config)\n\n  async def test_udf_query(self):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n    gt_engine, aidb_engine = await setup_gt_and_aidb_engine(DB_URL, data_dir)\n\n    aidb_engine.register_user_defined_function('SUM_FUNCTION', None)\n    aidb_engine.register_user_defined_function('IS_EQUAL', None)\n    aidb_engine.register_user_defined_function('POWER_FUNCTION', None)\n    aidb_engine.register_user_defined_function('MAX_FUNCTION', None)\n    aidb_engine.register_user_defined_function('MULTIPLY_FUNCTION', None)\n    aidb_engine.register_user_defined_function('FUNCTION1', None)\n    aidb_engine.register_user_defined_function('FUNCTION2', None)\n    aidb_engine.register_user_defined_function('COLORS_INFERENCE', None)\n\n    register_inference_services(aidb_engine, data_dir)\n    config = aidb_engine._config\n\n    queries={\n    # user defined function in SELECT clause\n    \"test_query_0\" : UdfTestQuery( \n        '''\n        SELECT x_min, FUNCTION1(x_min, y_min), y_max, FUNCTION2()\n        FROM objects00\n        WHERE x_min > 600\n        ''',\n        \"SELECT objects00.x_min AS col__0, objects00.y_min AS col__1, objects00.y_max AS col__2 \"\n        \"FROM objects00 \"\n        \"WHERE (objects00.x_min > 600)\",\n        DataframeSql(\n            [UdfMapping(\n                ['col__0', 'col__1'],\n                'FUNCTION1',\n                ['function__0']),\n             UdfMapping( \n                [],\n                'FUNCTION2',\n                ['function__1'])\n            ],\n            ['col__0', 'function__0', 'col__2', 'function__1'],\n            None\n        )\n    ),\n    \n    # test function with constant parameters\n    \"test_query_1\": UdfTestQuery(\n        '''\n        SELECT x_min, FUNCTION1(y_min, 2, 3)\n        FROM objects00\n        WHERE x_min > 600\n        ''',\n        \"SELECT objects00.x_min AS col__0, objects00.y_min AS col__1, 2 AS col__2, 3 AS col__3 \"\n        \"FROM objects00 \"\n        \"WHERE (objects00.x_min > 600)\",\n        DataframeSql(\n            [UdfMapping( \n                ['col__1', 'col__2', 'col__3'],\n                'FUNCTION1',\n                ['function__0'])\n            ],\n            ['col__0', 'function__0'],\n            None\n        )\n    ),\n\n    # user defined function in JOIN clause\n    \"test_query_2\": UdfTestQuery(\n        '''\n        SELECT objects00.frame, x_min, y_max, color\n        FROM objects00 JOIN colors02 ON IS_EQUAL(objects00.frame, colors02.frame) = TRUE\n            AND IS_EQUAL(objects00.object_id, colors02.object_id) = TRUE\n        WHERE color = 'blue'\n        ''',\n        \"SELECT objects00.frame AS col__0, objects00.x_min AS col__1, objects00.y_max AS col__2, \"\n        \"colors02.color AS col__3, colors02.frame AS col__4, objects00.object_id AS col__5, colors02.object_id AS col__6 \"\n        \"FROM objects00 CROSS JOIN colors02 \"\n        \"WHERE (colors02.color = 'blue')\",\n        DataframeSql(\n            [UdfMapping(\n                ['col__0', 'col__4'],\n                'IS_EQUAL',\n                ['function__0']),\n            UdfMapping(\n                ['col__5', 'col__6'],\n                'IS_EQUAL', \n                ['function__1'])\n            ],\n            ['col__0', 'col__1', 'col__2', 'col__3'],\n            '(function__0 = TRUE) AND (function__1 = TRUE)'\n        )\n    ),\n\n    # user defined function in WHERE clause\n    \"test_query_3\": UdfTestQuery(\n        '''\n        SELECT objects00.frame, x_min, y_max, color\n        FROM objects00 JOIN colors02\n        WHERE IS_EQUAL(objects00.frame, colors02.frame) = TRUE\n            AND IS_EQUAL(objects00.object_id, colors02.object_id) = TRUE AND SUM_FUNCTION(x_max, y_min) > 1500\n        ''',\n        \"SELECT objects00.frame AS col__0, objects00.x_min AS col__1, objects00.y_max AS col__2, \"\n        \"colors02.color AS col__3, colors02.frame AS col__4, objects00.object_id AS col__5, colors02.object_id AS col__6, \"\n        \"objects00.x_max AS col__7, objects00.y_min AS col__8 \"\n        \"FROM objects00 CROSS JOIN colors02\",\n        DataframeSql(\n            [UdfMapping(\n                ['col__0', 'col__4'],\n                'IS_EQUAL',\n                ['function__0']),\n            UdfMapping(\n                ['col__5', 'col__6'],\n                'IS_EQUAL',\n                ['function__1']),\n            UdfMapping(\n                ['col__7', 'col__8'],\n                'SUM_FUNCTION', \n                ['function__2'])\n            ],\n            ['col__0', 'col__1', 'col__2', 'col__3'],\n            '(function__0 = TRUE) AND (function__1 = TRUE) AND (function__2 > 1500)'\n        )\n    ),\n\n    # user defined function in SELECT, JOIN, WHERE clause\n    \"test_query_4\": UdfTestQuery(\n        '''\n        SELECT MULTIPLY_FUNCTION(x_min, y_max), color\n        FROM objects00 JOIN colors02 ON IS_EQUAL(objects00.frame, colors02.frame) = TRUE\n            AND IS_EQUAL(objects00.object_id, colors02.object_id) = TRUE\n        WHERE SUM_FUNCTION(x_min, y_min) > 1500\n        ''',\n        \"SELECT objects00.x_min AS col__0, objects00.y_max AS col__1, colors02.color AS col__2, \"\n        \"objects00.frame AS col__3, colors02.frame AS col__4, objects00.object_id AS col__5, colors02.object_id AS col__6, \"\n        \"objects00.y_min AS col__7 \"\n        \"FROM objects00 CROSS JOIN colors02\",\n        DataframeSql(\n            [UdfMapping(\n                  ['col__0', 'col__1'],\n                  'MULTIPLY_FUNCTION',\n                  ['function__0']),\n             UdfMapping(\n                  ['col__3', 'col__4'],\n                  'IS_EQUAL',\n                  ['function__1']),\n             UdfMapping(\n                  ['col__5', 'col__6'],\n                  'IS_EQUAL',\n                  ['function__2']),\n             UdfMapping(\n                  ['col__0', 'col__7'],\n                  'SUM_FUNCTION',\n                  ['function__3'])\n            ],\n            ['function__0', 'col__2'],\n            '(function__1 = TRUE) AND (function__2 = TRUE) AND (function__3 > 1500)'\n        ) \n    ),\n\n    # OR operator in WHERE clause\n    \"test_query_5\": UdfTestQuery(\n        '''\n        SELECT x_min, y_max, color\n        FROM objects00 JOIN colors02 ON IS_EQUAL(objects00.frame, colors02.frame) = TRUE\n            AND IS_EQUAL(objects00.object_id, colors02.object_id) = TRUE\n        WHERE SUM_FUNCTION(x_min, y_min) > 1500 OR color = 'blue'\n        ''',\n        \"SELECT objects00.x_min AS col__0, objects00.y_max AS col__1, colors02.color AS col__2, \"\n        \"objects00.frame AS col__3, colors02.frame AS col__4, objects00.object_id AS col__5, colors02.object_id AS col__6, \"\n        \"objects00.y_min AS col__7 \"\n        \"FROM objects00 CROSS JOIN colors02\",\n        DataframeSql(\n            [UdfMapping(\n                ['col__3', 'col__4'],\n                'IS_EQUAL',\n                ['function__0']),\n            UdfMapping(        \n                ['col__5', 'col__6'],\n                'IS_EQUAL',\n                ['function__1']),\n            UdfMapping(\n                ['col__0', 'col__7'],\n                'SUM_FUNCTION',\n                ['function__2'])\n            ], \n            ['col__0', 'col__1', 'col__2'],\n            \"(function__0 = TRUE) AND (function__1 = TRUE) AND (function__2 > 1500 OR col__2 = 'blue')\"\n        )\n    ),\n  \n    # comparison between user defined function\n    \"test_query_6\": UdfTestQuery(\n        '''\n        SELECT x_min, y_max, color\n        FROM objects00 JOIN colors02 ON IS_EQUAL(objects00.frame, colors02.frame) = TRUE\n            AND IS_EQUAL(objects00.object_id, colors02.object_id) = TRUE\n        WHERE SUM_FUNCTION(x_min, y_min) > MULTIPLY_FUNCTION(x_min, y_min)\n        ''',\n        \"SELECT objects00.x_min AS col__0, objects00.y_max AS col__1, colors02.color AS col__2, \"\n        \"objects00.frame AS col__3, colors02.frame AS col__4, objects00.object_id AS col__5, colors02.object_id AS col__6, \"\n        \"objects00.y_min AS col__7 \"\n        \"FROM objects00 CROSS JOIN colors02\",\n        DataframeSql(\n            [UdfMapping( \n                ['col__3', 'col__4'],\n                'IS_EQUAL',\n                ['function__0']),\n            UdfMapping( \n                ['col__5', 'col__6'],\n                'IS_EQUAL',\n                ['function__1']),\n            UdfMapping( \n                ['col__0', 'col__7'],\n                'SUM_FUNCTION',\n                ['function__2']),\n            UdfMapping( \n                ['col__0', 'col__7'],\n                'MULTIPLY_FUNCTION',\n                ['function__3'])\n            ],\n            ['col__0', 'col__1', 'col__2'], \n            \"(function__0 = TRUE) AND (function__1 = TRUE) AND (function__2 > function__3)\"\n        )\n    ),\n\n    # test user defined function with alias\n    \"test_query_7\": UdfTestQuery(\n        '''\n        SELECT COLORS_INFERENCE(frame, object_id) AS (output1, output2, output3), x_min, y_max\n        FROM objects00\n        WHERE (x_min > 600 AND output1 LIKE 'blue') OR (y_max < 1000 AND x_max < 1000)\n        ''',\n        \"SELECT objects00.frame AS col__0, objects00.object_id AS col__1, objects00.x_min AS col__2, \"\n        \"objects00.y_max AS col__3, objects00.x_max AS col__4 \"\n        \"FROM objects00 \"\n        \"WHERE (objects00.x_min > 600 OR objects00.y_max < 1000) AND (objects00.x_min > 600 OR objects00.x_max < 1000)\",\n        DataframeSql(\n            [UdfMapping( \n                ['col__0', 'col__1'],\n                'COLORS_INFERENCE',\n                ['output1', 'output2', 'output3'])\n            ],\n            ['output1', 'output2', 'output3', 'col__2', 'col__3'],\n            \"(output1 LIKE 'blue' OR col__3 < 1000) AND (output1 LIKE 'blue' OR col__4 < 1000)\"\n        )\n    ),\n    \"test_query_8\": UdfTestQuery(\n        '''\n        SELECT COLORS_INFERENCE(frame, object_id) AS (output1, output2, output3), x_min AS col1, y_max AS col2\n        FROM objects00\n        WHERE (col1 > 600 AND output1 LIKE 'blue') OR (col2 < 1000 AND x_max < 1000)\n        ''',\n        \"SELECT objects00.frame AS col__0, objects00.object_id AS col__1, objects00.x_min AS col__2, \"\n        \"objects00.y_max AS col__3, objects00.x_max AS col__4 \"\n        \"FROM objects00 \"\n        \"WHERE (objects00.x_min > 600 OR objects00.y_max < 1000) AND (objects00.x_min > 600 OR objects00.x_max < 1000)\",\n        DataframeSql(\n            [UdfMapping( \n                ['col__0', 'col__1'], \n                'COLORS_INFERENCE',\n                ['output1', 'output2', 'output3'])\n            ], \n            ['output1', 'output2', 'output3', 'col__2', 'col__3'],\n            \"(output1 LIKE 'blue' OR col__3 < 1000) AND (output1 LIKE 'blue' OR col__4 < 1000)\"\n        )\n    ),\n\n    # single output user defined function with alias\n    \"test_query_9\": UdfTestQuery(\n        '''\n        SELECT MAX_FUNCTION(y_max, y_min) AS output1, POWER_FUNCTION(x_min, 2) AS output2, y_min, color\n        FROM objects00 join colors02\n        WHERE IS_EQUAL(objects00.frame, colors02.frame) = TRUE AND IS_EQUAL(objects00.object_id, colors02.object_id)\n            = TRUE AND (x_min > 600 OR (x_max >600 AND y_min > 800)) AND output1 > 1000 AND output2 > 640000\n        ''',\n        \"SELECT objects00.y_max AS col__0, objects00.y_min AS col__1, objects00.x_min AS col__2, 2 AS col__3, \"\n        \"colors02.color AS col__4, objects00.frame AS col__5, colors02.frame AS col__6, objects00.object_id AS col__7, \"\n        \"colors02.object_id AS col__8 \"\n        \"FROM objects00 CROSS JOIN colors02 \"\n        \"WHERE (objects00.x_min > 600 OR objects00.x_max > 600) AND (objects00.x_min > 600 OR objects00.y_min > 800)\",\n        DataframeSql(\n           [UdfMapping(\n               ['col__0', 'col__1'],\n               'MAX_FUNCTION',\n               ['output1']),\n            UdfMapping(        \n               ['col__2', 'col__3'],\n               'POWER_FUNCTION',\n               ['output2']),\n            UdfMapping(\n               ['col__5', 'col__6'],\n               'IS_EQUAL',\n               ['function__2']),\n            UdfMapping(\n               ['col__7', 'col__8'],\n               'IS_EQUAL',\n               ['function__3']),\n           ], \n           ['output1', 'output2', 'col__1', 'col__4'],\n           \"(function__2 = TRUE) AND (function__3 = TRUE) AND (output1 > 1000) AND (output2 > 640000)\"\n         )\n    ),\n    \n    # test user defined functions both within the database and within AIDB\n    \"test_query_10\": UdfTestQuery(\n        '''\n        SELECT MULTIPLY_FUNCTION(x_min, y_min), DATABASE_MULTIPLY_FUNCTION(x_min, y_min), x_max, y_max\n        FROM objects00\n        WHERE x_min > 600 AND y_max < 1000\n        ''',\n        \"SELECT objects00.x_min AS col__0, objects00.y_min AS col__1, DATABASE_MULTIPLY_FUNCTION(objects00.x_min, \"\n        \"objects00.y_min) AS col__2, objects00.x_max AS col__3, objects00.y_max AS col__4 \"\n        \"FROM objects00 \"\n        \"WHERE (objects00.x_min > 600) AND (objects00.y_max < 1000)\",\n        DataframeSql(\n            [UdfMapping(\n                ['col__0', 'col__1'], \n                'MULTIPLY_FUNCTION',\n                ['function__0'])\n            ],\n            ['function__0', 'col__2', 'col__3', 'col__4'],\n            None\n        )\n    ),\n    \n  \n    \"test_query_11\": UdfTestQuery(\n        '''\n        SELECT DATABASE_ADD_FUNCTION(y_max, x_min), MULTIPLY_FUNCTION(y_min, y_max), color\n        FROM objects00 join colors02 ON IS_EQUAL(objects00.frame, colors02.frame) = TRUE\n            AND IS_EQUAL(objects00.object_id, colors02.object_id) = TRUE\n        WHERE x_min > 600 OR (x_max >600 AND y_min > 800)\n        ''',\n        \"SELECT DATABASE_ADD_FUNCTION(objects00.y_max, objects00.x_min) AS col__0, objects00.y_min AS col__1, \"\n        \"objects00.y_max AS col__2, colors02.color AS col__3, objects00.frame AS col__4, colors02.frame AS col__5, \"\n        \"objects00.object_id AS col__6, colors02.object_id AS col__7 \"\n        \"FROM objects00 CROSS JOIN colors02 \"\n        \"WHERE (objects00.x_min > 600 OR objects00.x_max > 600) AND (objects00.x_min > 600 OR objects00.y_min > 800)\",\n        DataframeSql(\n            [UdfMapping(\n                ['col__1', 'col__2'],\n                'MULTIPLY_FUNCTION',\n                ['function__0']),\n             UdfMapping(\n                ['col__4', 'col__5'],\n                'IS_EQUAL',\n                ['function__1']),\n             UdfMapping(\n                ['col__6', 'col__7'],\n                'IS_EQUAL',\n                ['function__2']),\n            ],\n                ['col__0', 'function__0', 'col__3'],\n                '(function__1 = TRUE) AND (function__2 = TRUE)'\n        )\n    ),    \n    \n    \"test_query_12\": UdfTestQuery(\n        '''\n        SELECT frame, DATABASE_MULTIPLY_FUNCTION(x_min, y_min), SUM_FUNCTION(x_max, y_max)\n        FROM objects00\n        WHERE (MULTIPLY_FUNCTION(x_min, y_min) > 400000 AND DATABASE_ADD_FUNCTION(y_max, x_min) < 1600)\n            OR DATABASE_MULTIPLY_FUNCTION(x_min, y_min) > 500000\n        ''',\n        \"SELECT objects00.frame AS col__0, DATABASE_MULTIPLY_FUNCTION(objects00.x_min, objects00.y_min) AS col__1, \"\n        \"objects00.x_max AS col__2, objects00.y_max AS col__3, objects00.x_min AS col__4, objects00.y_min AS col__5 \"\n        \"FROM objects00 \"\n        \"WHERE (DATABASE_ADD_FUNCTION(objects00.y_max, objects00.x_min) < 1600 OR \"\n        \"DATABASE_MULTIPLY_FUNCTION(objects00.x_min, objects00.y_min) > 500000)\",\n        DataframeSql(\n            [UdfMapping(\n                ['col__2', 'col__3'],\n                'SUM_FUNCTION',\n                ['function__0']),\n             UdfMapping(\n                ['col__4', 'col__5'],\n                'MULTIPLY_FUNCTION',\n                ['function__1']),\n            ],\n              ['col__0', 'col__1', 'function__0'],\n              '(function__1 > 400000 OR col__1 > 500000)'\n        )\n    ),\n    \n    \"test_query_13\": UdfTestQuery(\n        '''\n        SELECT frame, DATABASE_MULTIPLY_FUNCTION(x_min, y_min), SUM_FUNCTION(x_max, y_max) AS output1\n        FROM objects00\n        WHERE (MULTIPLY_FUNCTION(x_min, y_min) > 400000 AND output1 < 1600)\n            OR DATABASE_MULTIPLY_FUNCTION(x_min, y_min) > 500000\n        ''',\n        \"SELECT objects00.frame AS col__0, DATABASE_MULTIPLY_FUNCTION(objects00.x_min, objects00.y_min) AS col__1, \"\n        \"objects00.x_max AS col__2, objects00.y_max AS col__3, objects00.x_min AS col__4, objects00.y_min AS col__5 \"\n        \"FROM objects00\",\n        DataframeSql(\n            [UdfMapping(\n                ['col__2', 'col__3'],\n                'SUM_FUNCTION',\n                ['output1']),\n              UdfMapping(\n                ['col__4', 'col__5'],\n                'MULTIPLY_FUNCTION',\n                ['function__1']),\n              ],\n                ['col__0', 'col__1', 'output1'], \n                '(function__1 > 400000 OR col__1 > 500000) AND (output1 < 1600 OR col__1 > 500000)'\n        )\n    )\n  }\n    for i in range(len(queries)):\n      queries[f'test_query_{i}']._test_equality(config)\n\n\n  async def test_invalid_udf_query(self):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n    gt_engine, aidb_engine = await setup_gt_and_aidb_engine(DB_URL, data_dir)\n\n    aidb_engine.register_user_defined_function('FUNCTION1', None)\n    aidb_engine.register_user_defined_function('FUNCTION2', None)\n\n    register_inference_services(aidb_engine, data_dir)\n    config = aidb_engine._config\n\n    invalid_query_str = [\n      '''SELECT FUNCTION1(AVG(x_min), AVG(y_min)) FROM objects00 ERROR_TARGET 10% CONFIDENCE 95%;''',\n      '''SELECT FUNCTION1(x_min) FROM objects00 RECALL_TARGET 90% CONFIDENCE 95%;''',\n      '''SELECT FUNCTION1(x_min) FROM objects00 LIMIT 100;''',\n      '''SELECT FUNCTION1(x_min) FROM objects00 WHERE y_min > (SELECT AVG(y_min) FROM objects00);''',\n      '''SELECT FUNCTION1(FUNCTION2(x_min)) FROM objects00 WHERE y_min > (SELECT AVG(y_min) FROM objects00);''',\n      '''SELECT FUNCTION1(SUM(x_min), SUM(y_max)) FROM objects00;'''\n    ]\n    for query_str in invalid_query_str:\n      query = Query(query_str, config)\n      assert query.is_udf_query == True\n      with self.assertRaises(Exception):\n       query.check_udf_query_validity()\n\n\n  async def test_filtering_predicates(self):\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, 'data/jackson')\n    gt_engine, aidb_engine = await setup_gt_and_aidb_engine(DB_URL, data_dir)\n    config = aidb_engine._config\n\n    queries = {\n      'test_query_0': QueryFilteringPredicatesParsing(\n        '''\n        SELECT *\n        FROM objects00\n        WHERE x_min > 600 OR objects00.frame > 700 AND objects00.frame < 1000 OR x_max = 1\n        ''',\n        \"(objects00.x_min > 600 OR blobs_00.frame > 700 OR objects00.x_max = 1) \"\n        \"AND (objects00.x_min > 600 OR blobs_00.frame < 1000 OR objects00.x_max = 1)\"\n      ),\n      'test_query_1': QueryFilteringPredicatesParsing(\n        '''\n        SELECT *\n        FROM objects00\n        WHERE (y_min < 300 OR y_max > 700) AND (frame >= 500 AND frame <= 600) OR x_min = 100\n        ''',\n        \"(blobs_00.frame >= 500 OR objects00.x_min = 100) \"\n        \"AND (blobs_00.frame <= 600 OR objects00.x_min = 100) \"\n        \"AND (objects00.y_min < 300 OR objects00.y_max > 700 OR objects00.x_min = 100)\"\n      ),\n      'test_query_2': QueryFilteringPredicatesParsing(\n        '''\n        SELECT *\n        FROM objects00\n        WHERE (x_min < 200 OR y_max > 800) AND frame < 400 OR NOT (x_max = 900)\n        ''',\n        \"(blobs_00.frame < 400 OR NOT objects00.x_max = 900) \"\n        \"AND (objects00.x_min < 200 OR objects00.y_max > 800 OR NOT objects00.x_max = 900)\"\n      ),\n      'test_query_3': QueryFilteringPredicatesParsing(\n        '''\n        SELECT *\n        FROM objects00\n        WHERE (x_max > 500 OR (y_min < 250 AND y_max > 750)) AND NOT (frame >= 300 OR frame <= 800)   \n        ''',\n        \"(NOT blobs_00.frame >= 300) \"\n        \"AND (NOT blobs_00.frame <= 800) \"\n        \"AND (objects00.x_max > 500 OR objects00.y_min < 250) \"\n        \"AND (objects00.x_max > 500 OR objects00.y_max > 750)\"\n      ),\n      'test_query_4': QueryFilteringPredicatesParsing(\n        '''\n        SELECT *\n        FROM objects00\n        WHERE (frame >= 100 AND frame <= 300) OR (x_min < 100 AND y_min > 600) AND NOT (x_max = y_max)\n        ''',\n        \"(blobs_00.frame >= 100 OR objects00.x_min < 100) \"\n        \"AND (blobs_00.frame >= 100 OR objects00.y_min > 600) \"\n        \"AND (blobs_00.frame <= 300 OR objects00.x_min < 100) \"\n        \"AND (blobs_00.frame <= 300 OR objects00.y_min > 600) \"\n        \"AND (blobs_00.frame >= 100 OR NOT objects00.x_max = objects00.y_max) \"\n        \"AND (blobs_00.frame <= 300 OR NOT objects00.x_max = objects00.y_max)\"\n      )\n    }\n    for i in range(len(queries)):\n      queries[f'test_query_{i}']._test_filtering_predicates(config)\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "test_file", "path": "tests/test_order_optimization.py", "content": "from aidb.utils.order_optimization_utils import get_currently_supported_filtering_predicates_for_ordering\nfrom aidb.query.query import Query\n\nfrom multiprocessing import Process\nimport os\nfrom sqlalchemy.sql import text\nimport time\nimport unittest\nfrom unittest import IsolatedAsyncioTestCase\nimport numpy as np\nimport pandas as pd\nimport multiprocessing as mp\n\nfrom aidb.utils.logger import logger\nfrom tests.inference_service_utils.inference_service_setup import register_inference_services\nfrom tests.inference_service_utils.http_inference_service_setup import run_server\nfrom tests.tasti_test.tasti_test import TastiTests, VectorDatabaseType\nfrom tests.utils import setup_gt_and_aidb_engine, setup_test_logger\nfrom aidb.vector_database.faiss_vector_database import FaissVectorDatabase\nfrom aidb.vector_database.tasti import Tasti\n\nsetup_test_logger('order_optimization')\n\nPOSTGRESQL_URL = 'postgresql+asyncpg://user:testaidb@localhost:5432'\nSQLITE_URL = 'sqlite+aiosqlite://'\nMYSQL_URL = 'mysql+aiomysql://root:testaidb@localhost:3306'\n\nDATA_SET = 'twitter'\nBUDGET = 5000\n\nclass LimitEngineTests(IsolatedAsyncioTestCase):\n\n  async def test_jackson_number_objects(self):\n\n    dirname = os.path.dirname(__file__)\n    data_dir = os.path.join(dirname, f'data/{DATA_SET}')\n    p = Process(target=run_server, args=[str(data_dir), 8050])\n    p.start()\n    time.sleep(1)\n\n    # vector database configuration\n    index_path = './'\n    index_name = DATA_SET\n    embedding = np.load(f'./tests/data/embedding/{DATA_SET}_embeddings.npy')\n    embedding_df = pd.DataFrame({'id': range(embedding.shape[0]), 'values': embedding.tolist()})\n\n    embedding_dim = embedding.shape[1]\n    user_database = FaissVectorDatabase(index_path)\n    user_database.create_index(index_name, embedding_dim, recreate_index=True)\n    user_database.insert_data(index_name, embedding_df)\n    seed = mp.current_process().pid\n    tasti = Tasti(index_name, user_database, BUDGET, seed=seed)\n\n    queries = [\n      (\n        '''SELECT * FROM entity00 join sentiment01 on entity00.tweet_id = sentiment01.tweet_id WHERE type LIKE 'ORG' AND label LIKE 'POSITIVE';''',\n        '''SELECT * FROM entity00 join sentiment01 on entity00.tweet_id = sentiment01.tweet_id WHERE type LIKE 'ORG' AND label LIKE 'POSITIVE';'''\n      ),\n      (\n        '''SELECT * FROM entity00 join sentiment01 on entity00.tweet_id = sentiment01.tweet_id WHERE type LIKE 'ORG' AND label LIKE 'NEGATIVE';''',\n        '''SELECT * FROM entity00 join sentiment01 on entity00.tweet_id = sentiment01.tweet_id WHERE type LIKE 'ORG' AND label LIKE 'NEGATIVE';'''\n      ),\n      (\n        '''SELECT * FROM entity00 join sentiment01 on entity00.tweet_id = sentiment01.tweet_id WHERE type IN ('ORG', 'CARDINAL', 'PERSON') AND label LIKE 'POSITIVE';''',\n        '''SELECT * FROM entity00 join sentiment01 on entity00.tweet_id = sentiment01.tweet_id WHERE type IN ('ORG', 'CARDINAL', 'PERSON') AND label LIKE 'POSITIVE';'''\n      ),\n      (\n        '''SELECT * FROM entity00 join sentiment01 on entity00.tweet_id = sentiment01.tweet_id WHERE type IN ('ORG', 'CARDINAL', 'PERSON') AND label LIKE 'NEGATIVE';''',\n        '''SELECT * FROM entity00 join sentiment01 on entity00.tweet_id = sentiment01.tweet_id WHERE type IN ('ORG', 'CARDINAL', 'PERSON') AND label LIKE 'NEGATIVE';'''\n      )\n    ]\n    db_url_list = [ SQLITE_URL]\n    for db_url in db_url_list:\n      dialect = db_url.split('+')[0]\n      logger.info(f'Test {dialect} database')\n      for aidb_query, exact_query in queries:\n        logger.info(f'Running query {aidb_query} in full scan engine to test the optimization of ML ordering')\n\n        gt_engine, aidb_engine = await setup_gt_and_aidb_engine(db_url, data_dir, tasti, port=8050)\n        cost_dict = {'entity00': 0.0010, 'sentiment01': 0.0010}\n        register_inference_services(aidb_engine, data_dir, port=8050, cost_dict=cost_dict)\n        aidb_res = aidb_engine.execute(aidb_query)\n\n        logger.info(f'Running query {exact_query} in ground truth database')\n        try:\n          async with gt_engine.begin() as conn:\n            gt_res = await conn.execute(text(exact_query))\n            gt_res = gt_res.fetchall()\n        finally:\n          await gt_engine.dispose()\n\n        logger.info(f'There are {len(aidb_res)} elements in limit engine results '\n              f'and {len(gt_res)} elements in ground truth results')\n        logger.info(f'difference: {set(aidb_res) - set(gt_res)}')\n        assert len(set(aidb_res) - set(gt_res)) == 0\n\n      del gt_engine\n      del aidb_engine\n    p.terminate()\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"type": "source_file", "path": "aidb/engine/limit_engine.py", "content": "import numpy as np\nimport pandas as pd\nfrom sqlalchemy.sql import text\nfrom typing import List\n\nfrom aidb.engine.tasti_engine import TastiEngine\nfrom aidb.query.query import Query\nfrom aidb.utils.constants import VECTOR_ID_COLUMN\n\nclass LimitEngine(TastiEngine):\n  async def _execute_limit_query(self, query: Query):\n    '''\n    execute service inference based on proxy score, stop when the limit number meets\n    '''\n    # generate proxy score for each blob\n    proxy_score_for_all_blobs = await self.get_proxy_scores_for_all_blobs(query)\n\n    # sorted blob id based on proxy score\n    id_score = [(i, s) for i, s in zip(proxy_score_for_all_blobs.index, proxy_score_for_all_blobs.values)]\n    sorted_list = sorted(id_score, key=lambda x: x[1], reverse=True)\n    desired_cardinality = query.limit_cardinality\n\n    # TODO: rewrite query, use full scan to execute query\n    bound_service_list = query.inference_engines_required_for_query\n\n    limit_engine_batch_size = len(sorted_list)\n    for bound_service in bound_service_list:\n      limit_engine_batch_size = min(bound_service.service.preferred_batch_size, limit_engine_batch_size)\n\n    batched_indexes_list = [\n        [item[0] for item in sorted_list[i:i + limit_engine_batch_size]]\n        for i in range(0, len(sorted_list), limit_engine_batch_size)\n    ]\n\n    for batched_indexes in batched_indexes_list:\n      for bound_service in bound_service_list:\n        inp_query_str = self.get_input_query_for_inference_service_filtered_index(bound_service,\n                                                                                  self.blob_mapping_table_name,\n                                                                                  batched_indexes)\n        async with self._sql_engine.begin() as conn:\n          inp_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(inp_query_str), conn))\n        inp_df.set_index(VECTOR_ID_COLUMN, inplace=True, drop=True)\n        await bound_service.infer(inp_df)\n\n      # FIXME: Currently, we select the whole database, need to rewrite sql text to select specific blob id\n      async with self._sql_engine.begin() as conn:\n        res = await conn.execute(text(query.sql_query_text))\n      res = res.fetchall()\n\n      if len(res) == desired_cardinality:\n        break\n\n    return res\n"}
{"type": "source_file", "path": "aidb/engine/__init__.py", "content": "from .engine import Engine\n"}
{"type": "source_file", "path": "aidb/engine/engine.py", "content": "from aidb.engine.approx_aggregate_join_engine import ApproximateAggregateJoinEngine\nfrom aidb.engine.approx_select_engine import ApproxSelectEngine\nfrom aidb.engine.limit_engine import LimitEngine\nfrom aidb.engine.non_select_query_engine import NonSelectQueryEngine\nfrom aidb.utils.asyncio import asyncio_run\nfrom aidb.query.query import Query\n\n\nclass Engine(LimitEngine, NonSelectQueryEngine, ApproxSelectEngine, ApproximateAggregateJoinEngine):\n  def execute(self, query: str, **kwargs):\n    '''\n    Executes a query and returns the results.\n    '''\n    try:\n      parsed_query = Query(query, self._config)\n      all_queries = parsed_query.all_queries_in_expressions\n      # FIXME: We have many validity checks for different queries.\n      #     It's better to put them together and check the validity first.\n      # check validity of user defined function query\n\n      if parsed_query.is_udf_query:\n        parsed_query.check_udf_query_validity()\n      result = None\n\n      for parsed_single_query, _ in all_queries:\n        if parsed_single_query.is_approx_agg_query:\n          if parsed_single_query.is_aqp_join_query:\n            result = asyncio_run(self.execute_aggregate_join_query(parsed_single_query, **kwargs))\n          else:\n            result = asyncio_run(self.execute_aggregate_query(parsed_single_query, **kwargs))\n        elif parsed_single_query.is_approx_select_query:\n          result = asyncio_run(self.execute_approx_select_query(parsed_single_query, **kwargs))\n        elif parsed_single_query.is_limit_query():\n          result = asyncio_run(self._execute_limit_query(parsed_single_query, **kwargs))\n        elif parsed_single_query.is_select_query():\n          result = asyncio_run(self.execute_full_scan(parsed_single_query, **kwargs))\n        else:\n          result = asyncio_run(self.execute_non_select(parsed_single_query))\n\n      return result\n    except Exception as e:\n      raise e\n    finally:\n      self.__del__()\n"}
{"type": "source_file", "path": "aidb/config/config_types.py", "content": "from dataclasses import dataclass\nfrom enum import Enum\nfrom functools import cached_property\nfrom typing import Dict, List, NamedTuple, Tuple\n\nimport networkx as nx\nimport sqlalchemy\n\n\nAIDBListType = type('AIDBListType', (), {})\n\n\nColumn = sqlalchemy.schema.Column\nGraph = nx.DiGraph\n\nclass InferenceBinding(NamedTuple):\n  input_columns: Tuple[str]\n  output_columns: Tuple[str]\n\n# TODO: think about this architecture\n@dataclass\nclass Table:\n  _table: sqlalchemy.Table\n\n  @cached_property\n  def name(self) -> str:\n    return self._table.name.lower()\n\n  @cached_property\n  def primary_key(self) -> List[str]:\n    return self._table.primary_key.columns.keys()\n\n  # Name -> Column\n  @cached_property\n  def columns(self) -> Dict[str, Column]:\n    # Note: this is not actually a dict but I think this is fine\n    return self._table.columns\n\n  @cached_property\n  def foreign_keys(self) -> Dict[str, str]:\n    fkeys = {}\n    for col in self._table.columns:\n      for fk in col.foreign_keys:\n        fkeys[col.name] = fk.target_fullname\n    return fkeys\n\n\ndef python_type_to_sqlalchemy_type(python_type):\n  if python_type == int or python_type == 'int':\n    return sqlalchemy.Integer\n  elif python_type == float or python_type == 'float':\n    return sqlalchemy.Float\n  # TODO: think if this is the best way.\n  elif python_type == str or python_type == object or python_type == 'str':\n    return sqlalchemy.String\n  elif python_type == bool or python_type == 'bool':\n    return sqlalchemy.Boolean\n  else:\n    raise ValueError(f'Unknown python type {python_type}')\n"}
{"type": "source_file", "path": "aidb/engine/approx_aggregate_engine.py", "content": "import math\r\nimport pandas as pd\r\nimport scipy\r\nfrom sqlalchemy.sql import text\r\nimport sqlglot.expressions as exp\r\nimport statsmodels.stats.proportion\r\nfrom typing import List\r\n\r\nfrom aidb.engine.full_scan_engine import FullScanEngine\r\nfrom aidb.estimator.estimator import (Estimator, WeightedMeanSetEstimator,\r\n                                      WeightedCountSetEstimator, WeightedSumSetEstimator)\r\nfrom aidb.query.query import Query\r\nfrom aidb.utils.constants import MASS_COL_NAME, NUM_ITEMS_COL_NAME, WEIGHT_COL_NAME\r\nfrom aidb.utils.logger import logger\r\n\r\n_NUM_PILOT_SAMPLES = 1000\r\n\r\n\r\nclass ApproximateAggregateEngine(FullScanEngine):\r\n  async def execute_aggregate_query(self, query: Query):\r\n    '''\r\n    Execute aggregation query using approximate processing\r\n    '''\r\n    query.is_valid_aqp_query()\r\n\r\n    blob_tables = query.blob_tables_required_for_query\r\n    filtering_predicates = query.filtering_predicates\r\n    blob_key_filtering_predicates = []\r\n    inference_engines_required = query.inference_engines_required_for_filtering_predicates\r\n    for filtering_predicate, engine_required in zip(filtering_predicates, inference_engines_required):\r\n      if len(engine_required) == 0:\r\n        blob_key_filtering_predicates.append(filtering_predicate)\r\n    blob_key_filtering_predicates_str = self._get_where_str(blob_key_filtering_predicates)\r\n    blob_key_filtering_predicates_str = f'WHERE {blob_key_filtering_predicates_str}' \\\r\n                                         if blob_key_filtering_predicates_str else ''\r\n\r\n    async with self._sql_engine.begin() as conn:\r\n      blob_count_query_str = self.get_blob_count_query(blob_tables, blob_key_filtering_predicates_str)\r\n      blob_count_res = await conn.execute(text(blob_count_query_str))\r\n      self.blob_count = blob_count_res.fetchone()[0]\r\n      # run inference on pilot blobs\r\n      sample_results = await self.get_results_on_sampled_data(\r\n          _NUM_PILOT_SAMPLES,\r\n          query,\r\n          blob_tables,\r\n          blob_key_filtering_predicates_str,\r\n          conn\r\n      )\r\n\r\n      if len(sample_results) == 0:\r\n        raise Exception(\r\n            '''We found no records that match your predicate in 1000 samples, so we can't guarantee the\r\n            error target. Try running without the error target if you are certain you want to run this query.'''\r\n        )\r\n\r\n      aggregation_type_list = query.aggregation_type_list_in_query\r\n      num_aggregations = len(aggregation_type_list)\r\n      alpha = (1. - (query.confidence / 100.)) / num_aggregations\r\n      conf = 1. - alpha\r\n\r\n      num_samples = self.get_additional_required_num_samples(query, sample_results, alpha)\r\n      logger.info(f'num_samples: {num_samples}')\r\n      # when there is not enough data samples, directly run full scan engine and get exact result\r\n      if num_samples + _NUM_PILOT_SAMPLES >= self.blob_count:\r\n        query_no_aqp = query.base_sql_no_aqp\r\n        res = await self.execute_full_scan(query_no_aqp)\r\n        return res\r\n\r\n      new_sample_results = await self.get_results_on_sampled_data(\r\n          num_samples,\r\n          query,\r\n          blob_tables,\r\n          blob_key_filtering_predicates_str,\r\n          conn\r\n      )\r\n\r\n    concatenated_results = pd.concat([sample_results, new_sample_results], ignore_index=True)\r\n    # TODO:  figure out what should parameter num_samples be for COUNT/SUM query\r\n\r\n    estimates = []\r\n    fixed_cols = concatenated_results[[NUM_ITEMS_COL_NAME, WEIGHT_COL_NAME, MASS_COL_NAME]]\r\n    for index, (agg_type, _) in enumerate(aggregation_type_list):\r\n      selected_index_col = concatenated_results.iloc[:, [index]]\r\n      extracted_sample_results = pd.concat([selected_index_col, fixed_cols], axis=1)\r\n\r\n      estimator = self._get_estimator(agg_type)\r\n      estimates.append(\r\n          estimator.estimate(\r\n              extracted_sample_results,\r\n              _NUM_PILOT_SAMPLES + num_samples,\r\n              conf\r\n          ).estimate\r\n      )\r\n\r\n    # For approximate aggregation, we currently do not support the GROUP BY clause, so there is only one row result.\r\n    # We still return the result a list of tuple to maintain the format\r\n    return [tuple(estimates)]\r\n\r\n\r\n  def get_blob_count_query(self, table_names: List[str], blob_key_filtering_predicates_str: str):\r\n    '''Function that returns a query, to get total number of blob ids'''\r\n    join_str = self._get_inner_join_query(table_names)\r\n    return f'''\r\n            SELECT COUNT(*)\r\n            {join_str}\r\n            {blob_key_filtering_predicates_str};\r\n            '''\r\n\r\n\r\n  def _get_estimator(self, agg_type: exp.Expression) -> Estimator:\r\n    if agg_type == exp.Avg:\r\n      return WeightedMeanSetEstimator(self.blob_count)\r\n    elif agg_type == exp.Sum:\r\n      return WeightedSumSetEstimator(self.blob_count)\r\n    elif agg_type == exp.Count:\r\n      return WeightedCountSetEstimator(self.blob_count)\r\n    else:\r\n      raise NotImplementedError('We only support AVG, COUNT, and SUM for approximate aggregations.')\r\n\r\n\r\n  def get_sample_blobs_query(self, blob_tables: List[str], num_samples: int, blob_key_filtering_predicates_str: str):\r\n    '''Function to return a query to get all blob keys'''\r\n    join_str = self._get_inner_join_query(blob_tables)\r\n\r\n    select_column = []\r\n    col_name_set = set()\r\n    for table in blob_tables:\r\n      for blob_key in self._config.blob_keys[table]:\r\n        col_name = blob_key.split('.')[1]\r\n        if col_name not in col_name_set:\r\n          select_column.append(blob_key)\r\n          col_name_set.add(col_name)\r\n\r\n    select_column_str = ', '.join(select_column)\r\n    if self._config.dialect == 'mysql':\r\n      random_func = 'RAND()'\r\n    else:\r\n      random_func = 'RANDOM()'\r\n    # FIXME: add condition that samples are not in previous sampled data\r\n    sample_blobs_query_str = f'''\r\n                              SELECT {select_column_str}\r\n                              {join_str}\r\n                              {blob_key_filtering_predicates_str}\r\n                              ORDER BY {random_func}\r\n                              LIMIT {num_samples};\r\n                              '''\r\n    return sample_blobs_query_str\r\n\r\n\r\n  async def execute_inference_services(self, query: Query, sample_df: pd.DataFrame, conn):\r\n    '''\r\n    Executed inference services on sampled data\r\n    '''\r\n    bound_service_list = query.inference_engines_required_for_query\r\n    inference_services_executed = set()\r\n    for bound_service in bound_service_list:\r\n      inp_query_str = self.get_input_query_for_inference_service_filter_service(\r\n          bound_service,\r\n          query,\r\n          inference_services_executed\r\n      )\r\n      new_query = Query(inp_query_str, self._config)\r\n      query_add_filter_key, _ = self.add_filter_key_into_query(\r\n          list(bound_service.binding.input_columns),\r\n          sample_df,\r\n          new_query\r\n      )\r\n\r\n      input_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(query_add_filter_key.sql_str), conn))\r\n      await bound_service.infer(input_df)\r\n      inference_services_executed.add(bound_service.service.name)\r\n\r\n\r\n  async def get_results_for_each_blob(self, query: Query, sample_df: pd.DataFrame) -> pd.DataFrame:\r\n    '''\r\n    Return aggregation results of each sample blob, contains weight, mass, statistics, num_items\r\n    For example, if there is 1000 blobs, blob A has two detected objects with x_min = 500 and x_min = 1000,\r\n    the query is Avg(x_min), the result row of blob A is (750, 2, 1/1000, 1)\r\n    mass is default value 1, weight is same for each blob in uniform sampling\r\n    '''\r\n    tables_in_query = query.tables_in_query\r\n    query_no_aqp = query.base_sql_no_aqp\r\n\r\n    table_columns = [f'{table}.{col.name}' for table in tables_in_query for col in self._config.tables[table].columns]\r\n    query_add_filter_key, selected_column = self.add_filter_key_into_query(\r\n        table_columns,\r\n        sample_df,\r\n        query_no_aqp\r\n    )\r\n    query_add_count = query_add_filter_key.add_select(f'COUNT(*) AS {NUM_ITEMS_COL_NAME}')\r\n    query_str = f'''\r\n                  {query_add_count.sql_str}\r\n                  GROUP BY {', '.join(selected_column)}\r\n                 '''\r\n    # In MySQL database, after running inference, the database will lose connection. We need to restart the engine,\r\n    async with self._sql_engine.begin() as conn:\r\n      res_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(query_str), conn))\r\n    res_df[WEIGHT_COL_NAME] = 1. / self.blob_count\r\n    res_df[MASS_COL_NAME] = 1\r\n\r\n    return res_df\r\n\r\n\r\n  async def get_results_on_sampled_data(\r\n      self,\r\n      num_samples: int,\r\n      query: Query,\r\n      blob_tables: List[str],\r\n      blob_key_filtering_predicates_str: str,\r\n      conn\r\n  ):\r\n    sample_blobs_query_str = self.get_sample_blobs_query(blob_tables, num_samples, blob_key_filtering_predicates_str)\r\n    sample_blobs_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(sample_blobs_query_str), conn))\r\n\r\n    if len(sample_blobs_df) < num_samples:\r\n      raise Exception(f'Require {num_samples} samples, but only get {len(sample_blobs_df)} samples')\r\n\r\n    await self.execute_inference_services(query, sample_blobs_df, conn)\r\n\r\n    results_for_each_blob = await self.get_results_for_each_blob(query, sample_blobs_df)\r\n\r\n    return results_for_each_blob\r\n\r\n\r\n  def _calculate_required_num_samples(\r\n      self,\r\n      extracted_sample_results: pd.DataFrame,\r\n      sample_size: int,\r\n      agg_type,\r\n      alpha,\r\n      error_target\r\n  ):\r\n    conf = 1 - alpha\r\n    estimator = self._get_estimator(agg_type)\r\n    pilot_estimate = estimator.estimate(extracted_sample_results, sample_size, conf)\r\n\r\n    if agg_type == exp.Avg:\r\n      p_lb = statsmodels.stats.proportion.proportion_confint(\r\n        len(extracted_sample_results),\r\n        sample_size,\r\n        alpha\r\n      )[0]\r\n    else:\r\n      p_lb = 1\r\n\r\n    return int(\r\n        (scipy.stats.norm.ppf(alpha / 2) * pilot_estimate.std_ub / (error_target * pilot_estimate.lower_bound)) ** 2 * \\\r\n        (1. / p_lb)\r\n    )\r\n\r\n\r\n  def get_additional_required_num_samples(\r\n      self,\r\n      query: Query,\r\n      sample_results: pd.DataFrame,\r\n      alpha\r\n  ) -> int:\r\n    error_target = query.error_target\r\n    num_samples = []\r\n\r\n    fixed_cols = sample_results[[NUM_ITEMS_COL_NAME, WEIGHT_COL_NAME, MASS_COL_NAME]]\r\n    for index, (agg_type, _) in enumerate(query.aggregation_type_list_in_query):\r\n      if agg_type == exp.Avg:\r\n        adjusted_error_target = min(1 - math.sqrt(1 - error_target), math.sqrt(error_target + 1) - 1)\r\n      else:\r\n        adjusted_error_target = error_target\r\n      selected_index_col = sample_results.iloc[:, [index]]\r\n      extracted_sample_results = pd.concat([selected_index_col, fixed_cols], axis=1)\r\n      num_samples.append(\r\n        self._calculate_required_num_samples(\r\n          extracted_sample_results,\r\n          _NUM_PILOT_SAMPLES,\r\n          agg_type,\r\n          alpha,\r\n          adjusted_error_target\r\n        )\r\n      )\r\n\r\n    return max(max(num_samples), 100)\r\n"}
{"type": "source_file", "path": "aidb/engine/tasti_engine.py", "content": "import numpy as np\nimport pandas as pd\nimport sqlalchemy\nfrom sqlalchemy.sql import text\nfrom typing import Dict, List, Optional\n\nfrom aidb.config.config_types import Table\nfrom aidb.engine.base_engine import BaseEngine\nfrom aidb.query.query import Query\nfrom aidb.utils.constants import table_name_for_rep_and_topk_and_blob_mapping, VECTOR_ID_COLUMN\nfrom aidb.vector_database.tasti import Tasti\n\n\nclass TastiEngine(BaseEngine):\n  def __init__(\n      self,\n      connection_uri: str,\n      infer_config: bool = True,\n      debug: bool = False,\n      tasti_index: Optional[Tasti] = None,\n      user_specified_vector_ids: Optional[pd.DataFrame] = None\n  ):\n    super().__init__(connection_uri, infer_config, debug)\n\n    self.rep_table_name = None\n    # TODO: modify to same rep table with different topk table\n    self.topk_table_name = None\n    # table for mapping blob keys to blob ids\n    self.blob_mapping_table_name = None\n    self.tasti_index = tasti_index\n    self.user_specified_vector_ids = user_specified_vector_ids\n\n\n  async def get_proxy_scores_for_all_blobs(self, query: Query, return_binary_score= False, **kwargs):\n    '''\n    1. create rep table and topk table if not exist, store the results from vector database\n    2. infer all bound services for all cluster representatives blobs\n    3. generate proxy score per predicate for all blobs based on topk rep ids and dists\n    '''\n    bound_service_list = query.inference_engines_required_for_query\n    blob_tables = query.blob_tables_required_for_query\n\n    self.rep_table_name, self.topk_table_name, self.blob_mapping_table_name = \\\n        table_name_for_rep_and_topk_and_blob_mapping(blob_tables)\n\n    if self.rep_table_name not in self._config.tables or self.topk_table_name not in self._config.tables:\n      await self.initialize_tasti()\n\n    async with self._sql_engine.begin() as conn:\n      reps_query_str = f'SELECT * FROM {self.rep_table_name}'\n      reps_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(reps_query_str), conn))\n\n    for bound_service in bound_service_list:\n      inp_query_str = self.get_input_query_for_inference_service_filtered_index(bound_service, self.rep_table_name)\n      async with self._sql_engine.begin() as conn:\n        inp_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(inp_query_str), conn))\n      inp_df.set_index(VECTOR_ID_COLUMN, inplace=True, drop=True)\n      await bound_service.infer(inp_df)\n\n    score_query_str = self.get_score_query_str(query, self.rep_table_name)\n    async with self._sql_engine.begin() as conn:\n      score_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(score_query_str), conn))\n    score_df = pd.merge(reps_df, score_df, on=VECTOR_ID_COLUMN, how='left')\n    score_df.fillna(0, inplace=True)\n    score_df.set_index(VECTOR_ID_COLUMN, inplace=True, drop=True)\n    score_df = score_df['score']\n    score_for_all_df = await self.propagate_score_for_all_vector_ids(score_df, return_binary_score)\n\n    # FIXME: decide what to return for different usage: Limit engine, Aggregation, Full scan optimize.\n    return score_for_all_df\n\n\n  def get_score_query_str(self, query: Query, rep_table_name: str) -> (str, List[List[str]]):\n    '''\n    Convert filtering condition into select clause, so we can use select result to compute proxy score for all blobs\n    '''\n    where_condition = query.convert_and_connected_fp_to_exp(query.filtering_predicates)\n    if where_condition:\n      where_str = f'WHERE {where_condition.sql()}'\n    else:\n      where_str = ''\n    filtering_predicates = [fp.sql() for and_connected in query.filtering_predicates for fp in and_connected]\n    tables = self._get_tables(filtering_predicates)\n\n    # some representative blobs may not have outputs from service, so left join is better\n    join_str = self._get_left_join_str(rep_table_name, tables)\n    score_query_str = f'''\n                      SELECT {rep_table_name}.{VECTOR_ID_COLUMN}, COUNT(*) AS score\n                      {join_str}\n                      {where_str}\n                      GROUP BY {rep_table_name}.{VECTOR_ID_COLUMN};\n                      '''\n    return score_query_str\n\n\n  async def propagate_score_for_all_vector_ids(self, score_df: pd.Series, return_binary_score = False) -> pd.Series:\n    topk_query_str = f'SELECT * FROM {self.topk_table_name}'\n    async with self._sql_engine.begin() as conn:\n      topk_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(topk_query_str), conn))\n    topk_df.set_index(VECTOR_ID_COLUMN, inplace=True, drop=True)\n\n    topk = len(topk_df.columns) // 2\n    topk_indices = np.arange(topk)\n    all_dists = np.zeros((len(topk_df), topk))\n    all_scores = np.zeros((len(topk_df), topk))\n\n    for idx, (index, row) in enumerate(topk_df.iterrows()):\n      reps = [int(row[f'topk_reps_{i}']) for i in topk_indices]\n      dists = [row[f'topk_dists_{i}'] for i in topk_indices]\n\n      if index in score_df.index:\n        reps = [index] * topk\n        dists = [1] * topk\n\n      all_dists[idx, :] = dists\n      all_scores[idx, :] = score_df.loc[reps].values\n    # to avoid division by zero error\n    all_dists += 1e-8\n\n    if return_binary_score:\n      all_scores = np.where(all_scores == 0, 0, 1)\n      weights = 1.0 / all_dists\n      votes_1 = np.sum(all_scores * weights, axis=1)\n      votes_0 = np.sum((1 - all_scores) * weights, axis=1)\n      # majority vote\n      y_pred = (votes_1 > votes_0).astype(int)\n    else:\n      weights = np.sum(all_dists, axis=1).reshape(-1, 1) - all_dists\n      weights = weights / weights.sum(axis=1).reshape(-1, 1)\n      y_pred = np.sum(all_scores * weights, axis=1)\n\n    return pd.Series(y_pred, index=topk_df.index)\n\n\n  def _create_tasti_table(self, topk:int,  conn: sqlalchemy.engine.base.Connection):\n    '''\n    create new rep_table and topk_table to store the results from vector database.\n    rep_table is used to store cluster representatives ids and blob keys,\n    topk_table is used to store topk rep_ids and dists for all blobs.\n    '''\n    metadata = sqlalchemy.MetaData()\n    metadata.reflect(conn)\n    blob_mapping_table = sqlalchemy.schema.Table(self.blob_mapping_table_name, metadata,\n                                                 autoload=True, autoload_with=conn)\n\n    # FIXME: there is a sqlalchemy SAWarning, This warning may become an exception in a future release\n    rep_table = sqlalchemy.schema.Table(self.rep_table_name, metadata,\n                                        *[column._copy() for column in blob_mapping_table.columns],\n                                        *[constraint._copy() for constraint in blob_mapping_table.constraints]\n                                        )\n\n    columns = []\n    columns.append(sqlalchemy.Column(VECTOR_ID_COLUMN, sqlalchemy.Integer, primary_key=True, autoincrement=False))\n    for i in range(topk):\n      columns.append(sqlalchemy.Column(f'topk_reps_{str(i)}', sqlalchemy.Integer))\n      columns.append(sqlalchemy.Column(f'topk_dists_{str(i)}', sqlalchemy.Float(32)))\n    topk_table = sqlalchemy.schema.Table(self.topk_table_name, metadata, *columns)\n    metadata.create_all(conn)\n\n    self._config.tables[self.rep_table_name] = Table(rep_table)\n    self._config.tables[self.topk_table_name] = Table(topk_table)\n\n\n  def _format_topk_for_all(self, topk_for_all: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Formats the top K for all data.\n    :param topk_for_all: Top K for all data.\n    \"\"\"\n    topk = max(topk_for_all['topk_reps'].str.len())\n    new_topk_for_all = {\n      f'topk_reps_{i}': topk_for_all['topk_reps'].str[i]\n      for i in range(topk)\n    }\n    new_topk_for_all.update({\n      f'topk_dists_{i}': topk_for_all['topk_dists'].str[i]\n      for i in range(topk)\n    })\n    new_topk_for_all.update({\n      VECTOR_ID_COLUMN: [vector_id for vector_id in topk_for_all.index]\n    })\n    return pd.DataFrame(new_topk_for_all, index=topk_for_all.index)\n\n\n  async def initialize_tasti(self):\n    \"\"\"\n    This function initializes the Tasti index and manages the insertion of data into\n    representative and topk tables.\n    It performs the following main steps:\n    1. Initializes a Tasti index and retrieves representative blob ids and topk reps and dists.\n    2. Creates Tasti tables and retrieves culster representative blob key columns.\n    3. Inserts data into the representative table and topk table\n    \"\"\"\n    if self.tasti_index is None:\n      raise Exception('TASTI hasn\\'t been initialized, please provide tasti_index')\n\n    if self.user_specified_vector_ids is not None:\n      vector_ids = self.user_specified_vector_ids\n    else:\n      vector_id_select_query_str = f'''\n                                    SELECT {self.blob_mapping_table_name}.{VECTOR_ID_COLUMN}\n                                    FROM {self.blob_mapping_table_name};\n                                    '''\n\n      async with self._sql_engine.begin() as conn:\n        vector_ids = await conn.run_sync(lambda conn: pd.read_sql_query(text(vector_id_select_query_str), conn))\n    self.tasti_index.set_vector_ids(vector_ids)\n\n    rep_ids = self.tasti_index.get_representative_vector_ids()\n    topk_for_all = self.tasti_index.get_topk_representatives_for_all()\n    new_topk_for_all = self._format_topk_for_all(topk_for_all)\n    topk = max(topk_for_all['topk_reps'].str.len())\n\n    rep_blob_query_str = f'''\n                          SELECT *\n                          FROM {self.blob_mapping_table_name}\n                          WHERE {self.blob_mapping_table_name}.{VECTOR_ID_COLUMN} IN {format(tuple(rep_ids.index))};\n                          '''\n\n    async with self._sql_engine.begin() as conn:\n      await conn.run_sync(lambda conn: self._create_tasti_table(topk, conn))\n      rep_blob_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(rep_blob_query_str), conn))\n\n      await conn.run_sync(lambda conn: rep_blob_df.to_sql(self.rep_table_name, conn, if_exists='append', index=False))\n      await conn.run_sync(lambda conn: new_topk_for_all.to_sql(self.topk_table_name, conn,\n                                                               if_exists='append', index=False))\n\n  # TODO: update topk table and representative table, query for new embeddings.\n"}
{"type": "source_file", "path": "aidb/inference/__init__.py", "content": ""}
{"type": "source_file", "path": "aidb/engine/base_engine.py", "content": "from collections import defaultdict\nfrom typing import Dict, List, Optional, Set, Tuple\n\nimport inspect\nimport numpy as np\nimport pandas as pd\n\nfrom aidb.config.config import Config\nfrom aidb.config.config_types import InferenceBinding\nfrom aidb.inference.bound_inference_service import (\n    BoundInferenceService, CachedBoundInferenceService)\nfrom aidb.inference.inference_service import InferenceService\nfrom aidb.query.query import Query\nfrom aidb.utils.asyncio import asyncio_run\nfrom aidb.utils.constants import VECTOR_ID_COLUMN\nfrom aidb.utils.db import infer_dialect, create_sql_engine\n\n\nclass BaseEngine():\n  def __init__(\n      self,\n      connection_uri: str,\n      infer_config: bool = True,\n      debug: bool = False,\n  ):\n    self._connection_uri = connection_uri\n    self._debug = debug\n\n    self._dialect = infer_dialect(connection_uri)\n    self._sql_engine = create_sql_engine(connection_uri, debug)\n\n    if infer_config:\n      self._config: Config = asyncio_run(self._infer_config())\n\n\n  def __del__(self):\n    asyncio_run(self._sql_engine.dispose())\n\n\n  # ---------------------\n  # Setup\n  # ---------------------\n  async def _infer_config(self) -> Config:\n    '''\n    Infer the database configuration from the sql engine.\n    Extracts:\n    - Tables, columns (+ types), and foriegn keys.\n    - Cache tables\n    - Blob tables\n    - Generated columns\n    '''\n\n    # We use an async engine, so we need a function that takes in a synchrnous connection\n    def config_from_conn(conn):\n      config = Config(\n        {},\n        [],\n        self._connection_uri,\n        None,\n        None,\n        None,\n        None,\n        None,\n      )\n      config.load_from_sqlalchemy(conn)\n      return config\n\n    async with self._sql_engine.begin() as conn:\n      config: Config = await conn.run_sync(config_from_conn)\n\n    if self._debug:\n      import prettyprinter as pp\n      pp.install_extras(\n        exclude=['django', 'ipython', 'ipython_repr_pretty'])\n      pp.pprint(config)\n      print(config.blob_tables)\n\n    return config\n\n\n  def register_inference_service(self, service: InferenceService):\n    self._config.add_inference_service(service.name, service)\n\n\n  def bind_inference_service(self, service_name: str, binding: InferenceBinding, copy_map: Dict[str, str]={}, verbose: bool=False):\n    bound_service = CachedBoundInferenceService(\n      self._config.inference_services[service_name],\n      binding,\n      copy_map,\n      self._sql_engine,\n      self._config.columns,\n      self._config.tables,\n      self._dialect,\n      verbose,\n    )\n    self._config.bind_inference_service(bound_service)\n\n\n  # ---------------------\n  # Properties\n  # ---------------------\n  @property\n  def dialect(self):\n    return self._dialect\n\n\n  # ---------------------\n  # Inference\n  # ---------------------\n  def prepare_multitable_inputs(self, raw_inputs: List[Tuple[str, pd.DataFrame]]) -> pd.DataFrame:\n    '''\n    Prepare the inputs for inference.\n    '''\n    assert len(raw_inputs) >= 1\n    final_df = raw_inputs[0][1]\n    for idx, (table_name, df) in enumerate(raw_inputs[1:]):\n      last_table_name = raw_inputs[idx][0]\n      table_relations = self._config.relations_by_table[table_name]\n      join_keys = [\n        fk for fk in table_relations if fk.startswith(last_table_name)]\n      final_df = final_df.merge(df, on=join_keys, how='inner')\n\n    return final_df\n\n\n  def process_inference_outputs(self, binding: InferenceBinding, joined_outputs: pd.DataFrame) -> pd.DataFrame:\n    '''\n    Process the outputs of inference by renaming the columns and selecting the\n    output columns.\n    '''\n    df_cols = list(joined_outputs.columns)\n    for idx, col in enumerate(binding.output_columns):\n      joined_outputs.rename(columns={df_cols[idx]: col}, inplace=True)\n    res = joined_outputs[list(binding.output_columns)]\n    return res\n\n\n  def inference(self, inputs: pd.DataFrame, bound_service: BoundInferenceService) -> List[pd.DataFrame]:\n    return bound_service.batch(inputs)\n\n\n  def execute(self, query: str):\n    raise NotImplementedError()\n\n\n  def _find_join_path(\n      self,\n      common_columns: Dict[Tuple[str, str], List[str]],\n      table_relations: Dict[str, List[str]],\n      table_names: List[str]\n  ) -> str:\n    \"\"\"\n    Find the path to join tables based on common columns and create the JOIN part of an SQL query.\n    :param common_columns: Dict containing common column names between table pairs.\n    :param table_relations: Dict containing related tables.\n    :param table_names: List of table names to be joined.\n    \"\"\"\n    join_strs = []\n    stack = [table_names[0]]\n    visited = {table_names[0]}\n    while stack:\n      current_table = stack.pop()\n      for neighbor_table in table_relations[current_table]:\n        if neighbor_table in visited:\n          continue\n        visited_col = []\n        join_condition = []\n        for visited_table in visited:\n          for col in common_columns[(neighbor_table, visited_table)]:\n            if col not in visited_col:\n              join_condition.append(\n                f'{visited_table}.{col} = {neighbor_table}.{col}')\n              visited_col.append(col)\n        join_strs.append(\n          f'INNER JOIN {neighbor_table} ON {\" AND \".join(join_condition)}')\n        visited.add(neighbor_table)\n        stack.append(neighbor_table)\n    return f\"FROM {table_names[0]}\\n\" + '\\n'.join(join_strs)\n\n\n  def _get_tables(self, columns: List[str]) -> List[str]:\n    tables = set()\n    for col in columns:\n      table_name = col.split('.')[0]\n      tables.add(table_name)\n    return list(tables)\n\n\n  def _get_inner_join_query(self, table_names: List[str]):\n    \"\"\"\n    Generate an SQL query to perform INNER JOIN operations on the provided tables.\n    :param selected_cols: List of column names to be selected in the SQL query.\n    :param table_names: List of table names to be joined.\n    \"\"\"\n    table_number = len(table_names)\n    table_relations = defaultdict(list)\n    common_columns = {}\n    for i in range(table_number - 1):\n      table1 = self._config.tables[table_names[i]]\n      table1_cols = [col.name for col in table1.columns]\n      for j in range(i + 1, table_number):\n        table2 = self._config.tables[table_names[j]]\n        table2_cols = [col.name for col in table2.columns]\n        common = list(set(table1_cols).intersection(table2_cols))\n\n        common_columns[(table_names[i], table_names[j])] = common\n        common_columns[(table_names[j], table_names[i])] = common\n        if common:\n          table_relations[table_names[i]].append(table_names[j])\n          table_relations[table_names[j]].append(table_names[i])\n\n    join_path_str = self._find_join_path(\n      common_columns, table_relations, table_names)\n    return join_path_str\n\n\n  def _get_select_join_str(self, bound_service: BoundInferenceService, filtering_predicate_tables: Optional[List] = [], vector_id_table: Optional[str] = None):\n    column_to_root_column = self._config.columns_to_root_column\n    binding = bound_service.binding\n    inp_cols = binding.input_columns\n    root_inp_cols = [column_to_root_column.get(col, col) for col in inp_cols]\n\n    # used to select inp rows based on blob ids\n    if vector_id_table:\n      root_inp_cols.append(f'{vector_id_table}.{VECTOR_ID_COLUMN}')\n\n    inp_cols_str = ', '.join(root_inp_cols)\n    inp_tables = self._get_tables(root_inp_cols)\n    for table in filtering_predicate_tables:\n      if table not in inp_tables:\n        inp_tables.append(table)\n    join_str = self._get_inner_join_query(inp_tables)\n\n    select_join_str = f'''\n                      SELECT {inp_cols_str}\n                      {join_str}\n                      '''\n\n    return inp_tables, select_join_str\n\n\n  def _get_where_str(self, filtering_predicates):\n    and_connected = []\n    for fp in filtering_predicates:\n      and_connected.append(' OR '.join([p.sql() for p in fp]))\n    and_connected = [f'({or_connected})' for or_connected in and_connected]\n    return ' AND '.join(and_connected)\n\n\n  def get_input_query_for_inference_service_filter_service(\n      self,\n      bound_service: BoundInferenceService,\n      user_query: Query,\n      already_executed_inference_services: Set[str]\n  ):\n    \"\"\"\n    this function returns the input query to fetch the input records for an inference service\n    input query will also contain the predicates that can be currently satisfied using the inference services\n    that are already executed\n    \"\"\"\n    filtering_predicates = user_query.filtering_predicates\n    inference_engines_required_for_filtering_predicates = user_query.inference_engines_required_for_filtering_predicates\n    tables_in_filtering_predicates = user_query.tables_in_filtering_predicates\n\n    # filtering predicates that can be satisfied by the currently executed inference engines\n    filtering_predicates_satisfied = []\n    filtering_predicates_tables = []\n    for p, e, t in zip(filtering_predicates, inference_engines_required_for_filtering_predicates,\n                       tables_in_filtering_predicates):\n      if len(already_executed_inference_services.intersection(e)) == len(e):\n        filtering_predicates_satisfied.append(p)\n        for table in t:\n          if table not in filtering_predicates_tables:\n            filtering_predicates_tables.append(table)\n        \n    inp_tables, select_join_str = self._get_select_join_str(bound_service, filtering_predicates_tables)\n    where_str = self._get_where_str(filtering_predicates_satisfied)\n\n    if len(filtering_predicates_satisfied) > 0:\n      inp_query_str = select_join_str + f'WHERE {where_str};'\n    else:\n      inp_query_str = select_join_str + ';'\n\n    return inp_query_str\n\n\n  def add_filter_key_into_query(\n    self,\n    table_columns: List[str],\n    sample_df: pd.DataFrame,\n    query: Query,\n  ):\n    '''\n    Add filter condition, like where keyA in (1,2,3)\n    param table_columns: list of filtering columns, like keyA in the example.\n    param sample_df: dataframe store filter values, {'keyA': [1,2,3]}\n    param query: query class\n    param query_expression: sqlglot parsed query expression\n    '''\n    selected_column = []\n    col_name_list = []\n\n    if len(sample_df) == 0:\n      return query, selected_column\n\n    for col in table_columns:\n      col_name = col.split('.')[1]\n      if col_name in sample_df.columns and col_name not in col_name_list:\n        selected_column.append(col)\n        col_name_list.append(col_name)\n\n    col_tuple = ', '.join(selected_column)\n    value_list = []\n    for index, row in sample_df[col_name_list].iterrows():\n      value_list.append(f'({\", \".join([str(value) for value in row])})')\n\n    filtered_key_str = f'({col_tuple}) IN ({\", \".join(value_list)})'\n    new_query = query.add_where_condition(filtered_key_str)\n\n    return new_query, selected_column\n\n\n  def get_input_query_for_inference_service_filtered_index(\n      self,\n      bound_service: BoundInferenceService,\n      vector_id_table: str,\n      filtered_id_list: Optional[List[int]] = None\n  ):\n    \"\"\"\n    This function returns the input query to fetch the input records for an inference service.\n    If filtered_id_list is provided, input query will only select those blobs in the list.\n    And this query doesn't filter vector id on original predicate.\n    * param bound_service: bounded inference service, used to know input columns\n    * param vector_id_table: a table which contains a column named 'vector_id' and maps blob keys to vector id\n    * param filtered_id_list: list of vector ids which will be selected in the query\n    \"\"\"\n\n    _, select_join_str = self._get_select_join_str(bound_service, vector_id_table=vector_id_table)\n    if filtered_id_list == None:\n      return select_join_str\n\n    new_query = Query(select_join_str, self._config)\n    sample_df = pd.DataFrame({VECTOR_ID_COLUMN: filtered_id_list})\n    filter_column = [f'{vector_id_table}.{VECTOR_ID_COLUMN}']\n    query_add_filter_key, _ = self.add_filter_key_into_query(filter_column,\n                                                         sample_df,\n                                                         new_query)\n\n    return query_add_filter_key.sql_str\n\n\n  def _get_left_join_str(self, rep_table_name: str, tables: List[str]) -> str:\n    \"\"\"\n    Constructs a LEFT JOIN SQL string based on the given representative table name and a list of table names.\n    :param rep_table_name: Name of the representative table.\n    :param tables: List of table names to join.\n    \"\"\"\n    join_strs = []\n    rep_cols = [rep_col.name.split('.')[0] for rep_col in self._config.tables[rep_table_name].columns]\n    for table_name in tables:\n      table_cols = [table_col.name.split('.')[0] for table_col in self._config.tables[table_name].columns]\n      join_conditions = [f'{rep_table_name}.{col} = {table_name}.{col}' for col in table_cols if col in rep_cols]\n      if join_conditions:\n        join_str = f\"LEFT JOIN {table_name} ON {' AND '.join(join_conditions)}\"\n        join_strs.append(join_str)\n      else:\n        raise Exception(f'Can\\'t join table {rep_table_name} and {table_name}')\n    return f'FROM {rep_table_name}\\n' + '\\n'.join(join_strs)\n\n\n  # ---------------------\n  # User Defined Function\n  # ---------------------\n  def register_user_defined_function(self, function_name, function):\n    self._config.add_user_defined_function(function_name, function)\n\n\n  def _call_user_function(self, res_df: pd.DataFrame, function_name: str, args_list: List[str]):\n    function_name = str.upper(function_name)\n\n    if inspect.iscoroutinefunction(self._config.user_defined_functions[function_name]):\n      list_function_results = asyncio_run(self._config.user_defined_functions[function_name](res_df[args_list]))\n    else:\n      list_function_results = self._config.user_defined_functions[function_name](res_df[args_list])\n\n    if len(list_function_results) != len(res_df):\n      raise Exception('The length of the UDF outputs should match that of the inputs.')\n\n    # Function return format\n    # 1. Dataframe -> directly return\n    # 2. List:\n    #    a. Nested List -> convert the inner lists to Dataframes, return a list of Dataframes\n    #    b. List of Dataframes -> directly return\n    #    c. others -> convert the entire list to a Dataframe\n    # 3. pd.Series or np.ndarray -> convert it to a Dataframe\n    if isinstance(list_function_results, pd.DataFrame):\n      return list_function_results\n    elif isinstance(list_function_results, list):\n      if isinstance(list_function_results[0], list):\n        return [pd.DataFrame(function_result) for function_result in list_function_results]\n      elif isinstance(list_function_results[0], pd.DataFrame):\n        return list_function_results\n      else:\n        return pd.DataFrame(list_function_results)\n    elif isinstance(list_function_results, pd.Series) or isinstance(list_function_results, np.ndarray):\n      return pd.DataFrame(list_function_results)\n    else:\n      raise Exception('Unsupported data type')\n\n\n  def _get_udf_result(self, res_df, dataframe_sql):\n    '''\n    this function get results of user defined function\n    the output of UDF has the following situations:\n    1. single value, e.g. sum, max function\n    2. zero row, e.g. no object in an image\n    3. multiple rows, e.g. several objects in an image\n    4. multiple columns in one row, e.g. output of object detection model has columns 'x_min, y_min, x_max, y_max\n    '''\n    expanded_columns_mapping = {}\n    for udf in dataframe_sql['udf_mapping']:\n\n      udf_results = self._call_user_function(res_df, udf['function_name'], udf['col_names'])\n      # if the result is a list of DataFrames, concatenate all the DataFrames into a single DataFrame.\n      if isinstance(udf_results, list):\n        repeat_counts = [len(df.dropna()) for df in udf_results]\n        res_df = pd.DataFrame(np.repeat(res_df.values, repeat_counts, axis=0), columns=res_df.columns)\n        udf_results = pd.concat(udf_results, axis=0).convert_dtypes().dropna()\n\n      # expand dataframe for multiple columns output\n      if len(udf_results.columns) == len(udf['result_col_name']):\n        udf_results.columns = udf['result_col_name']\n      elif len(udf['result_col_name']) == 1:\n        expanded_column_names = [f\"{udf['result_col_name'][0]}__{i}\" for i in range(len(udf_results.columns))]\n        udf_results.columns = expanded_column_names\n        expanded_columns_mapping[udf['result_col_name'][0]] = ', '.join(expanded_column_names)\n      else:\n        raise Exception(f\"The number of column alias in query doesn't match the number of output columns of \"\n                        f\"function{udf['function_name']}\")\n\n      # concatenate input_df with UDF result dataframe\n      for col in udf_results.columns:\n        res_df[col] = udf_results[col].values\n\n      res_df.dropna(inplace=True)\n    return res_df, expanded_columns_mapping\n\n\n  def execute_user_defined_function(\n      self,\n      res_df: pd.DataFrame,\n      dataframe_sql: Dict,\n      query: Query,\n      additional_select_col: List[str] = []\n  ) -> pd.DataFrame:\n    '''\n    This function receive the query result from database, and then applies user defined function to query result.\n    After getting function results, this function execute query which is extracted from original query and return final\n    result.\n    Args:\n      res_df: Query result from normal db.\n      dataframe_sql: Extracted information of udf query, used to execute udf and run sql query over udf results\n      query: original query\n      additional_select_col: used to specifically choose '__weight' and '__mass' columns,\n        which are not included in the standard query\n\n    Returns:\n      res_list_of_tuple: query result\n\n    Raises:\n      ImportError: If duckdb is not installed\n    '''\n\n    if res_df.empty:\n      return []\n\n    # duckdb is used to run sql query over dataframe\n    # FIXME(ttt-77): remove this dependency and implement it by ourselves\n    try:\n      import duckdb\n    except ImportError:\n      raise Exception('Package duckdb is needed for query with UDF. Please install it with version 0.9.2 first,')\n\n    processed_udf_result_df, expanded_columns_mapping = self._get_udf_result(res_df, dataframe_sql)\n\n    select_str = ', '.join(dataframe_sql['select_col'] + additional_select_col)\n    for k,v in expanded_columns_mapping.items():\n      select_str = select_str.replace(k, v)\n    where_condition = query.convert_and_connected_fp_to_exp(dataframe_sql['filter_predicate'])\n    if where_condition:\n      where_str = f'WHERE {where_condition.sql()}'\n    else:\n      where_str = ''\n    df_query = f'''SELECT {select_str} FROM processed_udf_result_df {where_str}'''\n\n    new_results_df = duckdb.sql(df_query).df()\n    return new_results_df\n"}
{"type": "source_file", "path": "aidb/estimator/estimator.py", "content": "import abc\nimport math\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport scipy.stats\nfrom statsmodels.stats.weightstats import DescrStatsW\n\nfrom aidb.utils.logger import logger\nfrom aidb.utils.constants import NUM_ITEMS_COL_NAME, WEIGHT_COL_NAME\n\n\n@dataclass\nclass Estimate:\n  estimate: float\n  upper_bound: float\n  lower_bound: float\n  std: float\n  std_ub: float\n\n\ndef _get_estimate_bennett(\n    estimate: float,\n    std: float,\n    max_statistic: float,\n    num_samples: int,\n    conf: float\n) -> Estimate:\n  delta_inp = 1. - conf\n  b = scipy.stats.chi2.ppf(delta_inp / 2, num_samples - 1)  # lower critical value\n  std_ub = std * np.sqrt(num_samples - 1) / np.sqrt(b)\n  var_ub = std_ub ** 2\n\n  logger.debug(\n      f'estimate: {estimate}, std: {std}, max_statistic: {max_statistic},num_samples: {num_samples}, conf: {conf}'\n  )\n\n  delta = delta_inp / 2.0\n  a = max_statistic\n  t = var_ub / a * \\\n      (np.exp(\n        scipy.special.lambertw(\n          (a ** 2 * np.log(1. / delta) - var_ub) \\\n          / (math.e * var_ub)\n        ) + 1) - 1)\n  t /= num_samples\n  t = t.real\n\n  lb = estimate - t\n  ub = estimate + t\n\n  ans = Estimate(estimate, ub, lb, std, std_ub)\n  return ans\n\n\nclass Estimator(abc.ABC):\n  def __init__(self, population_size: int) -> None:\n    self._population_size = population_size\n\n\n  @abc.abstractmethod\n  def estimate(self, samples: pd.DataFrame, num_samples: int, conf: float, **kwargs) -> Estimate:\n    pass\n\n\n# Set estimators\nclass WeightedMeanSetEstimator(Estimator):\n  def estimate(self, samples: pd.DataFrame, num_samples: int, conf: float, **kwargs) -> Estimate:\n    weights = samples[WEIGHT_COL_NAME].to_numpy()\n    statistics = samples.iloc[:, 0].to_numpy()\n    counts = samples[NUM_ITEMS_COL_NAME].to_numpy()\n    cstats = np.repeat(statistics, counts)\n    weights = np.repeat(weights, counts)\n    wstats = DescrStatsW(cstats, weights=weights, ddof=0)\n    return _get_estimate_bennett(\n      wstats.mean,\n      wstats.std,\n      np.abs(cstats).max(),\n      len(cstats),\n      conf\n    )\n\n\nclass WeightedCountSetEstimator(Estimator):\n  def estimate(self, samples: pd.DataFrame, num_samples: int, conf: float, **kwargs) -> Estimate:\n\n    # all weights are same, add weights for blobs that don't have outputs\n    weights = np.array([samples[WEIGHT_COL_NAME].to_numpy()[0]] * num_samples)\n\n    # Statistics are already counts\n    statistics = np.array(samples.iloc[:, 0].tolist() + [0.0] * (num_samples - len(samples)))\n    wstats = DescrStatsW(statistics, weights=weights, ddof=0)\n    mean_est = _get_estimate_bennett(\n      wstats.mean,\n      wstats.std,\n      np.abs(statistics).max(),\n      len(statistics),\n      conf\n    )\n\n    return Estimate(\n      mean_est.estimate * self._population_size,\n      mean_est.upper_bound,\n      mean_est.lower_bound,\n      mean_est.std,\n      mean_est.std_ub\n    )\n\n\n# Logic is exactly the same for the count estimator\nclass WeightedSumSetEstimator(WeightedCountSetEstimator):\n  pass\n"}
{"type": "source_file", "path": "aidb/engine/non_select_query_engine.py", "content": "from sqlalchemy.sql import text\n\nfrom aidb.engine.base_engine import BaseEngine\nfrom aidb.query.query import Query\n\n\nclass NonSelectQueryEngine(BaseEngine):\n  async def execute_non_select(self, query: Query):\n    '''\n    Executes non select query\n    '''\n    # The query is irrelevant since we do a full scan anyway\n    async with self._sql_engine.begin() as conn:\n      res = await conn.execute(text(query.sql_query_text))\n      return res\n"}
{"type": "source_file", "path": "aidb/engine/approx_aggregate_join_engine.py", "content": "from typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport sqlglot.expressions as exp\nfrom sqlalchemy.sql import text\n\nfrom aidb.engine.approx_aggregate_engine import ApproximateAggregateEngine\nfrom aidb.query.query import Query\nfrom aidb.utils.constants import (MASS_COL_NAME, NUM_ITEMS_COL_NAME,\n                                  WEIGHT_COL_NAME)\nfrom aidb.utils.logger import logger\n\n_NUM_PILOT_SAMPLES = 60000\n\nRETRIEVAL_BATCH_SIZE = 20000\n\nclass ApproximateAggregateJoinEngine(ApproximateAggregateEngine):\n  async def get_rows_in_join_tables(self, blob_tables: List[str], blob_key_filtering_predicates):\n    '''\n    Function to return a list of dataframes, with each DataFrame containing all rows from a corresponding blob table\n    '''\n    sample_df_list = []\n\n    for table in blob_tables:\n      cols = [col.name for col in self._config.tables[table].columns]\n      fp_list = []\n      for fp in blob_key_filtering_predicates:\n        if len(fp) > 1:\n          raise Exception('OR operator for blob keys filtering is not supported.')\n        if fp[0].find(exp.Column).args['table'].args['this'] == table:\n          fp_list.append(fp)\n\n      where_str = self._get_where_str(fp_list)\n      where_str = f'WHERE {where_str}' if where_str else ''\n      select_str = ', '.join(cols)\n      query_str = f'''\n                   SELECT {select_str}\n                   FROM {table}\n                   {where_str};\n                   '''\n      async with self._sql_engine.begin() as conn:\n        sample_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(query_str), conn))\n        sample_df_list.append(sample_df)\n\n    return sample_df_list\n\n\n  def execute_join_query_sampling(self, sample_df_list, num_samples, col_to_alias, agg_col_to_alias):\n    data_size = (len(sample_df_list[0]), len(sample_df_list[1]))\n\n    self.blob_count = np.prod(data_size)\n    sample = np.random.choice(self.blob_count, size=num_samples, replace=True)\n    sample_ids = np.array(np.unravel_index(sample, data_size))\n\n    sampled_cols = pd.concat(\n        [sample_df_list[0].iloc[sample_ids[0]].reset_index(drop=True),\n        sample_df_list[1].iloc[sample_ids[1]].reset_index(drop=True)],\n        axis=1\n    )\n\n    sample_res_df = sampled_cols[list(col_to_alias.keys()) + list(agg_col_to_alias.keys())]\n    sample_res_df.columns = list(col_to_alias.values()) + list(agg_col_to_alias.values())\n    sample_res_df = sample_res_df.copy()\n    sample_res_df[WEIGHT_COL_NAME] = 1. / self.blob_count\n    sample_res_df[MASS_COL_NAME] = 1\n\n    return sample_res_df\n\n\n  async def execute_inference_services_join(\n      self,\n      query: Query,\n      sample_df: pd.DataFrame,\n      blob_keys: List[str],\n      agg_list: List[tuple]\n  ) -> List[pd.DataFrame]:\n    '''\n    Executed inference services on sampled data\n    '''\n    dataframe_sql, query = query.udf_query\n\n    agg_cols = [agg_col for _, agg_col in agg_list]\n    fixed_cols = [WEIGHT_COL_NAME, MASS_COL_NAME]\n    res_satisfy_udf = self.execute_user_defined_function(\n        sample_df,\n        dataframe_sql,\n        query,\n        blob_keys + fixed_cols + agg_cols\n    )\n\n    # group by primary keys\n    grouped_data = res_satisfy_udf.groupby(by=blob_keys + fixed_cols)\n\n    agg_res_list = []\n\n    agg_type_mapping = {exp.Count: 'count', exp.Sum: 'sum', exp.Avg: 'mean'}\n    for agg_type, agg_col in agg_list:\n      group_by_col_to_type = {}\n      if agg_col == '*':\n        agg_res = grouped_data.count().iloc[:,:2]\n      else:\n        group_by_col_to_type[agg_col] = [agg_type_mapping[agg_type], 'count']\n        agg_res = grouped_data.agg(group_by_col_to_type)\n\n      agg_res.columns = [agg_col, NUM_ITEMS_COL_NAME]\n      agg_res_list.append(agg_res.reset_index())\n    return agg_res_list\n\n\n  async def execute_sampling_and_join_services(\n      self,\n      query: Query,\n      blob_tables: List[str],\n      num_samples: int,\n      col_to_alias: Dict[str, str],\n      agg_col_to_alias: Dict[str, str],\n      agg_list: List[tuple]\n  ) -> List[pd.DataFrame]:\n    _, udf_query = query.udf_query\n\n    # Extract filtering predicates for blob table queries\n    filtering_predicates = udf_query.filtering_predicates\n    blob_key_filtering_predicates = []\n    inference_engines_required = udf_query.inference_engines_required_for_filtering_predicates\n\n    for filtering_predicate, engine_required in zip(filtering_predicates, inference_engines_required):\n      if len(engine_required) == 0:\n        blob_key_filtering_predicates.append(filtering_predicate)\n\n    sample_df_list = await self.get_rows_in_join_tables(blob_tables, blob_key_filtering_predicates)\n\n    blob_keys = []\n    for blob_table in blob_tables:\n      for blob_key in self._config.tables[blob_table].primary_key:\n        if blob_key in col_to_alias:\n          blob_key = col_to_alias[blob_key]\n        blob_keys.append(blob_key)\n\n    res_store_list = []\n\n    # selectively sample data to avoid large memory usage\n    total_sampled_count = 0\n    while total_sampled_count < num_samples:\n      sample_res_df = self.execute_join_query_sampling(\n          sample_df_list,\n          RETRIEVAL_BATCH_SIZE,\n          col_to_alias,\n          agg_col_to_alias\n      )\n      if total_sampled_count + len(sample_res_df) > num_samples:\n        sample_res_df = sample_res_df.iloc[:num_samples-total_sampled_count]\n      total_sampled_count += len(sample_res_df)\n\n      agg_res_list = await self.execute_inference_services_join(query, sample_res_df, blob_keys, agg_list)\n      if len(agg_res_list) != 0:\n        if len(res_store_list) == 0:\n          res_store_list = agg_res_list\n        else:\n          res_store_list = [pd.concat([df_a, df_b], ignore_index=True)\n                            for df_a, df_b in zip(res_store_list, agg_res_list)]\n\n    return res_store_list\n\n\n  def get_additional_required_num_samples_join(\n      self,\n      query: Query,\n      sample_results: List[pd.DataFrame],\n      agg_type_with_cols: List[Tuple[exp.AggFunc, str]],\n      alpha\n  ) -> int:\n    error_target = query.error_target\n    num_samples = []\n\n    index = 0\n    while index < len(agg_type_with_cols):\n      agg_type, agg_col = agg_type_with_cols[index]\n      sample_result = sample_results[index]\n      extracted_sample_results = sample_result[[agg_col, NUM_ITEMS_COL_NAME, WEIGHT_COL_NAME, MASS_COL_NAME]]\n      num_samples.append(\n        self._calculate_required_num_samples(\n          extracted_sample_results,\n          _NUM_PILOT_SAMPLES,\n          agg_type,\n          alpha,\n          error_target\n        )\n      )\n      index += 1\n\n    return max(max(num_samples), 100)\n\n\n  async def execute_aggregate_join_query(self, query: Query):\n    '''\n    Execute aggregation query using approximate processing\n    '''\n    query.is_valid_aqp_query()\n\n    query_no_aqp = query.base_sql_no_aqp\n    agg_type_with_cols = query_no_aqp.query_after_normalizing.aggregation_type_list_in_query\n\n    dataframe_sql, udf_query = query_no_aqp.udf_query\n\n    _, alias_to_col = udf_query.table_and_column_aliases_in_query\n\n    agg_col_to_alias = {}\n    agg_list = []\n\n    for i, (agg_type, agg_col) in enumerate(agg_type_with_cols):\n      if agg_col == '*':\n        agg_list.append((agg_type, agg_col))\n        continue\n      if agg_col.split('.')[1] not in agg_col_to_alias:\n        agg_col_to_alias[agg_col.split('.')[1]] = f'agg_col__{i}'\n        udf_query = udf_query.add_select(f'{agg_col} AS agg_col__{i}')\n      agg_list.append((agg_type, agg_col_to_alias[agg_col.split('.')[1]]))\n\n    col_to_alias = {value: key for key, value in alias_to_col.items()}\n\n    # FIXME(ttt-77): retrieve join tables to support self-join\n    blob_tables = query_no_aqp.blob_tables_required_for_query\n    if len(blob_tables) != 2:\n      raise Exception('JOIN query require two blob tables')\n\n    filtering_predicates = udf_query.filtering_predicates\n    blob_key_filtering_predicates = []\n    inference_engines_required = udf_query.inference_engines_required_for_filtering_predicates\n\n    for filtering_predicate, engine_required in zip(filtering_predicates, inference_engines_required):\n      if len(engine_required) == 0:\n        blob_key_filtering_predicates.append(filtering_predicate)\n\n    # Perform sampling and then conduct the inference process\n    res_list = await self.execute_sampling_and_join_services(\n        query_no_aqp,\n        blob_tables,\n        _NUM_PILOT_SAMPLES,\n        col_to_alias,\n        agg_col_to_alias,\n        agg_list\n    )\n\n    for res in res_list:\n      if len(res) == 0:\n        raise Exception('No matched pair found. Please use a larger oracle budget.')\n\n    num_aggregations = len(agg_list)\n    alpha = (1. - (query.confidence / 100.)) / num_aggregations\n    conf = 1. - alpha\n    num_samples = self.get_additional_required_num_samples_join(query, res_list, agg_list, alpha)\n\n    logger.info(f'num_samples: {num_samples}')\n\n    new_res_list = await self.execute_sampling_and_join_services(\n        query_no_aqp,\n        blob_tables,\n        num_samples,\n        col_to_alias,\n        agg_col_to_alias,\n        agg_list\n    )\n\n    concatenated_results = []\n    if len(new_res_list) != 0:\n      for res, new_res in zip(res_list, new_res_list):\n        concatenated_results.append(pd.concat([res, new_res], ignore_index=True))\n    else:\n      concatenated_results = res_list\n\n    estimates = []\n    index = 0\n    while index < len(agg_type_with_cols):\n      res_i = concatenated_results[index]\n      agg_type, agg_col = agg_list[index]\n\n      estimator = self._get_estimator(agg_type)\n      sample_res = res_i[[agg_col, NUM_ITEMS_COL_NAME, WEIGHT_COL_NAME, MASS_COL_NAME]]\n      estimates.append(\n        estimator.estimate(\n          sample_res,\n          _NUM_PILOT_SAMPLES + num_samples,\n          conf\n        ).estimate\n      )\n      index += 1\n    return [tuple(estimates)]\n"}
{"type": "source_file", "path": "aidb/__init__.py", "content": ""}
{"type": "source_file", "path": "aidb/config/config.py", "content": "from collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom functools import cached_property\nfrom typing import Callable, Dict, List, Tuple\n\nimport networkx as nx\nimport sqlalchemy\n\nfrom aidb.config.config_types import Column, Graph, InferenceBinding, Table\nfrom aidb.inference.bound_inference_service import (\n    BoundInferenceService, CachedBoundInferenceService)\nfrom aidb.inference.inference_service import InferenceService\nfrom aidb.utils.constants import (BLOB_TABLE_NAMES_TABLE, CACHE_PREFIX,\n                                  CONFIG_PREFIX)\nfrom aidb.utils.logger import logger\n\n\n@dataclass\nclass Config:\n  '''\n  Data class that holds all the information required for an AIDB instance.\n  Although the data class is mutable, none of the fields should be mutated\n  externally.\n\n  The cached_properties must be deleted when Config is mutated.\n\n  '''\n  # Inference services. Required\n  inference_services: Dict[str, InferenceService] = field(default_factory=dict)\n  inference_bindings: List[BoundInferenceService] = field(default_factory=list)\n\n  # Metadata\n  db_uri: str = ''\n  blob_tables: List[str] = field(default_factory=list)\n  # table name -> blob key (possibly composite)\n  blob_keys: Dict[str, List[str]] = field(default_factory=dict)\n\n  # Schema\n  tables: Dict[str, Table] = field(default_factory=dict)\n  columns: Dict[str, Column] = field(default_factory=dict)\n  relations: Dict[str, str] = field(default_factory=dict)  # left -> right\n\n  # User defined function. Optional\n  user_defined_functions: Dict[str, Callable] = field(default_factory=dict)\n\n\n  @cached_property\n  def inference_graph(self) -> Graph:\n    '''\n    The inference graph _nodes_ are columns. The _edges_ are inference services.\n    '''\n    graph = nx.DiGraph()\n    for column_name in self.columns.keys():\n      graph.add_node(column_name)\n\n    for bound_service in self.inference_bindings:\n      binding = bound_service.binding\n      for inp in binding.input_columns:\n        for out in binding.output_columns:\n          graph.add_edge(inp, out, bound_service=bound_service)\n    return graph\n\n\n  @cached_property\n  def table_graph(self) -> Graph:\n    '''\n    Directed graph of foreign key relationship between tables.\n    The table graph _nodes_ are tables. The _edges_ are foreign key relations.\n    If A -> B, then A has a foreign key that refers to B's primary key.\n    '''\n    table_graph = nx.DiGraph()\n    for table_name in self.tables:\n      for fk_col, pk_other_table in self.tables[table_name].foreign_keys.items():\n        parent_table = pk_other_table.split('.')[0]\n        table_graph.add_edge(table_name, parent_table)\n    return table_graph\n\n  @cached_property\n  def column_graph(self) -> Graph:\n    '''\n    Directed graph of foreign key relationship between columns.\n    The column graph _nodes_ are columns. The _edges_ are foreign key relations.\n    If A -> B, then A is a foreign key column that refers to B.\n    '''\n    column_graph = nx.DiGraph()\n    for table_name in self.tables:\n      for fk_col, pk_other_table in self.tables[table_name].foreign_keys.items():\n        column_graph.add_edge(f\"{table_name}.{fk_col}\", pk_other_table)\n    return column_graph\n\n\n  @cached_property\n  def columns_to_root_column(self) -> Dict[str, str]:\n    # for e.g. blob -> object -> color, {'color.frame': 'blob.frame', 'object.frame': 'blob.frame'}\n    def get_original_column(column, column_graph: nx.DiGraph):\n      if column not in column_graph.nodes:\n        return column\n      column_derived_from = list(column_graph[column].keys())\n      assert len(column_derived_from) <= 1\n      if len(column_derived_from) == 0:\n        return column\n      else:\n        return get_original_column(column_derived_from[0], column_graph)\n\n    column_mappings = {}\n    for col in self.columns:\n      root_col = get_original_column(col, self.column_graph)\n      if root_col != col:\n        column_mappings[col] = root_col\n    return column_mappings\n\n\n  @cached_property\n  def inference_topological_order(self) -> List[BoundInferenceService]:\n    '''\n    Returns a topological ordering of the inference services.  Note that this is\n    not necessarily the same as the order in which AIDB runs the inference\n    services depending on the query.\n    '''\n    graph = self.inference_graph\n    column_order = nx.topological_sort(graph)\n    binding_order = []\n    seen_binding_idxes = set()\n    for node in column_order:\n      edges = graph.in_edges(node)\n      for edge in edges:\n        properties = graph.get_edge_data(*edge)\n        bound_service = properties['bound_service']\n        if bound_service in seen_binding_idxes:\n          continue\n        binding_order.append(bound_service)\n        seen_binding_idxes.add(bound_service)\n\n    return binding_order\n\n\n  @cached_property\n  def dialect(self):\n    # TODO: fix this, copied from base_engine\n    dialect = self.db_uri.split(':')[0]\n    if '+' in dialect:\n      dialect = dialect.split('+')[0]\n    return dialect\n\n\n  @cached_property\n  def column_by_service(self) -> Dict[str, BoundInferenceService]:\n    '''\n    Returns a dictionary mapping output column names to the inference service that produces them.\n    '''\n    column_service = dict()\n    for bound_service in self.inference_bindings:\n      actual_output_columns = set(bound_service.binding.output_columns) - set(bound_service.copy_map.values())\n      for output_col in actual_output_columns:\n        if output_col in column_service:\n          raise Exception(f'Column {output_col} is bound to multiple services')\n        else:\n          column_service[output_col] = bound_service\n    return column_service\n\n\n  @cached_property\n  def relations_by_table(self) -> Dict[str, List[str]]:\n    raise NotImplementedError()\n\n\n  def _check_blob_table(self):\n    '''\n    Check if the blob table is valid. It must satisfy the following conditions:\n    1. There must be at least one blob table.\n    2. The blob table must exist in the database schema.\n    3. The blob table's primary key must match the blob keys in metadata.\n    '''\n    if len(self.blob_tables) == 0:\n      raise Exception('No blob table defined')\n\n    for blob_table in self.blob_tables:\n      if blob_table not in self.tables:\n        raise Exception(f'{blob_table} doesn\\'t exist in database schema')\n\n      metadata_blob_key_set = set(self.blob_keys[blob_table])\n      primary_key_set = set([f'{blob_table}.{k}' for k in self.tables[blob_table].primary_key])\n      if metadata_blob_key_set != primary_key_set:\n        raise Exception(\n          f'The actual primary key of {blob_table} doesn\\'t match the blob keys in metadata.\\n'\n          f'Keys present in metadata but missing in primary key: {metadata_blob_key_set - primary_key_set}.\\n'\n          f'Keys present in primary key but missing in metadata: {primary_key_set - metadata_blob_key_set}.'\n        )\n\n\n  def check_schema_validity(self):\n    '''\n    Check config schema, including checking blob table and checking if the table relations form a DAG.\n    '''\n    self._check_blob_table()\n\n    if not nx.is_directed_acyclic_graph(self.table_graph):\n      raise Exception('Invalid Table Schema: Table relations can not have cycle')\n\n\n  def _check_foreign_key_refers_to_primary_key(self, input_tables, output_tables):\n    '''\n    1. Each output table should include the minimal set of primary key columns from the input tables.\n    2. To ensure that no primary key column in the output table is null, any column in the output table\n       with a foreign key relationship must exist in the primary key columns of the input tables.\n    '''\n    # Assumption:\n    # 1. If table A can join table B, then the join keys are those columns that have same name in both table A and B.\n    # 2. If table C is a derived table of table A, then the foreign key columns of table C have same name as\n    # corresponding columns in table A. e.x. objects.frame -> blob.frame\n    input_primary_key_columns = set()\n    for input_table in input_tables:\n      for pk_col in self.tables[input_table].primary_key:\n        input_primary_key_columns.add(f\"{input_table}.{pk_col}\")\n\n    for output_table in output_tables:\n      out_foreign_key_columns = set()\n      out_primary_key_columns = set()\n\n      # Each output table should include the minimal set of primary key columns from the input tables.\n      for fk_col, refers_to in self.tables[output_table].foreign_keys.items():\n        if fk_col in self.tables[output_table].primary_key:\n          if refers_to not in input_primary_key_columns:\n            raise Exception(f'{output_table} primary key column {fk_col} is not in input tables')\n        out_foreign_key_columns.add(refers_to)\n\n      for pk_col in self.tables[output_table].primary_key:\n        out_primary_key_columns.add(f\"{output_table}.{pk_col}\")\n\n      # Any column in the output table with a foreign key relationship\n      # must exist in the primary key columns of the input tables\n      # or the columns that the primary key columns of the input tables refer to.\n      for pk_col in input_primary_key_columns:\n        if pk_col not in out_foreign_key_columns and pk_col not in out_primary_key_columns:\n          current_table = pk_col.split('.')[0]\n          current_pk_col = pk_col.split('.')[1]\n          raise_exception = True\n          while current_pk_col in self.tables[current_table].foreign_keys:\n            current_pk_col = self.tables[current_table].foreign_keys[current_pk_col]\n            current_table = current_pk_col.split('.')[0]\n            if current_pk_col in out_foreign_key_columns or current_pk_col in out_primary_key_columns:\n              raise_exception = False\n              break\n          if raise_exception:\n            raise Exception(f'Primary key column {pk_col} in input table is not refered by output table {output_table}')\n\n\n  def check_inference_service_validity(self, bound_inference: BoundInferenceService):\n    '''\n    Check if the inference service is valid whenever adding a bound inference.\n    It must satisfy the following conditions:\n    1. The inference service must be defined in config.\n    2. The input columns and output columns must exist in the database schema.\n    3. The output column must be bound to only one inference service.\n    4. The input table must include the minimal set of primary key columns from the output table.\n       And to ensure that no primary key column in the output table is null, any column in the output table.\n       with a foreign key relationship must exist in the primary key columns of the input tables.\n    5. The graphs of table relations and column relations must form DAGs.\n    '''\n\n    # The inference service must be defined in config.\n    if bound_inference.service.name not in self.inference_services:\n      raise Exception(f'Inference service {bound_inference.service.name} is not defined in config')\n\n    input_tables = set()\n    output_tables = set()\n    binding = bound_inference.binding\n\n    # The input columns and output columns must exist in the database schema.\n    if not binding.input_columns or not binding.output_columns:\n      raise Exception(f'Inference service {bound_inference.service.name} has no input columns or output columns')\n\n    for column in binding.input_columns:\n      if column not in self.columns:\n        raise Exception(f'Input column {column} doesn\\'t exist in database')\n      input_tables.add(column.split('.')[0])\n\n    for column in binding.output_columns:\n      if column not in self.columns:\n        raise Exception(f'Output column {column} doesn\\'t exist in database')\n      output_table = column.split('.')[0]\n      output_tables.add(output_table)\n\n    for input_col, output_col in bound_inference.copy_map.items():\n      if input_col not in self.columns:\n        raise Exception(f'Input column in copy map {input_col} doesn\\'t exist in database')\n      if output_col not in self.columns:\n        raise Exception(f'Output column in copy map {output_col} doesn\\'t exist in database')\n\n    # Check if the output column is bound to only one inference service\n    self.column_by_service\n\n    # The input table must include the minimal set of primary key columns from the output table.\n    # And to ensure that no primary key column in the output table is null, any column in the output table.\n    # with a foreign key relationship must exist in the primary key columns of the input tables.\n    self._check_foreign_key_refers_to_primary_key(input_tables, output_tables)\n\n    # The graphs of table relations and column relations must form DAGs.\n    if not nx.is_directed_acyclic_graph(self.table_graph) or not nx.is_directed_acyclic_graph(self.inference_graph):\n      raise Exception(f'Inference service {bound_inference.service.name} will result in cycle in relations')\n\n\n  def clear_cached_properties(self):\n    # Need the keys because the cached properties are only created when they are accessed.\n    keys = [key for key, value in vars(Config).items() if isinstance(value, cached_property)]\n    for key in keys:\n      if key in self.__dict__:\n        del self.__dict__[key]\n\n\n  # Mutators. The cached properties must be cleared after these are called.\n  def load_from_sqlalchemy(self, conn: sqlalchemy.engine.base.Connection):\n    '''\n    Loads the tables, columns, and relations from a sqlalchemy connection.\n    '''\n    self.clear_cached_properties()\n\n    metadata = sqlalchemy.MetaData()\n    metadata.reflect(conn)\n\n    aidb_tables: Dict[str, Table] = {}\n    aidb_cols = {}\n    aidb_relations = {}\n\n    for sqlalchemy_table in metadata.sorted_tables:\n      table_name = sqlalchemy_table.name\n      if table_name.startswith(CONFIG_PREFIX):\n        continue\n      if table_name.startswith(CACHE_PREFIX):\n        continue\n\n      table_cols = {}\n      for column in sqlalchemy_table.columns:\n        aidb_cols[str(column)] = column\n        table_cols[str(column)] = column\n        for fk in column.foreign_keys:\n          aidb_relations[str(fk.column)] = str(column)\n          aidb_relations[str(column)] = str(fk.column)\n\n      aidb_tables[table_name] = Table(sqlalchemy_table)\n\n    blob_metadata_table = sqlalchemy.Table(BLOB_TABLE_NAMES_TABLE, metadata, autoload=True, autoload_with=conn)\n    try:\n      blob_keys_flat = conn.execute(blob_metadata_table.select()).fetchall()\n      blob_tables = list(set([row['table_name'] for row in blob_keys_flat]))\n      blob_tables.sort()\n      blob_keys = defaultdict(list)\n      for row in blob_keys_flat:\n        full_name = f'{row[\"table_name\"]}.{row[\"blob_key\"]}'\n        blob_keys[row['table_name']].append(full_name)\n      for table in blob_keys:\n        blob_keys[table].sort()\n    except:\n      raise ValueError(f'Could not find blob metadata table {BLOB_TABLE_NAMES_TABLE} or table is malformed')\n\n    # TODO: load metadata columns\n    # TODO: check the engines\n    # TODO: check that the cache tables are valid\n\n    self.tables = aidb_tables\n    self.columns = aidb_cols\n    self.relations = aidb_relations\n    self.blob_tables = blob_tables\n    self.blob_keys = blob_keys\n\n    self.check_schema_validity()\n\n\n  def add_inference_service(self, service_name: str, service: InferenceService):\n    self.clear_cached_properties()\n    logger.info(f'Adding inference service {service_name}')\n    self.inference_services[service_name] = service\n\n\n  def bind_inference_service(self, bound_service: BoundInferenceService):\n    '''\n    Both the inputs and outputs are lists of column names. The ordering is critical.\n\n    The cached properties are cleared, so the toplogical sort and columns by service are updated.\n    '''\n    self.clear_cached_properties()\n    self.inference_bindings.append(bound_service)\n    try:\n      self.check_inference_service_validity(bound_service)\n    except Exception:\n      self.inference_bindings.remove(bound_service)\n      self.clear_cached_properties()\n      raise\n\n\n  def add_user_defined_function(self, function_name: str, function: Callable):\n    self.clear_cached_properties()\n    logger.info(f'Added user defined function {function_name}')\n    self.user_defined_functions[str.upper(function_name)] = function\n"}
{"type": "source_file", "path": "aidb/inference/examples/google_inference_service.py", "content": "from typing import Dict, Union, List\n\nimport subprocess\n\nfrom aidb.inference.http_inference_service import HTTPInferenceService\n\n\ndef get_gcloud_access_token():\n  try:\n    command = \"gcloud auth application-default print-access-token\"\n    result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n    token = result.stdout.strip() \n    return token\n\n  except subprocess.CalledProcessError as e:\n    print(f\"An error occurred while getting token: {str(e)}\")\n    print(f\"stderr: {e.stderr.strip()}\")\n    return None\n\n\nclass GoogleVisionAnnotate(HTTPInferenceService):\n  def __init__(\n      self,\n      name: str='google_vision_annotate',\n      is_single: bool=False,\n      token: str=None,\n      default_args: Dict[str, Union[str, int]]=None,\n      columns_to_input_keys: Dict[str, Union[str, tuple]]=None,\n      response_keys_to_columns: Dict[Union[str, tuple], str]=None,\n      input_columns_types: Union[List, None]=None,\n      output_columns_types: Union[List, None]=None,\n      preferred_batch_size: int=1,\n      rate_limit: Union[int, None]=None,\n      project_id: str=None,\n      infer_type: str='images',\n  ):\n    assert infer_type in [\n        \"images\", \"files\"\n      ], \"infer_type must be images or files\"\n    assert project_id is not None, \"project_id must be specified\"\n    if token is None:\n      token = get_gcloud_access_token()\n    super().__init__(\n        name=name,\n        url=f'https://vision.googleapis.com/v1/{infer_type}:annotate',\n        headers={\n          'Content-Type': 'application/json; charset=utf-8',\n          'Authorization': f'Bearer {token}',\n          'x-goog-user-project': project_id,\n        },\n        default_args=default_args,\n        batch_supported=False,\n        is_single=is_single,\n        rate_limit=rate_limit,\n        columns_to_input_keys=columns_to_input_keys,\n        response_keys_to_columns=response_keys_to_columns,\n        input_columns_types=input_columns_types,\n        output_columns_types=output_columns_types,\n        preferred_batch_size=preferred_batch_size,\n    )\n"}
{"type": "source_file", "path": "aidb/inference/examples/openai_inference_service.py", "content": "from typing import Dict, Union, List\n\nimport os\n\nimport pandas as pd\n\nfrom aidb.inference.http_inference_service import HTTPInferenceService\n\n\nclass OpenAIAudio(HTTPInferenceService):\n  def __init__(\n      self,\n      name: str='openai_audio',\n      is_single: bool=False,\n      token: str=None,\n      default_args: Dict[str, Union[str, int]]=None,\n      columns_to_input_keys: Dict[str, Union[str, tuple]]=None,\n      response_keys_to_columns: Dict[Union[str, tuple], str]=None,\n      input_columns_types: Union[List, None]=None,\n      output_columns_types: Union[List, None]=None,\n      preferred_batch_size: int=1,\n      rate_limit: Union[int, None]=None,\n      infer_type: str='transcriptions'):\n    assert infer_type in [\n        \"transcriptions\",\n        \"translations\",\n      ], \"infer_type must be transcriptions or translations\"\n    if token is None:\n      token = os.environ['OPENAI_API_KEY']\n    super().__init__(\n      name=name,\n      url=f'https://api.openai.com/v1/audio/{infer_type}',\n      headers={\n        'Content-Type': 'application/json',\n        'Authorization': f'Bearer {token}',\n      },\n      default_args=default_args,\n      batch_supported=False,\n      is_single=is_single,\n      rate_limit=rate_limit,\n      columns_to_input_keys=columns_to_input_keys,\n      response_keys_to_columns=response_keys_to_columns,\n      input_columns_types=input_columns_types,\n      output_columns_types=output_columns_types,\n      preferred_batch_size=preferred_batch_size,\n    )\n\n\nclass OpenAIImage(HTTPInferenceService):\n  def __init__(\n      self,\n      name='openai_image', \n      is_single: bool=False,\n      token: str=None, \n      default_args: Dict[str, Union[str, int]]=None,\n      columns_to_input_keys: Dict[str, Union[str, tuple]]=None,\n      response_keys_to_columns: Dict[Union[str, tuple], str]=None,\n      input_columns_types: Union[List, None]=None,\n      output_columns_types: Union[List, None]=None,\n      preferred_batch_size: int=1,\n      rate_limit: Union[int, None]=None,\n      infer_type: str='generations'\n    ):\n    assert infer_type in [\n        \"generations\",\n        \"edits\",\n        \"variations\",\n      ], \"infer_type must be generations, edits or variations\"\n    if token is None:\n      token = os.environ['OPENAI_API_KEY']\n    super().__init__(\n      name=name,\n      url=f'https://api.openai.com/v1/images/{infer_type}',\n      headers={\n        'Content-Type': 'application/json',\n        'Authorization': f'Bearer {token}',\n      },\n      default_args=default_args,\n      batch_supported=False,\n      is_single=is_single,\n      rate_limit=rate_limit,\n      columns_to_input_keys=columns_to_input_keys,\n      response_keys_to_columns=response_keys_to_columns,\n      input_columns_types=input_columns_types,\n      output_columns_types=output_columns_types,\n      preferred_batch_size=preferred_batch_size,\n    )\n\n\nclass OpenAIText(HTTPInferenceService):\n  def __init__(\n      self,\n      name: str='openai_text',\n      is_single: bool=False,\n      token: str=None,\n      default_args: Dict[str, Union[str, int]]=None,\n      columns_to_input_keys: Dict[str, Union[str, tuple]]=None,\n      response_keys_to_columns: Dict[Union[str, tuple], str]=None,\n      input_columns_types: Union[List, None]=None,\n      output_columns_types: Union[List, None]=None,\n      preferred_batch_size: int=1,\n      rate_limit: Union[int, None]=None,\n      prompt_prefix: str='',\n      prompt_suffix: str='',\n    ):\n    if token is None:\n      token = os.environ['OPENAI_API_KEY']\n    super().__init__(\n      name=name,\n      url='https://api.openai.com/v1/chat/completions',\n      headers={\n        'Content-Type': 'application/json',\n        'Authorization': f'Bearer {token}',\n      },\n      default_args=default_args,\n      batch_supported=False,\n      is_single=is_single,\n      rate_limit=rate_limit,\n      columns_to_input_keys=columns_to_input_keys,\n      response_keys_to_columns=response_keys_to_columns,\n      input_columns_types=input_columns_types,\n      output_columns_types=output_columns_types,\n      preferred_batch_size=preferred_batch_size,\n    )\n    self.prompt_prefix = prompt_prefix\n    self.prompt_suffix = prompt_suffix\n\n\n  def convert_input_to_request(self, input: Union[pd.Series, pd.DataFrame]) -> Dict:\n    request = super().convert_input_to_request(input)\n    if 'messages' in request and isinstance(request['messages'], list):\n      for i in range(len(request['messages'])):\n        if 'content' in request['messages'][i]:\n          request['messages'][i]['content'] = self.prompt_prefix + request['messages'][i]['content'] + self.prompt_suffix\n    return request\n"}
{"type": "source_file", "path": "aidb/estimator/__init__.py", "content": ""}
{"type": "source_file", "path": "aidb/inference/examples/pytorch_local_detection.py", "content": "import pandas as pd\n\nfrom groundingdino.util.inference import Model\nfrom aidb.inference.cached_inference_service import CachedInferenceService\n\n\nclass PyTorchLocalObjectDetection(CachedInferenceService):\n  def __init__(\n      self,\n      name: str,\n      model_config_path: str,\n      model_checkpoint_path: str,\n      caption: str,\n      is_single: bool=False,\n      use_batch: bool=True,\n      batch_size: int=1,\n      box_threshold: float=0.35,\n      device: str=\"cuda\",\n  ):\n    super().__init__(name=name, preferred_batch_size=batch_size, is_single=is_single)\n    self._model = Model(model_config_path, model_checkpoint_path, device=device)\n    self._caption = caption\n    self._use_batch = use_batch\n    self._box_threshold = box_threshold\n\n\n  def signature(self):\n    raise NotImplementedError()\n\n\n  def infer_one(self, input: pd.Series) -> pd.DataFrame:\n    image = [list(input.to_dict().values())[0]]\n    output = self._model.predict_with_caption(image, self._caption, self._box_threshold)[0]\n    output = [\n      {\n        \"image\": image[0],\n        \"min_x\": xyxy[0],\n        \"min_y\": xyxy[1],\n        \"max_x\": xyxy[2],\n        \"max_y\": xyxy[3],\n        \"confidence\": conf,\n      } for xyxy, conf in zip(output.xyxy, output.confidence)]\n    return pd.DataFrame(output)\n\n\n  def infer_batch(self, inputs: pd.DataFrame) -> pd.DataFrame:\n    if not self._use_batch:\n      return super().infer_batch(inputs)\n\n    images = list(inputs.to_dict(orient=\"list\").values())[0]\n    outputs_merge = []\n    for i in range(0, len(images), self.preferred_batch_size):\n      image_batch = images[i:i+self.preferred_batch_size] if i+self.preferred_batch_size < len(images) else images[i:]\n      outputs = self._model.predict_with_caption(image_batch, self._caption, self._box_threshold)\n      outputs = [\n        {\n          \"image\": image,\n          \"min_x\": xyxy[0],\n          \"min_y\": xyxy[1],\n          \"max_x\": xyxy[2],\n          \"max_y\": xyxy[3],\n          \"confidence\": conf,\n        } for image, output in zip(image_batch, outputs) for xyxy, conf in zip(output.xyxy, output.confidence)]\n      outputs_merge.extend(outputs)\n    return pd.DataFrame(outputs_merge)\n"}
{"type": "source_file", "path": "aidb/inference/bound_inference_service.py", "content": "from dataclasses import dataclass, field\nfrom typing import Dict, List\n\nimport pandas as pd\nimport sqlalchemy.ext.asyncio\nfrom sqlalchemy import tuple_\nfrom sqlalchemy.schema import ForeignKeyConstraint\nfrom sqlalchemy.sql import text\n\nfrom aidb.config.config_types import Column, InferenceBinding, Table\nfrom aidb.inference.inference_service import InferenceService\nfrom aidb.utils.asyncio import asyncio_run\nfrom aidb.utils.constants import cache_table_name_from_inputs\nfrom aidb.utils.logger import logger\nfrom aidb.utils.type_conversion import pandas_dtype_to_native_type\n\n@dataclass\nclass BoundInferenceService():\n  service: InferenceService\n  binding: InferenceBinding\n  copy_map: Dict[str, str]\n\n  def infer(self, inputs):\n    raise NotImplementedError()\n\n  def __hash__(self):\n    # The binding must be unique\n    return hash(self.binding)\n\n\n@dataclass\nclass CachedBoundInferenceService(BoundInferenceService):\n  _engine: sqlalchemy.ext.asyncio.AsyncEngine\n  _columns: Dict[str, Column]\n  _tables: Dict[str, Table]\n  _dialect: str\n  _verbose: bool=False\n\n\n  def optional_tqdm(self, iterable, **kwargs):\n    if self._verbose:\n      from tqdm import tqdm\n      return tqdm(iterable, **kwargs)\n    return iterable\n\n\n  def convert_normalized_col_name_to_cache_col_name(self, column_name: str):\n    return column_name.replace('.', '__')\n\n\n  def convert_cache_column_name_to_normalized_column_name(self, column_name: str):\n    return column_name.replace('__', '.')\n\n\n  def __post_init__(self):\n    self._cache_table_name = cache_table_name_from_inputs(self.service.name, self.binding.input_columns)\n    logger.debug(f'Cache table name: {self._cache_table_name}')\n\n    # Get table takes a synchronous connection\n    def get_table(conn: sqlalchemy.engine.base.Connection):\n      inspector = sqlalchemy.inspect(conn)\n      metadata = sqlalchemy.MetaData()\n\n      columns = []\n      fk_constraints = {}\n      for column_name in self.binding.input_columns:\n        column = self._columns[column_name]\n        if column.primary_key:\n          new_table_col_name = self.convert_normalized_col_name_to_cache_col_name(column_name)\n          logger.debug(f'New table col name: {new_table_col_name}')\n          logger.debug(f'Ref column: {str(column)}')\n          fk_ref_table_name = str(column).split('.')[0]\n          if fk_ref_table_name not in fk_constraints:\n            fk_constraints[fk_ref_table_name] = {'cols': [], 'cols_refs': []}\n          # both tables will have same column name\n          fk_constraints[fk_ref_table_name]['cols'].append(new_table_col_name)\n          fk_constraints[fk_ref_table_name]['cols_refs'].append(column)\n          columns.append(sqlalchemy.schema.Column(new_table_col_name, column.type))\n\n      multi_table_fk_constraints = []\n      for _, fk_cons in fk_constraints.items():\n        multi_table_fk_constraints.append(ForeignKeyConstraint(fk_cons['cols'], fk_cons['cols_refs']))\n\n      table = sqlalchemy.schema.Table(self._cache_table_name, metadata, *columns, *multi_table_fk_constraints)\n      # Create the table if it doesn't exist\n      if not self._cache_table_name in inspector.get_table_names():\n        metadata.create_all(conn)\n\n      return table, columns\n\n    async def tmp():\n      async with self._engine.begin() as conn:\n        return await conn.run_sync(get_table)\n\n    self._cache_table, self._cache_columns = asyncio_run(tmp())\n\n    # Query the table to see if it works\n    async def query_table():\n      async with self._engine.begin() as conn:\n        tmp = await conn.execute(self._cache_table.select().limit(1))\n        tmp.fetchall()\n\n    try:\n      asyncio_run(query_table())\n    except:\n      raise ValueError(f'Could not query table {self._cache_table_name}')\n\n    self._cache_query_stub = sqlalchemy.sql.select(self._cache_columns)\n\n    output_tables = set()\n    output_cols_with_label = []\n    for col_name in self.binding.output_columns:\n      col = self._columns[col_name]\n      output_tables.add(str(col.table))\n      output_cols_with_label.append(col.label(col_name))\n\n    joined = self._cache_table\n    for table_name in output_tables:\n      condition = []\n      for col in self._tables[table_name].columns:\n        for cache_col in self._cache_columns:\n          normal_name = cache_col.name.split('__')[1]\n          if col.name == normal_name:\n            condition.append(getattr(self._cache_table.c, cache_col.name)\n                             == getattr(self._tables[table_name]._table.c, col.name))\n      if len(condition) != 0:\n        # Connected by 'AND' when the key is composite\n        join_condition = sqlalchemy.sql.and_(*condition)\n      else:\n        # Use CROSS JOIN in the absence of a specific condition.\n        join_condition = sqlalchemy.sql.true()\n      joined = sqlalchemy.join(joined, self._tables[table_name]._table, join_condition)\n    self._result_query_stub = sqlalchemy.sql.select(output_cols_with_label).select_from(joined)\n\n\n  def get_insert(self):\n    dialect = self._dialect\n    if dialect == 'sqlite':\n      return sqlalchemy.dialects.sqlite.insert\n    elif dialect == 'mysql':\n      return sqlalchemy.dialects.mysql.insert\n    elif dialect == 'postgresql':\n      return sqlalchemy.dialects.postgresql.insert\n    else:\n      raise NotImplementedError(f'Unknown dialect {dialect}')\n\n\n  def get_tables(self, columns: List[str]) -> List[str]:\n    tables = set()\n    for col in columns:\n      table_name = col.split('.')[0]\n      tables.add(table_name)\n    return list(tables)\n\n\n  async def _insert_in_cache_table(self, inp_rows_df: pd.DataFrame, conn):\n    inp_rows_df.columns = [self.convert_normalized_col_name_to_cache_col_name(col) for col in inp_rows_df.columns]\n    # convert the pandas datatype to python native type\n    # TODO: can we remove /resolve this?\n    inp_rows_df = inp_rows_df.astype('object')\n    # this doesn't support upsert queries\n    await conn.run_sync(lambda conn: inp_rows_df.to_sql(self._cache_table.name, conn, if_exists='append', index=False))\n\n\n  async def _insert_output_results_in_tables(self, output_data: List[pd.DataFrame], input_data: pd.DataFrame, conn):\n    if len(output_data) > 0:\n      for input_col, output_col in self.copy_map.items():\n        assert len(output_data) == len(input_data), 'Each input row should have 1 corresponding output dataframe, even if it is empty'\n        for idx, df in enumerate(output_data):\n          if len(df) > 0:\n            df[output_col] = input_data.iloc[idx][input_col]\n      inference_results = pd.concat(output_data, ignore_index=True)\n      for idx, col in enumerate(self.binding.output_columns):\n        inference_results.rename(columns={inference_results.columns[idx]: col}, inplace=True)\n      tables = self.get_tables(self.binding.output_columns)\n      for table in tables:\n        columns = [col for col in self.binding.output_columns if col.startswith(table + '.')]\n        tmp_df = inference_results[columns]\n        # convert the pandas datatype to python native type\n        tmp_df = tmp_df.astype('object')\n        tmp_df.columns = [col.split('.')[1] for col in tmp_df.columns]\n        # this doesn't support upsert queries\n        await conn.run_sync(lambda conn: tmp_df.to_sql(table, conn, if_exists='append', index=False))\n\n\n  async def _get_inputs_not_in_cache_table(self, inputs: pd.DataFrame, conn):\n    \"\"\"\n    Checks the presence of inputs in the cache table and returns the inputs that are not in the cache table.\n    \"\"\"\n    cache_entries = await conn.run_sync(lambda conn: pd.read_sql_query(text(str(self._cache_query_stub.compile())), conn))\n    cache_entries = cache_entries.set_index([col.name for col in self._cache_columns])\n    normalized_cache_cols = [self.convert_cache_column_name_to_normalized_column_name(col.name) for col in self._cache_columns]\n\n    if len(normalized_cache_cols) == 1:\n        # For a single column, use `isin` and negate the condition\n        col = normalized_cache_cols[0]\n        is_in_cache = inputs[col].isin(cache_entries.index)\n    else:\n        # For multiple columns, create tuples and use set operations\n        inputs_tuples = inputs[normalized_cache_cols].apply(tuple, axis=1)\n        cache_tuples = set(cache_entries.index)\n        is_in_cache = inputs_tuples.isin(cache_tuples)\n\n    in_cache_df_primary = inputs[is_in_cache][normalized_cache_cols]\n    out_cache_df = inputs[~is_in_cache]\n    out_cache_df_primary = out_cache_df[normalized_cache_cols]\n\n    return out_cache_df, out_cache_df_primary, in_cache_df_primary\n\n\n  async def infer(self, inputs: pd.DataFrame, return_inference_results=False):\n    # FIXME: figure out where to put the column renaming\n    for idx, col in enumerate(self.binding.input_columns):\n      inputs.rename(columns={inputs.columns[idx]: col}, inplace=True)\n\n    # Drop duplicate inputs\n    inputs_drop_duplicates = inputs.drop_duplicates()\n    # Note: the input columns are assumed to be in order\n    logger.info(f'{self.service.name} Require {len(inputs_drop_duplicates)} inputs')\n    async with self._engine.begin() as conn:\n      inputs_not_in_cache, inputs_not_in_cache_primary_cols, inputs_in_cache_primary_df = \\\n          await self._get_inputs_not_in_cache_table(inputs_drop_duplicates, conn)\n      logger.info(f'Inferencing {len(inputs_not_in_cache)} inputs')\n      records_to_insert_in_table = []\n      bs = self.service.preferred_batch_size\n      input_batches = [inputs_not_in_cache.iloc[i:i + bs] for i in range(0, len(inputs_not_in_cache), bs)]\n      # Batch inference service: move copy input logic to inference service and add \"copy_input\" to binding\n      for input_batch in self.optional_tqdm(input_batches):\n        inference_results = self.service.infer_batch(input_batch)\n        records_to_insert_in_table.extend(inference_results)\n      await self._insert_in_cache_table(inputs_not_in_cache_primary_cols, conn)\n      await self._insert_output_results_in_tables(records_to_insert_in_table, inputs_not_in_cache, conn)\n\n    if return_inference_results:\n      return_res = records_to_insert_in_table.copy()\n\n      # Retrieve cached results\n      sampled_key_list= []\n      for _, inp_row in inputs_in_cache_primary_df.iterrows():\n        key_tuple = tuple(\n          pandas_dtype_to_native_type(getattr(inp_row, col)) for col in inputs_in_cache_primary_df.columns\n        )\n        sampled_key_list.append(key_tuple)\n\n      if sampled_key_list:\n        columns = [self.convert_normalized_col_name_to_cache_col_name(col)\n                   for col in inputs_in_cache_primary_df.columns]\n        sql_columns = [getattr(self._cache_table.c, col_name) for col_name in columns]\n        where_condition = tuple_(*sql_columns).in_(sampled_key_list)\n        query = self._result_query_stub.where(where_condition)\n        async with self._engine.begin() as conn:\n          cached_df = await conn.run_sync(lambda conn: pd.read_sql(query, conn))\n        return_res.append(cached_df)\n\n      inference_df = pd.concat(return_res, ignore_index=True)\n      left_on_cols = []\n      right_on_cols = []\n      for left_col in inputs.columns:\n        for right_col in inference_df.columns:\n          if left_col.split('.')[1] == right_col.split('.')[1]:\n            left_on_cols.append(left_col)\n            right_on_cols.append(right_col)\n      # Duplicated inputs will be dropped when running inference.\n      # To obtain a result set that matches the size of inputs, use a LEFT JOIN\n      merged_df = pd.merge(inputs, inference_df, left_on=left_on_cols, right_on=right_on_cols)\n      return merged_df.drop(columns=left_on_cols)\n\n\n  def __hash__(self):\n    return super().__hash__()\n"}
{"type": "source_file", "path": "aidb/inference/examples/huggingface_inference_service.py", "content": "from typing import Dict, Union, List\nimport os\nimport requests\nimport pandas as pd\n\nfrom aidb.inference.http_inference_service import HTTPInferenceService\n\n\nclass HuggingFaceNLP(HTTPInferenceService):\n  def __init__(\n      self,\n      name: str='huggingface_nlp',\n      is_single: bool=False,\n      token: str=None,\n      default_args: Dict[str, Union[str, int]]=None,\n      columns_to_input_keys: Dict[str, Union[str, tuple]]=None,\n      response_keys_to_columns: Dict[Union[str, tuple], str]=None,\n      input_columns_types: Union[List, None]=None,\n      output_columns_types: Union[List, None]=None,\n      preferred_batch_size: int=1,\n      rate_limit: Union[int, None]=None,\n      model: str=None):\n    if token is None:\n      token = os.environ['HF_API_KEY']\n    super().__init__(\n      name=name,\n      url=f'https://api-inference.huggingface.co/models/{model}',\n      headers={\n        'Content-Type': 'application/json; charset=utf-8',\n        'Authorization': f'Bearer {token}',\n      },\n      default_args=default_args,\n      batch_supported=False,\n      is_single=is_single,\n      rate_limit=rate_limit,\n      columns_to_input_keys=columns_to_input_keys,\n      response_keys_to_columns=response_keys_to_columns,\n      input_columns_types=input_columns_types,\n      output_columns_types=output_columns_types,\n      preferred_batch_size=preferred_batch_size,\n    )\n\n\nclass HuggingFaceVisionAudio(HTTPInferenceService):\n  def __init__(\n      self,\n      name: str='huggingface_vision_audio',\n      is_single: bool=False,\n      token: str=None,\n      default_args: Dict[str, Union[str, int]]=None,\n      response_keys_to_columns: Dict[Union[str, tuple], str]=None,\n      output_columns_types: Union[List, None]=None,\n      preferred_batch_size: int=1,\n      rate_limit: Union[int, None]=None,\n      model: str=None):\n    if token is None:\n      token = os.environ['HF_API_KEY']\n    super().__init__(\n      name=name,\n      url=f'https://api-inference.huggingface.co/models/{model}',\n      headers={\n        'Content-Type': 'application/json; charset=utf-8',\n        'Authorization': f'Bearer {token}',\n      },\n      default_args=default_args,\n      batch_supported=False,\n      is_single=is_single,\n      rate_limit=rate_limit,\n      response_keys_to_columns=response_keys_to_columns,\n      output_columns_types=output_columns_types,\n      preferred_batch_size=preferred_batch_size,\n    )\n\n\n  def convert_input_to_request(self, input: pd.Series) -> Dict:\n    return input.to_dict().values()[0]\n\n\n  def request(self, request: str) -> Dict:\n    with open(request, 'rb') as f:\n      response = requests.post(self._url, data=f, headers=self._headers)\n    response.raise_for_status()\n    return response.json()\n"}
{"type": "source_file", "path": "aidb/inference/examples/llm_inference_service.py", "content": "import json\nfrom typing import Dict, List, Tuple, Union\nimport logging\nimport pandas as pd\nfrom jinja2 import Template\nfrom litellm import completion\n\nfrom aidb.inference.http_inference_service import CachedInferenceService\nfrom aidb.inference.utils import parse_output_schema\nfrom aidb.utils.perf_utils import call_counter\n\n\nclass LLMInference(CachedInferenceService):\n  def __init__(\n      self,\n      *args,\n      model: str = None,\n      output_schema: Dict[str, str] = None,\n      prompt: str = None,\n      model_config: dict = None,\n      **kwargs\n  ):\n    super().__init__(*args, **kwargs)\n    self.model = model\n    self.template = Template(prompt)\n    self.output_schema = parse_output_schema(\n        self.name, output_schema, self.is_single)\n    self.model_config = model_config\n\n  def signature(self) -> Tuple[List, List]:\n    raise NotImplementedError()\n\n  def _convert_response_to_output(self, response) -> pd.DataFrame:\n    \"\"\"\n    Converts the model response to a pandas DataFrame.\n\n    Args:\n        response: The response from the model.\n\n    Returns:\n        pd.DataFrame: The parsed response as a DataFrame.\n\n    Raises:\n        ValueError: If the response content cannot be parsed.\n    \"\"\"\n    try:\n      json_content = response['choices'][0]['message']['content']\n      parsed_content = json.loads(json_content)\n    except (KeyError, json.JSONDecodeError) as e:\n      raise Exception(\"Failed to parse response: %s\", response)\n\n    if self.is_single:\n      df = pd.DataFrame([parsed_content])\n    else:\n      dfs = [pd.DataFrame(value) for value in parsed_content.values()]\n      df = pd.concat(dfs, axis=1)\n    return df\n\n  @call_counter\n  def infer_one(self, input: pd.Series) -> pd.DataFrame:\n    \"\"\"\n    Performs inference on a single input.\n\n    Args:\n        input (pd.Series): The input data as a pandas Series.\n\n    Returns:\n        pd.DataFrame: The inference results as a DataFrame.\n    \"\"\"\n    context = {key.replace('.', '_'): value for key, value in input.items()}\n    data = self.template.render(**context)\n\n    try:\n      response = completion(\n          model=self.model,\n          messages=[{\"content\": data, \"role\": \"user\"}],\n          response_format=self.output_schema,\n          **self.model_config\n      )\n    except Exception as e:\n      logging.error(\"Inference failed: %s\", e)\n      raise\n    inference_results = self._convert_response_to_output(response)\n    return inference_results\n"}
{"type": "source_file", "path": "aidb/inference/examples/__init__.py", "content": ""}
{"type": "source_file", "path": "aidb/inference/utils.py", "content": "import json\nimport logging\nfrom typing import Any, List, Type\n\nfrom pydantic import BaseModel, create_model\n\nJSON_SCHEMA = 'json'\nPYDANTIC_SCHEMA = 'pydantic'\n\n\ndef create_pydantic_model(model_name: str, fields: dict, is_single: bool) -> Type[BaseModel]:\n  \"\"\"\n    Creates a Pydantic model for structured output based on the provided fields.\n\n    Args:\n        model_name (str): The name of the model to be created.\n        fields (dict): A dictionary where keys are field names and values are field types as strings.\n        is_single (bool): A flag indicating if the model should be a single instance or a list.\n\n    Returns:\n        Type[BaseModel]: A Pydantic model class.\n    \"\"\"\n  type_mapping = {\n      'str': str,\n      'int': int,\n      'float': float,\n      'bool': bool,\n      # Add more types as needed\n  }\n  model_fields = {}\n\n  for name, type_str in fields.items():\n    python_type = type_mapping.get(type_str, Any)\n    if python_type is Any:\n      logging.warning(f\"Warning: Unsupported type '{type_str}' for field '{name}'. Defaulting to 'Any'.\")\n    model_fields[name] = (python_type, ...)  # Default value\n\n  model = create_model(\n      model_name,\n      **model_fields,\n  )\n  if not is_single:\n    model = create_model(\n        f'{model_name}_list',\n        items=(List[model], (...)),\n    )\n  return model\n\n\ndef parse_output_schema(service_name, output_schema_config: dict, is_single: bool):\n  \"\"\"\n    Parses the output schema configuration and returns the appropriate schema.\n\n    Args:\n        service_name (str): The name of the service for which the schema is being parsed.\n        output_schema_config (dict): A dictionary containing the schema configuration.\n        is_single (bool): A flag indicating if the schema should be for a single instance or a list.\n\n    Returns:\n        dict or Type[BaseModel]: The parsed output schema, either as a JSON schema dictionary or a Pydantic model.\n    \"\"\"\n  if JSON_SCHEMA in output_schema_config:\n    with open(output_schema_config[JSON_SCHEMA], 'r') as f:\n      output_schema = json.load(f)\n    output_schema = {\n        'type': 'json_schema',\n        'json_schema': output_schema,\n    }\n  elif PYDANTIC_SCHEMA in output_schema_config:\n    output_schema = create_pydantic_model(\n        service_name, output_schema_config[PYDANTIC_SCHEMA], is_single)\n  else:\n    raise Exception('Invalid output schema configuration')\n\n  return output_schema\n"}
{"type": "source_file", "path": "aidb/engine/approx_select_engine.py", "content": "import math\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy.sql import text\nfrom typing import List\n\nfrom aidb.engine.tasti_engine import TastiEngine\nfrom aidb.query.query import Query\nfrom aidb.utils.constants import MASS_COL_NAME, PROXY_SCORE_COL_NAME, SEED_PARAMETER, VECTOR_ID_COLUMN, WEIGHT_COL_NAME\nfrom aidb.utils.logger import logger\n\nBUDGET = 10000\n\n\nclass ApproxSelectEngine(TastiEngine):\n  async def get_inference_results(self, query:Query, sampled_index: List[int], conn):\n    # TODO: rewrite query, use full scan to execute query\n    bound_service_list = query.inference_engines_required_for_query\n    for bound_service in bound_service_list:\n      inp_query_str = self.get_input_query_for_inference_service_filtered_index(bound_service,\n                                                                                self.blob_mapping_table_name,\n                                                                                sampled_index)\n      inp_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(inp_query_str), conn))\n      inp_df.set_index(VECTOR_ID_COLUMN, inplace=True, drop=True)\n      await bound_service.infer(inp_df)\n\n\n    query_no_aqp = query.base_sql_no_aqp\n    query_after_normalize = query_no_aqp.query_after_normalizing\n    query_after_adding_vector_id_column = query_after_normalize.add_select(\n        f'{self.blob_mapping_table_name}.{VECTOR_ID_COLUMN}'\n    )\n\n    blob_keys = [col.name for col in self._config.tables[self.blob_mapping_table_name].columns]\n    added_cols = set()\n    join_conditions = []\n    tables_in_query = query_no_aqp.tables_in_query\n    for table_name in tables_in_query:\n      for col in self._config.tables[table_name].columns:\n        col_name = col.name\n        if col_name in blob_keys and col_name not in added_cols:\n          join_conditions.append(f'{self.blob_mapping_table_name}.{col_name} = {table_name}.{col_name}')\n          added_cols.add(col_name)\n    join_conditions_str = ' AND '.join(join_conditions)\n\n    query_after_adding_join = query_after_adding_vector_id_column.add_join(\n        f'JOIN {self.blob_mapping_table_name} ON {join_conditions_str}'\n    )\n    \n    table_columns = [f'{self.blob_mapping_table_name}.{VECTOR_ID_COLUMN}']\n    sampled_df = pd.DataFrame({VECTOR_ID_COLUMN: sampled_index})\n\n    async with self._sql_engine.begin() as conn:\n      sample_query_add_filter_key, _ = self.add_filter_key_into_query(\n          table_columns,\n          sampled_df,\n          query_after_adding_join\n      )\n      res_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(sample_query_add_filter_key.sql_str), conn))\n      # We need to add '__vector_id' in SELECT clause. When 'SELECT *', there will be two '__vector_id' columns.\n      # So we need to drop duplicated columns\n      res_df = res_df.loc[:, ~res_df.columns.duplicated()]\n      res_df.set_index(VECTOR_ID_COLUMN, inplace=True, drop=True)\n      \n    return res_df\n\n\n  def get_sampled_proxy_blob(self, proxy_score_for_all_blobs, defensive_mixing: int = 0.1):\n    weights = proxy_score_for_all_blobs.values ** 0.5\n    normalized_weights = (1 - defensive_mixing) * (weights / sum(weights)) + defensive_mixing / len(weights)\n    mass = 1 / len(weights) / normalized_weights\n\n    dataset = pd.DataFrame({\n        PROXY_SCORE_COL_NAME: proxy_score_for_all_blobs.values,\n        WEIGHT_COL_NAME: normalized_weights,\n        MASS_COL_NAME: mass},\n        index=proxy_score_for_all_blobs.index\n    )\n\n    return dataset\n\n\n  def tau_estimate(self, recall_target, satisfied_sampled_results):\n    estimated_tau = 0.0\n    recall_score = 0\n\n    for index, blob in satisfied_sampled_results.iterrows():\n      recall_score += blob[MASS_COL_NAME]\n      if recall_score >= recall_target:\n        estimated_tau = blob[PROXY_SCORE_COL_NAME]\n        break\n    return estimated_tau\n\n\n  def _get_confidence_bounds(self, mu, sigma, s, delta):\n    if s == 0:\n      return 0.0, 0.0\n    val = (sigma / math.sqrt(s)) * math.sqrt(2 * math.log(1 / delta))\n    return mu - val, mu + val\n\n\n  def tau_modified(self, satisfied_sampled_results, recall_target, confidence, total_length):\n    z = satisfied_sampled_results[MASS_COL_NAME]\n    estimated_tau = self.tau_estimate(\n      recall_target * sum(z),\n      satisfied_sampled_results\n    )\n    grouped = satisfied_sampled_results.groupby(level=0)\n    aggregated = grouped.agg(sum_mass=(MASS_COL_NAME, 'sum'), __proxy_score=(PROXY_SCORE_COL_NAME, 'mean'))\n    samples_above_cutoff = aggregated[aggregated[PROXY_SCORE_COL_NAME] >= estimated_tau]\n    samples_below_cutoff = aggregated[aggregated[PROXY_SCORE_COL_NAME] < estimated_tau]\n\n    estimated_z1 = list(samples_above_cutoff['sum_mass']) + [0] * (total_length - len(samples_above_cutoff))\n    estimated_z2 = list(samples_below_cutoff['sum_mass']) + [0] * (total_length - len(samples_below_cutoff))\n    \n    z1_mean, z1_std = np.mean(estimated_z1), np.std(estimated_z1)\n    z2_mean, z2_std = np.mean(estimated_z2), np.std(estimated_z2)\n\n    delta = 1.0 - confidence\n    _, ub = self._get_confidence_bounds(z1_mean, z1_std, len(estimated_z1), delta / 2)\n    lb, _ = self._get_confidence_bounds(z2_mean, z2_std, len(estimated_z2), delta / 2)\n\n    # inflate recall to correct the sampling errors\n    if (ub+lb) == 0.0:\n      modified_recall_target = 1.0\n    else:\n      modified_recall_target = ub / (ub + lb)\n\n    modified_tau = self.tau_estimate(\n      modified_recall_target * sum(z),\n      satisfied_sampled_results\n    )\n    logger.info(f'modified_recall: {modified_recall_target}')\n    logger.info(f'modified_tau: {modified_tau}')\n    return modified_tau\n\n\n  async def execute_approx_select_query(self, query: Query, **kwargs):\n    if not query.is_valid_approx_select_query:\n      raise Exception('Approx select query should contain Confidence and should not contain Budget.')\n\n    # generate proxy score for each blob\n    proxy_score_for_all_blobs = await self.get_proxy_scores_for_all_blobs(query)\n\n    dataset = self.get_sampled_proxy_blob(proxy_score_for_all_blobs)\n\n    if SEED_PARAMETER in kwargs:\n      seed = kwargs[SEED_PARAMETER]\n    else:\n      seed = None\n    sampled_df = dataset.sample(BUDGET, replace=True, weights=WEIGHT_COL_NAME, random_state=seed)\n\n    async with self._sql_engine.begin() as conn:\n      satisfied_sampled_results = await self.get_inference_results(\n          query,\n          list(sampled_df.index),\n          conn\n      )\n\n      joined_satisfied_sampled_results = satisfied_sampled_results.join(sampled_df, how='inner')\n      sorted_satisfied_sampled_results = joined_satisfied_sampled_results.sort_values(by=PROXY_SCORE_COL_NAME, ascending=False)\n\n      recall_target = query.recall_target\n      confidence = query.confidence / 100\n      tau_modified = self.tau_modified(\n          sorted_satisfied_sampled_results,\n          recall_target,\n          confidence,\n          BUDGET\n      )\n\n      pilot_sample_index = sorted_satisfied_sampled_results.index\n      additional_sample_index = dataset[dataset[PROXY_SCORE_COL_NAME] >= tau_modified].index\n\n      additional_selected_blob = list(set(additional_sample_index) - set(pilot_sample_index))\n      logger.info(f'num_samples: {len(additional_selected_blob)}')\n\n      new_satisfied_sampled_results = await self.get_inference_results(\n          query,\n          additional_selected_blob,\n          conn\n      )\n      all_satisfied_sampled_results = pd.concat([satisfied_sampled_results, new_satisfied_sampled_results], ignore_index=True)\n\n    return all_satisfied_sampled_results\n"}
{"type": "source_file", "path": "aidb/config/__init__.py", "content": ""}
{"type": "source_file", "path": "aidb/inference/cached_inference_service.py", "content": "from aidb.inference.inference_service import InferenceService\n\n\n# TODO: implement\nclass CachedInferenceService(InferenceService):\n  pass"}
{"type": "source_file", "path": "aidb/inference/http_inference_service.py", "content": "from flatten_json import unflatten_list\nfrom typing import Dict, List, Tuple, Union\n\nimport pandas as pd\nimport requests\nimport time\n\nfrom aidb.config.config_types import AIDBListType\nfrom aidb.inference.cached_inference_service import CachedInferenceService\nfrom aidb.utils.perf_utils import call_counter\n\n\ndef convert_response_to_output(\n    response: Union[Dict, List],\n    _response_keys_to_columns: Dict[Union[str, tuple], int]) -> Dict:\n  output = {v: [] for v in _response_keys_to_columns.values()}\n  for k, v in _response_keys_to_columns.items():\n    if not isinstance(k, tuple):\n      k = (k,)\n    response_copy = response.copy()\n    for idx, key in enumerate(k):\n      if isinstance(key, AIDBListType) and isinstance(response_copy, list):\n        if idx != len(k) - 1:\n          new_response_copy = []\n          for r in response_copy:\n            new_key = k[(idx+1):]\n            current_response = convert_response_to_output(r, {new_key: new_key})\n            if current_response is not None and new_key in current_response:\n              if isinstance(current_response[new_key], list):\n                new_response_copy.extend(current_response[new_key])\n              else:\n                new_response_copy.append(current_response[new_key])\n          response_copy = new_response_copy if len(new_response_copy) > 0 else None\n          break\n      elif (isinstance(key, int) and \\\n           (isinstance(response_copy, list) and key < len(response_copy)) or \\\n           (isinstance(response_copy, dict) and key in response_copy)) or \\\n           (isinstance(key, str) and isinstance(response_copy, dict) and key in response_copy):\n        response_copy = response_copy[key]\n      else:\n        response_copy = None\n        break\n\n    if response_copy is not None:\n      output[v] = response_copy\n  return output\n\n\nclass HTTPInferenceService(CachedInferenceService):\n  def __init__(\n      self,\n      *args,\n      url: str=None,\n      headers: Union[Dict, None]=None,\n      default_args: Union[Dict, None]=None,\n      batch_supported: bool=False,\n      columns_to_input_keys: List[Union[str, tuple]]=None,\n      input_columns_types: Union[List, None]=None,\n      response_keys_to_columns: List[Union[str, tuple]]=None,\n      output_columns_types: Union[List, None]=None,\n      rate_limit: Union[int, None]=None,\n      **kwargs\n  ):\n    '''\n    :param str url: The URL to send the request to. The request will be a POST request.\n    :param dict headers: The headers to send with the request.\n    :param bool batch_supported: Whether the server supports batch requests.\n    '''\n    super().__init__(*args, **kwargs)\n    self._url = url\n    self._headers = headers\n    self._default_args = default_args\n    self._batch_supported = batch_supported\n    self._columns_to_input_keys = columns_to_input_keys\n    self._response_keys_to_columns = response_keys_to_columns\n\n    assert not input_columns_types or len(input_columns_types) == len(columns_to_input_keys), \\\n      f'input_columns_types must be None or have same length as columns_to_input_keys'\n    self._input_columns_types = input_columns_types\n    assert not output_columns_types or len(output_columns_types) == len(response_keys_to_columns), \\\n      f'output_columns_types must be None or have same length as response_keys_to_columns'\n    self._output_columns_types = output_columns_types\n\n    self._separator = '~'\n    self._rate_limit = rate_limit\n\n\n  def signature(self) -> Tuple[List, List]:\n    raise NotImplementedError()\n\n\n  def convert_input_to_request(self, input: Union[pd.Series, pd.DataFrame]) -> Dict:\n    request = {}\n    remove_ghost_key = False\n    if isinstance(input, pd.Series):\n      num_rows = 1\n      dict_input = {k: [v] for k, v in input.to_dict().items()}\n    else: # isinstance(input, pd.DataFrame)\n      num_rows = len(input)\n      dict_input = input.to_dict(orient='list')\n\n    dict_input_keys = list(dict_input.keys())\n\n    if self._input_columns_types is not None:\n      for k, _type in zip(dict_input_keys, self._input_columns_types):\n        if len(dict_input[k]) > 0:\n          assert isinstance(dict_input[k][0], _type), f'Input column {k} must be of type {_type}'\n\n    columns_to_input_keys = self._columns_to_input_keys.copy()\n    if self._default_args is not None:\n      idx = len(columns_to_input_keys)\n      for k, v in self._default_args.items():\n        if k not in dict_input_keys:\n          dict_input_keys.insert(idx, k)\n          dict_input[k] = [v] * num_rows\n          columns_to_input_keys.append(k)\n          idx += 1\n\n    # to support arbitrary batch size\n    # assume all numerical index form lists\n    for k, v in enumerate(columns_to_input_keys):\n      if k > len(dict_input_keys):\n        continue\n      k = dict_input_keys[k]\n      if isinstance(v, tuple):\n        aidb_list_count = sum(1 for e in v if isinstance(e, AIDBListType))\n        if aidb_list_count == 0:\n          key = tuple(str(_k) for _k in v)\n          key = self._separator.join(key)\n          request[key] = dict_input[k][0]\n        elif aidb_list_count == 1:\n          for i in range(num_rows):\n            key = tuple(f'{i}' if isinstance(_k, AIDBListType) else f'{_k}' for _k in v)\n            if isinstance(v[0], AIDBListType):\n              key = ('_', ) + key # all converted keys should start with AIDBListType\n              remove_ghost_key = True\n            key = self._separator.join(key)\n            request[key] = dict_input[k][i]\n        else:\n          raise ValueError(f'Cannot have more than 1 AIDBListType in columns_to_input_keys')\n      else: # isinstance(v, str)\n        request[v] = dict_input[k][0]\n    request = unflatten_list(request, self._separator)\n    if remove_ghost_key:\n      request = request['_']\n    return request\n\n\n  def request(self, request: Dict) -> Dict:\n    time_before_request = time.time()\n    response = requests.post(self._url, json=request, headers=self._headers)\n    response.raise_for_status()\n    if self._rate_limit is not None:\n      sleep_time = 60 / self._rate_limit - (time.time() - time_before_request)\n      if sleep_time > 0:\n        time.sleep(sleep_time)\n    return response.json()\n\n\n  def convert_response_to_output(self, response: Dict) -> pd.DataFrame:\n    self._response_keys_to_columns = {k: i for i, k in enumerate(self._response_keys_to_columns)}\n    output = convert_response_to_output(response, self._response_keys_to_columns)\n    if not any(isinstance(value, list) for value in output.values()):\n      output = {k: [v] for k, v in output.items()}\n\n    if self._output_columns_types is not None:\n      for k, _type in zip(output.keys(), self._output_columns_types):\n        if len(output[k]) > 0:\n          assert isinstance(output[k][0], _type), f'Output column {k} must be of type {_type}'\n\n    return output\n\n\n  @call_counter\n  def infer_one(self, input: pd.Series) -> pd.DataFrame:\n    request = self.convert_input_to_request(input)\n    response = self.request(request)\n    output = self.convert_response_to_output(response[0])\n\n    return pd.DataFrame(output)\n\n\n  def infer_batch(self, inputs: pd.DataFrame):\n    if not self._batch_supported:\n      return super().infer_batch(inputs)\n\n    body = inputs.to_dict(orient='list')\n    response = requests.post(self._url, json=body, headers=self._headers)\n    response.raise_for_status()\n    response = response.json()\n\n    # for join query, directly convert the result into a dataframe and return it\n    if self._url.endswith('__join'):\n      return [pd.DataFrame(response)]\n    else:\n      # We assume the server returns a list of responses\n      # We assume the length of the list of responses should match that of the inputs\n      # Each element in response list must correspond to the input with the same index\n      # and an element can represent 0 / 1 / multiple outputs\n      if len(response) != len(inputs):\n        raise Exception('The length of the inference results should match that of the inputs.')\n      response_df_list = [pd.DataFrame(item) for item in response]\n      return response_df_list\n"}
{"type": "source_file", "path": "aidb/launch.py", "content": "import argparse\nfrom typing import Dict, List\n\nimport pandas as pd\nimport yaml\n\nfrom aidb.utils.asyncio import asyncio_run\nfrom aidb_utilities.aidb_setup.aidb_factory import AIDB\nfrom aidb_utilities.command_line_setup.command_line_setup import \\\n    command_line_utility\nfrom aidb_utilities.db_setup.blob_table import BaseTablesSetup\nfrom aidb_utilities.db_setup.create_tables import create_output_tables\n\n\ndef setup_blob_tables(db_config: Dict[str, str], blob_tables: List[dict]):\n  base_table_setup = BaseTablesSetup(f\"{db_config['url']}/{db_config['name']}\")\n  for blob_table in blob_tables:\n    input_blobs = pd.read_csv(blob_table['path'])\n    base_table_setup.insert_blob_meta_data(\n      blob_table['name'], input_blobs, blob_table['keys'])\n\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\"--config\", type=str)\n  parser.add_argument(\"--setup-blob-table\", action='store_true')\n  parser.add_argument(\"--setup-output-tables\", action='store_true')\n  parser.add_argument(\"--verbose\", action='store_true')\n\n  args = parser.parse_args()\n  with open(args.config, \"r\") as file:  # Replace with the actual path to your YAML file\n    config = yaml.safe_load(file)\n\n  if args.setup_blob_table:\n    setup_blob_tables(config['db_config'], config['blob_tables'])\n\n  if args.setup_output_tables:\n    asyncio_run(create_output_tables(\n      config['db_config'], config['output_tables']))\n\n  aidb_engine = AIDB.from_config(config, args.verbose)\n  command_line_utility(aidb_engine)\n"}
{"type": "source_file", "path": "aidb/inference/inference_service.py", "content": "import abc\nfrom dataclasses import dataclass, field\nfrom typing import List, Tuple, Union\n\nimport pandas as pd\n\n\n@dataclass\nclass InferenceService(abc.ABC):\n  '''\n  The inference service is a wrapper around calling external ML models. For the\n  inference service, we enforce that the input is a single pandas row (Series).\n  If inputs span multiple tables, the caller will join the data into the Series.\n  The output is a single pandas DataFrame with zero or more rows.\n\n  Even though the input is a series, the inference service type signature is a\n  _list_ of arguments. The series column order is assumed to be the same as the\n  list order.\n\n  Because the input is assumed to be a list, THE COLUMN ORDER MATTERS.\n\n  The batch inference also takes in a single pandas DataFrame, but with multiple\n  rows. The output is a list of DataFrames.\n\n  '''\n  name: str\n  is_single: bool  # Return a single value or a list of values\n  cost: Union[float, None] = None\n  preferred_batch_size: int = 1\n  copied_input_columns: List[int] = field(default_factory=list)\n\n\n  @abc.abstractproperty\n  def signature(self) -> Tuple[List, List]:\n    pass\n\n\n  @abc.abstractmethod\n  def infer_one(self, input: pd.Series) -> pd.DataFrame:\n    pass\n\n\n  def infer_batch(self, inputs: pd.DataFrame) -> List[pd.DataFrame]:\n    return [self.infer_one(row) for _, row in inputs.iterrows()]\n\n\n  async def infer_one_async(self, input: pd.Series) -> pd.DataFrame:\n    return self.infer_one(input)\n\n\n  async def infer_batch_async(self, inputs: pd.DataFrame) -> List[pd.DataFrame]:\n    return self.infer_batch(inputs)"}
{"type": "source_file", "path": "aidb/inference/examples/detectron_local_ocr.py", "content": "\"\"\"\nInput: document segmentation model weights (model_final.pth), csv with paths to pdfs.\n\nOutput: csv with OCR'd text.\n\"\"\"\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nimport numpy as np\nimport pandas as pd\nimport pdf2image\nimport pytesseract\nfrom typing import List\n\nfrom aidb.inference.cached_inference_service import CachedInferenceService\n\n\nclass DetectronLocalOCR(CachedInferenceService):\n  def __init__(\n      self,\n      name: str,\n      model_path: str,\n      device: str=\"cuda\",\n      is_single: bool=False,\n  ):\n    super().__init__(name=name, preferred_batch_size=1, is_single=is_single)\n    cfg = get_cfg()\n    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n    cfg.MODEL.DEVICE = device\n    cfg.DATALOADER.NUM_WORKERS = 1\n    cfg.SOLVER.IMS_PER_BATCH = 2\n    cfg.SOLVER.MAX_ITER = 300\n    cfg.SOLVER.STEPS = []\n    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n    cfg.MODEL.WEIGHTS = model_path\n    cfg.TEST.DETECTIONS_PER_IMAGE = 1\n    self.od_model = DefaultPredictor(cfg)\n\n\n  def signature(self):\n    raise NotImplementedError()\n\n\n  def infer_one(self, input: pd.Series) -> pd.DataFrame:\n    pdf_path = list(input.to_dict().values())[0]\n    im = np.array(pdf2image.convert_from_path(pdf_path, size=(2538, 3334))[0])\n    outputs = self.od_model(im)\n    boxes = list(outputs['instances'].pred_boxes)\n    ocr_text = ''\n    if len(boxes) > 0:\n      box = boxes[0].detach().cpu().numpy()\n      x0, y0, x1, y1 = box.astype(int)\n      margin_x0 = max(0, x0-30)\n      margin_x1 = min(im.shape[1], x1+30)\n      margin_y0 = max(0, y0-30)\n      margin_y1 = min(im.shape[0], y1+30)\n      cropped_im = im[margin_y0:margin_y1, margin_x0:margin_x1]\n      ocr_text = pytesseract.image_to_string(cropped_im)\n    output = {\"ocr_text\": [ocr_text]}\n    return pd.DataFrame(output)\n\n\n  def infer_batch(self, inputs: pd.DataFrame) -> pd.DataFrame:\n    return super().infer_batch(inputs)\n"}
{"type": "source_file", "path": "aidb/query/__init__.py", "content": ""}
{"type": "source_file", "path": "aidb/engine/full_scan_engine.py", "content": "from collections import defaultdict\nfrom typing import List\n\nimport pandas as pd\nfrom aidb.engine.tasti_engine import TastiEngine\nfrom aidb.query.query import Query\nfrom aidb.utils.logger import logger\nfrom aidb.utils.order_optimization_utils import (\n    get_currently_supported_filtering_predicates_for_ordering,\n    reorder_inference_engine)\nfrom sqlalchemy.sql import text\n\nRETRIEVAL_BATCH_SIZE = 10000\n\n\n          \nclass FullScanEngine(TastiEngine):\n  async def execute_full_scan(self, query: Query, **kwargs):\n    '''\n    Executes a query by doing a full scan and returns the results.\n    '''\n    # The query is irrelevant since we do a full scan anyway\n  \n    is_udf_query = query.is_udf_query\n    if is_udf_query:\n      query.check_udf_query_validity()\n      dataframe_sql, query = query.udf_query\n\n    bound_service_list = query.inference_engines_required_for_query\n    if self.tasti_index:\n      supported_filtering_predicates = get_currently_supported_filtering_predicates_for_ordering(self._config, query)\n      engine_to_proxy_score = {}\n      bound_service_list = query.inference_engines_required_for_query\n      for engine, related_predicates in supported_filtering_predicates.items():\n        where_str = self._get_where_str(related_predicates)\n        select_join_str = f'select * from {engine.service.name} '\n        if len(related_predicates) > 0:\n          adjusted_query = select_join_str + f'WHERE {where_str};'\n        else:\n          adjusted_query = select_join_str + ';'\n\n        adjusted_query = Query(adjusted_query, self._config)\n        proxy_score_for_all_blobs = await self.get_proxy_scores_for_all_blobs(adjusted_query, return_binary_score=True)\n        engine_to_proxy_score[engine] = proxy_score_for_all_blobs.sum() / len(proxy_score_for_all_blobs)\n      logger.info(f'Proxy scores for all engines: {engine_to_proxy_score}')\n      bound_service_list = reorder_inference_engine(engine_to_proxy_score, bound_service_list)\n      logger.info(f'The order of inference engines: {bound_service_list}')\n      \n    inference_services_executed = set()\n    for bound_service in bound_service_list:\n      inp_query_str = self.get_input_query_for_inference_service_filter_service(bound_service,\n                                                                                query,\n                                                                                inference_services_executed)\n\n      async with self._sql_engine.begin() as conn:\n        inp_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(inp_query_str), conn))\n\n      # The bound inference service is responsible for populating the database\n      await bound_service.infer(inp_df)\n\n      inference_services_executed.add(bound_service.service.name)\n\n    async with self._sql_engine.begin() as conn:\n      if is_udf_query:\n        offset = 0\n        res = []\n        # selectively load data from database to avoid large memory usage\n        while True:\n          query = query.add_limit_keyword(RETRIEVAL_BATCH_SIZE)\n          query = query.add_offset_keyword(offset)\n          res_df = await conn.run_sync(lambda conn: pd.read_sql_query(text(query.sql_query_text), conn))\n          row_length = len(res_df)\n          res_satisfy_udf = self.execute_user_defined_function(res_df, dataframe_sql, query)\n          res.extend([tuple(row) for row in res_satisfy_udf.itertuples(index=False)])\n          if row_length != RETRIEVAL_BATCH_SIZE:\n            return res\n          offset += RETRIEVAL_BATCH_SIZE\n\n      else:\n        res = await conn.execute(text(query.sql_query_text))\n        return res.fetchall()"}
{"type": "source_file", "path": "aidb/vector_database/tasti.py", "content": "import numpy as np\nimport pandas as pd\nfrom numba import njit, prange\nfrom typing import Optional\n\nfrom aidb.vector_database.vector_database_config import TastiConfig\nfrom aidb.utils.constants import VECTOR_ID_COLUMN\n\n\n@njit(parallel=True)\ndef get_and_update_dists(x: np.ndarray, embeddings: np.ndarray, min_dists: np.ndarray):\n  '''\n  :param x: embedding of cluster representatives\n  :param embeddings: embeddings of all data\n  :min_dists: array to record the minimum distance for each embedding to the embedding of cluster representatives\n  '''\n  for i in prange(len(embeddings)):\n    dists = np.sqrt(np.sum((x - embeddings[i]) ** 2))\n    if dists < min_dists[i]:\n      min_dists[i] = dists\n\n\nclass Tasti(TastiConfig):\n  def __post_init__(self):\n    self.rep_index_name = f'{self.index_name}__representatives'\n    self.vector_ids = None\n    self.reps = None\n    self.rand = np.random.RandomState(self.seed)\n\n\n  def set_vector_ids(self, vector_ids: pd.DataFrame):\n    self.vector_ids = vector_ids\n    self.embeddings = self.vector_database.get_embeddings_by_id(\n      self.index_name,\n      self.vector_ids.values.reshape(1, -1)[0],\n      reload=True\n    )\n\n\n  def set_existing_reps(self, reps: np.ndarray):\n    self.reps = reps\n\n\n  # # TODO: Add memory efficient FPF Random Bucketter\n  def _FPF(self, nb_buckets: Optional[int] = None):\n    '''\n    FPF mining algorithm and return cluster representative ids, old cluster representatives will always be kept\n    '''\n    if nb_buckets is not None:\n      buckets = nb_buckets\n    else:\n      buckets = self.nb_buckets\n\n    if self.vector_ids is None:\n      raise Exception('Vector_ids is None, please set it first.')\n\n    reps = np.full(buckets, -1)\n    min_dists = np.full(len(self.embeddings), np.Inf, dtype=np.float32)\n    num_random = int((1 - self.percent_fpf) * len(reps))\n    random_reps = self.rand.choice(len(self.embeddings), num_random, replace=False)\n\n    reps[0] = random_reps[0]\n    get_and_update_dists(self.embeddings[reps[0]], self.embeddings, min_dists)\n\n    for i in range(1, num_random):\n      reps[i] = random_reps[i]\n      get_and_update_dists(self.embeddings[reps[i]], self.embeddings, min_dists)\n\n    for i in range(num_random, buckets):\n      reps[i] = np.argmax(min_dists)\n      get_and_update_dists(self.embeddings[reps[i]], self.embeddings, min_dists)\n\n    if self.reps is not None:\n      self.reps = np.unique(np.concatenate((self.reps, reps)))\n    else:\n      self.reps = np.unique(reps)\n\n\n  def get_representative_vector_ids(self) -> pd.DataFrame:\n    '''\n    get cluster representatives blob ids\n    '''\n    if self.reps is None:\n      self._FPF()\n    rep_id = self.vector_ids.iloc[self.reps]\n    rep_id.set_index(VECTOR_ID_COLUMN, inplace=True, drop=True)\n    return rep_id\n\n\n  def get_topk_representatives_for_all(self, top_k: int = 5) -> pd.DataFrame:\n    '''\n    get topk representatives and distances for all blob index\n    '''\n    if self.reps is None:\n      self._FPF()\n    topk_reps, topk_dists = self.vector_database.execute(self.rep_index_name, self.embeddings, self.reps, top_k)\n    topk_reps = self.vector_ids.iloc[np.concatenate(topk_reps)].values.reshape(-1, top_k)\n    data = {'topk_reps': list(topk_reps), 'topk_dists': list(topk_dists)}\n    return pd.DataFrame(data, index=self.vector_ids.squeeze())\n\n\n  def get_topk_representatives_for_new_embeddings(\n      self,\n      new_vector_ids: pd.DataFrame,\n      top_k: int = 5\n  ) -> pd.DataFrame:\n    '''\n    get topk representatives and distances for new embeddings using stale cluster representatives,\n    in other words, we don't need to use FPF to reselect cluster representatives\n    '''\n    new_embeddings = self.vector_database.get_embeddings_by_id(\n        self.index_name,\n        new_vector_ids.values.reshape(1, -1)[0],\n        reload=True\n    )\n    topk_reps, topk_dists = self.vector_database.query_by_embedding(self.rep_index_name, new_embeddings, top_k)\n    topk_reps = self.vector_ids.iloc[np.concatenate(topk_reps)].values.reshape(-1, top_k)\n    data = {'topk_reps': list(topk_reps), 'topk_dists': list(topk_dists)}\n    return pd.DataFrame(data, index=new_vector_ids.squeeze())\n\n\n  def update_topk_representatives_for_all(\n      self,\n      new_vector_ids: pd.DataFrame,\n      top_k: int = 5,\n      nb_buckets: Optional[int] = None\n  ) -> pd.DataFrame:\n    '''\n    when new embeddings are added, we update cluster representative ids and dists for all blob index\n    '''\n    #TODO: do we need to check if there is override bewteen vector_ids and new_vector_ids?\n    self.vector_ids = pd.concat([self.vector_ids, new_vector_ids])\n    new_embeddings = self.vector_database.get_embeddings_by_id(\n        self.index_name,\n        new_vector_ids.values.reshape(1, -1)[0],\n        reload=True\n    )\n    self.embeddings = np.concatenate((self.embeddings, new_embeddings), axis=0)\n\n    self._FPF(nb_buckets)\n    return self.get_topk_representatives_for_all(top_k)"}
{"type": "source_file", "path": "aidb/query/query.py", "content": "from collections import defaultdict\nfrom dataclasses import dataclass\nfrom functools import cached_property\nfrom typing import Dict, List\n\nimport sqlglot.expressions as exp\nfrom sqlglot import Parser, Tokenizer, parse_one\nfrom sympy import sympify\nfrom sympy.logic.boolalg import to_cnf\n\nfrom aidb.config.config import Config\n\n\ndef is_aqp_exp(node):\n  return isinstance(node, exp.ErrorTarget) \\\n      or isinstance(node, exp.RecallTarget) \\\n      or isinstance(node, exp.PrecisionTarget) \\\n      or isinstance(node, exp.Confidence) \\\n      or isinstance(node, exp.Budget)\n\n\ndef _remove_aqp(node):\n  if is_aqp_exp(node):\n    return None\n  return node\n\n\ndef _remove_where(node):\n  if isinstance(node, exp.Where):\n    return None\n  return node\n\n\n@dataclass(frozen=True)\nclass Query(object):\n  \"\"\"\n  class to hold sql query related data\n  this is immutable class\n  \"\"\"\n  sql_str: str\n  config: Config\n\n\n  @cached_property\n  def _tokens(self):\n    return Tokenizer().tokenize(self.sql_str)\n\n\n  @cached_property\n  def _expression(self) -> exp.Expression:\n    return Parser().parse(self._tokens)[0]\n\n\n  def get_expression(self):\n    return self._expression\n\n\n  @cached_property\n  def sql_query_text(self):\n    return self._expression.sql()\n\n\n  @cached_property\n  def _tables(self) -> Dict[str, Dict[str, str]]:\n    \"\"\"\n    dictionary of tables to columns and their types\n    \"\"\"\n    _tables = dict()\n    for table_name, table in self.config.tables.items():\n      _tables[table_name] = dict()\n      for column in table.columns:\n        _tables[table_name][column.name] = column.type\n    return _tables\n\n\n  @cached_property\n  def all_queries_in_expressions(self):\n    '''\n    This function is used to extract all queries in the expression, including the entire query and subqueries\n    '''\n    all_queries = []\n    for node in self._expression.walk():\n      if isinstance(node, exp.Select):\n        depth = 0\n        parent = node.parent\n        while parent:\n          depth += 1\n          parent = parent.parent\n\n        extracted_query = Query(node.sql(), self.config)\n        if depth != 0 :\n          if extracted_query.is_approx_agg_query or extracted_query.is_approx_select_query:\n            raise Exception(\"AIDB does not support using approx query as a subquery\")\n\n        all_queries.append((Query(node.sql(), self.config), depth))\n\n    all_queries = sorted(all_queries, key=lambda x:x[1], reverse=True)\n    return all_queries\n\n\n  def _check_in_subquery(self, node):\n    '''\n    this function check if current node is within a subquery\n    '''\n    node_parent = node\n    while node_parent and not isinstance(node_parent, exp.Select):\n      node_parent = node_parent.parent\n    if node_parent is None or node_parent.parent is None:\n      return False\n    else:\n      return True\n\n\n  @cached_property\n  def table_and_column_aliases_in_query(self):\n    \"\"\"\n    finds the mapping of alias and original name presenting in the query, excluding subquery part.\n\n    for e.g. for the query:\n    SELECT t2.id AS col1, t2.frame AS col2, t2.column5\n    FROM table_2 t2 JOIN blob_ids b\n    ON t2.frame = b.frame\n    WHERE b.frame > 102 and column1 > 950\n\n    table_alias_to_name will be {b: blob_ids, t2: table_2}\n    column_alias_to_name will be {col1: id, col2: frame}\n\n    :return: mapping of table names and alias\n    \"\"\"\n    table_alias_to_name = {}\n    column_alias_to_name = {}\n    for node in  self._expression.walk():\n      if isinstance(node, exp.Expression) and self._check_in_subquery(node):\n        continue\n      if isinstance(node, exp.Table) and node.args.get('alias'):\n        table_alias = node.alias\n        table_name = node.name\n        table_alias_to_name[str.lower(table_alias)] = str.lower(table_name)\n      if isinstance(node, exp.Alias) and isinstance(node.args['this'], exp.Column):\n        col_alias = node.alias\n        if str.lower(col_alias) in column_alias_to_name:\n          raise Exception('Duplicated alias found in query, please use another alias')\n        col_name = node.args['this'].args['this'].args['this']\n        column_alias_to_name[str.lower(col_alias)] = str.lower(col_name)\n\n    return table_alias_to_name, column_alias_to_name\n\n\n  @cached_property\n  def udf_outputs_aliases(self):\n    \"\"\"\n    Find alias of user defined function in query\n    \"\"\"\n    udf_output_to_alias_mapping = {}\n    # Dictionary of aliases for user-defined functions defined in AIDB\n    alias_to_udf_mapping = {}\n    alias_index = 0\n    for node in self._expression.walk(bfs=False):\n      if isinstance(node, exp.Expression) and self._check_in_subquery(node):\n        continue\n      if isinstance(node, exp.Alias) and 'alias' in node.args and 'this' in node.args:\n        if (isinstance(node.args['this'], exp.Anonymous) and node.args['alias'].args['this'] is not None\n            and node.args['this'].args['this'] in self.config.user_defined_functions):\n          udf_output_alias_key = f\"{node.args['this'].args['this']}__{alias_index}\"\n          alias_index += 1\n          udf_output_to_alias_mapping[udf_output_alias_key] = []\n          alias = str.lower(node.args['alias'].args['this'])\n          udf_output_to_alias_mapping[udf_output_alias_key].append(alias)\n          if alias not in alias_to_udf_mapping:\n            alias_to_udf_mapping[alias] = node.args['this']\n          else:\n            raise Exception('Duplicated alias is not allowed, please use another alias')\n      elif isinstance(node, exp.Aliases) and 'expressions' in node.args and 'this' in node.args:\n        if (isinstance(node.args['this'], exp.Anonymous) and len(node.args['expressions']) != 0\n            and node.args['this'].args['this'] in self.config.user_defined_functions):\n          udf_output_alias_key = f\"{node.args['this'].args['this']}__{alias_index}\"\n          alias_index += 1\n          udf_output_to_alias_mapping[udf_output_alias_key] = []\n          for expression in node.args['expressions']:\n            alias = str.lower(expression.args['this'])\n            udf_output_to_alias_mapping[udf_output_alias_key].append(alias)\n            if alias not in alias_to_udf_mapping:\n              alias_to_udf_mapping[alias] = node.args['this']\n            else:\n              raise Exception('Duplicated alias is not allowed, please use another alias')\n\n    return udf_output_to_alias_mapping, alias_to_udf_mapping\n\n\n  @cached_property\n  def columns_in_query(self):\n    \"\"\"\n    return the normalized column names in query, excluding subquery part\n    \"\"\"\n    column_set = self._get_normalized_column_set(self.query_after_normalizing.get_expression())\n    return column_set\n\n\n  @cached_property\n  def tables_in_query(self):\n    table_set = set()\n    for node in self._expression.walk():\n      if isinstance(node, exp.Expression) and self._check_in_subquery(node):\n        continue\n      if isinstance(node, exp.Table):\n        table_set.add(node.args['this'].args['this'])\n\n    return list(table_set)\n\n\n  def _get_predicate_name(self, predicate_count):\n    predicate_name = f\"P{predicate_count}\"\n    return predicate_name\n\n\n  def _get_sympify_form(self, node, predicate_count, predicate_mappings):\n    if node is None:\n      return None, predicate_count\n    elif isinstance(node, exp.Paren) or isinstance(node, exp.Where):\n      assert \"this\" in node.args\n      expression, predicate_count = self._get_sympify_form(node.args[\"this\"], predicate_count, predicate_mappings)\n      return f'({expression})', predicate_count\n    elif isinstance(node, exp.And):\n      assert \"this\" in node.args and \"expression\" in node.args\n      e1, p1 = self._get_sympify_form(node.args['this'], predicate_count, predicate_mappings)\n      e2, p2 = self._get_sympify_form(node.args['expression'], p1, predicate_mappings)\n      return f\"{e1} & {e2}\", p2\n    elif isinstance(node, exp.Or):\n      assert \"this\" in node.args and \"expression\" in node.args\n      e1, p1 = self._get_sympify_form(node.args['this'], predicate_count, predicate_mappings)\n      e2, p2 = self._get_sympify_form(node.args['expression'], p1, predicate_mappings)\n      return f\"{e1} | {e2}\", p2\n    elif isinstance(node, exp.Not):\n      expression, predicate_count = self._get_sympify_form(node.args['this'], predicate_count, predicate_mappings)\n      return f'~{expression}', predicate_count\n    elif isinstance(node, exp.GT) or \\\n            isinstance(node, exp.LT) or \\\n            isinstance(node, exp.GTE) or \\\n            isinstance(node, exp.LTE) or \\\n            isinstance(node, exp.EQ) or \\\n            isinstance(node, exp.Like) or \\\n            isinstance(node, exp.NEQ) or \\\n            isinstance(node, exp.In) or \\\n            isinstance(node, exp.Column):\n      predicate_name = self._get_predicate_name(predicate_count)\n      predicate_mappings[predicate_name] = node\n      return predicate_name, predicate_count + 1\n    else:\n      raise NotImplementedError\n\n\n  def _get_or_clause_representation(self, or_expression, predicate_mappings):\n    connected_by_ors = list(or_expression.args)\n    filtering_predicate_list = []\n    if len(connected_by_ors) <= 1:\n      filtering_predicate_list.append(or_expression)\n    else:\n      for fp in connected_by_ors:\n        filtering_predicate_list.append(fp)\n    predicates_in_ors = []\n    for fp in filtering_predicate_list:\n      if str(fp)[0] == '~':\n        predicates_in_ors.append(exp.Not(this=predicate_mappings[str(fp)[1:]]))\n      else:\n        predicates_in_ors.append(predicate_mappings[str(fp)])\n    return predicates_in_ors\n\n\n  def _get_filtering_predicate_cnf_representation(self, cnf_expression, predicate_mappings):\n    if '&' not in str(cnf_expression):\n      return [self._get_or_clause_representation(cnf_expression, predicate_mappings)]\n\n    or_expressions_connected_by_ands = list(cnf_expression.args)\n    or_expressions_connected_by_ands_repr = []\n    for or_expression in or_expressions_connected_by_ands:\n      connected_by_ors = self._get_or_clause_representation(or_expression, predicate_mappings)\n      or_expressions_connected_by_ands_repr.append(connected_by_ors)\n    return or_expressions_connected_by_ands_repr\n\n\n  def _replace_col_in_filter_predicate_with_root_col(self, expression):\n    '''\n    This function replace column with root columns,\n    for e.g. in the filtering predicate 'colors.frame > 10000'\n    the root column of 'colors.frame' is 'blob.frame'\n    so filtering predicate will be converted into 'blob.frame > 10000'\n    '''\n    copied_expression = expression.copy()\n    for node in copied_expression.walk():\n      if isinstance(node, exp.Expression) and self._check_in_subquery(node):\n        continue\n      if isinstance(node, exp.Column):\n        table_name = str.lower(node.args['table'].args['this'])\n        col_name = str.lower(node.args['this'].args['this'])\n        normalized_col_name = f'{table_name}.{col_name}'\n        originated_from = self.config.columns_to_root_column.get(normalized_col_name, normalized_col_name)\n        table_name = originated_from.split('.')[0]\n        node.args['table'].set('this', table_name)\n    return copied_expression\n\n\n  def _convert_logical_condition_to_cnf(self, logic_condition):\n    # predicate name (used for sympy package) to expression\n    predicate_mappings = {}\n    # predicate mapping will be filled by this function\n    sympy_representation, _ = self._get_sympify_form(logic_condition, 0, predicate_mappings)\n    sympy_expression = sympify(sympy_representation)\n    cnf_expression = to_cnf(sympy_expression)\n    converted_logical_expression = self._get_filtering_predicate_cnf_representation(cnf_expression, predicate_mappings)\n    return converted_logical_expression\n\n\n  @cached_property\n  def filtering_predicates(self):\n    \"\"\"\n    this class contains the functionality to convert filtering predicate in SQL query to\n    conjunctive normal form.\n    for e.g. in the following query\n\n    SELECT c.color , COUNT(c.blob_id)\n    FROM Colors c JOIN Blobs b\n    ON c.blob_id = b.blob_id\n    WHERE (b.timestamp > 10 and b.timestamp < 12) or c.color=red\n    GROUP BY c.blob_id\n\n    the filtering predicate will be converted to\n\n    (blobs.timestamp > 10 or colors.color = red) and (blobs.timestamp < 12 or colors.color = red)\n    \"\"\"\n    normalized_exp = self.query_after_normalizing.get_expression()\n    if normalized_exp.find(exp.Where) is not None:\n      if self._tables is None:\n        raise Exception(\"Need table and column information to support alias\")\n      filtering_predicates = self._convert_logical_condition_to_cnf(normalized_exp.find(exp.Where))\n      filtering_clauses = []\n      for or_connected_filtering_predicate in filtering_predicates:\n        or_connected_clauses = []\n        for fp in or_connected_filtering_predicate:\n          new_fp = self._replace_col_in_filter_predicate_with_root_col(fp)\n          or_connected_clauses.append(new_fp)\n        filtering_clauses.append(or_connected_clauses)\n      return filtering_clauses\n    else:\n      return []\n\n\n  def _get_table_of_column(self, col_name):\n    tables_of_column = []\n    for table in self.tables_in_query:\n      if col_name in self._tables[table]:\n        tables_of_column.append(table)\n    if len(tables_of_column) == 0:\n      raise Exception(f\"Column - {col_name} is not present in any table or it not any alias of user defined function\")\n    elif len(tables_of_column) > 1:\n      raise Exception(f\"Ambiguity in identifying column - {col_name}, it is present in multiple tables\")\n    else:\n      return tables_of_column[0]\n\n\n  @cached_property\n  def query_after_normalizing(self):\n    \"\"\"\n    this function traverse expression and return the normalized query excluding subquery part.\n    Specifically, normalizing expression includes removing alias, adding affiliate table name for columns, replacing\n    alias with original name\n    e.g. for this query: SELECT frame FROM blobs b WHERE b.timestamp > 100\n    the expression will be converted into SELECT blobs.frame FROM blobs WHERE blobs.timestamp > 100\n    \"\"\"\n    copied_expression = self._expression.copy()\n    table_alias_to_name, column_alias_to_name = self.table_and_column_aliases_in_query\n    udf_output_to_alias_mapping, alias_to_udf_mapping = self.udf_outputs_aliases\n    \n    for node in copied_expression.walk():\n      if isinstance(node, exp.Expression) and self._check_in_subquery(node):\n        continue\n      if isinstance(node, exp.Column):\n        if isinstance(node.args['this'], exp.Identifier):\n          col_name = str.lower(node.name)\n          if col_name in column_alias_to_name:\n            col_name = column_alias_to_name[col_name]\n            node.args['this'].set('this', col_name)\n          if 'table' in node.args and node.args['table'] is not None:\n            table_name = str.lower(node.args['table'].args['this'])\n            if table_name in table_alias_to_name:\n              table_name = table_alias_to_name[table_name]\n          elif col_name in alias_to_udf_mapping:\n            continue\n          else:\n            table_name = self._get_table_of_column(col_name)\n\n          node.set('table', exp.Identifier(this=table_name, quoted=False))\n      elif isinstance(node, exp.Star):\n        if isinstance(node.parent, exp.AggFunc):\n          continue\n        select_exp_list = []\n        for table_name in self.tables_in_query:\n          for col_name, _ in self._tables[table_name].items():\n            new_table = exp.Identifier(this=table_name, quoted=False)\n            new_column = exp.Identifier(this=col_name, quoted=False)\n            select_exp_list.append(exp.Column(this=new_column, table=new_table))\n        copied_expression.set('expressions', select_exp_list)\n      \n      # remove alias\n      if isinstance(node, exp.Table):\n          node.set('alias', None)\n      if isinstance(node, exp.Alias):\n          parent = node.parent\n          if isinstance(parent, exp.Select):\n              idx = parent.args['expressions'].index(node)\n              parent.args['expressions'][idx] = node.this\n    return Query(copied_expression.sql(), self.config)\n\n\n  def _get_normalized_column_set(self, normalized_expression):\n    \"\"\"\n      this function assume the input expression has been normalized, the function traverses normalized expression\n      and return the list of normalized column names excluding subquery part\n      e.g. for this query: SELECT blobs.frame FROM blobs WHERE blobs.timestamp > 100\n      it will return [blobs.frame, blobs.timestamp],\n      \"\"\"\n    normalized_column_set = set()\n    for node in normalized_expression.walk():\n      if isinstance(node, exp.Expression) and self._check_in_subquery(node):\n        continue\n      if isinstance(node, exp.Column):\n        if isinstance(node.args['this'], exp.Identifier):\n          col_name = node.args['this'].args['this']\n          table_name = node.args['table'].args['this']\n          normalized_column_set.add(f'{table_name}.{col_name}')\n      elif isinstance(node, exp.Star):\n        for table_name in self.tables_in_query:\n          for col_name, _ in self._tables[table_name].items():\n            normalized_column_set.add(f'{table_name}.{col_name}')\n    return normalized_column_set\n\n\n  @cached_property\n  def inference_engines_required_for_filtering_predicates(self):\n    \"\"\"\n    Inference services required to run to satisfy the columns present in each filtering predicate\n    for e.g. if predicates are [[color=red],[frame>20],[object_class=car]]\n    it returns [{colors02}, [], {objects00}]\n    \"\"\"\n    inference_engines_required_predicates = []\n    for filtering_predicate in self.filtering_predicates:\n      inference_engines_required = set()\n      for or_connected_predicate in filtering_predicate:\n        normalized_col_set = self._get_normalized_column_set(or_connected_predicate)\n        for normalized_col_name in normalized_col_set:\n          if normalized_col_name in self.config.column_by_service:\n            inference_engines_required.add(self.config.column_by_service[normalized_col_name].service.name)\n      inference_engines_required_predicates.append(inference_engines_required)\n    return inference_engines_required_predicates\n\n\n  @cached_property\n  def tables_in_filtering_predicates(self):\n    \"\"\"\n    Tables needed to satisfy the columns present in each filtering predicate\n    for e.g. if predicates are [[color=red],[frame>20],[object_class=car]]\n    it returns [{color}, {blob}, {object}]\n    \"\"\"\n    tables_required_predicates = []\n    for filtering_predicate in self.filtering_predicates:\n      tables_required = set()\n      for or_connected_predicate in filtering_predicate:\n        normalized_col_set = self._get_normalized_column_set(or_connected_predicate)\n        for normalized_col_name in normalized_col_set:\n          if normalized_col_name in self.config.column_by_service:\n            tables_required.add(normalized_col_name.split('.')[0])\n      tables_required_predicates.append(tables_required)\n    return tables_required_predicates\n\n\n  @cached_property\n  def columns_in_filtering_predicate(self):\n    \"\"\"\n    Columns needed to satisfy the columns present in each filtering predicate\n    for e.g. if predicates are [color=red, frame>20, object_class=car]\n    it returns [color, frame, object_class]\n    \"\"\"\n    columns_required_for_filtering_predicates = defaultdict(set)\n    for connected_by_or_filtering_predicate in self.filtering_predicates:\n      for filtering_predicate in connected_by_or_filtering_predicate:\n        for column in filtering_predicate.find_all(exp.Column):\n          columns_required_for_filtering_predicates[filtering_predicate].add(column.sql())\n    return columns_required_for_filtering_predicates\n  \n  \n  @cached_property\n  def inference_engines_required_for_query(self):\n    \"\"\"\n    Inference services required for sql query, will return a list of inference service\n    \"\"\"\n    visited = self.columns_in_query.copy()\n    stack = list(visited)\n    inference_engines_required = set()\n\n    while stack:\n      col = stack.pop()\n\n      if col in self.config.column_by_service:\n        inference = self.config.column_by_service[col]\n\n        if inference not in inference_engines_required:\n          inference_engines_required.add(inference)\n\n          for inference_col in inference.binding.input_columns:\n            if inference_col not in visited:\n              stack.append(inference_col)\n              visited.add(inference_col)\n\n    inference_engines_ordered = [\n      inference_engine\n      for inference_engine in self.config.inference_topological_order\n      if inference_engine in inference_engines_required\n    ]\n\n    return inference_engines_ordered\n\n\n  @cached_property\n  def blob_tables_required_for_query(self):\n    '''\n    get required blob tables for a query\n    '''\n    blob_tables = set()\n    for inference_engine in self.inference_engines_required_for_query:\n      for input_col in inference_engine.binding.input_columns:\n        input_table = input_col.split('.')[0]\n        if input_table in self.config.blob_tables:\n          blob_tables.add(input_table)\n\n    for table in self.tables_in_query:\n      if table in self.config.blob_tables:\n        blob_tables.add(table)\n\n    return list(blob_tables)\n\n\n  def _get_keyword_arg(self, exp_type):\n    value = None\n    for node in self._expression.walk():\n      if isinstance(node, exp_type):\n        if value is not None:\n          raise Exception(f'Multiple unexpected keywords found')\n        else:\n          if node.args['this']:\n            value = float(node.args['this'].args['this'])\n          elif node.args['expression']:\n            value = float(node.args['expression'].args['this'])\n    return value\n\n\n  @cached_property\n  def limit_cardinality(self):\n    return self._get_keyword_arg(exp.Limit)\n\n\n  def is_limit_query(self):\n    cardinality = self.limit_cardinality\n    if cardinality is None:\n      return False\n    return True\n\n\n  @cached_property\n  def error_target(self):\n    error_target = self._get_keyword_arg(exp.ErrorTarget)\n    return error_target / 100. if error_target else None\n\n\n  @cached_property\n  def confidence(self):\n    return self._get_keyword_arg(exp.Confidence)\n\n\n  @cached_property\n  def is_approx_agg_query(self):\n    return self.error_target is not None and self.confidence is not None\n\n\n  # Get aggregation types with columns corresponding\n  @cached_property\n  def aggregation_type_list_in_query(self):\n    '''\n    Return list of aggregation types\n    eg: SELECT AVG(col1), AVG(col2), COUNT(*) from table1;\n    the function will return [exp.AVG, exp.AVG, exp.COUNT]\n    '''\n    select_exp = self._expression.args['expressions']\n    agg_type_with_cols = []\n    for expression in select_exp:\n      aggregate_expression = expression.find(exp.AggFunc)\n      if aggregate_expression is not None:\n        agg_col = aggregate_expression.args['this'].sql()\n      else:\n        agg_col = None\n      if isinstance(aggregate_expression, exp.Avg):\n        agg_type_with_cols.append((exp.Avg, agg_col))\n      elif isinstance(aggregate_expression, exp.Count):\n        agg_type_with_cols.append((exp.Count, agg_col))\n      elif isinstance(aggregate_expression, exp.Sum):\n        agg_type_with_cols.append((exp.Sum, agg_col))\n      else:\n        raise Exception('AIDB only support approximation for Avg, Sum and Count query currently.')\n    return agg_type_with_cols\n\n\n  # Validate AQP\n  def is_valid_aqp_query(self):\n    # Only accept select statements\n    query_no_aqp_expression = self.base_sql_no_aqp.get_expression()\n    if not isinstance(query_no_aqp_expression, exp.Select):\n      raise Exception('Not a select statement')\n\n    if self._expression.find(exp.Group) is not None:\n      raise Exception(\n          '''AIDB does not support GROUP BY for approximate aggregation queries. \n          Try running without the error target and confidence.'''\n      )\n\n    # check aggregation function in SELECT clause\n    _ = self.aggregation_type_list_in_query\n\n    if not self.error_target or not self.confidence:\n      raise Exception('Aggregation query should contain error target and confidence')\n\n    return True\n\n\n  @cached_property\n  def is_aqp_join_query(self):\n    return self._expression.find(exp.Join) is not None and self._expression.find(exp.Anonymous) is not None\n\n\n  @cached_property\n  def recall_target(self):\n    recall_target = self._get_keyword_arg(exp.RecallTarget)\n    return recall_target / 100. if recall_target else None\n\n\n  @cached_property\n  def precision_target(self):\n    precision_target = self._get_keyword_arg(exp.PrecisionTarget)\n    return precision_target / 100. if precision_target else None\n\n\n  @cached_property\n  def oracle_budget(self):\n    oracle_budget = self._get_keyword_arg(exp.Budget)\n    return oracle_budget if oracle_budget else None\n\n\n  @cached_property\n  def is_approx_select_query(self):\n    if self.precision_target is not None:\n      raise Exception(\"AIDB has not support approx select query with precision target.\")\n    return self.recall_target is not None\n\n\n  @cached_property\n  def is_valid_approx_select_query(self):\n    if self.oracle_budget or self.confidence is None:\n      return False\n    else:\n      return True\n\n\n  def is_select_query(self):\n    return isinstance(self._expression, exp.Select)\n\n\n  @cached_property\n  def base_sql_no_aqp(self):\n    _exp_no_aqp = self._expression.transform(_remove_aqp)\n    if _exp_no_aqp is None:\n      raise Exception('SQL contains no non-AQP statements')\n    return Query(_exp_no_aqp.sql(), self.config)\n\n\n  @cached_property\n  def base_sql_no_where(self):\n    _exp_no_where = self._expression.transform(_remove_where)\n    if _exp_no_where is None:\n      raise Exception('SQL contains no non-AQP statements')\n    return Query(_exp_no_where.sql(), self.config)\n\n\n  # FIXME: move it to sqlglot.rewriter\n  def add_where_condition(self, where_condition):\n    expression = self._expression.copy()\n    new_sql = expression.where(where_condition)\n    return Query(new_sql.sql(), self.config)\n\n\n  def add_select(self, new_select):\n    expression = self._expression.copy()\n    new_sql = expression.select(new_select)\n    return Query(new_sql.sql(), self.config)\n\n\n  def add_join(self, new_join):\n    expression = self._expression.copy()\n    new_sql = expression.join(new_join)\n    return Query(new_sql.sql(), self.config)\n\n\n  def add_offset_keyword(self, offset):\n    expression = self._expression.copy()\n    if offset != 0:\n      expression = expression.offset(offset)\n    return Query(expression.sql(), self.config)\n\n\n  def add_limit_keyword(self, limit):\n    expression = self._expression.copy()\n    expression = expression.limit(limit)\n    return Query(expression.sql(), self.config)\n\n  @staticmethod\n  def convert_and_connected_fp_to_exp(and_connected_fp_list):\n    '''\n    This function connected filter predicates in a list of list\n    e.g. [[fp1, fp2], [fp3, fp4]] will be converted to the expression of '(fp1 OR fp2) AND (fp3 OR fp4)'\n    '''\n    def _build_tree(elements, operator):\n      if len(elements) == 0:\n        return None\n      elif len(elements) == 1:\n        return elements[0]\n      else:\n        return operator(this=elements[0], expression=_build_tree(elements[1:], operator))\n    new_or_connected_expression_list = []\n    for or_connected_fp in and_connected_fp_list:\n      new_or_connected_expression = _build_tree(or_connected_fp, exp.Or)\n      if new_or_connected_expression:\n        new_or_connected_expression_list.append(exp.Paren(this=new_or_connected_expression))\n\n    new_and_connected_expression = _build_tree(new_or_connected_expression_list, exp.And)\n    return new_and_connected_expression\n\n\n  @cached_property\n  def is_udf_query(self):\n    for expression in self._expression.find_all(exp.Anonymous):\n      if expression.args['this'] in self.config.user_defined_functions:\n        return True\n    return False\n\n\n  def check_udf_query_validity(self):\n    for expression in self._expression.find_all(exp.Anonymous):\n      if isinstance(expression.parent, exp.Anonymous):\n        if (expression.parent.args['this'] in self.config.user_defined_functions\n            and expression.args['this'] in self.config.user_defined_functions):\n          raise Exception(\"AIDB does not support nested user defined function currently\")\n\n    if self._expression.find(exp.AggFunc) is not None:\n      if not self.is_aqp_join_query:\n        raise Exception(\"AIDB does not support user defined function for exact aggregation query\")\n\n    if  self.is_approx_select_query or self.is_limit_query():\n      raise Exception('AIDB only support user defined function for exact query '\n                      'and approximate aggregation query currently.')\n\n    if len(self.all_queries_in_expressions) > 1:\n      raise Exception(\"AIDB does not support user defined function with nested query currently\")\n\n\n\n  def _expression_contains_udf(self, expression):\n    # check if the expression contain user defined function\n    for expression in expression.find_all(exp.Anonymous):\n      if expression.args['this'] in self.config.user_defined_functions:\n        return True\n\n    udf_output_to_alias_mapping, alias_to_udf_mapping = self.udf_outputs_aliases\n    for node in expression.walk():\n      if isinstance(node, exp.Expression) and self._check_in_subquery(node):\n        continue\n      if isinstance(node, exp.Column):\n        # Check if the udf corresponding to the alias is a udf in the AIDB engine or a udf in the database\n        if node.args['this'].args['this'] in alias_to_udf_mapping:\n          if alias_to_udf_mapping[node.args['this'].args['this']].args['this'] in self.config.user_defined_functions:\n            return True\n\n    return False\n\n\n  @cached_property\n  def udf_query(self):\n    '''\n    This function is used to parse the query that includes user defined functions.\n    return parameters:\n    * dataframe_sql (store information for query of dataframe)\n    * parsed_query (executed by normal db)\n    e.g. 'SELECT udf(x_max, y_max) from objects00'\n      the parsed query will be 'SELECT objects00.x_max AS col__0, objects00.y_max AS col__1 from objects00\n      dataframe_sql ={\n          'udf_mapping': [{'col_names': ['col__0', 'col__1'], 'function_name': 'udf','result_col_name': ['function__0']}],\n          'select_col': ['function__0'],\n          'filter_predicate': None\n      }\n      udf_mapping: store information for user defined function\n      select_col: columns in SELECT clause in dataframe query\n      filter_predicate: filter_predicate for dataframe query\n    '''\n\n    # this class is used to extract needed information from query\n    class QueryModifier:\n      def __init__(self):\n        self.added_select = set()\n        self.new_select_exp_list = []\n        self.col_index_mapping = {}\n        self.new_select_col_index = 0\n        self.function_index = 0\n        self.udf_output_alias_index = 0\n        self.udf_mapping_list = []\n        self.dataframe_select_col_list = []\n\n\n      def add_column_with_alias(self, node, is_select_col = False, predefined_alias = None):\n        if isinstance(node, exp.AggFunc):\n          return\n\n        if node not in self.added_select:\n          self.added_select.add(node)\n          if predefined_alias:\n            alias = predefined_alias\n          else:\n            alias = f'col__{self.new_select_col_index}'\n          select_col_with_alias = exp.Alias(this=node, alias=alias)\n          self.new_select_exp_list.append(select_col_with_alias)\n          self.col_index_mapping[node] = alias\n          self.new_select_col_index += 1\n        if is_select_col:\n          self.dataframe_select_col_list.append(self.col_index_mapping[node])\n\n\n      def add_udf(self, user_defined_function, udf_output_to_alias_mapping, is_select_col = False):\n        udf_with_index = f\"{user_defined_function.args['this']}__{self.udf_output_alias_index}\"\n        if udf_with_index in udf_output_to_alias_mapping:\n          udf_output_alias = udf_output_to_alias_mapping[udf_with_index]\n          self.udf_output_alias_index += 1\n        else:\n          udf_output_alias = [f'function__{self.function_index}']\n        self.function_index += 1\n\n        function_col_dict = {\n          'col_names': [],\n          'function_name': user_defined_function.args['this'],\n          'result_col_name': udf_output_alias\n        }\n        for col in user_defined_function.args['expressions']:\n          self.add_column_with_alias(col)\n          function_col_dict['col_names'].append(self.col_index_mapping[col])\n        self.udf_mapping_list.append(function_col_dict)\n        if is_select_col:\n          for col_name in function_col_dict['result_col_name']:\n            self.dataframe_select_col_list.append(col_name)\n\n        return udf_output_alias\n\n\n    aidb_user_defined_functions = self.config.user_defined_functions\n    modified_query = QueryModifier()\n    normalized_query = self.query_after_normalizing\n    expression = normalized_query.get_expression().copy()\n    udf_output_to_alias_mapping, alias_to_udf_mapping = self.udf_outputs_aliases\n    for select_exp in expression.args['expressions']:\n      user_function = select_exp.find(exp.Anonymous)\n      if user_function and user_function.args['this'] in aidb_user_defined_functions:\n        _ = modified_query.add_udf(user_function, udf_output_to_alias_mapping, is_select_col=True)\n      else:\n        modified_query.add_column_with_alias(select_exp, is_select_col=True)\n\n    filter_predicates = []\n\n    # find user defined function in JOIN condition, if exists, add them into filter predicates,\n    # then remove this join condition\n    if expression.find(exp.Join) is not None:\n      for join_exp in expression.args['joins']:\n        if 'on' in join_exp.args:\n          user_function = join_exp.args['on'].find(exp.Anonymous)\n          if user_function and user_function.args['this'] in aidb_user_defined_functions:\n            filter_predicates.extend(self._convert_logical_condition_to_cnf(join_exp.args['on']).copy())\n            join_exp.set('on', None)\n        if 'on' not in join_exp.args or join_exp.args['on'] is None:\n          join_exp.set('kind', 'CROSS')\n    # we don't need to replace column with root column here, so we don't use query.filtering_predicates directly\n    if expression.find(exp.Where):\n      filter_predicates.extend(self._convert_logical_condition_to_cnf(expression.find(exp.Where)))\n\n    filter_predicates_in_sql = []\n    filter_predicates_in_dataframe = []\n\n    for or_connected in filter_predicates:\n      include_udf = False\n      for fp in or_connected:\n        if self._expression_contains_udf(fp):\n          include_udf = True\n\n      if not include_udf:\n        filter_predicates_in_sql.append(or_connected)\n        continue\n\n      new_or_connected_fp = []\n      for fp in or_connected:\n        fp_copy = fp.copy()\n        for node in fp_copy.walk(bfs=False):\n          if isinstance(node, exp.Expression) and self._check_in_subquery(node):\n            continue\n          if isinstance(node, exp.Anonymous):\n            if node.args['this'] in aidb_user_defined_functions:\n              output_alias = modified_query.add_udf(node.copy(), udf_output_to_alias_mapping)\n              if len(output_alias) > 1:\n                raise Exception(\n                    'A user-defined function in the filter predicate should produce a single output. '\n                    'To apply filters for multiple outputs, assign an alias to each output and '\n                    'then specify the filter conditions accordingly.')\n              converted_fp = exp.Column(this=exp.Identifier(this=output_alias[0]))\n              # FIXME: for IN operator\n              node.parent.set(node.arg_key, converted_fp)\n            else:\n              node_copy = node.copy()\n              modified_query.add_column_with_alias(node_copy)\n              node.parent.set(node.arg_key, modified_query.col_index_mapping[node_copy])\n\n          elif isinstance(node, exp.Column):\n            # for the column that uses udf alias, we directly use it\n            if node.args['this'].args['this'] in alias_to_udf_mapping:\n              continue\n            node_copy = node.copy()\n            modified_query.add_column_with_alias(node_copy)\n            node.args['this'].set('this', modified_query.col_index_mapping[node_copy])\n            node.set('table', None)\n        new_or_connected_fp.append(fp_copy)\n      filter_predicates_in_dataframe.append(new_or_connected_fp)\n\n    new_where_condition = self.convert_and_connected_fp_to_exp(filter_predicates_in_sql)\n    if new_where_condition:\n      new_where_clause = exp.Where(this=new_where_condition)\n    else:\n      new_where_clause = None\n    expression.set('where', new_where_clause)\n    expression.set('expressions', modified_query.new_select_exp_list)\n\n    dataframe_sql = {\n      'udf_mapping': modified_query.udf_mapping_list,\n      'select_col': modified_query.dataframe_select_col_list,\n      'filter_predicate': filter_predicates_in_dataframe\n    }\n    return dataframe_sql, Query(expression.sql(), self.config)\n"}
{"type": "source_file", "path": "aidb/utils/asyncio.py", "content": "import asyncio\n\nimport nest_asyncio\n\n\ndef asyncio_run(future, as_task=True):\n  try:\n    loop = asyncio.get_running_loop()\n  except RuntimeError:  # no event loop running\n    loop = asyncio.new_event_loop()\n    ret = loop.run_until_complete(_to_task(future, as_task, loop))\n    loop.close()\n    return ret\n  else:\n    nest_asyncio.apply(loop)\n    return asyncio.run(_to_task(future, as_task, loop))\n  \ndef _to_task(future, as_task, loop):\n  if not as_task or isinstance(future, asyncio.Task):\n    return future\n  return loop.create_task(future)\n\n\n"}
{"type": "source_file", "path": "aidb_utilities/__init__.py", "content": ""}
{"type": "source_file", "path": "aidb/utils/db.py", "content": "import sqlalchemy\nimport sqlalchemy.ext.asyncio\nimport sqlalchemy.ext.automap\n\nfrom aidb.utils.logger import logger\n\n\ndef infer_dialect(connection_uri: str):\n  # Conection URIs have the following format:\n  # dialect+driver://username:password@host:port/database\n  # See https://docs.sqlalchemy.org/en/20/core/engines.html\n  dialect = connection_uri.split(':')[0]\n  if '+' in dialect:\n    dialect = dialect.split('+')[0]\n\n  supported_dialects = [\n    'mysql',\n    'postgresql',\n    'sqlite',\n  ]\n\n  if dialect not in supported_dialects:\n    logger.warning(\n      f'Unsupported dialect: {dialect}. Defaulting to mysql')\n    dialect = 'mysql'\n\n  return dialect\n\n\ndef create_sql_engine(connection_uri, debug=False):\n  dialect = infer_dialect(connection_uri)\n  logger.info(f'Creating SQL engine for {dialect}')\n  if dialect == 'mysql':\n    kwargs = {\n      'echo': debug,\n      'max_overflow': -1,\n    }\n  else:\n    kwargs = {}\n\n  engine = sqlalchemy.ext.asyncio.create_async_engine(\n    connection_uri,\n    **kwargs,\n  )\n\n  return engine\n"}
{"type": "source_file", "path": "aidb/vector_database/vector_database.py", "content": "import abc\nimport numpy as np\nimport pandas as pd\n\nclass VectorDatabase(abc.ABC):\n  @abc.abstractmethod\n  def create_index(\n      self,\n      index_name: str,\n      similarity: str,\n      recreate_index: bool = False\n  ):\n    '''\n    Create a new index of vector database\n    :param index_name: index name, similar concept to table name in relational database\n    :param similarity: similarity function\n    :param recreate_index: whether to recreate index\n    '''\n    raise NotImplementedError\n\n\n  @abc.abstractmethod\n  def delete_index(self, index_name: str):\n    '''\n    Delete index by index name from vector database if it exists\n    '''\n    raise NotImplementedError\n\n\n  @abc.abstractmethod\n  def insert_data(self, index_name: str, data: pd.DataFrame):\n    '''\n    Insert data into index\n    :param data: data to be inserted, usually contains id and embedding, metadata is optional\n    '''\n    raise NotImplementedError\n\n\n  @abc.abstractmethod\n  def get_embeddings_by_id(self, index_name: str, ids: np.ndarray, reload = False) -> np.ndarray:\n    '''\n    Get embeddings by id and return results\n    :param ids: ids of data\n    '''\n    raise NotImplementedError\n\n\n  @abc.abstractmethod\n  def query_by_embedding(\n      self,\n      index_name: str,\n      query_embeddings: np.ndarray,\n      top_k: int = 5\n  ) -> (np.ndarray, np.ndarray):\n    '''\n    Query nearest k embeddings, return topk ids and distances\n    :param query_embeddings: embeddings to be queried\n    :param top_k: top k nearest neighbors\n    '''\n    raise NotImplementedError\n\n\n  @abc.abstractmethod\n  def execute(\n      self,\n      index_name: str,\n      embeddings: np.ndarray,\n      reps: np.ndarray,\n      top_k: int = 5\n  ) -> (np.ndarray, np.ndarray):\n    '''\n    create a new index, query topk representatives and distances for each blob id\n    :param embeddings: embeddings for all data\n    :param reps: cluster representatives sequential idx\n    :param top_k: top k nearest neighbors\n    '''\n    raise NotImplementedError"}
{"type": "source_file", "path": "aidb/utils/constants.py", "content": "import hashlib\nfrom typing import List\n\n\nCACHE_PREFIX = '__cache'\nCONFIG_PREFIX = '__config'\nREP_PREFIX = '__rep'\nTOPK_PREFIX = '__topk'\nBLOB_MAPPING_PREFIX = '__blob_mapping'\nVECTOR_ID_COLUMN = '__vector_id'\n\n# use parameter '__seed' in approximate select engine function to pass a fixed seed\nSEED_PARAMETER = '__seed'\n\n# constant parameter for column name in sampling\nMASS_COL_NAME = '__mass'\nNUM_ITEMS_COL_NAME = '__num_items'\nPROXY_SCORE_COL_NAME = '__proxy_score'\nWEIGHT_COL_NAME = '__weight'\n\n'''\nThe schema of the blob metadata table is as follows:\n  table_name: str\n  blob_key: str\n'''\nBLOB_TABLE_NAMES_TABLE = CONFIG_PREFIX + '_blob_tables'\n\ndef cache_table_name_from_inputs(service_name: str, columns: List[str]):\n  cache_table_postfix = ''\n  for idx, column in enumerate(columns):\n    # Special characters are not allowed in table names\n    column = column.replace('.', '__')\n    cache_table_postfix += f'__{idx}_{column}__'\n  # limit to 10 characters to avoid long names in the table names\n  hash_length = 10\n  column_input_hash = hashlib.sha1(cache_table_postfix.encode()).hexdigest()[:hash_length]\n  return f\"{CACHE_PREFIX}__{service_name}__{column_input_hash}\"\n\n\ndef table_name_for_rep_and_topk_and_blob_mapping(blob_tables: List[str]):\n  blob_tables.sort()\n  blob_table_postfix = ''\n  for idx, blob_table in enumerate(blob_tables):\n    # Special characters are not allowed in table names\n    blob_table_postfix += f'__{blob_table}__'\n  # limit to 10 characters to avoid long names in the table names\n  hash_length = 10\n  table_input_hash = hashlib.sha1(blob_table_postfix.encode()).hexdigest()[:hash_length]\n  return f'{REP_PREFIX}__{table_input_hash}', f'{TOPK_PREFIX}__{table_input_hash}', \\\n            f'{BLOB_MAPPING_PREFIX}__{table_input_hash}'"}
{"type": "source_file", "path": "aidb/utils/type_conversion.py", "content": "def pandas_dtype_to_native_type(val):\n  try:\n    return val.item()\n  except AttributeError:\n    # in case of string or bytes\n    return val"}
{"type": "source_file", "path": "aidb/utils/order_optimization_utils.py", "content": "from collections import defaultdict\n\nfrom aidb.config.config import Config\nfrom aidb.query.query import Query\n\n\ndef get_currently_supported_filtering_predicates_for_ordering(config: Config, query: Query):\n  inference_engines_on_blob_ids = []\n  all_columns_produced_by_blob_inference = {}\n  for bound_inference_service in query.inference_engines_required_for_query:\n    input_columns = bound_inference_service.binding.input_columns\n    output_columns = bound_inference_service.binding.output_columns\n    if all([input_column.split('.')[0] in config.blob_tables for input_column in input_columns]):\n      inference_engines_on_blob_ids.append(bound_inference_service)\n      for output_column in output_columns:\n        all_columns_produced_by_blob_inference[output_column] = bound_inference_service\n  supported_filtering_predicates = defaultdict(list)\n  for connected_by_or_filtering_predicate in query.filtering_predicates:\n    inference_engine_needed = set()\n    for filtering_predicate in connected_by_or_filtering_predicate:\n      for col in query.columns_in_filtering_predicate[filtering_predicate]:\n        if col in all_columns_produced_by_blob_inference:\n          inference_engine_needed.add(all_columns_produced_by_blob_inference[col])\n    if len(inference_engine_needed) == 1:\n      inference_engine = list(inference_engine_needed)[0]\n      if inference_engine in inference_engines_on_blob_ids:\n        supported_filtering_predicates[inference_engine].append(connected_by_or_filtering_predicate)\n  return supported_filtering_predicates\n\n\ndef reorder_inference_engine(engine_to_proxy_score, static_order):\n  engines_with_proxy_score_0 = []\n  engine_scores = []\n  for engine, score in engine_to_proxy_score.items():\n    if engine.service.cost == 0:\n      engines_with_proxy_score_0.append((engine, 0))\n    elif score == 0:\n      engines_with_proxy_score_0.append((engine, engine.service.cost))\n    else:\n      engine_scores.append((engine, engine.service.cost * score))\n      \n  engine_scores.sort(key=lambda a: a[1])\n  engines_with_proxy_score_0.sort(key=lambda a: a[1])\n  # TODO: Add support for cached engines\n  ordered_engines = [e for e, s in engines_with_proxy_score_0] + [e for e, s in engine_scores]\n  for e in static_order:\n    if e not in ordered_engines:\n      ordered_engines.append(e)\n  return ordered_engines"}
{"type": "source_file", "path": "aidb/utils/__init__.py", "content": ""}
{"type": "source_file", "path": "aidb/utils/perf_utils.py", "content": "def call_counter(func):\n  def wrapper(*args, **kwargs):\n    wrapper.calls += 1\n    return func(*args, **kwargs)\n\n  wrapper.calls = 0\n  return wrapper\n"}
{"type": "source_file", "path": "aidb/vector_database/chroma_vector_database.py", "content": "import chromadb\nimport numpy as np\nimport pandas as pd\nfrom typing import Dict, Optional\n\nfrom aidb.utils.logger import logger\nfrom aidb.vector_database.vector_database import VectorDatabase\n\n\nclass ChromaVectorDatabase(VectorDatabase):\n  def __init__(self, path: str):\n    '''\n    Authentication\n    :param path: path to store vector database\n    '''\n    if path is None:\n      raise ValueError('Chroma requires index path')\n    self.path = path\n    self.client = chromadb.PersistentClient(path=path)\n    self.index_list = [collection.name for collection in self.client.list_collections()]\n\n\n  def create_index(\n      self,\n      index_name: str,\n      similarity: str = 'l2',\n      recreate_index: bool = False\n  ):\n    '''\n    Create a new index of vector database\n    :similarity: similarity function, it should be one of l2, cosine and ip\n    '''\n    if recreate_index:\n      self.delete_index(index_name)\n\n    if not index_name:\n      raise Exception(f'Index name must not be none')\n\n    if similarity not in ['l2', 'cosine', 'ip']:\n      raise Exception('Similarity function must be one of euclidean, cosine and dotproduct')\n\n    metadata = {'hnsw:space': similarity}\n\n    if index_name in self.index_list:\n      raise Exception(f'Index {index_name} already exists, please use another name')\n    else:\n      self.client.create_collection(name=index_name, metadata=metadata)\n      self.index_list.append(index_name)\n\n\n  def delete_index(self, index_name: str):\n    '''\n    delete an index\n    '''\n    if index_name in self.index_list:\n      self.client.delete_collection(index_name)\n      self.index_list.remove(index_name)\n      logger.info(\"Index '%s' deleted.\", index_name)\n\n\n  def _connect_by_index(self, index_name: str):\n\n    if index_name not in self.index_list:\n      raise Exception(f'Couldn\\'t find index {index_name}, please create it first')\n\n    return self.client.get_collection(index_name)\n\n\n  def insert_data(self, index_name: str, data: pd.DataFrame):\n    '''\n    insert data into an index\n    '''\n    connected_index = self._connect_by_index(index_name)\n\n    ids = np.array(data['id']).astype('str').tolist()\n    embeddings = data['values'].tolist()\n    metadata = data.drop(['id', 'values'], axis=1)\n    if metadata.empty:\n      metadata = None\n    connected_index.upsert(ids=ids, embeddings=embeddings, metadatas=metadata)\n\n\n  def get_embeddings_by_id(self, index_name: str, ids: np.ndarray, reload = False) -> np.ndarray:\n    '''\n    Get data by id and return results\n    '''\n    if reload:\n      self.client = chromadb.PersistentClient(path=self.path)\n    connected_index = self._connect_by_index(index_name)\n    id_list = ids.astype('str').tolist()\n    fetch_response = connected_index.get(ids=id_list, include=['embeddings'])\n    result = np.array(fetch_response['embeddings'])\n    return result\n\n\n  def query_by_embedding(\n      self,\n      index_name: str,\n      query_embeddings: np.ndarray,\n      top_k: int = 5,\n      filters: Optional[Dict[str, str]] = None\n  ) -> (np.ndarray, np.ndarray):\n    '''\n    Query nearest k embeddings, return embeddings and ids\n    '''\n    connected_index = self._connect_by_index(index_name)\n    response = connected_index.query(query_embeddings=query_embeddings.tolist(), n_results=top_k, where=filters)\n    all_topk_reps, all_topk_dists = response['ids'], response['distances']\n\n    return np.array(all_topk_reps).astype('int64'), np.array(all_topk_dists)\n\n\n  def execute(\n      self,\n      index_name: str,\n      embeddings: np.ndarray,\n      reps: np.ndarray,\n      top_k: int = 5\n  ) -> (np.ndarray, np.ndarray):\n    '''\n    create a new index storing cluster representatives, get topk representatives and distances for each blob index\n    '''\n    self.create_index(index_name, recreate_index=True)\n    data = pd.DataFrame({'id': reps.tolist(), 'values': embeddings[reps].tolist()})\n    self.insert_data(index_name, data)\n    topk_reps, topk_dists = self.query_by_embedding(index_name, embeddings, top_k=top_k)\n\n    return topk_reps, topk_dists"}
{"type": "source_file", "path": "aidb/utils/logger.py", "content": "import logging\n\nlogging.basicConfig()\nlogger = logging.getLogger('aidb')"}
{"type": "source_file", "path": "aidb/vector_database/__init__.py", "content": ""}
{"type": "source_file", "path": "aidb/vector_database/faiss_vector_database.py", "content": "import faiss\nimport numpy as np\nimport pandas as pd\n\nfrom aidb.vector_database.vector_database import VectorDatabase\nfrom aidb.utils.logger import logger\n\n\nclass FaissVectorDatabase(VectorDatabase):\n  def __init__(self, path: str, use_gpu: bool = False):\n    '''\n    Authentication\n    :param path: path to store vector database\n    :param use_gpu: whether use gpu\n    '''\n    if path is None:\n      raise ValueError('FAISS requires index path')\n    self.index_list = dict()\n    self.path = path\n    if use_gpu:\n      self.gpu_resources = faiss.StandardGpuResources()\n    else:\n      self.gpu_resources = None\n\n\n  def create_index(\n      self,\n      index_name: str,\n      embedding_dim: int,\n      similarity: str = 'l2',\n      index_factory: str = 'Flat',\n      n_links: int = 64,\n      ef_search: int = 20,\n      ef_construction: int = 80,\n      recreate_index: bool = False\n  ):\n    '''\n    Create a new index of vector database\n    :similarity: similarity function, it should be one of l2, cosine and dot_product\n    :index_factory: index type\n                    Recommended options:\n                    - 'Flat': exact search, best accuracy\n                    - 'HNSW': Graph-based heuristic search\n                    - 'IVFxxx,Flat': Inverted file search, xxx is the number of centroids aka nlist.\n                    For more details see:\n                    - [Overview of indices](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes)\n                    - [Guideline for choosing an index]\n                      (https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index)\n                    - [FAISS Index factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)\n    :n_links: number of neighbors, only used for HNSW\n    :ef_search: expansion factor at search time, only used for HNSW\n    :ef_construction: expansion factor at construction time, only used for HNSW\n    '''\n    if recreate_index:\n      self.delete_index(index_name)\n\n    if not index_name:\n      raise Exception(f'Index name must not be none')\n\n    if similarity in (\"dot_product\", \"cosine\"):\n      metric_type = faiss.METRIC_INNER_PRODUCT\n    elif similarity == \"l2\":\n      metric_type = faiss.METRIC_L2\n    else:\n      raise Exception('Similarity function must be one of l2, cosine and dot_product')\n\n    if index_name in self.index_list:\n      raise Exception(f'Index {index_name} already exists, please use another name')\n\n    if index_factory == 'HNSW':\n      new_index = faiss.IndexHNSWFlat(embedding_dim, n_links, metric_type)\n      new_index.hnsw.efSearch = ef_search\n      new_index.hnsw.efConstruction = ef_construction\n    else:\n      new_index = faiss.index_factory(embedding_dim, index_factory, metric_type)\n\n    new_index = faiss.IndexIDMap2(new_index)\n    # use to add data with ids\n    self.index_list[index_name] = new_index\n    if self.gpu_resources is not None:\n      self.index_list[index_name] = faiss.index_cpu_to_gpu(\n        self.gpu_resources, 0, self.index_list[index_name])\n\n    self.save_index(index_name)\n\n\n  def load_index(self, index_name: str):\n    '''\n    Read index from disk\n    '''\n    load_path = f'{self.path}/{index_name}.index'\n    self.index_list[index_name] = faiss.read_index(load_path)\n    if self.gpu_resources is not None:\n      self.index_list[index_name] = faiss.index_cpu_to_gpu(\n        self.gpu_resources, 0, self.index_list[index_name])\n\n\n  def save_index(self, index_name:str):\n    save_path = f'{self.path}/{index_name}.index'\n    faiss.write_index(self.index_list[index_name], save_path)\n\n\n  def delete_index(self, index_name: str):\n    '''\n    delete an index\n    '''\n    if index_name in self.index_list:\n      del self.index_list[index_name]\n      logger.info(\"Index '%s' deleted.\", index_name)\n\n\n  def _connect_by_index(self, index_name: str):\n\n    if index_name not in self.index_list:\n      raise Exception(f'Couldn\\'t find index {index_name}, please create it first')\n\n    return self.index_list[index_name]\n\n\n  def insert_data(self, index_name: str, data: pd.DataFrame):\n    '''\n    insert data into an index\n    '''\n    connected_index = self._connect_by_index(index_name)\n\n    if not connected_index.is_trained:\n      raise Exception(f'FAISS index of type {connected_index} must be trained before adding vectors')\n\n    embedding_list = np.array(list(data['values'])).astype('float32')\n    id_list = np.array(list(data['id'])).astype('int64')\n\n    self.index_list[index_name].add_with_ids(embedding_list, id_list)\n    self.save_index(index_name)\n\n\n  def get_embeddings_by_id(self, index_name: str, ids: np.ndarray, reload = False) -> np.ndarray:\n    '''\n    Get data by id and return results\n    '''\n    if reload:\n      self.load_index(index_name)\n    connected_index = self._connect_by_index(index_name)\n    result = []\n    for id in ids.tolist():\n      record = connected_index.reconstruct(id)\n      result.append(record)\n    return np.array(result)\n\n\n  def query_by_embedding(\n      self,\n      index_name: str,\n      query_embeddings: np.ndarray,\n      top_k: int = 5\n  ) -> (np.ndarray, np.ndarray):\n    '''\n    Query nearest k embeddings, return embeddings and ids\n    :param filter_ids: will only select ids in this array\n    '''\n    connected_index = self._connect_by_index(index_name)\n    all_topk_dists, all_topk_reps = connected_index.search(query_embeddings.astype('float32'), top_k)\n\n    return np.array(all_topk_reps).astype('int64'), np.array(all_topk_dists).astype('float32')\n\n\n  def execute(\n      self,\n      index_name: str,\n      embeddings: np.ndarray,\n      reps: np.ndarray,\n      top_k: int = 5\n  ) -> (np.ndarray, np.ndarray):\n    '''\n    create a new index storing all data, query topk representatives and distances for each blob id by doing filter\n    '''\n    self.create_index(index_name, embeddings.shape[1], recreate_index=True)\n    data = pd.DataFrame({'id': reps.tolist(), 'values': embeddings[reps].tolist()})\n    self.insert_data(index_name, data)\n    topk_reps, topk_dists = self.query_by_embedding(index_name, embeddings, top_k=top_k)\n    return topk_reps, topk_dists\n"}
{"type": "source_file", "path": "aidb/vector_database/vector_database_config.py", "content": "from dataclasses import dataclass, field\nimport numpy as np\nimport pandas as pd\nfrom typing import Optional\n\nfrom aidb.vector_database.vector_database import VectorDatabase\n\n\n@dataclass\nclass TastiConfig:\n  '''\n  :param index_name: vector database index name\n  :param blob_ids: blob index in blob table, it should be unique for each data record\n  :param vector_database: initialized vector database, currently support FAISS, Chroma or Weaviate\n  :param nb_buckets: number of buckets for FPF, it should be same as the number of buckets for oracle\n  :param percent_fpf: percent of randomly selected buckets in FPF\n  :param seed: random seed\n  :param reps: representative ids\n  '''\n  index_name: str\n  vector_database: VectorDatabase\n  nb_buckets: int = 1000\n  percent_fpf: float = 0.75\n  seed: int = 1234\n\n\n  # Initialize supplementary parameters in Tasti; without this step, these parameters remain uninitialized\n  def __post_init__(self):\n    pass"}
{"type": "source_file", "path": "aidb/vector_database/weaviate_vector_database.py", "content": "from dataclasses import dataclass, field\nimport numpy as np\nimport pandas as pd\nimport weaviate\nfrom typing import Dict,  Optional\n\nfrom aidb.utils.logger import logger\nfrom aidb.vector_database.vector_database import VectorDatabase\nfrom weaviate.util import generate_uuid5\nfrom weaviate import AuthApiKey, AuthClientPassword\n\n@dataclass\nclass WeaviateAuth:\n  \"\"\"\n  :param url: weaviate url\n  :param username: weaviate username\n  :param pwd: weaviate password\n  :param api_key: weaviate api key, user should choose input either username/pwd or api_key\n  \"\"\"\n  url: Optional[str] = field(default=None)\n  username: Optional[str] = field(default=None)\n  pwd: Optional[str] = field(default=None)\n  api_key: Optional[str] = field(default=None)\n\n\nclass WeaviateVectorDatabase(VectorDatabase):\n  def __init__(self, weaviate_auth: WeaviateAuth):\n    '''\n    Authentication\n    '''\n    if weaviate_auth.url is None:\n      raise ValueError('Weaviate requires URL to connect')\n    auth_client_secret = self._get_auth_secret(weaviate_auth.username, weaviate_auth.pwd, weaviate_auth.api_key)\n    self.weaviate_client = weaviate.Client(\n      url=weaviate_auth.url,\n      auth_client_secret=auth_client_secret\n    )\n\n    status = self.weaviate_client.is_ready()\n    if not status:\n      raise Exception('Initial connection to Weaviate failed')\n\n    self.weaviate_client.batch.configure(batch_size=200, dynamic=True)\n\n    self.index_list = [c['class'] for c in self.weaviate_client.schema.get()['classes']]\n\n\n  @staticmethod\n  def _get_auth_secret(username: Optional[str] = None, password: Optional[str] = None, api_key: Optional[str] = None):\n    '''\n    verify the user information\n    '''\n    if api_key:\n      return AuthApiKey(api_key=api_key)\n    elif username and password:\n      return AuthClientPassword(username, password)\n    else:\n      raise Exception('Please provide api or username and password to connect Weaviate')\n\n\n  def create_index(\n    self,\n    index_name: str,\n    similarity: str = 'l2-squared',\n    recreate_index: bool = False\n  ):\n    '''\n    Create a new index of vectordatabase\n    :similarity: similarity function, it should be one of l2-squared, cosine and dot\n    '''\n    index_name = self._sanitize_index_name(index_name)\n    if recreate_index:\n      self.delete_index(index_name)\n\n    if not index_name:\n      raise Exception('Index name must not be none')\n\n    if similarity not in ['cosine', 'dot', 'l2-squared']:\n      raise Exception('Similarity function must be one of euclidean, cosine and dotproduct')\n\n    if index_name in self.index_list:\n      raise Exception(f'Index {index_name} already exists, please use another name')\n\n    schema = {\n      'class': index_name,\n      'description': f'Index {index_name} to store embedding',\n      'vectorizer': 'none',\n      'properties': [{'name': 'original_id', 'dataType': ['int']}],\n      'vectorIndexConfig': {'distance': similarity}\n    }\n\n    self.weaviate_client.schema.create_class(schema)\n    self.index_list.append(index_name)\n\n\n  def load_index(self):\n    '''\n    Reload index from weaviate\n    '''\n    self.index_list = [c['class'] for c in self.weaviate_client.schema.get()['classes']]\n\n\n  def delete_index(self, index_name: str):\n    '''\n    delete an index\n    '''\n    index_name = self._sanitize_index_name(index_name)\n    if index_name in self.index_list:\n      self.weaviate_client.schema.delete_class(index_name)\n      self.index_list.remove(index_name)\n      logger.info(\"Index '%s' deleted.\", index_name)\n\n\n  def _sanitize_index_name(self, index_name: str) -> str:\n    '''\n    index should start with a capital\n    '''\n    return index_name[0].upper() + index_name[1:]\n\n\n  def _check_index_validity(self, index_name: str):\n    index_name = self._sanitize_index_name(index_name)\n    if index_name not in self.index_list:\n      raise Exception(f'Couldn\\'t find index {index_name}, please create it first')\n    return index_name\n\n\n  def insert_data(self, index_name: str, data: pd.DataFrame):\n    '''\n    insert data into an index\n    '''\n    index_name = self._check_index_validity(index_name)\n\n    with self.weaviate_client.batch as batch:\n      for _, item in data.iterrows():\n        metadata = dict()\n        metadata['original_id'] = item['id']\n        uuid = generate_uuid5(item['id'])\n        vector = item['values']\n        batch.add_data_object(metadata, index_name, uuid, vector)\n\n\n  def get_embeddings_by_id(self, index_name: str, ids: np.ndarray, reload = False) -> np.ndarray:\n    '''\n    Get data by id and return results\n    '''\n    if reload:\n      self.load_index()\n    index_name = self._check_index_validity(index_name)\n    result = []\n    for id in ids:\n      uuid = generate_uuid5(id)\n      fetch_response = self.weaviate_client.data_object.get(uuid=uuid, with_vector=True, class_name=index_name)\n      result.append(fetch_response['vector'])\n    return np.array(result)\n\n\n  def query_by_embedding(\n      self,\n      index_name: str,\n      query_emb_list: np.ndarray,\n      top_k: int = 5,\n      filters: Optional[Dict[str, str]] = None\n  ) -> (np.ndarray, np.ndarray):\n    '''\n    Query nearest k embeddings, return embeddings and ids\n    :param filters: do filter by metadata\n    '''\n    index_name = self._check_index_validity(index_name)\n    properties = ['original_id', '_additional {distance}']\n    multi_query = []\n    alias_list = ['alias' + str(i) for i in range(len(query_emb_list))]\n    for alias, query_emb in zip(alias_list, query_emb_list):\n      new_query = self.weaviate_client.query.get(class_name=index_name, properties=properties)\n      new_query.with_near_vector({'vector': query_emb})\n      new_query.with_limit(top_k)\n      new_query.with_alias(alias)\n      if filters:\n        new_query.with_where(filters)\n      multi_query.append(new_query)\n\n    response = self.weaviate_client.query.multi_get(multi_query).do()\n    response = response['data']['Get']\n\n    all_topk_reps, all_topk_dists = [], []\n    for alias in alias_list:\n      topk_rep, topk_dist = [], []\n      for record in response[alias]:\n        topk_rep.append(record['original_id'])\n        topk_dist.append(record['_additional']['distance'])\n      all_topk_reps.append(topk_rep)\n      all_topk_dists.append(topk_dist)\n\n    return np.array(all_topk_reps), np.array(all_topk_dists)\n\n\n  def execute(\n      self,\n      index_name: str,\n      embeddings: np.ndarray,\n      reps: np.ndarray,\n      top_k: int = 5\n  ) -> (np.ndarray, np.ndarray):\n    '''\n    create a new index storing cluster representatives, get topk representatives and distances for each blob index\n    '''\n    self.create_index(index_name, recreate_index=True)\n    data = pd.DataFrame({'id': reps.tolist(), 'values': embeddings[reps].tolist()})\n    self.insert_data(index_name, data)\n    topk_reps, topk_dists = self.query_by_embedding(index_name, embeddings, top_k=top_k)\n\n    return topk_reps, topk_dists\n"}
{"type": "source_file", "path": "aidb_utilities/aidb_setup/aidb_factory.py", "content": "import pandas as pd\n\nfrom aidb.config.config_types import InferenceBinding\nfrom aidb.engine import Engine\nfrom aidb.inference.examples.llm_inference_service import LLMInference\nfrom aidb.vector_database.chroma_vector_database import ChromaVectorDatabase\nfrom aidb.vector_database.faiss_vector_database import FaissVectorDatabase\nfrom aidb.vector_database.weaviate_vector_database import WeaviateVectorDatabase\nfrom aidb.vector_database.tasti import Tasti\n\ndef get_tasti_config(tasti_config):\n  vector_database = {\n    'FAISS': FaissVectorDatabase,\n    'CHROMA': ChromaVectorDatabase,\n    'WEAVIATE': WeaviateVectorDatabase\n  }\n\n  vector_database_type = tasti_config['type'].upper()\n  try:\n    user_vector_database = vector_database[vector_database_type](**tasti_config['auth'])\n  except KeyError:\n    raise ValueError(f'{vector_database_type} is not a supported type. We support FAISS, Chroma and Weaviate.')\n  tasti_index = Tasti(vector_database=user_vector_database, **tasti_config['tasti_engine'])\n\n  selected_vector_id_df = None\n\n  if 'vector_id_csv' in tasti_config:\n    selected_vector_id_df = pd.read_csv(tasti_config['vector_id_csv'])\n    if len(selected_vector_id_df.columns) != 1:\n      raise Exception('Vector id csv file should contain one column for vector id')\n    selected_vector_id_df.columns.values[0] = 'vector_id'\n\n  return tasti_index, selected_vector_id_df\n\n\ndef setup_inference(service_name, service, service_config):\n  inference_dict ={\n    'LLM': LLMInference\n  }\n  service_config['name'] = service_name\n  return inference_dict[service.upper()](**service_config)\n  \n  \nclass AIDB:\n  @staticmethod\n  def from_config(config, verbose=False):\n    db_config = f\"{config['db_config']['url']}/{config['db_config']['name']}\"\n    if 'vector_database' in config: \n      tasti_index, vector_id_df = get_tasti_config(config['vector_database'])\n      aidb_engine = Engine(\n          db_config,\n          debug=False,\n          tasti_index=tasti_index,\n          user_specified_vector_ids=vector_id_df\n      )\n    else:\n      aidb_engine = Engine(db_config, debug=False)\n\n    for inference_engine in config['services']:\n      service = setup_inference(inference_engine['name'], inference_engine[\"service\"], inference_engine['service_config'])\n      copy_map = inference_engine.get(\"copy\", {})\n      aidb_engine.register_inference_service(service)\n      aidb_engine.bind_inference_service(\n        service.name,\n        InferenceBinding(tuple(inference_engine[\"input_cols\"]), tuple(inference_engine[\"output_cols\"])),\n        copy_map,\n        verbose)\n    return aidb_engine\n"}
{"type": "source_file", "path": "aidb_utilities/aidb_setup/__init__.py", "content": ""}
