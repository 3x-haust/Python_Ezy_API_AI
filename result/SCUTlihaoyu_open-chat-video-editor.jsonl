{"repo_info": {"repo_name": "open-chat-video-editor", "repo_owner": "SCUTlihaoyu", "repo_url": "https://github.com/SCUTlihaoyu/open-chat-video-editor"}}
{"type": "test_file", "path": "generator/comm/test_meta_server.py", "content": "from meta_sever import download_image\nfrom PIL import Image\n\n\ndef test_download_image():\n    url = \"https://cdn.shopify.com/s/files/1/0282/0804/products/pulp_1024x1024.jpg?v=1474264437\"\n    buffer = download_image(url)\n    img = Image.open(buffer)\n    img.show()\n    \nif __name__ == \"__main__\":\n    test_download_image()"}
{"type": "test_file", "path": "tests/test_textclip.py", "content": "from moviepy.editor import TextClip, ImageClip,CompositeVideoClip\nimport math\nfrom PIL import Image\n\nimport numpy \ndef zoom_in_effect(clip, zoom_ratio=0.04):\n    def effect(get_frame, t):\n        img = Image.fromarray(get_frame(t))\n        base_size = img.size\n\n        new_size = [\n            math.ceil(img.size[0] * (1 + (zoom_ratio * t))),\n            math.ceil(img.size[1] * (1 + (zoom_ratio * t)))\n        ]\n\n        # The new dimensions must be even.\n        new_size[0] = new_size[0] + (new_size[0] % 2)\n        new_size[1] = new_size[1] + (new_size[1] % 2)\n\n        img = img.resize(new_size, Image.LANCZOS)\n\n        x = math.ceil((new_size[0] - base_size[0]) / 2)\n        y = math.ceil((new_size[1] - base_size[1]) / 2)\n\n        img = img.crop([\n            x, y, new_size[0] - x, new_size[1] - y\n        ]).resize(base_size, Image.LANCZOS)\n\n        result = numpy.array(img)\n        img.close()\n\n        return result\n\n    return clip.fl(effect)\none_text = \"中文测试\"\nimg  = \"data/10012.jpg\"\nclip = ImageClip(img,duration=5)\nclip = zoom_in_effect(clip, zoom_ratio=0.04)\n\ntext_clip = TextClip(one_text, font='幼圆',fontsize=70, color='red')\ntext_clip = text_clip.set_duration(5)\n\nclip = CompositeVideoClip([clip,text_clip.set_position(('center','bottom'))])\nclip.write_videofile(\"text_test.mp4\", fps=24)\n\n# print(TextClip.list('font'))"}
{"type": "test_file", "path": "tests/test_paddle.py", "content": "from paddlespeech.cli.tts.infer import TTSExecutor\n\n\n\ntts = TTSExecutor()\nam = 'speedyspeech_csmsc'\nlang = 'zh'\nout_path='test1.wav'\ntext='你好，测试一下数据'\ntts(text=text,lang=lang,am=am,output=out_path)"}
{"type": "test_file", "path": "tests/test_image_generator.py", "content": "import sys\nimport os\n# sys.path.append(\"F:\\Workspace\\github\\open-chat-video-editor\")\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom PIL import Image\nfrom generator.image.build import build_image_generator\nfrom configs.config import get_cfg_defaults  # local variable usage pattern, or:\n\ndef test_image_by_retrieval():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs\\image_by_retrieval.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    # build image generator\n    image_generator = build_image_generator(cfg)\n    text = [\"a cat\",'a cat and a kid']\n    image_resp = image_generator.batch_run(text)\n    print(image_resp)\n    for item in image_resp:\n        img = Image.open(item['img_local_path'])\n\ndef test_multilang_image_by_retrieval():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs\\multilang_image_by_retrieval.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    # build image generator\n    image_generator = build_image_generator(cfg)\n    text = [\"一只猫\",'一只猫和一个孩子']\n    image_resp = image_generator.batch_run(text)\n    print(image_resp)\n    for item in image_resp:\n        img = Image.open(item['img_local_path'])\n \nif __name__ == \"__main__\":\n    test_image_by_retrieval()"}
{"type": "test_file", "path": "tests/test_gradio.py", "content": "import gradio as gr \n\ndef show_video(input_text,style):\n    print(input_text,style)\n    return \"out text\",\"image_by_retrieval_then_diffusion_chatgpt_zh.mp4\"\n\ndef run_show_video():\n    gr.Interface(\n        show_video,\n        [gr.inputs.Textbox(placeholder=\"Enter sentence here...\"),  gr.Radio([\"realism style\", \"cartoon style\"], label=\"video style\", info=\"Please select a video style\"),],\n        outputs =  ['text',gr.Video()],\n        title='test',\n        allow_flagging=\"never\",\n\n    ).launch()\n    \nif __name__ == \"__main__\":\n    run_show_video()\n    "}
{"type": "test_file", "path": "tests/test_moviepy.py", "content": "from moviepy.editor import ImageClip,VideoFileClip,TextClip\n# from PIL import Image \n# import cv2\n\ndef test_image_clip():\n    fname = \"data/10012.jpg\"\n    img = cv2.imread(fname)\n    img = cv2.resize(img, (640, 480))\n    # img = Image.open(fname)\n    print(img.shape)\n    print(type(img))\n\n    clip = ImageClip(img,duration=1)\n    clip.write_videofile(\"test.mp4\",fps=24)\n\n\ndef test_video_clip():\n    video_fname = \"tmp.mp4\"\n    video_clip = VideoFileClip(video_fname)\n    video_clip = video_clip.set_duration(3)\n    print(video_clip.duration)\n    # save \n    video_clip.write_videofile(\"tmp_3.mp4\",fps=24)\n\ndef test_font():\n    print( TextClip.list('font'))\n\nif __name__ == \"__main__\":\n    test_font()"}
{"type": "test_file", "path": "tests/test_url.py", "content": "from bs4 import BeautifulSoup\nimport requests\nimport json\n\ndef get_paragraph_texts(url: str):\n    html: str = requests.get(url).text\n    soup = BeautifulSoup(html, \"html.parser\")\n    pes = soup.findAll('p')\n    texts: list[str] = []\n    for e in pes:\n        texts.append(e.get_text())\n    return texts\n\ntexts = get_paragraph_texts('https://zh.wikipedia.org/wiki/%E7%BE%8E%E5%9B%BD%E7%9F%AD%E6%AF%9B%E7%8C%AB')\nprint(texts)\nprint(len(texts))\n\n# print(\" \".join(texts)\n# with open('out.json','r') as f:\n#     texts = json.load(f)\ncnt = 0\nfor s in texts:\n    print(len(s))\n    cnt += len(s)\nprint(cnt)\n\n"}
{"type": "test_file", "path": "tests/test_chatgpt.py", "content": "import sys\nimport os\n# sys.path.append(\"F:\\Workspace\\github\\open-chat-video-editor\")\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nimport json\nimport openai\nimport requests\nfrom generator.text.models.chatgpt import ChatGPTModel,URL2TextChatGPTModel\n\nfrom bs4 import BeautifulSoup\nimport requests\ndef get_paragraph_texts(url: str):\n    html: str = requests.get(url).text\n    soup = BeautifulSoup(html, \"html.parser\")\n    pes = soup.findAll('p')\n    texts: list[str] = []\n    for e in pes:\n        texts.append(e.get_text())\n    return texts\n\n\n\norganization = \"your_organization\"\napi_key = \"your_api_key\"\nopenai.organization = organization\nopenai.api_key = api_key\nopenai.Model.list()\n\n\ndef ask(question: str):\n    return openai.Completion.create(\n        model=\"text-davinci-003\",\n        prompt=question,\n        max_tokens=400,\n        stream=False,\n        echo=False,\n    )\n\nmodel = URL2TextChatGPTModel(\"\",organization,api_key)\nurl = \"https://zh.wikipedia.org/wiki/%E7%BE%8E%E5%9B%BD%E7%9F%AD%E6%AF%9B%E7%8C%AB\"\nresp = model.run(url)\nprint(resp)\nzh_sentences = []\nen_sentences = []\nfor item in resp[\"out_text\"]:\n    zh_sentences.append(item[\"zh\"])\n    en_sentences.append(item[\"en\"])\nprint(zh_sentences)\nprint(en_sentences)"}
{"type": "test_file", "path": "tests/test_tts_generator.py", "content": "import sys\nimport os\n# sys.path.append(\"F:\\Workspace\\github\\open-chat-video-editor\")\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom generator.tts.build import TTSGenerator \nfrom configs.config import get_cfg_defaults  # local variable usage pattern, or:\nfrom generator.tts.build import build_tts_generator\n\ndef test_PaddleSpeechTTS():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs\\multilang_image_by_retrieval.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    tts_generator = build_tts_generator(cfg)\n    text = [\"a cat on the road\",'a cat and a kid playing on the road']\n    resp = tts_generator.batch_run(text)\n    print(resp)\nif __name__ == \"__main__\":\n    test_PaddleSpeechTTS()\n    "}
{"type": "test_file", "path": "tests/test_diffusion.py", "content": ""}
{"type": "test_file", "path": "tests/test_chat_edtor.py", "content": "import sys\nimport os\n# sys.path.append(\"F:\\Workspace\\github\\open-chat-video-editor\")\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom editor.build import build_editor\nfrom configs.config import get_cfg_defaults  # local variable usage pattern, or:\n\ndef test_editor():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs\\multilang_image_by_retrieval.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    editor.run('toy test')\n\ndef test_image_by_retrieval_text_by_zhtoytextgen():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs/image_by_retrieval_text_by_zhtoytextgen.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    editor.run('toy test',\"\",\"image_by_retrieval_text_by_zhtoytextgen.mp4\")\n\ndef test_editor_by_video_retrieval():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs/video_by_retrieval.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    editor.run('toy test')\n# video_by_retrieval_text_by_zhtoytextgen\n\n\ndef test_editor_by_video_by_retrieval_text_by_zhtoytextgen():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs/video_by_retrieval_text_by_zhtoytextgen.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    editor.run('小孩子养宠物',\"\",\"video_by_retrieval_text_by_zhtoytextgen.mp4\")\n    \n    \ndef test_editor_by_video_retrieval_chatgpt_zh():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs/video_by_retrieval_text_by_chatgpt_zh.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    editor.run('小孩子养宠物',\"\",\"video_retrieval_chatgpt_zh.mp4\")\n\ndef test_editor_by_image_diffusion():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs/image_by_diffusion.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    editor.run('toy test',\"\",'test_en_image_diffusion.mp4')\n\ndef test_editor_zh_by_image_diffusion():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs/image_by_diffusion_text_by_zhtoytextgen.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    editor.run('toy test',\"\",'test_zh_image_diffusion.mp4')\n\ndef test_editor_chatgpt_zh_by_image_diffusion():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs/image_by_diffusion_text_by_chatgpt_zh.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    editor.run('小孩子养宠物',\"picture of no words\",'test_chatgpt_zh_image_diffusion.mp4')\n\n\ndef test_editor_by_image_retrieval_then_diffusion():\n\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs/image_by_retrieval_then_diffusion_zhtoytextgen.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    style = \"cartoon style\"\n    editor.run('toy test',\"\",'image_by_retrieval_then_diffusion_zhtoytextgen.mp4')\n    \n\ndef test_editor_by_image_retrieval_then_diffusion_chatgpt_zh():\n\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs/image_by_retrieval_then_diffusion_chatgpt_zh.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    style = \"cartoon style\"\n    editor.run('小孩子养宠物',\"\",'image_by_retrieval_then_diffusion_chatgpt_zh.mp4')\n\ndef test_image_by_diffusion_text_by_ZhToyURL2TextModel():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs/url2video/image_by_diffusion_text_by_ZhToyURL2TextModel.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    style = \"cartoon style\"\n    editor.run('',\"\",'image_by_diffusion_text_by_ZhToyURL2TextModel.mp4')\n\ndef test_image_by_retrieval_text_by_ZhToyURL2TextModel():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs/url2video/image_by_retrieval_text_by_ZhToyURL2TextModel.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    style = \"cartoon style\"\n    editor.run('',\"\",'image_by_retrieval_text_by_ZhToyURL2TextModel.mp4')\n\n# image_by_retrieval_then_diffusion_ZhToyURL2TextModel\n\ndef test_image_by_retrieval_then_diffusion_ZhToyURL2TextModel():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs/url2video/image_by_retrieval_then_diffusion_ZhToyURL2TextModel.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    style = \"cartoon style\"\n    editor.run('',\"\",'image_by_retrieval_then_diffusion_ZhToyURL2TextModel.mp4')\n\ndef test_video_by_retrieval_text_by_ZhToyURL2TextModel():\n    cfg = get_cfg_defaults()\n    cfg_path = \"configs/url2video/video_by_retrieval_text_by_ZhToyURL2TextModel.yaml\"\n    cfg.merge_from_file(cfg_path)\n    print(cfg)\n    editor = build_editor(cfg)\n    style = \"cartoon style\"\n    editor.run('',\"\",'video_by_retrieval_text_by_ZhToyURL2TextModel.mp4')\n\nif __name__ == \"__main__\":\n    test_video_by_retrieval_text_by_ZhToyURL2TextModel()\n    "}
{"type": "source_file", "path": "editor/chat_editor.py", "content": "from comm.mylog import logger\nfrom moviepy.editor import AudioFileClip, VideoFileClip,ImageClip, concatenate_videoclips\nfrom moviepy.editor import TextClip,CompositeVideoClip,CompositeAudioClip\nfrom moviepy.editor import afx\nfrom editor.image_effect import zoom_in_effect\n\n\n\nclass Text2VideoEditor(object):\n    def __init__(self,\n                 cfg,\n                 text_generator,\n                 vision_generator,\n                 audio_generator,\n                 bgm_generator,\n                \n                 ) -> None:\n        self.text_generator = text_generator\n        self.vision_generator = vision_generator\n        self.audio_generator = audio_generator\n        self.bgm_generator = bgm_generator\n        self.cfg = cfg\n        # self.style = style\n    \n    def run(self,input_text,style=\"\",out_file=\"test.mp4\"):\n        # setence to passage\n        logger.info('input_text: {}'.format(input_text))\n        text_resp = self.text_generator.run(input_text)\n        text_lang = text_resp['lang']\n        if text_lang == 'zh':\n            zh_out_text = [val['zh'] for val in text_resp['out_text']]\n            logger.info('zh_out_text: {}'.format(zh_out_text))\n\n        en_out_text = [val['en'] for val in text_resp['out_text']]\n        # if \n        logger.info('en_out_text: {}'.format(en_out_text))\n        # stylized text\n        out_text_stylized = [val+','+style for val in en_out_text]\n        \n        # text 2 voice\n        if text_lang == 'zh':\n            tts_in_text = zh_out_text\n            sub_title_text = zh_out_text\n            final_text = zh_out_text\n        else:\n            tts_in_text = en_out_text\n            sub_title_text = en_out_text\n            final_text = en_out_text\n            \n        tts_resp = self.audio_generator.batch_run(tts_in_text)\n    \n    \n        # text 2 vision\n        vision_resp = self.vision_generator.batch_run(out_text_stylized)\n        \n        # merge media \n        final_clips = []\n        \n        for idx,(tts_info,vision_info,one_text) in enumerate(zip(tts_resp, vision_resp,sub_title_text)):\n            audio_file= tts_info['audio_path']\n            # load audio\n            audio_clip = AudioFileClip(audio_file)\n            \n            # text clip \n            if self.cfg.video_editor.subtitle.font:\n                text_clip = TextClip(one_text, font=self.cfg.video_editor.subtitle.font,fontsize=30, color='black')\n            else:\n                text_clip = TextClip(one_text,fontsize=30, color='black')\n                \n            text_clip = text_clip.set_duration(audio_clip.duration)\n            vision_type = vision_info['data_type']\n            if vision_type == 'image':\n                vision_file = vision_info['img_local_path']\n                vision_clip = ImageClip(vision_file)\n                vision_clip = vision_clip.resize((640,360))\n                \n                # set duration\n                vision_clip = vision_clip.set_duration(audio_clip.duration)\n                vision_clip = zoom_in_effect(vision_clip, zoom_ratio=0.04)\n                \n                vision_clip = vision_clip.set_audio(audio_clip)\n            elif vision_type == 'video':\n                vision_file = vision_info['video_local_path']\n                vision_clip = VideoFileClip(vision_file)\n                vision_clip = vision_clip.set_duration(audio_clip.duration)\n                vision_clip = vision_clip.resize((640,360))\n                vision_clip = vision_clip.set_audio(audio_clip)\n\n            # \n            vision_clip = CompositeVideoClip([vision_clip,text_clip.set_position(('center','bottom'))])\n            \n            # save for debug \n            vision_clip.write_videofile(\"test_{}.mp4\".format(idx), fps=24)\n            final_clips.append(vision_clip)\n        logger.info('final_clips: {}'.format(len(final_clips)))\n        video = concatenate_videoclips(final_clips)\n        \n        video_audio = video.audio.volumex(1.0)\n        # add bgm \n        bgm_resp = self.bgm_generator.run()\n        local_bgm = bgm_resp['bgm_local_file']\n        bgm_clip = AudioFileClip(local_bgm).volumex(0.2)\n        bgm_clip = afx.audio_loop(bgm_clip,duration=video.duration)\n        \n        video_audio = CompositeAudioClip([video_audio,bgm_clip])\n        \n        # add audio and bgm \n        video = video.set_audio(video_audio)\n        \n        video.write_videofile(out_file, fps=24)\n        \n        return final_text, out_file\n\n\n\n\n"}
{"type": "source_file", "path": "generator/__init__.py", "content": ""}
{"type": "source_file", "path": "generator/comm/logger.py", "content": ""}
{"type": "source_file", "path": "editor/editor.py", "content": ""}
{"type": "source_file", "path": "generator/image/image_generator.py", "content": "# 在这里实现基于检索和基于生成的外部接口逻辑\n\nfrom generator.comm.media_generator import MediaGeneratorBase\nimport urllib\nimport urllib.request\nimport io\nimport traceback\nimport os\nfrom PIL import Image\nfrom typing import List\n# logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\nfrom comm.mylog import logger\ndef download_image(url):\n    urllib_request = urllib.request.Request(\n        url,\n        data=None,\n        headers={\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:72.0) Gecko/20100101 Firefox/72.0\"},\n    )\n    with urllib.request.urlopen(urllib_request, timeout=10) as r:\n        img_stream = io.BytesIO(r.read())\n    return img_stream\n\n\n\nclass ImageGenbyRetrieval(MediaGeneratorBase):\n    '''\n    generate image by text retrieval\n    \n    '''\n    def __init__(self, config,\n                    query_embed_server,\n                    index_server,\n                    meta_server,\n                 ):\n        super(ImageGenbyRetrieval, self).__init__(config)\n        self.config = config\n        self.query_embed_server = query_embed_server\n        self.index_server = index_server\n        self.meta_server = meta_server\n        self.tmp_dir = \"./tmp/image\"\n        self.data_type = \"image\"\n\n        if not os.path.exists(self.tmp_dir):\n            os.makedirs(self.tmp_dir)\n        \n        \n    def batch_run(self, query:List,**kwargs):\n        '''\n        run image generator by retrieval\n        support multi query\n        '''\n        assert type(query) == list\n        prompt = 'a picture without text'\n        query = [ val + prompt for val in query]\n        # get query embed\n        query_embed = self.query_embed_server.get_query_embed(query)\n        \n        # knn search, indices: [batch_size, top_k]\n        distances, indices = self.index_server.search(query_embed)\n\n        # get meta \n        resp = []\n        for batch_idx,topk_ids in  enumerate(indices):\n            # one_info = {}\n            # one query topk urls\n            urls = self.meta_server.batch_get_meta(topk_ids) \n            # logging.error('urls: {}'.format(urls))\n            # download one of the topk images\n            for url_id,url in enumerate(urls):\n                \n                try:\n                    img_stream = download_image(url)\n                    # try to open\n                    url_md5 = self.get_url_md5(url)\n                    img_tmp_name = os.path.join(self.tmp_dir, \"{}_{}_{}.jpg\".format(batch_idx,url_id, url_md5))\n                    logger.info('tmp img name: {}'.format(img_tmp_name))\n                    img = Image.open(img_stream).convert('RGB')\n                    img.save(img_tmp_name)\n                    one_info = {'url':url,'topk_ids':url_id,'img_local_path':img_tmp_name,'data_type':self.data_type}\n                    resp.append(one_info)\n                    break\n                \n                except Exception as e:\n                    logger.error(e)\n                    logger.error(traceback.format_exc())\n                    \n                    continue\n        \n                \n    \n        return resp\n    \n    \n        \n        \nclass ImageGenByDiffusion(MediaGeneratorBase):\n    '''\n    generate image by stable diffusion\n    '''\n    def __init__(self, config,\n                 img_gen_model,\n                 ):\n        super(ImageGenByDiffusion, self).__init__(config)\n        self.config = config\n        self.img_gen_model = img_gen_model\n        self.tmp_dir = \"./tmp/image\"\n        self.data_type = \"image\"\n        if not os.path.exists(self.tmp_dir):\n            os.makedirs(self.tmp_dir)\n            \n    def batch_run(self, query:List,**kwargs):\n\n        assert type(query) == list\n        resp = []\n        for idx,text in enumerate(query):\n            img = self.img_gen_model.run(text)\n            pil_md5 = self.get_pil_md5(img)\n            img_tmp_name = os.path.join(self.tmp_dir, \"{}_{}.jpg\".format(idx,pil_md5))\n            img.save(img_tmp_name)\n            one_info = {'img_local_path':img_tmp_name,'data_type':self.data_type}\n            resp.append(one_info)\n        return resp\n    \n            \n        \n    \n    \n    \n\n\nclass ImageGenByRetrievalThenDiffusion(MediaGeneratorBase):\n    '''\n    generate image by retrieval then stable diffusion\n    '''\n    def __init__(self, config,\n                    img_gen_by_retrieval_server,\n                    img_gen_model,\n                    ):\n        super(ImageGenByRetrievalThenDiffusion, self).__init__(config)\n        self.config = config\n        self.img_gen_by_retrieval_server = img_gen_by_retrieval_server\n        self.img_gen_model = img_gen_model\n        \n    def batch_run(self, query, **kwargs):\n        '''\n        run image generator by retrieval the diffusion\n        '''\n        assert type(query) == list\n\n        # (1) img retrieval\n        retrieval_resp_list = self.img_gen_by_retrieval_server.batch_run(query)\n        \n        # (2) img2img by diffusion\n        for text,item in  zip(query,retrieval_resp_list):\n            local_img_path = item[\"img_local_path\"]\n            img = self.img_gen_model.run(text,local_img_path)\n            # save back \n            img.save(local_img_path)\n        return retrieval_resp_list\n    \n"}
{"type": "source_file", "path": "generator/image/retrieval/build.py", "content": "from generator.image.retrieval.models.build import build_image_query_model\nfrom generator.image.retrieval.server.embed import QueryTextEmbedServer\nfrom generator.image.retrieval.server.knn import FiassKnnServer\nfrom comm.mylog import logger\n\ndef build_QueryTextEmbedServer(cfg):\n    model_name = cfg.video_editor.visual_gen.image_by_retrieval.model\n    device = cfg.video_editor.visual_gen.image_by_retrieval.device\n    \n    model = build_image_query_model(model_name = model_name, device = device)\n    return QueryTextEmbedServer(model)\n\n\n\ndef build_FiassKnnServer(cfg):\n    index_path = cfg.video_editor.visual_gen.image_by_retrieval.index_path\n    return FiassKnnServer(index_path)\n\n\n    \n\n\n"}
{"type": "source_file", "path": "comm/mylog.py", "content": "import logging\ndef build_logger():\n    \n    logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    return logger\n\nlogger = build_logger()\n"}
{"type": "source_file", "path": "generator/comm/meta_sever.py", "content": "import dbm \nimport logging\nlogging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n\n\nclass MetaServerBase(object):\n    def get_meta(self,ids):\n        raise NotImplementedError\n\n\nclass ImgMetaServer(MetaServerBase):\n    def __init__(self,db_path) -> None:\n        \n        logging.info('db_path: {}'.format(db_path))\n        self.db_path = db_path\n        self.db = dbm.open(self.db_path,'r')\n        \n    def get_meta(self,ids):\n        ids = str(ids)\n        url = self.db[ids].decode('utf-8')\n        return url\n    \n    def batch_get_meta(self,ids_list):\n        \n        urls = [self.db[str(idx)].decode('utf-8') for idx in ids_list]\n        return urls \n\n    \nclass VideoMetaServer(MetaServerBase):\n    def __init__(self,db_path) -> None:\n        \n        logging.info('db_path: {}'.format(db_path))\n        self.db_path = db_path\n        self.db = dbm.open(self.db_path,'r')\n        \n    def get_meta(self,ids):\n        ids = str(ids)\n        url = self.db[ids].decode('utf-8')\n        return url\n    \n    def batch_get_meta(self,ids_list):\n        \n        urls = [self.db[str(idx)].decode('utf-8') for idx in ids_list]\n        return urls \n\n        \n        \n    "}
{"type": "source_file", "path": "editor/image_effect.py", "content": "import math\nfrom PIL import Image\n\nimport numpy \ndef zoom_in_effect(clip, zoom_ratio=0.04):\n    def effect(get_frame, t):\n        img = Image.fromarray(get_frame(t))\n        base_size = img.size\n\n        new_size = [\n            math.ceil(img.size[0] * (1 + (zoom_ratio * t))),\n            math.ceil(img.size[1] * (1 + (zoom_ratio * t)))\n        ]\n\n        # The new dimensions must be even.\n        new_size[0] = new_size[0] + (new_size[0] % 2)\n        new_size[1] = new_size[1] + (new_size[1] % 2)\n\n        img = img.resize(new_size, Image.LANCZOS)\n\n        x = math.ceil((new_size[0] - base_size[0]) / 2)\n        y = math.ceil((new_size[1] - base_size[1]) / 2)\n\n        img = img.crop([\n            x, y, new_size[0] - x, new_size[1] - y\n        ]).resize(base_size, Image.LANCZOS)\n\n        result = numpy.array(img)\n        img.close()\n\n        return result\n\n    return clip.fl(effect)"}
{"type": "source_file", "path": "generator/comm/media_generator.py", "content": "import os \nimport hashlib\nclass MediaGeneratorBase(object):\n    '''\n    各种媒体的生成器基类\n    '''\n    def __init__(self,config) -> None:\n        self.config = config\n    def run(self, **kwargs):\n        '''\n        生成器的入口函数\n        '''\n        raise NotImplementedError\n    \n    def get_url_md5(self,url):\n        '''\n        获取url的md5值\n        '''\n        m = hashlib.md5()\n        m.update(url.encode('utf-8'))\n        return m.hexdigest()\n    \n    def get_pil_md5(self,img):\n        md5 = hashlib.md5(img.tobytes()).hexdigest()\n        return md5\n    \n\n    def get_str_md5(self,str_val):\n        '''\n        获取字符串的md5值\n        '''\n        m = hashlib.md5()\n        m.update(str_val.encode('utf-8'))\n        return m.hexdigest()"}
{"type": "source_file", "path": "generator/text/build.py", "content": "# import logging\nfrom comm.mylog import logger\nfrom generator.text.models.toy import EnToyTextGenModel, ZhToyTextGenModel,ZhToyURL2TextModel\nfrom generator.text.models.chatgpt import ChatGPTModel,URL2TextChatGPTModel\n\n\ndef build_text_generator(cfg):\n    text_gen_type = cfg.video_editor.text_gen.type\n    logger.info('text_gen_type: {}'.format(text_gen_type))\n    text_generator = None\n    if text_gen_type == \"EnToyTextGenModel\":\n        text_generator = EnToyTextGenModel()\n    elif text_gen_type == \"ZhToyTextGenModel\":\n        text_generator = ZhToyTextGenModel()\n    elif text_gen_type == \"ZhToyURL2TextModel\":\n        text_generator = ZhToyURL2TextModel()\n        \n    elif text_gen_type == \"ChatGPTModel\":\n        organization = cfg.video_editor.text_gen.organization\n        api_key = cfg.video_editor.text_gen.api_key\n        text_generator = ChatGPTModel(cfg,organization,api_key)\n    elif text_gen_type == \"URL2TextChatGPTModel\":\n        organization = cfg.video_editor.text_gen.organization\n        api_key = cfg.video_editor.text_gen.api_key\n        text_generator = URL2TextChatGPTModel(cfg,organization,api_key)\n        \n    \n    return text_generator\n"}
{"type": "source_file", "path": "generator/image/build.py", "content": "# import logging\nfrom generator.image.retrieval.build import build_QueryTextEmbedServer,build_FiassKnnServer\n# from generator.image.generation.build import build_img_gen_model\nfrom generator.image.image_generator import ImageGenbyRetrieval,ImageGenByDiffusion,ImageGenByRetrievalThenDiffusion\nfrom generator.comm.meta_sever import ImgMetaServer\nfrom generator.image.generation.build import build_img_gen_model,build_img2img_gen_model\nfrom comm.mylog import logger\n# logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n\ndef build_image_generator(cfg):\n    '''\n    所有的build的入参都是cfg对象\n    '''\n    image_generator = None\n    visual_gen_type = cfg.video_editor.visual_gen.type\n    logger.info('visual_gen_type: {}'.format(visual_gen_type))\n    if visual_gen_type == \"image_by_retrieval\":\n        logger.info('start build_QueryTextEmbedServer')\n        query_model = build_QueryTextEmbedServer(cfg)\n        \n        # build faiss index \n        logger.info('start build_FiassKnnServer')\n        index_server = build_FiassKnnServer(cfg)\n\n        # build meta server \n        logger.info('start build_ImgMetaServer')    \n        \n        meta_server = ImgMetaServer(cfg.video_editor.visual_gen.image_by_retrieval.meta_path)\n    \n        image_generator = ImageGenbyRetrieval(cfg,query_model,index_server,meta_server)\n    elif visual_gen_type == \"image_by_diffusion\":\n        logger.info(\"start build_img_gen_model\")\n        img_gen_model = build_img_gen_model(cfg)\n        image_generator = ImageGenByDiffusion(cfg,img_gen_model)\n    elif visual_gen_type == \"image_by_retrieval_then_diffusion\":\n        # build img retrieval generator\n        logger.info('start build_QueryTextEmbedServer')\n        query_model = build_QueryTextEmbedServer(cfg)\n        \n        # build faiss index \n        logger.info('start build_FiassKnnServer')\n        index_server = build_FiassKnnServer(cfg)\n\n        # build meta server \n        logger.info('start build_ImgMetaServer')    \n        \n        meta_server = ImgMetaServer(cfg.video_editor.visual_gen.image_by_retrieval.meta_path)\n    \n        image_retrieval_generator = ImageGenbyRetrieval(cfg,query_model,index_server,meta_server)\n        img2img_model = build_img2img_gen_model(cfg)\n        image_generator = ImageGenByRetrievalThenDiffusion(cfg,image_retrieval_generator,img2img_model)\n    else:\n        raise ValueError('visual_gen_type: {} not support'.format(visual_gen_type))\n    return image_generator"}
{"type": "source_file", "path": "generator/image/retrieval/models/clip_model.py", "content": "import clip\nfrom multilingual_clip import pt_multilingual_clip\nimport transformers\nimport torch\nimport numpy as np\n\n\ndef build_clip_model(model_name = \"Vit-L/14\", device=\"cpu\"):\n    model, preprocess = clip.load(model_name, device = device)\n    return model,preprocess, lambda t: clip.tokenize(t, truncate=True)\n\n\ndef build_mclip_model(model_name = \"M-CLIP/XLM-Roberta-Large-Vit-L-14\", device=\"cpu\"):\n    model = MClip(model_name,device)\n    return model,None,model.get_tokenizer\n\nclass ClipTextEmbed(object):\n    def __init__(self,model_name,device) -> None:\n        self.model_name = model_name\n        self.device = device\n        self.model, self.preprocess, self.tokenizer = build_clip_model(model_name = model_name, device = device)\n        self.model.eval()\n    def get_text_embed(self,text):\n        '''\n        text：list[str]\n        '''\n        assert type(text) == list\n        text = self.tokenizer(text)\n        \n        with torch.no_grad():\n            query_features = self.model.encode_text(text)\n        # norm\n        query_features /= query_features.norm(dim=-1, keepdim=True)\n        query_features = query_features.cpu().to(torch.float32).detach().numpy()\n\n        return query_features\n        \n        \nclass MClipTextEmbed(object):\n    def __init__(self,model_name,device) -> None:\n        self.model_name = model_name\n        self.device = device\n        self.model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name)\n        self.model.eval()\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n    \n\n    \n    def get_text_embed(self,text):\n        '''\n        text：list[str]\n        '''\n        assert type(text) == list\n        with torch.no_grad():\n            embed = self.model.forward(text, self.tokenizer).detach().cpu().numpy()\n        # l2 norm \n        embed = embed / np.linalg.norm(embed, ord=2 ,axis=1, keepdims=True)\n        return embed\n\n\ndef test_mclip():\n\n    model = MClip(\"M-CLIP/XLM-Roberta-Large-Vit-L-14\",\"cpu\")\n    text = [\"hello world\",\"你好\"]\n    embed = model.get_text_embed(text)\n    print(embed.shape)\n\nif __name__ == \"__main__\":\n    test_mclip()"}
{"type": "source_file", "path": "generator/image/retrieval/server/knn.py", "content": "import numpy as np\nimport os\nimport faiss\nimport logging\n\nclass FiassKnnServer(object):\n    def __init__(self,\n                 index_path,\n                 ):\n        # loading faiss index\n        # self.top_k = 10\n        self.nprobe = 1024 \n        self.index_path = index_path\n        \n        self.index = faiss.read_index(index_path)\n        if isinstance(self.index,faiss.swigfaiss.IndexPreTransform):\n            faiss.ParameterSpace().set_index_parameter(self.index, \"nprobe\", self.nprobe)\n        else:\n            self.index.nprobe = self.nprobe\n        \n    def search(self,query_emebed,top_k=50):\n        '''\n        query_emebed: numpy array\n        '''\n        query_emebed = query_emebed.astype('float32')\n        distances, indices = self.index.search(query_emebed, top_k)\n        return  distances, indices\n\n    def batch_search(self,query_list):\n        pass"}
{"type": "source_file", "path": "generator/music/build.py", "content": "from generator.music.toy import ToyBgmGenerator\nfrom comm.mylog import logger\n\ndef build_bgm_generator(cfg):\n    bgm_gen_type = cfg.video_editor.bgm_gen.type\n    logger.info('bgm_gen_type: {}'.format(bgm_gen_type))\n    bgm_generator = None\n    if bgm_gen_type == \"toy\":\n        bgm_generator = ToyBgmGenerator()\n    return bgm_generator\n\n"}
{"type": "source_file", "path": "app/app.py", "content": "import gradio as gr\nimport argparse\nimport sys\nimport os\n# sys.path.append(\"F:\\Workspace\\github\\open-chat-video-editor\")\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom editor.build import build_editor\nfrom configs.config import get_cfg_defaults  # local variable usage pattern, or:\nfrom comm.mylog import logger\n\ndef get_args():\n\n    parser = argparse.ArgumentParser(description='config for open chat editor')\n    parser.add_argument('--cfg', type=str, required=True,help='input cfg file path')\n    parser.add_argument('--func', type=str,default='Text2VideoEditor',help='editor function name')\n    args = parser.parse_args()\n    return args\nif __name__ == \"__main__\":\n    args = get_args()\n    cfg_path = args.cfg\n    if args.func == \"Text2VideoEditor\":\n        logger.info('building Text2VideoEditor')\n        cfg = get_cfg_defaults()\n        # cfg_path = \"configs/video_by_retrieval_text_by_chatgpt_zh.yaml\"\n        cfg.merge_from_file(cfg_path)\n        print(cfg)\n        editor = build_editor(cfg)\n        def run_Text2VideoEditor_logit(input_text, style_text):\n            out_video = \"test.mp4\"\n            out_text,video_out = editor.run(input_text,style_text,out_video)\n            return out_text,video_out\n        def run_Text2VideoEditor_ui():\n            gr.Interface(\n                run_Text2VideoEditor_logit,\n                [gr.inputs.Textbox(placeholder=\"Enter sentence here...\"),  gr.Radio([\"realism style\", \"cartoon style\"], label=\"video style\", info=\"Please select a video style\"),],\n                outputs =  ['text',gr.Video()],\n                title='Text2VideoEditor',\n                allow_flagging=\"never\",\n                \n            ).launch()\n        run_Text2VideoEditor_ui()\n    elif args.func == \"URL2VideoEditor\":\n        logger.info('building Text2VideoEditor')\n        cfg = get_cfg_defaults()\n        # cfg_path = \"configs/video_by_retrieval_text_by_chatgpt_zh.yaml\"\n        cfg.merge_from_file(cfg_path)\n        print(cfg)\n        editor = build_editor(cfg)\n        def run_URL2VideoEditor_logit(input_text, style_text):\n            out_video = \"test.mp4\"\n            out_text,video_out = editor.run(input_text,style_text,out_video)\n            return out_text,video_out\n    \n        def run_URL2VideoEditor_ui():\n            gr.Interface(\n                run_URL2VideoEditor_logit,\n                [gr.inputs.Textbox(placeholder=\"Enter url here...\",label='enter url'),  gr.Radio([\"realism style\", \"cartoon style\"], label=\"video style\", info=\"Please select a video style\"),],\n                outputs =  ['text',gr.Video()],\n                title='URL2VideoEditor',\n                allow_flagging=\"never\",\n                \n            ).launch()\n        run_URL2VideoEditor_ui()\n    \n        \n"}
{"type": "source_file", "path": "generator/comm/__init__.py", "content": ""}
{"type": "source_file", "path": "comm/url_parser.py", "content": "from bs4 import BeautifulSoup\nimport requests\nimport json\n\ndef get_paragraph_texts(url: str):\n    html: str = requests.get(url).text\n    soup = BeautifulSoup(html, \"html.parser\")\n    pes = soup.findAll('p')\n    texts: list[str] = []\n    for e in pes:\n        texts.append(e.get_text())\n    return texts"}
{"type": "source_file", "path": "editor/build.py", "content": "from generator.image.build import build_image_generator\nfrom generator.video.build import build_video_generator\nfrom generator.tts.build import build_tts_generator\nfrom generator.text.build import build_text_generator\nfrom generator.music.build import build_bgm_generator\nfrom editor.chat_editor import Text2VideoEditor\nfrom comm.mylog import logger\n\ndef build_editor(cfg):\n    visual_gen_type = cfg.video_editor.visual_gen.type\n    logger.info('visual_gen_type: {}'.format(visual_gen_type))\n    # image_by_diffusion  video_by_retrieval image_by_retrieval_then_diffusion video_by_diffusion\n    if visual_gen_type in [\"image_by_retrieval\",\"image_by_diffusion\",\"image_by_retrieval_then_diffusion\"]:\n        vision_generator = build_image_generator(cfg)\n    else:\n        vision_generator = build_video_generator(cfg)\n    \n    text_generator = build_text_generator(cfg)\n    audio_generator = build_tts_generator(cfg)\n    bgm_generator = build_bgm_generator(cfg)\n    \n    editor = Text2VideoEditor(cfg,text_generator, vision_generator, audio_generator,bgm_generator)\n    return editor"}
{"type": "source_file", "path": "configs/config.py", "content": "from yacs.config import CfgNode as CN\n_C = CN()\n_C.video_editor = CN()\n_C.video_editor.type = \"Text2Video\"\n\n\n# 视频信息的生成模式\n_C.video_editor.visual_gen = CN()\n_C.video_editor.visual_gen.type = \"image_by_retrieval\" \n# 其他类型: image_by_diffusion  video_by_retrieval image_by_retrieval_then_diffusion video_by_diffusion\n_C.video_editor.visual_gen.image_by_retrieval = CN()\n# query model\n_C.video_editor.visual_gen.image_by_retrieval.model = \"ViT-L/14\" \n_C.video_editor.visual_gen.image_by_retrieval.model_path = \"\"\n_C.video_editor.visual_gen.image_by_retrieval.device = \"cpu\" # index file path\n\n# index\n_C.video_editor.visual_gen.image_by_retrieval.index_path = \"\" # index file path\n\n# meta\n_C.video_editor.visual_gen.image_by_retrieval.meta_path = \"\" # meta file path\n\n\n# video query model \n_C.video_editor.visual_gen.video_by_retrieval = CN()\n_C.video_editor.visual_gen.video_by_retrieval.model = \"ViT-B/32\"\n_C.video_editor.visual_gen.video_by_retrieval.model_path = \"\"\n_C.video_editor.visual_gen.video_by_retrieval.device = \"cpu\"\n\n# video index \n_C.video_editor.visual_gen.video_by_retrieval.index_path = \"\"\n_C.video_editor.visual_gen.video_by_retrieval.meta_path = \"\"\n\n\n# image gen by diffusion\n_C.video_editor.visual_gen.image_by_diffusion = CN()\n_C.video_editor.visual_gen.image_by_diffusion.model_id = \"stabilityai/stable-diffusion-2-1\"\n\n# image_by_retrieval_then_diffusion\n_C.video_editor.visual_gen.image_by_retrieval_then_diffusion = CN()\n\n_C.video_editor.visual_gen.image_by_retrieval_then_diffusion.model_id = \"stabilityai/stable-diffusion-2-1\"\n\n\n\n\n\n# text gen\n_C.video_editor.text_gen = CN()\n_C.video_editor.text_gen.type = \"toy\"\n_C.video_editor.text_gen.organization = \"\"\n_C.video_editor.text_gen.api_key = \"\"\n\n# tts \n_C.video_editor.tts_gen = CN()\n_C.video_editor.tts_gen.model = \"PaddleSpeechTTS\"\n# set am\n_C.video_editor.tts_gen.am = 'fastspeech2_mix'\n_C.video_editor.tts_gen.lang = 'mix'\n\n# subtitle\n_C.video_editor.subtitle = CN()\n_C.video_editor.subtitle.font=\"\"\n\n# bgm \n_C.video_editor.bgm_gen = CN()\n_C.video_editor.bgm_gen.type = \"toy\"\n\n\ndef get_cfg_defaults():\n  \"\"\"Get a yacs CfgNode object with default values for my_project.\"\"\"\n  # Return a clone so that the defaults will not be altered\n  # This is for the \"local variable\" use pattern\n  return _C.clone()"}
{"type": "source_file", "path": "generator/image/retrieval/models/build.py", "content": "from comm.mylog import logger\nfrom generator.image.retrieval.models.clip_model import ClipTextEmbed, MClipTextEmbed\n\ndef build_image_query_model(model_name, device):\n    \n    if model_name == \"ViT-L/14\":\n        model = ClipTextEmbed(model_name = model_name, device = device)\n    elif model_name == \"M-CLIP/XLM-Roberta-Large-Vit-L-14\":\n        model = MClipTextEmbed(model_name = model_name, device = device)\n        \n    return model\n"}
{"type": "source_file", "path": "generator/image/generation/stable_diffusion.py", "content": "\nfrom diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler,StableDiffusionImg2ImgPipeline\nimport torch\nfrom PIL import Image\nclass StableDiffusionImgModel(object):\n    def __init__(self,model_id=\"stabilityai/stable-diffusion-2-1\") -> None:\n        self.model_id = model_id\n        self.pipe = StableDiffusionPipeline.from_pretrained(self.model_id, torch_dtype=torch.float16)\n        self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config)\n        self.pipe = self.pipe.to(\"cuda\")\n    \n    def run(self,prompt):\n        image = self.pipe(prompt).images[0]\n        width, height = image.size\n        new_width = 640\n        new_height = 360\n        left = (width - new_width)/2\n        top = (height - new_height)/2\n        right = (width + new_width)/2\n        bottom = (height + new_height)/2\n        # Crop the center of the image\n        image = image.crop((left, top, right, bottom))\n        \n        return image\n\nclass StableDiffusionImg2ImgModel(object):\n    def __init__(self,model_id=\"stabilityai/stable-diffusion-2-1\") -> None:\n        self.model_id = model_id \n        self.pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n        self.pipe = self.pipe.to(\"cuda\")\n\n    def run(self,prompt,init_image_path):\n        init_image = Image.open(init_image_path).convert('RGB')\n        init_image = init_image.resize((768, 768))\n        image = self.pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5,num_inference_steps=100).images[0]\n        width, height = image.size\n        new_width = 640\n        new_height = 360\n        left = (width - new_width)/2\n        top = (height - new_height)/2\n        right = (width + new_width)/2\n        bottom = (height + new_height)/2\n        # Crop the center of the image\n        image = image.crop((left, top, right, bottom))\n        return image\n"}
{"type": "source_file", "path": "generator/image/retrieval/server/embed.py", "content": "import torch\nimport logging\n\nclass QueryTextEmbedServer(object):\n    '''\n    query text -> embed\n    '''\n    def __init__(self,model):\n        self.model = model\n \n    \n    def get_query_embed(self,query):\n        '''\n        query: str\n        support batch\n        '''\n        query_features = self.model.get_text_embed(query)\n \n\n        return query_features\n    \n    "}
{"type": "source_file", "path": "generator/music/toy.py", "content": "import random\nclass ToyBgmGenerator(object):\n    def __init__(self) -> None:\n        self.bgms = [\n            \"generator/music/12Mornings.mp3\",\n            \"generator/music/AcousticBlues.mp3\",\n            \"generator/music/AllGoodInTheWood.mp3\",\n            \"generator/music/ClapAlong.mp3\",\n        ]\n    \n    def run(self,):\n        local_file = random.choice(self.bgms)\n        resp = {\n            'bgm_local_file':local_file,\n\n        }\n        return resp"}
{"type": "source_file", "path": "generator/image/generation/build.py", "content": "from generator.image.generation.stable_diffusion import StableDiffusionImgModel,StableDiffusionImg2ImgModel\n\ndef build_img_gen_model(cfg):\n    \n    model_id = cfg.video_editor.visual_gen.image_by_diffusion.model_id\n    model = StableDiffusionImgModel(model_id)\n    return model\n\ndef build_img2img_gen_model(cfg):\n    model_id = cfg.video_editor.visual_gen.image_by_retrieval_then_diffusion.model_id\n    model = StableDiffusionImg2ImgModel(model_id)\n    return model"}
{"type": "source_file", "path": "generator/video/build.py", "content": "from generator.video.retrieval.build import build_QueryTextVideoEmbedServer, build_VideoFiassKnnServer\nfrom generator.video.video_generator import VideoGenByRetrieval\nfrom generator.comm.meta_sever import VideoMetaServer\nfrom comm.mylog import logger\n\ndef build_video_generator(cfg):\n    video_generator = None\n    visual_gen_type = cfg.video_editor.visual_gen.type\n    logger.info('visual_gen_type: {}'.format(visual_gen_type))\n    if visual_gen_type == \"video_by_retrieval\":\n        logger.info('start build_QueryTextEmbedServer')\n        query_model = build_QueryTextVideoEmbedServer(cfg)\n        \n        # build faiss index \n        logger.info('start build_VideoFiassKnnServer')\n        index_server = build_VideoFiassKnnServer(cfg)\n        \n        # build meta server \n        logger.info('start build_VideoMetaServer')\n          \n        meta_server = VideoMetaServer(cfg.video_editor.visual_gen.video_by_retrieval.meta_path)\n        \n        video_generator = VideoGenByRetrieval(cfg,query_model,index_server,meta_server)\n    else:\n        raise ValueError('visual_gen_type: {} not support'.format(visual_gen_type))\n    \n    return video_generator"}
{"type": "source_file", "path": "generator/text/models/chatgpt.py", "content": "\nimport openai \nimport os\nimport re\nfrom comm.mylog import logger\nfrom comm.url_parser import get_paragraph_texts\n\n# openai.organization = os.getenv(\"OPENAI_ORG_ID\")\n# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\ndef is_all_chinese(strs):\n    for _char in strs:\n        if not '\\u4e00' <= _char <= '\\u9fa5':\n            return False\n    return True\n\ndef is_contains_chinese(strs):\n    for _char in strs:\n        if '\\u4e00' <= _char <= '\\u9fa5':\n            return True\n    return False\nclass ChatGPTModel(object):\n    def __init__(self,cfg,\n                 organization,\n                 api_key,\n                 ) -> None:\n        self.cfg = cfg\n        openai.organization = organization\n        openai.api_key = api_key\n        # ch_prompt = ''\n    def run(self, input_text):\n        contain_ch = False\n        if is_contains_chinese(input_text):\n            prompt = \"请以{}为内容，生成100字的短视频文案\".format(input_text)\n            contain_ch = True\n        else:\n            prompt = \"Please use {} as the content to generate a 50-word short video copy\".format(input_text)\n        \n        response = openai.Completion.create(\n        model=\"text-davinci-003\",\n        prompt=prompt,\n        max_tokens=400,\n        stream=False,\n        echo=False,)\n        text = response.choices[0].text\n        logger.info(\"chatgpt response: {}\".format(text))\n        text = text.replace('\\n','')\n        \n        # split text \n        sentences = re.split(\"[,|，|！|.|?|!|。]\",text)\n        sentences = [s for s in sentences if len(s) > 0]\n        logger.info('sentences: {}'.format(sentences))\n        out_info = []\n        resp = {}\n        # 生成的文案是中文文案\n        if contain_ch:\n            resp['lang'] = 'zh'\n            for s in sentences:\n                en_s = self._translate(s)\n                info = {\n                    'zh':s,\n                    'en':en_s,\n                }\n                out_info.append(info)\n        # 生成的文案是英文文案\n        else:\n            resp[\"lang\"] = 'en'\n            for s in sentences:\n                info = {\n                    'en':s,\n                }\n                out_info.append(info)\n        resp[\"out_text\"] = out_info\n        return resp\n            \n    \n    def _translate(self,text):\n        prompt = \"将以下句子翻译成英文:\\n\\n\" + text +'\\n\\n1'\n        response = openai.Completion.create(\n        model=\"text-davinci-003\",\n        prompt=prompt,\n        max_tokens=400,\n        stream=False,\n        echo=False,)\n        out_text = response.choices[0].text\n        logger.info('_translate out_text: {}'.format(out_text))\n\n        out_text = out_text.replace('\\n','').replace('. ','')\n        \n        return out_text\n\n\nclass URL2TextChatGPTModel(object):\n    \n    def __init__(self,cfg,\n                 organization,\n                 api_key,\n                 ) -> None:\n        self.cfg = cfg\n        openai.organization = organization\n        openai.api_key = api_key\n    \n    def _ask(self,question):\n        response = openai.Completion.create(\n        model=\"text-davinci-003\",\n        prompt=question,\n        max_tokens=400,\n        stream=False,\n        echo=False,)\n        return response.choices[0].text\n\n    def _translate(self,text):\n        prompt = \"将以下句子翻译成英文:\\n\\n\" + text +'\\n\\n1'\n        response = openai.Completion.create(\n        model=\"text-davinci-003\",\n        prompt=prompt,\n        max_tokens=400,\n        stream=False,\n        echo=False,)\n        out_text = response.choices[0].text\n        logger.info('_translate out_text: {}'.format(out_text))\n\n        out_text = out_text.replace('\\n','').replace('. ','')\n        \n        return out_text\n    def run(self, url):\n        texts = get_paragraph_texts(url)\n        logger.info('url out texts: {}'.format(texts))\n        resp = dict()\n        # resp['']\n        out_texts = []\n        out_info = []\n        for s in texts:\n            # info = {}\n            if len(s) >= 100:\n                prompt = \"请将以下内容：{}\\n 摘要出30字的短视频文案\".format(s)\n                response_text = self._ask(prompt)\n                logger.info('response_text: {}'.format(response_text))\n                out_texts.append(response_text)\n                \n                # final_text.append(response_text)\n            else:\n                out_texts.append(s)\n\n        out_texts = \"\".join(out_texts)\n        logger.info('tmp out_texts: {}'.format(out_texts))\n        prompt = '请根据一下内容{}\\n，生成一个100字左右的不含英文的短视频文案'.format(out_texts)\n        \n        final_text = self._ask(prompt)\n        logger.info('final_text: {}'.format(final_text))\n        \n        sentences = re.split(\"[,|，|！|.|?|!|。]\",final_text)\n        sentences = [s for s in sentences if len(s) > 0]\n        logger.info('sentences: {}'.format(sentences))\n        out_info = []\n        for s in sentences:\n            en_s = self._translate(s)\n            info = {\n                'zh':s,\n                'en':en_s,\n            }\n            out_info.append(info)\n            \n        resp['lang'] = 'zh'\n        resp[\"out_text\"] = out_info\n        return resp\n               \n \n        \n    \n    \n    \n        \n            \n        \n    "}
{"type": "source_file", "path": "generator/tts/build.py", "content": "from generator.tts.tts_generator import TTSGenerator\nfrom generator.tts.paddlespeech_model import PaddleSpeechTTS\nfrom comm.mylog import logger\ndef build_tts_generator(cfg):\n    tts_model = cfg.video_editor.tts_gen.model\n    logger.info('tts_model: {}'.format(tts_model))\n    tts_generator = None\n    if tts_model == \"PaddleSpeechTTS\":\n        model = PaddleSpeechTTS(\n                                lang=cfg.video_editor.tts_gen.lang,\n                                am=cfg.video_editor.tts_gen.am,\n                                )\n        tts_generator = TTSGenerator(cfg,model)\n    \n    else:\n        raise ValueError('tts_model: {} not support'.format(tts_model))\n    return tts_generator\n\n    "}
{"type": "source_file", "path": "generator/video/retrieval/models/clip_model.py", "content": "import clip\nfrom multilingual_clip import pt_multilingual_clip\nimport transformers\nimport torch\nimport numpy as np\n\n\ndef build_clip_model(model_name = \"Vit-L/14\", device=\"cpu\"):\n    model, preprocess = clip.load(model_name, device = device)\n    return model,preprocess, lambda t: clip.tokenize(t, truncate=True)\n\n\ndef build_mclip_model(model_name = \"M-CLIP/XLM-Roberta-Large-Vit-L-14\", device=\"cpu\"):\n    model = MClip(model_name,device)\n    return model,None,model.get_tokenizer\n\nclass ClipTextEmbed(object):\n    def __init__(self,model_name,device) -> None:\n        self.model_name = model_name\n        self.device = device\n        self.model, self.preprocess, self.tokenizer = build_clip_model(model_name = model_name, device = device)\n        self.model.eval()\n    def get_text_embed(self,text):\n        '''\n        text：list[str]\n        '''\n        assert type(text) == list\n        text = self.tokenizer(text)\n        \n        with torch.no_grad():\n            query_features = self.model.encode_text(text)\n        # norm\n        query_features /= query_features.norm(dim=-1, keepdim=True)\n        query_features = query_features.cpu().to(torch.float32).detach().numpy()\n\n        return query_features\n        \n        \nclass MClipTextEmbed(object):\n    def __init__(self,model_name,device) -> None:\n        self.model_name = model_name\n        self.device = device\n        self.model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name)\n        self.model.eval()\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n    \n\n    \n    def get_text_embed(self,text):\n        '''\n        text：list[str]\n        '''\n        assert type(text) == list\n        with torch.no_grad():\n            embed = self.model.forward(text, self.tokenizer).detach().cpu().numpy()\n        # l2 norm \n        embed = embed / np.linalg.norm(embed, ord=2 ,axis=1, keepdims=True)\n        return embed\n\n\ndef test_mclip():\n\n    model = MClip(\"M-CLIP/XLM-Roberta-Large-Vit-L-14\",\"cpu\")\n    text = [\"hello world\",\"你好\"]\n    embed = model.get_text_embed(text)\n    print(embed.shape)\n\nif __name__ == \"__main__\":\n    test_mclip()"}
{"type": "source_file", "path": "generator/tts/tts_generator.py", "content": "from generator.comm.media_generator import MediaGeneratorBase\nimport os\nclass TTSGenerator(MediaGeneratorBase):\n    def __init__(self, config,\n                 tts_model = None,\n                 ) -> None:\n        super().__init__(config)\n        self.tmp_dir = './tmp/tts'\n        self.tts_model = tts_model\n        if not os.path.exists(self.tmp_dir):\n            os.makedirs(self.tmp_dir)\n    def run_tts(self,text):\n        \n        out_path = os.path.join(self.tmp_dir,self.get_str_md5(text)+'.wav')\n        self.tts_model.run_tts(text,out_path)\n        resp = {'audio_path':out_path}\n        return resp \n    \n    def batch_run(self,text_list):\n        '''\n        text_list: [text1,text2,...]\n        return:[{'audio_path':audio_path1},{'audio_path':audio_path2},...]\n        '''\n        assert type(text_list) == list\n        resp = []\n        for text in text_list:\n            resp.append(self.run_tts(text))\n        return resp\n            \n        "}
{"type": "source_file", "path": "generator/video/retrieval/models/build.py", "content": "from comm.mylog import logger\nfrom generator.video.retrieval.models.clip_model import ClipTextEmbed, MClipTextEmbed\n\ndef build_video_query_model(model_name, device):\n    \n    if model_name == \"ViT-B/32\":\n        model = ClipTextEmbed(model_name = model_name, device = device)\n    elif model_name == \"M-CLIP/XLM-Roberta-Large-Vit-L-14\":\n        model = MClipTextEmbed(model_name = model_name, device = device)\n        \n    return model\n"}
{"type": "source_file", "path": "generator/tts/paddlespeech_model.py", "content": "from paddlespeech.cli.tts.infer import TTSExecutor\nfrom comm.mylog import logger\n\nclass PaddleSpeechTTS(object):\n    def __init__(self,\n                 lang='mix',\n                 am='fastspeech2_mix',\n                 ) -> None:\n        self.tts = TTSExecutor()\n        self.am = am\n        self.lang = lang\n        logger.info('building PaddleSpeechTTS, am: {}, lang: {}'.format(am,lang))\n        \n    \n    def run_tts(self,text,out_path):\n        self.tts(text=text,lang=self.lang,am=self.am,output=out_path)\n    "}
{"type": "source_file", "path": "generator/text/models/toy.py", "content": "class EnToyTextGenModel(object):\n    def __init__(self):\n        self.text = [\n            \"A kitten walks on the road\", \"meets a little pig\", \"meets a lamb\"   \n        ]\n\n        # self.text = [\n        #     '一只小猫在马路上','遇到一只小狗','遇到一只小羊'\n        # ]\n        \n    def run(self,input_text):\n        resp = {\n            'lang':'en',\n            'out_text':[]\n        }\n        for t in self.text:\n            resp['out_text'].append({\n                'en':t,\n            })\n        return resp\n\nclass ZhToyTextGenModel(object):\n    def __init__(self):\n        self.text =['小孩子养宠物', '可以更好地提升小孩子的责任感和独立感', '但也要慎重的选择合适的宠物', '因为只有经过一定的训练养成',\n                    '它们才能够成长起 来', '一起玩耍和度过一段欢快的时光', '宠物不仅能够陪伴小孩子渡过寂寞时光', '还能培养小孩子处事冷静、自信以及情感交流和沟通能力', '在养宠物的过程中', '小孩子们可以唤醒和发掘他们被磨练出来的坚毅和耐力',\n                    '能够亲身体验到勤勉 和坚持的重要性']\n        \n        self.en_text =  ['Children have pets.', \"It can better enhance children's sense of responsibility and independence.\", 'But also choose the suitable pet carefully.', 'Only through certain training and cultivation.', 'They can only grow.', 'Have fun and enjoy a pleasant time together.', \n                         'Pets can not only accompany children to pass the lonely time.', 'It can also cultivate children to be calm, confident in their dealings, and with the ability of emotional communication and communication.', 'During the process of keeping pets', \n                         'Children can awaken and tap into the strength and endurance they have forged.', 'Being able to experience the importance of diligence and persistence in person.']\n    def run(self,input_text):\n        resp = {\n            'lang':'zh',\n            'out_text':[]\n        }\n        for en_t,zh_t in zip(self.en_text,self.text):\n            resp['out_text'].append({\n                'en':en_t,\n                'zh':zh_t,\n            })\n        return resp\n    \n    \nclass ZhToyURL2TextModel(object):\n    def __init__(self):\n        self.text =['美国短毛猫', '是一种神奇又魔幻的宠物猫品种', '短毛猫们优雅可爱', '活力无比', \n                    '能拥有多达80多种头毛色彩', '最出名的是银虎斑', '其银色毛发中透着浓厚的黑色斑纹', \n                    '除此之外', '它们还非常温柔', '是非常适合家庭和人类相处的宠物',\n                    '并且平均寿命达15-20年', '这种可爱的猫品种', '正在受到越来越多人的喜爱',\n                    '不妨试试你也来养一只吧']\n        \n        self.en_text =  ['American shorthair cats', 'It is a pet cat breed that is both magical and magical.',\n                         'horthair cats are elegant and adorable.', \n                         'horthair cats have incredible amount of energy.', 'horthair cats can have up to over 80 hair color variations.',\n                         'The most famous one is the Silver Tiger.',\n                         '.There are thick black stripes in shorthair cats’ silver hair.', \n                         'Apart from this2In addition to this.', 'shorthair cats are also very gentle.', \n                         'It is a very suitable pet for families and for humans to interact with.',\n                         '.Life expectancy averages from 15-20 years.', 'This adorable breed of cats.', \n                         'Being increasingly loved by more and more people.', 'Why not give it a try and take care of one?']\n    def run(self,input_text):\n        resp = {\n            'lang':'zh',\n            'out_text':[]\n        }\n        for en_t,zh_t in zip(self.en_text,self.text):\n            resp['out_text'].append({\n                'en':en_t,\n                'zh':zh_t,\n            })\n        return resp"}
{"type": "source_file", "path": "generator/video/retrieval/build.py", "content": "from generator.video.retrieval.models.build import build_video_query_model\nfrom generator.video.retrieval.server.embed import QueryTextEmbedServer\nfrom generator.video.retrieval.server.knn import VideoFiassKnnServer\n\n\ndef build_QueryTextVideoEmbedServer(cfg):\n    model_name = cfg.video_editor.visual_gen.video_by_retrieval.model\n    device = cfg.video_editor.visual_gen.video_by_retrieval.device\n    \n    model = build_video_query_model(model_name = model_name, device = device)\n    return QueryTextEmbedServer(model)\n\n\ndef build_VideoFiassKnnServer(cfg):\n    index_path = cfg.video_editor.visual_gen.video_by_retrieval.index_path\n    return VideoFiassKnnServer(index_path)\n"}
{"type": "source_file", "path": "generator/text/text_generator.py", "content": "\n\n     \n    "}
{"type": "source_file", "path": "generator/video/retrieval/server/embed.py", "content": "import torch\nimport logging\n\nclass QueryTextEmbedServer(object):\n    '''\n    query text -> embed\n    '''\n    def __init__(self,model):\n        self.model = model\n \n    \n    def get_query_embed(self,query):\n        '''\n        query: str\n        support batch\n        '''\n        query_features = self.model.get_text_embed(query)\n \n\n        return query_features\n    \n    "}
{"type": "source_file", "path": "generator/video/retrieval/server/knn.py", "content": "import numpy as np\nimport os\nimport faiss\nimport logging\n\nclass VideoFiassKnnServer(object):\n    def __init__(self,\n                 index_path,\n                 ):\n        # loading faiss index\n        # self.top_k = 10\n        self.nprobe = 2048 \n        self.index_path = index_path\n        \n        self.index = faiss.read_index(index_path)\n        if isinstance(self.index,faiss.swigfaiss.IndexPreTransform):\n            faiss.ParameterSpace().set_index_parameter(self.index, \"nprobe\", self.nprobe)\n        else:\n            logging.info('set nprobe: {}'.format(self.nprobe))\n            self.index.nprobe = self.nprobe\n\n        \n    def search(self,query_emebed,top_k=50):\n        '''\n        query_emebed: numpy array\n        '''\n        query_emebed = query_emebed.astype('float32')\n        distances, indices = self.index.search(query_emebed, top_k)\n        return  distances, indices\n\n    def batch_search(self,query_list):\n        pass"}
{"type": "source_file", "path": "generator/video/video_generator.py", "content": "    \nfrom generator.comm.media_generator import MediaGeneratorBase\nimport urllib\nimport urllib.request\nimport io\nimport traceback\nimport os\nfrom PIL import Image\nfrom typing import List\n# logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\nfrom comm.mylog import logger\n\ndef download_video(url):\n    urllib_request = urllib.request.Request(\n        url,\n        data=None,\n        headers={\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:72.0) Gecko/20100101 Firefox/72.0\"},\n    )\n    with urllib.request.urlopen(urllib_request, timeout=30) as r:\n        video_stream = io.BytesIO(r.read())\n    return video_stream\n\n\nclass VideoGenByRetrieval(MediaGeneratorBase):\n    '''\n    generate video by retrieval\n    '''\n    def __init__(self, config,\n                 query_embed_server,\n                 index_server,\n                 meta_server,\n                 ):\n        super(VideoGenByRetrieval, self).__init__(config)\n        self.config = config\n        self.query_embed_server = query_embed_server\n        self.index_server = index_server\n        self.meta_server = meta_server\n        self.tmp_dir = \"./tmp/video\"\n        self.data_type = \"video\"\n        if not os.path.exists(self.tmp_dir):\n            os.makedirs(self.tmp_dir)\n\n    def batch_run(self, query:List,**kwargs):\n        '''\n        run video generator by retrieval\n        support multi query\n        '''\n        assert type(query) == list\n    \n        # get query embed\n        prompt = 'a picture without text'\n        query = [ val + prompt for val in query]\n        query_embed = self.query_embed_server.get_query_embed(query)\n\n        # knn search, indices: [batch_size, top_k]\n        distances, indices = self.index_server.search(query_embed)\n\n        # get meta \n        resp = []\n        for batch_idx,topk_ids in  enumerate(indices):\n            # one_info = {}\n            # one query topk urls\n            urls = self.meta_server.batch_get_meta(topk_ids) \n            # logging.error('urls: {}'.format(urls))\n            # download one of the topk videos\n            for url_id,url in enumerate(urls):\n                try:\n                    video_stream = download_video(url)\n                    # try to open\n                    url_md5 = self.get_url_md5(url)\n                    video_tmp_name = os.path.join(self.tmp_dir, \"{}_{}_{}.mp4\".format(batch_idx,url_id, url_md5))\n                    logger.info('tmp video name: {}'.format(video_tmp_name))\n                    with open(video_tmp_name, \"wb\") as f:\n                        f.write(video_stream.getbuffer())\n                    one_info = {'url':url,'topk_ids':url_id,'video_local_path':video_tmp_name,'data_type':self.data_type}\n                    resp.append(one_info)\n                    break\n                except Exception as e:\n                    logger.error(e)\n                    logger.error(traceback.format_exc())\n                    \n                    continue\n        return resp\n    "}
