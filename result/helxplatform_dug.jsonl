{"repo_info": {"repo_name": "dug", "repo_owner": "helxplatform", "repo_url": "https://github.com/helxplatform/dug"}}
{"type": "test_file", "path": "tests/integration/mocks/mock_config.py", "content": "from dataclasses import dataclass, field\n\n\n@dataclass\nclass MockConfig:\n\n    # Preprocessor config that will be passed to annotate.Preprocessor constructor\n    preprocessor: dict = field(default_factory=lambda: {\n        \"debreviator\": {\n            \"BMI\": \"body mass index\"\n        },\n        \"stopwords\": [\"the\"]\n    })\n\n\n    # Annotator config that will be passed to annotate.Annotator constructor\n    annotator_type: str = \"monarch\"\n\n    annotator_args: dict = field(\n        default_factory=lambda: {\n            \"monarch\": {\n                \"url\": \"http://annotator.api/?content=\"\n            },\n            \"sapbert\": {\n                \"classification_url\": \"https://med-nemo.apps.renci.org/annotate/\",\n                \"annotator_url\": \"https://med-nemo.apps.renci.org/annotate/\",\n            },\n        }\n    )\n\n    # Normalizer config that will be passed to annotate.Normalizer constructor\n    normalizer: dict = field(default_factory=lambda: {\n        \"url\": \"http://normalizer.api/?curie=\"\n    })\n\n    # Synonym service config that will be passed to annotate.SynonymHelper constructor\n    synonym_service: dict = field(default_factory=lambda: {\n        \"url\": \"http://synonyms.api\"\n    })\n\n    @classmethod\n    def test_from_env(cls):\n        kwargs = {}\n        return cls(**kwargs)"}
{"type": "test_file", "path": "tests/integration/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/integration/conftest.py", "content": "from pathlib import Path\n\nimport json\nimport urllib.parse\nfrom dataclasses import dataclass\nfrom typing import Dict\n\nimport pytest_asyncio\n\nTEST_DATA_DIR = Path(__file__).parent.resolve() / \"data\"\n\n\n@dataclass\nclass MockResponse:\n    text: str\n    status_code: int = 200\n\n    def json(self):\n        return json.loads(self.text)\n\n\nclass MockApiService:\n    def __init__(self, urls: Dict[str, list]):\n        self.urls = urls\n\n    def get(self, url, params: dict = None):\n        if params:\n            qstr = urllib.parse.urlencode(params, quote_via=urllib.parse.quote)\n            url = f\"{url}?{qstr}\"\n\n        text, status_code = self.urls.get(url)\n\n        if text is None:\n            return MockResponse(text=\"{}\", status_code=404)\n        return MockResponse(text, status_code=status_code)\n\n    def post(self, url, params: dict = None, json: dict = {}):\n        if params:\n            qstr = urllib.parse.urlencode(params, quote_via=urllib.parse.quote)\n            url = f\"{url}?{qstr}\"\n        text, status_code = self.urls.get(url)\n\n        if text is None:\n            return MockResponse(text=\"{}\", status_code=404)\n        return MockResponse(text, status_code=status_code)\n\n\n@pytest_asyncio.fixture\ndef monarch_annotator_api():\n    base_url = \"http://annotator.api/?content={query}\"\n\n    def _(keyword):\n        return base_url.format(query=urllib.parse.quote(keyword))\n\n    urls = {\n        _(\"heart attack\"): [\n            json.dumps(\n                {\n                    \"content\": \"heart attack\",\n                    \"spans\": [\n                        {\n                            \"start\": 0,\n                            \"end\": 5,\n                            \"text\": \"heart\",\n                            \"token\": [\n                                {\n                                    \"id\": \"UBERON:0007100\",\n                                    \"category\": [\"anatomical entity\"],\n                                    \"terms\": [\"primary circulatory organ\"],\n                                }\n                            ],\n                        },\n                        {\n                            \"start\": 0,\n                            \"end\": 5,\n                            \"text\": \"heart\",\n                            \"token\": [\n                                {\n                                    \"id\": \"XAO:0000336\",\n                                    \"category\": [],\n                                    \"terms\": [\"heart primordium\"],\n                                }\n                            ],\n                        },\n                    ],\n                }\n            ),\n            200,\n        ],\n    }\n\n    return MockApiService(\n        urls=urls,\n    )\n\n\n@pytest_asyncio.fixture\ndef token_classifier_api():\n    return MockApiService(\n        urls={\n            \"https://med-nemo.apps.renci.org/annotate/\": [\n                json.dumps(\n                    {\n                        \"text\": \"Have you ever had a heart attack?\",\n                        \"denotations\": [\n                            {\n                                \"id\": \"I5-\",\n                                \"span\": {\"begin\": 20, \"end\": 32},\n                                \"obj\": \"biolink:Disease\",\n                                \"text\": \"heart attack\",\n                            }\n                        ],\n                    }\n                ),\n                200,\n            ]\n        }\n    )\n\n\n@pytest_asyncio.fixture\ndef sapbert_annotator_api():\n    return MockApiService(\n        urls={\n            \"https://med-nemo.apps.renci.org/annotate/\": [\n                json.dumps(\n                    [\n                        {\n                            \"name\": \"attack; cardiovascular\",\n                            \"curie\": \"UBERON:0007100\",\n                            \"category\": \"biolink:Disease\",\n                            \"score\": 0.85857231617\n                        },\n                        {\n                            \"name\": \"Angina attack\",\n                            \"curie\": \"XAO:0000336\",\n                            \"category\": \"biolink:Disease\",\n                            \"score\": 0.806502258778\n                        },\n                    ]\n                ),\n                200,\n            ]\n        }\n    )\n\n\n@pytest_asyncio.fixture\ndef normalizer_api():\n    base_url = \"http://normalizer.api/?curie={curie}\"\n\n    def _(curie):\n        return base_url.format(\n            curie=urllib.parse.quote(curie),\n        )\n\n    urls = {\n        _(\"UBERON:0007100\"): [\n            json.dumps(\n                {\n                    \"UBERON:0007100\": {\n                        \"id\": {\n                            \"identifier\": \"UBERON:0007100\",\n                            \"label\": \"primary circulatory organ\",\n                        },\n                        \"equivalent_identifiers\": [\n                            {\n                                \"identifier\": \"UBERON:0007100\",\n                                \"label\": \"primary circulatory organ\",\n                            }\n                        ],\n                        \"type\": [\n                            \"biolink:AnatomicalEntity\",\n                            \"biolink:OrganismalEntity\",\n                            \"biolink:BiologicalEntity\",\n                            \"biolink:NamedThing\",\n                            \"biolink:Entity\",\n                        ],\n                    }\n                },\n            ),\n            200,\n        ],\n    }\n\n    return MockApiService(\n        urls=urls,\n    )\n\n\n@pytest_asyncio.fixture\ndef null_normalizer_api():\n    base_url = \"http://normalizer.api/?curie={curie}\"\n\n    def _(curie):\n        return base_url.format(\n            curie=urllib.parse.quote(curie),\n        )\n\n    urls = {\n        _(\"XAO:0000336\"): [\n            json.dumps(\n                {\"XAO:0000336\": None},\n            ),\n            200,\n        ],\n    }\n\n    return MockApiService(\n        urls=urls,\n    )\n\n\n@pytest_asyncio.fixture\ndef synonym_api():\n    return MockApiService(\n        urls={\n            \"http://synonyms.api\": [\n                json.dumps(\n                    {\n                        \"UBERON:0007100\": {\n                            \"names\": [\n                                \"primary circulatory organ\",\n                                \"dorsal tube\",\n                                \"adult heart\",\n                                \"heart\",\n                            ]\n                        }\n                    }\n                ),\n                200,\n            ]\n        }\n    )\n\n\n@pytest_asyncio.fixture\ndef null_synonym_api():\n    return MockApiService(\n        urls={\"http://synonyms.api\": [json.dumps({\"XAO:0000336\": {\"names\":[]}}), 200]}\n    )\n"}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/unit/test_api.py", "content": "\nimport json\n\nfrom unittest.mock import patch, Mock\n\nimport pytest\npytest.skip(\"skipping as dug.api is no longer present\", allow_module_level=True)\nfrom pytest import mark\nimport pytest_asyncio\n\nfrom dug.api import app, main, DugResource\n\n\n\n@pytest_asyncio.fixture\ndef dug_api_test_client():\n    with app.test_client() as client:\n        yield client\n\n\n@pytest_asyncio.fixture\ndef mock_g_object():\n    with patch('dug.api.dug') as g:\n        yield g\n\n\n@pytest_asyncio.fixture\ndef mock_search_concepts(mock_g_object):\n    mock_g_object().search_concepts.return_value = {'hits': {'hits': [\n        {'_type': '_doc',\n         '_id': 'UBERON:0001638',\n         '_source': {'id': 'UBERON:0001638',\n                     'name': 'vein',\n                     'description': 'some vessels that carry blood.',\n                     'type': 'anatomical entity'\n                     }\n         }\n    ]\n    }}\n\n\n@pytest_asyncio.fixture\ndef mock_search_kg(mock_g_object):\n    mock_g_object().search_kg.return_value = {'hits': {'hits': [\n        {'_type': '_doc', '_id': 'MEDDRA:10047249'}\n    ]}}\n\n\n@pytest_asyncio.fixture\ndef mock_search_variables(mock_g_object):\n    mock_g_object().search_variables.return_value = {'hits': {'hits': [\n        {'_type': '_doc', '_id': 'MEDDRA:10047249'}\n    ]}}\n\n\n@pytest_asyncio.fixture\ndef mock_agg_data_types(mock_g_object):\n    mock_g_object().agg_data_type.return_value = [\"DBGaP\"]\n\n\ndef resp_decode(resp):\n    resp_data = resp.data\n    resp_json = json.loads(resp_data.decode('utf-8'))\n    return resp_json\n\n\n@mark.api\ndef test_dug_search_resource(dug_api_test_client, mock_search_concepts):\n    resp = dug_api_test_client.post('/search',\n                                    json={\"index\": \"concepts_index\", \"query\": \"heart attack\"}\n                                    )\n    resp_json = resp_decode(resp)\n\n    assert resp_json['status'] == 'success'\n    assert len(resp_json['result']) > 0\n    assert resp.status_code == 200\n\n\n@mark.api\ndef test_dug_search_kg_resource(dug_api_test_client, mock_search_kg):\n    resp = dug_api_test_client.post('/search_kg',\n                                    json={\"index\": \"concepts\", \"unique_id\": \"id_001\", \"query\": \"cough\"})\n    resp_json = resp_decode(resp)\n\n    assert resp_json['status'] == 'success'\n    assert len(resp_json['result']) > 0\n    assert resp.status_code == 200\n\n\n@mark.api\ndef test_dug_search_variable_resource(dug_api_test_client, mock_search_variables):\n    resp = dug_api_test_client.post('/search_var',\n                                    json={\"index\": \"concepts\", \"unique_id\": \"id_001\", \"query\": \"cough\"})\n    resp_json = resp_decode(resp)\n\n    assert resp_json['status'] == 'success'\n    assert len(resp_json['result']) > 0\n    assert resp.status_code == 200\n\n\n@mark.api\ndef test_dug_agg_data_type_resource(dug_api_test_client, mock_agg_data_types):\n    resp = dug_api_test_client.post('/agg_data_types',\n                                    json={\"index\": \"concepts\"})\n    resp_json = resp_decode(resp)\n\n    assert resp_json['status'] == 'success'\n    assert len(resp_json['result']) > 0\n    assert resp.status_code == 200\n\n\n@mark.api\ndef test_create_response_raises_exception():\n    error_exception = Mock()\n    error_exception.side_effect = Exception\n    dr = DugResource()\n    resp = dr.create_response(exception=error_exception)\n\n    assert resp[\"status\"] == 'error'\n\n\n@mark.api\n@patch('dug.api.app.run')\ndef test_main(api_app_run):\n    api_app_run.side_effect = \"I am running!\"\n    main([\"-p\", \"8000\", \"-d\"])"}
{"type": "test_file", "path": "tests/unit/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/integration/test_annotators.py", "content": "from copy import copy\nfrom typing import List\nfrom attr import field\n\nimport pytest\nfrom dug.core.annotators.utils.biolink_purl_util import BioLinkPURLerizer\n\n\nfrom tests.integration.mocks.mock_config import MockConfig\nfrom dug.core.annotators import (\n    DugIdentifier,\n    AnnotateMonarch,\n    DefaultNormalizer,\n    DefaultSynonymFinder,\n    AnnotateSapbert,\n)\n\n\ndef test_monarch_annotation_full(\n    monarch_annotator_api,\n    normalizer_api,\n    null_normalizer_api,\n    synonym_api,\n    null_synonym_api,\n):\n    cfg = MockConfig.test_from_env()\n    normalizer = DefaultNormalizer(**cfg.normalizer)\n    synonym_finder = DefaultSynonymFinder(**cfg.synonym_service)\n\n    annotator = AnnotateMonarch(\n        normalizer=normalizer, synonym_finder=synonym_finder, config=cfg, **cfg.annotator_args[\"monarch\"]\n    )\n    input_text = \"heart attack\"\n\n    text = annotator.preprocess_text(input_text)\n\n    # Fetch identifiers\n    raw_identifiers: List[DugIdentifier] = annotator.annotate_text(\n        text, monarch_annotator_api\n    )\n\n    processed_identifiers: List[DugIdentifier] = []\n    for identifier in raw_identifiers:\n        if identifier.id == \"UBERON:0007100\":\n            # Perform normal normalization\n            output = annotator.normalizer(identifier, normalizer_api)\n\n            assert isinstance(output, DugIdentifier)\n            assert output.id == \"UBERON:0007100\"\n            assert output.label == \"primary circulatory organ\"\n            assert output.equivalent_identifiers == [\"UBERON:0007100\"]\n            assert output.types == \"anatomical entity\"\n        else:\n            # act as if this is null\n            output = annotator.normalizer(identifier, null_normalizer_api)\n\n        # Should be returning normalized identifier for each identifier passed in\n        if output is None:\n            output = identifier\n            # Test normalizer when null\n            assert output.id == \"XAO:0000336\"\n            assert output.label == \"heart primordium\"\n\n        # Add synonyms to identifier\n        if identifier.id == \"UBERON:0007100\":\n            output.synonyms = annotator.synonym_finder(output.id, synonym_api)\n            print(output.synonyms)\n            assert output.synonyms == [\n                \"primary circulatory organ\",\n                \"dorsal tube\",\n                \"adult heart\",\n                \"heart\",\n            ]\n        else:\n            output.synonyms = annotator.synonym_finder(output.id, null_synonym_api)\n            assert output.synonyms == []\n        # Get pURL for ontology identifer for more info\n        output.purl = BioLinkPURLerizer.get_curie_purl(output.id)\n        processed_identifiers.append(output)\n\n    assert isinstance(processed_identifiers, List)\n    assert len(processed_identifiers) == 2\n    assert isinstance(processed_identifiers[0], DugIdentifier)\n\n\ndef test_sapbert_annotation_full(\n    token_classifier_api,\n    sapbert_annotator_api,\n    normalizer_api,\n    null_normalizer_api,\n    synonym_api,\n    null_synonym_api,\n):\n    cfg = MockConfig.test_from_env()\n    normalizer = DefaultNormalizer(**cfg.normalizer)\n    synonym_finder = DefaultSynonymFinder(**cfg.synonym_service)\n\n    annotator = AnnotateSapbert(normalizer=normalizer, synonym_finder=synonym_finder, **cfg.annotator_args[\"sapbert\"])\n    input_text = \"Have you ever had a heart attack?\"\n\n    # Fetch Classifiers\n    classifiers: List = annotator.text_classification(input_text, token_classifier_api)\n\n    # Fetch identifiers\n    raw_identifiers: List[DugIdentifier] = annotator.annotate_classifiers(\n        classifiers, sapbert_annotator_api\n    )\n    processed_identifiers: List[DugIdentifier] = []\n    for entity, identifiers in raw_identifiers.items():\n        for identifier in identifiers:\n            if identifier.id == \"UBERON:0007100\":\n                # Perform normal normalization\n                output = annotator.normalizer(identifier, normalizer_api)\n                print(output)\n\n                assert isinstance(output, DugIdentifier)\n                assert output.id == \"UBERON:0007100\"\n                assert output.label == \"primary circulatory organ\"\n                assert output.equivalent_identifiers == [\"UBERON:0007100\"]\n                assert output.types == \"anatomical entity\"\n            else:\n                # act as if this is null\n                output = annotator.normalizer(identifier, null_normalizer_api)\n\n            # Should be returning normalized identifier for each identifier passed in\n            if output is None:\n                output = identifier\n                # Test normalizer when null\n                assert output.id == \"XAO:0000336\"\n                assert output.label == \"Angina attack\"\n\n            # Add synonyms to identifier\n            if identifier.id == \"UBERON:0007100\":\n                output.synonyms = annotator.synonym_finder(output.id, synonym_api)\n                assert output.synonyms == [\n                    \"primary circulatory organ\",\n                    \"dorsal tube\",\n                    \"adult heart\",\n                    \"heart\",\n                ]\n            else:\n                output.synonyms = annotator.synonym_finder(output.id, null_synonym_api)\n                assert output.synonyms == []\n            # Get pURL for ontology identifer for more info\n            output.purl = BioLinkPURLerizer.get_curie_purl(output.id)\n            processed_identifiers.append(output)\n\n    assert isinstance(processed_identifiers, List)\n    assert len(processed_identifiers) == 2\n    assert isinstance(processed_identifiers[0], DugIdentifier)\n"}
{"type": "test_file", "path": "tests/unit/test_config.py", "content": "import os\nfrom unittest import mock\n\nfrom dug.config import Config\n\n\n@mock.patch.dict(os.environ, {\n    \"ELASTIC_PASSWORD\": \"ohwhoa\",\n    \"REDIS_PASSWORD\": \"thatsprettyneat\"\n})\ndef test_config_created_from_env_vars():\n    cfg = Config.from_env()\n\n    assert cfg.elastic_password == \"ohwhoa\"\n    assert cfg.redis_password == \"thatsprettyneat\"\n"}
{"type": "test_file", "path": "tests/unit/mocks/MockCrawler.py", "content": "from unittest.mock import MagicMock, Mock\n\nimport pytest\nimport os\nimport json\n\n\nfrom dug.core.annotators import DugIdentifier\nfrom dug.core.tranql import QueryFactory, QueryKG\n\n# Makes some simple mokes\nParserMock = MagicMock()\nHTTPSessionMock = MagicMock()\n\n# mocking tranql queries\nTranqlQueriesMock = {}\nfor key, query in {\n    \"disease\": [\"disease\", \"phenotypic_feature\"],\n    \"pheno\": [\"phenotypic_feature\", \"disease\"]\n}.items():\n    TranqlQueriesMock[key] = QueryFactory(query, source=\"test\")\n\n\n# for testing no id exclusion\nExcludedIDs = []\n\nANNOTATED_IDS = [\n    DugIdentifier(\"MONDO:0\", \"0\", [\"disease\"]),\n    DugIdentifier(\"PUBCHEM.COMPOUND:1\", \"1\", [\"chemical\"])\n    ]\nfor ids in ANNOTATED_IDS:\n    ids.type = ids.types[0]\n# annotator with annotate method returning mocked concepts\nAnnotatorMock = MagicMock()\nAnnotatorMock = Mock(return_value=ANNOTATED_IDS)\n\n# tranqlizer returning mock kg when expanding concepts\nTranqlizerMock = MagicMock()\n\n# Get example tranql answer\nwith open(os.path.join(os.path.dirname(__file__), \"data\", \"tranql_response.json\")) as stream:\n    tranql_json = json.load(stream)\n    kg_answer = QueryKG(kg_json=tranql_json)\n    TRANQL_ANSWERS = []\n    for answer in kg_answer.answers:\n        TRANQL_ANSWERS.append(kg_answer.get_answer_subgraph(answer))\n\nTranqlizerMock.expand_identifier = Mock(return_value=TRANQL_ANSWERS)\n\n#mock a crawler with mock dependencies\n@pytest.fixture\ndef crawler_init_args_no_graph_extraction():\n    return {\n        \"crawl_file\": \"test\",\n        \"parser\": ParserMock,\n        \"annotator\": AnnotatorMock,\n        \"tranqlizer\": TranqlizerMock,\n        \"tranql_queries\": TranqlQueriesMock,\n        \"http_session\": HTTPSessionMock,\n        \"exclude_identifiers\": ExcludedIDs,\n        \"element_type\": \"TestElement\",\n        \"element_extraction\": None\n    }\n"}
{"type": "test_file", "path": "tests/unit/mocks/data/mock_config.py", "content": "from dataclasses import dataclass, field\n\n\n@dataclass\nclass MockConfig:\n\n    # Preprocessor config that will be passed to annotate.Preprocessor constructor\n    preprocessor: dict = field(default_factory=lambda: {\n        \"debreviator\": {\n            \"BMI\": \"body mass index\"\n        },\n        \"stopwords\": [\"the\"]\n    })\n\n    # Annotator config that will be passed to annotate.Annotator constructor\n    annotator_type: str = \"monarch\"\n\n    annotator_args: dict = field(\n        default_factory=lambda: {\n            \"monarch\": {\n                \"url\": \"http://annotator.api/?content=\"\n            },\n            \"sapbert\": {\n                \"classification_url\": \"http://classifier.api/annotate/\",\n                \"annotator_url\": \"http://entity-link.api/annotate/\",\n            },\n        }\n    )\n\n    # Normalizer config that will be passed to annotate.Normalizer constructor\n    normalizer: dict = field(default_factory=lambda: {\n        \"url\": \"http://normalizer.api/?curie=\"\n    })\n\n    # Synonym service config that will be passed to annotate.SynonymHelper constructor\n    synonym_service: dict = field(default_factory=lambda: {\n        \"url\": \"http://synonyms.api\"\n    })\n\n    @classmethod\n    def test_from_env(cls):\n        kwargs = {}\n        return cls(**kwargs)"}
{"type": "test_file", "path": "tests/unit/test_async_search.py", "content": "\"Unit tests for the async_search module\"\n\nimport asyncio\nimport json\nfrom importlib import reload\nfrom unittest import TestCase, mock\nfrom fastapi.testclient import TestClient\n\nfrom dug.core import async_search\nfrom dug.config import Config\n\nasync def _mock_search(*args, **kwargs):\n    \"Mock of elasticsearch search function. Ignores argument\"\n    return _brain_search_result()\n\nasync def _mock_count(*args, **kwargs):\n    \"Mock of elasticsearch count function. Ignores argument\"\n    return {'count': 90, '_shards': {'total': 1, 'successful': 1,\n                                     'skipped': 0, 'failed': 0}}\nes_mock = mock.AsyncMock()\nes_mock.search = _mock_search\nes_mock.count = _mock_count\n\nclass SearchTestCase(TestCase):\n    \"Mocked unit tests for async_search\"\n\n    def setUp(self):\n        \"Build mock elasticsearch responses\"\n        search_result = _brain_search_result()\n        self.search = async_search.Search(Config.from_env())\n        self.query_body = self.search._get_concepts_query(\"brain\")\n        self.search.es = es_mock\n\n    def test_concepts_search(self):\n        \"Test async_search concepts search\"\n        result = asyncio.run(\n            self.search.search_concepts(\"brain\"))\n        self.assertIn('total_items', result)\n        self.assertEqual(result['total_items'], 90)\n        self.assertIn('concept_types', result)\n        self.assertIsInstance(result['concept_types'], dict)\n        self.assertEqual(len(result['concept_types']), 9)\n        self.assertEqual(result['concept_types']['anatomical entity'], 10)\n\n\nbrain_result_json = \"\"\"{\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_type\": \"_doc\",\n        \"_id\": \"MONDO:0005560\",\n        \"_score\": 274.8391,\n        \"_source\": {\n          \"id\": \"MONDO:0005560\",\n          \"name\": \"brain disease\",\n          \"description\": \"A disease affecting the brain or part of the brain.\",\n          \"type\": \"disease\",\n          \"search_terms\": [\n            \"brain disease\",\n            \"disorder of brain\",\n            \"disease of brain\",\n            \"disease or disorder of brain\",\n            \"brain disease or disorder\"\n          ],\n          \"optional_terms\": [\n            \"alcohol use disorder measurement\",\n            \"GBA carrier status\",\n            \"systemising measurement\",\n            \"Abnormal nervous system physiology\",\n            \"Hypoglycemic encephalopathy\",\n            \"Nervous System Part\",\n            \"linguistic error measurement\",\n            \"Brain abscess\",\n            \"anatomical entity\",\n            \"Phenotypic abnormality\",\n            \"time to first cigarette measurement\",\n            \"Progressive encephalopathy\",\n            \"Epileptic encephalopathy\",\n            \"Necrotizing encephalopathy\",\n            \"Recurrent encephalopathy\",\n            \"alcohol dependence measurement\",\n            \"brain disease\",\n            \"cognitive inhibition measurement\",\n            \"Mitochondrial encephalopathy\",\n            \"Chronic hepatic encephalopathy\",\n            \"cocaine use measurement\",\n            \"Nonprogressive encephalopathy\",\n            \"Profound static encephalopathy\",\n            \"Brain\",\n            \"Acute encephalopathy\",\n            \"ADHD symptom measurement\",\n            \"cannabis dependence measurement\",\n            \"Infantile encephalopathy\",\n            \"opioid overdose severity measurement\",\n            \"delayed reward discounting measurement\",\n            \"attention function measurement\",\n            \"Herpes simplex encephalitis\",\n            \"Abnormality of neuronal migration\",\n            \"Acute necrotizing encephalopathy\",\n            \"Congenital encephalopathy\",\n            \"vascular brain injury measurement\",\n            \"Primary microcephaly\",\n            \"Central Nervous System Part\",\n            \"executive function measurement\",\n            \"syntactic complexity measurement\"\n          ],\n          \"concept_action\": \"\",\n          \"identifiers\": [\n            {\n              \"id\": \"MONDO:0005560\",\n              \"label\": \"brain disease\",\n              \"equivalent_identifiers\": [\n                \"MONDO:0005560\",\n                \"DOID:936\",\n                \"UMLS:C0006111\",\n                \"UMLS:C0085584\",\n                \"MESH:D001927\",\n                \"MEDDRA:10006120\",\n                \"MEDDRA:10014623\",\n                \"MEDDRA:10014625\",\n                \"MEDDRA:10014636\",\n                \"MEDDRA:10014641\",\n                \"NCIT:C26920\",\n                \"NCIT:C96413\",\n                \"SNOMEDCT:81308009\",\n                \"ICD10:G93.40\",\n                \"ICD10:G93.9\",\n                \"ICD9:348.30\",\n                \"ICD9:348.9\",\n                \"HP:0001298\"\n              ],\n              \"type\": [\n                \"biolink:Disease\",\n                \"biolink:DiseaseOrPhenotypicFeature\",\n                \"biolink:BiologicalEntity\",\n                \"biolink:NamedThing\",\n                \"biolink:Entity\",\n                \"biolink:ThingWithTaxon\"\n              ],\n              \"synonyms\": [\n                \"brain disease\",\n                \"brain disease or disorder\",\n                \"disease of brain\",\n                \"disease or disorder of brain\",\n                \"disorder of brain\"\n              ]\n            }\n          ]\n        }\n      },\n      {\n        \"_type\": \"_doc\",\n        \"_id\": \"MONDO:0005394\",\n        \"_score\": 253.45584,\n        \"_source\": {\n          \"id\": \"MONDO:0005394\",\n          \"name\": \"brain infarction\",\n          \"description\": \"Tissue necrosis in any area of the brain, including the cerebral hemispheres, the cerebellum, and the brain stem. Brain infarction is the result of a cascade of events initiated by inadequate blood flow through the brain that is followed by hypoxia and hypoglycemia in brain tissue. Damage may be temporary, permanent, selective or pan-necrosis.\",\n          \"type\": \"disease\",\n          \"search_terms\": [\n            \"BRAIN INFARCTION\"\n          ],\n          \"optional_terms\": [\n            \"blood vasculature\",\n            \"brain infarction\"\n          ],\n          \"concept_action\": \"\",\n          \"identifiers\": [\n            {\n              \"id\": \"MONDO:0005394\",\n              \"label\": \"brain infarction\",\n              \"equivalent_identifiers\": [\n                \"MONDO:0005394\",\n                \"DOID:3454\",\n                \"UMLS:C0751955\",\n                \"MESH:D020520\",\n                \"MEDDRA:10072154\"\n              ],\n              \"type\": [\n                \"biolink:Disease\",\n                \"biolink:DiseaseOrPhenotypicFeature\",\n                \"biolink:BiologicalEntity\",\n                \"biolink:NamedThing\",\n                \"biolink:Entity\",\n                \"biolink:ThingWithTaxon\"\n              ],\n              \"synonyms\": []\n            }\n          ]\n        }\n      },\n      {\n        \"_type\": \"_doc\",\n        \"_id\": \"UBERON:0000955\",\n        \"_score\": 252.57217,\n        \"_source\": {\n          \"id\": \"UBERON:0000955\",\n          \"name\": \"brain\",\n          \"description\": \"The brain is the center of the nervous system in all vertebrate, and most invertebrate, animals. Some primitive animals such as jellyfish and starfish have a decentralized nervous system without a brain, while sponges lack any nervous system at all. In vertebrates, the brain is located in the head, protected by the skull and close to the primary sensory apparatus of vision, hearing, balance, taste, and smell[WP].\",\n          \"type\": \"anatomical entity\",\n          \"search_terms\": [\n            \"the brain\",\n            \"suprasegmental levels of nervous system\",\n            \"brain\",\n            \"suprasegmental structures\",\n            \"synganglion\",\n            \"encephalon\"\n          ],\n          \"optional_terms\": [],\n          \"concept_action\": \"\",\n          \"identifiers\": [\n            {\n              \"id\": \"UBERON:0000955\",\n              \"label\": \"brain\",\n              \"equivalent_identifiers\": [\n                \"UBERON:0000955\"\n              ],\n              \"type\": [\n                \"biolink:GrossAnatomicalStructure\",\n                \"biolink:AnatomicalEntity\",\n                \"biolink:OrganismalEntity\",\n                \"biolink:BiologicalEntity\",\n                \"biolink:NamedThing\",\n                \"biolink:Entity\",\n                \"biolink:ThingWithTaxon\",\n                \"biolink:PhysicalEssence\",\n                \"biolink:PhysicalEssenceOrOccurrent\"\n              ],\n              \"synonyms\": [\n                \"encephalon\",\n                \"suprasegmental levels of nervous system\",\n                \"suprasegmental structures\",\n                \"synganglion\",\n                \"the brain\"\n              ]\n            }\n          ]\n        }\n      },\n      {\n        \"_type\": \"_doc\",\n        \"_id\": \"MONDO:0017998\",\n        \"_score\": 136.03186,\n        \"_source\": {\n          \"id\": \"MONDO:0017998\",\n          \"name\": \"PLA2G6-associated neurodegeneration\",\n          \"description\": \"Any neurodegeneration with brain iron accumulation in which the cause of the disease is a mutation in the PLA2G6 gene.\",\n          \"type\": \"disease\",\n          \"search_terms\": [\n            \"plans\",\n            \"neurodegeneration with brain iron accumulation caused by mutation in PLA2G6\",\n            \"PLA2G6 neurodegeneration with brain iron accumulation\",\n            \"PLAN\"\n          ],\n          \"optional_terms\": [],\n          \"concept_action\": \"\",\n          \"identifiers\": [\n            {\n              \"id\": \"MONDO:0017998\",\n              \"label\": \"PLA2G6-associated neurodegeneration\",\n              \"equivalent_identifiers\": [\n                \"MONDO:0017998\",\n                \"ORPHANET:329303\"\n              ],\n              \"type\": [\n                \"biolink:Disease\",\n                \"biolink:DiseaseOrPhenotypicFeature\",\n                \"biolink:BiologicalEntity\",\n                \"biolink:NamedThing\",\n                \"biolink:Entity\",\n                \"biolink:ThingWithTaxon\"\n              ],\n              \"synonyms\": [\n                \"neurodegeneration with brain iron accumulation caused by mutation in PLA2G6\",\n                \"PLA2G6 neurodegeneration with brain iron accumulation\",\n                \"PLAN\"\n              ]\n            }\n          ]\n        }\n      },\n      {\n        \"_type\": \"_doc\",\n        \"_id\": \"MONDO:0002679\",\n        \"_score\": 128.80138,\n        \"_source\": {\n          \"id\": \"MONDO:0002679\",\n          \"name\": \"cerebral infarction\",\n          \"description\": \"An ischemic condition of the brain, producing a persistent focal neurological deficit in the area of distribution of the cerebral arteries.\",\n          \"type\": \"disease\",\n          \"search_terms\": [\n            \"cerebral infarct\",\n            \"infarction, cerebral\",\n            \"cerebral infarction\",\n            \"CVA - cerebral infarction\",\n            \"cerebral ischemia\",\n            \"brain infarction of telencephalon\",\n            \"telencephalon brain infarction\",\n            \"cerebral, infarction\"\n          ],\n          \"optional_terms\": [\n            \"Abnormal nervous system morphology\",\n            \"structure with developmental contribution from neural crest\",\n            \"stroke outcome severity measurement\",\n            \"brain infarction of telencephalon\",\n            \"Phenotypic abnormality\",\n            \"cerebral, infarction\",\n            \"cerebral infarct\",\n            \"Abnormal arterial physiology\",\n            \"CVA - cerebral infarction\",\n            \"telencephalon brain infarction\",\n            \"Abnormality of brain morphology\",\n            \"Tissue ischemia\",\n            \"cerebral infarction\",\n            \"Pontine ischemic lacunes\",\n            \"cerebral ischemia\",\n            \"Abnormal vascular morphology\",\n            \"Lacunar stroke\",\n            \"Abnormality of the vasculature\",\n            \"Abnormal cerebral vascular morphology\",\n            \"Abnormal vascular physiology\",\n            \"infarction, cerebral\",\n            \"Abnormal cardiovascular system physiology\",\n            \"Abnormality of cardiovascular system morphology\"\n          ],\n          \"concept_action\": \"\",\n          \"identifiers\": [\n            {\n              \"id\": \"MONDO:0002679\",\n              \"label\": \"cerebral infarction\",\n              \"equivalent_identifiers\": [\n                \"MONDO:0002679\",\n                \"DOID:3526\",\n                \"OMIM:601367\",\n                \"UMLS:C0007785\",\n                \"UMLS:C0948008\",\n                \"MESH:D000083242\",\n                \"MESH:D002544\",\n                \"MEDDRA:10008117\",\n                \"MEDDRA:10008118\",\n                \"MEDDRA:10021755\",\n                \"MEDDRA:10023027\",\n                \"MEDDRA:10055221\",\n                \"MEDDRA:10061256\",\n                \"NCIT:C50486\",\n                \"NCIT:C95802\",\n                \"SNOMEDCT:422504002\",\n                \"SNOMEDCT:432504007\",\n                \"ICD10:I63\",\n                \"HP:0002140\"\n              ],\n              \"type\": [\n                \"biolink:Disease\",\n                \"biolink:DiseaseOrPhenotypicFeature\",\n                \"biolink:BiologicalEntity\",\n                \"biolink:NamedThing\",\n                \"biolink:Entity\",\n                \"biolink:ThingWithTaxon\"\n              ],\n              \"synonyms\": [\n                \"brain infarction of telencephalon\",\n                \"cerebral infarct\",\n                \"cerebral infarction\",\n                \"cerebral ischemia\",\n                \"cerebral, infarction\",\n                \"CVA - cerebral infarction\",\n                \"infarction, cerebral\",\n                \"telencephalon brain infarction\"\n              ]\n            }\n          ]\n        }\n      },\n      {\n        \"_type\": \"_doc\",\n        \"_id\": \"UBERON:6110636\",\n        \"_score\": 120.47298,\n        \"_source\": {\n          \"id\": \"UBERON:6110636\",\n          \"name\": \"insect adult cerebral ganglion\",\n          \"description\": \"The pre-oral neuropils of the adult brain located above, around and partially below the esophagus, including the optic lobes. It excludes the gnathal ganglion. Developmentally, it comprises three fused neuromeres: protocerebrum, deutocerebrum, and tritocerebrum.\",\n          \"type\": \"anatomical entity\",\n          \"search_terms\": [\n            \"supraesophageal ganglion\",\n            \"SPG\",\n            \"cerebrum\",\n            \"brain\",\n            \"CRG\"\n          ],\n          \"optional_terms\": [],\n          \"concept_action\": \"\",\n          \"identifiers\": [\n            {\n              \"id\": \"UBERON:6110636\",\n              \"label\": \"insect adult cerebral ganglion\",\n              \"equivalent_identifiers\": [\n                \"UBERON:6110636\"\n              ],\n              \"type\": [\n                \"biolink:GrossAnatomicalStructure\",\n                \"biolink:AnatomicalEntity\",\n                \"biolink:OrganismalEntity\",\n                \"biolink:BiologicalEntity\",\n                \"biolink:NamedThing\",\n                \"biolink:Entity\",\n                \"biolink:ThingWithTaxon\",\n                \"biolink:PhysicalEssence\",\n                \"biolink:PhysicalEssenceOrOccurrent\"\n              ],\n              \"synonyms\": [\n                \"CRG\",\n                \"SPG\",\n                \"brain\",\n                \"cerebrum\",\n                \"supraesophageal ganglion\"\n              ]\n            }\n          ]\n        }\n      },\n      {\n        \"_type\": \"_doc\",\n        \"_id\": \"MONDO:0045057\",\n        \"_score\": 115.8625,\n        \"_source\": {\n          \"id\": \"MONDO:0045057\",\n          \"name\": \"delirium\",\n          \"description\": \"A disorder characterized by confusion; inattentiveness; disorientation; illusions; hallucinations; agitation; and in some instances autonomic nervous system overactivity. It may result from toxic/metabolic conditions or structural brain lesions. (From Adams et al., Principles of Neurology, 6th ed, pp411-2)\",\n          \"type\": \"disease\",\n          \"search_terms\": [\n            \"delirium\",\n            \"OBS syndrome\",\n            \"organic brain syndrome\"\n          ],\n          \"optional_terms\": [\n            \"Confusion\",\n            \"Abnormality of higher mental function\",\n            \"Abnormal nervous system physiology\",\n            \"delirium\",\n            \"Reduced consciousness/confusion\",\n            \"Phenotypic abnormality\"\n          ],\n          \"concept_action\": \"\",\n          \"identifiers\": [\n            {\n              \"id\": \"MONDO:0045057\",\n              \"label\": \"delirium\",\n              \"equivalent_identifiers\": [\n                \"MONDO:0045057\",\n                \"UMLS:C0011206\",\n                \"UMLS:C0029221\",\n                \"UMLS:C1285577\",\n                \"UMLS:C1306588\",\n                \"MESH:D003693\",\n                \"MEDDRA:10000685\",\n                \"MEDDRA:10000693\",\n                \"MEDDRA:10000694\",\n                \"MEDDRA:10000702\",\n                \"MEDDRA:10006150\",\n                \"MEDDRA:10012217\",\n                \"MEDDRA:10012218\",\n                \"MEDDRA:10012219\",\n                \"MEDDRA:10031077\",\n                \"MEDDRA:10042790\",\n                \"NCIT:C2981\",\n                \"NCIT:C34868\",\n                \"SNOMEDCT:130987000\",\n                \"SNOMEDCT:2776000\",\n                \"SNOMEDCT:419567006\",\n                \"HP:0031258\"\n              ],\n              \"type\": [\n                \"biolink:Disease\",\n                \"biolink:DiseaseOrPhenotypicFeature\",\n                \"biolink:BiologicalEntity\",\n                \"biolink:NamedThing\",\n                \"biolink:Entity\",\n                \"biolink:ThingWithTaxon\"\n              ],\n              \"synonyms\": [\n                \"OBS syndrome\",\n                \"organic brain syndrome\"\n              ]\n            }\n          ]\n        }\n      },\n      {\n        \"_type\": \"_doc\",\n        \"_id\": \"UBERON:0002298\",\n        \"_score\": 90.43253,\n        \"_source\": {\n          \"id\": \"UBERON:0002298\",\n          \"name\": \"brainstem\",\n          \"description\": \"Stalk-like part of the brain that includes amongst its parts the medulla oblongata of the hindbrain and the tegmentum of the midbrain[ZFA,MP,generalized].\",\n          \"type\": \"anatomical entity\",\n          \"search_terms\": [\n            \"truncus encephalicus\",\n            \"truncus encephali\",\n            \"lamella pallidi incompleta\",\n            \"accessory medullary lamina of pallidum\",\n            \"lamina pallidi incompleta\",\n            \"lamina medullaris incompleta pallidi\",\n            \"brain stem\",\n            \"brainstem\",\n            \"lamina medullaris accessoria\"\n          ],\n          \"optional_terms\": [],\n          \"concept_action\": \"\",\n          \"identifiers\": [\n            {\n              \"id\": \"UBERON:0002298\",\n              \"label\": \"brainstem\",\n              \"equivalent_identifiers\": [\n                \"UBERON:0002298\",\n                \"UMLS:C0006121\",\n                \"NCIT:C12441\"\n              ],\n              \"type\": [\n                \"biolink:GrossAnatomicalStructure\",\n                \"biolink:AnatomicalEntity\",\n                \"biolink:OrganismalEntity\",\n                \"biolink:BiologicalEntity\",\n                \"biolink:NamedThing\",\n                \"biolink:Entity\",\n                \"biolink:ThingWithTaxon\",\n                \"biolink:PhysicalEssence\",\n                \"biolink:PhysicalEssenceOrOccurrent\"\n              ],\n              \"synonyms\": [\n                \"brain stem\",\n                \"truncus encephali\",\n                \"accessory medullary lamina of pallidum\",\n                \"lamella pallidi incompleta\",\n                \"lamina medullaris accessoria\",\n                \"lamina medullaris incompleta pallidi\",\n                \"lamina pallidi incompleta\",\n                \"truncus encephalicus\"\n              ]\n            }\n          ]\n        }\n      },\n      {\n        \"_type\": \"_doc\",\n        \"_id\": \"UBERON:0001894\",\n        \"_score\": 73.00175,\n        \"_source\": {\n          \"id\": \"UBERON:0001894\",\n          \"name\": \"diencephalon\",\n          \"description\": \"The division of the forebrain that develops from the foremost primary cerebral vesicle.\",\n          \"type\": \"anatomical entity\",\n          \"search_terms\": [\n            \"mature diencephalon\",\n            \"thalamencephalon\",\n            \"between brain\",\n            \"interbrain\",\n            \"betweenbrain\",\n            \"diencephalon\",\n            \"died.\"\n          ],\n          \"optional_terms\": [],\n          \"concept_action\": \"\",\n          \"identifiers\": [\n            {\n              \"id\": \"UBERON:0001894\",\n              \"label\": \"diencephalon\",\n              \"equivalent_identifiers\": [\n                \"UBERON:0001894\",\n                \"UMLS:C0012144\"\n              ],\n              \"type\": [\n                \"biolink:GrossAnatomicalStructure\",\n                \"biolink:AnatomicalEntity\",\n                \"biolink:OrganismalEntity\",\n                \"biolink:BiologicalEntity\",\n                \"biolink:NamedThing\",\n                \"biolink:Entity\",\n                \"biolink:ThingWithTaxon\",\n                \"biolink:PhysicalEssence\",\n                \"biolink:PhysicalEssenceOrOccurrent\"\n              ],\n              \"synonyms\": [\n                \"between brain\",\n                \"interbrain\",\n                \"mature diencephalon\",\n                \"thalamencephalon\",\n                \"diencephalon\",\n                \"betweenbrain\"\n              ]\n            }\n          ]\n        }\n      },\n      {\n        \"_type\": \"_doc\",\n        \"_id\": \"MONDO:0013792\",\n        \"_score\": 69.71182,\n        \"_source\": {\n          \"id\": \"MONDO:0013792\",\n          \"name\": \"intracerebral hemorrhage\",\n          \"description\": \"Bleeding into one or both cerebral hemispheres including the basal ganglia and the cerebral cortex. It is often associated with hypertension and craniocerebral trauma.\",\n          \"type\": \"disease\",\n          \"search_terms\": [\n            \"stroke, hemorrhagic\",\n            \"'bleeding in brain'\",\n            \"hemorrhage, intracerebral, susceptibility to\",\n            \"ich\",\n            \"stroke, hemorrhagic, susceptibility to\"\n          ],\n          \"optional_terms\": [\n            \"Abnormality of the musculoskeletal system\",\n            \"Abnormal nervous system morphology\",\n            \"Abnormality of head or neck\",\n            \"Recurrent cerebral hemorrhage\",\n            \"Intraventricular hemorrhage\",\n            \"Phenotypic abnormality\",\n            \"Grade I preterm intraventricular hemorrhage\",\n            \"stroke, hemorrhagic, susceptibility to\",\n            \"Hemorrhage\",\n            \"Intraventricular Hemorrhage Related to Birth\",\n            \"intracerebral hemorrhage\",\n            \"Antenatal intracerebral hemorrhage\",\n            \"Periventricular Hemorrhage of the Newborn\",\n            \"stroke, hemorrhagic\",\n            \"ich\",\n            \"Abnormal bleeding\",\n            \"Abnormality of brain morphology\",\n            \"Internal hemorrhage\",\n            \"Cerebral Hemorrhage Related to Birth\",\n            \"Finding\",\n            \"Abnormality of the skeletal system\",\n            \"Intraparenchymal Hemorrhage of the Newborn\",\n            \"Abnormal vascular morphology\",\n            \"Abnormality of the vasculature\",\n            \"Abnormal cerebral vascular morphology\",\n            \"Finding by Cause\",\n            \"Abnormality of the head\",\n            \"Abnormality of cardiovascular system morphology\",\n            \"Intraventricular Hemorrhage with Parenchymal Hemorrhage of the Newborn\",\n            \"Abnormal cardiovascular system physiology\",\n            \"Abnormality of blood circulation\",\n            \"hemorrhage, intracerebral, susceptibility to\"\n          ],\n          \"concept_action\": \"\",\n          \"identifiers\": [\n            {\n              \"id\": \"MONDO:0013792\",\n              \"label\": \"intracerebral hemorrhage\",\n              \"equivalent_identifiers\": [\n                \"MONDO:0013792\",\n                \"OMIM:614519\",\n                \"UMLS:C0472369\",\n                \"UMLS:C0553692\",\n                \"UMLS:C1862876\",\n                \"UMLS:C2937358\",\n                \"UMLS:C3281105\",\n                \"UMLS:C5234922\",\n                \"MESH:D000083302\",\n                \"MESH:D002543\",\n                \"MEDDRA:10008111\",\n                \"MEDDRA:10008114\",\n                \"MEDDRA:10018972\",\n                \"MEDDRA:10019005\",\n                \"MEDDRA:10019016\",\n                \"MEDDRA:10019529\",\n                \"MEDDRA:10019531\",\n                \"MEDDRA:10019551\",\n                \"MEDDRA:10022737\",\n                \"MEDDRA:10022751\",\n                \"MEDDRA:10022753\",\n                \"MEDDRA:10022754\",\n                \"MEDDRA:10048863\",\n                \"MEDDRA:10055278\",\n                \"MEDDRA:10055293\",\n                \"MEDDRA:10055800\",\n                \"MEDDRA:10055815\",\n                \"MEDDRA:10071793\",\n                \"MEDDRA:10077620\",\n                \"MEDDRA:10077622\",\n                \"NCIT:C50485\",\n                \"NCIT:C95803\",\n                \"SNOMEDCT:230706003\",\n                \"SNOMEDCT:274100004\",\n                \"HP:0001342\"\n              ],\n              \"type\": [\n                \"biolink:Disease\",\n                \"biolink:DiseaseOrPhenotypicFeature\",\n                \"biolink:BiologicalEntity\",\n                \"biolink:NamedThing\",\n                \"biolink:Entity\",\n                \"biolink:ThingWithTaxon\"\n              ],\n              \"synonyms\": [\n                \"stroke, hemorrhagic\",\n                \"hemorrhage, intracerebral, susceptibility to\",\n                \"ich\",\n                \"stroke, hemorrhagic, susceptibility to\"\n              ]\n            }\n          ]\n        }\n      }\n    ]\n  },\n  \"aggregations\": {\n    \"type-count\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"phenotype\",\n          \"doc_count\": 36\n        },\n        {\n          \"key\": \"disease\",\n          \"doc_count\": 28\n        },\n        {\n          \"key\": \"anatomical entity\",\n          \"doc_count\": 10\n        },\n        {\n          \"key\": \"TOPMed Phenotype Concept\",\n          \"doc_count\": 8\n        },\n        {\n          \"key\": \"drug\",\n          \"doc_count\": 3\n        },\n        {\n          \"key\": \"\",\n          \"doc_count\": 2\n        },\n        {\n          \"key\": \"biological process\",\n          \"doc_count\": 1\n        },\n        {\n          \"key\": \"clinical_course\",\n          \"doc_count\": 1\n        },\n        {\n          \"key\": \"molecular entity\",\n          \"doc_count\": 1\n        }\n      ]\n    }\n  }\n}\n\"\"\"\n\ndef _brain_search_result():\n    \"\"\"Stuck in a function just so I can shove it down here at the end\n    of the test module\"\"\"\n    return json.loads(brain_result_json)\n"}
{"type": "test_file", "path": "tests/unit/test_annotators.py", "content": "from copy import copy\nfrom typing import List\nfrom attr import field\n\nimport pytest\nfrom dug.core.annotators.utils.biolink_purl_util import BioLinkPURLerizer\n\nfrom tests.unit.mocks.data.mock_config import MockConfig\nfrom dug.core.annotators import (\n    DugIdentifier,\n    AnnotateMonarch,\n    DefaultNormalizer,\n    DefaultSynonymFinder,\n)\nfrom unittest.mock import MagicMock\n\n\ndef test_identifier():\n    ident_1 = DugIdentifier(\n        \"PrimaryIdent:1\", \"first identifier\", types=[], search_text=\"\", description=\"\"\n    )\n\n    assert \"PrimaryIdent\" == ident_1.id_type\n\n\ndef test_annotator(annotator_api):\n    cfg = MockConfig.test_from_env()\n    normalizer = DefaultNormalizer(cfg.normalizer)\n    synonym_finder = DefaultSynonymFinder(cfg.synonym_service)\n\n    annotator = AnnotateMonarch(\n        normalizer=normalizer, synonym_finder=synonym_finder, config=cfg , **cfg.annotator_args[\"monarch\"]\n    )\n    text = \"heart attack\"\n    identifiers: List[DugIdentifier] = annotator.annotate_text(\n        text, annotator_api\n    )\n\n    assert len(identifiers) == 7\n    assert isinstance(identifiers[0], DugIdentifier)\n\n\ndef test_normalizer(normalizer_api):\n    url = \"http://normalizer.api/?curie=\"\n\n    identifier = DugIdentifier(\n        \"UBERON:0007100\",\n        label='primary circulatory organ',\n        types=['anatomical entity'],\n        description=\"\",\n        search_text=['heart'],\n    )\n\n    normalizer = DefaultNormalizer(url)\n    output = normalizer(identifier, normalizer_api)\n    assert isinstance(output, DugIdentifier)\n    assert output.id == 'UBERON:0007100'\n    assert output.label == \"primary circulatory organ\"\n    assert output.equivalent_identifiers == ['UBERON:0007100']\n    assert output.types == 'anatomical entity'\n\n\ndef test_synonym_finder(synonym_api):\n    curie = \"UBERON:0007100\"\n    url = f\"http://synonyms.api\"\n    finder = DefaultSynonymFinder(url)\n    result = finder(\n        curie,\n        synonym_api,\n    )\n    assert result == [\n            \"primary circulatory organ\",\n            \"dorsal tube\",\n            \"adult heart\",\n            \"heart\"\n        ]\n\n\n# def test_yield_partial_text():\n#     annotator = Annotator('foo')\n#     # text contains 800 characters + 9 new lines\n#     text = \"\"\"COG Protocol number on which the patient was enrolled [901=Trial of mouse monoclonal Anti-GD-2 antibody 14.G2A plus IL-2 with or without GM-CSF in children with refractory NBL or melanoma; 911=I-131-MIBG for therapy of advanced neuroblastoma; 914=A dose escalation study of cisplatin, doxorubicin, VP-16, and ifosfamide followed by GM-CSF in advanced NBL and peripheral neuroepithelioma; 925=Study of topotecan; 935=Study of ch14.18 with GM-CSF in children with NBL and other GD2 positive malignancies immediately post ABMT or PBSC; 937=Phase I trial of ZD1694, an inhibitor of thymidylate synthase, in pediatric patients with advanced neoplastic disease; 9709=A phase I study of fenretinide in children with high risk solid tumors; 321P2=New intensive chemotherapy for CCG stage II (with N-myc amplification), stage III and stage IV neuroblastoma; 321P3=Treatment of poor prognosis neuroblastoma before disease progression with intensive multimodal therapy and BMT; 323P=Cyclic combination chemotherapy for newly diagnosed stage III neuroblastoma age 2 and older and stage IV Nneuroblastoma all ages; 3881=Biology and therapy of good, intermediate, and selected poor prognosis neuroblastoma; 3891=Conventional dose chemoradiotherapy vs ablative chemoradiotherapy with autologous BMT for high-risk neuroblastoma; 3951=Phase I pilot study of multiple cycles of high dose chemotherapy with peripheral blood stem cell infusions in advanced stage neuroblastoma.; 4941=National Wilms tumor study V - therapeutic trial & biology study; 8605=Study of the combination of ifosfamide, mesna, and VP-16 in children and young adults with recurrent sarcomas, PNET and other tumors; 8742=Phase III portion of 8741 for neuroblastoma; 9047=Neuroblastoma biology protocol; 9082=Protocol for the development of intervention strategies to reduce the time between symptom onset and diagnosis of childhood cancer -a pediatric oncology group cancer control study; 9140=Therapy for patients with recurrent or refractory neuroblastoma - a phase II study; 9262=A Phase II study of taxol in children with recurrent/refractory soft-tissue sarcoma, rhabdomyosarcoma, osteosarcoma, Ewing's sarcoma, neuroblastoma, germ cell tumors, Wilms' tumor, hepatoblastoma, and hepatocellular carcinoma, a POG study; 9280=Neuroblastoma epidemiology protocol - A Non-Therapeutic Study - A Joint Project of: The University of North Carolina, The Pediatric Oncology Group and The Children's Cancer Study Group; 9340=Treatment of patients >365 days at diagnosis with stage IV NBL: Upfront Phase II Window - A Phase II Study; 9341=Treatment of patients >365 days at diagnosis with stage IV and stage IIB/III (N-myc) NBL - a phase III study; 9342=Neuroblastoma #5, bone marrow transplant - a phase III study; 9343=Interleukin-6 in children receiving autologous bone marrow transplantation for advanced neuroblastoma - a pediatric oncology group phase I trial; 9361=Topotecan in pediatric patients with recurrent or progressive solid tumors - a pediatric oncology group phase II study; 9375=Topotecan plus cyclophosphamide in children with solid tumors - a pediatric oncology group phase I trial; 9464=Cyclophosphamide plus topotecan in children with recurrent or refractory solid tumors - a pediatric oncology group phase II study; 9640=Treatment of patients with high risk neuroblastoma (a feasibility pilot) using two cycles of marrow ablative chemotherapy followed by rescue With peripheral blood stem cells (PBSC), radiation therapy; A3973=A randomized study of purged vs. unpurged PBSC transplant following dose intensive induction therapy for high risk NBL; AADM01P1=Protocol for registration and consent to the childhood cancer research network: a limited institution pilot; AAML00P2=A dose finding study of the safety of gemtuzumab ozogamicin combined with conventional chemotherapy for patients with relapsed or refractory acute myeloid leukemia; ACCL0331=A Randomized double blind placebo controlled clinical trial to assess the efficacy of traumeel® S (IND # 66649) for the prevention and treatment of mucositis in children undergoing hematopoietic stem cell transplantation; ACCRN07=Protocol for the enrollment on the official COG registry, The Childhood Cancer Research Network (CCRN); ADVL0018=Phase I study of hu14.18-IL2 fusion protein in patients with refractory neuroblastoma and other refractory GD2 expressing tumors; ADVL0212=A Phase I study of depsipeptide (NSC#630176, IND# 51810) in pediatric patients with refractory solid tumors and leukemias; ADVL0214=A phase I study of single agent OSI-774 (Tarceva) (NSC # 718781, IND #63383) followed by OSI-774 with temozolomide for patients with selected recurrent/refractory solid tumors, including brain tumors; ADVL0215=A phase I study of decitabine in combination with doxorubicin and cyclophosphamide in the treatment of relapsed or refractory solid tumors; ADVL0421=A phase II study of oxaliplatin in children with recurrent solid tumors; ADVL0524=Phase II trial of ixabepilone (BMS-247550), an epothilone B analog, in children and young adults with refractory solid tumors; ADVL0525=A phase II study of pemetrexed in children with recurrent malignancies; ADVL06B1=A pharmacokinetic-pharmacodynamic-pharmacogenetic study of actinomycin-D and vincristine in children with cancer; ADVL0714=A phase I study of VEGF trap (NSC# 724770, IND# 100137) in children with refractory solid tumors; ALTE03N1=Key adverse events after childhood cancer; ALTE05N1=Umbrella long-term follow-up protocol; ANBL0032=Phase III randomized study of chimeric antibody 14.18 (Ch14.18) in high risk neuroblastoma following myeloablative therapy and autologous stem cell rescue; ANBL00B1=Neuroblastoma biology studies; ANBL00P1=A pilot study of tandem high dose chemotherapy with stem cell rescue following induction therapy in children with high risk neuroblastoma; ANBL02P1=A pilot induction regimen incorporating dose-intensive topotecan and cyclophosphamide for treatment of newly diagnosed high risk neuroblastoma; ANBL0321=Phase II study of fenretinide in pediatric patients with resistant or recurrent neuroblastoma; ANBL0322=A phase II study of hu14.18-IL2 (BB-IND-9728) in children with recurrent or refractory neuroblastoma; ANBL0532=Phase III randomized trial of single vs. tandem myeloablative as consolidation therapy for high-risk neuroblastoma; ANBL0621=A phase II study of ABT-751, an orally bioavailable tubulin binding agent, in children with relapsed or refractory neuroblastoma; B003=Diagnostic & prognostic studies in NBL; B903=Childhood cancer genetics; B947=Protocol for collection of biology specimens for research studies; B954=Opsoclonus-myoclonus-ataxia syndrome, neuroblastoma and the presence of anti-neuronal antibodies; B973=Laboratory-clinical studies of neuroblastoma; E04=Self-administered epidemiology questionnaire; E18=A case-control study of risk factors for neuroblastoma; I03=Neuroblastoma, diagnostic/prognostic; N891=Parents' perceptions of randomization; P9462=Randomized treatment of recurrent neuroblastoma with topotecan regimens following desferrioxamine (POG only) in an investigational window; P9641=Primary surgical therapy for biologically defined low-risk neuroblastoma; P9761=A phase II trial of irinotecan in children with refractory solid tumors; P9963=A phase II trial of rebeccamycin analogue (NSC #655649) in children with solid tumors; R9702=Prognostic implications of MIBG uptake in patients with neuroblastoma previously treated on CCG-3891; S31=Right atrial catheter study; S921=Comparison of urokinase vs heparin in preventing Infection in central venous devices in children with malignancies]\"\"\"\n#     chunks = \"\"\n#     is_the_beginning = True\n#     max_chars = 2000\n#     padding_words = 3\n#     counter = 0\n#     print(len(text))\n#     # divvy up into chunks,  sum of each chunk should equal the original text.\n#     for chunk in annotator.sliding_window(text=text, max_characters=max_chars, padding_words= padding_words):\n#         assert len(chunk) <= max_chars\n#         counter += 1\n#         if is_the_beginning:\n#             chunks += chunk\n#         else:\n#             # remove redundand padded words from final result\n#             chunks += \" \".join(chunk.split(\" \")[padding_words:])\n#         is_the_beginning = False\n\n#     print(counter)\n#     # since spaces are trimmed by tokenizer , we can execuled all spaces and do char\n#     assert chunks == text"}
{"type": "test_file", "path": "tests/integration/test_index.py", "content": "import os\n\nimport pytest\nfrom elasticsearch import Elasticsearch\n\nfrom dug.core.async_search import Search\nfrom dug.config import Config\n\n\ndef is_elastic_up():\n    host = os.environ.get('ELASTIC_API_HOST')\n    port = 9200\n    hosts = [\n        {\n            'host': host,\n            'port': port\n        }\n    ]\n    username = os.environ.get('ELASTIC_USERNAME')\n    password = os.environ.get('ELASTIC_PASSWORD')\n    try:\n        es = Elasticsearch(\n            hosts=hosts,\n            basic_auth=(username, password)\n        )\n        return es.ping()\n    except Exception:\n        return False\n\n\n@pytest.mark.skipif(not is_elastic_up(), reason=\"ElasticSearch is down\")\ndef test_search_init():\n    \"\"\"\n    Tests if we can create a Search instance without it blowing up :D\n    \"\"\"\n    Search(cfg=Config.from_env())\n"}
{"type": "test_file", "path": "tests/integration/test_parsers.py", "content": "from dug.core.parsers import DbGaPParser, NIDAParser, TOPMedTagParser, SciCrunchParser, AnvilDbGaPParser,\\\n    CRDCDbGaPParser, KFDRCDbGaPParser, SPRINTParser, BACPACParser, CTNParser\nfrom tests.integration.conftest import TEST_DATA_DIR\nfrom pathlib import Path\n\ndef test_dbgap_parse_study_name_from_filename():\n    parser = DbGaPParser()\n    filename = \"whatever/phs000166.v2.pht000700.v1.CAMP_CData.data_dict_2009_09_03.xml\"\n    studyname = parser.parse_study_name_from_filename(filename)\n    assert studyname == \"CAMP_CData\"\n    # test if version numbers are > 9\n    filename = \"whatever/phs000166.v23.pht000700.v13.CAMP_CData.data_dict_2009_09_03.xml\"\n    studyname = parser.parse_study_name_from_filename(filename)\n    assert studyname == \"CAMP_CData\"\n\ndef test_nida_parse_study_name_from_filename():\n    parser = NIDAParser()\n    filename = \"whatever/NIDA-CPU0008-Dictionary.xml\"\n    studyname = parser.parse_study_name_from_filename(filename)\n    assert studyname == \"NIDA-CPU0008\"\n    filename = \"whatever/NIDA-CSP1019_DD.xml\"\n    studyname = parser.parse_study_name_from_filename(filename)\n    assert studyname == \"NIDA-CSP1019\"\n\ndef test_dbgap_parse_study_name_from_gap_exchange_file():\n    parser = DbGaPParser()\n    parse_filepath = Path(TEST_DATA_DIR / \"phs001252.v1.p1\" / \"phs001252.v1.pht006366.v1.ECLIPSE_Subject.data_dict.xml\")\n    studyname = parser.parse_study_name_from_gap_exchange_file(parse_filepath)\n    assert studyname == \"Evaluation of COPD Longitudinally to Identify Predictive Surrogate Endpoints (ECLIPSE)\"\n\ndef test_dbgap_parser():\n    parser = DbGaPParser()\n    parse_file = str(TEST_DATA_DIR / \"phs000166.v2.pht000700.v1.CAMP_CData.data_dict_2009_09_03.xml\")\n    elements = parser(parse_file)\n    assert len(elements) > 0\n\ndef test_db_gap_scicrunch_parser():\n    parser = SciCrunchParser()\n    parse_file = str(TEST_DATA_DIR / \"DOI:10.26275-0ce8-cuwi.xml\")\n    elements = parser(parse_file)\n    assert len(elements) == 6\n    for element in elements:\n        assert element.collection_action == \"https://DOI.org/10.26275/0ce8-cuwi\"\n        assert element.collection_name == \"Identification of peripheral neural circuits that regulate heart rate using optogenetic and viral vector strategies\"\n\n    parse_file2 = str(TEST_DATA_DIR / \"DOI:10.26275-zupz-yhtf.xml\")\n    elements2 = parser(parse_file2)\n    assert len(elements2) == 1\n    for element in elements2:\n        assert element.collection_action == \"https://DOI.org/10.26275/zupz-yhtf\"\n        assert element.collection_name == \"Lower urinary tract nerve responses to high-density epidural sacral spinal cord stimulation\"\n\n    # the source SciCrunch file has some unicode characters. This test makes sure they have been succesfully\n    # converted to utf8\n    parse_file3 = str(TEST_DATA_DIR / \"DOI:10.26275-c4xq-9kl0.xml\")\n    elements3 = parser(parse_file3)\n    assert len(elements3) == 5\n    for element in elements3:\n        assert element.collection_action == \"https://DOI.org/10.26275/c4xq-9kl0\"\n        assert element.collection_name == \"Effect of Intermittent Hypoxia Preconditioning in Rats with Chronic Cervical Spinal Cord Injury – An electrophysiological Study\"\n\ndef test_nida_parser():\n    parser = NIDAParser()\n    parse_file = str(TEST_DATA_DIR / \"NIDA-CPU0008-Dictionary.xml\")\n    elements = parser(parse_file)\n    assert len(elements) > 0\n\ndef test_topmed_tag_parser():\n    parser = TOPMedTagParser()\n    parse_file = str(TEST_DATA_DIR / \"test_variables_v2.0.csv\")\n    elements = parser(parse_file)\n    assert len(elements) == 62\n    for element in elements:\n        assert element.name != element.id\n        assert element.description != element.id\n\n\ndef test_anvil_parser():\n    parser = AnvilDbGaPParser()\n    parse_file = str(TEST_DATA_DIR / \"phs001547.v1.pht009987.v1.TOPMed_CCDG_GENAF_Subject.data_dict.xml\")\n    elements = parser(parse_file)\n    assert len(elements) == 3\n    for element in elements:\n        assert element.type == \"AnVIL\"\n\n\ndef test_crdc_parser():\n    parser = CRDCDbGaPParser()\n    parse_file = str(TEST_DATA_DIR / \"phs001547.v1.pht009987.v1.TOPMed_CCDG_GENAF_Subject.data_dict.xml\")\n    elements = parser(parse_file)\n    assert len(elements) == 3\n    for element in elements:\n        assert element.type == \"Cancer Data Commons\"\n\n\ndef test_kfdrc_parser():\n    parser = KFDRCDbGaPParser()\n    parse_file = str(TEST_DATA_DIR / \"phs001547.v1.pht009987.v1.TOPMed_CCDG_GENAF_Subject.data_dict.xml\")\n    elements = parser(parse_file)\n    assert len(elements) == 3\n    for element in elements:\n        assert element.type == \"Kids First\"\n\n\ndef test_sprint_parser():\n    parser = SPRINTParser()\n    parse_file = str(TEST_DATA_DIR / \"adolescent_sleep_wake_scale_short_form_aswssf.xml\")\n    elements = parser(parse_file)\n    assert len(elements) == 27\n    for element in elements:\n        assert element.type == \"SPRINT\"\n    element_names = [e.name for e in elements]\n    assert \"awsw_i2\" in element_names\n\ndef test_sprint_parser_form_name():\n    filename = \"/opt/***/share/data/dug/input_files/sprint/adolescent_sleep_wake_scale_short_form_aswssf.xml\"\n    assert SPRINTParser.parse_study_name_from_filename(filename) == \"adolescent_sleep_wake_scale_short_form_aswssf\"\n\ndef test_bacpac_parser():\n    parser = BACPACParser()\n    parse_file = str(TEST_DATA_DIR / \"bacpac_baseline_do_measures.xml\")\n    elements = parser(parse_file)\n    assert len(elements) == 3\n    for element in elements:\n        assert element.type == \"BACPAC\"\n    element_names = [e.name for e in elements]\n    assert \"record_id.demographic_and_baseline_characteristic_core_data_elements\" in element_names\n\n\ndef test_ctn_parser():\n    parser = CTNParser()\n    parse_file = str(TEST_DATA_DIR / \"ctn_test.xml\")\n    elements = parser(parse_file)\n    assert len(elements) == 5\n    for element in elements:\n        assert element.type == \"ctn\"\n    element_names = [e.name for e in elements]\n\n    assert \"RANDDT\" in element_names"}
{"type": "test_file", "path": "tests/integration/test_crawler.py", "content": "import tempfile\nfrom pathlib import Path\nfrom unittest.mock import MagicMock\n\nfrom dug.core.crawler import Crawler\nfrom tests.integration.conftest import TEST_DATA_DIR\n\n\ndef test_crawler_init():\n    crawl_file = TEST_DATA_DIR / \"crawler_sample_file.csv\"\n    parser = MagicMock()\n    annotator = MagicMock()\n    tranqlizer = MagicMock()\n    tranql_queries = MagicMock()\n    http_session = MagicMock()\n\n    crawler = Crawler(\n        crawl_file=crawl_file,\n        parser=parser,\n        annotator=annotator,\n        tranqlizer=tranqlizer,\n        tranql_queries=tranql_queries,\n        http_session=http_session,\n    )\n\n    assert crawler.crawlspace == \"crawl\"\n    assert len(crawler.elements) == 0\n    assert len(crawler.concepts) == 0\n\n\ndef test_make_crawlspace():\n    crawl_file = TEST_DATA_DIR / \"crawler_sample_file.csv\"\n    parser = MagicMock()\n    annotator = MagicMock()\n    tranqlizer = MagicMock()\n    tranql_queries = MagicMock()\n    http_session = MagicMock()\n\n    crawler = Crawler(\n        crawl_file=crawl_file,\n        parser=parser,\n        annotator=annotator,\n        tranqlizer=tranqlizer,\n        tranql_queries=tranql_queries,\n        http_session=http_session,\n    )\n    with tempfile.TemporaryDirectory() as temp_dir:\n        crawler.crawlspace = str(Path(temp_dir) / 'crawl')\n        assert not Path(crawler.crawlspace).exists()\n        crawler.make_crawlspace()\n        assert Path(crawler.crawlspace).exists()\n\n\ndef test_crawl():\n    crawl_file = TEST_DATA_DIR / \"crawler_sample_file.csv\"\n    parser = MagicMock()\n    annotator = MagicMock()\n    tranqlizer = MagicMock()\n    tranql_queries = MagicMock()\n    http_session = MagicMock()\n\n    crawler = Crawler(\n        crawl_file=crawl_file,\n        parser=parser,\n        annotator=annotator,\n        tranqlizer=tranqlizer,\n        tranql_queries=tranql_queries,\n        http_session=http_session,\n    )\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        crawler.crawlspace = str(Path(temp_dir) / 'crawl')\n        crawler.crawl()\n"}
{"type": "test_file", "path": "tests/integration/test_loaders.py", "content": "import tempfile\nfrom pathlib import Path\n\nimport pytest\n\nfrom dug.core.loaders.filesystem_loader import load_from_filesystem\nfrom dug.core.loaders.network_loader import load_from_network\nfrom tests.integration.conftest import TEST_DATA_DIR\n\n\ndef test_filesystem_loader():\n    targets = load_from_filesystem(\n        filepath=TEST_DATA_DIR / 'phs000166.v2.pht000700.v1.CAMP_CData.data_dict_2009_09_03.xml'\n    )\n    assert len(list(targets)) == 1\n\n    targets = load_from_filesystem(\n        filepath=TEST_DATA_DIR,\n    )\n    files = list(targets)\n    assert len(files) == 16\n\n    with pytest.raises(ValueError):\n        targets = load_from_filesystem(\n            filepath=TEST_DATA_DIR / \"foo/bar\"\n        )\n        next(targets)\n\n    with pytest.raises(ValueError):\n        targets = load_from_filesystem(\n            filepath=TEST_DATA_DIR / \"*.xml\"\n        )\n        next(targets)\n\n\ndef test_network_loader():\n\n    with tempfile.TemporaryDirectory(dir=TEST_DATA_DIR) as tmp_dir:\n        tmp_dir_path = Path(tmp_dir)\n        url = \"https://github.com/helxplatform/dug/blob/develop/README.md\"\n        actual = next(load_from_network(tmp_dir, url))\n        expected = tmp_dir_path / \"github.com\" / \"helxplatform\" / \"dug\" / \"blob\" / \"develop\" / \"README.md\"\n        assert actual == expected\n        assert actual.exists()\n\n    with tempfile.TemporaryDirectory(dir=TEST_DATA_DIR) as tmp_dir:\n        tmp_dir_path = Path(tmp_dir)\n        with pytest.raises(ValueError):\n            url = \"https://github.com/helxplatform/dug/blob/develop/404 expected\"\n            next(load_from_network(tmp_dir, url))\n        assert list(tmp_dir_path.iterdir()) == []\n"}
{"type": "test_file", "path": "tests/unit/test_cli.py", "content": "from unittest.mock import patch\n\nfrom pytest import mark\n\nfrom dug.cli import main, get_argparser\n\n\n@mark.cli\ndef test_dug_cli_parser():\n    parser = get_argparser()\n    parsed_log_level = parser.parse_args([\"-l\", \"DEBUG\"])\n    parsed_crawl = parser.parse_args([\"crawl\", \"somefile.csv\", \"--parser\", \"topmedtag\"])\n    parsed_search = parser.parse_args([\"search\", \"-q\", \"heart attack\", \"-t\", \"variables\", \"-k\", \"namespace=default\"])\n\n    assert parsed_log_level.log_level == \"DEBUG\"\n\n    assert parsed_crawl.target == \"somefile.csv\"\n    assert parsed_crawl.parser_type == \"topmedtag\"\n\n    assert parsed_search.target == \"variables\"\n    assert parsed_search.query == \"heart attack\"\n\n\n@mark.cli\n@patch('dug.cli.crawl')\ndef test_dug_cli_main_crawl(mock_crawl):\n    main([\"crawl\", \"somefile.csv\", \"--parser\", \"topmedtag\"])\n    mock_crawl.assert_called_once()\n\n@mark.cli\n@patch('dug.cli.crawl')\ndef test_dug_cli_main_extract_dug_elements(mock_crawl):\n    main([\"crawl\", \"somefile.csv\", \"--parser\", \"topmedtag\", \"-x\"])\n    mock_crawl.assert_called_once()\n    assert mock_crawl.call_args_list[0].args[0].extract_dug_elements\n\n@mark.cli\n@patch('dug.cli.crawl')\ndef test_dug_cli_main_extract_dug_elements_none(mock_crawl):\n    main([\"crawl\", \"somefile.csv\", \"--parser\", \"topmedtag\"])\n    mock_crawl.assert_called_once()\n    assert not mock_crawl.call_args_list[0].args[0].extract_dug_elements\n\n@mark.cli\n@patch('dug.cli.crawl')\ndef test_dug_cli_main_annotator(mock_crawl):\n    main([\"crawl\", \"somefile.csv\",\"--parser\", \"topmedtag\", \"--annotator\", \"annotator-monarch\"])\n    mock_crawl.assert_called_once()\n\n@mark.cli\n@patch('dug.cli.search')\ndef test_dug_cli_main_search(mock_search):\n    # mock_search.search.return_value = \"Searching!\"\n    main([\"search\", \"-q\", \"heart attack\", \"-t\", \"variables\", \"-k\", \"namespace=default\"])\n    mock_search.assert_called_once()\n"}
{"type": "test_file", "path": "tests/unit/test_parsers.py", "content": "from dug.core.parsers._base import DugElement, DugConcept\nfrom dug.core.annotators import DugIdentifier, AnnotateMonarch\n# from dug.core.annotators.monarch_annotator import AnnotateMonarch\n\n\ndef test_dug_concept():\n    concept = DugConcept(\"concept-1\", 'Concept-1', 'The first concept', 'secondary')\n\n    ident_1 = DugIdentifier(\"ident-1\", \"Identifier-1\")\n    ident_2 = DugIdentifier(\"ident-2\", \"Identifier-2\")\n\n    concept.add_identifier(ident_1)\n    concept.add_identifier(ident_2)\n\n    concept.clean()\n\n\ndef test_dug_concept_searchable_dict():\n\n    concept_id = \"concept-1\"\n    concept_name = 'Concept-1'\n    concept_description = 'The first concept'\n    concept_type = 'secondary'\n    concept = DugConcept(\n        concept_id,\n        concept_name,\n        concept_description,\n        concept_type,\n    )\n\n    assert concept.get_searchable_dict() == {\n        'id': concept_id,\n        'name': concept_name,\n        'description': concept_description,\n        'type': concept_type,\n        'search_terms': [],\n        'optional_terms': [],\n        'concept_action': \"\",\n        'identifiers': [],\n    }\n\n\ndef test_dug_element():\n    elem_id = \"1\"\n    elem_name = \"Element-1\"\n    elem_desc = \"The first element\"\n    elem_type = \"primary\"\n    element = DugElement(\n        elem_id, elem_name, elem_desc, elem_type, collection_id=\"\", collection_name=\"\", collection_desc=\"\"\n    )\n\n    assert len(element.concepts) == 0\n    element.add_concept(DugConcept(\"concept-1\", 'Concept-1', 'The first concept', 'secondary'))\n    assert len(element.concepts) == 1\n    element.add_concept(DugConcept(\"concept-1\", 'Concept-1', 'The first concept', 'secondary'))\n    assert len(element.concepts) == 1\n\n\ndef test_dug_element_searchable_dict():\n    elem_id = \"1\"\n    elem_name = \"Element-1\"\n    elem_desc = \"The first element\"\n    elem_type = \"primary\"\n    elem_collection_id = \"C-1\"\n    elem_collection_name = \"Collection 1\"\n    elem_collection_desc = \"First collection\"\n    element = DugElement(\n        elem_id, elem_name, elem_desc, elem_type,\n        collection_id=elem_collection_id,\n        collection_name=elem_collection_name,\n        collection_desc=elem_collection_desc,\n    )\n    searchable = element.get_searchable_dict()\n    assert searchable == {\n        'element_id': elem_id,\n        'element_name': elem_name,\n        'element_desc': elem_desc,\n        'collection_id': elem_collection_id,\n        'collection_name': elem_collection_name,\n        'collection_desc': elem_collection_desc,\n        'element_action': \"\",\n        'collection_action': \"\",\n        'data_type': elem_type,\n        \"metadata\": {},\n        'identifiers': [],\n        'search_terms': [],\n        'optional_terms': []\n    }\n\n"}
{"type": "test_file", "path": "tests/unit/test_crawler.py", "content": "import pytest\nfrom unittest.mock import patch\n\nfrom dug.core import DugConcept\nfrom dug.core.parsers import DugElement\nfrom tests.unit.mocks.MockCrawler import *\n\n\nfrom dug.core.crawler import Crawler\n\n\n@pytest.fixture\ndef crawler(crawler_init_args_no_graph_extraction):\n    return Crawler(\n        **crawler_init_args_no_graph_extraction\n    )\n\n\ndef test_init(crawler):\n    assert crawler.crawlspace == \"crawl\"\n\n\ndef test_annotate_element(crawler):\n    element = DugElement(\n        \"test-id\",\n        \"name\",\n        \"some_desc\",\n        \"test-type\",\n        \"collection-id\",\n        \"collection-name\",\n        \"collection-desc\"\n    )\n    crawler.annotate_element(element)\n    AnnotatorMock.assert_called_with(**{\n        \"text\": element.ml_ready_desc,\n        \"http_session\": HTTPSessionMock\n    })\n    assert len(crawler.concepts) == len(ANNOTATED_IDS)\n    assert len(element.concepts) == len(ANNOTATED_IDS)\n\n\ndef test_annotate_elements(crawler):\n    elements = [DugElement(\n        \"test-1\",\n        \"name\",\n        \"some_desc\",\n        \"test-type\",\n        \"collection-id\",\n        \"collection-name\",\n        \"collection-desc\"\n    ), DugElement(\n        \"test-2\",\n        \"name\",\n        \"some_desc\",\n        \"test-type\",\n        \"collection-id\",\n        \"collection-name\",\n        \"collection-desc\"\n    )]\n    crawler.elements = elements\n    crawler.annotate_elements()\n    # annotate elements mutates the original elements\n    for element in elements:\n        # assert all elements have the fake concepts added\n        assert len(element.concepts) == len(ANNOTATED_IDS)\n        # assert concept labels  are set on the element's search terms\n        for ANNOTATED_ID in ANNOTATED_IDS:\n            assert ANNOTATED_ID.label in element.search_terms\n\n\ndef test_expand_concept(crawler):\n    identifier = ANNOTATED_IDS[0]\n    concept = DugConcept(concept_id=identifier.id, name=\"test-concept\", desc=\"\" , concept_type=identifier.types[0])\n    concept.add_identifier(identifier)\n    crawler.expand_concept(concept=concept)\n    TranqlizerMock.expand_identifier.assert_called_with(\n        identifier.id, TranqlQueriesMock.get(\"disease\"), crawler.crawlspace + '/' + identifier.id + '_disease.json'\n    )\n    assert len(concept.kg_answers) == len(TRANQL_ANSWERS)\n\ndef test_expand_to_dug_element(crawler):\n    identifier = ANNOTATED_IDS[0]\n    concept = DugConcept(concept_id=identifier.id, name=\"test-concept\", desc=\"\", concept_type=identifier.types[0])\n    concept.add_identifier(identifier)\n    new_elements = crawler.expand_to_dug_element(\n        concept=concept,\n        casting_config={\n            \"node_type\": \"biolink:Publication\",\n            \"curie_prefix\": \"HEALCDE\",\n            \"attribute_mapping\": {\n                \"name\": \"name\",\n                \"desc\": \"summary\",\n                \"collection_name\": \"cde_category\",\n                \"collection_id\":  \"cde_category\"\n            },\n            \"list_field_choose_first\": []\n        },\n        dug_element_type=\"test-element\",\n        tranql_source=\"test:graph\"\n    )\n    assert len(new_elements) == len(TRANQL_ANSWERS)\n"}
{"type": "test_file", "path": "tests/unit/test_core/test_search.py", "content": "import os\nfrom dataclasses import dataclass, field\nfrom unittest.mock import patch\n\nimport pytest\nimport pytest_asyncio\n\nfrom dug.core.index import Index, SearchException\nfrom dug.config import Config\n\ndefault_indices = [\"concepts_index\", \"variables_index\", \"kg_index\"]\n\nhost = \"localhost\"\nport = 9200\nusername = \"elastic\"\npassword = \"hunter2\"\nnboost_host = \"localhost\"\nhosts = [{\"host\": host, \"port\": port, \"scheme\": \"https\"}]\n\n\nclass MockEsNode:\n    def info():\n        return {\"_nodes\": {\"total\": 1}}\n\n\n@dataclass\nclass MockIndex:\n    settings: dict\n    mappings: dict\n    values: dict = field(default_factory=dict)\n\n    def index(self, id, body):\n        self.values[id] = body\n\n    def update(self, id, body):\n        return self.index(id, body)\n\n    def get(self, id):\n        return self.values.get(id)\n\n    def count(self, body):\n        return len(self.values)\n\n\nclass MockIndices:\n    def __init__(self):\n        self._indices = {}\n        self.call_count = 0\n        self.number_of_replicas = 1\n\n    def exists(self, index):\n        return index in self._indices\n\n    def create(self, index, body, **_kwargs):\n        self.call_count += 1\n        self._indices[index] = MockIndex(**body)\n\n    def get_index(self, index) -> MockIndex:\n        return self._indices.get(index)\n\n    def get_settings(self, index):\n        index_schema = {\"settings\": {\"index\": {\"number_of_replicas\": self.number_of_replicas}}}\n        settings = {\n            \"kg_index\": index_schema,\n            \"concepts_index\": index_schema,\n            \"variables_index\": index_schema,\n        }\n        return settings\n\nclass MockElastic:\n    def __init__(self, indices: MockIndices):\n        self.indices = indices\n        self._up = True\n        self.nodes = MockEsNode\n\n    def index(self, index, id=None, body=None):\n        self.indices.get_index(index).index(id, body)\n\n    def update(self, index, id=None, body=None):\n        self.indices.get_index(index).update(id, body)\n\n    def ping(self):\n        return self._up\n\n    def connect(self):\n        self._up = True\n\n    def disconnect(self):\n        self._up = False\n\n    def count(self, body, index):\n        return {\"count\": self.indices.get_index(index).count(body)}\n\n    def search(self, index, body, **kwargs):\n        values = self.indices.get_index(index).values\n        return {\"results\": {k: v for k, v in values.items() if body in v}}\n\n\n@pytest_asyncio.fixture\ndef elastic():\n    with patch(\"dug.core.index.Elasticsearch\") as es_class:\n        es_instance = MockElastic(indices=MockIndices())\n        es_class.return_value = es_instance\n        yield es_instance\n\n\ndef test_init(elastic):\n    cfg = Config(\n        elastic_host=\"localhost\",\n        elastic_username=\"elastic\",\n        elastic_password=\"hunter2\",\n        nboost_host=\"localhost\",\n        elastic_scheme=\"https\"\n    )\n\n    search = Index(cfg)\n\n    assert search.indices == default_indices\n    assert search.hosts == hosts\n    assert search.es is elastic\n\n\ndef test_init_no_ping(elastic):\n    elastic.disconnect()\n    with pytest.raises(SearchException):\n        _search = Index(Config.from_env())\n\n\n@pytest.mark.asyncio\nasync def test_init_indices(elastic):\n    search = Index(Config.from_env())\n    assert elastic.indices.call_count == 3\n\n    # Should take no action if called again\n    search.init_indices()\n    assert elastic.indices.call_count == 3\n\n\ndef test_index_doc(elastic: MockElastic):\n    search = Index(Config.from_env())\n\n    assert len(elastic.indices.get_index(\"concepts_index\").values) == 0\n    search.index_doc(\"concepts_index\", {\"name\": \"sample\"}, \"ID:1\")\n    assert len(elastic.indices.get_index(\"concepts_index\").values) == 1\n    assert elastic.indices.get_index(\"concepts_index\").get(\"ID:1\") == {\"name\": \"sample\"}\n\n\ndef test_update_doc(elastic: MockElastic):\n    search = Index(Config.from_env())\n\n    search.index_doc(\"concepts_index\", {\"name\": \"sample\"}, \"ID:1\")\n    search.update_doc(\"concepts_index\", {\"name\": \"new value!\"}, \"ID:1\")\n    assert elastic.indices.get_index(\"concepts_index\").get(\"ID:1\") == {\n        \"name\": \"new value!\"\n    }\n"}
{"type": "test_file", "path": "tests/unit/test_utils.py", "content": "# import pytest\n\n# from dug.utils import get_nida_study_link\n# import requests\n\n# @pytest.mark.skip(\"Implement this test\")\n# def test_object_factory():\n#     pass\n\n\n# @pytest.mark.skip(\"Implement this test\")\n# def test_complex_handler():\n#     pass\n\n\n# @pytest.mark.skip(\"Implement this test\")\n# def test_get_dbgap_var_link():\n#     pass\n\n\n# @pytest.mark.skip(\"Implement this test\")\n# def test_get_dbgap_study_link():\n#     pass\n\n\n# def test_get_nida_study_link():\n#     study_id = \"NIDA-CPU-0008\"\n#     link = get_nida_study_link(study_id=study_id)\n#     response = requests.post(\n#         url=link\n#     )\n#     content = str(response.text)\n#     assert content.count(study_id) > 0\n"}
{"type": "test_file", "path": "tests/integration/test_async_search.py", "content": "\"Integration tests for the async_search module\"\n\nimport asyncio\nfrom unittest import TestCase\n\nfrom fastapi.testclient import TestClient\nfrom elasticsearch.exceptions import ConnectionError\nfrom dug.config import Config\n\nclass APISearchTestCase(TestCase):\n    \"API search with mocked elasticsearch\"\n\n    def test_concepts_types_parameter(self):\n        \"Test API concepts search with types parameter\"\n        cfg = Config.from_env()\n        if cfg.elastic_password == \"changeme\":\n            # Dummy config is in place, skip the test\n            self.skipTest(\n                \"For the integration test, a populated elasticsearch \"\n                \"instance must be available and configured in the \"\n                \"environment variables. See dug.config for more.\")\n\n        from dug.server import APP\n        client = TestClient(APP)\n        types = ['anatomical entity', 'drug']\n        body = {\n            \"index\": \"concepts_index\",\n            \"query\": \"brain\",\n            \"offset\": 0,\n            \"size\":20,\n            \"types\": types\n        }\n        try:\n            response = client.post(\"/search\", json=body)\n        except ConnectionError:\n            self.fail(\"For the integration test, a populated elasticsearch \"\n                      \"instance must be available and configured in the \"\n                      \"environment variables. See dug.config for more.\")\n        self.assertEqual(response.status_code, 200)\n        response_obj = response.json()\n        response_types = set(hit['_source']['type'] for hit in\n                 response_obj['result']['hits']['hits'])\n        self.assertEqual(response_types, set(types))\n"}
{"type": "test_file", "path": "tests/unit/mocks/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/unit/test_core/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/unit/conftest.py", "content": "import json\nimport urllib.parse\nfrom dataclasses import dataclass\nfrom typing import Dict\n\nimport pytest_asyncio\n\n@dataclass\nclass MockResponse:\n    text: str\n    status_code: int = 200\n\n    def json(self):\n        return json.loads(self.text)\n\n\nclass MockApiService:\n    def __init__(self, urls: Dict[str, list]):\n        self.urls = urls\n\n    def get(self, url, params: dict = None):\n        if params:\n            qstr = urllib.parse.urlencode(params, quote_via=urllib.parse.quote)\n            url = f\"{url}?{qstr}\"\n\n        text, status_code = self.urls.get(url)\n\n        if text is None:\n            return MockResponse(text=\"{}\", status_code=404)\n        return MockResponse(text, status_code=status_code)\n\n    def post(self, url, params: dict = None, json: dict = {}):\n        if params:\n            qstr = urllib.parse.urlencode(params, quote_via=urllib.parse.quote)\n            url = f\"{url}?{qstr}\"\n        text, status_code = self.urls.get(url)\n\n        if text is None:\n            return MockResponse(text=\"{}\", status_code=404)\n        return MockResponse(text, status_code=status_code)\n\n\n@pytest_asyncio.fixture\ndef annotator_api():\n    base_url = \"http://annotator.api/?content={query}\"\n\n    def _(keyword):\n        return base_url.format(query=urllib.parse.quote(keyword))\n\n    urls = {\n        _(\"heart attack\"): [\n            json.dumps(\n                {\n                    \"content\": \"heart attack\",\n                    \"spans\": [\n                        {\n                            \"start\": 0,\n                            \"end\": 5,\n                            \"text\": \"heart\",\n                            \"token\": [\n                                {\n                                    \"id\": \"UBERON:0015230\",\n                                    \"category\": [\"anatomical entity\"],\n                                    \"terms\": [\"dorsal vessel heart\"],\n                                }\n                            ],\n                        },\n                        {\n                            \"start\": 0,\n                            \"end\": 5,\n                            \"text\": \"heart\",\n                            \"token\": [\n                                {\n                                    \"id\": \"UBERON:0007100\",\n                                    \"category\": [\"anatomical entity\"],\n                                    \"terms\": [\"primary circulatory organ\"],\n                                }\n                            ],\n                        },\n                        {\n                            \"start\": 0,\n                            \"end\": 5,\n                            \"text\": \"heart\",\n                            \"token\": [\n                                {\n                                    \"id\": \"UBERON:0015228\",\n                                    \"category\": [\"anatomical entity\"],\n                                    \"terms\": [\"circulatory organ\"],\n                                }\n                            ],\n                        },\n                        {\n                            \"start\": 0,\n                            \"end\": 5,\n                            \"text\": \"heart\",\n                            \"token\": [\n                                {\n                                    \"id\": \"ZFA:0000114\",\n                                    \"category\": [\"anatomical entity\"],\n                                    \"terms\": [\"heart\"],\n                                }\n                            ],\n                        },\n                        {\n                            \"start\": 0,\n                            \"end\": 5,\n                            \"text\": \"heart\",\n                            \"token\": [\n                                {\n                                    \"id\": \"UBERON:0000948\",\n                                    \"category\": [\"anatomical entity\"],\n                                    \"terms\": [\"heart\"],\n                                }\n                            ],\n                        },\n                        {\n                            \"start\": 0,\n                            \"end\": 12,\n                            \"text\": \"heart attack\",\n                            \"token\": [\n                                {\n                                    \"id\": \"MONDO:0005068\",\n                                    \"category\": [\"disease\"],\n                                    \"terms\": [\"myocardial infarction (disease)\"],\n                                }\n                            ],\n                        },\n                        {\n                            \"start\": 0,\n                            \"end\": 12,\n                            \"text\": \"heart attack\",\n                            \"token\": [\n                                {\n                                    \"id\": \"HP:0001658\",\n                                    \"category\": [\"phenotype\", \"quality\"],\n                                    \"terms\": [\"Myocardial infarction\"],\n                                }\n                            ],\n                        },\n                    ],\n                }\n            ),\n            200,\n        ],\n    }\n\n    return MockApiService(\n        urls=urls,\n    )\n\n\n@pytest_asyncio.fixture\ndef normalizer_api():\n    base_url = \"http://normalizer.api/?curie={curie}\"\n\n    def _(curie):\n        return base_url.format(\n            curie=urllib.parse.quote(curie),\n        )\n\n    urls = {\n        _(\"UBERON:0007100\"): [\n            json.dumps(\n                {\n                    \"UBERON:0007100\": {\n                        \"id\": {\n                            \"identifier\": \"UBERON:0007100\",\n                            \"label\": \"primary circulatory organ\",\n                        },\n                        \"equivalent_identifiers\": [\n                            {\n                                \"identifier\": \"UBERON:0007100\",\n                                \"label\": \"primary circulatory organ\",\n                            }\n                        ],\n                        \"type\": [\n                            \"biolink:AnatomicalEntity\",\n                            \"biolink:OrganismalEntity\",\n                            \"biolink:BiologicalEntity\",\n                            \"biolink:NamedThing\",\n                            \"biolink:Entity\",\n                        ],\n                    }\n                },\n            ),\n            200,\n        ],\n    }\n\n    return MockApiService(\n        urls=urls,\n    )\n\n\n@pytest_asyncio.fixture\ndef synonym_api():\n    return MockApiService(\n        urls={\n            \"http://synonyms.api\": [\n                json.dumps(\n                    {\n                        \"UBERON:0007100\": {\n                            \"names\": [\n                                \"primary circulatory organ\",\n                                \"dorsal tube\",\n                                \"adult heart\",\n                                \"heart\",\n                            ]\n                        }\n                    }\n                ),\n                200,\n            ]\n        }\n    )\n\n\n@pytest_asyncio.fixture()\ndef ontology_api():\n    base_url = \"http://ontology.api/?curie={curie}\"\n\n    def _(curie):\n        return base_url.format(\n            curie=urllib.parse.quote(curie),\n        )\n\n    return MockApiService(\n        urls={\n            _(\"UBERON:0007100\"): [\n                json.dumps(\n                    {\n                        \"taxon\": {\"id\": None, \"label\": None},\n                        \"association_counts\": None,\n                        \"xrefs\": [\"SPD:0000130\", \"FBbt:00003154\", \"TADS:0000147\"],\n                        \"description\": \"A hollow, muscular organ, which, by contracting rhythmically, keeps up the circulation of the blood or analogs[GO,modified].\",\n                        \"types\": None,\n                        \"synonyms\": [\n                            {\"val\": \"dorsal tube\", \"pred\": \"synonym\", \"xrefs\": None},\n                            {\"val\": \"adult heart\", \"pred\": \"synonym\", \"xrefs\": None},\n                            {\"val\": \"heart\", \"pred\": \"synonym\", \"xrefs\": None},\n                        ],\n                        \"deprecated\": None,\n                        \"replaced_by\": None,\n                        \"consider\": None,\n                        \"id\": \"UBERON:0007100\",\n                        \"label\": \"primary circulatory organ\",\n                        \"iri\": \"http://purl.obolibrary.org/obo/UBERON_0007100\",\n                        \"category\": [\"anatomical entity\"],\n                    }\n                ),\n                200,\n            ]\n        }\n    )\n"}
{"type": "source_file", "path": "bin/get_topmed_data_dicts.py", "content": "output_dir = \"/Users/awaldrop/Desktop/test/new_bdc_dbgap_data_dicts\"\nstudies = [\n#\"phs000007.v29.p10\",\n#\"phs000179.v5.p2\",\n#\"phs000200.v11.p3\",\n#\"phs000209.v13.p3\",\n#\"phs000280.v3.p1\",\n#\"phs000284.v1.p1\",\n\"phs000285.v3.p2\",\n#\"phs000286.v5.p1\",\n#\"phs000287.v6.p1\",\n#\"phs000289.v2.p1\",\n#\"phs000741.v2.p1\",\n#\"phs000810.v1.p1\",\n#\"phs000914.v1.p1\",\n#\"phs000956.v2.p1\",\n#\"phs000988.v2.p1\",\n#\"phs001013.v2.p2\",\n#\"phs001238.v1.p1\"\n]\n\nimport os\nfrom ftplib import FTP\nfor study_id in studies:\n\n    study_variable = study_id.split('.')[0]\n    print(\"Begin-------------------------\")\n    print(f\"{study_id}\")\n    os.makedirs(f\"{output_dir}/{study_id}\")\n\n    ftp = FTP('ftp.ncbi.nlm.nih.gov')\n    ftp.login()\n\n    # Step 1: First we try and get all the data_dict files\n    ftp.cwd(f\"/dbgap/studies/{study_variable}/{study_id}/pheno_variable_summaries\")\n    ftp_filelist = ftp.nlst(\".\")\n    for ftp_filename in ftp_filelist:\n        if 'data_dict' in ftp_filename:\n            with open(f\"{output_dir}/{study_id}/{ftp_filename}\", \"wb\") as data_dict_file:\n                    ftp.retrbinary(f\"RETR {ftp_filename}\", data_dict_file.write)\n\n    # Step 2: Check to see if there's a GapExchange file in the parent folder\n    #         and if there is, get it.\n    ftp.cwd(f\"/dbgap/studies/{study_variable}/{study_id}\")\n    ftp_filelist = ftp.nlst(\".\")\n    for ftp_filename in ftp_filelist:\n        if 'GapExchange' in ftp_filename:\n            with open(f\"{output_dir}/{study_id}/{ftp_filename}\", \"wb\") as data_dict_file:\n                ftp.retrbinary(f\"RETR {ftp_filename}\", data_dict_file.write)\n    ftp.quit()\n    print(\"End---------------------------\")"}
{"type": "source_file", "path": "src/dug/core/annotators/utils/biolink_purl_util.py", "content": "class BioLinkPURLerizer:\n    # Static class for the sole purpose of doing lookups of different ontology PURLs\n    # Is it pretty? No. But it gets the job done.\n    biolink_lookup = {\"APO\": \"http://purl.obolibrary.org/obo/APO_\",\n                      \"Aeolus\": \"http://translator.ncats.nih.gov/Aeolus_\",\n                      \"BIOGRID\": \"http://identifiers.org/biogrid/\",\n                      \"BIOSAMPLE\": \"http://identifiers.org/biosample/\",\n                      \"BSPO\": \"http://purl.obolibrary.org/obo/BSPO_\",\n                      \"CAID\": \"http://reg.clinicalgenome.org/redmine/projects/registry/genboree_registry/by_caid?caid=\",\n                      \"CHEBI\": \"http://purl.obolibrary.org/obo/CHEBI_\",\n                      \"CHEMBL.COMPOUND\": \"http://identifiers.org/chembl.compound/\",\n                      \"CHEMBL.MECHANISM\": \"https://www.ebi.ac.uk/chembl/mechanism/inspect/\",\n                      \"CHEMBL.TARGET\": \"http://identifiers.org/chembl.target/\",\n                      \"CID\": \"http://pubchem.ncbi.nlm.nih.gov/compound/\",\n                      \"CL\": \"http://purl.obolibrary.org/obo/CL_\",\n                      \"CLINVAR\": \"http://identifiers.org/clinvar/\",\n                      \"CLO\": \"http://purl.obolibrary.org/obo/CLO_\",\n                      \"COAR_RESOURCE\": \"http://purl.org/coar/resource_type/\",\n                      \"CPT\": \"https://www.ama-assn.org/practice-management/cpt/\",\n                      \"CTD\": \"http://translator.ncats.nih.gov/CTD_\",\n                      \"ClinVarVariant\": \"http://www.ncbi.nlm.nih.gov/clinvar/variation/\",\n                      \"DBSNP\": \"http://identifiers.org/dbsnp/\",\n                      \"DGIdb\": \"https://www.dgidb.org/interaction_types\",\n                      \"DOID\": \"http://purl.obolibrary.org/obo/DOID_\",\n                      \"DRUGBANK\": \"http://identifiers.org/drugbank/\",\n                      \"DrugCentral\": \"http://translator.ncats.nih.gov/DrugCentral_\",\n                      \"EC\": \"http://www.enzyme-database.org/query.php?ec=\",\n                      \"ECTO\": \"http://purl.obolibrary.org/obo/ECTO_\",\n                      \"EDAM-DATA\": \"http://edamontology.org/data_\",\n                      \"EDAM-FORMAT\": \"http://edamontology.org/format_\",\n                      \"EDAM-OPERATION\": \"http://edamontology.org/operation_\",\n                      \"EDAM-TOPIC\": \"http://edamontology.org/topic_\",\n                      \"EFO\": \"http://identifiers.org/efo/\",\n                      \"ENSEMBL\": \"http://identifiers.org/ensembl/\",\n                      \"ExO\": \"http://purl.obolibrary.org/obo/ExO_\",\n                      \"FAO\": \"http://purl.obolibrary.org/obo/FAO_\",\n                      \"FB\": \"http://identifiers.org/fb/\",\n                      \"FBcv\": \"http://purl.obolibrary.org/obo/FBcv_\",\n                      \"FlyBase\": \"http://flybase.org/reports/\",\n                      \"GAMMA\": \"http://translator.renci.org/GAMMA_\",\n                      \"GO\": \"http://purl.obolibrary.org/obo/GO_\",\n                      \"GOLD.META\": \"http://identifiers.org/gold.meta/\",\n                      \"GOP\": \"http://purl.obolibrary.org/obo/go#\",\n                      \"GOREL\": \"http://purl.obolibrary.org/obo/GOREL_\",\n                      \"GSID\": \"https://scholar.google.com/citations?user=\",\n                      \"GTEx\": \"https://www.gtexportal.org/home/gene/\",\n                      \"HANCESTRO\": \"http://www.ebi.ac.uk/ancestro/ancestro_\",\n                      \"HCPCS\": \"http://purl.bioontology.org/ontology/HCPCS/\",\n                      \"HGNC\": \"http://identifiers.org/hgnc/\",\n                      \"HGNC.FAMILY\": \"http://identifiers.org/hgnc.family/\",\n                      \"HMDB\": \"http://identifiers.org/hmdb/\",\n                      \"HP\": \"http://purl.obolibrary.org/obo/HP_\",\n                      \"ICD0\": \"http://translator.ncats.nih.gov/ICD0_\",\n                      \"ICD10\": \"http://translator.ncats.nih.gov/ICD10_\",\n                      \"ICD9\": \"http://translator.ncats.nih.gov/ICD9_\",\n                      \"INCHI\": \"http://identifiers.org/inchi/\",\n                      \"INCHIKEY\": \"http://identifiers.org/inchikey/\",\n                      \"INTACT\": \"http://identifiers.org/intact/\",\n                      \"IUPHAR.FAMILY\": \"http://identifiers.org/iuphar.family/\",\n                      \"KEGG\": \"http://identifiers.org/kegg/\",\n                      \"LOINC\": \"http://loinc.org/rdf/\",\n                      \"MEDDRA\": \"http://identifiers.org/meddra/\",\n                      \"MESH\": \"http://identifiers.org/mesh/\",\n                      \"MGI\": \"http://identifiers.org/mgi/\",\n                      \"MI\": \"http://purl.obolibrary.org/obo/MI_\",\n                      \"MIR\": \"http://identifiers.org/mir/\",\n                      \"MONDO\": \"http://purl.obolibrary.org/obo/MONDO_\",\n                      \"MP\": \"http://purl.obolibrary.org/obo/MP_\",\n                      \"MSigDB\": \"https://www.gsea-msigdb.org/gsea/msigdb/\",\n                      \"MetaCyc\": \"http://translator.ncats.nih.gov/MetaCyc_\",\n                      \"NCBIGENE\": \"http://identifiers.org/ncbigene/\",\n                      \"NCBITaxon\": \"http://purl.obolibrary.org/obo/NCBITaxon_\",\n                      \"NCIT\": \"http://purl.obolibrary.org/obo/NCIT_\",\n                      \"NDDF\": \"http://purl.bioontology.org/ontology/NDDF/\",\n                      \"NLMID\": \"https://www.ncbi.nlm.nih.gov/nlmcatalog/?term=\",\n                      \"OBAN\": \"http://purl.org/oban/\",\n                      \"OBOREL\": \"http://purl.obolibrary.org/obo/RO_\",\n                      \"OIO\": \"http://www.geneontology.org/formats/oboInOwl#\",\n                      \"OMIM\": \"http://purl.obolibrary.org/obo/OMIM_\",\n                      \"ORCID\": \"https://orcid.org/\",\n                      \"ORPHA\": \"http://www.orpha.net/ORDO/Orphanet_\",\n                      \"ORPHANET\": \"http://identifiers.org/orphanet/\",\n                      \"PANTHER.FAMILY\": \"http://identifiers.org/panther.family/\",\n                      \"PANTHER.PATHWAY\": \"http://identifiers.org/panther.pathway/\",\n                      \"PATO-PROPERTY\": \"http://purl.obolibrary.org/obo/pato#\",\n                      \"PDQ\": \"https://www.cancer.gov/publications/pdq#\",\n                      \"PHARMGKB.DRUG\": \"http://identifiers.org/pharmgkb.drug/\",\n                      \"PHARMGKB.PATHWAYS\": \"http://identifiers.org/pharmgkb.pathways/\",\n                      \"PHAROS\": \"http://pharos.nih.gov\",\n                      \"PMID\": \"http://www.ncbi.nlm.nih.gov/pubmed/\",\n                      \"PO\": \"http://purl.obolibrary.org/obo/PO_\",\n                      \"POMBASE\": \"http://identifiers.org/pombase/\",\n                      \"PR\": \"http://purl.obolibrary.org/obo/PR_\",\n                      \"PUBCHEM.COMPOUND\": \"http://identifiers.org/pubchem.compound/\",\n                      \"PUBCHEM.SUBSTANCE\": \"http://identifiers.org/pubchem.substance/\",\n                      \"PathWhiz\": \"http://smpdb.ca/pathways/#\",\n                      \"REACT\": \"http://www.reactome.org/PathwayBrowser/#/\",\n                      \"REPODB\": \"http://apps.chiragjpgroup.org/repoDB/\",\n                      \"RGD\": \"http://identifiers.org/rgd/\",\n                      \"RHEA\": \"http://identifiers.org/rhea/\",\n                      \"RNACENTRAL\": \"http://identifiers.org/rnacentral/\",\n                      \"RO\": \"http://purl.obolibrary.org/obo/RO_\",\n                      \"RTXKG1\": \"http://kg1endpoint.rtx.ai/\",\n                      \"RXNORM\": \"http://purl.bioontology.org/ontology/RXNORM/\",\n                      \"ResearchID\": \"https://publons.com/researcher/\",\n                      \"SEMMEDDB\": \"https://skr3.nlm.nih.gov/SemMedDB\",\n                      \"SGD\": \"http://identifiers.org/sgd/\",\n                      \"SIO\": \"http://semanticscience.org/resource/SIO_\",\n                      \"SMPDB\": \"http://identifiers.org/smpdb/\",\n                      \"SNOMEDCT\": \"http://identifiers.org/snomedct/\",\n                      \"SNPEFF\": \"http://translator.ncats.nih.gov/SNPEFF_\",\n                      \"ScopusID\": \"https://www.scopus.com/authid/detail.uri?authorId=\",\n                      \"TAXRANK\": \"http://purl.obolibrary.org/obo/TAXRANK_\",\n                      \"UBERGRAPH\": \"http://translator.renci.org/ubergraph-axioms.ofn#\",\n                      \"UBERON\": \"http://purl.obolibrary.org/obo/UBERON_\",\n                      \"UBERON_CORE\": \"http://purl.obolibrary.org/obo/uberon/core#\",\n                      \"UMLS\": \"http://identifiers.org/umls/\",\n                      \"UMLSSC\": \"https://metamap.nlm.nih.gov/Docs/SemanticTypes_2018AB.txt/code#\",\n                      \"UMLSSG\": \"https://metamap.nlm.nih.gov/Docs/SemGroups_2018.txt/group#\",\n                      \"UMLSST\": \"https://metamap.nlm.nih.gov/Docs/SemanticTypes_2018AB.txt/type#\",\n                      \"UNII\": \"http://identifiers.org/unii/\",\n                      \"UPHENO\": \"http://purl.obolibrary.org/obo/UPHENO_\",\n                      \"UniProtKB\": \"http://identifiers.org/uniprot/\",\n                      \"VANDF\": \"https://www.nlm.nih.gov/research/umls/sourcereleasedocs/current/VANDF/\",\n                      \"VMC\": \"https://github.com/ga4gh/vr-spec/\",\n                      \"WB\": \"http://identifiers.org/wb/\",\n                      \"WBPhenotype\": \"http://purl.obolibrary.org/obo/WBPhenotype_\",\n                      \"WBVocab\": \"http://bio2rdf.org/wormbase_vocabulary\",\n                      \"WIKIDATA\": \"https://www.wikidata.org/wiki/\",\n                      \"WIKIDATA_PROPERTY\": \"https://www.wikidata.org/wiki/Property:\",\n                      \"WIKIPATHWAYS\": \"http://identifiers.org/wikipathways/\",\n                      \"WormBase\": \"https://www.wormbase.org/get?name=\",\n                      \"ZFIN\": \"http://identifiers.org/zfin/\",\n                      \"ZP\": \"http://purl.obolibrary.org/obo/ZP_\",\n                      \"alliancegenome\": \"https://www.alliancegenome.org/\",\n                      \"biolink\": \"https://w3id.org/biolink/vocab/\",\n                      \"biolinkml\": \"https://w3id.org/biolink/biolinkml/\",\n                      \"chembio\": \"http://translator.ncats.nih.gov/chembio_\",\n                      \"dcterms\": \"http://purl.org/dc/terms/\",\n                      \"dictyBase\": \"http://dictybase.org/gene/\",\n                      \"doi\": \"https://doi.org/\",\n                      \"fabio\": \"http://purl.org/spar/fabio/\",\n                      \"foaf\": \"http://xmlns.com/foaf/0.1/\",\n                      \"foodb.compound\": \"http://foodb.ca/compounds/\",\n                      \"gff3\": \"https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md#\",\n                      \"gpi\": \"https://github.com/geneontology/go-annotation/blob/master/specs/gpad-gpi-2-0.md#\",\n                      \"gtpo\": \"https://rdf.guidetopharmacology.org/ns/gtpo#\",\n                      \"hetio\": \"http://translator.ncats.nih.gov/hetio_\",\n                      \"interpro\": \"https://www.ebi.ac.uk/interpro/entry/\",\n                      \"isbn\": \"https://www.isbn-international.org/identifier/\",\n                      \"isni\": \"https://isni.org/isni/\",\n                      \"issn\": \"https://portal.issn.org/resource/ISSN/\",\n                      \"medgen\": \"https://www.ncbi.nlm.nih.gov/medgen/\",\n                      \"oboformat\": \"http://www.geneontology.org/formats/oboInOWL#\",\n                      \"pav\": \"http://purl.org/pav/\",\n                      \"prov\": \"http://www.w3.org/ns/prov#\",\n                      \"qud\": \"http://qudt.org/1.1/schema/qudt#\",\n                      \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n                      \"rdfs\": \"http://www.w3.org/2000/01/rdf-schema#\",\n                      \"skos\": \"https://www.w3.org/TR/skos-reference/#\",\n                      \"wgs\": \"http://www.w3.org/2003/01/geo/wgs84_pos\",\n                      \"xsd\": \"http://www.w3.org/2001/XMLSchema#\",\n                      \"@vocab\": \"https://w3id.org/biolink/vocab/\"}\n\n    @staticmethod\n    def get_curie_purl(curie):\n        # Split into prefix and suffix\n        suffix = curie.split(\":\")[1]\n        prefix = curie.split(\":\")[0]\n\n        # Check to see if the prefix exists in the hash\n        if prefix not in BioLinkPURLerizer.biolink_lookup:\n            return None\n\n        return f\"{BioLinkPURLerizer.biolink_lookup[prefix]}{suffix}\""}
{"type": "source_file", "path": "src/dug/_version.py", "content": "__version__ = \"3.0.0.dev\"\n"}
{"type": "source_file", "path": "src/dug/core/__init__.py", "content": "import asyncio\nimport logging\nimport os\nimport sys\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Iterable\n\nimport pluggy\nfrom dug.core.loaders.filesystem_loader import load_from_filesystem\nfrom dug.core.loaders.network_loader import load_from_network\n\nfrom dug import hookspecs\nfrom dug.core import parsers\nfrom dug.core import annotators\nfrom dug.core.factory import DugFactory\nfrom dug.core.parsers import DugConcept, Parser, get_parser\nfrom dug.core.annotators import DugIdentifier, Annotator, get_annotator\n\nlogger = logging.getLogger('dug')\nstdout_log_handler = logging.StreamHandler(sys.stdout)\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nstdout_log_handler.setFormatter(formatter)\nlogger.addHandler(stdout_log_handler)\n\nlogging.getLogger(\"elasticsearch\").setLevel(logging.WARNING)\n\n\ndef get_plugin_manager() -> pluggy.PluginManager:\n    pm = pluggy.PluginManager(\"dug\")\n    pm.add_hookspecs(hookspecs)\n    pm.load_setuptools_entrypoints(\"dug\")\n    pm.register(parsers)\n    pm.register(annotators)\n    return pm\n\n\ndef get_targets(target_name) -> Iterable[Path]:\n    if target_name.startswith(\"http://\") or target_name.startswith(\"https://\"):\n        loader = partial(load_from_network, os.getenv(\"DUG_DATA_DIR\", \"data\"))\n    else:\n        loader = load_from_filesystem\n    return loader(target_name)\n\n\nclass Dug:\n    concepts_index = \"concepts_index\"\n    variables_index = \"variables_index\"\n    kg_index = \"kg_index\"\n\n    def __init__(self, factory: DugFactory):\n        self._factory = factory\n        self._search = self._factory.build_search_obj(indices=[\n            self.concepts_index, self.variables_index, self.kg_index\n        ])\n        self._index = self._factory.build_indexer_obj(\n            indices=[\n                self.concepts_index, self.variables_index, self.kg_index\n            ]\n        )\n\n    def crawl(self, target_name: str, parser_type: str, annotator_type: str, element_type: str = None):\n\n        pm = get_plugin_manager()\n        parser = get_parser(pm.hook, parser_type)\n        annotator = get_annotator(pm.hook, annotator_type, self._factory.config)\n        targets = get_targets(target_name)\n\n        for target in targets:\n            self._crawl(target, parser, annotator, element_type)\n\n    def _crawl(self, target: Path, parser: Parser, annotator: Annotator, element_type):\n\n        # Initialize crawler\n        crawler = self._factory.build_crawler(target, parser, annotator, element_type)\n        # Read elements, annotate, and expand using tranql queries\n        crawler.crawl()\n\n        # Index Annotated Elements\n        for element in crawler.elements:\n            # Only index DugElements as concepts will be indexed differently in next step\n            if not isinstance(element, DugConcept):\n                self._index.index_element(element, index=self.variables_index)\n\n        # Index Annotated/TranQLized Concepts and associated knowledge graphs\n        for concept_id, concept in crawler.concepts.items():\n            self._index.index_concept(concept, index=self.concepts_index)\n\n            # Index knowledge graph answers for each concept\n            for kg_answer_id, kg_answer in concept.kg_answers.items():\n                self._index.index_kg_answer(concept_id=concept_id,\n                                             kg_answer=kg_answer,\n                                             index=self.kg_index,\n                                             id_suffix=kg_answer_id)\n\n    def search(self, target, query, **kwargs):\n        event_loop = asyncio.get_event_loop()\n        targets = {\n            'concepts': partial(\n                self._search.search_concepts),\n            'variables': partial(\n                self._search.search_variables, concept=kwargs.pop('concept', None)),\n            'kg': partial(\n                self._search.search_kg, unique_id=kwargs.pop('unique_id', None))\n        }\n        kwargs.pop('index', None)\n        func = targets.get(target)\n        if func is None:\n            raise ValueError(f\"Target must be one of {', '.join(targets.keys())}\")\n        results = event_loop.run_until_complete(func(query=query, **kwargs))\n        event_loop.run_until_complete(self._search.es.close())\n        return results\n\n    def status(self):\n        ...\n"}
{"type": "source_file", "path": "src/dug/__init__.py", "content": "from ._version import __version__"}
{"type": "source_file", "path": "src/dug/core/annotators/utils/__init__.py", "content": ""}
{"type": "source_file", "path": "src/dug/core/annotators/sapbert_annotator.py", "content": "import logging\nfrom typing import List, Dict\nfrom requests import Session, RequestException\nfrom retrying import retry\nimport time\nfrom dug.core.annotators._base import DugIdentifier, Input\nfrom dug.core.annotators.utils.biolink_purl_util import BioLinkPURLerizer\nfrom functools import reduce\n\nlogger = logging.getLogger(\"dug\")\n\nlogging.getLogger(\"requests\").setLevel(logging.WARNING)\nlogging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n\n\nclass BagelWrapper:\n    def __init__(self, prompt_name, llm_args, url):\n        self.llm_args = llm_args\n        self.url = url\n        self.prompt_name = prompt_name\n\n    @retry(stop_max_attempt_number=3)\n    def __call__(self, description_text, entity, ids: List[DugIdentifier], http_session: Session):\n        response = self.make_request(description_text=description_text, entity=entity, ids=ids, http_session=http_session)\n        result = self.handle_response(ids, response)\n        return result\n\n    def make_request(self, description_text, entity, ids: List[DugIdentifier], http_session: Session):\n        url = self.url\n        if ids:\n            payload = {\n                \"prompt_name\": self.prompt_name,\n                \"context\": {\n                    \"text\": description_text,\n                    \"entity\": entity,\n                    \"synonyms\": [{\n                        \"label\": i.label,\n                        \"identifier\": i.id,\n                        \"description\": i.description,\n                        \"entity_type\": i.types.split(':')[-1],\n                        \"color-code\": \"red\"\n                    } for i in ids]\n                },\n                \"config\": self.llm_args\n            }\n            return http_session.post(self.url, json=payload).json()\n\n        return []\n\n    def handle_response(self, ids: List[DugIdentifier], bagel_json_result: dict):\n        selected_ids = [x['identifier'] for x in bagel_json_result]\n        return list(filter(lambda x: x.id in selected_ids, ids))\n\n\nclass AnnotateSapbert:\n    \"\"\"\n    Use the RENCI Sapbert API service to fetch ontology IDs found in text\n    \"\"\"\n\n    def __init__(\n        self,\n        normalizer,\n        synonym_finder,\n        ontology_greenlist=[],\n        **kwargs\n    ):\n        self.classificationUrl = kwargs.get('classification_url')\n        self.annotatorUrl = kwargs.get('annotator_url')\n\n        if not self.classificationUrl:\n            raise TypeError('Classification url needs to be defined for sapbert annotator')\n        if not self.annotatorUrl:\n            raise TypeError('Annotator url needs to be defined for sapbert annotator')\n        self.normalizer = normalizer\n        self.synonym_finder = synonym_finder\n        self.ontology_greenlist = ontology_greenlist\n        # threshold marking cutoff point\n        self.score_threshold = float(kwargs.get(\"score_threshold\", 0.8))\n        # indicate if we want values above or below the threshold.\n        self.score_direction_up = True if kwargs.get(\"score_direction\", \"up\") == \"up\" else False\n\n        self.bagel_args = kwargs.get(\"bagel\")\n        if self.bagel_args:\n            self.bagel_enabled = self.bagel_args['enabled']\n            self.bagel = BagelWrapper(\n                prompt_name=self.bagel_args[\"prompt\"],\n                llm_args= self.bagel_args[\"llm_args\"],\n                url=self.bagel_args[\"url\"]\n            )\n        else:\n            self.bagel_enabled = False\n\n    @retry(stop_max_attempt_number=3)\n    def __call__(self, text, http_session) -> List[DugIdentifier]:\n        # Fetch identifiers\n        classifiers: List = self.text_classification(text, http_session)\n\n        raw_identifiers_dict: Dict[str, DugIdentifier] = self.annotate_classifiers(\n            classifiers, http_session\n        )\n\n        # Write out to file if text fails to annotate\n        if not raw_identifiers_dict:\n            logger.warning(f\"Failed to annotate: {text}\\n\")\n\n        processed_identifiers = {}\n        for entity, raw_identifiers in raw_identifiers_dict.items():\n            # normalize all ids\n            for identifier in raw_identifiers:\n                # Normalize identifier using normalization service\n                norm_id = self.normalizer(identifier, http_session)\n\n                # Skip adding id if it doesn't normalize\n                if norm_id is None:\n                    # Write out to file if identifier doesn't normalize\n                    logger.warning(f\"Failed to normalize: {identifier.id}\\n\")\n\n                    # Discard non-normalized ident if not in greenlist\n                    if identifier.id_type not in self.ontology_greenlist:\n                        continue\n\n                    # If it is in greenlist just keep moving forward\n                    norm_id = identifier\n\n                # Add synonyms to identifier\n                norm_id.synonyms = self.synonym_finder(norm_id.id, http_session)\n\n                # Get pURL for ontology identifer for more info\n                norm_id.purl = BioLinkPURLerizer.get_curie_purl(norm_id.id)\n                processed_identifiers[entity] = processed_identifiers.get(entity, [])\n                processed_identifiers[entity].append(norm_id)\n\n\n            # filter using bagel\n            if self.bagel_enabled:\n                processed_identifiers[entity] = self.bagel(description_text=text,\n                                                           entity=entity,\n                                                           ids=processed_identifiers.get(entity, []),\n                                                           http_session=http_session)\n        return reduce(lambda bucket, key: bucket + processed_identifiers[key], processed_identifiers, [])\n\n    def text_classification(self, text, http_session) -> List:\n        \"\"\"\n        Send variable text to a token classifier API and return list of classified terms and biolink types\n\n        Param:\n          text: String -- Full variable text, API does text preprocessing\n\n        Request:\n          {\n              \"text\": \"{{text}}\",\n              \"model_name\": \"token_classification\"\n          }\n\n        Response: List of dicts from which we want to extract the following:\n          {\n              \"obj\": \"{{Biolink Classification}}\",\n              \"text\": \"{{Classified Term}}\"\n          }\n\n        Returns: List Dicts each with a Classified Term and Biolink Classification\n        \"\"\"\n        logger.debug(f\"Classification\")\n        response = self.make_classification_request(text, http_session)\n        classifiers = self.handle_classification_response(response)\n        return classifiers\n\n    def make_classification_request(self, text: Input, http_session: Session):\n        url = self.classificationUrl\n        logger.debug(f\"Requesting classification for text: {text}\")\n        payload = {\n            \"text\": text,\n            \"model_name\": \"token_classification\",\n        }\n\n        NUM_TRIES = 5\n        initial_delay = 1  # seconds\n        backoff_factor = 2\n        max_delay = 10  # seconds\n        retryable_status_codes = {500, 502, 503, 504, 429}\n\n        last_exception = None\n        response = None\n        delay = initial_delay\n        success = False\n\n        for attempt in range(NUM_TRIES):\n            if attempt > 0:\n                logger.debug(f\"Retrying in {delay} seconds...\")\n                time.sleep(delay)\n                delay = min(delay * backoff_factor, max_delay)\n\n            try:\n                response = http_session.post(url, json=payload)\n            except RequestException as e:\n                logger.warning(f\"Attempt {attempt + 1} failed with error: {str(e)}\")\n                last_exception = e\n                continue\n\n            if response.status_code // 100 == 2:\n                success = True\n                break  # Successful response\n            elif response.status_code in retryable_status_codes:\n                logger.warning(f\"Retryable status {response.status_code} on attempt {attempt + 1}\")\n                continue\n            else:\n                if response.status_code == 403:\n                    raise RuntimeError(f\"Authorization error accessing {url}\")\n                else:\n                    raise RuntimeError(f\"API error {response.status_code}: {response.text}\")\n\n        if not success:\n            logger.warning(f\"No response after {attempt} -- returning empty annotations....\")\n            return {\n                \"text\": text,\n                \"denotations\": []\n            }\n\n\n        return response.json()\n\n    def handle_classification_response(self, response: dict) -> List:\n        classifiers = []\n        \"\"\" Parse each identifier and initialize identifier object \"\"\"\n        for denotation in response.get(\"denotations\", []):\n            text = denotation.get(\"text\", None)\n            bl_type = denotation.get(\"obj\", None)\n            classifiers.append(\n                {\"text\": text, \"bl_type\": bl_type}\n            )\n        return classifiers\n\n    def annotate_classifiers(\n        self, classifiers: List, http_session\n    ) -> Dict[str, DugIdentifier]:\n        \"\"\"\n        Send Classified Terms to Sapbert API\n\n        Param:\n          List: [\n              term_dict: Dict {\n                  \"text\": String -- Classified term received from token classification API\n                  \"bl_type\": String -- Biolink Classification\n              }\n          ]\n\n        Request:\n          {\n              \"text\": \"{{term_dict['text']}}\",\n              \"model_name\": \"sapbert\",\n              \"count\": {{Limits the number of results}},\n              \"args\": {\n                  \"bl_type\": \"{{ term_dict['bl_type'] -- NOTE omit `biolink:`}}\"\n              }\n          }\n\n        Response: List of dicts with the following structure:\n              {\n                  \"name\": \"{{Identified Name}}\",\n                  \"curie\": \"{{Curie ID}}\",\n                  \"category\": \"{{Biolink term with `biolink:`}}\",\n                  \"score\": \"{{Float confidence in the annotation}}\"\n              }\n          TBD: Organize the results by highest score\n          Return: List of DugIdentifiers with a Curie ID\n        \"\"\"\n        identifiers = {}\n        for term_dict in classifiers:\n            logger.debug(f\"Annotating: {term_dict['text']}\")\n\n            response = self.make_annotation_request(term_dict, http_session)\n            identifiers[term_dict['text']] =  self.handle_annotation_response(term_dict, response)\n\n        return identifiers\n\n    def make_annotation_request(self, term_dict: Input, http_session: Session):\n        url = self.annotatorUrl\n        payload = {\n            \"text\": term_dict[\"text\"],\n            \"model_name\": \"sapbert\",\n            \"count\": 10,\n            # \"args\": {\"bl_type\": term_dict[\"bl_type\"]},\n        }\n        # This could be moved to a config file\n        NUM_TRIES = 5\n        for _ in range(NUM_TRIES):\n            response = http_session.post(url, json=payload)\n            if response is not None:\n                # looks like it worked\n                break\n        # if the reponse is still None here, throw an error\n        if response is None:\n            raise RuntimeError(f\"no response from {url}\")\n        if response.status_code == 403:\n            raise RuntimeError(f\"You are not authorized to use this API -- {url}\")\n        if response.status_code == 500:\n            raise RuntimeError(f\"Annotation API is temporarily down -- vist docs here: {url.replace('annotate', 'docs')}\")\n        return response.json()\n\n    def handle_annotation_response(self, value, response: dict) -> List[DugIdentifier]:\n        identifiers = []\n        \"\"\" Parse each identifier and initialize identifier object \"\"\"\n        for identifier in response:\n            search_text = value.get(\"text\", None)\n            curie = identifier.get(\"curie\", None)\n            if not curie:\n                continue\n\n            biolink_type = identifier.get('category')\n            score = float(identifier.get(\"score\", 0))\n            label = identifier.get(\"name\")\n            if score >= self.score_threshold and self.score_direction_up:\n                identifiers.append(\n                    DugIdentifier(id=curie, label=label, types=[biolink_type], search_text=search_text)\n                )\n            elif score <= self.score_threshold and not self.score_direction_up:\n                identifiers.append(\n                    DugIdentifier(id=curie, label=label, types=[biolink_type], search_text=search_text)\n                )\n        return identifiers\n"}
{"type": "source_file", "path": "src/dug/core/annotators/__init__.py", "content": "import logging\nfrom typing import Dict\n\nimport pluggy\n\nfrom dug.config import Config\nfrom dug.core.annotators._base import DugIdentifier, Indexable, Annotator, DefaultNormalizer, DefaultSynonymFinder\nfrom dug.core.annotators.monarch_annotator import AnnotateMonarch\nfrom dug.core.annotators.sapbert_annotator import AnnotateSapbert\n\nlogger = logging.getLogger('dug')\n\nhookimpl = pluggy.HookimplMarker(\"dug\")\n\n@hookimpl\ndef define_annotators(annotator_dict: Dict[str, Annotator], config: Config):\n    annotator_dict[\"monarch\"] = build_monarch_annotator(\"monarch\", config=config)\n    annotator_dict[\"sapbert\"] = build_sapbert_annotator(\"sapbert\", config=config)\n\n\nclass AnnotatorNotFoundException(Exception):\n    ...\n\n\ndef get_annotator(hook, annotator_name, config: Config) -> Annotator:\n    \"\"\"Get the annotator from all annotators registered via the define_annotators hook\"\"\"\n\n    available_annotators = {}\n    hook.define_annotators(annotator_dict=available_annotators, config=config)\n    annotator = available_annotators.get(annotator_name.lower())\n    if annotator is not None:\n        logger.info(f'Annotating with {annotator}')\n        return annotator\n\n    err_msg = f\"Cannot find annotator of type '{annotator_name}'\\n\" \\\n              f\"Supported annotators: {', '.join(available_annotators.keys())}\"\n    logger.error(err_msg)\n    raise AnnotatorNotFoundException(err_msg)\n\ndef build_monarch_annotator(annotate_type: str, config: Config):    \n    logger.info(f\"Building Monarch annotator with args: {config.annotator_args[annotate_type]}\")\n    annotator = AnnotateMonarch(\n        normalizer=DefaultNormalizer(**config.normalizer),\n        synonym_finder=DefaultSynonymFinder(**config.synonym_service),\n        config=config,\n        **config.annotator_args[annotate_type]\n    )\n    return annotator\n\ndef build_sapbert_annotator(annotate_type, config: Config):\n    logger.info(f\"Building Sapbert annotator with args: {config.annotator_args[annotate_type]}\")\n    annotator = AnnotateSapbert(\n        normalizer=DefaultNormalizer(**config.normalizer),\n        synonym_finder=DefaultSynonymFinder(**config.synonym_service),\n        **config.annotator_args[annotate_type]\n    )\n    return annotator\n\n"}
{"type": "source_file", "path": "setup.py", "content": "#!/usr/bin/env python\n\nimport setuptools\n\nif __name__ == \"__main__\":\n    setuptools.setup()"}
{"type": "source_file", "path": "src/dug/core/annotators/monarch_annotator.py", "content": "import logging\nimport urllib.parse\nfrom typing import List\nfrom requests import Session\n\nfrom dug.core.annotators._base import DugIdentifier, Input\nfrom dug.core.annotators.utils.biolink_purl_util import BioLinkPURLerizer\n\nlogger = logging.getLogger('dug')\n\nlogging.getLogger(\"requests\").setLevel(logging.WARNING)\nlogging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n\nclass AnnotateMonarch:\n    \"\"\"\n    Use monarch API service to fetch ontology IDs found in text\n    \"\"\"\n    def __init__(\n            self,\n            normalizer,\n            synonym_finder,\n            config,\n            ontology_greenlist=[],\n            **kwargs\n    ):\n\n        self.annotatorUrl = kwargs['url']\n        self.normalizer = normalizer\n        self.synonym_finder = synonym_finder\n        self.ontology_greenlist = ontology_greenlist\n        self.norm_fails_file = \"norm_fails.txt\"\n        self.anno_fails_file = \"anno_fails.txt\"\n\n        debreviator = config.preprocessor['debreviator'] if 'debreviator' in config.preprocessor else None\n        stopwords = config.preprocessor['stopwords'] if 'stopwords' in  config.preprocessor else None\n\n        if debreviator is None:\n            debreviator = self.default_debreviator_factory()\n        self.decoder = debreviator\n\n        if stopwords is None:\n            stopwords = []\n        self.stopwords = stopwords\n\n    def __call__(self, text, http_session) -> List[DugIdentifier]:\n        # Preprocess text (debraviate, remove stopwords, etc.)\n        text = self.preprocess_text(text)\n\n        # Fetch identifiers\n        raw_identifiers = self.annotate_text(text, http_session)\n\n        # Write out to file if text fails to annotate\n        if not raw_identifiers:\n            with open(self.anno_fails_file, \"a\") as fh:\n                fh.write(f'{text}\\n')\n\n        processed_identifiers = []\n        for identifier in raw_identifiers:\n\n            # Normalize identifier using normalization service\n            norm_id = self.normalizer(identifier, http_session)\n\n            # Skip adding id if it doesn't normalize\n            if norm_id is None:\n                # Write out to file if identifier doesn't normalize\n                with open(self.norm_fails_file, \"a\") as fh:\n                    fh.write(f'{identifier.id}\\n')\n\n                # Discard non-normalized ident if not in greenlist\n                if identifier.id_type not in self.ontology_greenlist:\n                    continue\n\n                # If it is in greenlist just keep moving forward\n                norm_id = identifier\n\n            # Add synonyms to identifier\n            norm_id.synonyms = self.synonym_finder(norm_id.id, http_session)\n\n            # Get pURL for ontology identifer for more info\n            norm_id.purl = BioLinkPURLerizer.get_curie_purl(norm_id.id)\n            processed_identifiers.append(norm_id)\n\n        return processed_identifiers\n    \n    def sliding_window(self, text, max_characters=2000, padding_words=5):\n        \"\"\"\n        For long texts sliding window works as the following\n        \"aaaa bbb ccc ddd eeee\"\n        with a sliding max chars 8 and padding 1\n        first yeild would be \"aaaa bbb\"\n        next subsequent yeilds \"bbb ccc\", \"ccc ddd\" , \"ddd eeee\"\n        allowing context to be preserved with the scope of padding\n        For a text of length 7653 , with max_characters 2000 and padding 5 , 4 chunks are yielded.\n        \"\"\"\n        words = text.split(' ')\n        total_words = len(words)\n        window_end = False\n        current_index = 0\n        while not window_end:\n            current_string = \"\"\n            for index, word in enumerate(words[current_index: ]):\n                if len(current_string) + len(word) + 1 >= max_characters:\n                    yield current_string + \" \"\n                    current_index += index - padding_words\n                    break\n                appendee = word if index == 0 else \" \" + word\n                current_string += appendee\n\n            if current_index + index == len(words) - 1:\n                window_end = True\n                yield current_string\n\n    def annotate_text(self, text, http_session) -> List[DugIdentifier]:\n        logger.debug(f\"Annotating: {text}\")\n        identifiers = []\n        for chunk_text in self.sliding_window(text):\n            response = self.make_request(chunk_text, http_session)\n            identifiers += self.handle_response(chunk_text, response)\n        return identifiers\n\n    def make_request(self, value: Input, http_session: Session):\n        value = urllib.parse.quote(value)\n        url = f'{self.annotatorUrl}{value}'\n\n        # This could be moved to a config file\n        NUM_TRIES = 5\n        for _ in range(NUM_TRIES):\n            response = http_session.get(url)\n            if response is not None:\n              # looks like it worked\n                break\n        # if the reponse is still None here, throw an error         \n        if response is None:\n            raise RuntimeError(f\"no response from {url}\")\n        return response.json()\n\n    def handle_response(self, value, response: dict) -> List[DugIdentifier]:\n        identifiers = []\n        \"\"\" Parse each identifier and initialize identifier object \"\"\"\n        for span in response.get('spans', []):\n            search_text = span.get('text', None)\n            for token in span.get('token', []):\n                curie = token.get('id', None)\n                if not curie:\n                    continue\n\n                biolink_types = token.get('category')\n                label = token.get('terms')[0]\n                identifiers.append(DugIdentifier(id=curie,\n                                              label=label,\n                                              types=biolink_types,\n                                              search_text=search_text))\n        return identifiers\n    \n    def preprocess_text(self, text: str) -> str:\n        \"\"\"\n        Apply debreviator to replace abbreviations and other characters\n\n        # >>> pp = PreprocessorMonarch({\"foo\": \"bar\"}, [\"baz\"])\n        # >>> pp.preprocess(\"Hello foo\")\n        # 'Hello bar'\n\n        # >>> pp.preprocess(\"Hello baz world\")\n        'Hello world'\n        \"\"\"\n\n        for key, value in self.decoder.items():\n            text = text.replace(key, value)\n\n        # Remove any stopwords\n        text = \" \".join([word for word in text.split() if word not in self.stopwords])\n        return text\n\n    @staticmethod\n    def default_debreviator_factory():\n        return {\"bmi\": \"body mass index\", \"_\": \" \"}"}
{"type": "source_file", "path": "src/dug/cli.py", "content": "#!/usr/bin/env python3\n\"\"\"\nRepresents the entrypoint for command line tools.\n\"\"\"\n\nimport argparse\nimport os\nimport json\n\nfrom dug.config import Config\nfrom dug.core import Dug, logger, DugFactory\n\n\nclass KwargParser(argparse.Action):\n    def __call__(self, parser, namespace, values, option_string=None):\n        setattr(namespace, self.dest, dict())\n        for value in values:\n            key, value = value.split('=', maxsplit=1)\n            getattr(namespace, self.dest)[key] = value\n\n\ndef get_argparser():\n    argument_parser = argparse.ArgumentParser(description='Dug: Semantic Search')\n    argument_parser.set_defaults(func=lambda _args: argument_parser.print_usage())\n    argument_parser.add_argument(\n        '-l', '--level',\n        type=str,\n        choices=[\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\"],\n        dest='log_level',\n        default=os.getenv(\"DUG_LOG_LEVEL\", \"INFO\"),\n        help=\"Log level\"\n    )\n\n    subparsers = argument_parser.add_subparsers(\n        title=\"Commands\",\n    )\n\n    # Crawl subcommand\n    crawl_parser = subparsers.add_parser('crawl', help='Crawl and index some input')\n    crawl_parser.set_defaults(func=crawl)\n    crawl_parser.add_argument(\n        'target',\n        type=str,\n        help='Input file containing things you want to crawl/index',\n    )\n\n    crawl_parser.add_argument(\n        '-p', '--parser',\n        help='Parser to use for parsing elements from crawl file',\n        dest=\"parser_type\",\n        required=True\n    )\n\n    crawl_parser.add_argument(\n        '-a', '--annotator',\n        help='Annotator used to annotate identifiers in crawl file',\n        dest=\"annotator_type\",\n        default=\"monarch\"\n    )\n\n    crawl_parser.add_argument(\n        '-e', '--element-type',\n        help='[Optional] Coerce all elements to a certain data type (e.g. DbGaP Variable).\\n'\n             'Determines what tab elements will appear under in Dug front-end',\n        dest=\"element_type\",\n        default=None\n    )\n\n    crawl_parser.add_argument(\n        \"-x\", \"--extract-from-graph\",\n        help=\"[Optional] Extract dug elements for tranql using concepts from annotation\",\n        dest=\"extract_dug_elements\",\n        default=False,\n        action=\"store_true\"\n    )\n\n    # Search subcommand\n    search_parser = subparsers.add_parser('search', help='Apply semantic search')\n    search_parser.set_defaults(func=search)\n\n    search_parser.add_argument(\n        '-t', '--target',\n        dest='target',\n        help=\"Defines which search strategy to use (e.g. variables, concepts, kg, etc.)\",\n    )\n\n    search_parser.add_argument(\n        '-q', '--query',\n        dest='query',\n        help='Query to search for',\n    )\n\n    search_parser.add_argument(\n        '-k',\n        '--kwargs',\n        nargs='*',\n        dest='kwargs',\n        default={},\n        action=KwargParser,\n        help=\"Optional keyword arguments that will be passed into the search target\",\n    )\n\n    # Status subcommand\n    # TODO implement this\n    # status_parser = subparsers.add_parser('status', help='Check status of dug server')\n    # status_parser.set_defaults(func=status)\n\n    return argument_parser\n\n\ndef crawl(args):\n    config = Config.from_env()\n    if not args.extract_dug_elements:\n        # disable extraction\n        config.node_to_element_queries = {}\n    factory = DugFactory(config)\n    dug = Dug(factory)\n    dug.crawl(args.target, args.parser_type, args.annotator_type, args.element_type)\n\n\ndef search(args):\n    config = Config.from_env()\n    factory = DugFactory(config)\n    dug = Dug(factory)\n    # dug = Dug()\n    response = dug.search(args.target, args.query, **args.kwargs)\n    # Using json.dumps raises 'TypeError: Object of type ObjectApiResponse is not JSON serializable'\n    #jsonResponse = json.dumps(response, indent = 2)\n    print(response)\n\ndef datatypes(args):\n    config = Config.from_env()\n    factory = DugFactory(config)\n    dug = Dug(factory)\n    # dug = Dug()\n    response = dug.info(args.target, **args.kwargs)\n\n\ndef status(args):\n    print(\"Status check is not implemented yet!\")\n\n\ndef main(args=None):\n\n    arg_parser = get_argparser()\n\n    args = arg_parser.parse_args(args)\n\n    try:\n        logger.setLevel(args.log_level)\n    except ValueError:\n        print(f\"Log level must be one of CRITICAL, ERROR, WARNING, INFO, DEBUG. You entered {args.log_level}\")\n\n    args.func(args)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "bin/get_ncpi_data_dicts.py", "content": "####### ANVIL Syncing Script\n\n# This script is used to generate the input to index Anvil Datasets on Dug\n# Parse, Download dbgap datasets currently hosted on Anvil Platform (tsv downloaded from https://anvilproject.org/data)\n# Output all datasets to an output tarball into the data directory to be indexed\n# NOTE: The ncpi-dataset-catalog-results.tsv should be updated manually to ensure you sync all current Anvil datasets\n\n#######\n\nimport os\nimport shutil\nfrom ftplib import FTP, error_perm\nimport csv\n\n# Hard-coded relative paths for the anvil catalog input file and output bolus\n# This obviously isn't very elegant but it'll do for now\ninput_file = \"../data/ncpi-dataset-catalog-results.tsv\"\noutput_dir = \"../data/\"\n\n\n# Helper function\ndef download_dbgap_study(study_id, output_dir):\n    # Download a dbgap study to a specific directory\n\n    ftp = FTP('ftp.ncbi.nlm.nih.gov')\n    ftp.login()\n    study_variable = study_id.split('.')[0]\n    os.makedirs(f\"{output_dir}/{study_id}\")\n\n    # Step 1: First we try and get all the data_dict files\n    try:\n        ftp.cwd(f\"/dbgap/studies/{study_variable}/{study_id}/pheno_variable_summaries\")\n    except error_perm:\n        print(f\"WARN: Unable to find data dicts for study: {study_id}\")\n        # Delete subdirectory so we don't think it's full\n        shutil.rmtree(f\"{output_dir}/{study_id}\")\n        return False\n\n    ftp_filelist = ftp.nlst(\".\")\n    for ftp_filename in ftp_filelist:\n        if 'data_dict' in ftp_filename:\n            with open(f\"{output_dir}/{study_id}/{ftp_filename}\", \"wb\") as data_dict_file:\n                    ftp.retrbinary(f\"RETR {ftp_filename}\", data_dict_file.write)\n\n    # Step 2: Check to see if there's a GapExchange file in the parent folder\n    #         and if there is, get it.\n    ftp.cwd(f\"/dbgap/studies/{study_variable}/{study_id}\")\n    ftp_filelist = ftp.nlst(\".\")\n    for ftp_filename in ftp_filelist:\n        if 'GapExchange' in ftp_filename:\n            with open(f\"{output_dir}/{study_id}/{ftp_filename}\", \"wb\") as data_dict_file:\n                ftp.retrbinary(f\"RETR {ftp_filename}\", data_dict_file.write)\n    ftp.quit()\n    return True\n\n\ndef main():\n    # Delete any existing output dirs so you can ensure all datasets are fresh\n    #if os.path.isdir(output_dir):\n    #    shutil.rmtree(output_dir)\n\n    # Make new output dir\n    os.makedirs(f\"{output_dir}/\", exist_ok=True)\n\n    # Parse input table and download all valid dbgap datasets to output\n    missing_data_dict_studies = {}\n    studies = {}\n\n    with open(input_file) as csv_file:\n        csv_reader = csv.DictReader(csv_file, delimiter=\"\\t\")\n        header = False\n        for row in csv_reader:\n            if not header:\n                # Check to make sure tsv contains column for Study Accession\n                if \"Study Accession\" not in row:\n                    # Throw error if expected column is missing\n                    raise IOError(\"Input file must contain 'Study Accession' column\")\n                header = True\n                continue\n\n            # Get platform and make subdir if necessary\n            platform = row[\"Platform\"].split(\";\")\n            platform = platform[0] if \"BDC\" not in platform else \"BDC\"\n\n            # Add any phs dbgap studies to queue of files to get\n            study_id = row[\"Study Accession\"]\n            if study_id.startswith(\"phs\") and study_id not in studies:\n                studies[study_id] = True\n                try:\n                    # Try to download to output folder if the study hasn't already been downloaded\n                    if not os.path.exists(f\"{output_dir}/{platform}/{study_id}\"):\n                        print(f\"Downloading: {study_id}\")\n                        if not download_dbgap_study(study_id, f\"{output_dir}/{platform}\"):\n                            missing_data_dict_studies[study_id] = True\n\n                except Exception as e:\n                    # If anything happens, delete the folder so we don't mistake it for success\n                    shutil.rmtree(f\"{output_dir}/{platform}/{study_id}\")\n\n    # Count the number subdir currently in output_dir as the number of downloaded\n    num_downloaded  = len([path for path in os.walk(output_dir) if path[0] != output_dir])\n\n    # Get number of failed for missing data dicts\n    num_missing_data_dicts = len(list(missing_data_dict_studies.keys()))\n\n    # Total number of possible unique studies\n    num_possible = len(list(studies.keys()))\n\n    # Write out list of datasets with no data dicts\n    with open(f\"{output_dir}/download_summary.txt\", \"w\") as sum_file:\n        sum_file.write(f\"Unique dbgap datasets in ncpi table: {num_possible}\\n\")\n        sum_file.write(f\"Successfully Downloaded: {num_downloaded}\\n\")\n        sum_file.write(f\"Total dbgap datasests missing data dicts: {num_missing_data_dicts}\\n\")\n        sum_file.write(f\"Dbgap datasests missing data dicts:\\n\")\n        for item in missing_data_dict_studies:\n            sum_file.write(f\"{item}\\n\")\n\n    print(f\"Unique dbgap datasets in ncpi table: {num_possible}\\n\")\n    print(f\"Successfully Downloaded: {num_downloaded}\\n\")\n    print(f\"Total dbgap datasests missing data dicts: {num_missing_data_dicts}\\n\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "src/dug/config.py", "content": "import os\n\nfrom dataclasses import dataclass, field\n\n\nTRANQL_SOURCE: str = \"redis:test\"\n\n\n@dataclass\nclass Config:\n    \"\"\"\n    TODO: Populate description\n    \"\"\"\n\n    elastic_password: str = \"changeme\"\n    redis_password: str = \"changeme\"\n\n    elastic_host: str = \"elasticsearch\"\n    elastic_port: int = 9200\n    elastic_username: str = \"elastic\"\n    elastic_scheme: str = \"https\"\n    elastic_ca_path: str = \"\"\n\n    redis_host: str = \"redis\"\n    redis_port: int = 6379\n\n    nboost_host: str = \"nboost\"\n    nboost_port: int = 8000\n\n    program_sort_list: str = \"\"\n    program_description: dict=field(default_factory=lambda:{})\n    consent_id_path: str= \"\"\n    missing_studies_path: str=\"\"\n    missing_program_path: str=\"\"\n    \n\n    # Preprocessor config that will be passed to annotate.Preprocessor constructor\n    preprocessor: dict = field(\n        default_factory=lambda: {\n            \"debreviator\": {\"BMI\": \"body mass index\"},\n            \"stopwords\": [\"the\"],\n        }\n    )\n    annotator_type: str = \"monarch\"\n    # Annotator config that will be passed to annotate.Annotator constructor\n    annotator_args: dict = field(\n        default_factory=lambda: {\n            \"monarch\": {\n                \"url\": \"https://api.monarchinitiative.org/api/nlp/annotate/entities?min_length=4&longest_only=false&include_abbreviation=false&include_acronym=false&include_numbers=false&content=\"\n            },\n            \"sapbert\": {\n                \"classification_url\": \"https://med-nemo.apps.renci.org/annotate/\",\n                \"annotator_url\": \"https://sap-qdrant.apps.renci.org/annotate/\",\n                \"score_threshold\": 0.8,\n                \"bagel\": {\n                    \"enabled\": False,\n                    \"url\": \"https://bagel.apps.renci.org/group_synonyms_openai\",\n                    \"prompt\": \"bagel/ask_classes\",\n                    \"llm_args\": {\n                        \"llm_model_name\": \"gpt-4o-2024-05-13\",\n                        \"organization\": \"\",\n                        \"access_key\": \"\",\n                        \"llm_model_args\": {\n                            \"top_p\": 0,\n                            \"temperature\": 0.1\n                        }\n                    }\n                }\n            }\n        }\n    )\n\n    # Normalizer config that will be passed to annotate.Normalizer constructor\n    normalizer: dict = field(\n        default_factory=lambda: {\n            \"url\": \"https://nodenormalization-dev.apps.renci.org/get_normalized_nodes?conflate=false&description=true&curie=\"\n        }\n    )\n\n    # Synonym service config that will be passed to annotate.SynonymHelper constructor\n    synonym_service: dict = field(\n        default_factory=lambda: {\n            \"url\": \"https://name-resolution-sri.renci.org/reverse_lookup\"\n        }\n    )\n\n    # Ontology metadata helper config that will be passed to annotate.OntologyHelper constructor\n    ontology_helper: dict = field(\n        default_factory=lambda: {\n            \"url\": \"https://api.monarchinitiative.org/api/bioentity/\"\n        }\n    )\n\n    # Redlist of identifiers not to expand via TranQL\n    tranql_exclude_identifiers: list = field(default_factory=lambda: [\"CHEBI:17336\"])\n\n    tranql_queries: dict = field(\n        default_factory=lambda: {\n            \"disease\": [\"disease\", \"phenotypic_feature\"],\n            \"pheno\": [\"phenotypic_feature\", \"disease\"],\n            \"anat\": [\"disease\", \"anatomical_entity\"],\n            \"chem_to_disease\": [\"chemical_entity\", \"disease\"],\n            \"small_molecule_to_disease\": [\"small_molecule\", \"disease\"],\n            \"chemical_mixture_to_disease\": [\"chemical_mixture\", \"disease\"],\n            \"phen_to_anat\": [\"phenotypic_feature\", \"anatomical_entity\"],\n        }\n    )\n\n    node_to_element_queries: dict = field(\n        default_factory=lambda: {\n            # Dug element type to cast the query kg nodes to\n            \"cde\": {\n                # Parse nodes matching criteria in kg\n                \"node_type\": \"biolink:Publication\",\n                \"curie_prefix\": \"HEALCDE\",\n                # list of attributes that are lists to be casted to strings\n                \"list_field_choose_first\": [\"files\"],\n                \"attribute_mapping\": {\n                    # \"DugElement Attribute\" : \"KG Node attribute\"\n                    \"name\": \"name\",\n                    \"desc\": \"summary\",\n                    \"collection_name\": \"cde_category\",\n                    \"collection_id\": \"cde_category\",\n                    \"action\": \"files\",\n                },\n            }\n        }\n    )\n\n    concept_expander: dict = field(\n        default_factory=lambda: {\n            \"url\": \"https://tranql-dev.renci.org/tranql/query?dynamic_id_resolution=true&asynchronous=false\",\n            \"min_tranql_score\": 0.0,\n        }\n    )\n\n    # List of ontology types that can be used even if they fail normalization\n    ontology_greenlist: list = field(\n        default_factory=lambda: [\n            \"PATO\",\n            \"CHEBI\",\n            \"MONDO\",\n            \"UBERON\",\n            \"HP\",\n            \"MESH\",\n            \"UMLS\",\n        ]\n    )\n\n    @classmethod\n    def from_env(cls):\n        env_vars = {\n            \"elastic_host\": \"ELASTIC_API_HOST\",\n            \"elastic_port\": \"ELASTIC_API_PORT\",\n            \"elastic_scheme\": \"ELASTIC_API_SCHEME\",\n            \"elastic_ca_path\": \"ELASTIC_CA_PATH\",\n            \"elastic_username\": \"ELASTIC_USERNAME\",\n            \"elastic_password\": \"ELASTIC_PASSWORD\",\n            \"redis_host\": \"REDIS_HOST\",\n            \"redis_port\": \"REDIS_PORT\",\n            \"redis_password\": \"REDIS_PASSWORD\",\n            \"program_description\": \"PROGRAM_DESCRIPTION\",\n            \"consent_id_path\": \"CONSENT_ID_PATH\",\n            \"missing_studies_path\": \"MISSING_STUDIES_PATH\",\n            \"missing_program_path\": \"MISSING_PROGRAM_PATH\"\n        }\n\n        kwargs = {}\n\n        for kwarg, env_var in env_vars.items():\n            env_value = os.environ.get(env_var)\n            if env_value:\n                kwargs[kwarg] = env_value\n                if kwarg in ['redis_port', 'elastic_port']:\n                    kwargs[kwarg] = int(env_value)\n        return cls(**kwargs)\n"}
{"type": "source_file", "path": "src/dug/core/concept_expander.py", "content": "import json\nimport logging\nimport os\nimport requests\n\nimport dug.core.tranql as tql\n\nlogger = logging.getLogger('dug')\n\nlogging.getLogger(\"requests\").setLevel(logging.WARNING)\nlogging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n\nclass ConceptExpander:\n    def __init__(self, url, min_tranql_score=0.2):\n        self.url = url\n        self.min_tranql_score = min_tranql_score\n        self.include_node_keys = [\"id\", \"name\", \"synonyms\"]\n        self.include_edge_keys = []\n        self.tranql_headers = {\"accept\": \"application/json\", \"Content-Type\": \"text/plain\"}\n\n    def is_acceptable_answer(self, answer):\n        return True\n\n    def expand_identifier(self, identifier, query_factory, kg_filename, include_all_attributes=False):\n\n        answer_kgs = []\n\n        # Skip TranQL query if a file exists in the crawlspace exists already, but continue w/ answers\n        if os.path.exists(kg_filename):\n            logger.info(f\"identifier {identifier} is already crawled. Skipping TranQL query.\")\n            with open(kg_filename, 'r') as stream:\n                response = json.load(stream)\n        else:\n            query = query_factory.get_query(identifier)\n            logger.debug(query)\n            response = requests.post(\n                url=self.url,\n                headers=self.tranql_headers,\n                data=query).json()\n\n            # Case: Skip if empty KG\n            try:\n                if response[\"message\"] == 'Internal Server Error' or len(response[\"message\"][\"knowledge_graph\"][\"nodes\"]) == 0:\n                    logger.debug(f\"Did not find a knowledge graph for {query}\")\n                    logger.debug(f\"{self.url} returned response: {response}\")\n                    return []\n            except KeyError as e:\n                logger.error(f\"Could not find key: {e} in response: {response}\")\n\n            # Dump out to file if there's a knowledge graph\n            with open(kg_filename, 'w') as stream:\n                json.dump(response, stream, indent=2)\n\n        # Get nodes in knowledge graph hashed by ids for easy lookup\n        noMessage = (len(response.get(\"message\",{})) == 0)\n        statusError = (response.get(\"status\",\"\") == 'Error')\n        if noMessage or statusError:\n            # Skip on error\n            logger.info(f\"Error with identifier: {identifier}, response: {response}, kg_filename: '{kg_filename}'\")\n            return []\n        kg = tql.QueryKG(response)\n\n        for answer in kg.answers:\n            # Filter out answers that don't meet some criteria\n            # Right now just don't filter anything\n            logger.debug(f\"Answer: {answer}\")\n            if not self.is_acceptable_answer(answer):\n                logger.warning(\"Skipping answer as it failed one or more acceptance criteria. See log for details.\")\n                continue\n\n            # Get subgraph containing only information for this answer\n            try:\n                # Temporarily surround in try/except because sometimes the answer graphs\n                # contain invalid references to edges/nodes\n                # This will be fixed in Robokop but for now just silently warn if answer is invalid\n                node_attributes_filter = None if include_all_attributes else self.include_node_keys\n                edge_attributes_filter = None if include_all_attributes else self.include_edge_keys\n                answer_kg = kg.get_answer_subgraph(answer,\n                                                   include_node_keys=node_attributes_filter,\n                                                   include_edge_keys=edge_attributes_filter)\n\n                # Add subgraph to list of acceptable answers to query\n                answer_kgs.append(answer_kg)\n\n            except tql.MissingNodeReferenceError:\n                # TEMPORARY: Skip answers that have invalid node references\n                # Need this to be fixed in Robokop\n                logger.warning(\"Skipping answer due to presence of non-preferred id! \"\n                               \"See err msg for details.\")\n                continue\n            except tql.MissingEdgeReferenceError:\n                # TEMPORARY: Skip answers that have invalid edge references\n                # Need this to be fixed in Robokop\n                logger.warning(\"Skipping answer due to presence of invalid edge reference! \"\n                               \"See err msg for details.\")\n                continue\n\n        return answer_kgs\n    "}
{"type": "source_file", "path": "src/dug/core/factory.py", "content": "from typing import Dict\n\nimport redis\nfrom requests_cache import CachedSession\n\nimport dug.core.tranql as tql\nfrom dug.core.concept_expander import ConceptExpander\nfrom dug.config import Config as DugConfig, TRANQL_SOURCE\nfrom dug.core.crawler import Crawler\nfrom dug.core.parsers import Parser\nfrom dug.core.annotators import Annotator\nfrom dug.core.async_search import Search\nfrom dug.core.index import Index\n\n\nclass DugFactory:\n\n    def __init__(self, config: DugConfig):\n        self.config = config\n\n    def build_http_session(self) -> CachedSession:\n\n        redis_config = {\n            'host': self.config.redis_host,\n            'port': self.config.redis_port,\n            'password': self.config.redis_password,\n        }\n\n        return CachedSession(\n            cache_name='annotator',\n            backend='redis',\n            connection=redis.StrictRedis(**redis_config)\n        )\n\n    def build_crawler(self, target, parser: Parser, annotator: Annotator, element_type: str, tranql_source=None) -> Crawler:\n        crawler = Crawler(\n            crawl_file=str(target),\n            parser=parser,\n            annotator=annotator,\n            tranqlizer=self.build_tranqlizer(),\n            tranql_queries=self.build_tranql_queries(tranql_source),\n            http_session=self.build_http_session(),\n            exclude_identifiers=self.config.tranql_exclude_identifiers,\n            element_type=element_type,\n            element_extraction=self.build_element_extraction_parameters(),\n        )\n\n        return crawler\n\n    def build_tranqlizer(self) -> ConceptExpander:\n        return ConceptExpander(**self.config.concept_expander)\n\n    def build_tranql_queries(self, source=None) -> Dict[str, tql.QueryFactory]:\n\n        if source is None:\n            source = TRANQL_SOURCE\n        return {\n            key: tql.QueryFactory(self.config.tranql_queries[key], source)\n            for key\n            in self.config.tranql_queries\n        }\n\n    def build_search_obj(self, indices) -> Search:\n        return Search(self.config, indices=indices)\n\n    def build_indexer_obj(self, indices) -> Index:\n        return Index(self.config, indices=indices)\n\n    def build_element_extraction_parameters(self, source=None):\n        # Method reformats the node_to_element_queries object\n        # Uses tranql source use for concept crawling\n        if source is None:\n            source = TRANQL_SOURCE\n        queries = self.config.node_to_element_queries\n        # reformat config as array , in the crawler this is looped over \n        # to make calls to the expansion logic.\n        # casting config will be a set of conditions to perform casting on. \n        # Currently we are casting based on node type returned from the tranql query\n        # we might want to filter those based on curie type or other conditions , if \n        # node type is too broad.\n        return [\n            {\n                \"output_dug_type\": dug_type,\n                \"casting_config\": {\n                    \"node_type\": queries[dug_type][\"node_type\"],\n                    \"curie_prefix\": queries[dug_type][\"curie_prefix\"],\n                    \"attribute_mapping\": queries[dug_type][\"attribute_mapping\"],\n                    \"list_field_choose_first\": queries[dug_type][\"list_field_choose_first\"]\n                    # CDE's are only ones\n                    # but if we had two biolink:Publication nodes we want to conditionally\n                    # cast to other output_dug_type, we could extend this config\n                },\n                \"tranql_source\": source\n             } for dug_type in queries\n        ]\n"}
{"type": "source_file", "path": "bin/vlmd_to_dbgap_xml.py", "content": "#\n# Script to convert HEAL VLMD formatted files into dbGaP XML files for import into Dug.\n#\n# USAGE:\n#   python bin/vlmd_to_dbgap_xml.py input.csv --output output.xml\n#\n# This currently only supports HEAL VLMD CSV format files (with a focus on supporting the files produced\n# by the vlmd tool, see https://github.com/norc-heal/healdata-utils), but we hope to extend this to\n# support HEAL VLMD JSON files as well. See format documentation at https://github.com/HEAL/heal-metadata-schemas\n#\nimport csv\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport xml.dom.minidom as minidom\nimport xml.etree.ElementTree as ETree\nfrom collections import defaultdict\nfrom datetime import datetime\n\nimport click\n\n# Some defaults.\nHDP_ID_PREFIX = 'HEALDATAPLATFORM:'\n\n# Turn on logging\nlogging.basicConfig(level=logging.INFO)\n\n\n# Set up command line arguments.\n@click.command()\n@click.argument('input_file', type=click.Path(exists=True), required=True)\n@click.option('--file-format', '--format',\n              type=click.Choice(['CSV'], case_sensitive=False), default='CSV',\n              help=\"The file format to convert from (CSV in the VLMD format is the default, and the only one \"\n                   \"currently supported)\")\n@click.option('--output', '-o', type=click.File(mode='w'), default=sys.stdout,\n              help=\"Where the output dbGaP XML file is written. Defaults to STDOUT.\")\n@click.option('--study-id', type=str, help=\"The HEAL Data Platform study ID of this study. Should start with 'HDP'.\")\n@click.option('--appl-id', type=str, help=\"The NIH APPLication ID of this study. Usually a numerical identifier.\")\n@click.option('--study-name', type=str, help=\"The name or title of this study.\")\ndef vlmd_to_dbgap_xml(input_file, output, file_format, study_id, appl_id, study_name):\n    \"\"\"\n    Convert a VLMD file (INPUT_FILE) into a dbGaP XML file for ingest into Dug.\n    \\f\n    # \\f truncates the help text as per https://click.palletsprojects.com/en/8.1.x/documentation/#truncating-help-texts\n\n    :param input_file: The VLMD file to read.\n    :param output: Where the dbGaP XML file should be written.\n    :param file_format: The file format to read. Only [HEAL VLMD] 'CSV' is currently supported.\n    :param study_id: The HEAL Data Platform Study ID to use in Dug (without a prefix).\n    :param appl_id: The APPL ID to use in Dug.\n    :param study_name: The study name ot use.\n    \"\"\"\n\n    assert file_format == 'CSV', 'HEAL VLMD CSV is the only currently supported input format.'\n\n    with open(click.format_filename(input_file), 'r') as input:\n        # dbGaP files are represented in XML as `data_table`s. We start by creating one.\n        data_table = ETree.Element('data_table')\n\n        # Write out the study_id.\n        if not study_id:\n            # If no study ID is provided, use the input filename.\n            # TODO: once we support JSON, we can use either root['title'] or root['description'] here.\n            study_id = os.path.basename(input_file)\n        else:\n            # Assume it is an HDP identifier, so add the HDP_ID_PREFIX.\n            study_id = HDP_ID_PREFIX + study_id\n        data_table.set('study_id', study_id)\n\n        # Add the APPL ID.\n        if appl_id:\n            data_table.set('appl_id', appl_id)\n\n        # Add the study title.\n        # Note: not a dbGaP XML field! We make this up for communication.\n        if study_name:\n            data_table.set('study_name', study_name)\n\n        # Record the creation date as this moment.\n        data_table.set('date_created', datetime.now().isoformat())\n\n        # Read input file and convert variables into\n        if file_format == 'CSV':\n            reader = csv.DictReader(input)\n\n            # Some counts that are currently useful.\n            counters = defaultdict(int)\n\n            unique_variable_ids = set()\n            for index, row in enumerate(reader):\n                counters['row'] += 1\n                row_index = index + 1  # Convert from zero-based index to one-based index.\n\n                variable = ETree.SubElement(data_table, 'variable')\n\n                # Variable name\n                var_name = row.get('name')\n                if not var_name:\n                    logging.error(f\"No variable name found in row on line {index + 1}, skipping.\")\n                    counters['no_varname'] += 1\n                    continue\n                counters['has_varname'] += 1\n                # Make sure the variable ID is unique (by adding `_1`, `_2`, ... to the end of it).\n                variable_index = 0\n                while var_name in unique_variable_ids:\n                    variable_index += 1\n                    var_name = row['name'] + '_' + variable_index\n                variable.set('id', var_name)\n                if var_name != row['name']:\n                    logging.warning(f\"Duplicate variable ID detected for {row['name']}, so replaced it with \"\n                                    f\"{var_name} -- note that the name element is unchanged.\")\n                name = ETree.SubElement(variable, 'name')\n                name.text = var_name\n\n                # Variable title\n                # NOTE: this is not yet supported by Dug!\n                if row.get('title'):\n                    title = ETree.SubElement(variable, 'title')\n                    title.text = row['title']\n                    counters['has_title'] += 1\n                else:\n                    counters['no_title'] += 1\n\n                # Variable description\n                if row.get('description'):\n                    desc = ETree.SubElement(variable, 'description')\n                    desc.text = row['description']\n                    counters['has_description'] += 1\n                else:\n                    counters['no_description'] += 1\n\n                # Module (questionnaire/subsection name) Export the `module` field so that we can look for\n                # instruments.\n                #\n                # TODO: this is a custom field. Instead of this, we could export each data dictionary as a separate\n                # dbGaP file. Need to check to see what works better for Dug ingest.\n                if row.get('module'):\n                    variable.set('module', row['module'])\n                    if 'module_counts' not in counters:\n                        counters['module_counts'] = defaultdict(int)\n                    counters['module_counts'][row['module']] += 1\n                else:\n                    counters['no_module'] += 1\n\n                # Constraints\n\n                # Minium and maximum values\n                if row.get('constraints.maximum'):\n                    logical_max = ETree.SubElement(variable, 'logical_max')\n                    logical_max.text = str(row['constraints.maximum'])\n                if row.get('constraints.minimum'):\n                    logical_min = ETree.SubElement(variable, 'logical_min')\n                    logical_min.text = str(row['constraints.minimum'])\n\n                # Maximum length ('constraints.maxLength') is not supported in dbGaP XML, so we ignore it.\n\n                # We ignore 'constraints.pattern' and 'format' for now, but we can try to include them in the\n                # description later if that is useful.\n                if row.get('constraints.pattern'):\n                    counters['constraints.pattern'] += 1\n                    logging.warning(f\"`constraints.pattern` of {row['constraints.pattern']} found in row {row_index}, \"\n                                    f\"but pattern constraints are not currently being written.\")\n                if row.get('format'):\n                    counters['format'] += 1\n                    logging.warning(f\"Found `format` of {row['format']} found in row {row_index}, but format is not \"\n                                    f\"currently being written.\")\n\n                # Process enumerated and encoded values.\n                encs = {}\n                if row.get('encodings'):\n                    counters['encodings'] += 1\n\n                    for encoding in re.split(\"\\\\s*\\\\|\\\\s*\", row['encodings']):\n                        m = re.fullmatch(\"^\\\\s*(.*?)\\\\s*=\\\\s*(.*)\\\\s*$\", encoding)\n                        if not m:\n                            raise RuntimeError(\n                                f\"Could not parse encodings {row['encodings']} on row {row_index}\")\n                        key = m.group(1)\n                        value = m.group(2)\n\n                        if key in encs:\n                            raise RuntimeError(\n                                f\"Duplicate key detected in encodings {row['encodings']} on row {row_index}\")\n                        encs[key] = value\n\n                for key, value in encs.items():\n                    value_element = ETree.SubElement(variable, 'value')\n                    value_element.set('code', key)\n                    value_element.text = value\n\n                # Double-check encodings with constraints.enum\n                if row.get('constraints.enum'):\n                    enums = re.split(\"\\\\s*\\\\|\\\\s*\", row['constraints.enum'])\n                    if set(enums) != set(encs.keys()):\n                        logging.error(f\"`constraints.enum` ({row['constraints.enum']}) and `encodings` ({row['encodings']}) do not match.\")\n                        counters['enum_encoding_mismatch'] += 1\n\n                # Variable type.\n                typ = row.get('type')\n                if encs:\n                    typ = 'encoded value'\n                if typ:\n                    type_element = ETree.SubElement(variable, 'type')\n                    type_element.text = typ\n\n                # We currently ignore metadata fields not usually filled in for input VLMD files:\n                # ordered, missingValues, trueValues, falseValues, repo_link\n\n                # We currently ignore all standardMappings: standardsMappings.type, standardsMappings.label,\n                # standardsMappings.url, standardsMappings.source, standardsMappings.id\n                # We currently ignore all relatedConcepts: relatedConcepts.type, relatedConcepts.label,\n                # relatedConcepts.url, relatedConcepts.source, relatedConcepts.id\n\n                # We currently ignore all univarStats vars: univarStats.median, univarStats.mean, univarStats.std,\n                # univarStats.min, univarStats.max, univarStats.mode, univarStats.count,\n                # univarStats.twentyFifthPercentile, univarStats.seventyFifthPercentile,\n                # univarStats.categoricalMarginals.name, univarStats.categoricalMarginals.count\n        else:\n            # This shouldn't be needed, since Click should catch any file format not in the accepted list.\n            raise RuntimeError(f\"Unsupported file format {file_format}\")\n\n        # Write out dbGaP XML.\n        xml_str = ETree.tostring(data_table, encoding='unicode')\n        pretty_xml_str = minidom.parseString(xml_str).toprettyxml()\n        print(pretty_xml_str, file=output)\n\n        # Display counters.\n        logging.info(f\"Counters: {json.dumps(counters, sort_keys=True, indent=2)}\")\n\n\n# Run vlmd_to_dbgap_xml() if not used as a library.\nif __name__ == \"__main__\":\n    vlmd_to_dbgap_xml()\n"}
{"type": "source_file", "path": "src/dug/core/crawler.py", "content": "import json\nimport logging\nimport os\nimport traceback\nfrom typing import List\n\nfrom dug.core.parsers import Parser, DugElement, DugConcept\nfrom dug.core.annotators import Annotator, DugIdentifier\nimport dug.core.tranql as tql\nfrom dug.utils import biolink_snake_case, get_formatted_biolink_name\n\nlogger = logging.getLogger('dug')\n\n\nclass Crawler:\n    def __init__(self, crawl_file: str, parser: Parser, annotator: Annotator,\n                 tranqlizer, tranql_queries,\n                 http_session, exclude_identifiers=None, element_type=None,\n                 element_extraction=None):\n\n        if exclude_identifiers is None:\n            exclude_identifiers = []\n\n        self.crawl_file = crawl_file\n        self.parser: Parser = parser\n        self.element_type = element_type\n        self.annotator: Annotator = annotator\n        self.tranqlizer = tranqlizer\n        self.tranql_queries = tranql_queries\n        self.http_session = http_session\n        self.exclude_identifiers = exclude_identifiers\n        self.element_extraction = element_extraction\n        self.elements = []\n        self.concepts = {}\n        self.crawlspace = \"crawl\"\n\n    def make_crawlspace(self):\n        if not os.path.exists(self.crawlspace):\n            try:\n                os.makedirs(self.crawlspace)\n            except Exception as e:\n                print(f\"-----------> {e}\")\n                traceback.print_exc()\n\n    def crawl(self):\n\n        # Create directory for storing temporary results\n        self.make_crawlspace()\n\n        # Read in elements from parser\n        self.elements = self.parser(self.crawl_file)\n\n        # Optionally coerce all elements to be a specific type\n        for element in self.elements:\n            if isinstance(element, DugElement) and self.element_type is not None:\n                element.type = self.element_type\n\n        # Annotate elements\n        self.annotate_elements()\n\n        # if elements are extracted from the graph this array will contain the new dug elements\n        dug_elements_from_graph = []\n\n        # Expand concepts to other concepts\n        concept_file = open(f\"{self.crawlspace}/concept_file.json\", \"w\")\n        for concept_id, concept in self.concepts.items():\n            # Use TranQL queries to fetch knowledge graphs containing related but not synonymous biological terms\n            self.expand_concept(concept)\n\n            # Traverse identifiers to create single list of of search targets/synonyms for concept\n            concept.set_search_terms()\n\n            # Traverse kg answers to create list of optional search targets containing related concepts\n            concept.set_optional_terms()\n\n            # Remove duplicate search terms and optional search terms\n            concept.clean()\n\n            # Write concept out to a file\n            concept_file.write(f\"{json.dumps(concept.get_searchable_dict(), indent=2)}\")\n\n            if self.element_extraction:\n                for element_extraction_config in self.element_extraction:\n                    casting_config = element_extraction_config['casting_config']\n                    tranql_source = element_extraction_config['tranql_source']\n                    dug_element_type = element_extraction_config['output_dug_type']\n                    dug_elements_from_graph += self.expand_to_dug_element(\n                        concept=concept,\n                        casting_config=casting_config,\n                        dug_element_type=dug_element_type,\n                        tranql_source=tranql_source\n                    )\n\n        # add new elements to parsed elements\n        self.elements += dug_elements_from_graph\n\n        # Set element optional terms now that concepts have been expanded\n        # Open variable file for writing\n        variable_file = open(f\"{self.crawlspace}/element_file.json\", \"w\")\n        for element in self.elements:\n            if isinstance(element, DugElement):\n                element.set_optional_terms()\n                variable_file.write(f\"{element.get_searchable_dict()}\\n\")\n\n        # Close concept, element files\n        concept_file.close()\n        variable_file.close()\n\n    def annotate_elements(self):\n\n        # Annotate elements/concepts and create new concepts based on the ontology identifiers returned\n        logger.info(f\"annotate {len(self.elements)} elements\")\n        for n, element in enumerate(self.elements):\n            # If element is actually a pre-loaded concept (e.g. TOPMed Tag), add that to list of concepts\n            if isinstance(element, DugConcept):\n                self.concepts[element.id] = element\n\n            # Annotate element with normalized ontology identifiers\n            logger.info(f\"annotate element #{n+1}/{len(self.elements)} '{element.id}'\")\n            self.annotate_element(element)\n            if isinstance(element, DugElement):\n                element.set_search_terms()\n\n        # Now that we have our concepts and elements fully annotated, we need to\n        # Make sure elements inherit the identifiers from their user-defined parent concepts\n        # E.g. TOPMedTag1 was annotated with HP:123 and MONDO:12.\n        # Each element assigned to TOPMedTag1 needs to be associated with those concepts as well\n        for element in self.elements:\n            # Skip user-defined concepts\n            if isinstance(element, DugConcept):\n                continue\n\n            # Associate identifiers from user-defined concepts (see example above)\n            # with child elements of those concepts\n            concepts_to_add = []\n            for concept_id, concept in element.concepts.items():\n                for ident_id, identifier in concept.identifiers.items():\n                    if ident_id not in element.concepts and ident_id in self.concepts:\n                        concepts_to_add.append(self.concepts[ident_id])\n\n            for concept_to_add in concepts_to_add:\n                element.add_concept(concept_to_add)\n\n    def annotate_element(self, element):\n\n        # Annotate with a set of normalized ontology identifiers\n        # self.DugAnnotator.annotator()\n        identifiers: List[DugIdentifier] = self.annotator(text=element.ml_ready_desc,\n                                              http_session=self.http_session)\n        # Future thoughts... should we be passing in the stpe DugIdentifier here instead?\n\n\n        # Each identifier then becomes a concept that links elements together\n        logger.info(\"Got %d identifiers for %s\", len(identifiers) , element.ml_ready_desc)\n        for identifier in identifiers:\n            if identifier.id not in self.concepts:\n                # Create concept for newly seen identifier\n                concept = DugConcept(concept_id=identifier.id,\n                                                       name=identifier.label,\n                                                       desc=identifier.description,\n                                                       concept_type=identifier.types)\n                # Add to list of concepts\n                self.concepts[identifier.id] = concept\n\n            # Add identifier to list of identifiers associated with concept\n            self.concepts[identifier.id].add_identifier(identifier)\n\n            # Create association between newly created concept and element\n            # (unless element is actually a user-defined concept)\n            if isinstance(element, DugElement):\n                element.add_concept(self.concepts[identifier.id])\n\n            # If element is actually a user defined concept (e.g. TOPMedTag), associate ident with concept\n            # Child elements of these user-defined concepts will inherit all these identifiers as well.\n            elif isinstance(element, DugConcept):\n                element.add_identifier(identifier)\n\n    def expand_concept(self, concept):\n\n        # Get knowledge graphs of terms related to each identifier\n        for ident_id, identifier in concept.identifiers.items():\n\n            # Conditionally skip some identifiers if they are listed in config\n            if ident_id in self.exclude_identifiers:\n                continue\n\n            # Use pre-defined queries to search for related knowledge graphs that include the identifier\n            for query_name, query_factory in self.tranql_queries.items():\n\n                # Skip query if the identifier is not a valid query for the query class\n                if not query_factory.is_valid_curie(ident_id):\n                    logger.info(f\"identifier {ident_id} is not valid for query type {query_name}. Skipping!\")\n                    continue\n\n                # Fetch kg and answer\n                kg_outfile = f\"{self.crawlspace}/{ident_id}_{query_name}.json\"\n                answers = self.tranqlizer.expand_identifier(ident_id, query_factory, kg_outfile)\n\n                # Add any answer knowledge graphs to\n                for answer in answers:\n                    concept.add_kg_answer(answer, query_name=query_name)\n\n    def expand_to_dug_element(self,\n                              concept,\n                              casting_config,\n                              dug_element_type,\n                              tranql_source):\n        \"\"\"\n        Given a concept look up the knowledge graph to construct dug elements out of kg results\n        does concept -> target_node_type crawls and converts target_node_type to dug element of type `dug_element_type`\n        \"\"\"\n        elements = []\n        # using node_type as the primary criteria for matching nodes to element type.\n        target_node_type = casting_config[\"node_type\"]\n        curie_filter = casting_config[\"curie_prefix\"]\n        attribute_mapping = casting_config[\"attribute_mapping\"]\n        array_to_string = casting_config[\"list_field_choose_first\"]\n        # converts any of the following notations \n        # biolink:Publication , biolink.Publication  to publication \n        target_node_type_snake_case = biolink_snake_case(target_node_type.replace(\"biolink.\", \"\").replace(\"biolink:\", \"\"))\n        for ident_id, identifier in concept.identifiers.items():\n\n            # Check to see if the concept identifier has types defined, this is used to create\n            # tranql queries below.\n            if not identifier.types:\n                continue\n\n            # convert the first type to snake case to be used in tranql query.\n            # first type is the leaf type, this is coming from Node normalization.\n            # note when using bmt it returns biolink: prefix so we need to replace biolink: and snake case it for tranql.\n            node_type = biolink_snake_case(get_formatted_biolink_name(identifier.types).replace(\"biolink:\", \"\"))\n            try:\n                # Tranql query factory currently supports select node types as valid query\n                # Types missing from QueryFactory.data_types will be skipped with this try catch\n                query = tql.QueryFactory([node_type, target_node_type_snake_case], tranql_source)\n            except tql.InvalidQueryError as exception:\n                logger.debug(f\"Skipping  {ident_id}, {exception}\")\n                continue\n\n            # check if tranql query object can use the curie.\n            if query.is_valid_curie(ident_id):\n                logger.info(f\"Expanding {ident_id} to other dug elements\")\n                # Fetch kg and answer\n                # Fetch kg and answer\n                # replace \":\" with \"~\" to avoid windows os errors\n                kg_outfile = f\"{self.crawlspace}/\" + f\"{ident_id}_{target_node_type}.json\".replace(\":\", \"~\")\n\n                # query tranql, answers will include all node and edge attributes\n                answers = self.tranqlizer.expand_identifier(ident_id, query,\n                                                            kg_filename=kg_outfile,\n                                                            include_all_attributes=True)\n                # for each answer construct a dug element\n                for answer in answers:\n                    # here we will inspect the answers create new dug elements based on target node type\n                    # and return the variables.\n                    for node_id, node in answer.nodes.items():\n                        # support both biolink. and biolink: prefixes\n                        snake_case_category = [\n                            biolink_snake_case(cat.replace(\"biolink.\", \"\").replace(\"biolink:\", \"\")) \n                            for cat in node['category']\n                            ]\n                        if target_node_type_snake_case in snake_case_category:\n                            if node['id'].startswith(curie_filter):\n                                element_attribute_args = {\"elem_id\": node_id, \"elem_type\": dug_element_type}\n                                for key in attribute_mapping:\n                                    mapped_value = node.get(attribute_mapping[key], \"\")\n                                    # treat all attributes as strings \n                                    if attribute_mapping[key] in array_to_string and isinstance(mapped_value, list) and len(mapped_value) > 0:\n                                        mapped_value = mapped_value[0]\n                                    element_attribute_args.update({key: mapped_value})\n                                element = DugElement(\n                                    **element_attribute_args\n                                )\n                                element.add_concept(concept)\n                                elements.append(element)\n        return elements\n"}
{"type": "source_file", "path": "src/dug/core/async_search.py", "content": "\"\"\"Implements search methods using async interfaces\"\"\"\nimport logging\nfrom elasticsearch import AsyncElasticsearch\nfrom elasticsearch.helpers import async_scan\nimport ssl,json\nfrom dug.config import Config\n\n\nlogger = logging.getLogger('dug')\n\n\nclass SearchException(Exception):\n    def __init__(self, message, details):\n        self.message = message\n        self.details = details\n\n\nclass Search:\n    \"\"\" Search -\n    1. Lexical fuzziness; (a) misspellings - a function of elastic.\n    2. Fuzzy ontologically;\n       (a) expand based on core queries\n         * phenotype->study\n         * phenotype->disease->study\n         * disease->study\n         * disease->phenotype->study\n    \"\"\"\n\n    def __init__(self, cfg: Config, indices=None):\n\n        if indices is None:\n            indices = ['concepts_index', 'variables_index', 'kg_index']\n\n        self._cfg = cfg\n        logger.debug(f\"Connecting to elasticsearch host: \"\n                     f\"{self._cfg.elastic_host} at port: \"\n                     f\"{self._cfg.elastic_port}\")\n\n        self.indices = indices\n        self.hosts = [{'host': self._cfg.elastic_host,\n                       'port': self._cfg.elastic_port,\n                       'scheme': self._cfg.elastic_scheme}]\n\n        logger.debug(f\"Authenticating as user \"\n                     f\"{self._cfg.elastic_username} \"\n                     f\"to host:{self.hosts}\")\n        if self._cfg.elastic_scheme == \"https\":\n            ssl_context = ssl.create_default_context(\n                cafile=self._cfg.elastic_ca_path\n            )\n            self.es = AsyncElasticsearch(hosts=self.hosts,\n                                     basic_auth=(self._cfg.elastic_username,\n                                                self._cfg.elastic_password),\n                                                ssl_context=ssl_context)\n        else:\n            self.es = AsyncElasticsearch(hosts=self.hosts,\n                                     basic_auth=(self._cfg.elastic_username,\n                                                self._cfg.elastic_password))\n\n    async def dump_concepts(self, index, query={}, size=None,\n                            fuzziness=1, prefix_length=3):\n        \"\"\"\n        Get everything from concept index\n        \"\"\"\n        query = {\n            \"match_all\": {}\n        }\n        body = {\"query\": query}\n        await self.es.ping()\n        total_items = await self.es.count(body=body, index=index)\n        counter = 0\n        all_docs = []\n        async for doc in async_scan(\n                client=self.es,\n                query=body,\n                index=index\n        ):\n            if counter == size and size != 0:\n                break\n            counter += 1\n            all_docs.append(doc)\n        return {\n            \"status\": \"success\",\n            \"result\": {\n                \"hits\": {\n                    \"hits\": all_docs\n                },\n                \"total_items\": total_items\n            },\n            \"message\": \"Search result\"\n        }\n\n    async def agg_data_type(self):\n        aggs = {\n            \"data_type\": {\n                \"terms\": {\n                    \"field\": \"data_type.keyword\",\n                }\n            }\n        }\n\n        body = {'aggs': aggs}\n        results = await self.es.search(\n            index=\"variables_index\",\n            body=body\n        )\n        data_type_list = [data_type['key'] for data_type in\n                          results['aggregations']['data_type']['buckets']]\n        results.update({'data type list': data_type_list})\n        return data_type_list\n\n    @staticmethod\n    def _get_concepts_query(query, fuzziness=1, prefix_length=3):\n        \"Static data structure populator, pulled for easier testing\"\n        query_object = {\n            \"query\" : {\n                \"bool\": {\n                    \"filter\": {\n                        \"bool\": {\n                            \"must\": [\n                                {\"wildcard\": {\"description\": \"?*\"}},\n                                {\"wildcard\": {\"name\": \"?*\"}}\n                            ]\n                        }\n                    },\n                    \"should\": [\n                        {\n                            \"match_phrase\": {\n                                \"name\": {\n                                    \"query\": query,\n                                    \"boost\": 10\n                                }\n                            }\n                        },\n                        {\n                            \"match_phrase\": {\n                                \"description\": {\n                                    \"query\": query,\n                                    \"boost\": 6\n                                }\n                            }\n                        },\n                        {\n                            \"match_phrase\": {\n                                \"search_terms\": {\n                                    \"query\": query,\n                                    \"boost\": 8\n                                }\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"name\": {\n                                    \"query\": query,\n                                    \"fuzziness\": fuzziness,\n                                    \"prefix_length\": prefix_length,\n                                    \"operator\": \"and\",\n                                    \"boost\": 4\n                                }\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"search_terms\": {\n                                    \"query\": query,\n                                    \"fuzziness\": fuzziness,\n                                    \"prefix_length\": prefix_length,\n                                    \"operator\": \"and\",\n                                    \"boost\": 5\n                                }\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"description\": {\n                                    \"query\": query,\n                                    \"fuzziness\": fuzziness,\n                                    \"prefix_length\": prefix_length,\n                                    \"operator\": \"and\",\n                                    \"boost\": 3\n                                }\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"description\": {\n                                    \"query\": query,\n                                    \"fuzziness\": fuzziness,\n                                    \"prefix_length\": prefix_length,\n                                    \"boost\": 2\n                                }\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"search_terms\": {\n                                    \"query\": query,\n                                    \"fuzziness\": fuzziness,\n                                    \"prefix_length\": prefix_length,\n                                    \"boost\": 1\n                                }\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"optional_terms\": {\n                                    \"query\": query,\n                                    \"fuzziness\": fuzziness,\n                                    \"prefix_length\": prefix_length\n                                }\n                            }\n                        }\n                    ],\n                    \"minimum_should_match\": 1,\n                }\n            }\n        }\n        return query_object\n    \n    def is_simple_search_query(self, query):\n        return \"*\" in query or \"\\\"\" in query or \"+\" in query or \"-\" in query\n\n    async def search_concepts(self, query, offset=0, size=None, types=None,\n                              **kwargs):\n        \"\"\"\n        Changed to a long boolean match query to optimize search results\n        \"\"\"\n        if self.is_simple_search_query(query):\n            search_body = self.get_simple_concept_search_query(query)\n        else:\n            search_body = self._get_concepts_query(query, **kwargs)\n        # Get aggregated counts of biolink types\n        search_body['aggs'] = {'type-count': {'terms': {'field': 'type'}}}\n        if isinstance(types, list):\n            search_body['post_filter'] = {\n                \"bool\": {\n                    \"should\": [\n                        {'term': {'type': {'value': t}}} for t in types\n                    ],\n                    \"minimum_should_match\": 1\n                }\n            }\n        search_results = await self.es.search(\n            index=\"concepts_index\",\n            body=search_body,\n            filter_path=['hits.hits._id', 'hits.hits._type',\n                         'hits.hits._source', 'hits.hits._score',\n                         'hits.hits._explanation', 'aggregations'],\n            from_=offset,\n            size=size,\n            explain=True\n        )\n        # Aggs/post_filter aren't supported by count\n        del search_body[\"aggs\"]\n        if \"post_filter\" in search_body:\n            # We'll move the post_filter into the actual filter\n            search_body[\"query\"][\"bool\"][\"filter\"][\"bool\"].update(\n                search_body[\"post_filter\"][\"bool\"]\n            )\n            del search_body[\"post_filter\"]\n        total_items = await self.es.count(\n            body=search_body,\n            index=\"concepts_index\"\n        )\n\n        # Simplify the data structure we get from aggregations to put into the\n        # return value. This should be a count of documents hit for every type\n        # in the search results.\n        aggregations = search_results.pop('aggregations')\n        concept_types = {\n            bucket['key']: bucket['doc_count'] for bucket in\n            aggregations['type-count']['buckets']\n        }\n        search_results.update({'total_items': total_items['count']})\n        search_results.update({'concept_types': concept_types})\n        return search_results\n\n    async def search_variables(self, concept=\"\", query=\"\", size=None,\n                               data_type=None, offset=0, fuzziness=1,\n                               prefix_length=3, index=None):\n        \"\"\"\n        In variable search, the concept MUST match one of the identifiers in the list\n        The query can match search_terms (hence, \"should\") for ranking.\n\n        Results Return\n        The search result is returned in JSON format {collection_id:[elements]}\n\n        Filter\n        If a data_type is passed in, the result will be filtered to only contain\n        the passed-in data type.\n        \"\"\"\n        if self.is_simple_search_query(query):\n            es_query = self.get_simple_variable_search_query(concept, query)\n        else:\n            es_query = self._get_var_query(concept, fuzziness, prefix_length, query)\n        \n        if index is None:\n            index = \"variables_index\"\n\n        total_items = await self.es.count(body=es_query, index=index)\n        search_results = await self.es.search(\n            index=\"variables_index\",\n            body=es_query,\n            filter_path=['hits.hits._id', 'hits.hits._type',\n                         'hits.hits._source', 'hits.hits._score'],\n            from_=offset,\n            size=size\n        )\n\n        search_result_hits = []\n\n        if \"hits\" in search_results:\n            search_result_hits = search_results['hits']['hits']\n\n        return self._make_result(data_type, search_result_hits , total_items, True)\n\n    async def search_vars_unscored(self, concept=\"\", query=\"\",\n                                   size=None, data_type=None,\n                                   offset=0, fuzziness=1,\n                                   prefix_length=3):\n        \"\"\"\n        In variable search, the concept MUST match one of the identifiers in the list\n        The query can match search_terms (hence, \"should\") for ranking.\n\n        Results Return\n        The search result is returned in JSON format {collection_id:[elements]}\n\n        Filter\n        If a data_type is passed in, the result will be filtered to only contain\n        the passed-in data type.\n        \"\"\"\n        es_query = self._get_var_query(concept, fuzziness, prefix_length, query)\n        total_items = await self.es.count(body=es_query, index=\"variables_index\")\n        search_results = []\n        async for r in async_scan(self.es, query=es_query):\n            search_results.append(r)\n\n        return self._make_result(data_type, search_results, total_items, False)\n\n    def _make_result(self, data_type, search_results, total_items, scored: bool):\n        # Reformat Results\n        new_results = {}\n        if not search_results:\n            # we don't want to error on a search not found\n            new_results.update({'total_items': total_items['count']})\n            return new_results\n        for elem in search_results:\n            elem_s = elem['_source']\n            elem_type = elem_s['data_type']\n            if elem_type not in new_results:\n                new_results[elem_type] = {}\n\n            elem_id = elem_s['element_id']\n            coll_id = elem_s['collection_id']\n            elem_info = {\n                \"description\": elem_s['element_desc'],\n                \"e_link\": elem_s['element_action'],\n                \"id\": elem_id,\n                \"name\": elem_s['element_name'],\n                \"metadata\": elem_s.get('metadata', {})\n            }\n\n            if scored:\n                elem_info[\"score\"] = round(elem['_score'], 6)\n\n            # Case: collection not in dictionary for given data_type\n            if coll_id not in new_results[elem_type]:\n                # initialize document\n                doc = {\n                    'c_id': coll_id,\n                    'c_link': elem_s['collection_action'],\n                    'c_name': elem_s['collection_name'],\n                    'elements': [elem_info]\n                }\n                # save document\n                new_results[elem_type][coll_id] = doc\n\n            # Case: collection already in dictionary for given\n            # element_type; append elem_info.  Assumes no duplicate\n            # elements\n            else:\n                new_results[elem_type][coll_id]['elements'].append(elem_info)\n        # Flatten dicts to list\n        for i in new_results:\n            new_results[i] = list(new_results[i].values())\n        # Return results\n        if bool(data_type):\n            if data_type in new_results:\n                new_results = new_results[data_type]\n            else:\n                new_results = {}\n\n        # better to update UI to accept optional \"total_items\" so it does not fail while fetching data for studies tab\n        # and remove this if\n        if not scored:\n            new_results.update({'total_items': total_items['count']})\n\n        return new_results\n\n    async def search_kg(self, unique_id, query, offset=0, size=None,\n                        fuzziness=1, prefix_length=3):\n        \"\"\"\n        In knowledge graph search the concept MUST match the unique ID\n        The query MUST match search_targets.  The updated query allows for\n        fuzzy matching and for the default OR behavior for the query.\n        \"\"\"\n        query = {\n            \"bool\": {\n                \"must\": [\n                    {\"term\": {\n                        \"concept_id.keyword\": unique_id\n                    }\n                    },\n                    {'query_string': {\n                        \"query\": query,\n                        \"fuzziness\": fuzziness,\n                        \"fuzzy_prefix_length\": prefix_length,\n                        \"default_field\": \"search_targets\"\n                    }\n                    }\n                ]\n            }\n        }\n        body = {'query': query}\n        total_items = await self.es.count(body=body, index=\"kg_index\")\n        search_results = await self.es.search(\n            index=\"kg_index\",\n            body=body,\n            filter_path=['hits.hits._id', 'hits.hits._type',\n                         'hits.hits._source'],\n            from_=offset,\n            size=size\n        )\n        search_results.update({'total_items': total_items['count']})\n        return search_results\n\n    async def search_study(self, study_id=None, study_name=None, offset=0, size=None):\n        \"\"\"\n        Search for studies by unique_id (ID or name) and/or study_name.\n        \"\"\"\n        # Define the base query\n         # Define the base query\n        query_body = {\n            \"bool\": {\n                \"must\": []\n            }\n        }\n        \n        # Add conditions based on user input\n        if study_id:\n            query_body[\"bool\"][\"must\"].append({\n                \"match\": {\"collection_id\": study_id}\n            })\n        \n        if study_name:\n            query_body[\"bool\"][\"must\"].append({\n                \"match\": {\"collection_name\": study_name}\n            })\n\n        print(\"query_body\",query_body)\n        body = {'query': query_body}\n        total_items = await self.es.count(body=body, index=\"variables_index\")\n        search_results = await self.es.search(\n            index=\"variables_index\",\n            body=body,\n            filter_path=['hits.hits._id', 'hits.hits._type', 'hits.hits._source'],\n            from_=offset,\n            size=size\n        )\n        search_results.update({'total_items': total_items['count']})\n        return search_results\n\n    async def search_program(self, program_name=None, offset=0, size=None):\n        \"\"\"\n        Search for studies by unique_id (ID or name) and/or study_name.\n        \"\"\"\n # Initialize the query_body with the outer structure\n        query_body = {\n            \"query\": {\n                \"bool\": {\n                    \"must\": []\n                }\n            },\n            \"aggs\": {\n                \"unique_collection_ids\": {\n                    \"terms\": {\n                        \"field\": \"collection_id.keyword\",\n                        \"size\":1000\n                    },\n                    \"aggs\": {\n                        \"collection_details\": {\n                            \"top_hits\": {\n                                \"_source\": [\"collection_id\", \"collection_name\", \"collection_action\"],\n                                \"size\": 1\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        # Add conditions based on user input\n        if program_name:\n            # Lowercase the program_name before adding it to the query\n            # program_name = program_name.lower()\n            query_body[\"query\"][\"bool\"][\"must\"].append(\n                {\"term\": {\"data_type.keyword\": program_name}}\n            )\n\n        #print(\"query_body\", query_body)\n\n        # Prepare the query body for execution\n        body = query_body\n    \n        # Execute the search query\n        search_results = await self.es.search(\n            index=\"variables_index\",\n            body=body,\n            from_=offset,\n            size=size\n        )\n\n        # The unique collection_ids and their details will be in the 'aggregations' field of the response\n        unique_collection_ids = search_results['aggregations']['unique_collection_ids']['buckets']\n\n        # Prepare a list to hold the collection details\n        collection_details_list = []\n\n        for bucket in unique_collection_ids:\n            collection_details = bucket['collection_details']['hits']['hits'][0]['_source']\n            # Append the details to the list in the desired format\n            collection_details_list.append(collection_details)\n\n    \n            \n\n       #Adding consent to the studies \n        with open(self._cfg.consent_id_path, 'r') as file:\n            consent_id_mappings = json.load(file)\n        # Add consent_id to the study\n        updated_studies = []\n        for study in collection_details_list:\n            collection_id = study[\"collection_id\"]\n            if collection_id in consent_id_mappings:\n                consent_ids = consent_id_mappings[collection_id]\n                for consent_id in consent_ids:\n                    updated_study = study.copy()\n                    updated_study[\"collection_id\"] = f\"{collection_id}.{consent_id}\"\n                    updated_study[\"collection_action\"] = f\"{study['collection_action']}\"\n                    updated_studies.append(updated_study)\n            else:\n                updated_studies.append(study)\n        \n\n\n        #Adding missing studies\n                \n        with open(self._cfg.missing_studies_path, 'r') as file:\n            missing_studies = json.load(file)\n        for program in missing_studies:\n            if program_name.lower() == program['program_name'].lower():\n                updated_studies.extend(program['collections'])\n\n                \n        return updated_studies\n\n\n    async def search_program_list(self):\n        query_body = {\n            \"size\": 0,  # We don't need the documents themselves, so set the size to 0\n            \"aggs\": {\n                \"unique_program_names\": {\n                    \"terms\": {\n                        \"field\": \"data_type.keyword\",\n                        \"size\": 10000\n                    },\n                    \"aggs\": {\n                        \"No_of_studies\": {\n                            \"cardinality\": {\n                                \"field\": \"collection_id.keyword\"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n        # Execute the search query\n        search_results = await self.es.search(\n            index=\"variables_index\",\n            body=query_body\n        )\n        # The unique data_types and their counts of unique collection_ids will be in the 'aggregations' field of the response\n        unique_data_types = search_results['aggregations']['unique_program_names']['buckets']\n        data=unique_data_types\n\n        #Remove Parent program and add Training program\n        \n        data = [item for item in data if item['key'] != 'Parent']\n\n        with open(self._cfg.missing_program_path, 'r') as file:\n            missing_programs = json.load(file)\n        data.extend(missing_programs)\n\n\n        # Sorting the data alphabetically based on 'key'\n        sorted_data = sorted(data, key=lambda x: (x['key'].casefold(), x['key'][1:]))\n\n        #Add description as another field in exisiting data based on the program name\n        descriptions_json = self._cfg.program_description\n        descriptions = json.loads(descriptions_json)\n        description_dict = {item['key']: {'description': item['description'], 'parent_program': item['parent_program']} for item in descriptions}\n\n        # Add descriptions and parent programs to the sorted data\n        for item in sorted_data:\n            desc_info = description_dict.get(item['key'], {'description': '', 'parent_program': []})\n            item['description'] = desc_info['description']\n            item['parent_program'] = desc_info['parent_program']\n\n        return sorted_data\n\n\n    def _get_var_query(self, concept, fuzziness, prefix_length, query):\n        \"\"\"Returns ES query for variable search\"\"\"\n        es_query = {\n            \"query\": {\n                'bool': {\n                    'should': [\n                        {\n                            \"match_phrase\": {\n                                \"element_name\": {\n                                    \"query\": query,\n                                    \"boost\": 10\n                                }\n                            }\n                        },\n                        {\n                            \"match_phrase\": {\n                                \"element_desc\": {\n                                    \"query\": query,\n                                    \"boost\": 6\n                                }\n                            }\n                        },\n                        {\n                            \"match_phrase\": {\n                                \"search_terms\": {\n                                    \"query\": query,\n                                    \"boost\": 8\n                                }\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"element_name\": {\n                                    \"query\": query,\n                                    \"fuzziness\": fuzziness,\n                                    \"prefix_length\": prefix_length,\n                                    \"operator\": \"and\",\n                                    \"boost\": 4\n                                }\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"search_terms\": {\n                                    \"query\": query,\n                                    \"fuzziness\": fuzziness,\n                                    \"prefix_length\": prefix_length,\n                                    \"operator\": \"and\",\n                                    \"boost\": 5\n                                }\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"element_desc\": {\n                                    \"query\": query,\n                                    \"fuzziness\": fuzziness,\n                                    \"prefix_length\": prefix_length,\n                                    \"operator\": \"and\",\n                                    \"boost\": 3\n                                }\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"element_desc\": {\n                                    \"query\": query,\n                                    \"fuzziness\": fuzziness,\n                                    \"prefix_length\": prefix_length,\n                                    \"boost\": 2\n                                }\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"element_name\": {\n                                    \"query\": query,\n                                    \"fuzziness\": fuzziness,\n                                    \"prefix_length\": prefix_length,\n                                    \"boost\": 2\n                                }\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"search_terms\": {\n                                    \"query\": query,\n                                    \"fuzziness\": fuzziness,\n                                    \"prefix_length\": prefix_length,\n                                    \"boost\": 1\n                                }\n                            }\n                        },\n                        {\n                            \"match\": {\n                                \"optional_terms\": {\n                                    \"query\": query,\n                                    \"fuzziness\": fuzziness,\n                                    \"prefix_length\": prefix_length\n                                }\n                            }\n                        }\n                    ]\n                }\n            }\n        }\n        if concept:\n            es_query[\"query\"][\"bool\"][\"must\"] = {\n                \"match\": {\n                    \"identifiers\": concept\n                }\n            }\n        return es_query\n\n    def get_simple_concept_search_query(self, query):\n        \"\"\"Returns ES query that allows to use basic operators like AND, OR, NOT...\n        More info here https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html.\"\"\"\n        simple_query_string_search = {\n            \"query\": query,\n            \"default_operator\": \"and\",\n            \"flags\": \"OR|AND|NOT|PHRASE|PREFIX\"\n        }\n        search_query = {\n            \"query\": {\n                \"bool\": {\n                    \"filter\": {\n                        \"bool\": {\n                            \"must\": [\n                                {\"wildcard\": {\"description\": \"?*\"}},\n                                {\"wildcard\": {\"name\": \"?*\"}}\n                            ]\n                        }\n                    },\n                    \"must\": {\n                        \"function_score\": {\n                            \"query\": {\n                                \"bool\": {\n                                    \"should\": [\n                                        {\n                                            \"simple_query_string\": {\n                                                **simple_query_string_search,\n                                                \"fields\": [\"name\"]\n                                            }\n                                        },\n                                        {\n                                            \"simple_query_string\": {\n                                                **simple_query_string_search,\n                                                \"fields\": [\"description\"]\n                                            }\n                                        },\n                                        {\n                                            \"simple_query_string\": {\n                                                **simple_query_string_search,\n                                                \"fields\": [\"search_terms\"]\n                                            }\n                                        }\n                                    ]\n                                }\n                            },\n                            \"score_mode\": \"sum\"\n                        }\n                    }\n                }\n            }\n        }\n        return search_query\n\n    def get_simple_variable_search_query(self, concept, query):\n        \"\"\"Returns ES query that allows to use basic operators like AND, OR, NOT...\n        More info here https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html.\"\"\"\n        simple_query_string_search = {\n            \"query\": query,\n            \"default_operator\": \"and\",\n            \"flags\": \"OR|AND|NOT|PHRASE|PREFIX\"\n        }\n        search_query = {\n            \"query\": {\n                \"bool\": {\n                    \"must\": [\n                        {\"function_score\": {\n                            \"query\": {\n                                \"bool\": {\n                                    \"should\": [\n                                        \n                                        {\n                                            \"simple_query_string\": {\n                                                **simple_query_string_search,\n                                                \"fields\": [\"element_name\"]\n                                            }\n                                        },\n                                        {\n                                            \"simple_query_string\": {\n                                                **simple_query_string_search,\n                                                \"fields\": [\"element_desc\"]\n                                            }\n                                        },\n                                        {\n                                            \"simple_query_string\": {\n                                                **simple_query_string_search,\n                                                \"fields\": [\"search_terms\"]\n                                            }\n                                        }\n                                    ]\n                                }\n                            },\n                            \"score_mode\": \"sum\"\n                        }}\n                    ]\n                }\n            }\n        }\n        if concept:\n            search_query[\"query\"][\"bool\"][\"must\"].append({\n                \"match\": {\n                    \"identifiers\": concept\n                }\n            })\n        return search_query"}
{"type": "source_file", "path": "src/dug/core/annotators/_base.py", "content": "import json\nimport logging\nimport re\nimport logging\nimport urllib.parse\nfrom typing import Union, Callable, Any, Iterable, TypeVar, Generic, List, Optional\nfrom dug import utils as utils\nfrom requests import Session\nimport bmt\nfrom retrying import retry\n\nlogger = logging.getLogger(\"dug\")\n\nlogging.getLogger(\"requests\").setLevel(logging.WARNING)\nlogging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n\nclass DugIdentifier:\n    \"\"\"Core information about a concept, produced from annotator request\n\n    The Dug Identifier is the core piece of information about a concept that\n    produced from a request to an annotator based on a some original source of\n    data.\n\n    \\n The information that is being stored is mostly meant to support the\n    Monarch API but should be adjusted accordingly to suit new Annotators needs\n    in the future.\n    \\n The information that will be needed for all annotators are:\n        \\n id: The CURIE identifier\n        \\n label: The CURIE identifier\n        \\n description: The CURIE identifier\n    \\n When there is another supported Normalizer it will be seperated into a\n    separate plugin like annotator.\n    \"\"\"\n\n    def __init__(self, id, label, types=None, search_text=\"\", description=\"\"):\n        \"custom init stores parameters to initial values\"\n\n        self.id = id\n        self.label = label\n        self.description = description\n        if types is None:\n            types = []\n        self.types = types\n        self.search_text = sorted([search_text]) if search_text else []\n        self.equivalent_identifiers = []\n        self.synonyms = []\n        self.purl = \"\"\n\n    @property\n    def id_type(self):\n        return self.id.split(\":\")[0]\n\n    def add_search_text(self, text):\n        \"Add text only if it's unique and if not empty string\"\n        if text and text not in self.search_text:\n            self.search_text = sorted(self.search_text + [text])\n\n    def get_searchable_dict(self):\n        \"Return version of identifier compatible with what's in ElasticSearch\"\n        es_ident = {\n            \"id\": self.id,\n            \"label\": self.label,\n            \"equivalent_identifiers\": self.equivalent_identifiers,\n            \"type\": self.types,\n            \"synonyms\": self.synonyms,\n        }\n        return es_ident\n\n    def jsonable(self):\n        \"Output pickleable object (used by utils.complex_handler)\"\n        return self.__dict__\n\n\n    def __str__(self):\n        return json.dumps(self.__dict__, indent=2, default=utils.complex_handler)\n\n\nInput = TypeVar(\"Input\")\nOutput = TypeVar(\"Output\")\n\n\nclass AnnotatorSession(Generic[Input, Output]):\n    def make_request(self, value: Input, http_session: Session):\n        raise NotImplementedError()\n\n    def handle_response(self, value, response: Union[dict, list]) -> Output:\n        raise NotImplementedError()\n\n    def __call__(self, value: Input, http_session: Session) -> Output:\n        response = self.make_request(value, http_session)\n\n        result = self.handle_response(value, response)\n\n        return result\n\n\nclass DefaultNormalizer():\n    \"\"\"Default concept normalizer class\n\n    After annotation there must be a Normalizing step to collasce equivalent\n    concepts into one official concept. This is a needed step for the knowledge\n    graph to map between different concepts.\n\n    The reason why this class in integrated into the annotators.py is because\n    currently there is only one supported Normalizer through the NCATs\n    Translator API.\n\n    When there is another supported Normalizer it will be seperated into a\n    separate plugin like annotator.\n    \"\"\"\n\n    def __init__(self, url):\n        self.bl_toolkit = bmt.Toolkit()\n        self.url = url\n\n    def __call__(self, identifier: DugIdentifier, http_session: Session) -> DugIdentifier:\n        # Use RENCI's normalization API service to get the preferred version of an identifier\n        logger.debug(f\"Normalizing: {identifier.id}\")\n        response = self.make_request(identifier, http_session)\n        result = self.handle_response(identifier, response)\n        return result\n\n    def make_request(self, value: DugIdentifier, http_session: Session) -> dict:\n        curie = value.id\n        url = f\"{self.url}{urllib.parse.quote(curie)}\"\n        try:\n            response = http_session.get(url)\n        except Exception as get_exc:\n            logger.info(f\"Error normalizing {value} at {url}\")\n            logger.error(f\"Error {get_exc.__class__.__name__}: {get_exc}\")\n            return {}\n        try:\n            normalized = response.json()\n        except Exception as json_exc:\n            logger.info(\n                f\"Error processing response: {response.text} (HTTP {response.status_code})\"\n            )\n            logger.error(f\"Error {json_exc.__class__.__name__}: {json_exc}\")\n            return {}\n\n        return normalized\n\n    def handle_response(\n        self, identifier: DugIdentifier, normalized: dict\n    ) -> Optional[DugIdentifier]:\n        \"\"\"Record normalized results.\"\"\"\n        curie = identifier.id\n        normalization = normalized.get(curie, {})\n        if normalization is None:\n            logger.info(\n                f\"Normalization service did not return normalization for: {curie}\"\n            )\n            return None\n\n        preferred_id = normalization.get(\"id\", {})\n        equivalent_identifiers = normalization.get(\"equivalent_identifiers\", [])\n        biolink_type = normalization.get(\"type\", [])\n\n        # Return none if there isn't actually a preferred id\n        if \"identifier\" not in preferred_id:\n            logger.debug(f\"ERROR: normalize({curie})=>({preferred_id}). No identifier?\")\n            return None\n\n        logger.debug(f\"Preferred id: {preferred_id}\")\n        identifier.id = preferred_id.get(\"identifier\", \"\")\n        identifier.label = preferred_id.get(\"label\", \"\")\n        identifier.description = preferred_id.get(\"description\", \"\")\n        identifier.equivalent_identifiers = [\n            v[\"identifier\"] for v in equivalent_identifiers\n        ]\n        try:\n            identifier.types = self.bl_toolkit.get_element(biolink_type[0]).name\n        except:\n            # converts biolink:SmallMolecule to small molecule\n            identifier.types = (\n                \" \".join(\n                    re.split(\"(?=[A-Z])\", biolink_type[0].replace(\"biolink:\", \"\"))[1:]\n                )\n            ).lower()\n        return identifier\n\n\nclass DefaultSynonymFinder():\n    \"\"\" The SynonymFinder stores synonyms for concepts in the knowledge graph so users in the Dug User Interface can find concepts that match their search criteria. \n    \\n The reason why this class in integrated into the annotators.py is because currently there is only one supported SynonymFinder through the deployed by RENCI.\n    \\n When there is another supported SynonymFinder it will be seperated into a separate plugin like annotator.\n    \"\"\"\n\n    def __init__(self, url: str):\n        self.url = url\n\n    # def get_identifier_synonyms\n    def __call__(self, curie: str, http_session):\n        \"\"\"\n        This function uses the NCATS translator service to return a list of synonyms for\n        curie id\n        \"\"\"\n        response = self.make_request(curie, http_session)\n        result = self.handle_response(curie, response)\n        return result\n\n    @retry(stop_max_attempt_number=3)\n    def make_request(self, curie: str, http_session: Session):\n        # Get response from namelookup reverse lookup op\n        # example (https://name-resolution-sri.renci.org/docs#/lookup/lookup_names_reverse_lookup_post)\n        url = f\"{self.url}\"\n        payload = {\"curies\": [curie]}\n        try:\n            response = http_session.post(url, json=payload)\n            if str(response.status_code).startswith(\"4\"):\n                logger.error(\n                    f\"No synonyms returned for: `{curie}`. Validation error: {response.text}\"\n                )\n                return {curie: {\"names\": []}}\n            if str(response.status_code).startswith(\"5\"):\n                logger.error(\n                    f\"No synonyms returned for: `{curie}`. Internal server error from {self.url}. Error: {response.text}\"\n                )\n                return {curie: {\"names\": []}}\n            return response.json()\n        except json.decoder.JSONDecodeError as e:\n            logger.error(\n                f\"Json parse error for response from `{url}`. Exception: {str(e)}\"\n            )\n            return {curie: {\"names\": []}}\n\n    def handle_response(self, curie: str, raw_synonyms: List[dict]) -> List[str]:\n        # Return curie synonyms\n        return raw_synonyms.get(curie, {}).get('names', [])\n\n\nIndexable = Union[DugIdentifier, AnnotatorSession]\n# Indexable = DugIdentifier\nAnnotator = Callable[[Any], Iterable[Indexable]]\n# Annotator = Callable[[Any], Iterable[DugIdentifier]]\n"}
{"type": "source_file", "path": "src/dug/core/parsers/heal_dp_parser.py", "content": "import logging\nimport os\nfrom typing import List\nfrom xml.etree import ElementTree as ET\n\nfrom dug import utils as utils\nfrom ._base import DugElement, FileParser, Indexable, InputFile\n\nlogger = logging.getLogger('dug')\n\n\nclass HEALDPParser(FileParser):\n    # Class for parsers Heal data platform converted Data dictionary into a set of Dug Elements\n\n    def __init__(self, study_type=\"HEAL Studies\"):\n        super()\n        self.study_type = study_type\n\n\n    def get_study_type(self):\n        return self.study_type\n    \n    def set_study_type(self, study_type):\n        self.study_type = study_type\n\n    def __call__(self, input_file: InputFile) -> List[Indexable]:\n        logger.debug(input_file)\n        tree = ET.parse(input_file)\n        root = tree.getroot()\n        study_id = root.attrib['study_id']\n\n        # Parse study name from file handle\n        study_name = root.get('study_name')\n\n        if study_name is None:\n            err_msg = f\"Unable to parse study name from data dictionary: {input_file}!\"\n            logger.error(err_msg)\n            raise IOError(err_msg)\n\n        elements = []\n        for variable in root.iter('variable'):\n            elem = DugElement(elem_id=f\"{variable.attrib['id']}\",\n                              name=variable.find('name').text,\n                              desc=variable.find('description').text.lower(),\n                              elem_type=self.get_study_type(),\n                              collection_id=f\"{study_id}\",\n                              collection_name=study_name)\n\n            # Create NIDA links as study/variable actions\n            elem.collection_action = utils.get_heal_platform_link(study_id=study_id)\n            # Add to set of variables\n            logger.debug(elem)\n            elements.append(elem)\n\n        # You don't actually create any concepts\n        return elements\n"}
{"type": "source_file", "path": "src/dug/core/parsers/dbgap_parser.py", "content": "import logging\nimport re, os\nfrom typing import List\nfrom xml.etree import ElementTree as ET\n\nfrom dug import utils as utils\nfrom pathlib import Path\nfrom ._base import DugElement, FileParser, Indexable, InputFile\n\nlogger = logging.getLogger('dug')\n\n\nclass DbGaPParser(FileParser):\n    # Class for parsers DBGaP Data dictionary into a set of Dug Elements\n\n    @staticmethod\n    def parse_study_name_from_filename(filename: str) -> str:\n        # Parse the study name from the xml filename if it exists. Return None if filename isn't right format to get id from\n        dbgap_file_pattern = re.compile(r'.*/*phs[0-9]+\\.v[0-9]+\\.pht[0-9]+\\.v[0-9]+\\.(.+)\\.data_dict.*')\n        match = re.match(dbgap_file_pattern, filename)\n        if match is not None:\n            return match.group(1)\n        return None\n    \n    @staticmethod\n    def parse_study_name_from_gap_exchange_file(filepath: Path) -> str:\n        # Parse the study name from the GapExchange file adjacent to the file passed in\n        parent_dir = filepath.parent.absolute()\n        gap_exchange_filename_str = \"GapExchange_\" + parent_dir.name\n        gap_exchange_filepath = None\n        for item in os.scandir(parent_dir):\n            if item.is_file and gap_exchange_filename_str in item.name:\n                gap_exchange_filepath = item.path\n        if gap_exchange_filepath is None:\n            return None\n        tree = ET.parse(gap_exchange_filepath, ET.XMLParser(encoding='iso-8859-5'))\n        tree_root = tree.getroot()\n        return tree_root.find(\"./Studies/Study/Configuration/StudyNameEntrez\").text\n\n\n    def _get_element_type(self):\n        return \"dbGaP\"\n\n    def __call__(self, input_file: InputFile) -> List[Indexable]:\n        logger.debug(input_file)\n        if \"GapExchange\" in str(input_file).split(\"/\")[-1]:\n            msg = f\"Skipping parsing for GapExchange file: {input_file}!\"\n            logger.info(msg)\n            return []\n        tree = ET.parse(input_file, ET.XMLParser(encoding='iso-8859-5'))\n        root = tree.getroot()\n        study_id = root.attrib['study_id']\n        participant_set = root.get('participant_set','0')\n\n        # Parse study name from GapExchange file, and if that fails try from file handle\n        # If still None, raise an error message\n        study_name = self.parse_study_name_from_gap_exchange_file(Path(input_file))\n        if study_name is None:\n            study_name = self.parse_study_name_from_filename(str(input_file))\n        if study_name is None:\n            err_msg = f\"Unable to parse DbGaP study name from data dictionary: {input_file}!\"\n            logger.error(err_msg)\n            raise IOError(err_msg)\n\n        elements = []\n        for variable in root.iter('variable'):\n            elem = DugElement(elem_id=f\"{variable.attrib['id']}.p{participant_set}\",\n                              name=variable.find('name').text,\n                              desc=variable.find('description').text.lower(),\n                              elem_type=self._get_element_type(),\n                              collection_id=f\"{study_id}.p{participant_set}\",\n                              collection_name=study_name)\n\n            # Create DBGaP links as study/variable actions\n            elem.collection_action = utils.get_dbgap_study_link(study_id=elem.collection_id)\n            elem.action = utils.get_dbgap_var_link(study_id=elem.collection_id,\n                                                   variable_id=elem.id.split(\".\")[0].split(\"phv\")[1])\n            # Add to set of variables\n            logger.debug(elem)\n            elements.append(elem)\n\n        # You don't actually create any concepts\n        return elements\n\n\nclass AnvilDbGaPParser(DbGaPParser):\n    def _get_element_type(self):\n        return \"AnVIL\"\n\n\nclass CRDCDbGaPParser(DbGaPParser):\n    def _get_element_type(self):\n        return \"Cancer Data Commons\"\n\n\nclass KFDRCDbGaPParser(DbGaPParser):\n    def _get_element_type(self):\n        return \"Kids First\"\n\n\nclass BioLINCCDbGaPParser(DbGaPParser):\n    def _get_element_type(self):\n        return \"BioLINCC\"\n\n\nclass Covid19DbGaPParser(DbGaPParser):\n    def _get_element_type(self):\n        return \"COVID19\"\n\n\nclass DIRDbGaPParser(DbGaPParser):\n    def _get_element_type(self):\n        return \"DIR\"\n\n\nclass LungMAPDbGaPParser(DbGaPParser):\n    def _get_element_type(self):\n        return \"LungMAP\"\n\n\nclass NSRRDbGaPParser(DbGaPParser):\n    def _get_element_type(self):\n        return \"NSRR\"\n\n\nclass ParentDBGaPParser(DbGaPParser):\n    def _get_element_type(self):\n        return \"Parent\"\n\n\nclass PCGCDbGaPParser(DbGaPParser):\n    def _get_element_type(self):\n        return \"PCGC\"\n\n\nclass RECOVERDBGaPParser(DbGaPParser):\n    def _get_element_type(self):\n        return \"RECOVER\"\n\n\nclass TopmedDBGaPParser(DbGaPParser):\n    def _get_element_type(self):\n        return \"TOPMed\"\n\n\nclass CureSC(DbGaPParser):\n    def _get_element_type(self):\n        return \"CureSC\"\n"}
{"type": "source_file", "path": "src/dug/core/index.py", "content": "\"\"\"\nThis class is used for adding documents to elastic search index\n\"\"\"\nimport logging\n\nfrom elasticsearch import Elasticsearch\nimport ssl\n\nfrom dug.config import Config\n\nlogger = logging.getLogger('dug')\n\n\nclass Index:\n    def __init__(self, cfg: Config, indices=None):\n\n        if indices is None:\n            indices = ['concepts_index', 'variables_index', 'kg_index']\n\n        self._cfg = cfg\n        logger.debug(f\"Connecting to elasticsearch host: {self._cfg.elastic_host} at port: {self._cfg.elastic_port}\")\n\n        self.indices = indices\n        self.hosts = [{'host': self._cfg.elastic_host, 'port': self._cfg.elastic_port, 'scheme': self._cfg.elastic_scheme}]\n\n        logger.debug(f\"Authenticating as user {self._cfg.elastic_username} to host:{self.hosts}\")\n        if self._cfg.elastic_scheme == \"https\":\n            ssl_context = ssl.create_default_context(\n                cafile=self._cfg.elastic_ca_path\n            )\n            self.es = Elasticsearch(\n                hosts=self.hosts,\n                basic_auth=(self._cfg.elastic_username, self._cfg.elastic_password),\n                ssl_context=ssl_context)\n        else:\n            self.es = Elasticsearch(\n                hosts=self.hosts,\n                basic_auth=(self._cfg.elastic_username, self._cfg.elastic_password))\n        self.replicas = self.get_es_node_count()\n\n        if self.es.ping():\n            logger.info('connected to elasticsearch')\n            self.init_indices()\n        else:\n            print(f\"Unable to connect to elasticsearch at {self._cfg.elastic_host}:{self._cfg.elastic_port}\")\n            logger.error(f\"Unable to connect to elasticsearch at {self._cfg.elastic_host}:{self._cfg.elastic_port}\")\n            raise SearchException(\n                message='failed to connect to elasticsearch',\n                details=f\"connecting to host {self._cfg.elastic_host} and port {self._cfg.elastic_port}\")\n        \n    def get_es_node_count(self):\n        return self.es.nodes.info()[\"_nodes\"][\"total\"]\n        \n\n    def init_indices(self):\n        # The concepts and variable indices include an analyzer that utilizes the english\n        # stopword facility from elastic search.  We also instruct each of the text mappings\n        # to use this analyzer. Note that we have not upgraded the kg index, because the fields\n        # in that index are primarily dynamic. We could eventually either add mappings so that\n        # the fields are no longer dynamic or we could use the dynamic template capabilities\n        # described in\n        # https://www.elastic.co/guide/en/elasticsearch/reference/current/dynamic-templates.html\n\n        kg_index = {\n            \"settings\": {\n                \"number_of_shards\": 1,\n                \"number_of_replicas\": self.replicas\n            },\n            \"mappings\": {\n                \"properties\": {\n                    \"name\": {\n                        \"type\": \"text\"\n                    },\n                    \"type\": {\n                        \"type\": \"text\"\n                    }\n                }\n            }\n        }\n        concepts_index = {\n            \"settings\": {\n                \"index.mapping.coerce\": \"false\",\n                \"number_of_shards\": 1,\n                \"number_of_replicas\": self.replicas,\n                \"analysis\": {\n                    \"analyzer\": {\n                        \"std_with_stopwords\": {\n                            \"type\": \"standard\",\n                            \"stopwords\": \"_english_\"\n                        }\n                    }\n                }\n            },\n            \"mappings\": {\n                \"dynamic\": \"strict\",\n                \"properties\": {\n                    \"id\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\",\n                           \"fields\": {\"keyword\": {\"type\": \"keyword\"}}},\n                    \"name\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"},\n                    \"description\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"},\n                    \"type\": {\"type\": \"keyword\"},\n                    \"search_terms\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"},\n                    \"identifiers\": {\n                        \"properties\": {\n                            \"id\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\",\n                                   \"fields\": {\"keyword\": {\"type\": \"keyword\"}}},\n                            \"label\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"},\n                            \"equivalent_identifiers\": {\"type\": \"keyword\"},\n                            \"type\": {\"type\": \"keyword\"},\n                            \"synonyms\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"}\n                        }\n                    },\n                    \"optional_terms\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"},\n                    \"concept_action\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"}\n                }\n            }\n        }\n        variables_index = {\n            \"settings\": {\n                \"index.mapping.coerce\": \"false\",\n                \"number_of_shards\": 1,\n                \"number_of_replicas\": self.replicas,\n                \"analysis\": {\n                    \"analyzer\": {\n                        \"std_with_stopwords\": {\n                            \"type\": \"standard\",\n                            \"stopwords\": \"_english_\"\n                        }\n                    }\n                }\n            },\n            \"mappings\": {\n                \"dynamic\": \"strict\",\n                \"properties\": {\n                    \"element_id\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\",\n                                   \"fields\": {\"keyword\": {\"type\": \"keyword\"}}},\n                    \"element_name\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"},\n                    \"element_desc\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"},\n                    \"element_action\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"},\n                    \"search_terms\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"},\n                    \"optional_terms\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"},\n                    \"identifiers\": {\"type\": \"keyword\"},\n                    \"collection_id\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\",\n                                      \"fields\": {\"keyword\": {\"type\": \"keyword\"}}},\n                    \"collection_name\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"},\n                    \"collection_desc\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"},\n                    \"collection_action\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\"},\n                    \"data_type\": {\"type\": \"text\", \"analyzer\": \"std_with_stopwords\",\n                                  \"fields\": {\"keyword\": {\"type\": \"keyword\"}}},\n                    \"metadata\": {\n                        \"type\": \"object\",\n                        \"dynamic\": True\n                    }\n                    # typed as keyword for bucket aggs\n                }\n            }\n        }\n\n        settings = {\n            'kg_index': kg_index,\n            'concepts_index': concepts_index,\n            'variables_index': variables_index,\n        }\n\n        logger.info(f\"creating indices\")\n        logger.debug(self.indices)\n        for index in self.indices:\n            try:\n                if self.es.indices.exists(index=index):\n                    # if index exists check if replication is good \n                    index_replicas = self.es.indices.get_settings(index=index)[index][\"settings\"][\"index\"][\"number_of_replicas\"]\n                    if index_replicas != self.replicas:\n                        self.es.indices.put_settings(index=index, body={\"number_of_replicas\": (self.replicas - 1) or 1 })\n                        self.es.indices.refresh(index=index)\n                    logger.info(f\"Ignoring index {index} which already exists.\")\n                else:\n                    result = self.es.indices.create(\n                        index=index,\n                        body=settings[index],\n                        ignore=400)\n                    logger.info(f\"result created index {index}: {result}\")\n            except Exception as e:\n                logger.error(f\"exception: {e}\")\n                raise e\n\n    def index_doc(self, index, doc, doc_id):\n        self.es.index(\n            index=index,\n            id=doc_id,\n            body=doc)\n\n    def update_doc(self, index, doc, doc_id):\n        self.es.update(\n            index=index,\n            id=doc_id,\n            body=doc\n        )\n\n    def index_concept(self, concept, index):\n        # Don't re-index if already in index\n        if self.es.exists(index=index, id=concept.id):\n            return\n        \"\"\" Index the document. \"\"\"\n        self.index_doc(\n            index=index,\n            doc=concept.get_searchable_dict(),\n            doc_id=concept.id)\n\n    def index_element(self, elem, index):\n        if not self.es.exists(index=index, id=elem.get_id()):\n            # If the element doesn't exist, add it directly\n            self.index_doc(\n                index=index,\n                doc=elem.get_searchable_dict(),\n                doc_id=elem.get_id())\n        else:\n            # Otherwise update to add any new identifiers that weren't there last time around\n            results = self.es.get(index=index, id=elem.get_id())\n            identifiers = results['_source']['identifiers'] + list(elem.concepts.keys())\n            doc = {\"doc\": {}}\n            doc['doc']['identifiers'] = list(set(identifiers))\n            self.update_doc(index=index, doc=doc, doc_id=elem.get_id())\n\n    def index_kg_answer(self, concept_id, kg_answer, index, id_suffix=None):\n\n        # Get search targets by extracting names/synonyms from non-curie nodes in answer knoweldge graph\n        search_targets = kg_answer.get_node_names(include_curie=False)\n        search_targets += kg_answer.get_node_synonyms(include_curie=False)\n\n        # Create the Doc\n        doc = {\n            'concept_id': concept_id,\n            'search_targets': list(set(search_targets)),\n            'knowledge_graph': kg_answer.get_kg()\n        }\n\n        # Create unique ID\n        logger.debug(\"Indexing TranQL query answer...\")\n        id_suffix = list(kg_answer.nodes.keys()) if id_suffix is None else id_suffix\n        unique_doc_id = f\"{concept_id}_{id_suffix}\"\n\n        \"\"\" Index the document. \"\"\"\n        self.index_doc(\n            index=index,\n            doc=doc,\n            doc_id=unique_doc_id)\n\nclass SearchException(Exception):\n    def __init__(self, message, details):\n        self.message = message\n        self.details = details"}
{"type": "source_file", "path": "src/dug/core/parsers/scicrunch_parser.py", "content": "import logging\nimport os\nfrom typing import List\nfrom xml.etree import ElementTree as ET\n\nfrom dug import utils as utils\nfrom ._base import DugElement, FileParser, Indexable, InputFile\n\nlogger = logging.getLogger('dug')\n\n\nclass SciCrunchParser(FileParser):\n    # Class for parsing SciCrunch Data into a set of Dug Elements\n\n    @staticmethod\n    def get_study_name(filename: str):\n        \n        study_name = root.attrib['study_name']\n        stemname = os.path.splitext( os.path.basename(filename) )[0]\n        if stemname.startswith(\"DOI\"):\n            # we want to convert file names that look like \n            # \n            # DOI:10.26275-howg-tbhj.xml to study names that look like \n            # \n            # DOI:10.26275/howg-tbhj\n            #\n            # perform the needed subs\n            sn = stemname.replace(\"-\", \"/\").replace(\".xml\", \"\")\n            return sn\n        return None\n\n    @staticmethod\n    def get_scicrunch_study_link(filename: str):\n        # Parse the study name from the xml filename if it exists. Return None if filename isn't right format to get id from\n        stemname = os.path.splitext( os.path.basename(filename) )[0]\n        if stemname.startswith(\"DOI\"):\n            # we want to convert file names that look like\n            #\n            # DOI:10.26275-howg-tbhj.xml to URLs that look like\n            #\n            # https://DOI.org/10.26275/howg-tbhj.xml\n\n            # add https:// to the start\n            URL = \"https://\" + stemname\n\n            # perform the rest of the subs\n            sn = URL.replace(\"DOI:\", \"DOI.org/\").replace(\"-\", \"/\", 1).replace(\".xml\", \"\")\n            return sn\n        return None\n\n\n\n    def __call__(self, input_file: InputFile) -> List[Indexable]:\n        logger.debug(input_file)\n        tree = ET.parse(input_file)\n        root = tree.getroot()\n        study_id = root.attrib['study_id']\n        study_name = root.attrib['study_name']\n        participant_set = root.get('participant_set','0')\n\n        if study_name is None:\n            err_msg = f\"Unable to retrieve SciCrunch study name from {input_file}!\"\n            logger.error(err_msg)\n            raise IOError(err_msg)\n\n        elements = []\n        for variable in root.iter('variable'):\n            elem = DugElement(elem_id=f\"{variable.attrib['id']}.p{participant_set}\",\n                              name=variable.find('name').text,\n                              desc=variable.find('description').text.lower(),\n                              elem_type=\"SPARC\",\n                              collection_id=f\"{study_id}.p{participant_set}\",\n                              collection_name=study_name)\n\n            # Create links as study/variable actions\n            elem.collection_action = self.get_scicrunch_study_link(input_file)\n            # Add to set of variables\n            logger.debug(elem)\n            elements.append(elem)\n\n        # You don't actually create any concepts\n        return elements\n"}
{"type": "source_file", "path": "src/dug/core/loaders/network_loader.py", "content": "import logging\nfrom pathlib import Path\nfrom typing import Iterator\nfrom urllib.parse import urlparse\n\nimport requests\n\nfrom ._base import InputFile\n\nlogger = logging.getLogger('dug')\n\n\ndef load_from_network(data_storage_dir: InputFile, urls: str) -> Iterator[Path]:\n    data_storage_dir = Path(data_storage_dir).resolve()\n    url_list = urls.split(\",\")\n    for url in url_list:\n        logger.info(f\"Fetching {url}\")\n\n        parse_result = urlparse(url)\n        response = requests.get(url)\n\n        if not response.ok:\n            raise ValueError(f\"Could not fetch {url}: {response.status_code}, {response.text}\")\n\n        nonroot_path = parse_result.path.lstrip('/')\n\n        output_location = data_storage_dir / parse_result.netloc / nonroot_path\n        output_location.parent.mkdir(parents=True, exist_ok=True)\n\n        output_location.write_text(response.text)\n\n        yield output_location"}
{"type": "source_file", "path": "src/dug/core/parsers/nida_parser.py", "content": "import logging\nimport os\nfrom typing import List\nfrom xml.etree import ElementTree as ET\n\nfrom dug import utils as utils\nfrom ._base import DugElement, FileParser, Indexable, InputFile\n\nlogger = logging.getLogger('dug')\n\n\nclass NIDAParser(FileParser):\n    # Class for parsers NIDA Data dictionary into a set of Dug Elements\n\n    @staticmethod\n    def parse_study_name_from_filename(filename: str):\n        # Parse the study name from the xml filename if it exists. Return None if filename isn't right format to get id from\n        stemname = os.path.splitext( os.path.basename(filename) )[0]\n        if stemname.startswith(\"NIDA-\"):\n            sn = stemname\n            for s in [\"-Dictionary\", \"_DD\"]:\n                sn = sn.removesuffix(s) if sn.endswith(s) else sn\n            return sn\n        return None\n\n    def __call__(self, input_file: InputFile) -> List[Indexable]:\n        logger.debug(input_file)\n        tree = ET.parse(input_file)\n        root = tree.getroot()\n        study_id = root.attrib['study_id']\n        participant_set = root.get('participant_set','0')\n\n        # Parse study name from file handle\n        study_name = self.parse_study_name_from_filename(str(input_file))\n\n        if study_name is None:\n            err_msg = f\"Unable to parse NIDA study name from data dictionary: {input_file}!\"\n            logger.error(err_msg)\n            raise IOError(err_msg)\n\n        elements = []\n        for variable in root.iter('variable'):\n            elem = DugElement(elem_id=f\"{variable.attrib['id']}.p{participant_set}\",\n                              name=variable.find('name').text,\n                              desc=variable.find('description').text.lower(),\n                              elem_type=\"NIDA\",\n                              collection_id=f\"{study_id}.p{participant_set}\",\n                              collection_name=study_name)\n\n            # Create NIDA links as study/variable actions\n            elem.collection_action = utils.get_nida_study_link(study_id=study_id)\n            # Add to set of variables\n            logger.debug(elem)\n            elements.append(elem)\n\n        # You don't actually create any concepts\n        return elements\n"}
{"type": "source_file", "path": "src/dug/core/parsers/ctn_parser.py", "content": "import logging\nimport os\nfrom typing import List\nfrom xml.etree import ElementTree as ET\n\nfrom dug import utils as utils\nfrom dug.core.parsers._base import DugElement, FileParser, Indexable, InputFile\n\nlogger = logging.getLogger('dug')\n\n\nclass CTNParser(FileParser):\n    # Class for parsers CTN converted Data dictionary into a set of Dug Elements\n\n    def __init__(self):\n        super()\n        self.study_type = \"ctn\"\n\n    def get_study_type(self):\n        return self.study_type\n\n    def set_study_type(self, study_type):\n        self.study_type = study_type\n\n    def __call__(self, input_file: InputFile) -> List[Indexable]:\n        logger.debug(input_file)\n        tree = ET.parse(input_file)\n        root = tree.getroot()\n        study_id = root.attrib['study_id']\n\n        # Parse study name from file handle\n        study_name = root.get('study_name')\n\n        if study_name is None:\n            err_msg = f\"Unable to parse study name from data dictionary: {input_file}!\"\n            logger.error(err_msg)\n            raise IOError(err_msg)\n\n        elements = []\n        counter = 0\n        for variable in root.iter('variable'):\n\n            if not variable.text:\n                continue\n            description = variable.find('description').text.lower() if variable.find('description') is not None else \"\"\n\n            elem = DugElement(elem_id=f\"{variable.attrib['id']}\",\n                              name=variable.find('name').text,\n                              desc=description,\n                              elem_type=self.get_study_type(),\n                              collection_id=f\"{study_id}\",\n                              collection_name=study_name,\n                              collection_action=utils.get_ctn_link(study_id=study_id))\n            if elem.id==\"BSNAUSE\":\n                print(elem.collection_action)\n            counter+=1\n            # Add to set of variables\n            logger.debug(elem)\n            elements.append(elem)\n\n        # You don't actually create any concepts\n        return elements\n\n\n\n\n\n"}
{"type": "source_file", "path": "src/dug/core/loaders/__init__.py", "content": "from ._base import InputFile, Loader\n"}
{"type": "source_file", "path": "src/dug/core/parsers/_base.py", "content": "import json\nfrom typing import Union, Callable, Any, Iterable\n\nfrom dug.core.loaders import InputFile\n\nfrom dug import utils as utils\n\n\nclass DugElement:\n    # Basic class for holding information for an object you want to make searchable via Dug\n    # Could be a DbGaP variable, DICOM image, App, or really anything\n    # Optionally can hold information pertaining to a containing collection (e.g. dbgap study or dicom image series)\n    def __init__(self, elem_id, name, desc, elem_type, collection_id=\"\", collection_name=\"\", collection_desc=\"\", action=\"\", collection_action=\"\"):\n        self.id = elem_id\n        self.name = name\n        self.description = desc\n        self.type = elem_type\n        self.collection_id = collection_id\n        self.collection_name = collection_name\n        self.collection_desc = collection_desc\n        self.action = action\n        self.collection_action = collection_action\n        self.concepts = {}\n        self.ml_ready_desc = desc\n        self.search_terms = []\n        self.optional_terms = []\n        self.metadata = {}\n\n\n\n    def add_concept(self, concept):\n        self.concepts[concept.id] = concept\n\n    def jsonable(self):\n        \"\"\"Output a pickleable object\"\"\"\n        return self.__dict__\n\n    def get_searchable_dict(self):\n        # Translate DugElement to ES-style dict\n        es_elem = {\n            'element_id': self.id,\n            'element_name': self.name,\n            'element_desc': self.description,\n            'search_terms': self.search_terms,\n            'optional_terms': self.optional_terms,\n            'collection_id': self.collection_id,\n            'collection_name': self.collection_name,\n            'collection_desc': self.collection_desc,\n            'element_action': self.action,\n            'collection_action': self.collection_action,\n            'data_type': self.type,\n            'metadata': self.metadata,\n            'identifiers': list(self.concepts.keys())\n        }\n        return es_elem\n\n    def add_metadata(self, metadata):\n        self.metadata = metadata\n\n    def get_id(self):\n        return f'{self.id}-{self.collection_id}'\n\n    def set_search_terms(self):\n        search_terms = []\n        for concept_id, concept in self.concepts.items():\n            concept.set_search_terms()\n            search_terms.extend(concept.search_terms)\n            search_terms.append(concept.name)\n        search_terms = sorted(list(set(search_terms)))\n        self.search_terms = search_terms\n\n    def set_optional_terms(self):\n        optional_terms = []\n        for concept_id, concept in self.concepts.items():\n            concept.set_optional_terms()\n            optional_terms.extend(concept.optional_terms)\n        optional_terms = sorted(list(set(optional_terms)))\n        self.optional_terms = optional_terms\n\n    def __str__(self):\n        return json.dumps(self.__dict__, indent=2, default=utils.complex_handler)\n\n\nclass DugConcept:\n    # Basic class for holding information about concepts that are used to organize elements\n    # All Concepts map to at least one element\n    def __init__(self, concept_id, name, desc, concept_type):\n        self.id = concept_id\n        self.name = name\n        self.description = desc\n        self.type = concept_type\n        self.concept_action = \"\"\n        self.identifiers = {}\n        self.kg_answers = {}\n        self.search_terms = []\n        self.optional_terms = []\n        self.ml_ready_desc = desc\n\n    def add_identifier(self, ident):\n        if ident.id in self.identifiers:\n            for search_text in ident.search_text:\n                self.identifiers[ident.id].add_search_text(search_text)\n        else:\n            self.identifiers[ident.id] = ident\n\n    def add_kg_answer(self, answer, query_name):\n        answer_node_ids = list(answer.nodes.keys())\n        answer_id = f'{\"_\".join(answer_node_ids)}_{query_name}'\n        if answer_id not in self.kg_answers:\n            self.kg_answers[answer_id] = answer\n\n    def clean(self):\n        self.search_terms = sorted(list(set(self.search_terms)))\n        self.optional_terms = sorted(list(set(self.optional_terms)))\n\n    def set_search_terms(self):\n        # Traverse set of identifiers to determine set of search terms\n        search_terms = self.search_terms\n        for ident_id, ident in self.identifiers.items():\n            search_terms.extend(ident.search_text + ident.synonyms)\n        self.search_terms = sorted(list(set(search_terms)))\n\n    def set_optional_terms(self):\n        # Traverse set of knowledge graph answers to determine set of optional search terms\n        optional_terms = self.optional_terms\n        for kg_id, kg_answer in self.kg_answers.items():\n            optional_terms += kg_answer.get_node_names()\n            optional_terms += kg_answer.get_node_synonyms()\n        self.optional_terms = sorted(list(set(optional_terms)))\n\n    def get_searchable_dict(self):\n        # Translate DugConcept into Elastic-Compatible Concept\n        es_conc = {\n            'id': self.id,\n            'name': self.name,\n            'description': self.description,\n            'type': self.type,\n            'search_terms': self.search_terms,\n            'optional_terms': self.optional_terms,\n            'concept_action': self.concept_action,\n            'identifiers': [ident.get_searchable_dict() for ident_id, ident in self.identifiers.items()]\n        }\n        return es_conc\n\n    def jsonable(self):\n        \"\"\"Output a pickleable object\"\"\"\n        return self.__dict__\n\n    def __str__(self):\n        return json.dumps(self.__dict__, indent=2, default=utils.complex_handler)\n\n\nIndexable = Union[DugElement, DugConcept]\nParser = Callable[[Any], Iterable[Indexable]]\n\n\nFileParser = Callable[[InputFile], Iterable[Indexable]]"}
{"type": "source_file", "path": "src/dug/core/loaders/filesystem_loader.py", "content": "from pathlib import Path\nfrom typing import Iterator\n\nfrom ._base import InputFile\n\n\ndef load_from_filesystem(filepath: InputFile) -> Iterator[Path]:\n\n    filepath = Path(filepath)\n\n    if not filepath.exists():\n        raise ValueError(f\"Unable to locate {filepath}\")\n\n    if filepath.is_file():\n        yield filepath\n    else:\n        yield from filepath.glob(\"**/*\")\n"}
{"type": "source_file", "path": "src/dug/core/parsers/__init__.py", "content": "import logging\nfrom typing import Dict\n\nimport pluggy\n\nfrom ._base import DugElement, DugConcept, Indexable, Parser, FileParser\nfrom .dbgap_parser import *\nfrom .nida_parser import NIDAParser\nfrom .scicrunch_parser import SciCrunchParser\nfrom .topmed_tag_parser import TOPMedTagParser\nfrom .topmed_csv_parser import TOPMedCSVParser\nfrom .sprint_parser import SPRINTParser\nfrom .bacpac_parser import BACPACParser\nfrom .heal_dp_parser import HEALDPParser\nfrom .ctn_parser import CTNParser\nfrom .radx_parser import RADxParser\n\n\nlogger = logging.getLogger('dug')\n\nhookimpl = pluggy.HookimplMarker(\"dug\")\n\n\n@hookimpl\ndef define_parsers(parser_dict: Dict[str, Parser]):\n    parser_dict[\"dbgap\"] = DbGaPParser()\n    parser_dict[\"nida\"] = NIDAParser()\n    parser_dict[\"topmedtag\"] = TOPMedTagParser()\n    parser_dict[\"topmedcsv\"] = TOPMedCSVParser()\n    parser_dict[\"scicrunch\"] = SciCrunchParser()\n    parser_dict[\"anvil\"] = AnvilDbGaPParser()\n    parser_dict[\"crdc\"] = CRDCDbGaPParser()\n    parser_dict[\"kfdrc\"] = KFDRCDbGaPParser()\n    parser_dict[\"sprint\"] = SPRINTParser()\n    parser_dict[\"bacpac\"] = BACPACParser()\n    parser_dict[\"heal-studies\"] = HEALDPParser(study_type=\"HEAL Studies\")\n    parser_dict[\"heal-research\"] = HEALDPParser(study_type=\"HEAL Research Programs\")\n    parser_dict[\"ctn\"] = CTNParser()\n    parser_dict[\"biolincc\"] = BioLINCCDbGaPParser()\n    parser_dict[\"covid19\"] = Covid19DbGaPParser()\n    parser_dict[\"dir\"] = DIRDbGaPParser()\n    parser_dict[\"lungmap\"] = LungMAPDbGaPParser()\n    parser_dict[\"nsrr\"] = NSRRDbGaPParser()\n    parser_dict[\"parent\"] = ParentDBGaPParser()\n    parser_dict[\"pcgc\"] = PCGCDbGaPParser()\n    parser_dict[\"recover\"] = RECOVERDBGaPParser()\n    parser_dict[\"topmeddbgap\"] = TopmedDBGaPParser()\n    parser_dict[\"curesc\"] = CureSC()\n    parser_dict[\"radx\"] = RADxParser()\n\n\n    \n\n\n\nclass ParserNotFoundException(Exception):\n    ...\n\n\ndef get_parser(hook, parser_name) -> Parser:\n    \"\"\"Get the parser from all parsers registered via the define_parsers hook\"\"\"\n\n    available_parsers = {}\n    hook.define_parsers(parser_dict=available_parsers)\n    parser = available_parsers.get(parser_name.lower())\n    if parser is not None:\n        return parser\n\n    err_msg = f\"Cannot find parser of type '{parser_name}'\\n\" \\\n              f\"Supported parsers: {', '.join(available_parsers.keys())}\"\n    logger.error(err_msg)\n    raise ParserNotFoundException(err_msg)\n"}
{"type": "source_file", "path": "src/dug/core/parsers/radx_parser.py", "content": "import logging\r\nfrom typing import List\r\nfrom xml.etree import ElementTree as ET\r\n\r\nfrom dug import utils as utils\r\nfrom dug.core.parsers._base import DugElement, FileParser, Indexable, InputFile, DugConcept\r\nimport json\r\n\r\n\r\nlogger = logging.getLogger('dug')\r\n\r\n\r\nclass RADxParser(FileParser):\r\n\r\n    def __call__(self, input_file: InputFile) -> List[Indexable]:\r\n        with open(input_file) as stream:\r\n            json_raw_data = json.load(stream)\r\n        # get all records (records in radx json = variables)\r\n        records = json_raw_data['records']\r\n        elements = []\r\n        for r in records:\r\n            if r['studies']:\r\n                concepts = r['terms'] or []\r\n                concepts_objs = []\r\n                for c in concepts:\r\n                    concept_obj = DugConcept(\r\n                        concept_id=c['identifier'],\r\n                        name=c['label'],\r\n                        concept_type=\"biolink:NamedThing\",\r\n                        desc=\"\",\r\n                    )\r\n                    concept_obj.search_terms = c['synonyms']\r\n                    concepts_objs.append(concept_obj)\r\n\r\n                studies_dict = {x['id']: x for x in r['studies']}\r\n                for s_id, s in studies_dict.items():\r\n                    elem = DugElement(\r\n                        elem_id=r['id'],\r\n                        name=r['label'],\r\n                        desc=r['description'],\r\n                        elem_type=s['program'],\r\n                        collection_id=s['phs'],\r\n                        collection_name=s['study_name'],\r\n                        collection_action=f\"https://radxdatahub.nih.gov/study/{s['id']}\"\r\n                    )\r\n                    for c in concepts_objs:\r\n                        elem.add_concept(c)\r\n                    elem.add_metadata(\r\n                        {\r\n                            'datatype': r['datatype'] or None,\r\n                            'cardinality': r['cardinality'] or '',\r\n                            'section': r['section'] or '',\r\n                            'enumeration': r['enumeration'] or []\r\n                         }\r\n                    )\r\n                    elements.append(elem)\r\n        return elements\r\n"}
{"type": "source_file", "path": "src/dug/core/loaders/_base.py", "content": "from pathlib import Path\nfrom typing import Union, Iterable, Callable, Iterator\n\nInputFile = Union[str, Path]\n\nLoader = Callable[[str], Iterator[Path]]\n"}
{"type": "source_file", "path": "src/dug/core/parsers/bacpac_parser.py", "content": "import logging\nfrom typing import List\nfrom xml.etree import ElementTree as ET\n\nfrom dug import utils as utils\nfrom ._base import DugElement, FileParser, Indexable, InputFile\n\nlogger = logging.getLogger('dug')\n\n\nclass BACPACParser(FileParser):\n    # Class for parsing BACPAC data dictionaries in dbGaP XML format into a set of Dug Elements.\n\n    @staticmethod\n    def get_study_file_name():\n        # Parse the form name from the xml filename\n        return \"Back Pain Consortium (BACPAC) Minimum Dataset\"\n\n    def __call__(self, input_file: InputFile) -> List[Indexable]:\n        logger.debug(input_file)\n        tree = ET.parse(input_file)\n        root = tree.getroot()\n        study_id = \"HEALPLATFORM:HDP00692\"\n\n        # Parse study name from file handle\n        study_name = self.get_study_file_name()\n\n        if study_name is None:\n            err_msg = f\"Unable to parse BACPAC Form name from data dictionary: {input_file}!\"\n            logger.error(err_msg)\n            raise IOError(err_msg)\n\n        elements = []\n        for variable in root.iter('variable'):\n            description = variable.find('description').text or \"\"\n            elem = DugElement(elem_id=f\"{variable.attrib['id']}\",\n                              name=variable.find('name').text,\n                              desc=description.lower(),\n                              elem_type=\"BACPAC\",\n                              collection_id=f\"{study_id}\",\n                              collection_name=study_name\n            )\n            elem.action = \"https://healdata.org/portal/discovery/HDP00692\"      \n            elem.collection_action = \"https://healdata.org/portal/discovery/HDP00692\"\n            # Add to set of variables\n            logger.debug(elem)\n            elements.append(elem)\n\n        # You don't actually create any concepts\n        return elements\n"}
{"type": "source_file", "path": "src/dug/core/parsers/sprint_parser.py", "content": "import logging\nimport os\nfrom typing import List\nfrom xml.etree import ElementTree as ET\n\nfrom dug import utils as utils\nfrom ._base import DugElement, FileParser, Indexable, InputFile\n\nlogger = logging.getLogger('dug')\n\n\nclass SPRINTParser(FileParser):\n    # Class for parsers SPRINT Data dictionary into a set of Dug Elements\n\n    @staticmethod\n    def parse_study_name_from_filename(filename: str):\n        # Parse the form name from the xml filename\n        return filename.split('/')[-1].replace('.xml', '')\n\n    def __call__(self, input_file: InputFile) -> List[Indexable]:\n        logger.debug(input_file)\n        tree = ET.parse(input_file)\n        root = tree.getroot()\n        study_id = root.attrib['study_id']\n\n        # Parse study name from file handle\n        study_name = self.parse_study_name_from_filename(str(input_file))\n\n        if study_name is None:\n            err_msg = f\"Unable to parse SPRINT Form name from data dictionary: {input_file}!\"\n            logger.error(err_msg)\n            raise IOError(err_msg)\n\n        elements = []\n        for variable in root.iter('variable'):\n            description = variable.find('description').text or \"\"\n            elem = DugElement(elem_id=f\"{variable.attrib['id']}\",\n                              name=variable.find('name').text,\n                              desc=description.lower(),\n                              elem_type=\"SPRINT\",\n                              collection_id=f\"{study_id}\",\n                              collection_name=study_name)\n\n            # Add to set of variables\n            logger.debug(elem)\n            elements.append(elem)\n\n        # You don't actually create any concepts\n        return elements\n"}
{"type": "source_file", "path": "src/dug/hookspecs.py", "content": "from typing import Dict\n\nimport pluggy\n\nfrom dug.core.parsers import Parser\nfrom dug.core.annotators import Annotator\nfrom dug.config import Config\n\nhookspec = pluggy.HookspecMarker(\"dug\")\n\n\n@hookspec\ndef define_parsers(parser_dict: Dict[str, Parser]):\n    \"\"\"Defines what parsers are available to Dug\n    \"\"\"\n    ...\n\n@hookspec\ndef define_annotators(annotator_dict: Dict[str, Annotator], config: Config):\n    \"\"\"Defines what Annotators are available to Dug\n    \"\"\"\n    ...\n"}
{"type": "source_file", "path": "src/dug/core/parsers/topmed_csv_parser.py", "content": "import csv\nimport logging\nfrom typing import List\n\nfrom dug import utils as utils\nfrom ._base import DugElement, FileParser, Indexable, InputFile\n\nlogger = logging.getLogger('dug')\n\n\nclass TOPMedCSVParser(FileParser):\n\n    def __call__(self, input_file: InputFile) -> List[Indexable]:\n        \"\"\"\n        Load tagged variables.\n        Presumes a harmonized variable list as a CSV file as input.\n        A naming convention such that <prefix>_variables_<version>.csv will be the form of the filename.\n        An adjacent file called <prefix>_tags_<version.json will exist.\n        :param input_file: A list of study variables linked to tags, studies, and other relevant data.\n        :returns: Returns variables, a list of parsed variable dictionaries and tags, a list of parsed\n                  tag definitions linked to the variabels.\n        \"\"\"\n\n        logger.debug(input_file)\n        if not input_file.endswith(\".csv\"):\n            return []\n\n        # Now loop through associated variables and associate each with its parent concept/tag\n        elements: List[Indexable] = []\n        with open(input_file, newline='') as csvfile:\n            reader = csv.DictReader(csvfile, delimiter='\\t')\n            for row in reader:\n                row = {k.strip(): v for k, v in row.items()}\n                elem = DugElement(elem_id=row['variable_full_accession'],\n                                  name=row['variable_name'],\n                                  desc=row['variable_desc'],\n                                  elem_type=\"TOPMed\",\n                                  collection_id=row['study_full_accession'],\n                                  collection_name=row['study_name'])\n\n                # Create DBGaP links as study/variable actions\n                elem.collection_action = utils.get_dbgap_study_link(study_id=elem.collection_id)\n                elem.action = utils.get_dbgap_var_link(study_id=elem.collection_id,\n                                                       variable_id=elem.id.split(\".\")[0].split(\"phv\")[1])\n\n                # Add element to list of elements\n                elements.append(elem)\n                logger.debug(elem)\n\n        return elements\n"}
{"type": "source_file", "path": "src/dug/core/parsers/topmed_tag_parser.py", "content": "import csv\nimport json\nimport logging\nimport os\nfrom typing import List\n\nfrom dug import utils as utils\nfrom ._base import DugConcept, DugElement, FileParser, Indexable, InputFile\n\nlogger = logging.getLogger('dug')\n\n\nclass TOPMedTagParser(FileParser):\n\n    def __call__(self, input_file: InputFile) -> List[Indexable]:\n        \"\"\"\n        Load tagged variables.\n        Presumes a harmonized variable list as a CSV file as input.\n        A naming convention such that <prefix>_variables_<version>.csv will be the form of the filename.\n        An adjacent file called <prefix>_tags_<version.json will exist.\n\n        :param input_file: A list of study variables linked to tags, studies, and other relevant data.\n        :returns: Returns variables, a list of parsed variable dictionaries and tags, a list of parsed\n                  tag definitions linked to the variabels.\n        \"\"\"\n\n        logger.debug(input_file)\n        if not input_file.endswith(\".csv\"):\n            return []\n        tags_input_file = input_file.replace(\".csv\", \".json\").replace(\"_variables_\", \"_tags_\")\n        if not os.path.exists(tags_input_file):\n            raise ValueError(f\"Accompanying tags file: {tags_input_file} must exist.\")\n\n        # Read in huamn-created tags/concepts from json file before reading in elements\n        with open(tags_input_file, \"r\") as stream:\n            tags = json.load(stream)\n\n        # Loop through tags and create concepts for each one\n        concepts = {}\n        for tag in tags:\n            concept_id = f\"TOPMED.TAG:{tag['pk']}\"\n            concept = DugConcept(concept_id,\n                                 name=tag['fields']['title'],\n                                 desc=f'{tag[\"fields\"][\"description\"]}. {tag[\"fields\"][\"instructions\"]}',\n                                 concept_type=\"TOPMed Phenotype Concept\")\n\n            # Only use the description for annotation\n            concept.ml_ready_desc = tag[\"fields\"][\"description\"]\n            concepts[str(tag['pk'])] = concept\n            logger.debug(concept)\n\n        # Now loop through associated variables and associate each with its parent concept/tag\n        elements: List[Indexable] = []\n        with open(input_file, newline='') as csvfile:\n            reader = csv.DictReader(csvfile, delimiter='\\t')\n            for row in reader:\n                row = {k.strip(): v for k, v in row.items()}\n                elem = DugElement(\n                    elem_id=row['variable_full_accession'],\n                    name=row['variable_name'] if 'variable_name' in row else row['variable_full_accession'],\n                    desc=row['variable_description'] if 'variable_description' in row else row['variable_full_accession'],\n                    elem_type=\"TOPMed\",\n                    collection_id=row['study_full_accession'],\n                    collection_name=row['study_name']\n                )\n\n                # Create DBGaP links as study/variable actions\n                elem.collection_action = utils.get_dbgap_study_link(study_id=elem.collection_id)\n                elem.action = utils.get_dbgap_var_link(study_id=elem.collection_id,\n                                                       variable_id=elem.id.split(\".\")[0].split(\"phv\")[1])\n\n                # Add concept parsed from tag file to element\n                concept_group = row['tag_pk']\n                if concept_group not in concepts:\n                    # Raise error if the tag_id parsed from the variable file didn't exist in the tag file\n                    err_msg = f\"DbGaP variable '{elem.id}' maps to a tag group that doesn't exist in tag file: '{concept_group}'\"\n                    logger.error(err_msg)\n                    raise IOError(err_msg)\n                elem.add_concept(concepts[row['tag_pk']])\n\n                # Add element to list of elements\n                elements.append(elem)\n                logger.debug(elem)\n\n        return list(concepts.values()) + elements\n"}
{"type": "source_file", "path": "src/dug/core/tranql.py", "content": "import json\nfrom dug.utils import biolink_snake_case\n\n\nclass MissingNodeReferenceError(BaseException):\n    pass\n\n\nclass MissingEdgeReferenceError(BaseException):\n    pass\n\n\nclass QueryKG:\n    def __init__(self, kg_json):\n        self.kg = kg_json[\"message\"]\n        self.answers = self.kg.get(\"results\", [])\n        self.question = self.kg.get(\"query_graph\", {})\n        self.nodes = self.kg.get(\"knowledge_graph\", {}).get(\"nodes\") # {node[\"id\"]: node for node in kg_json.get('knowledge_graph', {}).get('nodes', [])}\n        self.edges = self.kg.get(\"knowledge_graph\", {}).get(\"edges\") # {edge[\"id\"]: edge for edge in kg_json.get('knowledge_graph', {}).get('edges', [])}\n\n    def get_answer_subgraph(self, answer, include_node_keys=[], include_edge_keys=[]):\n\n        # Get answer nodes\n        answer_nodes = {}\n        for binding_id, binding_nodes in answer[\"node_bindings\"].items():\n            # Add node info for each node included in answer graph\n            for answer_node in binding_nodes:\n                # Throw error if node doesn't actually exist in 'nodes' section of knowledge graph\n                if answer_node[\"id\"] not in self.nodes:\n                    err_msg = f\"Unable to assemble subraph for answer:\\n{json.dumps(answer, indent=2)}\\n\" \\\n                              f\"Parent graph doesn't contain node info for: {answer_node}\"\n                    raise MissingNodeReferenceError(err_msg)\n\n                # Add only node info that you actually want\n                answer_nodes[answer_node[\"id\"]] = self.get_node(answer_node[\"id\"], include_node_keys)\n\n        # Get answer edges\n        answer_edges = {}\n        for binding_id, binding_edges in answer[\"edge_bindings\"].items():\n            # Add edge info for each edge included in answer graph\n            for answer_edge in binding_edges:\n                # Throw error if edge doesn't actually exist in 'edges' section of knowledge graph\n                if answer_edge[\"id\"] not in self.edges:\n                    err_msg = f\"Unable to assemble subraph for answer:\\n{json.dumps(answer, indent=2)}\\n\" \\\n                        f\"Parent graph doesn't contain edge info for: {answer_edge}\"\n                    raise MissingEdgeReferenceError(err_msg)\n\n                # Add only information from edge that you actually want\n                answer_edges[answer_edge[\"id\"]] = self.get_edge(answer_edge[\"id\"], include_edge_keys)\n\n        kg = {\"message\":{\n                \"knowledge_graph\": {\n                    \"nodes\": answer_nodes,\n                    \"edges\": answer_edges\n                        },\n                \"results\": [answer],\n                \"query_graph\": self.question\n                }\n              }\n\n        return QueryKG(kg)\n\n    def _parse_attributes(self, kg_component):\n        \"\"\"\n        Extracts attributes to normal dict from Trapi 1.0 KG nodes / edges\n        Trapi 1.0 has {\"id\": \"xxx\", \"name\": \"xxx\", \"attributes\" : {\"name\": \"publication\", \"value\": \"xxx\",...}}\n        :param kg_component: Dict representing a node or an edge\n        :return:\n        \"\"\"\n        return {attr[\"name\"]: attr[\"value\"] for attr in kg_component.get(\"attributes\", {})}\n\n    def get_node(self, node_id, include_node_keys=[]):\n        # Return node with optionally subsetted information\n        # Trapi 1.0 has {\"id\": \"xxx\", \"name\": \"xxx\", \"attributes\" : [{\"name\": \"publication\", \"value\": \"xxx\"...}, {},...]}\n        node = self._parse_attributes(self.nodes[node_id])\n        node.update({k: v for k, v in self.nodes[node_id].items() if k != \"attributes\"})\n\n        node[\"id\"] = node_id\n        node[\"name\"] = self.nodes[node_id].get(\"name\", \"\")\n        # Optionally subset to get only certain information columns\n        if include_node_keys:\n            node = {key: node.get(key) for key in include_node_keys}\n        return node\n\n    def get_edge(self, edge_id, include_edge_keys=[]):\n        # Return edge with optionally subsetted information\n        edge = self._parse_attributes(self.edges[edge_id])\n        edge.update({k: v for k, v in self.edges[edge_id].items() if k != \"attributes\"})\n\n        edge[\"id\"] = edge_id\n        edge[\"publications\"] = edge.get(\"publications\", [])\n        if isinstance(edge[\"publications\"], str):\n            edge[\"publications\"] = [edge[\"publications\"]]\n        # Optionally subset to include only certain info\n        if include_edge_keys:\n            edge = {key: edge.get(key) for key in include_edge_keys}\n        return edge\n\n    def get_nodes(self):\n        nodes_dict = self.kg.get(\"knowledge_graph\", {}).get(\"nodes\", {})\n        return [self.get_node(curie) for curie in nodes_dict]\n\n    def get_edges(self):\n        edges_dict = self.kg.get(\"knowledge_graph\", {}).get(\"edges\", {})\n        return [self.get_edge(kg_id) for kg_id in edges_dict]\n\n    def get_node_names(self, include_curie=True):\n        node_names = []\n        curie_ids = self.get_curie_ids()\n        for node in self.get_nodes():\n            if include_curie or node['id'] not in curie_ids:\n                node_names.append(node['name'])\n        return node_names\n\n    def get_node_synonyms(self, include_curie=True):\n        # @TODO call name-resolver \n        node_synonyms = []\n        curie_ids = self.get_curie_ids()\n        for node in self.get_nodes():\n            if include_curie or node['id'] not in curie_ids:\n                syn = node.get('synonyms') \n                if isinstance(syn,list):\n                    node_synonyms +=  syn \n        return node_synonyms\n\n    def get_curie_ids(self):\n        question_nodes_dict = self.question.get('nodes', {})\n        return [question_nodes_dict[node]['id'] for node in question_nodes_dict if 'id' in question_nodes_dict[node]]\n\n    def get_kg(self):\n        # Parse out the KG in whatever form we want\n        # TODO: Make this parse out old-style json so ui doesn't break\n        old_kg_model = {\n            \"knowledge_map\": [],\n            \"knowledge_graph\": {\n                \"nodes\": [],\n                \"edges\": [],\n            },\n            \"question_graph\": {\n                \"nodes\": [],\n                \"edges\": []\n            }\n        }\n        query_graph = self.kg.get(\"query_graph\")\n        for q_id in query_graph[\"nodes\"]:\n            node_details = query_graph[\"nodes\"][q_id]\n            node_curie = node_details.get(\"id\", \"\")\n            category = node_details.get(\"category\", [])\n            if type(category) == list:\n                node_type = [self._snake_case(x.replace('biolink.', '')) for x in category]\n            else:\n                node_type = [self._snake_case(category.replace('biolink.', ''))]\n            old_node = {\"id\": q_id, \"type\": node_type}\n            if node_curie:\n                old_node.update({\"curie\": node_curie})\n            old_kg_model[\"question_graph\"][\"nodes\"].append(old_node)\n\n        for q_id in query_graph[\"edges\"]:\n            edge_details = query_graph[\"edges\"][q_id]\n            old_edge = {\"id\": q_id, \"source_id\": edge_details[\"subject\"], \"target_id\": edge_details[\"object\"]}\n            edge_type = edge_details.get(\"predicate\")\n            if edge_type:\n                old_edge.update({\"type\": edge_type})\n            old_kg_model[\"question_graph\"][\"edges\"].append(old_edge)\n\n        results = self.kg.get(\"results\")\n        for bindings in results:\n            old_binding = {}\n            for binding_type in bindings:\n                for q_id in bindings[binding_type]:\n                    kg_ids = [x[\"id\"] for x in bindings[binding_type][q_id]]\n                    old_binding[binding_type] = old_binding.get(binding_type, {})\n                    old_binding[binding_type][q_id] = old_binding[binding_type].get(q_id,[])\n                    old_binding[binding_type][q_id] = kg_ids\n            old_kg_model[\"knowledge_map\"].append(old_binding)\n        old_kg_model[\"knowledge_graph\"][\"nodes\"] = self.get_nodes()\n        for node in old_kg_model[\"knowledge_graph\"][\"nodes\"]:\n            # adds id for node name if no name is present\n            node[\"name\"] = node[\"name\"] if node[\"name\"] else node[\"id\"]\n        old_kg_model[\"knowledge_graph\"][\"edges\"] = self.get_edges()\n        for edge in old_kg_model[\"knowledge_graph\"][\"edges\"]:\n            # adds predicate as type for edges\n            edge[\"type\"] = edge[\"predicate\"]\n            # source_id and target_id should always be str\n            edge[\"source_id\"] = edge[\"subject\"]\n            edge[\"target_id\"] = edge[\"object\"]\n        return old_kg_model\n\n    def _snake_case(self, arg: str):\n        return biolink_snake_case(arg)\n\n\nclass InvalidQueryError(BaseException):\n    pass\n\n\nclass QueryFactory:\n\n    # Class member list of valid data types that can be included in query\n    data_types = [\"publication\", \"phenotypic_feature\", \"gene\", \"disease\", \"chemical_substance\",\n                  \"drug_exposure\", \"biological_process\", \"anatomical_entity\", \"small_molecule\",\n                  \"chemical_mixture\", \"chemical_entity\"]\n\n    # List of curie prefixes that are valid for certain curie types\n    curie_map = {\"disease\": [\"MONDO\", \"ORPHANET\", \"DOID\"],\n                 \"phenotypic_feature\": [\"HP\", \"HPO\", \"EFO\"],\n                 \"gene\": [\"HGNC\", \"NCBIGene\"],\n                 \"chemical_substance\": [\"CHEBI\", \"PUBCHEM.COMPOUND\", \"CHEMBL.COMPOUND\"],\n                 \"chemical_mixture\": [\"CHEBI\", \"PUBCHEM.COMPOUND\", \"CHEMBL.COMPOUND\"],\n                 \"chemical_entity\": [\"CHEBI\", \"PUBCHEM.COMPOUND\", \"CHEMBL.COMPOUND\"],\n                 \"small_molecule\": [\"CHEBI\", \"PUBCHEM.COMPOUND\", \"CHEMBL.COMPOUND\"],\n                 \"anatomical_entity\": [\"UBERON\"]}\n\n    def __init__(self, question_graph, source, curie_index=0):\n\n        # List of terms that are going to be connected to make a query\n        self.question_graph = question_graph\n\n        # Index in question graph that will be matched against curies\n        self.curie_index = curie_index\n\n        # Query source (e.g. /schema or /graph/gamma/quick)\n        self.source = source\n\n        # Check to make sure curie index isn't out of range\n        if self.curie_index >= len(self.question_graph):\n            raise InvalidQueryError(f\"Invalid query index ({curie_index})! Question graph only \"\n                                    f\"contains {len(self.question_graph)} entries!\")\n\n        # Set the type of the curie for downstream curie checking\n        self.curie_type = self.question_graph[self.curie_index]\n\n        # Validate that all entries in question graph are actually valid types\n        self.validate_factory()\n\n    def validate_factory(self):\n        # Check to make sure all the question types are valid\n        for question in self.question_graph:\n            if not question in QueryFactory.data_types:\n                raise InvalidQueryError(f\"Query contains invalid query type: {question}\")\n\n    def is_valid_curie(self, curie):\n        # Return whether a curie can be used to create a valid query\n\n        # Handle case where curie type has no limitations\n        if self.curie_type not in QueryFactory.curie_map:\n            return True\n\n        # Otherwise only return true if current query contains one of the acceptable prefixes\n        for curie_prefix in QueryFactory.curie_map[self.curie_type]:\n            if curie.startswith(curie_prefix):\n                return True\n\n        # Curie doesn't start with an acceptable prefix\n        return False\n\n    def get_query(self, curie):\n\n        # Return nothing if not valid curie\n        if not self.is_valid_curie(curie):\n            return None\n\n        question = []\n        seen = []\n        curie_id = \"\"\n        for i in range(len(self.question_graph)):\n            query_type = self.question_graph[i]\n            if self.question_graph.count(query_type) > 1:\n                # Add alias to beginning of types we've seen before\n                alias = f\"{query_type[0:3]}{len([x for x in seen if x == query_type])}\"\n                query = f\"{alias}:{query_type}\"\n            else:\n                alias = query_type\n                query = query_type\n\n            # Set curie id to the alias if this is the correct index\n            if i == self.curie_index:\n                curie_id = alias\n\n            # Append to list of query_types currently in query\n            seen.append(query_type)\n            # Append to list of actual terms that will appear in query\n            question.append(query)\n\n        # Build and return query\n        return f\"select {'->'.join(question)} from '{self.source}' where {curie_id}='{curie}'\"\n"}
{"type": "source_file", "path": "src/dug/utils.py", "content": "import re\nimport bmt\n\nbmt_tk = bmt.Toolkit()\n\nclass ObjectFactory:\n    def __init__(self):\n        self._builders = {}\n\n    def register_builder(self, key, builder):\n        self._builders[key] = builder\n\n    def create(self, key, **kwargs):\n        builder = self._builders.get(key)\n        if not builder:\n            raise ValueError(key)\n        return builder(**kwargs)\n\n    def get_builder_types(self):\n        return list(self._builders.keys())\n\n\ndef complex_handler(obj):\n    if hasattr(obj, 'jsonable'):\n        return obj.jsonable()\n    else:\n        raise TypeError(f'Object of type {type(obj)} with value of {type(obj)} is not JSON serializable')\n\n\ndef get_dbgap_var_link(study_id, variable_id):\n    base_url = \"https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/variable.cgi\"\n    return f'{base_url}?study_id={study_id}&phv={variable_id}'\n\n\ndef get_dbgap_study_link(study_id):\n    base_url = \"https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi\"\n    return f'{base_url}?study_id={study_id}'\n\ndef get_nida_study_link(study_id):\n    base_url = \"https://datashare.nida.nih.gov/study\"\n    return f'{base_url}/{study_id}'\n\ndef get_heal_platform_link(study_id):\n    base_url = \"https://healdata.org/portal/discovery\"\n    accession = study_id.split(':')[1]\n    return f'{base_url}/{accession}'\n\ndef get_ctn_link(study_id):\n    base_url = \"https://ctnlibrary.org/protocol\"\n    accession = study_id.split(':')[1]\n    return f'{base_url}/{accession}'\n\ndef biolink_snake_case(arg):\n    \"\"\"Convert such SnakeCase to snake_case.\n       Non-alphanumeric characters are replaced with _.\n       CamelCase is replaced with snake_case.\n    \"\"\"\n    # replace non-alphanumeric characters with _\n    tmp = re.sub(r'\\W', '_', arg)\n    # replace X with _x\n    tmp = re.sub(\n        r'(?<=[a-z])[A-Z](?=[a-z])',\n        lambda c: '_' + c.group(0).lower(),\n        tmp\n    )\n    # lower-case first character\n    tmp = re.sub(\n        r'^[A-Z](?=[a-z])',\n        lambda c: c.group(0).lower(),\n        tmp\n    )\n    return tmp\n\ndef get_formatted_biolink_name(bl_type):\n    category = bl_type\n    if isinstance(bl_type, str):\n        bl_element = bmt_tk.get_element(bl_type)\n        category = bl_element.class_uri or bl_element.slot_uri\n    if isinstance(bl_type, list):\n        return get_formatted_biolink_name(bl_type[0])\n    return category"}
{"type": "source_file", "path": "src/dug/server.py", "content": "import logging\nimport os\nimport uvicorn\n\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom dug.config import Config\nfrom dug.core.async_search import Search\nfrom pydantic import BaseModel\nimport asyncio\nfrom typing import Optional\n\nlogger = logging.getLogger (__name__)\n\nAPP = FastAPI(\n    title=\"Dug Search API\",\n    root_path=os.environ.get(\"ROOT_PATH\", \"\"),\n)\n\nAPP.add_middleware(\n    CORSMiddleware,\n    allow_origins=['*'],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\nclass GetFromIndex(BaseModel):\n    index: str = \"concepts_index\"\n    size: int = 0\n\n\nclass SearchConceptQuery(BaseModel):\n    query: str\n    index: str = \"concepts_index\"\n    offset: int = 0\n    size: int = 20\n    types: list = None\n\nclass SearchVariablesQuery(BaseModel):\n    query: str\n    index: str = \"variables_index\"\n    concept: str = \"\"\n    offset: int = 0\n    size: int = 1000\n\nclass SearchKgQuery(BaseModel):\n    query: str\n    unique_id: str\n    index: str = \"kg_index\"\n    size:int = 100\n\nclass SearchStudyQuery(BaseModel):\n    #query: str\n    study_id: Optional[str] = None\n    study_name: Optional[str] = None\n    #index: str = \"variables_index\"\n    size:int = 100\nclass SearchProgramQuery(BaseModel):\n    #query: str\n    program_id: Optional[str] = None\n    program_name: Optional[str] = None\n    #index: str = \"variables_index\"\n    size:int = 100   \n\nsearch = Search(Config.from_env())\n\n@APP.on_event(\"shutdown\")\ndef shutdown_event():\n    asyncio.run(search.es.close())\n\n\n@APP.post('/dump_concepts')\nasync def dump_concepts(request: GetFromIndex):\n    return {\n        \"message\": \"Dump result\",\n        \"result\": await search.dump_concepts(**request.dict()),\n        \"status\": \"success\"\n    }\n\n\n@APP.get('/agg_data_types')\nasync def agg_data_types():\n    return {\n        \"message\": \"Dump result\",\n        \"result\": await search.agg_data_type(),\n        \"status\": \"success\"\n    }\n\n\n@APP.post('/search')\nasync def search_concepts(search_query: SearchConceptQuery):\n    return {\n        \"message\": \"Search result\",\n        # Although index in provided by the query we will keep it around for backward compatibility, but\n        # search concepts should always search against \"concepts_index\"\n        \"result\": await search.search_concepts(**search_query.dict(exclude={\"index\"})),\n        \"status\": \"success\"\n    }\n\n\n@APP.post('/search_kg')\nasync def search_kg(search_query: SearchKgQuery):\n    return {\n        \"message\": \"Search result\",\n        # Although index in provided by the query we will keep it around for backward compatibility, but\n        # search concepts should always search against \"kg_index\"\n        \"result\": await search.search_kg(**search_query.dict(exclude={\"index\"})),\n        \"status\": \"success\"\n    }\n\n\n@APP.post('/search_var')\nasync def search_var(search_query: SearchVariablesQuery):\n    return {\n        \"message\": \"Search result\",\n        # Although index in provided by the query we will keep it around for backward compatibility, but\n        # search concepts should always search against \"variables_index\"\n        \"result\": await search.search_variables(**search_query.dict(exclude={\"index\"})),\n        \"status\": \"success\"\n    }\n\n\n@APP.post('/search_var_grouped')\nasync def search_var_grouped(search_query: SearchVariablesQuery):\n    if search_query.query == \"\":\n        results = await search.dump_concepts(search_query.index, size=search_query.size )\n        search_result_hits = results['result']['hits']['hits']\n        results = search._make_result(None, search_result_hits, {\"count\": search_query}, False)\n\n    else:\n        results = await search.search_variables(**search_query.dict(exclude={\"index\"}))\n    all_elements = []\n    for program_name in filter(lambda x: x != 'total_items', results.keys()):\n        studies = results[program_name]\n        for s in studies:\n            elements = s['elements']\n            for e in elements:\n                new_element = e\n                new_element.update(\n                    {k: v for k, v in s.items() if k != 'elements'}\n                )\n                new_element['program_name'] = program_name\n                all_elements.append(new_element)\n    # regroup by variables\n    by_id = {}\n    for e in all_elements:\n        by_id[e['id']] = by_id.get(e['id'], [])\n        by_id[e['id']].append(e)\n    var_info = None\n    study_info_keys = [\n        'c_id', 'c_link', 'c_name', 'program_name'\n    ]\n    final_variables = []\n    count_keys = set()\n    for var_id in by_id:\n        var_studies = by_id[var_id]\n        for s in var_studies:\n            if not var_info:\n                var_info = {\n                    k: v for k, v in s.items() if k not in study_info_keys\n                }\n                var_info.update(var_info['metadata'])\n                for k in var_info['metadata']:\n                    if isinstance(var_info['metadata'][k], str):\n                        count_keys.add(k)\n                var_info.pop('metadata')\n                var_info['studies'] = []\n            study_data = {k: v for k, v in s.items() if k in study_info_keys}\n            var_info['studies'].append(study_data)\n        final_variables.append(var_info)\n        var_info = None\n    agg_counts = {}\n    for var in final_variables:\n        for key in count_keys:\n            if key in var:\n                val = var[key]\n                agg_counts[key] = agg_counts.get(key , {})\n                agg_counts[key][val] = agg_counts[key].get(val, 0)\n                agg_counts[key][val] += 1\n    return {\n        \"variables\": final_variables,\n        \"agg_counts\": agg_counts\n    }\n\n\n@APP.get('/search_study')\nasync def search_study(study_id: Optional[str] = None, study_name: Optional[str] = None):\n    \"\"\"\n    Search for studies by unique_id (ID or name) and/or study_name.\n    \"\"\"\n    result = await search.search_study(study_id=study_id, study_name=study_name)\n    return {\n        \"message\": \"Search result\",\n        \"result\": result,\n        \"status\": \"success\"\n    }\n\n\n@APP.get('/search_program')\nasync def search_program( program_name: Optional[str] = None):\n    \"\"\"\n    Search for studies by unique_id (ID or name) and/or study_name.\n    \"\"\"\n    result = await search.search_program(program_name=program_name)\n    return {\n        \"message\": \"Search result\",\n        \"result\": result,\n        \"status\": \"success\"\n    }\n\n@APP.get('/program_list')\nasync def get_program_list():\n    \"\"\"\n    Search for program by program name.\n    \"\"\"\n    result = await search.search_program_list()\n    return {\n  \n        \"result\": result,\n        \"status\": \"success\"\n    }\nif __name__ == '__main__':\n    uvicorn.run(APP,port=8181)\n"}
