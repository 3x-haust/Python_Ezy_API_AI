{"repo_info": {"repo_name": "webspot", "repo_owner": "crawlab-team", "repo_url": "https://github.com/crawlab-team/webspot"}}
{"type": "test_file", "path": "webspot/test/crawler/test_crawler.py", "content": "import unittest\nfrom io import StringIO\nfrom unittest.mock import patch\n\nfrom scrapy.crawler import CrawlerProcess\n\nfrom webspot.crawler.actions.run_crawler import run_crawler\nfrom webspot.crawler.crawler import WebSpider\n\n\nclass TestCrawler(unittest.TestCase):\n    @patch('sys.stderr', new_callable=StringIO)\n    def test_settings(self, stderr):\n        domain = 'example.com'\n        urls = ['https://example.com']\n        process = CrawlerProcess(settings={\n            'domain': domain,\n            'urls': urls,\n            'is_test': True,\n        })\n        process.crawl(WebSpider)\n        process.start()\n        self.assertTrue(domain in stderr.getvalue())\n        self.assertTrue(urls[0] in stderr.getvalue())\n\n    def test_crawl(self):\n        domain = 'quotes.toscrape.com'\n        urls = ['https://quotes.toscrape.com/']\n        run_crawler(domain, urls, is_test=True)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "webspot/test/detect/test_plain_list.py", "content": "import pytest\n\nfrom webspot.detect.detectors.plain_list import run_plain_list_detector\nfrom webspot.logging import get_logger\n\nlogger = get_logger('webspot.test.detect.test_plain_list')\n\ntest_cases = [\n    {\n        'url': 'https://quotes.toscrape.com',\n        'result': {\n            'selectors': {\n                'list': 'body > div.container > div.row:last-child > div.col-md-8'\n            }\n        }\n    },\n    {\n        'url': 'https://books.toscrape.com',\n        'result': {\n            'selectors': {\n                'list': 'section > div:last-child > ol.row'\n            }\n        },\n    },\n    {\n        'url': 'https://github.com/trending',\n        'result': {\n            'selectors': {\n                'list': 'main > div.position-relative.container-lg.p-responsive.pt-6 > div.Box > div:last-child'\n            }\n        },\n    },\n    {\n        'url': 'https://github.com/search?q=spider&type=repositories',\n        'result': {\n            'selectors': {\n                'list': 'div.px-2 > ul.repo-list'\n            }\n        },\n        'index': 1,\n    },\n    {\n        'url': 'http://bang.dangdang.com/books/newhotsales',\n        'result': {\n            'selectors': {\n                'list': 'div.bang_list_box > ul.bang_list.clearfix.bang_list_mode'\n            }\n        }\n    },\n    {\n        'url': 'https://cuiqingcai.com/archives/',\n        'result': {\n            'selectors': {\n                'list': 'div.post-block > div.posts-collapse'\n            }\n        }\n    },\n    {\n        'url': 'https://github.com/crawlab-team/crawlab/actions',\n        'result': {\n            'selectors': {\n                'list': '#partial-actions-workflow-runs'\n            }\n        }\n    },\n    {\n        'url': 'https://huggingface.co/datasets',\n        'result': {\n            'selectors': {\n                'list': 'div.grid.grid-cols-1.gap-5'\n            }\n        }\n    },\n    {\n        'url': 'https://www.36kr.com/information/web_news/latest/',\n        'result': {\n            'selectors': {\n                'list': 'div.kr-loading-more > div.information-flow-list'\n            }\n        }\n    },\n    # {\n    #     'url': 'https://cuiqingcai.com',\n    #     'result': {\n    #         'selectors': {\n    #             'list': '.content-wrap > div.content.index.posts-expand'\n    #         }\n    #     }\n    # },\n]\n\n\n# test_cases = [test_cases[-1]]\n\n\n@pytest.mark.parametrize('test_case', test_cases)\ndef test_plain_list(test_case):\n    url = test_case.get('url')\n    method = test_case.get('method') or 'request'\n    target_index = test_case.get('target_index') or 0\n\n    logger.info(f'url: {url}')\n    logger.info(f'method: {method}')\n\n    plain_list_detector = run_plain_list_detector(url, method)\n\n    results = plain_list_detector.results\n    assert len(results) > 0, 'results should be more than 0'\n\n    test_case_result = test_case.get('result')\n    test_case_list_selector = test_case_result.get('selectors').get('list')\n    target_result = results[target_index]\n    assert test_case_list_selector in target_result.selectors.get('list').selector, 'selectors should be matched'\n    assert len(target_result.fields) > 0, 'target fields should be more than 0'\n    assert len(target_result.data) > 0, 'target data should be more than 0'\n"}
{"type": "test_file", "path": "webspot/test/logging/test_get_logger.py", "content": "import unittest\nfrom io import StringIO\nfrom unittest.mock import patch\n\nfrom webspot.logging import get_logger\n\n\nclass TestGetLogger(unittest.TestCase):\n    # @patch('sys.stdout', new_callable=StringIO)\n    @patch('sys.stderr', new_callable=StringIO)\n    def test_get_logger(self, stderr):\n        logger = get_logger('web.utils.time')\n        text = 'it works'\n        logger.info(text)\n        # print(stdout.getvalue())\n        # a = stdout.getvalue()\n        # print(stderr.getvalue())\n        # b = stderr.getvalue()\n        self.assertTrue(text in stderr.getvalue())\n        # self.assertTrue(text in stdout.getvalue())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "webspot/test/web/routes/api/test_request.py", "content": "import itertools\nimport json\nfrom unittest.mock import patch, MagicMock\n\nimport pytest\n\nfrom webspot.constants.request_status import REQUEST_STATUS_SUCCESS, REQUEST_STATUS_PENDING\nfrom webspot.test.web.routes import client\n\nparams_url = [\n    'https://quotes.toscrape.com',\n    'https://books.toscrape.com',\n]\nparams_method = [\n    'request',\n    # 'rod',\n]\nparams_no_async = [True, False]\nparams_html = [\n    '''<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n\t<meta charset=\"UTF-8\">\n\t<title>Quotes to Scrape</title>\n    <link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\">\n    <link rel=\"stylesheet\" href=\"/static/main.css\">\n</head>\n<body>\n    <div class=\"container\">\n        <div class=\"row header-box\">\n            <div class=\"col-md-8\">\n                <h1>\n                    <a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\n                </h1>\n            </div>\n            <div class=\"col-md-4\">\n                <p>\n\n                    <a href=\"/login\">Login</a>\n\n                </p>\n            </div>\n        </div>\n\n\n<div class=\"row\">\n    <div class=\"col-md-8\">\n\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n        <span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>\n        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n        <a href=\"/author/Albert-Einstein\">(about)</a>\n        </span>\n        <div class=\"tags\">\n            Tags:\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"change,deep-thoughts,thinking,world\" /    >\n\n            <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n\n            <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n\n            <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n\n            <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n\n        </div>\n    </div>\n\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n        <span class=\"text\" itemprop=\"text\">“It is our choices, Harry, that show what we truly are, far more than our abilities.”</span>\n        <span>by <small class=\"author\" itemprop=\"author\">J.K. Rowling</small>\n        <a href=\"/author/J-K-Rowling\">(about)</a>\n        </span>\n        <div class=\"tags\">\n            Tags:\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"abilities,choices\" /    >\n\n            <a class=\"tag\" href=\"/tag/abilities/page/1/\">abilities</a>\n\n            <a class=\"tag\" href=\"/tag/choices/page/1/\">choices</a>\n\n        </div>\n    </div>\n\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n        <span class=\"text\" itemprop=\"text\">“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”</span>\n        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n        <a href=\"/author/Albert-Einstein\">(about)</a>\n        </span>\n        <div class=\"tags\">\n            Tags:\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"inspirational,life,live,miracle,miracles\" /    >\n\n            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\n\n            <a class=\"tag\" href=\"/tag/life/page/1/\">life</a>\n\n            <a class=\"tag\" href=\"/tag/live/page/1/\">live</a>\n\n            <a class=\"tag\" href=\"/tag/miracle/page/1/\">miracle</a>\n\n            <a class=\"tag\" href=\"/tag/miracles/page/1/\">miracles</a>\n\n        </div>\n    </div>\n\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n        <span class=\"text\" itemprop=\"text\">“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”</span>\n        <span>by <small class=\"author\" itemprop=\"author\">Jane Austen</small>\n        <a href=\"/author/Jane-Austen\">(about)</a>\n        </span>\n        <div class=\"tags\">\n            Tags:\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"aliteracy,books,classic,humor\" /    >\n\n            <a class=\"tag\" href=\"/tag/aliteracy/page/1/\">aliteracy</a>\n\n            <a class=\"tag\" href=\"/tag/books/page/1/\">books</a>\n\n            <a class=\"tag\" href=\"/tag/classic/page/1/\">classic</a>\n\n            <a class=\"tag\" href=\"/tag/humor/page/1/\">humor</a>\n\n        </div>\n    </div>\n\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n        <span class=\"text\" itemprop=\"text\">“Imperfection is beauty, madness is genius and it&#39;s better to be absolutely ridiculous than absolutely boring.”</span>\n        <span>by <small class=\"author\" itemprop=\"author\">Marilyn Monroe</small>\n        <a href=\"/author/Marilyn-Monroe\">(about)</a>\n        </span>\n        <div class=\"tags\">\n            Tags:\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"be-yourself,inspirational\" /    >\n\n            <a class=\"tag\" href=\"/tag/be-yourself/page/1/\">be-yourself</a>\n\n            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\n\n        </div>\n    </div>\n\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n        <span class=\"text\" itemprop=\"text\">“Try not to become a man of success. Rather become a man of value.”</span>\n        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n        <a href=\"/author/Albert-Einstein\">(about)</a>\n        </span>\n        <div class=\"tags\">\n            Tags:\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"adulthood,success,value\" /    >\n\n            <a class=\"tag\" href=\"/tag/adulthood/page/1/\">adulthood</a>\n\n            <a class=\"tag\" href=\"/tag/success/page/1/\">success</a>\n\n            <a class=\"tag\" href=\"/tag/value/page/1/\">value</a>\n\n        </div>\n    </div>\n\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n        <span class=\"text\" itemprop=\"text\">“It is better to be hated for what you are than to be loved for what you are not.”</span>\n        <span>by <small class=\"author\" itemprop=\"author\">André Gide</small>\n        <a href=\"/author/Andre-Gide\">(about)</a>\n        </span>\n        <div class=\"tags\">\n            Tags:\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"life,love\" /    >\n\n            <a class=\"tag\" href=\"/tag/life/page/1/\">life</a>\n\n            <a class=\"tag\" href=\"/tag/love/page/1/\">love</a>\n\n        </div>\n    </div>\n\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n        <span class=\"text\" itemprop=\"text\">“I have not failed. I&#39;ve just found 10,000 ways that won&#39;t work.”</span>\n        <span>by <small class=\"author\" itemprop=\"author\">Thomas A. Edison</small>\n        <a href=\"/author/Thomas-A-Edison\">(about)</a>\n        </span>\n        <div class=\"tags\">\n            Tags:\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"edison,failure,inspirational,paraphrased\" /    >\n\n            <a class=\"tag\" href=\"/tag/edison/page/1/\">edison</a>\n\n            <a class=\"tag\" href=\"/tag/failure/page/1/\">failure</a>\n\n            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\n\n            <a class=\"tag\" href=\"/tag/paraphrased/page/1/\">paraphrased</a>\n\n        </div>\n    </div>\n\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n        <span class=\"text\" itemprop=\"text\">“A woman is like a tea bag; you never know how strong it is until it&#39;s in hot water.”</span>\n        <span>by <small class=\"author\" itemprop=\"author\">Eleanor Roosevelt</small>\n        <a href=\"/author/Eleanor-Roosevelt\">(about)</a>\n        </span>\n        <div class=\"tags\">\n            Tags:\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"misattributed-eleanor-roosevelt\" /    >\n\n            <a class=\"tag\" href=\"/tag/misattributed-eleanor-roosevelt/page/1/\">misattributed-eleanor-roosevelt</a>\n\n        </div>\n    </div>\n\n    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n        <span class=\"text\" itemprop=\"text\">“A day without sunshine is like, you know, night.”</span>\n        <span>by <small class=\"author\" itemprop=\"author\">Steve Martin</small>\n        <a href=\"/author/Steve-Martin\">(about)</a>\n        </span>\n        <div class=\"tags\">\n            Tags:\n            <meta class=\"keywords\" itemprop=\"keywords\" content=\"humor,obvious,simile\" /    >\n\n            <a class=\"tag\" href=\"/tag/humor/page/1/\">humor</a>\n\n            <a class=\"tag\" href=\"/tag/obvious/page/1/\">obvious</a>\n\n            <a class=\"tag\" href=\"/tag/simile/page/1/\">simile</a>\n\n        </div>\n    </div>\n\n    <nav>\n        <ul class=\"pager\">\n\n\n            <li class=\"next\">\n                <a href=\"/page/2/\">Next <span aria-hidden=\"true\">&rarr;</span></a>\n            </li>\n\n        </ul>\n    </nav>\n    </div>\n    <div class=\"col-md-4 tags-box\">\n\n            <h2>Top Ten tags</h2>\n\n            <span class=\"tag-item\">\n            <a class=\"tag\" style=\"font-size: 28px\" href=\"/tag/love/\">love</a>\n            </span>\n\n            <span class=\"tag-item\">\n            <a class=\"tag\" style=\"font-size: 26px\" href=\"/tag/inspirational/\">inspirational</a>\n            </span>\n\n            <span class=\"tag-item\">\n            <a class=\"tag\" style=\"font-size: 26px\" href=\"/tag/life/\">life</a>\n            </span>\n\n            <span class=\"tag-item\">\n            <a class=\"tag\" style=\"font-size: 24px\" href=\"/tag/humor/\">humor</a>\n            </span>\n\n            <span class=\"tag-item\">\n            <a class=\"tag\" style=\"font-size: 22px\" href=\"/tag/books/\">books</a>\n            </span>\n\n            <span class=\"tag-item\">\n            <a class=\"tag\" style=\"font-size: 14px\" href=\"/tag/reading/\">reading</a>\n            </span>\n\n            <span class=\"tag-item\">\n            <a class=\"tag\" style=\"font-size: 10px\" href=\"/tag/friendship/\">friendship</a>\n            </span>\n\n            <span class=\"tag-item\">\n            <a class=\"tag\" style=\"font-size: 8px\" href=\"/tag/friends/\">friends</a>\n            </span>\n\n            <span class=\"tag-item\">\n            <a class=\"tag\" style=\"font-size: 8px\" href=\"/tag/truth/\">truth</a>\n            </span>\n\n            <span class=\"tag-item\">\n            <a class=\"tag\" style=\"font-size: 6px\" href=\"/tag/simile/\">simile</a>\n            </span>\n\n\n    </div>\n</div>\n\n    </div>\n    <footer class=\"footer\">\n        <div class=\"container\">\n            <p class=\"text-muted\">\n                Quotes by: <a href=\"https://www.goodreads.com/quotes\">GoodReads.com</a>\n            </p>\n            <p class=\"copyright\">\n                Made with <span class='sh-red'>❤</span> by <a href=\"https://scrapinghub.com\">Scrapinghub</a>\n            </p>\n        </div>\n    </footer>\n</body>\n</html>'''\n]\n\n\n@pytest.mark.parametrize('url, method, no_async', list(itertools.product(params_url, params_method, params_no_async)))\n@patch('webspot.models.request.Request')\ndef test_post(mocked_request_model, url, method, no_async):\n    # mock the model\n    mock_instance = MagicMock()\n    mock_instance.save.return_value = None\n    mocked_request_model.return_value = mock_instance\n\n    # post\n    res = client.post('/api/requests', json={'url': url, 'method': method, 'no_async': no_async})\n\n    # assert\n    assert res.status_code == 200\n    data = res.json()\n    assert data is not None\n    assert data.get('url') == url\n    assert data.get('method') == method\n    assert data.get('no_async') == no_async\n    if no_async:\n        assert data.get('status') == REQUEST_STATUS_PENDING\n    else:\n        assert data.get('status') == REQUEST_STATUS_SUCCESS\n\n\n@pytest.mark.parametrize('html', list(itertools.product(params_html)))\n@patch('webspot.models.request.Request')\ndef test_post_html(mocked_request_model, html):\n    # mock the model\n    mock_instance = MagicMock()\n    mock_instance.save.return_value = None\n    mocked_request_model.return_value = mock_instance\n\n    # post\n    res = client.post('/api/requests', json={'html': json.dumps(html)})\n\n    # assert\n    assert res.status_code == 200\n    data = res.json()\n    assert data is not None\n"}
{"type": "source_file", "path": "webspot/cmd/request.py", "content": "from webspot.request.html_requester import HtmlRequester\n\n\ndef cmd_request(args):\n    html_requester = HtmlRequester(url=args.url, request_method=args.method)\n    html_requester.run()\n    print(html_requester.html)\n"}
{"type": "source_file", "path": "webspot/__init__.py", "content": ""}
{"type": "source_file", "path": "webspot/crawler/crawler/pipelines.py", "content": "# Define your item pipelines here\n#\n# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n\n\n# useful for handling different item types with a single interface\nfrom itemadapter import ItemAdapter\n\n\nclass CrawlerPipeline:\n    def process_item(self, item, spider):\n        return item\n"}
{"type": "source_file", "path": "webspot/constants/field_extract_rule_type.py", "content": "FIELD_EXTRACT_RULE_TYPE_TEXT = 'text'\nFIELD_EXTRACT_RULE_TYPE_LINK_URL = 'link_url'\nFIELD_EXTRACT_RULE_TYPE_IMAGE_URL = 'image_url'\n"}
{"type": "source_file", "path": "webspot/cmd/crawl.py", "content": "from urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom pandas import DataFrame\n\nfrom webspot.constants.detector import DETECTOR_PLAIN_LIST, DETECTOR_PAGINATION\nfrom webspot.extract.extract_results import extract_rules\n\n\ndef cmd_crawl(args):\n    url = args.url\n\n    results, execution_time, html_requester, graph_loader, detectors = extract_rules(\n        url=url,\n    )\n\n    # plain list\n    result_plain_list = results.get(DETECTOR_PLAIN_LIST)\n    if result_plain_list is None or len(result_plain_list) == 0:\n        return\n\n    # items\n    full_items = result_plain_list[0].get('selectors').get('full_items')\n    items_selector = full_items.get('selector')\n    # print(items_selector)\n\n    # fields\n    fields = result_plain_list[0].get('fields')\n    # print(fields)\n\n    # pagination\n    result_pagination = results.get(DETECTOR_PAGINATION)\n    pagination_selector = None\n    if result_pagination is not None and len(result_pagination) > 0:\n        pagination_selector = result_pagination[0].get('selectors').get('next').get('selector')\n    # print(pagination_selector)\n\n    res = crawl_page(url, items_selector, fields, pagination_selector)\n    print(DataFrame(list(res)))\n\n\ndef crawl_page(url, items_selector, fields, pagination_selector):\n    print(f'requesting {url}')\n    res = requests.get(url)\n    soup = BeautifulSoup(res.content, features='lxml')\n\n    for el_item in soup.select(items_selector):\n        row = {}\n        for f in fields:\n            try:\n                if f.get('attribute'):\n                    row[f.get('name')] = el_item.select_one(f.get('selector')).attrs.get(f.get('attribute'))\n                else:\n                    row[f.get('name')] = el_item.select_one(f.get('selector')).text.strip()\n            except:\n                pass\n        yield row\n\n    if pagination_selector is not None:\n        try:\n            href = soup.select_one(pagination_selector).attrs.get('href')\n            next_url = urljoin(url, href)\n\n            for row in crawl_page(next_url, items_selector, fields, pagination_selector):\n                yield row\n        except:\n            pass\n"}
{"type": "source_file", "path": "webspot/crawler/crawler/middlewares.py", "content": "# Define here the models for your spider middleware\n#\n# See documentation in:\n# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nfrom scrapy import signals\n\n# useful for handling different item types with a single interface\nfrom itemadapter import is_item, ItemAdapter\n\n\nclass CrawlerSpiderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the spider middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_spider_input(self, response, spider):\n        # Called for each response that goes through the spider\n        # middleware and into the spider.\n\n        # Should return None or raise an exception.\n        return None\n\n    def process_spider_output(self, response, result, spider):\n        # Called with the results returned from the Spider, after\n        # it has processed the response.\n\n        # Must return an iterable of Request, or item objects.\n        for i in result:\n            yield i\n\n    def process_spider_exception(self, response, exception, spider):\n        # Called when a spider or process_spider_input() method\n        # (from other spider middleware) raises an exception.\n\n        # Should return either None or an iterable of Request or item objects.\n        pass\n\n    def process_start_requests(self, start_requests, spider):\n        # Called with the start requests of the spider, and works\n        # similarly to the process_spider_output() method, except\n        # that it doesn’t have a response associated.\n\n        # Must return only requests (not items).\n        for r in start_requests:\n            yield r\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n\n\nclass CrawlerDownloaderMiddleware:\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the downloader middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_request(self, request, spider):\n        # Called for each request that goes through the downloader\n        # middleware.\n\n        # Must either:\n        # - return None: continue processing this request\n        # - or return a Response object\n        # - or return a Request object\n        # - or raise IgnoreRequest: process_exception() methods of\n        #   installed downloader middleware will be called\n        return None\n\n    def process_response(self, request, response, spider):\n        # Called with the response returned from the downloader.\n\n        # Must either;\n        # - return a Response object\n        # - return a Request object\n        # - or raise IgnoreRequest\n        return response\n\n    def process_exception(self, request, exception, spider):\n        # Called when a download handler or a process_request()\n        # (from other downloader middleware) raises an exception.\n\n        # Must either:\n        # - return None: continue processing this exception\n        # - return a Response object: stops process_exception() chain\n        # - return a Request object: stops process_exception() chain\n        pass\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n"}
{"type": "source_file", "path": "webspot/cmd/web.py", "content": "import uvicorn\n\n\ndef cmd_web(args):\n    uvicorn.run('webspot.web.app:app', reload=True, host=args.host, port=args.port, log_level=args.log_level)\n"}
{"type": "source_file", "path": "webspot/constants/__init__.py", "content": ""}
{"type": "source_file", "path": "webspot/constants/detector.py", "content": "DETECTOR_PLAIN_LIST = 'plain_list'\nDETECTOR_PAGINATION = 'pagination'\n\nALL_DETECTORS = [\n    DETECTOR_PLAIN_LIST,\n    DETECTOR_PAGINATION,\n]\n"}
{"type": "source_file", "path": "main.py", "content": "from argparse import ArgumentParser\n\nfrom dotenv import load_dotenv\n\nfrom webspot.cmd.crawl import cmd_crawl\nfrom webspot.cmd.request import cmd_request\nfrom webspot.cmd.web import cmd_web\nfrom webspot.constants.html_request_method import HTML_REQUEST_METHOD_REQUEST\n\nload_dotenv()\n\nparser = ArgumentParser()\n\nsubparsers = parser.add_subparsers(\n    title='subcommands',\n    description='valid subcommands',\n    help='additional help',\n)\n\ncrawl_parser = subparsers.add_parser('crawl')\ncrawl_parser.add_argument('--url', '-U', help='url to crawl')\ncrawl_parser.add_argument('--output', '-o', help='output file path')\ncrawl_parser.set_defaults(func=cmd_crawl)\n\nweb_parser = subparsers.add_parser('web')\nweb_parser.add_argument('--host', '-H', default='0.0.0.0', help='port')\nweb_parser.add_argument('--port', '-P', default=80, type=int, help='port')\nweb_parser.add_argument('--log-level', '-L', default='debug', help='log level')\nweb_parser.set_defaults(func=cmd_web)\n\nrequest_parser = subparsers.add_parser('request')\nrequest_parser.add_argument('--url', '-U', help='url to request', required=True)\nrequest_parser.add_argument('--method', '-M', help='request method', default=HTML_REQUEST_METHOD_REQUEST)\nrequest_parser.set_defaults(func=cmd_request)\n\nif __name__ == '__main__':\n    args = parser.parse_args()\n    args.func(args)\n"}
{"type": "source_file", "path": "webspot/constants/request_status.py", "content": "REQUEST_STATUS_PENDING = 'pending'\nREQUEST_STATUS_SUCCESS = 'success'\nREQUEST_STATUS_ERROR = 'error'\n"}
{"type": "source_file", "path": "webspot/db/__init__.py", "content": ""}
{"type": "source_file", "path": "webspot/detect/models/list_result.py", "content": "from typing import List\n\nfrom webspot.detect.models.result import Result\nfrom webspot.detect.models.selector import Selector\n\n\nclass ListResult(Result):\n    fields: List[Selector]\n    data: List[dict]\n"}
{"type": "source_file", "path": "webspot/detect/detectors/plain_table.py", "content": "from bs4 import BeautifulSoup\nimport numpy as np\n\nfrom webspot.detect.detectors.base import BaseDetector\n\n\nclass PlainTableDetector(BaseDetector):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # data\n        self._table_nodes = None\n\n    def highlight_html(self, html: str, **kwargs) -> str:\n        pass\n\n    @property\n    def table_nodes(self):\n        return self.get_nodes_by_feature(feature_key='tag', feature_value='table')\n\n    def _pre_process(self):\n        self._table_nodes = self.table_nodes\n\n    def _train(self):\n        pass\n\n    def run(self):\n        self._pre_process()\n\n        self._train()\n"}
{"type": "source_file", "path": "webspot/crawler/crawler/__init__.py", "content": ""}
{"type": "source_file", "path": "webspot/cmd/__init__.py", "content": ""}
{"type": "source_file", "path": "setup.py", "content": "from setuptools import setup, find_packages\n\nwith open('README.md', 'r') as f:\n    long_description = f.read()\n\nwith open('requirements.txt', 'r') as f:\n    install_requires = f.read().split('\\n')\n\nsetup(\n    name='webspot',\n    version='0.1.4',\n    packages=find_packages('.'),\n    url='https://github.com/tikazyq/webspot',\n    license='BSD-3-Clause',\n    author='tikazyq',\n    author_email='tikazyq@163.com',\n    description='An intelligent web service to automatically detect web content and extract information from it.',\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    scripts=[],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: BSD License\",\n        \"Operating System :: OS Independent\",\n    ],\n    install_requires=install_requires,\n    entry_points={\n        'console_scripts': [\n            'crawlab-cli=crawlab.cli.main:main'\n        ]\n    }\n)\n"}
{"type": "source_file", "path": "webspot/crawler/crawler/items.py", "content": "# Define here the models for your scraped items\n#\n# See documentation in:\n# https://docs.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass CrawlerItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    pass\n"}
{"type": "source_file", "path": "webspot/crawler/actions/__init__.py", "content": ""}
{"type": "source_file", "path": "webspot/crawler/crawler/settings.py", "content": "# Scrapy settings for crawler project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://docs.scrapy.org/en/latest/topics/settings.html\n#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = 'crawler'\n\nSPIDER_MODULES = ['crawler.spiders']\nNEWSPIDER_MODULE = 'crawler.spiders'\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\nUSER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = True\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\nCONCURRENT_REQUESTS = 2\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n# DOWNLOAD_DELAY = 3\n# The download delay setting will honor only one of:\n# CONCURRENT_REQUESTS_PER_DOMAIN = 16\n# CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n# COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n# TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n# DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n# }\n\n# Enable or disable spider middlewares\n# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n# SPIDER_MIDDLEWARES = {\n#    'crawler.middlewares.CrawlerSpiderMiddleware': 543,\n# }\n\n# Enable or disable downloader middlewares\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n# DOWNLOADER_MIDDLEWARES = {\n#    'crawler.middlewares.CrawlerDownloaderMiddleware': 543,\n# }\n\n# Enable or disable extensions\n# See https://docs.scrapy.org/en/latest/topics/extensions.html\n# EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n# }\n\n# Configure item pipelines\n# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n# ITEM_PIPELINES = {\n#    'crawler.pipelines.CrawlerPipeline': 300,\n# }\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n# AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n# AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n# AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n# AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n# AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n# HTTPCACHE_ENABLED = True\n# HTTPCACHE_EXPIRATION_SECS = 0\n# HTTPCACHE_DIR = 'httpcache'\n# HTTPCACHE_IGNORE_HTTP_CODES = []\n# HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n\n# Set settings whose default value is deprecated to a future-proof value\nREQUEST_FINGERPRINTER_IMPLEMENTATION = '2.7'\nTWISTED_REACTOR = 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'\n"}
{"type": "source_file", "path": "webspot/crawler/crawler/spiders/__init__.py", "content": "# This package will contain the spiders of your Scrapy project\n#\n# Please refer to the documentation for information on how to create and manage\n# your spiders.\n"}
{"type": "source_file", "path": "migrations/env.py", "content": "from logging.config import fileConfig\n\nfrom sqlalchemy import engine_from_config\nfrom sqlalchemy import pool\n\nfrom alembic import context\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = None\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection, target_metadata=target_metadata\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n"}
{"type": "source_file", "path": "webspot/constants/html_request_method.py", "content": "HTML_REQUEST_METHOD_ROD = 'rod'\nHTML_REQUEST_METHOD_REQUEST = 'request'\n"}
{"type": "source_file", "path": "webspot/crawler/__init__.py", "content": ""}
{"type": "source_file", "path": "webspot/detect/models/stats.py", "content": "class Stats(dict):\n    @property\n    def entropy(self) -> float:\n        return self.get('entropy')\n\n    @property\n    def score(self) -> float:\n        return self.get('score')\n"}
{"type": "source_file", "path": "webspot/db/connect.py", "content": "import os\n\nfrom mongoengine import connect as mongo_connect\n\nfrom webspot.utils.test import is_running_unit_tests\n\nDEFAULT_DATABASE_URL = 'mongodb://localhost:27017/webspot'\n\n\ndef connect():\n    if is_running_unit_tests():\n        mongo_connect(\"mockdb\", host=\"mongomock://localhost\")\n        return\n\n    conn_str = get_connection_str()\n    mongo_connect(host=conn_str)\n\n\ndef get_connection_str():\n    return os.environ.get('WEBSPOT_DATABASE_URL', DEFAULT_DATABASE_URL)\n"}
{"type": "source_file", "path": "webspot/crawler/crawler/spiders/web_spider.py", "content": "import json\nimport os.path\n\nimport html_to_json_enhanced\nimport scrapy\nfrom scrapy.crawler import logger\nfrom scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n\n\nclass WebSpider(scrapy.Spider):\n    name = 'web'\n\n    allowed_domains = []\n\n    link_extractor = None\n\n    @property\n    def domain(self) -> str:\n        return self.settings.get('domain')\n\n    @property\n    def urls(self) -> list:\n        return self.settings.get('urls') or []\n\n    @property\n    def url_paths(self) -> list:\n        return self.settings.get('urls_paths') or []\n\n    @property\n    def data_root_dir(self) -> str:\n        return self.settings.get('data_root_dir') or os.path.join(os.path.dirname(__file__), '..', '..', '..', 'data')\n\n    @property\n    def data_dir(self) -> str:\n        return os.path.join(self.data_root_dir, self.domain)\n\n    @property\n    def data_html_dir(self) -> str:\n        return os.path.join(self.data_dir, 'html')\n\n    @property\n    def data_json_dir(self) -> str:\n        return os.path.join(self.data_dir, 'json')\n\n    @property\n    def is_test(self) -> bool:\n        return self.settings.get('is_test') or False\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def start_requests(self):\n        # log settings\n        logger.info(f'settings.domain: {self.domain}')\n        logger.info(f'settings.urls: {self.urls}')\n        logger.info(f'settings.url_paths: {self.url_paths}')\n\n        # set allowed domains\n        self.allowed_domains = [self.domain]\n\n        # link extractor\n        self.link_extractor = LxmlLinkExtractor(\n            allow=self.url_paths,\n            allow_domains=self.allowed_domains,\n        )\n\n        for url in self.urls:\n            if self.is_test and 'example' in url:\n                continue\n\n            yield scrapy.Request(url=url)\n\n    def parse(self, response: scrapy.http.Response, **kwargs):\n        # create data html directory if not exists\n        if not os.path.exists(self.data_html_dir):\n            os.makedirs(self.data_html_dir)\n\n        # create data json directory if not exists\n        if not os.path.exists(self.data_json_dir):\n            os.makedirs(self.data_json_dir)\n\n        # file name prefix\n        filename_prefix = response.url.replace('/', '_').replace(':', '_').replace('.', '_')\n\n        # save response html\n        filename = os.path.join(self.data_html_dir, f'{filename_prefix}.html')\n        with open(filename, 'wb') as f:\n            f.write(response.body)\n\n        # save response json\n        filename = os.path.join(self.data_json_dir, f'{filename_prefix}.json')\n        with open(filename, 'w') as f:\n            json_data = html_to_json_enhanced.convert(response.body, with_id=True)\n            f.write(json.dumps(json_data, indent=2))\n\n        # follow links\n        for link in self.link_extractor.extract_links(response):\n            yield scrapy.Request(link.url, callback=self.parse)\n"}
{"type": "source_file", "path": "webspot/detect/detectors/base.py", "content": "from abc import abstractmethod\nfrom typing import List\n\nimport numpy as np\n\nfrom webspot.detect.models.result import Result\nfrom webspot.graph.graph_loader import GraphLoader\nfrom webspot.request.html_requester import HtmlRequester\n\n\nclass BaseDetector(object):\n    def __init__(\n        self,\n        graph_loader: GraphLoader,\n        html_requester: HtmlRequester,\n    ):\n        # graph loader\n        self.graph_loader = graph_loader\n\n        # html requester\n        self.html_requester = html_requester\n\n        # results\n        self.results: List[Result] = []\n\n    def get_nodes_idx_by_feature(self, feature_key: str, feature_value: str):\n        feature = f'{feature_key}={feature_value}'\n        feature_tag_a_idx = np.argwhere(np.array(self.graph_loader.nodes_features_enc.feature_names_) == feature)[0]\n        return np.argwhere(self.graph_loader.nodes_features_tensor[:, feature_tag_a_idx].T[0] == 1)[0].detach().numpy()\n\n    def get_nodes_by_feature(self, feature_key: str, feature_value: str):\n        nodes_idx = self.get_nodes_idx_by_feature(feature_key, feature_value)\n        return [n for n in self.graph_loader.nodes[nodes_idx]]\n\n    @abstractmethod\n    def highlight_html(self, html: str, **kwargs) -> str:\n        pass\n\n    @abstractmethod\n    def run(self):\n        pass\n"}
{"type": "source_file", "path": "webspot/detect/detectors/plain_list.py", "content": "import base64\nimport json\nimport math\nimport time\nfrom collections import defaultdict\nfrom typing import List, Set, Dict\nfrom urllib.parse import urljoin\n\nimport numpy as np\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom scipy.sparse import csr_matrix\nfrom scipy.stats import entropy\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import normalize\n\nfrom webspot.constants.detector import DETECTOR_PLAIN_LIST\nfrom webspot.constants.field_extract_rule_type import FIELD_EXTRACT_RULE_TYPE_TEXT, FIELD_EXTRACT_RULE_TYPE_LINK_URL, \\\n    FIELD_EXTRACT_RULE_TYPE_IMAGE_URL\nfrom webspot.detect.detectors.base import BaseDetector\nfrom webspot.detect.models.selector import Selector\nfrom webspot.detect.utils.math import log_positive\nfrom webspot.detect.utils.highlight_html import add_class, add_label\nfrom webspot.detect.models.list_result import ListResult\nfrom webspot.graph.graph_loader import GraphLoader\nfrom webspot.graph.models.node import Node\nfrom webspot.logging import get_logger\nfrom webspot.request.html_requester import HtmlRequester\n\nlogger = get_logger('webspot.detect.detectors.plain_list')\n\n\nclass PlainListDetector(BaseDetector):\n    def __init__(\n        self,\n        dbscan_eps: float = 0.01,\n        dbscan_min_samples: int = 5,\n        dbscan_metric: str = 'euclidean',\n        dbscan_n_jobs: int = -1,\n        pca_n_components: int = 50,\n        entropy_threshold: float = 1e-3,\n        score_threshold: float = 1.,\n        sample_item_nodes: int = 10,\n        min_item_nodes: int = 5,\n        node2vec_ratio: float = 10.,\n        result_name_prefix: str = 'List',\n        text_length_discount: float = 0.1,\n        max_text_length: float = 2048,\n        max_item_count: int = 10,\n        min_item_nodes_ratio: float = 0.5,\n        max_feature_count: int = 10,\n        max_result_count: int = 10,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n\n        # settings\n        self.pca_n_components = pca_n_components\n        self.entropy_threshold = entropy_threshold\n        self.score_threshold = score_threshold\n        self.sample_item_nodes = sample_item_nodes\n        self.min_item_nodes = min_item_nodes\n        self.node2vec_ratio = node2vec_ratio\n        self.result_name_prefix = result_name_prefix\n        self.text_length_discount = text_length_discount\n        self.max_text_length = max_text_length\n        self.max_item_count = max_item_count\n        self.min_item_nodes_ratio = min_item_nodes_ratio\n        self.max_feature_count = max_feature_count\n        self.max_result_count = max_result_count\n\n        # dbscan model\n        self.dbscan = DBSCAN(\n            metric=dbscan_metric,\n            eps=dbscan_eps,\n            min_samples=dbscan_min_samples,\n            n_jobs=dbscan_n_jobs,\n        )\n\n        # data\n        self.results: List[ListResult] = []\n\n    @property\n    def _html(self):\n        return self.graph_loader.html\n\n    @property\n    def url(self):\n        return self.html_requester.url\n\n    def highlight_html(self, html: str, **kwargs) -> str:\n        soup = BeautifulSoup(html, 'html.parser')\n        for i, result in enumerate(self.results):\n            # list\n            list_selector = result.selectors.get('list')\n            list_el = soup.select_one(list_selector.selector)\n            if not list_el:\n                continue\n            add_class(list_el, ['webspot-highlight-container', 'webspot-highlight-node-color__blue'])\n            add_label(list_el, soup, f'List {i + 1}', 'primary')\n\n            # items\n            items_selector = result.selectors.get('items')\n            item_els = list_el.select(items_selector.selector)\n            for j, item_el in enumerate(item_els):\n                add_class(item_el, ['webspot-highlight-container', 'webspot-highlight-node-color__orange'])\n                # _add_label(item_el, soup, f'Item {j + 1}', 'warning')\n\n                # fields\n                for k, field in enumerate(result.fields):\n                    try:\n                        field_els = item_el.select(field.selector)\n                    except Exception as e:\n                        # logging.warning(e)\n                        field_els = []\n                    for field_el in field_els:\n                        add_class(field_el, ['webspot-highlight-container', 'webspot-highlight-node-color__green'])\n                        # _add_label(field_el, soup, f'Field {k + 1}', 'success')\n        return str(soup)\n\n    @property\n    def html(self):\n        return self.highlight_html(self._html, )\n\n    @property\n    def html_base64(self):\n        return base64.b64encode(self.html.encode('utf-8')).decode('utf-8')\n\n    @property\n    def results_base64(self):\n        return base64.b64encode(json.dumps(self.results).encode('utf-8')).decode('utf-8')\n\n    @property\n    def pruned_nodes_features(self):\n        features = self.graph_loader.nodes_features_tensor.detach().numpy()\n        features_count = features.sum(axis=0)\n        features_idx = np.argwhere(features_count > 1).T[0]\n        return features[:, features_idx]\n\n    def _get_nodes_features_tags_attrs(self, nodes_idx: np.ndarray = None):\n        \"\"\"\n        nodes features (tags + attributes)\n        \"\"\"\n        if nodes_idx is None:\n            features = self.pruned_nodes_features\n        else:\n            features = self.pruned_nodes_features[nodes_idx].sum(axis=1)\n\n        return normalize(\n            features,\n            norm='l1',\n            axis=1,\n        )\n\n    def _get_nodes_features_node2vec(self, nodes_idx: np.ndarray = None):\n        \"\"\"\n        nodes features (node2vec)\n        \"\"\"\n        if nodes_idx is None:\n            embedded_tensor = self.graph_loader.nodes_embedded_tensor\n        else:\n            embedded_tensor = self.graph_loader.nodes_embedded_tensor[nodes_idx.T[0]]\n\n        embedded_features = self.pruned_nodes_features[embedded_tensor].sum(axis=1)\n\n        return normalize(\n            embedded_features,\n            norm='l1',\n            axis=1,\n        )\n\n    def _get_nodes_features(self, nodes_idx: np.ndarray = None, to_sparse: bool = False):\n        \"\"\"\n        nodes features (tags + attributes + node2vec)\n        \"\"\"\n        x1 = self._get_nodes_features_tags_attrs(nodes_idx)\n        x2 = self._get_nodes_features_node2vec(nodes_idx) * self.node2vec_ratio\n        x = normalize(\n            np.concatenate(\n                (x1, x2),\n                axis=1,\n            ),\n            norm='l2',\n            axis=1,\n        )\n        logger.debug(f'nodes features size: {x.shape}')\n\n        if x.shape[1] > self.pca_n_components:\n            x = PCA(\n                n_components=self.pca_n_components,\n                svd_solver='randomized',\n            ).fit_transform(x)\n\n        if to_sparse:\n            return csr_matrix(x)\n        else:\n            return x\n\n    def _extract_fields_by_id(self, list_id: int, item_nodes: List[Node]) -> List[Selector]:\n        # fields\n        fields: List[Selector] = []\n\n        # extract rules dict (css selector path, type, attribute)\n        fields_extract_rules_dict: Dict[(str, str), int] = defaultdict(int)\n\n        # list child nodes\n        list_child_nodes = self.graph_loader.get_node_children_by_id(list_id)\n\n        # iterate list child nodes\n        i = 0\n        for c in np.random.choice(list_child_nodes, self.sample_item_nodes):\n            if c.tag != item_nodes[0].tag:\n                continue\n\n            # stop if item nodes samples reached\n            if i == self.min_item_nodes:\n                break\n\n            # item child nodes\n            item_child_nodes = self.graph_loader.get_node_children_recursive_by_id(c.id)\n\n            # iterate item child nodes\n            for n in item_child_nodes:\n                # extract text\n                if n.text is not None and len(n.text.strip()) > 0:\n                    extract_rule_css = self.graph_loader.get_node_css_selector_path(node=n, root_id=list_id,\n                                                                                    numbered=False, no_id=True)\n                    fields_extract_rules_dict[(extract_rule_css, FIELD_EXTRACT_RULE_TYPE_TEXT, '')] += 1\n\n                # extract link\n                if n.tag == 'a' and n.attrs.get('href') is not None and len(n.attrs.get('href').strip()) > 0:\n                    extract_rule_css = self.graph_loader.get_node_css_selector_path(node=n, root_id=list_id,\n                                                                                    numbered=False, no_id=True)\n                    fields_extract_rules_dict[(extract_rule_css, FIELD_EXTRACT_RULE_TYPE_LINK_URL, 'href')] += 1\n\n                # extract image url\n                if n.tag == 'img' and n.attrs.get('src') is not None and len(n.attrs.get('src').strip()) > 0:\n                    extract_rule_css = self.graph_loader.get_node_css_selector_path(node=n, root_id=list_id,\n                                                                                    numbered=False, no_id=True)\n                    fields_extract_rules_dict[(extract_rule_css, FIELD_EXTRACT_RULE_TYPE_IMAGE_URL, 'src')] += 1\n\n            i += 1\n\n        for i, (item, count) in enumerate(fields_extract_rules_dict.items()):\n            if float(count) / self.min_item_nodes < self.min_item_nodes_ratio:\n                continue\n            extract_rule_css, type_, attribute = item\n            name = f'Field_{type_}_{i + 1}'\n            fields.append(Selector(\n                name=name,\n                selector=extract_rule_css,\n                type=type_,\n                attribute=attribute,\n            ))\n        return fields\n\n    def _extract_data(self, soup: BeautifulSoup, items_selector_full: str, fields: List[Selector]):\n        data = []\n        for item_el in soup.select(items_selector_full):\n            row = {}\n            for f in fields:\n                try:\n                    field_el = item_el.select_one(f.selector)\n                    if f.type == FIELD_EXTRACT_RULE_TYPE_TEXT:\n                        row[f.name] = field_el.text.strip()\n                    elif f.type == FIELD_EXTRACT_RULE_TYPE_LINK_URL:\n                        row[f.name] = urljoin(self.url, field_el.attrs.get(f.attribute))\n                    elif f.type == FIELD_EXTRACT_RULE_TYPE_IMAGE_URL:\n                        row[f.name] = urljoin(self.url, field_el.attrs.get(f.attribute))\n                    else:\n                        continue\n                except Exception as e:\n                    # logging.warning(e)\n                    continue\n            data.append(row)\n        return data\n\n    def _train(self):\n        self.dbscan.fit(self._get_nodes_features(to_sparse=True))\n\n    def _pre_filter(self) -> (List[Node], List[List[Node]]):\n        df_nodes = pd.DataFrame({\n            'id': [n.id for n in self.graph_loader.nodes_],\n            'parent_id': [n.parent_id for n in self.graph_loader.nodes_],\n            'label': self.dbscan.labels_,\n        })\n        df_nodes_filtered = df_nodes[df_nodes.label != -1]\n        logger.debug('nodes before pre-filter: %s' % df_nodes.shape[0])\n        logger.debug('nodes after pre-filter: %s' % df_nodes_filtered.shape[0])\n\n        df_labels = df_nodes_filtered \\\n            .groupby('label')[['parent_id']] \\\n            .agg(lambda s: entropy(s.value_counts())) \\\n            .rename(columns={'parent_id': 'entropy'}) \\\n            .sort_values(by='entropy', ascending=True)\n\n        # threshold_mask = df_labels.entropy < self.entropy_threshold\n        # df_labels_filtered = df_labels[threshold_mask]\n\n        item_nodes_list = []\n        list_node_list = []\n        for label in df_labels.index:\n            # item nodes (filtered by label only)\n            df_nodes_filtered_by_label = df_nodes_filtered[df_nodes_filtered.label == label]\n\n            # iterate parent id\n            for parent_id in df_nodes_filtered_by_label.parent_id.unique().tolist():\n                # item nodes (further filtered parent id)\n                df_nodes_filtered_by_label_parent_id = df_nodes_filtered_by_label[\n                    df_nodes_filtered.parent_id == parent_id]\n\n                # skip if item nodes count is less than required\n                if df_nodes_filtered_by_label_parent_id.shape[0] < self.min_item_nodes:\n                    continue\n\n                # item node ids\n                item_nodes_ids = df_nodes_filtered_by_label_parent_id.id.values\n\n                # item nodes\n                item_nodes = self.graph_loader.get_nodes_by_ids(item_nodes_ids)\n                item_nodes_list.append(item_nodes)\n\n                # list node\n                list_node = self.graph_loader.get_node_by_id(item_nodes[0].parent_id)\n                list_node_list.append(list_node)\n\n        return list_node_list, item_nodes_list\n\n    def _filter(\n        self,\n        list_node_list: List[Node],\n        item_nodes_list: List[List[Node]],\n    ) -> (List[Node], List[List[Node]], List[float], List[Dict[str, float]]):\n        score_list = []\n        scores_list = []\n        idx: List[int] = []\n        for i, list_node, item_nodes in zip(range(len(list_node_list)), list_node_list, item_nodes_list):\n            nodes_ids = np.random.choice(np.array([n.id for n in item_nodes]), size=10)\n            try:\n                score_text_richness = 0\n                score_complexity = 0\n                for node_id in nodes_ids:\n                    child_nodes_idx = self.graph_loader.get_node_children_idx_recursive_by_id(node_id)\n                    if len(child_nodes_idx) == 0:\n                        continue\n                    nodes_text_length_vec = self.graph_loader.nodes_text_length_vec[child_nodes_idx]\n                    nodes_text_length_vec_non_zero = nodes_text_length_vec[nodes_text_length_vec > 0]\n                    _score_text_richness = log_positive(\n                        min(nodes_text_length_vec_non_zero.sum(), self.max_text_length) * self.text_length_discount)\n                    _score_complexity = log_positive(min(len(nodes_text_length_vec_non_zero), self.max_feature_count))\n                    if _score_text_richness > score_text_richness:\n                        score_text_richness = _score_text_richness\n                    if _score_complexity > score_complexity:\n                        score_complexity = _score_complexity\n                score_item_count = log_positive(min(len(item_nodes), self.max_item_count))\n\n                logger.debug(f'score_text_richness: {score_text_richness}')\n                logger.debug(f'score_complexity: {score_complexity}')\n                logger.debug(f'score_item_count: {score_item_count}')\n\n                # score\n                # score = score_text_richness + score_complexity + score_item_count\n                score = score_text_richness + score_item_count\n                logger.debug(f'score: {score}')\n\n                # skip score less than threshold\n                if score < self.score_threshold:\n                    continue\n\n                # skip zero sub-score\n                if score_text_richness == 0 or score_complexity == 0 or score_item_count == 0:\n                    continue\n\n                # add to scores list\n                scores_list.append({\n                    'text_richness': score_text_richness,\n                    'complexity': score_complexity,\n                    'item_count': score_item_count,\n                })\n\n                # add to score list\n                score_list.append(score)\n\n                # add to idx\n                idx.append(i)\n\n            except ValueError as e:\n                # logging.warning(f'ValueError: {e}')\n                pass\n\n        res_list_node_list = [list_node_list[i] for i in idx]\n        res_item_nodes_list = [item_nodes_list[i] for i in idx]\n\n        return res_list_node_list, res_item_nodes_list, score_list, scores_list\n\n    def _extract(\n        self,\n        list_node_list: List[Node],\n        item_nodes_list: List[List[Node]],\n        score_list: List[float],\n        scores_list: List[Dict[str, float]],\n    ) -> List[ListResult]:\n        # soup\n        soup = BeautifulSoup(self._html, features='lxml')\n\n        # results\n        results = []\n\n        # iterate inputs\n        for i, list_node, item_nodes, score, scores in zip(range(len(list_node_list)),\n                                                           list_node_list,\n                                                           item_nodes_list,\n                                                           score_list,\n                                                           scores_list):\n            # break if max result count reached\n            if i == self.max_result_count:\n                break\n\n            # list node\n            list_node = list_node\n            if not list_node:\n                continue\n\n            # item nodes\n            item_nodes = item_nodes\n\n            # list selector\n            list_selector = Selector(\n                name='list',\n                selector=self.graph_loader.get_node_css_selector_path(list_node),\n                type='css',\n                node_id=list_node.id,\n            )\n\n            # items node extract rule (css)\n            items_selector = Selector(\n                name='items',\n                selector=self.graph_loader.get_node_css_selector_repr(item_nodes[0], False, True),\n                type='css',\n            )\n\n            # items node full extract rule (css)\n            full_items_selector = Selector(\n                name='items',\n                selector=f'{list_selector.selector} > {items_selector.selector}',\n                type='css',\n            )\n\n            # fields node extract rule\n            fields_selectors = self._extract_fields_by_id(list_id=list_node.id, item_nodes=item_nodes)\n            if len(fields_selectors) == 0:\n                continue\n\n            # data\n            data = self._extract_data(soup, full_items_selector.selector, fields_selectors)\n            if len(data) == 0:\n                continue\n\n            # result\n            result = ListResult(\n                selectors={\n                    'list': list_selector,\n                    'items': items_selector,\n                    'full_items': full_items_selector,\n                },\n                score=score,\n                scores=scores,\n                fields=fields_selectors,\n                data=data,\n                detector=DETECTOR_PLAIN_LIST,\n            )\n            results.append(result)\n\n        return results\n\n    def _post_extract_filter(self, results: List[ListResult]) -> List[ListResult]:\n        for i, result in enumerate(results):\n            # prune if item nodes is fewer than threshold\n            if len(result.data) < self.min_item_nodes:\n                del results[i]\n                i -= 1\n\n        return results\n\n    def _sort(self, results: List[ListResult]) -> List[ListResult]:\n        results = sorted(results, key=lambda x: x.score, reverse=True)\n\n        # assign name\n        for i, result in enumerate(results):\n            results[i].name = f'{self.result_name_prefix} {i + 1}'\n\n        return results\n\n    def run(self):\n        tic = time.time()\n\n        # train clustering model\n        self._train()\n\n        # pre-filter nodes\n        res = self._pre_filter()\n\n        # filter nodes\n        res = self._filter(*res)\n\n        # extract fields into results\n        results = self._extract(*res)\n\n        # extract fields into results\n        results = self._post_extract_filter(results)\n\n        # sort results\n        self.results = self._sort(results)\n\n        toc = time.time()\n        logger.debug(f'PlainListExtractor: {toc - tic:.2f}s')\n\n        return self.results\n\n\ndef run_plain_list_detector(url: str, method: str = None) -> PlainListDetector:\n    html_requester = HtmlRequester(url=url, request_method=method)\n    html_requester.run()\n\n    graph_loader = GraphLoader(html=html_requester.html_, json_data=html_requester.json_data)\n    graph_loader.run()\n\n    plain_list_detector = PlainListDetector(html_requester=html_requester, graph_loader=graph_loader)\n    plain_list_detector.run()\n\n    return plain_list_detector\n\n\nif __name__ == '__main__':\n    urls = [\n        'https://cuiqingcai.com'\n    ]\n    for url in urls:\n        run_plain_list_detector(url)\n"}
{"type": "source_file", "path": "webspot/crawler/actions/run_crawler.py", "content": "from typing import List\n\nfrom scrapy.crawler import CrawlerProcess\n\nfrom webspot.crawler.crawler.spiders.web_spider import WebSpider\n\n\ndef run_crawler(domain: str, urls: List[str], url_paths: List[str] = None, data_root_dir: str = None, **kwargs):\n    process = CrawlerProcess(settings={\n        'domain': domain,\n        'urls': urls,\n        'url_paths': url_paths,\n        'data_root_dir': data_root_dir,\n        **kwargs,\n    })\n    process.crawl(WebSpider)\n    process.start()\n\n\nif __name__ == '__main__':\n    run_crawler('https://www.google.com')\n"}
{"type": "source_file", "path": "webspot/detect/utils/highlight_html.py", "content": "import logging\nimport os.path\nfrom typing import List\n\nfrom bs4 import BeautifulSoup, Tag\n\n\ndef get_embed_highlight_css() -> str:\n    file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../web/static/css/embed/highlight.css'))\n    with open(file_path, 'r') as f:\n        return f.read()\n\n\ndef get_embed_annotate_css() -> str:\n    file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../web/static/css/embed/annotate.css'))\n    with open(file_path, 'r') as f:\n        return f.read()\n\n\ndef get_embed_annotate_js() -> str:\n    file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../web/static/js/embed/annotate.js'))\n    with open(file_path, 'r') as f:\n        return f.read()\n\n\ndef embed_highlight(html: str) -> str:\n    # soup\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # css\n    style_el = soup.new_tag('style')\n    style_el.append(get_embed_highlight_css())\n    soup.select_one('head').append(style_el)\n\n    # html\n    return str(soup)\n\n\ndef embed_annotate(html: str) -> str:\n    # soup\n    soup = BeautifulSoup(html, 'html.parser')\n\n    # css\n    style_el = soup.new_tag('style')\n    style_el.append(get_embed_annotate_css())\n    soup.select_one('head').append(style_el)\n\n    # js\n    script_el = soup.new_tag('script')\n    script_el.append(get_embed_annotate_js())\n    body_el = soup.select_one('body')\n    body_el.append(script_el)\n\n    # links\n    for a_el in soup.select('a'):\n        a_el.attrs['href'] = 'javascript:'\n\n    # html\n    return str(soup)\n\n\ndef _add_class(el: Tag, classes: List[str]):\n    if not el:\n        return\n    el_class = el.get('class') or ''\n    if isinstance(el_class, str):\n        el['class'] = ' '.join([el_class, *classes])\n    elif isinstance(el_class, list):\n        el['class'] = ' '.join([*el_class, *classes])\n\n\ndef _add_label(el: Tag, soup: BeautifulSoup, label: str, type_: str = None):\n    label_el = soup.new_tag('div')\n    label_el.append(label)\n    el.append(label_el)\n    _add_class(label_el, ['webspot-highlight-label'])\n    if type_:\n        _add_class(label_el, [f'webspot-highlight-label-{type_}'])\n\n\ndef add_class(el: Tag, classes: List[str]):\n    return _add_class(el, classes)\n\n\ndef add_label(el: Tag, soup: BeautifulSoup, label: str, type_: str = None):\n    return _add_label(el, soup, label, type_)\n"}
{"type": "source_file", "path": "webspot/detect/utils/__init__.py", "content": ""}
{"type": "source_file", "path": "webspot/detect/__init__.py", "content": ""}
{"type": "source_file", "path": "webspot/detect/detectors/pagination.py", "content": "from typing import Optional\n\nimport autopager\nimport numpy as np\nimport parsel\nfrom bs4 import BeautifulSoup\n\nfrom webspot.constants.detector import DETECTOR_PAGINATION\nfrom webspot.detect.detectors.base import BaseDetector\nfrom webspot.detect.models.result import Result\nfrom webspot.detect.models.selector import Selector\nfrom webspot.detect.utils.highlight_html import add_class, add_label\nfrom webspot.detect.utils.transform_html_links import transform_url\nfrom webspot.detect.utils.url import get_url_domain\n\n\nclass PaginationDetector(BaseDetector):\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self._at_res: list = []\n        self._el_next: Optional[parsel.selector.Selector] = None\n        self.next_url: Optional[str] = None\n\n    def highlight_html(self, html: str, **kwargs) -> str:\n        soup = BeautifulSoup(html, 'html.parser')\n\n        if len(self.results) == 0:\n            return html\n\n        result = self.results[0]\n        next_selector = result.selectors.get('next')\n        next_el = soup.select_one(next_selector.selector)\n        if next_el is None:\n            return html\n\n        add_class(next_el, ['webspot-highlight-container', 'webspot-highlight-node-color__red'])\n        add_label(next_el, soup, f'Pagination', 'primary')\n\n        return str(soup)\n\n    @property\n    def root_url(self):\n        return self.html_requester.url\n\n    @property\n    def link_nodes(self):\n        return [n for n in self.get_nodes_by_feature(feature_key='tag', feature_value='a')\n                if n.attrs.get('href') is not None]\n\n    def _get_internal_link_nodes(self):\n        self.internal_link_nodes = [\n            n\n            for n in self.link_nodes\n            if get_url_domain(transform_url(self.root_url, n.attrs.get('href'))) == get_url_domain(self.root_url)\n        ]\n\n    @property\n    def internal_link_nodes_ids(self):\n        return [n.id for n in self.internal_link_nodes]\n\n    @property\n    def internal_link_nodes_idx(self):\n        nodes_ids = self.graph_loader.nodes_ids_tensor.detach().numpy()\n        internal_link_nodes_ids_enc = self.graph_loader.node_ids_enc.transform(self.internal_link_nodes_ids)\n        return np.argwhere(np.isin(nodes_ids, internal_link_nodes_ids_enc))\n\n    def _train(self) -> np.array:\n        self._at_res = autopager.extract(self.html_requester.html_)\n        _els_next = [t[1] for t in self._at_res if t[0] == 'NEXT']\n        if len(_els_next) == 0:\n            return\n        self._el_next = _els_next[0]\n        self.next_url = self._el_next.attrib.get('href')\n\n    def _extract(self):\n        if not self.next_url:\n            return\n\n        _nodes = [n for n in self.link_nodes if\n                  transform_url(self.root_url, n.attrs.get('href')) == transform_url(self.root_url, self.next_url)]\n        if len(_nodes) == 0:\n            return\n        next_node = _nodes[-1]\n        next_selector = Selector(\n            name='pagination',\n            selector=self.graph_loader.get_node_css_selector_path(next_node),\n            type='css',\n            node_id=next_node.id,\n        )\n        score = 1.0\n        self.results.append(Result(\n            name='Next',\n            score=score,\n            scores={'score': score},\n            selectors={\n                'next': next_selector,\n            },\n            detector=DETECTOR_PAGINATION,\n        ))\n\n    def run(self):\n        self._train()\n\n        self._extract()\n\n        return self.results\n"}
{"type": "source_file", "path": "webspot/detect/models/table_result.py", "content": "from typing import Optional, List\n\nfrom webspot.detect.models.result import Result\n\n\nclass TableResult(Result):\n    columns: Optional[List[str]]\n    data: Optional[List[dict]]\n"}
{"type": "source_file", "path": "webspot/detect/models/result.py", "content": "from typing import Dict, Any, Optional\n\nfrom pydantic import BaseModel\n\nfrom webspot.detect.models.selector import Selector\n\n\nclass Result(BaseModel):\n    name: Optional[str]\n    selectors: Optional[Dict[str, Selector]]\n    score: Optional[float]\n    scores: Optional[Dict[str, Optional[float]]]\n    detector: Optional[str]\n"}
{"type": "source_file", "path": "webspot/detect/models/__init__.py", "content": ""}
{"type": "source_file", "path": "webspot/detect/models/selector.py", "content": "from typing import Optional\n\nfrom pydantic import BaseModel\n\n\nclass Selector(BaseModel):\n    name: str\n    selector: str\n    type: Optional[str]\n    attribute: Optional[str]\n    node_id: Optional[int]\n"}
{"type": "source_file", "path": "webspot/detect/utils/math.py", "content": "import numpy as np\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef log_positive(x) -> float:\n    return float(np.log(x + 1))\n"}
{"type": "source_file", "path": "webspot/detect/detectors/__init__.py", "content": ""}
{"type": "source_file", "path": "webspot/extract/extract_results.py", "content": "from datetime import datetime\nfrom typing import List\n\nfrom webspot.constants.detector import DETECTOR_PAGINATION, DETECTOR_PLAIN_LIST\nfrom webspot.constants.html_request_method import HTML_REQUEST_METHOD_REQUEST\nfrom webspot.detect.detectors.pagination import PaginationDetector\nfrom webspot.detect.detectors.plain_list import PlainListDetector\nfrom webspot.graph.graph_loader import GraphLoader\nfrom webspot.request.html_requester import HtmlRequester\n\n\ndef extract_rules(\n    url: str = None,\n    method: str = None,\n    duration: int = None,\n    html: str = None,\n    detectors: List[str] = None,\n):\n    if method is None or method == '':\n        method = HTML_REQUEST_METHOD_REQUEST\n\n    if detectors is None:\n        detectors = [DETECTOR_PLAIN_LIST, DETECTOR_PAGINATION]\n\n    execution_time = {\n        'html_requester': None,\n        'graph_loader': None,\n        'detectors': {},\n    }\n\n    # html requester\n    tic = datetime.now()\n    html_requester = HtmlRequester(\n        url=url,\n        html=html,\n        request_method=method,\n        request_rod_duration=duration,\n    )\n    html_requester.run()\n    execution_time['html_requester'] = round((datetime.now() - tic).total_seconds() * 1000)\n\n    # graph loader\n    tic = datetime.now()\n    graph_loader = GraphLoader(\n        html=html_requester.html_,\n        json_data=html_requester.json_data,\n    )\n    graph_loader.run()\n    execution_time['graph_loader'] = round((datetime.now() - tic).total_seconds() * 1000)\n\n    # run detectors\n    html = html_requester.html\n    results = {}\n    detectors_ = []\n    for detector_name in detectors:\n        # start time\n        tic = datetime.now()\n\n        # detector class\n        if detector_name == DETECTOR_PLAIN_LIST:\n            detector_cls = PlainListDetector\n        elif detector_name == DETECTOR_PAGINATION:\n            detector_cls = PaginationDetector\n        else:\n            raise Exception(f'Invalid detector: {detector_name}')\n\n        # run detector\n        detector = detector_cls(\n            graph_loader=graph_loader,\n            html_requester=html_requester,\n        )\n        detector.run()\n\n        # highlight html\n        html = detector.highlight_html(html)\n\n        # add to results\n        results[detector_name] = [r.dict() for r in detector.results]\n\n        # add to detectors_\n        detectors_.append(detector)\n\n        # execution time\n        execution_time['detectors'][detector_name] = round((datetime.now() - tic).total_seconds() * 1000)\n\n    return results, execution_time, html_requester, graph_loader, detectors_\n"}
{"type": "source_file", "path": "webspot/graph/graph_loader.py", "content": "import json\nfrom typing import List, Tuple, Dict, Optional\n\nimport dgl\nimport html_to_json_enhanced\nimport networkx as nx\nimport numpy as np\nimport torch\nfrom bs4 import BeautifulSoup\nfrom html_to_json_enhanced import iterate\nfrom networkx import DiGraph, dfs_successors\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom dgl import DGLGraph\n\nfrom webspot.graph.models.node import Node\nfrom webspot.logging import get_logger\nfrom webspot.utils.selector import is_valid_css_selector\n\nlogger = get_logger('webspot.graph.graph_loader')\n\nSVG_TAG_NAMES = ['circle', 'clipPath', 'defs', 'ellipse', 'g', 'image', 'line', 'linearGradient',\n                 'mask', 'path', 'pattern', 'polygon', 'polyline', 'radialGradient', 'rect',\n                 'stop', 'svg', 'text', 'tspan']\nESCAPED_TAG_NAMES = [\n    'script',\n    'link',\n    'meta',\n    *SVG_TAG_NAMES,\n]\n\n\nclass GraphLoader(object):\n    def __init__(\n        self,\n        html: str,\n        json_data: dict,\n        body_only: bool = True,\n        embed_walk_length: int = 8,\n        dfs_depth: int = 8,\n    ):\n        # settings\n        self.body_only = body_only\n        self.embed_walk_length = embed_walk_length\n        self.available_feature_keys = [\n            'tag',\n            'id',\n            'class',\n            'style',\n        ]\n        self.dfs_depth = dfs_depth\n\n        # data\n        self.html = html\n        self.json_data = json_data\n        self.root_node_json: dict = {}\n        self.nodes_: List[Node] = []\n        self.nodes_ids: List[int] = []\n        self.nodes_features: List[Dict[str, int]] = []\n        self.edge_nodes: List[Node] = []\n\n        # internals\n        self._nodes_dict = None\n        self._css_selector_repr_cache = {}\n        self._unique_node_feature_id_dict: Optional[dict] = None\n        self._nodes_texts: Optional[List[str]] = None\n        self._nodes_ids_idx_dict = None\n        self._soup: Optional[BeautifulSoup] = None\n        self._g_nx: Optional[DiGraph] = None\n\n        # encoders\n        self.nodes_features_enc = DictVectorizer()\n        self.node_ids_enc = LabelEncoder()\n\n        # tensors\n        self.nodes_ids_tensor = torch.LongTensor()\n        self.nodes_features_tensor = torch.LongTensor()\n        self.edges_source_tensor = torch.LongTensor()\n        self.edges_target_tensor = torch.LongTensor()\n        self.nodes_embedded_tensor = torch.LongTensor()\n        self.nodes_text_length_tensor = torch.LongTensor()\n\n        # vectors\n        self.nodes_text_length_vec: Optional[np.ndarray] = None\n        self.nodes_full_text_length_vec: Optional[np.ndarray] = None\n\n        # graph\n        self.g_dgl: Optional[DGLGraph] = None\n\n    @property\n    def nodes_dict(self):\n        if self._nodes_dict:\n            return self._nodes_dict\n        self._nodes_dict = {n.id: n for n in self.nodes_}\n        return self._nodes_dict\n\n    @property\n    def nodes_ids_idx_dict(self):\n        if self._nodes_ids_idx_dict:\n            return self._nodes_ids_idx_dict\n        self._nodes_ids_idx_dict = {n.id: i for i, n in enumerate(self.nodes_)}\n        return self._nodes_ids_idx_dict\n\n    @property\n    def g_nx(self) -> nx.DiGraph:\n        if self._g_nx:\n            return self._g_nx\n        self._g_nx = DiGraph(self.g_dgl.to_networkx())\n        return self._g_nx\n\n    @property\n    def soup(self):\n        return self._soup\n\n    def get_nodes_json_data_by_filename(self, filename: str) -> (List[dict], dict):\n        with open(filename) as f:\n            json_data = json.loads(f.read())\n        return self.get_nodes_json_data(json_data)\n\n    def get_nodes_json_data(self, json_data: str) -> (List[dict], dict):\n        # by default html as root\n        root = json_data\n\n        # if body only\n        if self.body_only:\n            arr = [c for c in root.get('_children') if c.get('_tag') == 'body']\n            if len(arr) > 0:\n                root = arr[0]\n            else:\n                raise Exception('No body tag found')\n\n        return list(iterate(root)), root\n\n    def _load_graph_data(self, nodes_json_data: List[dict]):\n        for i, node_json_data in enumerate(nodes_json_data):\n            # node\n            node = self._get_node(node_json_data)\n\n            # skip escaped tags\n            if node.tag in ESCAPED_TAG_NAMES:\n                continue\n\n            # add to all nodes\n            self.nodes_.append(node)\n\n            # add to all node features\n            # self.nodes_features.append(node.features_dict)\n            self.nodes_features.append({k: v for k, v in node.features_dict.items()\n                                        if k.split('=')[0] in self.available_feature_keys\n                                        })\n\n    def _get_node(self, node_json_data: dict) -> Node:\n        node_id = node_json_data.get('_id')\n        parent_id = node_json_data.get('_parent')\n\n        # node\n        node = Node({\n            'id': node_id,\n            'parent_id': parent_id,\n            'features': self._get_node_features(node_json_data),\n            'text': self._get_node_text(node_json_data),\n        })\n\n        return node\n\n    @staticmethod\n    def _get_node_features(node_json_data: dict) -> List[Tuple[str, str]]:\n        # tags\n        features = [('tag', node_json_data.get('_tag'))]\n\n        # attributes\n        attributes = node_json_data.get('_attributes')\n        if attributes is not None:\n            for key, value in attributes.items():\n                if isinstance(value, list):\n                    for v in value:\n                        features.append((key, v))\n                else:\n                    features.append((key, value))\n\n        return features\n\n    @staticmethod\n    def _get_node_text(node_json_data: dict) -> Optional[str]:\n        if node_json_data.get('_text') is not None:\n            return node_json_data.get('_text')\n\n        elif node_json_data.get('_texts') is not None:\n            return ' '.join(node_json_data.get('_texts'))\n\n        else:\n            return\n\n    def load_graph_data(self):\n        # get list data and root node\n        self.json_data = html_to_json_enhanced.convert_html.convert(self.html, with_id=True)\n        list_data, self.root_node_json = self.get_nodes_json_data(self.json_data)\n\n        # load graph data\n        self._load_graph_data(list_data)\n\n        # encode node ids\n        self.nodes_ids = self.node_ids_enc.fit_transform(\n            np.array([node.id for node in self.nodes_]).reshape(-1, 1)\n        )\n\n    def load_tensors(self):\n        # nodes tensor\n        encoded_nodes = self.node_ids_enc.transform([n.id for n in self.nodes_])\n        self.nodes_ids_tensor = torch.LongTensor(encoded_nodes)\n\n        # nodes features tensor\n        features = self.nodes_features_enc.fit_transform(self.nodes_features).todense()\n        self.nodes_features_tensor = torch.tensor(data=features, dtype=torch.float32, requires_grad=True)\n\n        # edge nodes\n        self.edge_nodes = [n for n in self.nodes_ if self.nodes_dict.get(n.parent_id) is not None]\n\n        # edges tensors\n        encoded_source = self.node_ids_enc.transform([n.parent_id for n in self.edge_nodes])\n        encoded_target = self.node_ids_enc.transform([n.id for n in self.edge_nodes])\n        self.edges_source_tensor = torch.LongTensor(encoded_source)\n        self.edges_target_tensor = torch.LongTensor(encoded_target)\n\n    def load_dgl_graph(self):\n        self.g_dgl = dgl.graph((self.edges_source_tensor, self.edges_target_tensor))\n\n    def load_embeddings(self):\n        # embedded nodes tensor\n        self.nodes_embedded_tensor = dgl.sampling.node2vec_random_walk(\n            g=self.g_dgl,\n            nodes=self.nodes_ids_tensor,\n            p=1,\n            q=1,\n            walk_length=self.embed_walk_length,\n        )\n\n    def load_soup(self):\n        self._soup = BeautifulSoup(self.html, 'html.parser')\n\n    def load_texts(self):\n        self._nodes_texts = [n.text for n in self.nodes_]\n        self.nodes_text_length_vec = np.array([len(t or []) for t in self._nodes_texts])\n\n    def run(self):\n        self.load_graph_data()\n        self.load_tensors()\n        self.load_dgl_graph()\n        self.load_embeddings()\n        self.load_soup()\n        self.load_texts()\n\n    @property\n    def nodes(self):\n        return np.array(self.nodes_)\n\n    def get_node_by_id(self, id: int) -> Optional[Node]:\n        return self.nodes_dict.get(id)\n\n    def get_nodes_by_ids(self, ids: List[int]) -> List[Node]:\n        return [self.get_node_by_id(id) for id in ids]\n\n    def get_node_children_recursive_by_id(self, id: int) -> List[Node]:\n        idx = self.node_ids_enc.transform([id])[0]\n        child_nodes_idx = self.get_node_children_idx_recursive_by_id(id)\n        child_ids = self.node_ids_enc.inverse_transform([nid for nid in child_nodes_idx if nid != idx])\n        return self.get_nodes_by_ids(child_ids)\n\n    def get_node_children_idx_recursive_by_id(self, id: int) -> np.ndarray:\n        idx = self.node_ids_enc.transform([id])[0]\n        return self.get_node_children_idx_recursive_by_idx(idx)\n\n    def get_node_children_idx_recursive_by_idx(self, idx: int) -> np.ndarray:\n        successors = dfs_successors(G=self.g_nx, source=idx, depth_limit=self.dfs_depth)\n        child_nodes_idx = [item for sublist in successors.values() for item in sublist]\n        return np.array(list(child_nodes_idx))\n\n    def get_node_text_length(self, n: Node, max_length: int = 1024) -> int:\n        selector = self.get_node_css_selector_path(n)\n        el = self._soup.select_one(selector)\n        if el is None:\n            return 0\n        return min(len(el.text), max_length)\n\n    def get_node_children_by_id(self, id: int) -> List[Node]:\n        return [n for n in self.nodes_ if n.parent_id == id]\n\n    @property\n    def unique_features_idx(self):\n        # features counts vector\n        features_counts = self.nodes_features_tensor.detach().numpy().sum(axis=0)\n\n        # indexes of features each of which is associated to only one node\n        unique_features_idx = np.where(features_counts == 1)\n\n        return unique_features_idx\n\n    @property\n    def available_features_idx(self):\n        available_features = [f for f in self.nodes_features_enc.feature_names_\n                              if f.split('=')[0] in self.available_feature_keys]\n        available_features_mask = np.isin(self.nodes_features_enc.feature_names_, np.array(available_features))\n        available_features_idx = np.argwhere(available_features_mask).T\n\n        return available_features_idx\n\n    @property\n    def unique_available_features_idx(self):\n        return np.intersect1d(self.unique_features_idx, self.available_features_idx)\n\n    @property\n    def unique_node_feature_pairs_idx(self):\n        \"\"\"\n        2-dimensional node-feature pairs index\n        \"\"\"\n        nodes_feat = self.nodes_features_tensor.detach().numpy()\n        uniq_avail_nodes_feat = np.zeros(nodes_feat.shape)\n        uniq_idx = self.unique_available_features_idx\n        uniq_avail_nodes_feat[:, uniq_idx] = nodes_feat[:, uniq_idx]\n        return np.argwhere(uniq_avail_nodes_feat > 0)\n\n    @property\n    def unique_node_feature_pairs(self):\n        nodes = self.nodes[self.unique_node_feature_pairs_idx[:, 0]].tolist()\n        feats = np.array(self.nodes_features_enc.feature_names_)[self.unique_node_feature_pairs_idx[:, 1]].tolist()\n        return [(n, f) for n, f in zip(nodes, feats)]\n\n    @property\n    def unique_node_feature_id_dict(self) -> Dict[int, str]:\n        if self._unique_node_feature_id_dict is not None:\n            return self._unique_node_feature_id_dict\n        self._unique_node_feature_id_dict = {n.id: f for n, f in self.unique_node_feature_pairs}\n        return self._unique_node_feature_id_dict\n\n    def _get_node_previous_siblings(self, node: Node) -> List[Node]:\n        return [n for n in self.nodes_ if n.parent_id == node.parent_id\n                and n.id < node.id\n                and n.feature_tag == node.feature_tag]\n\n    def _get_node_previous_siblings_with_classes(self, node: Node) -> List[Node]:\n        node_feature_classes_set = set(node.feature_classes)\n        return [n for n in self.nodes_ if n.parent_id == node.parent_id\n                and n.id < node.id\n                and n.feature_tag == node.feature_tag\n                and node_feature_classes_set.issubset(set(n.feature_classes))]\n\n    def _is_node_last_child(self, node: Node) -> bool:\n        parent = self.get_node_by_id(node.parent_id)\n        children = self.get_node_children_by_id(parent.id)\n        return node.id == children[-1].id\n\n    def get_node_css_selector_repr(self, node: Node, numbered: bool = True, no_id: bool = False) -> str:\n        if numbered:\n            return self._get_node_css_selector_repr(node, numbered, no_id)\n\n        if self._css_selector_repr_cache.get(node.id):\n            return self._get_node_css_selector_repr(node, numbered, no_id)\n\n        self._css_selector_repr_cache[node.id] = self._get_node_css_selector_repr(node, numbered, no_id)\n        return self._css_selector_repr_cache.get(node.id)\n\n    def _get_node_css_selector_repr(self, node: Node, numbered: bool = True, no_id: bool = False) -> str:\n        # id\n        if node.feature_id is not None and not no_id and is_valid_css_selector(f'#{node.feature_id}'):\n            return f'{node.feature_tag}#{node.feature_id}'\n\n        # class\n        elif len(node.feature_classes) > 0 and is_valid_css_selector('.'.join(node.feature_classes)):\n            if numbered:\n                previous_siblings = self._get_node_previous_siblings_with_classes(node)\n                length = len(previous_siblings) + 1\n                if length > 1:\n                    if self._is_node_last_child(node):\n                        return f'{node.feature_tag}.{\".\".join(node.feature_classes)}:last-child'\n                    return f'{node.feature_tag}.{\".\".join(node.feature_classes)}:nth-of-type({length})'\n            return f'{node.feature_tag}.{\".\".join(node.feature_classes)}'\n\n        # tag\n        else:\n            if numbered:\n                length = len(self._get_node_previous_siblings(node)) + 1\n                if length > 1:\n                    if self._is_node_last_child(node):\n                        return f'{node.feature_tag}:last-child'\n                    return f'{node.feature_tag}:nth-of-type({length})'\n            return f'{node.feature_tag}'\n\n    @staticmethod\n    def _get_node_css_selector_repr_from_feature(feat: str):\n        k, v = feat.split('=')\n        if k == 'tag':\n            return v\n        elif k == 'class':\n            return f'.{v}'\n        else:\n            return f'[{k}=\"{v}\"]'\n\n    def get_node_css_selector_path(self, node: Node, root_id: int = None, numbered: bool = True,\n                                   no_id: bool = False) -> str:\n        # return if no parent\n        if node.parent_id is None:\n            return self.get_node_css_selector_repr(node=node, numbered=numbered, no_id=no_id)\n\n        # css selector path\n        path = [self.get_node_css_selector_repr(node=node, numbered=numbered, no_id=no_id)]\n\n        # iterate node parent until reaching end condition\n        while node.parent_id is not None:\n            # parent\n            parent = self.get_node_by_id(node.parent_id)\n\n            # end if parent is root\n            if root_id is not None and parent.id == root_id:\n                break\n\n            # end if no parent\n            if parent is None:\n                break\n\n            # end if parent node is unique-feature\n            if not no_id and self.unique_node_feature_id_dict.get(parent.id) is not None:\n                feat = self.unique_node_feature_id_dict.get(parent.id)\n                path.insert(0, self.get_node_css_selector_repr(parent, numbered=numbered, no_id=no_id))\n                break\n\n            # add to path\n            parent_path = self.get_node_css_selector_repr(parent, numbered=numbered, no_id=no_id)\n            path.insert(0, parent_path)\n\n            # set node to its parent\n            node = parent\n\n        return ' > '.join(path)\n"}
{"type": "source_file", "path": "webspot/graph/models/__init__.py", "content": ""}
{"type": "source_file", "path": "webspot/graph/models/node.py", "content": "import re\nfrom typing import Tuple, List, Dict, Optional\n\n\nclass Node(dict):\n    @property\n    def id(self) -> int:\n        return self.get('id')\n\n    @property\n    def parent_id(self):\n        return self.get('parent_id')\n\n    @property\n    def features(self) -> Tuple[Tuple[str, str]]:\n        features: List[Tuple[str, str]] = []\n        for k, v in self.get('features'):\n            # skip pseudo-classes\n            if k == 'class':\n                if ':' in v:\n                    continue\n            features.append((k, v))\n        return tuple(features)\n\n    @property\n    def features_dict(self) -> Dict[str, int]:\n        return {f'{k}={v}': 1 for k, v in self.features}\n\n    @property\n    def feature_tag(self) -> Optional[str]:\n        for k, v in self.features:\n            if k == 'tag':\n                return v\n        return None\n\n    @property\n    def feature_classes(self) -> List[str]:\n        classes = []\n        for k, v in self.features:\n            if k == 'class':\n                # css parser cannot parse class starting with a digit\n                if re.search(r'^\\d', v) is not None:\n                    continue\n                classes.append(v)\n        return classes\n\n    @property\n    def feature_id(self) -> Optional[str]:\n        for k, v in self.features:\n            if k == 'id':\n                return v\n        return None\n\n    @property\n    def text(self) -> str:\n        return self.get('text')\n\n    @property\n    def tag(self):\n        return self.feature_tag\n\n    @property\n    def attrs(self):\n        return {k: v for k, v in self.features if k != 'tag'}\n"}
{"type": "source_file", "path": "webspot/graph/__init__.py", "content": ""}
{"type": "source_file", "path": "webspot/detect/utils/transform_html_links.py", "content": "import re\nfrom urllib.parse import urljoin\n\nfrom bs4 import BeautifulSoup, Tag\n\n\ndef transform_html_links(html: str, url: str) -> str:\n    soup = BeautifulSoup(html, 'html.parser')\n\n    for el in soup.select('a'):\n        _transform_a(el, url)\n\n    for el in soup.select('img'):\n        _transform(el, 'src', url)\n\n    for el in soup.select('link'):\n        _transform(el, 'href', url)\n\n    for el in soup.select('script'):\n        if _is_valid_asset_url(el.get('src')):\n            _transform(el, 'src', url)\n        else:\n            el.decompose()\n\n    return str(soup)\n\n\ndef _transform(el: Tag, key: str, root_url: str):\n    if el.get(key) and _is_relative_url_path(el[key]):\n        el[key] = transform_url(root_url, el[key])\n\n\ndef _transform_a(el: Tag, root_url: str, target_blank: bool = True):\n    key = 'href'\n    if el.get(key) and _is_relative_url_path(el[key]):\n        url = transform_url(root_url, el[key])\n        el[key] = f'/?url={url}'\n        if target_blank:\n            el['target'] = '_blank'\n\n\ndef _is_valid_asset_url(url: str) -> bool:\n    if url is None:\n        return False\n    for keyword in ['bootstrap', 'jquery']:\n        if keyword in url.lower():\n            return True\n    return False\n\n\ndef _is_relative_url_path(url: str) -> bool:\n    return not re.search(re.escape(url), r'^(https?:)?//')\n\n\ndef transform_url(root_url: str = None, url: str = None) -> str:\n    if root_url is None:\n        return url\n    if not _is_relative_url_path(url):\n        return url\n    return urljoin(root_url, url)\n"}
{"type": "source_file", "path": "webspot/detect/utils/url.py", "content": "from urllib.parse import urlparse\n\n\ndef get_url_domain(url: str) -> str:\n    \"\"\"Get the domain from a url.\"\"\"\n    return urlparse(url).netloc\n\n\ndef get_url_path(url: str) -> str:\n    \"\"\"Get the path from a url.\"\"\"\n    return urlparse(url).path\n"}
{"type": "source_file", "path": "webspot/extract/__init__.py", "content": ""}
{"type": "source_file", "path": "webspot/logging/__init__.py", "content": "import logging\nimport os\nimport sys\n\n_loggers = {}\n\nlog_level_name = os.environ.get('WEBSPOT_LOG_LEVEL', 'info')\nlog_level = logging.getLevelName(log_level_name.upper())\n\n\ndef get_logger(name: str, stream=sys.stdout):\n    # check if logger already exists\n    if _loggers.get(name) is not None:\n        return _loggers.get(name)\n\n    # logger\n    logger = logging.getLogger(name)\n    logger.setLevel(log_level)\n\n    # handler\n    handler = logging.StreamHandler(stream)\n    handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n\n    # add handler\n    logger.addHandler(handler)\n\n    # store logger\n    _loggers[name] = logger\n\n    return logger\n"}
{"type": "source_file", "path": "webspot/models/__init__.py", "content": ""}
