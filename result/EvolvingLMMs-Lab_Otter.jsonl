{"repo_info": {"repo_name": "Otter", "repo_owner": "EvolvingLMMs-Lab", "repo_url": "https://github.com/EvolvingLMMs-Lab/Otter"}}
{"type": "test_file", "path": "unit_tests/test_mmc4_dataset.py", "content": "import unittest\nfrom unittest.mock import Mock\nfrom pipeline.mimicit_utils.data import get_mmc4_dataset\n\n\nclass TestGetMMC4Dataset(unittest.TestCase):\n    def test_get_mmc4_dataset(self):\n        # Mock the required inputs\n        args = Mock(\n            mmc4_shards=\"/home/luodian/projects/Otter/archived/000000000.tar\",\n            train_num_samples_mmc4=1000,\n            mmc4_textsim_threshold=0.32,\n            batch_size_mmc4=10,\n            seed=0,\n            workers=2,\n            world_size=1,\n        )\n        image_processor = Mock()\n        tokenizer = Mock()\n\n        # Call the function to test\n        data_info = get_mmc4_dataset(args, image_processor, tokenizer)\n\n        # Check if the dataloader's attributes are as expected\n        self.assertEqual(data_info.dataloader.num_batches, 100)\n        self.assertEqual(data_info.dataloader.num_samples, 1000)\n"}
{"type": "test_file", "path": "unit_tests/test_prerun.py", "content": "# Inside tests/unit_tests/test_prerun.py\nimport os\nimport yaml\nimport pytest\nimport orjson\nimport pandas as pd\n\n\n# Define the pytest fixture\n@pytest.fixture\ndef yaml_data(request):\n    yaml_path = request.config.getoption(\"--yaml-path\")\n    if not yaml_path or not os.path.exists(yaml_path):\n        pytest.fail(f\"YAML file path '{yaml_path}' does not exist.\")\n    with open(yaml_path, \"r\") as f:\n        data = yaml.safe_load(f)\n    return data\n\n\n# Your test function\n@pytest.mark.prerun\ndef test_yaml_structure(yaml_data):\n    required_categories = [\n        \"IMAGE_TEXT\",\n        \"TEXT_ONLY\",\n        \"VIDEO_TEXT\",\n        \"IMAGE_TEXT_IN_CONTEXT\",\n    ]\n\n    for category, datasets in yaml_data.items():\n        assert category in required_categories, f\"Unexpected category '{category}' in YAML. Expected categories are {required_categories}.\"\n\n        for dataset_name, data in datasets.items():\n            for path_key, path_value in data.items():\n                if path_key.endswith(\"_path\"):\n                    assert os.path.exists(path_value), f\"Dataset path {path_value} specified under {category} -> {dataset_name} does not exist.\"\n                elif path_key == \"num_samples\":\n                    assert isinstance(path_value, int), f\"'num_samples' should be an integer but got {type(path_value)} under {category} -> {dataset_name}.\"\n\n                # checking mimicit path aligns with corresponding format.\n                if path_key == \"mimicit_path\":\n                    print(f\"Checking -> {path_value} in MIMICIT format.\")\n                    with open(path_value, \"rb\") as f:\n                        data = orjson.loads(f.read())\n\n                    assert \"data\" in data\n\n                if path_key == \"images_path\":\n                    print(f\"Checking -> {path_value} in images format.\")\n                    assert os.path.exists(path_value), f\"Dataset path {path_value} specified under {category} -> {dataset_name} does not exist.\"\n                    # # Read the parquet file using pandas\n                    # df = pd.read_parquet(path_value)\n\n                    # # Check for the 'base64' column\n                    # assert \"base64\" in df.columns, f\"The 'base64' column was not found in the dataset {path_value}.\"\n"}
{"type": "test_file", "path": "pipeline/serve/test_message.py", "content": "import argparse\nimport json\n\nimport requests\n\nfrom pipeline.serve.conversation import default_conversation\n\n\ndef main():\n    if args.worker_address:\n        worker_addr = args.worker_address\n    else:\n        controller_addr = args.controller_address\n        ret = requests.post(controller_addr + \"/refresh_all_workers\")\n        ret = requests.post(controller_addr + \"/list_models\")\n        models = ret.json()[\"models\"]\n        models.sort()\n        print(f\"Models: {models}\")\n\n        ret = requests.post(controller_addr + \"/get_worker_address\", json={\"model\": args.model_name})\n        worker_addr = ret.json()[\"address\"]\n        print(f\"worker_addr: {worker_addr}\")\n\n    if worker_addr == \"\":\n        return\n\n    conv = default_conversation.copy()\n    conv.append_message(conv.roles[0], args.message)\n    prompt = conv.get_prompt()\n\n    headers = {\"User-Agent\": \"Otter Client\"}\n    pload = {\n        \"model\": args.model_name,\n        \"prompt\": prompt,\n        \"max_new_tokens\": args.max_new_tokens,\n        \"temperature\": 0.7,\n        \"stop\": conv.sep,\n    }\n    response = requests.post(\n        worker_addr + \"/worker_generate_stream\",\n        headers=headers,\n        json=pload,\n        stream=True,\n    )\n\n    print(prompt.replace(conv.sep, \"\\n\"), end=\"\")\n    for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"):\n        if chunk:\n            data = json.loads(chunk.decode(\"utf-8\"))\n            output = data[\"text\"].split(conv.sep)[-1]\n            print(output, end=\"\\r\")\n    print(\"\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--controller_address\", type=str, default=\"http://localhost:21001\")\n    parser.add_argument(\"--worker_address\", type=str)\n    parser.add_argument(\"--model_name\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--max_new_tokens\", type=int, default=32)\n    parser.add_argument(\"--message\", type=str, default=\"Tell me a story with more than 1000 words.\")\n    args = parser.parse_args()\n\n    main()\n"}
{"type": "source_file", "path": "mimic-it/convert-it/__init__.py", "content": ""}
{"type": "source_file", "path": "mimic-it/convert-it/abstract_dataset.py", "content": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Tuple\nimport importlib\n\nAVAILABLE_DATASETS: List[str] = [\n    \"change.SpotTheDifference\",\n    \"change.CocoGeneralDifference\",\n    \"video.DenseCaptions\",\n    \"video.TVCaptions\",\n    \"video.VisualStoryTelling\",\n    \"3d.SceneNavigation\",\n    \"fpv.EGO4D\",\n    \"2d.Llava\",\n]\n\n\nclass AbstractDataset(ABC):\n    def __init__(self, name: str, short_name: str, image_path: str, num_threads: int):\n        \"\"\"\n        Initialize an AbstractDataset object.\n\n        Args:\n            name (str): The name of the dataset.\n            short_name (str): The short name of the dataset.\n            image_path (str): The path to the images of the dataset.\n            num_threads (int): The number of threads to use for processing the images.\n        \"\"\"\n        self.name: str = name\n        self.short_name: str = short_name\n        self.images: Dict[str, bytes] = self._load_images(image_path, num_threads)\n\n    @abstractmethod\n    def _load_images(self, image_path: str, num_thread: int) -> dict[str, bytes]:\n        \"\"\"\n        Load the images from the videos or albums.\n\n        Args:\n            image_path (str): The path storing the videos or albums.\n            num_thread (int): The number of threads to use for loading the images.\n\n        Returns:\n            Dict[str, bytes]: A dictionary of images, where the keys are the IDs of the images.\n        \"\"\"\n        pass\n\n    def __getitem__(self, key: str) -> Dict[str, Any]:\n        \"\"\"\n        Get the item at the given key as a dictionary.\n\n        Args:\n            key (str): The key of the item to retrieve.\n\n        Returns:\n            Dict[str, Any]: The item at the given key.\n        \"\"\"\n        return self.images[key]\n\n    def __iter__(self) -> \"AbstractDataset\":\n        \"\"\"\n        Return the iterator object for the dataset.\n\n        Returns:\n            AbstractDataset: The iterator object.\n        \"\"\"\n        self.keys = iter(self.images.keys())\n        return self\n\n    def __next__(self) -> Tuple[str, bytes]:\n        \"\"\"\n        Return the next item in the iteration.\n\n        Returns:\n            Tuple[str, bytes]: The next item as a tuple of key and image.\n\n        Raises:\n            StopIteration: If there are no more items in the iteration.\n        \"\"\"\n        try:\n            key = next(self.keys)\n            image = self.images[key]\n            return key, image\n        except StopIteration:\n            raise StopIteration\n\n    def __len__(self) -> int:\n        \"\"\"\n        Return the length of the dataset.\n\n        Returns:\n            int: The length of the dataset.\n        \"\"\"\n        return len(self.query_inputs)\n\n    def __str__(self) -> str:\n        \"\"\"\n        Return a string representation of the dataset.\n\n        Returns:\n            str: The string representation of the dataset.\n        \"\"\"\n        return f\"{self.name} dataset\"\n\n\ndef get_dataset_by_path(path: str, dataset_args: dict[str, str]) -> AbstractDataset:\n    \"\"\"\n    Get an instance of a dataset class based on the given path.\n\n    Args:\n        path (str): The path to the dataset class in the format \"<module>.<class>\".\n        dataset_args (Dict[str, str]): Additional arguments to pass to the dataset class constructor.\n\n    Returns:\n        AbstractDataset: An instance of the dataset class.\n\n    Raises:\n        AssertionError: If the given path is not an available dataset.\n    \"\"\"\n    assert path in AVAILABLE_DATASETS, f\"{path} is not an available dataset.\"\n    module_path, dataset_name = path.split(\".\")\n    module_path = \"datasets.\" + module_path\n\n    # Import the module and load the class\n\n    imported_module = importlib.import_module(module_path)\n    dataset_class = getattr(imported_module, dataset_name)\n\n    # Instantiate the class and return the instance\n    return dataset_class(**dataset_args)\n\n\ndef get_available_datasets() -> List[str]:\n    \"\"\"\n    Get a list of available dataset paths.\n\n    Returns:\n        List[str]: A list of available dataset paths.\n    \"\"\"\n    return AVAILABLE_DATASETS\n"}
{"type": "source_file", "path": "mimic-it/convert-it/datasets/utils/visual_story_telling_utils.py", "content": "import json\nimport requests\n\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\nfrom image_utils import resize_image, create_folder\n\n\ndef get_url(image: dict[str]):\n    \"\"\"\n    Retrieve the URL of the image.\n\n    Args:\n        image: A dictionary containing image information.\n\n    Returns:\n        The URL of the image.\n\n    \"\"\"\n    if \"url_o\" in image:\n        return image[\"url_o\"]\n    else:\n        return image[\"url_m\"]\n\n\ndef download_single_image(image: dict[str]) -> tuple[str, bytes]:\n    \"\"\"\n    Download a single image and resize it.\n\n    Args:\n        image: A dictionary containing image information.\n\n    Returns:\n        A tuple containing the image ID and the resized image as bytes.\n\n    \"\"\"\n    url = get_url(image)\n    id = image[\"id\"]\n    try:\n        pic = requests.get(url)\n        return id, resize_image(pic.content)\n    except:\n        return id, None\n\n\ndef download(images: list[dict[str]], num_threads: int):\n    \"\"\"\n    Download multiple images concurrently using thread pooling.\n\n    Args:\n        images: A list of dictionaries, each containing image information.\n        num_threads: The number of threads to use for concurrent downloading.\n\n    Returns:\n        A dictionary mapping image IDs to their corresponding resized images as bytes.\n\n    \"\"\"\n    output = {}\n    process_bar = tqdm(total=len(images), unit=\"image\", desc=\"Downloading images\")\n    expired_images = []\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        for id, image in executor.map(download_single_image, images):\n            if image is not None:\n                output[id] = image\n            else:\n                expired_images.append(id)\n            process_bar.update(1)\n    process_bar.close()\n    create_folder(\"output\")\n    with open(\"output/expired_images.json\", \"w\") as f:\n        json.dump(expired_images, f, indent=4)\n    return output\n"}
{"type": "source_file", "path": "mimic-it/syphus/datasets/__init__.py", "content": ""}
{"type": "source_file", "path": "mimic-it/syphus/datasets/3d.py", "content": "\"\"\"\nThis file contains the implementation of the SceneNavigation and SceneRef and Scene QA datasets.\n\"\"\"\n\nimport json\nimport random\n\nfrom abstract_dataset import AbstractDataset\n\n\nclass SceneNavigation(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"SceneNavigation\",\n        in_context_path: str = \"prompts/scene_navigation.json\",\n        query_inputs_path: str = \"annotations/scene_navigation/scan_info.json\",\n    ):\n        super().__init__(name, in_context_path, query_inputs_path)\n\n    def _load_query_inputs(self, path: str) -> list[str]:\n        with open(path, \"r\") as f:\n            json_data = json.load(f)\n        results = []\n        counter = 0\n        for scene_id, inner_dict in json_data.items():\n            # if counter > 7:\n            #     break\n            descriptions = inner_dict[\"description\"]\n            random.shuffle(descriptions)\n            real_descriptions = []\n            for cur_description in descriptions[:50]:\n                real_descriptions.append(cur_description[1])\n            results.append(\n                {\n                    \"id\": scene_id,\n                    \"sentences\": \"\\n\".join(real_descriptions),\n                }\n            )\n            counter += 1\n        return results\n"}
{"type": "source_file", "path": "mimic-it/convert-it/datasets/__init__.py", "content": ""}
{"type": "source_file", "path": "mimic-it/syphus/datasets/funqa.py", "content": "\"\"\"\nThis file contains the implementation of the FunQA datasets.\n\"\"\"\n\nimport json\nfrom abstract_dataset import AbstractDataset\n\n\nclass FunQA_translation(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"FunQA_translation\",\n        prompt_path: str = \"prompts/funqa_translation.json\",\n        query_inputs_path: str = \"annotations/funqa_annotation/annotations_tr.json\",\n    ):\n        super().__init__(name, prompt_path, query_inputs_path)\n\n    def _load_query_inputs(self, path: str) -> list[dict[str, str]]:\n        with open(path, \"r\") as f:\n            json_data = json.load(f)\n        q_dic = {\n            \"H1\": \"Find the video\" \"s humorous moment.\",\n            \"H2\": \"Description of the video\" \"s humorous moment.\",\n            \"H3\": \"Why is the whole video humorous.\",\n            \"H4\": \"Please provide a caption for the video.\",\n            \"C1\": \"Find the video\" \"s creative moment.\",\n            \"C2\": \"Description of the video\" \"s creative moment.\",\n            \"C3\": \"Why is the whole video creative.\",\n            \"C4\": \"Please provide a caption for the video.\",\n            \"C5\": \"Please score the video\" \"s creativity in [0-20].\",\n            \"M1\": \"Find the video\" \"s magic moment.\",\n            \"M2\": \"Description of the video\" \"s magic moment.\",\n            \"M3\": \"Why is the whole video magic.\",\n        }\n        query_inputs = []\n        for item in json_data:\n            task = list(q_dic)[list(q_dic.values()).index(item[\"instruction\"])]\n            if task[-1] == \"1\" or task[-1] == \"5\":\n                continue\n            query_inputs.append(\n                {\n                    \"id\": item[\"visual_input\"].split(\"/\")[-1] + \"_\" + task,\n                    \"sentences\": item[\"output\"],\n                }\n            )\n        return query_inputs\n\n\nclass FunQA_mcqa(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"DenseCaptions\",\n        prompt_path: str = \"prompts/funqa_mcqa.json\",\n        query_inputs_path: str = \"annotations/funqa_annotation/annotations_mcqa.json\",\n    ):\n        super().__init__(name, prompt_path, query_inputs_path)\n\n    def _load_query_inputs(self, path: str) -> list[dict[str, str]]:\n        with open(path, \"r\") as f:\n            json_data = json.load(f)\n        q_dic = {\n            \"H1\": \"Find the video\" \"s humorous moment.\",\n            \"H2\": \"Description of the video\" \"s humorous moment.\",\n            \"H3\": \"Why is the whole video humorous.\",\n            \"H4\": \"Please provide a caption for the video.\",\n            \"C1\": \"Find the video\" \"s creative moment.\",\n            \"C2\": \"Description of the video\" \"s creative moment.\",\n            \"C3\": \"Why is the whole video creative.\",\n            \"C4\": \"Please provide a caption for the video.\",\n            \"C5\": \"Please score the video\" \"s creativity in [0-20].\",\n            \"M1\": \"Find the video\" \"s magic moment.\",\n            \"M2\": \"Description of the video\" \"s magic moment.\",\n            \"M3\": \"Why is the whole video magic.\",\n        }\n        query_inputs = []\n\n        for item in json_data:\n            task = list(q_dic)[list(q_dic.values()).index(item[\"instruction\"])]\n            if task[-1] == \"2\":\n                now_query_input = \"description: \" + item[\"output\"] + \"\\n\"\n                continue\n            elif task[-1] == \"3\":\n                now_query_input += \"counter-intuitive reason: \" + item[\"output\"]\n                query_inputs.append(\n                    {\n                        \"id\": item[\"visual_input\"].split(\"/\")[-1],\n                        \"sentences\": now_query_input,\n                    }\n                )\n        return query_inputs\n\n\nclass FunQA_dia(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"DenseCaptions\",\n        prompt_path: str = \"prompts/funqa_dia.json\",\n        query_inputs_path: str = \"annotations/funqa_annotation/annotations_dia.json\",\n    ):\n        super().__init__(name, prompt_path, query_inputs_path)\n\n    def _load_query_inputs(self, path: str) -> list[dict[str, str]]:\n        with open(path, \"r\") as f:\n            json_data = json.load(f)\n        q_dic = {\n            \"H1\": \"Find the video\" \"s humorous moment.\",\n            \"H2\": \"Description of the video\" \"s humorous moment.\",\n            \"H3\": \"Why is the whole video humorous.\",\n            \"H4\": \"Please provide a caption for the video.\",\n            \"C1\": \"Find the video\" \"s creative moment.\",\n            \"C2\": \"Description of the video\" \"s creative moment.\",\n            \"C3\": \"Why is the whole video creative.\",\n            \"C4\": \"Please provide a caption for the video.\",\n            \"C5\": \"Please score the video\" \"s creativity in [0-20].\",\n            \"M1\": \"Find the video\" \"s magic moment.\",\n            \"M2\": \"Description of the video\" \"s magic moment.\",\n            \"M3\": \"Why is the whole video magic.\",\n        }\n        query_inputs = []\n\n        for item in json_data:\n            task = list(q_dic)[list(q_dic.values()).index(item[\"instruction\"])]\n            if task[-1] == \"2\":\n                now_query_input = \"description: \" + item[\"output\"] + \"\\n\"\n                continue\n            elif task[-1] == \"3\":\n                now_query_input += \"counter-intuitive reason: \" + item[\"output\"]\n                query_inputs.append(\n                    {\n                        \"id\": item[\"visual_input\"].split(\"/\")[-1],\n                        \"sentences\": now_query_input,\n                    }\n                )\n        return query_inputs\n"}
{"type": "source_file", "path": "pipeline/benchmarks/__init__.py", "content": ""}
{"type": "source_file", "path": "mimic-it/convert-it/datasets/2d.py", "content": "import json\n\nfrom abstract_dataset import AbstractDataset\n\n\nclass Llava(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"Llava\",\n        short_name=\"LA\",\n        *,\n        image_root: str,\n        image_path: str,\n        num_threads: int,\n    ):\n        \"\"\"\n        Initializes a Llava in-context dataset.\n\n        Args:\n            name (str): The name of the dataset. Defaults to \"Llava\".\n            short_name (str): The short name of the dataset. Defaults to \"LA\".\n            image_root (str): The root path to the COCO image train split.\n            image_path (str): The path to the JSON file containing the dataset images.\n                              The images can be downloaded from:\n                              https://drive.google.com/file/d/1OVb4_3Uec_xbyUk90aWC6LFpKsIOtR7v/view?usp=sharing.\n            num_threads (int): The number of threads to use for processing the images.\n        \"\"\"\n        self.image_root = image_root\n        super().__init__(name, short_name, image_path, num_threads)\n\n    def _load_images(self, image_path: str, num_thread: int) -> dict[str, bytes]:\n        \"\"\"\n        Loads the images from the dataset.\n\n        Args:\n            image_path (str): The path to the JSON file containing the dataset images.\n            num_threads (int): The number of threads to use for processing the images.\n\n        Returns:\n            dict[str, bytes]: A dictionary where the keys are image identifiers and the values are bytes objects representing the images.\n        \"\"\"\n\n        def read_image(file_name) -> bytes:\n            with open(file_name, \"rb\") as f:\n                return f.read()\n\n        images = {}\n        with open(image_path) as f:\n            image_ids = json.load(f).keys()\n\n        for cur_image_id in image_ids:\n            images[cur_image_id] = read_image(f\"{self.image_root}/{cur_image_id}.jpg\")\n\n        return images\n"}
{"type": "source_file", "path": "mimic-it/convert-it/datasets/3d.py", "content": "from abstract_dataset import AbstractDataset\n\n\nclass SceneNavigation(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"SceneNavigation\",\n        short_name=\"SN\",\n        *,\n        image_path: str,\n        num_threads: int,\n    ):\n        \"\"\"\n        Initializes a SceneNavigation dataset.\n\n        Args:\n            name (str): The name of the dataset. Defaults to \"SceneNavigation\".\n            short_name (str): The short name of the dataset. Defaults to \"SN\".\n            image_path (str): The directory path of the folder named \"scannet_frames_25k\" obtained by downloading a compressed file from http://www.scan-net.org/ and extracting it.\n            num_threads (int): The number of threads to use for processing the images.\n        \"\"\"\n        super().__init__(name, short_name, image_path, num_threads)\n\n    def _load_images(self, image_path: str, num_thread: int) -> dict[str, bytes]:\n        \"\"\"\n        Loads the images from the dataset.\n\n        Args:\n            image_path (str): The path to the directory containing the images downloaded from http://www.scan-net.org/.\n            num_threads (int): The number of threads to use for processing the images.\n\n        Returns:\n            dict[str, bytes]: A dictionary where the keys are image identifiers and the values are byte strings representing the images.\n        \"\"\"\n        from datasets.utils.scene_navigation_utils import process_data\n\n        return process_data(image_path, num_thread)\n"}
{"type": "source_file", "path": "mimic-it/convert-it/datasets/utils/scene_navigation_utils.py", "content": "import os\n\nfrom glob import glob\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom image_utils import process_image\n\n\ndef process(cur_dir, img_root):\n    \"\"\"\n    Process images in a directory.\n\n    Args:\n        cur_dir (str): The name of the current directory.\n        img_root (str): The root directory of the images.\n\n    Returns:\n        dict: A dictionary containing processed images. The keys are unique identifiers\n        for each image, and the values are the processed images.\n\n    \"\"\"\n    root = os.path.join(img_root, cur_dir, \"color\")\n    file_list = os.listdir(root)\n    images = {}\n    for cur_file in file_list:\n        file_name = os.path.join(img_root, cur_dir, \"color\", cur_file)\n        with open(file_name, \"rb\") as f:\n            img = f.read()\n        image_id = f\"{cur_dir}_color_{cur_file[:-4]}\"\n        images[image_id] = process_image(img)\n    return images\n\n\ndef process_data(img_root: str, num_threads: int):\n    \"\"\"\n    Process images in parallel using multiple threads.\n\n    Args:\n        img_root (str): The root directory of the images.\n        num_threads (int): The number of threads to use for parallel processing.\n\n    Returns:\n        dict: A dictionary containing processed images. The keys are unique identifiers\n        for each image, and the values are the processed images.\n\n    \"\"\"\n    keys_dir = glob(os.path.join(img_root, \"scene*_00\"))\n    keys = list(map(os.path.basename, keys_dir))\n    all_images = {}\n    process_bar = tqdm(total=len(keys), unit=\"image\", desc=\"Loading images\")\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        for images in executor.map(process, keys, [img_root] * len(keys)):\n            all_images.update(images)\n            process_bar.update()\n    process_bar.close()\n    return all_images\n"}
{"type": "source_file", "path": "mimic-it/convert-it/datasets/fpv.py", "content": "import os\n\nfrom glob import glob\n\nfrom abstract_dataset import AbstractDataset\nfrom image_utils import frame_video\n\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\n\n\nclass EGO4D(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"EGO4D\",\n        short_name=\"E4D\",\n        *,\n        image_path: str,\n        num_threads: int,\n    ):\n        \"\"\"\n        Initializes an EGO4D dataset.\n\n        Args:\n            name (str): The name of the dataset. Defaults to \"EGO4D\".\n            short_name (str): The short name of the dataset. Defaults to \"E4D\".\n            image_path (str): The directory path of the folder downloaded from https://ego4d-data.org/#download.\n            num_threads (int): The number of threads to use for processing the images.\n        \"\"\"\n        super().__init__(name, short_name, image_path, num_threads)\n\n    def _load_images(self, image_path: str, num_thread: int) -> dict[str, bytes]:\n        \"\"\"\n        Loads the images from the dataset.\n\n        Args:\n            image_path (str): The path to the directory containing the images downloaded from https://ego4d-data.org/#download.\n            num_threads (int): The number of threads to use for processing the images.\n\n        Returns:\n            dict[str, bytes]: A dictionary where the keys are image identifiers and the values are image bytes.\n\n        Raises:\n            FileNotFoundError: If the specified image path does not exist.\n        \"\"\"\n        video_paths = glob(os.path.join(image_path, \"*\"))\n\n        def get_image(video_path):\n            images = frame_video(video_path)\n            images_dict = {}\n            video_name = os.path.basename(video_path).split(\".\")[0]\n            for index, image in enumerate(images):\n                images_dict[f\"{video_name}_{index:08d}\"] = image\n            return images_dict\n\n        final_images_dict = {}\n\n        with ThreadPoolExecutor(max_workers=num_thread) as executor:\n            process_bar = tqdm(\n                total=len(video_paths),\n                unit=\"video\",\n                desc=\"Processing videos into images\",\n            )\n            for images_dict in executor.map(get_image, video_paths):\n                final_images_dict.update(images_dict)\n                process_bar.update()\n            process_bar.close()\n\n        return final_images_dict\n"}
{"type": "source_file", "path": "mimic-it/syphus/datasets/fpv.py", "content": "import json\nimport random\nfrom typing import List, Dict\nimport re\nfrom abstract_dataset import AbstractDataset\n\n\nclass SceneNavigation(AbstractDataset):\n    \"\"\"\n    Implementation of the SceneNavigation dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"SceneNavigation\",\n        in_context_path: str = \"prompts/scene_navigation.json\",\n        query_inputs_path: str = \"annotations/scene_navigation/scan_info.json\",\n    ):\n        super().__init__(name, in_context_path, query_inputs_path)\n\n    def _load_query_inputs(self, path: str) -> List[Dict[str, str]]:\n        \"\"\"\n        Load the query inputs from the given path and return them as a list of dictionaries.\n        \"\"\"\n        with open(path, \"r\") as f:\n            query_inputs = json.load(f)\n\n        results = []\n        for scene_id, inner_dict in query_inputs.items():\n            descriptions = inner_dict[\"description\"]\n            random.shuffle(descriptions)\n            formatted_descriptions = [cur_description[1] for cur_description in descriptions[:50]]\n            results.append(\n                {\n                    \"id\": scene_id,\n                    \"sentences\": \"\\n\".join(formatted_descriptions),\n                }\n            )\n\n        return results\n\n\nclass EGO4D(AbstractDataset):\n    \"\"\"\n    Implementation of the EGO4D dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"EGO4D\",\n        in_context_path: str = \"prompts/ego4d.json\",\n        query_inputs_path: str = \"annotations/ego4d/processed_all_anns.json\",\n    ):\n        super().__init__(name, in_context_path, query_inputs_path)\n\n    def _get_restrict_words(self):\n        # sample = 'You are not allowed to include exact timestamps. For example, timestamp 107.7 is not allowed in your outputs. Do not mention \"Based on the description\". You could say \"According what I observed\".\\nYou are not allowed to include exact timestamps in your outputs. For example, timestamp 107.7 is not allowed in your outputs. Do not mention \"Based on the descriptions\". You could say \"According what I observed\".\\nYou are not allowed to mention woman X, woman Y, man X, man Y, Person A, Person B, in your outputs, STRICTLY! You are only allowed to refer them as \"the person\" or \"a person\". Also remember, person C or C both refer to the cameraman, or the user, so you cannot use the word cameraman in your questions or answers. Since cameraman is user, you should refer cameraman as \"I\" in the questions, and \"you\" in the answers.\\nPlease ask at least 6 questions. Questions should be the first-person view (I or me). Questions that are valuable and suggestive are preferable, get rid of simple questions like \"what do I see in the video?\". Also remember what you see is the real world, so do not refer what you see as \"videos\". For example, it is unnatural to have the question \"Do you see any flotation devices in the video that I could use for support?\", and the better question is \"Do you see any flotation devices around that I could use for support?\". Also, make sure the generated questions and answers are concise.'\n\n        sample = \"Remember, in your responses, avoid directly referencing specific timestamps, such as 'timestamp 107.7'. Instead, refer to events or objects observed in the sequence of events. Rather than stating 'Based on the descriptions', you should phrase it as 'According to what I observed' to emphasize the perspective of an observing AI. You are not allowed to mention woman X, woman Y, man X, man Y, Person A, Person B, in your outputs, STRICTLY! You are only allowed to refer them as 'the person' or 'a person'. Additionally, 'Person C' or 'C' are synonymous with the user or the individual wearing the AR glasses. Avoid using the term 'cameraman'; instead, refer to the user as 'I' in questions and 'you' in answers. Strive to produce at least six questions in your output, maintaining a first-person perspective (using 'I' or 'me'). Prioritize generating valuable and suggestive questions over simplistic ones like 'what do I see?'. Keep in mind that what you are observing is the real world, not a video, so avoid referring to it as 'footage' or 'video', but use some words like 'my observation' or 'what I saw'. For instance, a question like 'Do you see any flotation devices in the video that I could use for support?' should be replaced with 'Do you see any flotation devices around that I could use for support?'. Lastly, ensure the questions and answers you generate are concise and succinct for clear communication.\"\n        return sample\n\n    def _load_query_inputs(self, path: str) -> List[Dict[str, str]]:\n        \"\"\"\n        Load the query inputs from the given path and return them as a list of dictionaries.\n        \"\"\"\n        with open(path, \"r\") as f:\n            query_inputs = json.load(f)\n\n        results = []\n        for video_name, video_data in query_inputs.items():\n            processed_timestamps = set()\n            for clip_id, clip_data in enumerate(video_data[\"clips\"]):\n                narrations = clip_data.get(\"narrations\", [])\n                formatted_descriptions = []\n                for narration in narrations:\n                    timestamp = narration.get(\"time\", 0)\n                    rounded_timestamp = round(timestamp)\n                    description = narration[\"text\"]\n                    dense_caption = narration[\"object_description\"]\n                    dense_caption = \"; \".join(dense_caption)\n                    if rounded_timestamp in processed_timestamps:\n                        formatted_descriptions.append(f\"description: {description}\")\n                    else:\n                        processed_timestamps.add(rounded_timestamp)\n                        formatted_descriptions.append(f\"timestamp: {rounded_timestamp}\\ndescription: {description}\\nobjects: {dense_caption}\")\n\n                formatted_descriptions = \"\\n\".join(formatted_descriptions)\n                formatted_descriptions = formatted_descriptions + \"\\n\" + self._get_restrict_words()\n                filled_clip_id = str(clip_id).zfill(6)\n                results.append(\n                    {\n                        \"id\": f\"{video_name}_clip{filled_clip_id}\",\n                        \"sentences\": formatted_descriptions,\n                    }\n                )\n\n        return results\n"}
{"type": "source_file", "path": "mimic-it/syphus/abstract_dataset.py", "content": "from abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Union\nimport importlib\nimport json\n\nAVAILABLE_DATASETS: List[str] = [\n    \"change.SpotTheDifference\",\n    \"change.CocoSpotTheDifference\",\n    \"video.DenseCaptions\",\n    \"video.TVCaptions\",\n    \"video.VisualStoryTelling\",\n    \"3d.SceneNavigation\",\n    \"funqa.FunQA_translation\",\n    \"funqa.FunQA_mcqa\",\n    \"funqa.FunQA_dia\",\n    \"fpv.EGO4D\",\n    \"translate.Translation\",\n]\n\n\nclass AbstractDataset(ABC):\n    def __init__(self, name: str, prompt_path: str, query_inputs_path: str):\n        \"\"\"Constructor.\"\"\"\n        self.name: str = name\n        self.prompt: Dict[str, Union[str, List[Dict[str, Union[str, List[Dict[str, str]]]]]]] = self._load_prompt(prompt_path)\n        self.query_inputs: List[str] = self._load_query_inputs(query_inputs_path)\n\n    def _load_prompt(self, path: str) -> Dict[str, Union[str, List[Dict[str, Union[str, List[Dict[str, str]]]]]]]:\n        with open(path, \"r\") as f:\n            json_data: Dict[str, Any] = json.load(f)\n        in_context: List[Dict[str, Union[str, List[Dict[str, str]]]]] = json_data[\"in_context\"].copy()\n        for n, conv in enumerate(json_data[\"in_context\"]):\n            role, content = conv[\"role\"], conv[\"content\"]\n            # we need to convert the QA pair into a string\n            if role == \"assistant\":\n                content_string = \"\"\n                if isinstance(content, str):\n                    content_string = content\n                else:\n                    for qa_pair in content:\n                        for prefix, text in qa_pair.items():\n                            content_string += prefix + \": \" + text + \"\\n\"\n            elif role == \"user\":\n                content_string = content\n            else:\n                raise ValueError(\"wrong role. Only user and assistant are allowed.\")\n            in_context[n] = {\"role\": role, \"content\": content_string}\n        results: Dict[str, Union[str, List[Dict[str, Union[str, List[Dict[str, str]]]]]]] = {\n            \"system_message\": json_data[\"system_message\"],\n            \"in_context\": in_context,\n        }\n        return results\n\n    @abstractmethod\n    def _load_query_inputs(self, path: str) -> List[str]:\n        \"\"\"\n        Load the query_inputs from the given path.\n        \"\"\"\n        pass\n\n    def __getitem__(self, index: int) -> Dict[str, Any]:\n        \"\"\"\n        Return the item at the given index as a dictionary.\n        \"\"\"\n        return self.data[index]\n\n    def __iter__(self) -> \"AbstractDataset\":\n        self.index = 0\n        return self\n\n    def __next__(\n        self,\n    ) -> Dict[str, Union[str, List[Dict[str, Union[str, List[Dict[str, str]]]]]]]:\n        if self.index < len(self.query_inputs):\n            outputs = {\n                \"system_messages\": self.prompt[\"system_message\"],\n                \"in_context\": self.prompt[\"in_context\"],\n                \"query_input\": self.query_inputs[self.index],\n            }\n            self.index += 1\n            return outputs\n        raise StopIteration\n\n    def __len__(self) -> int:\n        return len(self.query_inputs)\n\n    def __str__(self) -> str:\n        return f\"{self.name} dataset\"\n\n\ndef get_dataset_by_path(path: str, dataset_args: dict[str, str]) -> AbstractDataset:\n    assert path in AVAILABLE_DATASETS, f\"{path} is not an available dataset.\"\n    module_path, dataset_name = path.split(\".\")\n    module_path = \"datasets.\" + module_path\n\n    # Import the module and load the class\n    imported_module = importlib.import_module(module_path)\n    dataset_class = getattr(imported_module, dataset_name)\n\n    # TODO:remove later, Print the imported class for debugging\n    print(f\"Imported class: {dataset_class}\")\n\n    # Instantiate the class and return the instance\n    return dataset_class(**dataset_args)\n\n\ndef get_available_datasets() -> List[str]:\n    return AVAILABLE_DATASETS\n"}
{"type": "source_file", "path": "mimic-it/syphus/datasets/translate.py", "content": "import importlib\nimport json\n\nfrom abstract_dataset import AbstractDataset\n\n\nclass TranslationDataset(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"Translations\",\n        prompt_path: str = \"prompts.translation_prompt\",\n        query_inputs_path: str = None,\n    ):\n        super().__init__(name, prompt_path, query_inputs_path)\n\n    def _load_prompt(self, path: str) -> dict[str, str]:\n        prompt_file = importlib.import_module(path)\n        return {\n            \"system_message\": prompt_file.system_message,\n            \"in_context\": prompt_file.in_context,\n        }\n\n    def _load_query_inputs(self, path: str) -> list[str]:\n        with open(path, \"r\") as f:\n            json_data = json.load(f)[\"data\"]\n        temp = []\n        for file_id, i in json_data.items():\n            all_string = \"<a>\" + i[\"instruction\"] + \"</a>\" + \"<b>\" + i[\"answer\"] + \"</b>\"\n            temp.extend([{\"id\": file_id, \"sentences\": all_string}])\n        return temp\n"}
{"type": "source_file", "path": "mimic-it/syphus/main.py", "content": "import os\nimport argparse\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import Dict, List, Union\n\nimport openai\nfrom tqdm import tqdm\n\nfrom abstract_dataset import get_dataset_by_path\nfrom file_utils import (\n    save_query_json,\n    export_output_json,\n    format_output,\n    query_gpt,\n)\n\n\ndef task(inputs: Dict[str, Union[str, Dict[str, Union[str, int]]]]) -> Dict[str, Union[Dict[str, int], List[str]]]:\n    global dataset_name\n    try:\n        gpt_output, file_id = query_gpt(inputs, dataset_name)\n        tokens = dict(gpt_output[\"usage\"])\n        valid_outputs, invalid_outputs = format_output(gpt_output[\"choices\"][0][\"message\"][\"content\"], file_id, dataset_name)\n        result = {\n            \"tokens\": tokens,\n            \"valid_outputs\": valid_outputs,\n            \"invalid_outputs\": invalid_outputs,\n        }\n    except Exception as e:\n        result = {\"error_message\": str(e)}\n    return result\n\n\nif __name__ == \"__main__\":\n    openai.api_type = os.environ.get(\"OPENAI_API_TYPE\", \"local\")\n    openai.api_base = os.environ.get(\"OPENAI_API_BASE\", \"http://localhost:8000\")\n    openai.api_version = os.environ.get(\"OPENAI_API_VERSION\", \"2020-04-01\")\n    openai.api_key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--name\", type=str, required=True, help=\"Path to the dataset class.\")\n    parser.add_argument(\"--num_threads\", type=int, default=8, help=\"Number of threads.\")\n    parser.add_argument(\"--slice_start\", type=int, default=0, help=\"Dryrun test, on querying N samples.\")\n    parser.add_argument(\"--slice_end\", type=int, default=-1, help=\"Dryrun test, on querying N samples.\")\n    parser.add_argument(\"--random_sample\", action=\"store_true\", help=\"Random sample.\")\n    parser.add_argument(\"--dataset_version\", default=\"v1\", help=\"Dataset version.\")\n    parser.add_argument(\"--prompt_path\", help=\"Path to the prompt file.\")\n    parser.add_argument(\"--query_inputs_path\", \"-in\", help=\"Path to the query input file.\")\n\n    args = parser.parse_args()\n    dataset_args = {}\n    if args.prompt_path is not None:\n        dataset_args[\"prompt_path\"] = args.prompt_path\n    if args.query_inputs_path is not None:\n        dataset_args[\"query_inputs_path\"] = args.query_inputs_path\n    dataset = get_dataset_by_path(args.name, dataset_args)\n    dataset_name = args.name\n    dataset = list(dataset)\n    if args.random_sample:\n        import random\n\n        random.shuffle(dataset)\n    if args.slice_end > 0:\n        dataset = dataset[args.slice_start : args.slice_end]\n    results = []\n    query_inputs = []\n    start_time = time.time()\n\n    if args.num_threads == 0:\n        progress_bar = tqdm(total=len(dataset), unit=\"task\")\n        for n, d in enumerate(dataset):\n            query_inputs.append(d[\"query_input\"])\n            results.append(task(d))\n            progress_bar.update(1)\n        progress_bar.close()\n    else:\n        progress_bar = tqdm(total=len(dataset))\n\n        def update_progress(_):\n            progress_bar.update(1)\n\n        # Submit the tasks to the thread pool\n        progress_bar = tqdm(total=len(dataset), unit=\"task\")\n        batch_size = args.num_threads\n        for i in range(0, len(dataset), batch_size):\n            # Create a thread pool with the specified number of threads\n            with ThreadPoolExecutor(max_workers=args.num_threads) as executor:\n                current_batch = dataset[i : i + batch_size]\n                futures = [executor.submit(task, d) for d in current_batch]\n                query_inputs.extend([d[\"query_input\"] for d in current_batch])\n                # Retrieve the results as they become available\n                for future, num in zip(futures, dataset):\n                    results.append(future.result())\n                    progress_bar.update(1)\n        progress_bar.close()\n\n    duration = time.time() - start_time\n    save_query_json(query_inputs, f\"{dataset_name}_{args.dataset_version}\")\n    export_output_json(results, f\"{dataset_name}_{args.dataset_version}\", duration)\n    print(f\"Total time: {duration:.2f}s\")\n"}
{"type": "source_file", "path": "mimic-it/convert-it/datasets/video.py", "content": "import json\nimport os\nimport gc\n\nfrom abstract_dataset import AbstractDataset\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\nfrom glob import glob\nfrom image_utils import frame_video, get_image_name, resize_image, image_to_bytes\nfrom natsort import natsorted\n\n\nclass DenseCaptions(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"DenseCaptions\",\n        short_name=\"DC\",\n        *,\n        image_path: str,\n        num_threads: int,\n    ):\n        \"\"\"\n        Initializes a DenseCaptions dataset.\n\n        Args:\n            name (str): The name of the dataset. Defaults to \"DenseCaptions\".\n            short_name (str): The short name of the dataset. Defaults to \"DC\".\n            image_path (str): The path to the directory containing the dataset images.\n            num_threads (int): The number of threads to use for processing the images.\n        \"\"\"\n        super().__init__(name, short_name, image_path, num_threads)\n\n    def _load_images(self, image_path: str, num_thread: int) -> dict[str, bytes]:\n        \"\"\"\n        Loads the images from the dataset.\n\n        Args:\n            image_path (str): The path to the directory containing the dataset images.\n            num_threads (int): The number of threads to use for processing the images.\n\n        Returns:\n            dict[str, bytes]: A dictionary where the keys are image identifiers and the values are image bytes.\n        \"\"\"\n        videos = glob(f\"{image_path}/*.mp4\")\n        if len(videos) <= 100:\n            raise ValueError(\"Not enough videos in the dataset, please check the path.\")\n        with ThreadPoolExecutor(max_workers=num_thread) as executor:\n            results = {}\n            process_bar = tqdm(total=len(videos), desc=\"Processing videos into images\", unit=\"video\")\n            cnt = 0\n            for video, framed_results in executor.map(lambda x: (get_image_name(x), frame_video(x)), videos):\n                for index, result in enumerate(framed_results):\n                    # print(\"video\", video)\n                    name = video + \"_\" + str(index).zfill(4)\n                    results[name] = result\n                process_bar.update(1)\n\n                cnt = cnt + 1\n                if cnt % 100 == 0:\n                    gc.collect()\n            process_bar.close()\n            return results\n\n\nclass VisualStoryTelling(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"VisualStroryTelling\",\n        short_name=\"VST\",\n        *,\n        image_path: str,\n        num_threads: int,\n    ):\n        \"\"\"\n        Initializes a VisualStoryTelling dataset.\n\n        Args:\n            name (str): The name of the dataset. Defaults to \"VisualStoryTelling\".\n            short_name (str): The short name of the dataset. Defaults to \"VST\".\n            image_path (str): The path to the JSON file containing the dataset images.\n            num_threads (int): The number of threads to use for processing the images.\n        \"\"\"\n        super().__init__(name, short_name, image_path, num_threads)\n\n    def _load_images(self, image_path: str, num_thread: int) -> dict[str, bytes]:\n        \"\"\"\n        Loads the images from the VisualStoryTelling dataset.\n\n        Args:\n            image_path (str): The path to the JSON file containing the dataset images.\n            num_threads (int): The number of threads to use for processing the images.\n\n        Returns:\n            dict[str, bytes]: A dictionary of images, where the keys are the IDs of the images.\n        \"\"\"\n        from datasets.utils.visual_story_telling_utils import download\n\n        with open(image_path, \"r\") as f:\n            data = json.load(f)\n        return download(data[\"images\"], num_thread)\n\n\nclass TVCaptions(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"TVCaptions\",\n        short_name=\"TVC\",\n        *,\n        image_path: str,\n        num_threads: int,\n    ):\n        \"\"\"\n        Initializes a TVCaptions dataset.\n\n        Args:\n            name (str): The name of the dataset. Defaults to \"TVCaptions\".\n            short_name (str): The short name of the dataset. Defaults to \"TVC\".\n            image_path (str): The path to the directory containing the dataset images, downloaded from https://tvqa.cs.unc.edu/download_tvqa.html#tvqa-download-4\n            num_threads (int): The number of threads to use for processing the images.\n        \"\"\"\n        super().__init__(name, short_name, image_path, num_threads)\n\n    def _load_images(self, image_path: str, num_thread: int) -> dict[str, bytes]:\n        \"\"\"\n        Load the images from the TVCaptions dataset.\n\n        Args:\n            image_path (str): The path to the directory containing the dataset images, downloaded from https://tvqa.cs.unc.edu/download_tvqa.html#tvqa-download-4.\n            num_threads (int): The number of threads to use for loading the images.\n\n        Returns:\n            Dict[str, Image.Image]: A dictionary of images, where the keys are the IDs of the images.\n        \"\"\"\n\n        def get_frames(directory, frames=16):\n            \"\"\"\n            Generate a list of image filenames.\n\n            Args:\n                directory (str): The directory path containing the frames.\n                frames (int): The number of frames to sample. Defaults to 16.\n\n            Returns:\n                list[str]: A list of image filenames.\n            \"\"\"\n\n            # Generate a list of image filenames\n            image_filenames = natsorted(glob(os.path.join(directory, \"*\")))\n\n            # Calculate the stride length to achieve an average sample\n            stride = max(1, len(image_filenames) // frames)\n\n            # Initialize the starting index for sampling\n            start_index = stride // 2\n\n            # Sample 16 images evenly\n            sampled_images = [image_filenames[i] for i in range(start_index, len(image_filenames), stride)]\n\n            return sampled_images\n\n        def get_images(frames, frame_name, clip_name):\n            \"\"\"\n            Load and resize images from frames.\n\n            Args:\n                frames (list[str]): List of image filenames.\n                frame_name (str): Name of the frame.\n                clip_name (str): Name of the clip.\n\n            Returns:\n                dict[str, bytes]: A dictionary where the keys are image identifiers and the values are image bytes.\n            \"\"\"\n\n            images = {}\n            for frame in frames:\n                image_name = os.path.basename(frame).split(\".\")[0]\n                # print(frame_name, clip_name, image_name)\n                if clip_name.startswith(frame_name):\n                    image_id = f\"{clip_name}_{image_name}\"\n                else:\n                    image_id = f\"{frame_name}_{clip_name}_{image_name}\"\n                # print(image_id)\n                with open(frame, \"rb\") as f:\n                    frame_bytes = f.read()\n                images[image_id] = resize_image(frame_bytes)\n            return images\n\n        frames = glob(os.path.join(image_path, \"*\"))\n        all_images = {}\n        for frame in frames:\n            frame_name = os.path.basename(frame).split(\"_\")[0]\n            clips = glob(os.path.join(frame, \"*\"))\n            progress_bar = tqdm(total=len(clips), desc=f\"Processing clips in {frame_name}\", unit=\"clip\")\n            with ThreadPoolExecutor(max_workers=num_thread) as executor:\n\n                def get_images_dict(clip):\n                    clip_name = os.path.basename(clip)\n                    frames = get_frames(clip)\n                    return get_images(frames, frame_name, clip_name)\n\n                for images in executor.map(get_images_dict, clips):\n                    all_images.update(images)\n                    progress_bar.update()\n                    # print(\"The type of all_images is\", type(all_images))\n                    # print(\"The types of the keys and values of all_images are\", type(list(all_images.keys())[0]), type(list(all_images.values())[0]))\n            progress_bar.close()\n\n        return all_images\n"}
{"type": "source_file", "path": "mimic-it/syphus/prompts/coco_spot_the_difference_prompt.py", "content": "system_message = \"\"\"You are an AI assistant playing the spot the difference game. You will be presented with several sentences that describe the images. The sentences include the captions of the image, the normalized bounding box coordinates and the name of objects shown in the image. The two images are sampled from COCO 2017 training set.\\n\\nYour response shall always contain two pairs of question and answer. Your generated questions should include asking the differences between the two images or the differences between the two similar objects in the image.\\n\\nThe output format must be \"Question: xxx\\nAnswer: xxx\". The question you generate should have definite and concrete answer according to the description. You may use diverse sentences to ask the questions but they should be only concentrating on difference. When generating answer, you should answer as if you are watching the images. You do not have to provide a specific answer and you should not ask for more details.\\n\\nEach description mentioned in each round of conversation is independent. You should not assume that the descriptions in a new round of conversation are related to the previous context.\"\"\"\n\nin_context = [\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"image a:\\nA smiling old lady holds a pizza on a plate.\\nA woman holding a plate with a pizza on it\\nA Woman carrying homemade pizza to the table.\\nA woman holding a pizza on a red plate.\\nA woman walking with a pan in her hands with a whole pizza on it. \\nperson: [64.72, 5.75, 414.2, 627.06]\\npizza: [51.2, 460.8, 206.31, 115.95]\\nmicrowave: [5.75, 136.63, 192.72, 181.21]\\nimage b:\\nA woman at a stainless steel counter making pizza.\\nA woman in the kitchen spreading dough next to a pizza.\\na woman in a blue shirt making some pizza dough and a pizza \\nA woman standing next to a pizza on top of a metal counter.\\nWoman making fresh gourmet pizzas in her kitchen\\nrefrigerator: [2.88, 44.34, 97.79, 568.09]\\nchair: [293.73, 260.8, 178.86, 113.34]\\ndining table: [46.02, 339.18, 433.98, 293.39]\\nperson: [43.15, 73.35, 353.79, 506.25]\\npizza: [146.5, 484.0, 325.0, 156.0]\\nbowl: [437.35, 338.81, 42.65, 36.28]\\nchair: [383.41, 170.61, 33.22, 48.35]\\n\"\"\",\n        \"role\": \"assistant\",\n        \"content\": \"\"\"Question: Can you see any difference between these two images?\\nAnswer:In the previous image, the woman is holding the pizza while the second woman is standing next to the pizza.\\nQuestion: What is the color different between these two women?\\nAnswer: The first woman is wearing a brown T-shirt while the second woman wearing a blue one.\"\"\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"image a:\\nA living room with a couch in front of a TV.\\nA small living room with shelving by the window, a small sofa and coffee table and a television\\nA living area with a couch, coffee table and a television.\\nA homey comfortable room with sun streaming through a window.\\nA living room with a sunny window, couch and a tv.\\ntv: [566.75, 216.47, 73.02, 122.84]\\nbottle: [381.1, 261.65, 24.53, 62.02]\\ncouch: [2.06, 254.97, 301.42, 217.8]\\ndining table: [313.98, 302.68, 172.8, 168.28]\\nremote: [347.3, 334.83, 63.96, 13.04]\\nbook: [269.83, 278.65, 56.07, 15.61]\\nbook: [215.29, 289.09, 45.63, 7.6]\\nbook: [217.13, 277.32, 53.15, 10.18]\\nbook: [491.59, 258.14, 11.29, 18.53]\\nbook: [272.36, 271.8, 52.12, 14.41]\\nbird: [112.28, 236.03, 13.72, 23.97]\\npotted plant: [380.79, 158.62, 100.95, 87.75]\\nbook: [265.48, 259.9, 41.03, 8.5]\\nbook: [403.92, 282.83, 45.88, 27.98]\\nbook: [215.96, 271.12, 43.02, 9.44]\\nbook: [482.55, 302.69, 20.27, 29.42]\\nbook: [396.62, 276.81, 46.59, 24.73]\\nbook: [263.74, 263.63, 47.82, 7.79]\\nbook: [275.38, 291.13, 53.9, 7.63]\\nvase: [516.25, 92.92, 35.0, 39.92]\\nremote: [612.91, 205.18, 27.09, 8.1]\\nbook: [466.5, 258.38, 13.22, 20.34]\\nbook: [114, 238, 424, 70]\\nimage b:\\nA living room filled with furniture and a book shelf filled with books.\\nthere is a very large bookcase with many books in this room\\na cluttered living room with book shelves and furniture.\\na room full of book cases couches and chairs \\nA room is cluttered with furniture and a large full bookcase.\\nchair: [192.56, 268.97, 136.96, 115.4]\\ncouch: [0.0, 239.17, 196.13, 235.36]\\nvase: [116.4, 71.22, 19.19, 25.25]\\nvase: [553.88, 20.63, 17.86, 30.32]\\nbowl: [437.55, 26.27, 37.64, 22.78]\\nbowl: [491.45, 31.35, 31.41, 19.93]\\nbowl: [358.66, 20.04, 39.75, 25.94]\\nbowl: [323.68, 31.25, 23.74, 14.44]\\nbowl: [266.96, 30.14, 41.25, 14.74]\\nbowl: [270.73, 16.69, 34.76, 14.48]\\nchair: [441.17, 203.87, 143.46, 184.44]\\nbook: [232.21, 73.25, 29.22, 2.89]\\nbook: [352.4, 138.35, 6.67, 33.76]\\nbook: [225.88, 132.57, 82.08, 37.57]\\nbook: [342.79, 183.47, 8.12, 34.17]\\nbook: [142.39, 226.24, 48.24, 13.45]\\nbook: [460.94, 101.87, 9.03, 25.95]\\nbook: [361.71, 138.4, 5.7, 35.35]\\nbook: [423.1, 177.21, 79.75, 41.71]\\nbook: [279.45, 60.46, 27.91, 9.91]\\nbook: [361.99, 57.36, 17.31, 5.37]\\nbook: [470.78, 58.71, 37.29, 18.99]\\nbook: [370.2, 183.3, 8.68, 34.73]\\nbowl: [407.42, 31.44, 35.19, 17.24]\\nbowl: [592.96, 31.65, 40.76, 11.33]\\nbook: [401.6, 137.36, 6.37, 35.67]\\npotted plant: [101.59, 23.28, 46.71, 61.16]\\ncouch: [574.0, 276.34, 66.0, 133.28]\\nbook: [141, 85, 368, 185]\\n\"\"\",\n        \"role\": \"assistant\",\n        \"content\": \"\"\"Question: What's different between these two living room?\\nAnswer: The first living room has a TV in front of the couch while the second living room has no TV.\\nQuestion: How about the bookshelf?\\nAnswer: The second bookshelf contains more books than the first one.\"\"\",\n    },\n    # \"role\": \"user\",\n    # \"content\": \"\"\"image a:\\nA young man is petting an elephants trunk.\\nA man stands in front of two elephants, touching the trunk of one.\\nA man with a backpack is touching a elephant's trunk.\\nA man standing beside an elephant with his hand on its trunk.\\nA man petting one of two elephants on the trunk.\\nperson: [206.78, 145.51, 312.08, 273.79]\\nperson: [26.64, 221.58, 36.26, 60.33]\\nperson: [6.7, 227.67, 31.96, 55.93]\\nperson: [495.31, 207.66, 15.58, 43.33]\\nperson: [544.94, 223.66, 9.85, 26.02]\\nperson: [584.98, 222.73, 12.68, 25.6]\\nelephant: [203.91, 2.87, 289.1, 380.05]\\nelephant: [0.0, 28.72, 226.88, 295.81]\\nbackpack: [361.5, 221.09, 124.86, 140.11]\\nelephant: [254.38, 237.56, 26.56, 45.03]\\nperson: [510.65, 230.67, 8.71, 19.34]\\nimage b:\\na man that is sitting on a fake zebra\\nA person sitting on top of a fake zebra. \\nA man with a hat riding on a giraffe carving\\na man sitting a on  a zebra statue \\nA man is sitting on a fake zebra\\nperson: [1.44, 0.72, 170.09, 585.23]\\nzebra: [4.96, 56.39, 464.7, 567.23]\\n\"\"\",\n]\n\nper_example_prefix = None\n"}
{"type": "source_file", "path": "mimic-it/syphus/datasets/change.py", "content": "\"\"\"\nThis file contains the implementation of the SpotTheDifference and CleverChange datasets.\n\"\"\"\n\nimport importlib\nimport json\n\nfrom abstract_dataset import AbstractDataset\n\n\nclass SpotTheDifference(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"SpotTheDifference\",\n        prompt_path: str = \"prompts/spot_the_difference.json\",\n        query_inputs_path: str = \"annotations/spot_the_difference/train.json\",\n    ):\n        super().__init__(name, prompt_path, query_inputs_path)\n\n    def _load_query_inputs(self, path: str) -> list[str]:\n        with open(path, \"r\") as f:\n            json_data = json.load(f)\n        results = []\n        for inner_dict in json_data:\n            file_id = inner_dict[\"img_id\"]\n            sentences = inner_dict[\"sentences\"]\n            results.append(\n                {\n                    \"id\": file_id,\n                    \"sentences\": \"\\n\".join(sentences),\n                }\n            )\n        return results\n\n\nclass CocoSpotTheDifference(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"CocoSpotTheDifference\",\n        prompt_path: str = \"prompts.coco_spot_the_difference_prompt\",\n        query_inputs_path: str = \"annotations/coco_spot_the_difference/csd_query.json\",\n    ):\n        super().__init__(name, prompt_path, query_inputs_path)\n\n    def _load_query_inputs(self, path: str) -> list[dict[str, str]]:\n        with open(path) as f:\n            json_data = json.load(f)\n        results = []\n        for file_id, inner_dict in json_data.items():\n            sentences = inner_dict[\"sentences\"]\n            results.append(\n                {\n                    \"id\": file_id,\n                    \"sentences\": sentences,\n                }\n            )\n        return results\n\n    def _load_prompt(self, path: str) -> dict[str, str]:\n        prompt_file = importlib.import_module(path)\n        return {\n            \"system_message\": prompt_file.system_message,\n            \"in_context\": prompt_file.in_context,\n        }\n"}
{"type": "source_file", "path": "pipeline/benchmarks/datasets/scienceqa.py", "content": "import os\nimport re\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom datasets import load_dataset\nfrom .base_eval_dataset import BaseEvalDataset\nimport pytz\nimport datetime\nimport json\n\nutc_plus_8 = pytz.timezone(\"Asia/Singapore\")  # You can also use 'Asia/Shanghai', 'Asia/Taipei', etc.\nutc_now = pytz.utc.localize(datetime.datetime.utcnow())\nutc_plus_8_time = utc_now.astimezone(utc_plus_8)\n\n\nclass ScienceQADataset(BaseEvalDataset):\n    def __init__(\n        self, data_path: str = \"Otter-AI/ScienceQA\", *, split=\"test\", cache_dir=None, default_output_path=\"./logs/ScienceQA\", batch=1, debug=False, prompt='Please answer the question in the following format: \"The answer is {A/B/C/D}\".'\n    ):\n        super().__init__(\"ScienceQADataset\", data_path, max_batch_size=batch)\n        self.split = split\n        self.data = load_dataset(data_path, split=self.split, cache_dir=cache_dir)\n        self.default_output_path = default_output_path\n        self.cur_datetime = utc_plus_8_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.debug = debug\n        self.prompt = prompt\n\n    def format_question(self, question, choices, answer):\n        len_choices = len(choices)\n        options = [chr(ord(\"A\") + i) for i in range(len_choices)]\n        answer = options[answer]\n        choices_dict = dict(zip(options, choices))\n        choices_str = \"\\n\".join([f\"{option}. {choice}\" for option, choice in zip(options, choices)])\n        return f\"{self.prompt}\\n{question}\\n{choices_str}\\n\", choices_dict, answer\n\n    def parse_pred_ans(self, pred_ans, options):\n        match = re.search(r\"The answer is ([A-D])\", pred_ans)\n        if match:\n            return match.group(1)\n        for c, option in options.items():\n            option = option.strip()\n            if option.upper() in pred_ans.upper():\n                return c\n        choices = set(options.keys())\n        for selection in choices:\n            if selection in pred_ans:\n                return selection\n        for selection in choices:\n            if selection in pred_ans.upper():\n                return selection\n        return \"other\"\n\n    def _evaluate(self, model, *, batch=1):\n        if not os.path.exists(self.default_output_path):\n            os.makedirs(self.default_output_path)\n\n        output_file = os.path.join(self.default_output_path, f\"{model.name}_scienceqa_eval_result_{self.cur_datetime}.json\")\n        result_file = os.path.join(self.default_output_path, f\"{model.name}_scienceqa_eval_score_{self.cur_datetime}.json\")\n        results = []\n\n        total = 0\n        total_correct = 0\n\n        for data in tqdm(self.data, desc=\"Evaluating\", total=len(self.data)):\n            question, choices_dict, answer = self.format_question(data[\"question\"], data[\"choices\"], data[\"answer\"])\n            output = model.generate(question, data[\"image\"])\n            phrased_output = self.parse_pred_ans(output, choices_dict)\n            correct = phrased_output == answer\n            if correct:\n                total_correct += 1\n            total += 1\n            results.append(\n                {\n                    \"question\": data[\"question\"],\n                    \"choices\": data[\"choices\"],\n                    \"answer\": answer,\n                    \"output\": output,\n                    \"prediction\": phrased_output,\n                    \"correct\": correct,\n                }\n            )\n            with open(output_file, \"w\") as f:\n                json.dump(results, f, indent=4)\n\n        score = total_correct / total\n        print(f\"ScienceQA Evaluator: Total: {total}\")\n        print(f\"ScienceQA Evaluator: Total correct: {total_correct}\")\n        print(f\"ScienceQA Evaluator: Score: {score}\")\n        with open(result_file, \"w\") as f:\n            final_score = {\n                \"score\": score,\n                \"total\": total,\n                \"correct\": total_correct,\n            }\n            json.dump(final_score, f, indent=4)\n\n        print(f\"ScienceQA Evaluator: Result saved to {os.path.abspath(output_file)}.\")\n        print(f\"ScienceQA Evaluator: Score saved to {os.path.abspath(result_file)}.\")\n\n\nif __name__ == \"__main__\":\n    dataset = ScienceQADataset(cache_dir=\"/data/pufanyi/cache\")\n    data = dataset.data\n    print(\"=============================\")\n    import json\n\n    print(json.dumps(data[1], indent=4))\n    print(\"=============================\")\n    print(build_prompt(dataset.data[1], \"QCM-ALE\"))\n    print(\"=============================\")\n"}
{"type": "source_file", "path": "pipeline/benchmarks/datasets/pope.py", "content": "import os\nimport datetime\nfrom tqdm import tqdm, trange\nfrom .base_eval_dataset import BaseEvalDataset\nfrom datasets import load_dataset\nimport json\nfrom typing import Union\n\n\nclass PopeDataset(BaseEvalDataset):\n    def __init__(\n        self,\n        data_path=\"Otter-AI/POPE\",\n        split=\"test\",\n        default_output_path=\"./logs/POPE\",\n        cache_dir=None,\n        batch_size=1,\n    ):\n        super().__init__(\"PopeDataset\", data_path, max_batch_size=batch_size)\n        print(\"Loading dataset from\", data_path)\n        self.data = load_dataset(data_path, split=split, cache_dir=cache_dir)\n        print(\"Dataset loaded\")\n        self.default_output_path = default_output_path\n        if not os.path.exists(default_output_path):\n            os.makedirs(default_output_path)\n        self.batch_gen_size = batch_size\n\n    def parse_pred(self, text):\n        if text.find(\".\") != -1:\n            text = text.split(\".\")[0]\n\n        text = text.replace(\",\", \"\").lower()\n        words = text.split(\" \")\n\n        if \"not\" in words or \"no\" in words:\n            return \"no\"\n        else:\n            return \"yes\"\n\n    def _evaluate(self, model, batch=1):\n        cur_datetime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n        output_path = os.path.join(self.default_output_path, f\"pope_{model.name}_test_submit_{cur_datetime}.json\")\n\n        metrics = {\n            \"adversarial\": {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0, \"yes_count\": 0, \"no_count\": 0},\n            \"popular\": {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0, \"yes_count\": 0, \"no_count\": 0},\n            \"random\": {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0, \"yes_count\": 0, \"no_count\": 0},\n            \"overall\": {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0, \"yes_count\": 0, \"no_count\": 0},\n        }\n\n        batch_size = self.batch_gen_size\n        num_batches = len(self.data) // batch_size + 1\n\n        with tqdm(total=len(self.data), desc=\"Evaluating\") as pbar:\n            for i in range(num_batches):\n                start_idx = i * batch_size\n                end_idx = (i + 1) * batch_size\n                batch_data = self.data[start_idx:end_idx]\n\n                batch_questions = batch_data[\"question\"]\n                batch_answers = batch_data[\"answer\"]\n                batch_images = batch_data[\"image\"]\n\n                # if model has batch_generate, use it\n                if hasattr(model, \"batch_generate\") and self.batch_gen_size > 1:\n                    batch_responses = model.batch_generate(batch_questions, batch_images)\n                else:\n                    batch_responses = [model.generate(question, image) for question, image in zip(batch_questions, batch_images)]\n\n                batch_preds = [self.parse_pred(response) for response in batch_responses]\n\n                for j in range(len(batch_preds)):\n                    answer = batch_answers[j]\n                    pred = batch_preds[j]\n                    category = batch_data[\"category\"][j]\n\n                    if answer == \"yes\":\n                        metrics[category][\"yes_count\"] += 1\n                        metrics[\"overall\"][\"yes_count\"] += 1\n                    else:\n                        metrics[category][\"no_count\"] += 1\n                        metrics[\"overall\"][\"no_count\"] += 1\n\n                    if pred == answer and pred == \"yes\":\n                        metrics[category][\"TP\"] += 1\n                        metrics[\"overall\"][\"TP\"] += 1\n                    elif pred == answer and pred == \"no\":\n                        metrics[category][\"TN\"] += 1\n                        metrics[\"overall\"][\"TN\"] += 1\n                    elif pred != answer and pred == \"yes\":\n                        metrics[category][\"FP\"] += 1\n                        metrics[\"overall\"][\"FP\"] += 1\n                    else:\n                        metrics[category][\"FN\"] += 1\n                        metrics[\"overall\"][\"FN\"] += 1\n\n                pbar.update(batch_size)\n\n        for category in metrics:\n            print(f\"----------- {category} -----------\")\n\n            TP = metrics[category][\"TP\"]\n            TN = metrics[category][\"TN\"]\n            FP = metrics[category][\"FP\"]\n            FN = metrics[category][\"FN\"]\n            yes_count = metrics[category][\"yes_count\"]\n            no_count = metrics[category][\"no_count\"]\n\n            print(\"TP\\tFP\\tTN\\tFN\\t\")\n            print(\"{}\\t{}\\t{}\\t{}\".format(TP, FP, TN, FN))\n\n            if TP + FP == 0:\n                metrics[category][\"precision\"] = precision = 0\n            else:\n                metrics[category][\"precision\"] = precision = float(TP) / float(TP + FP)\n\n            if TP + FN == 0:\n                metrics[category][\"recall\"] = recall = 0\n            else:\n                metrics[category][\"recall\"] = recall = float(TP) / float(TP + FN)\n\n            if precision + recall == 0:\n                metrics[category][\"f1\"] = f1 = 0\n            else:\n                metrics[category][\"f1\"] = f1 = 2 * precision * recall / float(precision + recall)\n\n            metrics[category][\"acc\"] = acc = float(TP + TN) / float(TP + TN + FP + FN)\n\n            if yes_count + no_count == 0:\n                metrics[category][\"yes_ratio\"] = yes_ratio = 0\n            else:\n                metrics[category][\"yes_ratio\"] = yes_ratio = yes_count / float(yes_count + no_count)\n\n            print(\"Accuracy: {}\".format(acc))\n            print(\"Precision: {}\".format(precision))\n            print(\"Recall: {}\".format(recall))\n            print(\"F1 score: {}\".format(f1))\n            print(\"Yes ratio: {}\".format(yes_ratio))\n\n        print(f\"----------- overall -----------\")\n\n        TP = metrics[\"overall\"][\"TP\"]\n        TN = metrics[\"overall\"][\"TN\"]\n        FP = metrics[\"overall\"][\"FP\"]\n        FN = metrics[\"overall\"][\"FN\"]\n        yes_count = metrics[\"overall\"][\"yes_count\"]\n        no_count = metrics[\"overall\"][\"no_count\"]\n\n        print(\"TP\\tFP\\tTN\\tFN\\t\")\n        print(\"{}\\t{}\\t{}\\t{}\".format(TP, FP, TN, FN))\n\n        metrics[\"overall\"][\"precision\"] = precision = float(TP) / float(TP + FP)\n        metrics[\"overall\"][\"recall\"] = recall = float(TP) / float(TP + FN)\n        metrics[\"overall\"][\"f1\"] = f1 = 2 * precision * recall / float(precision + recall)\n        metrics[\"overall\"][\"acc\"] = acc = float(TP + TN) / float(TP + TN + FP + FN)\n        metrics[\"overall\"][\"yes_ratio\"] = yes_ratio = float(yes_count) / float(yes_count + no_count)\n\n        print(\"Accuracy: {}\".format(acc))\n        print(\"Precision: {}\".format(precision))\n        print(\"Recall: {}\".format(recall))\n        print(\"F1 score: {}\".format(f1))\n        print(\"Yes ratio: {}\".format(yes_ratio))\n\n        output_f = open(output_path, \"a\")\n        output_f.write(json.dumps(metrics) + \"\\n\")\n        output_f.close()\n        return metrics\n"}
{"type": "source_file", "path": "mimic-it/convert-it/main.py", "content": "import argparse\nimport orjson\n\nfrom abstract_dataset import get_dataset_by_path\nfrom image_utils import get_json_data_generator, create_folder\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--name\", type=str, required=True, help=\"Path to the dataset class.\")\n    parser.add_argument(\"--num_threads\", type=int, default=8, help=\"Number of threads.\")\n    parser.add_argument(\"--image_path\", help=\"Path to the prompt file.\")\n    parser.add_argument(\"--image_root\", default=None, help=\"Path to the image root.\")\n\n    args = parser.parse_args()\n    dataset_args = {}\n    if args.image_path is not None:\n        dataset_args[\"image_path\"] = args.image_path\n    if args.num_threads is not None:\n        dataset_args[\"num_threads\"] = args.num_threads\n    if args.image_root is not None:\n        dataset_args[\"image_root\"] = args.image_root\n    dataset = get_dataset_by_path(args.name, dataset_args)\n    dataset_short_name = dataset.short_name\n    dataset = dict(dataset)\n    create_folder(\"output\")\n\n    # Open the output JSON file in text mode, since we'll be writing strings\n    with open(f\"output/{dataset_short_name}.json\", \"w\") as f:\n        # Write the opening brace for the JSON object\n        f.write(\"{\")\n\n        # Use a flag to track whether a comma is needed before the next key-value pair\n        need_comma = False\n\n        # Iterate over the generator, which yields key-value pairs one at a time\n        for image_key, base64_data in get_json_data_generator(dataset, dataset_short_name, args.num_threads):\n            # Write a comma before the next key-value pair if needed\n            if need_comma:\n                f.write(\", \")\n\n            # Write the key-value pair as a string to the file\n            f.write(f'\"{image_key}\": \"{base64_data}\"')\n\n            # Set the flag to True so that a comma is written before the next key-value pair\n            need_comma = True\n\n        # Write the closing brace for the JSON object\n        f.write(\"}\")\n"}
{"type": "source_file", "path": "pipeline/benchmarks/datasets/seedbench.py", "content": "import numpy as np\nfrom tqdm import tqdm\nfrom .base_eval_dataset import BaseEvalDataset\nfrom datasets import load_dataset\nimport json\nimport os\nimport datetime\n\n\nclass SEEDBenchDataset(BaseEvalDataset):\n    def __init__(self, data_path: str = \"Otter-AI/SEEDBench\", split=\"test\", default_output_path=\"./logs\", cache_dir=None):\n        super().__init__(\"SEEDBenchDataset\", data_path)\n        print(\"Loading dataset from\", data_path)\n        self.data = load_dataset(data_path, split=split, cache_dir=cache_dir)\n        self.default_output_path = default_output_path\n        if not os.path.exists(default_output_path):\n            os.makedirs(default_output_path)\n\n    def _evaluate(self, model):\n        count = 0\n        num_correct = 0\n        cur_datetime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n        output_path = os.path.join(self.default_output_path, f\"seedbench_{model.name}_test_submit_{cur_datetime}.json\")\n        output_f = open(output_path, \"a\")\n        with tqdm(total=len(self.data), desc=\"Evaluating\") as pbar:\n            for data_dict in self.data:\n                image = data_dict[\"image\"]\n                question = data_dict[\"question\"] + \" There are several options:\"\n                option_index = [\"A\", \"B\", \"C\", \"D\"]\n                for cur_idx in range(4):\n                    question += f\" {option_index[cur_idx]}. {data_dict[f'choice_{option_index[cur_idx].lower()}']}\"\n\n                answer = data_dict[\"answer\"]\n                options = [\n                    data_dict[\"choice_a\"],\n                    data_dict[\"choice_b\"],\n                    data_dict[\"choice_c\"],\n                    data_dict[\"choice_d\"],\n                ]\n\n                option_losses = []\n                for idx, option in enumerate(options):\n                    option = option_index[idx] + \". \" + option\n                    loss = model.eval_forward(question, option, image)\n                    option_losses.append(loss.item())\n\n                prediction_idx = np.argmin(option_losses)\n                prediction = [\"A\", \"B\", \"C\", \"D\"][prediction_idx]\n                if prediction == answer:\n                    num_correct += 1\n                count += 1\n\n            answer_record = {\"question_id\": data_dict[\"question_id\"], \"prediction\": prediction}\n            output_f.write(json.dumps(answer_record) + \"\\n\")\n\n            answer_record = {\"question_id\": data_dict[\"question_id\"], \"prediction\": prediction}\n            output_f.write(json.dumps(answer_record) + \"\\n\")\n\n            accuracy = num_correct / count * 100\n            pbar.set_postfix(accuracy=f\"{accuracy:.2f}\")\n            pbar.update(1)\n\n        accuracy = num_correct / count * 100\n        print(f\"Accuracy: {accuracy:.2f}%\")\n        return accuracy\n"}
{"type": "source_file", "path": "mimic-it/syphus/file_utils.py", "content": "\"\"\"\nfile utils\n\"\"\"\n\nimport json\nimport os\nimport time\n\nimport openai\nimport random\nfrom litellm import completion\n\nengine = os.environ.get(\"OPENAI_API_ENGINE\", \"davinci\")\n\n\ndef query_gpt(inputs: dict[str], dataset_name: str) -> tuple[dict[str, str], str]:\n    \"\"\"\n    Query the GPT API with the given inputs.\n    Returns:\n        Response (dict[str, str]): the response from GPT API.\n        Input ID (str): the id that specifics the input.\n    \"\"\"\n    if dataset_name == \"3d.SceneNavigation\":\n        with open(\"./candidates.txt\") as f:\n            candidates = f.readlines()\n        cur_candidates = random.sample(candidates, 9)\n        cur_candidates_string = \"\\n\".join(cur_candidates)\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": inputs[\"system_messages\"],\n        }\n    ]\n    # multi-round conversation in the in_context\n    messages.extend(inputs[\"in_context\"])\n    if dataset_name == \"3d.SceneNavigation\":\n        messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": f\"Sentences: {inputs['query_input']['sentences']}\\nCandidate activities and the role who want to do these activities:{cur_candidates_string}\\nPlease give me three conversations for three activities. Each conversation should have three rounds. You should select activities from the candidates. At the beginning of conversation (after introducing the human role), must giving me the reason why the current activity is selected for this room, in the format:reason:XXX\\nPlease ensuring that the assistant should not always answer in the format of listing.\",\n            },\n        )\n    else:\n        messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": inputs[\"query_input\"][\"sentences\"],\n            },\n        )\n    succuss = True\n    while succuss:\n        try:\n            response = completion(\n                engine=engine,  # defined by os.environ, default engine=\"chatgpt0301\",\n                messages=messages,\n                temperature=0.7,\n                max_tokens=3200,\n                top_p=0.95,\n                frequency_penalty=0,\n                presence_penalty=0,\n                stop=None,\n            )\n            succuss = False\n        except Exception as e:\n            print(f\"Error: {e}\")\n            if \"have exceeded call rate limit\" in str(e):\n                print(\"Sleeping for 3 seconds\")\n                succuss = True\n                time.sleep(3)\n            else:\n                succuss = False\n                response = {\"error_message\": str(e)}\n    return response, inputs[\"query_input\"][\"id\"]\n\n\ndef split_question_and_answer(pair_of_answer: str, file_id: str) -> tuple[bool, dict[str, str]]:\n    \"\"\"\n    Split the question and answer from the pair of question and answer.\n    Args:\n        pair_of_answer (str): the pair of question and answer.\n        file_id (str): the id of the file.\n    \"\"\"\n    try:\n        question, answer = pair_of_answer.split(\"\\n\")\n        question_prefix, question = question.split(\": \")\n        answer_prefix, answer = answer.split(\": \")\n        if question_prefix != \"Question\":\n            raise ValueError(\"The prefix is not Question\")\n        if answer_prefix != \"Answer\":\n            raise ValueError(\"The prefix is not Answer\")\n        return True, {\"id\": file_id, \"question\": question, \"answer\": answer}\n    except Exception as e:\n        return False, {\n            \"id\": file_id,\n            \"response\": pair_of_answer,\n            \"error_message\": str(e),\n        }\n\n\ndef format_output(response: str, file_id: str, dataset_name: str) -> tuple[list[dict[str, str]], list[dict[str, str]]]:\n    \"\"\"\n    Format the output of ChatGPT.\n\n    Args:\n        response (str): the output from ChatGPT.\n        file_id (str): the id of the input.\n\n    Returns:\n        valid_output (list[dict[str]]): a list of valid output, each item is a dict with keys \"id\", \"question\", and \"answer\".\n        invalid_output (list[dict[str]]): a list of invalid output, each item is a dict with keys \"id\", \"response\", and \"error_message\".\n    \"\"\"\n    valid_output = []\n    invalid_output = []\n    if dataset_name == \"3d.SceneNavigation\":\n        for pair_of_answer in response.strip().split(\"Conversation 1\")[1:]:\n            is_valid = True\n            formatted = {\"id\": file_id, \"results\": f\"Conversation 1{pair_of_answer}\"}\n            if is_valid:\n                valid_output.append(formatted)\n            else:\n                invalid_output.append(formatted)\n    # elif dataset_name == \"video.DenseCaptions\":\n    else:\n        formatted = {\"id\": file_id, \"results\": response}\n        valid_output.append(formatted)\n    # else:\n    #     for pair_of_answer in response.strip().split(\"\\n\\n\"):\n    #         is_valid, formatted = split_question_and_answer(pair_of_answer, file_id)\n    #         if is_valid:\n    #             valid_output.append(formatted)\n    #         else:\n    #             invalid_output.append(formatted)\n    return valid_output, invalid_output\n\n\ndef export_single_output_json(result: dict[str, str], file_name: str, dataset_name: str, duration: float) -> None:\n    \"\"\"\n    Export the output of ChatGPT to a json file.\n\n    Args:\n    \"\"\"\n    valid_output = []\n    invalid_output = []\n    output_folder = f\"output_{dataset_name}\"\n    os.makedirs(output_folder, exist_ok=True)\n    # if len(data := result[\"valid_outputs\"]) > 0:\n    if \"valid_outputs\" in result:\n        valid_output = result[\"valid_outputs\"]\n        with open(f\"{output_folder}/{file_name}_valid_output.json\", \"w\") as f:\n            json.dump(valid_output, f, indent=4)\n    if \"invalid_outputs\" in result:\n        invalid_output = result[\"invalid_outputs\"]\n        with open(f\"{output_folder}/{file_name}_invalid_output.json\", \"w\") as f:\n            json.dump(invalid_output, f, indent=4)\n    if \"error_messages\" in result:\n        error_messages = result[\"error_messages\"]\n        with open(f\"{output_folder}/{file_name}_error_messages.json\", \"w\") as f:\n            json.dump(error_messages, f, indent=4)\n    with open(f\"{output_folder}/{file_name}_meta.json\", \"w\") as f:\n        meta_data = {}\n        meta_data[\"completion_tokens\"] = result[\"tokens\"][\"completion_tokens\"] if \"tokens\" in result else 0\n        meta_data[\"prompt_tokens\"] = result[\"tokens\"][\"prompt_tokens\"] if \"tokens\" in result else 0\n        meta_data[\"total_tokens\"] = meta_data[\"completion_tokens\"] + meta_data[\"prompt_tokens\"]\n        meta_data[\"valid_outputs\"] = len(valid_output)\n        meta_data[\"invalid_outputs\"] = len(invalid_output)\n        meta_data[\"time\"] = round(duration, 2)\n        json.dump(meta_data, f, indent=4)\n\n\ndef export_output_json(results: list[dict[str, str]], name: str, duration: float) -> None:\n    \"\"\"\n    Export the output of ChatGPT to a json file.\n\n    Args:\n    \"\"\"\n    valid_output = []\n    invalid_output = []\n    error_messages = []\n    output_folder = f\"output_{name}\"\n    num_completion_tokens = 0\n    num_prompt_tokens = 0\n    os.makedirs(output_folder, exist_ok=True)\n    for result in results:\n        if \"error_message\" in result:\n            error_messages.append(result)\n            continue\n        valid_output.extend(result[\"valid_outputs\"])\n        invalid_output.extend(result[\"invalid_outputs\"])\n        num_completion_tokens += result[\"tokens\"][\"completion_tokens\"]\n        num_prompt_tokens += result[\"tokens\"][\"prompt_tokens\"]\n\n    with open(f\"{output_folder}/valid_output.json\", \"w\") as f:\n        json.dump(valid_output, f, indent=4)\n    if len(invalid_output) > 0:\n        with open(f\"{output_folder}/invalid_output.json\", \"w\") as f:\n            json.dump(invalid_output, f, indent=4)\n\n    if len(error_messages) > 0:\n        with open(f\"{output_folder}/error_messages.json\", \"w\") as f:\n            json.dump(error_messages, f, indent=4)\n    with open(f\"{output_folder}/meta.json\", \"w\") as f:\n        json.dump(\n            {\n                \"completion_tokens\": num_completion_tokens,\n                \"prompt_tokens\": num_prompt_tokens,\n                \"total_tokens\": num_completion_tokens + num_prompt_tokens,\n                \"valid_outputs\": len(valid_output),\n                \"invalid_outputs\": len(invalid_output),\n                \"error_messages\": len(error_messages),\n                \"time\": round(duration, 2),\n                \"total_examples\": len(results),\n            },\n            f,\n            indent=4,\n        )\n\n\ndef save_query_json(inputs: dict[str], name: str) -> None:\n    \"\"\"\n    Save the query json to a file.\n\n    Args:\n        inputs (dict[str]): the inputs to query the GPT API.\n        name (str): the name of the file.\n    \"\"\"\n    output_folder = f\"output_{name}\"\n    os.makedirs(output_folder, exist_ok=True)\n    with open(f\"{output_folder}/query_input.json\", \"w\") as f:\n        json.dump(inputs, f, indent=4)\n"}
{"type": "source_file", "path": "mimic-it/convert-it/image_utils.py", "content": "import base64\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\nfrom io import BytesIO\nfrom typing import Generator, Tuple\n\nimport cv2\nfrom PIL import Image\nfrom tqdm import tqdm\n\n\ndef get_image_id(image_name: str, dataset_name: str) -> str:\n    \"\"\"\n    Extracts the image identifier from a given image name.\n\n    Args:\n        image_name (str): The name of the image.\n        dataset_name (str): The name of the dataset.\n\n    Returns:\n        str: The image identifier.\n    \"\"\"\n    return f\"{dataset_name}_IMG_{get_image_name(image_name)}\"\n\n\ndef image_to_bytes(image: Image.Image) -> bytes:\n    image_stream = BytesIO()\n    image.save(image_stream, format=\"PNG\")\n    image_bytes = image_stream.getvalue()\n    image_stream.close()\n    return image_bytes\n\n\ndef resize_image(img: bytes, target_size: tuple[int, int] = (224, 224)) -> bytes:\n    with Image.open(BytesIO(img)) as image:\n        if image.size != target_size:\n            resized_image = image.resize(target_size, Image.LANCZOS)\n            image.close()\n            image = resized_image\n        resized_image_bytes = image_to_bytes(image)\n    return resized_image_bytes\n\n\ndef process_image(image: bytes, target_size=(224, 224)) -> bytes:\n    \"\"\"\n    Processes the input image by resizing it, converting it to RGB mode, and save as byte string.\n\n    Args:\n        image (bytes): The input image to be processed.\n\n    Returns:\n        bytes: The processed image as a byte string.\n    \"\"\"\n    with Image.open(BytesIO(image)) as img:\n        if img.size != target_size:\n            resized_img = img.resize(target_size, Image.LANCZOS)\n            img.close()\n            img = resized_img\n        if img.mode != \"RGB\":\n            converted_img = img.convert(\"RGB\")\n            img.close()\n            img = converted_img\n        processed_image = image_to_bytes(img)\n    return processed_image\n\n\ndef get_b64_data(image: bytes) -> str:\n    \"\"\"\n    Converts an image to a base64 encoded string.\n\n    Args:\n        image (bytes): the image to be converted.\n\n    Returns:\n        str: the base64 encoded string representation of the image.\n    \"\"\"\n    return base64.b64encode(image).decode(\"utf-8\")\n\n\ndef get_json_data_generator(images: dict[str, bytes], dataset_name: str, num_threads: int) -> Generator[Tuple[str, str], None, None]:\n    \"\"\"\n    Converts a dictionary of images to a JSON-compatible dictionary with base64 encoded strings.\n    This generator function will yield the processed image data one at a time, allowing you to write the results to a file without needing to store the entire dictionary in memory.\n    Args:\n        images (Dict[str, bytes]): A dictionary of images, where the keys are image identifiers and the values are byte strings.\n        dataset_name (str): The name of the dataset.\n        num_threads (int): The number of threads to use for processing the images.\n\n    Returns:\n        Dict[str, str]: A dictionary where the keys are formatted as \"{dataset_name}_IMG_{key}\" and the values are base64 encoded string representations of the processed images.\n    \"\"\"\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        process_bar = tqdm(total=len(images), desc=\"Processing images\", unit=\"image\")\n\n        def process_image_wrapper(args):\n            key, img = args\n            new_key = get_image_id(key, dataset_name)\n            result = get_b64_data(process_image(img))\n\n            process_bar.update()\n            return new_key, result\n\n        for result in executor.map(process_image_wrapper, images.items()):\n            yield result\n\n        process_bar.close()\n\n\ndef frame_video(video_file: str, fps: int = 1) -> list[bytes]:\n    \"\"\"\n    Extracts frames from a video file at a specified frame rate and returns them as base64 encoded strings.\n\n    Args:\n        video_file (str): The path to the video file.\n        fps (int): The frame rate at which frames should be extracted. Defaults to 1 frame per second.\n\n    Returns:\n        List[bytes]: A list of byte strings representing the extracted frames.\n    \"\"\"\n    if not os.path.exists(video_file):\n        raise FileNotFoundError(f\"Video file {video_file} does not exist.\")\n\n    cap = cv2.VideoCapture(video_file)\n    video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n    frame_count = 0\n    saved_frame_count = 0\n    frames = []\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n\n        if not ret:\n            break\n\n        if frame_count % (video_fps // fps) == 0:\n            # Check if the frame resolution is not 224x224 and resize if necessary\n            if frame.shape[0] != 224 or frame.shape[1] != 224:\n                frame = cv2.resize(frame, (224, 224))\n\n            success, buffer = cv2.imencode(\".png\", frame)\n            if not success:\n                print(f\"Failed to encode frame {frame_count} of video {video_file}.\")\n            frames.append(process_image(buffer))\n            saved_frame_count += 1\n\n            del buffer\n\n        frame_count += 1\n\n        del frame\n\n    cap.release()\n    return frames\n\n\ndef get_image_name(image_path: str) -> str:\n    \"\"\"\n    Extracts the image name from a given image path.\n\n    Args:\n        image_path (str): The path to the image.\n\n    Returns:\n        str: The image name.\n    \"\"\"\n    return image_path.split(\"/\")[-1].split(\".\")[0]\n\n\ndef create_folder(folder_name: str):\n    \"\"\"\n    Creates a folder if it does not already exist.\n\n    Args:\n        folder_name (str): The name of the folder to create.\n    \"\"\"\n    if not os.path.exists(folder_name):\n        os.makedirs(folder_name)\n"}
{"type": "source_file", "path": "mimic-it/syphus/datasets/video.py", "content": "\"\"\"\nThis file contains the implementation of the DenseCaptions and TVCaptions datasets.\n\"\"\"\n\nimport json\n\nfrom abstract_dataset import AbstractDataset\n\n\nclass DenseCaptions(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"DenseCaptions\",\n        prompt_path: str = \"prompts/dense_captions.json\",\n        query_inputs_path: str = \"annotations/dense_captions/train.json\",\n    ):\n        super().__init__(name, prompt_path, query_inputs_path)\n\n    def _load_query_inputs(self, path: str) -> list[dict[str, str]]:\n        with open(path, \"r\") as f:\n            json_data = json.load(f)\n        query_inputs = []\n        for item in json_data:\n            now_video = {}\n            now_video[\"id\"] = item\n            now_query_input = \"\"\n            now_time_stamps = json_data[item][\"timestamps\"]\n            for i in range(len(now_time_stamps)):\n                now_time_stamps[i][0] = round(float(now_time_stamps[i][0]))\n                now_time_stamps[i][1] = round(float(now_time_stamps[i][1]))\n            now_query_input += \"timestamps: \" + str(now_time_stamps) + \"\\n\"\n            now_query_input += \"sentences: \" + json.dumps(json_data[item][\"sentences\"])\n            query_inputs.append(\n                {\n                    \"id\": item,\n                    \"sentences\": now_query_input,\n                }\n            )\n        return query_inputs\n\n\nclass TVCaptions(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"TVCaptions\",\n        in_context_path: str = \"prompts/tv_captions.json\",\n        annotation_path: str = \"annotations/tvc_annotations/tvc_train_release.jsonl\",\n    ):\n        super().__init__(name, in_context_path, annotation_path)\n\n    def _load_query_inputs(self, path: str) -> list[dict[str]]:\n        query_inputs = []\n        with open(path, \"r\") as json_file:\n            for json_str in json_file:\n                video = json.loads(json_str)\n                video_id = video[\"vid_name\"]\n                now_query_input = []\n                for disc_id, desc in enumerate(video[\"descs\"], 1):\n                    now_query_input.append(str(disc_id) + \". \" + desc[\"desc\"])\n                query_inputs.append({\"id\": video_id, \"sentences\": \"\\n\".join(now_query_input)})\n        return query_inputs\n\n\nclass VisualStoryTelling(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"VisualStoryTelling\",\n        in_context_path: str = \"prompts/visual_story_telling.json\",\n        annotation_path: str = \"annotations/story_in_sequence/train.story-in-sequence.json\",\n    ):\n        super().__init__(name, in_context_path, annotation_path)\n\n    def generate_single_query_input(self, album: dict):\n        query_input = \"\"\n        query_input += \"title: \" + album[\"title\"] + \"\\n\"\n        query_input += \"description: \" + album[\"description\"] + \"\\n\"\n        for image in album[\"images\"]:\n            query_input += \"\\n\"\n            query_input += \"image: \" + image[\"title\"] + \"\\n\"\n            query_input += \"tags: \" + image[\"tags\"] + \"\\n\"\n            query_input += \"annotations: \" + json.dumps(image[\"annotations\"]) + \"\\n\"\n        return query_input\n\n    def _load_query_inputs(self, path: str) -> list[dict[str]]:\n        query_inputs = []\n        with open(path) as f:\n            json_data = json.load(f)\n\n            # create images dictionary and add basic information\n            images = {}\n            for image in json_data[\"images\"]:\n                # url = \"\"\n                # if \"url_o\" not in image:\n                #     url = image[\"url_m\"]\n                # else:\n                #     url = image[\"url_o\"]\n                images[image[\"id\"]] = {\n                    \"title\": image[\"title\"],\n                    # \"url\": url,\n                    \"tags\": image[\"tags\"],\n                    \"annotations\": [],\n                }\n\n            # add annotations to images\n            for annotation_list in json_data[\"annotations\"]:\n                for annotation in annotation_list:\n                    images[annotation[\"photo_flickr_id\"]][\"annotations\"].append(annotation[\"text\"])\n\n            # create albums dictionary and add basic information\n            albums = {}\n            for album in json_data[\"albums\"]:\n                albums[album[\"id\"]] = {\n                    \"description\": album[\"description\"],\n                    \"title\": album[\"title\"],\n                    \"images\": [],\n                }\n\n            # add images to albums\n            for image in json_data[\"images\"]:\n                albums[image[\"album_id\"]][\"images\"].append(images[image[\"id\"]])\n\n            # create query inputs\n            for album in albums:\n                query_inputs.append(\n                    {\n                        \"id\": album,\n                        \"sentences\": self.generate_single_query_input(albums[album]),\n                    }\n                )\n\n        return query_inputs\n"}
{"type": "source_file", "path": "mimic-it/syphus/prompts/translation_prompt.py", "content": "system_message = \"\"\"As an AI assistant, you are an expert in translating English to natural Chinese(zh), Spanish(es), Japanese(ja), German(de), French(fr), Korean(ko), and Arabic(ar). You will provide professional translations without any grammar mistakes. Your translation should be in a tone of native speaker. The input format will be <a>xxx</a><b>xxx</b>. You should translate the content inside the <a> and <b> tag. Your output should strictly follow the format of this json file:\\n\n                {\n                    \"a\": {\n                        \"zh\": \"xxx\",\n                        \"es\": \"xxx\",\n                        \"ja\": \"xxx\",\n                        \"de\": \"xxx\",\n                        \"fr\": \"xxx\",\n                        \"ko\": \"xxx\",\n                        \"ar\": \"xxx\"\n                    },\n                    \"b\": {\n                        \"zh\": \"xxx\",\n                        \"es\": \"xxx\",\n                        \"ja\": \"xxx\",\n                        \"de\": \"xxx\",\n                        \"fr\": \"xxx\",\n                        \"ko\": \"xxx\",\n                        \"ar\": \"xxx\"\n                    }\n                }\"\"\"\nin_context = [\n    {\n        \"role\": \"user\",\n        \"content\": \"<a>Hey, bro, I feel like having a cup of tea here!</a><b>Having tea is a great choice! Right now, you have two options in front of you: green tea and oolong tea. Which one would you like to have? Of course, if you want to enhance the tea-drinking experience even more, you can dim the lights here and play some classic Chinese music on your phone!</b>\",\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"\"\"{\n                \"a\": {\n                    \"zh\": \"嘿，兄弟，我想在这里喝口茶!\",\n                    \"es\": \"¡eh, tío, me apetece tomar una taza de café aquí!\",\n                    \"ja\": \"ねえ、ここでお茶を飲みたいな！\",\n                    \"de\": \"Hey, ich möchte hier gerne eine Tasse Tee trinken!\",\n                    \"fr\": \"Bonjour, je voudrais prendre une tasse de thé ici!\",\n                    \"ko\": \"헤이, 브로, 난 여기에 자 한 잔 바시고 싶어!\",\n                    \"ar\": \"يا أخي ، أشعر وكأنني أرغب في شرب كوب من الشاي هنا!\",\n                },\n                \"b\":{\n                    \"zh\": \"嗨！喝茶是个不错的选择！现在你面前有绿茶和乌龙茶，你想喝哪种呢？当然，如果你想有更好的品茶氛围的话，你可以把这里的灯光调暗，再用你的手机来点经典的中国音乐\",\n                    \"es\": \"¡Tomar té es una muy buena elección! ¿Cuál te gustaría tomar? Por supuesto, si quieres disfrutar aún más de la experiencia de tomar té, puedes atenuar las luces de aquí y reproducir música clásica china  desde tu teléfono móvil.\",\n                    \"ja\": \"お茶を飲むのはいい選択だよ！今、目の前には緑茶とウーロン茶がありますが、どちらが飲みたいですか？もちろん、もっと良いお茶の時間を過ごしたいなら、ここでの照明を暗くして、あなたの携帯で中国のクラシック音楽を流すといいよ！\",\n                    \"de\": \"Tee zu trinken ist eine gute Wahl! Vor dir hast du jetzt die Wahl zwischen grünem Tee und Oolong-Tee. Welche Sorte möchtest du trinken? Natürlich, wenn du ein noch besseres Teetrinkerlebnis haben möchtest, kannst du hier das Licht abdunkeln und mit deinem Handy klassische chinesische Musik abspielen!\",\n                    \"fr\": \"Boire du thé est un excellent choix ! Maintenant, vous avez deux options devant vous, le thé vert et le thé oolong. Lequel aimeriez-vous prendre? Bien sûr, si vous souhaitez améliorer votre expérience de dégustation, vous pouvez baisser les lumières ici et écouter de la musique chinoise classique sur votre téléphone!\",\n                    \"ko\": \"하이, 좋은 선택이야. 지금 녹차와 우롱차가 있는데 뭘 마시고 싶어? 물론 차를 더욱 풍미있게 즐기고 싶으면 여기 조명을 어둡게 조절하고, 핸드폰으로 중국 클래식 음악을 틀어도 좋아!\",\n                    \"ar\": \"شرب الشاي هو اختيار رائع! الآن ، لديك خياران أمامك: الشاي الأخضر والشاي الأوولونغ. أيهما تفضل؟ بالطبع ، إذا أردت تعزيز تجربة شرب الشاي أكثر ، يمكنك خفت الأنوار هنا وتشغيل بعض الموسيقى الصينية الكلاسيكية على هاتفك!\"\n                }\n                }\n                \"\"\",\n    },\n]\n"}
{"type": "source_file", "path": "pipeline/benchmarks/datasets/mathvista.py", "content": "import base64\nimport os\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom .base_eval_dataset import BaseEvalDataset\nimport json\nfrom io import BytesIO\nimport pytz\nimport datetime\nimport openai\nimport time\nimport re\nimport io\nfrom Levenshtein import distance\n\nutc_plus_8 = pytz.timezone(\"Asia/Singapore\")  # You can also use 'Asia/Shanghai', 'Asia/Taipei', etc.\nutc_now = pytz.utc.localize(datetime.datetime.utcnow())\nutc_plus_8_time = utc_now.astimezone(utc_plus_8)\n\ndemo_prompt = \"\"\"\nPlease read the following example. Then extract the answer from the model response and type it at the end of the prompt.\n\nPlease answer the question requiring an integer answer and provide the final value, e.g., 1, 2, 3, at the end.\nQuestion: Which number is missing?\n\nModel response: The number missing in the sequence is 14.\n\nExtracted answer: 14\n\nPlease answer the question requiring a floating-point number with one decimal place and provide the final value, e.g., 1.2, 1.3, 1.4, at the end.\nQuestion: What is the fraction of females facing the camera?\n\nModel response: The fraction of females facing the camera is 0.6, which means that six out of ten females in the group are facing the camera.\n\nExtracted answer: 0.6\n\nPlease answer the question requiring a floating-point number with two decimal places and provide the final value, e.g., 1.23, 1.34, 1.45, at the end.\nQuestion: How much money does Luca need to buy a sour apple candy and a butterscotch candy? (Unit: $)\n\nModel response: Luca needs $1.45 to buy a sour apple candy and a butterscotch candy.\n\nExtracted answer: 1.45\n\nPlease answer the question requiring a Python list as an answer and provide the final list, e.g., [1, 2, 3], [1.2, 1.3, 1.4], at the end.\nQuestion: Between which two years does the line  graph saw its maximum peak?\n\nModel response: The line graph saw its maximum peak between 2007 and 2008.\n\nExtracted answer: [2007, 2008]\n\nPlease answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\nQuestion: What fraction of the shape is blue?\\nChoices:\\n(A) 3/11\\n(B) 8/11\\n(C) 6/11\\n(D) 3/5\n\nModel response: The correct answer is (B) 8/11.\n\nExtracted answer: B\n\"\"\"\n\n\nimport time\nimport requests\nimport json\nimport ast\n\n\ndef get_chat_response(promot, api_key, model=\"gpt-3.5-turbo\", temperature=0, max_tokens=256, n=1, patience=5, sleep_time=5):\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n        {\"role\": \"user\", \"content\": promot},\n    ]\n\n    payload = {\"model\": model, \"messages\": messages}\n\n    while patience > 0:\n        patience -= 1\n        try:\n            response = requests.post(\n                \"https://api.openai.com/v1/chat/completions\",\n                headers=headers,\n                data=json.dumps(payload),\n                timeout=30,\n            )\n            response.raise_for_status()\n            response_data = response.json()\n\n            prediction = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n            if prediction != \"\" and prediction is not None:\n                return prediction\n\n        except Exception as e:\n            if \"Rate limit\" not in str(e):\n                print(e)\n            time.sleep(sleep_time)\n\n    return \"\"\n\n\ndef create_test_prompt(demo_prompt, query, response):\n    demo_prompt = demo_prompt.strip()\n    test_prompt = f\"{query}\\n\\n{response}\"\n    full_prompt = f\"{demo_prompt}\\n\\n{test_prompt}\\n\\nExtracted answer: \"\n    return full_prompt\n\n\ndef extract_answer(response, problem, quick_extract=False, api_key=None, pid=None, gpt_model=\"gpt-4-0613\"):\n    question_type = problem[\"question_type\"]\n    answer_type = problem[\"answer_type\"]\n    choices = problem[\"choices\"]\n    query = problem[\"query\"]\n\n    if response == \"\":\n        return \"\"\n\n    if question_type == \"multi_choice\" and response in choices:\n        return response\n\n    if answer_type == \"integer\":\n        try:\n            extraction = int(response)\n            return str(extraction)\n        except:\n            pass\n\n    if answer_type == \"float\":\n        try:\n            extraction = str(float(response))\n            return extraction\n        except:\n            pass\n\n    # quick extraction\n    if quick_extract:\n        # The answer is \"text\". -> \"text\"\n        try:\n            result = re.search(r'The answer is \"(.*)\"\\.', response)\n            if result:\n                extraction = result.group(1)\n                return extraction\n        except:\n            pass\n\n    else:\n        # general extraction\n        try:\n            full_prompt = create_test_prompt(demo_prompt, query, response)\n            extraction = get_chat_response(full_prompt, api_key=api_key, model=gpt_model, n=1, patience=5, sleep_time=5)\n            return extraction\n        except Exception as e:\n            print(e)\n            print(f\"Error in extracting answer for {pid}\")\n\n    return \"\"\n\n\ndef get_acc_with_contion(res_pd, key, value):\n    if key == \"skills\":\n        # if value in res_pd[key]:\n        total_pd = res_pd[res_pd[key].apply(lambda x: value in x)]\n    else:\n        total_pd = res_pd[res_pd[key] == value]\n\n    correct_pd = total_pd[total_pd[\"true_false\"] == True]\n    acc = \"{:.2f}\".format(len(correct_pd) / len(total_pd) * 100)\n    return len(correct_pd), len(total_pd), acc\n\n\ndef get_most_similar(prediction, choices):\n    \"\"\"\n    Use the Levenshtein distance (or edit distance) to determine which of the choices is most similar to the given prediction\n    \"\"\"\n    distances = [distance(prediction, choice) for choice in choices]\n    ind = distances.index(min(distances))\n    return choices[ind]\n    # return min(choices, key=lambda choice: distance(prediction, choice))\n\n\ndef normalize_extracted_answer(extraction, choices, question_type, answer_type, precision):\n    \"\"\"\n    Normalize the extracted answer to match the answer type\n    \"\"\"\n    if question_type == \"multi_choice\":\n        # make sure the extraction is a string\n        if isinstance(extraction, str):\n            extraction = extraction.strip()\n        else:\n            try:\n                extraction = str(extraction)\n            except:\n                extraction = \"\"\n\n        # extract \"A\" from \"(A) text\"\n        letter = re.findall(r\"\\(([a-zA-Z])\\)\", extraction)\n        if len(letter) > 0:\n            extraction = letter[0].upper()\n\n        options = [chr(ord(\"A\") + i) for i in range(len(choices))]\n\n        if extraction in options:\n            # convert option letter to text, e.g. \"A\" -> \"text\"\n            ind = options.index(extraction)\n            extraction = choices[ind]\n        else:\n            # select the most similar option\n            extraction = get_most_similar(extraction, choices)\n        assert extraction in choices\n\n    elif answer_type == \"integer\":\n        try:\n            extraction = str(int(float(extraction)))\n        except:\n            extraction = None\n\n    elif answer_type == \"float\":\n        try:\n            extraction = str(round(float(extraction), precision))\n        except:\n            extraction = None\n\n    elif answer_type == \"list\":\n        try:\n            extraction = str(extraction)\n        except:\n            extraction = None\n\n    return extraction\n\n\ndef get_pil_image(raw_image_data) -> Image.Image:\n    if isinstance(raw_image_data, Image.Image):\n        return raw_image_data\n\n    elif isinstance(raw_image_data, dict) and \"bytes\" in raw_image_data:\n        return Image.open(io.BytesIO(raw_image_data[\"bytes\"]))\n\n    elif isinstance(raw_image_data, str):  # Assuming this is a base64 encoded string\n        image_bytes = base64.b64decode(raw_image_data)\n        return Image.open(io.BytesIO(image_bytes))\n\n    else:\n        raise ValueError(\"Unsupported image data format\")\n\n\ndef safe_equal(prediction, answer):\n    \"\"\"\n    Check if the prediction is equal to the answer, even if they are of different types\n    \"\"\"\n    try:\n        if prediction == answer:\n            return True\n        return False\n    except Exception as e:\n        print(e)\n        return False\n\n\nclass MathVistaDataset(BaseEvalDataset):\n    def __init__(\n        self,\n        data_path=\"Otter-AI/MathVista\",\n        split=\"test\",\n        default_output_path=\"./logs/MathVista\",\n        cache_dir=None,\n        api_key=None,\n        gpt_model=\"gpt-4-0613\",\n        debug=False,\n        quick_extract=False,\n    ):\n        super().__init__(\"MathVistaDataset\", data_path)\n        name_converter = {\"dev\": \"validation\", \"test\": \"test\"}\n        self.data = load_dataset(\"Otter-AI/MathVista\", split=name_converter[split], cache_dir=cache_dir).to_pandas()\n        if debug:\n            self.data = self.data.sample(5)\n        # data_path = \"/home/luodian/projects/Otter/archived/testmini_image_inside.json\"\n        # with open(data_path, \"r\", encoding=\"utf-8\") as f:\n        #     self.data = json.load(f)\n\n        self.debug = debug\n        self.quick_extract = quick_extract\n\n        self.default_output_path = default_output_path\n        if os.path.exists(self.default_output_path) is False:\n            os.makedirs(self.default_output_path)\n        self.cur_datetime = utc_plus_8_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.api_key = api_key\n        self.gpt_model = gpt_model\n\n    def create_query(self, problem, shot_type):\n        ### [2] Test query\n        # problem info\n        question = problem[\"question\"]\n        unit = problem[\"unit\"]\n        choices = problem[\"choices\"]\n        precision = problem[\"precision\"]\n        question_type = problem[\"question_type\"]\n        answer_type = problem[\"answer_type\"]\n\n        # hint\n        if shot_type == \"solution\":\n            if question_type == \"multi_choice\":\n                assert answer_type == \"text\"\n                hint_text = f\"Please answer the question and provide the correct option letter, e.g., A, B, C, D, at the end.\"\n            else:\n                assert answer_type in [\"integer\", \"float\", \"list\"]\n                if answer_type == \"integer\":\n                    hint_text = f\"Please answer the question requiring an integer answer and provide the final value, e.g., 1, 2, 3, at the end.\"\n\n                elif answer_type == \"float\" and str(precision) == \"1\":\n                    hint_text = f\"Please answer the question requiring a floating-point number with one decimal place and provide the final value, e.g., 1.2, 1.3, 1.4, at the end.\"\n\n                elif answer_type == \"float\" and str(precision) == \"2\":\n                    hint_text = f\"Please answer the question requiring a floating-point number with two decimal places and provide the final value, e.g., 1.23, 1.34, 1.45, at the end.\"\n\n                elif answer_type == \"list\":\n                    hint_text = f\"Please answer the question requiring a Python list as an answer and provide the final list, e.g., [1, 2, 3], [1.2, 1.3, 1.4], at the end.\"\n        else:\n            assert shot_type == \"code\"\n            hint_text = \"Please generate a python code to solve the problem\"\n\n        # question\n        question_text = f\"Question: {question}\"\n        if unit:\n            question_text += f\" (Unit: {unit})\"\n\n        # choices\n        if choices is not None and len(choices) != 0:\n            # choices: (A) 1.2 (B) 1.3 (C) 1.4 (D) 1.5\n            texts = [\"Choices:\"]\n            for i, choice in enumerate(choices):\n                texts.append(f\"({chr(ord('A')+i)}) {choice}\")\n            choices_text = \"\\n\".join(texts)\n        else:\n            choices_text = \"\"\n\n        # prompt\n        if shot_type == \"solution\":\n            prompt = \"Solution: \"\n        else:\n            assert shot_type == \"code\"\n            prompt = \"Python code: \"\n\n        elements = [hint_text, question_text, choices_text, prompt]\n        query = \"\\n\".join([e for e in elements if e != \"\"])\n\n        query = query.strip()\n        return query\n\n    def _evaluate(self, model):\n        output_file = os.path.join(self.default_output_path, f\"{model.name}_mathvista_eval_submit_{self.cur_datetime}.json\")  # directly match Lu Pan's repo format e.g. output_bard.json\n\n        results = {}\n\n        print(f\"Number of test problems in total: {len(self.data)}\")\n        for idx_key, query_data in tqdm(self.data.iterrows(), desc=f\"Evaluating {model.name}\", total=len(self.data)):\n            # query_data = self.data[idx_key]\n            results[idx_key] = {}\n            results[idx_key].update(query_data)\n            if results[idx_key][\"choices\"] is not None:\n                results[idx_key][\"choices\"] = list(results[idx_key][\"choices\"])\n            results[idx_key].pop(\"image\")\n            # problem = query_data[\"problem\"]\n            query = self.create_query(problem=query_data, shot_type=\"solution\")\n            base64_image = query_data[\"image\"]\n            # image = Image.open(BytesIO(base64.b64decode(base64_image)))\n            image = get_pil_image(base64_image)\n            response = model.generate(query, image)\n            if self.debug:\n                print(f\"\\n# Query: {query}\")\n                print(f\"\\n# Response: {response}\")\n            results[idx_key].update({\"query\": query})\n            results[idx_key].update({\"response\": response})\n\n        with open(output_file, \"w\") as outfile:\n            json.dump(results, outfile)\n\n        results = json.load(open(output_file, \"r\"))\n\n        print(f\"MathVista Evaluator: Results saved to {output_file}\")\n\n        for idx_key, row in tqdm(self.data.iterrows(), desc=f\"Extracting answers from {model.name}\", total=len(self.data)):\n            idx_key = str(idx_key)\n            response = results[idx_key][\"response\"]\n            extraction = extract_answer(\n                response,\n                results[idx_key],\n                quick_extract=self.quick_extract,\n                api_key=self.api_key,\n                pid=idx_key,\n                gpt_model=self.gpt_model,\n            )\n            results[idx_key].update({\"extraction\": extraction})\n            answer = results[idx_key][\"answer\"]\n            choices = results[idx_key][\"choices\"]\n            question_type = results[idx_key][\"question_type\"]\n            answer_type = results[idx_key][\"answer_type\"]\n            precision = results[idx_key][\"precision\"]\n            extraction = results[idx_key][\"extraction\"]\n\n            prediction = normalize_extracted_answer(extraction, choices, question_type, answer_type, precision)\n            true_false = safe_equal(prediction, answer)\n\n            results[idx_key][\"prediction\"] = prediction\n            results[idx_key][\"true_false\"] = true_false\n\n        full_pids = list(results.keys())\n        ## [2] Calculate the average accuracy\n        total = len(full_pids)\n        correct = 0\n        for pid in full_pids:\n            if results[pid][\"true_false\"]:\n                correct += 1\n        accuracy = str(round(correct / total * 100, 2))\n        print(f\"\\nCorrect: {correct}, Total: {total}, Accuracy: {accuracy}%\")\n\n        scores = {\"average\": {\"accuracy\": accuracy, \"correct\": correct, \"total\": total}}\n        ## [3] Calculate the fine-grained accuracy scores\n        # merge the 'metadata' attribute into the data\n        success_parse = True\n        try:\n            for pid in results:\n                cur_meta = results[pid][\"metadata\"]\n                cur_meta_dict = ast.literal_eval(cur_meta)\n                results[pid].update(cur_meta_dict)\n        except:\n            success_parse = False\n            # results[pid].update(results[pid].pop(\"metadata\"))\n\n        # convert the data to a pandas DataFrame\n        df = pd.DataFrame(results).T\n\n        print(\"Number of test problems:\", len(df))\n        # assert len(df) == 1000 # Important!!!\n\n        if success_parse:\n            # asign the target keys for evaluation\n            target_keys = [\n                \"question_type\",\n                \"answer_type\",\n                \"language\",\n                \"source\",\n                \"category\",\n                \"task\",\n                \"context\",\n                \"grade\",\n                \"skills\",\n            ]\n\n            for key in target_keys:\n                print(f\"\\nType: [{key}]\")\n                # get the unique values of the key\n                if key == \"skills\":\n                    # the value is a list\n                    values = []\n                    for i in range(len(df)):\n                        values += df[key][i]\n                    values = list(set(values))\n                else:\n                    values = df[key].unique()\n                # calculate the accuracy for each value\n                scores[key] = {}\n                for value in values:\n                    correct, total, acc = get_acc_with_contion(df, key, value)\n                    if total > 0:\n                        print(f\"[{value}]: {acc}% ({correct}/{total})\")\n                        scores[key][value] = {\"accuracy\": acc, \"correct\": correct, \"total\": total}\n\n                # sort the scores by accuracy\n                scores[key] = dict(sorted(scores[key].items(), key=lambda item: float(item[1][\"accuracy\"]), reverse=True))\n\n        # save the scores\n        scores_file = os.path.join(self.default_output_path, f\"{model.name}_mathvista_eval_score_{self.cur_datetime}.json\")\n        print(f\"MathVista Evaluator: Score results saved to {scores_file}...\")\n        with open(scores_file, \"w\") as outfile:\n            json.dump(scores, outfile)\n"}
{"type": "source_file", "path": "pipeline/benchmarks/models/frozen_bilm.py", "content": ""}
{"type": "source_file", "path": "pipeline/benchmarks/models/fuyu.py", "content": "from typing import List\nfrom transformers import AutoTokenizer, FuyuImageProcessor\nfrom transformers import FuyuForCausalLM\nfrom src.otter_ai.models.fuyu.processing_fuyu import FuyuProcessor\nfrom PIL import Image\nfrom .base_model import BaseModel\nimport torch\nimport numpy as np\nimport warnings\nimport io\nimport base64\nimport math\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef get_pil_image(raw_image_data) -> Image.Image:\n    if isinstance(raw_image_data, Image.Image):\n        return raw_image_data\n\n    elif isinstance(raw_image_data, dict) and \"bytes\" in raw_image_data:\n        return Image.open(io.BytesIO(raw_image_data[\"bytes\"]))\n\n    elif isinstance(raw_image_data, str):  # Assuming this is a base64 encoded string\n        image_bytes = base64.b64decode(raw_image_data)\n        return Image.open(io.BytesIO(image_bytes))\n\n    else:\n        raise ValueError(\"Unsupported image data format\")\n\n\nclass Fuyu(BaseModel):\n    def __init__(self, model_path: str = \"adept/fuyu-8b\", cuda_id: int = 0, resolution: int = -1, max_new_tokens=256):\n        super().__init__(\"fuyu\", model_path)\n        self.resolution = resolution\n        self.device = f\"cuda:{cuda_id}\" if torch.cuda.is_available() else \"cpu\"\n        self.model = FuyuForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16).to(self.device)\n        self.tokenizer = AutoTokenizer.from_pretrained(\"adept/fuyu-8b\")\n        self.image_processor = FuyuImageProcessor()\n        self.processor = FuyuProcessor(image_processor=self.image_processor, tokenizer=self.tokenizer)\n        self.max_new_tokens = max_new_tokens\n        self.bad_words_list = [\"User:\", \"Assistant:\"]\n        self.bad_words_ids = self.tokenizer(self.bad_words_list, add_special_tokens=False).input_ids\n\n    def generate(self, text_prompt: str, raw_image_data: str):\n        raw_image_data = get_pil_image(raw_image_data)\n        raw_image_data = raw_image_data.convert(\"RGB\")\n        # make sure the image is in RGB format and resize to match the width\n        if self.resolution != -1:\n            width, height = raw_image_data.size\n            short_edge = min(width, height)\n            scaling_factor = self.resolution / short_edge\n            new_width = math.ceil(width * scaling_factor)\n            new_height = math.ceil(height * scaling_factor)\n            raw_image_data = raw_image_data.resize((new_width, new_height), Image.ANTIALIAS)\n        # formated_prompt = f\"User: {text_prompt} Assistant:\"\n        model_inputs = self.processor(text=text_prompt, images=[raw_image_data], device=self.device)\n        for k, v in model_inputs.items():\n            model_inputs[k] = v.to(self.device)\n\n        model_inputs[\"image_patches\"] = model_inputs[\"image_patches\"].to(dtype=next(self.model.parameters()).dtype)\n        generation_output = self.model.generate(**model_inputs, max_new_tokens=self.max_new_tokens, pad_token_id=self.tokenizer.eos_token_id, bad_words_ids=self.bad_words_ids)\n        generation_text = self.processor.batch_decode(generation_output, skip_special_tokens=True)\n        return generation_text[0].split(\"\\x04\")[1].strip(\" \").strip(\"\\n\")\n\n    def eval_forward(self, **kwargs):\n        return super().eval_forward(**kwargs)\n\n\nif __name__ == \"__main__\":\n    model = Fuyu()\n    print(model.generate(\"Generate a coco-style caption.\\n\", Image.open(\"/home/luodian/projects/Otter/archived/test_images/rabbit.png\").convert(\"RGB\")))\n"}
{"type": "source_file", "path": "pipeline/benchmarks/datasets/base_eval_dataset.py", "content": "from abc import ABC, abstractmethod\nfrom PIL import Image\nfrom typing import Dict, List, Any\n\nimport importlib\n\nAVAILABLE_EVAL_DATASETS: Dict[str, str] = {\n    \"mmbench\": \"MMBenchDataset\",\n    \"mme\": \"MMEDataset\",\n    \"mathvista\": \"MathVistaDataset\",\n    \"mmvet\": \"MMVetDataset\",\n    \"seedbench\": \"SEEDBenchDataset\",\n    \"pope\": \"PopeDataset\",\n    \"scienceqa\": \"ScienceQADataset\",\n    \"magnifierbench\": \"MagnifierBenchDataset\",\n}\n\n\nclass BaseEvalDataset(ABC):\n    def __init__(self, name: str, dataset_path: str, *, max_batch_size: int = 1):\n        self.name = name\n        self.dataset_path = dataset_path\n        self.max_batch_size = max_batch_size\n\n    def evaluate(self, model, **kwargs):\n        return self._evaluate(model, **kwargs)\n        # batch = min(model.max_batch_size, self.max_batch_size)\n        # if batch == 1:\n        #     return self._evaluate(model, **kwargs)\n        # else:\n        #     kwargs[\"batch\"] = batch\n        #     return self._evaluate(model, **kwargs)\n\n    @abstractmethod\n    def _evaluate(self, model: str):\n        pass\n\n\ndef load_dataset(dataset_name: str, dataset_args: Dict[str, str] = {}) -> BaseEvalDataset:\n    assert dataset_name in AVAILABLE_EVAL_DATASETS, f\"{dataset_name} is not an available eval dataset.\"\n    module_path = \"pipeline.benchmarks.datasets.\" + dataset_name\n    dataset_formal_name = AVAILABLE_EVAL_DATASETS[dataset_name]\n    imported_module = importlib.import_module(module_path)\n    dataset_class = getattr(imported_module, dataset_formal_name)\n    print(f\"Imported class: {dataset_class}\")\n    # import pdb;pdb.set_trace()\n    # get dataset args without \"name\"\n    init_args = dataset_args.copy()\n    init_args.pop(\"name\")\n    return dataset_class(**init_args)\n"}
{"type": "source_file", "path": "pipeline/benchmarks/models/gpt4v.py", "content": "import requests\nimport base64\nfrom .base_model import BaseModel\nfrom PIL import Image\nimport io\nimport time\n\n\ndef get_pil_image(raw_image_data) -> Image.Image:\n    if isinstance(raw_image_data, Image.Image):\n        return raw_image_data\n\n    elif isinstance(raw_image_data, dict) and \"bytes\" in raw_image_data:\n        return Image.open(io.BytesIO(raw_image_data[\"bytes\"]))\n\n    elif isinstance(raw_image_data, str):  # Assuming this is a base64 encoded string\n        image_bytes = base64.b64decode(raw_image_data)\n        return Image.open(io.BytesIO(image_bytes))\n\n    else:\n        raise ValueError(\"Unsupported image data format\")\n\n\nclass OpenAIGPT4Vision(BaseModel):\n    def __init__(self, api_key: str, max_new_tokens: int = 256):\n        super().__init__(\"openai-gpt4\", \"gpt-4-vision-preview\")\n        self.api_key = api_key\n        self.headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n        self.max_new_tokens = max_new_tokens\n\n    @staticmethod\n    def encode_image_to_base64(raw_image_data) -> str:\n        if isinstance(raw_image_data, Image.Image):\n            buffered = io.BytesIO()\n            raw_image_data.save(buffered, format=\"JPEG\")\n            return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n        raise ValueError(\"The input image data must be a PIL.Image.Image\")\n\n    def generate(self, text_prompt: str, raw_image_data):\n        raw_image_data = get_pil_image(raw_image_data).convert(\"RGB\")\n        base64_image = self.encode_image_to_base64(raw_image_data)\n\n        payload = {\n            \"model\": \"gpt-4-vision-preview\",\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": text_prompt},\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n                    ],\n                }\n            ],\n            \"max_tokens\": self.max_new_tokens,\n        }\n\n        retry = True\n        retry_times = 0\n        while retry and retry_times < 5:\n            response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=self.headers, json=payload)\n            if response.status_code == 200:\n                response_data = response.json()\n                return response_data[\"choices\"][0][\"message\"][\"content\"]\n            else:\n                print(f\"Failed to connect to OpenAI API: {response.status_code} - {response.text}. Retrying...\")\n                time.sleep(10)\n                retry_times += 1\n        return \"Failed to connect to OpenAI GPT4V API\"\n\n    def eval_forward(self, **kwargs):\n        return super().eval_forward(**kwargs)\n\n\nif __name__ == \"__main__\":\n    # Use your own API key here\n    api_key = \"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\"\n    model = OpenAIGPT4Vision(api_key)\n    image = Image.open(\"/home/luodian/projects/Otter/archived/data/G4_IMG_00001.png\").convert(\"RGB\")\n    print(model.generate(\"What’s in this image?\", image))\n"}
{"type": "source_file", "path": "pipeline/benchmarks/models/base_model.py", "content": "from abc import ABC, abstractmethod\nfrom PIL import Image\nfrom typing import Dict\n\nimport importlib\n\nAVAILABLE_MODELS: Dict[str, str] = {\n    \"video_chat\": \"VideoChat\",\n    \"otter_video\": \"OtterVideo\",\n    \"llama_adapter\": \"LlamaAdapter\",\n    \"mplug_owl\": \"mPlug_owl\",\n    \"video_chatgpt\": \"Video_ChatGPT\",\n    \"otter_image\": \"OtterImage\",\n    \"frozen_bilm\": \"FrozenBilm\",\n    \"idefics\": \"Idefics\",\n    \"fuyu\": \"Fuyu\",\n    \"otterhd\": \"OtterHD\",\n    \"instructblip\": \"InstructBLIP\",\n    \"qwen_vl\": \"QwenVL\",\n    \"llava_model\": \"LLaVA_Model\",\n    \"instructblip\": \"InstructBLIP\",\n    \"gpt4v\": \"OpenAIGPT4Vision\",\n}\n\n\nclass BaseModel(ABC):\n    def __init__(self, model_name: str, model_path: str, *, max_batch_size: int = 1):\n        self.name = model_name\n        self.model_path = model_path\n        self.max_batch_size = max_batch_size\n\n    @abstractmethod\n    def generate(self, **kwargs):\n        pass\n\n    @abstractmethod\n    def eval_forward(self, **kwargs):\n        pass\n\n\ndef load_model(model_name: str, model_args: Dict[str, str]) -> BaseModel:\n    assert model_name in AVAILABLE_MODELS, f\"{model_name} is not an available model.\"\n    module_path = \"pipeline.benchmarks.models.\" + model_name\n    model_formal_name = AVAILABLE_MODELS[model_name]\n    imported_module = importlib.import_module(module_path)\n    model_class = getattr(imported_module, model_formal_name)\n    print(f\"Imported class: {model_class}\")\n    model_args.pop(\"name\")\n    return model_class(**model_args)\n"}
{"type": "source_file", "path": "pipeline/benchmarks/datasets/mme.py", "content": "import base64\nimport io\nfrom PIL import Image\nimport json\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nimport os\nimport numpy as np\nfrom datasets import load_dataset\nfrom typing import Union\nfrom .base_eval_dataset import BaseEvalDataset\nfrom tqdm import tqdm\nimport datetime\nimport pytz\n\nutc_plus_8 = pytz.timezone(\"Asia/Singapore\")  # You can also use 'Asia/Shanghai', 'Asia/Taipei', etc.\nutc_now = pytz.utc.localize(datetime.datetime.utcnow())\nutc_plus_8_time = utc_now.astimezone(utc_plus_8)\n\neval_type_dict = {\n    \"Perception\": [\n        \"existence\",\n        \"count\",\n        \"position\",\n        \"color\",\n        \"posters\",\n        \"celebrity\",\n        \"scene\",\n        \"landmark\",\n        \"artwork\",\n        \"ocr\",\n    ],\n    \"Cognition\": [\"commonsense\", \"numerical\", \"text\", \"code\"],\n}\n\n\nclass MMEDataset(BaseEvalDataset):\n    def decode_base64_to_image(self, base64_string):\n        image_data = base64.b64decode(base64_string)\n        image = Image.open(io.BytesIO(image_data))\n        return image\n\n    def __init__(\n        self,\n        data_path: str = \"Otter-AI/MME\",\n        *,\n        cache_dir: Union[str, None] = None,\n        default_output_path: str = \"./logs/MME\",\n        split: str = \"test\",\n        debug: bool = False,\n    ):\n        super().__init__(\"MMEDataset\", data_path)\n\n        self.default_output_path = default_output_path\n        self.cur_datetime = utc_plus_8_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.data = load_dataset(data_path, split=split, cache_dir=cache_dir)\n        self.debug = debug\n\n        self.category_data = {}\n        # for idx in range(len(self.ids)):\n        for item in tqdm(self.data, desc=\"Loading data\"):\n            id = item[\"id\"]\n            category = id.split(\"_\")[0].lower()\n            question = item[\"instruction\"]\n            answer = item[\"answer\"]\n            image_id = item[\"image_ids\"][0]\n            image = item[\"images\"][0]\n\n            data = {\"question\": question, \"answer\": answer, \"image\": image}\n\n            if category in eval_type_dict[\"Cognition\"]:\n                eval_type = \"Cognition\"\n            elif category in eval_type_dict[\"Perception\"]:\n                eval_type = \"Perception\"\n            else:\n                raise ValueError(f\"Unknown category {category}\")\n\n            if eval_type not in self.category_data:\n                self.category_data[eval_type] = {}\n\n            if category not in self.category_data[eval_type]:\n                self.category_data[eval_type][category] = {}\n\n            if image_id not in self.category_data[eval_type][category]:\n                self.category_data[eval_type][category][image_id] = []\n\n            self.category_data[eval_type][category][image_id].append(data)\n\n    def parse_pred_ans(self, pred_ans):\n        pred_ans = pred_ans.lower().strip().replace(\".\", \"\")\n        pred_label = None\n        if pred_ans in [\"yes\", \"no\"]:\n            pred_label = pred_ans\n        else:\n            prefix_pred_ans = pred_ans[:4]\n            if \"yes\" in prefix_pred_ans:\n                pred_label = \"yes\"\n            elif \"no\" in prefix_pred_ans:\n                pred_label = \"no\"\n            else:\n                pred_label = \"other\"\n        return pred_label\n\n    def compute_metric(self, gts, preds):\n        assert len(gts) == len(preds)\n\n        label_map = {\n            \"yes\": 1,\n            \"no\": 0,\n            \"other\": -1,\n        }\n\n        gts = [label_map[x] for x in gts]\n        preds = [label_map[x] for x in preds]\n\n        acc = accuracy_score(gts, preds)\n\n        clean_gts = []\n        clean_preds = []\n        other_num = 0\n        for gt, pred in zip(gts, preds):\n            if pred == -1:\n                other_num += 1\n                continue\n            clean_gts.append(gt)\n            clean_preds.append(pred)\n\n        conf_mat = confusion_matrix(clean_gts, clean_preds, labels=[1, 0])\n        precision = precision_score(clean_gts, clean_preds, average=\"binary\")\n        recall = recall_score(clean_gts, clean_preds, average=\"binary\")\n        tp, fn = conf_mat[0]\n        fp, tn = conf_mat[1]\n\n        metric_dict = dict()\n        metric_dict = {\n            \"TP\": tp,\n            \"FN\": fn,\n            \"TN\": tn,\n            \"FP\": fp,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"other_num\": other_num,\n            \"acc\": acc,\n        }\n\n        for key, value in metric_dict.items():\n            if isinstance(value, np.int64):\n                metric_dict[key] = int(value)\n\n        return metric_dict\n\n    def _evaluate(self, model):\n        model_score_dict = {}\n\n        self.default_output_path = os.path.join(self.default_output_path, f\"{model.name}_{self.cur_datetime}\")\n        if not os.path.exists(self.default_output_path):\n            os.makedirs(self.default_output_path)\n\n        for eval_type in self.category_data.keys():\n            print(\"===========\", eval_type, \"===========\")\n\n            scores = 0\n            task_score_dict = {}\n            for task_name in tqdm(self.category_data[eval_type].keys(), desc=f\"Evaluating {eval_type}\"):\n                img_num = len(self.category_data[eval_type][task_name])\n                task_other_ans_num = 0\n                task_score = 0\n                acc_plus_correct_num = 0\n                gts = []\n                preds = []\n                for image_pair in tqdm(self.category_data[eval_type][task_name].values(), desc=f\"Evaluating {eval_type} {task_name}\"):\n                    assert len(image_pair) == 2\n                    img_correct_num = 0\n\n                    for item in image_pair:\n                        question = item[\"question\"]\n                        image = item[\"image\"]\n                        gt_ans = item[\"answer\"].lower().strip().replace(\".\", \"\")\n                        response = model.generate(question, image)\n                        if self.debug:\n                            print(f\"\\n# Query: {question}\")\n                            print(f\"\\n# Response: {response}\")\n                        pred_ans = self.parse_pred_ans(response)\n\n                        assert gt_ans in [\"yes\", \"no\"]\n                        assert pred_ans in [\"yes\", \"no\", \"other\"]\n\n                        gts.append(gt_ans)\n                        preds.append(pred_ans)\n\n                        if gt_ans == pred_ans:\n                            img_correct_num += 1\n\n                        if pred_ans not in [\"yes\", \"no\"]:\n                            task_other_ans_num += 1\n\n                    if img_correct_num == 2:\n                        acc_plus_correct_num += 1\n\n                # cal TP precision acc, etc.\n                metric_dict = self.compute_metric(gts, preds)\n                acc_plus = acc_plus_correct_num / img_num\n                metric_dict[\"acc_plus\"] = acc_plus\n\n                for k, v in metric_dict.items():\n                    if k in [\"acc\", \"acc_plus\"]:\n                        task_score += v * 100\n\n                task_score_dict[task_name] = task_score\n                scores += task_score\n\n                output_path = os.path.join(self.default_output_path, f\"{task_name}.json\")\n                with open(output_path, \"w\") as f:\n                    json.dump(metric_dict, f)\n\n            print(f\"total score: {scores}\")\n            for task_name, score in task_score_dict.items():\n                print(f\"\\t {task_name} score: {score}\")\n"}
{"type": "source_file", "path": "pipeline/benchmarks/datasets/__init__.py", "content": ""}
{"type": "source_file", "path": "pipeline/benchmarks/datasets/mmvet.py", "content": "import base64\nimport os\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom .base_eval_dataset import BaseEvalDataset\nfrom collections import Counter\nfrom typing import Union\nimport numpy as np\nfrom openai import OpenAI\nimport time\nimport json\nimport pytz\nimport datetime\nfrom Levenshtein import distance\n\nutc_plus_8 = pytz.timezone(\"Asia/Singapore\")  # You can also use 'Asia/Shanghai', 'Asia/Taipei', etc.\nutc_now = pytz.utc.localize(datetime.datetime.utcnow())\nutc_plus_8_time = utc_now.astimezone(utc_plus_8)\n\nMM_VET_PROMPT = \"\"\"Compare the ground truth and prediction from AI models, to give a correctness score for the prediction. <AND> in the ground truth means it is totally right only when all elements in the ground truth are present in the prediction, and <OR> means it is totally right when any one element in the ground truth is present in the prediction. The correctness score is 0.0 (totally wrong), 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, or 1.0 (totally right). Just complete the last space of the correctness score.\n\nQuestion | Ground truth | Prediction | Correctness\n--- | --- | --- | ---\nWhat is x in the equation? | -1 <AND> -5 | x = 3 | 0.0\nWhat is x in the equation? | -1 <AND> -5 | x = -1 | 0.5\nWhat is x in the equation? | -1 <AND> -5 | x = -5 | 0.5\nWhat is x in the equation? | -1 <AND> -5 | x = -5 or 5 | 0.5\nWhat is x in the equation? | -1 <AND> -5 | x = -1 or x = -5 | 1.0\nCan you explain this meme? | This meme is poking fun at the fact that the names of the countries Iceland and Greenland are misleading. Despite its name, Iceland is known for its beautiful green landscapes, while Greenland is mostly covered in ice and snow. The meme is saying that the person has trust issues because the names of these countries do not accurately represent their landscapes. | The meme talks about Iceland and Greenland. It's pointing out that despite their names, Iceland is not very icy and Greenland isn't very green. | 0.4\nCan you explain this meme? | This meme is poking fun at the fact that the names of the countries Iceland and Greenland are misleading. Despite its name, Iceland is known for its beautiful green landscapes, while Greenland is mostly covered in ice and snow. The meme is saying that the person has trust issues because the names of these countries do not accurately represent their landscapes. | The meme is using humor to point out the misleading nature of Iceland's and Greenland's names. Iceland, despite its name, has lush green landscapes while Greenland is mostly covered in ice and snow. The text 'This is why I have trust issues' is a playful way to suggest that these contradictions can lead to distrust or confusion. The humor in this meme is derived from the unexpected contrast between the names of the countries and their actual physical characteristics. | 1.0\n\"\"\"\n\n\nclass MMVetDataset(BaseEvalDataset):\n    def __init__(\n        self,\n        data_path: str = \"Otter-AI/MMVet\",\n        gpt_model: str = \"gpt-4-0613\",\n        *,\n        api_key: str,\n        split: str = \"test\",\n        cache_dir: Union[str, None] = None,\n        default_output_path: str = \"./logs/MMVet\",\n        num_run: int = 1,\n        prompt: str = MM_VET_PROMPT,\n        decimail_places: int = 1,  # number of decimal places to round to\n        debug: bool = False,\n    ):\n        super().__init__(\"MMVetDataset\", data_path)\n        self.df = load_dataset(data_path, split=split, cache_dir=cache_dir).to_pandas()\n        self.default_output_path = default_output_path\n        self.prompt = prompt\n        self.gpt_model = gpt_model\n        self.num_run = num_run\n        self.decimal_places = decimail_places\n        self.api_key = api_key\n        self.cur_datetime = utc_plus_8_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.debug = debug\n        self.prepare()\n        self.client = OpenAI(api_key=api_key)\n\n    def prepare(self):\n        self.counter = Counter()\n        self.cap_set_list = []\n        self.cap_set_counter = []\n        self.len_data = 0\n        self.caps = {}\n\n        for index, row in self.df.iterrows():\n            self.caps[row[\"id\"]] = row[\"capability\"]\n\n        for cap in self.df[\"capability\"]:\n            cap = set(cap)\n            self.counter.update(cap)\n            if cap not in self.cap_set_list:\n                self.cap_set_list.append(cap)\n                self.cap_set_counter.append(1)\n            else:\n                self.cap_set_counter[self.cap_set_list.index(cap)] += 1\n\n            self.len_data += 1\n\n        sorted_list = self.counter.most_common()\n        self.columns = [k for k, v in sorted_list]\n        self.columns.append(\"total\")\n        self.columns.append(\"std\")\n        self.columns.append(\"runs\")\n        self.result1 = pd.DataFrame(columns=self.columns)\n\n        cap_set_sorted_indices = np.argsort(-np.array(self.cap_set_counter))\n        new_cap_set_list = []\n        new_cap_set_counter = []\n        for index in cap_set_sorted_indices:\n            new_cap_set_list.append(self.cap_set_list[index])\n            new_cap_set_counter.append(self.cap_set_counter[index])\n\n        self.cap_set_list = new_cap_set_list\n        self.cap_set_counter = new_cap_set_counter\n        self.cap_set_names = [\"_\".join(list(cap_set)) for cap_set in self.cap_set_list]\n\n        self.columns2 = self.cap_set_names\n        self.columns2.append(\"total\")\n        self.columns2.append(\"std\")\n        self.columns2.append(\"runs\")\n        self.result2 = pd.DataFrame(columns=self.columns2)\n\n    def get_output_file_name(self, model, *, output_path: str = None, num_run: int = 1) -> str:\n        if output_path is None:\n            result_path = self.default_output_path\n        else:\n            result_path = output_path\n        if not os.path.exists(result_path):\n            os.makedirs(result_path)\n        model_results_file = os.path.join(result_path, f\"{model.name}.json\")\n        grade_file = f\"{model.name}-{self.gpt_model}-grade-{num_run}runs-{self.cur_datetime}.json\"\n        grade_file = os.path.join(result_path, grade_file)\n        cap_score_file = f\"{model.name}-{self.gpt_model}-cap-score-{num_run}runs-{self.cur_datetime}.csv\"\n        cap_score_file = os.path.join(result_path, cap_score_file)\n        cap_int_score_file = f\"{model.name}-{self.gpt_model}-cap-int-score-{num_run}runs-{self.cur_datetime}.csv\"\n        cap_int_score_file = os.path.join(result_path, cap_int_score_file)\n        return model_results_file, grade_file, cap_score_file, cap_int_score_file\n\n    def _evaluate(self, model):\n        model_results_file, grade_file, cap_score_file, cap_int_score_file = self.get_output_file_name(model)\n\n        if os.path.exists(grade_file):\n            with open(grade_file, \"r\") as f:\n                grade_results = json.load(f)\n        else:\n            grade_results = {}\n\n        def need_more_runs():\n            need_more_runs = False\n            if len(grade_results) > 0:\n                for k, v in grade_results.items():\n                    if len(v[\"score\"]) < self.num_run:\n                        need_more_runs = True\n                        break\n            return need_more_runs or len(grade_results) < self.len_data\n\n        print(f\"grade results saved to {grade_file}\")\n        while need_more_runs():\n            for j in range(self.num_run):\n                print(f\"eval run {j}\")\n                for _, line in tqdm(self.df.iterrows(), total=len(self.df)):\n                    id = line[\"id\"]\n                    # if sub_set is not None and id not in sub_set:\n                    #     continue\n                    if id in grade_results and len(grade_results[id][\"score\"]) >= (j + 1):\n                        continue\n\n                    model_pred = model.generate(line[\"instruction\"], line[\"images\"][0])\n                    if self.debug:\n                        print(f\"# Query: {line['instruction']}\")\n                        print(f\"# Response: {model_pred}\")\n                        print(f\"# Ground Truth: {line['answer']}\")\n\n                    question = (\n                        self.prompt\n                        + \"\\n\"\n                        + \" | \".join(\n                            [\n                                line[\"instruction\"],\n                                line[\"answer\"].replace(\"<AND>\", \" <AND> \").replace(\"<OR>\", \" <OR> \"),\n                                model_pred,\n                                \"\",\n                            ]\n                        )\n                    )\n                    messages = [\n                        {\"role\": \"user\", \"content\": question},\n                    ]\n\n                    if id not in grade_results:\n                        sample_grade = {\"model\": [], \"content\": [], \"score\": []}\n                    else:\n                        sample_grade = grade_results[id]\n\n                    grade_sample_run_complete = False\n                    temperature = 0.0\n\n                    while not grade_sample_run_complete:\n                        try:\n                            response = self.client.chat.completions.create(model=self.gpt_model, max_tokens=3, temperature=temperature, messages=messages, timeout=15)\n                            content = response[\"choices\"][0][\"message\"][\"content\"]\n                            flag = True\n                            try_time = 1\n                            while flag:\n                                try:\n                                    content = content.split(\" \")[0].strip()\n                                    score = float(content)\n                                    if score > 1.0 or score < 0.0:\n                                        assert False\n                                    flag = False\n                                except:\n                                    question = (\n                                        self.prompt\n                                        + \"\\n\"\n                                        + \" | \".join(\n                                            [\n                                                line[\"instruction\"],\n                                                line[\"answer\"].replace(\"<AND>\", \" <AND> \").replace(\"<OR>\", \" <OR> \"),\n                                                model_pred,\n                                                \"\",\n                                            ]\n                                        )\n                                        + \"\\nPredict the correctness of the answer (digit): \"\n                                    )\n                                    messages = [\n                                        {\"role\": \"user\", \"content\": question},\n                                    ]\n                                    response = self.client.chat.completions.create(model=self.gpt_model, max_tokens=3, temperature=temperature, messages=messages, timeout=15)\n                                    content = response[\"choices\"][0][\"message\"][\"content\"]\n                                    try_time += 1\n                                    temperature += 0.5\n                                    print(f\"{id} try {try_time} times\")\n                                    print(content)\n                                    if try_time > 5:\n                                        score = 0.0\n                                        flag = False\n                            grade_sample_run_complete = True\n                        except Exception as e:\n                            # gpt4 may have token rate limit\n                            print(e)\n                            print(\"sleep 15s\")\n                            time.sleep(15)\n\n                    if len(sample_grade[\"model\"]) >= j + 1:\n                        sample_grade[\"model\"][j] = response[\"model\"]\n                        sample_grade[\"content\"][j] = content\n                        sample_grade[\"score\"][j] = score\n                    else:\n                        sample_grade[\"model\"].append(response[\"model\"])\n                        sample_grade[\"content\"].append(content)\n                        sample_grade[\"score\"].append(score)\n                        sample_grade[\"query\"] = line[\"instruction\"]\n                        sample_grade[\"response\"] = model_pred\n                        sample_grade[\"ground_truth\"] = line[\"answer\"]\n                    grade_results[id] = sample_grade\n\n                    with open(grade_file, \"w\") as f:\n                        json.dump(grade_results, f, indent=4)\n\n        cap_socres = {k: [0.0] * self.num_run for k in self.columns[:-2]}\n        self.counter[\"total\"] = self.len_data\n\n        cap_socres2 = {k: [0.0] * self.num_run for k in self.columns2[:-2]}\n        counter2 = {self.columns2[i]: self.cap_set_counter[i] for i in range(len(self.cap_set_counter))}\n        counter2[\"total\"] = self.len_data\n\n        for k, v in grade_results.items():\n            # if sub_set is not None and k not in sub_set:\n            #     continue\n            for i in range(self.num_run):\n                score = v[\"score\"][i]\n                caps = set(self.caps[k])\n                for c in caps:\n                    cap_socres[c][i] += score\n\n                cap_socres[\"total\"][i] += score\n\n                index = self.cap_set_list.index(caps)\n                cap_socres2[self.cap_set_names[index]][i] += score\n                cap_socres2[\"total\"][i] += score\n\n        for k, v in cap_socres.items():\n            cap_socres[k] = np.array(v) / self.counter[k] * 100\n\n        std = round(cap_socres[\"total\"].std(), self.decimal_places)\n        total_copy = cap_socres[\"total\"].copy()\n        runs = str(list(np.round(total_copy, self.decimal_places)))\n\n        for k, v in cap_socres.items():\n            cap_socres[k] = round(v.mean(), self.decimal_places)\n\n        cap_socres[\"std\"] = std\n        cap_socres[\"runs\"] = runs\n        self.result1.loc[model.name] = cap_socres\n\n        for k, v in cap_socres2.items():\n            cap_socres2[k] = round(np.mean(np.array(v) / counter2[k] * 100), self.decimal_places)\n        cap_socres2[\"std\"] = std\n        cap_socres2[\"runs\"] = runs\n        self.result2.loc[model.name] = cap_socres2\n\n        self.result1.to_csv(cap_score_file)\n        self.result2.to_csv(cap_int_score_file)\n\n        print(f\"cap score saved to {cap_score_file}\")\n        print(f\"cap int score saved to {cap_int_score_file}\")\n        print(\"=\" * 20)\n        print(f\"cap score:\")\n        print(self.result1)\n        print(\"=\" * 20)\n        print(f\"cap int score:\")\n        print(self.result2)\n        print(\"=\" * 20)\n\n\nif __name__ == \"__main__\":\n    data = MMVetDataset(api_key=None, cache_dir=\"/data/pufanyi/cache\")\n"}
{"type": "source_file", "path": "pipeline/benchmarks/models/__init__.py", "content": ""}
{"type": "source_file", "path": "pipeline/benchmarks/datasets/mmbench.py", "content": "import os\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom datasets import load_dataset\nfrom .base_eval_dataset import BaseEvalDataset\nimport pytz\nimport datetime\n\nutc_plus_8 = pytz.timezone(\"Asia/Singapore\")  # You can also use 'Asia/Shanghai', 'Asia/Taipei', etc.\nutc_now = pytz.utc.localize(datetime.datetime.utcnow())\nutc_plus_8_time = utc_now.astimezone(utc_plus_8)\n\n\nclass MMBenchDataset(BaseEvalDataset):\n    def __init__(\n        self,\n        data_path: str = \"Otter-AI/MMBench\",\n        *,\n        sys_prompt=\"There are several options:\",\n        version=\"20230712\",\n        split=\"test\",\n        cache_dir=None,\n        default_output_path=\"./logs/MMBench\",\n        debug=False,\n    ):\n        super().__init__(\"MMBenchDataset\", data_path)\n        self.version = str(version)\n        self.name_converter = {\"dev\": \"validation\", \"test\": \"test\"}\n        self.df = load_dataset(data_path, self.version, split=self.name_converter[split], cache_dir=cache_dir).to_pandas()\n        self.default_output_path = default_output_path\n        if os.path.exists(self.default_output_path) is False:\n            os.makedirs(self.default_output_path)\n        self.sys_prompt = sys_prompt\n        self.cur_datetime = utc_plus_8_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.debug = debug\n\n    def load_from_df(self, idx, key):\n        if key in self.df.columns:\n            value = self.df.loc[idx, key]\n            return value if pd.notna(value) else None\n        return None\n\n    def create_options_prompt(self, idx, option_candidate):\n        available_keys = set(self.df.columns) & set(option_candidate)\n        options = {cand: self.load_from_df(idx, cand) for cand in available_keys if self.load_from_df(idx, cand)}\n        sorted_options = dict(sorted(options.items()))\n        options_prompt = f\"{self.sys_prompt}\\n\"\n        for key, item in sorted_options.items():\n            options_prompt += f\"{key}. {item}\\n\"\n        return options_prompt.rstrip(\"\\n\"), sorted_options\n\n    def get_data(self, idx):\n        row = self.df.loc[idx]\n        option_candidate = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n        options_prompt, options_dict = self.create_options_prompt(idx, option_candidate)\n\n        data = {\n            \"img\": row[\"image\"],\n            \"question\": row[\"question\"],\n            \"answer\": row.get(\"answer\"),\n            \"options\": options_prompt,\n            \"category\": row[\"category\"],\n            \"l2-category\": row[\"l2-category\"],\n            \"options_dict\": options_dict,\n            \"index\": row[\"index\"],\n            \"hint\": self.load_from_df(idx, \"hint\"),\n            \"source\": row[\"source\"],\n            \"split\": row[\"split\"],\n        }\n        return data\n\n    def query_batch(self, model, batch_data):\n        batch_data = list(map(self.get_data, batch_data))\n        batch_img = [data[\"img\"] for data in batch_data]\n        batch_prompt = [f\"{data['hint']} {data['question']} {data['options']}\" if pd.notna(data[\"hint\"]) else f\"{data['question']} {data['options']}\" for data in batch_data]\n        if len(batch_prompt) == 1:\n            batch_pred_answer = [model.generate(batch_prompt[0], batch_img[0])]\n        else:\n            batch_pred_answer = model.generate(batch_prompt, batch_img)\n        return [\n            {\n                \"question\": data[\"question\"],\n                \"answer\": data[\"answer\"],\n                **data[\"options_dict\"],\n                \"prediction\": pred_answer,\n                \"hint\": data[\"hint\"],\n                \"source\": data[\"source\"],\n                \"split\": data[\"split\"],\n                \"category\": data[\"category\"],\n                \"l2-category\": data[\"l2-category\"],\n                \"index\": data[\"index\"],\n            }\n            for data, pred_answer in zip(batch_data, batch_pred_answer)\n        ]\n\n    def _evaluate(self, model, *, batch=1):\n        output_file = os.path.join(self.default_output_path, f\"{model.name}_mmbench_eval_result_{self.cur_datetime}.xlsx\")\n        results = []\n\n        for idx in tqdm(range(len(self.df))):\n            cur_data = self.get_data(idx)\n            cur_prompt = f\"{cur_data['hint']} {cur_data['question']} {cur_data['options']}\" if pd.notna(cur_data[\"hint\"]) and cur_data[\"hint\"] != \"nan\" else f\"{cur_data['question']} {cur_data['options']}\"\n            pred_answer = model.generate(cur_prompt, cur_data[\"img\"])\n\n            if self.debug:\n                print(f\"# Query: {cur_prompt}\")\n                print(f\"# Response: {pred_answer}\")\n\n            result = {\n                \"question\": cur_data[\"question\"],\n                \"answer\": cur_data[\"answer\"],\n                **cur_data[\"options_dict\"],\n                \"prediction\": pred_answer,\n                \"hint\": cur_data[\"hint\"],\n                \"source\": cur_data[\"source\"],\n                \"split\": cur_data[\"split\"],\n                \"category\": cur_data[\"category\"],\n                \"l2-category\": cur_data[\"l2-category\"],\n                \"index\": cur_data[\"index\"],\n            }\n            results.append(result)\n\n        df = pd.DataFrame(results)\n        with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n            df.to_excel(writer, index=False)\n        print(f\"MMBench Evaluator: Result saved to {output_file}.\")\n"}
{"type": "source_file", "path": "pipeline/benchmarks/evaluate.py", "content": "import sys\nimport argparse\nimport os\nimport yaml\nimport contextlib\n\nsys.path.append(\"../..\")\nfrom .models.base_model import load_model\nfrom .datasets.base_eval_dataset import load_dataset\n\n\ndef get_info(info):\n    if \"name\" not in info:\n        raise ValueError(\"Model name is not specified.\")\n    name = info[\"name\"]\n    # info.pop(\"name\")\n    return name, info\n\n\ndef load_models(model_infos):\n    for model_info in model_infos:\n        name, info = get_info(model_info)\n        model = load_model(name, info)\n        yield model\n\n\ndef load_datasets(dataset_infos):\n    for dataset_info in dataset_infos:\n        name, info = get_info(dataset_info)\n        dataset = load_dataset(name, info)\n        yield dataset\n\n\nclass DualOutput:\n    def __init__(self, file, stdout):\n        self.file = file\n        self.stdout = stdout\n\n    def write(self, data):\n        self.file.write(data)\n        self.stdout.write(data)\n\n    def flush(self):\n        self.file.flush()\n        self.stdout.flush()\n\n\nif __name__ == \"__main__\":\n    args = argparse.ArgumentParser()\n    args.add_argument(\n        \"--config\",\n        \"-c\",\n        type=str,\n        help=\"Path to the config file, suppors more specific configurations.\",\n        default=None,\n    )\n    args.add_argument(\n        \"--models\",\n        type=str,\n        nargs=\"?\",\n        help=\"Specify model names as comma separated values.\",\n        default=None,\n    )\n    args.add_argument(\n        \"--model_paths\",\n        type=str,\n        nargs=\"?\",\n        help=\"Specify model paths as comma separated values.\",\n        default=None,\n    )\n    args.add_argument(\n        \"--datasets\",\n        type=str,\n        nargs=\"?\",\n        help=\"Specify dataset names as comma separated values.\",\n        default=None,\n    )\n    args.add_argument(\n        \"--output\",\n        \"-o\",\n        type=str,\n        help=\"Output file path for logging results.\",\n        default=\"./logs/evaluation.txt\",\n    )\n    args.add_argument(\n        \"--cache_dir\",\n        type=str,\n        help=\"Cache directory for datasets.\",\n        default=None,\n    )\n\n    phrased_args = args.parse_args()\n\n    if phrased_args.config:\n        with open(phrased_args.config, \"r\") as f:\n            config = yaml.safe_load(f)\n        model_infos = config[\"models\"]\n        dataset_infos = config[\"datasets\"]\n        phrased_args.output = config[\"output\"] if \"output\" in config else phrased_args.output\n    else:\n        # Zip the models and their respective paths\n        model_names = phrased_args.models.split(\",\")\n        if phrased_args.model_paths is not None:\n            model_paths = phrased_args.model_paths.split(\",\")\n            model_infos = [{\"name\": name, \"model_path\": path} for name, path in zip(model_names, model_paths)]\n        else:\n            model_infos = [{\"name\": name} for name in model_names]\n        dataset_infos = [{\"name\": dataset_name, \"cache_dir\": phrased_args.cache_dir} for dataset_name in phrased_args.datasets.split(\",\")]\n\n    if not os.path.exists(os.path.dirname(phrased_args.output)):\n        os.makedirs(os.path.dirname(phrased_args.output))\n\n    with open(phrased_args.output, \"w\") as outfile, contextlib.redirect_stdout(DualOutput(outfile, sys.stdout)):\n        print(\"=\" * 80)\n        print(\" \" * 30 + \"EVALUATION REPORT\")\n        print(\"=\" * 80)\n        print()\n\n        for model_info in model_infos:\n            print(\"\\nMODEL INFO:\", model_info)\n            print(\"-\" * 80)\n            model = load_model(model_info[\"name\"], model_info)\n\n            dataset_count = 0\n            for dataset in load_datasets(dataset_infos):\n                dataset_count += 1\n                print(f\"\\nDATASET: {dataset.name}\")\n                print(\"-\" * 20)\n\n                dataset.evaluate(model)  # Assuming this function now prints results directly.\n                print()\n\n            print(\"-\" * 80)\n            print(f\"Total Datasets Evaluated: {dataset_count}\\n\")\n\n        print(\"=\" * 80)\n\n# python evaluate.py --models otter_image --datasets mmbench\n"}
{"type": "source_file", "path": "mimic-it/convert-it/datasets/change.py", "content": "import os\nimport json\n\nfrom abstract_dataset import AbstractDataset\nfrom tqdm import tqdm\nfrom glob import glob\nfrom concurrent.futures import ThreadPoolExecutor\nfrom image_utils import create_folder\n\n\nclass SpotTheDifference(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"SpotTheDifference\",\n        short_name=\"SD\",\n        *,\n        image_path: str,\n        num_threads: int,\n    ):\n        \"\"\"\n        Initializes a SpotTheDifference dataset.\n\n        Args:\n            name (str): The name of the dataset. Defaults to \"SpotTheDifference\".\n            short_name (str): The short name of the dataset. Defaults to \"SD\".\n            image_path (str): The path containing the dataset images, downloaded from\n                https://drive.google.com/file/d/1OVb4_3Uec_xbyUk90aWC6LFpKsIOtR7v/view?usp=sharing.\n            num_threads (int): The number of threads to use for processing the images.\n        \"\"\"\n        super().__init__(name, short_name, image_path, num_threads)\n\n    def _load_images(self, image_path: str, num_thread: int) -> dict[str, bytes]:\n        \"\"\"\n        Loads the images from the dataset.\n\n        Args:\n            image_path (str): The path to the directory containing the dataset images.\n            num_threads (int): The number of threads to use for processing the images.\n\n        Returns:\n            dict[str, bytes]: A dictionary where the keys are image identifiers and the values are image bytes.\n        \"\"\"\n        file_names = glob(os.path.join(image_path, \"*\"))\n        names = set()\n        for file_name in file_names:\n            image_name = file_name.split(\"/\")[-1].split(\".\")[0]\n            id = image_name.split(\"_\")[0]\n            names.add(id)\n        ids = list(sorted(list(names)))\n\n        jpgs_path = glob(os.path.join(image_path, \"*.jpg\"))\n        jpegs_path = glob(os.path.join(image_path, \"*.jpeg\"))\n        pngs_path = glob(os.path.join(image_path, \"*.png\"))\n        jpgs = set()\n        pngs = set()\n        for path in jpgs_path:\n            jpgs.add(path.split(\"/\")[-1].split(\".\")[0])\n        for path in pngs_path:\n            pngs.add(path.split(\"/\")[-1].split(\".\")[0])\n\n        def get_path(file_name):\n            if file_name in jpgs:\n                return os.path.join(image_path, file_name + \".jpg\")\n            elif file_name in pngs:\n                # print(\"file_name\", file_name, os.path.join(image_path, file_name + \".png\"))\n                return os.path.join(image_path, file_name + \".png\")\n            elif file_name in jpegs_path:\n                return os.path.join(image_path, file_name + \".jpeg\")\n            else:\n                # print(\"===================================\", file_name)\n                raise Exception(\"File not found\")\n\n        def read_image(file_name) -> bytes:\n            with open(file_name, \"rb\") as f:\n                return f.read()\n\n        file_not_found = []\n\n        images = {}\n\n        for id in tqdm(ids, desc=\"Reading images\"):\n            try:\n                file_1 = get_path(id)\n                file_2 = get_path(id + \"_2\")\n                # print(file_1, file_2)\n                images[id.zfill(5) + \"_1\"] = read_image(file_1)\n                images[id.zfill(5) + \"_2\"] = read_image(file_2)\n            except Exception as e:\n                file_not_found.append(id)\n                print(f\"File not found: {id}\")\n                # print(f\"Error: {e}\")\n\n        create_folder(\"log\")\n        with open(\"log/file_not_found.log\", \"w\") as f:\n            json.dump(file_not_found, f, indent=4)\n\n        return images\n\n\nclass CocoGeneralDifference(AbstractDataset):\n    def __init__(\n        self,\n        name: str = \"CocoGeneralDifference\",\n        short_name=\"CGD\",\n        *,\n        image_path: str,\n        num_threads: int,\n    ):\n        \"\"\"\n        Initializes a CocoGeneralDifference dataset.\n\n        Args:\n            name (str): The name of the dataset. Defaults to \"CocoGeneralDifference\".\n            short_name (str): The short name of the dataset. Defaults to \"CGD\".\n            image_path (str): The path containing the dataset images, downloaded from\n                http://images.cocodataset.org/zips/train2017.zip.\n            num_threads (int): The number of threads to use for processing the images.\n        \"\"\"\n        super().__init__(name, short_name, image_path, num_threads)\n\n    def _load_images(self, image_path: str, num_thread: int) -> dict[str, bytes]:\n        \"\"\"\n        Loads the images from the dataset.\n\n        Args:\n            image_path (str): The path to the directory containing the dataset images.\n            num_threads (int): The number of threads to use for processing the images.\n\n        Returns:\n            dict[str, bytes]: A dictionary where the keys are image identifiers and the values are image bytes.\n        \"\"\"\n        file_names = glob(os.path.join(image_path, \"*\"))\n\n        def read_image(file_name):\n            image_name = os.path.basename(file_name).split(\".\")[0]\n            with open(file_name, \"rb\") as f:\n                return image_name, f.read()\n\n        images = {}\n\n        pbar = tqdm(total=len(file_names), desc=\"Loading images\", unit=\"image\")\n\n        with ThreadPoolExecutor(max_workers=num_thread) as executor:\n            for image_name, image in executor.map(read_image, file_names):\n                images[image_name] = image\n                pbar.update(1)\n        pbar.close()\n\n        return images\n"}
{"type": "source_file", "path": "pipeline/benchmarks/models/instructblip.py", "content": "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\nfrom PIL import Image\nfrom .base_model import BaseModel\nimport torch\nimport numpy as np\nimport warnings\nimport io\nimport base64\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef get_pil_image(raw_image_data) -> Image.Image:\n    if isinstance(raw_image_data, Image.Image):\n        return raw_image_data\n\n    elif isinstance(raw_image_data, dict) and \"bytes\" in raw_image_data:\n        return Image.open(io.BytesIO(raw_image_data[\"bytes\"]))\n\n    elif isinstance(raw_image_data, str):  # Assuming this is a base64 encoded string\n        image_bytes = base64.b64decode(raw_image_data)\n        return Image.open(io.BytesIO(image_bytes))\n\n    else:\n        raise ValueError(\"Unsupported image data format\")\n\n\nclass InstructBLIP(BaseModel):\n    def __init__(self, model_path: str = \"Salesforce/instructblip-vicuna-7b\", cuda_id: int = 0, max_new_tokens=32):\n        super().__init__(\"instructblip\", model_path)\n        self.device = f\"cuda:{cuda_id}\" if torch.cuda.is_available() else \"cpu\"\n        self.model = InstructBlipForConditionalGeneration.from_pretrained(model_path).to(self.device)\n        self.processor = InstructBlipProcessor.from_pretrained(model_path)\n        self.max_new_tokens = max_new_tokens\n\n    def generate(self, text_prompt: str, raw_image_data: str):\n        raw_image_data = get_pil_image(raw_image_data)\n        raw_image_data = raw_image_data.convert(\"RGB\")\n        formatted_prompt = f\"{text_prompt}\\nAnswer:\"\n        # Accordling to https://huggingface.co/Salesforce/instructblip-vicuna-7b . Seems that is is no special prompt format for instruct blip\n        model_inputs = self.processor(images=raw_image_data, text=formatted_prompt, return_tensors=\"pt\").to(self.device)\n        # We follow the recommended parameter here:https://huggingface.co/Salesforce/instructblip-vicuna-7b\n        generation_output = self.model.generate(**model_inputs, do_sample=False, max_new_tokens=self.max_new_tokens, min_length=1)\n        generation_text = self.processor.batch_decode(generation_output, skip_special_tokens=True)\n        return generation_text[0]\n\n    def eval_forward(self, question, answer, image):\n        raise NotImplementedError\n"}
{"type": "source_file", "path": "pipeline/benchmarks/models/idefics.py", "content": "import io\nimport torch\nfrom typing import List\nfrom transformers import IdeficsForVisionText2Text, AutoProcessor\nfrom PIL import Image\nfrom .base_model import BaseModel\nfrom pipeline.train.train_utils import find_and_remove_tokens, get_image_attention_mask\nimport base64\nimport numpy as np\n\n\ndef get_pil_image(raw_image_data) -> Image.Image:\n    if isinstance(raw_image_data, Image.Image):\n        return raw_image_data\n\n    elif isinstance(raw_image_data, dict) and \"bytes\" in raw_image_data:\n        return Image.open(io.BytesIO(raw_image_data[\"bytes\"]))\n\n    elif isinstance(raw_image_data, str):  # Assuming this is a base64 encoded string\n        image_bytes = base64.b64decode(raw_image_data)\n        return Image.open(io.BytesIO(image_bytes))\n\n    else:\n        raise ValueError(\"Unsupported image data format\")\n\n\ndef get_single_formatted_prompt(question, image=None, answer=\"\") -> List[str]:\n    if answer == \"\":\n        return [\n            f\"User:\",\n            get_pil_image(image),\n            question,\n            \"<end_of_utterance>\\n\",\n            \"Assistant:\",\n        ]\n    else:\n        return [\n            f\"User:\",\n            get_pil_image(image),\n            question,\n            \"<end_of_utterance>\\n\",\n            f\"Assistant:<answer> {answer}\",\n            \"<end_of_utterance>\",\n        ]\n\n\ndef get_formatted_prompt(questions, images, answers=\"\"):\n    single_prompt = False\n    if not isinstance(questions, list):\n        questions = [questions]\n        single_prompt = True\n    if not isinstance(images, list):\n        images = [images]\n    if not isinstance(answers, list):\n        answers = [answers] * len(questions)\n    result = []\n    for question, image, answer in zip(questions, images, answers):\n        result.append(get_single_formatted_prompt(question, image, answer))\n    if single_prompt:\n        return result[0]\n    else:\n        return result\n\n\nclass Idefics(BaseModel):\n    def __init__(self, model_path: str = \"HuggingFaceM4/idefics-9b-instruct\", batch=8):\n        super().__init__(\"idefics\", model_path, max_batch_size=batch)\n        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n        self.model = IdeficsForVisionText2Text.from_pretrained(model_path, device_map={\"\": self.device}, torch_dtype=torch.bfloat16).to(self.device)\n        self.processor = AutoProcessor.from_pretrained(model_path)\n        if \"<answer>\" not in self.processor.tokenizer.special_tokens_map[\"additional_special_tokens\"]:\n            past_special_tokens = self.processor.tokenizer.special_tokens_map[\"additional_special_tokens\"]\n            self.processor.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<answer>\"] + past_special_tokens})\n\n        self.fake_token_image_token_id = self.processor.tokenizer(\"<fake_token_around_image>\", add_special_tokens=False)[\"input_ids\"][-1]\n        self.endofchunk_text = \"<end_of_utterance>\"\n        self.endofchunk_token_id = self.processor.tokenizer(self.endofchunk_text, add_special_tokens=False)[\"input_ids\"][-1]\n        self.answer_token_id = self.processor.tokenizer(\"<answer>\", add_special_tokens=False)[\"input_ids\"][-1]\n        self.eos_token_id = self.processor.tokenizer(self.processor.tokenizer.eos_token, add_special_tokens=False)[\"input_ids\"][-1]\n        self.patch_resize_transform = self.processor.image_processor.preprocess\n\n    def generate(self, question, raw_image_data):\n        formatted_prompt = get_formatted_prompt(question, raw_image_data)\n        inputs = self.processor(formatted_prompt, return_tensors=\"pt\").to(self.device)\n        exit_condition = self.processor.tokenizer(\"<end_of_utterance>\", add_special_tokens=False).input_ids\n        bad_words_ids = self.processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n        generated_ids = self.model.generate(\n            **inputs,\n            eos_token_id=exit_condition,\n            bad_words_ids=bad_words_ids,\n            max_new_tokens=768,\n            temperature=0.2,\n            do_sample=True,\n            top_p=0.5,\n        )\n        generated_text = self.processor.batch_decode(generated_ids)\n        results = list(map(lambda text: text.strip().split(\"Assistant:\")[-1].split(\"<end_of_utterance>\")[0].strip(), generated_text))\n        if isinstance(question, str):\n            return results[0]\n        else:\n            return results\n\n    def prepare_labels(self, input_ids, eos_token_id, answer_token_id, endofchunk_token_id, fake_token_image_token_id, masking_number: int = -100):\n        labels = torch.empty(input_ids.shape, dtype=torch.int64)\n        for i in range(input_ids.shape[0]):\n            labels[i] = torch.where(input_ids[i] == eos_token_id, eos_token_id, masking_number)\n            answer_token_ids_all = torch.where(input_ids[i] == answer_token_id)[0]\n            endofchunk_token_ids_all = torch.where(input_ids[i] == endofchunk_token_id)[0]\n\n            j = 0  # Counter for endofchunk_token_ids\n            for answer_token_idx in answer_token_ids_all:\n                # Find the closest endofchunk_token_id that is greater than answer_token_id\n                while j < len(endofchunk_token_ids_all) and endofchunk_token_ids_all[j] < answer_token_idx:\n                    j += 1\n\n                if j < len(endofchunk_token_ids_all):\n                    endofchunk_token_idx = endofchunk_token_ids_all[j]\n                    labels[i, answer_token_idx + 1 : endofchunk_token_idx + 1] = input_ids[i, answer_token_idx + 1 : endofchunk_token_idx + 1]\n\n                    # Increment j for the next iteration\n                    j += 1\n\n        labels[:, 0] = masking_number\n        labels[labels == fake_token_image_token_id] = masking_number\n        return labels\n\n    def eval_forward(self, question, answer, image):\n        forward_prompt = f\"User:<fake_token_around_image><image><fake_token_around_image>{question}<end_of_utterance>\\nAssistant:<answer>{answer}<end_of_utterance>\"\n        inputs = self.processor.tokenizer(forward_prompt, return_tensors=\"pt\")\n        vision_x = self.patch_resize_transform(image).unsqueeze(0).to(self.device)\n        labels = self.prepare_labels(\n            inputs[\"input_ids\"],\n            self.eos_token_id,\n            self.answer_token_id,\n            self.endofchunk_token_id,\n            self.fake_token_image_token_id,\n        )\n        input_ids, labels, attention_mask = find_and_remove_tokens(\n            inputs[\"input_ids\"], labels, inputs[\"attention_mask\"], self.answer_token_id, self.processor.tokenizer\n        )  # find and remove certain tokens from input_ids, labels, and attention_mask\n        # input_ids = inputs[\"input_ids\"]\n        # attention_mask = inputs[\"attention_mask\"]\n        image_attention_mask = get_image_attention_mask(input_ids, 1, self.processor.tokenizer)\n        # vision_x = inputs[\"pixel_values\"]\n        # query = get_formatted_forward_prompt(question, answer)\n        # tokens = self.tokenizer(query, return_tensors=\"pt\")\n        # input_ids = tokens[\"input_ids\"]\n        # attention_mask = tokens[\"attention_mask\"]\n        with torch.no_grad():\n            loss = self.model(\n                pixel_values=vision_x,\n                input_ids=input_ids.to(self.device),\n                attention_mask=attention_mask.to(self.device),\n                image_attention_mask=image_attention_mask.to(self.device),\n                labels=labels.to(self.device),\n                # input_ids=input_ids,\n                # attention_mask=attention_mask,\n                # image_attention_mask=image_attention_mask,\n                # vision_x=vision_x,\n                # labels=labels,\n            ).loss\n        return loss\n\n    def eval_forward_batch(self, batch_questions, batch_options, batch_images):\n        batch_size = len(batch_questions)\n        all_option_losses = []\n        tensor_images = [self.patch_resize_transform(image).unsqueeze(0) for image in batch_images]\n\n        # Prepare batched inputs and put them on the device\n        batch_input_ids = []\n        batch_attention_mask = []\n        batch_prompt = []\n\n        for i in range(batch_size):\n            question = batch_questions[i]\n            option = batch_options[i]\n            forward_prompt = f\"User:<fake_token_around_image><image><fake_token_around_image>{question}<end_of_utterance>\\nAssistant:<answer>{option}<end_of_utterance>\"\n            batch_prompt.append(forward_prompt)\n\n        inputs = self.processor.tokenizer(batch_prompt, return_tensors=\"pt\", padding=\"longest\", truncation=True, max_length=512)\n        batch_input_ids.append(inputs[\"input_ids\"])\n        batch_attention_mask.append(inputs[\"attention_mask\"])\n\n        batch_input_ids = torch.cat(batch_input_ids, dim=0)\n        batch_attention_mask = torch.cat(batch_attention_mask, dim=0)\n        batch_labels = self.prepare_labels(\n            batch_input_ids,\n            self.eos_token_id,\n            self.answer_token_id,\n            self.endofchunk_token_id,\n            self.fake_token_image_token_id,\n        )\n\n        batch_input_ids, batch_labels, batch_attention_mask = find_and_remove_tokens(batch_input_ids, batch_labels, batch_attention_mask, self.answer_token_id, self.processor.tokenizer)\n\n        # to device\n        batch_image_tensors = torch.stack(tensor_images).to(self.device)\n        batch_input_ids = batch_input_ids.to(self.device)\n        batch_labels = batch_labels.to(self.device)\n        batch_attention_mask = batch_attention_mask.to(self.device)\n\n        # Perform batch inference\n        with torch.no_grad():\n            # Your forward function can go here, adjusted for batches\n            outputs = self.model(\n                pixel_values=batch_image_tensors.squeeze(2),\n                input_ids=batch_input_ids,\n                attention_mask=batch_attention_mask,\n                image_attention_mask=get_image_attention_mask(batch_input_ids, 1, self.processor.tokenizer).to(self.device),\n                labels=batch_labels,\n                # more arguments as needed\n            )\n\n        # Assuming `outputs.per_token_loss` contains the loss for each token for each item in the batch\n        per_token_loss = outputs.per_token_loss  # Shape would be [batch_size, sequence_length]\n\n        # Summing along the sequence length dimension to get per-item loss\n        per_item_loss = torch.sum(per_token_loss, dim=1)  # Shape [batch_size]\n        all_option_losses = np.split(per_item_loss, batch_size)\n\n        return all_option_losses\n\n\nif __name__ == \"__main__\":\n    model = Idefics(\"/data/pufanyi/training_data/checkpoints/idefics-9b-instruct\")\n    print(\n        model.generate(\n            \"What is in this image?\",\n            Image.open(\"/data/pufanyi/project/Otter-2/pipeline/evaluation/test_data/test.jpg\"),\n        )\n    )\n"}
{"type": "source_file", "path": "pipeline/benchmarks/datasets/magnifierbench.py", "content": "import base64\nimport io\nfrom PIL import Image\nimport json\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nimport os\nimport numpy as np\nfrom datasets import load_dataset\nfrom typing import Union\nfrom .base_eval_dataset import BaseEvalDataset\nfrom tqdm import tqdm\nimport datetime\nimport pytz\nimport re\n\nimport time\nimport requests\n\nutc_plus_8 = pytz.timezone(\"Asia/Singapore\")  # You can also use 'Asia/Shanghai', 'Asia/Taipei', etc.\nutc_now = pytz.utc.localize(datetime.datetime.utcnow())\nutc_plus_8_time = utc_now.astimezone(utc_plus_8)\n\n\ndef get_chat_response(promot, api_key, model=\"gpt-4-0613\", temperature=0, max_tokens=256, n=1, patience=5, sleep_time=5):\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\",\n    }\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant. Your task is to judge whether the model response is correct to answer the given question or not.\"},\n        {\"role\": \"user\", \"content\": promot},\n    ]\n\n    payload = {\"model\": model, \"messages\": messages}\n\n    while patience > 0:\n        patience -= 1\n        try:\n            response = requests.post(\n                \"https://api.openai.com/v1/chat/completions\",\n                headers=headers,\n                data=json.dumps(payload),\n                timeout=30,\n            )\n            response.raise_for_status()\n            response_data = response.json()\n\n            prediction = response_data[\"choices\"][0][\"message\"][\"content\"].strip()\n            if prediction != \"\" and prediction is not None:\n                return prediction\n\n        except Exception as e:\n            if \"Rate limit\" not in str(e):\n                print(e)\n            time.sleep(sleep_time)\n\n    return \"\"\n\n\ndef prepare_query(model_answer_item, api_key):\n    freeform_question = model_answer_item[\"freeform_question\"]\n    freeform_response = model_answer_item[\"freeform_response\"]\n    correct_answer = model_answer_item[\"freeform_answer\"]\n\n    # Formulating the prompt for ChatGPT\n    prompt = f\"Question: {freeform_question}\\nModel Response: {freeform_response}\\nGround Truth: {correct_answer}\\nWill the model response be considered correct? You should only answer yes or no.\"\n\n    # Querying ChatGPT\n    chat_response = get_chat_response(prompt, api_key)\n\n    return chat_response\n\n\nclass MagnifierBenchDataset(BaseEvalDataset):\n    def __init__(\n        self,\n        data_path: str = \"Otter-AI/MagnifierBench\",\n        *,\n        cache_dir: Union[str, None] = None,\n        default_output_path: str = \"./logs/MagBench\",\n        split: str = \"test\",\n        debug: bool = False,\n        prompt=\"\",\n        api_key=None,\n    ):\n        super().__init__(\"MagnifierBench\", data_path)\n\n        self.default_output_path = default_output_path\n        if not os.path.exists(self.default_output_path):\n            os.makedirs(self.default_output_path)\n\n        self.cur_datetime = utc_plus_8_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n        self.data = load_dataset(data_path, split=split, cache_dir=cache_dir, revision=\"main\")\n        self.debug = debug\n        self.prompt = prompt\n        self.api_key = api_key\n\n    def parse_pred_ans(self, pred_ans, question):\n        match = re.search(r\"The answer is ([A-D])\", pred_ans)\n        if match:\n            return match.group(1)\n        choices = [\"A\", \"B\", \"C\", \"D\"]\n        for selection in choices:\n            if selection in pred_ans:\n                return selection\n        pattern = \"A\\\\. (.+?), B\\\\. (.+?), C\\\\. (.+?), D\\\\. (.+)\"\n        matches = re.search(pattern, question)\n        if matches:\n            options = {\"A\": matches.group(1), \"B\": matches.group(2), \"C\": matches.group(3), \"D\": matches.group(4)}\n            for c, option in options.items():\n                option = option.strip()\n                if option.endswith(\".\") or option.endswith(\",\") or option.endswith(\"?\"):\n                    option = option[:-1]\n                if option.upper() in pred_ans.upper():\n                    return c\n        for selection in choices:\n            if selection in pred_ans.upper():\n                return selection\n        return \"other\"\n\n    def _evaluate(self, model):\n        model_score_dict = {}\n\n        # output_path = os.path.join(self.default_output_path, f\"{model.name}_{self.cur_datetime}\")\n        # if not os.path.exists(output_path):\n        #     os.makedirs(output_path)\n        # model_path: str = \"Salesforce/instructblip-vicuna-7b\"\n        model_version = model.name.split(\"/\")[-1]\n        model_answer_path = os.path.join(self.default_output_path, f\"{model_version}_{self.cur_datetime}_answer.json\")\n        result_path = os.path.join(self.default_output_path, f\"{model_version}_{self.cur_datetime}_score.json\")\n        model_answer = {}\n\n        score = 0\n        num_data = 0\n\n        ff_score = 0\n\n        for data in tqdm(self.data, desc=\"Evaluating\", total=len(self.data)):\n            question = f\"{self.prompt} {data['instruction']}\" if self.prompt else data[\"instruction\"]\n            if len(data[\"images\"]) != 1:\n                print(f\"Warning: {data['id']} has {len(data['images'])} images.\")\n                print(f\"Skipping {data['id']}\")\n                continue\n\n            model_response = model.generate(question, data[\"images\"][0])\n\n            pred_ans = self.parse_pred_ans(model_response, question)\n\n            freeform_question = (question.split(\"?\")[0] + \"?\").replace(self.prompt, \"\").strip()\n            options = question.split(\"?\")[1]\n            answer_option = data[\"answer\"]\n            for single_opt in options.split(\",\"):\n                single_opt = single_opt.strip()\n                if single_opt.startswith(answer_option.upper()):\n                    freeform_answer = single_opt.split(\".\")[1].strip()\n                    break\n\n            ff_response = model.generate(freeform_question, data[\"images\"][0])\n            if self.debug:\n                print(f\"Question: {question}\")\n                print(f\"Answer: {data['answer']}\")\n                print(f\"Raw prediction: {model_response}\")\n                print(f\"Parsed prediction: {pred_ans}\\n\")\n                print(f\"Freeform question: {freeform_question}\")\n                print(f\"Freeform answer: {freeform_answer}\")\n                print(f\"Freeform response: {ff_response}\\n\")\n\n            num_data += 1\n            if pred_ans == data[\"answer\"]:\n                score += 1\n            model_answer[data[\"id\"]] = {\n                \"question\": question,\n                \"options\": options,\n                \"model_response\": model_response,\n                \"parsed_output\": pred_ans,\n                \"answer\": data[\"answer\"],\n                \"freeform_question\": freeform_question,\n                \"freeform_response\": ff_response,\n                \"freeform_answer\": freeform_answer,\n            }\n            with open(model_answer_path, \"w\") as f:\n                json.dump(model_answer, f, indent=2)\n\n        model_score_dict[\"score\"] = score\n        model_score_dict[\"total\"] = len(self.data)\n        model_score_dict[\"accuracy\"] = score / len(self.data)\n\n        print(f\"Start query GPT-4 for free-form evaluation...\")\n        for data_id in tqdm(model_answer.keys(), desc=\"Querying GPT-4\"):\n            model_answer_item = model_answer[data_id]\n            gpt_response = prepare_query(model_answer_item, self.api_key)\n            if gpt_response.lower() == \"yes\":\n                ff_score += 1\n            elif gpt_response.lower() == \"no\":\n                ff_score += 0\n            else:\n                print(f\"Warning: {data_id} has invalid GPT-4 response: {gpt_response}\")\n                print(f\"Skipping {data_id}\")\n                continue\n\n        model_score_dict[\"freeform_score\"] = ff_score\n        model_score_dict[\"freeform_accuracy\"] = ff_score / len(model_answer)\n\n        with open(result_path, \"w\") as f:\n            json.dump(model_score_dict, f, indent=2)\n\n        print(f\"Model answer saved to {model_answer_path}\")\n        print(f\"Model score saved to {result_path}\")\n        print(json.dumps(model_score_dict, indent=2))\n\n        return model_score_dict\n"}
{"type": "source_file", "path": "pipeline/benchmarks/models/otter_video.py", "content": "import mimetypes\nimport os\nfrom io import BytesIO\nfrom typing import Union\nimport cv2\nimport requests\nimport torch\nimport transformers\nfrom PIL import Image\nimport sys\n\nsys.path.append(\"/mnt/petrelfs/zhangyuanhan/Otter/\")\nfrom src.otter_ai.models.otter.modeling_otter import OtterForConditionalGeneration\nfrom .base_model import BaseModel\n\n# Disable warnings\nrequests.packages.urllib3.disable_warnings()\n\n\nclass OtterVideo(BaseModel):\n    def __init__(self, model_path=\"luodian/OTTER-Video-LLaMA7B-DenseCaption\", load_bit=\"bf16\"):\n        super().__init__(\"otter_video\", model_path)\n        precision = {}\n        if load_bit == \"bf16\":\n            precision[\"torch_dtype\"] = torch.bfloat16\n        elif load_bit == \"fp16\":\n            precision[\"torch_dtype\"] = torch.float16\n        elif load_bit == \"fp32\":\n            precision[\"torch_dtype\"] = torch.float32\n        self.model = OtterForConditionalGeneration.from_pretrained(model_path, device_map=\"sequential\", **precision)\n        self.tensor_dtype = {\n            \"fp16\": torch.float16,\n            \"bf16\": torch.bfloat16,\n            \"fp32\": torch.float32,\n        }[load_bit]\n        self.model.text_tokenizer.padding_side = \"left\"\n        self.tokenizer = self.model.text_tokenizer\n        self.image_processor = transformers.CLIPImageProcessor()\n        self.model.eval()\n\n    def get_formatted_prompt(self, prompt: str) -> str:\n        return f\"<image>User: {prompt} GPT:<answer>\"\n\n    def extract_frames(self, video_path, num_frames=16):\n        video = cv2.VideoCapture(video_path)\n        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n        frame_step = total_frames // num_frames\n        frames = []\n\n        for i in range(num_frames):\n            video.set(cv2.CAP_PROP_POS_FRAMES, i * frame_step)\n            ret, frame = video.read()\n            if ret:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frame = Image.fromarray(frame).convert(\"RGB\")\n                frames.append(frame)\n\n        video.release()\n        return frames\n\n    def get_response(\n        self,\n        input_data,\n        prompt: str,\n        model=None,\n        image_processor=None,\n        tensor_dtype=None,\n    ) -> str:\n        if isinstance(input_data, Image.Image):\n            vision_x = image_processor.preprocess([input_data], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n        elif isinstance(input_data, list):  # list of video frames\n            vision_x = image_processor.preprocess(input_data, return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(0).unsqueeze(0)\n        else:\n            raise ValueError(\"Invalid input data. Expected PIL Image or list of video frames.\")\n\n        lang_x = self.tokenizer(\n            [\n                self.get_formatted_prompt(prompt),\n            ],\n            return_tensors=\"pt\",\n        )\n\n        bad_words_id = self.tokenizer([\"User:\", \"GPT1:\", \"GFT:\", \"GPT:\"], add_special_tokens=False).input_ids\n        # import pdb;pdb.set_trace()\n        generated_text = self.model.generate(\n            vision_x=vision_x.to(model.device, dtype=tensor_dtype),\n            lang_x=lang_x[\"input_ids\"].to(model.device),\n            attention_mask=lang_x[\"attention_mask\"].to(model.device),\n            max_new_tokens=512,\n            num_beams=3,\n            no_repeat_ngram_size=3,\n            bad_words_ids=bad_words_id,\n        )\n        parsed_output = model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].lstrip().rstrip().split(\"<|endofchunk|>\")[0].lstrip().rstrip().lstrip('\"').rstrip('\"')\n        return parsed_output\n\n    def generate(self, input_data):\n        video_dir = input_data.get(\"video_root\", \"\")\n        frames_list = self.extract_frames(input_data[\"video_path\"])\n\n        object_description = input_data[\"object_description\"]\n\n        if object_description != \"None\":\n            context = f\"Given context:{object_description}. \"\n        else:\n            context = \"\"\n        prompts_input = context + input_data[\"question\"]\n\n        response = self.get_response(\n            frames_list,\n            prompts_input,\n            self.model,\n            self.image_processor,\n            self.tensor_dtype,\n        )\n\n        return response\n\n\nif __name__ == \"__main__\":\n    pass\n"}
{"type": "source_file", "path": "pipeline/benchmarks/models/llava_model.py", "content": "import numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.io import read_video\n\nfrom .base_model import BaseModel\nfrom .llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom .llava.conversation import conv_templates, SeparatorStyle\nfrom .llava.model.builder import load_pretrained_model\nfrom .llava.utils import disable_torch_init\nfrom .llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n\ndefault_model_path = \"liuhaotian/llava-v1.5-7b\"\n\n\nclass LLaVA_Model(BaseModel):\n    def __init__(\n        self,\n        model_path: str = default_model_path,\n        model_base: str = None,\n        model_name: str = \"llava-v1.5\",\n        conv_mode: str = \"llava_v1\",\n    ):\n        super().__init__(model_name, model_path)\n        init_model_name = get_model_name_from_path(model_path)\n        self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(model_path, model_base, init_model_name)\n        self.conv_mode = conv_mode\n\n    def generate(self, text_prompt: str, raw_image_data: str):\n        if self.model.config.mm_use_im_start_end:\n            prompts_input = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + text_prompt\n        else:\n            prompts_input = DEFAULT_IMAGE_TOKEN + \"\\n\" + text_prompt\n\n        input_data = self.image_processor.preprocess(raw_image_data, return_tensors=\"pt\")[\"pixel_values\"][0]\n\n        conv = conv_templates[self.conv_mode].copy()\n        conv.append_message(conv.roles[0], prompts_input)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n        input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).cuda()\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n        keywords = [stop_str]\n        stopping_criteria = KeywordsStoppingCriteria(keywords, self.tokenizer, input_ids)\n\n        with torch.inference_mode():\n            output_ids = self.model.generate(\n                input_ids,\n                images=input_data.unsqueeze(0).half().cuda(),\n                do_sample=True,\n                temperature=0.2,\n                top_p=None,\n                num_beams=1,\n                # no_repeat_ngram_size=3,\n                max_new_tokens=512,\n                use_cache=True,\n            )\n\n        input_token_len = input_ids.shape[1]\n        n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\n        if n_diff_input_output > 0:\n            print(f\"[Warning] {n_diff_input_output} output_ids are not the same as the input_ids\")\n        outputs = self.tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\n        outputs = outputs.strip()\n        if outputs.endswith(stop_str):\n            outputs = outputs[: -len(stop_str)]\n        outputs = outputs.strip()\n\n        return outputs\n\n    def eval_forward(self, text_prompt: str, raw_image_data: str):\n        pass\n"}
{"type": "source_file", "path": "pipeline/benchmarks/models/llama_adapter.py", "content": "from .LLaMA_Adapter.imagebind_LLM.ImageBind import data as data_utils\nfrom .LLaMA_Adapter.imagebind_LLM import llama\n\nfrom .base_model import BaseModel\n\nimport os\n\n\nllama_dir = \"/mnt/petrelfs/share_data/zhangyuanhan/llama_adapter_v2_multimodal\"\n\n\nclass LlamaAdapter(BaseModel):\n    # checkpoint will be automatically downloaded\n    def __init__(self, model_path: str):\n        super().__init__(\"llama_adapter\", model_path)\n        self.model = llama.load(model_path, llama_dir)\n        self.model.eval()\n\n    def generate(self, input_data):\n        inputs = {}\n        video_dir = input_data.get(\"video_root\", \"\")\n        image = data_utils.load_and_transform_video_data([input_data[\"video_path\"]], device=\"cuda\")\n        inputs[\"Image\"] = [image, 1]\n\n        object_description = input_data[\"object_description\"]\n        if object_description != \"None\":\n            context = f\"Given context:{object_description}. \"\n        else:\n            context = \"\"\n        prompts_input = context + input_data[\"question\"]\n\n        results = self.model.generate(inputs, [llama.format_prompt(prompts_input)], max_gen_len=256)\n        result = results[0].strip()\n        return result\n\n\nif __name__ == \"__main__\":\n    model = LlamaAdapter(\"\", \"\")\n    data = {\n        \"video_idx\": \"03f2ed96-1719-427d-acf4-8bf504f1d66d.mp4\",\n        \"question\": \"What is in this image?\",\n    }\n    print(model.generate(data))\n"}
{"type": "source_file", "path": "pipeline/benchmarks/models/qwen_vl.py", "content": "import os\n\nimport numpy as np\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\nfrom .base_model import BaseModel\n\ndefault_path = \"Qwen/Qwen-VL-Chat\"\n\n\nclass QwenVL(BaseModel):\n    def __init__(self, model_name: str = \"qwen_vl\", model_path: str = default_path):\n        super().__init__(model_name, model_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n        self.model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True).eval()\n        self.model.generation_config = GenerationConfig.from_pretrained(model_path, trust_remote_code=True)\n        self.temp_dir = \".log/temp\"\n        if not os.path.exists(self.temp_dir):\n            os.makedirs(self.temp_dir)\n\n    def generate(self, text_prompt: str, raw_image_data: str):\n        image_path = os.path.join(self.temp_dir, \"temp.jpg\")\n        raw_image_data.save(image_path)\n        query = []\n        query.append({\"image\": image_path})\n        query.append({\"text\": text_prompt})\n        query = self.tokenizer.from_list_format(query)\n        response, history = self.model.chat(self.tokenizer, query=query, history=None)\n        return response\n\n    def eval_forward(self, text_prompt: str, image_path: str):\n        # Similar to the Idefics' eval_forward but adapted for QwenVL\n        pass\n"}
{"type": "source_file", "path": "pipeline/benchmarks/models/otterhd.py", "content": "from transformers import FuyuForCausalLM, AutoTokenizer, FuyuImageProcessor, FuyuProcessor\nfrom PIL import Image\nfrom .base_model import BaseModel\nimport torch\nimport numpy as np\nimport warnings\nimport io\nimport base64\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef get_pil_image(raw_image_data) -> Image.Image:\n    if isinstance(raw_image_data, Image.Image):\n        return raw_image_data\n\n    elif isinstance(raw_image_data, dict) and \"bytes\" in raw_image_data:\n        return Image.open(io.BytesIO(raw_image_data[\"bytes\"]))\n\n    elif isinstance(raw_image_data, str):  # Assuming this is a base64 encoded string\n        image_bytes = base64.b64decode(raw_image_data)\n        return Image.open(io.BytesIO(image_bytes))\n\n    else:\n        raise ValueError(\"Unsupported image data format\")\n\n\nimport math\n\n\nclass OtterHD(BaseModel):\n    def __init__(self, model_path: str = \"Otter-AI/OtterHD-8B\", cuda_id: int = 0, resolution: int = -1, max_new_tokens=256):\n        super().__init__(\"otterhd\", model_path)\n        self.resolution = resolution\n        self.device = f\"cuda:{cuda_id}\" if torch.cuda.is_available() else \"cpu\"\n        self.model = FuyuForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=self.device)\n        self.model.eval()\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n        self.image_processor = FuyuImageProcessor()\n        self.processor = FuyuProcessor(image_processor=self.image_processor, tokenizer=self.tokenizer)\n        self.max_new_tokens = max_new_tokens\n\n    def generate(self, text_prompt: str, raw_image_data: str):\n        raw_image_data = get_pil_image(raw_image_data)\n        # make sure the image is in RGB format and resize to match the width\n        raw_image_data = raw_image_data.convert(\"RGB\")\n        if self.resolution != -1:\n            width, height = raw_image_data.size\n            short_edge = min(width, height)\n            scaling_factor = self.resolution / short_edge\n            new_width = math.ceil(width * scaling_factor)\n            new_height = math.ceil(height * scaling_factor)\n            raw_image_data = raw_image_data.resize((new_width, new_height), Image.ANTIALIAS)\n\n        formated_prompt = f\"User: {text_prompt} Assistant:\"\n        model_inputs = self.processor(text=formated_prompt, images=[raw_image_data], device=self.device)\n        for k, v in model_inputs.items():\n            model_inputs[k] = v.to(self.device, non_blocking=True) if isinstance(v, torch.Tensor) else [vv.to(self.device, non_blocking=True) for vv in v]\n\n        model_inputs[\"image_patches\"][0] = model_inputs[\"image_patches\"][0].to(dtype=next(self.model.parameters()).dtype)\n        generation_output = self.model.generate(**model_inputs, max_new_tokens=self.max_new_tokens, pad_token_id=self.tokenizer.eos_token_id)\n        generation_text = self.processor.batch_decode(generation_output, skip_special_tokens=True)\n        response = generation_text[0].split(\"\\x04\")[1].strip(\" \").strip(\"\\n\")\n        return response\n\n    def eval_forward(self, text_prompt: str, image_path: str):\n        # Similar to the Idefics' eval_forward but adapted for Fuyu\n        pass\n"}
{"type": "source_file", "path": "pipeline/benchmarks/models/otter_image.py", "content": "import mimetypes\nimport os\nfrom io import BytesIO\nfrom typing import Union\nimport cv2\nimport requests\nimport torch\nimport transformers\nfrom PIL import Image\n\nfrom otter_ai import OtterForConditionalGeneration\nfrom .base_model import BaseModel\n\n\n# Disable warnings\nrequests.packages.urllib3.disable_warnings()\n\n\ndef get_pil_image(raw_image_data) -> Image.Image:\n    if isinstance(raw_image_data, Image.Image):\n        return raw_image_data\n    else:\n        return Image.open(BytesIO(raw_image_data[\"bytes\"]))\n\n\ndef get_formatted_prompt(prompt: str) -> str:\n    return f\"<image>User: {prompt} GPT:<answer>\"\n\n\ndef get_formatted_forward_prompt(question: str, answer: str) -> str:\n    return f\"<image>User: {question} GPT:<answer> {answer}\"\n\n\nclass OtterImage(BaseModel):\n    def __init__(self, model_path=\"luodian/OTTER-Image-MPT7B\", load_bit=\"bf16\"):\n        super().__init__(\"otter\", model_path)\n        precision = {}\n        if load_bit == \"bf16\":\n            precision[\"torch_dtype\"] = torch.bfloat16\n        elif load_bit == \"fp16\":\n            precision[\"torch_dtype\"] = torch.float16\n        elif load_bit == \"fp32\":\n            precision[\"torch_dtype\"] = torch.float32\n        self.model = OtterForConditionalGeneration.from_pretrained(model_path, device_map=\"sequential\", **precision)\n        self.model.text_tokenizer.padding_side = \"left\"\n        self.tokenizer = self.model.text_tokenizer\n        self.image_processor = transformers.CLIPImageProcessor()\n        self.model.eval()\n\n    def generate(self, question: str, raw_image_data):\n        input_data = get_pil_image(raw_image_data)\n        if isinstance(input_data, Image.Image):\n            if input_data.size == (224, 224) and not any(input_data.getdata()):  # Check if image is blank 224x224 image\n                vision_x = torch.zeros(1, 1, 1, 3, 224, 224, dtype=next(self.model.parameters()).dtype)\n            else:\n                vision_x = self.image_processor.preprocess([input_data], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n        else:\n            raise ValueError(\"Invalid input data. Expected PIL Image.\")\n\n        lang_x = self.model.text_tokenizer(\n            [\n                get_formatted_prompt(question),\n            ],\n            return_tensors=\"pt\",\n        )\n\n        model_dtype = next(self.model.parameters()).dtype\n        vision_x = vision_x.to(dtype=model_dtype)\n        lang_x_input_ids = lang_x[\"input_ids\"]\n        lang_x_attention_mask = lang_x[\"attention_mask\"]\n\n        generated_text = self.model.generate(\n            vision_x=vision_x.to(self.model.device),\n            lang_x=lang_x_input_ids.to(self.model.device),\n            attention_mask=lang_x_attention_mask.to(self.model.device),\n            max_new_tokens=512,\n            num_beams=3,\n            no_repeat_ngram_size=3,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n        parsed_output = self.model.text_tokenizer.decode(generated_text[0]).split(\"<answer>\")[-1].split(\"<|endofchunk|>\")[0].strip()\n        return parsed_output\n\n    def get_vision_x(self, input_data):\n        if isinstance(input_data, Image.Image):\n            if input_data.size == (224, 224) and not any(input_data.getdata()):  # Check if image is blank 224x224 image\n                vision_x = torch.zeros(1, 1, 1, 3, 224, 224, dtype=next(self.model.parameters()).dtype)\n            else:\n                vision_x = self.image_processor.preprocess([input_data], return_tensors=\"pt\")[\"pixel_values\"].unsqueeze(1).unsqueeze(0)\n        else:\n            raise ValueError(\"Invalid input data. Expected PIL Image.\")\n        model_dtype = next(self.model.parameters()).dtype\n        vision_x = vision_x.to(dtype=model_dtype)\n        return vision_x\n\n    def eval_forward(self, question, answer, image):\n        query = get_formatted_forward_prompt(question, answer)\n        tokens = self.tokenizer(query, return_tensors=\"pt\")\n        input_ids = tokens[\"input_ids\"]\n        attention_mask = tokens[\"attention_mask\"]\n        with torch.no_grad():\n            vision_x = self.get_vision_x(image)\n            loss = self.model(vision_x=vision_x.to(self.model.device), lang_x=input_ids.to(self.model.device), attention_mask=attention_mask.to(self.model.device))[0]\n        return loss\n\n\nif __name__ == \"__main__\":\n    model = OtterImage(\"/data/pufanyi/training_data/checkpoints/OTTER-Image-MPT7B\")\n    image = Image.open(\"/data/pufanyi/project/Otter-2/pipeline/evaluation/test_data/test.jpg\")\n    response = model.generate(\"What is this?\", image)\n    print(response)\n    response = model.generate(\"What is this?\", image)\n    print(response)\n"}
{"type": "source_file", "path": "pipeline/benchmarks/models/mplug_owl.py", "content": "import os\n\nimport torch\nfrom transformers import AutoTokenizer\nfrom mplug_owl_video.modeling_mplug_owl import MplugOwlForConditionalGeneration\nfrom mplug_owl_video.processing_mplug_owl import (\n    MplugOwlImageProcessor,\n    MplugOwlProcessor,\n)\n\nfrom .base_model import BaseModel\n\npretrained_ckpt = \"MAGAer13/mplug-owl-llama-7b-video\"\n\n\nclass mPlug_owl(BaseModel):\n    def __init__(self, model_path: str):\n        super().__init__(\"mplug_owl\", model_path)\n        self.model = MplugOwlForConditionalGeneration.from_pretrained(\n            pretrained_ckpt,\n            torch_dtype=torch.bfloat16,\n        )\n        self.image_processor = MplugOwlImageProcessor.from_pretrained(pretrained_ckpt)\n        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_ckpt)\n        self.processor = MplugOwlProcessor(self.image_processor, self.tokenizer)\n        self.model.eval()\n\n    def format_prompt(self, question):\n        prompts = [f\" <|video|> Question : {question} Answer : \"]\n        return prompts\n\n    def generate(self, input_data: dict):\n        questions = input_data[\"question\"]\n        video_dir = input_data.get(\"video_root\", \"\")\n        video_list = input_data[\"video_path\"]\n        generate_kwargs = {\"do_sample\": True, \"top_k\": 5, \"max_length\": 512}\n\n        object_description = input_data[\"object_description\"]\n        if object_description != \"None\":\n            context = f\"Given context:{object_description}. \"\n        else:\n            context = \"\"\n        prompts_input = context + input_data[\"question\"]\n\n        prompt = self.format_prompt(prompts_input)\n        inputs = self.processor(text=prompt, videos=video_list, num_frames=4, return_tensors=\"pt\")\n        inputs = {k: v.bfloat16() if v.dtype == torch.float else v for k, v in inputs.items()}\n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        with torch.no_grad():\n            res = self.model.generate(**inputs, **generate_kwargs)\n        sentence = self.tokenizer.decode(res.tolist()[0], skip_special_tokens=True)\n        return sentence\n\n\nif __name__ == \"__main__\":\n    model = mPlug_owl(\"\")\n    device = torch.device(\"cuda\")\n    model.model = model.model.to(device)\n    data = {\n        \"video_idx\": [\"./data_source/multi_hop_reasoning/03f2ed96-1719-427d-acf4-8bf504f1d66d.mp4\"],\n        \"question\": \"What is in this image?\",\n    }\n    print(model.generate(data))\n"}
