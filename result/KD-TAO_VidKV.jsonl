{"repo_info": {"repo_name": "VidKV", "repo_owner": "KD-TAO", "repo_url": "https://github.com/KD-TAO/VidKV"}}
{"type": "test_file", "path": "transformers/examples/flax/test_flax_examples.py", "content": "# coding=utf-8\n# Copyright 2021 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport argparse\nimport json\nimport logging\nimport os\nimport sys\nfrom unittest.mock import patch\n\nfrom transformers.testing_utils import TestCasePlus, get_gpu_count, slow\n\n\nSRC_DIRS = [\n    os.path.join(os.path.dirname(__file__), dirname)\n    for dirname in [\n        \"text-classification\",\n        \"language-modeling\",\n        \"summarization\",\n        \"token-classification\",\n        \"question-answering\",\n        \"speech-recognition\",\n    ]\n]\nsys.path.extend(SRC_DIRS)\n\n\nif SRC_DIRS is not None:\n    import run_clm_flax\n    import run_flax_glue\n    import run_flax_ner\n    import run_flax_speech_recognition_seq2seq\n    import run_mlm_flax\n    import run_qa\n    import run_summarization_flax\n    import run_t5_mlm_flax\n\n\nlogging.basicConfig(level=logging.DEBUG)\n\nlogger = logging.getLogger()\n\n\ndef get_setup_file():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-f\")\n    args = parser.parse_args()\n    return args.f\n\n\ndef get_results(output_dir, split=\"eval\"):\n    path = os.path.join(output_dir, f\"{split}_results.json\")\n    if os.path.exists(path):\n        with open(path, \"r\") as f:\n            return json.load(f)\n    raise ValueError(f\"can't find {path}\")\n\n\nstream_handler = logging.StreamHandler(sys.stdout)\nlogger.addHandler(stream_handler)\n\n\nclass ExamplesTests(TestCasePlus):\n    def test_run_glue(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_glue.py\n            --model_name_or_path distilbert/distilbert-base-uncased\n            --output_dir {tmp_dir}\n            --train_file ./tests/fixtures/tests_samples/MRPC/train.csv\n            --validation_file ./tests/fixtures/tests_samples/MRPC/dev.csv\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --learning_rate=1e-4\n            --eval_steps=2\n            --warmup_steps=2\n            --seed=42\n            --max_seq_length=128\n            \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_flax_glue.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.75)\n\n    @slow\n    def test_run_clm(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_clm_flax.py\n            --model_name_or_path distilbert/distilgpt2\n            --train_file ./tests/fixtures/sample_text.txt\n            --validation_file ./tests/fixtures/sample_text.txt\n            --do_train\n            --do_eval\n            --block_size 128\n            --per_device_train_batch_size 4\n            --per_device_eval_batch_size 4\n            --num_train_epochs 2\n            --logging_steps 2 --eval_steps 2\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_clm_flax.main()\n            result = get_results(tmp_dir)\n            self.assertLess(result[\"eval_perplexity\"], 100)\n\n    @slow\n    def test_run_summarization(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_summarization.py\n            --model_name_or_path google-t5/t5-small\n            --train_file tests/fixtures/tests_samples/xsum/sample.json\n            --validation_file tests/fixtures/tests_samples/xsum/sample.json\n            --test_file tests/fixtures/tests_samples/xsum/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --num_train_epochs=3\n            --warmup_steps=8\n            --do_train\n            --do_eval\n            --do_predict\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --predict_with_generate\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_summarization_flax.main()\n            result = get_results(tmp_dir, split=\"test\")\n            self.assertGreaterEqual(result[\"test_rouge1\"], 10)\n            self.assertGreaterEqual(result[\"test_rouge2\"], 2)\n            self.assertGreaterEqual(result[\"test_rougeL\"], 7)\n            self.assertGreaterEqual(result[\"test_rougeLsum\"], 7)\n\n    @slow\n    def test_run_mlm(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_mlm.py\n            --model_name_or_path distilbert/distilroberta-base\n            --train_file ./tests/fixtures/sample_text.txt\n            --validation_file ./tests/fixtures/sample_text.txt\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --max_seq_length 128\n            --per_device_train_batch_size 4\n            --per_device_eval_batch_size 4\n            --logging_steps 2 --eval_steps 2\n            --do_train\n            --do_eval\n            --num_train_epochs=1\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_mlm_flax.main()\n            result = get_results(tmp_dir)\n            self.assertLess(result[\"eval_perplexity\"], 42)\n\n    @slow\n    def test_run_t5_mlm(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_t5_mlm_flax.py\n            --model_name_or_path google-t5/t5-small\n            --train_file ./tests/fixtures/sample_text.txt\n            --validation_file ./tests/fixtures/sample_text.txt\n            --do_train\n            --do_eval\n            --max_seq_length 128\n            --per_device_train_batch_size 4\n            --per_device_eval_batch_size 4\n            --num_train_epochs 2\n            --logging_steps 2 --eval_steps 2\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_t5_mlm_flax.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.42)\n\n    @slow\n    def test_run_ner(self):\n        # with so little data distributed training needs more epochs to get the score on par with 0/1 gpu\n        epochs = 7 if get_gpu_count() > 1 else 2\n\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_flax_ner.py\n            --model_name_or_path google-bert/bert-base-uncased\n            --train_file tests/fixtures/tests_samples/conll/sample.json\n            --validation_file tests/fixtures/tests_samples/conll/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --do_train\n            --do_eval\n            --warmup_steps=2\n            --learning_rate=2e-4\n            --logging_steps 2 --eval_steps 2\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=2\n            --num_train_epochs={epochs}\n            --seed 7\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_flax_ner.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.75)\n            self.assertGreaterEqual(result[\"eval_f1\"], 0.3)\n\n    @slow\n    def test_run_qa(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_qa.py\n            --model_name_or_path google-bert/bert-base-uncased\n            --version_2_with_negative\n            --train_file tests/fixtures/tests_samples/SQUAD/sample.json\n            --validation_file tests/fixtures/tests_samples/SQUAD/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --num_train_epochs=3\n            --warmup_steps=2\n            --do_train\n            --do_eval\n            --logging_steps 2 --eval_steps 2\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_qa.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_f1\"], 30)\n            self.assertGreaterEqual(result[\"eval_exact\"], 30)\n\n    @slow\n    def test_run_flax_speech_recognition_seq2seq(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_flax_speech_recognition_seq2seq.py\n            --model_name_or_path openai/whisper-tiny.en\n            --dataset_name hf-internal-testing/librispeech_asr_dummy\n            --dataset_config clean\n            --train_split_name validation\n            --eval_split_name validation\n            --trust_remote_code\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --num_train_epochs=2\n            --max_train_samples 10\n            --max_eval_samples 10\n            --warmup_steps=8\n            --do_train\n            --do_eval\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --predict_with_generate\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_flax_speech_recognition_seq2seq.main()\n            result = get_results(tmp_dir, split=\"eval\")\n            self.assertLessEqual(result[\"eval_wer\"], 0.05)\n"}
{"type": "test_file", "path": "llava/serve/test_message.py", "content": "import argparse\nimport json\n\nimport requests\n\nfrom llava.conversation import default_conversation\n\n\ndef main():\n    if args.worker_address:\n        worker_addr = args.worker_address\n    else:\n        controller_addr = args.controller_address\n        ret = requests.post(controller_addr + \"/refresh_all_workers\")\n        ret = requests.post(controller_addr + \"/list_models\")\n        models = ret.json()[\"models\"]\n        models.sort()\n        print(f\"Models: {models}\")\n\n        ret = requests.post(controller_addr + \"/get_worker_address\", json={\"model\": args.model_name})\n        worker_addr = ret.json()[\"address\"]\n        print(f\"worker_addr: {worker_addr}\")\n\n    if worker_addr == \"\":\n        return\n\n    conv = default_conversation.copy()\n    conv.append_message(conv.roles[0], args.message)\n    prompt = conv.get_prompt()\n\n    headers = {\"User-Agent\": \"LLaVA Client\"}\n    pload = {\n        \"model\": args.model_name,\n        \"prompt\": prompt,\n        \"max_new_tokens\": args.max_new_tokens,\n        \"temperature\": 0.7,\n        \"stop\": conv.sep,\n    }\n    response = requests.post(worker_addr + \"/worker_generate_stream\", headers=headers, json=pload, stream=True)\n\n    print(prompt.replace(conv.sep, \"\\n\"), end=\"\")\n    for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"):\n        if chunk:\n            data = json.loads(chunk.decode(\"utf-8\"))\n            output = data[\"text\"].split(conv.sep)[-1]\n            print(output, end=\"\\r\")\n    print(\"\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--controller-address\", type=str, default=\"http://localhost:21001\")\n    parser.add_argument(\"--worker-address\", type=str)\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--max-new-tokens\", type=int, default=32)\n    parser.add_argument(\"--message\", type=str, default=\"Tell me a story with more than 1000 words.\")\n    args = parser.parse_args()\n\n    main()\n"}
{"type": "test_file", "path": "transformers/tests/generation/__init__.py", "content": ""}
{"type": "test_file", "path": "transformers/tests/models/altclip/test_processor_altclip.py", "content": "# coding=utf-8\n# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport tempfile\nimport unittest\n\nfrom transformers import AltCLIPProcessor, CLIPImageProcessor, XLMRobertaTokenizer, XLMRobertaTokenizerFast\nfrom transformers.testing_utils import require_vision\n\nfrom ...test_processing_common import ProcessorTesterMixin\n\n\n@require_vision\nclass AltClipProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n    processor_class = AltCLIPProcessor\n\n    def setUp(self):\n        self.model_id = \"BAAI/AltCLIP\"\n        self.tmpdirname = tempfile.mkdtemp()\n        image_processor = CLIPImageProcessor()\n        tokenizer = XLMRobertaTokenizer.from_pretrained(self.model_id)\n\n        processor = self.processor_class(image_processor, tokenizer)\n\n        processor.save_pretrained(self.tmpdirname)\n\n    def get_tokenizer(self, **kwargs):\n        return XLMRobertaTokenizer.from_pretrained(self.model_id, **kwargs)\n\n    def get_rust_tokenizer(self, **kwargs):\n        return XLMRobertaTokenizerFast.from_pretrained(self.model_id, **kwargs)\n\n    def get_image_processor(self, **kwargs):\n        return CLIPImageProcessor.from_pretrained(self.model_id, **kwargs)\n"}
{"type": "test_file", "path": "transformers/tests/models/albert/test_modeling_albert.py", "content": "# coding=utf-8\n# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport unittest\n\nfrom packaging import version\n\nfrom transformers import AlbertConfig, AutoTokenizer, is_torch_available\nfrom transformers.models.auto import get_values\nfrom transformers.testing_utils import require_torch, slow, torch_device\n\nfrom ...test_configuration_common import ConfigTester\nfrom ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\nfrom ...test_pipeline_mixin import PipelineTesterMixin\n\n\nif is_torch_available():\n    import torch\n\n    from transformers import (\n        MODEL_FOR_PRETRAINING_MAPPING,\n        AlbertForMaskedLM,\n        AlbertForMultipleChoice,\n        AlbertForPreTraining,\n        AlbertForQuestionAnswering,\n        AlbertForSequenceClassification,\n        AlbertForTokenClassification,\n        AlbertModel,\n    )\n\n\nclass AlbertModelTester:\n    def __init__(\n        self,\n        parent,\n        batch_size=13,\n        seq_length=7,\n        is_training=True,\n        use_input_mask=True,\n        use_token_type_ids=True,\n        use_labels=True,\n        vocab_size=99,\n        embedding_size=16,\n        hidden_size=36,\n        num_hidden_layers=2,\n        # this needs to be the same as `num_hidden_layers`!\n        num_hidden_groups=2,\n        num_attention_heads=6,\n        intermediate_size=37,\n        hidden_act=\"gelu\",\n        hidden_dropout_prob=0.1,\n        attention_probs_dropout_prob=0.1,\n        max_position_embeddings=512,\n        type_vocab_size=16,\n        type_sequence_label_size=2,\n        initializer_range=0.02,\n        num_labels=3,\n        num_choices=4,\n        scope=None,\n    ):\n        self.parent = parent\n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.is_training = is_training\n        self.use_input_mask = use_input_mask\n        self.use_token_type_ids = use_token_type_ids\n        self.use_labels = use_labels\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_hidden_groups = num_hidden_groups\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.max_position_embeddings = max_position_embeddings\n        self.type_vocab_size = type_vocab_size\n        self.type_sequence_label_size = type_sequence_label_size\n        self.initializer_range = initializer_range\n        self.num_labels = num_labels\n        self.num_choices = num_choices\n        self.scope = scope\n\n    def prepare_config_and_inputs(self):\n        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n\n        input_mask = None\n        if self.use_input_mask:\n            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n\n        token_type_ids = None\n        if self.use_token_type_ids:\n            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n\n        sequence_labels = None\n        token_labels = None\n        choice_labels = None\n        if self.use_labels:\n            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n\n        config = self.get_config()\n\n        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n\n    def get_config(self):\n        return AlbertConfig(\n            vocab_size=self.vocab_size,\n            hidden_size=self.hidden_size,\n            num_hidden_layers=self.num_hidden_layers,\n            num_attention_heads=self.num_attention_heads,\n            intermediate_size=self.intermediate_size,\n            hidden_act=self.hidden_act,\n            hidden_dropout_prob=self.hidden_dropout_prob,\n            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n            max_position_embeddings=self.max_position_embeddings,\n            type_vocab_size=self.type_vocab_size,\n            initializer_range=self.initializer_range,\n            num_hidden_groups=self.num_hidden_groups,\n        )\n\n    def create_and_check_model(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        model = AlbertModel(config=config)\n        model.to(torch_device)\n        model.eval()\n        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids)\n        result = model(input_ids, token_type_ids=token_type_ids)\n        result = model(input_ids)\n        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n\n    def create_and_check_for_pretraining(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        model = AlbertForPreTraining(config=config)\n        model.to(torch_device)\n        model.eval()\n        result = model(\n            input_ids,\n            attention_mask=input_mask,\n            token_type_ids=token_type_ids,\n            labels=token_labels,\n            sentence_order_label=sequence_labels,\n        )\n        self.parent.assertEqual(result.prediction_logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n        self.parent.assertEqual(result.sop_logits.shape, (self.batch_size, config.num_labels))\n\n    def create_and_check_for_masked_lm(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        model = AlbertForMaskedLM(config=config)\n        model.to(torch_device)\n        model.eval()\n        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, labels=token_labels)\n        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n\n    def create_and_check_for_question_answering(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        model = AlbertForQuestionAnswering(config=config)\n        model.to(torch_device)\n        model.eval()\n        result = model(\n            input_ids,\n            attention_mask=input_mask,\n            token_type_ids=token_type_ids,\n            start_positions=sequence_labels,\n            end_positions=sequence_labels,\n        )\n        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n\n    def create_and_check_for_sequence_classification(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        config.num_labels = self.num_labels\n        model = AlbertForSequenceClassification(config)\n        model.to(torch_device)\n        model.eval()\n        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, labels=sequence_labels)\n        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n\n    def create_and_check_for_token_classification(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        config.num_labels = self.num_labels\n        model = AlbertForTokenClassification(config=config)\n        model.to(torch_device)\n        model.eval()\n        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, labels=token_labels)\n        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n\n    def create_and_check_for_multiple_choice(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        config.num_choices = self.num_choices\n        model = AlbertForMultipleChoice(config=config)\n        model.to(torch_device)\n        model.eval()\n        multiple_choice_inputs_ids = input_ids.unsqueeze(1).expand(-1, self.num_choices, -1).contiguous()\n        multiple_choice_token_type_ids = token_type_ids.unsqueeze(1).expand(-1, self.num_choices, -1).contiguous()\n        multiple_choice_input_mask = input_mask.unsqueeze(1).expand(-1, self.num_choices, -1).contiguous()\n        result = model(\n            multiple_choice_inputs_ids,\n            attention_mask=multiple_choice_input_mask,\n            token_type_ids=multiple_choice_token_type_ids,\n            labels=choice_labels,\n        )\n        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_choices))\n\n    def prepare_config_and_inputs_for_common(self):\n        config_and_inputs = self.prepare_config_and_inputs()\n        (\n            config,\n            input_ids,\n            token_type_ids,\n            input_mask,\n            sequence_labels,\n            token_labels,\n            choice_labels,\n        ) = config_and_inputs\n        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": input_mask}\n        return config, inputs_dict\n\n\n@require_torch\nclass AlbertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n    all_model_classes = (\n        (\n            AlbertModel,\n            AlbertForPreTraining,\n            AlbertForMaskedLM,\n            AlbertForMultipleChoice,\n            AlbertForSequenceClassification,\n            AlbertForTokenClassification,\n            AlbertForQuestionAnswering,\n        )\n        if is_torch_available()\n        else ()\n    )\n    pipeline_model_mapping = (\n        {\n            \"feature-extraction\": AlbertModel,\n            \"fill-mask\": AlbertForMaskedLM,\n            \"question-answering\": AlbertForQuestionAnswering,\n            \"text-classification\": AlbertForSequenceClassification,\n            \"token-classification\": AlbertForTokenClassification,\n            \"zero-shot\": AlbertForSequenceClassification,\n        }\n        if is_torch_available()\n        else {}\n    )\n    fx_compatible = True\n\n    # special case for ForPreTraining model\n    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n\n        if return_labels:\n            if model_class in get_values(MODEL_FOR_PRETRAINING_MAPPING):\n                inputs_dict[\"labels\"] = torch.zeros(\n                    (self.model_tester.batch_size, self.model_tester.seq_length), dtype=torch.long, device=torch_device\n                )\n                inputs_dict[\"sentence_order_label\"] = torch.zeros(\n                    self.model_tester.batch_size, dtype=torch.long, device=torch_device\n                )\n        return inputs_dict\n\n    def setUp(self):\n        self.model_tester = AlbertModelTester(self)\n        self.config_tester = ConfigTester(self, config_class=AlbertConfig, hidden_size=37)\n\n    def test_config(self):\n        self.config_tester.run_common_tests()\n\n    def test_model(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_model(*config_and_inputs)\n\n    def test_for_pretraining(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_for_pretraining(*config_and_inputs)\n\n    def test_for_masked_lm(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)\n\n    def test_for_multiple_choice(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_for_multiple_choice(*config_and_inputs)\n\n    def test_for_question_answering(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_for_question_answering(*config_and_inputs)\n\n    def test_for_sequence_classification(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n\n    def test_model_various_embeddings(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n            config_and_inputs[0].position_embedding_type = type\n            self.model_tester.create_and_check_model(*config_and_inputs)\n\n    @slow\n    def test_model_from_pretrained(self):\n        model_name = \"albert/albert-base-v1\"\n        model = AlbertModel.from_pretrained(model_name)\n        self.assertIsNotNone(model)\n\n\n@require_torch\nclass AlbertModelIntegrationTest(unittest.TestCase):\n    @slow\n    def test_inference_no_head_absolute_embedding(self):\n        model = AlbertModel.from_pretrained(\"albert/albert-base-v2\")\n        input_ids = torch.tensor([[0, 345, 232, 328, 740, 140, 1695, 69, 6078, 1588, 2]])\n        attention_mask = torch.tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n        with torch.no_grad():\n            output = model(input_ids, attention_mask=attention_mask)[0]\n        expected_shape = torch.Size((1, 11, 768))\n        self.assertEqual(output.shape, expected_shape)\n        expected_slice = torch.tensor(\n            [[[-0.6513, 1.5035, -0.2766], [-0.6515, 1.5046, -0.2780], [-0.6512, 1.5049, -0.2784]]]\n        )\n\n        torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)\n\n    @slow\n    def test_export(self):\n        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n\n        distilbert_model = \"albert/albert-base-v2\"\n        device = \"cpu\"\n        attn_implementation = \"sdpa\"\n        max_length = 64\n\n        tokenizer = AutoTokenizer.from_pretrained(distilbert_model)\n        inputs = tokenizer(\n            f\"Paris is the {tokenizer.mask_token} of France.\",\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            max_length=max_length,\n        )\n\n        model = AlbertForMaskedLM.from_pretrained(\n            distilbert_model,\n            device_map=device,\n            attn_implementation=attn_implementation,\n        )\n\n        logits = model(**inputs).logits\n        eg_predicted_mask = tokenizer.decode(logits[0, 4].topk(5).indices)\n        self.assertEqual(\n            eg_predicted_mask.split(),\n            [\"capital\", \"capitol\", \"comune\", \"arrondissement\", \"bastille\"],\n        )\n\n        exported_program = torch.export.export(\n            model,\n            args=(inputs[\"input_ids\"],),\n            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n            strict=True,\n        )\n\n        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n        ep_predicted_mask = tokenizer.decode(result.logits[0, 4].topk(5).indices)\n        self.assertEqual(eg_predicted_mask, ep_predicted_mask)\n"}
{"type": "test_file", "path": "transformers/tests/fsdp/test_fsdp.py", "content": "# Copyright 2023 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport itertools\nimport os\nimport subprocess\nimport unittest\nfrom copy import deepcopy\nfrom functools import partial\n\nfrom parameterized import parameterized\n\nimport tests.trainer.test_trainer\nfrom tests.trainer.test_trainer import TrainerIntegrationCommon  # noqa\nfrom transformers import is_torch_available\nfrom transformers.testing_utils import (\n    TestCasePlus,\n    backend_device_count,\n    execute_subprocess_async,\n    mockenv_context,\n    require_accelerate,\n    require_fsdp,\n    require_torch_accelerator,\n    require_torch_multi_accelerator,\n    run_first,\n    slow,\n    torch_device,\n)\nfrom transformers.trainer_callback import TrainerState\nfrom transformers.trainer_utils import FSDPOption, set_seed\nfrom transformers.utils import (\n    is_accelerate_available,\n    is_torch_bf16_available_on_device,\n    is_torch_fp16_available_on_device,\n)\n\n\nif is_torch_available():\n    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_1\n    from transformers.trainer import FSDP_MODEL_NAME\nelse:\n    is_torch_greater_or_equal_than_2_1 = False\n\n# default torch.distributed port\nDEFAULT_MASTER_PORT = \"10999\"\n\ndtypes = []\nif is_torch_bf16_available_on_device(torch_device):\n    dtypes += [\"bf16\"]\nif is_torch_fp16_available_on_device(torch_device):\n    dtypes += [\"fp16\"]\n\nsharding_strategies = [\"full_shard\", \"shard_grad_op\"]\nstate_dict_types = [\"FULL_STATE_DICT\", \"SHARDED_STATE_DICT\"]\nparams = list(itertools.product(sharding_strategies, dtypes))\n\nset_seed(42)\n\n\ndef get_master_port(real_launcher=False):\n    \"\"\"\n    When using a single gpu launcher emulation (i.e. not deepspeed or python -m torch.distributed)\n    the issue is that once the port is tied it can't be used anywhere else outside of this process,\n    since torch.dist doesn't free the port until the process exits. Therefore for the sake of being\n    able to run both emulated launcher and normal launcher tests we need 2 distinct ports.\n\n    This function will give the right port in the right context. For real launcher it'll give the\n    base port, for emulated launcher it'll give the base port + 1. In both cases a string is\n    returned.\n\n    Args:\n        `real_launcher`: whether a real launcher is going to be used, or the emulated one\n\n    \"\"\"\n\n    master_port_base = os.environ.get(\"DS_TEST_PORT\", DEFAULT_MASTER_PORT)\n    if not real_launcher:\n        master_port_base = str(int(master_port_base) + 1)\n    return master_port_base\n\n\nif is_torch_available():\n    from tests.trainer.test_trainer import (  # noqa\n        RegressionModelConfig,\n        RegressionPreTrainedModel,\n    )\n\n    # hack to restore original logging level pre #21700\n    get_regression_trainer = partial(tests.trainer.test_trainer.get_regression_trainer, log_level=\"info\")\n\nrequire_fsdp_version = require_fsdp\nif is_accelerate_available():\n    from accelerate.utils.constants import (\n        FSDP_PYTORCH_VERSION,\n        FSDP_SHARDING_STRATEGY,\n    )\n\n    require_fsdp_version = partial(require_fsdp, min_version=FSDP_PYTORCH_VERSION)\n\n\ndef get_launcher(distributed=False, use_accelerate=False):\n    # 1. explicitly set --num_nodes=1 just in case these tests end up run on a multi-node setup\n    # - it won't be able to handle that\n    # 2. for now testing with just 2 gpus max (since some quality tests may give different\n    # results with mode gpus because we use very little data)\n    num_gpus = min(2, backend_device_count(torch_device)) if distributed else 1\n    master_port = get_master_port(real_launcher=True)\n    if use_accelerate:\n        return f\"\"\"accelerate launch\n        --num_processes {num_gpus}\n        --main_process_port {master_port}\n        --use_fsdp\n        --fsdp_auto_wrap_policy TRANSFORMER_BASED_WRAP\n        --fsdp_state_dict_type SHARDED_STATE_DICT\n        --fsdp_transformer_layer_cls_to_wrap BertLayer\"\"\".split()\n    return f\"torchrun --nnodes 1 --nproc-per-node {num_gpus} --master-port {master_port}\".split()\n\n\ndef _parameterized_custom_name_func(func, param_num, param):\n    # customize the test name generator function as we want both params to appear in the sub-test\n    # name, as by default it shows only the first param\n    param_based_name = parameterized.to_safe_name(\"_\".join(str(x) for x in param.args))\n    return f\"{func.__name__}_{param_based_name}\"\n\n\n@require_accelerate\n@require_torch_accelerator\n@require_fsdp_version\nclass TrainerIntegrationFSDP(TestCasePlus, TrainerIntegrationCommon):\n    def setUp(self):\n        super().setUp()\n        master_port = get_master_port(real_launcher=False)\n        self.dist_env_1_gpu = {\n            \"MASTER_ADDR\": \"localhost\",\n            \"MASTER_PORT\": master_port,\n            \"RANK\": \"0\",\n            \"LOCAL_RANK\": \"0\",\n            \"WORLD_SIZE\": \"1\",\n        }\n\n        self.fsdp_config = {\n            \"backward_prefetch\": \"BACKWARD_PRE\",\n            \"forward_prefetch\": \"false\",\n            \"limit_all_gathers\": \"false\",\n            \"use_orig_params\": \"true\",\n            \"sync_module_states\": \"true\",\n            \"cpu_ram_efficient_loading\": \"true\",\n            \"activation_checkpointing\": \"false\",\n            \"min_num_params\": 1,\n        }\n\n    def tearDown(self):\n        super().tearDown()\n\n    @parameterized.expand(params, name_func=_parameterized_custom_name_func)\n    def test_fsdp_config(self, sharding_strategy, dtype):\n        output_dir = self.get_auto_remove_tmp_dir()\n        kwargs = {\n            \"output_dir\": output_dir,\n            \"train_len\": 128,\n            \"save_steps\": 5,\n            \"learning_rate\": 0.1,\n            \"fsdp\": f\"{sharding_strategy} offload auto_wrap\",\n            \"fsdp_config\": self.fsdp_config,\n        }\n        kwargs[dtype] = True\n        with mockenv_context(**self.dist_env_1_gpu):\n            trainer = get_regression_trainer(**kwargs)\n            self.assertEqual(trainer.args.fsdp[0], sharding_strategy)\n            self.assertEqual(trainer.args.fsdp[1], FSDPOption.OFFLOAD)\n            self.assertEqual(trainer.args.fsdp[2], FSDPOption.AUTO_WRAP)\n            for k, v in trainer.args.fsdp_config.items():\n                self.assertEqual(v, self.fsdp_config[k])\n            self.assertEqual(os.environ.get(\"ACCELERATE_USE_FSDP\", \"false\"), \"true\")\n\n    @parameterized.expand(params, name_func=_parameterized_custom_name_func)\n    def test_fsdp_config_transformers_auto_wrap(self, sharding_strategy, dtype):\n        output_dir = self.get_auto_remove_tmp_dir()\n        fsdp_config = deepcopy(self.fsdp_config)\n        del fsdp_config[\"min_num_params\"]\n        fsdp_config[\"transformer_layer_cls_to_wrap\"] = \"BertLayer\"\n        kwargs = {\n            \"output_dir\": output_dir,\n            \"train_len\": 128,\n            \"save_steps\": 5,\n            \"learning_rate\": 0.1,\n            \"fsdp\": f\"{sharding_strategy} offload auto_wrap\",\n            \"fsdp_config\": fsdp_config,\n        }\n        kwargs[dtype] = True\n        prefix = \"FSDP_\"\n        with mockenv_context(**self.dist_env_1_gpu):\n            trainer = get_regression_trainer(**kwargs)\n            self.assertEqual(trainer.args.fsdp[0], sharding_strategy)\n            self.assertEqual(trainer.args.fsdp[1], FSDPOption.OFFLOAD)\n            self.assertEqual(trainer.args.fsdp[2], FSDPOption.AUTO_WRAP)\n            fsdp_sharding_strategy = str(FSDP_SHARDING_STRATEGY.index(sharding_strategy.upper()) + 1)\n            self.assertEqual(os.environ[f\"{prefix}SHARDING_STRATEGY\"], fsdp_sharding_strategy)\n            self.assertEqual(os.environ[f\"{prefix}OFFLOAD_PARAMS\"], \"true\")\n            self.assertEqual(os.environ[f\"{prefix}AUTO_WRAP_POLICY\"], \"TRANSFORMER_BASED_WRAP\")\n            self.assertEqual(\n                os.environ[f\"{prefix}TRANSFORMER_CLS_TO_WRAP\"], \",\".join(fsdp_config[\"transformer_layer_cls_to_wrap\"])\n            )\n            self.assertEqual(os.environ[f\"{prefix}BACKWARD_PREFETCH\"], fsdp_config[\"backward_prefetch\"])\n            self.assertEqual(os.environ[f\"{prefix}FORWARD_PREFETCH\"], fsdp_config[\"forward_prefetch\"])\n            self.assertEqual(os.environ[f\"{prefix}USE_ORIG_PARAMS\"], fsdp_config[\"use_orig_params\"])\n            self.assertEqual(os.environ[f\"{prefix}SYNC_MODULE_STATES\"], fsdp_config[\"sync_module_states\"])\n            self.assertEqual(\n                os.environ[f\"{prefix}CPU_RAM_EFFICIENT_LOADING\"], fsdp_config[\"cpu_ram_efficient_loading\"]\n            )\n            self.assertEqual(os.environ.get(\"ACCELERATE_USE_FSDP\", \"false\"), \"true\")\n\n    @parameterized.expand(params, name_func=_parameterized_custom_name_func)\n    @require_torch_multi_accelerator\n    @run_first\n    @slow\n    def test_basic_run(self, sharding_strategy, dtype):\n        launcher = get_launcher(distributed=True, use_accelerate=False)\n        output_dir = self.get_auto_remove_tmp_dir()\n        args = self.get_base_args(output_dir, 1, 50).split() + [f\"--{dtype}\"]\n        fsdp_args = [\"--fsdp\", f\"{sharding_strategy} auto_wrap\", \"--fsdp_transformer_layer_cls_to_wrap\", \"BertLayer\"]\n        script = [f\"{self.examples_dir_str}/pytorch/text-classification/run_glue.py\"]\n        cmd = launcher + script + args + fsdp_args\n        execute_subprocess_async(cmd, env=self.get_env())\n\n    @parameterized.expand(params, name_func=_parameterized_custom_name_func)\n    @require_torch_multi_accelerator\n    @run_first\n    @slow\n    def test_basic_run_with_gradient_accumulation(self, sharding_strategy, dtype):\n        launcher = get_launcher(distributed=True, use_accelerate=False)\n        output_dir = self.get_auto_remove_tmp_dir()\n        args = self.get_base_args(output_dir, 1, 50).split() + [f\"--{dtype}\", \"--gradient_accumulation_steps\", \"2\"]\n        fsdp_args = [\"--fsdp\", f\"{sharding_strategy} auto_wrap\", \"--fsdp_transformer_layer_cls_to_wrap\", \"BertLayer\"]\n        script = [f\"{self.examples_dir_str}/pytorch/text-classification/run_glue.py\"]\n        cmd = launcher + script + args + fsdp_args\n        execute_subprocess_async(cmd, env=self.get_env())\n\n    @parameterized.expand(dtypes)\n    @require_torch_multi_accelerator\n    @run_first\n    @slow\n    @unittest.skipIf(not is_torch_greater_or_equal_than_2_1, reason=\"This test on pytorch 2.0 takes 4 hours.\")\n    def test_basic_run_with_cpu_offload(self, dtype):\n        launcher = get_launcher(distributed=True, use_accelerate=False)\n        output_dir = self.get_auto_remove_tmp_dir()\n        args = self.get_base_args(output_dir, 1, 50).split() + [f\"--{dtype}\", \"--max_steps\", \"10\"]\n        fsdp_args = [\"--fsdp\", \"full_shard auto_wrap offload\", \"--fsdp_transformer_layer_cls_to_wrap\", \"BertLayer\"]\n        script = [f\"{self.examples_dir_str}/pytorch/text-classification/run_glue.py\"]\n        cmd = launcher + script + args + fsdp_args\n        execute_subprocess_async(cmd, env=self.get_env())\n\n    @parameterized.expand(state_dict_types, name_func=_parameterized_custom_name_func)\n    @require_torch_multi_accelerator\n    @run_first\n    @slow\n    def test_training_and_can_resume_normally(self, state_dict_type):\n        output_dir = self.get_auto_remove_tmp_dir(\"./xxx\", after=False)\n\n        sharding_strategy = \"full_shard\"\n        use_accelerate = state_dict_type == \"SHARDED_STATE_DICT\"\n        launcher = get_launcher(True, use_accelerate=use_accelerate)\n        args = self.get_base_args(output_dir, 2, 25).split()\n        script = [f\"{self.examples_dir_str}/pytorch/text-classification/run_glue.py\"]\n        logs = self.run_cmd_and_get_logs(use_accelerate, sharding_strategy, launcher, script, args, output_dir)\n\n        # resume from ckpt\n        checkpoint = os.path.join(output_dir, \"checkpoint-115\")\n        resume_args = args + f\"--resume_from_checkpoint {checkpoint}\".split()\n\n        is_fsdp_ckpt = os.path.isdir(checkpoint) and (\n            # this checks the FSDP state dict when `SHARDED_STATE_DICT` is used\n            any(\n                FSDP_MODEL_NAME in folder_name\n                for folder_name in os.listdir(checkpoint)\n                if os.path.isdir(os.path.join(checkpoint, folder_name))\n            )\n            # this checks the FSDP state dict when `FULL_STATE_DICT` is used\n            or os.path.isfile(os.path.join(checkpoint, f\"{FSDP_MODEL_NAME}.bin\"))\n        )\n        self.assertTrue(is_fsdp_ckpt)\n\n        logs_resume = self.run_cmd_and_get_logs(\n            use_accelerate, sharding_strategy, launcher, script, resume_args, output_dir\n        )\n\n        for log, log1 in zip(logs, logs_resume):\n            if \"learning_rate\" in log:\n                self.assertAlmostEqual(log[\"learning_rate\"], log1[\"learning_rate\"], delta=1e-5)\n\n    @require_torch_multi_accelerator\n    @run_first\n    @slow\n    def test_fsdp_cpu_offloading(self):\n        # TODO: This file is missing and should be added or the test should be removed\n        if not os.path.exists(\"utils/testing_scripts/fsdp_cpu_offloading.py\"):\n            raise unittest.SkipTest(\"FSDP CPU offloading script not found!\")\n\n        try:\n            subprocess.run(\n                \"accelerate launch utils/testing_scripts/fsdp_cpu_offloading.py --config utils/testing_scripts/dummy_fsdp_config.yml\",\n                shell=True,\n                check=True,\n            )\n        except:  # noqa\n            raise AssertionError(\"CPU offloading failed with FSDP!\")\n\n    def run_cmd_and_get_logs(self, use_accelerate, sharding_strategy, launcher, script, args, output_dir):\n        if not use_accelerate:\n            fsdp_args = [\n                \"--fsdp\",\n                f\"{sharding_strategy} auto_wrap\",\n                \"--fsdp_transformer_layer_cls_to_wrap\",\n                \"BertLayer\",\n            ]\n            cmd = launcher + script + args + fsdp_args\n        else:\n            fsdp_config = f\"\"\"\n                --fsdp_sharding_strategy {FSDP_SHARDING_STRATEGY.index(sharding_strategy.upper()) + 1}\n            \"\"\".split()\n            cmd = launcher + fsdp_config + script + args\n\n        # keep for quick debug\n        # print(\" \".join([f\"\\nPYTHONPATH={self.src_dir_str}\"] +cmd)); die\n        execute_subprocess_async(cmd, env=self.get_env())\n        logs = TrainerState.load_from_json(os.path.join(output_dir, \"trainer_state.json\")).log_history\n        return logs\n\n    def get_base_args(self, output_dir, num_epochs, logging_steps):\n        return f\"\"\"\n            --model_name_or_path google-bert/bert-base-cased\n            --task_name mrpc\n            --output_dir {output_dir}\n            --overwrite_output_dir\n            --do_train\n            --max_seq_length 128\n            --per_device_train_batch_size 16\n            --learning_rate 5e-5\n            --num_train_epochs {num_epochs}\n            --lr_scheduler_type cosine\n            --logging_steps {logging_steps}\n            --save_strategy epoch\n            --do_eval\n            --eval_strategy epoch\n            --report_to none\n        \"\"\"\n"}
{"type": "test_file", "path": "transformers/tests/generation/test_logits_process.py", "content": "# coding=utf-8\n# Copyright 2020 The HuggingFace Team Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a clone of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom typing import List, Union\n\nimport numpy as np\nfrom parameterized import parameterized\n\nfrom transformers import is_torch_available\nfrom transformers.testing_utils import require_torch, torch_device\n\nfrom ..test_modeling_common import ids_tensor\n\n\nif is_torch_available():\n    import torch\n    from torch import nn\n\n    from transformers.generation import (\n        EncoderNoRepeatNGramLogitsProcessor,\n        EncoderRepetitionPenaltyLogitsProcessor,\n        EpsilonLogitsWarper,\n        EtaLogitsWarper,\n        ExponentialDecayLengthPenalty,\n        ForcedBOSTokenLogitsProcessor,\n        ForcedEOSTokenLogitsProcessor,\n        HammingDiversityLogitsProcessor,\n        InfNanRemoveLogitsProcessor,\n        LogitNormalization,\n        LogitsProcessorList,\n        MinLengthLogitsProcessor,\n        MinNewTokensLengthLogitsProcessor,\n        MinPLogitsWarper,\n        NoBadWordsLogitsProcessor,\n        NoRepeatNGramLogitsProcessor,\n        PrefixConstrainedLogitsProcessor,\n        RepetitionPenaltyLogitsProcessor,\n        SequenceBiasLogitsProcessor,\n        SynthIDTextWatermarkLogitsProcessor,\n        TemperatureLogitsWarper,\n        TopKLogitsWarper,\n        TopPLogitsWarper,\n        TypicalLogitsWarper,\n        UnbatchedClassifierFreeGuidanceLogitsProcessor,\n        WatermarkLogitsProcessor,\n    )\n    from transformers.generation.logits_process import BarkEosPrioritizerLogitsProcessor\n\n\n@require_torch\nclass LogitsProcessorTest(unittest.TestCase):\n    def _get_uniform_logits(self, batch_size: int, length: int):\n        scores = torch.ones((batch_size, length), device=torch_device, dtype=torch.float) / length\n        return scores\n\n    def test_min_length_dist_processor(self):\n        vocab_size = 20\n        batch_size = 4\n        eos_token_id = 0\n\n        min_dist_processor = MinLengthLogitsProcessor(min_length=10, eos_token_id=eos_token_id, device=torch_device)\n\n        # check that min length is applied at length 5\n        input_ids = ids_tensor((batch_size, 5), vocab_size=20)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        scores_before_min_length = min_dist_processor(input_ids, scores)\n        self.assertListEqual(scores_before_min_length[:, eos_token_id].tolist(), 4 * [-float(\"inf\")])\n\n        # check that min length is not applied anymore at length 15\n        input_ids = ids_tensor((batch_size, 15), vocab_size=20)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        scores_before_min_length = min_dist_processor(input_ids, scores)\n        self.assertFalse(torch.isinf(scores_before_min_length).any())\n\n    @parameterized.expand([(0,), ([0, 18],)])\n    def test_new_min_length_dist_processor(self, eos_token_id: Union[int, List[int]]):\n        vocab_size = 20\n        batch_size = 4\n\n        # check that first input is skipped (min new length applying)\n        input_ids = ids_tensor((batch_size, 5), vocab_size=20)\n        new_min_dist_processor = MinNewTokensLengthLogitsProcessor(\n            prompt_length_to_skip=input_ids.shape[-1], min_new_tokens=3, eos_token_id=eos_token_id, device=torch_device\n        )\n\n        expected_eos_scores_before_min_length = batch_size * [-float(\"inf\")]\n        if isinstance(eos_token_id, list):\n            expected_eos_scores_before_min_length *= len(eos_token_id)\n\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        scores_before_min_length = new_min_dist_processor(input_ids, scores)\n        self.assertListEqual(\n            scores_before_min_length[:, eos_token_id].flatten().tolist(), expected_eos_scores_before_min_length\n        )\n\n        # check that, for skipping, now prompt length is 5, after that we expect first 5 tokens will be skipped\n        self.assertTrue(new_min_dist_processor.prompt_length_to_skip == 5)\n\n        # check that min length is applied at length 2\n        input_ids = ids_tensor((batch_size, 2), vocab_size=20)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        scores_before_min_length = new_min_dist_processor(input_ids, scores)\n        self.assertListEqual(\n            scores_before_min_length[:, eos_token_id].flatten().tolist(), expected_eos_scores_before_min_length\n        )\n\n        # check that min new length is applied at length 6 (because it has only 1 new token)\n        input_ids = ids_tensor((batch_size, 6), vocab_size=20)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        scores_before_min_length = new_min_dist_processor(input_ids, scores)\n        self.assertListEqual(\n            scores_before_min_length[:, eos_token_id].flatten().tolist(), expected_eos_scores_before_min_length\n        )\n\n        # check that min new length is applied at length 7 (because it has only 2 new tokens)\n        input_ids = ids_tensor((batch_size, 7), vocab_size=20)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        scores_before_min_length = new_min_dist_processor(input_ids, scores)\n        self.assertListEqual(\n            scores_before_min_length[:, eos_token_id].flatten().tolist(), expected_eos_scores_before_min_length\n        )\n\n        # check that min new length is not applied anymore at length 8\n        input_ids = ids_tensor((batch_size, 8), vocab_size=20)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        scores_before_min_length = new_min_dist_processor(input_ids, scores)\n        self.assertFalse(torch.isinf(scores_before_min_length).any())\n\n        # check that min new length is not applied anymore at length 15\n        input_ids = ids_tensor((batch_size, 15), vocab_size=20)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        scores_before_min_length = new_min_dist_processor(input_ids, scores)\n        self.assertFalse(torch.isinf(scores_before_min_length).any())\n\n    def test_temperature_dist_warper(self):\n        input_ids = None\n        length = 20\n\n        scores = self._get_uniform_logits(batch_size=2, length=length)\n\n        # tweak scores to not be uniform anymore\n        scores[1, 5] = (1 / length) + 0.1  # peak, 1st batch\n        scores[1, 10] = (1 / length) - 0.4  # valley, 1st batch\n\n        # compute softmax\n        probs = nn.functional.softmax(scores, dim=-1)\n\n        temp_dist_warper_sharper = TemperatureLogitsWarper(temperature=0.5)\n        temp_dist_warper_smoother = TemperatureLogitsWarper(temperature=1.3)\n\n        warped_prob_sharp = nn.functional.softmax(temp_dist_warper_sharper(input_ids, scores), dim=-1)\n        warped_prob_smooth = nn.functional.softmax(temp_dist_warper_smoother(input_ids, scores), dim=-1)\n        processed_scores = temp_dist_warper_smoother(input_ids, scores)\n\n        # uniform distribution stays uniform\n        torch.testing.assert_close(probs[0, :], warped_prob_sharp[0, :], rtol=1e-3, atol=1e-3)\n        torch.testing.assert_close(probs[0, :], warped_prob_smooth[0, :], rtol=1e-3, atol=1e-3)\n\n        # sharp peaks get higher, valleys get lower\n        self.assertLess(probs[1, :].max(), warped_prob_sharp[1, :].max())\n        self.assertGreater(probs[1, :].min(), warped_prob_sharp[1, :].min())\n\n        # smooth peaks get lower, valleys get higher\n        self.assertGreater(probs[1, :].max(), warped_prob_smooth[1, :].max())\n        self.assertLess(probs[1, :].min(), warped_prob_smooth[1, :].min())\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == processed_scores))\n\n    def test_repetition_penalty_dist_process(self):\n        input_ids = torch.tensor([[0, 1], [5, 0]], device=torch_device, dtype=torch.long)\n        vocab_size = 10\n\n        scores = self._get_uniform_logits(batch_size=2, length=vocab_size)\n\n        # give values special values\n        scores[0, 0] = -(1 / vocab_size)\n        scores[1, 5] = 4 / vocab_size\n\n        rep_penalty_proc = RepetitionPenaltyLogitsProcessor(penalty=2.0)\n\n        processed_scores = rep_penalty_proc(input_ids, scores)\n\n        # check that values were correctly changed\n        self.assertAlmostEqual(processed_scores[0, 0].item(), -(1 / vocab_size) * 2)\n        self.assertAlmostEqual(processed_scores[0, 1].item(), (1 / vocab_size) / 2)\n\n        self.assertAlmostEqual(processed_scores[1, 0].item(), (1 / vocab_size) / 2)\n        self.assertAlmostEqual(processed_scores[1, 5].item(), (4 / vocab_size) / 2)\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == processed_scores))\n\n    def test_encoder_repetition_penalty_dist_process(self):\n        input_ids = torch.tensor([[0, 1], [5, 0]], device=torch_device, dtype=torch.long)\n        vocab_size = 10\n\n        scores = self._get_uniform_logits(batch_size=2, length=vocab_size)\n\n        # give values special values\n        scores[0, 0] = -(1 / vocab_size)\n        scores[1, 5] = 4 / vocab_size\n\n        rep_penalty_proc = EncoderRepetitionPenaltyLogitsProcessor(penalty=2.0, encoder_input_ids=input_ids)\n\n        processed_scores = rep_penalty_proc(input_ids, scores)\n\n        # check that values were correctly changed\n        self.assertAlmostEqual(processed_scores[0, 0].item(), -(1 / vocab_size) / 2)\n        self.assertAlmostEqual(processed_scores[0, 1].item(), (1 / vocab_size) * 2)\n\n        self.assertAlmostEqual(processed_scores[1, 0].item(), (1 / vocab_size) * 2)\n        self.assertAlmostEqual(processed_scores[1, 5].item(), (4 / vocab_size) * 2)\n\n        # check that values not in the encoder ids were NOT changed\n        self.assertAlmostEqual(processed_scores[0, 2].item(), (1 / vocab_size))\n        self.assertAlmostEqual(processed_scores[1, 2].item(), (1 / vocab_size))\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == processed_scores))\n\n    def test_top_k_dist_warper(self):\n        input_ids = None\n        vocab_size = 10\n        batch_size = 2\n\n        # create ramp distribution\n        ramp_logits = (\n            torch.arange(vocab_size, device=torch_device, dtype=torch.float).unsqueeze(0).repeat(batch_size, 1)\n        )\n        ramp_logits[1:, : vocab_size // 2] = ramp_logits[1:, : vocab_size // 2] + vocab_size\n\n        top_k_warp = TopKLogitsWarper(3)\n\n        scores = top_k_warp(input_ids, ramp_logits)\n\n        # check that correct tokens are filtered\n        self.assertListEqual(torch.isinf(scores[0]).tolist(), 7 * [True] + 3 * [False])\n        self.assertListEqual(torch.isinf(scores[1]).tolist(), 2 * [True] + 3 * [False] + 5 * [True])\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == ramp_logits))\n\n        # check special cases\n        length = 5\n\n        logits = self._get_uniform_logits(batch_size=batch_size, length=length)\n        top_k_warp_safety_check = TopKLogitsWarper(top_k=1, filter_value=0.0, min_tokens_to_keep=3)\n\n        scores = top_k_warp_safety_check(input_ids, logits)\n        # uniform dist is not changed\n        self.assertListEqual((scores == 0.0).to(torch.long).sum(dim=-1).tolist(), [0, 0])\n\n        ramp_logits = torch.arange(length, device=torch_device, dtype=torch.float).unsqueeze(0).repeat(batch_size, 1)\n        scores = top_k_warp_safety_check(input_ids, ramp_logits)\n\n        # min_tokens overwrites k: 3 tokens are kept => 2 tokens are nullified\n        self.assertListEqual((scores == 0.0).to(torch.long).sum(dim=-1).tolist(), [2, 2])\n\n    def test_top_p_dist_warper(self):\n        input_ids = None\n        vocab_size = 10\n        batch_size = 2\n\n        # create distribution and take log (inverse to Softmax as taken in TopPLogitsWarper)\n        dist = torch.log(\n            torch.tensor([[0.3, 0.1, 0.1, 0.5], [0.15, 0.3, 0.3, 0.25]], device=torch_device, dtype=torch.float)\n        )\n\n        top_p_warp = TopPLogitsWarper(0.8)\n        filtered_dist = torch.exp(top_p_warp(input_ids, dist))\n\n        # dist should be filtered to keep min num values so that sum is >= top_p\n        # exp (-inf) => 0\n        EXPECTED_FILTERED_DIST = torch.tensor(\n            [[0.3, 0.0, 0.0, 0.5], [0.0, 0.3, 0.3, 0.25]], device=torch_device, dtype=torch.float\n        )\n        torch.testing.assert_close(filtered_dist, EXPECTED_FILTERED_DIST, rtol=1e-3, atol=1e-3)\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(top_p_warp(input_ids, dist) == dist))\n\n        # check edge cases with negative and extreme logits\n        ramp_logits = torch.arange(vocab_size, device=torch_device, dtype=torch.float).unsqueeze(0).repeat(\n            batch_size, 1\n        ) - (vocab_size // 2)\n\n        # make ramp_logits more extreme\n        ramp_logits[1] = ramp_logits[1] * 100.0\n\n        # make sure at least 2 tokens are kept\n        top_p_warp = TopPLogitsWarper(0.9, min_tokens_to_keep=2, filter_value=0.0)\n        filtered_dist = top_p_warp(input_ids, ramp_logits)\n\n        # first batch should keep three tokens, second batch would keep only 1, but due to `min_tokens_to_keep=2` keeps 2.\n        self.assertListEqual((filtered_dist != 0.0).to(torch.long).sum(dim=-1).tolist(), [3, 2])\n\n    def test_min_p_dist_warper(self):\n        input_ids = None\n        vocab_size = 10\n        batch_size = 2\n\n        # create distribution and take log (inverse to Softmax as taken in MinPLogitsWarper)\n        dist = torch.log(\n            torch.tensor(\n                [\n                    [0.9, 0.0274, 0.047, 0.0274],  # two tokens should be kept (0.047 > 0.9*0.05=0.045)\n                    [0.15, 0.3, 0.3, 0.25],  # all should be kept -- no high-probability token\n                    [0.97, 0.01, 0.01, 0.01],  # only the first token should be kept\n                ],\n                device=torch_device,\n                dtype=torch.float,\n            )\n        )\n\n        min_p_warp = MinPLogitsWarper(0.05)\n        filtered_dist = torch.exp(min_p_warp(input_ids, dist))\n\n        # exp (-inf) => 0\n        EXPECTED_FILTERED_DIST = torch.tensor(\n            [[0.9, 0.0, 0.047, 0.0], [0.15, 0.3, 0.3, 0.25], [0.97, 0.0, 0.0, 0.0]],\n            device=torch_device,\n            dtype=torch.float,\n        )\n        torch.testing.assert_close(filtered_dist, EXPECTED_FILTERED_DIST, rtol=1e-3, atol=1e-3)\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(min_p_warp(input_ids, dist) == dist))\n\n        # check edge cases with negative and extreme logits\n        ramp_logits = torch.arange(vocab_size, device=torch_device, dtype=torch.float) - (vocab_size // 2)\n        ramp_logits = ramp_logits.unsqueeze(0).repeat(batch_size, 1)\n\n        # make ramp_logits more extreme\n        ramp_logits[1] = ramp_logits[1] * 100.0\n\n        # make sure at least 2 tokens are kept\n        min_p_warp = MinPLogitsWarper(0.9, min_tokens_to_keep=2, filter_value=0.0)\n        filtered_dist = min_p_warp(input_ids, ramp_logits)\n\n        # first batch should keep two tokens, second batch would keep only 1, but due to `min_tokens_to_keep=2` keeps 2.\n        self.assertListEqual((filtered_dist != 0.0).to(torch.long).sum(dim=-1).tolist(), [2, 2])\n\n    def test_typical_dist_warper(self):\n        input_ids = None\n        vocab_size = 10\n        batch_size = 2\n\n        # create distribution and take log (inverse to Softmax as taken in TopPLogitsWarper)\n        dist = torch.log(\n            torch.tensor([[0.97, 0.01, 0.01, 0.01], [0.4, 0.2, 0.2, 0.2]], device=torch_device, dtype=torch.float)\n        )\n\n        typical_warp = TypicalLogitsWarper(0.5)\n        filtered_dist = torch.exp(typical_warp(input_ids, dist))\n\n        # dist should be filtered to keep min num values so that sum is >= 0.7\n        # exp (-inf) => 0\n        EXPECTED_FILTERED_DIST = torch.tensor(\n            [[0.97, 0.0, 0.0, 0.0], [0.0, 0.2, 0.2, 0.2]], device=torch_device, dtype=torch.float\n        )\n        torch.testing.assert_close(filtered_dist, EXPECTED_FILTERED_DIST, rtol=1e-3, atol=1e-3)\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(typical_warp(input_ids, dist) == dist))\n\n        # check special cases\n        length = 5\n\n        logits = self._get_uniform_logits(batch_size=batch_size, length=length)\n        typical_warp_safety_check = TypicalLogitsWarper(mass=0.5, filter_value=0.0, min_tokens_to_keep=3)\n\n        scores = typical_warp_safety_check(input_ids, logits)\n        # uniform dist is not changed\n        self.assertListEqual((scores == 0.0).to(torch.long).sum(dim=-1).tolist(), [0, 0])\n\n        # check edge cases with negative and extreme logits\n        ramp_logits = torch.arange(vocab_size, device=torch_device, dtype=torch.float).unsqueeze(0).repeat(\n            batch_size, 1\n        ) - (vocab_size // 2)\n\n        # make ramp_logits more extreme\n        ramp_logits[1] = ramp_logits[1] * 100.0\n\n        # make sure at least 2 tokens are kept\n        typical_warp = TypicalLogitsWarper(0.7, min_tokens_to_keep=2, filter_value=0.0)\n        filtered_dist = typical_warp(input_ids, ramp_logits)\n\n        # first batch should keep two tokens, second batch would keep only 1, but due to `min_tokens_to_keep=2` keeps 2.\n        self.assertListEqual((filtered_dist != 0.0).to(torch.long).sum(dim=-1).tolist(), [2, 2])\n\n    def test_epsilon_dist_warper(self):\n        input_ids = None\n        vocab_size = 10\n        batch_size = 2\n\n        # create distribution and take log (inverse to Softmax as taken in TopPLogitsWarper)\n        dist = torch.log(\n            torch.tensor(\n                [[0.87, 0.099, 0.001, 0.03], [0.4, 0.299, 0.101, 0.2]], device=torch_device, dtype=torch.float\n            )\n        )\n\n        epsilon_warp = EpsilonLogitsWarper(0.1)\n        filtered_dist = torch.exp(epsilon_warp(input_ids, dist))\n\n        # dist should be filtered to only keep values with proba >= 0.1\n        # exp (-inf) => 0\n        EXPECTED_FILTERED_DIST = torch.tensor(\n            [[0.87, 0, 0, 0], [0.4, 0.299, 0.101, 0.2]], device=torch_device, dtype=torch.float\n        )\n        torch.testing.assert_close(filtered_dist, EXPECTED_FILTERED_DIST, rtol=1e-3, atol=1e-3)\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(epsilon_warp(input_ids, dist) == dist))\n\n        # check edge cases with negative and extreme logits\n        ramp_logits = torch.arange(vocab_size, device=torch_device, dtype=torch.float).unsqueeze(0).repeat(\n            batch_size, 1\n        ) - (vocab_size // 2)\n\n        # make ramp_logits more extreme\n        ramp_logits[1] = ramp_logits[1] * 100.0\n\n        # make sure at least 2 tokens are kept\n        epsilon_warp = EpsilonLogitsWarper(5e-2, min_tokens_to_keep=2, filter_value=0.0)\n        filtered_dist = epsilon_warp(input_ids, ramp_logits)\n\n        # first batch should keep 3 tokens, second batch would keep only 1, but due to `min_tokens_to_keep=2` keeps 2.\n        self.assertListEqual((filtered_dist != 0.0).to(torch.long).sum(dim=-1).tolist(), [3, 2])\n\n    def test_eta_dist_warper(self):\n        input_ids = None\n        vocab_size = 10\n        batch_size = 2\n\n        # create distribution and take log (inverse to Softmax as taken in TopPLogitsWarper)\n        dist = torch.log(\n            torch.tensor([[0.0, 0.1, 0.8, 0.1], [0.01, 0.04, 0.9, 0.05]], device=torch_device, dtype=torch.float)\n        )\n\n        eta_warp = EtaLogitsWarper(0.0625, device=torch_device)\n        filtered_dist = torch.exp(eta_warp(input_ids, dist))\n\n        # dist should be filtered to only keep values with proba >= min(0.0625, sqrt(0.0625) * e^-H(p))\n        # min(0.0625, 0.1320) is the cutoff for the first row and min(0.0625, 0.1644) is for the second\n        # where H is the entropy function and p is the probability vector.\n        # exp (-inf) => 0\n        EXPECTED_FILTERED_DIST = torch.tensor(\n            [[0.0, 0.1, 0.8, 0.1], [0.0, 0.0, 0.9, 0.0]], device=torch_device, dtype=torch.float\n        )\n        torch.testing.assert_close(filtered_dist, EXPECTED_FILTERED_DIST, rtol=1e-3, atol=1e-3)\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(eta_warp(input_ids, dist) == dist))\n\n        # check edge cases with negative and extreme logits\n        ramp_logits = torch.arange(vocab_size, device=torch_device, dtype=torch.float).unsqueeze(0).repeat(\n            batch_size, 1\n        ) - (vocab_size // 2)\n\n        # make ramp_logits more extreme\n        ramp_logits[1] = ramp_logits[1] * 100.0\n\n        # make sure at least 2 tokens are kept\n        eta_warp = EtaLogitsWarper(0.1, min_tokens_to_keep=2, filter_value=0.0, device=torch_device)\n        filtered_dist = eta_warp(input_ids, ramp_logits)\n\n        # first batch should keep 2 tokens, second batch would keep only 1, but due to `min_tokens_to_keep=2` keeps 2.\n        self.assertListEqual((filtered_dist != 0.0).to(torch.long).sum(dim=-1).tolist(), [2, 2])\n\n    def test_no_repeat_ngram_dist_processor(self):\n        vocab_size = 3\n        batch_size = 2\n\n        input_ids = torch.tensor([[1, 1, 2, 1], [0, 1, 0, 1]], device=torch_device, dtype=torch.long)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n\n        no_repeat_proc_2_gram = NoRepeatNGramLogitsProcessor(2)\n        no_repeat_proc_3_gram = NoRepeatNGramLogitsProcessor(3)\n\n        filtered_scores_2_gram = no_repeat_proc_2_gram(input_ids, scores)\n        filtered_scores_3_gram = no_repeat_proc_3_gram(input_ids, scores)\n\n        # 2-gram would forbid 2nd and 3rd token (1,2) at 1st batch and 1st token (0) at 2nd batch\n        self.assertListEqual(torch.isinf(filtered_scores_2_gram).tolist(), [[False, True, True], [True, False, False]])\n\n        # 3-gram would forbid no token at 1st batch and 1st token (0) at 2nd batch\n        self.assertListEqual(\n            torch.isinf(filtered_scores_3_gram).tolist(), [[False, False, False], [True, False, False]]\n        )\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == filtered_scores_2_gram))\n        self.assertFalse(torch.all(scores == filtered_scores_3_gram))\n\n    def test_encoder_no_repeat_ngram_dist_processor(self):\n        vocab_size = 3\n        num_beams = 2\n        batch_size = 1\n\n        encoder_input_ids = torch.tensor([1, 2, 1, 1], device=torch_device, dtype=torch.long)\n\n        input_ids = torch.tensor([[1, 2, 1], [8, 0, 2]], device=torch_device, dtype=torch.long)\n        scores = self._get_uniform_logits(batch_size * num_beams, vocab_size)\n\n        no_repeat_proc_2_gram = EncoderNoRepeatNGramLogitsProcessor(2, encoder_input_ids=encoder_input_ids)\n        no_repeat_proc_3_gram = EncoderNoRepeatNGramLogitsProcessor(3, encoder_input_ids=encoder_input_ids)\n\n        filtered_scores_2_gram = no_repeat_proc_2_gram(input_ids, scores)\n        filtered_scores_3_gram = no_repeat_proc_3_gram(input_ids, scores)\n\n        # 2-gram would forbid 1st and 2nd token at 1st beam and 1st token (0) at 2nd beam\n        self.assertListEqual(torch.isinf(filtered_scores_2_gram).tolist(), [[False, True, True], [False, True, False]])\n\n        # 3-gram would forbid 1st token at 1st beam and no token at 2nd beam\n        self.assertListEqual(\n            torch.isinf(filtered_scores_3_gram).tolist(), [[False, True, False], [False, False, False]]\n        )\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == filtered_scores_2_gram))\n        self.assertFalse(torch.all(scores == filtered_scores_3_gram))\n\n        # Batched input\n        vocab_size = 3\n        num_beams = 2\n        batch_size = 2\n        encoder_input_ids = torch.tensor([[1, 2, 1, 1], [0, 0, 2, 1]], device=torch_device, dtype=torch.long)\n\n        input_ids = torch.tensor([[1, 2, 1], [1, 0, 2], [0, 0, 0], [0, 2, 2]], device=torch_device, dtype=torch.long)\n        scores = self._get_uniform_logits(batch_size * num_beams, vocab_size)\n\n        no_repeat_proc_2_gram = EncoderNoRepeatNGramLogitsProcessor(2, encoder_input_ids=encoder_input_ids)\n        no_repeat_proc_3_gram = EncoderNoRepeatNGramLogitsProcessor(3, encoder_input_ids=encoder_input_ids)\n\n        filtered_scores_2_gram = no_repeat_proc_2_gram(input_ids, scores.clone())\n        filtered_scores_3_gram = no_repeat_proc_3_gram(input_ids, scores.clone())\n\n        # 2gram\n        # Batch 1\n        #   - Beam 1: tokens (1, 2) forbidden\n        #   - Beam 2: tokens (1) forbidden\n        # Batch 2\n        #   - Beam 1: tokens (0, 2) forbidden\n        #   - Beam 2: tokens (1) forbidden\n        self.assertListEqual(\n            torch.isinf(filtered_scores_2_gram).tolist(),\n            [[False, True, True], [False, True, False], [True, False, True], [False, True, False]],\n        )\n\n        # Batch 1\n        #   - Beam 1: tokens (1) forbidden\n        #   - Beam 2: tokens () forbidden\n        # Batch 2\n        #   - Beam 1: tokens (2) forbidden\n        #   - Beam 2: tokens () forbidden\n        self.assertListEqual(\n            torch.isinf(filtered_scores_3_gram).tolist(),\n            [[False, True, False], [False, False, False], [False, False, True], [False, False, False]],\n        )\n\n    def test_no_bad_words_dist_processor(self):\n        vocab_size = 5\n        batch_size = 2\n        eos_token_id = 4\n\n        input_ids = torch.tensor([[0, 1, 3, 1], [0, 1, 0, 1]], device=torch_device, dtype=torch.long)\n        bad_word_tokens = [[1], [4], [1, 0], [0, 1, 2], [1, 3, 1, 3]]\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n\n        no_bad_words_dist_proc = NoBadWordsLogitsProcessor(bad_words_ids=bad_word_tokens, eos_token_id=eos_token_id)\n\n        filtered_scores = no_bad_words_dist_proc(input_ids, scores)\n\n        # batch 1: 1st, 2nd, and 4th (0, 1, 3) token are forbidden\n        # batch 2: 1st, 2nd, and 3rd (0, 1, 2) token are forbidden\n        # Note that 5th element cannot be forbidden as it is EOS token\n        self.assertListEqual(\n            torch.isinf(filtered_scores).tolist(), [[True, True, False, True, False], [True, True, True, False, False]]\n        )\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == filtered_scores))\n\n        # check edge case\n        no_bad_words_dist_proc = NoBadWordsLogitsProcessor(bad_words_ids=[[4]], eos_token_id=eos_token_id)\n        filtered_scores = no_bad_words_dist_proc(input_ids, scores)\n        torch.testing.assert_close(scores, filtered_scores, rtol=1e-3, atol=1e-3)\n\n    def test_bias_dist_processor(self):\n        vocab_size = 5\n        batch_size = 2\n\n        input_ids = torch.tensor([[0, 1, 3, 1], [0, 1, 0, 1]], device=torch_device, dtype=torch.long)\n        positive_bias = {(1,): 100.0, (4,): 100.0}\n        negative_bias = {(1, 0): -100.0, (0, 1, 2): -100.0, (1, 3, 1, 3): -100.0}\n        # biases the same termination twice, to ensure we can handle overlapping terminations (it won't have an effect\n        # on the test cases, though)\n        negative_bias.update({(1, 3, 1, 3, 1, 3): -100.0})\n        sequence_bias = {**positive_bias, **negative_bias}\n\n        # scores = 0 to facilitate checks\n        scores = torch.zeros((batch_size, vocab_size), dtype=torch.float, device=torch_device)\n\n        bias_dist_proc = SequenceBiasLogitsProcessor(sequence_bias=sequence_bias)\n        filtered_scores = bias_dist_proc(input_ids, scores)\n\n        # batch 1: positive bias: tokens (1, 4); negative bias: tokens (0, 3); neutral: tokens (2)\n        # batch 2: positive bias: tokens (1, 4); negative bias: tokens (0, 2); neutral: tokens (3)\n        self.assertListEqual(\n            filtered_scores.tolist(), [[-100.0, 100.0, 0.0, -100.0, 100.0], [-100.0, 100.0, -100.0, 0.0, 100.0]]\n        )\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == filtered_scores))\n\n    def test_processor_list(self):\n        batch_size = 4\n        sequence_length = 10\n        vocab_size = 15\n        eos_token_id = 0\n\n        # dummy input_ids and scores\n        input_ids = ids_tensor((batch_size, sequence_length), vocab_size)\n        input_ids_comp = input_ids.clone()\n\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        scores_comp = scores.clone()\n\n        # instantiate all dist processors\n        min_dist_proc = MinLengthLogitsProcessor(min_length=10, eos_token_id=eos_token_id, device=torch_device)\n        temp_dist_warp = TemperatureLogitsWarper(temperature=0.5)\n        rep_penalty_proc = RepetitionPenaltyLogitsProcessor(penalty=2.0)\n        top_k_warp = TopKLogitsWarper(3)\n        top_p_warp = TopPLogitsWarper(0.8)\n        no_repeat_proc = NoRepeatNGramLogitsProcessor(2)\n        no_bad_words_dist_proc = NoBadWordsLogitsProcessor(bad_words_ids=[[1]], eos_token_id=eos_token_id)\n\n        # no processor list\n        scores = min_dist_proc(input_ids, scores)\n        scores = temp_dist_warp(input_ids, scores)\n        scores = rep_penalty_proc(input_ids, scores)\n        scores = top_k_warp(input_ids, scores)\n        scores = top_p_warp(input_ids, scores)\n        scores = no_repeat_proc(input_ids, scores)\n        scores = no_bad_words_dist_proc(input_ids, scores)\n\n        # with processor list\n        processor = LogitsProcessorList(\n            [\n                min_dist_proc,\n                temp_dist_warp,\n                rep_penalty_proc,\n                top_k_warp,\n                top_p_warp,\n                no_repeat_proc,\n                no_bad_words_dist_proc,\n            ]\n        )\n        scores_comp = processor(input_ids, scores_comp)\n\n        # scores should be equal\n        torch.testing.assert_close(scores, scores_comp, rtol=1e-3, atol=1e-3)\n\n        # input_ids should never be changed\n        self.assertListEqual(input_ids.tolist(), input_ids_comp.tolist())\n\n    def test_prefix_constrained_logits_processor(self):\n        vocab_size = 5\n        batch_size = 2\n\n        input_ids = torch.tensor([[0, 1, 3, 1], [0, 1, 0, 1]], device=torch_device, dtype=torch.long)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n\n        def prefix_allowed_tokens_fn(batch_id, inputs_ids):\n            return [[0, 1], [2, 3]][batch_id]\n\n        prefix_constrained_logits_proc = PrefixConstrainedLogitsProcessor(prefix_allowed_tokens_fn, 1)\n\n        filtered_scores = prefix_constrained_logits_proc(input_ids, scores)\n\n        # batch 1: 1st, 2nd (0, 1) token are allowed\n        # batch 2: 3rd, 4th (2, 3) token are allowed\n        self.assertListEqual(\n            torch.isinf(filtered_scores).tolist(), [[False, False, True, True, True], [True, True, False, False, True]]\n        )\n\n        def empty_prefix_allowed_tokens_fn(batch_id, inputs_ids):\n            return []\n\n        prefix_constrained_logits_proc = PrefixConstrainedLogitsProcessor(empty_prefix_allowed_tokens_fn, 1)\n\n        self.assertRaises(ValueError, prefix_constrained_logits_proc, input_ids, scores)\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == filtered_scores))\n\n    def test_hamming_diversity(self):\n        vocab_size = 4\n        num_beams = 2\n        num_beam_groups = 2\n\n        scores = self._get_uniform_logits(num_beams, vocab_size)\n        # batch_idx = 0 -> index batch_idx * num_beam_groups -> idx = 0 * 2 = 0 -> penalises tokens 1\n        # batch_idx = 1 -> index batch_idx * num_beam_groups -> idx = 1 * 2 = 2 -> penalises tokens 1\n        current_tokens = torch.tensor([0, 3, 1, 2], device=torch_device, dtype=torch.long)\n\n        diversity_logits_processor = HammingDiversityLogitsProcessor(\n            diversity_penalty=1.0, num_beams=num_beams, num_beam_groups=num_beam_groups\n        )\n\n        processed_scores = diversity_logits_processor(None, scores, current_tokens, 1)\n\n        self.assertTrue(\n            torch.allclose(\n                processed_scores[0], torch.tensor([-0.7500, 0.2500, 0.2500, 0.2500], device=torch_device), atol=1e-3\n            )\n        )\n        self.assertTrue(\n            torch.allclose(\n                processed_scores[1], torch.tensor([0.2500, -0.7500, 0.2500, 0.2500], device=torch_device), atol=1e-3\n            )\n        )\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == processed_scores))\n\n    def test_forced_bos_token_logits_processor(self):\n        vocab_size = 20\n        batch_size = 4\n        bos_token_id = 0\n\n        logits_processor = ForcedBOSTokenLogitsProcessor(bos_token_id=bos_token_id)\n\n        # check that all scores are -inf except the bos_token_id score\n        input_ids = ids_tensor((batch_size, 1), vocab_size=20)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        processed_scores = logits_processor(input_ids, scores)\n        self.assertTrue(torch.isneginf(processed_scores[:, bos_token_id + 1 :]).all())\n        # score for bos_token_id should be zero\n        self.assertListEqual(processed_scores[:, bos_token_id].tolist(), 4 * [0])\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == processed_scores))\n\n        # check that bos_token_id is not forced if current length is greater than 1\n        input_ids = ids_tensor((batch_size, 4), vocab_size=20)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        processed_scores = logits_processor(input_ids, scores)\n        self.assertFalse(torch.isinf(processed_scores).any())\n\n    def test_forced_eos_token_logits_processor(self):\n        vocab_size = 20\n        batch_size = 4\n        eos_token_id = 0\n        max_length = 5\n\n        logits_processor = ForcedEOSTokenLogitsProcessor(\n            max_length=max_length, eos_token_id=eos_token_id, device=torch_device\n        )\n\n        # check that all scores are -inf except the eos_token_id when max_length-1 is reached\n        input_ids = ids_tensor((batch_size, 4), vocab_size=20)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        processed_scores = logits_processor(input_ids, scores)\n        self.assertTrue(torch.isneginf(processed_scores[:, eos_token_id + 1 :]).all())\n        # score for eos_token_id should be zero\n        self.assertListEqual(processed_scores[:, eos_token_id].tolist(), 4 * [0])\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == processed_scores))\n\n        # check that eos_token_id is not forced if max_length-1 is not reached\n        input_ids = ids_tensor((batch_size, 3), vocab_size=20)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        processed_scores = logits_processor(input_ids, scores)\n        self.assertFalse(torch.isinf(processed_scores).any())\n\n    def test_remove_nan_inf_logits_processor(self):\n        scores = torch.tensor(\n            [[0.0, 0.7, 0.8, float(\"nan\")], [0.1, float(\"inf\"), 0.3, float(\"-inf\")]], device=torch_device\n        )\n        input_ids = ids_tensor((2, 4), vocab_size=20)\n\n        logits_processor = InfNanRemoveLogitsProcessor()\n\n        processed_scores = logits_processor(input_ids, scores)\n\n        self.assertTrue(\n            torch.allclose(\n                processed_scores,\n                torch.tensor(\n                    [\n                        [0.0, 0.7, 0.8, 0.0],\n                        [0.1, torch.finfo(processed_scores.dtype).max, 0.3, torch.finfo(processed_scores.dtype).min],\n                    ],\n                    device=torch_device,\n                ),\n                atol=1e-6,\n            )\n        )\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == processed_scores))\n\n    def test_exponential_decay_length_penalty(self):\n        vocab_size = 20\n        batch_size = 4\n        eos_token_id = 0\n\n        penalty_start = 5\n        penalty_factor = 1.1\n\n        input_ids = ids_tensor((batch_size, 2), vocab_size=vocab_size)\n        input_ids_seq_length = input_ids.shape[-1]\n\n        length_decay_processor = ExponentialDecayLengthPenalty(\n            exponential_decay_length_penalty=(penalty_start, penalty_factor),\n            eos_token_id=eos_token_id,\n            input_ids_seq_length=input_ids_seq_length,\n        )\n\n        # check that penalty is not applied before start\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        scores_before_start = length_decay_processor(input_ids, scores)\n        self.assertListEqual(scores_before_start[:, eos_token_id].tolist(), scores[:, eos_token_id].tolist())\n\n        # check that penalty is applied after start\n        input_ids = ids_tensor((batch_size, 20), vocab_size=vocab_size)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n        scores_after_start = length_decay_processor(input_ids, scores)\n        self.assertTrue(torch.gt(scores_after_start[:, eos_token_id], scores[:, eos_token_id]).all())\n\n        # check the penalty increases negative scores\n        input_ids = ids_tensor((batch_size, 20), vocab_size=vocab_size)\n        scores = torch.neg(self._get_uniform_logits(batch_size, vocab_size))\n        scores_after_start = length_decay_processor(input_ids, scores)\n        self.assertTrue(torch.gt(scores_after_start[:, eos_token_id], scores[:, eos_token_id]).all())\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == scores_after_start))\n\n    def test_normalization(self):\n        input_ids = None\n\n        scores = torch.tensor(\n            [[-23.18, -29.96, -43.54, 47.77], [-33.58, -26.87, -32.96, 22.51]], device=torch_device, dtype=torch.float\n        )\n\n        logit_normalization = LogitNormalization()\n        normalized_scores = logit_normalization(input_ids, scores).exp()\n\n        ones = torch.ones(scores.shape[0], device=torch_device, dtype=torch.float)\n        self.assertTrue(normalized_scores.sum(dim=-1).allclose(ones))\n\n        self.assertTrue(normalized_scores.allclose(scores.softmax(dim=-1)))\n\n        # processor should not change logits in-place\n        self.assertFalse(torch.all(scores == normalized_scores))\n\n    def test_classifier_free_guidance(self):\n        class Namespace(dict):\n            pass\n\n        logits_uncond = torch.tensor([[[1.0, 0, 1.5]]])\n        logits_cond = torch.tensor([[[1.0, 1.0, 1.0]]])\n\n        def dummy_model(input_ids, attention_mask, use_cache=True, past_key_values=None):\n            out = Namespace()\n            out.logits = logits_uncond\n            out.past_key_values = None\n            return out\n\n        def lsm(x):\n            return torch.nn.functional.log_softmax(x, dim=-1)\n\n        # explicit unconditional prompt + attention mask\n        input_ids = torch.LongTensor([[0]])\n        cfg = UnbatchedClassifierFreeGuidanceLogitsProcessor(\n            1.5, dummy_model, input_ids, torch.ones_like(input_ids, dtype=torch.long)\n        )\n        out = cfg(input_ids, logits_cond)[0, -1]\n\n        res = (lsm(logits_uncond) + 1.5 * (lsm(logits_cond) - lsm(logits_uncond)))[0, -1]\n\n        self.assertAlmostEqual(out[0].item(), res[0].item())\n        self.assertAlmostEqual(out[1].item(), res[1].item())\n        self.assertAlmostEqual(out[2].item(), res[2].item())\n\n        # explicit unconditional prompt\n        input_ids = torch.LongTensor([[0]])\n        cfg = UnbatchedClassifierFreeGuidanceLogitsProcessor(1.5, dummy_model, input_ids)\n        out = cfg(input_ids, logits_cond)[0, -1]\n\n        res = (lsm(logits_uncond) + 1.5 * (lsm(logits_cond) - lsm(logits_uncond)))[0, -1]\n\n        self.assertAlmostEqual(out[0].item(), res[0].item())\n        self.assertAlmostEqual(out[1].item(), res[1].item())\n        self.assertAlmostEqual(out[2].item(), res[2].item())\n\n        # all implicit\n        input_ids = torch.LongTensor([[0]])\n        cfg = UnbatchedClassifierFreeGuidanceLogitsProcessor(1.5, dummy_model)\n        out = cfg(input_ids, logits_cond)[0, -1]\n\n        res = (lsm(logits_uncond) + 1.5 * (lsm(logits_cond) - lsm(logits_uncond)))[0, -1]\n\n        self.assertAlmostEqual(out[0].item(), res[0].item())\n        self.assertAlmostEqual(out[1].item(), res[1].item())\n        self.assertAlmostEqual(out[2].item(), res[2].item())\n\n    def test_early_stop_processor(self):\n        input_ids = None\n        eos_token_id = 2\n        min_eos_p = 0.1  ## some small float\n\n        scores = self._get_uniform_logits(2, 4)\n        scores[0][eos_token_id] = -6  ## less than log(min_eos_p)\n\n        esp = BarkEosPrioritizerLogitsProcessor(eos_token_id=eos_token_id, min_eos_p=min_eos_p, device=torch_device)\n        actual_scores = esp(input_ids, scores)\n        expected_scores_list = [\n            scores[0].tolist(),\n            [float(\"-inf\"), float(\"-inf\"), scores[0][0], float(\"-inf\")],\n        ]\n        self.assertListEqual(actual_scores.tolist(), expected_scores_list)\n\n    def test_early_stop_processor_multi_eos(self):\n        input_ids = None\n        eos_token_id = [2, 3]\n        min_eos_p = 0.1  ## some small float\n\n        scores = self._get_uniform_logits(2, 4)\n        scores[0][eos_token_id] = -6  ## less than log(min_eos_p)\n\n        esp = BarkEosPrioritizerLogitsProcessor(eos_token_id=eos_token_id, min_eos_p=min_eos_p, device=torch_device)\n        actual_scores = esp(input_ids, scores)\n        expected_scores_list = [\n            scores[0].tolist(),\n            [float(\"-inf\"), float(\"-inf\"), scores[0][0], scores[0][0]],\n        ]\n        self.assertListEqual(actual_scores.tolist(), expected_scores_list)\n\n    def test_watermarking_processor(self):\n        batch_size = 3\n        vocab_size = 20\n\n        input_ids = ids_tensor((batch_size, 5), vocab_size=20)\n        scores = self._get_uniform_logits(batch_size, vocab_size)\n\n        # raise error if incorrect seeding_scheme is passed\n        with self.assertRaises(ValueError):\n            WatermarkLogitsProcessor(vocab_size=vocab_size, device=\"cpu\", seeding_scheme=\"hash\")\n\n        # raise error if the greenlist_ratio in not in range (0.0, 1.0)\n        with self.assertRaises(ValueError):\n            WatermarkLogitsProcessor(vocab_size=vocab_size, device=\"cpu\", greenlist_ratio=1.2)\n\n        watermark = WatermarkLogitsProcessor(vocab_size=vocab_size, device=input_ids.device)\n\n        # use fixed id for last token, needed for reproducibility and tests\n        input_ids[:, -1] = 10\n        scores_wo_bias = scores[:, -1].clone()\n        out = watermark(input_ids=input_ids, scores=scores)\n        self.assertTrue((out[:, 1] == scores_wo_bias + watermark.bias).all())\n\n    @parameterized.expand([(5, 3, 10000), (10, 5, 1000)])\n    def test_synthidtext_watermarking_processor_bias_uniformity(self, ngram_len, num_layers, vocab_size):\n        \"\"\"Test SynthID watermarked distribution bias uniformity over iterations.\"\"\"\n        torch.manual_seed(0)\n        np.random.seed(0)\n        watermarking_config = {\n            \"ngram_len\": ngram_len,\n            \"keys\": np.random.randint(low=0, high=2**16, size=(num_layers,)),\n            \"sampling_table_size\": 2**16,\n            \"sampling_table_seed\": 0,\n            \"context_history_size\": 512,\n            \"device\": torch_device,\n        }\n        batch_size = 100000\n        ngrams = torch.randint(\n            low=0,\n            high=vocab_size,\n            size=(batch_size, ngram_len),\n            device=torch_device,\n        )\n\n        logits_processor = SynthIDTextWatermarkLogitsProcessor(**watermarking_config)\n        g_values = logits_processor.compute_g_values(ngrams)\n        g_values_mean = torch.mean(torch.mean(g_values.float(), dim=0))\n        self.assertAlmostEqual(g_values_mean, 0.5, delta=0.01)\n\n    @parameterized.expand([(10000, 3), (1000, 20)])\n    def test_synthidtext_watermark_processor_bias_uniformity_across_vocab(self, vocab_size, num_layers):\n        \"\"\"Test SynthID watermarked distribution bias uniformity over vocabs of the model.\"\"\"\n        batch_size = 1000\n        ngram_len = 5\n        torch.manual_seed(0)\n        np.random.seed(0)\n        watermarking_config = {\n            \"ngram_len\": ngram_len,\n            \"keys\": np.random.randint(low=0, high=2**16, size=(num_layers,)),\n            \"sampling_table_size\": 2**16,\n            \"sampling_table_seed\": 0,\n            \"context_history_size\": 512,\n            \"device\": torch_device,\n        }\n        n_minus_1_grams = torch.randint(\n            low=0,\n            high=vocab_size,\n            size=(batch_size, watermarking_config[\"ngram_len\"] - 1),\n            device=torch_device,\n        )\n\n        logits_processor = SynthIDTextWatermarkLogitsProcessor(**watermarking_config)\n        ngram_keys, _ = logits_processor._compute_keys(\n            n_minus_1_grams,\n            torch.stack([torch.arange(vocab_size, device=torch_device) for _ in range(batch_size)]),\n        )\n\n        g_values = logits_processor.sample_g_values(ngram_keys)\n        # g_values shape should be [batch_size, vocab_size, num_layers]\n        g_values_mean = torch.mean(torch.mean(g_values.float(), dim=1))\n        self.assertAlmostEqual(g_values_mean, 0.5, delta=0.001)\n\n    @parameterized.expand([(2, \"uniform\"), (10, \"uniform\"), (2, \"random\"), (10, \"random\")])\n    def test_synthidtext_watermark_processor_distributional_convergence(self, vocab_size, logits_type):\n        \"\"\"Check if watermarked distribution converges to unwatermarked logits distribution.\"\"\"\n        batch_size = 1500\n        num_keys = 1000\n\n        updated_softmaxes = 0\n        np.random.seed(0)\n        torch.manual_seed(0)\n        if logits_type == \"uniform\":\n            fixed_logits = torch.ones((batch_size, vocab_size), device=torch_device)\n        elif logits_type == \"random\":\n            fixed_logits = torch.rand(\n                (\n                    1,\n                    vocab_size,\n                ),\n                device=torch_device,\n            )\n            fixed_logits = fixed_logits.repeat(batch_size, 1)\n        else:\n            raise ValueError(f\"Unrecognized logits_type {logits_type}\")\n        for _ in range(num_keys):\n            watermarking_config = {\n                \"ngram_len\": 5,\n                \"keys\": np.random.randint(0, 10**9, size=(1,), dtype=np.int64),\n                \"sampling_table_size\": 2**16,\n                \"sampling_table_seed\": 0,\n                \"context_history_size\": 1024,\n                \"device\": torch_device,\n            }\n\n            logits_processor = SynthIDTextWatermarkLogitsProcessor(**watermarking_config)\n\n            ngrams = torch.randint(\n                low=0,\n                high=vocab_size,\n                size=(batch_size, watermarking_config[\"ngram_len\"]),\n                device=torch_device,\n            )\n\n            # Insert ngram-1 into logit_processor state.\n            for idx in range(watermarking_config[\"ngram_len\"] - 1):\n                _ = logits_processor(ngrams[:, :idx], fixed_logits)\n\n            updated_scores = logits_processor(ngrams, fixed_logits)\n            updated_softmaxes += torch.nn.functional.softmax(updated_scores, dim=1).cpu().numpy()\n\n        updated_softmaxes = np.mean(updated_softmaxes, axis=0) / num_keys\n        is_close = torch.all(\n            torch.isclose(\n                torch.tensor(updated_softmaxes, device=torch_device),\n                torch.nn.Softmax()(fixed_logits[0]),  # Take any batch entry, all are same.\n                atol=1e-3,\n                rtol=0,\n            )\n        )\n        self.assertTrue(is_close)\n\n    @parameterized.expand([(2, 10, 1, 0.01), (100, 5, 1, 0.01), (100, 10, 2, 0.02)])\n    def test_synthidtext_watermark_processor_bias_test(self, vocab_size, ngram_len, num_layers, atol):\n        \"\"\"Test SynthID watermarking bias matches theoretical value.\"\"\"\n        batch_size = 20000\n        generator = torch.Generator(device=torch_device).manual_seed(0)\n        np.random.seed(0)\n\n        keys = [np.random.randint(0, 10**9) for _ in range(num_layers)]\n        # Use 10**9 rather than vocab_size to ensure variety in (n-1)-grams.\n        context = torch.randint(\n            low=0,\n            high=10**9,\n            size=(batch_size, ngram_len - 1),\n            dtype=torch.int64,\n            generator=generator,\n            device=torch_device,\n        )\n\n        context_history_size = 1024\n        logits_processor = SynthIDTextWatermarkLogitsProcessor(\n            ngram_len=ngram_len,\n            keys=keys,\n            sampling_table_size=2**16,\n            sampling_table_seed=0,\n            context_history_size=context_history_size,\n            device=torch_device,\n        )\n\n        scores = torch.ones(\n            (batch_size, vocab_size),\n            dtype=torch.float64,\n            device=torch_device,\n        )\n        # Init state of the logits processor.\n        logits_processor(context, scores)\n        # insert context into the state.\n        for idx in range(1, ngram_len - 1):\n            _ = logits_processor(context[:, :idx], scores)\n\n        updated_scores = logits_processor(context, scores)\n\n        probs = torch.nn.functional.softmax(updated_scores, dim=1)\n        generator = torch.Generator(device=torch_device).manual_seed(0)\n        next_tokens = torch.multinomial(\n            probs,\n            num_samples=1,\n            generator=generator,\n        )\n\n        ngrams = torch.concat((context, next_tokens), dim=1)\n        g_values = logits_processor.compute_g_values(ngrams)\n        mean_g_values = g_values.mean(dtype=torch.float64, dim=(0, 1))\n\n        expected_mean_g_value = logits_processor.expected_mean_g_value(\n            vocab_size=vocab_size,\n        )\n        is_close = torch.all(\n            torch.isclose(\n                mean_g_values,\n                torch.tensor(expected_mean_g_value, dtype=torch.float64, device=torch_device),\n                atol=atol,\n                rtol=0,\n            )\n        )\n        self.assertTrue(is_close)\n"}
{"type": "test_file", "path": "transformers/tests/generation/test_configuration_utils.py", "content": "# coding=utf-8\n# Copyright 2022 The HuggingFace Team Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a clone of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport os\nimport tempfile\nimport unittest\nimport warnings\n\nfrom huggingface_hub import HfFolder, create_pull_request\nfrom parameterized import parameterized\n\nfrom transformers import AutoConfig, GenerationConfig, WatermarkingConfig, is_torch_available\n\n\nif is_torch_available():\n    import torch\n\nfrom transformers.generation import (\n    ClassifierFreeGuidanceLogitsProcessor,\n    EncoderNoRepeatNGramLogitsProcessor,\n    EncoderRepetitionPenaltyLogitsProcessor,\n    EpsilonLogitsWarper,\n    EtaLogitsWarper,\n    ExponentialDecayLengthPenalty,\n    ForcedBOSTokenLogitsProcessor,\n    ForcedEOSTokenLogitsProcessor,\n    GenerationMode,\n    HammingDiversityLogitsProcessor,\n    MinLengthLogitsProcessor,\n    MinNewTokensLengthLogitsProcessor,\n    MinPLogitsWarper,\n    NoBadWordsLogitsProcessor,\n    NoRepeatNGramLogitsProcessor,\n    PrefixConstrainedLogitsProcessor,\n    RepetitionPenaltyLogitsProcessor,\n    SequenceBiasLogitsProcessor,\n    SuppressTokensAtBeginLogitsProcessor,\n    SuppressTokensLogitsProcessor,\n    TemperatureLogitsWarper,\n    TopKLogitsWarper,\n    TopPLogitsWarper,\n    TypicalLogitsWarper,\n    UnbatchedClassifierFreeGuidanceLogitsProcessor,\n    WatermarkLogitsProcessor,\n)\nfrom transformers.testing_utils import TOKEN, TemporaryHubRepo, is_staging_test, torch_device\n\n\nclass GenerationConfigTest(unittest.TestCase):\n    @parameterized.expand([(None,), (\"foo.json\",)])\n    def test_save_load_config(self, config_name):\n        config = GenerationConfig(\n            do_sample=True,\n            temperature=0.7,\n            length_penalty=1.0,\n            bad_words_ids=[[1, 2, 3], [4, 5]],\n        )\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            config.save_pretrained(tmp_dir, config_name=config_name)\n            loaded_config = GenerationConfig.from_pretrained(tmp_dir, config_name=config_name)\n\n        # Checks parameters that were specified\n        self.assertEqual(loaded_config.do_sample, True)\n        self.assertEqual(loaded_config.temperature, 0.7)\n        self.assertEqual(loaded_config.length_penalty, 1.0)\n        self.assertEqual(loaded_config.bad_words_ids, [[1, 2, 3], [4, 5]])\n\n        # Checks parameters that were not specified (defaults)\n        self.assertEqual(loaded_config.top_k, 50)\n        self.assertEqual(loaded_config.max_length, 20)\n        self.assertEqual(loaded_config.max_time, None)\n\n    def test_from_model_config(self):\n        model_config = AutoConfig.from_pretrained(\"openai-community/gpt2\")\n        generation_config_from_model = GenerationConfig.from_model_config(model_config)\n        default_generation_config = GenerationConfig()\n\n        # The generation config has loaded a few non-default parameters from the model config\n        self.assertNotEqual(generation_config_from_model, default_generation_config)\n\n        # One of those parameters is eos_token_id -- check if it matches\n        self.assertNotEqual(generation_config_from_model.eos_token_id, default_generation_config.eos_token_id)\n        self.assertEqual(generation_config_from_model.eos_token_id, model_config.eos_token_id)\n\n    def test_update(self):\n        generation_config = GenerationConfig()\n        update_kwargs = {\n            \"max_new_tokens\": 1024,\n            \"foo\": \"bar\",\n        }\n        update_kwargs_copy = copy.deepcopy(update_kwargs)\n        unused_kwargs = generation_config.update(**update_kwargs)\n\n        # update_kwargs was not modified (no side effects)\n        self.assertEqual(update_kwargs, update_kwargs_copy)\n\n        # update_kwargs was used to update the config on valid attributes\n        self.assertEqual(generation_config.max_new_tokens, 1024)\n\n        # `.update()` returns a dictionary of unused kwargs\n        self.assertEqual(unused_kwargs, {\"foo\": \"bar\"})\n\n    # TODO: @Arthur and/or @Joao\n    # FAILED tests/generation/test_configuration_utils.py::GenerationConfigTest::test_initialize_new_kwargs - AttributeError: 'GenerationConfig' object has no attribute 'get_text_config'\n    # See: https://app.circleci.com/pipelines/github/huggingface/transformers/104831/workflows/e5e61514-51b7-4c8c-bba7-3c4d2986956e/jobs/1394252\n    @unittest.skip(\"failed with `'GenerationConfig' object has no attribute 'get_text_config'`\")\n    def test_initialize_new_kwargs(self):\n        generation_config = GenerationConfig()\n        generation_config.foo = \"bar\"\n\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        # update_kwargs was used to update the config on valid attributes\n        self.assertEqual(new_config.foo, \"bar\")\n\n        generation_config = GenerationConfig.from_model_config(new_config)\n        assert not hasattr(generation_config, \"foo\")  # no new kwargs should be initialized if from config\n\n    def test_kwarg_init(self):\n        \"\"\"Tests that we can overwrite attributes at `from_pretrained` time.\"\"\"\n        default_config = GenerationConfig()\n        self.assertEqual(default_config.temperature, 1.0)\n        self.assertEqual(default_config.do_sample, False)\n        self.assertEqual(default_config.num_beams, 1)\n\n        config = GenerationConfig(\n            do_sample=True,\n            temperature=0.7,\n            length_penalty=1.0,\n            bad_words_ids=[[1, 2, 3], [4, 5]],\n        )\n        self.assertEqual(config.temperature, 0.7)\n        self.assertEqual(config.do_sample, True)\n        self.assertEqual(config.num_beams, 1)\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            config.save_pretrained(tmp_dir)\n            loaded_config = GenerationConfig.from_pretrained(tmp_dir, temperature=1.0)\n\n        self.assertEqual(loaded_config.temperature, 1.0)\n        self.assertEqual(loaded_config.do_sample, True)\n        self.assertEqual(loaded_config.num_beams, 1)  # default value\n\n    def test_validate(self):\n        \"\"\"\n        Tests that the `validate` method is working as expected. Note that `validate` is called at initialization time\n        \"\"\"\n        # A correct configuration will not throw any warning\n        with warnings.catch_warnings(record=True) as captured_warnings:\n            GenerationConfig()\n        self.assertEqual(len(captured_warnings), 0)\n\n        # Inconsequent but technically wrong configuration will throw a warning (e.g. setting sampling\n        # parameters with `do_sample=False`). May be escalated to an error in the future.\n        with warnings.catch_warnings(record=True) as captured_warnings:\n            GenerationConfig(do_sample=False, temperature=0.5)\n        self.assertEqual(len(captured_warnings), 1)\n\n        with warnings.catch_warnings(record=True) as captured_warnings:\n            GenerationConfig(return_dict_in_generate=False, output_scores=True)\n        self.assertEqual(len(captured_warnings), 1)\n\n        # Expanding on the case above, we can update a bad configuration to get rid of the warning. Ideally,\n        # that is done by unsetting the parameter (i.e. setting it to None)\n        generation_config_bad_temperature = GenerationConfig(do_sample=False, temperature=0.5)\n        with warnings.catch_warnings(record=True) as captured_warnings:\n            # BAD - 0.9 means it is still set, we should warn\n            generation_config_bad_temperature.update(temperature=0.9)\n        self.assertEqual(len(captured_warnings), 1)\n        generation_config_bad_temperature = GenerationConfig(do_sample=False, temperature=0.5)\n        with warnings.catch_warnings(record=True) as captured_warnings:\n            # CORNER CASE - 1.0 is the default, we can't detect whether it is set by the user or not, we shouldn't warn\n            generation_config_bad_temperature.update(temperature=1.0)\n        self.assertEqual(len(captured_warnings), 0)\n        generation_config_bad_temperature = GenerationConfig(do_sample=False, temperature=0.5)\n        with warnings.catch_warnings(record=True) as captured_warnings:\n            # OK - None means it is unset, nothing to warn about\n            generation_config_bad_temperature.update(temperature=None)\n        self.assertEqual(len(captured_warnings), 0)\n\n        # Impossible sets of constraints/parameters will raise an exception\n        with self.assertRaises(ValueError):\n            GenerationConfig(do_sample=False, num_beams=1, num_return_sequences=2)\n        with self.assertRaises(ValueError):\n            # dummy constraint\n            GenerationConfig(do_sample=True, num_beams=2, constraints=[\"dummy\"])\n        with self.assertRaises(ValueError):\n            GenerationConfig(do_sample=True, num_beams=2, force_words_ids=[[[1, 2, 3]]])\n\n        # Passing `generate()`-only flags to `validate` will raise an exception\n        with self.assertRaises(ValueError):\n            GenerationConfig(logits_processor=\"foo\")\n\n        # Model-specific parameters will NOT raise an exception or a warning\n        with warnings.catch_warnings(record=True) as captured_warnings:\n            GenerationConfig(foo=\"bar\")\n        self.assertEqual(len(captured_warnings), 0)\n\n    def test_refuse_to_save(self):\n        \"\"\"Tests that we refuse to save a generation config that fails validation.\"\"\"\n\n        # setting the temperature alone is invalid, as we also need to set do_sample to True -> throws a warning that\n        # is caught, doesn't save, and raises an exception\n        config = GenerationConfig()\n        config.temperature = 0.5\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            with self.assertRaises(ValueError) as exc:\n                config.save_pretrained(tmp_dir)\n            self.assertTrue(\"Fix these issues to save the configuration.\" in str(exc.exception))\n            self.assertTrue(len(os.listdir(tmp_dir)) == 0)\n\n        # greedy decoding throws an exception if we try to return multiple sequences -> throws an exception that is\n        # caught, doesn't save, and raises a warning\n        config = GenerationConfig()\n        config.num_return_sequences = 2\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            with self.assertRaises(ValueError) as exc:\n                config.save_pretrained(tmp_dir)\n            self.assertTrue(\"Fix these issues to save the configuration.\" in str(exc.exception))\n            self.assertTrue(len(os.listdir(tmp_dir)) == 0)\n\n        # final check: no warnings/exceptions thrown if it is correct, and file is saved\n        config = GenerationConfig()\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            with warnings.catch_warnings(record=True) as captured_warnings:\n                config.save_pretrained(tmp_dir)\n            self.assertEqual(len(captured_warnings), 0)\n            self.assertTrue(len(os.listdir(tmp_dir)) == 1)\n\n    def test_generation_mode(self):\n        \"\"\"Tests that the `get_generation_mode` method is working as expected.\"\"\"\n        config = GenerationConfig()\n        self.assertEqual(config.get_generation_mode(), GenerationMode.GREEDY_SEARCH)\n\n        config = GenerationConfig(do_sample=True)\n        self.assertEqual(config.get_generation_mode(), GenerationMode.SAMPLE)\n\n        config = GenerationConfig(num_beams=2)\n        self.assertEqual(config.get_generation_mode(), GenerationMode.BEAM_SEARCH)\n\n        config = GenerationConfig(top_k=10, do_sample=False, penalty_alpha=0.6)\n        self.assertEqual(config.get_generation_mode(), GenerationMode.CONTRASTIVE_SEARCH)\n\n        config = GenerationConfig()\n        self.assertEqual(config.get_generation_mode(assistant_model=\"foo\"), GenerationMode.ASSISTED_GENERATION)\n\n    def test_static_cache_without_cache_config(self):\n        \"\"\"Regression test for #35026 -- static cache should work without a cache config.\"\"\"\n        config = GenerationConfig(cache_implementation=\"static\")\n        self.assertEqual(config.cache_implementation, \"static\")\n        self.assertEqual(config.cache_config, None)\n\n\nclass GenerationConfigSerializationTest(unittest.TestCase):\n    def test_serialize_generation_sequence_bias(self):\n        \"\"\"Tests that GenerationConfig is serialized and SequenceBiasLogitsProcessor is initialized with sequence_bias parameter\"\"\"\n        generation_config = GenerationConfig()\n        sequence_bias = [[[45, 67], -0.6], [[89], 1.2]]\n        generation_config.sequence_bias = sequence_bias\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertSequenceEqual(new_config.sequence_bias, sequence_bias)\n\n        expected_sequence_bias = {(45, 67): -0.6, (89,): 1.2}\n        bias_logits_processor = SequenceBiasLogitsProcessor(new_config.sequence_bias)\n        self.assertDictEqual(bias_logits_processor.sequence_bias, expected_sequence_bias)\n\n    def test_serialize_generation_min_length_eos_token(self):\n        \"\"\"Tests that GenerationConfig is serialized and MinLengthLogitsProcessor is initialized with min_length and eos_token_id\"\"\"\n        eos_token_id = 0\n        min_length = 10\n\n        generation_config = GenerationConfig(min_length=min_length, eos_token_id=eos_token_id)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.min_length, min_length)\n        self.assertEqual(new_config.eos_token_id, eos_token_id)\n\n        min_dist_processor = MinLengthLogitsProcessor(\n            min_length=new_config.min_length, eos_token_id=new_config.eos_token_id\n        )\n        self.assertEqual(min_dist_processor.min_length, min_length)\n        self.assertEqual(min_dist_processor.eos_token_id, eos_token_id)\n\n    def test_serialize_generation_min_new_tokens(self):\n        \"\"\"Tests that GenerationConfig is serialized and MinNewTokensLengthLogitsProcessor is initialized with min_new_tokens\"\"\"\n        eos_token_id = 0\n        min_new_tokens = 5\n        prompt_length_to_skip = 2\n\n        generation_config = GenerationConfig(min_new_tokens=min_new_tokens)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.min_new_tokens, min_new_tokens)\n\n        min_new_tokens_processor = MinNewTokensLengthLogitsProcessor(\n            prompt_length_to_skip=prompt_length_to_skip,\n            min_new_tokens=new_config.min_new_tokens,\n            eos_token_id=eos_token_id,\n        )\n        self.assertEqual(min_new_tokens_processor.min_new_tokens, min_new_tokens)\n\n    def test_serialize_generation_temperature(self):\n        \"\"\"Tests that GenerationConfig is serialized and TemperatureLogitsWarper is initialized with temperature\"\"\"\n        temperature = 2.0\n\n        generation_config = GenerationConfig(temperature=temperature, do_sample=True)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.temperature, temperature)\n\n        temperature_logits_warper = TemperatureLogitsWarper(temperature=new_config.temperature)\n        self.assertEqual(temperature_logits_warper.temperature, temperature)\n\n    def test_serialize_generation_repetition_penalty(self):\n        \"\"\"Tests that GenerationConfig is serialized and RepetitionPenaltyLogitsProcessor is initialized with repetition_penalty\"\"\"\n        penalty = 2.0\n\n        generation_config = GenerationConfig(repetition_penalty=penalty)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.repetition_penalty, penalty)\n\n        rep_penalty_proc = RepetitionPenaltyLogitsProcessor(penalty=new_config.repetition_penalty)\n        self.assertEqual(rep_penalty_proc.penalty, penalty)\n\n    def test_serialize_generation_encoder_repetition_penalty(self):\n        \"\"\"Tests that GenerationConfig is serialized and EncoderRepetitionPenaltyLogitsProcessor is initialized with penalty and input_ids\"\"\"\n        penalty = 2.0\n        input_ids = torch.tensor([[0, 1], [5, 0]], device=torch_device, dtype=torch.long)\n\n        generation_config = GenerationConfig(encoder_repetition_penalty=penalty)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.encoder_repetition_penalty, penalty)\n\n        rep_penalty_proc = EncoderRepetitionPenaltyLogitsProcessor(\n            penalty=new_config.encoder_repetition_penalty, encoder_input_ids=input_ids\n        )\n        self.assertEqual(rep_penalty_proc.penalty, 1 / penalty)\n        torch.testing.assert_close(rep_penalty_proc.encoder_input_ids, input_ids)\n\n    def test_serialize_generation_top_p(self):\n        \"\"\"Tests that GenerationConfig is serialized and TopPLogitsWarper is initialized with top_p\"\"\"\n        top_p = 0.8\n\n        generation_config = GenerationConfig(top_p=top_p, do_sample=True)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.top_p, top_p)\n\n        rep_penalty_proc = TopPLogitsWarper(top_p=new_config.top_p)\n        self.assertEqual(rep_penalty_proc.top_p, top_p)\n\n    def test_serialize_generation_top_k(self):\n        \"\"\"Tests that GenerationConfig is serialized and TopKLogitsWarper is initialized with top_k\"\"\"\n        top_k = 2\n\n        generation_config = GenerationConfig(top_k=top_k, do_sample=True)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.top_k, top_k)\n\n        top_k_logits_wrap = TopKLogitsWarper(top_k=new_config.top_k)\n        self.assertEqual(top_k_logits_wrap.top_k, top_k)\n\n    def test_serialize_generation_min_p(self):\n        \"\"\"Tests that GenerationConfig is serialized and MinPLogitsWarper is initialized with min_p\"\"\"\n        min_p = 0.8\n\n        generation_config = GenerationConfig(min_p=min_p, do_sample=True)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.min_p, min_p)\n\n        min_k_logits_wrap = MinPLogitsWarper(min_p=new_config.min_p)\n        self.assertEqual(min_k_logits_wrap.min_p, min_p)\n\n    def test_serialize_generation_typical_p(self):\n        \"\"\"Tests that GenerationConfig is serialized and TypicalLogitsWarper is initialized with mass\"\"\"\n        mass = 0.8\n\n        generation_config = GenerationConfig(typical_p=mass, do_sample=True)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.typical_p, mass)\n\n        typical_p_logits_wrap = TypicalLogitsWarper(mass=new_config.typical_p)\n        self.assertEqual(typical_p_logits_wrap.mass, mass)\n\n    def test_serialize_generation_epsilon_cutoff(self):\n        \"\"\"Tests that GenerationConfig is serialized and EpsilonLogitsWarper is initialized with epsilon\"\"\"\n        epsilon = 0.8\n\n        generation_config = GenerationConfig(epsilon_cutoff=epsilon, do_sample=True)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.epsilon_cutoff, epsilon)\n\n        epsilon_logits_wrap = EpsilonLogitsWarper(epsilon=new_config.epsilon_cutoff)\n        self.assertEqual(epsilon_logits_wrap.epsilon, epsilon)\n\n    def test_serialize_generation_eta_cutoff(self):\n        \"\"\"Tests that GenerationConfig is serialized and EtaLogitsWarper is initialized with epsilon\"\"\"\n        epsilon = 0.8\n\n        generation_config = GenerationConfig(eta_cutoff=epsilon, do_sample=True)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.eta_cutoff, epsilon)\n\n        eta_logits_wrap = EtaLogitsWarper(epsilon=new_config.eta_cutoff)\n        self.assertEqual(eta_logits_wrap.epsilon, epsilon)\n\n    def test_serialize_generation_ngram_size(self):\n        \"\"\"Tests that GenerationConfig is serialized and NoRepeatNGramLogitsProcessor is initialized with ngram_size\"\"\"\n        ngram_size = 2\n\n        generation_config = GenerationConfig(no_repeat_ngram_size=ngram_size, do_sample=True)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.no_repeat_ngram_size, ngram_size)\n\n        no_repeat_ngram_proc = NoRepeatNGramLogitsProcessor(ngram_size=new_config.no_repeat_ngram_size)\n        self.assertEqual(no_repeat_ngram_proc.ngram_size, ngram_size)\n\n    def test_serialize_generation_encoder_ngram_size(self):\n        \"\"\"Tests that GenerationConfig is serialized and EncoderNoRepeatNGramLogitsProcessor is initialized with ngram_size\"\"\"\n        ngram_size = 2\n        input_ids = torch.tensor([[0, 1], [5, 0]], device=torch_device, dtype=torch.long)\n\n        generation_config = GenerationConfig(encoder_no_repeat_ngram_size=ngram_size, do_sample=True)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.encoder_no_repeat_ngram_size, ngram_size)\n\n        encoder_no_repeat_ngram_proc = EncoderNoRepeatNGramLogitsProcessor(\n            encoder_ngram_size=new_config.encoder_no_repeat_ngram_size, encoder_input_ids=input_ids\n        )\n        self.assertEqual(encoder_no_repeat_ngram_proc.ngram_size, ngram_size)\n\n    def test_serialize_generation_bad_words_ids(self):\n        \"\"\"Tests that GenerationConfig is serialized and NoBadWordsLogitsProcessor is initialized with bad_words_ids\"\"\"\n        bad_word_tokens = [[1], [4], [1, 0], [0, 1, 2], [1, 3, 1, 3]]\n\n        generation_config = GenerationConfig(bad_words_ids=bad_word_tokens)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertSequenceEqual(new_config.bad_words_ids, bad_word_tokens)\n\n        no_bad_words_dist_proc = NoBadWordsLogitsProcessor(bad_words_ids=new_config.bad_words_ids)\n        self.assertSequenceEqual(no_bad_words_dist_proc.bad_word_ids, bad_word_tokens)\n\n    def test_serialize_generation_num_beams(self):\n        \"\"\"Tests that GenerationConfig is serialized and PrefixConstrainedLogitsProcessor is initialized with num_beams\"\"\"\n        num_beams = 1\n\n        def prefix_allowed_tokens_fn(batch_id, inputs_ids):\n            return [[0, 1], [2, 3]][batch_id]\n\n        generation_config = GenerationConfig(num_beams=num_beams)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.num_beams, num_beams)\n\n        prefix_constrained_logits_proc = PrefixConstrainedLogitsProcessor(\n            prefix_allowed_tokens_fn, num_beams=new_config.num_beams\n        )\n        self.assertEqual(prefix_constrained_logits_proc._num_beams, num_beams)\n\n    def test_serialize_generation_diversity_penalty_and_num_bean_groups(self):\n        \"\"\"Tests that GenerationConfig is serialized and HammingDiversityLogitsProcessor is initialized with diversity_penalty_and_num_bean_groups\"\"\"\n        num_beams = 2\n        num_beam_groups = 2\n        diversity_penalty = 1.0\n\n        generation_config = GenerationConfig(\n            num_beams=num_beams, diversity_penalty=diversity_penalty, num_beam_groups=num_beam_groups\n        )\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.num_beams, num_beams)\n        self.assertEqual(new_config.diversity_penalty, diversity_penalty)\n        self.assertEqual(new_config.num_beam_groups, num_beam_groups)\n\n        diversity_logits_processor = HammingDiversityLogitsProcessor(\n            diversity_penalty=new_config.diversity_penalty,\n            num_beams=new_config.num_beams,\n            num_beam_groups=new_config.num_beam_groups,\n        )\n        self.assertEqual(diversity_logits_processor._num_beams, num_beams)\n        self.assertEqual(diversity_logits_processor._diversity_penalty, diversity_penalty)\n        self.assertEqual(diversity_logits_processor._num_sub_beams, num_beams // num_beam_groups)\n\n    def test_serialize_generation_bos_token_id(self):\n        \"\"\"Tests that GenerationConfig is serialized and ForcedBOSTokenLogitsProcessor is initialized with bos_token_id\"\"\"\n        bos_token_id = 0\n\n        generation_config = GenerationConfig(bos_token_id=bos_token_id)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.bos_token_id, bos_token_id)\n\n        logits_processor = ForcedBOSTokenLogitsProcessor(bos_token_id=new_config.bos_token_id)\n        self.assertEqual(logits_processor.bos_token_id, bos_token_id)\n\n    def test_serialize_generation_eos_token_id(self):\n        \"\"\"Tests that GenerationConfig is serialized and ForcedEOSTokenLogitsProcessor is initialized with eos_token_id\"\"\"\n        eos_token_id = 0\n        max_length = 5\n\n        generation_config = GenerationConfig(eos_token_id=eos_token_id)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.eos_token_id, eos_token_id)\n\n        logits_processor = ForcedEOSTokenLogitsProcessor(\n            max_length=max_length, eos_token_id=new_config.eos_token_id, device=torch_device\n        )\n        self.assertEqual(logits_processor.eos_token_id, eos_token_id)\n\n    def test_serialize_generation_exponential_decay_length_penalty(self):\n        \"\"\"Tests that GenerationConfig is serialized and ExponentialDecayLengthPenalty is initialized with regulation_start and regulation_factor\"\"\"\n        eos_token_id = 0\n        penalty_start = 5\n        penalty_factor = 1.1\n        input_ids_seq_length = 10\n        exponential_decay_length_penalty = (penalty_start, penalty_factor)\n\n        generation_config = GenerationConfig(exponential_decay_length_penalty=exponential_decay_length_penalty)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.exponential_decay_length_penalty, [penalty_start, penalty_factor])\n\n        exponential_decay_processor = ExponentialDecayLengthPenalty(\n            exponential_decay_length_penalty=new_config.exponential_decay_length_penalty,\n            eos_token_id=eos_token_id,\n            input_ids_seq_length=input_ids_seq_length,\n        )\n        self.assertEqual(\n            exponential_decay_processor.regulation_start, exponential_decay_length_penalty[0] + input_ids_seq_length\n        )\n        self.assertEqual(exponential_decay_processor.regulation_factor, exponential_decay_length_penalty[1])\n\n    def test_serialize_generation_begin_suppress_tokens(self):\n        \"\"\"Tests that GenerationConfig is serialized and SuppressTokensAtBeginLogitsProcessor is initialized with begin_suppress_token and begin_index\"\"\"\n\n        begin_suppress_tokens = [220, 50256]\n        begin_index = 0\n        generation_config = GenerationConfig(begin_suppress_tokens=begin_suppress_tokens)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertSequenceEqual(new_config.begin_suppress_tokens, begin_suppress_tokens)\n\n        suppress_processor = SuppressTokensAtBeginLogitsProcessor(\n            begin_suppress_tokens=new_config.begin_suppress_tokens, begin_index=begin_index\n        )\n        self.assertSequenceEqual(suppress_processor.begin_suppress_tokens, begin_suppress_tokens)\n        self.assertEqual(suppress_processor.begin_index, begin_index)\n\n    def test_serialize_generation_suppress_tokens(self):\n        \"\"\"Tests that GenerationConfig is serialized and SuppressTokensLogitsProcessor is initialized with suppress_token\"\"\"\n        suppress_tokens = [220, 50256]\n\n        generation_config = GenerationConfig(suppress_tokens=suppress_tokens)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertSequenceEqual(new_config.suppress_tokens, suppress_tokens)\n\n        suppress_processor = SuppressTokensLogitsProcessor(suppress_tokens=new_config.suppress_tokens)\n        self.assertSequenceEqual(suppress_processor.suppress_tokens, suppress_tokens)\n\n    def test_serialize_generation_guidance_scale(self):\n        \"\"\"Tests that GenerationConfig is serialized and ClassifierFreeGuidanceLogitsProcessor is initialized with guidance_scale\"\"\"\n        guidance_scale = 2.0\n        generation_config = GenerationConfig(guidance_scale=guidance_scale)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.guidance_scale, guidance_scale)\n\n        classifier_processor = ClassifierFreeGuidanceLogitsProcessor(guidance_scale=new_config.guidance_scale)\n        self.assertEqual(classifier_processor.guidance_scale, guidance_scale)\n\n    def test_serialize_generation_guidance_scale_unbatched(self):\n        \"\"\"Tests that GenerationConfig is serialized and UnbatchedClassifierFreeGuidanceLogitsProcessor is initialized with guidance_scale\"\"\"\n        guidance_scale = 2.0\n\n        input_ids = torch.LongTensor([[0]])\n\n        generation_config = GenerationConfig(guidance_scale=guidance_scale)\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.guidance_scale, guidance_scale)\n\n        cfg = UnbatchedClassifierFreeGuidanceLogitsProcessor(new_config.guidance_scale, {}, input_ids)\n        self.assertEqual(cfg.guidance_scale, guidance_scale)\n\n    def test_serialize_generation_watermarking_config(self):\n        \"\"\"Tests that GenerationConfig is serialized and WatermarkLogitsProcessor is initialized with WatermarkingConfig parameters\"\"\"\n\n        vocab_size = 20\n        bias = 2.0\n        greenlist_ratio = 0.5\n        hashing_key = 10\n        seeding_scheme = \"lefthash\"\n        context_width = 10\n        watermarking_config = WatermarkingConfig(\n            bias=bias,\n            greenlist_ratio=greenlist_ratio,\n            hashing_key=hashing_key,\n            seeding_scheme=seeding_scheme,\n            context_width=context_width,\n        )\n        generation_config = GenerationConfig(watermarking_config=watermarking_config)\n\n        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n            generation_config.save_pretrained(tmp_dir)\n            new_config = GenerationConfig.from_pretrained(tmp_dir)\n        self.assertEqual(new_config.watermarking_config.bias, bias)\n        self.assertEqual(new_config.watermarking_config.greenlist_ratio, greenlist_ratio)\n        self.assertEqual(new_config.watermarking_config.hashing_key, hashing_key)\n        self.assertEqual(new_config.watermarking_config.seeding_scheme, seeding_scheme)\n        self.assertEqual(new_config.watermarking_config.context_width, context_width)\n\n        watermark = WatermarkLogitsProcessor(\n            vocab_size=vocab_size,\n            device=torch_device,\n            greenlist_ratio=new_config.watermarking_config.greenlist_ratio,\n            bias=new_config.watermarking_config.bias,\n            hashing_key=new_config.watermarking_config.hashing_key,\n            seeding_scheme=new_config.watermarking_config.seeding_scheme,\n            context_width=new_config.watermarking_config.context_width,\n        )\n        self.assertEqual(watermark.bias, bias)\n        self.assertEqual(watermark.greenlist_size, int(vocab_size * greenlist_ratio))\n        self.assertEqual(watermark.hash_key, hashing_key)\n        self.assertEqual(watermark.seeding_scheme, seeding_scheme)\n        self.assertEqual(watermark.context_width, context_width)\n\n\n@is_staging_test\nclass ConfigPushToHubTester(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls._token = TOKEN\n        HfFolder.save_token(TOKEN)\n\n    def test_push_to_hub(self):\n        with TemporaryHubRepo(token=self._token) as tmp_repo:\n            config = GenerationConfig(\n                do_sample=True,\n                temperature=0.7,\n                length_penalty=1.0,\n            )\n            config.push_to_hub(tmp_repo.repo_id, token=self._token)\n\n            new_config = GenerationConfig.from_pretrained(tmp_repo.repo_id)\n            for k, v in config.to_dict().items():\n                if k != \"transformers_version\":\n                    self.assertEqual(v, getattr(new_config, k))\n\n    def test_push_to_hub_via_save_pretrained(self):\n        with TemporaryHubRepo(token=self._token) as tmp_repo:\n            config = GenerationConfig(\n                do_sample=True,\n                temperature=0.7,\n                length_penalty=1.0,\n            )\n            # Push to hub via save_pretrained\n            with tempfile.TemporaryDirectory() as tmp_dir:\n                config.save_pretrained(tmp_dir, repo_id=tmp_repo.repo_id, push_to_hub=True, token=self._token)\n\n            new_config = GenerationConfig.from_pretrained(tmp_repo.repo_id)\n            for k, v in config.to_dict().items():\n                if k != \"transformers_version\":\n                    self.assertEqual(v, getattr(new_config, k))\n\n    def test_push_to_hub_in_organization(self):\n        with TemporaryHubRepo(namespace=\"valid_org\", token=self._token) as tmp_repo:\n            config = GenerationConfig(\n                do_sample=True,\n                temperature=0.7,\n                length_penalty=1.0,\n            )\n            config.push_to_hub(tmp_repo.repo_id, token=self._token)\n\n            new_config = GenerationConfig.from_pretrained(tmp_repo.repo_id)\n            for k, v in config.to_dict().items():\n                if k != \"transformers_version\":\n                    self.assertEqual(v, getattr(new_config, k))\n\n    def test_push_to_hub_in_organization_via_save_pretrained(self):\n        with TemporaryHubRepo(namespace=\"valid_org\", token=self._token) as tmp_repo:\n            config = GenerationConfig(\n                do_sample=True,\n                temperature=0.7,\n                length_penalty=1.0,\n            )\n            # Push to hub via save_pretrained\n            with tempfile.TemporaryDirectory() as tmp_dir:\n                config.save_pretrained(tmp_dir, repo_id=tmp_repo.repo_id, push_to_hub=True, token=self._token)\n\n            new_config = GenerationConfig.from_pretrained(tmp_repo.repo_id)\n            for k, v in config.to_dict().items():\n                if k != \"transformers_version\":\n                    self.assertEqual(v, getattr(new_config, k))\n\n    def test_push_to_hub_on_pr_revision(self):\n        with TemporaryHubRepo(token=self._token) as tmp_repo:\n            # create a PR\n            pr = create_pull_request(repo_id=tmp_repo.repo_id, title=\"Test PR\", token=self._token)\n            revision = f\"refs/pr/{pr.num}\"\n\n            # push to PR ref\n            config = GenerationConfig(\n                do_sample=True,\n                temperature=0.7,\n                length_penalty=1.0,\n            )\n            config.push_to_hub(tmp_repo.repo_id, token=self._token, revision=revision)\n\n            # load from PR ref\n            new_config = GenerationConfig.from_pretrained(tmp_repo.repo_id, revision=revision)\n            for k, v in config.to_dict().items():\n                if k != \"transformers_version\":\n                    self.assertEqual(v, getattr(new_config, k))\n"}
{"type": "test_file", "path": "transformers/tests/generation/test_beam_constraints.py", "content": "# coding=utf-8\n# Copyright 2020 The HuggingFace Team Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a clone of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport unittest\n\nfrom transformers import is_torch_available\nfrom transformers.testing_utils import require_torch\n\n\nif is_torch_available():\n    import torch\n\n    from transformers.generation import DisjunctiveConstraint\n\n\n@require_torch\nclass ConstraintTest(unittest.TestCase):\n    def test_input_types(self):\n        # For consistency across different places the DisjunctiveConstraint is called,\n        # dc.token_ids is a list of integers. It is also initialized only by integers.\n\n        cset = [[1, 2, 4], [1, 2, 3, 4]]\n        dc = DisjunctiveConstraint(cset)\n        self.assertTrue(isinstance(dc.token_ids, list))\n\n        with self.assertRaises(ValueError):\n            DisjunctiveConstraint(torch.LongTensor([[1, 2, 4], [1, 2, 3]]))\n\n        with self.assertRaises(ValueError):\n            DisjunctiveConstraint([torch.LongTensor([1, 2, 4]), torch.LongTensor([1, 2, 3, 4, 5])])\n\n    def test_check_illegal_input(self):\n        # We can't have constraints that are complete subsets of another. This leads to a perverse\n        # interpretation of \"constraint fulfillment\": does generating [1,2,3] fulfill the constraint?\n        # It would mean that it generated [1,2] which fulfills it, but it's in the middle of potentially\n        # fulfilling [1,2,3,4]. If we believe that [1,2,3] does fulfill the constraint, then the algorithm\n        # will necessarily never reach [1,2,3,4], giving users a false sense of control (better to just not allow it).\n        cset = [[1, 2], [1, 2, 3, 4]]\n\n        with self.assertRaises(ValueError):\n            DisjunctiveConstraint(cset)  # fails here\n\n    def test_example_progression(self):\n        cset = [[1, 2, 3], [1, 2, 4]]\n\n        dc = DisjunctiveConstraint(cset)\n\n        stepped, completed, reset = dc.update(1)\n        desired = stepped is True and completed is False and reset is False\n        self.assertTrue(desired)\n        self.assertTrue(not dc.completed)\n        self.assertTrue(dc.current_seq == [1])\n\n        stepped, completed, reset = dc.update(2)\n        desired = stepped is True and completed is False and reset is False\n        self.assertTrue(desired)\n        self.assertTrue(not dc.completed)\n        self.assertTrue(dc.current_seq == [1, 2])\n\n        stepped, completed, reset = dc.update(3)\n        desired = stepped is True and completed is True and reset is False\n        self.assertTrue(desired)\n        self.assertTrue(dc.completed)  # Completed!\n        self.assertTrue(dc.current_seq == [1, 2, 3])\n\n    def test_example_progression_unequal_three_mid_and_reset(self):\n        cset = [[1, 2, 3], [1, 2, 4, 5], [1, 2, 5]]\n\n        dc = DisjunctiveConstraint(cset)\n\n        stepped, completed, reset = dc.update(1)\n        self.assertTrue(not dc.completed)\n        self.assertTrue(dc.current_seq == [1])\n\n        stepped, completed, reset = dc.update(2)\n        self.assertTrue(not dc.completed)\n        self.assertTrue(dc.current_seq == [1, 2])\n\n        stepped, completed, reset = dc.update(4)\n        self.assertTrue(not dc.completed)\n        self.assertTrue(dc.current_seq == [1, 2, 4])\n\n        stepped, completed, reset = dc.update(5)\n        self.assertTrue(dc.completed)  # Completed!\n        self.assertTrue(dc.current_seq == [1, 2, 4, 5])\n\n        dc.reset()\n\n        stepped, completed, reset = dc.update(1)\n        self.assertTrue(not dc.completed)\n        self.assertTrue(dc.remaining() == 3)\n        self.assertTrue(dc.current_seq == [1])\n\n        stepped, completed, reset = dc.update(2)\n        self.assertTrue(not dc.completed)\n        self.assertTrue(dc.remaining() == 2)\n        self.assertTrue(dc.current_seq == [1, 2])\n\n        stepped, completed, reset = dc.update(5)\n        self.assertTrue(dc.completed)  # Completed!\n        self.assertTrue(dc.remaining() == 0)\n        self.assertTrue(dc.current_seq == [1, 2, 5])\n"}
{"type": "test_file", "path": "transformers/tests/models/albert/__init__.py", "content": ""}
{"type": "test_file", "path": "transformers/tests/models/__init__.py", "content": ""}
{"type": "test_file", "path": "transformers/tests/models/align/test_modeling_align.py", "content": "# coding=utf-8\n# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Testing suite for the PyTorch ALIGN model.\"\"\"\n\nimport inspect\nimport os\nimport tempfile\nimport unittest\n\nimport requests\n\nfrom transformers import AlignConfig, AlignProcessor, AlignTextConfig, AlignVisionConfig\nfrom transformers.testing_utils import (\n    require_torch,\n    require_vision,\n    slow,\n    torch_device,\n)\nfrom transformers.utils import is_torch_available, is_vision_available\n\nfrom ...test_configuration_common import ConfigTester\nfrom ...test_modeling_common import (\n    ModelTesterMixin,\n    _config_zero_init,\n    floats_tensor,\n    ids_tensor,\n    random_attention_mask,\n)\nfrom ...test_pipeline_mixin import PipelineTesterMixin\n\n\nif is_torch_available():\n    import torch\n\n    from transformers import (\n        AlignModel,\n        AlignTextModel,\n        AlignVisionModel,\n    )\n\n\nif is_vision_available():\n    from PIL import Image\n\n\nclass AlignVisionModelTester:\n    def __init__(\n        self,\n        parent,\n        batch_size=12,\n        image_size=32,\n        num_channels=3,\n        kernel_sizes=[3, 3, 5],\n        in_channels=[32, 16, 24],\n        out_channels=[16, 24, 30],\n        hidden_dim=64,\n        strides=[1, 1, 2],\n        num_block_repeats=[1, 1, 2],\n        expand_ratios=[1, 6, 6],\n        is_training=True,\n        hidden_act=\"gelu\",\n    ):\n        self.parent = parent\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.num_channels = num_channels\n        self.kernel_sizes = kernel_sizes\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.hidden_dim = hidden_dim\n        self.strides = strides\n        self.num_block_repeats = num_block_repeats\n        self.expand_ratios = expand_ratios\n        self.is_training = is_training\n        self.hidden_act = hidden_act\n\n    def prepare_config_and_inputs(self):\n        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n        config = self.get_config()\n\n        return config, pixel_values\n\n    def get_config(self):\n        return AlignVisionConfig(\n            num_channels=self.num_channels,\n            kernel_sizes=self.kernel_sizes,\n            in_channels=self.in_channels,\n            out_channels=self.out_channels,\n            hidden_dim=self.hidden_dim,\n            strides=self.strides,\n            num_block_repeats=self.num_block_repeats,\n            expand_ratios=self.expand_ratios,\n            hidden_act=self.hidden_act,\n        )\n\n    def create_and_check_model(self, config, pixel_values):\n        model = AlignVisionModel(config=config)\n        model.to(torch_device)\n        model.eval()\n        with torch.no_grad():\n            result = model(pixel_values)\n\n        patch_size = self.image_size // 4\n        self.parent.assertEqual(\n            result.last_hidden_state.shape, (self.batch_size, config.hidden_dim, patch_size, patch_size)\n        )\n        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, config.hidden_dim))\n\n    def prepare_config_and_inputs_for_common(self):\n        config_and_inputs = self.prepare_config_and_inputs()\n        config, pixel_values = config_and_inputs\n        inputs_dict = {\"pixel_values\": pixel_values}\n        return config, inputs_dict\n\n\n@require_torch\nclass AlignVisionModelTest(ModelTesterMixin, unittest.TestCase):\n    \"\"\"\n    Here we also overwrite some of the tests of test_modeling_common.py, as ALIGN does not use input_ids, inputs_embeds,\n    attention_mask and seq_length.\n    \"\"\"\n\n    all_model_classes = (AlignVisionModel,) if is_torch_available() else ()\n    fx_compatible = False\n    test_pruning = False\n    test_resize_embeddings = False\n    test_head_masking = False\n    has_attentions = False\n\n    def setUp(self):\n        self.model_tester = AlignVisionModelTester(self)\n        self.config_tester = ConfigTester(\n            self,\n            config_class=AlignVisionConfig,\n            has_text_modality=False,\n            hidden_size=37,\n            common_properties=[\"num_channels\", \"image_size\"],\n        )\n\n    def test_config(self):\n        self.config_tester.run_common_tests()\n\n    @unittest.skip(reason=\"AlignVisionModel does not use inputs_embeds\")\n    def test_inputs_embeds(self):\n        pass\n\n    @unittest.skip(reason=\"AlignVisionModel does not use inputs_embeds\")\n    def test_inputs_embeds_matches_input_ids(self):\n        pass\n\n    @unittest.skip(reason=\"AlignVisionModel does not support input and output embeddings\")\n    def test_model_get_set_embeddings(self):\n        pass\n\n    def test_forward_signature(self):\n        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n\n        for model_class in self.all_model_classes:\n            model = model_class(config)\n            signature = inspect.signature(model.forward)\n            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n            arg_names = [*signature.parameters.keys()]\n\n            expected_arg_names = [\"pixel_values\"]\n            self.assertListEqual(arg_names[:1], expected_arg_names)\n\n    def test_model(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_model(*config_and_inputs)\n\n    def test_hidden_states_output(self):\n        def check_hidden_states_output(inputs_dict, config, model_class):\n            model = model_class(config)\n            model.to(torch_device)\n            model.eval()\n\n            with torch.no_grad():\n                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n\n            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n            num_blocks = sum(config.num_block_repeats) * 4\n            self.assertEqual(len(hidden_states), num_blocks)\n\n            self.assertListEqual(\n                list(hidden_states[0].shape[-2:]),\n                [self.model_tester.image_size // 2, self.model_tester.image_size // 2],\n            )\n\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n\n        for model_class in self.all_model_classes:\n            inputs_dict[\"output_hidden_states\"] = True\n            check_hidden_states_output(inputs_dict, config, model_class)\n\n            # check that output_hidden_states also work using config\n            del inputs_dict[\"output_hidden_states\"]\n            config.output_hidden_states = True\n\n            check_hidden_states_output(inputs_dict, config, model_class)\n\n    @unittest.skip\n    def test_training(self):\n        pass\n\n    @unittest.skip\n    def test_training_gradient_checkpointing(self):\n        pass\n\n    @unittest.skip(\n        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n    )\n    def test_training_gradient_checkpointing_use_reentrant(self):\n        pass\n\n    @unittest.skip(\n        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n    )\n    def test_training_gradient_checkpointing_use_reentrant_false(self):\n        pass\n\n    @slow\n    def test_model_from_pretrained(self):\n        model_name = \"kakaobrain/align-base\"\n        model = AlignVisionModel.from_pretrained(model_name)\n        self.assertIsNotNone(model)\n\n\nclass AlignTextModelTester:\n    def __init__(\n        self,\n        parent,\n        batch_size=12,\n        seq_length=7,\n        is_training=True,\n        use_input_mask=True,\n        use_token_type_ids=True,\n        vocab_size=99,\n        hidden_size=32,\n        num_hidden_layers=2,\n        num_attention_heads=4,\n        intermediate_size=37,\n        hidden_act=\"gelu\",\n        hidden_dropout_prob=0.1,\n        attention_probs_dropout_prob=0.1,\n        max_position_embeddings=512,\n        type_vocab_size=16,\n        type_sequence_label_size=2,\n        initializer_range=0.02,\n        scope=None,\n    ):\n        self.parent = parent\n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.is_training = is_training\n        self.use_input_mask = use_input_mask\n        self.use_token_type_ids = use_token_type_ids\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.max_position_embeddings = max_position_embeddings\n        self.type_vocab_size = type_vocab_size\n        self.type_sequence_label_size = type_sequence_label_size\n        self.initializer_range = initializer_range\n        self.scope = scope\n\n    def prepare_config_and_inputs(self):\n        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n\n        input_mask = None\n        if self.use_input_mask:\n            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n\n        token_type_ids = None\n        if self.use_token_type_ids:\n            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n\n        config = self.get_config()\n\n        return config, input_ids, token_type_ids, input_mask\n\n    def get_config(self):\n        return AlignTextConfig(\n            vocab_size=self.vocab_size,\n            hidden_size=self.hidden_size,\n            num_hidden_layers=self.num_hidden_layers,\n            num_attention_heads=self.num_attention_heads,\n            intermediate_size=self.intermediate_size,\n            hidden_act=self.hidden_act,\n            hidden_dropout_prob=self.hidden_dropout_prob,\n            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n            max_position_embeddings=self.max_position_embeddings,\n            type_vocab_size=self.type_vocab_size,\n            is_decoder=False,\n            initializer_range=self.initializer_range,\n        )\n\n    def create_and_check_model(self, config, input_ids, token_type_ids, input_mask):\n        model = AlignTextModel(config=config)\n        model.to(torch_device)\n        model.eval()\n        with torch.no_grad():\n            result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids)\n            result = model(input_ids, token_type_ids=token_type_ids)\n            result = model(input_ids)\n        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n\n    def prepare_config_and_inputs_for_common(self):\n        config_and_inputs = self.prepare_config_and_inputs()\n        (\n            config,\n            input_ids,\n            token_type_ids,\n            input_mask,\n        ) = config_and_inputs\n        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": input_mask}\n        return config, inputs_dict\n\n\n@require_torch\nclass AlignTextModelTest(ModelTesterMixin, unittest.TestCase):\n    all_model_classes = (AlignTextModel,) if is_torch_available() else ()\n    fx_compatible = False\n    test_pruning = False\n    test_head_masking = False\n\n    def setUp(self):\n        self.model_tester = AlignTextModelTester(self)\n        self.config_tester = ConfigTester(self, config_class=AlignTextConfig, hidden_size=37)\n\n    def test_config(self):\n        self.config_tester.run_common_tests()\n\n    def test_model(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_model(*config_and_inputs)\n\n    @unittest.skip\n    def test_training(self):\n        pass\n\n    @unittest.skip\n    def test_training_gradient_checkpointing(self):\n        pass\n\n    @unittest.skip(\n        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n    )\n    def test_training_gradient_checkpointing_use_reentrant(self):\n        pass\n\n    @unittest.skip(\n        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n    )\n    def test_training_gradient_checkpointing_use_reentrant_false(self):\n        pass\n\n    @unittest.skip(reason=\"ALIGN does not use inputs_embeds\")\n    def test_inputs_embeds(self):\n        pass\n\n    @unittest.skip(reason=\"Align does not use inputs_embeds\")\n    def test_inputs_embeds_matches_input_ids(self):\n        pass\n\n    @unittest.skip(reason=\"AlignTextModel has no base class and is not available in MODEL_MAPPING\")\n    def test_save_load_fast_init_from_base(self):\n        pass\n\n    @unittest.skip(reason=\"AlignTextModel has no base class and is not available in MODEL_MAPPING\")\n    def test_save_load_fast_init_to_base(self):\n        pass\n\n    @slow\n    def test_model_from_pretrained(self):\n        model_name = \"kakaobrain/align-base\"\n        model = AlignTextModel.from_pretrained(model_name)\n        self.assertIsNotNone(model)\n\n\nclass AlignModelTester:\n    def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):\n        if text_kwargs is None:\n            text_kwargs = {}\n        if vision_kwargs is None:\n            vision_kwargs = {}\n\n        self.parent = parent\n        self.text_model_tester = AlignTextModelTester(parent, **text_kwargs)\n        self.vision_model_tester = AlignVisionModelTester(parent, **vision_kwargs)\n        self.batch_size = self.text_model_tester.batch_size  # need bs for batching_equivalence test\n        self.is_training = is_training\n\n    def prepare_config_and_inputs(self):\n        test_config, input_ids, token_type_ids, input_mask = self.text_model_tester.prepare_config_and_inputs()\n        vision_config, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n\n        config = self.get_config()\n\n        return config, input_ids, token_type_ids, input_mask, pixel_values\n\n    def get_config(self):\n        return AlignConfig.from_text_vision_configs(\n            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n        )\n\n    def create_and_check_model(self, config, input_ids, token_type_ids, attention_mask, pixel_values):\n        model = AlignModel(config).to(torch_device).eval()\n        with torch.no_grad():\n            result = model(input_ids, pixel_values, attention_mask, token_type_ids)\n        self.parent.assertEqual(\n            result.logits_per_image.shape, (self.vision_model_tester.batch_size, self.text_model_tester.batch_size)\n        )\n        self.parent.assertEqual(\n            result.logits_per_text.shape, (self.text_model_tester.batch_size, self.vision_model_tester.batch_size)\n        )\n\n    def prepare_config_and_inputs_for_common(self):\n        config_and_inputs = self.prepare_config_and_inputs()\n        config, input_ids, token_type_ids, input_mask, pixel_values = config_and_inputs\n        inputs_dict = {\n            \"input_ids\": input_ids,\n            \"token_type_ids\": token_type_ids,\n            \"attention_mask\": input_mask,\n            \"pixel_values\": pixel_values,\n            \"return_loss\": True,\n        }\n        return config, inputs_dict\n\n\n@require_torch\nclass AlignModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n    all_model_classes = (AlignModel,) if is_torch_available() else ()\n    pipeline_model_mapping = {\"feature-extraction\": AlignModel} if is_torch_available() else {}\n    fx_compatible = False\n    test_head_masking = False\n    test_pruning = False\n    test_resize_embeddings = False\n    test_attention_outputs = False\n\n    def setUp(self):\n        self.model_tester = AlignModelTester(self)\n        self.config_tester = ConfigTester(\n            self,\n            config_class=AlignConfig,\n            has_text_modality=False,\n            common_properties=[\"projection_dim\", \"temperature_init_value\"],\n        )\n\n    def test_model(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_model(*config_and_inputs)\n\n    def test_config(self):\n        self.config_tester.run_common_tests()\n\n    @unittest.skip(reason=\"Start to fail after using torch `cu118`.\")\n    def test_multi_gpu_data_parallel_forward(self):\n        super().test_multi_gpu_data_parallel_forward()\n\n    @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n    def test_hidden_states_output(self):\n        pass\n\n    @unittest.skip(reason=\"Inputs_embeds is tested in individual model tests\")\n    def test_inputs_embeds(self):\n        pass\n\n    @unittest.skip(reason=\"Align does not use inputs_embeds\")\n    def test_inputs_embeds_matches_input_ids(self):\n        pass\n\n    @unittest.skip(reason=\"Retain_grad is tested in individual model tests\")\n    def test_retain_grad_hidden_states_attentions(self):\n        pass\n\n    @unittest.skip(reason=\"AlignModel does not have input/output embeddings\")\n    def test_model_get_set_embeddings(self):\n        pass\n\n    # override as the `temperature` parameter initialization is different for ALIGN\n    def test_initialization(self):\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n\n        configs_no_init = _config_zero_init(config)\n        for model_class in self.all_model_classes:\n            model = model_class(config=configs_no_init)\n            for name, param in model.named_parameters():\n                if param.requires_grad:\n                    # check if `temperature` is initialized as per the original implementation\n                    if name == \"temperature\":\n                        self.assertAlmostEqual(\n                            param.data.item(),\n                            1.0,\n                            delta=1e-3,\n                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                        )\n                    elif name == \"text_projection.weight\":\n                        self.assertTrue(\n                            -1.0 <= ((param.data.mean() * 1e9).round() / 1e9).item() <= 1.0,\n                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                        )\n                    else:\n                        self.assertIn(\n                            ((param.data.mean() * 1e9).round() / 1e9).item(),\n                            [0.0, 1.0],\n                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                        )\n\n    def _create_and_check_torchscript(self, config, inputs_dict):\n        if not self.test_torchscript:\n            self.skipTest(reason=\"test_torchscript is set to False\")\n\n        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n        configs_no_init.torchscript = True\n        configs_no_init.return_dict = False\n        for model_class in self.all_model_classes:\n            model = model_class(config=configs_no_init)\n            model.to(torch_device)\n            model.eval()\n\n            try:\n                input_ids = inputs_dict[\"input_ids\"]\n                pixel_values = inputs_dict[\"pixel_values\"]  # ALIGN needs pixel_values\n                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n            except RuntimeError:\n                self.fail(\"Couldn't trace module.\")\n\n            with tempfile.TemporaryDirectory() as tmp_dir_name:\n                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n\n                try:\n                    torch.jit.save(traced_model, pt_file_name)\n                except Exception:\n                    self.fail(\"Couldn't save module.\")\n\n                try:\n                    loaded_model = torch.jit.load(pt_file_name)\n                except Exception:\n                    self.fail(\"Couldn't load module.\")\n\n            model.to(torch_device)\n            model.eval()\n\n            loaded_model.to(torch_device)\n            loaded_model.eval()\n\n            model_state_dict = model.state_dict()\n            loaded_model_state_dict = loaded_model.state_dict()\n\n            non_persistent_buffers = {}\n            for key in loaded_model_state_dict.keys():\n                if key not in model_state_dict.keys():\n                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n\n            loaded_model_state_dict = {\n                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n            }\n\n            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n\n            model_buffers = list(model.buffers())\n            for non_persistent_buffer in non_persistent_buffers.values():\n                found_buffer = False\n                for i, model_buffer in enumerate(model_buffers):\n                    if torch.equal(non_persistent_buffer, model_buffer):\n                        found_buffer = True\n                        break\n\n                self.assertTrue(found_buffer)\n                model_buffers.pop(i)\n\n            models_equal = True\n            for layer_name, p1 in model_state_dict.items():\n                p2 = loaded_model_state_dict[layer_name]\n                if p1.data.ne(p2.data).sum() > 0:\n                    models_equal = False\n\n            self.assertTrue(models_equal)\n\n    def test_load_vision_text_config(self):\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n\n        # Save AlignConfig and check if we can load AlignVisionConfig from it\n        with tempfile.TemporaryDirectory() as tmp_dir_name:\n            config.save_pretrained(tmp_dir_name)\n            vision_config = AlignVisionConfig.from_pretrained(tmp_dir_name)\n            self.assertDictEqual(config.vision_config.to_dict(), vision_config.to_dict())\n\n        # Save AlignConfig and check if we can load AlignTextConfig from it\n        with tempfile.TemporaryDirectory() as tmp_dir_name:\n            config.save_pretrained(tmp_dir_name)\n            text_config = AlignTextConfig.from_pretrained(tmp_dir_name)\n            self.assertDictEqual(config.text_config.to_dict(), text_config.to_dict())\n\n    @slow\n    def test_model_from_pretrained(self):\n        model_name = \"kakaobrain/align-base\"\n        model = AlignModel.from_pretrained(model_name)\n        self.assertIsNotNone(model)\n\n\n# We will verify our results on an image of cute cats\ndef prepare_img():\n    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n    im = Image.open(requests.get(url, stream=True).raw)\n    return im\n\n\n@require_vision\n@require_torch\nclass AlignModelIntegrationTest(unittest.TestCase):\n    @slow\n    def test_inference(self):\n        model_name = \"kakaobrain/align-base\"\n        model = AlignModel.from_pretrained(model_name).to(torch_device)\n        processor = AlignProcessor.from_pretrained(model_name)\n\n        image = prepare_img()\n        texts = [\"a photo of a cat\", \"a photo of a dog\"]\n        inputs = processor(images=image, text=texts, return_tensors=\"pt\").to(torch_device)\n\n        # forward pass\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # verify the logits\n        self.assertEqual(\n            outputs.logits_per_image.shape,\n            torch.Size((inputs.pixel_values.shape[0], inputs.input_ids.shape[0])),\n        )\n        self.assertEqual(\n            outputs.logits_per_text.shape,\n            torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n        )\n        expected_logits = torch.tensor([[9.7093, 3.4679]], device=torch_device)\n        torch.testing.assert_close(outputs.logits_per_image, expected_logits, rtol=1e-3, atol=1e-3)\n"}
{"type": "test_file", "path": "transformers/tests/generation/test_fsdp.py", "content": "# Copyright 2024 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nfrom typing import Any, Callable\n\nfrom transformers import is_torch_available\nfrom transformers.testing_utils import (\n    TestCasePlus,\n    execute_subprocess_async,\n    get_torch_dist_unique_port,\n    require_torch_multi_gpu,\n)\n\n\nif is_torch_available():\n    import functools\n\n    import torch\n    import torch.distributed\n    from torch.distributed._composable.fsdp import fully_shard, register_fsdp_forward_method\n    from torch.distributed.device_mesh import init_device_mesh\n    from torch.distributed.fsdp import FullyShardedDataParallel\n    from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n\n    data = 4 * [\n        \"Hello world!\",\n        \"The quick brown fox jumps over the lazy dog.\",\n    ]\n\n    def manage_process_group(func: Callable[..., Any]) -> Callable[..., Any]:\n        \"\"\"Manage the creation and destruction of the distributed process group for the wrapped function.\"\"\"\n\n        def wrapped(*args: Any, **kwargs: Any) -> Any:\n            torch.distributed.init_process_group(world_size=torch.cuda.device_count())\n            try:\n                return func(*args, **kwargs)\n            finally:\n                torch.distributed.destroy_process_group()\n\n        return wrapped\n\n    @manage_process_group\n    def fsdp_generate():\n        torch.cuda.set_device(device := torch.device(rank := torch.distributed.get_rank()))\n\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(device)\n\n        fsdp_model = FullyShardedDataParallel(\n            model,\n            auto_wrap_policy=functools.partial(transformer_auto_wrap_policy, transformer_layer_cls={GPT2Block}),\n            limit_all_gathers=True,\n            use_orig_params=True,\n        )\n\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        batch = tokenizer(data[rank], return_tensors=\"pt\", return_attention_mask=True).to(device)\n\n        with FullyShardedDataParallel.summon_full_params(fsdp_model):\n            _ = fsdp_model.module.generate(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"],\n                max_length=30,\n            )\n\n    @manage_process_group\n    def fsdp2_generate():\n        torch.cuda.set_device(device := torch.device(rank := torch.distributed.get_rank()))\n\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(device)\n\n        mesh = init_device_mesh(\"cuda\", (torch.distributed.get_world_size(),))\n        for submodule in model.modules():\n            if isinstance(submodule, GPT2Block):\n                fully_shard(submodule, mesh=mesh)\n        fully_shard(model, mesh=mesh)\n\n        register_fsdp_forward_method(model, \"generate\")\n\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        batch = tokenizer(data[rank], return_tensors=\"pt\", return_attention_mask=True).to(device)\n\n        _ = model.generate(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            max_length=30,\n        )\n\n\nclass TestFSDPGeneration(TestCasePlus):\n    @require_torch_multi_gpu\n    def test_fsdp_generate(self):\n        distributed_args = f\"\"\"--nproc_per_node={torch.cuda.device_count()}\n            --master_port={get_torch_dist_unique_port()}\n            {self.test_file_dir}/test_fsdp.py\n        \"\"\".split()\n        args = \"--fsdp\".split()\n        cmd = [\"torchrun\"] + distributed_args + args\n        execute_subprocess_async(cmd, env=self.get_env())\n        # successful return here == success - any errors would have caused an error in the sub-call\n\n    @require_torch_multi_gpu\n    def test_fsdp2_generate(self):\n        distributed_args = f\"\"\"--nproc_per_node={torch.cuda.device_count()}\n            --master_port={get_torch_dist_unique_port()}\n            {self.test_file_dir}/test_fsdp.py\n        \"\"\".split()\n        args = \"--fsdp2\".split()\n        cmd = [\"torchrun\"] + distributed_args + args\n        execute_subprocess_async(cmd, env=self.get_env())\n        # successful return here == success - any errors would have caused an error in the sub-call\n\n\nif __name__ == \"__main__\":\n    # The script below is meant to be run under torch.distributed, on a machine with multiple GPUs:\n    #\n    # PYTHONPATH=\"src\" python -m torch.distributed.run --nproc_per_node 2 --output_dir output_dir ./tests/generation/test_fsdp.py --fsdp\n\n    class CLIArgs(argparse.Namespace):\n        fsdp: bool\n        fsdp2: bool\n\n    parser = argparse.ArgumentParser()\n    group = parser.add_mutually_exclusive_group()\n    group.add_argument(\"--fsdp\", action=\"store_true\")\n    group.add_argument(\"--fsdp2\", action=\"store_true\")\n    args = parser.parse_args(namespace=CLIArgs())\n\n    if args.fsdp:\n        fsdp_generate()\n    elif args.fsdp2:\n        fsdp2_generate()\n    else:\n        raise ValueError(\"Missing test selection\")\n"}
{"type": "test_file", "path": "transformers/tests/models/albert/test_modeling_flax_albert.py", "content": "# Copyright 2021 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\n\nfrom transformers import AlbertConfig, is_flax_available\nfrom transformers.testing_utils import require_flax, slow\n\nfrom ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor, random_attention_mask\n\n\nif is_flax_available():\n    import jax.numpy as jnp\n\n    from transformers.models.albert.modeling_flax_albert import (\n        FlaxAlbertForMaskedLM,\n        FlaxAlbertForMultipleChoice,\n        FlaxAlbertForPreTraining,\n        FlaxAlbertForQuestionAnswering,\n        FlaxAlbertForSequenceClassification,\n        FlaxAlbertForTokenClassification,\n        FlaxAlbertModel,\n    )\n\n\nclass FlaxAlbertModelTester:\n    def __init__(\n        self,\n        parent,\n        batch_size=13,\n        seq_length=7,\n        is_training=True,\n        use_attention_mask=True,\n        use_token_type_ids=True,\n        use_labels=True,\n        vocab_size=99,\n        hidden_size=32,\n        num_hidden_layers=2,\n        num_attention_heads=4,\n        intermediate_size=37,\n        hidden_act=\"gelu\",\n        hidden_dropout_prob=0.1,\n        attention_probs_dropout_prob=0.1,\n        max_position_embeddings=512,\n        type_vocab_size=16,\n        type_sequence_label_size=2,\n        initializer_range=0.02,\n        num_choices=4,\n    ):\n        self.parent = parent\n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.is_training = is_training\n        self.use_attention_mask = use_attention_mask\n        self.use_token_type_ids = use_token_type_ids\n        self.use_labels = use_labels\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.max_position_embeddings = max_position_embeddings\n        self.type_vocab_size = type_vocab_size\n        self.type_sequence_label_size = type_sequence_label_size\n        self.initializer_range = initializer_range\n        self.num_choices = num_choices\n\n    def prepare_config_and_inputs(self):\n        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n\n        attention_mask = None\n        if self.use_attention_mask:\n            attention_mask = random_attention_mask([self.batch_size, self.seq_length])\n\n        token_type_ids = None\n        if self.use_token_type_ids:\n            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n\n        config = AlbertConfig(\n            vocab_size=self.vocab_size,\n            hidden_size=self.hidden_size,\n            num_hidden_layers=self.num_hidden_layers,\n            num_attention_heads=self.num_attention_heads,\n            intermediate_size=self.intermediate_size,\n            hidden_act=self.hidden_act,\n            hidden_dropout_prob=self.hidden_dropout_prob,\n            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n            max_position_embeddings=self.max_position_embeddings,\n            type_vocab_size=self.type_vocab_size,\n            is_decoder=False,\n            initializer_range=self.initializer_range,\n        )\n\n        return config, input_ids, token_type_ids, attention_mask\n\n    def prepare_config_and_inputs_for_common(self):\n        config_and_inputs = self.prepare_config_and_inputs()\n        config, input_ids, token_type_ids, attention_mask = config_and_inputs\n        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\n        return config, inputs_dict\n\n\n@require_flax\nclass FlaxAlbertModelTest(FlaxModelTesterMixin, unittest.TestCase):\n    all_model_classes = (\n        (\n            FlaxAlbertModel,\n            FlaxAlbertForPreTraining,\n            FlaxAlbertForMaskedLM,\n            FlaxAlbertForMultipleChoice,\n            FlaxAlbertForQuestionAnswering,\n            FlaxAlbertForSequenceClassification,\n            FlaxAlbertForTokenClassification,\n            FlaxAlbertForQuestionAnswering,\n        )\n        if is_flax_available()\n        else ()\n    )\n\n    def setUp(self):\n        self.model_tester = FlaxAlbertModelTester(self)\n\n    @slow\n    def test_model_from_pretrained(self):\n        for model_class_name in self.all_model_classes:\n            model = model_class_name.from_pretrained(\"albert/albert-base-v2\")\n            outputs = model(np.ones((1, 1)))\n            self.assertIsNotNone(outputs)\n\n\n@require_flax\nclass FlaxAlbertModelIntegrationTest(unittest.TestCase):\n    @slow\n    def test_inference_no_head_absolute_embedding(self):\n        model = FlaxAlbertModel.from_pretrained(\"albert/albert-base-v2\")\n        input_ids = np.array([[0, 345, 232, 328, 740, 140, 1695, 69, 6078, 1588, 2]])\n        attention_mask = np.array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n        output = model(input_ids, attention_mask=attention_mask)[0]\n        expected_shape = (1, 11, 768)\n        self.assertEqual(output.shape, expected_shape)\n        expected_slice = np.array(\n            [[[-0.6513, 1.5035, -0.2766], [-0.6515, 1.5046, -0.2780], [-0.6512, 1.5049, -0.2784]]]\n        )\n\n        self.assertTrue(jnp.allclose(output[:, 1:4, 1:4], expected_slice, atol=1e-4))\n"}
{"type": "test_file", "path": "transformers/tests/models/albert/test_modeling_tf_albert.py", "content": "# coding=utf-8\n# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom __future__ import annotations\n\nimport unittest\n\nfrom transformers import AlbertConfig, is_tf_available\nfrom transformers.models.auto import get_values\nfrom transformers.testing_utils import require_tf, slow\n\nfrom ...test_configuration_common import ConfigTester\nfrom ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\nfrom ...test_pipeline_mixin import PipelineTesterMixin\n\n\nif is_tf_available():\n    import tensorflow as tf\n\n    from transformers import TF_MODEL_FOR_PRETRAINING_MAPPING\n    from transformers.models.albert.modeling_tf_albert import (\n        TFAlbertForMaskedLM,\n        TFAlbertForMultipleChoice,\n        TFAlbertForPreTraining,\n        TFAlbertForQuestionAnswering,\n        TFAlbertForSequenceClassification,\n        TFAlbertForTokenClassification,\n        TFAlbertModel,\n    )\n\n\nclass TFAlbertModelTester:\n    def __init__(\n        self,\n        parent,\n        batch_size=13,\n        seq_length=7,\n        is_training=True,\n        use_input_mask=True,\n        use_token_type_ids=True,\n        use_labels=True,\n        vocab_size=99,\n        embedding_size=16,\n        hidden_size=32,\n        num_hidden_layers=2,\n        num_attention_heads=4,\n        intermediate_size=37,\n        hidden_act=\"gelu\",\n        hidden_dropout_prob=0.1,\n        attention_probs_dropout_prob=0.1,\n        max_position_embeddings=512,\n        type_vocab_size=16,\n        type_sequence_label_size=2,\n        initializer_range=0.02,\n        num_labels=3,\n        num_choices=4,\n        scope=None,\n    ):\n        self.parent = parent\n        self.batch_size = 13\n        self.seq_length = 7\n        self.is_training = True\n        self.use_input_mask = True\n        self.use_token_type_ids = True\n        self.use_labels = True\n        self.vocab_size = 99\n        self.embedding_size = 16\n        self.hidden_size = 32\n        self.num_hidden_layers = 2\n        self.num_attention_heads = 4\n        self.intermediate_size = 37\n        self.hidden_act = \"gelu\"\n        self.hidden_dropout_prob = 0.1\n        self.attention_probs_dropout_prob = 0.1\n        self.max_position_embeddings = 512\n        self.type_vocab_size = 16\n        self.type_sequence_label_size = 2\n        self.initializer_range = 0.02\n        self.num_labels = 3\n        self.num_choices = 4\n        self.scope = None\n\n    def prepare_config_and_inputs(self):\n        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n\n        input_mask = None\n        if self.use_input_mask:\n            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n\n        token_type_ids = None\n        if self.use_token_type_ids:\n            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n\n        sequence_labels = None\n        token_labels = None\n        choice_labels = None\n        if self.use_labels:\n            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n\n        config = AlbertConfig(\n            vocab_size=self.vocab_size,\n            hidden_size=self.hidden_size,\n            embedding_size=self.embedding_size,\n            num_hidden_layers=self.num_hidden_layers,\n            num_attention_heads=self.num_attention_heads,\n            intermediate_size=self.intermediate_size,\n            hidden_act=self.hidden_act,\n            hidden_dropout_prob=self.hidden_dropout_prob,\n            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n            max_position_embeddings=self.max_position_embeddings,\n            type_vocab_size=self.type_vocab_size,\n            initializer_range=self.initializer_range,\n        )\n\n        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n\n    def create_and_check_albert_model(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        model = TFAlbertModel(config=config)\n        # inputs = {'input_ids': input_ids,\n        #           'attention_mask': input_mask,\n        #           'token_type_ids': token_type_ids}\n        # sequence_output, pooled_output = model(**inputs)\n        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n        result = model(inputs)\n\n        inputs = [input_ids, input_mask]\n        result = model(inputs)\n\n        result = model(input_ids)\n\n        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n\n    def create_and_check_albert_for_pretraining(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        config.num_labels = self.num_labels\n        model = TFAlbertForPreTraining(config=config)\n        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n        result = model(inputs)\n        self.parent.assertEqual(result.prediction_logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n        self.parent.assertEqual(result.sop_logits.shape, (self.batch_size, self.num_labels))\n\n    def create_and_check_albert_for_masked_lm(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        model = TFAlbertForMaskedLM(config=config)\n        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n        result = model(inputs)\n        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n\n    def create_and_check_albert_for_sequence_classification(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        config.num_labels = self.num_labels\n        model = TFAlbertForSequenceClassification(config=config)\n        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n        result = model(inputs)\n        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n\n    def create_and_check_albert_for_question_answering(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        model = TFAlbertForQuestionAnswering(config=config)\n        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n        result = model(inputs)\n        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n\n    def create_and_check_albert_for_multiple_choice(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        config.num_choices = self.num_choices\n        model = TFAlbertForMultipleChoice(config=config)\n        multiple_choice_inputs_ids = tf.tile(tf.expand_dims(input_ids, 1), (1, self.num_choices, 1))\n        multiple_choice_input_mask = tf.tile(tf.expand_dims(input_mask, 1), (1, self.num_choices, 1))\n        multiple_choice_token_type_ids = tf.tile(tf.expand_dims(token_type_ids, 1), (1, self.num_choices, 1))\n        inputs = {\n            \"input_ids\": multiple_choice_inputs_ids,\n            \"attention_mask\": multiple_choice_input_mask,\n            \"token_type_ids\": multiple_choice_token_type_ids,\n        }\n        result = model(inputs)\n        self.parent.assertListEqual(list(result[\"logits\"].shape), [self.batch_size, self.num_choices])\n\n    def create_and_check_albert_for_token_classification(\n        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n    ):\n        config.num_labels = self.num_labels\n        model = TFAlbertForTokenClassification(config=config)\n        inputs = {\n            \"input_ids\": input_ids,\n            \"attention_mask\": input_mask,\n            \"token_type_ids\": token_type_ids,\n        }\n        result = model(inputs)\n        self.parent.assertListEqual(list(result[\"logits\"].shape), [self.batch_size, self.seq_length, self.num_labels])\n\n    def prepare_config_and_inputs_for_common(self):\n        config_and_inputs = self.prepare_config_and_inputs()\n        (\n            config,\n            input_ids,\n            token_type_ids,\n            input_mask,\n            sequence_labels,\n            token_labels,\n            choice_labels,\n        ) = config_and_inputs\n        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": input_mask}\n        return config, inputs_dict\n\n\n@require_tf\nclass TFAlbertModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n    all_model_classes = (\n        (\n            TFAlbertModel,\n            TFAlbertForPreTraining,\n            TFAlbertForMaskedLM,\n            TFAlbertForSequenceClassification,\n            TFAlbertForQuestionAnswering,\n            TFAlbertForTokenClassification,\n            TFAlbertForMultipleChoice,\n        )\n        if is_tf_available()\n        else ()\n    )\n    pipeline_model_mapping = (\n        {\n            \"feature-extraction\": TFAlbertModel,\n            \"fill-mask\": TFAlbertForMaskedLM,\n            \"question-answering\": TFAlbertForQuestionAnswering,\n            \"text-classification\": TFAlbertForSequenceClassification,\n            \"token-classification\": TFAlbertForTokenClassification,\n            \"zero-shot\": TFAlbertForSequenceClassification,\n        }\n        if is_tf_available()\n        else {}\n    )\n    test_head_masking = False\n    test_onnx = False\n\n    # special case for ForPreTraining model\n    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n\n        if return_labels:\n            if model_class in get_values(TF_MODEL_FOR_PRETRAINING_MAPPING):\n                inputs_dict[\"sentence_order_label\"] = tf.zeros(self.model_tester.batch_size, dtype=tf.int32)\n\n        return inputs_dict\n\n    def setUp(self):\n        self.model_tester = TFAlbertModelTester(self)\n        self.config_tester = ConfigTester(self, config_class=AlbertConfig, hidden_size=37)\n\n    def test_config(self):\n        self.config_tester.run_common_tests()\n\n    def test_albert_model(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_albert_model(*config_and_inputs)\n\n    def test_for_pretraining(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_albert_for_pretraining(*config_and_inputs)\n\n    def test_for_masked_lm(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_albert_for_masked_lm(*config_and_inputs)\n\n    def test_for_multiple_choice(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_albert_for_multiple_choice(*config_and_inputs)\n\n    def test_for_sequence_classification(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_albert_for_sequence_classification(*config_and_inputs)\n\n    def test_for_question_answering(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_albert_for_question_answering(*config_and_inputs)\n\n    @slow\n    def test_model_from_pretrained(self):\n        model_name = \"albert/albert-base-v1\"\n        model = TFAlbertModel.from_pretrained(model_name)\n        self.assertIsNotNone(model)\n\n\n@require_tf\nclass TFAlbertModelIntegrationTest(unittest.TestCase):\n    @slow\n    def test_inference_masked_lm(self):\n        model = TFAlbertForPreTraining.from_pretrained(\"albert/albert-base-v2\")\n        input_ids = tf.constant([[0, 1, 2, 3, 4, 5]])\n        output = model(input_ids)[0]\n\n        expected_shape = [1, 6, 30000]\n        self.assertEqual(output.shape, expected_shape)\n\n        expected_slice = tf.constant(\n            [\n                [\n                    [4.595668, 0.74462754, -1.818147],\n                    [4.5954347, 0.7454184, -1.8188258],\n                    [4.5954905, 0.7448235, -1.8182316],\n                ]\n            ]\n        )\n        tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=1e-4)\n"}
{"type": "test_file", "path": "transformers/tests/generation/test_candidate_generator.py", "content": "import gc\nimport unittest\nimport weakref\nfrom unittest.mock import MagicMock\n\nimport torch\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\nfrom transformers.generation.candidate_generator import (\n    AssistantToTargetTranslator,\n    AssistantVocabTranslatorCache,\n    UniversalSpeculativeDecodingGenerator,\n)\nfrom transformers.testing_utils import require_torch, torch_device\n\n\n@require_torch\nclass TestAssistantToTargetTranslator(unittest.TestCase):\n    def setUp(self):\n        # Create mock tokenizers with predefined vocabularies\n        self.target_tokenizer = MagicMock()\n        self.assistant_tokenizer = MagicMock()\n\n        # Define mock vocabularies for the tokenizers\n        self.target_vocab = {\"hello\": 0, \"world\": 1, \"foo\": 2, \"bar\": 3}\n        self.assistant_vocab = {\"hello\": 0, \"world\": 1, \"foo\": 2, \"baz\": 4}\n\n        self.target_tokenizer.get_vocab.return_value = self.target_vocab\n        self.assistant_tokenizer.get_vocab.return_value = self.assistant_vocab\n        self.assistant_model_device = torch_device\n        self.target_vocab_size = 6\n\n        # Instantiate the class under test\n        self.translator = AssistantToTargetTranslator(\n            target_tokenizer=self.target_tokenizer,\n            assistant_tokenizer=self.assistant_tokenizer,\n            assistant_model_device=self.assistant_model_device,\n            target_vocab_size=self.target_vocab_size,\n        )\n\n    def test_get_assistant_to_target_input_ids(self):\n        \"\"\"Test the mapping from assistant tokens to target tokens.\"\"\"\n        expected_mapping = [0, 1, 2, self.translator.SUPPRESS_TOKEN_ID, self.translator.SUPPRESS_TOKEN_ID]\n        actual_mapping = self.translator._assistant_to_target_input_ids.tolist()\n        self.assertEqual(actual_mapping, expected_mapping)\n\n    def test_get_suppress_input_ids(self):\n        \"\"\"Test the suppression of assistant input IDs not present in the target vocabulary.\"\"\"\n        expected_suppress_ids = [3, 4]\n        actual_suppress_ids = self.translator._get_suppress_input_ids().tolist()\n        self.assertEqual(actual_suppress_ids, expected_suppress_ids)\n\n    def test_get_target_ids(self):\n        \"\"\"Test the translation of assistant candidate IDs to target candidate IDs.\"\"\"\n        assistant_input_ids = torch.LongTensor([[0, 1, 2]]).to(\n            self.assistant_model_device\n        )  # 'hello world foo' in assistant tokenizer\n        target_input_ids = torch.LongTensor([[0, 1, 2]]).to(\n            self.assistant_model_device\n        )  # 'hello world foo' in target tokenizer\n        assistant_candidate_ids = torch.LongTensor([[0, 1, 2, 4]]).to(\n            self.assistant_model_device\n        )  # 'hello world foo baz' in assistant tokenizer\n\n        expected_target_ids = torch.LongTensor(\n            [[0, 1, 2, self.translator.SUPPRESS_TOKEN_ID]]\n        ).to(\n            self.assistant_model_device\n        )  # 'hello world foo baz' in target tokenizer (baz is mapped to self.translator.suppress_tokens_id since it does not exist in target vocab)\n\n        actual_target_ids = self.translator.get_target_ids(\n            assistant_input_ids, target_input_ids, assistant_candidate_ids\n        )\n        self.assertTrue(torch.equal(actual_target_ids, expected_target_ids))\n\n    def test_get_target_logits(self):\n        \"\"\"Test the conversion of assistant logits to target logits.\"\"\"\n        # Assistant logits for IDs 0, 1, 2\n        assistant_logits = torch.FloatTensor([[[0.1, 0.2, 0.3, 0.4, self.translator.FILTER_VALUE]]]).to(\n            self.assistant_model_device\n        )  # Shape (1, 1, 5)\n\n        # Expected target logits (target_vocab_size = 4)\n        expected_target_logits = torch.full((1, 1, self.target_vocab_size), self.translator.FILTER_VALUE).to(\n            self.assistant_model_device\n        )\n        expected_target_logits[0, 0, 0] = 0.1  # 'hello'\n        expected_target_logits[0, 0, 1] = 0.2  # 'world'\n        expected_target_logits[0, 0, 2] = 0.3  # 'foo'\n        # The 'bar' token in target vocab remains at -inf\n\n        actual_target_logits = self.translator.get_target_logits(assistant_logits)\n        self.assertTrue(torch.equal(actual_target_logits, expected_target_logits))\n\n\nclass MockTokenizer:\n    \"\"\"A simple mock tokenizer class that supports weak references.\"\"\"\n\n    def __init__(self, vocab=None):\n        self._vocab = vocab or {}\n\n    def get_vocab(self):\n        return self._vocab\n\n    def __call__(self, text, add_special_tokens=True):\n        # Mock implementation of the __call__ method\n        tokens = text.split()\n        input_ids = [self._vocab.get(token, 0) for token in tokens]\n        return {\"input_ids\": input_ids}\n\n\n@require_torch\nclass TestAssistantVocabTranslatorCache(unittest.TestCase):\n    def setUp(self):\n        # Clear the cache before each test\n        AssistantVocabTranslatorCache._cache.clear()\n        # Create mock tokenizers with different vocabularies\n        self.target_tokenizer = MockTokenizer({\"hello\": 0, \"world\": 1})\n        self.assistant_tokenizer = MockTokenizer({\"hello\": 0, \"world\": 1, \"foo\": 2})\n        self.other_target_tokenizer = MockTokenizer({\"foo\": 2, \"bar\": 3})\n        self.other_assistant_tokenizer = MockTokenizer({\"baz\": 4, \"qux\": 5})\n        self.assistant_model_device = torch_device\n        self.target_vocab_size = 6\n\n    def test_same_instance_for_same_tokenizers(self):\n        \"\"\"Test that the same translator is returned for the same tokenizers.\"\"\"\n        translator1 = AssistantVocabTranslatorCache.get_translator(\n            self.target_tokenizer,\n            self.assistant_tokenizer,\n            assistant_model_device=self.assistant_model_device,\n            target_vocab_size=self.target_vocab_size,\n        )\n        translator2 = AssistantVocabTranslatorCache.get_translator(\n            self.target_tokenizer,\n            self.assistant_tokenizer,\n            assistant_model_device=self.assistant_model_device,\n            target_vocab_size=self.target_vocab_size,\n        )\n        self.assertIs(translator1, translator2, \"Translators should be cached and identical\")\n\n    def test_different_instances_for_different_tokenizers(self):\n        \"\"\"Test that different tokenizers produce different translators.\"\"\"\n        translator1 = AssistantVocabTranslatorCache.get_translator(\n            self.target_tokenizer,\n            self.assistant_tokenizer,\n            assistant_model_device=self.assistant_model_device,\n            target_vocab_size=self.target_vocab_size,\n        )\n        translator2 = AssistantVocabTranslatorCache.get_translator(\n            self.other_target_tokenizer,\n            self.other_assistant_tokenizer,\n            assistant_model_device=self.assistant_model_device,\n            target_vocab_size=self.target_vocab_size,\n        )\n        self.assertIsNot(translator1, translator2, \"Translators should differ for different tokenizers\")\n\n    def test_cache_with_weakref_key(self):\n        \"\"\"Ensure that the cache uses weak references as keys.\"\"\"\n        initial_cache_size = len(AssistantVocabTranslatorCache._cache)\n        target_tokenizer = MockTokenizer({\"hello\": 0})\n        assistant_tokenizer = MockTokenizer({\"hello\": 0})\n\n        # Store translator in a local variable to avoid it being kept alive\n        translator = AssistantVocabTranslatorCache.get_translator(\n            target_tokenizer,\n            assistant_tokenizer,\n            assistant_model_device=self.assistant_model_device,\n            target_vocab_size=self.target_vocab_size,\n        )\n        self.assertEqual(len(AssistantVocabTranslatorCache._cache), initial_cache_size + 1)\n\n        # Delete all strong references\n        del target_tokenizer\n        del assistant_tokenizer\n        del translator\n\n        # Force garbage collection\n        gc.collect()\n\n        # Call cleanup to remove dead entries\n        AssistantVocabTranslatorCache.cleanup()\n\n        # The cache size remains increased due to strong references\n        self.assertEqual(len(AssistantVocabTranslatorCache._cache), initial_cache_size + 1)\n\n    def test_weakref_cache_cleanup(self):\n        \"\"\"Test that the cache cleans up translators when tokenizers are garbage collected.\"\"\"\n\n        def create_translator():\n            target_tokenizer = MockTokenizer({\"hello\": 0})\n            assistant_tokenizer = MockTokenizer({\"hello\": 0})\n            translator = AssistantVocabTranslatorCache.get_translator(\n                target_tokenizer,\n                assistant_tokenizer,\n                assistant_model_device=self.assistant_model_device,\n                target_vocab_size=self.target_vocab_size,\n            )\n            # Create weak references before returning\n            refs = (weakref.ref(translator), weakref.ref(target_tokenizer), weakref.ref(assistant_tokenizer))\n            # Remove strong references inside the function\n            del target_tokenizer\n            del assistant_tokenizer\n            del translator\n            return refs\n\n        translator_ref, target_ref, assistant_ref = create_translator()\n\n        # Force garbage collection\n        gc.collect()\n\n        # Call cleanup to remove dead entries\n        AssistantVocabTranslatorCache.cleanup()\n\n        # The tokenizers and translator are not garbage collected due to strong references\n        self.assertIsNotNone(target_ref(), \"Target tokenizer should still be alive due to strong references\")\n        self.assertIsNotNone(assistant_ref(), \"Assistant tokenizer should still be alive due to strong references\")\n        self.assertIsNotNone(translator_ref(), \"Translator should still be alive due to strong references\")\n\n\n@require_torch\nclass TestUniversalSpeculativeDecoding(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        cls.target_name = \"hf-internal-testing/tiny-random-LlamaForCausalLM\"\n        cls.assistant_name = \"hf-internal-testing/tiny-random-PhiForCausalLM\"\n\n    def setUp(self):\n        self.target_tokenizer = AutoTokenizer.from_pretrained(self.target_name)\n        self.target_config = AutoConfig.from_pretrained(self.target_name)\n        self.assistant_model = AutoModelForCausalLM.from_pretrained(self.assistant_name).to(torch_device)\n        self.assistant_tokenizer = AutoTokenizer.from_pretrained(self.assistant_name)\n\n        self.generation_config = GenerationConfig()\n\n        # Ensure required tokens exist\n        if self.target_tokenizer.pad_token_id is None:\n            self.target_tokenizer.pad_token_id = self.target_tokenizer.eos_token_id\n        if self.target_tokenizer.bos_token_id is None:\n            self.target_tokenizer.bos_token_id = self.target_tokenizer.eos_token_id\n        if self.assistant_tokenizer.pad_token_id is None:\n            self.assistant_tokenizer.pad_token_id = self.assistant_tokenizer.eos_token_id\n        if self.target_tokenizer.bos_token_id is None:\n            self.assistant_tokenizer.bos_token_id = self.assistant_tokenizer.eos_token_id\n\n        self.input_ids = torch.tensor([[1, 2, 3]]).to(torch_device)\n        self.model_kwargs = {\n            \"attention_mask\": torch.ones_like(self.input_ids).to(torch_device),\n        }\n\n        atm_translator = AssistantVocabTranslatorCache.get_translator(\n            self.target_tokenizer, self.assistant_tokenizer, self.target_config.vocab_size, torch_device\n        )\n        self.generator = UniversalSpeculativeDecodingGenerator(\n            input_ids=self.input_ids,\n            assistant_model=self.assistant_model,\n            target_tokenizer=self.target_tokenizer,\n            assistant_tokenizer=self.assistant_tokenizer,\n            generation_config=self.generation_config,\n            model_kwargs=self.model_kwargs,\n            atm_translator=atm_translator,\n        )\n\n    def test_basic_generation(self):\n        \"\"\"Test basic speculative decoding works\"\"\"\n        input_text = \"The quick brown fox\"\n        input_ids = self.target_tokenizer.encode(input_text, return_tensors=\"pt\")\n        self.generator.input_ids = input_ids\n        candidates, scores = self.generator.get_candidates(input_ids)\n\n        self.assertIsNotNone(candidates)\n        self.assertIsNotNone(scores)\n        self.assertTrue(torch.is_tensor(candidates))\n        self.assertTrue(torch.is_tensor(scores))\n\n    def test_mismatched_vocabularies(self):\n        \"\"\"Test handling of mismatched vocabularies between models\"\"\"\n        # Create input with tokens present in main but not assistant vocab\n        # Find a token that is not in the assistant tokenizer but in\n        # the main tokenizer.\n        missing_token = next(\n            token\n            for token in self.target_tokenizer.get_vocab()\n            if token not in self.assistant_tokenizer.get_vocab()\n            and token not in self.target_tokenizer.all_special_tokens\n            and \"reserved_\" not in token\n        )\n        input_ids = torch.tensor([[self.target_tokenizer.convert_tokens_to_ids(missing_token)]])\n        self.generator.input_ids = input_ids\n        candidates, scores = self.generator.get_candidates(input_ids)\n        self.assertIsNotNone(candidates)\n\n    def test_speculation_depth(self):\n        \"\"\"Test different speculation depths\"\"\"\n        input_ids = self.target_tokenizer.encode(\"Test text\", return_tensors=\"pt\")\n        self.generator.input_ids = input_ids\n\n        for depth in [1, 8, 17]:\n            self.generator.num_assistant_tokens = depth\n            candidates, scores = self.generator.get_candidates(input_ids)\n            self.assertLessEqual(candidates.shape[1] - input_ids.shape[1], depth)\n\n    def test_device_consistency(self):\n        \"\"\"Test handling of inputs on different devices\"\"\"\n        input_ids = torch.tensor([[1, 2, 3]]).to(torch_device)\n        self.generator.input_ids = input_ids\n        candidates, _ = self.generator.get_candidates(input_ids)\n        self.assertEqual(candidates.device, input_ids.device)\n\n    def test_usd_vs_vanilla_sampling(cls):\n        \"\"\"Test that USD matches vanilla sampling with temperature set to nearly 0\"\"\"\n        prompt = \"Test text\"\n\n        pipe_usd = pipeline(\"text-generation\", model=cls.target_name, assistant_model=cls.assistant_name)\n        pipe_usd_output = pipe_usd(prompt, max_new_tokens=5, do_sample=True, temperature=1e-9)  # Nearly 0 temperature\n        usd_text = pipe_usd_output[0][\"generated_text\"]\n\n        pipe_vanilla = pipeline(\n            \"text-generation\",\n            model=cls.target_name,\n        )\n        pipe_vanilla_output = pipe_vanilla(prompt, max_new_tokens=5, do_sample=False)\n        vanilla_text = pipe_vanilla_output[0][\"generated_text\"]\n\n        # Assert that the outputs match\n        cls.assertEqual(usd_text, vanilla_text)\n"}
{"type": "test_file", "path": "transformers/tests/models/aria/__init__.py", "content": ""}
{"type": "test_file", "path": "transformers/tests/generation/test_stopping_criteria.py", "content": "# coding=utf-8\n# Copyright 2020 The HuggingFace Team Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a clone of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport time\nimport unittest\n\nfrom transformers import AutoTokenizer, is_torch_available\nfrom transformers.testing_utils import require_torch, torch_device\n\nfrom ..test_modeling_common import ids_tensor\n\n\nif is_torch_available():\n    import torch\n\n    from transformers.generation import (\n        ConfidenceCriteria,\n        EosTokenCriteria,\n        MaxLengthCriteria,\n        MaxTimeCriteria,\n        StoppingCriteriaList,\n        StopStringCriteria,\n        validate_stopping_criteria,\n    )\n\n\n@require_torch\nclass StoppingCriteriaTestCase(unittest.TestCase):\n    def _get_tensors(self, length):\n        batch_size = 3\n        vocab_size = 250\n\n        input_ids = ids_tensor((batch_size, length), vocab_size)\n        scores = torch.ones((batch_size, length), device=torch_device, dtype=torch.float) / length\n        return input_ids, scores\n\n    def test_list_criteria(self):\n        input_ids, scores = self._get_tensors(5)\n\n        criteria = StoppingCriteriaList(\n            [\n                MaxLengthCriteria(max_length=10),\n                MaxTimeCriteria(max_time=0.1),\n            ]\n        )\n\n        self.assertFalse(all(criteria(input_ids, scores)))\n\n        input_ids, scores = self._get_tensors(9)\n        self.assertFalse(all(criteria(input_ids, scores)))\n\n        input_ids, scores = self._get_tensors(10)\n        self.assertTrue(all(criteria(input_ids, scores)))\n\n    def test_max_length_criteria(self):\n        criteria = MaxLengthCriteria(max_length=10)\n\n        input_ids, scores = self._get_tensors(5)\n        self.assertFalse(all(criteria(input_ids, scores)))\n\n        input_ids, scores = self._get_tensors(9)\n        self.assertFalse(all(criteria(input_ids, scores)))\n\n        input_ids, scores = self._get_tensors(10)\n        self.assertTrue(all(criteria(input_ids, scores)))\n\n    def test_max_time_criteria(self):\n        input_ids, scores = self._get_tensors(5)\n\n        criteria = MaxTimeCriteria(max_time=0.1)\n        self.assertFalse(all(criteria(input_ids, scores)))\n\n        criteria = MaxTimeCriteria(max_time=0.1, initial_timestamp=time.time() - 0.2)\n        self.assertTrue(all(criteria(input_ids, scores)))\n\n    def test_eos_token_criteria(self):\n        criteria = EosTokenCriteria(eos_token_id=0)\n\n        input_ids, scores = self._get_tensors(5)\n        input_ids[:, -1] = 0\n        self.assertTrue(all(criteria(input_ids, scores)))\n\n        input_ids, scores = self._get_tensors(5)\n        input_ids[:2, -1] = 0\n        input_ids[2, -1] = 1\n        self.assertListEqual(criteria(input_ids, scores).tolist(), [True, True, False])\n\n        input_ids, scores = self._get_tensors(5)\n        input_ids[:, -1] = 1\n        self.assertListEqual(criteria(input_ids, scores).tolist(), [False, False, False])\n\n    def test_confidence_criteria(self):\n        criteria = ConfidenceCriteria(assistant_confidence_threshold=0.5)\n\n        vocab_size = 250\n        length = 5\n\n        input_ids = ids_tensor((1, length), vocab_size)\n        scores = (torch.randn((1, vocab_size)),)\n\n        # Simulate high confidence by setting the probability of the last token to be high\n        scores[0][0, input_ids[0, -1]] = 10.0  # Logits before softmax\n        self.assertFalse(criteria(input_ids, scores))\n\n        # Simulate low confidence by setting the probability of the last token to be low\n        scores[0][0, input_ids[0, -1]] = -10.0  # Logits before softmax\n        self.assertTrue(criteria(input_ids, scores))\n\n    def test_validate_stopping_criteria(self):\n        validate_stopping_criteria(StoppingCriteriaList([MaxLengthCriteria(10)]), 10)\n\n        with self.assertWarns(UserWarning):\n            validate_stopping_criteria(StoppingCriteriaList([MaxLengthCriteria(10)]), 11)\n\n        stopping_criteria = validate_stopping_criteria(StoppingCriteriaList(), 11)\n\n        self.assertEqual(len(stopping_criteria), 1)\n\n    def test_stop_string_criteria(self):\n        true_strings = [\n            \"<|im_start|><|im_end|>\",\n            \"<|im_start|><|im_end|<|im_end|>\",\n            \">><|im_start|>>stop\",\n            \"stop\",\n            \"e nd\",\n        ]\n        false_strings = [\n            \"<|im_start|><|im_end|\",\n            \"<|im_start|><|im_end|<|im_end|\",\n            \"<|im_end|><|im_start|>\",\n            \"<|im_end|<>stop<|im_end|\",\n            \"end\",\n            \"en d\",\n            \"eNd\",\n            \"<|im_end|\",\n            \"|im_end|>\",\n            \"s\",\n        ]\n        stop_strings = [\"<|im_end|>\", \"stop\", \"e nd\"]\n\n        # Use a tokenizer that won't actually have special tokens for these\n        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n        tokenizer.padding_side = \"left\"\n        true_input_ids = tokenizer(true_strings, return_tensors=\"pt\", padding=\"longest\", add_special_tokens=False)\n        false_input_ids = tokenizer(false_strings, return_tensors=\"pt\", padding=\"longest\", add_special_tokens=False)\n\n        scores = None\n        criteria = StopStringCriteria(tokenizer=tokenizer, stop_strings=stop_strings)\n        for i in range(len(true_strings)):\n            self.assertTrue(criteria(true_input_ids[\"input_ids\"][i : i + 1], scores))\n        for i in range(len(false_strings)):\n            self.assertFalse(criteria(false_input_ids[\"input_ids\"][i : i + 1], scores))\n\n        # Now try it with a tokenizer where those are actually special tokens\n        tokenizer = AutoTokenizer.from_pretrained(\"cognitivecomputations/dolphin-2.5-mixtral-8x7b\")\n        tokenizer.padding_side = \"left\"\n        true_input_ids = tokenizer(true_strings, return_tensors=\"pt\", padding=\"longest\", add_special_tokens=False)\n        false_input_ids = tokenizer(false_strings, return_tensors=\"pt\", padding=\"longest\", add_special_tokens=False)\n\n        criteria = StopStringCriteria(tokenizer=tokenizer, stop_strings=stop_strings)\n        for i in range(len(true_strings)):\n            self.assertTrue(criteria(true_input_ids[\"input_ids\"][i : i + 1], scores))\n        for i in range(len(false_strings)):\n            self.assertFalse(criteria(false_input_ids[\"input_ids\"][i : i + 1], scores))\n\n    def test_stop_string_criteria_vocab_size_mismatch(self):\n        \"\"\"Test that StopStringCriteria handles tokens above len(tokenizer) correctly.\"\"\"\n        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n\n        # Create input_ids with tokens above len(tokenizer)\n        input_ids = torch.tensor([[len(tokenizer) + 1024, 1, 2]], device=torch_device)\n        scores = None\n        criteria = StopStringCriteria(tokenizer=tokenizer, stop_strings=[\"test\"])\n\n        # This should not raise an error and should return False since no stop string is matched\n        self.assertFalse(criteria(input_ids, scores))\n\n    def test_stop_string_matching_positions(self):\n        stop_string = \"stop\"\n        token_list = [\"last\", \"top\", \"topper\", \"s\", \"p\"]\n        token_indices = list(range(len(token_list)))\n        all_token_valid_positions, all_token_end_overlaps = StopStringCriteria._stop_string_get_matching_positions(\n            token_list=token_list, token_indices=token_indices, stop_strings=[stop_string]\n        )\n        valid_positions = {\n            token_list[idx]: positions for idx, positions in all_token_valid_positions[stop_string].items()\n        }\n        end_overlaps = {token_list[idx]: overlaps for idx, overlaps in all_token_end_overlaps[stop_string].items()}\n        self.assertEqual(valid_positions, {\"s\": [3], \"last\": [2]})\n        self.assertEqual(end_overlaps, {\"top\": [3], \"topper\": [3], \"p\": [1]})\n\n    def test_stop_string_embedding_vecs(self):\n        stop_string = \"stop\"\n        token_list = [\"last\", \"top\", \"topper\", \"s\", \"p\"]\n        token_indices = list(range(len(token_list)))\n        embedding_vec, max_valid_positions, max_valid_end_lens = StopStringCriteria._stop_string_create_embedding_vec(\n            token_list=token_list, token_indices=token_indices, stop_strings=[stop_string]\n        )\n\n        # Positions inside the stop string where the token matches (excluding end overlaps)\n        valid_positions = embedding_vec[:, 0].tolist()\n        self.assertEqual(valid_positions, [2, -1, -1, 3, -1, -1])\n\n        # Overlap lengths between end of stop string and start of token\n        end_overlaps = embedding_vec[:, 1].tolist()\n        self.assertEqual(end_overlaps, [-1, 3, 3, -1, 1, -1])\n\n        # Length of each token\n        token_lengths = embedding_vec[:-1, 2].tolist()\n        self.assertEqual(token_lengths, [len(token) for token in token_list])\n\n    def test_single_letter_stop_string(self):\n        true_strings = [\"a\", \"baa\", \"abc\"]  # \"abc\" is a single token\n        false_strings = [\"abbbbbbb\", \"b\"]  # \"abbbbbbb\" is split into multiple tokens\n        stop_strings = [\"a\"]\n        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n        tokenizer.padding_side = \"left\"\n\n        true_input_ids = tokenizer(true_strings, return_tensors=\"pt\", padding=\"longest\", add_special_tokens=False)\n        false_input_ids = tokenizer(false_strings, return_tensors=\"pt\", padding=\"longest\", add_special_tokens=False)\n\n        scores = None\n        criteria = StopStringCriteria(tokenizer=tokenizer, stop_strings=stop_strings)\n        for input_ids in true_input_ids[\"input_ids\"]:\n            self.assertTrue(criteria(input_ids.unsqueeze(0), scores))\n        for input_ids in false_input_ids[\"input_ids\"]:\n            self.assertFalse(criteria(input_ids.unsqueeze(0), scores))\n\n    def test_criterias_per_row(self):\n        text = \"They completed the challenging puzzle, revealing the hidden image at the end\"\n        stop_strings = [\"end\"]\n\n        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n        inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n\n        scores = None\n        criteria = StoppingCriteriaList(\n            [\n                MaxLengthCriteria(max_length=20),\n                StopStringCriteria(tokenizer=tokenizer, stop_strings=stop_strings),\n            ]\n        )\n\n        # trigger stopping when at least one criteria is satisfied, one value per batch\n        self.assertTrue(criteria(inputs[\"input_ids\"], scores))\n\n        # return False when neither is satisfied\n        self.assertFalse(criteria(inputs[\"input_ids\"][:, :-1], scores))\n\n    def test_criterias_per_row_batched(self):\n        text = [\n            \"They completed the challenging puzzle, revealing the hidden image at the end\",\n            \"Today a dragon flew over France\",\n            \"The aroma of freshly baked pizza filled the kitchen\",\n        ]\n        stop_strings = [\"end\"]\n\n        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n        tokenizer.padding_side = \"left\"\n        inputs = tokenizer(text, return_tensors=\"pt\", padding=\"longest\", add_special_tokens=False)\n\n        scores = None\n        criteria = StoppingCriteriaList(\n            [\n                MaxLengthCriteria(max_length=20),\n                StopStringCriteria(tokenizer=tokenizer, stop_strings=stop_strings),\n            ]\n        )\n\n        # trigger stopping when at least one criteria is satisfied\n        self.assertListEqual(criteria(inputs[\"input_ids\"], scores).tolist(), [True, False, False])\n\n        # False when neither is satisfied\n        self.assertListEqual(criteria(inputs[\"input_ids\"][:, :-1], scores).tolist(), [False, False, False])\n"}
{"type": "test_file", "path": "transformers/tests/generation/test_beam_search.py", "content": "# coding=utf-8\n# Copyright 2020 The HuggingFace Team Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a clone of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport unittest\n\nfrom transformers import is_torch_available\nfrom transformers.testing_utils import require_torch, torch_device\n\nfrom ..test_modeling_common import floats_tensor, ids_tensor\n\n\nif is_torch_available():\n    import torch\n\n    from transformers.generation import (\n        BeamHypotheses,\n        BeamSearchScorer,\n        ConstrainedBeamSearchScorer,\n        DisjunctiveConstraint,\n        PhrasalConstraint,\n    )\n\n\nclass BeamSearchTester:\n    def __init__(\n        self,\n        parent,\n        batch_size=3,\n        sequence_length=10,\n        vocab_size=99,\n        pad_token_id=0,\n        max_length=20,\n        num_beams=4,\n        length_penalty=2.0,\n        do_early_stopping=True,\n        num_beam_hyps_to_keep=2,\n    ):\n        self.parent = parent\n        self.batch_size = batch_size\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.pad_token_id = pad_token_id\n        self.max_length = max_length\n        self.num_beams = num_beams\n        self.length_penalty = length_penalty\n        self.do_early_stopping = do_early_stopping\n        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n\n        # cannot be randomly generated\n        self.eos_token_id = vocab_size + 1\n\n    def prepare_beam_scorer(self, **kwargs):\n        return BeamSearchScorer(\n            batch_size=kwargs.get(\"batch_size\", self.batch_size),\n            num_beams=kwargs.get(\"num_beams\", self.num_beams),\n            device=torch_device,\n            length_penalty=kwargs.get(\"length_penalty\", self.length_penalty),\n            do_early_stopping=kwargs.get(\"do_early_stopping\", self.do_early_stopping),\n            num_beam_hyps_to_keep=kwargs.get(\"num_beam_hyps_to_keep\", self.num_beam_hyps_to_keep),\n        )\n\n    def prepare_inputs(self):\n        input_ids = ids_tensor((self.batch_size * self.num_beams, self.sequence_length), self.vocab_size)\n        next_tokens = ids_tensor((self.batch_size, 2 * self.num_beams), self.vocab_size).to(torch_device)\n        next_indices = ids_tensor((self.batch_size, 2 * self.num_beams), self.num_beams).to(torch_device)\n        next_scores, _ = (-floats_tensor((self.batch_size, 2 * self.num_beams)).to(torch_device)).sort(descending=True)\n        return (input_ids, next_tokens, next_indices, next_scores)\n\n    def check_beam_hypotheses(self, input_ids, *args):\n        # check that correct number of beam hypotheses is set in beam scorer\n        beam_scorer = self.prepare_beam_scorer(do_early_stopping=True)\n        beam_hyp = beam_scorer._beam_hyps[0]\n\n        self.parent.assertEqual(len(beam_scorer._beam_hyps), self.batch_size)\n\n        # check correct type\n        self.parent.assertTrue(isinstance(beam_hyp, BeamHypotheses))\n\n        # check that num_beams is correctly set\n        self.parent.assertEqual(beam_hyp.num_beams, self.num_beams)\n\n        # check for early stopping deactivated\n        for beam_idx in range(self.num_beams):\n            beam_hyp.add(input_ids[beam_idx], -10.0)\n\n        # if early stopping True -> score does not matter\n        self.parent.assertTrue(beam_hyp.is_done(-10.0, 5))\n\n        # re-init\n        beam_scorer = self.prepare_beam_scorer(do_early_stopping=False)\n        beam_hyp = beam_scorer._beam_hyps[0]\n\n        # add `num_beams + 1` beams to change `worst_score`\n        for beam_idx in range(self.num_beams + 1):\n            beam_hyp.add(input_ids[beam_idx], -10.0 + float(beam_idx))\n\n        # -10.0 is removed => -9.0 is worst score\n        self.parent.assertAlmostEqual(beam_hyp.worst_score, -9.0 / (self.sequence_length**beam_hyp.length_penalty))\n\n        # -5.0 is better than worst score => should not be finished\n        self.parent.assertFalse(beam_hyp.is_done(-5.0, self.sequence_length))\n\n        # -20.0 is worse than worst score => should be finished\n        self.parent.assertTrue(beam_hyp.is_done(-20.0, self.sequence_length))\n\n    def check_beam_scorer_update(self, input_ids, next_tokens, next_indices, next_scores):\n        # check too many eos tokens\n        beam_scorer = self.prepare_beam_scorer()\n\n        tokens = next_tokens.clone()\n        tokens[0, :] = self.eos_token_id\n\n        with self.parent.assertRaises(ValueError):\n            beam_scorer.process(input_ids, next_scores, tokens, next_indices, eos_token_id=self.eos_token_id)\n\n        # check all batches are done\n        beam_scorer = self.prepare_beam_scorer()\n\n        tokens = next_tokens.clone()\n        tokens[:, : self.num_beams] = self.eos_token_id\n        beam_indices = torch.zeros_like(input_ids) + torch.arange(input_ids.shape[-1], device=input_ids.device)\n        beam_indices = tuple(tuple(b) for b in beam_indices)\n        beam_scorer.process(\n            input_ids, next_scores, tokens, next_indices, eos_token_id=self.eos_token_id, beam_indices=beam_indices\n        )\n        # beam scorer should be done\n        self.parent.assertTrue(beam_scorer.is_done)\n\n        # check\n        beam_scorer = self.prepare_beam_scorer()\n\n        tokens = next_tokens.clone()\n        tokens[:, 1] = self.eos_token_id\n        beam_outputs = beam_scorer.process(\n            input_ids, next_scores, tokens, next_indices, eos_token_id=self.eos_token_id, beam_indices=beam_indices\n        )\n        output_scores = beam_outputs[\"next_beam_scores\"]\n        output_tokens = beam_outputs[\"next_beam_tokens\"]\n        output_indices = beam_outputs[\"next_beam_indices\"]\n\n        def cut_expected_tensor(tensor):\n            return torch.cat([tensor[:, :1], tensor[:, 2 : self.num_beams + 1]], dim=1).flatten()\n\n        # check all outptus\n        # cut out id of eos token and take best `num_beams` outputs\n        expected_output_tokens = cut_expected_tensor(tokens)\n        expected_output_scores = cut_expected_tensor(next_scores)\n\n        # add num_beams * batch_idx\n        offset = torch.div(\n            torch.arange(self.num_beams * self.batch_size, device=torch_device), self.num_beams, rounding_mode=\"floor\"\n        )\n        expected_output_indices = cut_expected_tensor(next_indices) + offset * self.num_beams\n\n        self.parent.assertListEqual(expected_output_tokens.tolist(), output_tokens.tolist())\n        self.parent.assertListEqual(expected_output_indices.tolist(), output_indices.tolist())\n        self.parent.assertTrue(torch.allclose(expected_output_scores, output_scores, atol=1e-3))\n\n        # make sure ids of eos token are correctly saved in beam_hyps of beam scorer\n        expected_beam_indices = list(range(10))\n        for batch_idx in range(self.batch_size):\n            correct_idx = batch_idx * self.num_beams + next_indices[batch_idx, 1]\n            self.parent.assertListEqual(\n                input_ids[correct_idx].tolist(), beam_scorer._beam_hyps[batch_idx].beams[0][1].tolist()\n            )\n            self.parent.assertListEqual(\n                expected_beam_indices + [correct_idx],\n                torch.tensor(beam_scorer._beam_hyps[batch_idx].beams[0][2]).tolist(),\n            )\n\n    def check_beam_scores_finalize(self, input_ids, next_tokens, next_indices, next_scores):\n        # max_length should be only one more than current input_ids to check that eos is correctly appended\n        max_length = self.sequence_length + 1\n        beam_scorer = self.prepare_beam_scorer(num_beam_hyps_to_keep=1, length_penalty=1.0, do_early_stopping=False)\n\n        # update beams and append to input_ids\n        tokens = next_tokens.clone()\n        # first batch, first output has to finish with eos token id since scores are correctly sorted\n        tokens[0, 0] = self.eos_token_id\n        # make sure corresponding score is as good as possible to surely be picked first\n        next_scores[0, 0] = 0.0\n        beam_outputs = beam_scorer.process(\n            input_ids, next_scores, tokens, next_indices, eos_token_id=self.eos_token_id\n        )\n        output_scores = beam_outputs[\"next_beam_scores\"]\n        output_tokens = beam_outputs[\"next_beam_tokens\"]\n        output_indices = beam_outputs[\"next_beam_indices\"]\n\n        input_ids = torch.cat([input_ids[output_indices, :], output_tokens.unsqueeze(-1)], dim=-1)\n\n        # finalize\n        beam_indices = torch.zeros_like(input_ids) + torch.arange(input_ids.shape[-1], device=input_ids.device)\n        beam_indices = tuple(tuple(b) for b in beam_indices)\n        sequence_output = beam_scorer.finalize(\n            input_ids,\n            output_scores,\n            output_tokens,\n            output_indices,\n            pad_token_id=self.pad_token_id,\n            eos_token_id=self.eos_token_id,\n            max_length=max_length,\n            beam_indices=beam_indices,\n        )\n\n        sequences = sequence_output[\"sequences\"]\n        sequence_scores = sequence_output[\"sequence_scores\"]\n\n        # since `num_beam_hyps_to_keep` = 1 => only return `batch_size` x `max_length`\n        self.parent.assertListEqual(list(sequences.shape), [self.batch_size, max_length])\n        self.parent.assertListEqual(list(sequence_scores.shape), [self.batch_size])\n\n        # check sequence_scores\n        self.parent.assertFalse((sequence_scores > 0).any().item())\n\n        # first batch has to finish with eos_token\n        self.parent.assertEqual(sequences[0, -1].item(), self.eos_token_id)\n\n        # other batches cannot finish with eos token\n        self.parent.assertNotEqual(sequences[1, -1].item(), self.eos_token_id)\n        self.parent.assertNotEqual(sequences[2, -1].item(), self.eos_token_id)\n\n        # now test that if `num_beam_hyps_to_keep` is 3 => all beams are returned\n        beam_scorer.num_beam_hyps_to_keep = self.num_beams\n        sequence_output = beam_scorer.finalize(\n            input_ids,\n            output_scores,\n            output_tokens,\n            output_indices,\n            pad_token_id=self.pad_token_id,\n            eos_token_id=self.eos_token_id,\n            max_length=max_length,\n            beam_indices=beam_indices,\n        )\n        sequences = sequence_output[\"sequences\"]\n        sequence_scores = sequence_output[\"sequence_scores\"]\n\n        self.parent.assertListEqual(list(sequences.shape), [self.num_beams * self.batch_size, max_length])\n        self.parent.assertListEqual(list(sequence_scores.shape), [self.num_beams * self.batch_size])\n\n\nclass ConstrainedBeamSearchTester:\n    def __init__(\n        self,\n        parent,\n        constraints=None,\n        batch_size=3,\n        sequence_length=10,\n        vocab_size=99,\n        pad_token_id=0,\n        max_length=20,\n        num_beams=4,\n        length_penalty=2.0,\n        do_early_stopping=True,\n        num_beam_hyps_to_keep=2,\n    ):\n        self.parent = parent\n        self.batch_size = batch_size\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.pad_token_id = pad_token_id\n        self.max_length = max_length\n        self.num_beams = num_beams\n        self.length_penalty = length_penalty\n        self.do_early_stopping = do_early_stopping\n        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n\n        if constraints is None:\n            force_tokens = torch.randint(10, 50, (1, 2))[0].tolist()\n            disjunctive_tokens = torch.randint(10, 50, (2, 2)).tolist()\n\n            constraints = [PhrasalConstraint(force_tokens), DisjunctiveConstraint(disjunctive_tokens)]\n            self.constraints = constraints\n        # cannot be randomly generated\n        self.eos_token_id = vocab_size + 1\n\n    def prepare_constrained_beam_scorer(self, **kwargs):\n        return ConstrainedBeamSearchScorer(\n            constraints=kwargs.get(\"constraints\", self.constraints),\n            batch_size=kwargs.get(\"batch_size\", self.batch_size),\n            num_beams=kwargs.get(\"num_beams\", self.num_beams),\n            device=torch_device,\n            length_penalty=kwargs.get(\"length_penalty\", self.length_penalty),\n            do_early_stopping=kwargs.get(\"do_early_stopping\", self.do_early_stopping),\n            num_beam_hyps_to_keep=kwargs.get(\"num_beam_hyps_to_keep\", self.num_beam_hyps_to_keep),\n        )\n\n    def prepare_inputs(self):\n        input_ids = ids_tensor((self.batch_size * self.num_beams, self.sequence_length), self.vocab_size)\n        next_tokens = ids_tensor((self.batch_size, 2 * self.num_beams), self.vocab_size).to(torch_device)\n        next_indices = ids_tensor((self.batch_size, 2 * self.num_beams), self.num_beams).to(torch_device)\n        next_scores, _ = (-floats_tensor((self.batch_size, 2 * self.num_beams)).to(torch_device)).sort(descending=True)\n        scores_for_all_vocab, _ = (\n            -floats_tensor((self.batch_size * self.num_beams, self.vocab_size)).to(torch_device)\n        ).sort(descending=True)\n        return (input_ids, next_tokens, next_indices, next_scores, scores_for_all_vocab)\n\n    def check_beam_hypotheses(self, input_ids, *args):\n        # check that correct number of beam hypotheses is set in beam scorer\n        constrained_beam_scorer = self.prepare_constrained_beam_scorer(do_early_stopping=True)\n        beam_hyp = constrained_beam_scorer._beam_hyps[0]\n\n        self.parent.assertEqual(len(constrained_beam_scorer._beam_hyps), self.batch_size)\n\n        # check correct type\n        self.parent.assertTrue(isinstance(beam_hyp, BeamHypotheses))\n\n        # check that num_beams is correctly set\n        self.parent.assertEqual(beam_hyp.num_beams, self.num_beams)\n\n        # check for early stopping deactivated\n        for beam_idx in range(self.num_beams):\n            beam_hyp.add(input_ids[beam_idx], -10.0)\n\n        # if early stopping True -> score does not matter\n        self.parent.assertTrue(beam_hyp.is_done(-10.0, 5))\n\n        # re-init\n        constrained_beam_scorer = self.prepare_constrained_beam_scorer(do_early_stopping=False)\n        beam_hyp = constrained_beam_scorer._beam_hyps[0]\n\n        # add `num_beams + 1` beams to change `worst_score`\n        for beam_idx in range(self.num_beams + 1):\n            beam_hyp.add(input_ids[beam_idx], -10.0 + float(beam_idx))\n\n        # -10.0 is removed => -9.0 is worst score\n        self.parent.assertAlmostEqual(beam_hyp.worst_score, -9.0 / (self.sequence_length**beam_hyp.length_penalty))\n\n        # -5.0 is better than worst score => should not be finished\n        self.parent.assertFalse(beam_hyp.is_done(-5.0, self.sequence_length))\n\n        # -20.0 is worse than worst score => should be finished\n        self.parent.assertTrue(beam_hyp.is_done(-20.0, self.sequence_length))\n\n    def check_constrained_beam_scorer_update(\n        self, input_ids, next_tokens, next_indices, next_scores, scores_for_all_vocab\n    ):\n        # check too many eos tokens\n        constrained_beam_scorer = self.prepare_constrained_beam_scorer()\n        stacked_token_ids = []\n        for constraint in self.constraints:\n            token_ids = constraint.token_ids\n            token_ids = token_ids[0] if isinstance(token_ids[0], list) else token_ids\n            stacked_token_ids = stacked_token_ids + token_ids\n\n        fulfilling_sequence = torch.LongTensor(stacked_token_ids)\n        fulfill_len = fulfilling_sequence.size(0)\n        input_ids[:, :fulfill_len] = fulfilling_sequence\n\n        tokens = next_tokens.clone()\n        tokens[0, :] = self.eos_token_id\n\n        with self.parent.assertRaises(ValueError):\n            constrained_beam_scorer.process(\n                input_ids, next_scores, tokens, next_indices, scores_for_all_vocab, eos_token_id=self.eos_token_id\n            )\n\n        # check all batches are done\n        constrained_beam_scorer = self.prepare_constrained_beam_scorer()\n\n        tokens = next_tokens.clone()\n        tokens[:, : self.num_beams] = self.eos_token_id\n        constrained_beam_scorer.process(\n            input_ids, next_scores, tokens, next_indices, scores_for_all_vocab, eos_token_id=self.eos_token_id\n        )\n        # beam scorer should be done\n        self.parent.assertTrue(constrained_beam_scorer.is_done)\n\n        # check\n        constrained_beam_scorer = self.prepare_constrained_beam_scorer()\n\n        tokens = next_tokens.clone()\n        tokens[:, 1] = self.eos_token_id\n        beam_outputs = constrained_beam_scorer.process(\n            input_ids, next_scores, tokens, next_indices, scores_for_all_vocab, eos_token_id=self.eos_token_id\n        )\n        output_scores = beam_outputs[\"next_beam_scores\"]\n        output_tokens = beam_outputs[\"next_beam_tokens\"]\n        output_indices = beam_outputs[\"next_beam_indices\"]\n\n        def cut_expected_tensor(tensor):\n            return torch.cat([tensor[:, :1], tensor[:, 2 : self.num_beams + 1]], dim=1).flatten()\n\n        # check all outptus\n        # cut out id of eos token and take best `num_beams` outputs\n        expected_output_tokens = cut_expected_tensor(tokens)\n        expected_output_scores = cut_expected_tensor(next_scores)\n\n        # add num_beams * batch_idx\n        offset = torch.div(\n            torch.arange(self.num_beams * self.batch_size, device=torch_device), self.num_beams, rounding_mode=\"floor\"\n        )\n        expected_output_indices = cut_expected_tensor(next_indices) + offset * self.num_beams\n\n        self.parent.assertListEqual(expected_output_tokens.tolist(), output_tokens.tolist())\n        self.parent.assertListEqual(expected_output_indices.tolist(), output_indices.tolist())\n        self.parent.assertTrue(torch.allclose(expected_output_scores, output_scores, atol=1e-3))\n\n        # make sure ids of eos token are correctly saved in beam_hyps of beam scorer\n        for batch_idx in range(self.batch_size):\n            correct_idx = batch_idx * self.num_beams + next_indices[batch_idx, 1]\n            self.parent.assertListEqual(\n                input_ids[correct_idx].tolist(), constrained_beam_scorer._beam_hyps[batch_idx].beams[0][1].tolist()\n            )\n\n    def check_constrained_beam_scorer_finalize(\n        self, input_ids, next_tokens, next_indices, next_scores, scores_for_all_vocab\n    ):\n        # max_length should be only one more than current input_ids to check that eos is correctly appended\n        max_length = self.sequence_length + 1\n\n        # for testing finalize, we do want to have fulfilled constraints\n        stacked_token_ids = []\n        for constraint in self.constraints:\n            token_ids = constraint.token_ids\n            token_ids = token_ids[0] if isinstance(token_ids[0], list) else token_ids\n            stacked_token_ids = stacked_token_ids + token_ids\n\n        fulfilling_sequence = torch.LongTensor(stacked_token_ids)\n\n        fulfill_len = fulfilling_sequence.size(0)\n        input_ids[:, :fulfill_len] = fulfilling_sequence\n\n        constrained_beam_scorer = self.prepare_constrained_beam_scorer(\n            num_beam_hyps_to_keep=1, length_penalty=1.0, do_early_stopping=False\n        )\n\n        constraints = constrained_beam_scorer.constraints\n        # update beams and append to input_ids\n        tokens = next_tokens.clone()\n        # first batch, first output has to finish with eos token id since scores are correctly sorted\n        tokens[0, 0] = self.eos_token_id\n        # make sure corresponding score is as good as possible to surely be picked first\n        next_scores[0, 0] = 0.0\n\n        beam_outputs = constrained_beam_scorer.process(\n            input_ids, next_scores, tokens, next_indices, scores_for_all_vocab, eos_token_id=self.eos_token_id\n        )\n        output_scores = beam_outputs[\"next_beam_scores\"]\n        output_tokens = beam_outputs[\"next_beam_tokens\"]\n        output_indices = beam_outputs[\"next_beam_indices\"]\n        input_ids = torch.cat([input_ids[output_indices, :], output_tokens.unsqueeze(-1)], dim=-1)\n\n        # finalize\n        sequence_output = constrained_beam_scorer.finalize(\n            input_ids,\n            output_scores,\n            output_tokens,\n            output_indices,\n            pad_token_id=self.pad_token_id,\n            eos_token_id=self.eos_token_id,\n            max_length=max_length,\n        )\n\n        sequences = sequence_output[\"sequences\"]\n        sequence_scores = sequence_output[\"sequence_scores\"]\n\n        # since `num_beam_hyps_to_keep` = 1 => only return `batch_size` x `max_length`\n        self.parent.assertListEqual(list(sequences.shape), [self.batch_size, max_length])\n        self.parent.assertListEqual(list(sequence_scores.shape), [self.batch_size])\n\n        # check sequence_scores\n        self.parent.assertFalse((sequence_scores > 0).any().item())\n\n        # first batch has to finish with eos_token\n        self.parent.assertEqual(sequences[0, -1].item(), self.eos_token_id)\n\n        # other batches cannot finish with eos token\n        self.parent.assertNotEqual(sequences[1, -1].item(), self.eos_token_id)\n        self.parent.assertNotEqual(sequences[2, -1].item(), self.eos_token_id)\n\n        # test that the constraint is indeed fulfilled\n        for output, constraint in [(s, c) for s in sequences for c in constraints]:\n            forced_token_ids = constraint.token_ids\n            if isinstance(forced_token_ids[0], list):\n                # disjunctive case\n                flag = False\n                for token_ids in forced_token_ids:\n                    if self._check_sequence_inside_sequence(output, token_ids):\n                        flag = True\n                        break\n                self.parent.assertEqual(flag, True)\n            else:\n                self.parent.assertEqual(self._check_sequence_inside_sequence(output, forced_token_ids), True)\n\n        # now test that if `num_beam_hyps_to_keep` is 3 => all beams are returned\n\n        # constrained_beam_scorer.num_beam_hyps_to_keep = self.num_beams\n        constrained_beam_scorer = self.prepare_constrained_beam_scorer(\n            num_beam_hyps_to_keep=self.num_beams, length_penalty=1.0, do_early_stopping=False\n        )\n\n        sequence_output = constrained_beam_scorer.finalize(\n            input_ids,\n            output_scores,\n            output_tokens,\n            output_indices,\n            pad_token_id=self.pad_token_id,\n            eos_token_id=self.eos_token_id,\n            max_length=max_length,\n        )\n        sequences = sequence_output[\"sequences\"]\n        sequence_scores = sequence_output[\"sequence_scores\"]\n\n        self.parent.assertListEqual(list(sequences.shape), [self.num_beams * self.batch_size, max_length])\n        self.parent.assertListEqual(list(sequence_scores.shape), [self.num_beams * self.batch_size])\n\n    def _check_sequence_inside_sequence(self, tensor_1, tensor_2):\n        # check if tensor_1 inside tensor_2 or tensor_2 inside tensor_1.\n        # set to same device. we don't care what device.\n\n        if not isinstance(tensor_1, list):\n            tensor_1 = tensor_1.cpu().tolist()\n        if not isinstance(tensor_2, list):\n            tensor_2 = tensor_2.cpu().tolist()\n\n        in_order = len(tensor_1) <= len(tensor_2)\n        longer = tensor_2 if in_order else tensor_1\n        shorter = tensor_1 if in_order else tensor_2\n\n        flag = False\n        chunk_size = len(shorter)\n        for chunk_idx in range(len(longer) - chunk_size + 1):\n            subseq = longer[chunk_idx : chunk_idx + chunk_size]\n            if subseq == shorter:\n                flag = True\n                break\n\n        return flag\n\n\n@require_torch\nclass BeamSearchTest(unittest.TestCase):\n    def setUp(self):\n        self.beam_search_tester = BeamSearchTester(self)\n\n    def test_beam_hypotheses(self):\n        inputs = self.beam_search_tester.prepare_inputs()\n        self.beam_search_tester.check_beam_hypotheses(*inputs)\n\n    def test_beam_scorer_update(self):\n        inputs = self.beam_search_tester.prepare_inputs()\n        self.beam_search_tester.check_beam_scorer_update(*inputs)\n\n    def test_beam_scorer_finalize(self):\n        inputs = self.beam_search_tester.prepare_inputs()\n        self.beam_search_tester.check_beam_scores_finalize(*inputs)\n\n\n@require_torch\nclass ConstrainedBeamSearchTest(unittest.TestCase):\n    def setUp(self):\n        self.constrained_beam_search_tester = ConstrainedBeamSearchTester(self)\n\n    def test_constrained_beam_hypotheses(self):\n        inputs = self.constrained_beam_search_tester.prepare_inputs()\n        self.constrained_beam_search_tester.check_beam_hypotheses(*inputs)\n\n    def test_constrained_beam_scorer_update(self):\n        inputs = self.constrained_beam_search_tester.prepare_inputs()\n        self.constrained_beam_search_tester.check_constrained_beam_scorer_update(*inputs)\n\n    def test_constrained_beam_scorer_finalize(self):\n        inputs = self.constrained_beam_search_tester.prepare_inputs()\n        self.constrained_beam_search_tester.check_constrained_beam_scorer_finalize(*inputs)\n"}
{"type": "test_file", "path": "transformers/tests/models/align/__init__.py", "content": ""}
{"type": "test_file", "path": "transformers/tests/models/altclip/__init__.py", "content": ""}
{"type": "test_file", "path": "transformers/tests/models/align/test_processor_align.py", "content": "# Copyright 2023 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nimport shutil\nimport tempfile\nimport unittest\n\nimport pytest\n\nfrom transformers import BertTokenizer, BertTokenizerFast\nfrom transformers.models.bert.tokenization_bert import VOCAB_FILES_NAMES\nfrom transformers.testing_utils import require_vision\nfrom transformers.utils import IMAGE_PROCESSOR_NAME, is_vision_available\n\nfrom ...test_processing_common import ProcessorTesterMixin\n\n\nif is_vision_available():\n    from transformers import AlignProcessor, EfficientNetImageProcessor\n\n\n@require_vision\nclass AlignProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n    processor_class = AlignProcessor\n\n    def setUp(self):\n        self.tmpdirname = tempfile.mkdtemp()\n\n        vocab_tokens = [\n            \"[UNK]\",\n            \"[CLS]\",\n            \"[SEP]\",\n            \"[PAD]\",\n            \"[MASK]\",\n            \"want\",\n            \"##want\",\n            \"##ed\",\n            \"wa\",\n            \"un\",\n            \"runn\",\n            \"##ing\",\n            \",\",\n            \"low\",\n            \"lowest\",\n        ]\n        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n            vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n\n        image_processor_map = {\n            \"do_resize\": True,\n            \"size\": 20,\n            \"do_normalize\": True,\n            \"image_mean\": [0.48145466, 0.4578275, 0.40821073],\n            \"image_std\": [0.26862954, 0.26130258, 0.27577711],\n        }\n        self.image_processor_file = os.path.join(self.tmpdirname, IMAGE_PROCESSOR_NAME)\n        with open(self.image_processor_file, \"w\", encoding=\"utf-8\") as fp:\n            json.dump(image_processor_map, fp)\n\n    def get_tokenizer(self, **kwargs):\n        return BertTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n\n    def get_rust_tokenizer(self, **kwargs):\n        return BertTokenizerFast.from_pretrained(self.tmpdirname, **kwargs)\n\n    def get_image_processor(self, **kwargs):\n        return EfficientNetImageProcessor.from_pretrained(self.tmpdirname, **kwargs)\n\n    def tearDown(self):\n        shutil.rmtree(self.tmpdirname)\n\n    def test_save_load_pretrained_default(self):\n        tokenizer_slow = self.get_tokenizer()\n        tokenizer_fast = self.get_rust_tokenizer()\n        image_processor = self.get_image_processor()\n\n        processor_slow = AlignProcessor(tokenizer=tokenizer_slow, image_processor=image_processor)\n        processor_slow.save_pretrained(self.tmpdirname)\n        processor_slow = AlignProcessor.from_pretrained(self.tmpdirname, use_fast=False)\n\n        processor_fast = AlignProcessor(tokenizer=tokenizer_fast, image_processor=image_processor)\n        processor_fast.save_pretrained(self.tmpdirname)\n        processor_fast = AlignProcessor.from_pretrained(self.tmpdirname)\n\n        self.assertEqual(processor_slow.tokenizer.get_vocab(), tokenizer_slow.get_vocab())\n        self.assertEqual(processor_fast.tokenizer.get_vocab(), tokenizer_fast.get_vocab())\n        self.assertEqual(tokenizer_slow.get_vocab(), tokenizer_fast.get_vocab())\n        self.assertIsInstance(processor_slow.tokenizer, BertTokenizer)\n        self.assertIsInstance(processor_fast.tokenizer, BertTokenizerFast)\n\n        self.assertEqual(processor_slow.image_processor.to_json_string(), image_processor.to_json_string())\n        self.assertEqual(processor_fast.image_processor.to_json_string(), image_processor.to_json_string())\n        self.assertIsInstance(processor_slow.image_processor, EfficientNetImageProcessor)\n        self.assertIsInstance(processor_fast.image_processor, EfficientNetImageProcessor)\n\n    def test_save_load_pretrained_additional_features(self):\n        processor = AlignProcessor(tokenizer=self.get_tokenizer(), image_processor=self.get_image_processor())\n        processor.save_pretrained(self.tmpdirname)\n\n        tokenizer_add_kwargs = self.get_tokenizer(bos_token=\"(BOS)\", eos_token=\"(EOS)\")\n        image_processor_add_kwargs = self.get_image_processor(do_normalize=False, padding_value=1.0)\n\n        processor = AlignProcessor.from_pretrained(\n            self.tmpdirname, bos_token=\"(BOS)\", eos_token=\"(EOS)\", do_normalize=False, padding_value=1.0\n        )\n\n        self.assertEqual(processor.tokenizer.get_vocab(), tokenizer_add_kwargs.get_vocab())\n        self.assertIsInstance(processor.tokenizer, BertTokenizerFast)\n\n        self.assertEqual(processor.image_processor.to_json_string(), image_processor_add_kwargs.to_json_string())\n        self.assertIsInstance(processor.image_processor, EfficientNetImageProcessor)\n\n    def test_image_processor(self):\n        image_processor = self.get_image_processor()\n        tokenizer = self.get_tokenizer()\n\n        processor = AlignProcessor(tokenizer=tokenizer, image_processor=image_processor)\n\n        image_input = self.prepare_image_inputs()\n\n        input_image_proc = image_processor(image_input, return_tensors=\"np\")\n        input_processor = processor(images=image_input, return_tensors=\"np\")\n\n        for key in input_image_proc.keys():\n            self.assertAlmostEqual(input_image_proc[key].sum(), input_processor[key].sum(), delta=1e-2)\n\n    def test_tokenizer(self):\n        image_processor = self.get_image_processor()\n        tokenizer = self.get_tokenizer()\n\n        processor = AlignProcessor(tokenizer=tokenizer, image_processor=image_processor)\n\n        input_str = \"lower newer\"\n\n        encoded_processor = processor(text=input_str)\n\n        encoded_tok = tokenizer(input_str, padding=\"max_length\", max_length=64)\n        for key in encoded_tok.keys():\n            self.assertListEqual(encoded_tok[key], encoded_processor[key])\n\n    def test_processor(self):\n        image_processor = self.get_image_processor()\n        tokenizer = self.get_tokenizer()\n\n        processor = AlignProcessor(tokenizer=tokenizer, image_processor=image_processor)\n\n        input_str = \"lower newer\"\n        image_input = self.prepare_image_inputs()\n\n        inputs = processor(text=input_str, images=image_input)\n\n        self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"token_type_ids\", \"attention_mask\", \"pixel_values\"])\n\n        # test if it raises when no input is passed\n        with pytest.raises(ValueError):\n            processor()\n\n    def test_tokenizer_decode(self):\n        image_processor = self.get_image_processor()\n        tokenizer = self.get_tokenizer()\n\n        processor = AlignProcessor(tokenizer=tokenizer, image_processor=image_processor)\n\n        predicted_ids = [[1, 4, 5, 8, 1, 0, 8], [3, 4, 3, 1, 1, 8, 9]]\n\n        decoded_processor = processor.batch_decode(predicted_ids)\n        decoded_tok = tokenizer.batch_decode(predicted_ids)\n\n        self.assertListEqual(decoded_tok, decoded_processor)\n\n    def test_model_input_names(self):\n        image_processor = self.get_image_processor()\n        tokenizer = self.get_tokenizer()\n\n        processor = AlignProcessor(tokenizer=tokenizer, image_processor=image_processor)\n\n        input_str = \"lower newer\"\n        image_input = self.prepare_image_inputs()\n\n        inputs = processor(text=input_str, images=image_input)\n\n        self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n"}
{"type": "test_file", "path": "transformers/tests/models/altclip/test_modeling_altclip.py", "content": "# coding=utf-8\n# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Testing suite for the PyTorch AltCLIP model.\"\"\"\n\nimport inspect\nimport os\nimport tempfile\nimport unittest\n\nimport numpy as np\nimport requests\n\nfrom transformers import AltCLIPConfig, AltCLIPProcessor, AltCLIPTextConfig, AltCLIPVisionConfig\nfrom transformers.testing_utils import require_torch, require_vision, slow, torch_device\nfrom transformers.utils import is_torch_available, is_vision_available\n\nfrom ...test_configuration_common import ConfigTester\nfrom ...test_modeling_common import (\n    ModelTesterMixin,\n    _config_zero_init,\n    floats_tensor,\n    ids_tensor,\n    random_attention_mask,\n)\nfrom ...test_pipeline_mixin import PipelineTesterMixin\n\n\nif is_torch_available():\n    import torch\n    import torch.nn as nn\n\n    from transformers import AltCLIPModel, AltCLIPTextModel, AltCLIPVisionModel\n\nif is_vision_available():\n    from PIL import Image\n\n\nclass AltCLIPVisionModelTester:\n    def __init__(\n        self,\n        parent,\n        batch_size=12,\n        image_size=30,\n        patch_size=2,\n        num_channels=3,\n        is_training=True,\n        hidden_size=32,\n        projection_dim=32,\n        num_hidden_layers=2,\n        num_attention_heads=4,\n        intermediate_size=37,\n        dropout=0.1,\n        attention_dropout=0.1,\n        initializer_range=0.02,\n        scope=None,\n    ):\n        self.parent = parent\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.num_channels = num_channels\n        self.is_training = is_training\n        self.hidden_size = hidden_size\n        self.projection_dim = projection_dim\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.dropout = dropout\n        self.attention_dropout = attention_dropout\n        self.initializer_range = initializer_range\n        self.scope = scope\n\n        # in ViT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n        num_patches = (image_size // patch_size) ** 2\n        self.seq_length = num_patches + 1\n\n    def prepare_config_and_inputs(self):\n        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n        config = self.get_config()\n\n        return config, pixel_values\n\n    def get_config(self):\n        return AltCLIPVisionConfig(\n            image_size=self.image_size,\n            patch_size=self.patch_size,\n            num_channels=self.num_channels,\n            hidden_size=self.hidden_size,\n            projection_dim=self.projection_dim,\n            num_hidden_layers=self.num_hidden_layers,\n            num_attention_heads=self.num_attention_heads,\n            intermediate_size=self.intermediate_size,\n            dropout=self.dropout,\n            attention_dropout=self.attention_dropout,\n            initializer_range=self.initializer_range,\n        )\n\n    def create_and_check_model(self, config, pixel_values):\n        model = AltCLIPVisionModel(config=config)\n        model.to(torch_device)\n        model.eval()\n        with torch.no_grad():\n            result = model(pixel_values)\n        # expected sequence length = num_patches + 1 (we add 1 for the [CLS] token)\n        image_size = (self.image_size, self.image_size)\n        patch_size = (self.patch_size, self.patch_size)\n        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, num_patches + 1, self.hidden_size))\n        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n\n    def prepare_config_and_inputs_for_common(self):\n        config_and_inputs = self.prepare_config_and_inputs()\n        config, pixel_values = config_and_inputs\n        inputs_dict = {\"pixel_values\": pixel_values}\n        return config, inputs_dict\n\n\n@require_torch\nclass AltCLIPVisionModelTest(ModelTesterMixin, unittest.TestCase):\n    \"\"\"\n    Here we also overwrite some of the tests of test_modeling_common.py, as CLIP does not use input_ids, inputs_embeds,\n    attention_mask and seq_length.\n    \"\"\"\n\n    all_model_classes = (AltCLIPVisionModel,) if is_torch_available() else ()\n    fx_compatible = False\n    test_pruning = False\n    test_resize_embeddings = False\n    test_head_masking = False\n\n    def setUp(self):\n        self.model_tester = AltCLIPVisionModelTester(self)\n        self.config_tester = ConfigTester(\n            self, config_class=AltCLIPVisionConfig, has_text_modality=False, hidden_size=37\n        )\n\n    def test_config(self):\n        self.config_tester.run_common_tests()\n\n    @unittest.skip(reason=\"CLIP does not use inputs_embeds\")\n    def test_inputs_embeds(self):\n        pass\n\n    def test_model_get_set_embeddings(self):\n        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n\n        for model_class in self.all_model_classes:\n            model = model_class(config)\n            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n            x = model.get_output_embeddings()\n            self.assertTrue(x is None or isinstance(x, nn.Linear))\n\n    def test_forward_signature(self):\n        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n\n        for model_class in self.all_model_classes:\n            model = model_class(config)\n            signature = inspect.signature(model.forward)\n            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n            arg_names = [*signature.parameters.keys()]\n\n            expected_arg_names = [\"pixel_values\"]\n            self.assertListEqual(arg_names[:1], expected_arg_names)\n\n    def test_model(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_model(*config_and_inputs)\n\n    @unittest.skip\n    def test_training(self):\n        pass\n\n    @unittest.skip\n    def test_training_gradient_checkpointing(self):\n        pass\n\n    @unittest.skip(\n        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n    )\n    def test_training_gradient_checkpointing_use_reentrant(self):\n        pass\n\n    @unittest.skip(\n        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n    )\n    def test_training_gradient_checkpointing_use_reentrant_false(self):\n        pass\n\n    @unittest.skip(reason=\"AltCLIPVisionModel has no base class and is not available in MODEL_MAPPING\")\n    def test_save_load_fast_init_from_base(self):\n        pass\n\n    @unittest.skip(reason=\"AltCLIPVisionModel has no base class and is not available in MODEL_MAPPING\")\n    def test_save_load_fast_init_to_base(self):\n        pass\n\n    @unittest.skip(reason=\"AltCLIPVisionModel use the same cv backbone with CLIP model.\")\n    def test_model_from_pretrained(self):\n        pass\n\n\nclass AltCLIPTextModelTester:\n    def __init__(\n        self,\n        parent,\n        batch_size=12,\n        seq_length=7,\n        is_training=True,\n        use_input_mask=True,\n        use_labels=True,\n        vocab_size=99,\n        hidden_size=32,\n        projection_dim=32,\n        project_dim=32,\n        num_hidden_layers=2,\n        num_attention_heads=4,\n        intermediate_size=37,\n        dropout=0.1,\n        attention_dropout=0.1,\n        max_position_embeddings=512,\n        initializer_range=0.02,\n        scope=None,\n    ):\n        self.parent = parent\n        self.batch_size = batch_size\n        self.seq_length = seq_length\n        self.is_training = is_training\n        self.use_input_mask = use_input_mask\n        self.use_labels = use_labels\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.projection_dim = projection_dim\n        self.project_dim = project_dim\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.dropout = dropout\n        self.attention_dropout = attention_dropout\n        self.max_position_embeddings = max_position_embeddings\n        self.initializer_range = initializer_range\n        self.scope = scope\n\n    def prepare_config_and_inputs(self):\n        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n\n        input_mask = None\n        if self.use_input_mask:\n            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n\n        if input_mask is not None:\n            batch_size, seq_length = input_mask.shape\n            rnd_start_indices = np.random.randint(1, seq_length - 1, size=(batch_size,))\n            for batch_idx, start_index in enumerate(rnd_start_indices):\n                input_mask[batch_idx, :start_index] = 1\n                input_mask[batch_idx, start_index:] = 0\n\n        config = self.get_config()\n\n        return config, input_ids, input_mask\n\n    def get_config(self):\n        return AltCLIPTextConfig(\n            vocab_size=self.vocab_size,\n            hidden_size=self.hidden_size,\n            projection_dim=self.projection_dim,\n            project_dim=self.project_dim,\n            num_hidden_layers=self.num_hidden_layers,\n            num_attention_heads=self.num_attention_heads,\n            intermediate_size=self.intermediate_size,\n            dropout=self.dropout,\n            attention_dropout=self.attention_dropout,\n            max_position_embeddings=self.max_position_embeddings,\n            initializer_range=self.initializer_range,\n            pad_token_id=1,\n        )\n\n    def create_and_check_model(self, config, input_ids, input_mask):\n        model = AltCLIPTextModel(config=config)\n        model.to(torch_device)\n        model.eval()\n        with torch.no_grad():\n            result = model(input_ids, attention_mask=input_mask)\n            result = model(input_ids)\n        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.projection_dim))\n\n    def prepare_config_and_inputs_for_common(self):\n        config_and_inputs = self.prepare_config_and_inputs()\n        config, input_ids, input_mask = config_and_inputs\n        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n        return config, inputs_dict\n\n\n@require_torch\nclass AltCLIPTextModelTest(ModelTesterMixin, unittest.TestCase):\n    all_model_classes = (AltCLIPTextModel,) if is_torch_available() else ()\n    fx_compatible = True\n    test_pruning = False\n    test_head_masking = False\n\n    # TODO (@SunMarc): Fix me\n    @unittest.skip(reason=\"It's broken.\")\n    def test_resize_tokens_embeddings(self):\n        super().test_resize_tokens_embeddings()\n\n    def setUp(self):\n        self.model_tester = AltCLIPTextModelTester(self)\n        self.config_tester = ConfigTester(self, config_class=AltCLIPTextConfig, hidden_size=37)\n\n    def test_config(self):\n        self.config_tester.run_common_tests()\n\n    def test_model(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_model(*config_and_inputs)\n\n    @unittest.skip\n    def test_training(self):\n        pass\n\n    @unittest.skip\n    def test_training_gradient_checkpointing(self):\n        pass\n\n    @unittest.skip(\n        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n    )\n    def test_training_gradient_checkpointing_use_reentrant(self):\n        pass\n\n    @unittest.skip(\n        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n    )\n    def test_training_gradient_checkpointing_use_reentrant_false(self):\n        pass\n\n    def test_model_outputs_equivalence(self):\n        pass\n\n    @unittest.skip(reason=\"Result of the model is a dict\")\n    def test_hidden_states_output(self):\n        pass\n\n    @unittest.skip(reason=\"AltCLIP does not use inputs_embeds\")\n    def test_inputs_embeds(self):\n        pass\n\n    @unittest.skip(reason=\"AltCLIPTextModel has no base class and is not available in MODEL_MAPPING\")\n    def test_save_load_fast_init_from_base(self):\n        pass\n\n    @unittest.skip(reason=\"AltCLIPTextModel has no base class and is not available in MODEL_MAPPING\")\n    def test_save_load_fast_init_to_base(self):\n        pass\n\n    @slow\n    def test_model_from_pretrained(self):\n        model_name = \"BAAI/AltCLIP\"\n        model = AltCLIPTextModel.from_pretrained(model_name)\n        self.assertIsNotNone(model)\n\n\nclass AltCLIPModelTester:\n    def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):\n        if text_kwargs is None:\n            text_kwargs = {}\n        if vision_kwargs is None:\n            vision_kwargs = {}\n\n        self.parent = parent\n        self.text_model_tester = AltCLIPTextModelTester(parent, **text_kwargs)\n        self.vision_model_tester = AltCLIPVisionModelTester(parent, **vision_kwargs)\n        self.batch_size = self.text_model_tester.batch_size  # need bs for batching_equivalence test\n        self.is_training = is_training\n\n    def prepare_config_and_inputs(self):\n        text_config, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n        vision_config, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n\n        config = self.get_config()\n        return config, input_ids, attention_mask, pixel_values\n\n    def get_config(self):\n        return AltCLIPConfig.from_text_vision_configs(\n            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n        )\n\n    def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n        model = AltCLIPModel(config=config)\n        model.to(torch_device)\n        model.eval()\n\n        with torch.no_grad():\n            model(input_ids, pixel_values, attention_mask)\n\n    def prepare_config_and_inputs_for_common(self):\n        config_and_inputs = self.prepare_config_and_inputs()\n        config, input_ids, attention_mask, pixel_values = config_and_inputs\n        inputs_dict = {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"pixel_values\": pixel_values,\n            \"return_loss\": True,\n        }\n        return config, inputs_dict\n\n\n# We will verify our results on an image of cute cats\ndef prepare_img():\n    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n    im = Image.open(requests.get(url, stream=True).raw)\n    return im\n\n\n@require_torch\nclass AltCLIPModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n    all_model_classes = (AltCLIPModel,) if is_torch_available() else ()\n    pipeline_model_mapping = {\"feature-extraction\": AltCLIPModel} if is_torch_available() else {}\n    fx_compatible = True\n    test_head_masking = False\n    test_pruning = False\n    test_resize_embeddings = False\n    test_attention_outputs = False\n\n    # TODO: Fix the failed tests when this model gets more usage\n    def is_pipeline_test_to_skip(\n        self,\n        pipeline_test_case_name,\n        config_class,\n        model_architecture,\n        tokenizer_name,\n        image_processor_name,\n        feature_extractor_name,\n        processor_name,\n    ):\n        if pipeline_test_case_name == \"FeatureExtractionPipelineTests\":\n            return True\n\n        return False\n\n    def setUp(self):\n        self.model_tester = AltCLIPModelTester(self)\n        self.config_tester = ConfigTester(\n            self,\n            config_class=AltCLIPConfig,\n            has_text_modality=False,\n            common_properties=[\"projection_dim\", \"logit_scale_init_value\"],\n        )\n\n    def test_model(self):\n        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n        self.model_tester.create_and_check_model(*config_and_inputs)\n\n    def test_config(self):\n        self.config_tester.run_common_tests()\n\n    @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n    def test_hidden_states_output(self):\n        pass\n\n    @unittest.skip(reason=\"Inputs_embeds is tested in individual model tests\")\n    def test_inputs_embeds(self):\n        pass\n\n    @unittest.skip(reason=\"Retain_grad is tested in individual model tests\")\n    def test_retain_grad_hidden_states_attentions(self):\n        pass\n\n    @unittest.skip(reason=\"CLIPModel does not have input/output embeddings\")\n    def test_model_get_set_embeddings(self):\n        pass\n\n    # override as the `logit_scale` parameter initialization is different for AltCLIP\n    def test_initialization(self):\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n        configs_no_init = _config_zero_init(config)\n        for model_class in self.all_model_classes:\n            model = model_class(config=configs_no_init)\n            for name, param in model.named_parameters():\n                if param.requires_grad:\n                    # check if `logit_scale` is initialized as per the original implementation\n                    if name == \"logit_scale\":\n                        self.assertAlmostEqual(\n                            param.data.item(),\n                            np.log(1 / 0.07),\n                            delta=1e-3,\n                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                        )\n                    else:\n                        self.assertIn(\n                            ((param.data.mean() * 1e9).round() / 1e9).item(),\n                            [0.0, 1.0],\n                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                        )\n\n    def _create_and_check_torchscript(self, config, inputs_dict):\n        if not self.test_torchscript:\n            self.skipTest(reason=\"test_torchscript is set to False\")\n\n        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n        configs_no_init.torchscript = True\n        configs_no_init.return_dict = False\n        for model_class in self.all_model_classes:\n            model = model_class(config=configs_no_init)\n            model.to(torch_device)\n            model.eval()\n\n            try:\n                input_ids = inputs_dict[\"input_ids\"]\n                pixel_values = inputs_dict[\"pixel_values\"]  # CLIP needs pixel_values\n                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n            except RuntimeError:\n                self.fail(\"Couldn't trace module.\")\n\n            with tempfile.TemporaryDirectory() as tmp_dir_name:\n                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n\n                try:\n                    torch.jit.save(traced_model, pt_file_name)\n                except Exception:\n                    self.fail(\"Couldn't save module.\")\n\n                try:\n                    loaded_model = torch.jit.load(pt_file_name)\n                except Exception:\n                    self.fail(\"Couldn't load module.\")\n\n            model.to(torch_device)\n            model.eval()\n\n            loaded_model.to(torch_device)\n            loaded_model.eval()\n\n            model_state_dict = model.state_dict()\n            loaded_model_state_dict = loaded_model.state_dict()\n\n            non_persistent_buffers = {}\n            for key in loaded_model_state_dict.keys():\n                if key not in model_state_dict.keys():\n                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n\n            loaded_model_state_dict = {\n                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n            }\n\n            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n\n            model_buffers = list(model.buffers())\n            for non_persistent_buffer in non_persistent_buffers.values():\n                found_buffer = False\n                for i, model_buffer in enumerate(model_buffers):\n                    if torch.equal(non_persistent_buffer, model_buffer):\n                        found_buffer = True\n                        break\n\n                self.assertTrue(found_buffer)\n                model_buffers.pop(i)\n\n            models_equal = True\n            for layer_name, p1 in model_state_dict.items():\n                p2 = loaded_model_state_dict[layer_name]\n                if p1.data.ne(p2.data).sum() > 0:\n                    models_equal = False\n\n            self.assertTrue(models_equal)\n\n    @slow\n    def test_model_from_pretrained(self):\n        model_name = \"BAAI/AltCLIP\"\n        model = AltCLIPModel.from_pretrained(model_name)\n        self.assertIsNotNone(model)\n\n\n@require_vision\n@require_torch\nclass AltCLIPModelIntegrationTest(unittest.TestCase):\n    @slow\n    def test_inference(self):\n        model_name = \"BAAI/AltCLIP\"\n        model = AltCLIPModel.from_pretrained(model_name).to(torch_device)\n        processor = AltCLIPProcessor.from_pretrained(model_name)\n\n        image = prepare_img()\n        inputs = processor(text=[\"一张猫的照片\", \"一张狗的照片\"], images=image, padding=True, return_tensors=\"pt\").to(torch_device)  # fmt: skip\n\n        # forward pass\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        # verify the logits\n        self.assertEqual(\n            outputs.logits_per_image.shape,\n            torch.Size((inputs.pixel_values.shape[0], inputs.input_ids.shape[0])),\n        )\n        self.assertEqual(\n            outputs.logits_per_text.shape,\n            torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n        )\n\n        probs = outputs.logits_per_image.softmax(dim=1)\n        expected_probs = torch.tensor([[9.9942e-01, 5.7805e-04]], device=torch_device)\n\n        torch.testing.assert_close(probs, expected_probs, rtol=5e-3, atol=5e-3)\n\n    @slow\n    def test_inference_interpolate_pos_encoding(self):\n        # ViT models have an `interpolate_pos_encoding` argument in their forward method,\n        # allowing to interpolate the pre-trained position embeddings in order to use\n        # the model on higher resolutions. The DINO model by Facebook AI leverages this\n        # to visualize self-attention on higher resolution images.\n        model_name = \"BAAI/AltCLIP\"\n        model = AltCLIPModel.from_pretrained(model_name).to(torch_device)\n\n        image_processor = AltCLIPProcessor.from_pretrained(\n            model_name, size={\"shortest_edge\": 180}, crop_size={\"height\": 180, \"width\": 180}\n        )\n\n        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n        inputs = image_processor(text=\"what's in the image\", images=image, return_tensors=\"pt\").to(torch_device)\n\n        # interpolate_pos_encodiung false should return value error\n        with self.assertRaises(ValueError, msg=\"doesn't match model\"):\n            with torch.no_grad():\n                model(**inputs, interpolate_pos_encoding=False)\n\n        # forward pass\n        with torch.no_grad():\n            outputs = model(**inputs, interpolate_pos_encoding=True)\n\n        # verify the logits\n        expected_shape = torch.Size((1, 145, 1024))\n        print(\"nilesh \")\n        print(outputs.vision_model_output.last_hidden_state.shape)\n        print(outputs.vision_model_output.last_hidden_state[0, :3, :3])\n\n        self.assertEqual(outputs.vision_model_output.last_hidden_state.shape, expected_shape)\n\n        expected_slice = torch.tensor(\n            [[-0.3589, -0.5939, 0.3534], [0.4346, 0.1647, 0.7071], [1.1404, -0.4716, 0.1664]]\n        ).to(torch_device)\n\n        torch.testing.assert_close(\n            outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4\n        )\n"}
{"type": "test_file", "path": "transformers/tests/models/albert/test_tokenization_albert.py", "content": "# coding=utf-8\n# Copyright 2019 Hugging Face inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom transformers import AlbertTokenizer, AlbertTokenizerFast\nfrom transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, slow\n\nfrom ...test_tokenization_common import TokenizerTesterMixin\n\n\nSAMPLE_VOCAB = get_tests_dir(\"fixtures/spiece.model\")\n\n\n@require_sentencepiece\n@require_tokenizers\nclass AlbertTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n    from_pretrained_id = \"albert/albert-base-v1\"\n    tokenizer_class = AlbertTokenizer\n    rust_tokenizer_class = AlbertTokenizerFast\n    test_rust_tokenizer = True\n    test_sentencepiece = True\n    test_sentencepiece_ignore_case = True\n\n    def setUp(self):\n        super().setUp()\n\n        # We have a SentencePiece fixture for testing\n        tokenizer = AlbertTokenizer(SAMPLE_VOCAB)\n        tokenizer.save_pretrained(self.tmpdirname)\n\n    def get_input_output_texts(self, tokenizer):\n        input_text = \"this is a test\"\n        output_text = \"this is a test\"\n        return input_text, output_text\n\n    def test_convert_token_and_id(self):\n        \"\"\"Test ``_convert_token_to_id`` and ``_convert_id_to_token``.\"\"\"\n        token = \"<pad>\"\n        token_id = 0\n\n        self.assertEqual(self.get_tokenizer()._convert_token_to_id(token), token_id)\n        self.assertEqual(self.get_tokenizer()._convert_id_to_token(token_id), token)\n\n    def test_get_vocab(self):\n        vocab_keys = list(self.get_tokenizer().get_vocab().keys())\n\n        self.assertEqual(vocab_keys[0], \"<pad>\")\n        self.assertEqual(vocab_keys[1], \"<unk>\")\n        self.assertEqual(vocab_keys[-1], \"▁eloquent\")\n        self.assertEqual(len(vocab_keys), 30_000)\n\n    def test_vocab_size(self):\n        self.assertEqual(self.get_tokenizer().vocab_size, 30_000)\n\n    def test_rust_and_python_full_tokenizers(self):\n        if not self.test_rust_tokenizer:\n            self.skipTest(reason=\"test_rust_tokenizer is set to False\")\n\n        tokenizer = self.get_tokenizer()\n        rust_tokenizer = self.get_rust_tokenizer()\n\n        sequence = \"I was born in 92000, and this is falsé.\"\n\n        tokens = tokenizer.tokenize(sequence)\n        rust_tokens = rust_tokenizer.tokenize(sequence)\n        self.assertListEqual(tokens, rust_tokens)\n\n        ids = tokenizer.encode(sequence, add_special_tokens=False)\n        rust_ids = rust_tokenizer.encode(sequence, add_special_tokens=False)\n        self.assertListEqual(ids, rust_ids)\n\n        rust_tokenizer = self.get_rust_tokenizer()\n        ids = tokenizer.encode(sequence)\n        rust_ids = rust_tokenizer.encode(sequence)\n        self.assertListEqual(ids, rust_ids)\n\n    def test_full_tokenizer(self):\n        tokenizer = AlbertTokenizer(SAMPLE_VOCAB, keep_accents=True)\n\n        tokens = tokenizer.tokenize(\"This is a test\")\n        self.assertListEqual(tokens, [\"▁this\", \"▁is\", \"▁a\", \"▁test\"])\n\n        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [48, 25, 21, 1289])\n\n        tokens = tokenizer.tokenize(\"I was born in 92000, and this is falsé.\")\n        self.assertListEqual(\n            tokens, [\"▁i\", \"▁was\", \"▁born\", \"▁in\", \"▁9\", \"2000\", \",\", \"▁and\", \"▁this\", \"▁is\", \"▁fal\", \"s\", \"é\", \".\"]\n        )\n        ids = tokenizer.convert_tokens_to_ids(tokens)\n        self.assertListEqual(ids, [31, 23, 386, 19, 561, 3050, 15, 17, 48, 25, 8256, 18, 1, 9])\n\n        back_tokens = tokenizer.convert_ids_to_tokens(ids)\n        self.assertListEqual(\n            back_tokens,\n            [\"▁i\", \"▁was\", \"▁born\", \"▁in\", \"▁9\", \"2000\", \",\", \"▁and\", \"▁this\", \"▁is\", \"▁fal\", \"s\", \"<unk>\", \".\"],\n        )\n\n    def test_sequence_builders(self):\n        tokenizer = AlbertTokenizer(SAMPLE_VOCAB)\n\n        text = tokenizer.encode(\"sequence builders\")\n        text_2 = tokenizer.encode(\"multi-sequence build\")\n\n        encoded_sentence = tokenizer.build_inputs_with_special_tokens(text)\n        encoded_pair = tokenizer.build_inputs_with_special_tokens(text, text_2)\n\n        assert encoded_sentence == [tokenizer.cls_token_id] + text + [tokenizer.sep_token_id]\n        assert encoded_pair == [tokenizer.cls_token_id] + text + [tokenizer.sep_token_id] + text_2 + [\n            tokenizer.sep_token_id\n        ]\n\n    @slow\n    def test_tokenizer_integration(self):\n        expected_encoding = {'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'input_ids': [[2, 21970, 13, 5, 6092, 167, 28, 7103, 2153, 673, 8, 7028, 12051, 18, 17, 7103, 2153, 673, 8, 3515, 18684, 8, 4461, 6, 1927, 297, 8, 12060, 2607, 18, 13, 5, 4461, 15, 10538, 38, 8, 135, 15, 822, 58, 15, 993, 10363, 15, 1460, 8005, 4461, 15, 993, 255, 2328, 9, 9, 9, 6, 26, 1112, 816, 3260, 13, 5, 103, 2377, 6, 17, 1112, 816, 2782, 13, 5, 103, 10641, 6, 29, 84, 2512, 2430, 782, 18684, 2761, 19, 808, 2430, 2556, 17, 855, 1480, 9477, 4091, 128, 11712, 15, 7103, 2153, 673, 17, 24883, 9990, 9, 3], [2, 11502, 25, 1006, 20, 782, 8, 11809, 855, 1732, 19393, 18667, 37, 367, 21018, 69, 1854, 34, 11860, 19124, 27, 156, 225, 17, 193, 4141, 19, 65, 9124, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 14, 2231, 886, 2385, 17659, 84, 14, 16792, 1952, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}  # fmt: skip\n\n        self.tokenizer_integration_test_util(\n            expected_encoding=expected_encoding,\n            model_name=\"albert/albert-base-v2\",\n            revision=\"6b6560eaf5ff2e250b00c50f380c5389a9c2d82e\",\n        )\n"}
{"type": "test_file", "path": "transformers/tests/generation/test_streamers.py", "content": "# coding=utf-8\n# Copyright 2023 The HuggingFace Team Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a clone of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom queue import Empty\nfrom threading import Thread\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom transformers import (\n    AsyncTextIteratorStreamer,\n    AutoTokenizer,\n    TextIteratorStreamer,\n    TextStreamer,\n    is_torch_available,\n)\nfrom transformers.testing_utils import CaptureStdout, require_torch, torch_device\nfrom transformers.utils.logging import _get_library_root_logger\n\nfrom ..test_modeling_common import ids_tensor\n\n\nif is_torch_available():\n    import torch\n\n    from transformers import AutoModelForCausalLM\n\n\n@require_torch\nclass StreamerTester(unittest.TestCase):\n    def test_text_streamer_matches_non_streaming(self):\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        model.config.eos_token_id = -1\n\n        input_ids = ids_tensor((1, 5), vocab_size=model.config.vocab_size).to(torch_device)\n        greedy_ids = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n        greedy_text = tokenizer.decode(greedy_ids[0])\n\n        with CaptureStdout() as cs:\n            streamer = TextStreamer(tokenizer)\n            model.generate(input_ids, max_new_tokens=10, do_sample=False, streamer=streamer)\n        # The greedy text should be printed to stdout, except for the final \"\\n\" in the streamer\n        streamer_text = cs.out[:-1]\n\n        self.assertEqual(streamer_text, greedy_text)\n\n    def test_iterator_streamer_matches_non_streaming(self):\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        model.config.eos_token_id = -1\n\n        input_ids = ids_tensor((1, 5), vocab_size=model.config.vocab_size).to(torch_device)\n        greedy_ids = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n        greedy_text = tokenizer.decode(greedy_ids[0])\n\n        streamer = TextIteratorStreamer(tokenizer)\n        generation_kwargs = {\"input_ids\": input_ids, \"max_new_tokens\": 10, \"do_sample\": False, \"streamer\": streamer}\n        thread = Thread(target=model.generate, kwargs=generation_kwargs)\n        thread.start()\n        streamer_text = \"\"\n        for new_text in streamer:\n            streamer_text += new_text\n\n        self.assertEqual(streamer_text, greedy_text)\n\n    def test_text_streamer_skip_prompt(self):\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        model.config.eos_token_id = -1\n\n        input_ids = ids_tensor((1, 5), vocab_size=model.config.vocab_size).to(torch_device)\n        greedy_ids = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n        new_greedy_ids = greedy_ids[:, input_ids.shape[1] :]\n        new_greedy_text = tokenizer.decode(new_greedy_ids[0])\n\n        with CaptureStdout() as cs:\n            streamer = TextStreamer(tokenizer, skip_prompt=True)\n            model.generate(input_ids, max_new_tokens=10, do_sample=False, streamer=streamer)\n        # The greedy text should be printed to stdout, except for the final \"\\n\" in the streamer\n        streamer_text = cs.out[:-1]\n\n        self.assertEqual(streamer_text, new_greedy_text)\n\n    def test_text_streamer_decode_kwargs(self):\n        # Tests that we can pass `decode_kwargs` to the streamer to control how the tokens are decoded. Must be tested\n        # with actual models -- the dummy models' tokenizers are not aligned with their models, and\n        # `skip_special_tokens=True` has no effect on them\n        tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n        model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\").to(torch_device)\n        model.config.eos_token_id = -1\n\n        input_ids = torch.ones((1, 5), device=torch_device).long() * model.config.bos_token_id\n\n        root = _get_library_root_logger()\n        with patch.object(root, \"propagate\", False):\n            with CaptureStdout() as cs:\n                streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n                model.generate(input_ids, max_new_tokens=1, do_sample=False, streamer=streamer)\n\n        # The prompt contains a special token, so the streamer should not print it. As such, the output text, when\n        # re-tokenized, must only contain one token\n        streamer_text = cs.out[:-1]  # Remove the final \"\\n\"\n        streamer_text_tokenized = tokenizer(streamer_text, return_tensors=\"pt\")\n        self.assertEqual(streamer_text_tokenized.input_ids.shape, (1, 1))\n\n    def test_iterator_streamer_timeout(self):\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        model.config.eos_token_id = -1\n\n        input_ids = ids_tensor((1, 5), vocab_size=model.config.vocab_size).to(torch_device)\n        streamer = TextIteratorStreamer(tokenizer, timeout=0.001)\n        generation_kwargs = {\"input_ids\": input_ids, \"max_new_tokens\": 10, \"do_sample\": False, \"streamer\": streamer}\n        thread = Thread(target=model.generate, kwargs=generation_kwargs)\n        thread.start()\n\n        # The streamer will timeout after 0.001 seconds, so an exception will be raised\n        with self.assertRaises(Empty):\n            streamer_text = \"\"\n            for new_text in streamer:\n                streamer_text += new_text\n\n\n@require_torch\n@pytest.mark.asyncio(loop_scope=\"class\")\nclass AsyncStreamerTester(unittest.IsolatedAsyncioTestCase):\n    async def test_async_iterator_streamer_matches_non_streaming(self):\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        model.config.eos_token_id = -1\n\n        input_ids = ids_tensor((1, 5), vocab_size=model.config.vocab_size).to(torch_device)\n        greedy_ids = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n        greedy_text = tokenizer.decode(greedy_ids[0])\n\n        streamer = AsyncTextIteratorStreamer(tokenizer)\n        generation_kwargs = {\"input_ids\": input_ids, \"max_new_tokens\": 10, \"do_sample\": False, \"streamer\": streamer}\n        thread = Thread(target=model.generate, kwargs=generation_kwargs)\n        thread.start()\n        streamer_text = \"\"\n        async for new_text in streamer:\n            streamer_text += new_text\n\n        self.assertEqual(streamer_text, greedy_text)\n\n    async def test_async_iterator_streamer_timeout(self):\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        model.config.eos_token_id = -1\n\n        input_ids = ids_tensor((1, 5), vocab_size=model.config.vocab_size).to(torch_device)\n        streamer = AsyncTextIteratorStreamer(tokenizer, timeout=0.001)\n        generation_kwargs = {\"input_ids\": input_ids, \"max_new_tokens\": 10, \"do_sample\": False, \"streamer\": streamer}\n        thread = Thread(target=model.generate, kwargs=generation_kwargs)\n        thread.start()\n\n        # The streamer will timeout after 0.001 seconds, so TimeoutError will be raised\n        with self.assertRaises(TimeoutError):\n            streamer_text = \"\"\n            async for new_text in streamer:\n                streamer_text += new_text\n"}
{"type": "test_file", "path": "transformers/tests/generation/test_utils.py", "content": "# coding=utf-8\n# Copyright 2020 The HuggingFace Team Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a clone of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport collections\nimport copy\nimport datetime\nimport gc\nimport inspect\nimport random\nimport tempfile\nimport unittest\nimport warnings\n\nimport numpy as np\nimport pytest\nfrom packaging import version\nfrom parameterized import parameterized\n\nfrom transformers import AutoConfig, AutoProcessor, AutoTokenizer, is_torch_available, pipeline\nfrom transformers.testing_utils import (\n    is_flaky,\n    require_accelerate,\n    require_flash_attn,\n    require_optimum_quanto,\n    require_torch,\n    require_torch_accelerator,\n    require_torch_gpu,\n    require_torch_multi_accelerator,\n    require_torch_multi_gpu,\n    require_torch_sdpa,\n    set_config_for_less_flaky_test,\n    set_model_for_less_flaky_test,\n    set_model_tester_for_less_flaky_test,\n    slow,\n    torch_device,\n)\nfrom transformers.utils import is_ipex_available\n\n\nif is_torch_available():\n    import torch\n    import torch.nn.functional as F\n\n    from transformers import (\n        AutoModelForCausalLM,\n        AutoModelForSeq2SeqLM,\n        AutoModelForSpeechSeq2Seq,\n        AutoModelForVision2Seq,\n        BartForConditionalGeneration,\n        BartTokenizer,\n        GPT2LMHeadModel,\n        GPT2Tokenizer,\n        ImageGPTForCausalImageModeling,\n        SpeechEncoderDecoderModel,\n        T5ForConditionalGeneration,\n    )\n    from transformers.cache_utils import (\n        Cache,\n        DynamicCache,\n        EncoderDecoderCache,\n        HybridCache,\n        QuantoQuantizedCache,\n        StaticCache,\n    )\n    from transformers.generation import (\n        BeamSampleDecoderOnlyOutput,\n        BeamSampleEncoderDecoderOutput,\n        BeamSearchDecoderOnlyOutput,\n        BeamSearchEncoderDecoderOutput,\n        DisjunctiveConstraint,\n        GenerateBeamDecoderOnlyOutput,\n        GenerateBeamEncoderDecoderOutput,\n        GenerateDecoderOnlyOutput,\n        GenerateEncoderDecoderOutput,\n        GenerationConfig,\n        GreedySearchDecoderOnlyOutput,\n        GreedySearchEncoderDecoderOutput,\n        LogitsProcessorList,\n        MaxLengthCriteria,\n        MinLengthLogitsProcessor,\n        PhrasalConstraint,\n        PromptLookupCandidateGenerator,\n        SampleDecoderOnlyOutput,\n        SampleEncoderDecoderOutput,\n        StoppingCriteria,\n        StoppingCriteriaList,\n        SynthIDTextWatermarkingConfig,\n        WatermarkDetector,\n        WatermarkingConfig,\n    )\n    from transformers.generation.candidate_generator import (\n        AssistedCandidateGenerator,\n        AssistedCandidateGeneratorDifferentTokenizers,\n    )\n    from transformers.generation.utils import _speculative_sampling\n\nfrom unittest.mock import patch\n\nfrom transformers.utils import is_sklearn_available\n\n\n# TODO: raushan remove this when VLMs start accepting input embeds\nVLM_CLASS_NAMES = [\n    \"llava\",\n    \"idefics2\",\n    \"idefics3\",\n    \"mllama\",\n    \"paligemma\",\n    \"emu3\",\n    \"gotocr2\",\n    \"qwen2vl\",\n    \"qwen2_5_vl\",\n    \"ayavision\",\n    \"gemma3\",\n    \"mistral3\",\n]\n\n\nclass GenerationTesterMixin:\n    input_name = \"input_ids\"\n    model_tester = None\n    max_new_tokens = 3\n\n    def prepare_config_and_inputs_for_generate(self, batch_size=2):\n        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n\n        # We don't want a few model inputs in our model input dictionary for generation tests\n        input_keys_to_ignore = [\n            # we don't want to mask attention heads\n            \"head_mask\",\n            \"decoder_head_mask\",\n            \"cross_attn_head_mask\",\n            # we don't want encoder-decoder models to start from filled decoder ids\n            \"decoder_input_ids\",\n            \"decoder_attention_mask\",\n            # we'll set cache use in each test differently\n            \"use_cache\",\n            # Ignore labels if it is in the input dict\n            \"labels\",\n            # model-specific exceptions should overload/overwrite this function\n        ]\n        filtered_inputs_dict = {\n            k: v[:batch_size, ...] if isinstance(v, torch.Tensor) else v\n            for k, v in inputs_dict.items()\n            if k not in input_keys_to_ignore\n        }\n\n        # It is important set `eos_token_id` to `None` to avoid early stopping (would break for length-based checks)\n        text_gen_config = config.get_text_config(decoder=True)\n        if text_gen_config.eos_token_id is not None and text_gen_config.pad_token_id is None:\n            text_gen_config.pad_token_id = (\n                text_gen_config.eos_token_id\n                if isinstance(text_gen_config.eos_token_id, int)\n                else text_gen_config.eos_token_id[0]\n            )\n        text_gen_config.eos_token_id = None\n        text_gen_config.forced_eos_token_id = None\n\n        return config, filtered_inputs_dict\n\n    def _check_similar_generate_outputs(self, output_1, output_2, atol=1e-5, rtol=1e-5):\n        \"\"\"\n        Checks whether a pair of generate outputs are similar. Two `generate` call outputs are considered similar in\n        the following situations:\n        1. The sequences are the same\n        2. The sequences are different, but the scores up to (and including) the first mismatch are nearly identical\n        \"\"\"\n        # scores doesn't include data regarding decoder input tokens\n        decoder_input_length = output_1.sequences.shape[1] - len(output_1.scores)\n        output_matches = output_1.sequences == output_2.sequences\n        has_matching_outputs = output_matches.all()\n        has_matching_scores = None\n        if not has_matching_outputs:\n            for batch_idx in range(output_1.sequences.shape[0]):\n                batch_matches = output_matches[batch_idx]\n                if batch_matches.all():\n                    continue\n                first_mismatch_idx = batch_matches.int().argmin()  # gets the index of the first False\n                first_mismatch_idx -= decoder_input_length\n                output_1_first_mismatch_scores = output_1.scores[first_mismatch_idx][batch_idx]\n                output_2_first_mismatch_scores = output_2.scores[first_mismatch_idx][batch_idx]\n                has_matching_scores = torch.allclose(\n                    output_1_first_mismatch_scores, output_2_first_mismatch_scores, rtol=atol, atol=rtol\n                )\n                if not has_matching_scores:\n                    break\n        self.assertTrue(has_matching_outputs or has_matching_scores)\n\n    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n        logits_processor_kwargs = {\n            \"bad_words_ids\": [[1, 0]],\n            \"repetition_penalty\": 1.2,\n            \"remove_invalid_values\": True,\n        }\n        if do_sample:\n            logits_processor_kwargs.update(\n                {\n                    \"top_k\": 10,\n                    \"top_p\": 0.7,\n                    \"temperature\": 0.7,\n                }\n            )\n        # TODO (joao, raushan): see this comment for a long-term fix\n        # https://github.com/huggingface/transformers/pull/33593#issuecomment-2361824264)\n        # This is a band-aid for VLM models, to ensure they don't generate image/video tokens which would cause them\n        # to crash. On pretrained models this isn't a risk, as they are trained to not generate these tokens.\n        if config is not None:\n            for key in [\n                \"image_token_index\",\n                \"image_token_id\",\n                \"video_token_index\",\n                \"video_token_id\",\n                \"vision_start_token_id\",\n            ]:\n                token_index = getattr(config, key, None)\n                if token_index is None and hasattr(self, \"model_tester\"):\n                    token_index = getattr(self.model_tester, key, None)\n                if token_index is not None and token_index < config.get_text_config().vocab_size:\n                    logits_processor_kwargs[\"bad_words_ids\"].append([token_index])\n\n        return logits_processor_kwargs\n\n    def _get_beam_kwargs(self, num_return_sequences=1):\n        beam_kwargs = {\n            \"early_stopping\": False,\n            \"length_penalty\": 2.0,\n            \"num_beams\": 2,\n            \"num_return_sequences\": num_return_sequences,\n        }\n        return beam_kwargs\n\n    def _get_diverse_beam_kwargs(self, num_return_sequences=1):\n        beam_kwargs = {\n            \"early_stopping\": False,\n            \"length_penalty\": 2.0,\n            \"num_beams\": 2,\n            \"num_return_sequences\": num_return_sequences,\n            \"num_beam_groups\": 2,  # one beam per group\n            \"diversity_penalty\": 2.0,\n        }\n        return beam_kwargs\n\n    def _get_constrained_beam_kwargs(self, num_return_sequences=1):\n        beam_kwargs = {\n            \"early_stopping\": False,\n            \"length_penalty\": 2.0,\n            \"num_beams\": num_return_sequences * 4,\n            \"num_return_sequences\": num_return_sequences,\n        }\n        return beam_kwargs\n\n    def _greedy_generate(\n        self,\n        model,\n        inputs_dict,\n        output_scores=False,\n        output_logits=False,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict_in_generate=False,\n        use_cache=True,\n    ):\n        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n        output_generate = model.generate(\n            do_sample=False,\n            num_beams=1,\n            max_new_tokens=self.max_new_tokens,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            output_scores=output_scores,\n            output_logits=output_logits,\n            return_dict_in_generate=return_dict_in_generate,\n            use_cache=use_cache,\n            **logits_processor_kwargs,\n            **inputs_dict,\n        )\n\n        return output_generate\n\n    def _sample_generate(\n        self,\n        model,\n        inputs_dict,\n        num_return_sequences,\n        output_scores=False,\n        output_logits=False,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict_in_generate=False,\n        use_cache=True,\n    ):\n        torch.manual_seed(0)\n        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True, config=model.config)\n        output_generate = model.generate(\n            do_sample=True,\n            num_beams=1,\n            max_new_tokens=self.max_new_tokens,\n            num_return_sequences=num_return_sequences,\n            output_scores=output_scores,\n            output_logits=output_logits,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict_in_generate=return_dict_in_generate,\n            use_cache=use_cache,\n            **logits_processor_kwargs,\n            **inputs_dict,\n        )\n\n        return output_generate\n\n    def _beam_search_generate(\n        self,\n        model,\n        inputs_dict,\n        beam_kwargs,\n        output_scores=False,\n        output_logits=False,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict_in_generate=False,\n        use_cache=True,\n    ):\n        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n        output_generate = model.generate(\n            do_sample=False,\n            max_new_tokens=self.max_new_tokens,\n            output_scores=output_scores,\n            output_logits=output_logits,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict_in_generate=return_dict_in_generate,\n            use_cache=use_cache,\n            **beam_kwargs,\n            **logits_processor_kwargs,\n            **inputs_dict,\n        )\n\n        return output_generate\n\n    def _beam_sample_generate(\n        self,\n        model,\n        inputs_dict,\n        beam_kwargs,\n        output_scores=False,\n        output_logits=False,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict_in_generate=False,\n        use_cache=True,\n    ):\n        torch.manual_seed(0)\n        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True, config=model.config)\n        output_generate = model.generate(\n            do_sample=True,\n            max_new_tokens=self.max_new_tokens,\n            output_scores=output_scores,\n            output_logits=output_logits,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict_in_generate=return_dict_in_generate,\n            use_cache=use_cache,\n            **beam_kwargs,\n            **logits_processor_kwargs,\n            **inputs_dict,\n        )\n\n        return output_generate\n\n    def _group_beam_search_generate(\n        self,\n        model,\n        inputs_dict,\n        beam_kwargs,\n        output_scores=False,\n        output_logits=False,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict_in_generate=False,\n        use_cache=True,\n    ):\n        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n        output_generate = model.generate(\n            do_sample=False,\n            max_new_tokens=self.max_new_tokens,\n            output_scores=output_scores,\n            output_logits=output_logits,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict_in_generate=return_dict_in_generate,\n            use_cache=use_cache,\n            **beam_kwargs,\n            **logits_processor_kwargs,\n            **inputs_dict,\n        )\n\n        return output_generate\n\n    def _constrained_beam_search_generate(\n        self,\n        model,\n        inputs_dict,\n        constraints,\n        beam_kwargs,\n        output_scores=False,\n        output_logits=False,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict_in_generate=False,\n        use_cache=True,\n    ):\n        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n        output_generate = model.generate(\n            do_sample=False,\n            max_new_tokens=self.max_new_tokens,\n            output_scores=output_scores,\n            output_logits=output_logits,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict_in_generate=return_dict_in_generate,\n            constraints=constraints,\n            use_cache=use_cache,\n            **beam_kwargs,\n            **logits_processor_kwargs,\n            **inputs_dict,\n        )\n\n        return output_generate\n\n    def _contrastive_generate(\n        self,\n        model,\n        inputs_dict,\n        output_scores=False,\n        output_logits=False,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict_in_generate=False,\n        use_cache=True,\n    ):\n        contrastive_search_kwargs = {\n            \"penalty_alpha\": 0.6,\n            \"top_k\": 5,\n        }\n\n        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n        output_generate = model.generate(\n            do_sample=False,\n            num_beams=1,\n            max_new_tokens=self.max_new_tokens,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            output_scores=output_scores,\n            output_logits=output_logits,\n            return_dict_in_generate=return_dict_in_generate,\n            use_cache=use_cache,\n            **logits_processor_kwargs,\n            **contrastive_search_kwargs,\n            **inputs_dict,\n        )\n\n        return output_generate\n\n    @pytest.mark.generate\n    def test_greedy_generate(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            model = model_class(config).to(torch_device).eval()\n            output_generate = self._greedy_generate(model=model, inputs_dict=inputs_dict)\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n            else:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n\n    @pytest.mark.generate\n    def test_greedy_generate_dict_outputs(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n            if self.has_attentions:\n                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n\n            model = model_class(config).to(torch_device).eval()\n            output_generate = self._greedy_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                output_scores=True,\n                output_logits=True,\n                output_hidden_states=True,\n                output_attentions=self.has_attentions,\n                return_dict_in_generate=True,\n                use_cache=False,\n            )\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n                self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n                # Retrocompatibility check\n                self.assertIsInstance(output_generate, GreedySearchEncoderDecoderOutput)\n            else:\n                self.assertTrue(\n                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                )\n                self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n                # Retrocompatibility check\n                self.assertIsInstance(output_generate, GreedySearchDecoderOnlyOutput)\n\n            self._check_generate_outputs(output_generate, model.config)\n\n    @pytest.mark.generate\n    def test_greedy_generate_dict_outputs_use_cache(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n            if self.has_attentions:\n                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n\n            if not hasattr(config.get_text_config(), \"use_cache\"):\n                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n            if any(model_name in model_class.__name__.lower() for model_name in [\"rwkv\"]):\n                self.skipTest(reason=\"Won't fix: model with non-standard dictionary output shapes\")\n\n            config.is_decoder = True\n            model = model_class(config).to(torch_device).eval()\n            output_generate = self._greedy_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                output_scores=True,\n                output_logits=True,\n                output_hidden_states=True,\n                output_attentions=self.has_attentions,\n                return_dict_in_generate=True,\n                use_cache=True,  # Enable cache\n            )\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n            else:\n                self.assertTrue(\n                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                )\n\n            self._check_generate_outputs(output_generate, model.config, use_cache=True)\n\n    @pytest.mark.generate\n    def test_sample_generate(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            model = model_class(config).to(torch_device).eval()\n            output_generate = self._sample_generate(model=model, inputs_dict=inputs_dict, num_return_sequences=1)\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n            else:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n\n    @pytest.mark.generate\n    def test_sample_generate_dict_output(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n            if self.has_attentions:\n                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n\n            model = model_class(config).to(torch_device).eval()\n            output_generate = self._sample_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                num_return_sequences=2,\n                output_scores=True,\n                output_logits=True,\n                output_hidden_states=True,\n                output_attentions=self.has_attentions,\n                return_dict_in_generate=True,\n                use_cache=False,\n            )\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n                self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n                # Retrocompatibility check\n                self.assertIsInstance(output_generate, SampleEncoderDecoderOutput)\n            else:\n                self.assertTrue(\n                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                )\n                self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n                # Retrocompatibility check\n                self.assertIsInstance(output_generate, SampleDecoderOnlyOutput)\n\n            self._check_generate_outputs(output_generate, model.config, num_return_sequences=2)\n\n    @pytest.mark.generate\n    def test_beam_search_generate(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            model = model_class(config).to(torch_device).eval()\n\n            beam_kwargs = self._get_beam_kwargs()\n            output_generate = self._beam_search_generate(model=model, inputs_dict=inputs_dict, beam_kwargs=beam_kwargs)\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n            else:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n\n    @pytest.mark.generate\n    def test_beam_search_generate_dict_output(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n            if self.has_attentions:\n                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n\n            model = model_class(config).to(torch_device).eval()\n            beam_kwargs = self._get_beam_kwargs()\n            output_generate = self._beam_search_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                beam_kwargs=beam_kwargs,\n                output_scores=True,\n                output_logits=True,\n                output_hidden_states=True,\n                output_attentions=self.has_attentions,\n                return_dict_in_generate=True,\n                use_cache=False,\n            )\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n                self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                # Retrocompatibility check\n                self.assertIsInstance(output_generate, BeamSearchEncoderDecoderOutput)\n            else:\n                self.assertTrue(\n                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                )\n                self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                # Retrocompatibility check\n                self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n\n            self._check_generate_outputs(\n                output_generate,\n                model.config,\n                num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n                num_beams=beam_kwargs[\"num_beams\"],\n            )\n\n    @pytest.mark.generate\n    def test_beam_search_generate_dict_outputs_use_cache(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            if not hasattr(config.get_text_config(), \"use_cache\"):\n                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n            if any(model_name in model_class.__name__.lower() for model_name in [\"rwkv\"]):\n                self.skipTest(reason=\"Won't fix: model with non-standard dictionary output shapes\")\n\n            if self.has_attentions:\n                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n            model = model_class(config).to(torch_device).eval()\n            beam_kwargs = self._get_beam_kwargs()\n\n            config.is_decoder = True\n            model = model_class(config).to(torch_device).eval()\n            output_generate = self._beam_search_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                beam_kwargs=beam_kwargs,\n                output_scores=True,\n                output_logits=True,\n                output_hidden_states=True,\n                output_attentions=self.has_attentions,\n                return_dict_in_generate=True,\n                use_cache=True,  # Enable cache\n            )\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n            else:\n                self.assertTrue(\n                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                )\n\n            self._check_generate_outputs(\n                output_generate,\n                model.config,\n                use_cache=True,\n                num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n                num_beams=beam_kwargs[\"num_beams\"],\n            )\n\n    @require_accelerate\n    @require_torch_multi_accelerator\n    @pytest.mark.generate\n    def test_model_parallel_beam_search(self):\n        if \"xpu\" in torch_device:\n            if not (is_ipex_available(\"2.5\") or version.parse(torch.__version__) >= version.parse(\"2.6\")):\n                self.skipTest(reason=\"device_map='auto' does not work with XPU devices\")\n\n        for model_class in self.all_generative_model_classes:\n            if model_class._no_split_modules is None:\n                continue\n\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            model = model_class(config).eval()\n            with tempfile.TemporaryDirectory() as tmp_dir:\n                model.cpu().save_pretrained(tmp_dir)\n                new_model = model_class.from_pretrained(tmp_dir, device_map=\"auto\")\n\n                new_model.generate(\n                    max_new_tokens=self.max_new_tokens,\n                    num_beams=2,\n                    **inputs_dict,\n                )\n\n    @pytest.mark.generate\n    def test_beam_sample_generate(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            model = model_class(config).to(torch_device).eval()\n            beam_kwargs = self._get_beam_kwargs()\n            output_generate = self._beam_sample_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                beam_kwargs=beam_kwargs,\n            )\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n            else:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n\n    @pytest.mark.generate\n    def test_beam_sample_generate_dict_output(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n            if self.has_attentions:\n                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n\n            model = model_class(config).to(torch_device).eval()\n            beam_kwargs = self._get_beam_kwargs()\n\n            output_generate = self._beam_sample_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                beam_kwargs=beam_kwargs,\n                output_scores=True,\n                output_logits=True,\n                output_hidden_states=True,\n                output_attentions=self.has_attentions,\n                return_dict_in_generate=True,\n                use_cache=False,\n            )\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n                self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                # Retrocompatibility check\n                self.assertIsInstance(output_generate, BeamSampleEncoderDecoderOutput)\n            else:\n                self.assertTrue(\n                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                )\n                self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                # Retrocompatibility check\n                self.assertIsInstance(output_generate, BeamSampleDecoderOnlyOutput)\n\n            self._check_generate_outputs(\n                output_generate,\n                model.config,\n                num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n                num_beams=beam_kwargs[\"num_beams\"],\n            )\n\n    @pytest.mark.generate\n    def test_generate_without_input_ids(self):\n        config, _ = self.prepare_config_and_inputs_for_generate()\n\n        # if no bos token id => cannot generate from None\n        if config.bos_token_id is None:\n            self.skipTest(reason=\"bos_token_id is None\")\n\n        # hack in case they are equal, otherwise the attn mask will be [0]\n        if config.bos_token_id == config.pad_token_id:\n            config.pad_token_id = None\n\n        for model_class in self.all_generative_model_classes:\n            model = model_class(config).to(torch_device)\n            model.eval()\n\n            output_ids_generate = model.generate(\n                do_sample=False, max_new_tokens=self.max_new_tokens, remove_invalid_values=True\n            )\n            self.assertIsNotNone(output_ids_generate)\n\n    @pytest.mark.generate\n    def test_group_beam_search_generate(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            model = model_class(config).to(torch_device).eval()\n            # check `generate()` and `group_beam_search()` are equal\n            beam_kwargs = self._get_diverse_beam_kwargs()\n            output_generate = self._group_beam_search_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                beam_kwargs=beam_kwargs,\n            )\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n            else:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n\n            # check `group_beam_search` for higher than 1 `num_return_sequences`\n            num_return_sequences = 2\n            beam_kwargs = self._get_diverse_beam_kwargs(num_return_sequences=num_return_sequences)\n            output_generate = self._group_beam_search_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                beam_kwargs=beam_kwargs,\n            )\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n            else:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n\n    @pytest.mark.generate\n    def test_group_beam_search_generate_dict_output(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n            if self.has_attentions:\n                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n\n            model = model_class(config).to(torch_device).eval()\n            beam_kwargs = self._get_diverse_beam_kwargs()\n            output_generate = self._group_beam_search_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                beam_kwargs=beam_kwargs,\n                output_scores=True,\n                output_logits=True,\n                output_hidden_states=True,\n                output_attentions=self.has_attentions,\n                return_dict_in_generate=True,\n                use_cache=False,\n            )\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n                self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                # Retrocompatibility check\n                self.assertIsInstance(output_generate, BeamSearchEncoderDecoderOutput)\n            else:\n                self.assertTrue(\n                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                )\n                self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                # Retrocompatibility check\n                self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n\n            self._check_generate_outputs(\n                output_generate,\n                model.config,\n                num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n                num_beams=beam_kwargs[\"num_beams\"],\n            )\n\n    # TODO: @gante check why it is flaky\n    @is_flaky()\n    @pytest.mark.generate\n    def test_constrained_beam_search_generate(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            model = model_class(config).to(torch_device).eval()\n\n            # Sample constraints\n            min_id = 3\n            max_id = config.get_text_config(decoder=True).vocab_size\n\n            force_tokens = torch.randint(min_id, max_id, (1, 2)).tolist()[0]\n            constraints = [\n                PhrasalConstraint(force_tokens),\n            ]\n\n            beam_kwargs = self._get_constrained_beam_kwargs()\n            output_generate = self._constrained_beam_search_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                constraints=constraints,\n                beam_kwargs=beam_kwargs,\n            )\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n            else:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n\n            for generation_output in output_generate:\n                self._check_sequence_inside_sequence(force_tokens, generation_output)\n\n            # check`constrained_beam_search` for higher than 1 `num_return_sequences`\n            # Sample constraints\n            force_tokens = torch.randint(min_id, max_id, (1, 2)).tolist()[0]\n            constraints = [\n                PhrasalConstraint(force_tokens),\n            ]\n\n            beam_kwargs = self._get_constrained_beam_kwargs(num_return_sequences=2)\n\n            output_generate = self._constrained_beam_search_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                constraints=constraints,\n                beam_kwargs=beam_kwargs,\n            )\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n            else:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n\n            for generation_output in output_generate:\n                self._check_sequence_inside_sequence(force_tokens, generation_output)\n\n    @pytest.mark.generate\n    def test_constrained_beam_search_generate_dict_output(self):\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n            if self.has_attentions:\n                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n\n            model = model_class(config).to(torch_device).eval()\n\n            # Sample constraints\n            min_id = 3\n            max_id = model.config.get_text_config(decoder=True).vocab_size\n            force_tokens = torch.randint(min_id, max_id, (1, 2)).tolist()[0]\n            constraints = [\n                PhrasalConstraint(force_tokens),\n            ]\n\n            beam_kwargs = self._get_constrained_beam_kwargs()\n            output_generate = self._constrained_beam_search_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                constraints=constraints,\n                beam_kwargs=beam_kwargs,\n                output_scores=True,\n                output_logits=True,\n                output_hidden_states=True,\n                output_attentions=self.has_attentions,\n                return_dict_in_generate=True,\n                use_cache=False,\n            )\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n                self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                # Retrocompatibility check\n                self.assertIsInstance(output_generate, BeamSearchEncoderDecoderOutput)\n            else:\n                self.assertTrue(\n                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                )\n                self.assertIsInstance(output_generate, GenerateBeamDecoderOnlyOutput)\n                # Retrocompatibility check\n                self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n\n            self._check_generate_outputs(\n                output_generate,\n                model.config,\n                num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n                num_beams=beam_kwargs[\"num_beams\"],\n            )\n\n    @pytest.mark.generate\n    def test_contrastive_generate(self):\n        for model_class in self.all_generative_model_classes:\n            if model_class._is_stateful:\n                self.skipTest(reason=\"Stateful models don't support contrastive search generation\")\n\n            # won't fix: FSMT and Reformer have a different cache variable type (and format).\n            if any(model_name in model_class.__name__.lower() for model_name in [\"fsmt\", \"reformer\"]):\n                self.skipTest(reason=\"Won't fix: old model with different cache format\")\n\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            # NOTE: contrastive search only works with cache on at the moment.\n            if not hasattr(config.get_text_config(), \"use_cache\"):\n                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n            config.is_decoder = True\n\n            # test old generation output for backwards compatibility\n            model = model_class(config).to(torch_device).eval()\n            output_generate = self._contrastive_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                use_cache=True,  # Enable cache\n            )\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n            else:\n                self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1])\n\n    @pytest.mark.generate\n    def test_contrastive_generate_dict_outputs_use_cache(self):\n        for model_class in self.all_generative_model_classes:\n            if model_class._is_stateful:\n                self.skipTest(reason=\"Stateful models don't support contrastive search generation\")\n\n            # won't fix: FSMT and Reformer have a different cache variable type (and format).\n            if any(model_name in model_class.__name__.lower() for model_name in [\"fsmt\", \"reformer\"]):\n                self.skipTest(reason=\"Won't fix: old model with different cache format\")\n\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            # NOTE: contrastive search only works with cache on at the moment.\n            if not hasattr(config.get_text_config(), \"use_cache\"):\n                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n            config.is_decoder = True\n            if self.has_attentions:\n                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n\n            model = model_class(config).to(torch_device).eval()\n            output_generate = self._contrastive_generate(\n                model=model,\n                inputs_dict=inputs_dict,\n                output_scores=True,\n                output_logits=True,\n                output_hidden_states=True,\n                output_attentions=self.has_attentions,\n                return_dict_in_generate=True,\n                use_cache=True,  # Enable cache\n            )\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n            else:\n                self.assertTrue(\n                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                )\n\n            self._check_generate_outputs(output_generate, model.config, use_cache=True)\n\n    @pytest.mark.generate\n    def test_contrastive_generate_low_memory(self):\n        # Check that choosing 'low_memory' does not change the model output\n        for model_class in self.all_generative_model_classes:\n            if model_class._is_stateful:\n                self.skipTest(reason=\"Stateful models don't support contrastive search generation\")\n\n            if any(model_name in model_class.__name__.lower() for model_name in [\"fsmt\", \"reformer\", \"speech2text\"]):\n                self.skipTest(reason=\"Won't fix: old model with different cache format\")\n            if any(model_name in model_class.__name__.lower() for model_name in [\"gptbigcode\"]):\n                self.skipTest(reason=\"TODO: fix me\")\n\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n\n            # NOTE: contrastive search only works with cache on at the moment.\n            if not hasattr(config.get_text_config(), \"use_cache\"):\n                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n\n            config.is_decoder = True\n\n            # test output equality of low versus high memory\n            model = model_class(config).to(torch_device).eval()\n\n            low_output = model.generate(\n                top_k=4,\n                penalty_alpha=0.6,\n                low_memory=True,\n                max_new_tokens=self.max_new_tokens,\n                **inputs_dict,\n                use_cache=True,\n            )\n\n            high_output = model.generate(\n                top_k=4,\n                penalty_alpha=0.6,\n                low_memory=False,\n                max_new_tokens=self.max_new_tokens,\n                **inputs_dict,\n                use_cache=True,\n            )\n            self.assertListEqual(low_output.tolist(), high_output.tolist())\n\n    @parameterized.expand([(\"random\",), (\"same\",)])\n    @pytest.mark.generate\n    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n        # This test ensures that the assisted generation does not introduce output changes over greedy search.\n        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535 for more info.\n        # NOTE: It breaks the pattern in the tests above, for multiple reasons:\n        # - assisted_decoding, contrarily to the other methods, can't be called on its own (e.g. needs to\n        # prepare the assistant encoder outputs in the main generate body);\n        # - assisted_decoding does not support `use_cache = False`\n        # - assisted_decoding does not support `batch_size > 1`\n\n        for model_class in self.all_generative_model_classes:\n            if model_class._is_stateful:\n                self.skipTest(reason=\"Stateful models don't support assisted generation\")\n            if any(model_name in model_class.__name__.lower() for model_name in [\"fsmt\", \"reformer\"]):\n                self.skipTest(reason=\"Won't fix: old model with different cache format\")\n            if any(\n                model_name in model_class.__name__.lower()\n                for model_name in [\n                    \"bigbirdpegasus\",\n                    \"led\",\n                    \"mega\",\n                    \"moshi\",\n                    \"speech2text\",\n                    \"git\",\n                    \"prophetnet\",\n                    \"seamlessm4t\",\n                    \"clvp\",\n                    \"mllama\",  # special cache sizes\n                    \"blip2\",  # overridden `generate()`\n                    \"instructblip\",\n                    \"instructblipvideo\",\n                ]\n            ):\n                self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n\n            # enable cache\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n\n            # NOTE: assisted generation only works with cache on at the moment.\n            if not hasattr(config.get_text_config(), \"use_cache\"):\n                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n\n            config.is_decoder = True\n            model = model_class(config).to(torch_device).eval()\n            # Sets assisted generation arguments such that:\n            # a) no EOS is generated, to ensure generation doesn't break early\n            # b) the assistant model always generates two tokens when it is called, to ensure the input preparation of\n            #    the assistant model is correct\n            # c) there are at least two forward passes in the main model, to ensure the input preparation of\n            #    the main model is correct\n            generation_kwargs = {\n                \"eos_token_id\": -1,  # see a)\n                \"max_new_tokens\": 4,  # see c)\n                \"num_beams\": 1,\n                \"do_sample\": False,\n                \"output_scores\": True,\n                \"output_logits\": True,\n                \"output_hidden_states\": True,\n                \"output_attentions\": self.has_attentions,\n                \"return_dict_in_generate\": True,\n                \"use_cache\": True,\n            }\n            logits_processor_kwargs = self._get_logits_processor_kwargs(config=model.config)\n\n            output_greedy = model.generate(**generation_kwargs, **inputs_dict, **logits_processor_kwargs)\n\n            # test with the same assistant model or randomly init one\n            # in the first case all candidate tokens are accepted, in the second none is accepted\n            # case when some are accepted and some not is hard to reproduce, so let's hope this catches most errors :)\n            if assistant_type == \"random\":\n                assistant_model = model_class(config).to(torch_device).eval()\n            else:\n                assistant_model = model\n            assistant_model.generation_config.num_assistant_tokens = 2  # see b)\n            assistant_model.generation_config.num_assistant_tokens_schedule = \"constant\"  # see b)\n            generation_kwargs.update({\"assistant_model\": assistant_model})\n            output_assisted = model.generate(**generation_kwargs, **inputs_dict, **logits_processor_kwargs)\n\n            # The two outputs must match and their shape must be as expected\n            self._check_similar_generate_outputs(output_greedy, output_assisted)\n            for output in (output_greedy, output_assisted):\n                self._check_generate_outputs(output, model.config, use_cache=True)\n\n    @pytest.mark.generate\n    def test_prompt_lookup_decoding_matches_greedy_search(self):\n        # This test ensures that the prompt lookup generation does not introduce output changes over greedy search.\n        # This test is mostly a copy of test_assisted_decoding_matches_greedy_search\n\n        for model_class in self.all_generative_model_classes:\n            if model_class._is_stateful:\n                self.skipTest(reason=\"Stateful models don't support assisted generation\")\n            if any(model_name in model_class.__name__.lower() for model_name in [\"fsmt\", \"reformer\"]):\n                self.skipTest(reason=\"Won't fix: old model with different cache format\")\n            if any(\n                model_name in model_class.__name__.lower()\n                for model_name in [\n                    \"bigbirdpegasus\",\n                    \"led\",\n                    \"mega\",\n                    \"moshi\",\n                    \"speech2text\",\n                    \"git\",\n                    \"prophetnet\",\n                    \"seamlessm4t\",\n                    \"clvp\",\n                    \"fuyu\",\n                    \"mllama\",  # special cache sizes\n                    \"blip2\",  # overridden `generate()`\n                    \"instructblip\",\n                    \"instructblipvideo\",\n                    *VLM_CLASS_NAMES,  # shouldn't suggest image tokens\n                ]\n            ):\n                self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n\n            # enable cache\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n\n            # NOTE: assisted generation only works with cache on at the moment.\n            if not hasattr(config.get_text_config(), \"use_cache\"):\n                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n\n            config.is_decoder = True\n            model = model_class(config).to(torch_device).eval()\n            # Sets assisted generation arguments such that:\n            # a) no EOS is generated, to ensure generation doesn't break early\n            # b) the prompt lookup tries to give the model 2 tokens, to ensure the input preparation of\n            #    prompt lookup is correct\n            # c) there are at least two forward passes in the main model, to ensure the input preparation of\n            #    the main model is correct\n            generation_kwargs = {\n                \"eos_token_id\": -1,  # see a)\n                \"max_new_tokens\": 4,  # see c)\n                \"num_beams\": 1,\n                \"do_sample\": False,\n                \"output_scores\": True,\n                \"output_logits\": True,\n                \"output_hidden_states\": True,\n                \"output_attentions\": self.has_attentions,\n                \"return_dict_in_generate\": True,\n                \"use_cache\": True,\n            }\n\n            output_greedy = model.generate(**generation_kwargs, **inputs_dict)\n\n            generation_kwargs.update({\"prompt_lookup_num_tokens\": 2})  # see b)\n            output_prompt_lookup = model.generate(**generation_kwargs, **inputs_dict)\n\n            # The two outputs must match and their shape must be as expected\n            self._check_similar_generate_outputs(output_greedy, output_prompt_lookup)\n            for output in (output_greedy, output_prompt_lookup):\n                self._check_generate_outputs(output, model.config, use_cache=True)\n\n    @pytest.mark.generate\n    def test_dola_decoding_sample(self):\n        # TODO (joao): investigate skips, try to reduce incompatibilities\n        for model_class in self.all_generative_model_classes:\n            if model_class._is_stateful:\n                self.skipTest(reason=\"Stateful models don't support DoLa decoding\")\n\n            if any(model_name in model_class.__name__.lower() for model_name in [\"reformer\"]):\n                self.skipTest(\"Skip Reformer as the lm_head input size is 2 * hidden size, adopted from Rev Nets.\")\n\n            if any(model_name in model_class.__name__.lower() for model_name in [\"marian\", \"mbart\", \"pegasus\"]):\n                self.skipTest(\"DoLa is not supported for models that don't return layerwise hidden states\")\n\n            if any(model_name == model_class.__name__ for model_name in [\"LlavaNextVideoForConditionalGeneration\"]):\n                self.skipTest(f\"DoLa is failing for {model_class.__name__}\")\n\n            # enable cache if the model is not openai-gpt, xlnet, cpm, or xlm\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            # Encoder-decoder models are not supported\n            if config.is_encoder_decoder:\n                self.skipTest(\"DoLa is not supported for encoder-decoder models\")\n            config.is_decoder = True\n            model = model_class(config).to(torch_device).eval()\n\n            if model.get_output_embeddings() is None:\n                self.skipTest(\"DoLa is not supported for models that don't have output embeddings\")\n\n            logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True, config=model.config)\n\n            # Sets dola generation arguments such that:\n            # a) no EOS is generated, to ensure generation doesn't break early\n            # b) there are at least two forward passes in the main model, to ensure the input preparation of\n            #    the main model is correct\n            generation_kwargs = {\n                \"eos_token_id\": -1,  # see a)\n                \"max_new_tokens\": 4,  # see b)\n                \"num_beams\": 1,\n                \"do_sample\": True,\n                \"output_scores\": True,\n                \"output_logits\": True,\n                \"output_hidden_states\": True,\n                \"output_attentions\": self.has_attentions,\n                \"return_dict_in_generate\": True,\n                \"use_cache\": getattr(config, \"use_cache\", False),  # Some models don't support the cache\n                \"dola_layers\": \"low\",\n            }\n            output_dola = model.generate(**generation_kwargs, **logits_processor_kwargs, **inputs_dict)\n            self._check_generate_outputs(output_dola, model.config, use_cache=getattr(config, \"use_cache\", False))\n\n    @pytest.mark.generate\n    def test_assisted_decoding_sample(self):\n        # In this test we don't check assisted vs non-assisted output -- seeded assisted decoding with sample will not\n        # match sample for the same seed, as the forward pass does not return the exact same logits (due to matmul with\n        # different shapes, see https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535).\n        for model_class in self.all_generative_model_classes:\n            if model_class._is_stateful:\n                self.skipTest(reason=\"Stateful models don't support assisted generation\")\n            if any(model_name in model_class.__name__.lower() for model_name in [\"fsmt\", \"reformer\"]):\n                self.skipTest(reason=\"Won't fix: old model with different cache format\")\n            if any(\n                model_name in model_class.__name__.lower()\n                for model_name in [\n                    \"bigbirdpegasus\",\n                    \"led\",\n                    \"mega\",\n                    \"moshi\",\n                    \"speech2text\",\n                    \"git\",\n                    \"prophetnet\",\n                    \"seamlessm4t\",\n                    \"clvp\",\n                    \"mllama\",  # special cache sizes\n                    \"blip2\",  # overridden `generate()`\n                    \"instructblip\",\n                    \"instructblipvideo\",\n                ]\n            ):\n                self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n\n            # enable cache\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n\n            # NOTE: assisted generation only works with cache on at the moment.\n            if not hasattr(config.get_text_config(), \"use_cache\"):\n                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n\n            config.is_decoder = True\n            model = model_class(config).to(torch_device).eval()\n            # Sets assisted generation arguments such that:\n            # a) no EOS is generated, to ensure generation doesn't break early\n            # b) the assistant model always generates two tokens when it is called, to ensure the input preparation of\n            #    the assistant model is correct\n            # c) there are at least two forward passes in the main model, to ensure the input preparation of\n            #    the main model is correct\n            assistant_model = model\n            assistant_model.generation_config.num_assistant_tokens = 2  # see b)\n            assistant_model.generation_config.num_assistant_tokens_schedule = \"constant\"  # see b)\n            generation_kwargs = {\n                \"eos_token_id\": -1,  # see a)\n                \"max_new_tokens\": 4,  # see c)\n                \"num_beams\": 1,\n                \"do_sample\": True,\n                \"assistant_model\": assistant_model,\n                \"output_scores\": True,\n                \"output_logits\": True,\n                \"output_hidden_states\": True,\n                \"output_attentions\": self.has_attentions,\n                \"return_dict_in_generate\": True,\n                \"use_cache\": True,\n            }\n            logits_processor_kwargs = self._get_logits_processor_kwargs(config=model.config)\n            output_assisted = model.generate(**generation_kwargs, **inputs_dict, **logits_processor_kwargs)\n\n            self._check_generate_outputs(output_assisted, config, use_cache=True)\n\n    @pytest.mark.generate\n    def test_prompt_lookup_decoding_stops_at_eos(self):\n        # This test ensures that the prompt lookup generation stops at eos token and does not suggest more tokens\n        # (see https://github.com/huggingface/transformers/pull/31301)\n\n        # The main idea is to have an ngram (unigram in our case) that is repeated twice in the input ids.\n        # First time at the very end, so input ends with the unigrams, and second any arbitrary location.\n        # Also, we need an EOS token which will be injected just after the arbitrary located ngram.\n        # We verify that PLD will not copy and propose candidated that contain an EOS token, even if there are overlapping ngrams\n        # in input ids. Otherwise a proposed EOS along with the trailing (ngrams-1) tokens might be accepted by the target model.\n        # That seems as if the model \"generated\" and EOS but didn't stop from user's perspective\n\n        input_ids = torch.randint(1, 50, (1, 10), device=torch_device)  # generate inputs in range from 1-50\n        arbitrary_ngram = 51  # this is the arbitrary ngram, specifically chosen OOV to prevent flaky tests\n        input_ids[:, 3] = arbitrary_ngram  # set pre-eos to arbitrary_ngram which is for sure not present in inputs\n        input_ids[:, -1] = arbitrary_ngram  # put arbitrary_ngram in the end for the necessary match to happen\n\n        eos_token_id = torch.tensor([0], device=torch_device)\n        input_ids[:, 4] = eos_token_id  # inject eos-token-id in input ids so that it is located after arbitrary_ngram\n\n        # init cand geenerator with max_matching_ngram_size=1 to match per-token\n        candidate_generator = PromptLookupCandidateGenerator(\n            eos_token_id=eos_token_id, num_output_tokens=4, max_matching_ngram_size=1\n        )\n        output_prompt_lookup = candidate_generator.get_candidates(input_ids)[0]\n\n        # PLD shouldn't propose any new tokens based on eos-match\n        self.assertTrue(output_prompt_lookup.shape[-1] == 10)\n\n    @pytest.mark.generate\n    def test_generate_with_head_masking(self):\n        \"\"\"Test designed for encoder-decoder models to ensure the attention head masking is used.\"\"\"\n        attention_names = [\"encoder_attentions\", \"decoder_attentions\", \"cross_attentions\"]\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n            text_config = config.get_text_config()\n            if self.has_attentions:\n                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n\n            # We want to test only encoder-decoder models\n            if not text_config.is_encoder_decoder:\n                continue\n            model = model_class(config).to(torch_device)\n\n            head_masking = {\n                \"head_mask\": torch.zeros(\n                    text_config.encoder_layers, text_config.encoder_attention_heads, device=torch_device\n                ),\n                \"decoder_head_mask\": torch.zeros(\n                    text_config.decoder_layers, text_config.decoder_attention_heads, device=torch_device\n                ),\n                \"cross_attn_head_mask\": torch.zeros(\n                    text_config.decoder_layers, text_config.decoder_attention_heads, device=torch_device\n                ),\n            }\n\n            signature = inspect.signature(model.forward)\n            # We want to test only models where encoder/decoder head masking is implemented\n            if not set(head_masking.keys()) < {*signature.parameters.keys()}:\n                continue\n\n            for attn_name, (name, mask) in zip(attention_names, head_masking.items()):\n                out = model.generate(\n                    num_beams=1,\n                    output_attentions=self.has_attentions,\n                    return_dict_in_generate=True,\n                    remove_invalid_values=True,\n                    **{name: mask},\n                    **inputs_dict,\n                )\n                # We check the state of decoder_attentions and cross_attentions just from the last step\n                attn_weights = out[attn_name] if attn_name == attention_names[0] else out[attn_name][-1]\n                self.assertEqual(sum([w.sum().item() for w in attn_weights]), 0.0)\n\n    @pytest.mark.generate\n    def test_left_padding_compatibility(self):\n        # NOTE: left-padding results in small numerical differences. This is expected.\n        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n\n        # First, filter out models that don't support left padding\n        # - The model must have generative capabilities\n        if len(self.all_generative_model_classes) == 0:\n            self.skipTest(reason=\"No generative architecture available for this model.\")\n\n        # - The model must support padding\n        if not self.has_attentions:\n            self.skipTest(reason=\"This model doesn't support padding.\")\n\n        # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n        decoder_only_classes = []\n        for model_class in self.all_generative_model_classes:\n            config, _ = self.prepare_config_and_inputs_for_generate()\n            if config.is_encoder_decoder:\n                continue\n            else:\n                decoder_only_classes.append(model_class)\n        if len(decoder_only_classes) == 0:\n            self.skipTest(reason=\"No decoder-only architecture available for this model.\")\n\n        # - Decoder-only architectures derived from encoder-decoder models could support it in theory, but we haven't\n        #   added support for it yet. We skip these models for now.\n        has_encoder_attributes = any(\n            attr_name\n            for attr_name in config.to_dict().keys()\n            if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n        )\n        if has_encoder_attributes:\n            self.skipTest(\n                reason=\"The decoder-only derived from encoder-decoder models are not expected to support left-padding.\"\n            )\n\n        # Then, test left-padding\n        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n            if \"position_ids\" in signature:\n                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n                position_ids.masked_fill_(attention_mask == 0, 1)\n                model_kwargs[\"position_ids\"] = position_ids\n            if \"cache_position\" in signature:\n                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n                model_kwargs[\"cache_position\"] = cache_position\n            return model_kwargs\n\n        for model_class in decoder_only_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n            input_ids = inputs_dict[\"input_ids\"]\n            attention_mask = inputs_dict.get(\"attention_mask\")\n            if attention_mask is None:\n                attention_mask = torch.ones_like(input_ids)\n\n            model = model_class(config).to(torch_device).eval()\n            signature = inspect.signature(model.forward).parameters.keys()\n\n            # no cache as some models require special cache classes to be init outside forward\n            model.generation_config.use_cache = False\n\n            # Without padding\n            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n            next_logits_wo_padding = model(**model_kwargs).logits[:, -1, :]\n\n            # With left-padding (length 32)\n            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n            pad_token_id = (\n                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n            )\n            pad_size = (input_ids.shape[0], 32)\n            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n            next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n\n            # They should result in very similar logits\n            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n\n    @pytest.mark.generate\n    def test_past_key_values_format(self):\n        # Test that the KV cache is formatted correctly. Exceptions need to explicitly overwrite this test. Having a\n        # standard KV cache format is important for a consistent API (and for advanced generation methods).\n        for model_class in self.all_generative_model_classes:\n            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n\n            # If it doesn't support cache, pass the test\n            if not hasattr(config.get_text_config(), \"use_cache\"):\n                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n\n            model = model_class(config).to(torch_device)\n            if \"use_cache\" not in inputs:\n                inputs[\"use_cache\"] = True\n            outputs = model(**inputs)\n\n            # If \"past_key_values\" is not returned, pass the test (e.g. RWKV uses a different cache name and format)\n            if \"past_key_values\" not in outputs:\n                self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n\n            text_config = config.get_text_config()\n            num_hidden_layers = (\n                getattr(text_config, \"decoder_layers\", None)\n                or getattr(text_config, \"num_decoder_layers\", None)\n                or text_config.num_hidden_layers\n            )\n            num_attention_heads = getattr(text_config, \"decoder_attention_heads\", text_config.num_attention_heads)\n            embed_dim = getattr(text_config, \"d_model\", text_config.hidden_size)\n            per_head_embed_dim = embed_dim // num_attention_heads\n\n            # some models have different num-head for query vs key/value so we need to assign correct value\n            # BUT only after `per_head_embed_dim` is set\n            num_attention_heads = (\n                text_config.num_key_value_heads\n                if getattr(text_config, \"num_key_value_heads\", None) is not None\n                else num_attention_heads\n            )\n\n            past_kv = outputs[\"past_key_values\"]\n            self.assertEqual(len(past_kv), num_hidden_layers)\n\n            # Encoder-Decoder checks\n            if config.is_encoder_decoder:\n                # encoder-decoder models usually don't have text config\n                # below is needed only for Pix2Struct which we cannot modify now due to BC\n                config = config.get_text_config()\n                encoder_num_attention_heads = (\n                    config.encoder_attention_heads\n                    if hasattr(config, \"encoder_attention_heads\")\n                    else config.num_attention_heads\n                )\n                encoder_per_head_embed_dim = embed_dim // encoder_num_attention_heads\n                batch_size, seq_length = inputs[\"decoder_input_ids\"].shape\n                for i in range(num_hidden_layers):\n                    self.assertEqual(len(past_kv[i]), 4)  # K V for the decoder + K V for the encoder = 4\n                    self.assertEqual(\n                        past_kv[i][0].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n                    )\n                    self.assertEqual(\n                        past_kv[i][1].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n                    )\n                    # The sequence length for the encoder K V depends on the model. Since it is not manipulated in\n                    # autoregressive generation, I'm keeping the test general and not checking the 3rd dim\n                    self.assertEqual(\n                        (past_kv[i][2].shape[0], past_kv[i][2].shape[1], past_kv[i][2].shape[3]),\n                        (batch_size, encoder_num_attention_heads, encoder_per_head_embed_dim),\n                    )\n                    self.assertEqual(\n                        (past_kv[i][3].shape[0], past_kv[i][3].shape[1], past_kv[i][3].shape[3]),\n                        (batch_size, encoder_num_attention_heads, encoder_per_head_embed_dim),\n                    )\n\n            # Decoder-only checks\n            else:\n                # TODO: this line is only needed because of imagegpt, where \"pixel_values\" = \"input_ids\". Fix the\n                # tests in imagegpt such that `prepare_config_and_inputs_for_common` returns the later (and the other\n                # tests use it)\n                key = \"input_ids\" if \"input_ids\" in inputs else \"pixel_values\"\n                batch_size, seq_length = inputs[key].shape\n                for i in range(num_hidden_layers):\n                    self.assertEqual(len(past_kv[0]), 2)  # K V for the decoder = 2\n                    self.assertEqual(\n                        past_kv[i][0].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n                    )\n                    self.assertEqual(\n                        past_kv[i][1].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n                    )\n\n    @pytest.mark.generate\n    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n    def test_generate_from_inputs_embeds(self, _, num_beams):\n        \"\"\"Tests that we can generate from `inputs_embeds` instead of `input_ids` in LLMs, VLMs, etc\"\"\"\n        # When supported, tests that the decoder model can generate from `inputs_embeds` instead of `input_ids`\n        # if fails, you should probably update the `prepare_inputs_for_generation` function\n        for model_class in self.all_generative_model_classes:\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            # This test is for decoder-only models (encoder-decoder models have native input embeddings support in the\n            # decoder)\n            if config.is_encoder_decoder:\n                continue\n            config.is_decoder = True\n\n            # Skip models without explicit support\n            model = model_class(config).to(torch_device).eval()\n            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n                continue\n\n            # There are a few exception patterns in this test:\n            # 1 - Some models can't generate without `input_ids`, when `inputs_embeds` are passed\n            requires_inputs_ids = any(model_name in model_class.__name__.lower() for model_name in [\"idefics\"])\n            # 2 - Complex `inputs_embeds` computation, i.e. the correct computation of inputs embeds is more complex\n            # than calling the embedding layer with `input_ids`. Subcases of this exception:\n            #   2.A - Ignore `scale_embedding`, if the model supports it (it is controlled by a model-dependent flag)\n            if hasattr(config, \"scale_embedding\"):\n                config.scale_embedding = False\n            #   2.B - Some VLMs assume `inputs_embeds` and `pixel_values` are mutually exclusive AND fall in the\n            #   exception above (complex `inputs_embeds` computation). Popping `pixel_values` allow us to run the\n            #   checks without adding test complexity. Ditto for `pixel_values_videos` and `pixel_values_images`\n            pixel_values_is_mutually_exclusive = any(\n                model_name in model_class.__name__.lower() for model_name in VLM_CLASS_NAMES\n            )\n            if pixel_values_is_mutually_exclusive:\n                inputs_dict.pop(\"pixel_values\", None)\n                inputs_dict.pop(\"pixel_values_videos\", None)\n                inputs_dict.pop(\"pixel_values_images\", None)\n            #   2.C - No easy fix, let's skip the check that compares the outputs from `input_ids` and `inputs_embeds`\n            has_complex_embeds_computation = any(\n                model_name in model_class.__name__.lower() for model_name in [\"moshi\"]\n            )\n            # 3 - `inputs_dict` doesn't contain `attention_mask`. When `attention_mask` is not passed to generate,\n            # we infer it from `input_ids`. The last test case will fail if there is a pad token in the original input.\n            missing_attention_mask = \"attention_mask\" not in inputs_dict\n\n            # Traditional way of generating text\n            input_ids = inputs_dict.pop(\"input_ids\")\n            generation_kwargs = {\n                \"return_dict_in_generate\": True,\n                \"output_scores\": True,\n                \"num_beams\": num_beams,\n                \"do_sample\": False,\n                \"max_new_tokens\": 5,\n                \"min_new_tokens\": 5,  # generate exactly 5 tokens\n            }\n            outputs_from_ids = model.generate(input_ids, **generation_kwargs, **inputs_dict)\n            self.assertEqual(outputs_from_ids.sequences.shape, (input_ids.shape[0], input_ids.shape[1] + 5))\n\n            # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output).\n            # The output of the two calls should be the same.\n            inputs_embeds = model.get_input_embeddings()(input_ids)\n            outputs_from_embeds = model.generate(\n                input_ids, inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict\n            )\n            if not has_complex_embeds_computation:\n                self._check_similar_generate_outputs(outputs_from_ids, outputs_from_embeds)\n\n            # If we pass different inputs_embeds, we should get different outputs (the output text may be the\n            # same, but the logits will almost surely be different)\n            random_embeds = torch.rand_like(inputs_embeds)\n            outputs_from_rand_embeds = model.generate(\n                input_ids, inputs_embeds=random_embeds, **generation_kwargs, **inputs_dict\n            )\n            for i in range(len(outputs_from_rand_embeds.scores)):\n                self.assertFalse(torch.allclose(outputs_from_embeds.scores[i], outputs_from_rand_embeds.scores[i]))\n\n            # input_ids is not a required input on most models -- if we don't pass it, the newly generated tokens will\n            # be the same\n            if not (requires_inputs_ids or missing_attention_mask):\n                outputs_from_embeds_wo_ids = model.generate(\n                    inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict\n                )\n                outputs_from_embeds.sequences = outputs_from_embeds.sequences[:, inputs_embeds.shape[1] :]\n                self._check_similar_generate_outputs(outputs_from_embeds_wo_ids, outputs_from_embeds)\n\n    @pytest.mark.generate\n    def test_generate_from_inputs_embeds_with_static_cache(self):\n        \"\"\"\n        Test that StaticCache can generate from inputs_embeds and calculates max_cache_length\n        correctly in `generate()`. We force the model to not stop generation until max-length is reached\n        to verify that the cache length is indeed set correctly and we don't run out of index when slicing the cache.\n        \"\"\"\n        for model_class in self.all_generative_model_classes:\n            if not model_class._supports_static_cache:\n                self.skipTest(reason=\"This model does not support the static cache format\")\n\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            if config.is_encoder_decoder:\n                self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n\n            model = model_class(config).to(torch_device).eval()\n            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n                self.skipTest(reason=\"This model does not support `inputs_embeds` in generation\")\n\n            #   Some VLMs assume `inputs_embeds` and `pixel_values` are mutually exclusive AND fall in the\n            #   exception above (complex `inputs_embeds` computation). Popping `pixel_values` allow us to run the\n            #   checks without adding test complexity. Ditto for `pixel_values_videos` and `pixel_values_images`\n            pixel_values_is_mutually_exclusive = any(\n                model_name in model_class.__name__.lower() for model_name in VLM_CLASS_NAMES\n            )\n            if pixel_values_is_mutually_exclusive:\n                inputs_dict.pop(\"pixel_values\", None)\n                inputs_dict.pop(\"pixel_values_videos\", None)\n                inputs_dict.pop(\"pixel_values_images\", None)\n\n            input_ids = inputs_dict.pop(\"input_ids\")\n\n            model.config.use_cache = True\n            model.config.is_decoder = True\n            batch_size = input_ids.shape[0]\n            max_new_tokens = 10\n\n            # here we force to not stop at eos and go until max-length\n            model.generation_config.eos_token_id = model.config.get_text_config().eos_token_id = -1\n            generation_kwargs = {\n                \"max_new_tokens\": max_new_tokens,\n                \"cache_implementation\": \"static\",\n                \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n            }\n\n            text_config = model.config.get_text_config()\n            head_dim = (\n                text_config.head_dim\n                if hasattr(text_config, \"head_dim\")\n                else text_config.hidden_size // text_config.num_attention_heads\n            )\n            num_key_value_heads = (\n                text_config.num_attention_heads\n                if getattr(text_config, \"num_key_value_heads\", None) is None\n                else text_config.num_key_value_heads\n            )\n            num_hidden_layers = text_config.num_hidden_layers\n\n            inputs_embeds = model.get_input_embeddings()(input_ids)\n            outputs = model.generate(inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict)\n\n            # we should get `max_length - 1` in shape, not `max_length - embeds_length`.\n            # -1 because the last generated token isn't yet in the cache.\n            max_length = max_new_tokens + inputs_embeds.shape[1] - 1\n            cache_shape = [batch_size, num_key_value_heads, max_length, head_dim]\n            self.assertIsInstance(outputs.past_key_values, StaticCache)\n            self.assertEqual(len(outputs.past_key_values.key_cache), num_hidden_layers)\n            self.assertListEqual(list(outputs.past_key_values.key_cache[0].shape), cache_shape)\n\n    @pytest.mark.generate\n    def test_generate_continue_from_past_key_values(self):\n        # Tests that we can continue generating from past key values, returned from a previous `generate` call\n        for model_class in self.all_generative_model_classes:\n            if any(model_name in model_class.__name__.lower() for model_name in [\"imagegpt\", \"mllama\"]):\n                self.skipTest(reason=\"Won't fix: old model with unique inputs/caches/other\")\n            if any(model_name in model_class.__name__.lower() for model_name in [\"umt5\"]):\n                self.skipTest(reason=\"TODO: needs modeling or test input preparation fixes for compatibility\")\n\n            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n\n            if not hasattr(config.get_text_config(), \"use_cache\"):\n                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n\n            # Let's make it always:\n            # 1. use cache (for obvious reasons)\n            # 2. generate to max length (which can be achieved by setting the eos token to an invalid value), which\n            #    would make the test flaky (e.g. EOS is generated on iteration 1 on both generations, but the\n            #    continuation would force it to generate beyond an EOS token)\n            # 3. ignore `token_type_ids` for simplicity\n            # 4. ignore `forced_eos_token_id`, which requires further manipulation of the continuation inputs and is\n            #    active by default on some models\n            # 5. ignore `encoder_no_repeat_ngram_size`, which is set by default in some encoder-decoder models. When\n            #    we use their decoder as a stand-alone model, `encoder_no_repeat_ngram_size` actually prevents\n            #    repetition exclusively from the prompt. This test relies on comparing one call vs 2 calls\n            #    with cache, what is considered a prompt is different in the two cases.\n\n            if \"token_type_ids\" in inputs:\n                del inputs[\"token_type_ids\"]\n\n            model = model_class(config).to(torch_device)\n            model.eval()\n            model.generation_config.pad_token_id = model.generation_config.eos_token_id = -1\n            model.generation_config.forced_eos_token_id = None\n            model.generation_config.encoder_no_repeat_ngram_size = 0\n            model.generation_config.use_cache = True\n\n            # If \"past_key_values\" is not returned, skip the test (e.g. RWKV uses a different cache name and format)\n            outputs = model(**inputs)\n            if \"past_key_values\" not in outputs:\n                self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n\n            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values\n            outputs = model.generate(**inputs, do_sample=False, max_new_tokens=4, return_dict_in_generate=True)\n\n            # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens). Note that the\n            # inputs may need to be tweaked across `generate` calls (like the attention mask).\n            outputs_cached = model.generate(**inputs, do_sample=False, max_new_tokens=3, return_dict_in_generate=True)\n\n            # Continue from the tokens generated above, preparing the inputs accordingly\n            inputs[\"past_key_values\"] = outputs_cached.past_key_values\n            new_attention_len = outputs_cached.sequences.shape[-1]\n            if config.is_encoder_decoder:\n                inputs[\"decoder_input_ids\"] = outputs_cached.sequences\n                if \"decoder_attention_mask\" in inputs:\n                    inputs[\"decoder_attention_mask\"] = torch.nn.functional.pad(\n                        inputs[\"decoder_attention_mask\"],\n                        (0, new_attention_len - inputs[\"decoder_attention_mask\"].shape[1]),\n                        mode=\"constant\",\n                        value=1,\n                    )\n            else:\n                inputs[\"input_ids\"] = outputs_cached.sequences\n                if \"attention_mask\" in inputs:\n                    inputs[\"attention_mask\"] = torch.nn.functional.pad(\n                        inputs[\"attention_mask\"],\n                        (0, new_attention_len - inputs[\"attention_mask\"].shape[1]),\n                        mode=\"constant\",\n                        value=1,\n                    )\n            outputs_cached = model.generate(**inputs, do_sample=False, max_new_tokens=1, return_dict_in_generate=True)\n\n            # The two sets of generated text and past kv should be equal to each other\n            self.assertListEqual(outputs.sequences.tolist(), outputs_cached.sequences.tolist())\n            for layer_idx in range(len(outputs_cached.past_key_values)):\n                for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n                    self.assertTrue(\n                        torch.allclose(\n                            outputs.past_key_values[layer_idx][kv_idx],\n                            outputs_cached.past_key_values[layer_idx][kv_idx],\n                        )\n                    )\n\n    @pytest.mark.generate\n    def test_generate_continue_from_inputs_embeds(self):\n        \"\"\"Tests that we can continue generation from `inputs_embeds` and past key values returned from a previous `generate` call.\"\"\"\n        for model_class in self.all_generative_model_classes:\n            if any(model_name in model_class.__name__.lower() for model_name in [\"imagegpt\"]):\n                self.skipTest(reason=\"Won't fix: old model with unique inputs/caches/other\")\n            if any(model_name in model_class.__name__.lower() for model_name in [\"umt5\"]):\n                self.skipTest(reason=\"TODO: needs modeling or test input preparation fixes for compatibility\")\n\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            if \"token_type_ids\" in inputs_dict:\n                del inputs_dict[\"token_type_ids\"]\n\n            if config.is_encoder_decoder:\n                self.skipTest(reason=\"This model is encoder-decoder\")\n            if not hasattr(config, \"use_cache\"):\n                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n\n            model = model_class(config).to(torch_device).eval()\n\n            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n                self.skipTest(reason=\"This model does not support `inputs_embeds` in generation\")\n\n            # If \"past_key_values\" is not returned, skip the test (e.g. RWKV uses a different cache name and format)\n            outputs = model(**inputs_dict)\n            if \"past_key_values\" not in outputs:\n                self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n\n            pixel_values_is_mutually_exclusive = any(\n                model_name in model_class.__name__.lower() for model_name in VLM_CLASS_NAMES\n            )\n            if pixel_values_is_mutually_exclusive:\n                inputs_dict.pop(\"pixel_values\", None)\n                inputs_dict.pop(\"pixel_values_videos\", None)\n                inputs_dict.pop(\"pixel_values_images\", None)\n\n            input_ids = inputs_dict.pop(\"input_ids\")\n\n            model.generation_config.pad_token_id = model.generation_config.eos_token_id = -1\n            model.generation_config.forced_eos_token_id = None\n            model.config.is_decoder = True\n            model.generation_config.use_cache = True\n\n            generation_kwargs = {\n                \"return_dict_in_generate\": True,\n                \"do_sample\": False,\n            }\n\n            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values.\n            input_embeds = model.get_input_embeddings()(input_ids)\n            outputs = model.generate(inputs_embeds=input_embeds, max_new_tokens=4, **generation_kwargs)\n\n            # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens)\n            initial_output = model.generate(inputs_embeds=input_embeds, max_new_tokens=3, **generation_kwargs)\n            continued_embeds = torch.cat([input_embeds, model.get_input_embeddings()(initial_output.sequences)], dim=1)\n            cached_output = model.generate(\n                inputs_embeds=continued_embeds,\n                max_new_tokens=1,\n                past_key_values=initial_output.past_key_values,\n                **generation_kwargs,\n            )\n\n            # Combine the (3 + 1) generated tokens and verify it matches with full generation.\n            combined_output_sequences = torch.concat([initial_output.sequences, cached_output.sequences], axis=1)\n            self.assertListEqual(outputs.sequences.tolist(), combined_output_sequences.tolist())\n            # The two sets of past kv should be equal to each other\n            for layer_idx in range(len(cached_output.past_key_values)):\n                for kv_idx in range(len(cached_output.past_key_values[layer_idx])):\n                    self.assertTrue(\n                        torch.allclose(\n                            outputs.past_key_values[layer_idx][kv_idx],\n                            cached_output.past_key_values[layer_idx][kv_idx],\n                        )\n                    )\n\n    @parameterized.expand([(\"offloaded\",)])  # (\"offloaded_static\",) TODO: @raushan fixme in some models (eg T5)\n    @require_torch_gpu\n    @pytest.mark.generate\n    def test_offloaded_cache_implementation(self, cache_implementation):\n        \"\"\"Tests we can generate by indicating `cache_implementation` for each possible cache class\"\"\"\n        for model_class in self.all_generative_model_classes:\n            if not model_class._supports_cache_class:\n                self.skipTest(reason=\"This model does not support the new cache format\")\n\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n\n            model = model_class(config).to(torch_device).eval()\n            generation_kwargs = {\n                \"max_new_tokens\": 5,\n                \"use_cache\": True,\n                \"cache_implementation\": cache_implementation,\n            }\n\n            legacy_results = model.generate(**generation_kwargs, **inputs_dict)\n\n            # Most cache classes have their own tests except for some that are tested here\n            # The ones here do not need special treatment when passing `cache_implementation`\n            # and are not bound to specific models only\n            new_results = model.generate(**generation_kwargs, **inputs_dict)\n            self.assertListEqual(legacy_results.tolist(), new_results.tolist())\n\n    @pytest.mark.generate\n    def test_generate_with_static_cache(self):\n        \"\"\"\n        Tests that generating with static cache give almost same results as with dynamic cache, and the output cache\n        has the expected shapes\n        \"\"\"\n        set_model_tester_for_less_flaky_test(self)\n        for model_class in self.all_generative_model_classes:\n            if not model_class._supports_static_cache:\n                self.skipTest(reason=\"This model does not support the static cache format\")\n\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n            set_config_for_less_flaky_test(config)\n            main_input = inputs_dict[model_class.main_input_name]\n\n            if config.is_encoder_decoder:\n                self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n\n            config.is_decoder = True\n            batch_size = main_input.shape[0]\n            seq_length = self.model_tester.seq_length\n            max_new_tokens = 20\n\n            for dtype in (torch.float32, torch.float16):\n                model = model_class(config).to(torch_device).to(dtype).eval()\n                inputs_dict = {\n                    k: v.to(dtype) if isinstance(v, torch.Tensor) and torch.is_floating_point(v) else v\n                    for k, v in inputs_dict.items()\n                }\n                set_model_for_less_flaky_test(model)\n\n                generation_kwargs = {\n                    \"max_new_tokens\": max_new_tokens,\n                    \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n                    \"output_scores\": True,\n                    \"use_cache\": True,\n                }\n\n                static_cache_generation = model.generate(\n                    **generation_kwargs, **inputs_dict, cache_implementation=\"static\"\n                )\n\n                # Check 1: The cache shapes must match the expected shapes\n                max_cache_len = seq_length + max_new_tokens - 1  # cache len = gen len - 1, the last token has no cache\n                text_config = config.text_config if hasattr(config, \"text_config\") else config\n                head_dim = (\n                    text_config.head_dim\n                    if hasattr(text_config, \"head_dim\")\n                    else text_config.hidden_size // text_config.num_attention_heads\n                )\n                num_key_value_heads = (\n                    text_config.num_attention_heads\n                    if getattr(text_config, \"num_key_value_heads\", None) is None\n                    else text_config.num_key_value_heads\n                )\n                num_hidden_layers = text_config.num_hidden_layers\n                cache_shape = (batch_size, num_key_value_heads, max_cache_len, head_dim)\n                self.assertTrue(isinstance(static_cache_generation.past_key_values, StaticCache))\n                self.assertTrue(len(static_cache_generation.past_key_values.key_cache) == num_hidden_layers)\n                self.assertTrue(static_cache_generation.past_key_values.key_cache[0].shape == cache_shape)\n\n                # Check 2: The outputs must be similar to the case with dynamic cache\n                dynamic_cache_generation = model.generate(**generation_kwargs, **inputs_dict)\n                self._check_similar_generate_outputs(dynamic_cache_generation, static_cache_generation)\n\n    @require_optimum_quanto\n    @pytest.mark.generate\n    def test_generate_with_quant_cache(self):\n        for model_class in self.all_generative_model_classes:\n            if not model_class._supports_quantized_cache:\n                self.skipTest(reason=\"This model does not support the quantized cache format\")\n\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n            config.is_decoder = True\n\n            model = model_class(config).to(torch_device).eval()\n            generation_kwargs = {\n                \"max_new_tokens\": 5,\n                \"cache_implementation\": \"quantized\",\n                # careful with group size, should be divisor of model's hidden size\n                \"cache_config\": {\"backend\": \"quanto\", \"nbits\": 2, \"q_group_size\": 8, \"residual_length\": 128},\n                \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n                \"use_cache\": True,\n            }\n\n            results = model.generate(**generation_kwargs, **inputs_dict)\n            self.assertTrue(isinstance(results.past_key_values, QuantoQuantizedCache))\n\n            # passing past key values of different type should raise Error\n            with self.assertRaises(ValueError):\n                model.generate(past_key_valyes=DynamicCache(), **generation_kwargs, **inputs_dict)\n\n            # setting incorrect cache_config args should raise an Error, i.e. nbits=60 does not make sense\n            generation_kwargs[\"cache_config\"] = {\"nbits\": 60, \"q_group_size\": 8, \"residual_length\": 128}\n            with self.assertRaises(ValueError):\n                model.generate(**generation_kwargs, **inputs_dict)\n\n    @pytest.mark.generate\n    def test_generate_compile_model_forward(self):\n        \"\"\"\n        Tests that `.generate` is compatible with torch.compile without graph breaks, keeping the same results.\n        ⚠️ Runs two sequential generations to ensure the cache doesn't get stuck after the first compiled run! ⚠️\n        \"\"\"\n        # Monkey-patching the HybridCache at test-time to continue testing compilation support\n        HybridCache.is_compileable = True\n\n        for model_class in self.all_generative_model_classes:\n            if not model_class._supports_static_cache:\n                self.skipTest(\"This model doesn't support static cache (= no expectations of compilation support)\")\n\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=4)\n\n            model = model_class(config).to(torch_device)\n            model.eval()  # otherwise `self.training` is `True` -- this flag is used at attn mask creation time\n\n            main_input = inputs_dict[model.main_input_name].to(torch_device)\n            # creates two sets of *different* inputs with the same shape\n            half_batch_size = main_input.shape[0] // 2\n            input_1 = {}\n            input_2 = {}\n            for key, value in inputs_dict.items():\n                if isinstance(value, torch.Tensor):\n                    input_1[key] = value[:half_batch_size, :].to(torch_device)\n                    input_2[key] = value[half_batch_size : half_batch_size * 2, :].to(torch_device)\n                else:\n                    input_1[key] = value\n                    input_2[key] = value\n            model_input_sets = [input_1, input_2]\n            self.assertTrue(\n                model_input_sets[0][model.main_input_name].shape == model_input_sets[1][model.main_input_name].shape\n            )\n\n            # compilation-specific setup\n            torch.compiler.reset()  # prevent cached compilation from being used in the test\n            has_defined_cache_implementation = model.generation_config.cache_implementation is not None\n\n            # BLIP is the only exception with custom generate which call `self.lm.generate()`\n            # We should avoid such calls in all subsequent multimodal models and try to make `generate()`\n            # compatible with multimodality\n            if \"blip\" in model.__class__.__name__.lower():\n                model.language_model.generation_config.compile_config._compile_all_devices = True\n            else:\n                # force compilation (e.g. fast CI, CPU\n                model.generation_config.compile_config._compile_all_devices = True\n\n            generation_kwargs = {\n                \"do_sample\": False,\n                \"max_new_tokens\": 5,\n                \"return_dict_in_generate\": True,\n                \"output_scores\": True,\n            }\n\n            # get eager + dynamic cache results for future comparison\n            dynamic_outputs = []\n            for model_inputs in model_input_sets:\n                gen_out = model.generate(**model_inputs, **generation_kwargs)\n                dynamic_outputs.append(gen_out)\n                # sanity checks for the default cache implementation\n                if not has_defined_cache_implementation:\n                    decoder_cache = (\n                        gen_out.past_key_values.self_attention_cache\n                        if config.is_encoder_decoder\n                        else gen_out.past_key_values\n                    )\n                    self.assertTrue(isinstance(decoder_cache, DynamicCache))\n                    self.assertFalse(decoder_cache.is_compileable)\n                    self.assertFalse(hasattr(model, \"_compiled_call\"))  # our auto compile should NOT have been called\n\n            # get compiled results -- relies on the automatic compilation triggered by specific \"cache_implementation\"\n            if not has_defined_cache_implementation:\n                generation_kwargs[\"cache_implementation\"] = \"static\"\n\n            compiled_outputs = []\n            for model_inputs in model_input_sets:\n                gen_out = model.generate(**model_inputs, **generation_kwargs)\n                compiled_outputs.append(gen_out)\n                # sanity checks\n                decoder_cache = (\n                    gen_out.past_key_values.self_attention_cache\n                    if config.is_encoder_decoder\n                    else gen_out.past_key_values\n                )\n                self.assertFalse(isinstance(decoder_cache, DynamicCache))\n                self.assertTrue(decoder_cache.is_compileable)\n\n                # BLIP is the only exception with custom generate which call `self.lm.generate()`\n                # We should avoid such calls in all subsequent multimodal models and try to make `generate()`\n                # compatible with multimodality\n                if \"blip\" in model.__class__.__name__.lower():\n                    self.assertTrue(hasattr(model.language_model, \"_compiled_call\"))\n                else:\n                    self.assertTrue(hasattr(model, \"_compiled_call\"))  # our auto compile should have been called\n\n            for dynamic_result, compiled_result in zip(dynamic_outputs, compiled_outputs):\n                self._check_similar_generate_outputs(dynamic_result, compiled_result)\n\n    @pytest.mark.generate\n    def test_generate_compilation_all_outputs(self):\n        \"\"\"\n        Tests that all optional outputs are behaving as expected when compilation is triggered.\n        In essence, it's the same as `test_greedy_generate_dict_outputs`, but with automatic compilation triggered.\n        \"\"\"\n        # Monkey-patching the HybridCache at test-time to continue testing compilation support\n        HybridCache.is_compileable = True\n\n        for model_class in self.all_generative_model_classes:\n            if not model_class._supports_static_cache:\n                self.skipTest(\"This model doesn't support static cache (= no expectations of compilation support)\")\n\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n            if self.has_attentions:\n                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n            model = model_class(config).to(torch_device).eval()\n\n            # compilation-specific setup\n            torch.compiler.reset()  # prevent cached compilation from being used in the test\n            has_defined_cache_implementation = model.generation_config.cache_implementation is not None\n\n            # BLIP is the only exception with custom generate which call `self.lm.generate()`\n            # We should avoid such calls in all subsequent multimodal models and try to make `generate()`\n            # compatible with multimodality\n            if \"blip\" in model.__class__.__name__.lower():\n                model.language_model.generation_config.compile_config._compile_all_devices = True\n                if not has_defined_cache_implementation:\n                    model.language_model.generation_config.cache_implementation = \"static\"\n            else:\n                # force compilation (e.g. fast CI, CPU)\n                model.generation_config.compile_config._compile_all_devices = True\n                if not has_defined_cache_implementation:\n                    model.generation_config.cache_implementation = \"static\"\n\n            logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n            output_generate = model.generate(\n                do_sample=False,\n                num_beams=1,\n                max_new_tokens=self.max_new_tokens,\n                min_new_tokens=self.max_new_tokens,\n                output_attentions=True,\n                output_hidden_states=True,\n                output_scores=True,\n                output_logits=True,\n                return_dict_in_generate=True,\n                use_cache=True,\n                **logits_processor_kwargs,\n                **inputs_dict,\n            )\n\n            if \"blip\" in model.__class__.__name__.lower():\n                self.assertTrue(hasattr(model.language_model, \"_compiled_call\"))\n            else:\n                self.assertTrue(hasattr(model, \"_compiled_call\"))  # our auto compile should have been called\n\n            if model.config.is_encoder_decoder:\n                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n                self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n            else:\n                self.assertTrue(\n                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                )\n                self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n\n            self._check_generate_outputs(output_generate, model.config, use_cache=True)\n\n    @pytest.mark.generate\n    def test_generate_methods_with_logits_to_keep(self):\n        for model_class in self.all_generative_model_classes:\n            if \"logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n                self.skipTest(reason=\"This model does not support `logits_to_keep` argument.\")\n\n            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n            config.use_cache = True\n            config.is_decoder = True\n\n            model = model_class(config).to(torch_device).eval()\n            # All generation methods (except assisted decoding) rely on always extracting the last token logits of the\n            # full logits matrix, so testing out only greedy search and assisted decoding is enough (if it works,\n            # other methods will work as well)\n            generation_kwargs = {\n                \"max_new_tokens\": 10,\n                \"do_sample\": False,\n            }\n\n            # Setting logits_to_keep at 0 keeps all logits (old behavior)\n            with_all_logits = model.generate(**generation_kwargs, **inputs_dict, logits_to_keep=0)\n            # By default, logits_to_keep is automatically set to 1 if not provided (new behavior)\n            without_all_logits = model.generate(**inputs_dict, **generation_kwargs)\n            self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n\n    @pytest.mark.generate\n    def test_inherits_generation_mixin(self):\n        \"\"\"\n        Tests that the model class directly inherits `GenerationMixin`, as opposed to relying on `PreTrainedModel`\n        to inherit it.\n        \"\"\"\n        for model_class in self.all_generative_model_classes:\n            self.assertTrue(\"GenerationMixin\" in str(model_class.__bases__))\n\n    def _test_attention_implementation(self, attn_implementation):\n        \"\"\"\n        Compares the output of generate with the eager attention implementation against other implementations.\n        NOTE: despite the test logic being the same, different implementations actually need different decorators, hence\n        this separate function.\n        \"\"\"\n        max_new_tokens = 30\n        support_flag = {\n            \"sdpa\": \"_supports_sdpa\",\n            \"flash_attention_2\": \"_supports_flash_attn_2\",\n        }\n\n        for model_class in self.all_generative_model_classes:\n            if not getattr(model_class, support_flag[attn_implementation]):\n                self.skipTest(f\"{model_class.__name__} does not support `attn_implementation={attn_implementation}`\")\n\n            config, original_inputs_dict = self.prepare_config_and_inputs_for_generate()\n            inputs_dict = {}\n            for input_name, input_data in original_inputs_dict.items():\n                if isinstance(input_data, torch.Tensor) and input_data.dtype in [torch.float32, torch.bfloat16]:\n                    inputs_dict[input_name] = input_data.to(torch.float16)\n                else:\n                    inputs_dict[input_name] = input_data\n            main_input = inputs_dict[model_class.main_input_name]\n\n            # make sure that all models have enough positions for generation\n            if hasattr(config, \"max_position_embeddings\"):\n                config.max_position_embeddings = max_new_tokens + main_input.shape[1] + 1\n\n            model = model_class(config)\n\n            with tempfile.TemporaryDirectory() as tmpdirname:\n                model.save_pretrained(tmpdirname)\n                del model\n                gc.collect()\n\n                generate_kwargs = {\n                    \"max_new_tokens\": max_new_tokens,\n                    \"do_sample\": False,\n                    \"return_dict_in_generate\": True,\n                    \"output_scores\": True,\n                    \"use_cache\": True,\n                }\n\n                model_eager = model_class.from_pretrained(\n                    tmpdirname,\n                    torch_dtype=torch.float16,\n                    low_cpu_mem_usage=True,\n                    attn_implementation=\"eager\",\n                ).to(torch_device)\n                res_eager = model_eager.generate(**inputs_dict, **generate_kwargs)\n                del model_eager\n                gc.collect()\n\n                model_attn = model_class.from_pretrained(\n                    tmpdirname,\n                    torch_dtype=torch.float16,\n                    low_cpu_mem_usage=True,\n                    attn_implementation=attn_implementation,\n                ).to(torch_device)\n                res_attn = model_attn.generate(**inputs_dict, **generate_kwargs)\n                del model_attn\n                gc.collect()\n\n                self._check_similar_generate_outputs(res_eager, res_attn, atol=1e-3, rtol=1e-3)\n\n    @pytest.mark.generate\n    @require_torch_sdpa\n    @slow\n    def test_eager_matches_sdpa_generate(self):\n        \"\"\"Tests that generate has equivalent outputs with SDPA and eager attention implementations.\"\"\"\n        self._test_attention_implementation(\"sdpa\")\n\n    @pytest.mark.flash_attn_test\n    @require_flash_attn\n    @require_torch_gpu\n    @slow\n    def test_eager_matches_fa2_generate(self):\n        \"\"\"Tests that generate has equivalent outputs with FA2 and eager attention implementations.\"\"\"\n        # TODO (@joao @raushan) -- this test is failing the output checks on most models, investigate. After fixing,\n        # check whether we still need the overwrites\n        self._test_attention_implementation(\"flash_attention_2\")\n\n    def _check_generate_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n        input_batch_size = int(output.sequences.shape[0] / num_return_sequences)\n        internal_batch_size = (\n            input_batch_size * num_beams if num_beams > 1 else input_batch_size * num_return_sequences\n        )\n\n        prompt_length = getattr(self.model_tester, \"seq_length\", None)\n        prompt_length = getattr(self.model_tester, \"encoder_seq_length\", prompt_length)\n        prompt_length = getattr(self.model_tester, \"text_seq_length\", prompt_length)\n\n        config = config.text_config if hasattr(config, \"text_config\") else config\n\n        generated_length = (\n            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - prompt_length\n        )\n        decoder_past_key_values = getattr(output, \"past_key_values\", None)\n        if config.is_encoder_decoder and isinstance(decoder_past_key_values, EncoderDecoderCache):\n            decoder_past_key_values = decoder_past_key_values.self_attention_cache\n\n        # in some models we subsample the sequence length in inner layers\n        if hasattr(self.model_tester, \"get_subsampled_output_lengths\"):\n            prompt_length = self.model_tester.get_subsampled_output_lengths(prompt_length)\n\n        # scores\n        self._check_scores(\n            batch_size=internal_batch_size, scores=output.scores, generated_length=generated_length, config=config\n        )\n\n        # unprocessed logits\n        self._check_logits(batch_size=internal_batch_size, logits=output.logits, config=config)\n\n        # Attentions\n        if self.has_attentions:\n            if config.is_encoder_decoder:\n                # encoder\n                self._check_encoder_attention_for_generate(\n                    attentions=output.encoder_attentions,\n                    batch_size=input_batch_size,\n                    config=config,\n                    prompt_length=prompt_length,\n                )\n                # decoder\n                self._check_attentions_for_generate(\n                    batch_size=internal_batch_size,\n                    attentions=output.decoder_attentions,\n                    prompt_length=1,  # the BOS token\n                    output_length=output.sequences.shape[-1],\n                    config=config,\n                    decoder_past_key_values=decoder_past_key_values,\n                )\n            else:\n                self._check_attentions_for_generate(\n                    batch_size=internal_batch_size,\n                    attentions=output.attentions,\n                    prompt_length=prompt_length,\n                    output_length=output.sequences.shape[-1],\n                    config=config,\n                    decoder_past_key_values=decoder_past_key_values,\n                )\n\n        # Hidden States\n        if config.is_encoder_decoder:\n            # encoder\n            self._check_encoder_hidden_states_for_generate(\n                hidden_states=output.encoder_hidden_states,\n                batch_size=input_batch_size,\n                config=config,\n                prompt_length=prompt_length,\n            )\n            # decoder\n            self._check_hidden_states_for_generate(\n                batch_size=internal_batch_size,\n                hidden_states=output.decoder_hidden_states,\n                prompt_length=1,  # the BOS token\n                output_length=output.sequences.shape[-1],\n                config=config,\n                use_cache=use_cache,\n            )\n        else:\n            self._check_hidden_states_for_generate(\n                batch_size=internal_batch_size,\n                hidden_states=output.hidden_states,\n                prompt_length=prompt_length,\n                output_length=output.sequences.shape[-1],\n                config=config,\n                use_cache=use_cache,\n            )\n\n        # Past Key Value States -- a few notes here:\n        # 1. Its inner sequence length is with respect to the inputs of the latest forward pass, hence the \"-1\"\n        # 2. We ignore models that have unique cache structures (e.g. mamba) or are in need of refatoring to match the\n        #    standard cache format (e.g.gptbigcode )\n        models_without_standard_cache = (\n            \"bamba\",\n            \"ctrl\",\n            \"fsmt\",\n            \"gptbigcode\",\n            \"mega\",\n            \"reformer\",\n            \"jamba\",\n            \"mamba\",\n            \"xlnet\",\n            \"zamba\",\n            \"zamba2\",\n        )\n        has_standard_cache = not any(\n            model_name in config.__class__.__name__.lower() for model_name in models_without_standard_cache\n        )\n        if has_standard_cache:\n            if use_cache:\n                cache_length = output.sequences.shape[-1] - 1\n                self._check_past_key_values_for_generate(\n                    batch_size=internal_batch_size,\n                    decoder_past_key_values=decoder_past_key_values,\n                    cache_length=cache_length,\n                    config=config,\n                )\n            elif use_cache is False:\n                self.assertTrue(decoder_past_key_values is None)\n\n    def _check_scores(self, batch_size, scores, generated_length, config):\n        vocab_size = config.get_text_config(decoder=True).vocab_size\n        expected_shape = (batch_size, vocab_size)\n        self.assertIsInstance(scores, tuple)\n        self.assertEqual(len(scores), generated_length)\n        self.assertListEqual([iter_scores.shape for iter_scores in scores], [expected_shape] * len(scores))\n\n    def _check_logits(self, batch_size, logits, config):\n        vocab_size = config.get_text_config(decoder=True).vocab_size\n        self.assertIsInstance(logits, tuple)\n        self.assertListEqual([iter_logits.shape[0] for iter_logits in logits], [batch_size] * len(logits))\n        # vocabulary difference equal to one (imagegptmodel?) or zero (all other models)\n        vocab_diff = vocab_size - logits[0].shape[-1]\n        self.assertTrue(vocab_diff in [0, 1])\n        self.assertListEqual([vocab_size - score.shape[-1] for score in logits], [vocab_diff] * len(logits))\n\n    def _check_attentions_for_generate(\n        self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n    ):\n        self.assertIsInstance(attentions, tuple)\n        self.assertListEqual(\n            [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n        )\n        self.assertEqual(len(attentions), (output_length - prompt_length))\n\n        use_cache = decoder_past_key_values is not None\n        has_static_cache = isinstance(decoder_past_key_values, (StaticCache, HybridCache))\n\n        # When `output_attentions=True`, each iteration of generate appends the attentions corresponding to the new\n        # token(s)\n        # NOTE: `HybridCache` may have different lengths on different layers, if this test starts failing add more\n        # elaborate checks\n        for generated_length, iter_attentions in enumerate(attentions):\n            # regardless of using cache, the first forward pass will have the full prompt as input\n            if use_cache and generated_length > 0:\n                model_input_length = 1\n            else:\n                model_input_length = prompt_length + generated_length\n            query_length = (\n                prompt_length + generated_length\n                if not has_static_cache\n                else decoder_past_key_values.get_max_cache_shape()\n            )\n\n            expected_shape = (\n                batch_size,\n                config.num_attention_heads,\n                model_input_length,\n                query_length,\n            )\n            # check attn size\n            self.assertListEqual(\n                [layer_attention.shape for layer_attention in iter_attentions], [expected_shape] * len(iter_attentions)\n            )\n\n    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, prompt_length):\n        encoder_expected_shape = (batch_size, config.num_attention_heads, prompt_length, prompt_length)\n        self.assertIsInstance(attentions, tuple)\n        self.assertListEqual(\n            [layer_attentions.shape for layer_attentions in attentions],\n            [encoder_expected_shape] * len(attentions),\n        )\n\n    def _check_hidden_states_for_generate(\n        self, batch_size, hidden_states, prompt_length, output_length, config, use_cache=False\n    ):\n        self.assertIsInstance(hidden_states, tuple)\n        self.assertListEqual(\n            [isinstance(iter_hidden_states, tuple) for iter_hidden_states in hidden_states],\n            [True] * len(hidden_states),\n        )\n        self.assertEqual(len(hidden_states), (output_length - prompt_length))\n\n        # When `output_hidden_states=True`, each iteration of generate appends the hidden states corresponding to the\n        # new token(s)\n        # NOTE: `HybridCache` may have different lengths on different layers, if this test starts failing add more\n        # elaborate checks\n        for generated_length, iter_hidden_states in enumerate(hidden_states):\n            # regardless of using cache, the first forward pass will have the full prompt as input\n            if use_cache and generated_length > 0:\n                model_input_length = 1\n            else:\n                model_input_length = prompt_length + generated_length\n            expected_shape = (batch_size, model_input_length, config.hidden_size)\n            # check hidden size\n            self.assertListEqual(\n                [layer_hidden_states.shape for layer_hidden_states in iter_hidden_states],\n                [expected_shape] * len(iter_hidden_states),\n            )\n\n    def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, config, prompt_length):\n        encoder_expected_shape = (batch_size, prompt_length, config.hidden_size)\n        self.assertIsInstance(hidden_states, tuple)\n        self.assertListEqual(\n            [layer_hidden_states.shape for layer_hidden_states in hidden_states],\n            [encoder_expected_shape] * len(hidden_states),\n        )\n\n    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n        self.assertIsInstance(decoder_past_key_values, (tuple, Cache))\n\n        # (batch, head, seq_length, head_features)\n        expected_shape = (\n            batch_size,\n            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n            cache_length,\n            config.hidden_size // config.num_attention_heads,\n        )\n\n        if isinstance(decoder_past_key_values, Cache):\n            self.assertListEqual(\n                [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n                [expected_shape] * len(decoder_past_key_values.key_cache),\n            )\n            self.assertListEqual(\n                [value_tensor.shape for value_tensor in decoder_past_key_values.value_cache],\n                [expected_shape] * len(decoder_past_key_values.value_cache),\n            )\n\n        # Legacy cache format checks. This branch should be removed when all models use `Cache` by default\n        else:\n            self.assertListEqual(\n                [isinstance(iter_past_key_values, tuple) for iter_past_key_values in decoder_past_key_values],\n                [True] * len(decoder_past_key_values),\n            )\n            # check shape key, value\n            self.assertListEqual(\n                [layer_past_key_values[0].shape for layer_past_key_values in decoder_past_key_values],\n                [expected_shape] * len(decoder_past_key_values),\n            )\n            self.assertListEqual(\n                [layer_past_key_values[1].shape for layer_past_key_values in decoder_past_key_values],\n                [expected_shape] * len(decoder_past_key_values),\n            )\n\n    def _check_sequence_inside_sequence(self, tensor_1, tensor_2):\n        # check if tensor_1 inside tensor_2 or tensor_2 inside tensor_1.\n        # set to same device. we don't care what device.\n\n        if not isinstance(tensor_1, list):\n            tensor_1 = tensor_1.cpu().tolist()\n        if not isinstance(tensor_2, list):\n            tensor_2 = tensor_2.cpu().tolist()\n\n        in_order = len(tensor_1) <= len(tensor_2)\n        longer = tensor_2 if in_order else tensor_1\n        shorter = tensor_1 if in_order else tensor_2\n\n        flag = False\n        chunk_size = len(shorter)\n        for chunk_idx in range(len(longer) - chunk_size + 1):\n            subseq = longer[chunk_idx : chunk_idx + chunk_size]\n            if subseq == shorter:\n                flag = True\n                break\n\n        self.assertTrue(flag)\n\n\n@require_torch\nclass UtilsFunctionsTest(unittest.TestCase):\n    def test_speculative_sampling(self):\n        # assume vocab size 10, input length 5 + 3 generated candidates\n        candidate_input_ids = torch.tensor([[8, 0, 3, 9, 8, 1, 4, 5]])  # input tokens\n        candidate_logits = torch.tensor(\n            [\n                [\n                    [-10.0, 10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0],  # generated 1\n                    [-10.0, -10.0, -10.0, -10.0, 10.0, -10.0, -10.0, -10.0, -10.0, -10.0],  # generated 4\n                    [-10.0, -10.0, -10.0, -10.0, -10.0, 10.0, -10.0, -10.0, -10.0, -10.0],  # generated 5\n                ]\n            ]\n        )\n        candidate_length = 3\n        inf = float(\"inf\")\n        new_logits = torch.tensor(\n            [\n                [\n                    [-10.0, 10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0],  # accepts 1\n                    [-10.0, -10.0, -10.0, -10.0, 10.0, -10.0, -10.0, -10.0, -10.0, -10.0],  # accepts 4\n                    [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 10.0, -inf],  # rejects 5, accepts 8\n                    [-10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0],  # N/A\n                ]\n            ]\n        )\n        last_assistant_token_is_eos = False\n        validated_tokens, n_matches = _speculative_sampling(\n            candidate_input_ids,\n            candidate_logits,\n            candidate_length,\n            new_logits,\n            last_assistant_token_is_eos,\n        )\n        self.assertTrue(n_matches.item() == 2)\n        self.assertTrue(validated_tokens.tolist()[0] == [1, 4, 8])\n\n    def test_speculative_sampling_target_distribution(self):\n        \"\"\"\n        Asserts that the target distribution is preserved.\n        Should help with catching issues like #32867.\n        \"\"\"\n        # assume vocab size 10, input length 5 + 3 generated candidates\n        candidate_input_ids = torch.tensor([[8, 0, 3, 9, 8, 1, 4, 5]])  # input tokens\n        candidate_logits = torch.tensor(\n            [\n                [\n                    [-10.0, 10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0],  # generated 1\n                    [-10.0, -10.0, -10.0, -10.0, 10.0, -10.0, -10.0, -10.0, -10.0, -10.0],  # generated 4\n                    [-10.0, -10.0, -10.0, -10.0, -10.0, 10.0, -10.0, -10.0, -10.0, -10.0],  # generated 5\n                ]\n            ]\n        )\n        candidate_length = 3\n        inf = float(\"inf\")\n        new_logits = torch.tensor(\n            [\n                [\n                    # accepts 1:\n                    [-inf, 10.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n                    # accepts 4:\n                    [-inf, -inf, -inf, -inf, 10.0, -inf, -inf, -inf, -inf, -inf],\n                    # most likely to be 1 or 8, less likely to be 3, then 7, and should never be any other value:\n                    [-inf, 2.0, -inf, 1.0, -inf, -inf, -inf, -0.01, 2.0, -inf],\n                    # N/A:\n                    [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n                ]\n            ]\n        )\n        last_assistant_token_is_eos = False\n        last_validated_token = []\n        for _ in range(10_000):\n            validated_tokens, n_matches = _speculative_sampling(\n                candidate_input_ids,\n                candidate_logits,\n                candidate_length,\n                new_logits,\n                last_assistant_token_is_eos,\n            )\n            self.assertTrue(n_matches.item() == 2)\n            self.assertTrue(validated_tokens.tolist()[0][0] == 1)\n            self.assertTrue(validated_tokens.tolist()[0][1] == 4)\n            self.assertTrue(validated_tokens.tolist()[0][2] in [1, 3, 7, 8])\n            last_validated_token.append(validated_tokens.tolist()[0][2])\n        # check that the most likely tokens are selected more often than the less likely ones\n        last_token_counts = collections.Counter(last_validated_token)\n        self.assertTrue(last_token_counts[1] > last_token_counts[3] > last_token_counts[7] > 0)\n        self.assertTrue(last_token_counts[8] > last_token_counts[3])\n\n\nglobal_rng = random.Random()\n\n\n# Copied from tests.test_modeling_common.ids_tensor\ndef ids_tensor(shape, vocab_size, rng=None, name=None):\n    #  Creates a random int32 tensor of the shape within the vocab size\n    if rng is None:\n        rng = global_rng\n\n    total_dims = 1\n    for dim in shape:\n        total_dims *= dim\n\n    values = []\n    for _ in range(total_dims):\n        values.append(rng.randint(0, vocab_size - 1))\n\n    return torch.tensor(data=values, dtype=torch.long, device=torch_device).view(shape).contiguous()\n\n\n# Copied from tests.test_modeling_common.floats_tensor\ndef floats_tensor(shape, scale=1.0, rng=None, name=None):\n    \"\"\"Creates a random float32 tensor\"\"\"\n    if rng is None:\n        rng = global_rng\n\n    total_dims = 1\n    for dim in shape:\n        total_dims *= dim\n\n    values = []\n    for _ in range(total_dims):\n        values.append(rng.random() * scale)\n\n    return torch.tensor(data=values, dtype=torch.float, device=torch_device).view(shape).contiguous()\n\n\n@pytest.mark.generate\n@require_torch\nclass GenerationIntegrationTests(unittest.TestCase):\n    @slow\n    def test_diverse_beam_search(self):\n        article = \"\"\"Justin Timberlake and Jessica Biel, welcome to parenthood.\n        The celebrity couple announced the arrival of their son, Silas Randall Timberlake, in statements to People.\n        \"Silas was the middle name of Timberlake's maternal grandfather Bill Bomar, who died in 2012, while Randall is the musician's own middle name, as well as his father's first,\" People reports.\n        The couple announced the pregnancy in January, with an Instagram post. It is the first baby for both.\"\"\"\n\n        bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n        bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(torch_device)\n        input_ids = bart_tokenizer(article, return_tensors=\"pt\").input_ids.to(torch_device)\n\n        outputs = bart_model.generate(\n            input_ids,\n            num_beams=4,\n            num_return_sequences=2,\n            num_beam_groups=4,\n            diversity_penalty=2.0,\n            remove_invalid_values=True,\n        )\n\n        generated_text = bart_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        self.assertListEqual(\n            generated_text,\n            [\n                \"The couple announced the birth of their son, Silas Randall Timberlake, in a statement. Silas was the\"\n                \" middle name of Timberlake's maternal grandfather Bill Bomar. Randall is the musician's own middle\"\n                \" name, as well as his father's first. It is the first baby for both of them.\",\n                \"Justin Timberlake and Jessica Biel have a son. The baby is named Silas Randall Timberlake. It is the\"\n                \" first child for both. The couple announced the pregnancy in January. The name Silas is the middle\"\n                \" name of Timberlake's maternal grandfather. It's also his own middle name.\",\n            ],\n        )\n\n    def test_max_length_if_input_embeds(self):\n        article = \"Today a dragon flew over Paris.\"\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        input_ids = tokenizer(article, return_tensors=\"pt\").input_ids.to(torch_device)\n        inputs_embeds = model.get_input_embeddings()(input_ids)\n\n        max_length = 20\n        input_len = input_ids.shape[-1]\n        out_gen = model.generate(input_ids=input_ids, max_length=max_length)\n        out_gen_embeds = model.generate(inputs_embeds=inputs_embeds, max_length=max_length)\n        self.assertEqual(out_gen.shape[-1], input_len + out_gen_embeds.shape[-1])\n\n    def test_min_length_if_input_embeds(self):\n        article = \"Today a dragon flew over Paris.\"\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        input_ids = tokenizer(article, return_tensors=\"pt\").input_ids.to(torch_device)\n        inputs_embeds = model.get_input_embeddings()(input_ids)\n\n        min_length = 10\n        input_len = input_ids.shape[-1]\n        out_gen = model.generate(input_ids=input_ids, min_length=min_length)\n        out_gen_embeds = model.generate(inputs_embeds=inputs_embeds, min_length=min_length)\n        self.assertEqual(out_gen.shape[-1], input_len + out_gen_embeds.shape[-1])\n\n    def test_custom_stopping_criteria_overload_error(self):\n        article = \"\"\"Justin Timberlake and Jessica Biel, welcome to parenthood.\"\"\"\n        bart_tokenizer = BartTokenizer.from_pretrained(\"sshleifer/bart-tiny-random\")\n        bart_model = BartForConditionalGeneration.from_pretrained(\"sshleifer/bart-tiny-random\").to(torch_device)\n\n        input_ids = bart_tokenizer(article, return_tensors=\"pt\").input_ids.to(torch_device)\n        stopping_criteria = StoppingCriteriaList()\n        stopping_criteria.append(MaxLengthCriteria(max_length=42))\n        with self.assertRaises(ValueError):\n            bart_model.generate(input_ids, stopping_criteria=stopping_criteria)\n        with self.assertRaises(ValueError):\n            bart_model.generate(input_ids, stopping_criteria=stopping_criteria, max_length=32)\n\n    def test_custom_stopping_criteria(self):\n        article = \"\"\"Justin Timberlake and Jessica Biel, welcome to parenthood.\"\"\"\n        bart_tokenizer = BartTokenizer.from_pretrained(\"sshleifer/bart-tiny-random\")\n        bart_model = BartForConditionalGeneration.from_pretrained(\"sshleifer/bart-tiny-random\").to(torch_device)\n        input_ids = bart_tokenizer(article, return_tensors=\"pt\").input_ids.to(torch_device)\n\n        class DummyCriteria(StoppingCriteria):\n            def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n                return input_ids.shape[-1] >= 20\n\n        stopping_criteria = StoppingCriteriaList()\n        stopping_criteria.append(DummyCriteria())\n\n        self.assertEqual(\n            list(bart_model.generate(input_ids, stopping_criteria=stopping_criteria, max_length=22).shape),\n            [1, 20],\n        )\n        self.assertEqual(\n            list(bart_model.generate(input_ids, stopping_criteria=stopping_criteria, max_length=18).shape),\n            [1, 18],\n        )\n\n    # TODO (joao): replace `stop_sequence` in the pipeline by the more recent `generate` functionality\n    def test_stop_sequence_stopping_criteria(self):\n        prompt = \"\"\"Hello I believe in\"\"\"\n        generator = pipeline(\"text-generation\", model=\"hf-internal-testing/tiny-random-bart\")\n        output = generator(prompt)\n        self.assertEqual(\n            output,\n            [{\"generated_text\": (\"Hello I believe in we we we we we we we we we\")}],\n        )\n\n        output = generator(prompt, stop_sequence=\" we\")\n        self.assertEqual(output, [{\"generated_text\": \"Hello I believe in we\"}])\n\n    def test_generate_non_nlp_input_ids_as_kwarg(self):\n        model = ImageGPTForCausalImageModeling.from_pretrained(\n            \"hf-internal-testing/tiny-random-imagegpt\", max_length=10\n        ).to(torch_device)\n        input_ids = ids_tensor((3, 5), vocab_size=10)\n\n        output_sequences_kwargs = model.generate(input_ids=input_ids).cpu()\n        output_sequences = model.generate(input_ids).cpu()\n\n        self.assertListEqual(output_sequences.tolist(), output_sequences_kwargs.tolist())\n        self.assertEqual(output_sequences.shape, (3, 10))\n\n    def test_generate_input_values_as_encoder_kwarg(self):\n        input_values = floats_tensor((2, 250))\n        model = SpeechEncoderDecoderModel.from_pretrained(\"hf-internal-testing/tiny-random-speech-encoder-decoder\")\n        model = model.to(torch_device)\n        output_sequences_kwargs = model.generate(input_values=input_values, max_length=5).cpu()\n        output_sequences = model.generate(input_values, max_length=5).cpu()\n\n        self.assertListEqual(output_sequences.tolist(), output_sequences_kwargs.tolist())\n        self.assertEqual(output_sequences.shape, (2, 5))\n\n    def test_transition_scores_group_beam_search_encoder_decoder(self):\n        articles = [\n            \"Justin Timberlake and Jessica Biel, welcome to parenthood.\",\n            \"Michael Phelps is arguably the most decorated Olympian of all time.\",\n        ]\n        tokenizer = BartTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n        model = BartForConditionalGeneration.from_pretrained(\n            \"hf-internal-testing/tiny-random-bart\",\n            max_length=10,\n            num_beams=2,\n            num_beam_groups=2,\n            num_return_sequences=2,\n            diversity_penalty=1.0,\n            eos_token_id=None,\n            return_dict_in_generate=True,\n            output_scores=True,\n            length_penalty=0.0,\n        )\n        model = model.to(torch_device)\n\n        input_ids = tokenizer(articles, return_tensors=\"pt\", padding=True).input_ids.to(torch_device)\n        outputs = model.generate(input_ids=input_ids)\n\n        transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, outputs.beam_indices)\n        transition_scores_sum = transition_scores.sum(-1)\n\n        torch.testing.assert_close(transition_scores_sum, outputs.sequences_scores, rtol=1e-3, atol=1e-3)\n\n    @slow\n    def test_green_red_watermark_generation(self):\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n        model_inputs = tokenizer(\"I will be\", return_tensors=\"pt\").to(torch_device)\n        input_len = model_inputs[\"input_ids\"].shape[-1]\n\n        # generation should work with both input types: WatermarkingConfig or Dict, so let's check it here :)\n        watermark_config = WatermarkingConfig(bias=2.5, seeding_scheme=\"selfhash\")\n        _ = model.generate(**model_inputs, watermarking_config=watermark_config, do_sample=False, max_length=15)\n\n        # We will not check watermarked text, since we check it in `logits_processors` tests\n        # Checking if generated ids are as expected fails on different hardware\n        args = {\n            \"bias\": 2.0,\n            \"context_width\": 1,\n            \"seeding_scheme\": \"selfhash\",\n            \"greenlist_ratio\": 0.25,\n            \"hashing_key\": 15485863,\n        }\n        output = model.generate(**model_inputs, do_sample=False, max_length=15)\n        output_selfhash = model.generate(**model_inputs, watermarking_config=args, do_sample=False, max_length=15)\n\n        # Check that the detector is detecting watermarked text\n        detector = WatermarkDetector(model_config=model.config, device=torch_device, watermarking_config=args)\n        detection_out_watermarked = detector(output_selfhash[:, input_len:], return_dict=True)\n        detection_out = detector(output[:, input_len:], return_dict=True)\n\n        self.assertListEqual(detection_out_watermarked.prediction.tolist(), [True])\n        self.assertListEqual(detection_out.prediction.tolist(), [False])\n\n    \"\"\"Check the mean bias inserted by the watermarking algorithm.\"\"\"\n\n    @slow\n    def test_synthid_text_watermark_generation_mean_expected_bias(self):\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n        model_inputs = tokenizer(\"I will be\", return_tensors=\"pt\").to(torch_device)\n        input_len = 5\n        batch_size = 200\n\n        # generation should work with both input types: WatermarkingConfig or Dict, so let's check it here :)\n        watermark_config = SynthIDTextWatermarkingConfig(keys=[10, 20], ngram_len=5, debug_mode=True)\n        logits_processor = watermark_config.construct_processor(model.config.vocab_size, torch_device)\n        mean_g_values_repeats = []\n        for _ in range(40):\n            input_ids = torch.zeros(\n                (batch_size, input_len),\n                dtype=torch.int64,\n                device=torch_device,\n            )\n            model_inputs = {\n                \"input_ids\": input_ids,\n                \"attention_mask\": torch.ones_like(input_ids, device=torch_device),\n            }\n            output = model.generate(\n                **model_inputs, watermarking_config=watermark_config, do_sample=True, max_length=500, top_k=1000\n            )\n            g_values = logits_processor.compute_g_values(input_ids=output[:, input_len:])\n            context_repetition_mask = logits_processor.compute_context_repetition_mask(\n                input_ids=output[:, input_len:],\n            ).unsqueeze(dim=2)\n\n            mean_g_values = torch.masked.mean(\n                g_values,\n                mask=context_repetition_mask,\n                dim=0,\n                keepdim=True,\n                dtype=torch.float64,\n            )\n            mean_g_values_repeats.append(mean_g_values)\n\n        mean_g_values = torch.concat(mean_g_values_repeats, dim=0).mean(dim=0)\n        expected_mean_g_value = logits_processor.expected_mean_g_value(\n            vocab_size=model.config.vocab_size,\n        )\n        atol = 0.03\n        is_close = torch.isclose(\n            mean_g_values,\n            torch.tensor(expected_mean_g_value, dtype=torch.float64),\n            atol=atol,\n            rtol=0,\n        )\n        self.assertTrue(torch.all(is_close))\n\n    @slow\n    def test_beam_search_example_integration(self):\n        # exactly the example provided in the docstrings of beam search, which previously\n        # failed after directly copying from it. Refer to PR #15555\n        tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n        model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n\n        encoder_input_str = \"translate English to German: How old are you?\"\n        encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n\n        # lets run beam search using 3 beams\n        num_beams = 3\n        # define decoder start token ids\n        input_ids = torch.ones((1, 1), device=model.device, dtype=torch.long)\n        input_ids = input_ids * model.config.decoder_start_token_id\n\n        # add encoder_outputs to model keyword arguments\n        model_kwargs = {\"encoder_outputs\": model.get_encoder()(encoder_input_ids, return_dict=True)}\n\n        outputs = model.generate(\n            input_ids, num_beams=num_beams, min_length=5, eos_token_id=model.config.eos_token_id, **model_kwargs\n        )\n        outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        self.assertListEqual(outputs, [\"Wie alt bist du?\"])\n\n    @slow\n    def test_constrained_beam_search(self):\n        model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\").to(torch_device)\n        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n\n        force_tokens = tokenizer(\"scared\", add_prefix_space=True, add_special_tokens=False).input_ids\n        force_tokens_2 = tokenizer(\"big weapons\", add_prefix_space=True, add_special_tokens=False).input_ids\n\n        constraints = [\n            PhrasalConstraint(force_tokens),\n            PhrasalConstraint(force_tokens_2),\n        ]\n\n        starting_text = [\"The soldiers were not prepared and\"]\n\n        input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids.to(torch_device)\n\n        outputs = model.generate(\n            input_ids,\n            constraints=constraints,\n            num_beams=10,\n            num_return_sequences=1,\n            no_repeat_ngram_size=1,\n            max_length=30,\n            remove_invalid_values=True,\n        )\n\n        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        self.assertListEqual(\n            generated_text,\n            [\n                \"The soldiers were not prepared and didn't know what to do. They had no idea how they would react if\"\n                \" the enemy attacked them, big weapons scared\"\n            ],\n        )\n\n    @slow\n    def test_constrained_beam_search_mixed(self):\n        model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\").to(torch_device)\n        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n\n        force_phrase = tokenizer(\"scared\", add_prefix_space=True, add_special_tokens=False).input_ids\n        flexible_phrases = tokenizer(\n            [\"scream\", \"screams\", \"screaming\", \"screamed\"], add_prefix_space=True, add_special_tokens=False\n        ).input_ids\n\n        constraints = [\n            PhrasalConstraint(force_phrase),\n            DisjunctiveConstraint(flexible_phrases),\n        ]\n\n        starting_text = [\"The soldiers\", \"The child\"]\n\n        input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids.to(torch_device)\n\n        outputs = model.generate(\n            input_ids,\n            constraints=constraints,\n            num_beams=10,\n            num_return_sequences=1,\n            no_repeat_ngram_size=1,\n            # max_length=20,\n            remove_invalid_values=True,\n        )\n\n        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        self.assertListEqual(\n            generated_text,\n            [\n                \"The soldiers, who had been stationed at the base for more than a year before being evacuated\"\n                \" screaming scared\",\n                \"The child was taken to a local hospital where he died.\\n 'I don't think screaming scared\",\n            ],\n        )\n\n    @slow\n    def test_constrained_beam_search_mixed_mixin(self):\n        model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\").to(torch_device)\n        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n\n        force_word = \"scared\"\n        force_flexible = [\"scream\", \"screams\", \"screaming\", \"screamed\"]\n\n        force_words_ids = [\n            tokenizer([force_word], add_prefix_space=True, add_special_tokens=False).input_ids,\n            tokenizer(force_flexible, add_prefix_space=True, add_special_tokens=False).input_ids,\n        ]\n\n        starting_text = [\"The soldiers\", \"The child\"]\n\n        input_ids = tokenizer(starting_text, return_tensors=\"pt\").input_ids.to(torch_device)\n\n        outputs = model.generate(\n            input_ids,\n            force_words_ids=force_words_ids,\n            num_beams=10,\n            num_return_sequences=1,\n            no_repeat_ngram_size=1,\n            remove_invalid_values=True,\n        )\n\n        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        self.assertListEqual(\n            generated_text,\n            [\n                \"The soldiers, who had been stationed at the base for more than a year before being evacuated\"\n                \" screaming scared\",\n                \"The child was taken to a local hospital where he died.\\n 'I don't think screaming scared\",\n            ],\n        )\n\n    @slow\n    def test_cfg_mixin(self):\n        model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\").to(torch_device)\n        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n\n        input = tokenizer([\"The dragon flew over Paris,\"], return_tensors=\"pt\", return_attention_mask=True)\n        input[\"input_ids\"] = input[\"input_ids\"].to(torch_device)\n        input[\"attention_mask\"] = input[\"attention_mask\"].to(torch_device)\n\n        outputs = model.generate(**input, max_new_tokens=32, guidance_scale=1.5)\n        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        self.assertListEqual(\n            generated_text,\n            [\n                \"The dragon flew over Paris, landing in the Rue de la Bastille. The crowd was so excited \"\n                'that they had to leave the city.\\n\\n\"We\\'re going to Paris!\"\\n'\n            ],\n        )\n\n        neg = tokenizer([\"France,\"], return_tensors=\"pt\", return_attention_mask=True)\n        neg[\"input_ids\"] = neg[\"input_ids\"].to(torch_device)\n        neg[\"attention_mask\"] = neg[\"attention_mask\"].to(torch_device)\n        outputs = model.generate(\n            **input,\n            max_new_tokens=32,\n            guidance_scale=1.5,\n            negative_prompt_ids=neg[\"input_ids\"],\n            negative_prompt_attention_mask=neg[\"attention_mask\"],\n        )\n        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        self.assertListEqual(\n            generated_text,\n            [\n                'The dragon flew over Paris, landing on the pavement.\\n\\n\"Paris!\"\\n\\n\"Paris!\"\\n\\n\"'\n                'Paris!\"\\n\\n\"Paris!\"\\n\\n\"Paris!\"\\n\\n'\n            ],\n        )\n\n    @slow\n    def test_constrained_beam_search_example_translation_mixin(self):\n        tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n        model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n\n        encoder_input_str = \"translate English to German: How old are you?\"\n        force_words = [\"sind\"]\n\n        input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n        force_words_ids = tokenizer(force_words, add_special_tokens=False).input_ids\n\n        outputs = model.generate(\n            input_ids,\n            force_words_ids=force_words_ids,\n            num_beams=10,\n            num_return_sequences=1,\n            no_repeat_ngram_size=1,\n            remove_invalid_values=True,\n        )\n\n        outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        self.assertListEqual(outputs, [\"Wie alt sind Sie?\"])\n\n    @slow\n    def test_constrained_beam_search_example_integration(self):\n        tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n        model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n\n        encoder_input_str = \"translate English to German: How old are you?\"\n        encoder_input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n\n        # lets run beam search using 5 beams\n        num_beams = 5\n        # define decoder start token ids\n        input_ids = torch.ones((1, 1), device=model.device, dtype=torch.long)\n        input_ids = input_ids * model.config.decoder_start_token_id\n\n        # add encoder_outputs to model keyword arguments\n        model_kwargs = {\"encoder_outputs\": model.get_encoder()(encoder_input_ids, return_dict=True)}\n\n        constraint_str = \"sind\"\n        constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # remove eos token\n\n        outputs = model.generate(\n            input_ids,\n            num_beams=num_beams,\n            force_words_ids=[constraint_token_ids],\n            min_length=5,\n            eos_token_id=model.config.eos_token_id,\n            **model_kwargs,\n        )\n        outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        self.assertListEqual(outputs, [\"Wie alt sind Sie?\"])\n\n    @slow\n    def test_per_row_stopping_criteria(self):\n        text = [\n            \"They completed the challenging puzzle, revealing the hidden\",\n            \"Today a dragon flew over France\",\n            \"The aroma of freshly baked pizza filled the kitchen\",\n        ]\n        stop_strings = [\"secrets\"]\n\n        model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").to(torch_device)\n        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n        tokenizer.padding_side = \"left\"\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n        input_ids = tokenizer(text, return_tensors=\"pt\", padding=\"longest\", add_special_tokens=False).input_ids.to(\n            torch_device\n        )\n\n        # normal generation with one stopping criteria\n        out = model.generate(input_ids, max_length=15)\n        out_text = tokenizer.batch_decode(out)\n        expected_out = [\n            \"They completed the challenging puzzle, revealing the hidden secrets of the world.\\n\",\n            \"<|endoftext|><|endoftext|><|endoftext|>Today a dragon flew over France and the French government was forced\",\n            \"The aroma of freshly baked pizza filled the kitchen with a sense of freshness\",\n        ]\n        self.assertListEqual(out_text, expected_out)\n\n        # generation should stop at \"secrets\" for first batch only, filling the rest with eos tokens\n        out = model.generate(input_ids, max_length=15, stop_strings=stop_strings, tokenizer=tokenizer)\n        out_text = tokenizer.batch_decode(out)\n        expected_out = [\n            \"They completed the challenging puzzle, revealing the hidden secrets<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\",\n            \"<|endoftext|><|endoftext|><|endoftext|>Today a dragon flew over France and the French government was forced\",\n            \"The aroma of freshly baked pizza filled the kitchen with a sense of freshness\",\n        ]\n        self.assertListEqual(out_text, expected_out)\n\n    def test_constrained_beam_search_mixin_type_checks(self):\n        tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/t5-tiny-random\")\n        model = AutoModelForSeq2SeqLM.from_pretrained(\"patrickvonplaten/t5-tiny-random\")\n\n        encoder_input_str = \"translate English to German: How old are you?\"\n        input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n\n        with self.assertRaises(ValueError):\n            force_words = [\"sind\"]\n            force_words_ids = tokenizer(force_words, return_tensors=\"pt\").input_ids\n            model.generate(\n                input_ids,\n                force_words_ids=force_words_ids,\n                num_beams=10,\n                num_return_sequences=1,\n                no_repeat_ngram_size=1,\n                remove_invalid_values=True,\n            )\n\n        with self.assertRaises(ValueError):\n            force_words = [\"sind\"]\n            force_words_ids = [tokenizer(force_words, return_tensors=\"pt\").input_ids]\n            model.generate(\n                input_ids,\n                force_words_ids=force_words_ids,\n                num_beams=10,\n                num_return_sequences=1,\n                no_repeat_ngram_size=1,\n                remove_invalid_values=True,\n            )\n\n        with self.assertRaises(ValueError):\n            model.generate(input_ids, force_words_ids=[])\n\n        with self.assertRaises(ValueError):\n            model.generate(input_ids, force_words_ids=[[-1]])\n\n        with self.assertRaises(ValueError):\n            model.generate(input_ids, force_words_ids=[[[-1]]])\n\n    def test_batched_decoder_start_id(self):\n        articles = [\n            \"Justin Timberlake and Jessica Biel, welcome to parenthood.\",\n            \"Michael Phelps is arguably the most decorated Olympian of all time.\",\n        ]\n        bart_tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n        bart_model = BartForConditionalGeneration.from_pretrained(\"hf-internal-testing/tiny-random-bart\").to(\n            torch_device\n        )\n        input_ids = bart_tokenizer(articles, return_tensors=\"pt\", padding=True).input_ids.to(torch_device)\n        decoder_start_token_id = bart_model.generation_config.decoder_start_token_id\n        decoder_start_token_id_batch = [decoder_start_token_id] * input_ids.shape[0]\n\n        outputs = bart_model.generate(input_ids, decoder_start_token_id=decoder_start_token_id)\n\n        outputs_batched_ids = bart_model.generate(input_ids, decoder_start_token_id=decoder_start_token_id_batch)\n\n        self.assertListEqual(outputs.tolist(), outputs_batched_ids.tolist())\n\n    def test_decoder_start_id_from_config(self):\n        # Refer to: (#30899)\n        articles = [\n            \"Justin Timberlake and Jessica Biel, welcome to parenthood.\",\n            \"Michael Phelps is arguably the most decorated Olympian of all time.\",\n        ]\n        bart_tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n        bart_model = BartForConditionalGeneration.from_pretrained(\"hf-internal-testing/tiny-random-bart\").to(\n            torch_device\n        )\n        input_ids = bart_tokenizer(articles, return_tensors=\"pt\", padding=True).input_ids.to(torch_device)\n        decoder_start_token_id = bart_model.generation_config.decoder_start_token_id\n\n        # we should be able to take `decoder_start_token_id` from model's generation config if user passes a `GenerationConfig` type\n        outputs = bart_model.generate(input_ids, generation_config=GenerationConfig(do_sample=False))\n\n        # If the generatoin config has no `decoder_start_token_id` or `bos_token_id`, we will raise an error unless user passes it in config\n        bart_model.generation_config.decoder_start_token_id = None\n        bart_model.generation_config.bos_token_id = None\n        outputs_with_user_id = bart_model.generate(\n            input_ids,\n            generation_config=GenerationConfig(do_sample=False, decoder_start_token_id=decoder_start_token_id),\n        )\n\n        self.assertListEqual(outputs.tolist(), outputs_with_user_id.tolist())\n\n        with self.assertRaises(ValueError):\n            outputs = bart_model.generate(input_ids, generation_config=GenerationConfig(do_sample=False))\n\n    def test_contrastive_search_batched(self):\n        # Tests that contrastive search works with batched inputs (i.e. has the same output as for non-batched inputs)\n        articles = [\"Foo\", \"Bar Baz\"]\n        tokenizer = BartTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n        model = BartForConditionalGeneration.from_pretrained(\"hf-internal-testing/tiny-random-bart\").to(torch_device)\n\n        model.config.eos_token_id = None\n        input_ids_batched = tokenizer(articles, padding=True, return_tensors=\"pt\").input_ids.to(torch_device)\n        input_ids = tokenizer(articles[1], return_tensors=\"pt\").input_ids.to(torch_device)\n\n        output_sequences_batched = model.generate(\n            input_ids=input_ids_batched, penalty_alpha=0.6, top_k=4, return_dict_in_generate=True, output_scores=True\n        )\n        output_sequences = model.generate(\n            input_ids=input_ids, penalty_alpha=0.6, top_k=4, return_dict_in_generate=True, output_scores=True\n        )\n\n        batched_out = tokenizer.decode(output_sequences_batched.sequences[1], skip_special_tokens=True)\n        out = tokenizer.decode(output_sequences.sequences[0], skip_special_tokens=True)\n        self.assertEqual(batched_out, out)\n\n        # output_sequences_batched.scores[0][1] -> 1st set of logits, 2nd sequence\n        max_score_diff = (output_sequences_batched.scores[0][1] - output_sequences.scores[0][0]).abs().max()\n        self.assertTrue(max_score_diff < 1e-5)\n\n    def test_logits_processor_not_inplace(self):\n        article = \"Today a dragon flew over Paris.\"\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        input_ids = tokenizer(article, return_tensors=\"pt\").input_ids.to(torch_device)\n\n        out = model.generate(input_ids, output_logits=True, output_scores=True, return_dict_in_generate=True)\n        out_with_temp = model.generate(\n            input_ids,\n            temperature=0.5,\n            do_sample=True,\n            output_logits=True,\n            output_scores=True,\n            return_dict_in_generate=True,\n        )\n\n        # if no logits processor is used, scores == logits. Otherwise, the processor has to modify the scores\n        self.assertListEqual(out.logits[-1].tolist(), out.scores[-1].tolist())\n        self.assertNotEqual(out_with_temp.logits[-1].tolist(), out_with_temp.scores[-1].tolist())\n\n    def test_eos_token_id_int_and_list_top_k_top_sampling(self):\n        # Has TF equivalent: this test relies on random sampling\n        generation_kwargs = {\n            \"do_sample\": True,\n            \"num_beams\": 1,\n            \"top_p\": 0.7,\n            \"top_k\": 10,\n            \"temperature\": 0.7,\n        }\n        expectation = 20\n\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        text = \"\"\"Hello, my dog is cute and\"\"\"\n        tokens = tokenizer(text, return_tensors=\"pt\").to(torch_device)\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n\n        # Only some seeds will work both on CPU/GPU for a fixed `expectation` value.\n        # The selected seed is not guaranteed to work on all torch versions.\n        torch.manual_seed(1)\n        eos_token_id = 846\n        generated_tokens = model.generate(**tokens, eos_token_id=eos_token_id, **generation_kwargs)\n        self.assertTrue(expectation == len(generated_tokens[0]))\n\n        torch.manual_seed(1)\n        eos_token_id = [846, 198]\n        generated_tokens = model.generate(**tokens, eos_token_id=eos_token_id, **generation_kwargs)\n        self.assertTrue(expectation == len(generated_tokens[0]))\n\n    def test_model_kwarg_encoder_signature_filtering(self):\n        # Has TF equivalent: ample use of framework-specific code\n        bart_tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n        article = \"\"\"Hugging Face is a technology company based in New York and Paris.\"\"\"\n        input_ids = bart_tokenizer(article, return_tensors=\"pt\").input_ids.to(torch_device)\n        bart_model = BartForConditionalGeneration.from_pretrained(\"hf-internal-testing/tiny-random-bart\").to(\n            torch_device\n        )\n        output = bart_model.generate(input_ids).cpu().numpy()\n\n        # Let's create a fake model that has a different signature. In particular, this fake model accepts \"foo\" as an\n        # argument. Because \"foo\" is not in the encoder signature and doesn't start with \"decoder_\", it will be part of\n        # the encoder kwargs prior to signature filtering, which would lead to an exception. But filtering kicks in and\n        # saves the day.\n        class FakeBart(BartForConditionalGeneration):\n            def forward(self, input_ids, foo=None, **kwargs):\n                return super().forward(input_ids, **kwargs)\n\n        bart_model = FakeBart.from_pretrained(\"hf-internal-testing/tiny-random-bart\").to(torch_device)\n        fake_output = bart_model.generate(input_ids, foo=\"bar\").cpu().numpy()\n        self.assertTrue(np.array_equal(output, fake_output))\n\n        # Encoder signature filtering only kicks in if it doesn't accept wildcard kwargs. The following test will fail\n        # because it doesn't do signature filtering.\n        class FakeEncoder(bart_model.model.encoder.__class__):\n            def forward(self, input_ids, **kwargs):\n                return super().forward(input_ids, **kwargs)\n\n        fake_encoder = FakeEncoder(bart_model.config, bart_model.model.shared).to(torch_device)\n        bart_model.model.encoder = fake_encoder\n\n        # Normal generation still works (the output will be different because the encoder weights are different)\n        fake_output = bart_model.generate(input_ids).cpu().numpy()\n        with self.assertRaises(TypeError):\n            # FakeEncoder.forward() accepts **kwargs -> no filtering -> type error due to unexpected input \"foo\"\n            bart_model.generate(input_ids, foo=\"bar\")\n\n    def test_default_max_length_warning(self):\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        model.generation_config.pad_token_id = tokenizer.eos_token_id\n\n        text = \"Hello world\"\n        tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n        input_ids = tokenized_inputs.input_ids.to(torch_device)\n\n        # Default generation config value of 20 -> emits warning\n        with self.assertWarns(UserWarning):\n            model.generate(input_ids)\n\n        # Explicitly setting max_length to 20 -> no warning\n        with warnings.catch_warnings(record=True) as warning_list:\n            model.generate(input_ids, max_length=20)\n            self.assertEqual(len(warning_list), 0)\n\n        # Generation config max_length != 20 -> no warning\n        with warnings.catch_warnings(record=True) as warning_list:\n            # generation_config is modified -> legacy mode is disabled = generation_config takes precedence\n            model.generation_config.max_length = 10\n            model.generate(input_ids)\n            self.assertEqual(len(warning_list), 0)\n\n    def test_length_warning_assisted_generation(self):\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        assistant = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        model.generation_config.pad_token_id = tokenizer.eos_token_id\n        assistant.generation_config.pad_token_id = tokenizer.eos_token_id\n\n        text = \"Hello world\"\n        tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n        input_ids = tokenized_inputs.input_ids.to(torch_device)\n\n        # This should not raise any warning that min length is not feasible in candidate generation\n        with warnings.catch_warnings(record=True) as warning_list:\n            model.generate(\n                input_ids,\n                assistant_model=assistant,\n                min_new_tokens=10,\n                max_length=20,\n            )\n            self.assertEqual(len(warning_list), 0)\n\n    def test_default_assisted_generation(self):\n        # Initialize the GenerationConfig object\n        config = GenerationConfig()\n\n        # Check the default values\n        self.assertEqual(config.num_assistant_tokens, 20)\n        self.assertEqual(config.num_assistant_tokens_schedule, \"constant\")\n        self.assertEqual(config.assistant_confidence_threshold, 0.4)\n        self.assertEqual(config.is_assistant, False)\n\n    def test_generated_length_assisted_generation(self):\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        assistant = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        model.generation_config.pad_token_id = tokenizer.eos_token_id\n        assistant.generation_config.pad_token_id = tokenizer.eos_token_id\n\n        text = \"Hello world\"\n        tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n        input_ids = tokenized_inputs.input_ids.to(torch_device)\n        input_length = input_ids.shape[-1]\n\n        out = model.generate(\n            input_ids,\n            assistant_model=assistant,\n            min_new_tokens=10,\n            max_new_tokens=20,\n        )\n        self.assertTrue((10 + input_length) <= out.shape[-1] <= (20 + input_length))\n\n        out = model.generate(\n            input_ids,\n            assistant_model=assistant,\n            min_new_tokens=10,\n        )\n        self.assertTrue((input_length + 10) <= out.shape[-1])\n\n        out = model.generate(\n            input_ids,\n            assistant_model=assistant,\n            max_new_tokens=7,\n        )\n        self.assertTrue(out.shape[-1] <= (input_length + 7))\n\n    def test_model_kwarg_assisted_decoding_decoder_only(self):\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        model.generation_config.pad_token_id = tokenizer.eos_token_id\n\n        text = \"Hello world\"\n        tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n        input_ids = tokenized_inputs.input_ids.to(torch_device)\n\n        # Traditional way of generating text\n        outputs_normal = model.generate(input_ids)\n        self.assertEqual(outputs_normal.shape, (1, 20))\n\n        # Should be different with token_type_ids\n        outputs_tti = model.generate(\n            input_ids,\n            token_type_ids=torch.zeros(input_ids.shape, dtype=torch.long).to(torch_device),\n        )\n        with self.assertRaises(AssertionError):\n            self.assertListEqual(outputs_tti.tolist(), outputs_normal.tolist())\n\n        # Assistant model\n        assistant = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        assistant.config.pad_token_id = tokenizer.eos_token_id\n\n        # If assisted generation passes model_kwargs correctly, should be same as previous\n        outputs_assisted = model.generate(\n            input_ids,\n            token_type_ids=torch.zeros(input_ids.shape, dtype=torch.long).to(torch_device),\n            assistant_model=assistant,\n        )\n        self.assertListEqual(outputs_assisted.tolist(), outputs_tti.tolist())\n\n    def test_assisted_decoding_num_assistant_tokens_heuristic_schedule(self):\n        # This test ensures that the assisted generation num_assistant_tokens 'heuristic' schedule works properly.\n\n        prompt = \"Alice and Bob\"\n        checkpoint = \"EleutherAI/pythia-160m-deduped\"\n        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        model = AutoModelForCausalLM.from_pretrained(checkpoint)\n\n        assistant_model = model\n        assistant_model.generation_config.num_assistant_tokens = 5\n        assistant_model.generation_config.num_assistant_tokens_schedule = \"heuristic\"\n        generation_kwargs = {\n            \"eos_token_id\": -1,\n            \"max_new_tokens\": 5,\n            \"do_sample\": False,\n            \"assistant_model\": assistant_model,\n        }\n        model.generate(**inputs, **generation_kwargs)\n        # update_candidate_strategy is called only once and therefore, assistant_model.generation_config.num_assistant_tokens should be either 4 or 7\n        self.assertTrue(assistant_model.generation_config.num_assistant_tokens in (4, 7))\n\n    def test_assisted_decoding_num_assistant_tokens_heuristic_transient_schedule(self):\n        # This test ensures that the assisted generation num_assistant_tokens 'heuristic' schedule works properly.\n\n        prompt = \"Alice and Bob\"\n        checkpoint = \"EleutherAI/pythia-160m-deduped\"\n        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        model = AutoModelForCausalLM.from_pretrained(checkpoint)\n\n        assistant_model = model\n        assistant_model.generation_config.num_assistant_tokens = 5\n        assistant_model.generation_config.num_assistant_tokens_schedule = \"heuristic_transient\"\n        generation_kwargs = {\n            \"eos_token_id\": -1,\n            \"max_new_tokens\": 5,\n            \"do_sample\": False,\n            \"assistant_model\": assistant_model,\n        }\n        model.generate(**inputs, **generation_kwargs)\n        # update_candidate_strategy is called once but assistant_model.generation_config.num_assistant_tokens should stay 5\n        self.assertEqual(assistant_model.generation_config.num_assistant_tokens, 5)\n\n    @slow\n    def test_validate_assistant(self):\n        # Generate a random sample:\n        inputs = np.random.rand(160000)\n\n        # Load a main encoder-decoder model:\n        model_id = \"openai/whisper-large-v2\"\n        processor = AutoProcessor.from_pretrained(model_id)\n        model = AutoModelForSpeechSeq2Seq.from_pretrained(\n            model_id,\n            low_cpu_mem_usage=True,\n            use_safetensors=True,\n        )\n        model.to(torch_device)\n\n        # process the input:\n        features = processor(inputs, return_tensors=\"pt\").to(torch_device)\n\n        # Load an encoder-decoder assistant with same encoder as the main model:\n        assistant_distil_model_id = \"distil-whisper/distil-large-v2\"\n        assistant_seq_to_seq = AutoModelForSpeechSeq2Seq.from_pretrained(\n            assistant_distil_model_id,\n            use_safetensors=True,\n        ).to(torch_device)\n        self.assertTrue(model.generate(**features, assistant_model=assistant_seq_to_seq).sum())\n\n        # Load its decoder only version:\n        assistant_causal_lm = AutoModelForCausalLM.from_pretrained(\n            assistant_distil_model_id,\n            low_cpu_mem_usage=True,\n            use_safetensors=True,\n        ).to(torch_device)\n        self.assertTrue(model.generate(**features, assistant_model=assistant_causal_lm).sum())\n\n        # Load an encoder-decoder assistant with a different encoder than the main model:\n        assistant_distil_model_id = \"openai/whisper-tiny\"\n        assistant_seq_to_seq = AutoModelForSpeechSeq2Seq.from_pretrained(\n            assistant_distil_model_id,\n            use_safetensors=True,\n        ).to(torch_device)\n        self.assertTrue(model.generate(**features, assistant_model=assistant_seq_to_seq).sum())\n\n        # Load its decoder only version:\n        assistant_causal_lm = AutoModelForCausalLM.from_pretrained(\n            assistant_distil_model_id,\n            low_cpu_mem_usage=True,\n            use_safetensors=True,\n        ).to(torch_device)\n        # It will raise an error as the encoder of the main and assistant model are not compatible:\n        with self.assertRaises(ValueError):\n            model.generate(**features, assistant_model=assistant_causal_lm)\n\n        # Load an encoder-decoder model with a different tokenizer than the main model:\n        assistant_distil_model_id = \"hf-internal-testing/tiny-random-SeamlessM4Tv2ForSpeechToText\"\n        assistant_seq_to_seq = AutoModelForSpeechSeq2Seq.from_pretrained(\n            assistant_distil_model_id,\n        ).to(torch_device)\n        # This should raise an error as the main and assistant model don't use the same tokenizer:\n        with self.assertRaises(ValueError):\n            model.generate(**features, assistant_model=assistant_seq_to_seq)\n\n    def test_compare_unprocessed_logit_scores(self):\n        # Get unprocessed logit scores back from model generate function.\n        # Assert that unprocessed logits from generate() are same as those from modal eval()\n\n        # tell model to generate text and return unprocessed/unwarped logit scores\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        text = \"generate yes or no: \"\n        input_ids = tokenizer([text], return_tensors=\"pt\").input_ids.to(torch_device)\n\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n\n        with torch.no_grad():\n            # Get logits for the next token from fwd pass\n            logits_fwd = model(input_ids).logits[:, -1, :][0]\n\n        # Get logits for the next token from generate function\n        outputs = model.generate(\n            input_ids=input_ids,\n            return_dict_in_generate=True,\n            output_logits=True,\n            max_new_tokens=1,\n            do_sample=True,\n        )\n        logits_gen = outputs.logits[0][0]\n\n        # assert that unprocessed logits from generate() are same as those from modal eval()\n        self.assertListEqual(logits_fwd.tolist(), logits_gen.tolist())\n\n    def test_return_unprocessed_logit_scores(self):\n        # tell model to generate text and return unprocessed/unwarped logit scores\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        text = \"generate yes or no: \"\n        input_ids = tokenizer([text], return_tensors=\"pt\").input_ids.to(torch_device)\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n\n        outputs = model.generate(\n            input_ids=input_ids, return_dict_in_generate=True, output_logits=True, max_new_tokens=3\n        )\n\n        # perform dummy check if unpreprocessed logits make sense.\n        # do preselection on high probabilities; find scores of y and n tokens\n        probs_all = torch.nn.functional.softmax(outputs.logits[2][0], dim=-1)\n        indices = torch.argwhere(probs_all > 0.001)\n        indices = indices[:, -1]\n        tokens_max = tokenizer.batch_decode(indices, skip_special_tokens=True)\n        probs_max = probs_all[probs_all > 0.001]\n\n        self.assertTrue(len(indices) >= 2)\n        next_token_dict = {str(t): p for t, p in zip(tokens_max, probs_max)}\n        self.assertTrue(\"n\" in next_token_dict)\n        self.assertTrue(\"y\" in next_token_dict)\n        y_prob = next_token_dict[\"y\"]\n        n_prob = next_token_dict[\"n\"]\n\n        self.assertTrue(y_prob > 0.001 and n_prob > 0.001)\n        self.assertTrue(y_prob <= 1.0 and n_prob <= 1.0)\n\n    @slow\n    @require_torch_multi_gpu\n    def test_assisted_decoding_in_different_gpu(self):\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\").to(\"cuda:0\")\n        assistant = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\").to(\n            \"cuda:1\"\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n        model.config.pad_token_id = tokenizer.eos_token_id\n        assistant.config.pad_token_id = tokenizer.eos_token_id\n\n        text = \"Hello world\"\n        tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n        input_ids = tokenized_inputs.input_ids.to(torch_device)\n        input_length = input_ids.shape[-1]\n\n        out = model.generate(\n            input_ids,\n            assistant_model=assistant,\n            max_new_tokens=20,\n        )\n        self.assertTrue(input_length <= out.shape[-1] <= input_length + 20)\n\n    @slow\n    @require_torch_accelerator\n    def test_assisted_decoding_model_in_gpu_assistant_in_cpu(self):\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\").to(\n            torch_device\n        )\n        assistant = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\").to(\n            \"cpu\"\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n        model.config.pad_token_id = tokenizer.eos_token_id\n        assistant.config.pad_token_id = tokenizer.eos_token_id\n\n        text = \"Hello world\"\n        tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n        input_ids = tokenized_inputs.input_ids.to(torch_device)\n        input_length = input_ids.shape[-1]\n\n        out = model.generate(\n            input_ids,\n            assistant_model=assistant,\n            max_new_tokens=20,\n        )\n        self.assertTrue(input_length <= out.shape[-1] <= input_length + 20)\n\n    def test_special_tokens_fall_back_to_model_default(self):\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\").to(\n            torch_device\n        )\n        test_bos_id = 50\n\n        # Sanity-check: the model has a BOS token set, and the first generated token is a BOS token\n        gen_output = model.generate()\n        self.assertTrue(model.generation_config.bos_token_id is not None)\n        self.assertTrue(model.generation_config.bos_token_id == gen_output[0, 0])\n\n        # If we pass a generation config **with** a BOS token, `generate` will use it\n        generation_config = GenerationConfig(bos_token_id=test_bos_id)\n        gen_output = model.generate(generation_config=generation_config)\n        self.assertFalse(model.generation_config.bos_token_id == gen_output[0, 0])\n        self.assertTrue(generation_config.bos_token_id == gen_output[0, 0])\n        self.assertTrue(test_bos_id == gen_output[0, 0])\n\n        # If we pass a generation config **without** a BOS token, `generate` will fetch the BOS token from\n        # `model.generation_config`\n        generation_config = GenerationConfig(bos_token_id=None)\n        gen_output = model.generate(generation_config=generation_config)\n        self.assertTrue(model.generation_config.bos_token_id == gen_output[0, 0])\n        self.assertFalse(test_bos_id == gen_output[0, 0])\n        self.assertTrue(generation_config.bos_token_id is None)\n\n        # Changing `model.generation_config` will affect fallback behavior\n        model.generation_config.bos_token_id = test_bos_id\n        gen_output = model.generate(generation_config=generation_config)\n        self.assertTrue(model.generation_config.bos_token_id == gen_output[0, 0])\n        self.assertTrue(test_bos_id == gen_output[0, 0])\n        self.assertTrue(generation_config.bos_token_id is None)\n\n    def test_speculative_decoding_equals_regular_decoding(self):\n        draft_name = \"double7/vicuna-68m\"\n        target_name = \"Qwen/Qwen2-0.5B-Instruct\"\n\n        draft_model = AutoModelForCausalLM.from_pretrained(draft_name)\n        target_model = AutoModelForCausalLM.from_pretrained(target_name)\n\n        assistant_tokenizer = AutoTokenizer.from_pretrained(draft_name)\n        target_tokenizer = AutoTokenizer.from_pretrained(target_name)\n\n        prompt_size = torch.randint(low=20, high=100, size=(1,))\n        max_new_tokens = torch.randint(low=10, high=50, size=(1,))\n        input_ids = (torch.rand(1, prompt_size[0]) * 100).to(int) + 50\n\n        max_new_tokens_item = max_new_tokens[0].item()\n        expected_out = target_model.generate(input_ids, do_sample=False, max_new_tokens=max_new_tokens_item)\n        predicted_out = target_model.generate(\n            input_ids,\n            do_sample=False,\n            max_new_tokens=max_new_tokens_item,\n            assistant_model=draft_model,\n            tokenizer=target_tokenizer,\n            assistant_tokenizer=assistant_tokenizer,\n        )\n\n        self.assertEqual(expected_out.shape, predicted_out.shape)\n        self.assertTrue((expected_out == predicted_out).all().item())\n\n    @pytest.mark.generate\n    @require_torch_multi_gpu\n    def test_generate_with_static_cache_multi_gpu(self):\n        \"\"\"\n        Tests if the static cache has been set correctly and if generate works correctly when we are using multi-gpus.\n        \"\"\"\n        # need to split manually as auto doesn't work well with unbalanced model\n        device_map = {\"model.embed_tokens\": 0, \"model.layers.0\": 0, \"model.layers.1\": 1, \"model.norm\": 1, \"lm_head\": 0}\n        model = AutoModelForCausalLM.from_pretrained(\n            \"hf-internal-testing/tiny-random-MistralForCausalLM\", device_map=device_map\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n\n        text = \"Hello world\"\n        tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n        input_ids = tokenized_inputs.input_ids.to(torch_device)\n\n        generation_kwargs = {\n            \"max_new_tokens\": 20,\n            \"cache_implementation\": \"static\",\n            \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n        }\n\n        results = model.generate(input_ids, **generation_kwargs)\n        self.assertTrue(isinstance(results.past_key_values, StaticCache))\n\n        # check device of each layer\n        key_cache_0 = results.past_key_values.key_cache[0]\n        value_cache_0 = results.past_key_values.value_cache[0]\n        self.assertTrue(key_cache_0.device == value_cache_0.device == torch.device(0))\n\n        key_cache_1 = results.past_key_values.key_cache[1]\n        value_cache_1 = results.past_key_values.value_cache[1]\n        self.assertTrue(key_cache_1.device == value_cache_1.device == torch.device(1))\n\n    @pytest.mark.generate\n    @require_torch_multi_gpu\n    def test_init_static_cache_multi_gpu(self):\n        \"\"\"\n        Tests if the static cache has been set correctly when we initialize it manually in a multi-gpu setup.\n        \"\"\"\n        # need to split manually as auto doesn't work well with unbalanced model\n        device_map = {\"model.embed_tokens\": 0, \"model.layers.0\": 0, \"model.layers.1\": 1, \"model.norm\": 1, \"lm_head\": 0}\n        model = AutoModelForCausalLM.from_pretrained(\n            \"hf-internal-testing/tiny-random-MistralForCausalLM\", device_map=device_map\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n\n        text = \"Hello world\"\n        tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n        input_ids = tokenized_inputs.input_ids.to(torch_device)\n\n        generation_kwargs = {\n            \"max_new_tokens\": 20,\n            \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n        }\n\n        # TODO: We need to raise a warning in case the cache is not set correctly\n        # with self.assertRaisesRegex(ValueError, \"If you are manually initializing the cache\"):\n        #     past_key_values = StaticCache(\n        #         config=model.config, batch_size=1, max_cache_len=30, device=torch_device, dtype=model.dtype\n        #     )\n        #     results = model.generate(input_ids, past_key_values=past_key_values, **generation_kwargs)\n\n        # deduced from the device_map : layer 0 on device 0 and layer 1 on device 1\n        layer_device_map = {0: 0, 1: 1}\n        past_key_values = StaticCache(\n            config=model.config,\n            batch_size=1,\n            max_cache_len=30,\n            device=torch_device,\n            dtype=model.dtype,\n            layer_device_map=layer_device_map,\n        )\n        results = model.generate(input_ids, past_key_values=past_key_values, **generation_kwargs)\n\n        # check device of each layer\n        key_cache_0 = results.past_key_values.key_cache[0]\n        value_cache_0 = results.past_key_values.value_cache[0]\n        self.assertTrue(key_cache_0.device == value_cache_0.device == torch.device(0))\n\n        key_cache_1 = results.past_key_values.key_cache[1]\n        value_cache_1 = results.past_key_values.value_cache[1]\n        self.assertTrue(key_cache_1.device == value_cache_1.device == torch.device(1))\n\n    @slow\n    def test_padding_input_contrastive_search_gpt2(self):\n        # Load the pre-trained GPT-2 model and tokenizer\n        model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n        model.to(torch_device)\n        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", clean_up_tokenization_spaces=True)\n\n        # Set the tokenizer to left-pad the sequences\n        tokenizer.padding_side = \"left\"\n\n        # Define the PAD token as the EOS token\n        tokenizer.pad_token = tokenizer.eos_token\n        model.generation_config.pad_token_id = model.generation_config.eos_token_id\n\n        # Define the input prompt\n        prompt_text = \"The whispered legends of the haunted mansion spoke\"\n\n        # Tokenize the input prompt\n        encoded_prompt = tokenizer(prompt_text, return_tensors=\"pt\", padding=True)\n        input_ids = encoded_prompt.input_ids.to(torch_device)\n        attention_mask = encoded_prompt.attention_mask.to(torch_device)\n\n        # Define the contrastive search params\n        penalty_alpha = 0.6\n        top_k = 4\n\n        # Define the padding length to add to the input IDs and attention mask\n        padding_length = 10\n\n        # Generate text without padding\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            do_sample=False,\n            penalty_alpha=penalty_alpha,\n            top_k=top_k,\n            max_new_tokens=64,\n        )\n        generated_text_no_padding = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        # Pad the input IDs and attention mask on the left\n        padded_input_ids = F.pad(\n            input_ids, (padding_length, 0), \"constant\", value=model.generation_config.pad_token_id\n        )\n        padded_attention_mask = F.pad(attention_mask, (padding_length, 0), \"constant\", value=0)\n\n        # Generate text with padded inputs\n        outputs_with_padding = model.generate(\n            input_ids=padded_input_ids,\n            attention_mask=padded_attention_mask,\n            do_sample=False,\n            penalty_alpha=penalty_alpha,\n            top_k=top_k,\n            max_new_tokens=64,\n        )\n        generated_text_with_padding = tokenizer.decode(outputs_with_padding[0], skip_special_tokens=True)\n\n        # Assert that the generated texts are identical for padded and non-padded inputs\n        self.assertEqual(generated_text_no_padding, generated_text_with_padding)\n        self.assertEqual(\n            generated_text_with_padding,\n            'The whispered legends of the haunted mansion spoke of the \"souls of the dead\" who were \"falling '\n            'out of the sky\" and \"falling into the sea.\"\\n\\nThe ghostly apparitions were said to have been '\n            'created by the spirits of the dead, who were \"falling out of the sky\" and \"falling into the sea',\n        )\n\n    @slow\n    def test_padding_input_contrastive_search_t5(self):\n        # Load the pre-trained T5 model and tokenizer\n        model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n        model.to(torch_device)\n        tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\", clean_up_tokenization_spaces=True)\n\n        # Define the input prompt\n        prompt_text = \"translate English to German: I need to finish this task before the end of the day.\"\n\n        # Tokenize the input prompt\n        encoded_prompt = tokenizer(prompt_text, return_tensors=\"pt\")\n        input_ids = encoded_prompt.input_ids.to(torch_device)\n        attention_mask = encoded_prompt.attention_mask.to(torch_device)\n\n        # Define the decoder prompt\n        decoder_prompt_text = \"Ich muss diese Aufgabe\"\n        encoded_decoder_prompt = tokenizer(decoder_prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n        decoder_input_ids = encoded_decoder_prompt.input_ids.to(torch_device)\n        decoder_attention_mask = encoded_decoder_prompt.attention_mask.to(torch_device)\n\n        # Define the contrastive search params\n        penalty_alpha = 0.6\n        top_k = 4\n\n        # Generate text without padding\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            do_sample=False,\n            penalty_alpha=penalty_alpha,\n            top_k=top_k,\n            max_new_tokens=64,\n        )\n        generated_text_no_padding = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        # Define the padding length to add to the input IDs and attention mask\n        padding_length = 10\n\n        # Pad the decoder input IDs and attention mask on the left\n        padded_decoder_input_ids = F.pad(\n            decoder_input_ids, (padding_length, 0), \"constant\", value=model.generation_config.pad_token_id\n        )\n        padded_decoder_attention_mask = F.pad(decoder_attention_mask, (padding_length, 0), \"constant\", value=0)\n        # Since the decoder_start_token_id is the same as the pad_token_id,\n        # the last padded token represents the decoder start token.\n        # Set the attention mask for the decoder_start_token_id to True (1).\n        padded_decoder_attention_mask[:, padding_length - 1] = 1\n        # Generate text with padded inputs\n        outputs_with_padding = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=padded_decoder_input_ids,\n            decoder_attention_mask=padded_decoder_attention_mask,\n            do_sample=False,\n            penalty_alpha=penalty_alpha,\n            top_k=top_k,\n            max_new_tokens=64,\n        )\n        generated_text_with_padding = tokenizer.decode(outputs_with_padding[0], skip_special_tokens=True)\n\n        # Assert that the generated texts are identical for padded and non-padded inputs\n        self.assertEqual(generated_text_no_padding, generated_text_with_padding)\n        self.assertEqual(generated_text_no_padding, \"Ich muss diese Aufgabe vor Ende des Tages beenden.\")\n\n    def test_prepare_inputs_for_generation_decoder_llm(self):\n        \"\"\"Tests GenerationMixin.prepare_inputs_for_generation against expected usage with decoder-only llms.\"\"\"\n\n        config = AutoConfig.from_pretrained(\"hf-internal-testing/tiny-random-LlamaForCausalLM\")\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-LlamaForCausalLM\")\n        model = model.to(torch_device)\n\n        # 1. Sanity check: the model's `prepare_inputs_for_generation` comes from `GenerationMixin`\n        self.assertTrue(\"GenerationMixin\" in str(model.prepare_inputs_for_generation))\n\n        # 2. If we pass input ids by themselves, we should get back the same input ids\n        input_ids = torch.tensor([[1, 2, 3], [4, 5, 6]]).to(torch_device)\n        model_inputs = model.prepare_inputs_for_generation(input_ids)\n        self.assertTrue(torch.all(model_inputs[\"input_ids\"] == input_ids))\n\n        # 3. If we pass the attention mask too, we will get back the attention mask and position ids built from it\n        attention_mask = torch.tensor([[1, 1, 1], [1, 1, 1]]).to(torch_device)\n        model_inputs = model.prepare_inputs_for_generation(input_ids, attention_mask=attention_mask)\n        self.assertTrue(torch.all(model_inputs[\"attention_mask\"] == attention_mask))\n        self.assertTrue(model_inputs[\"position_ids\"].shape == input_ids.shape)\n\n        # 4. `use_cache` (and other kwargs) are forwarded\n        self.assertFalse(\"use_cache\" in model_inputs)  # From the previous input, there is no `use_cache`\n        model_inputs = model.prepare_inputs_for_generation(input_ids, use_cache=True, foo=\"bar\")\n        self.assertTrue(model_inputs[\"use_cache\"] is True)\n        self.assertTrue(model_inputs[\"foo\"] == \"bar\")\n\n        # 5. When we pass a cache, we discard data related to already seen tokens in some tensors. We are now also\n        # forced to pass a correctly prepared `cache_positions` to slice the data accordingly.\n        init_input_ids = input_ids[:, :2]\n        dynamic_cache = DynamicCache()\n        dynamic_cache = model(init_input_ids, past_key_values=dynamic_cache).past_key_values\n        with self.assertRaises(AttributeError):  # past_key_values + no cache_position -> exception\n            model_inputs = model.prepare_inputs_for_generation(input_ids, past_key_values=dynamic_cache)\n\n        cache_position = torch.arange(input_ids.shape[-1], dtype=torch.long).to(torch_device)\n        cache_position = cache_position[dynamic_cache.get_seq_length() :]\n        model_inputs = model.prepare_inputs_for_generation(\n            input_ids, past_key_values=dynamic_cache, cache_position=cache_position, attention_mask=attention_mask\n        )\n        self.assertTrue(\"past_key_values\" in model_inputs)\n        self.assertTrue(torch.all(model_inputs[\"cache_position\"] == cache_position))\n        self.assertTrue(model_inputs[\"input_ids\"].shape[-1] == 1)  # 1 = 3 fed tokens - 2 tokens in the cache\n        self.assertTrue(model_inputs[\"position_ids\"].shape[-1] == 1)\n        self.assertTrue(model_inputs[\"attention_mask\"].shape[-1] == 3)  # we still need the full attention mask!\n\n        # 6. If we pass a `static_cache`, the attention mask will be prepared as a static shape 4D mask\n        max_cache_len = 10\n        batch_size = 2\n        query_length = input_ids.shape[-1] - init_input_ids.shape[-1]\n        static_cache = StaticCache(\n            config=config, batch_size=batch_size, max_cache_len=max_cache_len, device=torch_device, dtype=torch.float32\n        )\n        static_cache = model(init_input_ids, past_key_values=static_cache).past_key_values\n        model_inputs = model.prepare_inputs_for_generation(\n            input_ids, past_key_values=static_cache, cache_position=cache_position, attention_mask=attention_mask\n        )\n        self.assertTrue(\"past_key_values\" in model_inputs)\n        self.assertTrue(list(model_inputs[\"attention_mask\"].shape) == [batch_size, 1, query_length, max_cache_len])\n\n        # 7. We can also pass `inputs_embeds` as the embedded prompt. Because `generate` will append its result to\n        # `input_ids` and the models will only accept one of the two inputs (`input_ids` or `inputs_embeds`), we\n        # a) must use the cache b) must expect `input_ids` after the prompt is processed\n        init_inputs_embeds = model.get_input_embeddings()(init_input_ids)\n        init_cache_positions = torch.arange(init_input_ids.shape[-1], dtype=torch.long).to(torch_device)\n        empty_cache = DynamicCache()\n\n        # Prompt processing\n        model_inputs = model.prepare_inputs_for_generation(\n            init_input_ids,\n            past_key_values=empty_cache,\n            inputs_embeds=init_inputs_embeds,\n            cache_position=init_cache_positions,\n        )\n        self.assertTrue(model_inputs[\"input_ids\"] is None)\n        self.assertTrue(model_inputs[\"inputs_embeds\"] is not None)\n\n        # After prompt processing\n        model_inputs = model.prepare_inputs_for_generation(\n            input_ids, past_key_values=dynamic_cache, inputs_embeds=init_inputs_embeds, cache_position=cache_position\n        )\n        self.assertTrue(model_inputs[\"input_ids\"] is not None)\n        self.assertTrue(model_inputs[\"inputs_embeds\"] is None)\n\n    def test_prepare_inputs_for_generation_encoder_decoder_llm(self):\n        \"\"\"\n        Same as `test_prepare_inputs_for_generation_decoder_llm` but for encoder-decoder models. Main difference: we\n        should look for `decoder_input_ids`, instead of `input_ids`.\n        \"\"\"\n        model = AutoModelForSeq2SeqLM.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n        model = model.to(torch_device)\n\n        # 1. Sanity check: the model's `prepare_inputs_for_generation` comes from `GenerationMixin`\n        self.assertTrue(\"GenerationMixin\" in str(model.prepare_inputs_for_generation))\n\n        # 2. If we pass input ids by themselves, we should get back the same input ids -- with the encoder-decoder key\n        decoder_input_ids = torch.tensor([[1, 2, 3], [4, 5, 6]]).to(torch_device)\n        model_inputs = model.prepare_inputs_for_generation(decoder_input_ids)\n        self.assertTrue(torch.all(model_inputs[\"decoder_input_ids\"] == decoder_input_ids))\n\n        # 3. If we pass the attention mask too, we will get back the attention mask. Encoder-decoder models usually\n        # don't use `position_ids`\n        decoder_attention_mask = torch.tensor([[1, 1, 1], [1, 1, 1]]).to(torch_device)\n        model_inputs = model.prepare_inputs_for_generation(\n            decoder_input_ids, decoder_attention_mask=decoder_attention_mask\n        )\n        self.assertTrue(torch.all(model_inputs[\"decoder_attention_mask\"] == decoder_attention_mask))\n        self.assertTrue(\"position_ids\" not in model_inputs)\n\n        # 4. `use_cache` (and other kwargs, like the encoder outputs) are forwarded\n        self.assertFalse(\"use_cache\" in model_inputs)  # From the previous input, there is no `use_cache`\n        model_inputs = model.prepare_inputs_for_generation(decoder_input_ids, use_cache=True, encoder_outputs=\"foo\")\n        self.assertTrue(model_inputs[\"use_cache\"] is True)\n        self.assertTrue(model_inputs[\"encoder_outputs\"] == \"foo\")\n        # See the decoder-only test for more corner cases. The code is the same, so we don't repeat it here.\n\n    def test_generate_compile_fullgraph_tiny(self):\n        \"\"\"\n        Tests that we can call end-to-end generation with a tiny model (i.e. doesn't crash)\n        NOTE: this test is quite slow (~20s on a consumer desktop), but it is important that we keep it as part of the\n        non-slow tests to prevent regressions!\n        \"\"\"\n        model = AutoModelForCausalLM.from_pretrained(\n            \"hf-internal-testing/tiny-random-LlamaForCausalLM\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n        )\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-LlamaForCausalLM\")\n\n        # compile generate\n        compiled_generate = torch.compile(model.generate, fullgraph=True, mode=\"reduce-overhead\")\n\n        # compiled generate does NOT accept parameterization except a) model inputs b) a generation config\n        generation_config = copy.deepcopy(model.generation_config)\n        generation_config.pad_token_id = model.config.eos_token_id\n\n        model_inputs = tokenizer([\"Write a poem about the market crashing in summer\"], return_tensors=\"pt\")\n        model_inputs = model_inputs.to(model.device)\n        gen_out = compiled_generate(**model_inputs, generation_config=generation_config)\n        self.assertTrue(gen_out.shape[1] > model_inputs[\"input_ids\"].shape[1])  # some text was generated\n\n    def test_assisted_generation_early_exit(self):\n        \"\"\"\n        Tests that assisted generation with early exit works as expected. Under the hood, this has complex cache\n        manipulation, which will cause the test to fail if something goes wrong there.\n        \"\"\"\n        expected_output = \"Alice and Bob are playing a game of poker. Alice has a pair of 8s and Bob has a pair\"\n\n        prompt = \"Alice and Bob\"\n        checkpoint = \"facebook/layerskip-llama3.2-1B\"\n\n        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(torch_device)\n\n        model = AutoModelForCausalLM.from_pretrained(checkpoint).to(torch_device)\n        original_outputs = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n        original_decoded = tokenizer.batch_decode(original_outputs, skip_special_tokens=True)\n        self.assertEqual(original_decoded, [expected_output])\n\n        outputs_assisted = model.generate(**inputs, assistant_early_exit=4, do_sample=False, max_new_tokens=20)\n        decoded_assisted = tokenizer.batch_decode(outputs_assisted, skip_special_tokens=True)\n        self.assertEqual(decoded_assisted, [expected_output])\n\n    @slow\n    def test_beam_search_advanced_stopping_criteria(self):\n        \"\"\"\n        Tests that beam search works with a stopping criteria that is not max length or EOS token. Prior to the beam\n        search vectorization PR (#35802), beam search was not accepting other stopping criteria. Test inspired on\n        the original issue (#34843).\n        \"\"\"\n        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n        model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\").to(torch_device)\n\n        prompt = (\n            \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. \"\n            \"How many clips did Natalia sell altogether in April and May?\"\n        )\n        tokens = tokenizer(prompt, return_tensors=\"pt\").to(torch_device)\n        generation_config = GenerationConfig(num_beams=3, do_sample=False, length_penalty=1.0, max_new_tokens=100)\n\n        # This particular prompt should result in a \":\" being present in the answer\n        out = model.generate(**tokens, generation_config=generation_config, tokenizer=tokenizer)\n        output_text = tokenizer.decode(out[0], skip_special_tokens=True)\n        last_non_special_token_decoded = tokenizer.decode(out[out != tokenizer.pad_token_id][-1])\n        self.assertTrue(\":\" in output_text)\n        self.assertFalse(\":\" in output_text[-5:])\n        self.assertFalse(\":\" in last_non_special_token_decoded)\n\n        # Adding an advanced stopping criteria: text generation should stop when a \":\" is generated.\n        # Note that:\n        # 1 - the text up to \":\" doesn't have to be the same, it can belong to a different beam\n        # 2 - \":\" may not be the last char, but it must be in the last non-special token\n        generation_config.stop_strings = \":\"\n        out = model.generate(**tokens, generation_config=generation_config, tokenizer=tokenizer)\n        output_text = tokenizer.decode(out[0], skip_special_tokens=True)\n        last_non_special_token_decoded = tokenizer.decode(out[out != tokenizer.pad_token_id][-1])\n        self.assertTrue(\":\" in output_text)\n        self.assertTrue(\":\" in output_text[-5:])\n        self.assertTrue(\":\" in last_non_special_token_decoded)\n\n    def test_max_time(self):\n        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n        model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n        model.to(torch_device)\n\n        torch.manual_seed(0)\n        tokenized = tokenizer(\"Today is a nice day and\", return_tensors=\"pt\", return_token_type_ids=True)\n        input_ids = tokenized.input_ids.to(torch_device)\n\n        MAX_TIME = 0.1\n        MAX_LENGTH = 64\n\n        # sampling on\n        start = datetime.datetime.now()\n        model.generate(input_ids, do_sample=True, max_time=MAX_TIME, max_length=MAX_LENGTH)\n        duration = datetime.datetime.now() - start\n        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n\n        # sampling off\n        start = datetime.datetime.now()\n        model.generate(input_ids, do_sample=False, max_time=MAX_TIME, max_length=MAX_LENGTH)\n        duration = datetime.datetime.now() - start\n        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n\n        # beam search\n        start = datetime.datetime.now()\n        model.generate(input_ids, do_sample=False, num_beams=2, max_time=MAX_TIME, max_length=MAX_LENGTH)\n        duration = datetime.datetime.now() - start\n        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n\n        # sanity check: no time limit\n        start = datetime.datetime.now()\n        model.generate(input_ids, do_sample=False, max_time=None, max_length=MAX_LENGTH)\n        duration = datetime.datetime.now() - start\n        self.assertGreater(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n\n    def test_validate_generation_inputs(self):\n        \"\"\"Tests validation of inputs to `generate`\"\"\"\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n        model = AutoModelForSeq2SeqLM.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n\n        encoder_input_str = \"Hello world\"\n        input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_ids\n\n        # typos are quickly detected (the correct argument is `do_sample`)\n        with self.assertRaisesRegex(ValueError, \"do_samples\"):\n            model.generate(input_ids, do_samples=True)\n\n        # arbitrary arguments that will not be used anywhere are also not accepted\n        with self.assertRaisesRegex(ValueError, \"foo\"):\n            fake_model_kwargs = {\"foo\": \"bar\"}\n            model.generate(input_ids, **fake_model_kwargs)\n\n        # however, valid model_kwargs are accepted\n        valid_model_kwargs = {\"attention_mask\": torch.tensor(np.zeros_like(input_ids))}\n        model.generate(input_ids, **valid_model_kwargs)\n\n    def test_custom_logits_processor(self):\n        \"\"\"Tests that custom logits processors can be used in `generate`, and that redundant arguments are caught.\"\"\"\n        bart_tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n        article = \"\"\"Justin Timberlake and Jessica Biel, welcome to parenthood.\"\"\"\n        bart_model = AutoModelForSeq2SeqLM.from_pretrained(\"hf-internal-testing/tiny-random-bart\", min_length=1)\n        input_ids = bart_tokenizer(article, return_tensors=\"pt\").input_ids\n\n        logits_processor = LogitsProcessorList()\n        logits_processor.append(MinLengthLogitsProcessor(min_length=10, eos_token_id=0))\n\n        # it should not be allowed to both define `min_length` via config and `logits_processor` list\n        with self.assertRaises(ValueError):\n            bart_model.generate(input_ids, logits_processor=logits_processor, min_length=10)\n        bart_model.generate(input_ids, logits_processor=logits_processor)\n\n    def test_transition_scores_greedy_search(self):\n        \"\"\"Test that `compute_transition_scores` is working as expected with gready search\"\"\"\n        articles = [\"Justin Timberlake\", \"Michael Phelps\"]\n        tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\", padding_side=\"left\")\n        tokenizer.pad_token = tokenizer.eos_token\n\n        model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n        model.generation_config.eos_token_id = None\n        input_ids = tokenizer(articles, return_tensors=\"pt\", padding=True).input_ids\n        model = model.to(torch_device)\n        input_ids = input_ids.to(torch_device)\n\n        outputs = model.generate(\n            input_ids=input_ids,\n            max_new_tokens=5,\n            pad_token_id=tokenizer.eos_token_id,\n            return_dict_in_generate=True,\n            output_scores=True,\n        )\n\n        transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores)\n        transition_scores = transition_scores.cpu().numpy()\n\n        expected_scores = np.array(\n            [\n                [-57.8844, -60.45698, -70.16364, -65.50791, -66.35648],\n                [-54.417572, -60.216614, -62.661243, -58.621933, -58.298683],\n            ]\n        )\n        self.assertTrue(np.allclose(transition_scores, expected_scores, atol=1e-3))\n\n    def test_transition_scores_greedy_search_normalized(self):\n        \"\"\"\n        Test that `compute_transition_scores` is working as expected with gready search, with `normalize_logits=True`\n        \"\"\"\n        articles = [\"Justin Timberlake\", \"Michael Phelps\"]\n        tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\", padding_side=\"left\")\n        tokenizer.pad_token = tokenizer.eos_token\n\n        model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n        model.generation_config.eos_token_id = None\n        input_ids = tokenizer(articles, return_tensors=\"pt\", padding=True).input_ids\n        model = model.to(torch_device)\n        input_ids = input_ids.to(torch_device)\n\n        outputs = model.generate(\n            input_ids=input_ids,\n            max_new_tokens=5,\n            pad_token_id=tokenizer.eos_token_id,\n            return_dict_in_generate=True,\n            output_scores=True,\n        )\n\n        transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n        transition_scores = transition_scores.cpu().numpy()\n\n        expected_scores = np.array(\n            [\n                [-2.538938, -2.2694316, -2.1580915, -1.572299, -2.6719835],\n                [-1.8826028, -2.2461371, -1.7556462, -2.9644494, -1.7996008],\n            ]\n        )\n        self.assertTrue(np.allclose(transition_scores, expected_scores, atol=1e-3))\n\n    def test_transition_scores_beam_search_encoder_decoder(self):\n        \"\"\"\n        Test that `compute_transition_scores` is working as expected with beam search and encoder-decoder models\n        \"\"\"\n        articles = [\n            \"Justin Timberlake and Jessica Biel, welcome to parenthood.\",\n            \"Michael Phelps is arguably the most decorated Olympian of all time.\",\n        ]\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n        model = AutoModelForSeq2SeqLM.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n        input_ids = tokenizer(articles, return_tensors=\"pt\", padding=True).input_ids\n        model = model.to(torch_device)\n        input_ids = input_ids.to(torch_device)\n\n        outputs = model.generate(\n            input_ids=input_ids,\n            max_length=10,\n            num_beams=4,\n            num_return_sequences=2,\n            eos_token_id=None,\n            return_dict_in_generate=True,\n            output_scores=True,\n            length_penalty=0.0,\n        )\n\n        transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, outputs.beam_indices)\n        transition_scores = transition_scores.cpu().numpy()\n        outputs.sequences_scores = outputs.sequences_scores.cpu().numpy()\n\n        self.assertTrue(np.allclose(np.sum(transition_scores, axis=-1), outputs.sequences_scores, atol=1e-3))\n\n    def test_transition_scores_beam_search_encoder_decoder_with_eos(self):\n        \"\"\"\n        Test that `compute_transition_scores` is working as expected with beam search and encoder-decoder models, when\n        an EOS token is defined\n        \"\"\"\n        articles = [\n            \"Justin Timberlake and Jessica Biel, welcome to parenthood.\",\n            \"Michael Phelps is arguably the most decorated Olympian of all time.\",\n        ]\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n\n        model = AutoModelForSeq2SeqLM.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n        input_ids = tokenizer(articles, return_tensors=\"pt\", padding=True).input_ids\n        model = model.to(torch_device)\n        input_ids = input_ids.to(torch_device)\n\n        outputs = model.generate(\n            input_ids=input_ids,\n            max_length=10,\n            num_beams=4,\n            num_return_sequences=2,\n            return_dict_in_generate=True,\n            output_scores=True,\n            length_penalty=0.0,\n        )\n\n        transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, outputs.beam_indices)\n        transition_scores = transition_scores.cpu().numpy()\n        outputs.sequences_scores = outputs.sequences_scores.cpu().numpy()\n\n        self.assertTrue(np.allclose(np.sum(transition_scores, axis=-1), outputs.sequences_scores, atol=1e-3))\n\n    def test_transition_scores_beam_search_decoder_only(self):\n        \"\"\"\n        Test that `compute_transition_scores` is working as expected with beam search and decoder-only models\n        \"\"\"\n        articles = [\n            \"Justin Timberlake\",\n            \"Michael Phelps\",\n        ]\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        tokenizer.pad_token = tokenizer.eos_token\n\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        input_ids = tokenizer(articles, return_tensors=\"pt\", padding=True).input_ids\n        model = model.to(torch_device)\n        input_ids = input_ids.to(torch_device)\n\n        outputs = model.generate(\n            input_ids=input_ids,\n            max_length=10,\n            num_beams=4,\n            num_return_sequences=2,\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=None,\n            return_dict_in_generate=True,\n            output_scores=True,\n            length_penalty=0.0,\n        )\n\n        transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, outputs.beam_indices)\n        transition_scores = transition_scores.cpu().numpy()\n        outputs.sequences_scores = outputs.sequences_scores.cpu().numpy()\n\n        self.assertTrue(np.allclose(np.sum(transition_scores, axis=-1), outputs.sequences_scores, atol=1e-3))\n\n    @slow\n    def test_transition_scores_early_stopping(self):\n        \"\"\"\n        Test that `compute_transition_scores` is working as expected with beam search and early stopping\n\n        This is an aggressive test that makes sure that `beam_search's`\n        transition scores are computed correctly for varying `num_return_sequences`, `num_beams` and `batch_size > 1`\n        2 x input_ids for \"question: How are you? \\n context: I had a long day, \"\n        \"\"\"\n        input_ids = torch.tensor(2 * [[822, 10, 571, 33, 25, 58, 2625, 10, 27, 141, 3, 9, 307, 239, 6, 1]])\n        model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n        model = model.to(torch_device)\n        input_ids = input_ids.to(torch_device)\n\n        outputs = model.generate(\n            input_ids,\n            max_length=10,\n            return_dict_in_generate=True,\n            output_scores=True,\n            forced_eos_token_id=model.config.eos_token_id,\n            num_beams=4,\n            do_sample=False,\n            num_return_sequences=3,\n            length_penalty=0.0,\n        )\n\n        transition_scores = model.compute_transition_scores(\n            sequences=outputs.sequences, scores=outputs.scores, beam_indices=outputs.beam_indices\n        )\n        transition_scores = transition_scores.cpu().numpy()\n        outputs.sequences_scores = outputs.sequences_scores.cpu().numpy()\n\n        self.assertTrue(np.allclose(np.sum(transition_scores, axis=-1), outputs.sequences_scores))\n\n    def test_encoder_decoder_generate_attention_mask(self):\n        \"\"\"\n        Test that `generate` automagically creates the correct `attention_mask` for encoder-decoder models (which\n        has a different keyword)\n        \"\"\"\n        articles = [\"Timberlake\", \"Jessica Biel, welcome to parenthood among other things\"]\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n        # need extreme generation values here to force this test\n        # to fail when `attention_mask` is not correctly treated in generate\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            \"hf-internal-testing/tiny-random-bart\",\n        )\n        model.config.eos_token_id = None\n        input_ids = tokenizer(articles[0], return_tensors=\"pt\").input_ids\n        input_ids_batched = tokenizer(articles, padding=True, return_tensors=\"pt\").input_ids\n        model = model.to(torch_device)\n        input_ids = input_ids.to(torch_device)\n        input_ids_batched = input_ids_batched.to(torch_device)\n\n        generate_kwargs = {\n            \"return_dict_in_generate\": True,\n            \"output_scores\": True,\n            \"max_length\": 50,\n            \"num_beams\": 5,\n            \"num_return_sequences\": 5,\n        }\n\n        output_sequences_batched = model.generate(input_ids=input_ids_batched, **generate_kwargs)\n        output_sequences = model.generate(input_ids=input_ids, **generate_kwargs)\n\n        batched_out = output_sequences_batched.sequences_scores\n        out = output_sequences.sequences_scores\n        batched_out = batched_out.cpu().numpy()\n        out = out.cpu().numpy()\n\n        diff = np.abs(np.sum(batched_out[:5]) - np.sum(out))\n        self.assertTrue(diff < 1e-4)\n\n    def test_generate_input_ids_as_kwarg(self):\n        \"\"\"Test that `input_ids` work equally as a positional and keyword argument in decoder-only models\"\"\"\n        article = \"I need input_ids to generate\"\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\", max_length=15)\n        input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n        model = model.to(torch_device)\n        input_ids = input_ids.to(torch_device)\n\n        output_sequences_kwargs = model.generate(input_ids=input_ids)\n        output_sequences = model.generate(input_ids)\n        output_sequences_kwargs = output_sequences_kwargs.cpu().numpy()\n        output_sequences = output_sequences.cpu().numpy()\n\n        self.assertTrue(np.array_equal(output_sequences, output_sequences_kwargs))\n        self.assertEqual(output_sequences.shape, (1, 15))\n\n    def test_generate_input_ids_as_encoder_kwarg(self):\n        \"\"\"Test that `input_ids` work equally as a positional and keyword argument in encoder-decoder models\"\"\"\n        article = \"Justin Timberlake and Jessica Biel, welcome to parenthood.\"\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n        model = AutoModelForSeq2SeqLM.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n        model.config.eos_token_id = None\n        input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n        model = model.to(torch_device)\n        input_ids = input_ids.to(torch_device)\n\n        output_sequences_kwargs = model.generate(input_ids=input_ids, max_length=5)\n        output_sequences = model.generate(input_ids, max_length=5)\n        output_sequences_kwargs = output_sequences_kwargs.cpu().numpy()\n        output_sequences = output_sequences.cpu().numpy()\n\n        self.assertTrue(np.array_equal(output_sequences, output_sequences_kwargs))\n        self.assertEqual(output_sequences.shape, (1, 5))\n\n    def test_generate_inputs_and_encoder_kwargs(self):\n        \"\"\"\n        Test that an exception is thrown if the main tensor (`input_ids` in LLMs) is passed as both a positional and\n        keyword argument\n        \"\"\"\n        article = \"I need input_ids to generate\"\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\", max_length=10)\n        input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n        with self.assertRaises(ValueError):\n            model.generate(input_ids, input_ids=input_ids)\n\n    def test_generate_too_many_encoder_kwargs(self):\n        \"\"\"Test that passing redundant inputs results in an exception (`input_ids` and `inputs_embeds` in LLMs)\"\"\"\n        article = \"I need input_ids to generate\"\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bart\")\n        model = AutoModelForSeq2SeqLM.from_pretrained(\"hf-internal-testing/tiny-random-bart\", max_length=10)\n        input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n        with self.assertRaises(ValueError):\n            model.generate(input_ids=input_ids, inputs_embeds=input_ids)\n\n    def test_generate_input_features_as_encoder_kwarg(self):\n        \"\"\"Test that non-`input_ids` main model inputs are correctly handled as positional arguments\"\"\"\n        input_features = floats_tensor((3, 80, 60))\n        model = AutoModelForSpeechSeq2Seq.from_pretrained(\n            \"hf-internal-testing/tiny-random-WhisperForConditionalGeneration\"\n        )\n        input_features.to(torch_device)\n        model = model.to(torch_device)\n\n        output_sequences_kwargs = model.generate(input_features=input_features, max_length=5)\n        output_sequences = model.generate(input_features, max_length=5)\n        output_sequences_kwargs = output_sequences_kwargs.cpu().numpy()\n        output_sequences = output_sequences.cpu().numpy()\n\n        self.assertTrue(np.array_equal(output_sequences, output_sequences_kwargs))\n        self.assertEqual(output_sequences.shape, (3, 5))\n\n    def test_generate_encoder_outputs_attention_mask(self):\n        \"\"\"Test that `generate` can handle attention masks when the encoder outputs are passed\"\"\"\n        input_features = floats_tensor((3, 80, 60))\n        attention_mask = torch.randint(0, 2, input_features.shape).to(torch_device)\n        model = AutoModelForSpeechSeq2Seq.from_pretrained(\n            \"hf-internal-testing/tiny-random-WhisperForConditionalGeneration\"\n        )\n        input_features = input_features.to(torch_device)\n        attention_mask = attention_mask.to(torch_device)\n        model = model.to(torch_device)\n\n        encoder = model.get_encoder()\n        encoder_outputs = encoder(input_features)\n\n        output_sequences_no_mask = model.generate(encoder_outputs=encoder_outputs)\n        output_sequences_with_mask = model.generate(encoder_outputs=encoder_outputs, attention_mask=attention_mask)\n        output_sequences_no_mask = output_sequences_no_mask.cpu().numpy()\n        output_sequences_with_mask = output_sequences_with_mask.cpu().numpy()\n\n        self.assertFalse(np.array_equal(output_sequences_no_mask, output_sequences_with_mask))\n\n    def test_eos_token_id_int_and_list_greedy_search(self):\n        \"\"\"Test that `generate` can handle multiple EOS tokens\"\"\"\n        generation_kwargs = {\n            \"do_sample\": False,\n            \"num_beams\": 1,\n        }\n        expectation = 13\n\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        text = \"\"\"Hello, my dog is cute and\"\"\"\n        tokens = tokenizer(text, return_tensors=\"pt\")\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        model = model.to(torch_device)\n        tokens = tokens.to(torch_device)\n\n        eos_token_id = 873\n        generated_tokens = model.generate(**tokens, eos_token_id=eos_token_id, **generation_kwargs)\n        self.assertTrue(expectation == len(generated_tokens[0]))\n\n        eos_token_id = [873, 198]\n        generated_tokens = model.generate(**tokens, eos_token_id=eos_token_id, **generation_kwargs)\n        self.assertTrue(expectation == len(generated_tokens[0]))\n\n    def test_generate_vision2text_conditioning(self):\n        \"\"\"Test that `decoder_input_ids` can be used to condition the generation in vision-to-text models\"\"\"\n        pixel_values = floats_tensor((2, 3, 30, 30))\n        conditioning_input = torch.tensor([[10], [10]])  # this should be the 2nd output token, after the BOS token\n        model = AutoModelForVision2Seq.from_pretrained(\n            \"hf-internal-testing/tiny-random-VisionEncoderDecoderModel-vit-gpt2\"\n        )\n        pixel_values = pixel_values.to(torch_device)\n        model = model.to(torch_device)\n        conditioning_input = conditioning_input.to(torch_device)\n\n        # we can condition on decoder_input_ids (expected decoder input) and input_ids (which we pipe internally as\n        # decoder_input_ids, if the encoder is not a model with text input)\n        output_sequences_decoder_input_ids = model.generate(\n            pixel_values, max_length=5, decoder_input_ids=conditioning_input\n        )\n        output_sequences_input_ids = model.generate(pixel_values, max_length=5, input_ids=conditioning_input)\n        output_sequences_decoder_input_ids = output_sequences_decoder_input_ids.cpu().numpy()\n        output_sequences_input_ids = output_sequences_input_ids.cpu().numpy()\n        conditioning_input = conditioning_input.cpu().numpy()\n\n        self.assertTrue(np.array_equal(output_sequences_decoder_input_ids, output_sequences_input_ids))\n        self.assertTrue(np.array_equal(output_sequences_decoder_input_ids[:, 1:2], conditioning_input))\n\n\n@require_torch\nclass TokenHealingTestCase(unittest.TestCase):\n    @parameterized.expand(\n        [\n            (\"url\", 'The link is <a href=\"http:', 'The link is <a href=\"http://'),\n            # aggressive_healing: \"http\" shouldn't be replaced with \"https\"\n            (\"aggressive_healing\", 'The link is <a href=\"http', 'The link is <a href=\"http'),\n            (\"trailing_whitespace\", \"I read a book about \", \"I read a book about\"),\n            (\"nothing_to_heal\", \"I read a book about\", \"I read a book about\"),\n            (\"single_token\", \"I\", \"I\"),\n            (\"empty_prompt\", \"\", \"\"),\n        ]\n    )\n    def test_prompts(self, name, input, expected):\n        model_name_or_path = \"distilbert/distilgpt2\"\n        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n        completion_model = AutoModelForCausalLM.from_pretrained(\n            model_name_or_path,\n            device_map=\"auto\",\n            trust_remote_code=False,\n            revision=\"main\",\n            use_cache=True,\n        )\n\n        \"\"\"\n        tokenizer.pad_token value can be empty but it is required in the latter codes\n        so assigned it here with eos_token\n\t\t\"\"\"\n        tokenizer.pad_token = tokenizer.eos_token\n\n        input_ids = tokenizer(input, return_tensors=\"pt\").input_ids.to(completion_model.device)\n\n        healed_ids = completion_model.heal_tokens(input_ids, tokenizer=tokenizer)\n        predicted = tokenizer.decode(healed_ids[0], skip_special_tokens=True)\n\n        self.assertEqual(predicted, expected)\n\n    def test_generate_from_inputs_embeds_with_bos_token_id_is_none(self):\n        article = \"Today a dragon flew over Paris.\"\n        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n        input_ids = tokenizer(article, return_tensors=\"pt\").input_ids.to(torch_device)\n        inputs_embeds = model.get_input_embeddings()(input_ids)\n\n        model.generate(inputs_embeds=inputs_embeds, max_length=20, bos_token_id=None)\n\n        # bos_token_id is required when no input ids nor inputs_embeds is passed\n        with self.assertRaises(ValueError):\n            model.generate(max_length=20, bos_token_id=None)\n\n\nclass TestAssistedCandidateGeneratorDifferentTokenizers(unittest.TestCase):\n    def test_no_intersection(self):\n        prompt = np.array([[1, 2, 3]])\n        prompt_plus_new_tokens = np.array([[4, 5, 6]])\n        result = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(prompt, prompt_plus_new_tokens)\n        self.assertEqual(result, (None, None, None))\n\n    def test_complete_overlap(self):\n        prompt = np.array([[1, 2, 3]])\n        prompt_plus_new_tokens = np.array([[1, 2, 3, 4, 5]])\n        discrep_length, new_tokens_only, discrep_only = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n            prompt, prompt_plus_new_tokens\n        )\n        self.assertEqual(discrep_length, 0)\n        np.testing.assert_array_equal(new_tokens_only, np.array([[4, 5]]))\n        np.testing.assert_array_equal(discrep_only, np.array([[]]))\n\n    def test_partial_overlap(self):\n        prompt = np.array([[1, 2, 3]])\n        prompt_plus_new_tokens = np.array([[2, 3, 4, 5]])\n        discrep_length, new_tokens_only, discrep_only = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n            prompt, prompt_plus_new_tokens\n        )\n        self.assertEqual(discrep_length, 0)\n        np.testing.assert_array_equal(new_tokens_only, np.array([[4, 5]]))\n        np.testing.assert_array_equal(discrep_only, np.array([[]]))\n\n    def test_no_new_tokens(self):\n        prompt = np.array([[1, 2, 3]])\n        prompt_plus_new_tokens = np.array([[1, 2, 3]])\n        discrep_length, new_tokens_only, discrep_only = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n            prompt, prompt_plus_new_tokens\n        )\n        self.assertEqual(discrep_length, 0)\n        np.testing.assert_array_equal(new_tokens_only, np.array([[]]))\n        np.testing.assert_array_equal(discrep_only, np.array([[]]))\n\n\nclass TestAssistedCandidateGeneratorUpdateStrategy(unittest.TestCase):\n    def setUp(self):\n        checkpoint = \"EleutherAI/pythia-160m-deduped\"\n        self.assistant_model = AutoModelForCausalLM.from_pretrained(checkpoint)\n        self.assistant_model.generation_config.assistant_confidence_threshold = 0.4\n        self.model_kwargs = {}\n        self.input_ids = torch.randint(1, 10, (1, 9))\n        self.candidate_generator = AssistedCandidateGenerator(\n            input_ids=self.input_ids,\n            assistant_model=self.assistant_model,\n            generation_config=self.assistant_model.generation_config,\n            model_kwargs=self.model_kwargs,\n        )\n        self.candidate_generator.probs = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n        self.original_probs = self.candidate_generator.probs\n        self.original_threshold = self.assistant_model.generation_config.assistant_confidence_threshold\n\n    def assert_no_sklearn(self):\n        with patch(\"transformers.utils.import_utils._sklearn_available\", False):\n            self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n            self.assertEqual(self.candidate_generator.matches, self.original_matches)\n            self.assertEqual(self.candidate_generator.probs, self.original_probs)\n            self.assertEqual(\n                self.assistant_model.generation_config.assistant_confidence_threshold, self.original_threshold\n            )\n\n    @parameterized.expand([(is_sklearn_available(),), (False,)])\n    def test_update_candidate_strategy_no_matches_short(self, sklearn_available):\n        print(\"test_update_candidate_strategy_no_matches_short\")\n        self.original_matches = []\n        self.candidate_generator.matches = self.original_matches\n        self.num_matches = 0\n\n        if sklearn_available:\n            self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n            self.assertEqual(self.candidate_generator.matches, [0])\n            self.assertEqual(self.candidate_generator.probs, [0.9])\n            self.assertEqual(self.assistant_model.generation_config.assistant_confidence_threshold, 0.4)\n        else:\n            self.assert_no_sklearn()\n\n    @parameterized.expand([(is_sklearn_available(),), (False,)])\n    def test_update_candidate_strategy_with_mix_matches_3(self, sklearn_available):\n        self.original_matches = [1, 0, 1, 0, 1]\n        self.candidate_generator.matches = self.original_matches\n        self.num_matches = 3\n        if sklearn_available:\n            self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n            self.assertEqual(self.candidate_generator.matches, [1, 0, 1, 0, 1, 1, 1, 1, 0])\n            self.assertEqual(self.candidate_generator.probs, [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])\n            self.assertEqual(self.assistant_model.generation_config.assistant_confidence_threshold, 0.2)\n        else:\n            self.assert_no_sklearn()\n\n    @parameterized.expand([(is_sklearn_available(),), (False,)])\n    def test_update_candidate_strategy_with_matches_4(self, sklearn_available):\n        self.original_matches = [1, 1, 1, 1, 1]\n        self.candidate_generator.matches = self.original_matches\n        self.num_matches = 4\n        if sklearn_available:\n            self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n            self.assertEqual(self.candidate_generator.matches, [1, 1, 1, 1, 1, 1, 1, 1, 1])\n            self.assertEqual(self.candidate_generator.probs, [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])\n            self.assertEqual(self.assistant_model.generation_config.assistant_confidence_threshold, 0.4)\n        else:\n            self.assert_no_sklearn()\n\n    @parameterized.expand([(is_sklearn_available(),), (False,)])\n    def test_update_candidate_strategy_with_matches_3(self, sklearn_available):\n        self.original_matches = [1, 1, 1, 1, 1]\n        self.candidate_generator.matches = self.original_matches\n        self.num_matches = 3\n        if sklearn_available:\n            self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n            self.assertEqual(self.candidate_generator.matches, [1, 1, 1, 1, 1, 1, 1, 1, 0])\n            self.assertEqual(self.candidate_generator.probs, [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])\n            self.assertEqual(self.assistant_model.generation_config.assistant_confidence_threshold, 0.2)\n        else:\n            self.assert_no_sklearn()\n\n    @parameterized.expand([(is_sklearn_available(),), (False,)])\n    def test_update_candidate_strategy_with_matches_2(self, sklearn_available):\n        self.original_matches = [1, 1, 1, 1, 1]\n        self.candidate_generator.matches = self.original_matches\n        self.num_matches = 2\n        if sklearn_available:\n            self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n            self.assertEqual(self.candidate_generator.matches, [1, 1, 1, 1, 1, 1, 1, 0])\n            self.assertEqual(self.candidate_generator.probs, [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2])\n            self.assertEqual(self.assistant_model.generation_config.assistant_confidence_threshold, 0.3)\n        else:\n            self.assert_no_sklearn()\n\n    @parameterized.expand([(is_sklearn_available(),), (False,)])\n    def test_update_candidate_strategy_with_matches_1(self, sklearn_available):\n        self.original_matches = [1, 1, 1, 1, 1]\n        self.candidate_generator.matches = self.original_matches\n        self.num_matches = 1\n        if sklearn_available:\n            self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n            self.assertEqual(self.candidate_generator.matches, [1, 1, 1, 1, 1, 1, 0])\n            self.assertEqual(self.candidate_generator.probs, [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3])\n            self.assertEqual(self.assistant_model.generation_config.assistant_confidence_threshold, 0.4)\n        else:\n            self.assert_no_sklearn()\n"}
{"type": "test_file", "path": "transformers/examples/pytorch/test_pytorch_examples.py", "content": "# coding=utf-8\n# Copyright 2018 HuggingFace Inc..\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport json\nimport logging\nimport os\nimport sys\nfrom unittest.mock import patch\n\nfrom transformers import ViTMAEForPreTraining, Wav2Vec2ForPreTraining\nfrom transformers.testing_utils import (\n    CaptureLogger,\n    TestCasePlus,\n    backend_device_count,\n    is_torch_fp16_available_on_device,\n    slow,\n    torch_device,\n)\n\n\nSRC_DIRS = [\n    os.path.join(os.path.dirname(__file__), dirname)\n    for dirname in [\n        \"text-generation\",\n        \"text-classification\",\n        \"token-classification\",\n        \"language-modeling\",\n        \"multiple-choice\",\n        \"question-answering\",\n        \"summarization\",\n        \"translation\",\n        \"image-classification\",\n        \"speech-recognition\",\n        \"audio-classification\",\n        \"speech-pretraining\",\n        \"image-pretraining\",\n        \"semantic-segmentation\",\n        \"object-detection\",\n        \"instance-segmentation\",\n    ]\n]\nsys.path.extend(SRC_DIRS)\n\n\nif SRC_DIRS is not None:\n    import run_audio_classification\n    import run_clm\n    import run_generation\n    import run_glue\n    import run_image_classification\n    import run_instance_segmentation\n    import run_mae\n    import run_mlm\n    import run_ner\n    import run_object_detection\n    import run_qa as run_squad\n    import run_semantic_segmentation\n    import run_seq2seq_qa as run_squad_seq2seq\n    import run_speech_recognition_ctc\n    import run_speech_recognition_ctc_adapter\n    import run_speech_recognition_seq2seq\n    import run_summarization\n    import run_swag\n    import run_translation\n    import run_wav2vec2_pretraining_no_trainer\n\n\nlogging.basicConfig(level=logging.DEBUG)\n\nlogger = logging.getLogger()\n\n\ndef get_results(output_dir):\n    results = {}\n    path = os.path.join(output_dir, \"all_results.json\")\n    if os.path.exists(path):\n        with open(path, \"r\") as f:\n            results = json.load(f)\n    else:\n        raise ValueError(f\"can't find {path}\")\n    return results\n\n\nstream_handler = logging.StreamHandler(sys.stdout)\nlogger.addHandler(stream_handler)\n\n\nclass ExamplesTests(TestCasePlus):\n    def test_run_glue(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_glue.py\n            --model_name_or_path distilbert/distilbert-base-uncased\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --train_file ./tests/fixtures/tests_samples/MRPC/train.csv\n            --validation_file ./tests/fixtures/tests_samples/MRPC/dev.csv\n            --do_train\n            --do_eval\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --learning_rate=1e-4\n            --max_steps=10\n            --warmup_steps=2\n            --seed=42\n            --max_seq_length=128\n            \"\"\".split()\n\n        if is_torch_fp16_available_on_device(torch_device):\n            testargs.append(\"--fp16\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_glue.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.75)\n\n    def test_run_clm(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_clm.py\n            --model_name_or_path distilbert/distilgpt2\n            --train_file ./tests/fixtures/sample_text.txt\n            --validation_file ./tests/fixtures/sample_text.txt\n            --do_train\n            --do_eval\n            --block_size 128\n            --per_device_train_batch_size 5\n            --per_device_eval_batch_size 5\n            --num_train_epochs 2\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            \"\"\".split()\n\n        if backend_device_count(torch_device) > 1:\n            # Skipping because there are not enough batches to train the model + would need a drop_last to work.\n            return\n\n        if torch_device == \"cpu\":\n            testargs.append(\"--use_cpu\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_clm.main()\n            result = get_results(tmp_dir)\n            self.assertLess(result[\"perplexity\"], 100)\n\n    def test_run_clm_config_overrides(self):\n        # test that config_overrides works, despite the misleading dumps of default un-updated\n        # config via tokenizer\n\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_clm.py\n            --model_type gpt2\n            --tokenizer_name openai-community/gpt2\n            --train_file ./tests/fixtures/sample_text.txt\n            --output_dir {tmp_dir}\n            --config_overrides n_embd=10,n_head=2\n            \"\"\".split()\n\n        if torch_device == \"cpu\":\n            testargs.append(\"--use_cpu\")\n\n        logger = run_clm.logger\n        with patch.object(sys, \"argv\", testargs):\n            with CaptureLogger(logger) as cl:\n                run_clm.main()\n\n        self.assertIn('\"n_embd\": 10', cl.out)\n        self.assertIn('\"n_head\": 2', cl.out)\n\n    def test_run_mlm(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_mlm.py\n            --model_name_or_path distilbert/distilroberta-base\n            --train_file ./tests/fixtures/sample_text.txt\n            --validation_file ./tests/fixtures/sample_text.txt\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --do_train\n            --do_eval\n            --prediction_loss_only\n            --num_train_epochs=1\n        \"\"\".split()\n\n        if torch_device == \"cpu\":\n            testargs.append(\"--use_cpu\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_mlm.main()\n            result = get_results(tmp_dir)\n            self.assertLess(result[\"perplexity\"], 42)\n\n    def test_run_ner(self):\n        # with so little data distributed training needs more epochs to get the score on par with 0/1 gpu\n        epochs = 7 if backend_device_count(torch_device) > 1 else 2\n\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_ner.py\n            --model_name_or_path google-bert/bert-base-uncased\n            --train_file tests/fixtures/tests_samples/conll/sample.json\n            --validation_file tests/fixtures/tests_samples/conll/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --do_train\n            --do_eval\n            --warmup_steps=2\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=2\n            --num_train_epochs={epochs}\n            --seed 7\n        \"\"\".split()\n\n        if torch_device == \"cpu\":\n            testargs.append(\"--use_cpu\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_ner.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.75)\n            self.assertLess(result[\"eval_loss\"], 0.5)\n\n    def test_run_squad(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_qa.py\n            --model_name_or_path google-bert/bert-base-uncased\n            --version_2_with_negative\n            --train_file tests/fixtures/tests_samples/SQUAD/sample.json\n            --validation_file tests/fixtures/tests_samples/SQUAD/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --max_steps=10\n            --warmup_steps=2\n            --do_train\n            --do_eval\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_squad.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_f1\"], 30)\n            self.assertGreaterEqual(result[\"eval_exact\"], 30)\n\n    def test_run_squad_seq2seq(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_seq2seq_qa.py\n            --model_name_or_path google-t5/t5-small\n            --context_column context\n            --question_column question\n            --answer_column answers\n            --version_2_with_negative\n            --train_file tests/fixtures/tests_samples/SQUAD/sample.json\n            --validation_file tests/fixtures/tests_samples/SQUAD/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --max_steps=10\n            --warmup_steps=2\n            --do_train\n            --do_eval\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --predict_with_generate\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_squad_seq2seq.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_f1\"], 30)\n            self.assertGreaterEqual(result[\"eval_exact\"], 30)\n\n    def test_run_swag(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_swag.py\n            --model_name_or_path google-bert/bert-base-uncased\n            --train_file tests/fixtures/tests_samples/swag/sample.json\n            --validation_file tests/fixtures/tests_samples/swag/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --max_steps=20\n            --warmup_steps=2\n            --do_train\n            --do_eval\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_swag.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.8)\n\n    def test_generation(self):\n        testargs = [\"run_generation.py\", \"--prompt=Hello\", \"--length=10\", \"--seed=42\"]\n\n        if is_torch_fp16_available_on_device(torch_device):\n            testargs.append(\"--fp16\")\n\n        model_type, model_name = (\n            \"--model_type=gpt2\",\n            \"--model_name_or_path=sshleifer/tiny-gpt2\",\n        )\n        with patch.object(sys, \"argv\", testargs + [model_type, model_name]):\n            result = run_generation.main()\n            self.assertGreaterEqual(len(result[0]), 10)\n\n    @slow\n    def test_run_summarization(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_summarization.py\n            --model_name_or_path google-t5/t5-small\n            --train_file tests/fixtures/tests_samples/xsum/sample.json\n            --validation_file tests/fixtures/tests_samples/xsum/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --max_steps=50\n            --warmup_steps=8\n            --do_train\n            --do_eval\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --predict_with_generate\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_summarization.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_rouge1\"], 10)\n            self.assertGreaterEqual(result[\"eval_rouge2\"], 2)\n            self.assertGreaterEqual(result[\"eval_rougeL\"], 7)\n            self.assertGreaterEqual(result[\"eval_rougeLsum\"], 7)\n\n    @slow\n    def test_run_translation(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_translation.py\n            --model_name_or_path sshleifer/student_marian_en_ro_6_1\n            --source_lang en\n            --target_lang ro\n            --train_file tests/fixtures/tests_samples/wmt16/sample.json\n            --validation_file tests/fixtures/tests_samples/wmt16/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --max_steps=50\n            --warmup_steps=8\n            --do_train\n            --do_eval\n            --learning_rate=3e-3\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --predict_with_generate\n            --source_lang en_XX\n            --target_lang ro_RO\n            --max_source_length 512\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_translation.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_bleu\"], 30)\n\n    def test_run_image_classification(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_image_classification.py\n            --output_dir {tmp_dir}\n            --model_name_or_path google/vit-base-patch16-224-in21k\n            --dataset_name hf-internal-testing/cats_vs_dogs_sample\n            --trust_remote_code\n            --do_train\n            --do_eval\n            --learning_rate 1e-4\n            --per_device_train_batch_size 2\n            --per_device_eval_batch_size 1\n            --remove_unused_columns False\n            --overwrite_output_dir True\n            --dataloader_num_workers 16\n            --metric_for_best_model accuracy\n            --max_steps 10\n            --train_val_split 0.1\n            --seed 42\n            --label_column_name labels\n        \"\"\".split()\n\n        if is_torch_fp16_available_on_device(torch_device):\n            testargs.append(\"--fp16\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_image_classification.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.8)\n\n    def test_run_speech_recognition_ctc(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_speech_recognition_ctc.py\n            --output_dir {tmp_dir}\n            --model_name_or_path hf-internal-testing/tiny-random-wav2vec2\n            --dataset_name hf-internal-testing/librispeech_asr_dummy\n            --dataset_config_name clean\n            --train_split_name validation\n            --eval_split_name validation\n            --trust_remote_code\n            --do_train\n            --do_eval\n            --learning_rate 1e-4\n            --per_device_train_batch_size 2\n            --per_device_eval_batch_size 1\n            --remove_unused_columns False\n            --overwrite_output_dir True\n            --preprocessing_num_workers 16\n            --max_steps 10\n            --seed 42\n        \"\"\".split()\n\n        if is_torch_fp16_available_on_device(torch_device):\n            testargs.append(\"--fp16\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_speech_recognition_ctc.main()\n            result = get_results(tmp_dir)\n            self.assertLess(result[\"eval_loss\"], result[\"train_loss\"])\n\n    def test_run_speech_recognition_ctc_adapter(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_speech_recognition_ctc_adapter.py\n            --output_dir {tmp_dir}\n            --model_name_or_path hf-internal-testing/tiny-random-wav2vec2\n            --dataset_name hf-internal-testing/librispeech_asr_dummy\n            --dataset_config_name clean\n            --train_split_name validation\n            --eval_split_name validation\n            --trust_remote_code\n            --do_train\n            --do_eval\n            --learning_rate 1e-4\n            --per_device_train_batch_size 2\n            --per_device_eval_batch_size 1\n            --remove_unused_columns False\n            --overwrite_output_dir True\n            --preprocessing_num_workers 16\n            --max_steps 10\n            --target_language tur\n            --seed 42\n        \"\"\".split()\n\n        if is_torch_fp16_available_on_device(torch_device):\n            testargs.append(\"--fp16\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_speech_recognition_ctc_adapter.main()\n            result = get_results(tmp_dir)\n            self.assertTrue(os.path.isfile(os.path.join(tmp_dir, \"./adapter.tur.safetensors\")))\n            self.assertLess(result[\"eval_loss\"], result[\"train_loss\"])\n\n    def test_run_speech_recognition_seq2seq(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_speech_recognition_seq2seq.py\n            --output_dir {tmp_dir}\n            --model_name_or_path hf-internal-testing/tiny-random-speech-encoder-decoder\n            --dataset_name hf-internal-testing/librispeech_asr_dummy\n            --dataset_config_name clean\n            --train_split_name validation\n            --eval_split_name validation\n            --trust_remote_code\n            --do_train\n            --do_eval\n            --learning_rate 1e-4\n            --per_device_train_batch_size 2\n            --per_device_eval_batch_size 4\n            --remove_unused_columns False\n            --overwrite_output_dir True\n            --preprocessing_num_workers 16\n            --max_steps 10\n            --seed 42\n        \"\"\".split()\n\n        if is_torch_fp16_available_on_device(torch_device):\n            testargs.append(\"--fp16\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_speech_recognition_seq2seq.main()\n            result = get_results(tmp_dir)\n            self.assertLess(result[\"eval_loss\"], result[\"train_loss\"])\n\n    def test_run_audio_classification(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_audio_classification.py\n            --output_dir {tmp_dir}\n            --model_name_or_path hf-internal-testing/tiny-random-wav2vec2\n            --dataset_name anton-l/superb_demo\n            --trust_remote_code\n            --dataset_config_name ks\n            --train_split_name test\n            --eval_split_name test\n            --audio_column_name audio\n            --label_column_name label\n            --do_train\n            --do_eval\n            --learning_rate 1e-4\n            --per_device_train_batch_size 2\n            --per_device_eval_batch_size 1\n            --remove_unused_columns False\n            --overwrite_output_dir True\n            --num_train_epochs 10\n            --max_steps 50\n            --seed 42\n        \"\"\".split()\n\n        if is_torch_fp16_available_on_device(torch_device):\n            testargs.append(\"--fp16\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_audio_classification.main()\n            result = get_results(tmp_dir)\n            self.assertLess(result[\"eval_loss\"], result[\"train_loss\"])\n\n    def test_run_wav2vec2_pretraining(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_wav2vec2_pretraining_no_trainer.py\n            --output_dir {tmp_dir}\n            --model_name_or_path hf-internal-testing/tiny-random-wav2vec2\n            --dataset_name hf-internal-testing/librispeech_asr_dummy\n            --dataset_config_names clean\n            --dataset_split_names validation\n            --trust_remote_code\n            --learning_rate 1e-4\n            --per_device_train_batch_size 4\n            --per_device_eval_batch_size 4\n            --preprocessing_num_workers 16\n            --max_train_steps 2\n            --validation_split_percentage 5\n            --seed 42\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_wav2vec2_pretraining_no_trainer.main()\n            model = Wav2Vec2ForPreTraining.from_pretrained(tmp_dir)\n            self.assertIsNotNone(model)\n\n    def test_run_vit_mae_pretraining(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_mae.py\n            --output_dir {tmp_dir}\n            --dataset_name hf-internal-testing/cats_vs_dogs_sample\n            --trust_remote_code\n            --do_train\n            --do_eval\n            --learning_rate 1e-4\n            --per_device_train_batch_size 2\n            --per_device_eval_batch_size 1\n            --remove_unused_columns False\n            --overwrite_output_dir True\n            --dataloader_num_workers 16\n            --metric_for_best_model accuracy\n            --max_steps 10\n            --train_val_split 0.1\n            --seed 42\n        \"\"\".split()\n\n        if is_torch_fp16_available_on_device(torch_device):\n            testargs.append(\"--fp16\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_mae.main()\n            model = ViTMAEForPreTraining.from_pretrained(tmp_dir)\n            self.assertIsNotNone(model)\n\n    def test_run_semantic_segmentation(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_semantic_segmentation.py\n            --output_dir {tmp_dir}\n            --dataset_name huggingface/semantic-segmentation-test-sample\n            --do_train\n            --do_eval\n            --remove_unused_columns False\n            --overwrite_output_dir True\n            --max_steps 10\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --seed 32\n        \"\"\".split()\n\n        if is_torch_fp16_available_on_device(torch_device):\n            testargs.append(\"--fp16\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_semantic_segmentation.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_overall_accuracy\"], 0.1)\n\n    @patch.dict(os.environ, {\"WANDB_DISABLED\": \"true\"})\n    def test_run_object_detection(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_object_detection.py\n            --model_name_or_path qubvel-hf/detr-resnet-50-finetuned-10k-cppe5\n            --output_dir {tmp_dir}\n            --dataset_name qubvel-hf/cppe-5-sample\n            --do_train\n            --do_eval\n            --remove_unused_columns False\n            --overwrite_output_dir True\n            --eval_do_concat_batches False\n            --max_steps 10\n            --learning_rate=1e-6\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --seed 32\n        \"\"\".split()\n\n        if is_torch_fp16_available_on_device(torch_device):\n            testargs.append(\"--fp16\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_object_detection.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"test_map\"], 0.1)\n\n    @patch.dict(os.environ, {\"WANDB_DISABLED\": \"true\"})\n    def test_run_instance_segmentation(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_instance_segmentation.py\n            --model_name_or_path qubvel-hf/finetune-instance-segmentation-ade20k-mini-mask2former\n            --output_dir {tmp_dir}\n            --dataset_name qubvel-hf/ade20k-nano\n            --do_reduce_labels\n            --image_height 256\n            --image_width 256\n            --do_train\n            --num_train_epochs 1\n            --learning_rate 1e-5\n            --lr_scheduler_type constant\n            --per_device_train_batch_size 2\n            --per_device_eval_batch_size 1\n            --do_eval\n            --evaluation_strategy epoch\n            --seed 32\n        \"\"\".split()\n\n        if is_torch_fp16_available_on_device(torch_device):\n            testargs.append(\"--fp16\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_instance_segmentation.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"test_map\"], 0.1)\n"}
{"type": "test_file", "path": "transformers/tests/agents/test_agent_types.py", "content": "# coding=utf-8\n# Copyright 2023 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport tempfile\nimport unittest\nimport uuid\nfrom pathlib import Path\n\nfrom transformers.agents.agent_types import AgentAudio, AgentImage, AgentText\nfrom transformers.testing_utils import get_tests_dir, require_soundfile, require_torch, require_vision\nfrom transformers.utils import is_soundfile_available, is_torch_available, is_vision_available\n\n\nif is_torch_available():\n    import torch\n\nif is_soundfile_available():\n    import soundfile as sf\n\nif is_vision_available():\n    from PIL import Image\n\n\ndef get_new_path(suffix=\"\") -> str:\n    directory = tempfile.mkdtemp()\n    return os.path.join(directory, str(uuid.uuid4()) + suffix)\n\n\n@require_soundfile\n@require_torch\nclass AgentAudioTests(unittest.TestCase):\n    def test_from_tensor(self):\n        tensor = torch.rand(12, dtype=torch.float64) - 0.5\n        agent_type = AgentAudio(tensor)\n        path = str(agent_type.to_string())\n\n        # Ensure that the tensor and the agent_type's tensor are the same\n        torch.testing.assert_close(tensor, agent_type.to_raw(), rtol=1e-4, atol=1e-4)\n\n        del agent_type\n\n        # Ensure the path remains even after the object deletion\n        self.assertTrue(os.path.exists(path))\n\n        # Ensure that the file contains the same value as the original tensor\n        new_tensor, _ = sf.read(path)\n        torch.testing.assert_close(tensor, torch.tensor(new_tensor), rtol=1e-4, atol=1e-4)\n\n    def test_from_string(self):\n        tensor = torch.rand(12, dtype=torch.float64) - 0.5\n        path = get_new_path(suffix=\".wav\")\n        sf.write(path, tensor, 16000)\n\n        agent_type = AgentAudio(path)\n\n        torch.testing.assert_close(tensor, agent_type.to_raw(), rtol=1e-4, atol=1e-4)\n        self.assertEqual(agent_type.to_string(), path)\n\n\n@require_vision\n@require_torch\nclass AgentImageTests(unittest.TestCase):\n    def test_from_tensor(self):\n        tensor = torch.randint(0, 256, (64, 64, 3))\n        agent_type = AgentImage(tensor)\n        path = str(agent_type.to_string())\n\n        # Ensure that the tensor and the agent_type's tensor are the same\n        torch.testing.assert_close(tensor, agent_type._tensor, rtol=1e-4, atol=1e-4)\n\n        self.assertIsInstance(agent_type.to_raw(), Image.Image)\n\n        # Ensure the path remains even after the object deletion\n        del agent_type\n        self.assertTrue(os.path.exists(path))\n\n    def test_from_string(self):\n        path = Path(get_tests_dir(\"fixtures/tests_samples/COCO\")) / \"000000039769.png\"\n        image = Image.open(path)\n        agent_type = AgentImage(path)\n\n        self.assertTrue(path.samefile(agent_type.to_string()))\n        self.assertTrue(image == agent_type.to_raw())\n\n        # Ensure the path remains even after the object deletion\n        del agent_type\n        self.assertTrue(os.path.exists(path))\n\n    def test_from_image(self):\n        path = Path(get_tests_dir(\"fixtures/tests_samples/COCO\")) / \"000000039769.png\"\n        image = Image.open(path)\n        agent_type = AgentImage(image)\n\n        self.assertFalse(path.samefile(agent_type.to_string()))\n        self.assertTrue(image == agent_type.to_raw())\n\n        # Ensure the path remains even after the object deletion\n        del agent_type\n        self.assertTrue(os.path.exists(path))\n\n\nclass AgentTextTests(unittest.TestCase):\n    def test_from_string(self):\n        string = \"Hey!\"\n        agent_type = AgentText(string)\n\n        self.assertEqual(string, agent_type.to_string())\n        self.assertEqual(string, agent_type.to_raw())\n        self.assertEqual(string, agent_type)\n"}
{"type": "test_file", "path": "transformers/tests/agents/__init__.py", "content": ""}
{"type": "test_file", "path": "transformers/examples/pytorch/test_accelerate_examples.py", "content": "# coding=utf-8\n# Copyright 2018 HuggingFace Inc..\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport argparse\nimport json\nimport logging\nimport os\nimport shutil\nimport sys\nimport tempfile\nimport unittest\nfrom unittest import mock\n\nfrom accelerate.utils import write_basic_config\n\nfrom transformers.testing_utils import (\n    TestCasePlus,\n    backend_device_count,\n    run_command,\n    slow,\n    torch_device,\n)\n\n\nlogging.basicConfig(level=logging.DEBUG)\n\nlogger = logging.getLogger()\n\n\ndef get_setup_file():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-f\")\n    args = parser.parse_args()\n    return args.f\n\n\ndef get_results(output_dir):\n    results = {}\n    path = os.path.join(output_dir, \"all_results.json\")\n    if os.path.exists(path):\n        with open(path, \"r\") as f:\n            results = json.load(f)\n    else:\n        raise ValueError(f\"can't find {path}\")\n    return results\n\n\nstream_handler = logging.StreamHandler(sys.stdout)\nlogger.addHandler(stream_handler)\n\n\nclass ExamplesTestsNoTrainer(TestCasePlus):\n    @classmethod\n    def setUpClass(cls):\n        # Write Accelerate config, will pick up on CPU, GPU, and multi-GPU\n        cls.tmpdir = tempfile.mkdtemp()\n        cls.configPath = os.path.join(cls.tmpdir, \"default_config.yml\")\n        write_basic_config(save_location=cls.configPath)\n        cls._launch_args = [\"accelerate\", \"launch\", \"--config_file\", cls.configPath]\n\n    @classmethod\n    def tearDownClass(cls):\n        shutil.rmtree(cls.tmpdir)\n\n    @mock.patch.dict(os.environ, {\"WANDB_MODE\": \"offline\", \"DVCLIVE_TEST\": \"true\"})\n    def test_run_glue_no_trainer(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            {self.examples_dir}/pytorch/text-classification/run_glue_no_trainer.py\n            --model_name_or_path distilbert/distilbert-base-uncased\n            --output_dir {tmp_dir}\n            --train_file ./tests/fixtures/tests_samples/MRPC/train.csv\n            --validation_file ./tests/fixtures/tests_samples/MRPC/dev.csv\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --learning_rate=1e-4\n            --seed=42\n            --num_warmup_steps=2\n            --checkpointing_steps epoch\n            --with_tracking\n        \"\"\".split()\n\n        run_command(self._launch_args + testargs)\n        result = get_results(tmp_dir)\n        self.assertGreaterEqual(result[\"eval_accuracy\"], 0.75)\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"epoch_0\")))\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"glue_no_trainer\")))\n\n    @unittest.skip(\"Zach is working on this.\")\n    @mock.patch.dict(os.environ, {\"WANDB_MODE\": \"offline\", \"DVCLIVE_TEST\": \"true\"})\n    def test_run_clm_no_trainer(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            {self.examples_dir}/pytorch/language-modeling/run_clm_no_trainer.py\n            --model_name_or_path distilbert/distilgpt2\n            --train_file ./tests/fixtures/sample_text.txt\n            --validation_file ./tests/fixtures/sample_text.txt\n            --block_size 128\n            --per_device_train_batch_size 5\n            --per_device_eval_batch_size 5\n            --num_train_epochs 2\n            --output_dir {tmp_dir}\n            --checkpointing_steps epoch\n            --with_tracking\n        \"\"\".split()\n\n        if backend_device_count(torch_device) > 1:\n            # Skipping because there are not enough batches to train the model + would need a drop_last to work.\n            return\n\n        run_command(self._launch_args + testargs)\n        result = get_results(tmp_dir)\n        self.assertLess(result[\"perplexity\"], 100)\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"epoch_0\")))\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"clm_no_trainer\")))\n\n    @unittest.skip(\"Zach is working on this.\")\n    @mock.patch.dict(os.environ, {\"WANDB_MODE\": \"offline\", \"DVCLIVE_TEST\": \"true\"})\n    def test_run_mlm_no_trainer(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            {self.examples_dir}/pytorch/language-modeling/run_mlm_no_trainer.py\n            --model_name_or_path distilbert/distilroberta-base\n            --train_file ./tests/fixtures/sample_text.txt\n            --validation_file ./tests/fixtures/sample_text.txt\n            --output_dir {tmp_dir}\n            --num_train_epochs=1\n            --checkpointing_steps epoch\n            --with_tracking\n        \"\"\".split()\n\n        run_command(self._launch_args + testargs)\n        result = get_results(tmp_dir)\n        self.assertLess(result[\"perplexity\"], 42)\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"epoch_0\")))\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"mlm_no_trainer\")))\n\n    @mock.patch.dict(os.environ, {\"WANDB_MODE\": \"offline\", \"DVCLIVE_TEST\": \"true\"})\n    def test_run_ner_no_trainer(self):\n        # with so little data distributed training needs more epochs to get the score on par with 0/1 gpu\n        epochs = 7 if backend_device_count(torch_device) > 1 else 2\n\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            {self.examples_dir}/pytorch/token-classification/run_ner_no_trainer.py\n            --model_name_or_path google-bert/bert-base-uncased\n            --train_file tests/fixtures/tests_samples/conll/sample.json\n            --validation_file tests/fixtures/tests_samples/conll/sample.json\n            --output_dir {tmp_dir}\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=2\n            --num_train_epochs={epochs}\n            --seed 7\n            --checkpointing_steps epoch\n            --with_tracking\n        \"\"\".split()\n\n        run_command(self._launch_args + testargs)\n        result = get_results(tmp_dir)\n        self.assertGreaterEqual(result[\"eval_accuracy\"], 0.75)\n        self.assertLess(result[\"train_loss\"], 0.6)\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"epoch_0\")))\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"ner_no_trainer\")))\n\n    @mock.patch.dict(os.environ, {\"WANDB_MODE\": \"offline\", \"DVCLIVE_TEST\": \"true\"})\n    def test_run_squad_no_trainer(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            {self.examples_dir}/pytorch/question-answering/run_qa_no_trainer.py\n            --model_name_or_path google-bert/bert-base-uncased\n            --version_2_with_negative\n            --train_file tests/fixtures/tests_samples/SQUAD/sample.json\n            --validation_file tests/fixtures/tests_samples/SQUAD/sample.json\n            --output_dir {tmp_dir}\n            --seed=42\n            --max_train_steps=10\n            --num_warmup_steps=2\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --checkpointing_steps epoch\n            --with_tracking\n        \"\"\".split()\n\n        run_command(self._launch_args + testargs)\n        result = get_results(tmp_dir)\n        # Because we use --version_2_with_negative the testing script uses SQuAD v2 metrics.\n        self.assertGreaterEqual(result[\"eval_f1\"], 28)\n        self.assertGreaterEqual(result[\"eval_exact\"], 28)\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"epoch_0\")))\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"qa_no_trainer\")))\n\n    @mock.patch.dict(os.environ, {\"WANDB_MODE\": \"offline\", \"DVCLIVE_TEST\": \"true\"})\n    def test_run_swag_no_trainer(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            {self.examples_dir}/pytorch/multiple-choice/run_swag_no_trainer.py\n            --model_name_or_path google-bert/bert-base-uncased\n            --train_file tests/fixtures/tests_samples/swag/sample.json\n            --validation_file tests/fixtures/tests_samples/swag/sample.json\n            --output_dir {tmp_dir}\n            --max_train_steps=20\n            --num_warmup_steps=2\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --with_tracking\n        \"\"\".split()\n\n        run_command(self._launch_args + testargs)\n        result = get_results(tmp_dir)\n        self.assertGreaterEqual(result[\"eval_accuracy\"], 0.8)\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"swag_no_trainer\")))\n\n    @slow\n    @mock.patch.dict(os.environ, {\"WANDB_MODE\": \"offline\", \"DVCLIVE_TEST\": \"true\"})\n    def test_run_summarization_no_trainer(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            {self.examples_dir}/pytorch/summarization/run_summarization_no_trainer.py\n            --model_name_or_path google-t5/t5-small\n            --train_file tests/fixtures/tests_samples/xsum/sample.json\n            --validation_file tests/fixtures/tests_samples/xsum/sample.json\n            --output_dir {tmp_dir}\n            --max_train_steps=50\n            --num_warmup_steps=8\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --checkpointing_steps epoch\n            --with_tracking\n        \"\"\".split()\n\n        run_command(self._launch_args + testargs)\n        result = get_results(tmp_dir)\n        self.assertGreaterEqual(result[\"eval_rouge1\"], 10)\n        self.assertGreaterEqual(result[\"eval_rouge2\"], 2)\n        self.assertGreaterEqual(result[\"eval_rougeL\"], 7)\n        self.assertGreaterEqual(result[\"eval_rougeLsum\"], 7)\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"epoch_0\")))\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"summarization_no_trainer\")))\n\n    @slow\n    @mock.patch.dict(os.environ, {\"WANDB_MODE\": \"offline\", \"DVCLIVE_TEST\": \"true\"})\n    def test_run_translation_no_trainer(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            {self.examples_dir}/pytorch/translation/run_translation_no_trainer.py\n            --model_name_or_path sshleifer/student_marian_en_ro_6_1\n            --source_lang en\n            --target_lang ro\n            --train_file tests/fixtures/tests_samples/wmt16/sample.json\n            --validation_file tests/fixtures/tests_samples/wmt16/sample.json\n            --output_dir {tmp_dir}\n            --max_train_steps=50\n            --num_warmup_steps=8\n            --num_beams=6\n            --learning_rate=3e-3\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --source_lang en_XX\n            --target_lang ro_RO\n            --checkpointing_steps epoch\n            --with_tracking\n        \"\"\".split()\n\n        run_command(self._launch_args + testargs)\n        result = get_results(tmp_dir)\n        self.assertGreaterEqual(result[\"eval_bleu\"], 30)\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"epoch_0\")))\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"translation_no_trainer\")))\n\n    @slow\n    def test_run_semantic_segmentation_no_trainer(self):\n        stream_handler = logging.StreamHandler(sys.stdout)\n        logger.addHandler(stream_handler)\n\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            {self.examples_dir}/pytorch/semantic-segmentation/run_semantic_segmentation_no_trainer.py\n            --dataset_name huggingface/semantic-segmentation-test-sample\n            --output_dir {tmp_dir}\n            --max_train_steps=10\n            --num_warmup_steps=2\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --checkpointing_steps epoch\n        \"\"\".split()\n\n        run_command(self._launch_args + testargs)\n        result = get_results(tmp_dir)\n        self.assertGreaterEqual(result[\"eval_overall_accuracy\"], 0.10)\n\n    @mock.patch.dict(os.environ, {\"WANDB_MODE\": \"offline\", \"DVCLIVE_TEST\": \"true\"})\n    def test_run_image_classification_no_trainer(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            {self.examples_dir}/pytorch/image-classification/run_image_classification_no_trainer.py\n            --model_name_or_path google/vit-base-patch16-224-in21k\n            --dataset_name hf-internal-testing/cats_vs_dogs_sample\n            --trust_remote_code\n            --learning_rate 1e-4\n            --per_device_train_batch_size 2\n            --per_device_eval_batch_size 1\n            --max_train_steps 2\n            --train_val_split 0.1\n            --seed 42\n            --output_dir {tmp_dir}\n            --with_tracking\n            --checkpointing_steps 1\n            --label_column_name labels\n        \"\"\".split()\n\n        run_command(self._launch_args + testargs)\n        result = get_results(tmp_dir)\n        # The base model scores a 25%\n        self.assertGreaterEqual(result[\"eval_accuracy\"], 0.4)\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"step_1\")))\n        self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"image_classification_no_trainer\")))\n\n    @slow\n    @mock.patch.dict(os.environ, {\"WANDB_MODE\": \"offline\", \"DVCLIVE_TEST\": \"true\"})\n    def test_run_object_detection_no_trainer(self):\n        stream_handler = logging.StreamHandler(sys.stdout)\n        logger.addHandler(stream_handler)\n\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            {self.examples_dir}/pytorch/object-detection/run_object_detection_no_trainer.py\n            --model_name_or_path qubvel-hf/detr-resnet-50-finetuned-10k-cppe5\n            --dataset_name qubvel-hf/cppe-5-sample\n            --output_dir {tmp_dir}\n            --max_train_steps=10\n            --num_warmup_steps=2\n            --learning_rate=1e-6\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --checkpointing_steps epoch\n        \"\"\".split()\n\n        run_command(self._launch_args + testargs)\n        result = get_results(tmp_dir)\n        self.assertGreaterEqual(result[\"test_map\"], 0.10)\n\n    @slow\n    @mock.patch.dict(os.environ, {\"WANDB_MODE\": \"offline\", \"DVCLIVE_TEST\": \"true\"})\n    def test_run_instance_segmentation_no_trainer(self):\n        stream_handler = logging.StreamHandler(sys.stdout)\n        logger.addHandler(stream_handler)\n\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            {self.examples_dir}/pytorch/instance-segmentation/run_instance_segmentation_no_trainer.py\n            --model_name_or_path qubvel-hf/finetune-instance-segmentation-ade20k-mini-mask2former\n            --output_dir {tmp_dir}\n            --dataset_name qubvel-hf/ade20k-nano\n            --do_reduce_labels\n            --image_height 256\n            --image_width 256\n            --num_train_epochs 1\n            --per_device_train_batch_size 2\n            --per_device_eval_batch_size 1\n            --seed 1234\n        \"\"\".split()\n\n        run_command(self._launch_args + testargs)\n        result = get_results(tmp_dir)\n        self.assertGreaterEqual(result[\"test_map\"], 0.1)\n"}
{"type": "test_file", "path": "transformers/examples/legacy/seq2seq/test_data/fsmt/build-eval-data.py", "content": "#!/usr/bin/env python\n\nimport io\nimport json\nimport subprocess\n\n\npairs = [\n    [\"en\", \"ru\"],\n    [\"ru\", \"en\"],\n    [\"en\", \"de\"],\n    [\"de\", \"en\"],\n]\n\nn_objs = 8\n\n\ndef get_all_data(pairs, n_objs):\n    text = {}\n    for src, tgt in pairs:\n        pair = f\"{src}-{tgt}\"\n        cmd = f\"sacrebleu -t wmt19 -l {pair} --echo src\".split()\n        src_lines = subprocess.run(cmd, stdout=subprocess.PIPE).stdout.decode(\"utf-8\").splitlines()\n        cmd = f\"sacrebleu -t wmt19 -l {pair} --echo ref\".split()\n        tgt_lines = subprocess.run(cmd, stdout=subprocess.PIPE).stdout.decode(\"utf-8\").splitlines()\n        text[pair] = {\"src\": src_lines[:n_objs], \"tgt\": tgt_lines[:n_objs]}\n    return text\n\n\ntext = get_all_data(pairs, n_objs)\nfilename = \"./fsmt_val_data.json\"\nwith io.open(filename, \"w\", encoding=\"utf-8\") as f:\n    bleu_data = json.dump(text, f, indent=2, ensure_ascii=False)\n"}
{"type": "test_file", "path": "transformers/examples/tensorflow/test_tensorflow_examples.py", "content": "# coding=utf-8\n# Copyright 2022 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport argparse\nimport json\nimport logging\nimport os\nimport sys\nfrom unittest import skip\nfrom unittest.mock import patch\n\nimport tensorflow as tf\nfrom packaging.version import parse\n\n\ntry:\n    import tf_keras as keras\nexcept (ModuleNotFoundError, ImportError):\n    import keras\n\n    if parse(keras.__version__).major > 2:\n        raise ValueError(\n            \"Your currently installed version of Keras is Keras 3, but this is not yet supported in \"\n            \"Transformers. Please install the backwards-compatible tf-keras package with \"\n            \"`pip install tf-keras`.\"\n        )\n\nfrom transformers.testing_utils import TestCasePlus, get_gpu_count, slow\n\n\nSRC_DIRS = [\n    os.path.join(os.path.dirname(__file__), dirname)\n    for dirname in [\n        \"text-generation\",\n        \"text-classification\",\n        \"token-classification\",\n        \"language-modeling\",\n        \"multiple-choice\",\n        \"question-answering\",\n        \"summarization\",\n        \"translation\",\n        \"image-classification\",\n    ]\n]\nsys.path.extend(SRC_DIRS)\n\n\nif SRC_DIRS is not None:\n    import run_clm\n    import run_image_classification\n    import run_mlm\n    import run_ner\n    import run_qa as run_squad\n    import run_summarization\n    import run_swag\n    import run_text_classification\n    import run_translation\n\n\nlogging.basicConfig(level=logging.DEBUG)\n\nlogger = logging.getLogger()\n\n\ndef get_setup_file():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-f\")\n    args = parser.parse_args()\n    return args.f\n\n\ndef get_results(output_dir):\n    results = {}\n    path = os.path.join(output_dir, \"all_results.json\")\n    if os.path.exists(path):\n        with open(path, \"r\") as f:\n            results = json.load(f)\n    else:\n        raise ValueError(f\"can't find {path}\")\n    return results\n\n\ndef is_cuda_available():\n    return bool(tf.config.list_physical_devices(\"GPU\"))\n\n\nstream_handler = logging.StreamHandler(sys.stdout)\nlogger.addHandler(stream_handler)\n\n\nclass ExamplesTests(TestCasePlus):\n    @skip(\"Skipping until shape inference for to_tf_dataset PR is merged.\")\n    def test_run_text_classification(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_text_classification.py\n            --model_name_or_path distilbert/distilbert-base-uncased\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --train_file ./tests/fixtures/tests_samples/MRPC/train.csv\n            --validation_file ./tests/fixtures/tests_samples/MRPC/dev.csv\n            --do_train\n            --do_eval\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --learning_rate=1e-4\n            --max_steps=10\n            --warmup_steps=2\n            --seed=42\n            --max_seq_length=128\n            \"\"\".split()\n\n        if is_cuda_available():\n            testargs.append(\"--fp16\")\n\n        with patch.object(sys, \"argv\", testargs):\n            run_text_classification.main()\n            # Reset the mixed precision policy so we don't break other tests\n            keras.mixed_precision.set_global_policy(\"float32\")\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.75)\n\n    def test_run_clm(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_clm.py\n            --model_name_or_path distilbert/distilgpt2\n            --train_file ./tests/fixtures/sample_text.txt\n            --validation_file ./tests/fixtures/sample_text.txt\n            --do_train\n            --do_eval\n            --block_size 128\n            --per_device_train_batch_size 2\n            --per_device_eval_batch_size 1\n            --num_train_epochs 2\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            \"\"\".split()\n\n        if len(tf.config.list_physical_devices(\"GPU\")) > 1:\n            # Skipping because there are not enough batches to train the model + would need a drop_last to work.\n            return\n\n        with patch.object(sys, \"argv\", testargs):\n            run_clm.main()\n            result = get_results(tmp_dir)\n            self.assertLess(result[\"eval_perplexity\"], 100)\n\n    def test_run_mlm(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_mlm.py\n            --model_name_or_path distilbert/distilroberta-base\n            --train_file ./tests/fixtures/sample_text.txt\n            --validation_file ./tests/fixtures/sample_text.txt\n            --max_seq_length 64\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --do_train\n            --do_eval\n            --prediction_loss_only\n            --num_train_epochs=1\n            --learning_rate=1e-4\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_mlm.main()\n            result = get_results(tmp_dir)\n            self.assertLess(result[\"eval_perplexity\"], 42)\n\n    def test_run_ner(self):\n        # with so little data distributed training needs more epochs to get the score on par with 0/1 gpu\n        epochs = 7 if get_gpu_count() > 1 else 2\n\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_ner.py\n            --model_name_or_path google-bert/bert-base-uncased\n            --train_file tests/fixtures/tests_samples/conll/sample.json\n            --validation_file tests/fixtures/tests_samples/conll/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --do_train\n            --do_eval\n            --warmup_steps=2\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=2\n            --num_train_epochs={epochs}\n            --seed 7\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_ner.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"accuracy\"], 0.75)\n\n    def test_run_squad(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_qa.py\n            --model_name_or_path google-bert/bert-base-uncased\n            --version_2_with_negative\n            --train_file tests/fixtures/tests_samples/SQUAD/sample.json\n            --validation_file tests/fixtures/tests_samples/SQUAD/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --max_steps=10\n            --warmup_steps=2\n            --do_train\n            --do_eval\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_squad.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"f1\"], 30)\n            self.assertGreaterEqual(result[\"exact\"], 30)\n\n    def test_run_swag(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_swag.py\n            --model_name_or_path google-bert/bert-base-uncased\n            --train_file tests/fixtures/tests_samples/swag/sample.json\n            --validation_file tests/fixtures/tests_samples/swag/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --max_steps=20\n            --warmup_steps=2\n            --do_train\n            --do_eval\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_swag.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"val_accuracy\"], 0.8)\n\n    @slow\n    def test_run_summarization(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_summarization.py\n            --model_name_or_path google-t5/t5-small\n            --train_file tests/fixtures/tests_samples/xsum/sample.json\n            --validation_file tests/fixtures/tests_samples/xsum/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --max_steps=50\n            --warmup_steps=8\n            --do_train\n            --do_eval\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_summarization.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"rouge1\"], 10)\n            self.assertGreaterEqual(result[\"rouge2\"], 2)\n            self.assertGreaterEqual(result[\"rougeL\"], 7)\n            self.assertGreaterEqual(result[\"rougeLsum\"], 7)\n\n    @slow\n    def test_run_translation(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_translation.py\n            --model_name_or_path Rocketknight1/student_marian_en_ro_6_1\n            --source_lang en\n            --target_lang ro\n            --train_file tests/fixtures/tests_samples/wmt16/sample.json\n            --validation_file tests/fixtures/tests_samples/wmt16/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --warmup_steps=8\n            --do_train\n            --do_eval\n            --learning_rate=3e-3\n            --num_train_epochs 12\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --source_lang en_XX\n            --target_lang ro_RO\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_translation.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"bleu\"], 30)\n\n    def test_run_image_classification(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_image_classification.py\n            --dataset_name hf-internal-testing/cats_vs_dogs_sample\n            --trust_remote_code\n            --model_name_or_path microsoft/resnet-18\n            --do_train\n            --do_eval\n            --learning_rate 1e-4\n            --per_device_train_batch_size 2\n            --per_device_eval_batch_size 1\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --dataloader_num_workers 16\n            --num_train_epochs 2\n            --train_val_split 0.1\n            --seed 42\n            --ignore_mismatched_sizes True\n            \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_image_classification.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"accuracy\"], 0.7)\n"}
{"type": "test_file", "path": "transformers/tests/agents/test_document_question_answering.py", "content": "# coding=utf-8\n# Copyright 2023 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom datasets import load_dataset\n\nfrom transformers import load_tool\n\nfrom .test_tools_common import ToolTesterMixin\n\n\nclass DocumentQuestionAnsweringToolTester(unittest.TestCase, ToolTesterMixin):\n    def setUp(self):\n        self.tool = load_tool(\"document_question_answering\")\n        self.tool.setup()\n\n    def test_exact_match_arg(self):\n        dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n        document = dataset[0][\"image\"]\n\n        result = self.tool(document, \"When is the coffee break?\")\n        self.assertEqual(result, \"11-14 to 11:39 a.m.\")\n\n    def test_exact_match_kwarg(self):\n        dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n        document = dataset[0][\"image\"]\n\n        self.tool(document=document, question=\"When is the coffee break?\")\n"}
{"type": "test_file", "path": "transformers/templates/adding_a_missing_tokenization_test/cookiecutter-template-{{cookiecutter.modelname}}/test_tokenization_{{cookiecutter.lowercase_modelname}}.py", "content": "# coding=utf-8\n# Copyright 2022 {{cookiecutter.authors}}. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" Testing suite for the {{cookiecutter.modelname}} tokenizer. \"\"\"\n\n\nimport unittest\n\n{% if cookiecutter.has_slow_class == \"True\" and  cookiecutter.has_fast_class == \"True\" -%}\nfrom transformers import {{cookiecutter.camelcase_modelname}}Tokenizer, {{cookiecutter.camelcase_modelname}}TokenizerFast\n{% elif  cookiecutter.has_slow_class == \"True\" -%}\nfrom transformers import {{cookiecutter.camelcase_modelname}}Tokenizer\n{% elif  cookiecutter.has_fast_class == \"True\" -%}\nfrom transformers import {{cookiecutter.camelcase_modelname}}TokenizerFast\n{% endif -%}\n{% if cookiecutter.has_fast_class == \"True\" and  cookiecutter.slow_tokenizer_use_sentencepiece == \"True\" -%}\nfrom transformers.testing_utils import require_sentencepiece, require_tokenizers\nfrom ...test_tokenization_common import TokenizerTesterMixin\n\n\n@require_sentencepiece\n@require_tokenizers\n{% elif  cookiecutter.slow_tokenizer_use_sentencepiece == \"True\" -%}\nfrom transformers.testing_utils import require_sentencepiece\nfrom ...test_tokenization_common import TokenizerTesterMixin\n\n\n@require_sentencepiece\n{% elif  cookiecutter.has_fast_class == \"True\" -%}\nfrom transformers.testing_utils import require_tokenizers\nfrom ...test_tokenization_common import TokenizerTesterMixin\n\n\n@require_tokenizers\n{% else -%}\nfrom ...test_tokenization_common import TokenizerTesterMixin\n\n\n{% endif -%}\nclass {{cookiecutter.camelcase_modelname}}TokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n    {% if cookiecutter.has_slow_class == \"True\" -%}\n    tokenizer_class = {{cookiecutter.camelcase_modelname}}Tokenizer\n    test_slow_tokenizer = True\n    {% else -%}\n    tokenizer_class = None\n    test_slow_tokenizer = False\n    {% endif -%}\n    {% if cookiecutter.has_fast_class == \"True\" -%}\n    rust_tokenizer_class = {{cookiecutter.camelcase_modelname}}TokenizerFast\n    test_rust_tokenizer = True\n    {% else -%}\n    rust_tokenizer_class = None\n    test_rust_tokenizer = False\n    {% endif -%}\n    {% if  cookiecutter.slow_tokenizer_use_sentencepiece == \"True\" -%}\n    test_sentencepiece = True\n    {% endif -%}\n    # TODO: Check in `TokenizerTesterMixin` if other attributes need to be changed\n    def setUp(self):\n        super().setUp()\n\n        raise NotImplementedError(\n            \"Here you have to implement the saving of a toy tokenizer in \"\n            \"`self.tmpdirname`.\"\n        )\n\n    # TODO: add tests with hard-coded target values\n"}
{"type": "test_file", "path": "transformers/tests/__init__.py", "content": ""}
{"type": "test_file", "path": "transformers/tests/agents/test_agents.py", "content": "# coding=utf-8\n# Copyright 2024 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport tempfile\nimport unittest\nimport uuid\n\nimport pytest\n\nfrom transformers.agents.agent_types import AgentText\nfrom transformers.agents.agents import (\n    AgentMaxIterationsError,\n    CodeAgent,\n    ManagedAgent,\n    ReactCodeAgent,\n    ReactJsonAgent,\n    Toolbox,\n)\nfrom transformers.agents.default_tools import PythonInterpreterTool\nfrom transformers.testing_utils import require_torch\n\n\ndef get_new_path(suffix=\"\") -> str:\n    directory = tempfile.mkdtemp()\n    return os.path.join(directory, str(uuid.uuid4()) + suffix)\n\n\ndef fake_react_json_llm(messages, stop_sequences=None, grammar=None) -> str:\n    prompt = str(messages)\n\n    if \"special_marker\" not in prompt:\n        return \"\"\"\nThought: I should multiply 2 by 3.6452. special_marker\nAction:\n{\n    \"action\": \"python_interpreter\",\n    \"action_input\": {\"code\": \"2*3.6452\"}\n}\n\"\"\"\n    else:  # We're at step 2\n        return \"\"\"\nThought: I can now answer the initial question\nAction:\n{\n    \"action\": \"final_answer\",\n    \"action_input\": {\"answer\": \"7.2904\"}\n}\n\"\"\"\n\n\ndef fake_react_code_llm(messages, stop_sequences=None, grammar=None) -> str:\n    prompt = str(messages)\n    if \"special_marker\" not in prompt:\n        return \"\"\"\nThought: I should multiply 2 by 3.6452. special_marker\nCode:\n```py\nresult = 2**3.6452\n```<end_code>\n\"\"\"\n    else:  # We're at step 2\n        return \"\"\"\nThought: I can now answer the initial question\nCode:\n```py\nfinal_answer(7.2904)\n```<end_code>\n\"\"\"\n\n\ndef fake_react_code_llm_error(messages, stop_sequences=None) -> str:\n    prompt = str(messages)\n    if \"special_marker\" not in prompt:\n        return \"\"\"\nThought: I should multiply 2 by 3.6452. special_marker\nCode:\n```py\nprint = 2\n```<end_code>\n\"\"\"\n    else:  # We're at step 2\n        return \"\"\"\nThought: I can now answer the initial question\nCode:\n```py\nfinal_answer(\"got an error\")\n```<end_code>\n\"\"\"\n\n\ndef fake_react_code_functiondef(messages, stop_sequences=None) -> str:\n    prompt = str(messages)\n    if \"special_marker\" not in prompt:\n        return \"\"\"\nThought: Let's define the function. special_marker\nCode:\n```py\nimport numpy as np\n\ndef moving_average(x, w):\n    return np.convolve(x, np.ones(w), 'valid') / w\n```<end_code>\n\"\"\"\n    else:  # We're at step 2\n        return \"\"\"\nThought: I can now answer the initial question\nCode:\n```py\nx, w = [0, 1, 2, 3, 4, 5], 2\nres = moving_average(x, w)\nfinal_answer(res)\n```<end_code>\n\"\"\"\n\n\ndef fake_code_llm_oneshot(messages, stop_sequences=None, grammar=None) -> str:\n    return \"\"\"\nThought: I should multiply 2 by 3.6452. special_marker\nCode:\n```py\nresult = python_interpreter(code=\"2*3.6452\")\nfinal_answer(result)\n```\n\"\"\"\n\n\ndef fake_code_llm_no_return(messages, stop_sequences=None, grammar=None) -> str:\n    return \"\"\"\nThought: I should multiply 2 by 3.6452. special_marker\nCode:\n```py\nresult = python_interpreter(code=\"2*3.6452\")\nprint(result)\n```\n\"\"\"\n\n\nclass AgentTests(unittest.TestCase):\n    def test_fake_code_agent(self):\n        agent = CodeAgent(tools=[PythonInterpreterTool()], llm_engine=fake_code_llm_oneshot)\n        output = agent.run(\"What is 2 multiplied by 3.6452?\")\n        assert isinstance(output, str)\n        assert output == \"7.2904\"\n\n    def test_fake_react_json_agent(self):\n        agent = ReactJsonAgent(tools=[PythonInterpreterTool()], llm_engine=fake_react_json_llm)\n        output = agent.run(\"What is 2 multiplied by 3.6452?\")\n        assert isinstance(output, str)\n        assert output == \"7.2904\"\n        assert agent.logs[0][\"task\"] == \"What is 2 multiplied by 3.6452?\"\n        assert agent.logs[1][\"observation\"] == \"7.2904\"\n        assert agent.logs[1][\"rationale\"].strip() == \"Thought: I should multiply 2 by 3.6452. special_marker\"\n        assert (\n            agent.logs[2][\"llm_output\"]\n            == \"\"\"\nThought: I can now answer the initial question\nAction:\n{\n    \"action\": \"final_answer\",\n    \"action_input\": {\"answer\": \"7.2904\"}\n}\n\"\"\"\n        )\n\n    def test_fake_react_code_agent(self):\n        agent = ReactCodeAgent(tools=[PythonInterpreterTool()], llm_engine=fake_react_code_llm)\n        output = agent.run(\"What is 2 multiplied by 3.6452?\")\n        assert isinstance(output, float)\n        assert output == 7.2904\n        assert agent.logs[0][\"task\"] == \"What is 2 multiplied by 3.6452?\"\n        assert agent.logs[2][\"tool_call\"] == {\n            \"tool_arguments\": \"final_answer(7.2904)\",\n            \"tool_name\": \"code interpreter\",\n        }\n\n    def test_react_code_agent_code_errors_show_offending_lines(self):\n        agent = ReactCodeAgent(tools=[PythonInterpreterTool()], llm_engine=fake_react_code_llm_error)\n        output = agent.run(\"What is 2 multiplied by 3.6452?\")\n        assert isinstance(output, AgentText)\n        assert output == \"got an error\"\n        assert \"Evaluation stopped at line 'print = 2' because of\" in str(agent.logs)\n\n    def test_setup_agent_with_empty_toolbox(self):\n        ReactJsonAgent(llm_engine=fake_react_json_llm, tools=[])\n\n    def test_react_fails_max_iterations(self):\n        agent = ReactCodeAgent(\n            tools=[PythonInterpreterTool()],\n            llm_engine=fake_code_llm_no_return,  # use this callable because it never ends\n            max_iterations=5,\n        )\n        agent.run(\"What is 2 multiplied by 3.6452?\")\n        assert len(agent.logs) == 7\n        assert type(agent.logs[-1][\"error\"]) is AgentMaxIterationsError\n\n    @require_torch\n    def test_init_agent_with_different_toolsets(self):\n        toolset_1 = []\n        agent = ReactCodeAgent(tools=toolset_1, llm_engine=fake_react_code_llm)\n        assert (\n            len(agent.toolbox.tools) == 1\n        )  # when no tools are provided, only the final_answer tool is added by default\n\n        toolset_2 = [PythonInterpreterTool(), PythonInterpreterTool()]\n        agent = ReactCodeAgent(tools=toolset_2, llm_engine=fake_react_code_llm)\n        assert (\n            len(agent.toolbox.tools) == 2\n        )  # deduplication of tools, so only one python_interpreter tool is added in addition to final_answer\n\n        toolset_3 = Toolbox(toolset_2)\n        agent = ReactCodeAgent(tools=toolset_3, llm_engine=fake_react_code_llm)\n        assert (\n            len(agent.toolbox.tools) == 2\n        )  # same as previous one, where toolset_3 is an instantiation of previous one\n\n        # check that add_base_tools will not interfere with existing tools\n        with pytest.raises(KeyError) as e:\n            agent = ReactJsonAgent(tools=toolset_3, llm_engine=fake_react_json_llm, add_base_tools=True)\n        assert \"already exists in the toolbox\" in str(e)\n\n        # check that python_interpreter base tool does not get added to code agents\n        agent = ReactCodeAgent(tools=[], llm_engine=fake_react_code_llm, add_base_tools=True)\n        assert len(agent.toolbox.tools) == 7  # added final_answer tool + 6 base tools (excluding interpreter)\n\n    def test_function_persistence_across_steps(self):\n        agent = ReactCodeAgent(\n            tools=[], llm_engine=fake_react_code_functiondef, max_iterations=2, additional_authorized_imports=[\"numpy\"]\n        )\n        res = agent.run(\"ok\")\n        assert res[0] == 0.5\n\n    def test_init_managed_agent(self):\n        agent = ReactCodeAgent(tools=[], llm_engine=fake_react_code_functiondef)\n        managed_agent = ManagedAgent(agent, name=\"managed_agent\", description=\"Empty\")\n        assert managed_agent.name == \"managed_agent\"\n        assert managed_agent.description == \"Empty\"\n\n    def test_agent_description_gets_correctly_inserted_in_system_prompt(self):\n        agent = ReactCodeAgent(tools=[], llm_engine=fake_react_code_functiondef)\n        managed_agent = ManagedAgent(agent, name=\"managed_agent\", description=\"Empty\")\n        manager_agent = ReactCodeAgent(\n            tools=[], llm_engine=fake_react_code_functiondef, managed_agents=[managed_agent]\n        )\n        assert \"You can also give requests to team members.\" not in agent.system_prompt\n        assert \"<<managed_agents_descriptions>>\" not in agent.system_prompt\n        assert \"You can also give requests to team members.\" in manager_agent.system_prompt\n"}
{"type": "test_file", "path": "transformers/tests/agents/test_final_answer.py", "content": "# coding=utf-8\n# Copyright 2024 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom pathlib import Path\n\nimport numpy as np\nfrom PIL import Image\n\nfrom transformers import is_torch_available\nfrom transformers.agents.agent_types import AGENT_TYPE_MAPPING\nfrom transformers.agents.default_tools import FinalAnswerTool\nfrom transformers.testing_utils import get_tests_dir, require_torch\n\nfrom .test_tools_common import ToolTesterMixin\n\n\nif is_torch_available():\n    import torch\n\n\nclass FinalAnswerToolTester(unittest.TestCase, ToolTesterMixin):\n    def setUp(self):\n        self.inputs = {\"answer\": \"Final answer\"}\n        self.tool = FinalAnswerTool()\n\n    def test_exact_match_arg(self):\n        result = self.tool(\"Final answer\")\n        self.assertEqual(result, \"Final answer\")\n\n    def test_exact_match_kwarg(self):\n        result = self.tool(answer=self.inputs[\"answer\"])\n        self.assertEqual(result, \"Final answer\")\n\n    def create_inputs(self):\n        inputs_text = {\"answer\": \"Text input\"}\n        inputs_image = {\n            \"answer\": Image.open(Path(get_tests_dir(\"fixtures/tests_samples/COCO\")) / \"000000039769.png\").resize(\n                (512, 512)\n            )\n        }\n        inputs_audio = {\"answer\": torch.Tensor(np.ones(3000))}\n        return {\"string\": inputs_text, \"image\": inputs_image, \"audio\": inputs_audio}\n\n    @require_torch\n    def test_agent_type_output(self):\n        inputs = self.create_inputs()\n        for input_type, input in inputs.items():\n            output = self.tool(**input)\n            agent_type = AGENT_TYPE_MAPPING[input_type]\n            self.assertTrue(isinstance(output, agent_type))\n\n    @require_torch\n    def test_agent_types_inputs(self):\n        inputs = self.create_inputs()\n        for input_type, input in inputs.items():\n            output = self.tool(**input)\n            agent_type = AGENT_TYPE_MAPPING[input_type]\n            self.assertTrue(isinstance(output, agent_type))\n"}
{"type": "test_file", "path": "transformers/tests/agents/test_image_question_answering.py", "content": "# coding=utf-8\n# Copyright 2023 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\nfrom pathlib import Path\n\nfrom transformers import is_vision_available, load_tool\nfrom transformers.testing_utils import get_tests_dir\n\nfrom .test_tools_common import ToolTesterMixin\n\n\nif is_vision_available():\n    from PIL import Image\n\n\nclass ImageQuestionAnsweringToolTester(unittest.TestCase, ToolTesterMixin):\n    def setUp(self):\n        self.tool = load_tool(\"image_question_answering\")\n        self.tool.setup()\n\n    def test_exact_match_arg(self):\n        image = Image.open(Path(get_tests_dir(\"fixtures/tests_samples/COCO\")) / \"000000039769.png\")\n        result = self.tool(image, \"How many cats are sleeping on the couch?\")\n        self.assertEqual(result, \"2\")\n\n    def test_exact_match_kwarg(self):\n        image = Image.open(Path(get_tests_dir(\"fixtures/tests_samples/COCO\")) / \"000000039769.png\")\n        result = self.tool(image=image, question=\"How many cats are sleeping on the couch?\")\n        self.assertEqual(result, \"2\")\n"}
{"type": "test_file", "path": "transformers/tests/bettertransformer/test_integration.py", "content": "# coding=utf-8\n# Copyright 2023 The HuggingFace Team Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport tempfile\nimport unittest\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom transformers.testing_utils import (\n    is_torch_available,\n    require_optimum,\n    require_torch,\n    slow,\n)\n\n\nif is_torch_available():\n    import torch\n\n\n@require_torch\n@require_optimum\n@slow\nclass BetterTransformerIntegrationTest(unittest.TestCase):\n    # refer to the full test suite in Optimum library:\n    # https://github.com/huggingface/optimum/tree/main/tests/bettertransformer\n\n    def test_transform_and_reverse(self):\n        r\"\"\"\n        Classic tests to simply check if the conversion has been successful.\n        \"\"\"\n        model_id = \"hf-internal-testing/tiny-random-t5\"\n        tokenizer = AutoTokenizer.from_pretrained(model_id)\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n\n        inp = tokenizer(\"This is me\", return_tensors=\"pt\")\n\n        model = model.to_bettertransformer()\n\n        self.assertTrue(any(\"BetterTransformer\" in mod.__class__.__name__ for _, mod in model.named_modules()))\n\n        output = model.generate(**inp)\n\n        model = model.reverse_bettertransformer()\n\n        self.assertFalse(any(\"BetterTransformer\" in mod.__class__.__name__ for _, mod in model.named_modules()))\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            model.save_pretrained(tmpdirname)\n\n            model_reloaded = AutoModelForSeq2SeqLM.from_pretrained(tmpdirname)\n\n            self.assertFalse(\n                any(\"BetterTransformer\" in mod.__class__.__name__ for _, mod in model_reloaded.named_modules())\n            )\n\n            output_from_pretrained = model_reloaded.generate(**inp)\n            torch.testing.assert_close(output, output_from_pretrained)\n\n    def test_error_save_pretrained(self):\n        r\"\"\"\n        The save_pretrained method should raise a ValueError if the model is in BetterTransformer mode.\n        All should be good if the model is reversed.\n        \"\"\"\n        model_id = \"hf-internal-testing/tiny-random-t5\"\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n\n        model = model.to_bettertransformer()\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            with self.assertRaises(ValueError):\n                model.save_pretrained(tmpdirname)\n\n            model = model.reverse_bettertransformer()\n            model.save_pretrained(tmpdirname)\n"}
{"type": "test_file", "path": "transformers/tests/agents/test_search.py", "content": "# coding=utf-8\n# Copyright 2024 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom transformers import load_tool\n\nfrom .test_tools_common import ToolTesterMixin\n\n\nclass DuckDuckGoSearchToolTester(unittest.TestCase, ToolTesterMixin):\n    def setUp(self):\n        self.tool = load_tool(\"web_search\")\n        self.tool.setup()\n\n    def test_exact_match_arg(self):\n        result = self.tool(\"Agents\")\n        assert isinstance(result, list) and isinstance(result[0], dict)\n"}
{"type": "test_file", "path": "transformers/tests/agents/test_text_to_speech.py", "content": "# coding=utf-8\n# Copyright 2023 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom transformers import load_tool\nfrom transformers.utils import is_torch_available\n\n\nif is_torch_available():\n    import torch\n\nfrom transformers.testing_utils import require_torch\n\nfrom .test_tools_common import ToolTesterMixin\n\n\n@require_torch\nclass TextToSpeechToolTester(unittest.TestCase, ToolTesterMixin):\n    def setUp(self):\n        self.tool = load_tool(\"text_to_speech\")\n        self.tool.setup()\n\n    def test_exact_match_arg(self):\n        # SpeechT5 isn't deterministic\n        torch.manual_seed(0)\n        result = self.tool(\"hey\")\n        resulting_tensor = result.to_raw()\n        self.assertTrue(len(resulting_tensor.detach().shape) == 1)\n        self.assertTrue(resulting_tensor.detach().shape[0] > 1000)\n\n    def test_exact_match_kwarg(self):\n        # SpeechT5 isn't deterministic\n        torch.manual_seed(0)\n        result = self.tool(\"hey\")\n        resulting_tensor = result.to_raw()\n        self.assertTrue(len(resulting_tensor.detach().shape) == 1)\n        self.assertTrue(resulting_tensor.detach().shape[0] > 1000)\n"}
{"type": "test_file", "path": "transformers/tests/agents/test_python_interpreter.py", "content": "# coding=utf-8\n# Copyright 2024 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\nimport pytest\n\nfrom transformers import load_tool\nfrom transformers.agents.agent_types import AGENT_TYPE_MAPPING\nfrom transformers.agents.default_tools import BASE_PYTHON_TOOLS\nfrom transformers.agents.python_interpreter import InterpreterError, evaluate_python_code\n\nfrom .test_tools_common import ToolTesterMixin\n\n\n# Fake function we will use as tool\ndef add_two(x):\n    return x + 2\n\n\nclass PythonInterpreterToolTester(unittest.TestCase, ToolTesterMixin):\n    def setUp(self):\n        self.tool = load_tool(\"python_interpreter\", authorized_imports=[\"sqlite3\"])\n        self.tool.setup()\n\n    def test_exact_match_arg(self):\n        result = self.tool(\"(2 / 2) * 4\")\n        self.assertEqual(result, \"4.0\")\n\n    def test_exact_match_kwarg(self):\n        result = self.tool(code=\"(2 / 2) * 4\")\n        self.assertEqual(result, \"4.0\")\n\n    def test_agent_type_output(self):\n        inputs = [\"2 * 2\"]\n        output = self.tool(*inputs)\n        output_type = AGENT_TYPE_MAPPING[self.tool.output_type]\n        self.assertTrue(isinstance(output, output_type))\n\n    def test_agent_types_inputs(self):\n        inputs = [\"2 * 2\"]\n        _inputs = []\n\n        for _input, expected_input in zip(inputs, self.tool.inputs.values()):\n            input_type = expected_input[\"type\"]\n            if isinstance(input_type, list):\n                _inputs.append([AGENT_TYPE_MAPPING[_input_type](_input) for _input_type in input_type])\n            else:\n                _inputs.append(AGENT_TYPE_MAPPING[input_type](_input))\n\n        # Should not raise an error\n        output = self.tool(*inputs)\n        output_type = AGENT_TYPE_MAPPING[self.tool.output_type]\n        self.assertTrue(isinstance(output, output_type))\n\n\nclass PythonInterpreterTester(unittest.TestCase):\n    def test_evaluate_assign(self):\n        code = \"x = 3\"\n        state = {}\n        result = evaluate_python_code(code, {}, state=state)\n        assert result == 3\n        self.assertDictEqual(state, {\"x\": 3, \"print_outputs\": \"\"})\n\n        code = \"x = y\"\n        state = {\"y\": 5}\n        result = evaluate_python_code(code, {}, state=state)\n        # evaluate returns the value of the last assignment.\n        assert result == 5\n        self.assertDictEqual(state, {\"x\": 5, \"y\": 5, \"print_outputs\": \"\"})\n\n        code = \"a=1;b=None\"\n        result = evaluate_python_code(code, {}, state={})\n        # evaluate returns the value of the last assignment.\n        assert result is None\n\n    def test_assignment_cannot_overwrite_tool(self):\n        code = \"print = '3'\"\n        with pytest.raises(InterpreterError) as e:\n            evaluate_python_code(code, {\"print\": print}, state={})\n        assert \"Cannot assign to name 'print': doing this would erase the existing tool!\" in str(e)\n\n    def test_evaluate_call(self):\n        code = \"y = add_two(x)\"\n        state = {\"x\": 3}\n        result = evaluate_python_code(code, {\"add_two\": add_two}, state=state)\n        assert result == 5\n        self.assertDictEqual(state, {\"x\": 3, \"y\": 5, \"print_outputs\": \"\"})\n\n        # Should not work without the tool\n        with pytest.raises(InterpreterError) as e:\n            evaluate_python_code(code, {}, state=state)\n        assert \"tried to execute add_two\" in str(e.value)\n\n    def test_evaluate_constant(self):\n        code = \"x = 3\"\n        state = {}\n        result = evaluate_python_code(code, {}, state=state)\n        assert result == 3\n        self.assertDictEqual(state, {\"x\": 3, \"print_outputs\": \"\"})\n\n    def test_evaluate_dict(self):\n        code = \"test_dict = {'x': x, 'y': add_two(x)}\"\n        state = {\"x\": 3}\n        result = evaluate_python_code(code, {\"add_two\": add_two}, state=state)\n        self.assertDictEqual(result, {\"x\": 3, \"y\": 5})\n        self.assertDictEqual(state, {\"x\": 3, \"test_dict\": {\"x\": 3, \"y\": 5}, \"print_outputs\": \"\"})\n\n    def test_evaluate_expression(self):\n        code = \"x = 3\\ny = 5\"\n        state = {}\n        result = evaluate_python_code(code, {}, state=state)\n        # evaluate returns the value of the last assignment.\n        assert result == 5\n        self.assertDictEqual(state, {\"x\": 3, \"y\": 5, \"print_outputs\": \"\"})\n\n    def test_evaluate_f_string(self):\n        code = \"text = f'This is x: {x}.'\"\n        state = {\"x\": 3}\n        result = evaluate_python_code(code, {}, state=state)\n        # evaluate returns the value of the last assignment.\n        assert result == \"This is x: 3.\"\n        self.assertDictEqual(state, {\"x\": 3, \"text\": \"This is x: 3.\", \"print_outputs\": \"\"})\n\n    def test_evaluate_if(self):\n        code = \"if x <= 3:\\n    y = 2\\nelse:\\n    y = 5\"\n        state = {\"x\": 3}\n        result = evaluate_python_code(code, {}, state=state)\n        # evaluate returns the value of the last assignment.\n        assert result == 2\n        self.assertDictEqual(state, {\"x\": 3, \"y\": 2, \"print_outputs\": \"\"})\n\n        state = {\"x\": 8}\n        result = evaluate_python_code(code, {}, state=state)\n        # evaluate returns the value of the last assignment.\n        assert result == 5\n        self.assertDictEqual(state, {\"x\": 8, \"y\": 5, \"print_outputs\": \"\"})\n\n    def test_evaluate_list(self):\n        code = \"test_list = [x, add_two(x)]\"\n        state = {\"x\": 3}\n        result = evaluate_python_code(code, {\"add_two\": add_two}, state=state)\n        self.assertListEqual(result, [3, 5])\n        self.assertDictEqual(state, {\"x\": 3, \"test_list\": [3, 5], \"print_outputs\": \"\"})\n\n    def test_evaluate_name(self):\n        code = \"y = x\"\n        state = {\"x\": 3}\n        result = evaluate_python_code(code, {}, state=state)\n        assert result == 3\n        self.assertDictEqual(state, {\"x\": 3, \"y\": 3, \"print_outputs\": \"\"})\n\n    def test_evaluate_subscript(self):\n        code = \"test_list = [x, add_two(x)]\\ntest_list[1]\"\n        state = {\"x\": 3}\n        result = evaluate_python_code(code, {\"add_two\": add_two}, state=state)\n        assert result == 5\n        self.assertDictEqual(state, {\"x\": 3, \"test_list\": [3, 5], \"print_outputs\": \"\"})\n\n        code = \"test_dict = {'x': x, 'y': add_two(x)}\\ntest_dict['y']\"\n        state = {\"x\": 3}\n        result = evaluate_python_code(code, {\"add_two\": add_two}, state=state)\n        assert result == 5\n        self.assertDictEqual(state, {\"x\": 3, \"test_dict\": {\"x\": 3, \"y\": 5}, \"print_outputs\": \"\"})\n\n        code = \"vendor = {'revenue': 31000, 'rent': 50312}; vendor['ratio'] = round(vendor['revenue'] / vendor['rent'], 2)\"\n        state = {}\n        evaluate_python_code(code, {\"min\": min, \"print\": print, \"round\": round}, state=state)\n        assert state[\"vendor\"] == {\"revenue\": 31000, \"rent\": 50312, \"ratio\": 0.62}\n\n    def test_subscript_string_with_string_index_raises_appropriate_error(self):\n        code = \"\"\"\nsearch_results = \"[{'title': 'Paris, Ville de Paris, France Weather Forecast | AccuWeather', 'href': 'https://www.accuweather.com/en/fr/paris/623/weather-forecast/623', 'body': 'Get the latest weather forecast for Paris, Ville de Paris, France , including hourly, daily, and 10-day outlooks. AccuWeather provides you with reliable and accurate information on temperature ...'}]\"\nfor result in search_results:\n    if 'current' in result['title'].lower() or 'temperature' in result['title'].lower():\n        current_weather_url = result['href']\n        print(current_weather_url)\n        break\"\"\"\n        with pytest.raises(InterpreterError) as e:\n            evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n            assert \"You're trying to subscript a string with a string index\" in e\n\n    def test_evaluate_for(self):\n        code = \"x = 0\\nfor i in range(3):\\n    x = i\"\n        state = {}\n        result = evaluate_python_code(code, {\"range\": range}, state=state)\n        assert result == 2\n        self.assertDictEqual(state, {\"x\": 2, \"i\": 2, \"print_outputs\": \"\"})\n\n    def test_evaluate_binop(self):\n        code = \"y + x\"\n        state = {\"x\": 3, \"y\": 6}\n        result = evaluate_python_code(code, {}, state=state)\n        assert result == 9\n        self.assertDictEqual(state, {\"x\": 3, \"y\": 6, \"print_outputs\": \"\"})\n\n    def test_recursive_function(self):\n        code = \"\"\"\ndef recur_fibo(n):\n    if n <= 1:\n        return n\n    else:\n        return(recur_fibo(n-1) + recur_fibo(n-2))\nrecur_fibo(6)\"\"\"\n        result = evaluate_python_code(code, {}, state={})\n        assert result == 8\n\n    def test_evaluate_string_methods(self):\n        code = \"'hello'.replace('h', 'o').split('e')\"\n        result = evaluate_python_code(code, {}, state={})\n        assert result == [\"o\", \"llo\"]\n\n    def test_evaluate_slicing(self):\n        code = \"'hello'[1:3][::-1]\"\n        result = evaluate_python_code(code, {}, state={})\n        assert result == \"le\"\n\n    def test_access_attributes(self):\n        code = \"integer = 1\\nobj_class = integer.__class__\\nobj_class\"\n        result = evaluate_python_code(code, {}, state={})\n        assert result is int\n\n    def test_list_comprehension(self):\n        code = \"sentence = 'THESEAGULL43'\\nmeaningful_sentence = '-'.join([char.lower() for char in sentence if char.isalpha()])\"\n        result = evaluate_python_code(code, {}, state={})\n        assert result == \"t-h-e-s-e-a-g-u-l-l\"\n\n    def test_string_indexing(self):\n        code = \"\"\"text_block = [\n    \"THESE\",\n    \"AGULL\"\n]\nsentence = \"\"\nfor block in text_block:\n    for col in range(len(text_block[0])):\n        sentence += block[col]\n        \"\"\"\n        result = evaluate_python_code(code, {\"len\": len, \"range\": range}, state={})\n        assert result == \"THESEAGULL\"\n\n    def test_tuples(self):\n        code = \"x = (1, 2, 3)\\nx[1]\"\n        result = evaluate_python_code(code, {}, state={})\n        assert result == 2\n\n        code = \"\"\"\ndigits, i = [1, 2, 3], 1\ndigits[i], digits[i + 1] = digits[i + 1], digits[i]\"\"\"\n        evaluate_python_code(code, {\"range\": range, \"print\": print, \"int\": int}, {})\n\n        code = \"\"\"\ndef calculate_isbn_10_check_digit(number):\n    total = sum((10 - i) * int(digit) for i, digit in enumerate(number))\n    remainder = total % 11\n    check_digit = 11 - remainder\n    if check_digit == 10:\n        return 'X'\n    elif check_digit == 11:\n        return '0'\n    else:\n        return str(check_digit)\n\n# Given 9-digit numbers\nnumbers = [\n    \"478225952\",\n    \"643485613\",\n    \"739394228\",\n    \"291726859\",\n    \"875262394\",\n    \"542617795\",\n    \"031810713\",\n    \"957007669\",\n    \"871467426\"\n]\n\n# Calculate check digits for each number\ncheck_digits = [calculate_isbn_10_check_digit(number) for number in numbers]\nprint(check_digits)\n\"\"\"\n        state = {}\n        evaluate_python_code(\n            code, {\"range\": range, \"print\": print, \"sum\": sum, \"enumerate\": enumerate, \"int\": int, \"str\": str}, state\n        )\n\n    def test_listcomp(self):\n        code = \"x = [i for i in range(3)]\"\n        result = evaluate_python_code(code, {\"range\": range}, state={})\n        assert result == [0, 1, 2]\n\n    def test_break_continue(self):\n        code = \"for i in range(10):\\n    if i == 5:\\n        break\\ni\"\n        result = evaluate_python_code(code, {\"range\": range}, state={})\n        assert result == 5\n\n        code = \"for i in range(10):\\n    if i == 5:\\n        continue\\ni\"\n        result = evaluate_python_code(code, {\"range\": range}, state={})\n        assert result == 9\n\n    def test_call_int(self):\n        code = \"import math\\nstr(math.ceil(149))\"\n        result = evaluate_python_code(code, {\"str\": lambda x: str(x)}, state={})\n        assert result == \"149\"\n\n    def test_lambda(self):\n        code = \"f = lambda x: x + 2\\nf(3)\"\n        result = evaluate_python_code(code, {}, state={})\n        assert result == 5\n\n    def test_dictcomp(self):\n        code = \"x = {i: i**2 for i in range(3)}\"\n        result = evaluate_python_code(code, {\"range\": range}, state={})\n        assert result == {0: 0, 1: 1, 2: 4}\n\n        code = \"{num: name for num, name in {101: 'a', 102: 'b'}.items() if name not in ['a']}\"\n        result = evaluate_python_code(code, {\"print\": print}, state={}, authorized_imports=[\"pandas\"])\n        assert result == {102: \"b\"}\n\n        code = \"\"\"\nshifts = {'A': ('6:45', '8:00'), 'B': ('10:00', '11:45')}\nshift_minutes = {worker: ('a', 'b') for worker, (start, end) in shifts.items()}\n\"\"\"\n        result = evaluate_python_code(code, {}, state={})\n        assert result == {\"A\": (\"a\", \"b\"), \"B\": (\"a\", \"b\")}\n\n    def test_tuple_assignment(self):\n        code = \"a, b = 0, 1\\nb\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result == 1\n\n    def test_while(self):\n        code = \"i = 0\\nwhile i < 3:\\n    i += 1\\ni\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result == 3\n\n        # test infinite loop\n        code = \"i = 0\\nwhile i < 3:\\n    i -= 1\\ni\"\n        with pytest.raises(InterpreterError) as e:\n            evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert \"iterations in While loop exceeded\" in str(e)\n\n        # test lazy evaluation\n        code = \"\"\"\nhouse_positions = [0, 7, 10, 15, 18, 22, 22]\ni, n, loc = 0, 7, 30\nwhile i < n and house_positions[i] <= loc:\n    i += 1\n\"\"\"\n        state = {}\n        evaluate_python_code(code, BASE_PYTHON_TOOLS, state=state)\n\n    def test_generator(self):\n        code = \"a = [1, 2, 3, 4, 5]; b = (i**2 for i in a); list(b)\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result == [1, 4, 9, 16, 25]\n\n    def test_boolops(self):\n        code = \"\"\"if (not (a > b and a > c)) or d > e:\n    best_city = \"Brooklyn\"\nelse:\n    best_city = \"Manhattan\"\n    best_city\n    \"\"\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"e\": 5})\n        assert result == \"Brooklyn\"\n\n        code = \"\"\"if d > e and a < b:\n    best_city = \"Brooklyn\"\nelif d < e and a < b:\n    best_city = \"Sacramento\"\nelse:\n    best_city = \"Manhattan\"\n    best_city\n    \"\"\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"e\": 5})\n        assert result == \"Sacramento\"\n\n    def test_if_conditions(self):\n        code = \"\"\"char='a'\nif char.isalpha():\n    print('2')\"\"\"\n        state = {}\n        evaluate_python_code(code, BASE_PYTHON_TOOLS, state=state)\n        assert state[\"print_outputs\"] == \"2\\n\"\n\n    def test_imports(self):\n        code = \"import math\\nmath.sqrt(4)\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result == 2.0\n\n        code = \"from random import choice, seed\\nseed(12)\\nchoice(['win', 'lose', 'draw'])\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result == \"lose\"\n\n        code = \"import time, re\\ntime.sleep(0.1)\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result is None\n\n        code = \"from queue import Queue\\nq = Queue()\\nq.put(1)\\nq.get()\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result == 1\n\n        code = \"import itertools\\nlist(itertools.islice(range(10), 3))\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result == [0, 1, 2]\n\n        code = \"import re\\nre.search('a', 'abc').group()\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result == \"a\"\n\n        code = \"import stat\\nstat.S_ISREG(0o100644)\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result\n\n        code = \"import statistics\\nstatistics.mean([1, 2, 3, 4, 4])\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result == 2.8\n\n        code = \"import unicodedata\\nunicodedata.name('A')\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result == \"LATIN CAPITAL LETTER A\"\n\n        # Test submodules are handled properly, thus not raising error\n        code = \"import numpy.random as rd\\nrng = rd.default_rng(12345)\\nrng.random()\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={}, authorized_imports=[\"numpy\"])\n\n        code = \"from numpy.random import default_rng as d_rng\\nrng = d_rng(12345)\\nrng.random()\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={}, authorized_imports=[\"numpy\"])\n\n    def test_additional_imports(self):\n        code = \"import numpy as np\"\n        evaluate_python_code(code, authorized_imports=[\"numpy\"], state={})\n\n        code = \"import numpy.random as rd\"\n        evaluate_python_code(code, authorized_imports=[\"numpy.random\"], state={})\n        evaluate_python_code(code, authorized_imports=[\"numpy\"], state={})\n        with pytest.raises(InterpreterError):\n            evaluate_python_code(code, authorized_imports=[\"random\"], state={})\n\n    def test_multiple_comparators(self):\n        code = \"0 <= -1 < 4 and 0 <= -5 < 4\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert not result\n\n        code = \"0 <= 1 < 4 and 0 <= -5 < 4\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert not result\n\n        code = \"0 <= 4 < 4 and 0 <= 3 < 4\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert not result\n\n        code = \"0 <= 3 < 4 and 0 <= 3 < 4\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result\n\n    def test_print_output(self):\n        code = \"print('Hello world!')\\nprint('Ok no one cares')\"\n        state = {}\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state=state)\n        assert result is None\n        assert state[\"print_outputs\"] == \"Hello world!\\nOk no one cares\\n\"\n\n        # test print in function\n        code = \"\"\"\nprint(\"1\")\ndef function():\n    print(\"2\")\nfunction()\"\"\"\n        state = {}\n        evaluate_python_code(code, {\"print\": print}, state=state)\n        assert state[\"print_outputs\"] == \"1\\n2\\n\"\n\n    def test_tuple_target_in_iterator(self):\n        code = \"for a, b in [('Ralf Weikert', 'Austria'), ('Samuel Seungwon Lee', 'South Korea')]:res = a.split()[0]\"\n        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert result == \"Samuel\"\n\n    def test_classes(self):\n        code = \"\"\"\nclass Animal:\n    species = \"Generic Animal\"\n\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def sound(self):\n        return \"The animal makes a sound.\"\n\n    def __str__(self):\n        return f\"{self.name}, {self.age} years old\"\n\nclass Dog(Animal):\n    species = \"Canine\"\n\n    def __init__(self, name, age, breed):\n        super().__init__(name, age)\n        self.breed = breed\n\n    def sound(self):\n        return \"The dog barks.\"\n\n    def __str__(self):\n        return f\"{self.name}, {self.age} years old, {self.breed}\"\n\nclass Cat(Animal):\n    def sound(self):\n        return \"The cat meows.\"\n\n    def __str__(self):\n        return f\"{self.name}, {self.age} years old, {self.species}\"\n\n\n# Testing multiple instances\ndog1 = Dog(\"Fido\", 3, \"Labrador\")\ndog2 = Dog(\"Buddy\", 5, \"Golden Retriever\")\n\n# Testing method with built-in function\nanimals = [dog1, dog2, Cat(\"Whiskers\", 2)]\nnum_animals = len(animals)\n\n# Testing exceptions in methods\nclass ExceptionTest:\n    def method_that_raises(self):\n        raise ValueError(\"An error occurred\")\n\ntry:\n    exc_test = ExceptionTest()\n    exc_test.method_that_raises()\nexcept ValueError as e:\n    exception_message = str(e)\n\n\n# Collecting results\ndog1_sound = dog1.sound()\ndog1_str = str(dog1)\ndog2_sound = dog2.sound()\ndog2_str = str(dog2)\ncat = Cat(\"Whiskers\", 2)\ncat_sound = cat.sound()\ncat_str = str(cat)\n    \"\"\"\n        state = {}\n        evaluate_python_code(code, {\"print\": print, \"len\": len, \"super\": super, \"str\": str, \"sum\": sum}, state=state)\n\n        # Assert results\n        assert state[\"dog1_sound\"] == \"The dog barks.\"\n        assert state[\"dog1_str\"] == \"Fido, 3 years old, Labrador\"\n        assert state[\"dog2_sound\"] == \"The dog barks.\"\n        assert state[\"dog2_str\"] == \"Buddy, 5 years old, Golden Retriever\"\n        assert state[\"cat_sound\"] == \"The cat meows.\"\n        assert state[\"cat_str\"] == \"Whiskers, 2 years old, Generic Animal\"\n        assert state[\"num_animals\"] == 3\n        assert state[\"exception_message\"] == \"An error occurred\"\n\n    def test_variable_args(self):\n        code = \"\"\"\ndef var_args_method(self, *args, **kwargs):\n    return sum(args) + sum(kwargs.values())\n\nvar_args_method(1, 2, 3, x=4, y=5)\n\"\"\"\n        state = {}\n        result = evaluate_python_code(code, {\"sum\": sum}, state=state)\n        assert result == 15\n\n    def test_exceptions(self):\n        code = \"\"\"\ndef method_that_raises(self):\n    raise ValueError(\"An error occurred\")\n\ntry:\n    method_that_raises()\nexcept ValueError as e:\n    exception_message = str(e)\n    \"\"\"\n        state = {}\n        evaluate_python_code(code, {\"print\": print, \"len\": len, \"super\": super, \"str\": str, \"sum\": sum}, state=state)\n        assert state[\"exception_message\"] == \"An error occurred\"\n\n    def test_print(self):\n        code = \"print(min([1, 2, 3]))\"\n        state = {}\n        evaluate_python_code(code, {\"min\": min, \"print\": print}, state=state)\n        assert state[\"print_outputs\"] == \"1\\n\"\n\n    def test_types_as_objects(self):\n        code = \"type_a = float(2); type_b = str; type_c = int\"\n        state = {}\n        result = evaluate_python_code(code, {\"float\": float, \"str\": str, \"int\": int}, state=state)\n        assert result is int\n\n    def test_tuple_id(self):\n        code = \"\"\"\nfood_items = {\"apple\": 2, \"banana\": 3, \"orange\": 1, \"pear\": 1}\nunique_food_items = [item for item, count in food_item_counts.items() if count == 1]\n\"\"\"\n        state = {}\n        result = evaluate_python_code(code, {}, state=state)\n        assert result == [\"orange\", \"pear\"]\n\n    def test_nonsimple_augassign(self):\n        code = \"\"\"\ncounts_dict = {'a': 0}\ncounts_dict['a'] += 1\ncounts_list = [1, 2, 3]\ncounts_list += [4, 5, 6]\n\nclass Counter:\n    self.count = 0\n\na = Counter()\na.count += 1\n\"\"\"\n        state = {}\n        evaluate_python_code(code, {}, state=state)\n        assert state[\"counts_dict\"] == {\"a\": 1}\n        assert state[\"counts_list\"] == [1, 2, 3, 4, 5, 6]\n        assert state[\"a\"].count == 1\n\n    def test_adding_int_to_list_raises_error(self):\n        code = \"\"\"\ncounts = [1, 2, 3]\ncounts += 1\"\"\"\n        with pytest.raises(InterpreterError) as e:\n            evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert \"Cannot add non-list value 1 to a list.\" in str(e)\n\n    def test_error_highlights_correct_line_of_code(self):\n        code = \"\"\"# Ok this is a very long code\n# It has many commented lines\na = 1\nb = 2\n\n# Here is another piece\ncounts = [1, 2, 3]\ncounts += 1\nb += 1\"\"\"\n        with pytest.raises(InterpreterError) as e:\n            evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert \"Evaluation stopped at line 'counts += 1\" in str(e)\n\n    def test_assert(self):\n        code = \"\"\"\nassert 1 == 1\nassert 1 == 2\n\"\"\"\n        with pytest.raises(AssertionError) as e:\n            evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n        assert \"1 == 2\" in str(e) and \"1 == 1\" not in str(e)\n\n    def test_with_context_manager(self):\n        code = \"\"\"\nclass SimpleLock:\n    def __init__(self):\n        self.locked = False\n\n    def __enter__(self):\n        self.locked = True\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.locked = False\n\nlock = SimpleLock()\n\nwith lock as l:\n    assert l.locked == True\n\nassert lock.locked == False\n    \"\"\"\n        state = {}\n        tools = {}\n        evaluate_python_code(code, tools, state=state)\n\n    def test_default_arg_in_function(self):\n        code = \"\"\"\ndef f(a, b=333, n=1000):\n    return b + n\nn = f(1, n=667)\n\"\"\"\n        res = evaluate_python_code(code, {}, {})\n        assert res == 1000\n\n    def test_set(self):\n        code = \"\"\"\nS1 = {'a', 'b', 'c'}\nS2 = {'b', 'c', 'd'}\nS3 = S1.difference(S2)\nS4 = S1.intersection(S2)\n\"\"\"\n        state = {}\n        evaluate_python_code(code, {}, state=state)\n        assert state[\"S3\"] == {\"a\"}\n        assert state[\"S4\"] == {\"b\", \"c\"}\n\n    def test_break(self):\n        code = \"\"\"\ni = 0\n\nwhile True:\n    i+= 1\n    if i==3:\n        break\n\ni\"\"\"\n        result = evaluate_python_code(code, {\"print\": print, \"round\": round}, state={})\n        assert result == 3\n\n    def test_return(self):\n        # test early returns\n        code = \"\"\"\ndef add_one(n, shift):\n    if True:\n        return n + shift\n    return n\n\nadd_one(1, 1)\n\"\"\"\n        state = {}\n        result = evaluate_python_code(code, {\"print\": print, \"range\": range, \"ord\": ord, \"chr\": chr}, state=state)\n        assert result == 2\n\n        # test returning None\n        code = \"\"\"\ndef returns_none(a):\n    return\n\nreturns_none(1)\n\"\"\"\n        state = {}\n        result = evaluate_python_code(code, {\"print\": print, \"range\": range, \"ord\": ord, \"chr\": chr}, state=state)\n        assert result is None\n\n    def test_nested_for_loop(self):\n        code = \"\"\"\nall_res = []\nfor i in range(10):\n    subres = []\n    for j in range(i):\n        subres.append(j)\n    all_res.append(subres)\n\nout = [i for sublist in all_res for i in sublist]\nout[:10]\n\"\"\"\n        state = {}\n        result = evaluate_python_code(code, {\"print\": print, \"range\": range}, state=state)\n        assert result == [0, 0, 1, 0, 1, 2, 0, 1, 2, 3]\n\n    def test_pandas(self):\n        code = \"\"\"\nimport pandas as pd\n\ndf = pd.DataFrame.from_dict({'SetCount': ['5', '4', '5'], 'Quantity': [1, 0, -1]})\n\ndf['SetCount'] = pd.to_numeric(df['SetCount'], errors='coerce')\n\nparts_with_5_set_count = df[df['SetCount'] == 5.0]\nparts_with_5_set_count[['Quantity', 'SetCount']].values[1]\n\"\"\"\n        state = {}\n        result = evaluate_python_code(code, {}, state=state, authorized_imports=[\"pandas\"])\n        assert np.array_equal(result, [-1, 5])\n\n        code = \"\"\"\nimport pandas as pd\n\ndf = pd.DataFrame.from_dict({\"AtomicNumber\": [111, 104, 105], \"ok\": [0, 1, 2]})\nprint(\"HH0\")\n\n# Filter the DataFrame to get only the rows with outdated atomic numbers\nfiltered_df = df.loc[df['AtomicNumber'].isin([104])]\n\"\"\"\n        result = evaluate_python_code(code, {\"print\": print}, state={}, authorized_imports=[\"pandas\"])\n        assert np.array_equal(result.values[0], [104, 1])\n\n        code = \"\"\"import pandas as pd\ndata = pd.DataFrame.from_dict([\n    {\"Pclass\": 1, \"Survived\": 1},\n    {\"Pclass\": 2, \"Survived\": 0},\n    {\"Pclass\": 2, \"Survived\": 1}\n])\nsurvival_rate_by_class = data.groupby('Pclass')['Survived'].mean()\n\"\"\"\n        result = evaluate_python_code(code, {}, state={}, authorized_imports=[\"pandas\"])\n        assert result.values[1] == 0.5\n\n    def test_starred(self):\n        code = \"\"\"\nfrom math import radians, sin, cos, sqrt, atan2\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000  # Radius of the Earth in meters\n    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n    distance = R * c\n    return distance\n\ncoords_geneva = (46.1978, 6.1342)\ncoords_barcelona = (41.3869, 2.1660)\n\ndistance_geneva_barcelona = haversine(*coords_geneva, *coords_barcelona)\n\"\"\"\n        result = evaluate_python_code(code, {\"print\": print, \"map\": map}, state={}, authorized_imports=[\"math\"])\n        assert round(result, 1) == 622395.4\n\n    def test_for(self):\n        code = \"\"\"\nshifts = {\n    \"Worker A\": (\"6:45 pm\", \"8:00 pm\"),\n    \"Worker B\": (\"10:00 am\", \"11:45 am\")\n}\n\nshift_intervals = {}\nfor worker, (start, end) in shifts.items():\n    shift_intervals[worker] = end\nshift_intervals\n\"\"\"\n        result = evaluate_python_code(code, {\"print\": print, \"map\": map}, state={})\n        assert result == {\"Worker A\": \"8:00 pm\", \"Worker B\": \"11:45 am\"}\n"}
{"type": "test_file", "path": "transformers/tests/deepspeed/test_deepspeed.py", "content": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport dataclasses\nimport io\nimport itertools\nimport json\nimport os\nimport unittest\nfrom copy import deepcopy\nfrom functools import partial\n\nimport datasets\nfrom parameterized import parameterized\n\nimport tests.trainer.test_trainer\nimport transformers\nfrom tests.trainer.test_trainer import TrainerIntegrationCommon  # noqa\nfrom transformers import AutoModel, TrainingArguments, is_torch_available, logging\nfrom transformers.integrations.deepspeed import (\n    HfDeepSpeedConfig,\n    is_deepspeed_available,\n    unset_hf_deepspeed_config,\n)\nfrom transformers.testing_utils import (\n    CaptureLogger,\n    CaptureStd,\n    CaptureStderr,\n    LoggingLevel,\n    TestCasePlus,\n    backend_device_count,\n    execute_subprocess_async,\n    mockenv_context,\n    require_deepspeed,\n    require_optuna,\n    require_torch_accelerator,\n    require_torch_fp16,\n    require_torch_multi_accelerator,\n    run_first,\n    slow,\n    torch_device,\n)\nfrom transformers.trainer_utils import get_last_checkpoint, set_seed\nfrom transformers.utils import SAFE_WEIGHTS_NAME, is_torch_bf16_available_on_device, is_torch_fp16_available_on_device\n\n\nif is_torch_available():\n    import torch\n\n    from tests.trainer.test_trainer import (  # noqa\n        RegressionModelConfig,\n        RegressionPreTrainedModel,\n    )\n\n    # hack to restore original logging level pre #21700\n    get_regression_trainer = partial(tests.trainer.test_trainer.get_regression_trainer, log_level=\"info\")\n\n\nset_seed(42)\n\n# default torch.distributed port\nDEFAULT_MASTER_PORT = \"10999\"\n\nT5_SMALL = \"google-t5/t5-small\"\nT5_TINY = \"patrickvonplaten/t5-tiny-random\"\nGPT2_TINY = \"sshleifer/tiny-gpt2\"\nGPTJ_TINY = \"hf-internal-testing/tiny-random-gptj\"\n\n\ndef load_json(path):\n    with open(path) as f:\n        return json.load(f)\n\n\ndef get_master_port(real_launcher=False):\n    \"\"\"\n    When using a single gpu launcher emulation (i.e. not deepspeed or python -m torch.distributed)\n    the issue is that once the port is tied it can't be used anywhere else outside of this process,\n    since torch.dist doesn't free the port until the process exits. Therefore for the sake of being\n    able to run both emulated launcher and normal launcher tests we need 2 distinct ports.\n\n    This function will give the right port in the right context. For real launcher it'll give the\n    base port, for emulated launcher it'll give the base port + 1. In both cases a string is\n    returned.\n\n    Args:\n        `real_launcher`: whether a real launcher is going to be used, or the emulated one\n\n    \"\"\"\n\n    master_port_base = os.environ.get(\"DS_TEST_PORT\", DEFAULT_MASTER_PORT)\n    if not real_launcher:\n        master_port_base = str(int(master_port_base) + 1)\n    return master_port_base\n\n\ndef require_deepspeed_aio(test_case):\n    \"\"\"\n    Decorator marking a test that requires deepspeed aio (nvme)\n    \"\"\"\n    if not is_deepspeed_available():\n        return unittest.skip(reason=\"test requires deepspeed\")(test_case)\n\n    import deepspeed\n    from deepspeed.ops.aio import AsyncIOBuilder\n\n    if not deepspeed.ops.__compatible_ops__[AsyncIOBuilder.NAME]:\n        return unittest.skip(reason=\"test requires deepspeed async-io\")(test_case)\n    else:\n        return test_case\n\n\nif is_deepspeed_available():\n    from deepspeed.utils import logger as deepspeed_logger  # noqa\n    from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n    from transformers.integrations.deepspeed import deepspeed_config, is_deepspeed_zero3_enabled  # noqa\n\n\ndef get_launcher(distributed=False):\n    # 1. explicitly set --num_nodes=1 just in case these tests end up run on a multi-node setup\n    # - it won't be able to handle that\n    # 2. for now testing with just 2 gpus max (since some quality tests may give different\n    # results with mode gpus because we use very little data)\n    num_gpus = min(2, backend_device_count(torch_device)) if distributed else 1\n    master_port = get_master_port(real_launcher=True)\n    return f\"deepspeed --num_nodes 1 --num_gpus {num_gpus} --master_port {master_port}\".split()\n\n\nZERO2 = \"zero2\"\nZERO3 = \"zero3\"\n\nFP16 = \"fp16\"\nBF16 = \"bf16\"\n\nHF_OPTIM = \"hf_optim\"\nHF_SCHEDULER = \"hf_scheduler\"\nDS_OPTIM = \"ds_optim\"\nDS_SCHEDULER = \"ds_scheduler\"\n\noptims = [HF_OPTIM, DS_OPTIM]\nschedulers = [HF_SCHEDULER, DS_SCHEDULER]\n\nstages = [ZERO2, ZERO3]\n\ndtypes = []\nif is_torch_bf16_available_on_device(torch_device):\n    dtypes.append(BF16)\nif is_torch_fp16_available_on_device(torch_device):\n    dtypes.append(FP16)\n\n\ndef parameterized_custom_name_func(func, param_num, param):\n    # customize the test name generator function as we want both params to appear in the sub-test\n    # name, as by default it shows only the first param\n    param_based_name = parameterized.to_safe_name(\"_\".join(str(x) for x in param.args))\n    return f\"{func.__name__}_{param_based_name}\"\n\n\n# Cartesian-product of zero stages with models to test\nparams = list(itertools.product(stages, dtypes))\n\nparams_with_optims_and_schedulers = list(itertools.product(stages, dtypes, optims, schedulers))\n\n\n@require_deepspeed\nclass CoreIntegrationDeepSpeed(TestCasePlus, TrainerIntegrationCommon):\n    \"\"\"\n    Testing non-Trainer DeepSpeed integration\n    \"\"\"\n\n    def setUp(self):\n        super().setUp()\n\n        master_port = get_master_port(real_launcher=False)\n        self.dist_env_1_gpu = {\n            \"MASTER_ADDR\": \"localhost\",\n            \"MASTER_PORT\": master_port,\n            \"RANK\": \"0\",\n            \"LOCAL_RANK\": \"0\",\n            \"WORLD_SIZE\": \"1\",\n        }\n\n    def tearDown(self):\n        super().tearDown()\n\n        # reset the ds config global so that tests state doesn't leak\n        unset_hf_deepspeed_config()\n\n    def test_init_zero3(self):\n        # test that zero.Init() works correctly\n        ds_config = {\n            \"train_batch_size\": 1,\n            \"zero_optimization\": {\n                \"stage\": 3,\n            },\n        }\n\n        dschf = HfDeepSpeedConfig(ds_config)\n\n        self.assertTrue(dschf.is_zero3())\n        self.assertTrue(is_deepspeed_zero3_enabled())\n\n        with LoggingLevel(logging.INFO):\n            with mockenv_context(**self.dist_env_1_gpu):\n                logger = logging.get_logger(\"transformers.modeling_utils\")\n                with CaptureLogger(logger) as cl:\n                    AutoModel.from_pretrained(T5_TINY)\n        self.assertIn(\"Detected DeepSpeed ZeRO-3\", cl.out)\n\n        # now remove zero optimization\n        del ds_config[\"zero_optimization\"]\n        dschf = HfDeepSpeedConfig(ds_config)\n\n        self.assertFalse(dschf.is_zero3())\n        self.assertFalse(is_deepspeed_zero3_enabled())\n\n        with LoggingLevel(logging.INFO):\n            with mockenv_context(**self.dist_env_1_gpu):\n                logger = logging.get_logger(\"transformers.modeling_utils\")\n                with CaptureLogger(logger) as cl:\n                    AutoModel.from_pretrained(T5_TINY)\n        self.assertNotIn(\"Detected DeepSpeed ZeRO-3\", cl.out)\n\n    @require_torch_fp16\n    @require_torch_accelerator\n    def test_init_zero3_fp16(self):\n        # test that zero.Init() works correctly under zero3/fp16\n        ds_config = {\n            \"train_batch_size\": 1,\n            \"zero_optimization\": {\n                \"stage\": 3,\n            },\n            \"fp16\": {\n                \"enabled\": True,\n            },\n        }\n\n        dschf = HfDeepSpeedConfig(ds_config)\n\n        self.assertTrue(dschf.is_zero3())\n        self.assertTrue(is_deepspeed_zero3_enabled())\n\n        with LoggingLevel(logging.INFO):\n            with mockenv_context(**self.dist_env_1_gpu):\n                logger = logging.get_logger(\"transformers.modeling_utils\")\n                with CaptureLogger(logger) as cl:\n                    AutoModel.from_pretrained(T5_TINY)\n        self.assertIn(\"Detected DeepSpeed ZeRO-3\", cl.out)\n\n        # now remove zero optimization\n        del ds_config[\"zero_optimization\"]\n        dschf = HfDeepSpeedConfig(ds_config)\n\n        self.assertFalse(dschf.is_zero3())\n        self.assertFalse(is_deepspeed_zero3_enabled())\n\n        with LoggingLevel(logging.INFO):\n            with mockenv_context(**self.dist_env_1_gpu):\n                logger = logging.get_logger(\"transformers.modeling_utils\")\n                with CaptureLogger(logger) as cl:\n                    AutoModel.from_pretrained(T5_TINY)\n        self.assertNotIn(\"Detected DeepSpeed ZeRO-3\", cl.out)\n\n    def test_init_zero3_missing_params(self):\n        # test that zero.Init() for missing parameters works correctly under zero3\n        import deepspeed\n        import torch\n\n        from transformers.models.gpt2.modeling_gpt2 import GPT2PreTrainedModel\n\n        class TinyGPT2WithUninitializedWeights(GPT2PreTrainedModel):\n            def __init__(self, config):\n                super().__init__(config)\n                self.transformer = AutoModel.from_pretrained(GPT2_TINY, config=config)\n                self.new_head = torch.nn.Linear(config.hidden_size, config.vocab_size, bias=True)\n\n            def forward(self, *args, **kwargs):\n                transformer_outputs = self.transformer(*args, **kwargs)\n                hidden_states = transformer_outputs[0]\n                return self.new_head(hidden_states).float()\n\n            def _init_weights(self, module):\n                super()._init_weights(module)\n                if module is self.new_head:\n                    self.new_head.weight.data.fill_(-100.0)\n                    self.new_head.bias.data.fill_(+100.0)\n\n        ds_config = {\n            \"train_batch_size\": 1,\n            \"zero_optimization\": {\n                \"stage\": 3,\n            },\n        }\n\n        dschf = HfDeepSpeedConfig(ds_config)\n\n        self.assertTrue(dschf.is_zero3())\n        self.assertTrue(is_deepspeed_zero3_enabled())\n\n        with LoggingLevel(logging.INFO):\n            with mockenv_context(**self.dist_env_1_gpu):\n                logger = logging.get_logger(\"transformers.modeling_utils\")\n                with CaptureLogger(logger) as cl:\n                    model = TinyGPT2WithUninitializedWeights.from_pretrained(GPT2_TINY)\n        self.assertIn(\"Detected DeepSpeed ZeRO-3\", cl.out)\n        self.assertRegex(cl.out, r\"newly initialized.*new_head\\.bias.*new_head\\.weight\")\n        with deepspeed.zero.GatheredParameters([model.new_head.weight, model.new_head.bias]):\n            self.assertTrue(\n                torch.allclose(model.new_head.weight, torch.tensor(-100.0, device=model.new_head.weight.device)),\n            )\n            self.assertTrue(\n                torch.allclose(model.new_head.bias, torch.tensor(+100.0, device=model.new_head.bias.device)),\n            )\n\n        # now remove zero optimization\n        del ds_config[\"zero_optimization\"]\n        dschf = HfDeepSpeedConfig(ds_config)\n\n        self.assertFalse(dschf.is_zero3())\n        self.assertFalse(is_deepspeed_zero3_enabled())\n\n        with LoggingLevel(logging.INFO):\n            with mockenv_context(**self.dist_env_1_gpu):\n                logger = logging.get_logger(\"transformers.modeling_utils\")\n                with CaptureLogger(logger) as cl:\n                    model = TinyGPT2WithUninitializedWeights.from_pretrained(GPT2_TINY)\n        self.assertNotIn(\"Detected DeepSpeed ZeRO-3\", cl.out)\n        self.assertRegex(cl.out, r\"newly initialized.*new_head\\.bias.*new_head\\.weight\")\n        self.assertTrue(\n            torch.allclose(model.new_head.weight, torch.tensor(-100.0, device=model.new_head.weight.device)),\n        )\n        self.assertTrue(\n            torch.allclose(model.new_head.bias, torch.tensor(+100.0, device=model.new_head.bias.device)),\n        )\n\n    def test_arange_bf16(self):\n        # Tests that configuring DeepSpeed with 16 bits does not cause float `torch.arange()` tensors to be cast down.\n        # NOTE -- this assumes that the function calls have the following downcast-preventing pattern, i.e.\n        # `torch.arange(...,dtype=torch.int64)` followed by a cast like `.to(torch.float32)`. 🚨 If this pattern is\n        # NOT applied (e.g. `torch.arange(...,dtype=torch.float32)` is used), DeepSpeed can automatically cast it down\n        # at init time. See https://github.com/huggingface/transformers/issues/28685 for more info.\n\n        ds_config = {\n            \"train_batch_size\": 1,\n            \"zero_optimization\": {\n                \"stage\": 3,\n            },\n            \"bf16\": {\"enabled\": True},\n        }\n\n        dschf = HfDeepSpeedConfig(ds_config)\n\n        self.assertTrue(dschf.is_zero3())\n        self.assertTrue(is_deepspeed_zero3_enabled())\n\n        with LoggingLevel(logging.INFO):\n            with mockenv_context(**self.dist_env_1_gpu):\n                logger = logging.get_logger(\"transformers.modeling_utils\")\n                with CaptureLogger(logger) as cl:\n                    model = AutoModel.from_pretrained(GPTJ_TINY)\n        self.assertIn(\"Detected DeepSpeed ZeRO-3\", cl.out)\n\n        # The model weights are in BF16 as per deepspeed config\n        self.assertTrue(str(model.h[0].attn.q_proj.weight.dtype) == \"torch.bfloat16\")\n        good_deepspeed_sin_cos = model.h[0].attn.embed_positions\n\n        # Monkeypatches the function that creates RoPE embeddings using the INCORRECT torch.arange() pattern, and\n        # then recreates the model\n        def bad_deepspeed_create_sinusoidal_positions(num_pos: int, dim: int) -> torch.Tensor:\n            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64) / dim))\n            # Incorrect pattern here: torch.arange has dtype=torch.float32 as its argument, and it will automatically\n            # converted to BF16 by DeepSpeed\n            sinusoid_inp = torch.einsum(\"i , j -> i j\", torch.arange(num_pos, dtype=inv_freq.dtype), inv_freq)\n            return torch.cat((torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)), dim=1)\n\n        good_deepspeed_create_sinusoidal_positions = transformers.models.gptj.modeling_gptj.create_sinusoidal_positions\n        transformers.models.gptj.modeling_gptj.create_sinusoidal_positions = bad_deepspeed_create_sinusoidal_positions\n\n        with LoggingLevel(logging.INFO):\n            with mockenv_context(**self.dist_env_1_gpu):\n                logger = logging.get_logger(\"transformers.modeling_utils\")\n                with CaptureLogger(logger) as cl:\n                    model = AutoModel.from_pretrained(GPTJ_TINY)\n        self.assertIn(\"Detected DeepSpeed ZeRO-3\", cl.out)\n\n        self.assertTrue(str(model.h[0].attn.q_proj.weight.dtype) == \"torch.bfloat16\")\n        bad_deepspeed_sin_cos = model.h[0].attn.embed_positions\n\n        # Compares the two values: the two sets of values are different, and the correct one matches the torch\n        # (i.e. outside DeepSpeed) version.\n        good_torch_sin_cos = good_deepspeed_create_sinusoidal_positions(\n            model.config.max_position_embeddings, model.config.rotary_dim\n        )\n        self.assertFalse(torch.allclose(good_deepspeed_sin_cos, bad_deepspeed_sin_cos))\n        torch.testing.assert_close(good_torch_sin_cos, good_deepspeed_sin_cos.cpu())\n\n        # Finally, we can see that the incorrect pattern is okay on vanilla torch, demonstrating that this issue is\n        # exclusive to DeepSpeed\n        bad_torch_sin_cos = bad_deepspeed_create_sinusoidal_positions(\n            model.config.max_position_embeddings, model.config.rotary_dim\n        )\n        torch.testing.assert_close(bad_torch_sin_cos, good_torch_sin_cos)\n\n\nclass TrainerIntegrationDeepSpeedWithCustomConfig(TestCasePlus):\n    def setUp(self):\n        super().setUp()\n\n        args = TrainingArguments(\".\")\n        self.n_epochs = args.num_train_epochs\n        self.batch_size = args.train_batch_size\n\n        master_port = get_master_port(real_launcher=False)\n        self.dist_env_1_gpu = {\n            \"MASTER_ADDR\": \"localhost\",\n            \"MASTER_PORT\": master_port,\n            \"RANK\": \"0\",\n            \"LOCAL_RANK\": \"0\",\n            \"WORLD_SIZE\": \"1\",\n        }\n\n        self.ds_config_file = {\n            \"zero2\": f\"{self.test_file_dir_str}/ds_config_zero2.json\",\n            \"zero3\": f\"{self.test_file_dir_str}/ds_config_zero3.json\",\n        }\n\n        # use self.get_config_dict(stage) to use these to ensure the original is not modified\n        with io.open(self.ds_config_file[ZERO2], \"r\", encoding=\"utf-8\") as f:\n            config_zero2 = json.load(f)\n        with io.open(self.ds_config_file[ZERO3], \"r\", encoding=\"utf-8\") as f:\n            config_zero3 = json.load(f)\n            # The following setting slows things down, so don't enable it by default unless needed by a test.\n            # It's in the file as a demo for users since we want everything to work out of the box even if slower.\n            config_zero3[\"zero_optimization\"][\"stage3_gather_16bit_weights_on_model_save\"] = False\n\n        self.ds_config_dict = {\n            \"zero2\": config_zero2,\n            \"zero3\": config_zero3,\n        }\n\n    def tearDown(self):\n        super().tearDown()\n\n        # reset the ds config global so that tests state doesn't leak\n        unset_hf_deepspeed_config()\n\n    def get_config_dict(self, stage):\n        # As some tests modify the dict, always make a copy\n        return deepcopy(self.ds_config_dict[stage])\n\n\n@require_deepspeed\n@require_torch_fp16\n@require_torch_accelerator\nclass TrainerIntegrationDeepSpeed(TrainerIntegrationDeepSpeedWithCustomConfig, TrainerIntegrationCommon):\n    \"\"\"\n\n    This class is for testing directly via get_regression_trainer\n\n    It mixes in `TrainerIntegrationCommon` which already has a lot of helper validation methods\n    which we can re-use here.\n\n    Important: this class' setup can only work with a single gpu because it runs within the current\n    pytest worker. For multi-gpu tests use TestDeepSpeedWithLauncher.\n\n    Note: if any of the tests of this class get run there will be at least one gpu occupied by them\n    until this pytest worker exits. This is because the gpu memory allocated by the cuda-kernels\n    won't be released until this pytest worker exits.\n\n    This may appear as some run-away tests if you watch `nvidia-smi` while other tests that fork new\n    processes are run. So there will be one or two \"stale\" processes reported in `nvidia-smi`. This\n    is not a bug.\n    \"\"\"\n\n    # --- These tests are enough to run on one of zero stages --- #\n\n    def test_hf_ds_config_mismatch(self):\n        ds_config = self.get_config_dict(ZERO2)\n\n        # Purposefully configure these values to mismatch TrainingArguments values.\n        # This currently doesn't cover all keys (but it could)\n        per_device_train_batch_size = 2\n        ds_config[\"train_micro_batch_size_per_gpu\"] = per_device_train_batch_size + 2\n\n        ds_config[\"train_batch_size\"] = 1000\n\n        gradient_accumulation_steps = 2\n        ds_config[\"gradient_accumulation_steps\"] = gradient_accumulation_steps + 2\n\n        max_grad_norm = 1.0\n        ds_config[\"gradient_clipping\"] = max_grad_norm + 0.1\n\n        adam_beta1, adam_beta2 = 0.9, 0.99\n        ds_config[\"optimizer\"][\"params\"][\"betas\"] = [adam_beta1 - 0.1, adam_beta2 - 0.1]\n\n        fp16 = True\n        ds_config[\"fp16\"][\"enabled\"] = not fp16\n\n        keys = [\n            \"per_device_train_batch_size\",\n            \"train_batch_size\",\n            \"gradient_accumulation_steps\",\n            \"max_grad_norm\",\n            \"betas\",\n            \"fp16\",\n        ]\n\n        with mockenv_context(**self.dist_env_1_gpu):\n            trainer = get_regression_trainer(\n                local_rank=0,\n                fp16=fp16,\n                deepspeed=ds_config,\n                per_device_train_batch_size=per_device_train_batch_size,\n                gradient_accumulation_steps=gradient_accumulation_steps,\n                max_grad_norm=max_grad_norm,\n                adam_beta1=adam_beta1,\n                adam_beta2=adam_beta2,\n                output_dir=self.get_auto_remove_tmp_dir(),\n            )\n            with self.assertRaises(Exception) as context:\n                trainer.train()\n\n        for key in keys:\n            self.assertTrue(\n                key in str(context.exception),\n                f\"{key} is not in the exception message:\\n{context.exception}\",\n            )\n\n    # Test various combos\n    # 1. DS scheduler + DS optimizer: this is already tested by most other tests\n    # 2. HF scheduler + HF optimizer:\n    # 3. DS scheduler + HF optimizer:\n    # 4. HF scheduler + DS optimizer:\n\n    def test_hf_scheduler_hf_optimizer(self):\n        a = 0\n        with mockenv_context(**self.dist_env_1_gpu):\n            ds_config_zero2_dict = self.get_config_dict(ZERO2)\n            del ds_config_zero2_dict[\"optimizer\"]  # force default HF Trainer optimizer\n            del ds_config_zero2_dict[\"scheduler\"]  # force default HF Trainer scheduler\n            ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n            ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n            trainer = get_regression_trainer(\n                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n            )\n            trainer.train()\n        new_a = trainer.model.a.item()\n        self.assertNotEqual(new_a, a)\n\n    def test_ds_scheduler_hf_optimizer(self):\n        a = 0\n        with mockenv_context(**self.dist_env_1_gpu):\n            ds_config_zero2_dict = self.get_config_dict(ZERO2)\n            del ds_config_zero2_dict[\"optimizer\"]  # force default HF Trainer optimizer\n            ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n            ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n            trainer = get_regression_trainer(\n                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n            )\n            trainer.train()\n        new_a = trainer.model.a.item()\n        self.assertNotEqual(new_a, a)\n\n    def test_hf_scheduler_ds_optimizer(self):\n        a = 0\n        with mockenv_context(**self.dist_env_1_gpu):\n            ds_config_zero2_dict = self.get_config_dict(ZERO2)\n            del ds_config_zero2_dict[\"scheduler\"]  # force default HF Trainer scheduler\n            ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n            ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n            trainer = get_regression_trainer(\n                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n            )\n            trainer.train()\n        new_a = trainer.model.a.item()\n        self.assertNotEqual(new_a, a)\n\n    @require_deepspeed_aio\n    def test_stage3_nvme_offload(self):\n        with mockenv_context(**self.dist_env_1_gpu):\n            # this actually doesn't have to be on NVMe, any storage will do since this test only\n            # runs a simple check that we can use some directory as if it were NVMe\n            nvme_path = self.get_auto_remove_tmp_dir()\n            nvme_config = {\"device\": \"nvme\", \"nvme_path\": nvme_path}\n            ds_config_zero3_dict = self.get_config_dict(ZERO3)\n            ds_config_zero3_dict[\"zero_optimization\"][\"offload_optimizer\"] = nvme_config\n            ds_config_zero3_dict[\"zero_optimization\"][\"offload_param\"] = nvme_config\n            ds_config_zero3_dict[\"zero_optimization\"][\"stage3_gather_16bit_weights_on_model_save\"] = True\n            trainer = get_regression_trainer(\n                local_rank=0, fp16=True, deepspeed=ds_config_zero3_dict, output_dir=self.get_auto_remove_tmp_dir()\n            )\n            with CaptureLogger(deepspeed_logger) as cl:\n                trainer.train()\n            self.assertIn(\"DeepSpeed info\", cl.out, \"expected DeepSpeed logger output but got none\")\n\n    @require_optuna\n    def test_hyperparameter_search(self):\n        with mockenv_context(**self.dist_env_1_gpu):\n            ds_config_zero3_dict = self.get_config_dict(ZERO3)\n\n            # hyperparameter_search requires model_init() to recreate the model for each trial\n            def model_init():\n                config = RegressionModelConfig(a=0, b=0, double_output=False)\n                model = RegressionPreTrainedModel(config)\n                return model\n\n            trainer = get_regression_trainer(\n                local_rank=0,\n                fp16=True,\n                model_init=model_init,\n                deepspeed=ds_config_zero3_dict,\n                output_dir=self.get_auto_remove_tmp_dir(),\n            )\n\n            n_trials = 3\n            with CaptureLogger(deepspeed_logger) as cl:\n                with CaptureStd() as cs:\n                    trainer.hyperparameter_search(direction=\"maximize\", n_trials=n_trials)\n            self.assertIn(\"DeepSpeed info\", cl.out, \"expected DeepSpeed logger output but got none\")\n            self.assertIn(f\"Trial {n_trials-1} finished with value\", cs.err, \"expected hyperparameter_search output\")\n            self.assertIn(\"Best is trial\", cs.err, \"expected hyperparameter_search output\")\n\n    # --- These tests need to run on both zero stages --- #\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    def test_hf_optimizer_with_offload(self, stage, dtype):\n        # non-DS optimizers can be used with ZERO-offload (as long as they have both CPU and GPU implementation (except LAMB))\n        ds_config_dict = self.get_config_dict(stage)\n        del ds_config_dict[\"optimizer\"]  # force default HF Trainer optimizer\n        # force cpu offload\n        ds_config_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"cpu\"\n        ds_config_dict[\"zero_force_ds_cpu_optimizer\"] = False  # offload is not efficient w/o CPUAdam\n        with mockenv_context(**self.dist_env_1_gpu):\n            kwargs = {\"local_rank\": 0, \"deepspeed\": ds_config_dict, \"output_dir\": self.get_auto_remove_tmp_dir()}\n            kwargs[dtype] = True\n            trainer = get_regression_trainer(**kwargs)\n            with CaptureLogger(deepspeed_logger) as cl:\n                trainer.train()\n            self.assertIn(\"DeepSpeed info\", cl.out, \"expected DeepSpeed logger output but got none\")\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    def test_fake_notebook_no_launcher(self, stage, dtype):\n        # this setup emulates a notebook where a launcher needs to be emulated by hand\n\n        # note that unittest resets sys.stdout each test, so `CaptureStd` will work here to capture\n        # DeepSpeed log if this test happens to run first in this pytest worker. But it will fail if\n        # it's run not as a first test as `sys.stdout` will no longer be the same. So we either have\n        # to reset `deepspeed_logger.handlers[0].setStream(sys.stdout)` or directly capture from the deepspeed_logger.\n        with mockenv_context(**self.dist_env_1_gpu):\n            kwargs = {\n                \"local_rank\": 0,\n                \"deepspeed\": self.get_config_dict(stage),\n                \"output_dir\": self.get_auto_remove_tmp_dir(),\n            }\n            kwargs[dtype] = True\n            trainer = get_regression_trainer(**kwargs)\n\n            with CaptureLogger(deepspeed_logger) as cl:\n                trainer.train()\n            self.assertIn(\"DeepSpeed info\", cl.out, \"expected DeepSpeed logger output but got none\")\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    def test_early_get_last_lr(self, stage, dtype):\n        # with deepspeed's fp16 and dynamic loss scale enabled the optimizer/scheduler steps may\n        # not run for the first few dozen steps while loss scale is too large, and thus during\n        # that time `get_last_lr` will fail if called during that warm up stage,\n        #\n        # setting `logging_steps=1` forces an early `trainer._maybe_log_save_evaluate()` which calls\n        # `self.lr_scheduler.get_last_lr()` and originally it'd fail on the very first step.\n        with mockenv_context(**self.dist_env_1_gpu):\n            a = b = 0.0\n            kwargs = {\n                \"a\": a,\n                \"b\": b,\n                \"local_rank\": 0,\n                \"train_len\": 8,\n                \"deepspeed\": self.get_config_dict(stage),\n                \"per_device_train_batch_size\": 8,\n                \"logging_steps\": 1,\n                \"output_dir\": self.get_auto_remove_tmp_dir(),\n            }\n            kwargs[dtype] = True\n            trainer = get_regression_trainer(**kwargs)\n\n            trainer.train()\n            post_train_a = trainer.model.a.item()\n\n            # XXX: for some reason the following check fails with zero3/fp16 and any/bf16 - not a\n            # broken but a different qualitative outcome - as if optimizer did run\n            # oddly getting 1.0 for both a and b from 0.0 - there is a bug somewhere\n            # print(trainer.model.a.item())\n            # print(trainer.model.b.item())\n            # need to investigate at some point\n            if (stage == ZERO3 and dtype == FP16) or (dtype == BF16):\n                self.skipTest(reason=\"When using zero3/fp16 or any/bf16 the optimizer seems run oddly\")\n\n            # it's enough that train didn't fail for this test, but we must check that\n            # optimizer/scheduler didn't run (since if it did this test isn't testing the right thing)\n            self.assertEqual(post_train_a, a)\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    def test_gradient_accumulation(self, stage, dtype):\n        # this test measures that we get identical weights and similar loss with:\n        # 1. per_device_train_batch_size=8, gradient_accumulation_steps=1\n        # 2. per_device_train_batch_size=4, gradient_accumulation_steps=2\n        # since the 2nd should produce the effective batch of 1st, with the same results\n        #\n        # I can get an identical loss for a small train_len=32, plus the power of the initial\n        # dynamic loss scale value set to:\n        #   \"fp16.initial_scale_power\": 1\n        # plus having the same WarmupLR's warmup_min_lr == warmup_max_lr in the config file\n        # but for some reason going to train_len=64, the weights start to mismatch with this setup.\n        # the culprit seems to be `initial_scale_power` - putting it back to its default 32 keeps the weights identical\n\n        train_len = 64\n        a = b = 0.0\n\n        kwargs = {\n            \"a\": a,\n            \"b\": b,\n            \"local_rank\": 0,\n            \"train_len\": train_len,\n            \"deepspeed\": self.get_config_dict(stage),\n            \"output_dir\": self.get_auto_remove_tmp_dir(),\n        }\n        kwargs[dtype] = True\n\n        with mockenv_context(**self.dist_env_1_gpu):\n            no_grad_accum_trainer = get_regression_trainer(\n                **kwargs,\n                per_device_train_batch_size=16,\n                gradient_accumulation_steps=1,\n            )\n            no_grad_accum_result = no_grad_accum_trainer.train()\n            no_grad_accum_loss = no_grad_accum_result.training_loss\n            no_grad_accum_a = no_grad_accum_trainer.model.a.item()\n            no_grad_accum_b = no_grad_accum_trainer.model.b.item()\n            # make sure the optimizer kicked in - if it hasn't changed from the original value of a then make train_len bigger\n            self.assertNotEqual(no_grad_accum_a, a)\n\n        with mockenv_context(**self.dist_env_1_gpu):\n            yes_grad_accum_trainer = get_regression_trainer(\n                **kwargs,\n                per_device_train_batch_size=4,\n                gradient_accumulation_steps=4,\n            )\n            yes_grad_accum_result = yes_grad_accum_trainer.train()\n            yes_grad_accum_loss = yes_grad_accum_result.training_loss\n            yes_grad_accum_a = yes_grad_accum_trainer.model.a.item()\n            yes_grad_accum_b = yes_grad_accum_trainer.model.b.item()\n            self.assertNotEqual(yes_grad_accum_a, a)\n\n        # training with half the batch size but accumulation steps as 2 should give the same\n        # weights, but sometimes get a slight difference still of 1e-6\n        if torch_device == \"hpu\":\n            self.assertAlmostEqual(no_grad_accum_a, yes_grad_accum_a, delta=1e-4)\n            self.assertAlmostEqual(no_grad_accum_b, yes_grad_accum_b, delta=1e-4)\n        else:\n            self.assertAlmostEqual(no_grad_accum_a, yes_grad_accum_a, places=5)\n            self.assertAlmostEqual(no_grad_accum_b, yes_grad_accum_b, places=5)\n\n        # Relative difference. See the note above how to get identical loss on a small bs\n        self.assertTrue((no_grad_accum_loss - yes_grad_accum_loss) / (no_grad_accum_loss + 1e-15) <= 1e-3)\n\n    # NOTE: Currently a disabled test. In the future we should re-enable it.\n    # Issue resolves around Zero-3 w/ DPO/TRL + DeepSpeed\n    # As well as Zero-3 inference\n    # Related PR: https://github.com/huggingface/transformers/pull/32299\n    # def test_missed_zero3_init(self):\n    #     from transformers import Trainer  # noqa\n\n    #     with mockenv_context(**self.dist_env_1_gpu):\n    #         model = AutoModel.from_pretrained(T5_TINY)\n    #         training_args = TrainingArguments(\n    #             output_dir=\"./test_missed_zero3_init\",\n    #             deepspeed=self.get_config_dict(ZERO3),\n    #         )\n    #         with self.assertRaises(\n    #             ValueError, msg=\"Model was not initialized with `Zero-3` despite being configured.\"\n    #         ):\n    #             _ = Trainer(\n    #                 model=model,\n    #                 args=training_args,\n    #             )\n    #         # Now do it properly, triggered from our `TrainingArguments` earlier\n    #         model = AutoModel.from_pretrained(T5_TINY)\n    #         trainer = Trainer(\n    #             model=model,\n    #             args=training_args,\n    #         )\n    #         assert trainer.is_deepspeed_enabled\n    #         assert model._transformers_zero3_init_used\n\n    def check_saved_checkpoints_deepspeed(self, output_dir, freq, total, stage, dtype):\n        # adapted from TrainerIntegrationCommon.check_saved_checkpoints\n        file_list = [SAFE_WEIGHTS_NAME, \"training_args.bin\", \"trainer_state.json\", \"config.json\"]\n\n        if stage == ZERO2:\n            ds_file_list = [\"mp_rank_00_model_states.pt\"]\n        elif stage == ZERO3:\n            ds_file_list = [\"zero_pp_rank_0_mp_rank_00_model_states.pt\"]\n        else:\n            raise ValueError(f\"unknown stage {stage}\")\n\n        if dtype == \"bf16\":\n            ds_file_list.append(\"bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\")\n\n        for step in range(freq, total, freq):\n            checkpoint = os.path.join(output_dir, f\"checkpoint-{step}\")\n            self.assertTrue(os.path.isdir(checkpoint), f\"[{stage}] {checkpoint} dir is not found\")\n            # common files\n            for filename in file_list:\n                path = os.path.join(checkpoint, filename)\n                self.assertTrue(os.path.isfile(path), f\"[{stage}] {path} is not found\")\n\n            # ds files\n            ds_path = os.path.join(checkpoint, f\"global_step{step}\")\n            for filename in ds_file_list:\n                # filename = os.path.join(path, filename)\n                # print(filename)\n                path = os.path.join(ds_path, filename)\n                self.assertTrue(os.path.isfile(path), f\"[{stage}] {path} is not found\")\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    def test_save_checkpoints(self, stage, dtype):\n        # adapted from  TrainerIntegrationTest.test_save_checkpoints\n\n        freq = 5\n        output_dir = self.get_auto_remove_tmp_dir()\n        ds_config_dict = self.get_config_dict(stage)\n        if dtype == FP16:\n            ds_config_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n        # XXX:\n        if stage == ZERO3:\n            ds_config_dict[\"zero_optimization\"][\"stage3_gather_16bit_weights_on_model_save\"] = True\n\n        # save checkpoints\n        with mockenv_context(**self.dist_env_1_gpu):\n            kwargs = {\n                \"output_dir\": output_dir,\n                \"save_steps\": freq,\n                \"deepspeed\": ds_config_dict,\n            }\n            kwargs[dtype] = True\n            trainer = get_regression_trainer(**kwargs)\n            trainer.train()\n\n        total = int(self.n_epochs * 64 / self.batch_size)\n        self.check_saved_checkpoints_deepspeed(output_dir, freq, total, stage, dtype)\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    def test_can_resume_training_errors(self, stage, dtype):\n        with mockenv_context(**self.dist_env_1_gpu):\n            ds_config_dict = self.get_config_dict(stage)\n            output_dir = self.get_auto_remove_tmp_dir()\n            kwargs = {\"output_dir\": output_dir, \"deepspeed\": ds_config_dict}\n            kwargs[dtype] = True\n            trainer = get_regression_trainer(**kwargs)\n\n            # 1. fail to find any checkpoint - due a fresh output_dir\n            with self.assertRaises(Exception) as context:\n                trainer.train(resume_from_checkpoint=True)\n            self.assertTrue(\n                \"No valid checkpoint found in output directory\" in str(context.exception),\n                f\"got exception: {context.exception}\",\n            )\n\n            # 2. fail to find a bogus checkpoint\n            with self.assertRaises(Exception) as context:\n                checkpoint = os.path.join(output_dir, \"checkpoint-5\")\n                trainer.train(resume_from_checkpoint=f\"{checkpoint}-bogus\")\n\n    @parameterized.expand(params_with_optims_and_schedulers, name_func=parameterized_custom_name_func)\n    def test_can_resume_training_normal(self, stage, dtype, optim, scheduler):\n        # adapted from TrainerIntegrationTest.test_can_resume_training\n        # test normal resume for each stage separately, error-handling is tested in a different test\n\n        # ToDo: Currently, hf_optim + hf_scheduler resumes with the correct states and\n        # also has same losses for few steps but then slowly diverges. Need to figure it out.\n        if optim == HF_OPTIM and scheduler == HF_SCHEDULER:\n            self.skipTest(reason=\"hf_optim + hf_scheduler resumes with the correct states but slowly diverges\")\n\n        output_dir = self.get_auto_remove_tmp_dir(\"./xxx\", after=False)\n        ds_config_dict = self.get_config_dict(stage)\n        if dtype == FP16:\n            ds_config_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n        # XXX:\n        if stage == ZERO3:\n            ds_config_dict[\"zero_optimization\"][\"stage3_gather_16bit_weights_on_model_save\"] = True\n\n        if optim == HF_OPTIM:\n            del ds_config_dict[\"optimizer\"]\n\n        if scheduler == HF_SCHEDULER:\n            del ds_config_dict[\"scheduler\"]\n\n        kwargs = {\n            \"output_dir\": output_dir,\n            \"train_len\": 128,\n            \"save_steps\": 5,\n            \"learning_rate\": 0.1,\n            \"deepspeed\": ds_config_dict,\n        }\n        kwargs[dtype] = True\n\n        with mockenv_context(**self.dist_env_1_gpu):\n            trainer = get_regression_trainer(**kwargs)\n            trainer.train()\n            (a, b) = trainer.model.a.item(), trainer.model.b.item()\n            state = dataclasses.asdict(trainer.state)\n\n            checkpoint = os.path.join(output_dir, \"checkpoint-5\")\n\n            # Reinitialize trainer\n            trainer = get_regression_trainer(**kwargs)\n\n            trainer.train(resume_from_checkpoint=checkpoint)\n            (a1, b1) = trainer.model.a.item(), trainer.model.b.item()\n            state1 = dataclasses.asdict(trainer.state)\n            self.assertEqual(a, a1)\n            self.assertEqual(b, b1)\n            self.check_trainer_state_are_the_same(state, state1)\n\n            # Now check with a later checkpoint that it also works when we span over one epoch\n            checkpoint = os.path.join(output_dir, \"checkpoint-15\")\n\n            # Reinitialize trainer and load model\n            trainer = get_regression_trainer(**kwargs)\n\n            trainer.train(resume_from_checkpoint=checkpoint)\n            (a1, b1) = trainer.model.a.item(), trainer.model.b.item()\n            state1 = dataclasses.asdict(trainer.state)\n            self.assertEqual(a, a1)\n            self.assertEqual(b, b1)\n            self.check_trainer_state_are_the_same(state, state1)\n\n            # Finally, should be able to resume with the same trainer/same deepspeed engine instance\n            # XXX: but currently this not possible due DS bug: https://github.com/deepspeedai/DeepSpeed/issues/1612\n            # trainer.train(resume_from_checkpoint=checkpoint)\n            # a workaround needs to be used that re-creates the deepspeed engine\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    def test_load_state_dict_from_zero_checkpoint(self, stage, dtype):\n        # test that we can load fp32 weights directly from the zero checkpoint into the current model\n\n        output_dir = self.get_auto_remove_tmp_dir()  # \"./xxx\", after=False, before=False)\n\n        ds_config_dict = self.get_config_dict(stage)\n\n        kwargs = {\n            \"output_dir\": output_dir,\n            \"train_len\": 4,\n            \"per_device_train_batch_size\": 4,\n            \"num_train_epochs\": 1,\n            \"save_strategy\": \"steps\",\n            \"save_steps\": 1,\n            \"learning_rate\": 0.1,\n            \"deepspeed\": ds_config_dict,\n        }\n        kwargs[dtype] = True\n\n        with mockenv_context(**self.dist_env_1_gpu):\n            trainer = get_regression_trainer(**kwargs)\n            trainer.train()\n            (a, b) = trainer.model.a.item(), trainer.model.b.item()\n            state = dataclasses.asdict(trainer.state)\n\n            checkpoint_dir = get_last_checkpoint(output_dir)\n            model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)\n\n            (a1, b1) = model.a.item(), model.b.item()\n            state1 = dataclasses.asdict(trainer.state)\n            self.assertEqual(a, a1)\n            self.assertEqual(b, b1)\n            self.check_trainer_state_are_the_same(state, state1)\n\n    def test_ds_config_object(self):\n        # test that we can switch from zero2 to zero3 in the same process for example\n        # test is_zero, etc.\n        output_dir = self.get_auto_remove_tmp_dir()\n        kwargs = {\"output_dir\": output_dir, \"train_len\": 8, \"fp16\": True}\n\n        ds_config_zero3_dict = self.get_config_dict(ZERO3)\n        ds_config_zero2_dict = self.get_config_dict(ZERO2)\n\n        with mockenv_context(**self.dist_env_1_gpu):\n            trainer = get_regression_trainer(deepspeed=ds_config_zero3_dict, **kwargs)\n            self.assertTrue(is_deepspeed_zero3_enabled())\n\n            # test we can repeat that and with train this time\n            trainer = get_regression_trainer(deepspeed=ds_config_zero3_dict, **kwargs)\n            trainer.train()\n            self.assertTrue(is_deepspeed_zero3_enabled())\n\n            # test zero3 is disabled\n            trainer = get_regression_trainer(deepspeed=ds_config_zero2_dict, **kwargs)\n            self.assertFalse(is_deepspeed_zero3_enabled())\n\n            # check config obj\n            config = deepspeed_config()\n            self.assertTrue(bool(config), \"Deepspeed config should be accessible\")\n\n            # with accelerate integration below line is additionally required for this test to pass\n            trainer.accelerator.state._reset_state()\n            del trainer\n            # now weakref should gc the global and we shouldn't get anything here\n            config = deepspeed_config()\n            self.assertFalse(is_deepspeed_zero3_enabled())\n            self.assertFalse(bool(config), \"Deepspeed config should not be accessible\")\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    def test_load_best_model(self, stage, dtype):\n        # Test that forced deepspeed reinit doesn't break the model. the forced re-init after\n        # loading the best model in Trainer is there to workaround this bug in Deepspeed\n        # https://github.com/deepspeedai/DeepSpeed/issues/1612\n        #\n        # The test is derived from a repro script submitted in this Issue:\n        # https://github.com/huggingface/transformers/issues/17114\n        #\n        # One additional feature of this test is that we use a non-AdamW optimizer to test that\n        # deepspeed doesn't fallback to AdamW, which would prevent the optimizer states from loading\n        # correctly\n\n        from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer  # noqa\n\n        output_dir = self.get_auto_remove_tmp_dir()  # \"./xxx\", after=False, before=False)\n\n        ds_config_dict = self.get_config_dict(stage)\n        del ds_config_dict[\"optimizer\"]  # will use HF Trainer optimizer\n        del ds_config_dict[\"scheduler\"]  # will use HF Trainer scheduler\n        ds_config_dict[\"zero_force_ds_cpu_optimizer\"] = False  # offload is not efficient w/o CPUAdam\n        # must use this setting to get the reload path exercised\n        ds_config_dict[\"zero_optimization\"][\"stage3_gather_16bit_weights_on_model_save\"] = True\n\n        with mockenv_context(**self.dist_env_1_gpu):\n            args_dict = {\n                \"per_device_train_batch_size\": 1,\n                \"per_device_eval_batch_size\": 1,\n                \"gradient_accumulation_steps\": 1,\n                \"learning_rate\": 1e-4,\n                \"num_train_epochs\": 1,\n                \"do_train\": True,\n                \"do_eval\": True,\n                \"optim\": \"adafactor\",\n                \"eval_strategy\": \"steps\",\n                \"eval_steps\": 1,\n                \"save_strategy\": \"steps\",\n                \"save_steps\": 1,\n                \"load_best_model_at_end\": True,\n                \"max_steps\": 1,\n                \"deepspeed\": ds_config_dict,\n                \"report_to\": \"none\",\n            }\n\n            training_args = TrainingArguments(output_dir, **args_dict)\n            tokenizer = T5Tokenizer.from_pretrained(T5_TINY)\n            model = T5ForConditionalGeneration.from_pretrained(T5_TINY)\n\n            def _add_eos_to_examples(example):\n                example[\"input_text\"] = f\"question: {example['question']}  context: {example['context']}\"\n                example[\"target_text\"] = example[\"answers\"][\"text\"][0] if len(example[\"answers\"][\"text\"]) > 0 else \"\"\n                return example\n\n            def _convert_to_features(example_batch):\n                input_encodings = tokenizer.batch_encode_plus(\n                    example_batch[\"input_text\"], pad_to_max_length=True, max_length=512, truncation=True\n                )\n                target_encodings = tokenizer.batch_encode_plus(\n                    example_batch[\"target_text\"], pad_to_max_length=True, max_length=16, truncation=True\n                )\n\n                encodings = {\n                    \"input_ids\": input_encodings[\"input_ids\"],\n                    \"attention_mask\": input_encodings[\"attention_mask\"],\n                    \"labels\": target_encodings[\"input_ids\"],\n                }\n\n                return encodings\n\n            def get_dataset():\n                data_file = str(self.tests_dir / \"fixtures/tests_samples/SQUAD/sample.json\")\n                data_files = {\"train\": data_file, \"validation\": data_file}\n                raw_datasets = datasets.load_dataset(\"json\", data_files=data_files, field=\"data\")\n                train_dataset = raw_datasets[\"train\"].map(_add_eos_to_examples).map(_convert_to_features, batched=True)\n                valid_dataset = deepcopy(train_dataset)\n                return train_dataset, valid_dataset\n\n            train_dataset, eval_dataset = get_dataset()\n\n            trainer = Trainer(\n                model=model,\n                processing_class=tokenizer,\n                args=training_args,\n                train_dataset=train_dataset,\n                eval_dataset=eval_dataset,\n            )\n            trainer.train()  # crash 1 was here\n            trainer.evaluate()  # crash 2 was here\n\n\n@slow\n@run_first\n@require_deepspeed\n@require_torch_accelerator\nclass TestDeepSpeedWithLauncher(TestCasePlus):\n    \"\"\"This class is for testing via an external script - can do multiple gpus\"\"\"\n\n    # Tests to devise #\n    #\n    # 1. predict_with_generate on multigpu - need to figure out how to give input sequences so that\n    # the 2 gpus will generate prediction sequences that aren't of the same length - this is because\n    # we had to code a special feature to sync the gpus when the predicted sequences aren't of the\n    # same length. In general this will tested as a side-effect through a variety of other tests -\n    # it'll simply hang trying to synchronize with other gpus if this problem is encountered. So as\n    # long as we have a few full tests running on zero3 + predict_with_generate this should be\n    # mostly covered.\n    #\n    # but there are 5 variations on beam search in `generate`- with identical code branched with `if\n    # synced_gpus`\n    #\n    # 2. most tests should probably be run on both: zero2 and zero3 configs\n    #\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    @require_torch_multi_accelerator\n    def test_basic_distributed(self, stage, dtype):\n        self.run_and_check(stage=stage, dtype=dtype, distributed=True)\n\n    @require_torch_fp16\n    def test_do_eval_no_train(self):\n        # testing only zero3 since zero2 makes no sense with inference\n        self.run_and_check(\n            stage=ZERO3,\n            dtype=FP16,\n            eval_steps=1,\n            distributed=False,\n            do_train=False,\n            do_eval=True,\n        )\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    def test_fp32_non_distributed(self, stage, dtype):\n        # real model needs too much GPU memory under stage2+fp32, so using tiny random model here -\n        # therefore no quality checks, just basic completion checks are done\n        self.run_and_check(\n            stage=stage,\n            dtype=dtype,\n            model_name=T5_TINY,\n            distributed=False,\n            do_train=True,\n            do_eval=True,\n            quality_checks=False,\n            fp32=True,\n        )\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    @require_torch_multi_accelerator\n    def test_fp32_distributed(self, stage, dtype):\n        # real model needs too much GPU memory under stage2+fp32, so using tiny random model here -\n        # therefore no quality checks, just basic completion checks are done\n        self.run_and_check(\n            stage=stage,\n            dtype=dtype,\n            model_name=T5_TINY,\n            distributed=True,\n            do_train=True,\n            do_eval=True,\n            quality_checks=False,\n            fp32=True,\n        )\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    def test_resume_train_not_from_ds_checkpoint(self, stage, dtype):\n        # do normal training and then resume not from the deepspeed checkpoint but explicitly from\n        # the saved model dir\n\n        do_train = True\n        do_eval = False\n        kwargs = {\n            \"stage\": stage,\n            \"dtype\": dtype,\n            \"eval_steps\": 1,\n            \"distributed\": True,\n            \"do_train\": do_train,\n            \"do_eval\": do_eval,\n        }\n\n        # 1. normal training\n        output_dir = self.run_and_check(**kwargs)\n\n        # 2. now resume explicitly from the saved weights, by passing --model_name_or_path output_dir\n        # - i.e. the same path the model was saved to in step 1\n        output_dir = self.run_trainer(**kwargs, model_name=output_dir)\n\n        self.do_checks(output_dir, do_train=do_train, do_eval=do_eval)\n\n    @parameterized.expand([\"bf16\", \"fp16\", \"fp32\"])\n    @require_torch_multi_accelerator\n    def test_inference(self, dtype):\n        if dtype == \"bf16\" and not is_torch_bf16_available_on_device(torch_device):\n            self.skipTest(reason=\"test requires bfloat16 hardware support\")\n\n        if dtype == \"fp16\" and not is_torch_fp16_available_on_device(torch_device):\n            self.skipTest(reason=\"test requires fp16 hardware support\")\n\n        # this is just inference, so no optimizer should be loaded\n        # it only works for z3 (makes no sense with z1-z2)\n        fp32 = True if dtype == \"fp32\" else False\n        self.run_and_check(\n            stage=ZERO3,\n            dtype=dtype,\n            model_name=T5_TINY,\n            distributed=True,\n            do_train=False,\n            do_eval=True,\n            quality_checks=False,\n            fp32=fp32,\n        )\n\n    def do_checks(self, output_dir, do_train=True, do_eval=True, quality_checks=True):\n        if do_train:\n            train_metrics = load_json(os.path.join(output_dir, \"train_results.json\"))\n            self.assertIn(\"train_samples_per_second\", train_metrics)\n            if quality_checks:\n                self.assertGreater(train_metrics[\"train_samples_per_second\"], 0.5)\n\n        if do_eval:\n            eval_metrics = load_json(os.path.join(output_dir, \"eval_results.json\"))\n            self.assertIn(\"eval_bleu\", eval_metrics)\n            if quality_checks:\n                self.assertGreater(eval_metrics[\"eval_bleu\"], 1)\n\n    # XXX: need to do better validation beyond just that the run was successful\n    def run_and_check(\n        self,\n        stage,\n        dtype,\n        model_name: str = T5_SMALL,\n        eval_steps: int = 10,\n        distributed: bool = True,\n        do_train: bool = True,\n        do_eval: bool = True,\n        quality_checks: bool = True,\n        fp32: bool = False,\n        extra_args_str: str = None,\n        remove_args_str: str = None,\n    ):\n        # we are doing quality testing so using a small real model\n        output_dir = self.run_trainer(\n            stage=stage,\n            dtype=dtype,\n            model_name=model_name,\n            eval_steps=eval_steps,\n            num_train_epochs=1,\n            do_train=do_train,\n            do_eval=do_eval,\n            distributed=distributed,\n            fp32=fp32,\n            extra_args_str=extra_args_str,\n            remove_args_str=remove_args_str,\n        )\n\n        self.do_checks(output_dir, do_train=do_train, do_eval=do_eval, quality_checks=quality_checks)\n\n        return output_dir\n\n    def run_trainer(\n        self,\n        stage: str,\n        dtype: str,\n        model_name: str,\n        eval_steps: int = 10,\n        num_train_epochs: int = 1,\n        do_train: bool = False,\n        do_eval: bool = True,\n        distributed: bool = True,\n        fp32: bool = False,\n        extra_args_str: str = None,\n        remove_args_str: str = None,\n    ):\n        max_len = 32\n        data_dir = self.test_file_dir / \"../fixtures/tests_samples/wmt_en_ro\"\n        output_dir = self.get_auto_remove_tmp_dir()\n        args = f\"\"\"\n            --model_name_or_path {model_name}\n            --train_file {data_dir}/train.json\n            --validation_file {data_dir}/val.json\n            --output_dir {output_dir}\n            --overwrite_output_dir\n            --max_source_length {max_len}\n            --max_target_length {max_len}\n            --val_max_target_length {max_len}\n            --warmup_steps 8\n            --predict_with_generate\n            --save_steps 0\n            --eval_steps {eval_steps}\n            --group_by_length\n            --label_smoothing_factor 0.1\n            --source_lang en\n            --target_lang ro\n            --report_to none\n        \"\"\".split()\n        args.extend([\"--source_prefix\", '\"translate English to Romanian: \"'])\n\n        if not fp32:\n            args.extend([f\"--{dtype}\"])\n\n        actions = 0\n        if do_train:\n            actions += 1\n            args.extend(\n                f\"\"\"\n            --do_train\n            --num_train_epochs {str(num_train_epochs)}\n            --max_train_samples 16\n            --per_device_train_batch_size 2\n            --learning_rate 3e-3\n            \"\"\".split()\n            )\n\n        if do_eval:\n            actions += 1\n            args.extend(\n                \"\"\"\n            --do_eval\n            --max_eval_samples 16\n            --per_device_eval_batch_size 2\n            \"\"\".split()\n            )\n\n        assert actions > 0, \"need at least do_train or do_eval for the test to run\"\n\n        if extra_args_str is not None:\n            args.extend(extra_args_str.split())\n\n        # currently only works for bool args\n        if remove_args_str is not None:\n            remove_args = remove_args_str.split()\n            args = [x for x in args if x not in remove_args]\n\n        ds_args = f\"--deepspeed {self.test_file_dir_str}/ds_config_{stage}.json\".split()\n        script = [f\"{self.examples_dir_str}/pytorch/translation/run_translation.py\"]\n        launcher = get_launcher(distributed)\n\n        cmd = launcher + script + args + ds_args\n        # keep for quick debug\n        # print(\" \".join([f\"\\nPYTHONPATH={self.src_dir_str}\"] +cmd)); die\n        execute_subprocess_async(cmd, env=self.get_env())\n\n        return output_dir\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    def test_clm(self, stage, dtype):\n        # this test exercises model.resize_token_embeddings() which requires param gathering outside\n        # of forward - it's not used by `run_translation.py`, but it is in `run_clm.py`\n\n        data_dir = self.tests_dir / \"fixtures\"\n        output_dir = self.get_auto_remove_tmp_dir()\n        args = f\"\"\"\n            --model_name_or_path {GPT2_TINY}\n            --train_file {data_dir}/sample_text.txt\n            --validation_file {data_dir}/sample_text.txt\n            --output_dir {output_dir}\n            --overwrite_output_dir\n            --do_train\n            --do_eval\n            --max_train_samples 16\n            --max_eval_samples 16\n            --per_device_train_batch_size 2\n            --per_device_eval_batch_size 2\n            --num_train_epochs 1\n            --warmup_steps 8\n            --block_size 64\n            --report_to none\n            \"\"\".split()\n\n        args.extend([f\"--{dtype}\"])\n\n        ds_args = f\"--deepspeed {self.test_file_dir_str}/ds_config_{stage}.json\".split()\n        script = [f\"{self.examples_dir_str}/pytorch/language-modeling/run_clm.py\"]\n        launcher = get_launcher(distributed=True)\n\n        cmd = launcher + script + args + ds_args\n        # keep for quick debug\n        # print(\" \".join([f\"\\nPYTHONPATH={self.src_dir_str}\"] +cmd)); die\n        execute_subprocess_async(cmd, env=self.get_env())\n\n    @require_torch_fp16\n    def test_clm_from_config_zero3_fp16(self):\n        # this test exercises AutoModel.from_config(config) - to ensure zero.Init is called\n\n        data_dir = self.tests_dir / \"fixtures\"\n        output_dir = self.get_auto_remove_tmp_dir()\n        args = f\"\"\"\n            --model_type gpt2\n            --tokenizer_name {GPT2_TINY}\n            --train_file {data_dir}/sample_text.txt\n            --validation_file {data_dir}/sample_text.txt\n            --output_dir {output_dir}\n            --overwrite_output_dir\n            --do_train\n            --max_train_samples 4\n            --per_device_train_batch_size 2\n            --num_train_epochs 1\n            --warmup_steps 8\n            --block_size 8\n            --fp16\n            --report_to none\n            \"\"\".split()\n\n        ds_args = f\"--deepspeed {self.test_file_dir_str}/ds_config_zero3.json\".split()\n        script = [f\"{self.examples_dir_str}/pytorch/language-modeling/run_clm.py\"]\n        launcher = get_launcher(distributed=True)\n\n        cmd = launcher + script + args + ds_args\n        # keep for quick debug\n        # print(\" \".join([f\"\\nPYTHONPATH={self.src_dir_str}\"] +cmd)); die\n        with CaptureStderr() as cs:\n            execute_subprocess_async(cmd, env=self.get_env())\n        self.assertIn(\"Detected DeepSpeed ZeRO-3\", cs.err)\n"}
{"type": "test_file", "path": "transformers/tests/agents/test_monitoring.py", "content": "# coding=utf-8\n# Copyright 2024 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom transformers.agents.agent_types import AgentImage\nfrom transformers.agents.agents import AgentError, ReactCodeAgent, ReactJsonAgent\nfrom transformers.agents.monitoring import stream_to_gradio\n\n\nclass MonitoringTester(unittest.TestCase):\n    def test_code_agent_metrics(self):\n        class FakeLLMEngine:\n            def __init__(self):\n                self.last_input_token_count = 10\n                self.last_output_token_count = 20\n\n            def __call__(self, prompt, **kwargs):\n                return \"\"\"\nCode:\n```py\nfinal_answer('This is the final answer.')\n```\"\"\"\n\n        agent = ReactCodeAgent(\n            tools=[],\n            llm_engine=FakeLLMEngine(),\n            max_iterations=1,\n        )\n\n        agent.run(\"Fake task\")\n\n        self.assertEqual(agent.monitor.total_input_token_count, 10)\n        self.assertEqual(agent.monitor.total_output_token_count, 20)\n\n    def test_json_agent_metrics(self):\n        class FakeLLMEngine:\n            def __init__(self):\n                self.last_input_token_count = 10\n                self.last_output_token_count = 20\n\n            def __call__(self, prompt, **kwargs):\n                return 'Action:{\"action\": \"final_answer\", \"action_input\": {\"answer\": \"image\"}}'\n\n        agent = ReactJsonAgent(\n            tools=[],\n            llm_engine=FakeLLMEngine(),\n            max_iterations=1,\n        )\n\n        agent.run(\"Fake task\")\n\n        self.assertEqual(agent.monitor.total_input_token_count, 10)\n        self.assertEqual(agent.monitor.total_output_token_count, 20)\n\n    def test_code_agent_metrics_max_iterations(self):\n        class FakeLLMEngine:\n            def __init__(self):\n                self.last_input_token_count = 10\n                self.last_output_token_count = 20\n\n            def __call__(self, prompt, **kwargs):\n                return \"Malformed answer\"\n\n        agent = ReactCodeAgent(\n            tools=[],\n            llm_engine=FakeLLMEngine(),\n            max_iterations=1,\n        )\n\n        agent.run(\"Fake task\")\n\n        self.assertEqual(agent.monitor.total_input_token_count, 20)\n        self.assertEqual(agent.monitor.total_output_token_count, 40)\n\n    def test_code_agent_metrics_generation_error(self):\n        class FakeLLMEngine:\n            def __init__(self):\n                self.last_input_token_count = 10\n                self.last_output_token_count = 20\n\n            def __call__(self, prompt, **kwargs):\n                raise AgentError\n\n        agent = ReactCodeAgent(\n            tools=[],\n            llm_engine=FakeLLMEngine(),\n            max_iterations=1,\n        )\n\n        agent.run(\"Fake task\")\n\n        self.assertEqual(agent.monitor.total_input_token_count, 20)\n        self.assertEqual(agent.monitor.total_output_token_count, 40)\n\n    def test_streaming_agent_text_output(self):\n        def dummy_llm_engine(prompt, **kwargs):\n            return \"\"\"\nCode:\n```py\nfinal_answer('This is the final answer.')\n```\"\"\"\n\n        agent = ReactCodeAgent(\n            tools=[],\n            llm_engine=dummy_llm_engine,\n            max_iterations=1,\n        )\n\n        # Use stream_to_gradio to capture the output\n        outputs = list(stream_to_gradio(agent, task=\"Test task\", test_mode=True))\n\n        self.assertEqual(len(outputs), 3)\n        final_message = outputs[-1]\n        self.assertEqual(final_message.role, \"assistant\")\n        self.assertIn(\"This is the final answer.\", final_message.content)\n\n    def test_streaming_agent_image_output(self):\n        def dummy_llm_engine(prompt, **kwargs):\n            return 'Action:{\"action\": \"final_answer\", \"action_input\": {\"answer\": \"image\"}}'\n\n        agent = ReactJsonAgent(\n            tools=[],\n            llm_engine=dummy_llm_engine,\n            max_iterations=1,\n        )\n\n        # Use stream_to_gradio to capture the output\n        outputs = list(stream_to_gradio(agent, task=\"Test task\", image=AgentImage(value=\"path.png\"), test_mode=True))\n\n        self.assertEqual(len(outputs), 2)\n        final_message = outputs[-1]\n        self.assertEqual(final_message.role, \"assistant\")\n        self.assertIsInstance(final_message.content, dict)\n        self.assertEqual(final_message.content[\"path\"], \"path.png\")\n        self.assertEqual(final_message.content[\"mime_type\"], \"image/png\")\n\n    def test_streaming_with_agent_error(self):\n        def dummy_llm_engine(prompt, **kwargs):\n            raise AgentError(\"Simulated agent error\")\n\n        agent = ReactCodeAgent(\n            tools=[],\n            llm_engine=dummy_llm_engine,\n            max_iterations=1,\n        )\n\n        # Use stream_to_gradio to capture the output\n        outputs = list(stream_to_gradio(agent, task=\"Test task\", test_mode=True))\n\n        self.assertEqual(len(outputs), 3)\n        final_message = outputs[-1]\n        self.assertEqual(final_message.role, \"assistant\")\n        self.assertIn(\"Simulated agent error\", final_message.content)\n"}
{"type": "test_file", "path": "transformers/tests/bettertransformer/__init__.py", "content": ""}
{"type": "test_file", "path": "transformers/tests/agents/test_tools_common.py", "content": "# coding=utf-8\n# Copyright 2024 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport unittest\nfrom pathlib import Path\nfrom typing import Dict, Union\n\nimport numpy as np\nimport pytest\n\nfrom transformers import is_torch_available, is_vision_available\nfrom transformers.agents.agent_types import AGENT_TYPE_MAPPING, AgentAudio, AgentImage, AgentText\nfrom transformers.agents.tools import Tool, tool\nfrom transformers.testing_utils import get_tests_dir, is_agent_test\n\n\nif is_torch_available():\n    import torch\n\nif is_vision_available():\n    from PIL import Image\n\n\nAUTHORIZED_TYPES = [\"string\", \"boolean\", \"integer\", \"number\", \"audio\", \"image\", \"any\"]\n\n\ndef create_inputs(tool_inputs: Dict[str, Dict[Union[str, type], str]]):\n    inputs = {}\n\n    for input_name, input_desc in tool_inputs.items():\n        input_type = input_desc[\"type\"]\n\n        if input_type == \"string\":\n            inputs[input_name] = \"Text input\"\n        elif input_type == \"image\":\n            inputs[input_name] = Image.open(\n                Path(get_tests_dir(\"fixtures/tests_samples/COCO\")) / \"000000039769.png\"\n            ).resize((512, 512))\n        elif input_type == \"audio\":\n            inputs[input_name] = np.ones(3000)\n        else:\n            raise ValueError(f\"Invalid type requested: {input_type}\")\n\n    return inputs\n\n\ndef output_type(output):\n    if isinstance(output, (str, AgentText)):\n        return \"string\"\n    elif isinstance(output, (Image.Image, AgentImage)):\n        return \"image\"\n    elif isinstance(output, (torch.Tensor, AgentAudio)):\n        return \"audio\"\n    else:\n        raise TypeError(f\"Invalid output: {output}\")\n\n\n@is_agent_test\nclass ToolTesterMixin:\n    def test_inputs_output(self):\n        self.assertTrue(hasattr(self.tool, \"inputs\"))\n        self.assertTrue(hasattr(self.tool, \"output_type\"))\n\n        inputs = self.tool.inputs\n        self.assertTrue(isinstance(inputs, dict))\n\n        for _, input_spec in inputs.items():\n            self.assertTrue(\"type\" in input_spec)\n            self.assertTrue(\"description\" in input_spec)\n            self.assertTrue(input_spec[\"type\"] in AUTHORIZED_TYPES)\n            self.assertTrue(isinstance(input_spec[\"description\"], str))\n\n        output_type = self.tool.output_type\n        self.assertTrue(output_type in AUTHORIZED_TYPES)\n\n    def test_common_attributes(self):\n        self.assertTrue(hasattr(self.tool, \"description\"))\n        self.assertTrue(hasattr(self.tool, \"name\"))\n        self.assertTrue(hasattr(self.tool, \"inputs\"))\n        self.assertTrue(hasattr(self.tool, \"output_type\"))\n\n    def test_agent_type_output(self):\n        inputs = create_inputs(self.tool.inputs)\n        output = self.tool(**inputs)\n        if self.tool.output_type != \"any\":\n            agent_type = AGENT_TYPE_MAPPING[self.tool.output_type]\n            self.assertTrue(isinstance(output, agent_type))\n\n    def test_agent_types_inputs(self):\n        inputs = create_inputs(self.tool.inputs)\n        _inputs = []\n        for _input, expected_input in zip(inputs, self.tool.inputs.values()):\n            input_type = expected_input[\"type\"]\n            _inputs.append(AGENT_TYPE_MAPPING[input_type](_input))\n\n\nclass ToolTests(unittest.TestCase):\n    def test_tool_init_with_decorator(self):\n        @tool\n        def coolfunc(a: str, b: int) -> float:\n            \"\"\"Cool function\n\n            Args:\n                a: The first argument\n                b: The second one\n            \"\"\"\n            return b + 2, a\n\n        assert coolfunc.output_type == \"number\"\n\n    def test_tool_init_vanilla(self):\n        class HFModelDownloadsTool(Tool):\n            name = \"model_download_counter\"\n            description = \"\"\"\n            This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.\n            It returns the name of the checkpoint.\"\"\"\n\n            inputs = {\n                \"task\": {\n                    \"type\": \"string\",\n                    \"description\": \"the task category (such as text-classification, depth-estimation, etc)\",\n                }\n            }\n            output_type = \"integer\"\n\n            def forward(self, task):\n                return \"best model\"\n\n        tool = HFModelDownloadsTool()\n        assert list(tool.inputs.keys())[0] == \"task\"\n\n    def test_tool_init_decorator_raises_issues(self):\n        with pytest.raises(Exception) as e:\n\n            @tool\n            def coolfunc(a: str, b: int):\n                \"\"\"Cool function\n\n                Args:\n                    a: The first argument\n                    b: The second one\n                \"\"\"\n                return a + b\n\n            assert coolfunc.output_type == \"number\"\n        assert \"Tool return type not found\" in str(e)\n\n        with pytest.raises(Exception) as e:\n\n            @tool\n            def coolfunc(a: str, b: int) -> int:\n                \"\"\"Cool function\n\n                Args:\n                    a: The first argument\n                \"\"\"\n                return b + a\n\n            assert coolfunc.output_type == \"number\"\n        assert \"docstring has no description for the argument\" in str(e)\n"}
{"type": "test_file", "path": "transformers/tests/deepspeed/test_model_zoo.py", "content": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport itertools\nimport os\nimport subprocess\nfrom os.path import dirname\n\nfrom parameterized import parameterized\n\nfrom tests.trainer.test_trainer import TrainerIntegrationCommon  # noqa\nfrom transformers import is_torch_available\nfrom transformers.testing_utils import (\n    TestCasePlus,\n    backend_device_count,\n    execute_subprocess_async,\n    get_tests_dir,\n    require_deepspeed,\n    require_torch_accelerator,\n    slow,\n    torch_device,\n)\nfrom transformers.trainer_utils import set_seed\n\n\nif is_torch_available():\n    from tests.trainer.test_trainer import (  # noqa\n        RegressionModelConfig,\n        RegressionPreTrainedModel,\n        get_regression_trainer,\n    )\n\n\nset_seed(42)\n\nFIXTURE_DIRECTORY = get_tests_dir(\"fixtures\")\nROOT_DIRECTORY = os.path.join(dirname(get_tests_dir()))\nDS_TESTS_DIRECTORY = dirname(os.path.abspath(__file__))\n\n# default torch.distributed port\nDEFAULT_MASTER_PORT = \"10999\"\n\nT5_SMALL = \"google-t5/t5-small\"\n\n# *** Working Models ***\nALBERT_TINY = \"hf-internal-testing/tiny-albert\"\nBART_TINY = \"sshleifer/bart-tiny-random\"\nBERT_TINY = \"hf-internal-testing/tiny-bert\"\nBIGBIRD_PEGASUS_TINY = \"hf-internal-testing/tiny-random-bigbird_pegasus\"\nBIG_BIRD_TINY = \"hf-internal-testing/tiny-random-big_bird\"\nBLENDERBOT_TINY = \"hf-internal-testing/tiny-random-blenderbot\"\nBLOOM_TINY = \"bigscience/bigscience-small-testing\"\nDEBERTA_TINY = \"hf-internal-testing/tiny-random-deberta\"\nDEBERTA_V2_TINY = \"hf-internal-testing/tiny-random-deberta-v2\"\nDISTILBERT_TINY = \"sshleifer/tiny-distilbert-base-cased\"\nELECTRA_TINY = \"hf-internal-testing/tiny-electra\"\nFLAUBERT_TINY = \"hf-internal-testing/tiny-random-flaubert\"\nFSMT_TINY = \"stas/tiny-wmt19-en-de\"\nFUNNEL_TINY = \"hf-internal-testing/tiny-random-funnel\"\nGPT2_TINY = \"sshleifer/tiny-gpt2\"\nGPTJ_TINY = \"hf-internal-testing/tiny-random-gptj\"\nGPT_NEO_TINY = \"hf-internal-testing/tiny-random-gpt_neo\"\nLAYOUTLM_TINY = \"hf-internal-testing/tiny-layoutlm\"\nLED_TINY = \"hf-internal-testing/tiny-random-led\"\nLONGFORMER_TINY = \"hf-internal-testing/tiny-random-longformer\"\nM2M_100_TINY = \"stas/tiny-m2m_100\"  # hf tiny model is unsuitable\nMARIAN_TINY = \"sshleifer/tiny-marian-en-de\"\nMBART_TINY = \"sshleifer/tiny-mbart\"\nMOBILEBERT_TINY = \"hf-internal-testing/tiny-random-mobilebert\"\nMPNET_TINY = \"hf-internal-testing/tiny-random-mpnet\"\nPEGASUS_TINY = \"stas/pegasus-cnn_dailymail-tiny-random\"\nPROPHETNET_TINY = \"hf-internal-testing/tiny-random-prophetnet\"\nROBERTA_TINY = \"sshleifer/tiny-distilroberta-base\"\nSQUEEZEBERT_TINY = \"hf-internal-testing/tiny-random-squeezebert\"\nT5_TINY = \"patrickvonplaten/t5-tiny-random\"\nT5_V1_TINY = \"hf-internal-testing/tiny-random-t5-v1.1\"\nVIT_TINY = \"hf-internal-testing/tiny-random-vit\"\nXLM_ROBERTA_TINY = \"hf-internal-testing/tiny-xlm-roberta\"\nXLNET_TINY = \"sshleifer/tiny-xlnet-base-cased\"\n\n\n# *** To Fix ***\n\n\n# *** tiny model issues ***\n# missing model files:\nMT5_TINY = \"hf-internal-testing/tiny-random-mt5\"\nCAMEMBERT_TINY = \"hf-internal-testing/tiny-random-camembert\"\nOPENAI_GPT_TINY = \"hf-internal-testing/tiny-random-openai-gpt\"\n\n# missing tokenizer files\nCONVBERT_TINY = \"hf-internal-testing/tiny-random-convbert\"\nLAYOUTLMV2_TINY = \"hf-internal-testing/tiny-random-layoutlmv2\"\nHUBERT_TINY = \"hf-internal-testing/tiny-random-hubert\"\n\n# issues with tokenizer\nCTRL_TINY = \"hf-internal-testing/tiny-random-ctrl\"\nTRANSFO_XL_TINY = \"hf-internal-testing/tiny-random-transfo-xl\"  # same as Salesforce/ctrl\n\n# other issues with tiny models\nIBERT_TINY = \"hf-internal-testing/tiny-random-ibert\"  # multiple issues with either mlm/qa/clas\nREFORMER_TINY = \"hf-internal-testing/tiny-random-reformer\"  # multiple issues with either mlm/qa/clas\n\n# *** Lacking official examples to test with ***\n# or not working with examples\nDPR_TINY = \"hf-internal-testing/tiny-random-dpr\"\n# - \"dpr\"  examples/research_projects/rag-end2end-retriever/\nRAG_TINY = \"hf-internal-testing/tiny-random-rag\"\n# - \"rag\" research_projects\nLUKE_TINY = \"\"\n# - \"luke\" Entities classes - no plan to make such example\nLXMERT_TINY = \"hf-internal-testing/tiny-random-lxmert\"\n# - \"lxmert\" doesn't work with run_qa.py\nCLIP_TINY = \"hf-internal-testing/tiny-random-clip\"\n# - \"clip\" nothing under pytorch examples - XXX: Suraj is working on adding some - check by end of Sep\nSPEECH_TO_TEXT_TINY = \"hf-internal-testing/tiny-random-speech_to_text\"\n# - \"speech_to_text\", nothing under pytorch examples\n\n\n# *** Reactive mode ***\n# models with low usage, unstable API, things about to change - do nothing about the following until someone runs into a problem\nTAPAS_TINY = \"hf-internal-testing/tiny-random-tapas\"\n# additional notes on tapas\n# 1. \"Table must be of type pd.DataFrame\" failure\n\n\n# TODO: new models to add:\n#\n\n\ndef get_launcher(distributed=False):\n    # 1. explicitly set --num_nodes=1 just in case these tests end up run on a multi-node setup\n    # - it won't be able to handle that\n    # 2. for now testing with just 2 gpus max (since some quality tests may give different\n    # results with mode gpus because we use very little data)\n    num_gpus = min(2, backend_device_count(torch_device)) if distributed else 1\n    master_port = os.environ.get(\"DS_TEST_PORT\", DEFAULT_MASTER_PORT)\n    return f\"deepspeed --num_nodes 1 --num_gpus {num_gpus} --master_port {master_port}\".split()\n\n\ndef make_task_cmds():\n    data_dir_samples = f\"{FIXTURE_DIRECTORY}/tests_samples\"\n    data_dir_wmt = f\"{data_dir_samples}/wmt_en_ro\"\n    data_dir_xsum = f\"{data_dir_samples}/xsum\"\n    args_main = \"\"\"\n        --do_train\n        --max_train_samples 4\n        --per_device_train_batch_size 2\n        --num_train_epochs 1\n        --fp16\n        --report_to none\n        --overwrite_output_dir\n        \"\"\".split()\n\n    # try to cover as many models as possible once (it's enough to run on one task per model)\n    # but need a tiny model for each\n    #\n    # should have \"{model_type.upper()}_TINY\" corresponding vars defined, e.g., T5_TINY, etc.\n    tasks2models = {\n        \"trans\": [\n            \"bart\",\n            \"fsmt\",\n            \"m2m_100\",\n            \"marian\",\n            \"mbart\",\n            \"t5\",\n            \"t5_v1\",\n            # \"mt5\", missing model files\n        ],\n        \"sum\": [\n            \"pegasus\",\n        ],\n        \"clm\": [\n            \"big_bird\",\n            \"bigbird_pegasus\",\n            \"blenderbot\",\n            \"bloom\",\n            \"gpt2\",\n            \"gpt_neo\",\n            \"gptj\",\n            \"xlm-roberta\",\n            \"prophetnet\",\n            # \"camembert\", missing model files\n        ],\n        \"mlm\": [\n            \"albert\",\n            \"deberta\",\n            \"deberta-v2\",\n            \"distilbert\",\n            \"electra\",\n            \"flaubert\",\n            \"funnel\",\n            \"layoutlm\",\n            # \"reformer\", # multiple issues with either mlm/qa/clas\n        ],\n        \"qa\": [\n            \"led\",\n            \"longformer\",\n            \"mobilebert\",\n            \"mpnet\",\n            \"roberta\",\n            \"squeezebert\",\n            # \"convbert\", # missing tokenizer files\n            # \"layoutlmv2\", missing model files\n        ],\n        \"clas\": [\n            \"bert\",\n            \"xlnet\",\n            # \"hubert\", # missing tokenizer files\n            # \"ibert\", # multiple issues with either mlm/qa/clas\n            # \"transfo-xl\", # tokenizer issues as Salesforce/ctrl\n            # \"Salesforce/ctrl\", # tokenizer issues\n            # \"openai-community/openai-gpt\", missing model files\n            # \"tapas\", multiple issues\n        ],\n        \"img_clas\": [\n            \"vit\",\n        ],\n    }\n\n    scripts_dir = f\"{ROOT_DIRECTORY}/examples/pytorch\"\n\n    tasks = {\n        \"trans\": f\"\"\"\n        {scripts_dir}/translation/run_translation.py\n        --train_file {data_dir_wmt}/train.json\n        --source_lang en\n        --target_lang ro\n        --max_source_length 12\n        --max_target_length 12\n        \"\"\",\n        \"sum\": f\"\"\"\n        {scripts_dir}/summarization/run_summarization.py\n        --train_file {data_dir_xsum}/sample.json\n        --max_source_length 12\n        --max_target_length 12\n        --lang en\n        \"\"\",\n        \"clm\": f\"\"\"\n        {scripts_dir}/language-modeling/run_clm.py\n        --train_file {FIXTURE_DIRECTORY}/sample_text.txt\n        --block_size 8\n        \"\"\",\n        \"mlm\": f\"\"\"\n        {scripts_dir}/language-modeling/run_mlm.py\n        --train_file {FIXTURE_DIRECTORY}/sample_text.txt\n        \"\"\",\n        \"qa\": f\"\"\"\n        {scripts_dir}/question-answering/run_qa.py\n        --train_file {data_dir_samples}/SQUAD/sample.json\n        \"\"\",\n        \"clas\": f\"\"\"\n        {scripts_dir}/text-classification/run_glue.py\n        --train_file {data_dir_samples}/MRPC/train.csv\n        --max_seq_length 12\n        --task_name MRPC\n        \"\"\",\n        \"img_clas\": f\"\"\"\n        {scripts_dir}/image-classification/run_image_classification.py\n            --dataset_name hf-internal-testing/cats_vs_dogs_sample\n            --trust_remote_code\n            --remove_unused_columns False\n            --max_steps 10\n            --image_processor_name {DS_TESTS_DIRECTORY}/vit_feature_extractor.json\n            --label_column_name labels\n        \"\"\",\n    }\n\n    launcher = get_launcher(distributed=True)\n\n    cmds = {}\n    for task, args in tasks.items():\n        args = args.split()\n        for model in tasks2models[task]:\n            model_name = globals()[f\"{model.upper().replace('-', '_')}_TINY\"]\n            args_model = f\"--model_name_or_path {model_name}\".split()\n            cmds[f\"{task}_{model}\"] = launcher + args + args_model + args_main\n\n            # # generation special case\n            # if task == \"gen\":\n            #     launcher = f\"deepspeed --num_nodes 1 --num_gpus 1\".split()\n            #     args_model += f\"--model_type {model}\".split()\n            #     cmds[f\"{task}_{model}\"] = launcher + args + args_model\n            # else:\n\n    return cmds\n\n\ntask_cmds = make_task_cmds()\n\nZERO2 = \"zero2\"\nZERO3 = \"zero3\"\n\nstages = [ZERO2, ZERO3]\n\n# future preparation:\n# for now test just fp16, as these tests are quite slow\n# FP16 = \"fp16\"\n# BF16 = \"bf16\"\n#\n# dtypes = [FP16]\n# so just hardcoding --fp16 for now\n# if is_torch_bf16_gpu_available():\n#     dtypes += [BF16]\n\n\ndef parameterized_custom_name_func(func, param_num, param):\n    # customize the test name generator function as we want both params to appear in the sub-test\n    # name, as by default it shows only the first param\n    param_based_name = parameterized.to_safe_name(\"_\".join(str(x) for x in param.args))\n    return f\"{func.__name__}_{param_based_name}\"\n\n\n# Cartesian-product of zero stages with models to test\nparams = list(itertools.product(stages, task_cmds.keys()))\n\n\n@slow\n@require_deepspeed\n@require_torch_accelerator\nclass TestDeepSpeedModelZoo(TestCasePlus):\n    \"\"\"This class is for testing via an external script - can do multiple gpus\"\"\"\n\n    def get_task_cmd(self, task, stage):\n        # return a ready to run train cmd\n        if task not in task_cmds:\n            raise ValueError(f\"don't know of task {task}, have {task_cmds.keys()}\")\n\n        cmd = task_cmds[task]\n        args_ds = f\"--deepspeed {self.test_file_dir_str}/ds_config_{stage}.json\".split()\n\n        output_dir = self.get_auto_remove_tmp_dir()\n        args_out = f\"--output_dir {output_dir}\".split()\n\n        cmd += args_ds + args_out\n\n        return cmd, output_dir\n\n    @parameterized.expand(params, name_func=parameterized_custom_name_func)\n    def test_zero_to_fp32(self, stage, task):\n        # testing the ability to do a run followed by recovery of full fp32 weights\n\n        cmd, output_dir = self.get_task_cmd(task, stage)\n\n        # 1. generate the checkpoint\n        cmd += \"--save_steps 1\".split()\n        # keep for quick debug\n        # print(\" \".join([f\"\\nPYTHONPATH={self.src_dir_str}\"] + cmd)); die\n        execute_subprocess_async(cmd, env=self.get_env())\n\n        # 2. test that the fp32 weights get reconsolidated\n        chkpt_dir = f\"{output_dir}/checkpoint-1\"\n        recovered_model_path = f\"{chkpt_dir}/out.bin\"\n        cmd = f\"{chkpt_dir}/zero_to_fp32.py {chkpt_dir} {recovered_model_path}\"\n        # keep for quick debug\n        # print(\" \".join([f\"\\nPYTHONPATH={self.src_dir_str}\"] +cmd)); die\n        subprocess.check_call(cmd, shell=True)\n        assert os.path.exists(recovered_model_path), f\"{recovered_model_path} was not found\"\n\n        # possibly could also test that the resulting saved model is usable but given that we use\n        # random models we won't know if it's any good\n"}
{"type": "test_file", "path": "transformers/tests/agents/test_speech_to_text.py", "content": "# coding=utf-8\n# Copyright 2023 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nimport numpy as np\n\nfrom transformers import load_tool\n\nfrom .test_tools_common import ToolTesterMixin\n\n\nclass SpeechToTextToolTester(unittest.TestCase, ToolTesterMixin):\n    def setUp(self):\n        self.tool = load_tool(\"speech_to_text\")\n        self.tool.setup()\n\n    def test_exact_match_arg(self):\n        result = self.tool(np.ones(3000))\n        self.assertEqual(result, \" Thank you.\")\n\n    def test_exact_match_kwarg(self):\n        result = self.tool(audio=np.ones(3000))\n        self.assertEqual(result, \" Thank you.\")\n"}
{"type": "test_file", "path": "transformers/tests/extended/test_trainer_ext.py", "content": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport os\nimport re\nimport sys\nfrom pathlib import Path\nfrom typing import Tuple\nfrom unittest.mock import patch\n\nfrom parameterized import parameterized\n\nfrom transformers.testing_utils import (\n    CaptureStderr,\n    ExtendSysPath,\n    TestCasePlus,\n    backend_device_count,\n    execute_subprocess_async,\n    get_torch_dist_unique_port,\n    require_apex,\n    require_bitsandbytes,\n    require_non_xpu,\n    require_torch,\n    require_torch_gpu,\n    require_torch_multi_accelerator,\n    require_torch_non_multi_accelerator,\n    slow,\n    torch_device,\n)\nfrom transformers.trainer_callback import TrainerState\nfrom transformers.trainer_utils import set_seed\n\n\nbindir = os.path.abspath(os.path.dirname(__file__))\nwith ExtendSysPath(f\"{bindir}/../../examples/pytorch/translation\"):\n    from run_translation import main  # noqa\n\n\nset_seed(42)\nMARIAN_MODEL = \"sshleifer/student_marian_en_ro_6_1\"\nMBART_TINY = \"sshleifer/tiny-mbart\"\n\n\n@require_torch\nclass TestTrainerExt(TestCasePlus):\n    def run_seq2seq_quick(\n        self,\n        distributed=False,\n        extra_args_str=None,\n        predict_with_generate=True,\n        do_train=True,\n        do_eval=True,\n        do_predict=True,\n        n_gpus_to_use=None,\n    ):\n        output_dir = self.run_trainer(\n            eval_steps=1,\n            max_len=12,\n            model_name=MBART_TINY,\n            num_train_epochs=1,\n            distributed=distributed,\n            extra_args_str=extra_args_str,\n            predict_with_generate=predict_with_generate,\n            do_train=do_train,\n            do_eval=do_eval,\n            do_predict=do_predict,\n            n_gpus_to_use=n_gpus_to_use,\n        )\n        logs = TrainerState.load_from_json(os.path.join(output_dir, \"trainer_state.json\")).log_history\n\n        if not do_eval:\n            self.skipTest(reason=\"do_eval is False\")\n\n        eval_metrics = [log for log in logs if \"eval_loss\" in log.keys()]\n\n        first_step_stats = eval_metrics[0]\n        if predict_with_generate:\n            assert \"eval_bleu\" in first_step_stats\n\n            last_step_stats = eval_metrics[-1]\n            assert isinstance(last_step_stats[\"eval_bleu\"], float)\n            assert not math.isnan(float(last_step_stats[\"eval_loss\"])), \"eval_loss must not be `nan`\"\n\n    @require_torch_non_multi_accelerator\n    def test_run_seq2seq_no_dist(self):\n        self.run_seq2seq_quick()\n\n    # verify that the trainer can handle non-distributed with n_gpu > 1\n    @require_torch_multi_accelerator\n    def test_run_seq2seq_dp(self):\n        self.run_seq2seq_quick(distributed=False)\n\n    # verify that the trainer can handle distributed with n_gpu > 1\n    @require_torch_multi_accelerator\n    def test_run_seq2seq_ddp(self):\n        self.run_seq2seq_quick(distributed=True)\n\n    @require_non_xpu\n    @require_apex\n    @require_torch_gpu\n    def test_run_seq2seq_apex(self):\n        # XXX: apex breaks the trainer if it's run twice e.g. run_seq2seq.main() from the same\n        # program and it breaks other tests that run from the same pytest worker, therefore until this is\n        # sorted out it must be run only in an external program, that is distributed=True in this\n        # test and only under one or more gpus - if we want cpu will need to make a special test\n        #\n        # specifically to the problem traced it to self.optimizer.step() - if it's run 2nd time via\n        # 2nd main() call it botches the future eval.\n        #\n        self.run_seq2seq_quick(distributed=True, extra_args_str=\"--fp16 --fp16_backend=apex\")\n        # test 2nd time - was getting eval_loss': nan'\n        # to reproduce the problem set distributed=False\n        self.run_seq2seq_quick(distributed=True, extra_args_str=\"--fp16 --fp16_backend=apex\")\n\n    @parameterized.expand([\"base\", \"low\", \"high\", \"mixed\"])\n    @require_torch_multi_accelerator\n    def test_trainer_log_level_replica(self, experiment_id):\n        # as each sub-test is slow-ish split into multiple sub-tests to avoid CI timeout\n        experiments = {\n            # test with the default log_level - should be info and thus log info once\n            \"base\": {\"extra_args_str\": \"\", \"n_matches\": 1},\n            # test with low log_level and log_level_replica - should be noisy on all processes\n            # now the info string should appear twice on 2 processes\n            \"low\": {\"extra_args_str\": \"--log_level debug --log_level_replica debug\", \"n_matches\": 2},\n            # test with high log_level and low log_level_replica\n            # now the info string should appear once only on the replica\n            \"high\": {\"extra_args_str\": \"--log_level error --log_level_replica debug\", \"n_matches\": 1},\n            # test with high log_level and log_level_replica - should be quiet on all processes\n            \"mixed\": {\"extra_args_str\": \"--log_level error --log_level_replica error\", \"n_matches\": 0},\n        }\n\n        data = experiments[experiment_id]\n        kwargs = {\n            \"distributed\": True,\n            \"predict_with_generate\": False,\n            \"do_eval\": False,\n            \"do_predict\": False,\n            \"n_gpus_to_use\": 2,\n        }\n        log_info_string = \"Running training\"\n        with CaptureStderr() as cl:\n            self.run_seq2seq_quick(**kwargs, extra_args_str=data[\"extra_args_str\"])\n        n_matches = len(re.findall(log_info_string, cl.err))\n        self.assertEqual(n_matches, data[\"n_matches\"])\n\n    @slow\n    def test_run_seq2seq(self):\n        output_dir = self.run_trainer(\n            eval_steps=2,\n            max_len=128,\n            model_name=MARIAN_MODEL,\n            learning_rate=3e-4,\n            num_train_epochs=10,\n            distributed=False,\n        )\n\n        # Check metrics\n        logs = TrainerState.load_from_json(os.path.join(output_dir, \"trainer_state.json\")).log_history\n        eval_metrics = [log for log in logs if \"eval_loss\" in log.keys()]\n        first_step_stats = eval_metrics[0]\n        last_step_stats = eval_metrics[-1]\n\n        assert first_step_stats[\"eval_loss\"] > last_step_stats[\"eval_loss\"], \"model learned nothing\"\n        assert isinstance(last_step_stats[\"eval_bleu\"], float)\n\n        # test if do_predict saves generations and metrics\n        contents = os.listdir(output_dir)\n        contents = {os.path.basename(p) for p in contents}\n        assert \"generated_predictions.txt\" in contents\n        assert \"predict_results.json\" in contents\n\n    @slow\n    @require_bitsandbytes\n    def test_run_seq2seq_bnb(self):\n        from transformers.training_args import OptimizerNames\n\n        def train_and_return_metrics(optim: str) -> Tuple[int, float]:\n            extra_args = \"--skip_memory_metrics 0\"\n\n            output_dir = self.run_trainer(\n                max_len=128,\n                model_name=MARIAN_MODEL,\n                learning_rate=3e-4,\n                num_train_epochs=1,\n                optim=optim,\n                distributed=True,  # force run in a new process\n                extra_args_str=extra_args,\n                do_eval=False,\n                do_predict=False,\n                n_gpus_to_use=1,  # to allow deterministic fixed memory usage\n            )\n\n            # Check metrics\n            logs = TrainerState.load_from_json(Path(output_dir, \"trainer_state.json\")).log_history\n            gpu_peak_mem_mb = int(logs[0][\"train_mem_gpu_peaked_delta\"] / 2**20)\n            gpu_alloc_mem_mb = int(logs[0][\"train_mem_gpu_alloc_delta\"] / 2**20)\n\n            loss = logs[0][\"train_loss\"]\n            return gpu_peak_mem_mb, gpu_alloc_mem_mb, loss\n\n        gpu_peak_mem_orig, gpu_alloc_mem_orig, loss_orig = train_and_return_metrics(OptimizerNames.ADAMW_TORCH.value)\n        gpu_peak_mem_bnb, gpu_alloc_mem_bnb, loss_bnb = train_and_return_metrics(OptimizerNames.ADAMW_BNB.value)\n\n        gpu_alloc_mem_diff = gpu_alloc_mem_orig - gpu_alloc_mem_bnb\n\n        gpu_total_mem_orig = gpu_peak_mem_orig + gpu_alloc_mem_orig\n        gpu_total_mem_bnb = gpu_peak_mem_bnb + gpu_alloc_mem_bnb\n        gpu_total_mem_diff = gpu_total_mem_orig - gpu_total_mem_bnb\n\n        # sshleifer/student_marian_en_ro_6_1 has 54M parameter, 29M of which is `nn.Embedding` which\n        # doesn't get quantized and remains in fp32. Therefore we only have 25M parameters quantized\n        # in 2 bytes and the diff in optim memory usage is derived as so:\n        #\n        # - normal 25*8=~200MB (8 bytes per param)\n        # - bnb    25*2= ~50MB (2 bytes per param)\n        #\n        # Thus we should expect ~150MB total memory saved.\n        #\n        # Peak memory should be the same - the total should be different by about that same margin\n        #\n        # After leaving a small margin to accommodate for differences between gpus let's check\n        # that we have at least 120MB in savings\n        expected_savings = 120\n\n        # uncomment the following if this test starts failing - requires py38 for a new print feature\n        # gpu_peak_mem_diff = gpu_peak_mem_orig - gpu_peak_mem_bnb\n        # print(f\"{gpu_alloc_mem_orig=}MB {gpu_peak_mem_orig=}MB {gpu_alloc_mem_orig+gpu_peak_mem_orig=}MB\")\n        # print(f\" {gpu_alloc_mem_bnb=}MB  {gpu_peak_mem_bnb=}MB  {gpu_alloc_mem_bnb+gpu_peak_mem_bnb=}MB\")\n        # print(f\"{gpu_alloc_mem_diff=}MB\")\n        # print(f\"{gpu_peak_mem_diff=}MB\")\n        # print(f\"{gpu_total_mem_orig=}MB, {gpu_total_mem_bnb=}MB\")\n        # print(f\"{gpu_total_mem_diff=}MB, {gpu_total_mem_diff=}MB\")\n\n        self.assertGreater(\n            gpu_alloc_mem_diff,\n            expected_savings,\n            \"should use ~150MB less alloc gpu memory with BNB, compared to without it for this model but got\"\n            f\" a difference of {gpu_alloc_mem_diff}MB, with gpu_alloc_mem_orig={gpu_alloc_mem_orig}MB and\"\n            f\" gpu_alloc_mem_bnb={gpu_alloc_mem_bnb}MB\",\n        )\n\n        self.assertGreater(\n            gpu_total_mem_diff,\n            expected_savings,\n            \"should use ~150MB less total gpu memory with BNB, compared to without it for this model but got\"\n            f\" a difference of {gpu_total_mem_diff}MB, with gpu_total_mem_orig={gpu_total_mem_orig}MB and\"\n            f\" gpu_total_mem_bnb={gpu_total_mem_bnb}MB\",\n        )\n\n        self.assertEqual(\n            loss_orig, loss_bnb, f\"loss should be the same, but got loss_orig={loss_orig}, loss_bnb={loss_bnb}\"\n        )\n\n    def run_trainer(\n        self,\n        max_len: int,\n        model_name: str,\n        num_train_epochs: int,\n        learning_rate: float = 3e-3,\n        optim: str = \"adafactor\",\n        distributed: bool = False,\n        extra_args_str: str = None,\n        eval_steps: int = 0,\n        predict_with_generate: bool = True,\n        do_train: bool = True,\n        do_eval: bool = True,\n        do_predict: bool = True,\n        n_gpus_to_use: int = None,\n    ):\n        data_dir = self.test_file_dir / \"../fixtures/tests_samples/wmt_en_ro\"\n        output_dir = self.get_auto_remove_tmp_dir()\n        args_train = f\"\"\"\n            --model_name_or_path {model_name}\n            --train_file {data_dir}/train.json\n            --validation_file {data_dir}/val.json\n            --test_file {data_dir}/test.json\n            --output_dir {output_dir}\n            --overwrite_output_dir\n            --max_train_samples 8\n            --max_source_length {max_len}\n            --max_target_length {max_len}\n            --do_train\n            --num_train_epochs {str(num_train_epochs)}\n            --per_device_train_batch_size 4\n            --learning_rate {learning_rate}\n            --warmup_steps 8\n            --logging_steps 0\n            --logging_strategy no\n            --save_steps {str(eval_steps)}\n            --group_by_length\n            --label_smoothing_factor 0.1\n            --target_lang ro_RO\n            --source_lang en_XX\n            --report_to none\n        \"\"\".split()\n\n        args_eval = f\"\"\"\n            --do_eval\n            --per_device_eval_batch_size 4\n            --max_eval_samples 8\n            --val_max_target_length {max_len}\n            --eval_strategy steps\n            --eval_steps {str(eval_steps)}\n        \"\"\".split()\n\n        args_predict = \"\"\"\n            --do_predict\n        \"\"\".split()\n\n        args = []\n        if do_train:\n            args += args_train\n\n        if do_eval:\n            args += args_eval\n\n        if do_predict:\n            args += args_predict\n\n        if predict_with_generate:\n            args += \"--predict_with_generate\".split()\n\n        if do_train:\n            if optim == \"adafactor\":\n                args += \"--adafactor\".split()\n            else:\n                args += f\"--optim {optim}\".split()\n\n        if extra_args_str is not None:\n            args += extra_args_str.split()\n\n        if distributed:\n            if n_gpus_to_use is None:\n                n_gpus_to_use = backend_device_count(torch_device)\n            master_port = get_torch_dist_unique_port()\n            distributed_args = f\"\"\"\n                -m torch.distributed.run\n                --nproc_per_node={n_gpus_to_use}\n                --master_port={master_port}\n                {self.examples_dir_str}/pytorch/translation/run_translation.py\n            \"\"\".split()\n            cmd = [sys.executable] + distributed_args + args\n            # keep for quick debug\n            # print(\" \".join([f\"\\nPYTHONPATH={self.src_dir_str}\"] +cmd)); die\n            execute_subprocess_async(cmd, env=self.get_env())\n        else:\n            testargs = [\"run_translation.py\"] + args\n            with patch.object(sys, \"argv\", testargs):\n                main()\n\n        return output_dir\n"}
{"type": "test_file", "path": "transformers/tests/agents/test_translation.py", "content": "# coding=utf-8\n# Copyright 2024 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom transformers import load_tool\nfrom transformers.agents.agent_types import AGENT_TYPE_MAPPING\n\nfrom .test_tools_common import ToolTesterMixin, output_type\n\n\nclass TranslationToolTester(unittest.TestCase, ToolTesterMixin):\n    def setUp(self):\n        self.tool = load_tool(\"translation\")\n        self.tool.setup()\n        self.remote_tool = load_tool(\"translation\", remote=True)\n\n    def test_exact_match_arg(self):\n        result = self.tool(\"Hey, what's up?\", src_lang=\"English\", tgt_lang=\"French\")\n        self.assertEqual(result, \"- Hé, comment ça va?\")\n\n    def test_exact_match_kwarg(self):\n        result = self.tool(text=\"Hey, what's up?\", src_lang=\"English\", tgt_lang=\"French\")\n        self.assertEqual(result, \"- Hé, comment ça va?\")\n\n    def test_call(self):\n        inputs = [\"Hey, what's up?\", \"English\", \"Spanish\"]\n        output = self.tool(*inputs)\n\n        self.assertEqual(output_type(output), self.tool.output_type)\n\n    def test_agent_type_output(self):\n        inputs = [\"Hey, what's up?\", \"English\", \"Spanish\"]\n        output = self.tool(*inputs)\n        output_type = AGENT_TYPE_MAPPING[self.tool.output_type]\n        self.assertTrue(isinstance(output, output_type))\n\n    def test_agent_types_inputs(self):\n        example_inputs = {\n            \"text\": \"Hey, what's up?\",\n            \"src_lang\": \"English\",\n            \"tgt_lang\": \"Spanish\",\n        }\n\n        _inputs = []\n        for input_name in example_inputs.keys():\n            example_input = example_inputs[input_name]\n            input_description = self.tool.inputs[input_name]\n            input_type = input_description[\"type\"]\n            _inputs.append(AGENT_TYPE_MAPPING[input_type](example_input))\n\n        # Should not raise an error\n        output = self.tool(**example_inputs)\n        output_type = AGENT_TYPE_MAPPING[self.tool.output_type]\n        self.assertTrue(isinstance(output, output_type))\n"}
{"type": "source_file", "path": "demo.py", "content": "from operator import attrgetter\nfrom llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\nfrom llava.conversation import conv_templates, SeparatorStyle\nimport os\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\nimport requests\nimport copy\nimport warnings\nfrom decord import VideoReader, cpu\nwarnings.filterwarnings(\"ignore\")\n# Load the OneVision model\npretrained = \"llava-onevision-qwen2-7b-ov\"\n\nmodel_name = \"llava_qwen\"\ndevice = \"cuda\"\ndevice_map = \"auto\"\ntokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map, attn_implementation=\"sdpa\", torch_dtype=\"float16\")\n\nmodel.eval()\n\n# Function to extract frames from video\ndef load_video(video_path, max_frames_num):\n    if type(video_path) == str:\n        vr = VideoReader(video_path, ctx=cpu(0))\n    else:\n        vr = VideoReader(video_path[0], ctx=cpu(0))\n    total_frame_num = len(vr)\n    uniform_sampled_frames = np.linspace(0, total_frame_num - 1, max_frames_num, dtype=int)\n    frame_idx = uniform_sampled_frames.tolist()\n    spare_frames = vr.get_batch(frame_idx).asnumpy()\n    return spare_frames  # (frames, height, width, channels)\n\n\n# Load and process video\nvideo_path = \"YOUR_VIDEO_PATH\"\nvideo_frames = load_video(video_path, 32)\nprint(video_frames.shape) # (16, 1024, 576, 3)\n\nimage_tensors = []\nframes = image_processor.preprocess(video_frames, return_tensors=\"pt\")[\"pixel_values\"].half().cuda()\nimage_tensors.append(frames)\n\n# Prepare conversation input\nconv_template = \"qwen_1_5\"\nquestion = f\"{DEFAULT_IMAGE_TOKEN}\\nDescribe what's happening in this video.\"\nCUDA_LAUNCH_BLOCKING=1\nconv = copy.deepcopy(conv_templates[conv_template])\nconv.append_message(conv.roles[0], question)\nconv.append_message(conv.roles[1], None)\nprompt_question = conv.get_prompt()\n\n\ninput_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n# Create attention mask (1 for real tokens, 0 for padding)\nattention_mask = torch.ones_like(input_ids)\nimage_sizes = [frame.size for frame in video_frames]\n\noutputs = model.generate(\n    input_ids,\n    attention_mask=attention_mask,  # Add attention mask\n    images=image_tensors,\n    image_sizes=image_sizes,\n    do_sample=False,\n    temperature=0,\n    max_new_tokens=1000,\n    modalities=[\"video\"],   \n    output_attentions=False,\n    use_cache = True,\n    return_dict_in_generate=True,\n    output_hidden_states=True,\n    cache_implementation=\"quantized\",\n    cache_config={\"backend\": \"QuantizedCacheVLM\", \"nbits\": [1.5, 1.58], \"q_group_size\": 32, \"residual_length\": 128,\"axis_key\":-1, \"axis_value\":-1},\n)\n\ncont = outputs.sequences\ntext_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)\nprint(text_outputs[0])\n"}
{"type": "source_file", "path": "llava/constants.py", "content": "CONTROLLER_HEART_BEAT_EXPIRATION = 30\nWORKER_HEART_BEAT_INTERVAL = 15\n\nLOGDIR = \".\"\n\n# Model Constants\nIGNORE_INDEX = -100\nIMAGE_TOKEN_INDEX = -200\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\n"}
{"type": "source_file", "path": "llava/__init__.py", "content": "from .model import LlavaLlamaForCausalLM\n"}
{"type": "source_file", "path": "llava/conversation.py", "content": "import dataclasses\nfrom enum import auto, Enum\nfrom typing import List, Any, Dict, Union, Tuple\nimport re\nimport base64\nfrom io import BytesIO\nfrom PIL import Image\nfrom transformers import AutoTokenizer\n\n\nclass SeparatorStyle(Enum):\n    \"\"\"Different separator style.\"\"\"\n\n    SINGLE = auto()\n    TWO = auto()\n    MPT = auto()\n    PLAIN = auto()\n    CHATML = auto()\n    LLAMA_2 = auto()\n    LLAMA_3 = auto()\n    QWEN = auto()\n    GEMMA = auto()\n\n\n@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that keeps all conversation history.\"\"\"\n\n    system: str\n    roles: List[str]\n    messages: List[List[str]]\n    offset: int\n    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n    sep: str = \"###\"\n    sep2: str = None\n    version: str = \"Unknown\"\n\n    tokenizer_id: str = \"\"\n    tokenizer: Any = None\n    # Stop criteria (the default one is EOS token)\n    stop_str: Union[str, List[str]] = None\n    # Stops generation if meeting any token in this list\n    stop_token_ids: List[int] = None\n\n    skip_next: bool = False\n\n    def get_prompt(self):\n        messages = self.messages\n        if len(messages) > 0 and type(messages[0][1]) is tuple:\n            messages = self.messages.copy()\n            init_role, init_msg = messages[0].copy()\n            init_msg = init_msg[0]\n            if \"mmtag\" in self.version:\n                init_msg = init_msg.replace(\"<image>\", \"\").strip()\n                messages[0] = (init_role, init_msg)\n                messages.insert(0, (self.roles[0], \"<Image><image></Image>\"))\n                messages.insert(1, (self.roles[1], \"Received.\"))\n            elif not init_msg.startswith(\"<image>\"):\n                init_msg = init_msg.replace(\"<image>\", \"\").strip()\n                messages[0] = (init_role, \"<image>\\n\" + init_msg)\n            else:\n                messages[0] = (init_role, init_msg)\n\n        if self.sep_style == SeparatorStyle.SINGLE:\n            ret = self.system + self.sep\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + self.sep\n                else:\n                    ret += role + \":\"\n\n        elif self.sep_style == SeparatorStyle.TWO:\n            seps = [self.sep, self.sep2]\n            ret = self.system + seps[0]\n            for i, (role, message) in enumerate(messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n\n        elif self.sep_style == SeparatorStyle.CHATML:\n            ret = \"\" if self.system == \"\" else self.system + self.sep + \"\\n\"\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, images, _ = message\n                        message = \"<image>\" * len(images) + message\n                    ret += role + \"\\n\" + message + self.sep + \"\\n\"\n                else:\n                    ret += role + \"\\n\"\n            return ret\n\n        elif self.sep_style == SeparatorStyle.LLAMA_3:\n            if self.tokenizer is None:\n                raise ValueError(\"Llama 3 tokenizer is not available. Make sure you have the necessary permissions.\")\n            chat_template_messages = [{\"role\": \"system\", \"content\": self.system}]\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, images = message\n                        message = \"<image>\" * len(images) + message\n                    chat_template_messages.append({\"role\": role, \"content\": message})\n\n            # print(chat_template_messages)\n            return self.tokenizer.apply_chat_template(chat_template_messages, tokenize=False, add_generation_prompt=True)\n            # ret = \"\" if self.system == \"\" else self.system + self.sep + \"\\n\"\n            # for role, message in messages:\n            #     if message:\n            #         if type(message) is tuple:\n            #             message, images = message\n            #             message = \"<image>\" * len(images) + message\n            #         ret += role + \"\\n\" + message + self.sep + \"\\n\"\n            #     else:\n            #         ret += role + \"\\n\"\n            # return ret\n\n        elif self.sep_style == SeparatorStyle.MPT:\n            ret = self.system + self.sep\n            for role, message in messages:\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n\n        elif self.sep_style == SeparatorStyle.GEMMA:\n            ret = \"\"\n            for i, (role, message) in enumerate(messages):\n                assert role == self.roles[i % 2], \"Conversation should alternate user/assistant/user/assistant/...\"\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += role + message + self.sep\n                else:\n                    ret += role\n\n        elif self.sep_style == SeparatorStyle.LLAMA_2:\n            wrap_sys = lambda msg: f\"<<SYS>>\\n{msg}\\n<</SYS>>\\n\\n\" if len(msg) > 0 else msg\n            wrap_inst = lambda msg: f\"[INST] {msg} [/INST]\"\n            ret = \"\"\n\n            for i, (role, message) in enumerate(messages):\n                if i == 0:\n                    assert message, \"first message should not be none\"\n                    assert role == self.roles[0], \"first message should come from user\"\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    if i == 0:\n                        message = wrap_sys(self.system) + message\n                    if i % 2 == 0:\n                        message = wrap_inst(message)\n                        ret += self.sep + message\n                    else:\n                        ret += \" \" + message + \" \" + self.sep2\n                else:\n                    ret += \"\"\n            ret = ret.lstrip(self.sep)\n\n        elif self.sep_style == SeparatorStyle.PLAIN:\n            seps = [self.sep, self.sep2]\n            ret = self.system\n            for i, (role, message) in enumerate(messages):\n                if message:\n                    if type(message) is tuple:\n                        message, _, _ = message\n                    ret += message + seps[i % 2]\n                else:\n                    ret += \"\"\n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\n        return ret\n\n    def append_message(self, role, message):\n        self.messages.append([role, message])\n\n    def process_image(self, image, image_process_mode, return_pil=False, image_format=\"PNG\"):\n        if image_process_mode == \"Pad\":\n\n            def expand2square(pil_img, background_color=(122, 116, 104)):\n                width, height = pil_img.size\n                if width == height:\n                    return pil_img\n                elif width > height:\n                    result = Image.new(pil_img.mode, (width, width), background_color)\n                    result.paste(pil_img, (0, (width - height) // 2))\n                    return result\n                else:\n                    result = Image.new(pil_img.mode, (height, height), background_color)\n                    result.paste(pil_img, ((height - width) // 2, 0))\n                    return result\n\n            image = expand2square(image)\n        elif image_process_mode in [\"Default\", \"Crop\"]:\n            pass\n        elif image_process_mode == \"Resize\":\n            image = image.resize((336, 336))\n        else:\n            raise ValueError(f\"Invalid image_process_mode: {image_process_mode}\")\n\n        if type(image) is not Image.Image:\n            image = Image.open(image).convert(\"RGB\")\n\n        max_hw, min_hw = max(image.size), min(image.size)\n        aspect_ratio = max_hw / min_hw\n        max_len, min_len = 672, 448\n        shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n        longest_edge = int(shortest_edge * aspect_ratio)\n        W, H = image.size\n        if H > W:\n            H, W = longest_edge, shortest_edge\n        else:\n            H, W = shortest_edge, longest_edge\n        image = image.resize((W, H))\n        if return_pil:\n            return image\n        else:\n            buffered = BytesIO()\n            image.save(buffered, format=image_format)\n            img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n            return img_b64_str\n\n    def get_images(self, return_pil=False, return_path=False):\n        images = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    msg, image, image_process_mode = msg\n                    if type(image) != list:\n                        image = [image]\n                    for img in image:\n                        if not return_path and self.is_image_file(img):\n                            img = self.process_image(img, image_process_mode, return_pil=return_pil)\n                        else:\n                            images.append(img)\n        return images\n\n    def is_image_file(self, filename):\n        image_extensions = [\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\", \".webp\"]\n        return any(filename.lower().endswith(ext) for ext in image_extensions)\n\n    def is_video_file(self, filename):\n        video_extensions = [\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\", \".mpeg\", \".mpg\"]\n        return any(filename.lower().endswith(ext) for ext in video_extensions)\n\n    def to_gradio_chatbot(self):\n        ret = []\n        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n            if i % 2 == 0:\n                if type(msg) is tuple:\n                    msg, image, image_process_mode = msg\n                    if type(image) != list:\n                        image = [image]\n                    if len(image) == 1:\n                        msg = \"<image>\\n\" + msg.replace(\"<image>\", \"\").strip()\n                    else:\n                        msg = re.sub(r\"(<image>)\\n(?=<image>)\", r\"\\1 \", msg)\n\n                    img_str_list = []                         \n                    for img in image:\n                        if self.is_image_file(img):\n                            img_b64_str = self.process_image(img, \"Default\", return_pil=False, image_format=\"JPEG\")\n                            img_str = f'<img src=\"data:image/jpeg;base64,{img_b64_str}\" style=\"max-width: 256px; max-height: 256px; width: auto; height: auto; object-fit: contain;\"/>'\n                            img_str_list.append(img_str)\n                        elif self.is_video_file(img):\n                            ret.append(((img,), None))\n\n                    msg = msg.strip()\n                    img_place_holder = \"\"\n                    for img_str in img_str_list:\n                        img_place_holder += f\"{img_str}\\n\\n\"\n\n                    if len(img_str_list) > 0:\n                        msg = f\"{img_place_holder}\\n\\n{msg}\"\n\n                    if len(msg) > 0:\n                        ret.append([msg, None])\n                else:\n                    ret.append([msg, None])\n            else:\n                ret[-1][-1] = msg\n        return ret\n\n    def copy(self):\n        return Conversation(system=self.system, roles=self.roles, messages=[[x, y] for x, y in self.messages], offset=self.offset, sep_style=self.sep_style, sep=self.sep, sep2=self.sep2, version=self.version)\n\n    def dict(self):\n        if len(self.get_images()) > 0:\n            return {\n                \"system\": self.system,\n                \"roles\": self.roles,\n                \"messages\": [[x, y[0] if type(y) is tuple else y] for x, y in self.messages],\n                \"offset\": self.offset,\n                \"sep\": self.sep,\n                \"sep2\": self.sep2,\n            }\n        return {\n            \"system\": self.system,\n            \"roles\": self.roles,\n            \"messages\": self.messages,\n            \"offset\": self.offset,\n            \"sep\": self.sep,\n            \"sep2\": self.sep2,\n        }\n\n\nconv_vicuna_v0 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=[\n        [\"Human\", \"What are the key differences between renewable and non-renewable energy sources?\"],\n        [\n            \"Assistant\",\n            \"Renewable energy sources are those that can be replenished naturally in a relatively \"\n            \"short amount of time, such as solar, wind, hydro, geothermal, and biomass. \"\n            \"Non-renewable energy sources, on the other hand, are finite and will eventually be \"\n            \"depleted, such as coal, oil, and natural gas. Here are some key differences between \"\n            \"renewable and non-renewable energy sources:\\n\"\n            \"1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable \"\n            \"energy sources are finite and will eventually run out.\\n\"\n            \"2. Environmental impact: Renewable energy sources have a much lower environmental impact \"\n            \"than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, \"\n            \"and other negative effects.\\n\"\n            \"3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically \"\n            \"have lower operational costs than non-renewable sources.\\n\"\n            \"4. Reliability: Renewable energy sources are often more reliable and can be used in more remote \"\n            \"locations than non-renewable sources.\\n\"\n            \"5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different \"\n            \"situations and needs, while non-renewable sources are more rigid and inflexible.\\n\"\n            \"6. Sustainability: Renewable energy sources are more sustainable over the long term, while \"\n            \"non-renewable sources are not, and their depletion can lead to economic and social instability.\\n\",\n        ],\n    ],\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_vicuna_v1 = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_llama_2 = Conversation(\n    system=\"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_llava_llama_2 = Conversation(\n    system=\"You are a helpful language and vision assistant. \" \"You are able to understand the visual content that the user provides, \" \"and assist the user with a variety of tasks using natural language.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\ndef safe_load_tokenizer(tokenizer_id):\n    try:\n        return AutoTokenizer.from_pretrained(tokenizer_id)\n    except Exception:\n        return None\n\nconv_llava_llama_3 = Conversation(\n    system=\"You are a helpful language and vision assistant. \" \"You are able to understand the visual content that the user provides, \" \"and assist the user with a variety of tasks using natural language.\",\n    roles=(\"user\", \"assistant\"),\n    version=\"llama_v3\",\n    messages=[],\n    offset=0,\n    sep=\"<|eot_id|>\",\n    sep_style=SeparatorStyle.LLAMA_3,\n    tokenizer_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    tokenizer=safe_load_tokenizer(\"meta-llama/Meta-Llama-3-8B-Instruct\"),\n    stop_token_ids=[128009],\n)\n\nconv_mistral_instruct = Conversation(\n    system=\"\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"\",\n    sep2=\"</s>\",\n)\n\nconv_llava_llama_2_simple = Conversation(\n    system=\"Answer the questions about the visual content that the user provides.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_llava_llama_2_mmtag = Conversation(\n    system=\"Answer the questions about the visual content that the user provides.\" \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"llama_v2_mmtag\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.LLAMA_2,\n    sep=\"<s>\",\n    sep2=\"</s>\",\n)\n\nconv_mpt = Conversation(\n    system=\"\"\"<|im_start|>system\nA conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_qwen = Conversation(\n    system=\"\"\"<|im_start|>system\nYou are a helpful assistant.\"\"\",\n    roles=(\"<|im_start|>user\", \"<|im_start|>assistant\"),\n    version=\"qwen\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.CHATML,\n    sep=\"<|im_end|>\",\n)\n\nconv_gemma_instruct = Conversation(system=\"\", roles=(\"<start_of_turn>user\\n\", \"<start_of_turn>model\\n\"), version=\"gemma\", messages=[], offset=0, sep_style=SeparatorStyle.GEMMA, sep=\"<end_of_turn>\\n\")\n\nconv_llava_plain = Conversation(\n    system=\"\",\n    roles=(\"\", \"\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.PLAIN,\n    sep=\"\\n\",\n)\n\nconv_llava_v0 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\nconv_llava_v0_mmtag = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n    \"The assistant is able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n    \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n    version=\"v0_mmtag\",\n)\n\nconv_llava_v1 = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \" \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    version=\"v1\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_llava_v1_mmtag = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n    \"The assistant is able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\"\n    \"The visual content will be provided with the following format: <Image>visual content</Image>.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n    version=\"v1_mmtag\",\n)\n\nconv_mistral_orca = Conversation(\n    system=\"\"\"<|im_start|>system\nYou are MistralOrca, a large language model trained by Alignment Lab AI. Write out your reasoning step-by-step to be sure you get the right answers!\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_mistral_zephyr = Conversation(\n    system=\"\"\"<|system|>\nYou are a helpful AI assistant.\"\"\",\n    roles=(\"<|user|>\\n\", \"<|assistant|>\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"</s>\",\n)\n\nconv_mistral_direct = Conversation(\n    system=\"\"\"<|im_start|>system\nAnswer the questions.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\nconv_chatml_direct = Conversation(\n    system=\"\"\"<|im_start|>system\nAnswer the questions.\"\"\",\n    roles=(\"<|im_start|>user\\n\", \"<|im_start|>assistant\\n\"),\n    version=\"mpt\",\n    messages=[],\n    offset=0,\n    sep_style=SeparatorStyle.MPT,\n    sep=\"<|im_end|>\",\n)\n\ndefault_conversation = conv_vicuna_v0\nconv_templates = {\n    \"default\": conv_vicuna_v0,\n    \"v0\": conv_vicuna_v0,\n    \"v1\": conv_vicuna_v1,\n    \"vicuna_v1\": conv_vicuna_v1,\n    \"llama_2\": conv_llama_2,\n    \"mistral_instruct\": conv_mistral_instruct,\n    \"mistral_orca\": conv_mistral_orca,\n    \"mistral_zephyr\": conv_mistral_zephyr,\n    \"mistral_direct\": conv_mistral_direct,\n    \"plain\": conv_llava_plain,\n    \"v0_plain\": conv_llava_plain,\n    \"chatml_direct\": conv_chatml_direct,\n    \"llava_v0\": conv_llava_v0,\n    \"llava_v0_mmtag\": conv_llava_v0_mmtag,\n    \"llava_v1\": conv_llava_v1,\n    \"llava_v1_mmtag\": conv_llava_v1_mmtag,\n    \"llava_llama_2\": conv_llava_llama_2,\n    \"llava_llama_3\": conv_llava_llama_3,\n    \"llava_llama_2_simple\": conv_llava_llama_2_simple,\n    \"llava_llama_2_mmtag\": conv_llava_llama_2_mmtag,\n    \"llava_mistral_instruct\": conv_mistral_instruct,\n    \"mpt\": conv_mpt,\n    \"qwen_1_5\": conv_qwen,\n    \"qwen_2\": conv_qwen,\n    \"gemma_instruct\": conv_gemma_instruct,\n}\n\n\nif __name__ == \"__main__\":\n    print(default_conversation.get_prompt())\n"}
{"type": "source_file", "path": "llava/mm_utils.py", "content": "from PIL import Image\nfrom io import BytesIO\nimport base64\nimport math\nimport ast\nimport re\nimport torch\nfrom transformers import StoppingCriteria\nfrom llava.constants import IMAGE_TOKEN_INDEX\n\n\ndef resize_and_center_crop(image, shortest_edge_length):\n    # Calculate new dimensions and resize\n    aspect_ratio = float(image.width) / float(image.height)\n    if aspect_ratio > 1:\n        new_width = int(shortest_edge_length * aspect_ratio)\n        new_height = shortest_edge_length\n    else:\n        new_width = shortest_edge_length\n        new_height = int(shortest_edge_length / aspect_ratio)\n    resized_image = image.resize((new_width, new_height), Image.ANTIALIAS)\n\n    # Calculate the position and perform the center crop\n    left = (new_width - shortest_edge_length) / 2\n    top = (new_height - shortest_edge_length) / 2\n    right = (new_width + shortest_edge_length) / 2\n    bottom = (new_height + shortest_edge_length) / 2\n    cropped_image = resized_image.crop((left, top, right, bottom))\n\n    return cropped_image\n\n\ndef auto_pad_images(image, grid_params):\n    assert isinstance(image, Image.Image), \"Input should be a Pillow Image\"\n    assert len(grid_params) > 0, \"Grid parameters should not be empty\"\n\n    # Step 1: Calculate and find the closest aspect ratio\n    input_width, input_height = image.size\n    input_aspect_ratio = input_width / input_height\n    candidate_resolutions = [(w / h, w, h) for w in grid_params for h in grid_params]\n    closest_aspect_ratio = min(candidate_resolutions, key=lambda x: abs(input_aspect_ratio - x[0]))\n\n    candidate_resolutions = [(x[1], x[2]) for x in candidate_resolutions if abs(x[0] - closest_aspect_ratio[0]) < 1e-3]\n\n    target_resolution = min(candidate_resolutions, key=lambda res: abs(max(input_width, input_height) / max(res) - 1))\n\n    resize_width, resize_height = target_resolution\n    if input_width > input_height:\n        resize_height = int(resize_width / input_aspect_ratio)\n    else:\n        resize_width = int(resize_height * input_aspect_ratio)\n    resized_image = image.resize((resize_width, resize_height), Image.ANTIALIAS)\n\n    # Step 5: Pad the resized image if necessary to match the target resolution\n    pad_width = target_resolution[0] - resize_width\n    pad_height = target_resolution[1] - resize_height\n    padded_image = Image.new(\"RGB\", target_resolution, color=(0, 0, 0))\n    padded_image.paste(resized_image, (pad_width // 2, pad_height // 2))\n\n    return padded_image\n\n\ndef extract_patches(image, patch_size, overlap_ratio):\n    assert isinstance(image, Image.Image), \"Input should be a Pillow Image\"\n    assert patch_size > 0, \"Patch size should be greater than 0\"\n    assert 0 <= overlap_ratio < 1, \"Overlap ratio should be between 0 and 1\"\n\n    W, H = image.size\n    patches = []\n\n    stride = int(patch_size * (1 - overlap_ratio))\n\n    num_patches_y = (H - patch_size) // stride + 1\n    num_patches_x = (W - patch_size) // stride + 1\n\n    y_start = (H - (num_patches_y - 1) * stride - patch_size) // 2\n    x_start = (W - (num_patches_x - 1) * stride - patch_size) // 2\n\n    for y in range(y_start, y_start + num_patches_y * stride, stride):\n        for x in range(x_start, x_start + num_patches_x * stride, stride):\n            patch = image.crop((x, y, x + patch_size, y + patch_size))\n            patches.append(patch)\n\n    return patches\n\n\ndef process_highres_image_crop_split(image, data_args, processor=None):\n    crop_resolution = data_args.image_crop_resolution\n    split_resolution = data_args.image_split_resolution\n    if processor is None:\n        processor = data_args.image_processor\n    image_crop = resize_and_center_crop(image, crop_resolution)\n    image_patches = extract_patches(image_crop, patch_size=split_resolution, overlap_ratio=0)\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef process_highres_image(image, processor, grid_pinpoints):\n    grid_params = [int(x) for x in grid_pinpoints.split(\",\")]\n    width_height = max(image.size)\n    fit_grid_params = [x for x in grid_params if x >= width_height]\n    if len(fit_grid_params) == 0:\n        select_size = max(grid_params)\n    else:\n        select_size = min(fit_grid_params)\n    # FIXME: always select the 448\n    select_size = max(grid_params)\n    image_padded = expand2square(image, tuple(int(x * 255) for x in processor.image_mean))\n\n    # FIXME: this seems to be a bug that it always resizes instead of padding\n    image_original_resize = image.resize((processor.size[\"shortest_edge\"], processor.size[\"shortest_edge\"]))\n    image_padded = image_padded.resize((select_size, select_size))\n    image_patches = extract_patches(image_padded, patch_size=processor.size[\"shortest_edge\"], overlap_ratio=0)\n    image_patches = [image_original_resize] + image_patches\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef select_best_resolution(original_size, possible_resolutions):\n    \"\"\"\n    Selects the best resolution from a list of possible resolutions based on the original size.\n\n    Args:\n        original_size (tuple): The original size of the image in the format (width, height).\n        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].\n\n    Returns:\n        tuple: The best fit resolution in the format (width, height).\n    \"\"\"\n    original_width, original_height = original_size\n    best_fit = None\n    max_effective_resolution = 0\n    min_wasted_resolution = float(\"inf\")\n\n    for width, height in possible_resolutions:\n        # Calculate the downscaled size to keep the aspect ratio\n        scale = min(width / original_width, height / original_height)\n        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)\n\n        # Calculate effective and wasted resolutions\n        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)\n        wasted_resolution = (width * height) - effective_resolution\n\n        if effective_resolution > max_effective_resolution or (effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution):\n            max_effective_resolution = effective_resolution\n            min_wasted_resolution = wasted_resolution\n            best_fit = (width, height)\n\n    return best_fit\n\n\ndef resize_and_pad_image(image, target_resolution):\n    \"\"\"\n    Resize and pad an image to a target resolution while maintaining aspect ratio.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        target_resolution (tuple): The target resolution (width, height) of the image.\n\n    Returns:\n        PIL.Image.Image: The resized and padded image.\n    \"\"\"\n    original_width, original_height = image.size\n    target_width, target_height = target_resolution\n\n    # Determine which dimension (width or height) to fill\n    scale_w = target_width / original_width\n    scale_h = target_height / original_height\n\n    if scale_w < scale_h:\n        # Width will be filled completely\n        new_width = target_width\n        new_height = min(math.ceil(original_height * scale_w), target_height)\n    else:\n        # Height will be filled completely\n        new_height = target_height\n        new_width = min(math.ceil(original_width * scale_h), target_width)\n\n    # Resize the image\n    resized_image = image.resize((new_width, new_height))\n\n    # Create a new image with the target size and paste the resized image onto it\n    new_image = Image.new(\"RGB\", (target_width, target_height), (0, 0, 0))\n    paste_x = (target_width - new_width) // 2\n    paste_y = (target_height - new_height) // 2\n    new_image.paste(resized_image, (paste_x, paste_y))\n\n    return new_image\n\n\ndef divide_to_patches(image, patch_size):\n    \"\"\"\n    Divides an image into patches of a specified size.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n        patch_size (int): The size of each patch.\n\n    Returns:\n        list: A list of PIL.Image.Image objects representing the patches.\n    \"\"\"\n    patches = []\n    width, height = image.size\n    for i in range(0, height, patch_size):\n        for j in range(0, width, patch_size):\n            box = (j, i, j + patch_size, i + patch_size)\n            patch = image.crop(box)\n            patches.append(patch)\n\n    return patches\n\n\ndef get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n    \"\"\"\n    Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n\n    Args:\n        image_size (tuple): The size of the input image in the format (width, height).\n        grid_pinpoints (str): A string representation of a list of possible resolutions.\n        patch_size (int): The size of each image patch.\n\n    Returns:\n        tuple: The shape of the image patch grid in the format (width, height).\n    \"\"\"\n    if isinstance(grid_pinpoints, str) and \"x\" in grid_pinpoints:\n        assert patch_size in [224, 336, 384, 448, 512], \"patch_size should be in [224, 336, 384, 448, 512]\"\n        # Use regex to extract the range from the input string\n        matches = re.findall(r\"\\((\\d+)x(\\d+)\\)\", grid_pinpoints)\n        range_start = tuple(map(int, matches[0]))\n        range_end = tuple(map(int, matches[-1]))\n        # Generate a matrix of tuples from (range_start[0], range_start[1]) to (range_end[0], range_end[1])\n        grid_pinpoints = [(i, j) for i in range(range_start[0], range_end[0] + 1) for j in range(range_start[1], range_end[1] + 1)]\n        # Multiply all elements by patch_size\n        grid_pinpoints = [[dim * patch_size for dim in pair] for pair in grid_pinpoints]\n    if type(grid_pinpoints) is list:\n        possible_resolutions = grid_pinpoints\n    else:\n        possible_resolutions = ast.literal_eval(grid_pinpoints)\n    width, height = select_best_resolution(image_size, possible_resolutions)\n    return width // patch_size, height // patch_size\n\n\ndef process_anyres_image(image, processor, grid_pinpoints):\n    \"\"\"\n    Process an image with variable resolutions.\n\n    Args:\n        image (PIL.Image.Image): The input image to be processed.\n        processor: The image processor object.\n        grid_pinpoints (str): A string representation of a list of possible resolutions.\n\n    Returns:\n        torch.Tensor: A tensor containing the processed image patches.\n    \"\"\"\n    # Convert grid_pinpoints from string to list\n    if isinstance(grid_pinpoints, str) and \"x\" in grid_pinpoints:\n        try:\n            patch_size = processor.size[0]\n        except Exception as e:\n            patch_size = processor.size[\"shortest_edge\"]\n        assert patch_size in [224, 336, 384, 448, 512], \"patch_size should be in [224, 336, 384, 448, 512]\"\n        # Use regex to extract the range from the input string\n        matches = re.findall(r\"\\((\\d+)x(\\d+)\\)\", grid_pinpoints)\n        range_start = tuple(map(int, matches[0]))\n        range_end = tuple(map(int, matches[-1]))\n        # Generate a matrix of tuples from (range_start[0], range_start[1]) to (range_end[0], range_end[1])\n        grid_pinpoints = [(i, j) for i in range(range_start[0], range_end[0] + 1) for j in range(range_start[1], range_end[1] + 1)]\n        # Multiply all elements by patch_size\n        grid_pinpoints = [[dim * patch_size for dim in pair] for pair in grid_pinpoints]\n\n    if type(grid_pinpoints) is list:\n        possible_resolutions = grid_pinpoints\n    else:\n        possible_resolutions = ast.literal_eval(grid_pinpoints)\n    best_resolution = select_best_resolution(image.size, possible_resolutions)\n    image_padded = resize_and_pad_image(image, best_resolution)\n\n    patches = divide_to_patches(image_padded, processor.crop_size[\"height\"])\n\n    # FIXME: this seems to be a bug that it resizes instead of pad.\n    # but to keep it consistent with previous, i will keep it as it is\n    # TODO: uncomment below to ablate with the padding\n    if isinstance(processor.size, dict):\n        shortest_edge = processor.size[\"shortest_edge\"]\n    else:\n        shortest_edge = min(processor.size)\n    image_original_resize = image.resize((shortest_edge, shortest_edge))\n    # image_padded_square = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\n    # image_original_resize = image_padded_square.resize((processor.size['shortest_edge'], processor.size['shortest_edge']))\n\n    image_patches = [image_original_resize] + patches\n    image_patches = [processor.preprocess(image_patch, return_tensors=\"pt\")[\"pixel_values\"][0] for image_patch in image_patches]\n    return torch.stack(image_patches, dim=0)\n\n\ndef load_image_from_base64(image):\n    return Image.open(BytesIO(base64.b64decode(image)))\n\n\ndef expand2square(pil_img, background_color):\n    width, height = pil_img.size\n    if width == height:\n        return pil_img\n    elif width > height:\n        result = Image.new(pil_img.mode, (width, width), background_color)\n        result.paste(pil_img, (0, (width - height) // 2))\n        return result\n    else:\n        result = Image.new(pil_img.mode, (height, height), background_color)\n        result.paste(pil_img, ((height - width) // 2, 0))\n        return result\n\n\ndef process_images(images, image_processor, model_cfg):\n    image_aspect_ratio = getattr(model_cfg, \"image_aspect_ratio\", None)\n    new_images = []\n    if image_aspect_ratio == \"highres\":\n        for image in images:\n            image = process_highres_image(image, image_processor, model_cfg.image_grid_pinpoints)\n            new_images.append(image)\n    elif image_aspect_ratio == \"anyres\" or \"anyres_max\" in image_aspect_ratio:\n        for image in images:\n            image = process_anyres_image(image, image_processor, model_cfg.image_grid_pinpoints)\n            new_images.append(image)\n    elif image_aspect_ratio == \"crop_split\":\n        for image in images:\n            image = process_highres_image_crop_split(image, model_cfg, image_processor)\n            new_images.append(image)\n    elif image_aspect_ratio == \"pad\":\n        for image in images:\n            image = expand2square(image, tuple(int(x * 255) for x in image_processor.image_mean))\n            image = image_processor.preprocess(image, return_tensors=\"pt\")[\"pixel_values\"][0]\n            new_images.append(image)\n    else:\n        return image_processor.preprocess(images, return_tensors=\"pt\")[\"pixel_values\"]\n    if all(x.shape == new_images[0].shape for x in new_images):\n        new_images = torch.stack(new_images, dim=0)\n    return new_images\n\n\ndef tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split(\"<image>\")]\n\n    def insert_separator(X, sep):\n        return [ele for sublist in zip(X, [sep] * len(X)) for ele in sublist][:-1]\n\n    input_ids = []\n    offset = 0\n    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n        offset = 1\n        input_ids.append(prompt_chunks[0][0])\n\n    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n        input_ids.extend(x[offset:])\n\n    if return_tensors is not None:\n        if return_tensors == \"pt\":\n            return torch.tensor(input_ids, dtype=torch.long)\n        raise ValueError(f\"Unsupported tensor type: {return_tensors}\")\n    return input_ids\n\n\ndef get_model_name_from_path(model_path):\n    model_path = model_path.strip(\"/\")\n    model_paths = model_path.split(\"/\")\n    if model_paths[-1].startswith(\"checkpoint-\"):\n        return model_paths[-2] + \"_\" + model_paths[-1]\n    else:\n        return model_paths[-1]\n\n\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, keywords, tokenizer, input_ids):\n        self.keywords = keywords\n        self.keyword_ids = []\n        for keyword in keywords:\n            cur_keyword_ids = tokenizer(keyword).input_ids\n            if len(cur_keyword_ids) > 1 and cur_keyword_ids[0] == tokenizer.bos_token_id:\n                cur_keyword_ids = cur_keyword_ids[1:]\n            self.keyword_ids.append(torch.tensor(cur_keyword_ids))\n        self.tokenizer = tokenizer\n        self.start_len = input_ids.shape[1]\n\n    def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        assert output_ids.shape[0] == 1, \"Only support batch size 1 (yet)\"  # TODO\n        offset = min(output_ids.shape[1] - self.start_len, 3)\n        self.keyword_ids = [keyword_id.to(output_ids.device) for keyword_id in self.keyword_ids]\n        for keyword_id in self.keyword_ids:\n            if output_ids[0, -keyword_id.shape[0] :] == keyword_id:\n                return True\n        outputs = self.tokenizer.batch_decode(output_ids[:, -offset:], skip_special_tokens=True)[0]\n        for keyword in self.keywords:\n            if keyword in outputs:\n                return True\n        return False\n"}
{"type": "source_file", "path": "llava/eval/evaluate_interleave.py", "content": "import re\nfrom rouge import Rouge\nimport argparse\nimport os\nimport json\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\nspot_the_diff = [\"Spot-the-Diff\", \"Birds-to-Words\", \"CLEVR-Change\"]\nimage_edit_instruct = [\"IEdit\", \"HQ-Edit\", \"MagicBrush\"]\nvisual_story_telling = [\"AESOP\", \"FlintstonesSV\", \"PororoSV\", \"VIST\"]\nvisual_cloze = [\"COMICS_Dialogue\", \"RecipeQA_VisualCloze\"]\ntext_rich_vqa = [\"WebQA\", \"TQA\", \"OCR-VQA\", \"DocVQA\"]\nmulti_image_vqa = [\"MIT-States_StateCoherence\", \"MIT-States_PropertyCoherence\", \"VISION\", \"RecipeQA_ImageCoherence\"]\n\npuzzle = [\"RAVEN\"]\nnlrv2 = [\"NLVR2_Mantis\"]\nqbench = [\"QBench\"]\n\nclass Eval:\n    def __init__(self):\n        self.periodStrip = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n        self.commaStrip = re.compile(\"(\\d)(\\,)(\\d)\")\n        self.punct = [\n            \";\",\n            r\"/\",\n            \"[\",\n            \"]\",\n            '\"',\n            \"{\",\n            \"}\",\n            \"(\",\n            \")\",\n            \"=\",\n            \"+\",\n            \"\\\\\",\n            \"_\",\n            \"-\",\n            \">\",\n            \"<\",\n            \"@\",\n            \"`\",\n            \",\",\n            \"?\",\n            \"!\",\n        ]\n        \n    def processPunctuation(self, inText):\n        outText = inText\n        for p in self.punct:\n            if (p + \" \" in inText or \" \" + p in inText) or (\n                re.search(self.commaStrip, inText) != None\n            ):\n                outText = outText.replace(p, \"\")\n            else:\n                outText = outText.replace(p, \" \")\n        outText = self.periodStrip.sub(\"\", outText, re.UNICODE)\n        return outText\n    \n    def process(self, answer):\n        answer = answer.replace(\"\\n\", \" \")\n        answer = answer.replace(\"\\t\", \" \")\n        answer = answer.strip()\n        answer = self.processPunctuation(answer)\n        answer = answer.strip('\\'')\n        answer = answer.strip('\\\"')\n        answer = answer.strip(')')\n        answer = answer.strip('(')\n        answer = answer.strip().lower()\n        return answer\n\n    def evaluate_rouge(self,preds):\n        rouge = Rouge()\n        acc = {'f': []}\n        eval_list = []\n        for i, res in enumerate(preds):\n            sample_id = res['sample_id']\n            # print(sample_id)\n            gt_ans = self.process(res[\"gt_response\"])\n            pred_ans = self.process(res[\"pred_response\"])\n            # assert gt_ans != ''\n\n            if gt_ans == '':\n                continue\n            \n            if pred_ans == '':\n                s = 0\n            else:\n                if len(pred_ans) > 512:\n                    pred_ans = pred_ans[0: 512]\n                s = rouge.get_scores(pred_ans, gt_ans)[0]['rouge-l']['f']\n            acc['f'].append(s)\n            eval_list.append({'id':str(sample_id),'score':str(round(s,3))})\n        results = {'Rouge-L f': np.mean(acc['f'])}\n        return results,eval_list\n\n\n    def judge_multi_choice(self,sample):\n        sample_id = sample['sample_id']\n        gt_ans = sample[\"gt_response\"]\n        pred_ans = sample[\"pred_response\"]\n\n        if \":\" in pred_ans:\n            a_list = pred_ans.split(\":\")\n            a_list = [a.strip() for a in a_list ]\n            for a in a_list:\n                if len(a) == 1 and a[-1] in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]:\n                    pred_ans = a\n\n        if pred_ans == gt_ans:\n            return 1\n        else:\n            return 0\n\n    def process_sample(self,sample):\n        sample[\"gt_response\"] = self.process(sample[\"gt_response\"])\n        sample[\"pred_response\"] = self.process(sample[\"pred_response\"])\n\n    def evaluate_multichoice(self, preditions):\n        correct = 0\n        eval_list = []\n        for i, sample in enumerate(preditions):\n            self.process_sample(sample)\n            score = self.judge_multi_choice(sample)\n            sample_id = sample['sample_id']\n            sample['result'] = score\n            eval_list.append({'id':str(sample_id),'score':str(score)})\n            correct+=score\n        return {'Accuracy':correct/len(preditions)},eval_list\n\n    def evaluate_multi_choice_image(self,preditions):\n        correct = 0\n        eval_list = []\n        for i,sample in enumerate(preditions):\n            gt_ans = self.process(sample[\"gt_response\"])\n            pred_ans = self.process(sample[\"pred_response\"])\n            sample_id = sample['sample_id']\n\n            if \":\" in pred_ans:\n                a_list = pred_ans.split(\":\")\n                a_list = [a.strip() for a in a_list ]\n                for a in a_list:\n                    if len(a) == 1 and a[-1] in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]:\n                        pred_ans = a\n\n            if gt_ans == pred_ans:\n                score = 1\n            else:\n                score = 0\n            sample_id = sample['sample_id']\n            sample['result'] = score\n            eval_list.append({'id':str(sample_id),'score':str(score)})\n            correct+=score\n        return {'Accuracy':correct/len(preditions)},eval_list\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--result-dir', type=str, required=True)\n\n    args = parser.parse_args()\n    \n    result_file = os.path.join(args.result_dir, \"result.jsonl\")\n\n    if not os.path.exists(result_file):\n        print('No prediction file found')\n        exit(0)\n    with open(result_file, 'r') as f:\n        preds_all = [json.loads(line) for line in f]\n    \n    preds_all_dict = dict()\n    for pred in preds_all:\n        if pred[\"dataset\"] not in preds_all_dict:\n            preds_all_dict[pred[\"dataset\"]] = list()\n        preds_all_dict[pred[\"dataset\"]].append(pred)\n\n    image_choice_dataset_list = [\"recipeqa-RecipeQA_VisualCloze\", \"RecipeQA_ImageCoherence\", \"COMICS_Panel\"]\n    E = Eval()\n\n    eval_result_list = dict()\n    eval_result_list_detail = dict()\n\n    for dataset in preds_all_dict:\n        \n        preds = preds_all_dict[dataset]\n        question_type = preds[0][\"question_type\"]\n   \n        if question_type == 'open-ended':\n            eval_result, eval_list = E.evaluate_rouge(preds)\n\n        elif question_type == 'multi-choice' or dataset == 'nlrv2':\n            if dataset in image_choice_dataset_list:\n                eval_result, eval_list = E.evaluate_multi_choice_image(preds)\n            else:\n                eval_result, eval_list = E.evaluate_multichoice(preds)\n\n        else:\n            eval_result = 'Dataset not supported'\n            print('Dataset not supported')\n            exit(0)\n\n        print(dataset, end = ':  ')\n        print(eval_result)\n\n        eval_result_list[dataset] = eval_result\n        eval_result_list_detail[dataset] = eval_list\n\n    os.makedirs(args.result_dir, exist_ok=True)\n    with open(os.path.join(args.result_dir, 'eval_dataset.json'), 'w') as f:\n        json.dump(eval_result_list, f, indent=4)\n\n    with open(os.path.join(args.result_dir,'eval_dataset_details.json'), 'w') as f:\n        json.dump(eval_result_list_detail, f, indent=4)\n\n\n    eval_cat_list = dict()\n    print()\n\n    # spot_the_diff\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in spot_the_diff:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"spot_the_diff\"] = score\n        print(\"spot_the_diff\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # image_edit_instruct\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in image_edit_instruct:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"image_edit_instruct\"] = score\n        print(\"image_edit_instruct\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # visual_story_telling\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in visual_story_telling:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"visual_story_telling\"] = score\n        print(\"visual_story_telling\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # visual_cloze\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in visual_cloze:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"visual_cloze\"] = score\n        print(\"visual_cloze\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # text_rich_vqa\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in text_rich_vqa:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"text_rich_vqa\"] = score\n        print(\"text_rich_vqa\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # multi_image_vqa\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in multi_image_vqa:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"multi_image_vqa\"] = score\n        print(\"multi_image_vqa\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # puzzle\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in puzzle:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"puzzle\"] = score\n        print(\"puzzle\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # nlrv2\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in nlrv2:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"nlrv2\"] = score\n        print(\"nlrv2\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    # qbench\n    score = 0\n    count = 0\n    for dataset in eval_result_list:\n        if dataset in qbench:\n            count += 1\n            score += list(eval_result_list[dataset].values())[0]\n    if count > 0:\n        score /= count\n        eval_cat_list[\"qbench\"] = score\n        print(\"qbench\", end = ':  ')\n        print('{:.2f}'.format(100 * score))\n\n    with open(os.path.join(args.result_dir,'eval_cat.json'), 'w') as f:\n        json.dump(eval_cat_list, f, indent=4)"}
{"type": "source_file", "path": "llava/eval/model_vqa.py", "content": "import argparse\nimport torch\nimport os\nimport json\nfrom tqdm import tqdm\nimport shortuuid\n\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n\nfrom llava.constants import IGNORE_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_TOKEN_INDEX\nfrom typing import Dict, Optional, Sequence, List\nimport transformers\nimport re\n\nfrom PIL import Image\nimport math\n\n\ndef split_list(lst, n):\n    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n    chunk_size = math.ceil(len(lst) / n)  # integer division\n    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n\n\ndef get_chunk(lst, n, k):\n    chunks = split_list(lst, n)\n    return chunks[k]\n\ndef preprocess_qwen(sources, tokenizer: transformers.PreTrainedTokenizer, has_image: bool = False, max_len=2048, system_message: str = \"You are a helpful assistant.\") -> Dict:\n    roles = {\"human\": \"<|im_start|>user\", \"gpt\": \"<|im_start|>assistant\"}\n\n    im_start, im_end = tokenizer.additional_special_tokens_ids\n    nl_tokens = tokenizer(\"\\n\").input_ids\n    _system = tokenizer(\"system\").input_ids + nl_tokens\n    _user = tokenizer(\"user\").input_ids + nl_tokens\n    _assistant = tokenizer(\"assistant\").input_ids + nl_tokens\n\n    # Apply prompt templates\n    input_ids, targets = [], []\n\n    source = sources\n    if roles[source[0][\"from\"]] != roles[\"human\"]:\n        source = source[1:]\n\n    input_id, target = [], []\n    system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n    input_id += system\n    target += [im_start] + [IGNORE_INDEX] * (len(system) - 3) + [im_end] + nl_tokens\n    assert len(input_id) == len(target)\n    for j, sentence in enumerate(source):\n        role = roles[sentence[\"from\"]]\n        if has_image and sentence[\"value\"] is not None and \"<image>\" in sentence[\"value\"]:\n            num_image = len(re.findall(DEFAULT_IMAGE_TOKEN, sentence[\"value\"]))\n            texts = sentence[\"value\"].split('<image>')\n            _input_id = tokenizer(role).input_ids + nl_tokens \n            for i,text in enumerate(texts):\n                _input_id += tokenizer(text).input_ids \n                if i<len(texts)-1:\n                    _input_id += [IMAGE_TOKEN_INDEX] + nl_tokens\n            _input_id += [im_end] + nl_tokens\n            assert sum([i==IMAGE_TOKEN_INDEX for i in _input_id])==num_image\n        else:\n            if sentence[\"value\"] is None:\n                _input_id = tokenizer(role).input_ids + nl_tokens\n            else:\n                _input_id = tokenizer(role).input_ids + nl_tokens + tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\n        input_id += _input_id\n        if role == \"<|im_start|>user\":\n            _target = [im_start] + [IGNORE_INDEX] * (len(_input_id) - 3) + [im_end] + nl_tokens\n        elif role == \"<|im_start|>assistant\":\n            _target = [im_start] + [IGNORE_INDEX] * len(tokenizer(role).input_ids) + _input_id[len(tokenizer(role).input_ids) + 1 : -2] + [im_end] + nl_tokens\n        else:\n            raise NotImplementedError\n        target += _target\n\n    input_ids.append(input_id)\n    targets.append(target)\n    input_ids = torch.tensor(input_ids, dtype=torch.long)\n    targets = torch.tensor(targets, dtype=torch.long)\n    return input_ids\n\ndef eval_model(args):\n    \n    # Model\n    disable_torch_init()\n    model_path = os.path.expanduser(args.model_path)\n    model_name = get_model_name_from_path(model_path)\n    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)\n\n    # Data\n    with open(os.path.expanduser(args.question_file)) as f:\n        questions = json.load(f)\n    questions = get_chunk(questions, args.num_chunks, args.chunk_idx)\n    answers_file = os.path.expanduser(args.answers_file)\n    os.makedirs(os.path.dirname(answers_file), exist_ok=True)\n    ans_file = open(answers_file, \"w\")\n    \n    for line in tqdm(questions):\n        idx = line[\"sample_id\"]\n        question_type = line[\"metadata\"][\"question_type\"]\n        dataset_name = line[\"metadata\"][\"dataset\"]\n        gt = line[\"conversations\"][1][\"value\"]\n\n        image_files = line[\"image\"]\n        qs = line[\"conversations\"][0][\"value\"]\n        cur_prompt = args.extra_prompt + qs\n\n        args.conv_mode = \"qwen_1_5\"\n\n        conv = conv_templates[args.conv_mode].copy()\n        conv.append_message(conv.roles[0], qs)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n\n        input_ids = preprocess_qwen([line[\"conversations\"][0],{'from': 'gpt','value': None}], tokenizer, has_image=True).cuda()\n        img_num = list(input_ids.squeeze()).count(IMAGE_TOKEN_INDEX)\n\n        image_tensors = []\n        for image_file in image_files:\n            image = Image.open(os.path.join(args.image_folder, image_file))\n            image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values']\n            image_tensors.append(image_tensor.half().cuda())\n        # image_tensors = torch.cat(image_tensors, dim=0)\n\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n        keywords = [stop_str]\n        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n        with torch.inference_mode():\n            output_ids = model.generate(\n                input_ids,\n                images=image_tensors,\n                do_sample=True if args.temperature > 0 else False,\n                temperature=args.temperature,\n                top_p=args.top_p,\n                num_beams=args.num_beams,\n                # no_repeat_ngram_size=3,\n                max_new_tokens=1024,\n                use_cache=True)\n\n        \n        outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n        outputs = outputs.strip()\n        if outputs.endswith(stop_str):\n            outputs = outputs[:-len(stop_str)]\n        outputs = outputs.strip()\n\n        ans_id = shortuuid.uuid()\n        ans_file.write(json.dumps({\n                                   \"dataset\": dataset_name,\n                                   \"sample_id\": idx,\n                                   \"prompt\": cur_prompt,\n                                   \"pred_response\": outputs,\n                                   \"gt_response\": gt,\n                                   \"shortuuid\": ans_id,\n                                   \"model_id\": model_name,\n                                   \"question_type\": question_type,\n                                   }) + \"\\n\")\n        ans_file.flush()\n\n        if len(line[\"conversations\"]) > 2:\n\n            for i in range(2, len(line[\"conversations\"]), 2):\n                input_ids = torch.cat((input_ids, output_ids), dim=1)\n\n                gt = line[\"conversations\"][i + 1][\"value\"]\n                qs = line[\"conversations\"][i][\"value\"]\n                cur_prompt = args.extra_prompt + qs\n\n                args.conv_mode = \"qwen_1_5\"\n\n                conv = conv_templates[args.conv_mode].copy()\n                conv.append_message(conv.roles[0], qs)\n                conv.append_message(conv.roles[1], None)\n                prompt = conv.get_prompt()\n\n                input_ids_new = preprocess_qwen([line[\"conversations\"][i],{'from': 'gpt','value': None}], tokenizer, has_image=True).cuda()\n                input_ids = torch.cat((input_ids, input_ids_new), dim=1)\n                img_num = list(input_ids_new.squeeze()).count(IMAGE_TOKEN_INDEX)\n\n                stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n                keywords = [stop_str]\n                stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n                with torch.inference_mode():\n                    output_ids = model.generate(\n                        input_ids,\n                        images=image_tensors,\n                        do_sample=True if args.temperature > 0 else False,\n                        temperature=args.temperature,\n                        top_p=args.top_p,\n                        num_beams=args.num_beams,\n                        # no_repeat_ngram_size=3,\n                        max_new_tokens=1024,\n                        use_cache=True)\n        \n                outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n                outputs = outputs.strip()\n                if outputs.endswith(stop_str):\n                    outputs = outputs[:-len(stop_str)]\n                outputs = outputs.strip()\n\n                ans_id = shortuuid.uuid()\n                ans_file.write(json.dumps({\n                                        \"dataset\": dataset_name,\n                                        \"sample_id\": idx,\n                                        \"prompt\": cur_prompt,\n                                        \"pred_response\": outputs,\n                                        \"gt_response\": gt,\n                                        \"shortuuid\": ans_id,\n                                        \"model_id\": model_name,\n                                        \"question_type\": question_type,\n                                        }) + \"\\n\")\n                ans_file.flush()\n\n\n    ans_file.close()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-path\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--model-base\", type=str, default=None)\n    parser.add_argument(\"--image-folder\", type=str, default=\"\")\n    parser.add_argument(\"--extra-prompt\", type=str, default=\"\")\n    parser.add_argument(\"--question-file\", type=str, default=\"tables/question.jsonl\")\n    parser.add_argument(\"--answers-file\", type=str, default=\"answer.jsonl\")\n    parser.add_argument(\"--conv-mode\", type=str, default=\"llava_v1\")\n    parser.add_argument(\"--num-chunks\", type=int, default=1)\n    parser.add_argument(\"--chunk-idx\", type=int, default=0)\n    parser.add_argument(\"--temperature\", type=float, default=0.2)\n    parser.add_argument(\"--top_p\", type=float, default=None)\n    parser.add_argument(\"--num_beams\", type=int, default=1)\n    parser.add_argument(\"--test_size\", type=int, default=10000000)\n    args = parser.parse_args()\n\n    eval_model(args)"}
{"type": "source_file", "path": "llava/model/__init__.py", "content": "import os\n\nAVAILABLE_MODELS = {\n    \"llava_llama\": \"LlavaLlamaForCausalLM, LlavaConfig\",\n    \"llava_qwen\": \"LlavaQwenForCausalLM, LlavaQwenConfig\",\n    \"llava_mistral\": \"LlavaMistralForCausalLM, LlavaMistralConfig\",\n    \"llava_mixtral\": \"LlavaMixtralForCausalLM, LlavaMixtralConfig\",\n    # \"llava_qwen_moe\": \"LlavaQwenMoeForCausalLM, LlavaQwenMoeConfig\",    \n    # Add other models as needed\n}\n\nfor model_name, model_classes in AVAILABLE_MODELS.items():\n    try:\n        exec(f\"from .language_model.{model_name} import {model_classes}\")\n    except Exception as e:\n        print(f\"Failed to import {model_name} from llava.language_model.{model_name}. Error: {e}\")\n"}
{"type": "source_file", "path": "llava/model/apply_delta.py", "content": "\"\"\"\nUsage:\npython3 -m fastchat.model.apply_delta --base ~/model_weights/llama-7b --target ~/model_weights/vicuna-7b --delta lmsys/vicuna-7b-delta\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom llava import LlavaLlamaForCausalLM\n\n\ndef apply_delta(base_model_path, target_model_path, delta_path):\n    print(\"Loading base model\")\n    base = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Loading delta\")\n    delta = LlavaLlamaForCausalLM.from_pretrained(delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path)\n\n    print(\"Applying delta\")\n    for name, param in tqdm(delta.state_dict().items(), desc=\"Applying delta\"):\n        if name not in base.state_dict():\n            assert name in [\"model.mm_projector.weight\", \"model.mm_projector.bias\"], f\"{name} not in base model\"\n            continue\n        if param.data.shape == base.state_dict()[name].shape:\n            param.data += base.state_dict()[name]\n        else:\n            assert name in [\"model.embed_tokens.weight\", \"lm_head.weight\"], f\"{name} dimension mismatch: {param.data.shape} vs {base.state_dict()[name].shape}\"\n            bparam = base.state_dict()[name]\n            param.data[: bparam.shape[0], : bparam.shape[1]] += bparam\n\n    print(\"Saving target model\")\n    delta.save_pretrained(target_model_path)\n    delta_tokenizer.save_pretrained(target_model_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    apply_delta(args.base_model_path, args.target_model_path, args.delta_path)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_qwen_moe.py", "content": "#    Copyright 2024 Hao Zhang\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union, Dict\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nimport transformers\nfrom transformers import AutoConfig, AutoModelForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\n# from ...constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\nfrom transformers import Qwen2MoeConfig, Qwen2MoeModel, Qwen2MoeForCausalLM\n\n# from .qwen.modeling_qwen import QWenLMHeadModel, QWenModel\n# from .qwen.configuration_qwen import QWenConfig\n\n\nclass LlavaQwenMoeConfig(Qwen2MoeConfig):\n    model_type = \"llava_qwen_moe\"\n\n\nclass LlavaQwenMoeModel(LlavaMetaModel, Qwen2MoeModel):\n    config_class = LlavaQwenMoeConfig\n\n    def __init__(self, config: Qwen2MoeConfig):\n        super(LlavaQwenMoeModel, self).__init__(config)\n\n\nclass LlavaQwenMoeForCausalLM(Qwen2MoeForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaQwenMoeConfig\n\n    def __init__(self, config):\n        # super(Qwen2MoeForCausalLM, self).__init__(config)\n        Qwen2MoeForCausalLM.__init__(self, config)\n        config.model_type = \"llava_qwen_moe\"\n        config.rope_scaling = None\n\n        self.model = LlavaQwenMoeModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = False,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_qwen_moe\", LlavaQwenMoeConfig)\nAutoModelForCausalLM.register(LlavaQwenMoeConfig, LlavaQwenMoeForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_llama.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, LlamaConfig\n\nfrom torch.nn import CrossEntropyLoss\n\n\n# , LlamaModel, LlamaForCausalLM, GenerationConfig\n# from .modeling_llama import LlamaModel, LlamaForCausalLM\nfrom transformers import LlamaModel, LlamaForCausalLM\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaConfig(LlamaConfig):\n    model_type = \"llava_llama\"\n    temperature: float = 0.0  # reset to 0.0, previously 0.9 for Vicuna\n    max_new_tokens: int = 1024\n    do_sample: bool = False\n    top_p: Optional[float] = None\n    # rope_scaling: Optional[dict] = {}\n\n\nclass LlavaLlamaModel(LlavaMetaModel, LlamaModel):\n    config_class = LlavaConfig\n\n    def __init__(self, config: LlamaConfig):\n        super(LlavaLlamaModel, self).__init__(config)\n\n\nclass LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaConfig\n\n    def __init__(self, config):\n        LlamaForCausalLM.__init__(self, config)\n\n        # configure default generation settings\n        config.model_type = \"llava_llama\"\n        # config.rope_scaling = None\n\n        self.model = LlavaLlamaModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = None,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        modalities = kwargs.pop(\"modalities\", None) if \"modalities\" in kwargs and modalities is None else modalities\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_llama\", LlavaConfig)\nAutoModelForCausalLM.register(LlavaConfig, LlavaLlamaForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/hf_configs.py", "content": "# HF architecture dict:\narch_dict = {\n    # https://huggingface.co/docs/transformers/model_doc/roberta#roberta\n    \"roberta\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    # https://huggingface.co/docs/transformers/model_doc/xlm-roberta#transformers.XLMRobertaConfig\n    \"xlm-roberta\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    # https://huggingface.co/docs/transformers/model_doc/mt5#mt5\n    \"mt5\": {\n        \"config_names\": {\n            # unlimited seqlen\n            # https://github.com/google-research/text-to-text-transfer-transformer/issues/273\n            # https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/t5/modeling_t5.py#L374\n            \"context_length\": \"\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"d_model\",\n            \"heads\": \"num_heads\",\n            \"layers\": \"num_layers\",\n            \"layer_attr\": \"block\",\n            \"token_embeddings_attr\": \"embed_tokens\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n    \"bert\": {\n        \"config_names\": {\n            \"context_length\": \"max_position_embeddings\",\n            \"vocab_size\": \"vocab_size\",\n            \"width\": \"hidden_size\",\n            \"heads\": \"num_attention_heads\",\n            \"layers\": \"num_hidden_layers\",\n            \"layer_attr\": \"layer\",\n            \"token_embeddings_attr\": \"embeddings\",\n        },\n        \"pooler\": \"mean_pooler\",\n    },\n}\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/constants.py", "content": "OPENAI_DATASET_MEAN = (0.48145466, 0.4578275, 0.40821073)\nOPENAI_DATASET_STD = (0.26862954, 0.26130258, 0.27577711)\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_mistral.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, MistralConfig, MistralModel, MistralForCausalLM, GenerationConfig\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaMistralConfig(MistralConfig):\n    model_type = \"llava_mistral\"\n    temperature: float = 0.0  # reset to 0.0, previously 0.9 for Vicuna\n    max_new_tokens: int = 1024\n    do_sample: bool = False\n    top_p: Optional[float] = None\n\n\nclass LlavaMistralModel(LlavaMetaModel, MistralModel):\n    config_class = LlavaMistralConfig\n\n    def __init__(self, config: MistralConfig):\n        super(LlavaMistralModel, self).__init__(config)\n\n\nclass LlavaMistralForCausalLM(MistralForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaMistralConfig\n\n    def __init__(self, config):\n        super(MistralForCausalLM, self).__init__(config)\n\n        config.model_type = \"llava_mistral\"\n        config.rope_scaling = None\n\n        self.model = LlavaMistralModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes)\n\n        return super().forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_mistral\", LlavaMistralConfig)\nAutoModelForCausalLM.register(LlavaMistralConfig, LlavaMistralForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/builder.py", "content": "import os\nfrom .clip_encoder import CLIPVisionTower\nfrom .imagebind import ImageBindWrapper\nfrom .open_clip_encoder import OpenCLIPVisionTower\nfrom .hf_vision import HFVisionTower\nfrom .siglip_encoder import SigLipVisionTower\nfrom .clip_encoder import CLIPVisionTower, CLIPVisionTowerS2\nfrom .mlcd_encoder import MLCDVisionTower, MLCDVisionTowerS2\n# from .eva_clip.eva_clip_encoder import EvaClipVisionTower\n# from .dev_eva_clip.eva_vit import EvaViTWrapper\n\n\ndef build_vision_tower(vision_tower_cfg, **kwargs):\n    vision_tower = getattr(vision_tower_cfg, \"mm_vision_tower\", getattr(vision_tower_cfg, \"vision_tower\", None))\n    is_absolute_path_exists = os.path.exists(vision_tower)\n    use_s2 = getattr(vision_tower_cfg, \"s2\", False)\n    if is_absolute_path_exists or vision_tower.startswith(\"openai\") or vision_tower.startswith(\"laion\") or \"ShareGPT4V\" in vision_tower:\n        if use_s2:\n            return CLIPVisionTowerS2(vision_tower, args=vision_tower_cfg, **kwargs)\n        else:\n            return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    elif \"siglip\" in vision_tower:\n        return SigLipVisionTower(vision_tower, vision_tower_cfg=vision_tower_cfg, **kwargs)\n    elif vision_tower.startswith(\"hf:\"):\n        return HFVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    elif vision_tower in [\"imagebind_huge\"]:\n        return ImageBindWrapper(vision_tower, args=vision_tower_cfg, **kwargs)\n    elif vision_tower.startswith(\"open_clip_hub\"):\n        return OpenCLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    elif \"mlcd-vit-bigG-patch14\" in vision_tower:\n        if use_s2:\n            return MLCDVisionTowerS2(vision_tower, args=vision_tower_cfg, **kwargs)\n        else:\n            return MLCDVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n\n    # elif \"internal-eva\" in vision_tower.lower() or \"eva02\" in vision_tower.lower():\n    #     return EvaClipVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n    # elif vision_tower in [\"EVA-CLIP-8B\", \"EVA-CLIP-8B-plus\"]:\n    #     return EvaViTWrapper(vision_tower, args=vision_tower_cfg, **kwargs)\n\n    raise ValueError(f\"Unknown vision tower: {vision_tower}\")\n"}
{"type": "source_file", "path": "llava/model/llava_arch.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom abc import ABC, abstractmethod\n\nimport math\nimport re\nimport time\nimport torch\nimport torch.nn as nn\nfrom .multimodal_encoder.builder import build_vision_tower\nfrom .multimodal_resampler.builder import build_vision_resampler\nfrom .multimodal_projector.builder import build_vision_projector\n\nfrom llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n\nfrom llava.mm_utils import get_anyres_image_grid_shape\nfrom llava.utils import rank0_print, rank_print\nimport random\n\n\nclass LlavaMetaModel:\n\n    def __init__(self, config):\n        super(LlavaMetaModel, self).__init__(config)\n\n        if hasattr(config, \"mm_vision_tower\"):\n            delay_load = getattr(config, \"delay_load\", False)\n            self.vision_tower = build_vision_tower(config, delay_load=delay_load)\n            self.vision_resampler = build_vision_resampler(config, vision_tower=self.vision_tower)\n            self.mm_projector = build_vision_projector(config, vision_cfg=self.vision_tower.config)\n\n            if \"unpad\" in getattr(config, \"mm_patch_merge_type\", \"\"):\n                self.image_newline = nn.Parameter(torch.empty(config.hidden_size, dtype=self.dtype))\n\n    def get_vision_tower(self):\n        vision_tower = getattr(self, \"vision_tower\", None)\n        if type(vision_tower) is list:\n            vision_tower = vision_tower[0]\n        return vision_tower\n\n    def initialize_vision_modules(self, model_args, fsdp=None):\n        vision_tower = model_args.vision_tower\n        mm_vision_select_layer = model_args.mm_vision_select_layer\n        mm_vision_select_feature = model_args.mm_vision_select_feature\n        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n        mm_patch_merge_type = model_args.mm_patch_merge_type\n\n        self.config.mm_vision_tower = vision_tower\n        self.config.vision_tower_pretrained = getattr(model_args, \"vision_tower_pretrained\", \"\")\n\n        if self.get_vision_tower() is None:\n            vision_tower = build_vision_tower(model_args)\n            vision_resampler = build_vision_resampler(model_args, vision_tower=vision_tower)\n            for k, v in vision_resampler.config.items():\n                setattr(self.config, k, v)\n\n            if fsdp is not None and len(fsdp) > 0:\n                self.vision_tower = [vision_tower]\n                self.vision_resampler = [vision_resampler]\n            else:\n                self.vision_tower = vision_tower\n                self.vision_resampler = vision_resampler\n        else:\n            if fsdp is not None and len(fsdp) > 0:\n                vision_resampler = self.vision_resampler[0]\n                vision_tower = self.vision_tower[0]\n            else:\n                vision_resampler = self.vision_resampler\n                vision_tower = self.vision_tower\n            vision_tower.load_model()\n\n            # In case it is frozen by LoRA\n            for p in self.vision_resampler.parameters():\n                p.requires_grad = True\n\n        self.config.use_mm_proj = True\n        self.config.mm_projector_type = getattr(model_args, \"mm_projector_type\", \"linear\")\n        self.config.mm_hidden_size = getattr(vision_resampler, \"hidden_size\", vision_tower.hidden_size)\n        self.config.mm_vision_select_layer = mm_vision_select_layer\n        self.config.mm_vision_select_feature = mm_vision_select_feature\n        self.config.mm_patch_merge_type = mm_patch_merge_type\n\n        \n        if not hasattr(self.config, 'add_faster_video'):\n            if model_args.add_faster_video:\n                embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype))\n                self.faster_token = nn.Parameter(\n                    torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std\n                )\n\n        if getattr(self, \"mm_projector\", None) is None:\n            self.mm_projector = build_vision_projector(self.config, vision_cfg=vision_tower.config)\n\n            if \"unpad\" in mm_patch_merge_type:\n                embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype))\n                self.image_newline = nn.Parameter(torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std)\n        else:\n            # In case it is frozen by LoRA\n            for p in self.mm_projector.parameters():\n                p.requires_grad = True\n\n        if pretrain_mm_mlp_adapter is not None:\n            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location=\"cpu\")\n\n            def get_w(weights, keyword):\n                return {k.split(keyword + \".\")[1]: v for k, v in weights.items() if keyword in k}\n\n            incompatible_keys = self.mm_projector.load_state_dict(get_w(mm_projector_weights, \"mm_projector\"))\n            rank0_print(f\"Loaded mm projector weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}\")\n            incompatible_keys = self.vision_resampler.load_state_dict(get_w(mm_projector_weights, \"vision_resampler\"), strict=False)\n            rank0_print(f\"Loaded vision resampler weights from {pretrain_mm_mlp_adapter}. Incompatible keys: {incompatible_keys}\")\n\n\ndef unpad_image(tensor, original_size):\n    \"\"\"\n    Unpads a PyTorch tensor of a padded and resized image.\n\n    Args:\n    tensor (torch.Tensor): The image tensor, assumed to be in CxHxW format.\n    original_size (tuple): The original size of the image (height, width).\n\n    Returns:\n    torch.Tensor: The unpadded image tensor.\n    \"\"\"\n    original_width, original_height = original_size\n    current_height, current_width = tensor.shape[1:]\n\n    # Compute aspect ratios\n    original_aspect_ratio = original_width / original_height\n    current_aspect_ratio = current_width / current_height\n\n    # Determine padding size and direction\n    if original_aspect_ratio > current_aspect_ratio:\n        # Padding was added to the height\n        scale_factor = current_width / original_width\n        new_height = int(original_height * scale_factor)\n        padding = (current_height - new_height) // 2\n        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n    else:\n        # Padding was added to the width\n        scale_factor = current_height / original_height\n        new_width = int(original_width * scale_factor)\n        padding = (current_width - new_width) // 2\n        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n\n    return unpadded_tensor\n\n\nclass LlavaMetaForCausalLM(ABC):\n\n    @abstractmethod\n    def get_model(self):\n        pass\n\n    def get_vision_tower(self):\n        return self.get_model().get_vision_tower()\n\n    def get_2dPool(self, image_feature, stride=2):\n        height = width = self.get_vision_tower().num_patches_per_side\n        num_frames, num_tokens, num_dim = image_feature.shape\n        image_feature = image_feature.view(num_frames, height, width, -1)\n        image_feature = image_feature.permute(0, 3, 1, 2).contiguous()\n        # image_feature = nn.functional.max_pool2d(image_feature, self.config.mm_spatial_pool_stride)\n        if self.config.mm_spatial_pool_mode == \"average\":\n            image_feature = nn.functional.avg_pool2d(image_feature, stride)\n        elif self.config.mm_spatial_pool_mode == \"max\":\n            image_feature = nn.functional.max_pool2d(image_feature, stride)\n        elif self.config.mm_spatial_pool_mode == \"bilinear\":\n            height, width = image_feature.shape[2:]\n            scaled_shape = [math.ceil(height / stride), math.ceil(width / stride)]\n            image_feature = nn.functional.interpolate(image_feature, size=scaled_shape, mode='bilinear')\n\n        else:\n            raise ValueError(f\"Unexpected mm_spatial_pool_mode: {self.config.mm_spatial_pool_mode}\")\n        image_feature = image_feature.permute(0, 2, 3, 1)\n        image_feature = image_feature.view(num_frames, -1, num_dim)\n        return image_feature\n\n    def encode_images(self, images):\n        image_features = self.get_model().get_vision_tower()(images)\n        # image_features = self.get_model().vision_resampler(image_features, images=images)\n        image_features = self.get_model().mm_projector(image_features)\n        return image_features\n    \n    def encode_multimodals(self, videos_or_images, video_idx_in_batch, split_sizes=None):\n        videos_or_images_features = self.get_model().get_vision_tower()(videos_or_images)\n        per_videos_or_images_features = torch.split(videos_or_images_features, split_sizes, dim=0)  # tuple, (dim_1, 576, 4096)\n        all_videos_or_images_features = []\n        all_faster_video_features = []\n        cur_mm_spatial_pool_stride = self.config.mm_spatial_pool_stride\n\n        for idx, feat in enumerate(per_videos_or_images_features):\n            \n            feat = self.get_model().mm_projector(feat)\n            faster_video_feature = 0\n            slower_img_feat = 0\n            if idx in video_idx_in_batch and cur_mm_spatial_pool_stride > 1:\n                slower_img_feat = self.get_2dPool(feat,cur_mm_spatial_pool_stride)\n                if self.config.add_faster_video:\n                    cur_mm_spatial_pool_stride = cur_mm_spatial_pool_stride * 2\n                    faster_video_feature = self.get_2dPool(feat,cur_mm_spatial_pool_stride)\n            if slower_img_feat is not 0:\n                all_videos_or_images_features.append(slower_img_feat)\n            else:\n                all_videos_or_images_features.append(feat)\n            all_faster_video_features.append(faster_video_feature)\n        return all_videos_or_images_features,all_faster_video_features\n\n    def add_token_per_grid(self, image_feature):\n        resize_h = int(math.sqrt(image_feature.shape[1]))\n        num_frames = image_feature.shape[0]\n        feature_dim = image_feature.shape[-1]\n\n        image_feature = image_feature.view(num_frames, 1, resize_h, resize_h, -1)\n        image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n        image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n        image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n        if getattr(self.config, \"add_faster_video\", False):\n            # import pdb; pdb.set_trace()\n            # (3584, 832, 14) -> (3584, 64, 13, 14)\n            image_feature = image_feature.view(feature_dim, num_frames,resize_h, -1)\n            #  (3584, 64, 13, 14) -> (64, 13, 14, 3584)\n            image_feature = image_feature.permute(1, 2, 3, 0).contiguous()\n            # (64, 13, 14, 3584) -> (64, 13*14, 3584)\n            image_feature = image_feature.flatten(1, 2)\n            # import pdb; pdb.set_trace()\n            return image_feature\n        # import pdb; pdb.set_trace()\n        image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n        return image_feature\n\n    def add_token_per_frame(self, image_feature):\n        image_feature = image_feature.permute(2, 0, 1).contiguous()\n        image_feature =  torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n        image_feature = image_feature.permute(1, 2, 0).contiguous()\n        return image_feature\n\n    def prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities=[\"image\"], image_sizes=None):\n        vision_tower = self.get_vision_tower()\n        # rank_print(modalities)\n        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n            return input_ids, position_ids, attention_mask, past_key_values, None, labels\n\n        if isinstance(modalities, str):\n            modalities = [modalities]\n\n        # import pdb; pdb.set_trace()\n        if type(images) is list or images.ndim == 5:\n            if type(images) is list:\n                images = [x.unsqueeze(0) if x.ndim == 3 else x for x in images]\n\n            video_idx_in_batch = []\n            for _ in range(len(modalities)):\n                if modalities[_] == \"video\":\n                    video_idx_in_batch.append(_)\n\n            images_list = []\n            for image in images:\n                if image.ndim == 4:\n                    images_list.append(image)\n                else:\n                    images_list.append(image.unsqueeze(0))\n\n            concat_images = torch.cat([image for image in images_list], dim=0)\n            split_sizes = [image.shape[0] for image in images_list]\n            encoded_image_features = self.encode_images(concat_images)\n            # image_features,all_faster_video_features = self.encode_multimodals(concat_images, video_idx_in_batch, split_sizes)\n\n            # This is a list, each element is [num_images, patch * patch, dim]\n            # rank_print(f\"Concat images : {concat_images.shape}\")\n            encoded_image_features = torch.split(encoded_image_features, split_sizes)\n            image_features = []\n            for idx, image_feat in enumerate(encoded_image_features):\n                if idx in video_idx_in_batch:\n                    image_features.append(self.get_2dPool(image_feat))\n                else:\n                    image_features.append(image_feat)\n            # image_features = self.encode_multimodals(concat_images, video_idx_in_batch, split_sizes)\n            # rank_print(f\"Encoded image feats : {[x.shape for x in image_features]}\")\n            # image_features = torch.split(image_features, split_sizes, dim=0)\n            mm_patch_merge_type = getattr(self.config, \"mm_patch_merge_type\", \"flat\")\n            image_aspect_ratio = getattr(self.config, \"image_aspect_ratio\", \"square\")\n            mm_newline_position = getattr(self.config, \"mm_newline_position\", \"one_token\")\n\n            if mm_patch_merge_type == \"flat\":\n                image_features = [x.flatten(0, 1) for x in image_features]\n\n            elif mm_patch_merge_type.startswith(\"spatial\"):\n                new_image_features = []\n                for image_idx, image_feature in enumerate(image_features):\n                    # FIXME: now assume the image is square, and split to 2x2 patches\n                    # num_patches = h * w, where h = w = sqrt(num_patches)\n                    # currently image_feature is a tensor of shape (4, num_patches, hidden_size)\n                    # we want to first unflatten it to (2, 2, h, w, hidden_size)\n                    # rank0_print(\"At least we are reaching here\")\n                    # import pdb; pdb.set_trace()\n                    if image_idx in video_idx_in_batch:  # video operations\n                        # rank0_print(\"Video\")\n                        if mm_newline_position == \"grid\":\n                            # Grid-wise\n                            image_feature = self.add_token_per_grid(image_feature)\n                            if getattr(self.config, \"add_faster_video\", False):\n                                faster_video_feature = self.add_token_per_grid(all_faster_video_features[image_idx])\n                                # Add a token for each frame\n                                concat_slow_fater_token = []\n                                # import pdb; pdb.set_trace()\n                                for _ in range(image_feature.shape[0]):\n                                    if _ % self.config.faster_token_stride == 0:\n                                        concat_slow_fater_token.append(torch.cat((image_feature[_], self.model.faster_token[None].to(image_feature.device)), dim=0))\n                                    else:\n                                        concat_slow_fater_token.append(torch.cat((faster_video_feature[_], self.model.faster_token[None].to(image_feature.device)), dim=0))\n                                # import pdb; pdb.set_trace()\n                                image_feature = torch.cat(concat_slow_fater_token)\n\n                                # print(\"!!!!!!!!!!!!\")\n                        \n                            new_image_features.append(image_feature)\n                        elif mm_newline_position == \"frame\":\n                            # Frame-wise\n                            image_feature = self.add_token_per_frame(image_feature)\n\n                            new_image_features.append(image_feature.flatten(0, 1))\n                            \n                        elif mm_newline_position == \"one_token\":\n                            # one-token\n                            image_feature = image_feature.flatten(0, 1)\n                            if 'unpad' in mm_patch_merge_type:\n                                image_feature = torch.cat((\n                                    image_feature,\n                                    self.model.image_newline[None].to(image_feature.device)\n                                ), dim=0)\n                            new_image_features.append(image_feature)      \n                        elif mm_newline_position == \"no_token\":\n                            new_image_features.append(image_feature.flatten(0, 1))\n                        else:\n                            raise ValueError(f\"Unexpected mm_newline_position: {mm_newline_position}\")\n                    elif image_feature.shape[0] > 1:  # multi patches and multi images operations\n                        # rank0_print(\"Single-images\")\n                        base_image_feature = image_feature[0]\n                        image_feature = image_feature[1:]\n                        height = width = self.get_vision_tower().num_patches_per_side\n                        assert height * width == base_image_feature.shape[0]\n\n                        if \"anyres_max\" in image_aspect_ratio:\n                            matched_anyres_max_num_patches = re.match(r\"anyres_max_(\\d+)\", image_aspect_ratio)\n                            if matched_anyres_max_num_patches:\n                                max_num_patches = int(matched_anyres_max_num_patches.group(1))\n\n                        if image_aspect_ratio == \"anyres\" or \"anyres_max\" in image_aspect_ratio:\n                            if hasattr(self.get_vision_tower(), \"image_size\"):\n                                vision_tower_image_size = self.get_vision_tower().image_size\n                            else:\n                                raise ValueError(\"vision_tower_image_size is not found in the vision tower.\")\n                            try:\n                                num_patch_width, num_patch_height = get_anyres_image_grid_shape(image_sizes[image_idx], self.config.image_grid_pinpoints, vision_tower_image_size)\n                            except Exception as e:\n                                rank0_print(f\"Error: {e}\")\n                                num_patch_width, num_patch_height = 2, 2\n                            image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1)\n                        else:\n                            image_feature = image_feature.view(2, 2, height, width, -1)\n\n                        if \"maxpool2x2\" in mm_patch_merge_type:\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = nn.functional.max_pool2d(image_feature, 2)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        elif \"unpad\" in mm_patch_merge_type and \"anyres_max\" in image_aspect_ratio and matched_anyres_max_num_patches:\n                            unit = image_feature.shape[2]\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = unpad_image(image_feature, image_sizes[image_idx])\n                            c, h, w = image_feature.shape\n                            times = math.sqrt(h * w / (max_num_patches * unit**2))\n                            if times > 1.1:\n                                image_feature = image_feature[None]\n                                image_feature = nn.functional.interpolate(image_feature, [int(h // times), int(w // times)], mode=\"bilinear\")[0]\n                            image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        elif \"unpad\" in mm_patch_merge_type:\n                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n                            image_feature = unpad_image(image_feature, image_sizes[image_idx])\n                            image_feature = torch.cat((image_feature, self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)), dim=-1)\n                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n                        else:\n                            image_feature = image_feature.permute(0, 2, 1, 3, 4).contiguous()\n                            image_feature = image_feature.flatten(0, 3)\n                        if \"nobase\" in mm_patch_merge_type:\n                            pass\n                        else:\n                            image_feature = torch.cat((base_image_feature, image_feature), dim=0)\n                        new_image_features.append(image_feature)\n                    else:  # single image operations\n                        image_feature = image_feature[0]\n                        if \"unpad\" in mm_patch_merge_type:\n                            image_feature = torch.cat((image_feature, self.model.image_newline[None]), dim=0)\n\n                        new_image_features.append(image_feature)\n                image_features = new_image_features\n            else:\n                raise ValueError(f\"Unexpected mm_patch_merge_type: {self.config.mm_patch_merge_type}\")\n        else:\n            image_features = self.encode_images(images)\n\n        # TODO: image start / end is not implemented here to support pretraining.\n        if getattr(self.config, \"tune_mm_mlp_adapter\", False) and getattr(self.config, \"mm_use_im_start_end\", False):\n            raise NotImplementedError\n        # rank_print(f\"Total images : {len(image_features)}\")\n\n        # Let's just add dummy tensors if they do not exist,\n        # it is a headache to deal with None all the time.\n        # But it is not ideal, and if you have a better idea,\n        # please open an issue / submit a PR, thanks.\n        _labels = labels\n        _position_ids = position_ids\n        _attention_mask = attention_mask\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n        else:\n            attention_mask = attention_mask.bool()\n        if position_ids is None:\n            position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n        if labels is None:\n            labels = torch.full_like(input_ids, IGNORE_INDEX)\n\n        # remove the padding using attention_mask -- FIXME\n        _input_ids = input_ids\n        input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n        labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n\n        new_input_embeds = []\n        new_labels = []\n        cur_image_idx = 0\n        # rank_print(\"Inserting Images embedding\")\n        for batch_idx, cur_input_ids in enumerate(input_ids):\n            num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n            # rank0_print(num_images)\n            if num_images == 0:\n                cur_image_features = image_features[cur_image_idx]\n                cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\n                cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n                new_input_embeds.append(cur_input_embeds)\n                new_labels.append(labels[batch_idx])\n                cur_image_idx += 1\n                continue\n\n            image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\n            cur_input_ids_noim = []\n            cur_labels = labels[batch_idx]\n            cur_labels_noim = []\n            for i in range(len(image_token_indices) - 1):\n                cur_input_ids_noim.append(cur_input_ids[image_token_indices[i] + 1 : image_token_indices[i + 1]])\n                cur_labels_noim.append(cur_labels[image_token_indices[i] + 1 : image_token_indices[i + 1]])\n            split_sizes = [x.shape[0] for x in cur_labels_noim]\n            cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\n            cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n            cur_new_input_embeds = []\n            cur_new_labels = []\n\n            for i in range(num_images + 1):\n                cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n                cur_new_labels.append(cur_labels_noim[i])\n                if i < num_images:\n                    try:\n                        cur_image_features = image_features[cur_image_idx]\n                    except IndexError:\n                        cur_image_features = image_features[cur_image_idx - 1]\n                    cur_image_idx += 1\n                    cur_new_input_embeds.append(cur_image_features)\n                    cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n\n\n            self._process_token(cur_new_input_embeds, cur_new_labels, IGNORE_INDEX)\n            cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n\n            # import pdb; pdb.set_trace()\n            cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n            cur_new_labels = torch.cat(cur_new_labels)\n\n            new_input_embeds.append(cur_new_input_embeds)\n            new_labels.append(cur_new_labels)\n\n        # Truncate sequences to max length as image embeddings can make the sequence longer\n        tokenizer_model_max_length = getattr(self.config, \"tokenizer_model_max_length\", None)\n        # rank_print(\"Finishing Inserting\")\n\n        new_input_embeds = [x[:tokenizer_model_max_length] for x, modality in zip(new_input_embeds, modalities)]\n        new_labels = [x[:tokenizer_model_max_length] for x, modality in zip(new_labels, modalities)]\n        # TODO: Hard code for control loss spike\n        # if tokenizer_model_max_length is not None:\n        #     new_input_embeds = [x[:4096] if modality != \"video\" else x[:tokenizer_model_max_length] for x, modality in zip(new_input_embeds, modalities)]\n        #     new_labels = [x[:4096] if modality != \"video\" else x[:tokenizer_model_max_length] for x, modality in zip(new_labels, modalities)]\n\n        # Combine them\n        max_len = max(x.shape[0] for x in new_input_embeds)\n        batch_size = len(new_input_embeds)\n\n        new_input_embeds_padded = []\n        new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n        attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n        position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\n        # rank0_print(\"Prepare pos id\")\n\n        for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n            cur_len = cur_new_embed.shape[0]\n            if getattr(self.config, \"tokenizer_padding_side\", \"right\") == \"left\":\n                new_input_embeds_padded.append(torch.cat((torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device), cur_new_embed), dim=0))\n                if cur_len > 0:\n                    new_labels_padded[i, -cur_len:] = cur_new_labels\n                    attention_mask[i, -cur_len:] = True\n                    position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n            else:\n                new_input_embeds_padded.append(torch.cat((cur_new_embed, torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)), dim=0))\n                if cur_len > 0:\n                    new_labels_padded[i, :cur_len] = cur_new_labels\n                    attention_mask[i, :cur_len] = True\n                    position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n\n        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n        # rank0_print(\"tokenizer padding\")\n\n        if _labels is None:\n            new_labels = None\n        else:\n            new_labels = new_labels_padded\n\n        if _attention_mask is None:\n            attention_mask = None\n        else:\n            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n\n        if _position_ids is None:\n            position_ids = None\n        if getattr(self.config, \"use_pos_skipping\", False) and self.training:\n            position_ids = torch.arange(new_input_embeds.size(1), device=new_input_embeds.device).unsqueeze(0).to(new_input_embeds.device)\n            split_position = random.randint(0, new_input_embeds.size(1))\n            left_add = random.randint(0, self.config.pos_skipping_range)\n            right_add = random.randint(left_add, self.config.pos_skipping_range)\n            position_ids[:, :split_position] += left_add\n            position_ids[:, split_position:] += right_add\n        # import pdb; pdb.set_trace()\n        # rank0_print(\"Finish preparing\")\n        return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels\n\n    def initialize_vision_tokenizer(self, model_args, tokenizer):\n        if model_args.mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n\n        if model_args.mm_use_im_start_end:\n            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n            self.resize_token_embeddings(len(tokenizer))\n\n            if num_new_tokens > 0:\n                input_embeddings = self.get_input_embeddings().weight.data\n                output_embeddings = self.get_output_embeddings().weight.data\n\n                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n\n            if model_args.tune_mm_mlp_adapter:\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = True\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n\n            if model_args.pretrain_mm_mlp_adapter:\n                mm_projector_weights = torch.load(model_args.pretrain_mm_mlp_adapter, map_location=\"cpu\")\n                embed_tokens_weight = mm_projector_weights[\"model.embed_tokens.weight\"]\n                assert num_new_tokens == 2\n                if input_embeddings.shape == embed_tokens_weight.shape:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n                elif embed_tokens_weight.shape[0] == num_new_tokens:\n                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n                else:\n                    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n        elif model_args.mm_use_im_patch_token:\n            if model_args.tune_mm_mlp_adapter:\n                for p in self.get_input_embeddings().parameters():\n                    p.requires_grad = False\n                for p in self.get_output_embeddings().parameters():\n                    p.requires_grad = False\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/factory.py", "content": "import json\nimport logging\nimport os\nimport pathlib\nimport re\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union, Dict, Any\nimport torch\n\ntry:\n    import deepspeed\nexcept ImportError:\n    deepspeed = None\n\nfrom .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\nfrom .model import CLIP, CustomCLIP, convert_weights_to_lp, convert_to_custom_text_state_dict, get_cast_dtype\nfrom .openai import load_openai_model\nfrom .pretrained import is_pretrained_cfg, get_pretrained_cfg, download_pretrained, list_pretrained_tags_by_model\nfrom .transform import image_transform\nfrom .tokenizer import HFTokenizer, tokenize\nfrom .utils import resize_clip_pos_embed, resize_evaclip_pos_embed, resize_visual_pos_embed, resize_eva_pos_embed\n\n\n_MODEL_CONFIG_PATHS = [Path(__file__).parent / f\"model_configs/\"]\n_MODEL_CONFIGS = {}  # directory (model_name: config) of model architecture configs\n\n\ndef _natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\n\n\ndef _rescan_model_configs():\n    global _MODEL_CONFIGS\n\n    config_ext = (\".json\",)\n    config_files = []\n    for config_path in _MODEL_CONFIG_PATHS:\n        if config_path.is_file() and config_path.suffix in config_ext:\n            config_files.append(config_path)\n        elif config_path.is_dir():\n            for ext in config_ext:\n                config_files.extend(config_path.glob(f\"*{ext}\"))\n\n    for cf in config_files:\n        with open(cf, \"r\", encoding=\"utf8\") as f:\n            model_cfg = json.load(f)\n            if all(a in model_cfg for a in (\"embed_dim\", \"vision_cfg\", \"text_cfg\")):\n                _MODEL_CONFIGS[cf.stem] = model_cfg\n\n    _MODEL_CONFIGS = dict(sorted(_MODEL_CONFIGS.items(), key=lambda x: _natural_key(x[0])))\n\n\n_rescan_model_configs()  # initial populate of model config registry\n\n\ndef list_models():\n    \"\"\"enumerate available model architectures based on config files\"\"\"\n    return list(_MODEL_CONFIGS.keys())\n\n\ndef add_model_config(path):\n    \"\"\"add model config path or file and update registry\"\"\"\n    if not isinstance(path, Path):\n        path = Path(path)\n    _MODEL_CONFIG_PATHS.append(path)\n    _rescan_model_configs()\n\n\ndef get_model_config(model_name):\n    if model_name in _MODEL_CONFIGS:\n        return deepcopy(_MODEL_CONFIGS[model_name])\n    else:\n        return None\n\n\ndef get_tokenizer(model_name):\n    config = get_model_config(model_name)\n    tokenizer = HFTokenizer(config[\"text_cfg\"][\"hf_tokenizer_name\"]) if \"hf_tokenizer_name\" in config[\"text_cfg\"] else tokenize\n    return tokenizer\n\n\n# loading openai CLIP weights when is_openai=True for training\ndef load_state_dict(checkpoint_path: str, map_location: str = \"cpu\", model_key: str = \"model|module|state_dict\", is_openai: bool = False, skip_list: list = []):\n    if is_openai:\n        model = torch.jit.load(checkpoint_path, map_location=\"cpu\").eval()\n        state_dict = model.state_dict()\n        for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n            state_dict.pop(key, None)\n    else:\n        checkpoint = torch.load(checkpoint_path, map_location=map_location)\n        for mk in model_key.split(\"|\"):\n            if isinstance(checkpoint, dict) and mk in checkpoint:\n                state_dict = checkpoint[mk]\n                break\n            else:\n                state_dict = checkpoint\n        if next(iter(state_dict.items()))[0].startswith(\"module\"):\n            state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n    for k in skip_list:\n        if k in list(state_dict.keys()):\n            logging.info(f\"Removing key {k} from pretrained checkpoint\")\n            del state_dict[k]\n\n    if os.getenv(\"RoPE\") == \"1\":\n        for k in list(state_dict.keys()):\n            if \"freqs_cos\" in k or \"freqs_sin\" in k:\n                del state_dict[k]\n    return state_dict\n\n\ndef load_checkpoint(model, checkpoint_path, model_key=\"model|module|state_dict\", strict=True):\n    state_dict = load_state_dict(checkpoint_path, model_key=model_key, is_openai=False)\n    # detect old format and make compatible with new format\n    if \"positional_embedding\" in state_dict and not hasattr(model, \"positional_embedding\"):\n        state_dict = convert_to_custom_text_state_dict(state_dict)\n    if \"text.logit_scale\" in state_dict and hasattr(model, \"logit_scale\"):\n        state_dict[\"logit_scale\"] = state_dict[\"text.logit_scale\"]\n        del state_dict[\"text.logit_scale\"]\n\n    # resize_clip_pos_embed for CLIP and open CLIP\n    if \"visual.positional_embedding\" in state_dict:\n        resize_clip_pos_embed(state_dict, model)\n    # specified to eva_vit_model\n    elif \"visual.pos_embed\" in state_dict:\n        resize_evaclip_pos_embed(state_dict, model)\n\n    # resize_clip_pos_embed(state_dict, model)\n    incompatible_keys = model.load_state_dict(state_dict, strict=strict)\n    logging.info(f\"incompatible_keys.missing_keys: {incompatible_keys.missing_keys}\")\n    return incompatible_keys\n\n\ndef load_clip_visual_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n\n    for k in list(state_dict.keys()):\n        if not k.startswith(\"visual.\"):\n            del state_dict[k]\n    for k in list(state_dict.keys()):\n        if k.startswith(\"visual.\"):\n            new_k = k[7:]\n            state_dict[new_k] = state_dict[k]\n            del state_dict[k]\n    return state_dict\n\n\ndef load_clip_text_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n\n    for k in list(state_dict.keys()):\n        if k.startswith(\"visual.\"):\n            del state_dict[k]\n    return state_dict\n\n\ndef get_pretrained_tag(pretrained_model):\n    pretrained_model = pretrained_model.lower()\n    if \"laion\" in pretrained_model or \"open_clip\" in pretrained_model:\n        return \"open_clip\"\n    elif \"openai\" in pretrained_model:\n        return \"clip\"\n    elif \"eva\" in pretrained_model and \"clip\" in pretrained_model:\n        return \"eva_clip\"\n    else:\n        return \"other\"\n\n\ndef load_zero_partitions(model, state_dict, is_deepspeed_zero3_enabled, pretrained_model_path, ignore_mismatched_sizes=False):\n    \"\"\"\n    adept from pytorch lightning and transformers\n    with deepspeed.zero.Init():\n        model = MyModel()\n    state_dict = torch.load(model_path, map_location=\"cpu\")\n    load_zero_partitions(model, prefix=\"\")\n    \"\"\"\n\n    # because zero3 puts placeholders in model params, this context\n    # manager gathers (unpartitions) the params of the current layer, then loads from\n    # the state dict and then re-partitions them again\n    model_state_dict = model.state_dict()\n    expected_keys = list(model_state_dict.keys())\n    loaded_keys = list(state_dict.keys())\n    missing_keys = list(set(expected_keys) - set(loaded_keys))\n    unexpected_keys = list(set(loaded_keys) - set(expected_keys))\n\n    # Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\n    # matching the weights in the model.\n    mismatched_keys = []\n    if ignore_mismatched_sizes:\n        for checkpoint_key in loaded_keys:\n            model_key = checkpoint_key\n\n            if model_key in model_state_dict and state_dict[checkpoint_key].shape != model_state_dict[model_key].shape:\n                mismatched_keys.append((checkpoint_key, state_dict[checkpoint_key].shape, model_state_dict[model_key].shape))\n                del state_dict[checkpoint_key]\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(state_dict, \"_metadata\", None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    error_msgs = []\n\n    # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants\n    # so we need to apply the function recursively.\n    def load(module, prefix=\"\"):\n        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n        args = (state_dict, prefix, local_metadata, True, [], [], error_msgs)\n        if is_deepspeed_zero3_enabled:\n            # because zero3 puts placeholders in model params, this context\n            # manager gathers (unpartitions) the params of the current layer, then loads from\n            # the state dict and then re-partitions them again\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\n                if torch.distributed.get_rank() == 0:\n                    module._load_from_state_dict(*args)\n        else:\n            module._load_from_state_dict(*args)\n\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + \".\")\n\n    # Make sure we are able to load base models as well as derived models (with heads)\n    start_prefix = \"\"\n    model_to_load = model\n    load(model_to_load, prefix=start_prefix)\n    del state_dict\n    if len(error_msgs) > 0:\n        error_msg = \"\\n\\t\".join(error_msgs)\n        if \"size mismatch\" in error_msg:\n            error_msg += \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n        raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n    if len(unexpected_keys) > 0:\n        logging.warning(\n            f\"Some weights of the model checkpoint at {pretrained_model_path} were not used when\"\n            f\" initializing {model.__class__.__name__}: {unexpected_keys}\\n- This IS expected if you are\"\n            f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task or\"\n            \" with another architecture (e.g. initializing a BertForSequenceClassification model from a\"\n            \" BertForPreTraining model).\\n- This IS NOT expected if you are initializing\"\n            f\" {model.__class__.__name__} from the checkpoint of a model that you expect to be exactly identical\"\n            \" (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n        )\n    else:\n        logging.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n    if len(missing_keys) > 0:\n        logging.warning(\n            f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n            f\" {pretrained_model_path} and are newly initialized: {missing_keys}\\nYou should probably\"\n            \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n        )\n    elif len(mismatched_keys) == 0:\n        logging.info(\n            f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at\"\n            f\" {pretrained_model_path}.\\nIf your task is similar to the task the model of the checkpoint\"\n            f\" was trained on, you can already use {model.__class__.__name__} for predictions without further\"\n            \" training.\"\n        )\n    if len(mismatched_keys) > 0:\n        mismatched_warning = \"\\n\".join([f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\" for key, shape1, shape2 in mismatched_keys])\n        logging.warning(\n            f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n            f\" {pretrained_model_path} and are newly initialized because the shapes did not\"\n            f\" match:\\n{mismatched_warning}\\nYou should probably TRAIN this model on a down-stream task to be able\"\n            \" to use it for predictions and inference.\"\n        )\n\n\ndef load_pretrained_checkpoint(model, visual_checkpoint_path, text_checkpoint_path, strict=True, visual_model=None, text_model=None, model_key=\"model|module|state_dict\", skip_list=[]):\n    visual_tag = get_pretrained_tag(visual_model)\n    text_tag = get_pretrained_tag(text_model)\n\n    logging.info(f\"num of model state_dict keys: {len(model.state_dict().keys())}\")\n    visual_incompatible_keys, text_incompatible_keys = None, None\n    if visual_checkpoint_path:\n        if visual_tag == \"eva_clip\" or visual_tag == \"open_clip\":\n            visual_state_dict = load_clip_visual_state_dict(visual_checkpoint_path, is_openai=False, skip_list=skip_list)\n        elif visual_tag == \"clip\":\n            visual_state_dict = load_clip_visual_state_dict(visual_checkpoint_path, is_openai=True, skip_list=skip_list)\n        else:\n            visual_state_dict = load_state_dict(visual_checkpoint_path, model_key=model_key, is_openai=False, skip_list=skip_list)\n\n        # resize_clip_pos_embed for CLIP and open CLIP\n        if \"positional_embedding\" in visual_state_dict:\n            resize_visual_pos_embed(visual_state_dict, model)\n        # specified to EVA model\n        elif \"pos_embed\" in visual_state_dict:\n            resize_eva_pos_embed(visual_state_dict, model)\n\n        visual_incompatible_keys = model.visual.load_state_dict(visual_state_dict, strict=strict)\n        logging.info(f\"num of loaded visual_state_dict keys: {len(visual_state_dict.keys())}\")\n        logging.info(f\"visual_incompatible_keys.missing_keys: {visual_incompatible_keys.missing_keys}\")\n\n    if text_checkpoint_path:\n        if text_tag == \"eva_clip\" or text_tag == \"open_clip\":\n            text_state_dict = load_clip_text_state_dict(text_checkpoint_path, is_openai=False, skip_list=skip_list)\n        elif text_tag == \"clip\":\n            text_state_dict = load_clip_text_state_dict(text_checkpoint_path, is_openai=True, skip_list=skip_list)\n        else:\n            text_state_dict = load_state_dict(visual_checkpoint_path, model_key=model_key, is_openai=False, skip_list=skip_list)\n\n        text_incompatible_keys = model.text.load_state_dict(text_state_dict, strict=strict)\n\n        logging.info(f\"num of loaded text_state_dict keys: {len(text_state_dict.keys())}\")\n        logging.info(f\"text_incompatible_keys.missing_keys: {text_incompatible_keys.missing_keys}\")\n\n    return visual_incompatible_keys, text_incompatible_keys\n\n\ndef create_model(\n    model_name: str,\n    pretrained: Optional[str] = None,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    pretrained_image: str = \"\",\n    pretrained_text: str = \"\",\n    pretrained_hf: bool = True,\n    pretrained_visual_model: str = None,\n    pretrained_text_model: str = None,\n    cache_dir: Optional[str] = None,\n    skip_list: list = [],\n):\n    model_name = model_name.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n    if isinstance(device, str):\n        device = torch.device(device)\n\n    if pretrained and pretrained.lower() == \"openai\":\n        logging.info(f\"Loading pretrained {model_name} from OpenAI.\")\n        model = load_openai_model(\n            model_name,\n            precision=precision,\n            device=device,\n            jit=jit,\n            cache_dir=cache_dir,\n        )\n    else:\n        model_cfg = get_model_config(model_name)\n        if model_cfg is not None:\n            logging.info(f\"Loaded {model_name} model config.\")\n        else:\n            logging.error(f\"Model config for {model_name} not found; available models {list_models()}.\")\n            raise RuntimeError(f\"Model config for {model_name} not found.\")\n\n        if \"rope\" in model_cfg.get(\"vision_cfg\", {}):\n            if model_cfg[\"vision_cfg\"][\"rope\"]:\n                os.environ[\"RoPE\"] = \"1\"\n        else:\n            os.environ[\"RoPE\"] = \"0\"\n\n        if force_quick_gelu:\n            # override for use of QuickGELU on non-OpenAI transformer models\n            model_cfg[\"quick_gelu\"] = True\n\n        if force_patch_dropout is not None:\n            # override the default patch dropout value\n            model_cfg[\"vision_cfg\"][\"patch_dropout\"] = force_patch_dropout\n\n        cast_dtype = get_cast_dtype(precision)\n        custom_clip = model_cfg.pop(\"custom_text\", False) or force_custom_clip or (\"hf_model_name\" in model_cfg[\"text_cfg\"])\n\n        if custom_clip:\n            if \"hf_model_name\" in model_cfg.get(\"text_cfg\", {}):\n                model_cfg[\"text_cfg\"][\"hf_model_pretrained\"] = pretrained_hf\n            model = CustomCLIP(**model_cfg, cast_dtype=cast_dtype)\n        else:\n            model = CLIP(**model_cfg, cast_dtype=cast_dtype)\n\n        pretrained_cfg = {}\n        if pretrained:\n            checkpoint_path = \"\"\n            pretrained_cfg = get_pretrained_cfg(model_name, pretrained)\n            if pretrained_cfg:\n                checkpoint_path = download_pretrained(pretrained_cfg, cache_dir=cache_dir)\n            elif os.path.exists(pretrained):\n                checkpoint_path = pretrained\n\n            if checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name} weights ({pretrained}).\")\n                load_checkpoint(model, checkpoint_path, model_key=\"model|module|state_dict\", strict=False)\n            else:\n                error_str = f\"Pretrained weights ({pretrained}) not found for model {model_name}.\" f\"Available pretrained tags ({list_pretrained_tags_by_model(model_name)}.\"\n                logging.warning(error_str)\n                raise RuntimeError(error_str)\n        else:\n            visual_checkpoint_path = \"\"\n            text_checkpoint_path = \"\"\n\n            if pretrained_image:\n                pretrained_visual_model = pretrained_visual_model.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n                pretrained_image_cfg = get_pretrained_cfg(pretrained_visual_model, pretrained_image)\n                if \"timm_model_name\" in model_cfg.get(\"vision_cfg\", {}):\n                    # pretrained weight loading for timm models set via vision_cfg\n                    model_cfg[\"vision_cfg\"][\"timm_model_pretrained\"] = True\n                elif pretrained_image_cfg:\n                    visual_checkpoint_path = download_pretrained(pretrained_image_cfg, cache_dir=cache_dir)\n                elif os.path.exists(pretrained_image):\n                    visual_checkpoint_path = pretrained_image\n                else:\n                    logging.warning(f\"Pretrained weights ({visual_checkpoint_path}) not found for model {model_name}.visual.\")\n                    raise RuntimeError(f\"Pretrained weights ({visual_checkpoint_path}) not found for model {model_name}.visual.\")\n\n            if pretrained_text:\n                pretrained_text_model = pretrained_text_model.replace(\"/\", \"-\")  # for callers using old naming with / in ViT names\n                pretrained_text_cfg = get_pretrained_cfg(pretrained_text_model, pretrained_text)\n                if pretrained_image_cfg:\n                    text_checkpoint_path = download_pretrained(pretrained_text_cfg, cache_dir=cache_dir)\n                elif os.path.exists(pretrained_text):\n                    text_checkpoint_path = pretrained_text\n                else:\n                    logging.warning(f\"Pretrained weights ({text_checkpoint_path}) not found for model {model_name}.text.\")\n                    raise RuntimeError(f\"Pretrained weights ({text_checkpoint_path}) not found for model {model_name}.text.\")\n\n            if visual_checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name}.visual weights ({visual_checkpoint_path}).\")\n            if text_checkpoint_path:\n                logging.info(f\"Loading pretrained {model_name}.text weights ({text_checkpoint_path}).\")\n\n            if visual_checkpoint_path or text_checkpoint_path:\n                load_pretrained_checkpoint(model, visual_checkpoint_path, text_checkpoint_path, strict=False, visual_model=pretrained_visual_model, text_model=pretrained_text_model, model_key=\"model|module|state_dict\", skip_list=skip_list)\n\n        if \"fp16\" in precision or \"bf16\" in precision:\n            logging.info(f\"convert precision to {precision}\")\n            model = model.to(torch.bfloat16) if \"bf16\" in precision else model.to(torch.float16)\n\n        # model.to(device=device)\n\n        # set image / mean metadata from pretrained_cfg if available, or use default\n        model.visual.image_mean = pretrained_cfg.get(\"mean\", None) or OPENAI_DATASET_MEAN\n        model.visual.image_std = pretrained_cfg.get(\"std\", None) or OPENAI_DATASET_STD\n\n        if jit:\n            model = torch.jit.script(model)\n\n    return model\n\n\ndef create_model_and_transforms(\n    model_name: str,\n    pretrained: Optional[str] = None,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    pretrained_image: str = \"\",\n    pretrained_text: str = \"\",\n    pretrained_hf: bool = True,\n    pretrained_visual_model: str = None,\n    pretrained_text_model: str = None,\n    image_mean: Optional[Tuple[float, ...]] = None,\n    image_std: Optional[Tuple[float, ...]] = None,\n    cache_dir: Optional[str] = None,\n    skip_list: list = [],\n):\n    model = create_model(\n        model_name,\n        pretrained,\n        precision=precision,\n        device=device,\n        jit=jit,\n        force_quick_gelu=force_quick_gelu,\n        force_custom_clip=force_custom_clip,\n        force_patch_dropout=force_patch_dropout,\n        pretrained_image=pretrained_image,\n        pretrained_text=pretrained_text,\n        pretrained_hf=pretrained_hf,\n        pretrained_visual_model=pretrained_visual_model,\n        pretrained_text_model=pretrained_text_model,\n        cache_dir=cache_dir,\n        skip_list=skip_list,\n    )\n\n    image_mean = image_mean or getattr(model.visual, \"image_mean\", None)\n    image_std = image_std or getattr(model.visual, \"image_std\", None)\n    preprocess_train = image_transform(model.visual.image_size, is_train=True, mean=image_mean, std=image_std)\n    preprocess_val = image_transform(model.visual.image_size, is_train=False, mean=image_mean, std=image_std)\n\n    return model, preprocess_train, preprocess_val\n\n\ndef create_model_from_pretrained(\n    model_name: str,\n    pretrained: str,\n    precision: str = \"fp32\",\n    device: Union[str, torch.device] = \"cpu\",\n    jit: bool = False,\n    force_quick_gelu: bool = False,\n    force_custom_clip: bool = False,\n    force_patch_dropout: Optional[float] = None,\n    return_transform: bool = True,\n    image_mean: Optional[Tuple[float, ...]] = None,\n    image_std: Optional[Tuple[float, ...]] = None,\n    cache_dir: Optional[str] = None,\n    is_frozen: bool = False,\n):\n    if not is_pretrained_cfg(model_name, pretrained) and not os.path.exists(pretrained):\n        raise RuntimeError(f\"{pretrained} is not a valid pretrained cfg or checkpoint for {model_name}.\" f\" Use open_clip.list_pretrained() to find one.\")\n\n    model = create_model(\n        model_name,\n        pretrained,\n        precision=precision,\n        device=device,\n        jit=jit,\n        force_quick_gelu=force_quick_gelu,\n        force_custom_clip=force_custom_clip,\n        force_patch_dropout=force_patch_dropout,\n        cache_dir=cache_dir,\n    )\n\n    if is_frozen:\n        for param in model.parameters():\n            param.requires_grad = False\n\n    if not return_transform:\n        return model\n\n    image_mean = image_mean or getattr(model.visual, \"image_mean\", None)\n    image_std = image_std or getattr(model.visual, \"image_std\", None)\n    preprocess = image_transform(model.visual.image_size, is_train=False, mean=image_mean, std=image_std)\n\n    return model, preprocess\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/model.py", "content": "\"\"\" CLIP Model\n\nAdapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\nfrom functools import partial\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\ntry:\n    from .hf_model import HFTextEncoder\nexcept:\n    HFTextEncoder = None\nfrom .modified_resnet import ModifiedResNet\nfrom .timm_model import TimmModel\nfrom .eva_vit_model import EVAVisionTransformer\nfrom .transformer import LayerNorm, QuickGELU, Attention, VisionTransformer, TextTransformer\n\ntry:\n    from apex.normalization import FusedLayerNorm\nexcept:\n    FusedLayerNorm = LayerNorm\n    # print(\"Please 'pip install apex'\")\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass RMSnorm(nn.Module):\n    \"\"\"\n    adepted from transformers T5LayerNorm\n    \"\"\"\n\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Construct a layernorm module in the T5 style. No bias and no subtraction of mean.\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        # T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n        # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n        # half-precision inputs is done in fp32\n\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into half-precision if necessary\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return self.weight * hidden_states\n\n\n@dataclass\nclass CLIPVisionCfg:\n    layers: Union[Tuple[int, int, int, int], int] = 12\n    width: int = 768\n    head_width: int = 64\n    mlp_ratio: float = 4.0\n    patch_size: int = 16\n    image_size: Union[Tuple[int, int], int] = 224\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    patch_dropout: float = 0.0  # what fraction of patches to dropout during training (0 would mean disabled and no patches dropped) - 0.5 to 0.75 recommended in the paper for optimal results\n    global_average_pool: bool = False  # whether to global average pool the last embedding layer, instead of using CLS token (https://arxiv.org/abs/2205.01580)\n    drop_path_rate: Optional[float] = None  # drop path rate\n    timm_model_name: str = None  # a valid model name overrides layers, width, patch_size\n    timm_model_pretrained: bool = False  # use (imagenet) pretrained weights for named model\n    timm_pool: str = \"avg\"  # feature pooling for timm model ('abs_attn', 'rot_attn', 'avg', '')\n    timm_proj: str = \"linear\"  # linear projection for timm model output ('linear', 'mlp', '')\n    timm_proj_bias: bool = False  # enable bias final projection\n    eva_model_name: str = None  # a valid eva model name overrides layers, width, patch_size\n    qkv_bias: bool = True\n    fusedLN: bool = False\n    xattn: bool = False\n    postnorm: bool = False\n    rope: bool = False\n    pt_hw_seq_len: int = 16  # 224/14\n    intp_freq: bool = False\n    naiveswiglu: bool = False\n    subln: bool = False\n    use_rms_norm: bool = False\n\n\n@dataclass\nclass CLIPTextCfg:\n    context_length: int = 77\n    vocab_size: int = 49408\n    width: int = 512\n    heads: int = 8\n    layers: int = 12\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    hf_model_name: str = None\n    hf_tokenizer_name: str = None\n    hf_model_pretrained: bool = True\n    proj: str = \"mlp\"\n    pooler_type: str = \"mean_pooler\"\n    masked_language_modeling: bool = False\n    fusedLN: bool = False\n    xattn: bool = False\n    attn_mask: bool = True\n\n\ndef get_cast_dtype(precision: str):\n    cast_dtype = None\n    if precision == \"bf16\":\n        cast_dtype = torch.bfloat16\n    elif precision == \"fp16\":\n        cast_dtype = torch.float16\n    return cast_dtype\n\n\ndef _build_vision_tower(embed_dim: int, vision_cfg: CLIPVisionCfg, quick_gelu: bool = False, cast_dtype: Optional[torch.dtype] = None):\n    if isinstance(vision_cfg, dict):\n        vision_cfg = CLIPVisionCfg(**vision_cfg)\n\n    # OpenAI models are pretrained w/ QuickGELU but native nn.GELU is both faster and more\n    # memory efficient in recent PyTorch releases (>= 1.10).\n    # NOTE: timm models always use native GELU regardless of quick_gelu flag.\n    act_layer = QuickGELU if quick_gelu else nn.GELU\n\n    if vision_cfg.eva_model_name:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n\n        norm_layer = RMSnorm if vision_cfg.use_rms_norm else LayerNorm\n\n        visual = EVAVisionTransformer(\n            img_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            num_classes=embed_dim,\n            use_mean_pooling=vision_cfg.global_average_pool,  # False\n            init_values=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            embed_dim=vision_cfg.width,\n            depth=vision_cfg.layers,\n            num_heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            qkv_bias=vision_cfg.qkv_bias,\n            drop_path_rate=vision_cfg.drop_path_rate,\n            norm_layer=partial(norm_layer, eps=1e-6),\n            xattn=vision_cfg.xattn,\n            rope=vision_cfg.rope,\n            postnorm=vision_cfg.postnorm,\n            pt_hw_seq_len=vision_cfg.pt_hw_seq_len,  # 224/14\n            intp_freq=vision_cfg.intp_freq,\n            naiveswiglu=vision_cfg.naiveswiglu,\n            subln=vision_cfg.subln,\n        )\n    elif vision_cfg.timm_model_name:\n        visual = TimmModel(\n            vision_cfg.timm_model_name, pretrained=vision_cfg.timm_model_pretrained, pool=vision_cfg.timm_pool, proj=vision_cfg.timm_proj, proj_bias=vision_cfg.timm_proj_bias, embed_dim=embed_dim, image_size=vision_cfg.image_size\n        )\n        act_layer = nn.GELU  # so that text transformer doesn't use QuickGELU w/ timm models\n    elif isinstance(vision_cfg.layers, (tuple, list)):\n        vision_heads = vision_cfg.width * 32 // vision_cfg.head_width\n        visual = ModifiedResNet(layers=vision_cfg.layers, output_dim=embed_dim, heads=vision_heads, image_size=vision_cfg.image_size, width=vision_cfg.width)\n    else:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n        norm_layer = LayerNormFp32 if cast_dtype in (torch.float16, torch.bfloat16) else LayerNorm\n        visual = VisionTransformer(\n            image_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            width=vision_cfg.width,\n            layers=vision_cfg.layers,\n            heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            ls_init_value=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            global_average_pool=vision_cfg.global_average_pool,\n            output_dim=embed_dim,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n        )\n\n    return visual\n\n\ndef _build_text_tower(\n    embed_dim: int,\n    text_cfg: CLIPTextCfg,\n    quick_gelu: bool = False,\n    cast_dtype: Optional[torch.dtype] = None,\n):\n    if isinstance(text_cfg, dict):\n        text_cfg = CLIPTextCfg(**text_cfg)\n\n    if text_cfg.hf_model_name:\n        text = HFTextEncoder(text_cfg.hf_model_name, output_dim=embed_dim, tokenizer_name=text_cfg.hf_tokenizer_name, proj=text_cfg.proj, pooler_type=text_cfg.pooler_type, masked_language_modeling=text_cfg.masked_language_modeling)\n    else:\n        act_layer = QuickGELU if quick_gelu else nn.GELU\n        norm_layer = LayerNorm\n\n        text = TextTransformer(\n            context_length=text_cfg.context_length,\n            vocab_size=text_cfg.vocab_size,\n            width=text_cfg.width,\n            heads=text_cfg.heads,\n            layers=text_cfg.layers,\n            ls_init_value=text_cfg.ls_init_value,\n            output_dim=embed_dim,\n            act_layer=act_layer,\n            norm_layer=FusedLayerNorm if text_cfg.fusedLN else norm_layer,\n            xattn=text_cfg.xattn,\n            attn_mask=text_cfg.attn_mask,\n        )\n    return text\n\n\nclass CLIP(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        vision_cfg: CLIPVisionCfg,\n        text_cfg: CLIPTextCfg,\n        quick_gelu: bool = False,\n        cast_dtype: Optional[torch.dtype] = None,\n    ):\n        super().__init__()\n        self.visual = _build_vision_tower(embed_dim, vision_cfg, quick_gelu, cast_dtype)\n\n        text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n        self.transformer = text.transformer\n        self.vocab_size = text.vocab_size\n        self.token_embedding = text.token_embedding\n        self.positional_embedding = text.positional_embedding\n        self.ln_final = text.ln_final\n        self.text_projection = text.text_projection\n        self.register_buffer(\"attn_mask\", text.attn_mask, persistent=False)\n\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):\n        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n        self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.visual.set_grad_checkpointing(enable)\n        self.transformer.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"logit_scale\"}\n\n    def encode_image(self, image, normalize: bool = False):\n        features = self.visual(image)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def encode_text(self, text, normalize: bool = False):\n        cast_dtype = self.transformer.get_cast_dtype()\n\n        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.to(cast_dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x, attn_mask=self.attn_mask)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n        # take features from the eot embedding (eot_token is the highest number in each sequence)\n        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n        return F.normalize(x, dim=-1) if normalize else x\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image, normalize=True)\n        text_features = self.encode_text(text, normalize=True)\n        return image_features, text_features, self.logit_scale.exp()\n\n\nclass CustomCLIP(nn.Module):\n    def __init__(\n        self,\n        embed_dim: int,\n        vision_cfg: CLIPVisionCfg,\n        text_cfg: CLIPTextCfg,\n        quick_gelu: bool = False,\n        cast_dtype: Optional[torch.dtype] = None,\n        itm_task: bool = False,\n    ):\n        super().__init__()\n        self.visual = _build_vision_tower(embed_dim, vision_cfg, quick_gelu, cast_dtype)\n        self.text = _build_text_tower(embed_dim, text_cfg, quick_gelu, cast_dtype)\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n    def lock_image_tower(self, unlocked_groups=0, freeze_bn_stats=False):\n        # lock image tower as per LiT - https://arxiv.org/abs/2111.07991\n        self.visual.lock(unlocked_groups=unlocked_groups, freeze_bn_stats=freeze_bn_stats)\n\n    def lock_text_tower(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):\n        self.text.lock(unlocked_layers, freeze_layer_norm)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.visual.set_grad_checkpointing(enable)\n        self.text.set_grad_checkpointing(enable)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"logit_scale\"}\n\n    def encode_image(self, image, normalize: bool = False):\n        features = self.visual(image)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def encode_text(self, text, normalize: bool = False):\n        features = self.text(text)\n        return F.normalize(features, dim=-1) if normalize else features\n\n    def forward(self, image, text):\n        image_features = self.encode_image(image, normalize=True)\n        text_features = self.encode_text(text, normalize=True)\n        return image_features, text_features, self.logit_scale.exp()\n\n\ndef convert_weights_to_lp(model: nn.Module, dtype=torch.float16):\n    \"\"\"Convert applicable model parameters to low-precision (bf16 or fp16)\"\"\"\n\n    def _convert_weights(l):\n\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.to(dtype)\n            if l.bias is not None:\n                l.bias.data = l.bias.data.to(dtype)\n\n        if isinstance(l, (nn.MultiheadAttention, Attention)):\n            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n                tensor = getattr(l, attr, None)\n                if tensor is not None:\n                    tensor.data = tensor.data.to(dtype)\n\n        if isinstance(l, nn.Parameter):\n            l.data = l.data.to(dtype)\n\n        for name in [\"text_projection\", \"proj\"]:\n            if hasattr(l, name) and isinstance(l, nn.Parameter):\n                attr = getattr(l, name, None)\n                if attr is not None:\n                    attr.data = attr.data.to(dtype)\n\n    model.apply(_convert_weights)\n\n\nconvert_weights_to_fp16 = convert_weights_to_lp  # backwards compat\n\n\n# used to maintain checkpoint compatibility\ndef convert_to_custom_text_state_dict(state_dict: dict):\n    if \"text_projection\" in state_dict:\n        # old format state_dict, move text tower -> .text\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            if any(k.startswith(p) for p in (\"text_projection\", \"positional_embedding\", \"token_embedding\", \"transformer\", \"ln_final\", \"logit_scale\")):\n                k = \"text.\" + k\n            new_state_dict[k] = v\n        return new_state_dict\n    return state_dict\n\n\ndef build_model_from_openai_state_dict(\n    state_dict: dict,\n    quick_gelu=True,\n    cast_dtype=torch.float16,\n):\n    vit = \"visual.proj\" in state_dict\n\n    if vit:\n        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n        image_size = vision_patch_size * grid_size\n    else:\n        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n        vision_layers = tuple(counts)\n        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n        vision_patch_size = None\n        assert output_width**2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n        image_size = output_width * 32\n\n    embed_dim = state_dict[\"text_projection\"].shape[1]\n    context_length = state_dict[\"positional_embedding\"].shape[0]\n    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n    transformer_heads = transformer_width // 64\n    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"transformer.resblocks\")))\n\n    vision_cfg = CLIPVisionCfg(\n        layers=vision_layers,\n        width=vision_width,\n        patch_size=vision_patch_size,\n        image_size=image_size,\n    )\n    text_cfg = CLIPTextCfg(context_length=context_length, vocab_size=vocab_size, width=transformer_width, heads=transformer_heads, layers=transformer_layers)\n    model = CLIP(\n        embed_dim,\n        vision_cfg=vision_cfg,\n        text_cfg=text_cfg,\n        quick_gelu=quick_gelu,  # OpenAI models were trained with QuickGELU\n        cast_dtype=cast_dtype,\n    )\n\n    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n        state_dict.pop(key, None)\n\n    convert_weights_to_fp16(model)  # OpenAI state dicts are partially converted to float16\n    model.load_state_dict(state_dict)\n    return model.eval()\n\n\ndef trace_model(model, batch_size=256, device=torch.device(\"cpu\")):\n    model.eval()\n    image_size = model.visual.image_size\n    example_images = torch.ones((batch_size, 3, image_size, image_size), device=device)\n    example_text = torch.zeros((batch_size, model.context_length), dtype=torch.int, device=device)\n    model = torch.jit.trace_module(model, inputs=dict(forward=(example_images, example_text), encode_text=(example_text,), encode_image=(example_images,)))\n    model.visual.image_size = image_size\n    return model\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_gemma.py", "content": "#    Copyright 2024 Duc Q. Nguyen, Haotian Liu and Bo Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, GemmaConfig, GemmaModel, GemmaForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaGemmaConfig(GemmaConfig):\n    model_type = \"llava_gemma\"\n\n\nclass LlavaGemmaModel(LlavaMetaModel, GemmaModel):\n    config_class = LlavaGemmaConfig\n\n    def __init__(self, config: GemmaConfig):\n        super(LlavaGemmaModel, self).__init__(config)\n\n\nclass LlavaGemmaForCausalLM(GemmaForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaGemmaConfig\n\n    def __init__(self, config):\n        super(GemmaForCausalLM, self).__init__(config)\n        self.model = LlavaGemmaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes)\n\n        return super().forward(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n        )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_gemma\", LlavaGemmaConfig)\nAutoModelForCausalLM.register(LlavaGemmaConfig, LlavaGemmaForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/consolidate.py", "content": "\"\"\"\nUsage:\npython3 -m llava.model.consolidate --src ~/model_weights/llava-7b --dst ~/model_weights/llava-7b_consolidate\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom llava.model import *\nfrom llava.model.utils import auto_upgrade\n\n\ndef consolidate_ckpt(src_path, dst_path):\n    print(\"Loading model\")\n    auto_upgrade(src_path)\n    src_model = AutoModelForCausalLM.from_pretrained(src_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n    src_tokenizer = AutoTokenizer.from_pretrained(src_path, use_fast=False)\n    src_model.save_pretrained(dst_path)\n    src_tokenizer.save_pretrained(dst_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--src\", type=str, required=True)\n    parser.add_argument(\"--dst\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    consolidate_ckpt(args.src, args.dst)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/eva_vit_model.py", "content": "# --------------------------------------------------------\n# Adapted from  https://github.com/microsoft/unilm/tree/master/beit\n# --------------------------------------------------------\nimport math\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from timm.models.layers import drop_path, to_2tuple, trunc_normal_\nexcept:\n    from timm.layers import drop_path, to_2tuple, trunc_normal_\n\nfrom .transformer import PatchDropout\nfrom .rope import VisionRotaryEmbedding, VisionRotaryEmbeddingFast\n\nif os.getenv(\"ENV_TYPE\") == \"deepspeed\":\n    try:\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return \"p={}\".format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        drop=0.0,\n        subln=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.ffn_ln(x)\n\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass SwiGLU(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.SiLU, drop=0.0, norm_layer=nn.LayerNorm, subln=False):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        self.w1 = nn.Linear(in_features, hidden_features)\n        self.w2 = nn.Linear(in_features, hidden_features)\n\n        self.act = act_layer()\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n        self.w3 = nn.Linear(hidden_features, out_features)\n\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x1 = self.w1(x)\n        x2 = self.w2(x)\n        hidden = self.act(x1) * x2\n        x = self.ffn_ln(hidden)\n        x = self.w3(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0, window_size=None, attn_head_dim=None, xattn=False, rope=None, subln=False, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.subln = subln\n        if self.subln:\n            self.q_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.k_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.v_proj = nn.Linear(dim, all_head_dim, bias=False)\n        else:\n            self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.inner_attn_ln = norm_layer(all_head_dim) if subln else nn.Identity()\n        # self.proj = nn.Linear(all_head_dim, all_head_dim)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n\n        self.rope = rope\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        B, N, C = x.shape\n        if self.subln:\n            q = F.linear(input=x, weight=self.q_proj.weight, bias=self.q_bias)\n            k = F.linear(input=x, weight=self.k_proj.weight, bias=None)\n            v = F.linear(input=x, weight=self.v_proj.weight, bias=self.v_bias)\n\n            q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)  # B, num_heads, N, C\n            k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n            v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n        else:\n\n            qkv_bias = None\n            if self.q_bias is not None:\n                qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n\n            qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n            qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)  # 3, B, num_heads, N, C\n            q, k, v = qkv[0], qkv[1], qkv[2]\n\n        if self.rope:\n            # slightly fast impl\n            q_t = q[:, :, 1:, :]\n            ro_q_t = self.rope(q_t)\n            q = torch.cat((q[:, :, :1, :], ro_q_t), -2).type_as(v)\n\n            k_t = k[:, :, 1:, :]\n            ro_k_t = self.rope(k_t)\n            k = torch.cat((k[:, :, :1, :], ro_k_t), -2).type_as(v)\n\n        if self.xattn:\n            q = q.permute(0, 2, 1, 3)  # B, num_heads, N, C -> B, N, num_heads, C\n            k = k.permute(0, 2, 1, 3)\n            v = v.permute(0, 2, 1, 3)\n\n            x = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                p=self.xattn_drop,\n                scale=self.scale,\n            )\n            x = x.reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n\n            if self.relative_position_bias_table is not None:\n                relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0).type_as(attn)\n\n            if rel_pos_bias is not None:\n                attn = attn + rel_pos_bias.type_as(attn)\n\n            if attn_mask is not None:\n                attn_mask = attn_mask.bool()\n                attn = attn.masked_fill(~attn_mask[:, None, None, :], float(\"-inf\"))\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        init_values=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        window_size=None,\n        attn_head_dim=None,\n        xattn=False,\n        rope=None,\n        postnorm=False,\n        subln=False,\n        naiveswiglu=False,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim, xattn=xattn, rope=rope, subln=subln, norm_layer=norm_layer\n        )\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n\n        if naiveswiglu:\n            self.mlp = SwiGLU(\n                in_features=dim,\n                hidden_features=mlp_hidden_dim,\n                subln=subln,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, subln=subln, drop=drop)\n\n        if init_values is not None and init_values > 0:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n        self.postnorm = postnorm\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        if self.gamma_1 is None:\n            if self.postnorm:\n                x = x + self.drop_path(self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            if self.postnorm:\n                x = x + self.drop_path(self.gamma_1 * self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.gamma_2 * self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self):\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass EVAVisionTransformer(nn.Module):\n    \"\"\"Vision Transformer with support for patch or hybrid CNN input stage\"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=1000,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        init_values=None,\n        patch_dropout=0.0,\n        use_abs_pos_emb=True,\n        use_rel_pos_bias=False,\n        use_shared_rel_pos_bias=False,\n        rope=False,\n        use_mean_pooling=True,\n        init_scale=0.001,\n        grad_checkpointing=False,\n        xattn=False,\n        postnorm=False,\n        pt_hw_seq_len=16,\n        intp_freq=False,\n        naiveswiglu=False,\n        subln=False,\n    ):\n        super().__init__()\n        self.image_size = img_size\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        if rope:\n            half_head_dim = embed_dim // num_heads // 2\n            hw_seq_len = img_size // patch_size\n            self.rope = VisionRotaryEmbeddingFast(\n                dim=half_head_dim,\n                pt_seq_len=pt_hw_seq_len,\n                ft_seq_len=hw_seq_len if intp_freq else None,\n                # patch_dropout=patch_dropout\n            )\n        else:\n            self.rope = None\n\n        self.naiveswiglu = naiveswiglu\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    init_values=init_values,\n                    window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None,\n                    xattn=xattn,\n                    rope=self.rope,\n                    postnorm=postnorm,\n                    subln=subln,\n                    naiveswiglu=naiveswiglu,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n        self.head = nn.Linear(embed_dim, num_classes, bias=qkv_bias) if num_classes > 0 else nn.Identity()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=0.02)\n\n        trunc_normal_(self.cls_token, std=0.02)\n\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=0.02)\n            self.head.weight.data.mul_(init_scale)\n            if self.head.bias is not None:\n                self.head.bias.data.mul_(init_scale)\n\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0.0 else nn.Identity()\n\n        self.grad_checkpointing = grad_checkpointing\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            if self.naiveswiglu:\n                rescale(layer.mlp.w3.weight.data, layer_id + 1)\n            else:\n                rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.blocks[0].mlp.fc2.weight.dtype\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=\"\"):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x, return_all_features=False):\n\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        # if os.getenv(\"RoPE\") == \"1\":\n        #     if self.training and not isinstance(self.patch_dropout, nn.Identity):\n        #         x, patch_indices_keep = self.patch_dropout(x)\n        #         self.rope.forward = partial(self.rope.forward, patch_indices_keep=patch_indices_keep)\n        #     else:\n        #         self.rope.forward = partial(self.rope.forward, patch_indices_keep=None)\n        #         x = self.patch_dropout(x)\n        # else:\n        x = self.patch_dropout(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for blk in self.blocks:\n            if self.grad_checkpointing:\n                x = checkpoint(blk, x, (rel_pos_bias,))\n            else:\n                x = blk(x, rel_pos_bias=rel_pos_bias)\n\n        if not return_all_features:\n            x = self.norm(x)\n            if self.fc_norm is not None:\n                return self.fc_norm(x.mean(1))\n            else:\n                return x[:, 0]\n        return x\n\n    def forward(self, x, return_all_features=False):\n        if return_all_features:\n            return self.forward_features(x, return_all_features)\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_qwen.py", "content": "#    Copyright 2024 Hao Zhang\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union, Dict\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nimport transformers\nfrom transformers import AutoConfig, AutoModelForCausalLM, LlamaConfig, LlamaModel, LlamaForCausalLM\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\n# from ...constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\nfrom transformers import Qwen2Config, Qwen2Model, Qwen2ForCausalLM\n\n# from .qwen.modeling_qwen import QWenLMHeadModel, QWenModel\n# from .qwen.configuration_qwen import QWenConfig\n\n\nclass LlavaQwenConfig(Qwen2Config):\n    model_type = \"llava_qwen\"\n\n\nclass LlavaQwenModel(LlavaMetaModel, Qwen2Model):\n    config_class = LlavaQwenConfig\n\n    def __init__(self, config: Qwen2Config):\n        super(LlavaQwenModel, self).__init__(config)\n\n\nclass LlavaQwenForCausalLM(Qwen2ForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaQwenConfig\n\n    def __init__(self, config):\n        # super(Qwen2ForCausalLM, self).__init__(config)\n        Qwen2ForCausalLM.__init__(self, config)\n        config.model_type = \"llava_qwen\"\n        config.rope_scaling = None\n        self.rel_tokens = None\n        self.low_tokens = None\n        self.model = LlavaQwenModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n    \n    def _process_token(self, cur_new_input_embeds, cur_new_labels, IGNORE_INDEX):\n        self._process_token_mm(cur_new_input_embeds, IGNORE_INDEX)\n    \n    def _process_token_mm(self, cur_new_input_embeds, IGNORE_INDEX):\n        if not hasattr(self, 'vidkv_stp_radio'):\n            return None\n        assert len(cur_new_input_embeds) == 3\n        if isinstance(cur_new_input_embeds, (list, tuple)) and len(cur_new_input_embeds) == 3:\n            _, vision_labels, text_labels = cur_new_input_embeds\n            vision_labels_process = vision_labels\n        else:\n            raise ValueError(\"cur_new_input_embeds must be a list or tuple of length 3\")\n        N, _ = vision_labels_process.shape\n        text_labels = text_labels.to(vision_labels_process.device)\n        sim_matrix = vision_labels_process @ text_labels.T\n        avg_sim = sim_matrix.mean(dim=1)\n        k = max(32, int(N * self.vidkv_stp_radio // 32 * 32))  \n        topk_values, topk_indices = torch.topk(avg_sim, k=k, dim=0, largest=True)\n        topk_indices = torch.sort(topk_indices)[0][:-32]\n        self.rel_tokens = None\n        # 14 is the number of system tokens. Only for llava-onevision model.\n        self.low_tokens = topk_indices + 14\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = False,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        if \"vidkv_stp\" in kwargs[\"cache_config\"]:\n            self.vidkv_stp_radio = kwargs[\"cache_config\"][\"vidkv_stp\"]\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n        \n\n\nAutoConfig.register(\"llava_qwen\", LlavaQwenConfig)\nAutoModelForCausalLM.register(LlavaQwenConfig, LlavaQwenForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/builder.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nimport os\nimport warnings\nimport shutil\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\nimport torch\nfrom llava.model import *\nfrom llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom llava.utils import rank0_print\n\n\ndef load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", torch_dtype=\"float16\",attn_implementation=\"flash_attention_2\", customized_config=None, overwrite_config=None, **kwargs):\n    kwargs[\"device_map\"] = device_map\n\n    if load_8bit:\n        kwargs[\"load_in_8bit\"] = True\n    elif load_4bit:\n        kwargs[\"load_in_4bit\"] = True\n        kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n    elif torch_dtype == \"float16\":\n        kwargs[\"torch_dtype\"] = torch.float16\n    elif torch_dtype == \"bfloat16\":\n        kwargs[\"torch_dtype\"] = torch.bfloat16\n    else:\n        import pdb;pdb.set_trace()\n\n    if customized_config is not None:\n        kwargs[\"config\"] = customized_config\n\n    if \"multimodal\" in kwargs:\n        if kwargs[\"multimodal\"] is True:\n            is_multimodal = True\n            kwargs.pop(\"multimodal\")\n    else:\n        is_multimodal = False\n\n    if \"llava\" in model_name.lower() or is_multimodal:\n        # Load LLaVA model\n        if \"lora\" in model_name.lower() and model_base is None:\n            warnings.warn(\n                \"There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.\"\n            )\n        if \"lora\" in model_name.lower() and model_base is not None:\n            lora_cfg_pretrained = AutoConfig.from_pretrained(model_path)\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            rank0_print(\"Loading LLaVA from base model...\")\n            if \"mixtral\" in model_name.lower():\n                from llava.model.language_model.llava_mixtral import LlavaMixtralConfig\n\n                lora_cfg_pretrained = LlavaMixtralConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaMixtralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"mistral\" in model_name.lower():\n                from llava.model.language_model.llava_mistral import LlavaMistralConfig\n\n                lora_cfg_pretrained = LlavaMistralConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaMistralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"gemma\" in model_name.lower():\n                from llava.model.language_model.llava_gemma import LlavaGemmaConfig\n\n                lora_cfg_pretrained = LlavaGemmaConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaGemmaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            else:\n                from llava.model.language_model.llava_llama import LlavaConfig\n\n                lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n\n            token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\n            if model.lm_head.weight.shape[0] != token_num:\n                model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n                model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n\n            rank0_print(\"Loading additional LLaVA weights...\")\n            if os.path.exists(os.path.join(model_path, \"non_lora_trainables.bin\")):\n                non_lora_trainables = torch.load(os.path.join(model_path, \"non_lora_trainables.bin\"), map_location=\"cpu\")\n            else:\n                # this is probably from HF Hub\n                from huggingface_hub import hf_hub_download\n\n                def load_from_hf(repo_id, filename, subfolder=None):\n                    cache_file = hf_hub_download(repo_id=repo_id, filename=filename, subfolder=subfolder)\n                    return torch.load(cache_file, map_location=\"cpu\")\n\n                non_lora_trainables = load_from_hf(model_path, \"non_lora_trainables.bin\")\n            non_lora_trainables = {(k[11:] if k.startswith(\"base_model.\") else k): v for k, v in non_lora_trainables.items()}\n            if any(k.startswith(\"model.model.\") for k in non_lora_trainables):\n                non_lora_trainables = {(k[6:] if k.startswith(\"model.\") else k): v for k, v in non_lora_trainables.items()}\n            model.load_state_dict(non_lora_trainables, strict=False)\n\n            from peft import PeftModel\n\n            rank0_print(\"Loading LoRA weights...\")\n            model = PeftModel.from_pretrained(model, model_path)\n            rank0_print(\"Merging LoRA weights...\")\n            model = model.merge_and_unload()\n            rank0_print(\"Model is loaded...\")\n        elif model_base is not None:  # this may be mm projector only, loading projector with preset language mdoel\n            rank0_print(f\"Loading LLaVA from base model {model_base}...\")\n            if \"mixtral\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaMixtralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"mistral\" in model_name.lower() or \"zephyr\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaMistralForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif \"gemma\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaGemmaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            elif (\n                \"wizardlm-2\" in model_name.lower()\n                and \"vicuna\" in model_name.lower()\n                or \"llama\" in model_name.lower()\n                or \"yi\" in model_name.lower()\n                or \"nous-hermes\" in model_name.lower()\n                or \"llava-v1.6-34b\" in model_name.lower()\n                or \"llava-v1.5\" in model_name.lower()\n            ):\n                from llava.model.language_model.llava_llama import LlavaConfig\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                if customized_config is None:\n                    llava_cfg = LlavaConfig.from_pretrained(model_path)\n                    if \"v1.5\" in model_name.lower():\n                        llava_cfg.delay_load = True  # a workaround for correctly loading v1.5 models\n                else:\n                    llava_cfg = customized_config\n\n                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n                llava_cfg = LlavaConfig.from_pretrained(model_path)\n                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=llava_cfg, **kwargs)\n            else:\n                raise ValueError(f\"Model {model_name} not supported\")\n\n            mm_projector_weights = torch.load(os.path.join(model_path, \"mm_projector.bin\"), map_location=\"cpu\")\n            mm_projector_weights = {k: v.to(torch.float16) for k, v in mm_projector_weights.items()}\n            model.load_state_dict(mm_projector_weights, strict=False)\n        else:\n            rank0_print(f\"Loaded LLaVA model: {model_path}\")\n            if \"mixtral\" in model_name.lower():\n                from llava.model.language_model.llava_mixtral import LlavaMixtralConfig\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                if customized_config is None:\n                    llava_cfg = LlavaMixtralConfig.from_pretrained(model_path)\n                else:\n                    llava_cfg = customized_config\n\n                if overwrite_config is not None:\n                    rank0_print(f\"Overwriting config with {overwrite_config}\")\n                    for k, v in overwrite_config.items():\n                        setattr(llava_cfg, k, v)\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                model = LlavaMixtralForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n\n            elif \"mistral\" in model_name.lower() or \"zephyr\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                model = LlavaMistralForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, **kwargs)\n            elif (\n                \"wizardlm-2\" in model_name.lower()\n                and \"vicuna\" in model_name.lower()\n                or \"llama\" in model_name.lower()\n                or \"yi\" in model_name.lower()\n                or \"nous-hermes\" in model_name.lower()\n                or \"llava-v1.6-34b\" in model_name.lower()\n                or \"llava-v1.5\" in model_name.lower()\n            ):\n                from llava.model.language_model.llava_llama import LlavaConfig\n\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                if customized_config is None:\n                    llava_cfg = LlavaConfig.from_pretrained(model_path)\n                    if \"v1.5\" in model_name.lower():\n                        llava_cfg.delay_load = True  # a workaround for correctly loading v1.5 models\n                else:\n                    llava_cfg = customized_config\n\n                if overwrite_config is not None:\n                    rank0_print(f\"Overwriting config with {overwrite_config}\")\n                    for k, v in overwrite_config.items():\n                        setattr(llava_cfg, k, v)\n\n                model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n\n            elif \"qwen\" in model_name.lower() or \"quyen\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path)\n                if \"moe\" in model_name.lower() or \"A14B\" in model_name.lower():\n                    from llava.model.language_model.llava_qwen_moe import LlavaQwenMoeConfig\n                    if overwrite_config is not None:\n                        llava_cfg = LlavaQwenMoeConfig.from_pretrained(model_path)\n                        rank0_print(f\"Overwriting config with {overwrite_config}\")\n                        for k, v in overwrite_config.items():\n                            setattr(llava_cfg, k, v)\n                        model = LlavaQwenMoeForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n                    else:\n                        model = LlavaQwenMoeForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, **kwargs)\n\n                else:\n                    from llava.model.language_model.llava_qwen import LlavaQwenConfig\n                    if overwrite_config is not None:\n                        llava_cfg = LlavaQwenConfig.from_pretrained(model_path)\n                        rank0_print(f\"Overwriting config with {overwrite_config}\")\n                        for k, v in overwrite_config.items():\n                            setattr(llava_cfg, k, v)\n                        model = LlavaQwenForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n                    else:\n                        model = LlavaQwenForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, **kwargs)\n\n            elif \"gemma\" in model_name.lower():\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n                model = LlavaGemmaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, config=cfg_pretrained, attn_implementation=attn_implementation, **kwargs)\n            else:\n                try:\n                    from llava.model.language_model.llava_llama import LlavaConfig\n\n                    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                    if customized_config is None:\n                        llava_cfg = LlavaConfig.from_pretrained(model_path)\n                        if \"v1.5\" in model_path.lower():\n                            llava_cfg.delay_load = True  # a workaround for correctly loading v1.5 models\n                    else:\n                        llava_cfg = customized_config\n\n                    if overwrite_config is not None:\n                        rank0_print(f\"Overwriting config with {overwrite_config}\")\n                        for k, v in overwrite_config.items():\n                            setattr(llava_cfg, k, v)\n                    model = LlavaLlamaForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, attn_implementation=attn_implementation, config=llava_cfg, **kwargs)\n                except:\n                    raise ValueError(f\"Model {model_name} not supported\")\n\n    else:\n        # Load language model\n        if model_base is not None:\n            # PEFT model\n            from peft import PeftModel\n\n            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n            model = AutoModelForCausalLM.from_pretrained(model_base, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\n            print(f\"Loading LoRA weights from {model_path}\")\n            model = PeftModel.from_pretrained(model, model_path)\n            print(f\"Merging weights\")\n            model = model.merge_and_unload()\n            print(\"Convert to FP16...\")\n            model.to(torch.float16)\n        else:\n            use_fast = False\n            if \"mpt\" in model_name.lower().replace(\"prompt\", \"\"):\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\n            else:\n                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n\n    rank0_print(f\"Model Class: {model.__class__.__name__}\")\n    image_processor = None\n\n    if \"llava\" in model_name.lower() or is_multimodal:\n        mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n        mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\n        if mm_use_im_patch_token:\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n        if mm_use_im_start_end:\n            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n        model.resize_token_embeddings(len(tokenizer))\n\n        vision_tower = model.get_vision_tower()\n        if not vision_tower.is_loaded:\n            vision_tower.load_model(device_map=device_map)\n        if device_map != \"auto\":\n            vision_tower.to(device=\"cuda\", dtype=torch.float16)\n        image_processor = vision_tower.image_processor\n\n    if hasattr(model.config, \"max_sequence_length\"):\n        context_len = model.config.max_sequence_length\n    elif hasattr(model.config, \"max_position_embeddings\"):\n        context_len = model.config.max_position_embeddings\n    elif hasattr(model.config, \"tokenizer_model_max_length\"):\n        context_len = model.config.tokenizer_model_max_length\n    else:\n        context_len = 2048\n\n    return tokenizer, model, image_processor, context_len\n"}
{"type": "source_file", "path": "llava/model/make_delta.py", "content": "\"\"\"\nUsage:\npython3 -m llava.model.make_delta --base ~/model_weights/llama-7b --target ~/model_weights/llava-7b --delta ~/model_weights/llava-7b-delta --hub-repo-id liuhaotian/llava-7b-delta\n\"\"\"\n\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom llava.model.utils import auto_upgrade\n\n\ndef make_delta(base_model_path, target_model_path, delta_path, hub_repo_id):\n    print(\"Loading base model\")\n    base = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Loading target model\")\n    auto_upgrade(target_model_path)\n    target = AutoModelForCausalLM.from_pretrained(target_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Calculating delta\")\n    for name, param in tqdm(target.state_dict().items(), desc=\"Calculating delta\"):\n        if name not in base.state_dict():\n            assert name in [\"model.mm_projector.weight\", \"model.mm_projector.bias\"], f\"{name} not in base model\"\n            continue\n        if param.data.shape == base.state_dict()[name].shape:\n            param.data -= base.state_dict()[name]\n        else:\n            assert name in [\"model.embed_tokens.weight\", \"lm_head.weight\"], f\"{name} dimension mismatch: {param.data.shape} vs {base.state_dict()[name].shape}\"\n            bparam = base.state_dict()[name]\n            param.data[: bparam.shape[0], : bparam.shape[1]] -= bparam\n\n    print(\"Saving delta\")\n    if hub_repo_id:\n        kwargs = {\"push_to_hub\": True, \"repo_id\": hub_repo_id}\n    else:\n        kwargs = {}\n    target.save_pretrained(delta_path, **kwargs)\n    target_tokenizer = AutoTokenizer.from_pretrained(target_model_path)\n    target_tokenizer.save_pretrained(delta_path, **kwargs)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n    parser.add_argument(\"--hub-repo-id\", type=str, default=None)\n    args = parser.parse_args()\n\n    make_delta(args.base_model_path, args.target_model_path, args.delta_path, args.hub_repo_id)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/openai.py", "content": "\"\"\" OpenAI pretrained model functions\n\nAdapted from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport os\nimport warnings\nfrom typing import List, Optional, Union\n\nimport torch\n\nfrom .model import build_model_from_openai_state_dict, convert_weights_to_lp, get_cast_dtype\nfrom .pretrained import get_pretrained_url, list_pretrained_models_by_tag, download_pretrained_from_url\n\n__all__ = [\"list_openai_models\", \"load_openai_model\"]\n\n\ndef list_openai_models() -> List[str]:\n    \"\"\"Returns the names of available CLIP models\"\"\"\n    return list_pretrained_models_by_tag(\"openai\")\n\n\ndef load_openai_model(\n    name: str,\n    precision: Optional[str] = None,\n    device: Optional[Union[str, torch.device]] = None,\n    jit: bool = True,\n    cache_dir: Optional[str] = None,\n):\n    \"\"\"Load a CLIP model\n\n    Parameters\n    ----------\n    name : str\n        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict\n    precision: str\n        Model precision, if None defaults to 'fp32' if device == 'cpu' else 'fp16'.\n    device : Union[str, torch.device]\n        The device to put the loaded model\n    jit : bool\n        Whether to load the optimized JIT model (default) or more hackable non-JIT model.\n    cache_dir : Optional[str]\n        The directory to cache the downloaded model weights\n\n    Returns\n    -------\n    model : torch.nn.Module\n        The CLIP model\n    preprocess : Callable[[PIL.Image], torch.Tensor]\n        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\n    \"\"\"\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if precision is None:\n        precision = \"fp32\" if device == \"cpu\" else \"fp16\"\n\n    if get_pretrained_url(name, \"openai\"):\n        model_path = download_pretrained_from_url(get_pretrained_url(name, \"openai\"), cache_dir=cache_dir)\n    elif os.path.isfile(name):\n        model_path = name\n    else:\n        raise RuntimeError(f\"Model {name} not found; available models = {list_openai_models()}\")\n\n    try:\n        # loading JIT archive\n        model = torch.jit.load(model_path, map_location=device if jit else \"cpu\").eval()\n        state_dict = None\n    except RuntimeError:\n        # loading saved state dict\n        if jit:\n            warnings.warn(f\"File {model_path} is not a JIT archive. Loading as a state dict instead\")\n            jit = False\n        state_dict = torch.load(model_path, map_location=\"cpu\")\n\n    if not jit:\n        # Build a non-jit model from the OpenAI jitted model state dict\n        cast_dtype = get_cast_dtype(precision)\n        try:\n            model = build_model_from_openai_state_dict(state_dict or model.state_dict(), cast_dtype=cast_dtype)\n        except KeyError:\n            sd = {k[7:]: v for k, v in state_dict[\"state_dict\"].items()}\n            model = build_model_from_openai_state_dict(sd, cast_dtype=cast_dtype)\n\n        # model from OpenAI state dict is in manually cast fp16 mode, must be converted for AMP/fp32/bf16 use\n        model = model.to(device)\n        if precision.startswith(\"amp\") or precision == \"fp32\":\n            model.float()\n        elif precision == \"bf16\":\n            convert_weights_to_lp(model, dtype=torch.bfloat16)\n\n        return model\n\n    # patch the device names\n    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])\n    device_node = [n for n in device_holder.graph.findAllNodes(\"prim::Constant\") if \"Device\" in repr(n)][-1]\n\n    def patch_device(module):\n        try:\n            graphs = [module.graph] if hasattr(module, \"graph\") else []\n        except RuntimeError:\n            graphs = []\n\n        if hasattr(module, \"forward1\"):\n            graphs.append(module.forward1.graph)\n\n        for graph in graphs:\n            for node in graph.findAllNodes(\"prim::Constant\"):\n                if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\"cuda\"):\n                    node.copyAttributes(device_node)\n\n    model.apply(patch_device)\n    patch_device(model.encode_image)\n    patch_device(model.encode_text)\n\n    # patch dtype to float32 (typically for CPU)\n    if precision == \"fp32\":\n        float_holder = torch.jit.trace(lambda: torch.ones([]).float(), example_inputs=[])\n        float_input = list(float_holder.graph.findNode(\"aten::to\").inputs())[1]\n        float_node = float_input.node()\n\n        def patch_float(module):\n            try:\n                graphs = [module.graph] if hasattr(module, \"graph\") else []\n            except RuntimeError:\n                graphs = []\n\n            if hasattr(module, \"forward1\"):\n                graphs.append(module.forward1.graph)\n\n            for graph in graphs:\n                for node in graph.findAllNodes(\"aten::to\"):\n                    inputs = list(node.inputs())\n                    for i in [1, 2]:  # dtype can be the second or third argument to aten::to()\n                        if inputs[i].node()[\"value\"] == 5:\n                            inputs[i].node().copyAttributes(float_node)\n\n        model.apply(patch_float)\n        patch_float(model.encode_image)\n        patch_float(model.encode_text)\n        model.float()\n\n    # ensure image_size attr available at consistent location for both jit and non-jit\n    model.visual.image_size = model.input_resolution.item()\n    return model\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/hf_model.py", "content": "\"\"\" huggingface model adapter\n\nWraps HuggingFace transformers (https://github.com/huggingface/transformers) models for use as a text tower in CLIP model.\n\"\"\"\n\nimport re\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch import TensorType\n\ntry:\n    import transformers\n    from transformers import AutoModel, AutoModelForMaskedLM, AutoTokenizer, AutoConfig, PretrainedConfig\n    from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, BaseModelOutputWithPoolingAndCrossAttentions\nexcept ImportError as e:\n    transformers = None\n\n    class BaseModelOutput:\n        pass\n\n    class PretrainedConfig:\n        pass\n\n\nfrom .hf_configs import arch_dict\n\n\n# utils\ndef _camel2snake(s):\n    return re.sub(r\"(?<!^)(?=[A-Z])\", \"_\", s).lower()\n\n\n# TODO: ?last - for gpt-like models\n_POOLERS = {}\n\n\ndef register_pooler(cls):\n    \"\"\"Decorator registering pooler class\"\"\"\n    _POOLERS[_camel2snake(cls.__name__)] = cls\n    return cls\n\n\n@register_pooler\nclass MeanPooler(nn.Module):\n    \"\"\"Mean pooling\"\"\"\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n        masked_output = x.last_hidden_state * attention_mask.unsqueeze(-1)\n        return masked_output.sum(dim=1) / attention_mask.sum(-1, keepdim=True)\n\n\n@register_pooler\nclass MaxPooler(nn.Module):\n    \"\"\"Max pooling\"\"\"\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n        masked_output = x.last_hidden_state.masked_fill(attention_mask.unsqueeze(-1), -torch.inf)\n        return masked_output.max(1).values\n\n\n@register_pooler\nclass ClsPooler(nn.Module):\n    \"\"\"CLS token pooling\"\"\"\n\n    def __init__(self, use_pooler_output=True):\n        super().__init__()\n        self.cls_token_position = 0\n        self.use_pooler_output = use_pooler_output\n\n    def forward(self, x: BaseModelOutput, attention_mask: TensorType):\n\n        if self.use_pooler_output and isinstance(x, (BaseModelOutputWithPooling, BaseModelOutputWithPoolingAndCrossAttentions)) and (x.pooler_output is not None):\n            return x.pooler_output\n\n        return x.last_hidden_state[:, self.cls_token_position, :]\n\n\nclass HFTextEncoder(nn.Module):\n    \"\"\"HuggingFace model adapter\"\"\"\n\n    def __init__(self, model_name_or_path: str, output_dim: int, tokenizer_name: str = None, config: PretrainedConfig = None, pooler_type: str = None, proj: str = None, pretrained: bool = True, masked_language_modeling: bool = False):\n        super().__init__()\n\n        self.output_dim = output_dim\n\n        # TODO: find better way to get this information\n        uses_transformer_pooler = pooler_type == \"cls_pooler\"\n\n        if transformers is None:\n            raise RuntimeError(\"Please `pip install transformers` to use pre-trained HuggingFace models\")\n        if config is None:\n            self.config = AutoConfig.from_pretrained(model_name_or_path)\n            if masked_language_modeling:\n                create_func, model_args = (AutoModelForMaskedLM.from_pretrained, model_name_or_path) if pretrained else (AutoModelForMaskedLM.from_config, self.config)\n            else:\n                create_func, model_args = (AutoModel.from_pretrained, model_name_or_path) if pretrained else (AutoModel.from_config, self.config)\n            # TODO: do all model configs have this attribute? PretrainedConfig does so yes??\n            if hasattr(self.config, \"is_encoder_decoder\") and self.config.is_encoder_decoder:\n                self.transformer = create_func(model_args)\n                self.transformer = self.transformer.encoder\n            else:\n                self.transformer = create_func(model_args, add_pooling_layer=uses_transformer_pooler)\n        else:\n            self.config = config\n            if masked_language_modeling:\n                self.transformer = AutoModelForMaskedLM.from_config(config)\n            else:\n                self.transformer = AutoModel.from_config(config)\n\n        if pooler_type is None:  # get default arch pooler\n            self.pooler = _POOLERS[(arch_dict[self.config.model_type][\"pooler\"])]()\n        else:\n            self.pooler = _POOLERS[pooler_type]()\n\n        d_model = getattr(self.config, arch_dict[self.config.model_type][\"config_names\"][\"width\"])\n        if (d_model == output_dim) and (proj is None):  # do we always need a proj?\n            self.proj = nn.Identity()\n        elif proj == \"linear\":\n            self.proj = nn.Linear(d_model, output_dim, bias=False)\n        elif proj == \"mlp\":\n            hidden_size = (d_model + output_dim) // 2\n            self.proj = nn.Sequential(\n                nn.Linear(d_model, hidden_size, bias=False),\n                nn.GELU(),\n                nn.Linear(hidden_size, output_dim, bias=False),\n            )\n\n        # self.itm_proj = nn.Linear(d_model, 2, bias=False)\n        # self.mlm_proj = nn.Linear(d_model, self.config.vocab_size), bias=False)\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\n    # def forward_itm(self, x:TensorType, image_embeds:TensorType) -> TensorType:\n    #     image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(x.device)\n    #     attn_mask = (x != self.config.pad_token_id).long()\n    #     out = self.transformer(\n    #         input_ids=x,\n    #         attention_mask=attn_mask,\n    #         encoder_hidden_states = image_embeds,\n    #         encoder_attention_mask = image_atts,\n    #         )\n    #     pooled_out = self.pooler(out, attn_mask)\n\n    #     return self.itm_proj(pooled_out)\n\n    def mask(self, input_ids, vocab_size, device, targets=None, masked_indices=None, probability_matrix=None):\n        if masked_indices is None:\n            masked_indices = torch.bernoulli(probability_matrix).bool()\n\n        masked_indices[input_ids == self.tokenizer.pad_token_id] = False\n        masked_indices[input_ids == self.tokenizer.cls_token_id] = False\n\n        if targets is not None:\n            targets[~masked_indices] = -100  # We only compute loss on masked tokens\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool() & masked_indices\n        input_ids[indices_replaced] = self.tokenizer.mask_token_id\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(input_ids.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(vocab_size, input_ids.shape, dtype=torch.long).to(device)\n        input_ids[indices_random] = random_words[indices_random]\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n\n        if targets is not None:\n            return input_ids, targets\n        else:\n            return input_ids\n\n    def forward_mlm(self, input_ids, image_embeds, mlm_probability=0.25):\n        labels = input_ids.clone()\n        attn_mask = (input_ids != self.config.pad_token_id).long()\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(input_ids.device)\n        vocab_size = getattr(self.config, arch_dict[self.config.model_type][\"config_names\"][\"vocab_size\"])\n        probability_matrix = torch.full(labels.shape, mlm_probability)\n        input_ids, labels = self.mask(input_ids, vocab_size, input_ids.device, targets=labels, probability_matrix=probability_matrix)\n        mlm_output = self.transformer(\n            input_ids,\n            attention_mask=attn_mask,\n            encoder_hidden_states=image_embeds,\n            encoder_attention_mask=image_atts,\n            return_dict=True,\n            labels=labels,\n        )\n        return mlm_output.loss\n        # mlm_output = self.transformer(input_ids,\n        #                 attention_mask = attn_mask,\n        #                 encoder_hidden_states = image_embeds,\n        #                 encoder_attention_mask = image_atts,\n        #                 return_dict = True,\n        #             ).last_hidden_state\n        # logits = self.mlm_proj(mlm_output)\n\n        # # logits = logits[:, :-1, :].contiguous().view(-1, vocab_size)\n        # logits = logits[:, 1:, :].contiguous().view(-1, vocab_size)\n        # labels = labels[:, 1:].contiguous().view(-1)\n\n        # mlm_loss = F.cross_entropy(\n        #     logits,\n        #     labels,\n        #     # label_smoothing=0.1,\n        # )\n        # return mlm_loss\n\n    def forward(self, x: TensorType) -> TensorType:\n        attn_mask = (x != self.config.pad_token_id).long()\n        out = self.transformer(input_ids=x, attention_mask=attn_mask)\n        pooled_out = self.pooler(out, attn_mask)\n\n        return self.proj(pooled_out)\n\n    def lock(self, unlocked_layers: int = 0, freeze_layer_norm: bool = True):\n        if not unlocked_layers:  # full freezing\n            for n, p in self.transformer.named_parameters():\n                p.requires_grad = (not freeze_layer_norm) if \"LayerNorm\" in n.split(\".\") else False\n            return\n\n        encoder = self.transformer.encoder if hasattr(self.transformer, \"encoder\") else self.transformer\n        layer_list = getattr(encoder, arch_dict[self.config.model_type][\"config_names\"][\"layer_attr\"])\n        print(f\"Unlocking {unlocked_layers}/{len(layer_list) + 1} layers of hf model\")\n        embeddings = getattr(self.transformer, arch_dict[self.config.model_type][\"config_names\"][\"token_embeddings_attr\"])\n        modules = [embeddings, *layer_list][:-unlocked_layers]\n        # freeze layers\n        for module in modules:\n            for n, p in module.named_parameters():\n                p.requires_grad = (not freeze_layer_norm) if \"LayerNorm\" in n.split(\".\") else False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.gradient_checkpointing_enable()\n\n    def get_num_layers(self):\n        encoder = self.transformer.encoder if hasattr(self.transformer, \"encoder\") else self.transformer\n        layer_list = getattr(encoder, arch_dict[self.config.model_type][\"config_names\"][\"layer_attr\"])\n        return len(layer_list)\n\n    def init_parameters(self):\n        pass\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/loss.py", "content": "import math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ntry:\n    import torch.distributed.nn\n    from torch import distributed as dist\n\n    has_distributed = True\nexcept ImportError:\n    has_distributed = False\n\ntry:\n    import horovod.torch as hvd\nexcept ImportError:\n    hvd = None\n\nfrom timm.loss import LabelSmoothingCrossEntropy\n\n\ndef gather_features(image_features, text_features, local_loss=False, gather_with_grad=False, rank=0, world_size=1, use_horovod=False):\n    assert has_distributed, \"torch.distributed did not import correctly, please use a PyTorch version with support.\"\n    if use_horovod:\n        assert hvd is not None, \"Please install horovod\"\n        if gather_with_grad:\n            all_image_features = hvd.allgather(image_features)\n            all_text_features = hvd.allgather(text_features)\n        else:\n            with torch.no_grad():\n                all_image_features = hvd.allgather(image_features)\n                all_text_features = hvd.allgather(text_features)\n            if not local_loss:\n                # ensure grads for local rank when all_* features don't have a gradient\n                gathered_image_features = list(all_image_features.chunk(world_size, dim=0))\n                gathered_text_features = list(all_text_features.chunk(world_size, dim=0))\n                gathered_image_features[rank] = image_features\n                gathered_text_features[rank] = text_features\n                all_image_features = torch.cat(gathered_image_features, dim=0)\n                all_text_features = torch.cat(gathered_text_features, dim=0)\n    else:\n        # We gather tensors from all gpus\n        if gather_with_grad:\n            all_image_features = torch.cat(torch.distributed.nn.all_gather(image_features), dim=0)\n            all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features), dim=0)\n            # all_image_features = torch.cat(torch.distributed.nn.all_gather(image_features, async_op=True), dim=0)\n            # all_text_features = torch.cat(torch.distributed.nn.all_gather(text_features, async_op=True), dim=0)\n        else:\n            gathered_image_features = [torch.zeros_like(image_features) for _ in range(world_size)]\n            gathered_text_features = [torch.zeros_like(text_features) for _ in range(world_size)]\n            dist.all_gather(gathered_image_features, image_features)\n            dist.all_gather(gathered_text_features, text_features)\n            if not local_loss:\n                # ensure grads for local rank when all_* features don't have a gradient\n                gathered_image_features[rank] = image_features\n                gathered_text_features[rank] = text_features\n            all_image_features = torch.cat(gathered_image_features, dim=0)\n            all_text_features = torch.cat(gathered_text_features, dim=0)\n\n    return all_image_features, all_text_features\n\n\nclass ClipLoss(nn.Module):\n\n    def __init__(\n        self,\n        local_loss=False,\n        gather_with_grad=False,\n        cache_labels=False,\n        rank=0,\n        world_size=1,\n        use_horovod=False,\n        smoothing=0.0,\n    ):\n        super().__init__()\n        self.local_loss = local_loss\n        self.gather_with_grad = gather_with_grad\n        self.cache_labels = cache_labels\n        self.rank = rank\n        self.world_size = world_size\n        self.use_horovod = use_horovod\n        self.label_smoothing_cross_entropy = LabelSmoothingCrossEntropy(smoothing=smoothing) if smoothing > 0 else None\n\n        # cache state\n        self.prev_num_logits = 0\n        self.labels = {}\n\n    def forward(self, image_features, text_features, logit_scale=1.0):\n        device = image_features.device\n        if self.world_size > 1:\n            all_image_features, all_text_features = gather_features(image_features, text_features, self.local_loss, self.gather_with_grad, self.rank, self.world_size, self.use_horovod)\n\n            if self.local_loss:\n                logits_per_image = logit_scale * image_features @ all_text_features.T\n                logits_per_text = logit_scale * text_features @ all_image_features.T\n            else:\n                logits_per_image = logit_scale * all_image_features @ all_text_features.T\n                logits_per_text = logits_per_image.T\n        else:\n            logits_per_image = logit_scale * image_features @ text_features.T\n            logits_per_text = logit_scale * text_features @ image_features.T\n        # calculated ground-truth and cache if enabled\n        num_logits = logits_per_image.shape[0]\n        if self.prev_num_logits != num_logits or device not in self.labels:\n            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n            if self.world_size > 1 and self.local_loss:\n                labels = labels + num_logits * self.rank\n            if self.cache_labels:\n                self.labels[device] = labels\n                self.prev_num_logits = num_logits\n        else:\n            labels = self.labels[device]\n\n        if self.label_smoothing_cross_entropy:\n            total_loss = (self.label_smoothing_cross_entropy(logits_per_image, labels) + self.label_smoothing_cross_entropy(logits_per_text, labels)) / 2\n        else:\n            total_loss = (F.cross_entropy(logits_per_image, labels) + F.cross_entropy(logits_per_text, labels)) / 2\n\n        acc = None\n        i2t_acc = (logits_per_image.argmax(-1) == labels).sum() / len(logits_per_image)\n        t2i_acc = (logits_per_text.argmax(-1) == labels).sum() / len(logits_per_text)\n        acc = {\"i2t\": i2t_acc, \"t2i\": t2i_acc}\n        return total_loss, acc\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_mixtral.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, MixtralConfig, MixtralModel, MixtralForCausalLM, GenerationConfig\n\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nfrom transformers.generation.utils import GenerateOutput\n\nfrom ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaMixtralConfig(MixtralConfig):\n    model_type = \"llava_mixtral\"\n\n\nclass LlavaMixtralModel(LlavaMetaModel, MixtralModel):\n    config_class = LlavaMixtralConfig\n\n    def __init__(self, config: MixtralConfig):\n        super(LlavaMixtralModel, self).__init__(config)\n\n\nclass LlavaMixtralForCausalLM(MixtralForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaMixtralConfig\n\n    def __init__(self, config):\n        super(MixtralForCausalLM, self).__init__(config)\n\n        config.model_type = \"llava_mixtral\"\n        config.rope_scaling = None\n        self.model = LlavaMixtralModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        images: Optional[torch.FloatTensor] = None,\n        image_sizes: Optional[List[List[int]]] = None,\n        return_dict: Optional[bool] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        dpo_forward: Optional[bool] = None,\n        cache_position=None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        if inputs_embeds is None:\n            (input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels) = self.prepare_inputs_labels_for_multimodal(input_ids, position_ids, attention_mask, past_key_values, labels, images, modalities, image_sizes)\n\n        if dpo_forward:\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            logits = self.lm_head(hidden_states)\n            return logits, labels\n\n        else:\n            return super().forward(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        inputs: Optional[torch.Tensor] = None,\n        images: Optional[torch.Tensor] = None,\n        image_sizes: Optional[torch.Tensor] = None,\n        modalities: Optional[List[str]] = [\"image\"],\n        **kwargs,\n    ) -> Union[GenerateOutput, torch.LongTensor]:\n        position_ids = kwargs.pop(\"position_ids\", None)\n        attention_mask = kwargs.pop(\"attention_mask\", None)\n        if \"inputs_embeds\" in kwargs:\n            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n\n        if images is not None:\n            (inputs, position_ids, attention_mask, _, inputs_embeds, _) = self.prepare_inputs_labels_for_multimodal(inputs, position_ids, attention_mask, None, None, images, modalities, image_sizes=image_sizes)\n        else:\n            inputs_embeds = self.get_model().embed_tokens(inputs)\n\n        return super().generate(position_ids=position_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, **kwargs)\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        image_sizes = kwargs.pop(\"image_sizes\", None)\n        inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        if images is not None:\n            inputs[\"images\"] = images\n        if image_sizes is not None:\n            inputs[\"image_sizes\"] = image_sizes\n        return inputs\n\n\nAutoConfig.register(\"llava_mixtral\", LlavaMixtralConfig)\nAutoModelForCausalLM.register(LlavaMixtralConfig, LlavaMixtralForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/rope.py", "content": "from math import pi\nimport torch\nfrom torch import nn\nfrom einops import rearrange, repeat\nimport logging\n\n\ndef broadcat(tensors, dim=-1):\n    num_tensors = len(tensors)\n    shape_lens = set(list(map(lambda t: len(t.shape), tensors)))\n    assert len(shape_lens) == 1, \"tensors must all have the same number of dimensions\"\n    shape_len = list(shape_lens)[0]\n    dim = (dim + shape_len) if dim < 0 else dim\n    dims = list(zip(*map(lambda t: list(t.shape), tensors)))\n    expandable_dims = [(i, val) for i, val in enumerate(dims) if i != dim]\n    assert all([*map(lambda t: len(set(t[1])) <= 2, expandable_dims)]), \"invalid dimensions for broadcastable concatentation\"\n    max_dims = list(map(lambda t: (t[0], max(t[1])), expandable_dims))\n    expanded_dims = list(map(lambda t: (t[0], (t[1],) * num_tensors), max_dims))\n    expanded_dims.insert(dim, (dim, dims[dim]))\n    expandable_shapes = list(zip(*map(lambda t: t[1], expanded_dims)))\n    tensors = list(map(lambda t: t[0].expand(*t[1]), zip(tensors, expandable_shapes)))\n    return torch.cat(tensors, dim=dim)\n\n\ndef rotate_half(x):\n    x = rearrange(x, \"... (d r) -> ... d r\", r=2)\n    x1, x2 = x.unbind(dim=-1)\n    x = torch.stack((-x2, x1), dim=-1)\n    return rearrange(x, \"... d r -> ... (d r)\")\n\n\nclass VisionRotaryEmbedding(nn.Module):\n    def __init__(\n        self,\n        dim,\n        pt_seq_len,\n        ft_seq_len=None,\n        custom_freqs=None,\n        freqs_for=\"lang\",\n        theta=10000,\n        max_freq=10,\n        num_freqs=1,\n    ):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs_h = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs_h = repeat(freqs_h, \"... n -> ... (n r)\", r=2)\n\n        freqs_w = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs_w = repeat(freqs_w, \"... n -> ... (n r)\", r=2)\n\n        freqs = broadcat((freqs_h[:, None, :], freqs_w[None, :, :]), dim=-1)\n\n        self.register_buffer(\"freqs_cos\", freqs.cos())\n        self.register_buffer(\"freqs_sin\", freqs.sin())\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, start_index=0):\n        rot_dim = self.freqs_cos.shape[-1]\n        end_index = start_index + rot_dim\n        assert rot_dim <= t.shape[-1], f\"feature dimension {t.shape[-1]} is not of sufficient size to rotate in all the positions {rot_dim}\"\n        t_left, t, t_right = t[..., :start_index], t[..., start_index:end_index], t[..., end_index:]\n        t = (t * self.freqs_cos) + (rotate_half(t) * self.freqs_sin)\n\n        return torch.cat((t_left, t, t_right), dim=-1)\n\n\nclass VisionRotaryEmbeddingFast(nn.Module):\n    def __init__(self, dim, pt_seq_len, ft_seq_len=None, custom_freqs=None, freqs_for=\"lang\", theta=10000, max_freq=10, num_freqs=1, patch_dropout=0.0):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs = repeat(freqs, \"... n -> ... (n r)\", r=2)\n        freqs = broadcat((freqs[:, None, :], freqs[None, :, :]), dim=-1)\n\n        freqs_cos = freqs.cos().view(-1, freqs.shape[-1])\n        freqs_sin = freqs.sin().view(-1, freqs.shape[-1])\n\n        self.patch_dropout = patch_dropout\n\n        self.register_buffer(\"freqs_cos\", freqs_cos)\n        self.register_buffer(\"freqs_sin\", freqs_sin)\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, patch_indices_keep=None):\n        if patch_indices_keep is not None:\n            batch = t.size()[0]\n            batch_indices = torch.arange(batch)\n            batch_indices = batch_indices[..., None]\n\n            freqs_cos = repeat(self.freqs_cos, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n            freqs_sin = repeat(self.freqs_sin, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n\n            freqs_cos = freqs_cos[batch_indices, patch_indices_keep]\n            freqs_cos = rearrange(freqs_cos, \"n i m j -> n m i j\")\n            freqs_sin = freqs_sin[batch_indices, patch_indices_keep]\n            freqs_sin = rearrange(freqs_sin, \"n i m j -> n m i j\")\n\n            return t * freqs_cos + rotate_half(t) * freqs_sin\n\n        return t * self.freqs_cos + rotate_half(t) * self.freqs_sin\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/pretrained.py", "content": "import hashlib\nimport os\nimport urllib\nimport warnings\nfrom typing import Dict, Union\n\nfrom tqdm import tqdm\n\ntry:\n    from huggingface_hub import hf_hub_download\n\n    _has_hf_hub = True\nexcept ImportError:\n    hf_hub_download = None\n    _has_hf_hub = False\n\n\ndef _pcfg(url=\"\", hf_hub=\"\", filename=\"\", mean=None, std=None):\n    return dict(\n        url=url,\n        hf_hub=hf_hub,\n        mean=mean,\n        std=std,\n    )\n\n\n_VITB32 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e32-46683a32.pt\"),\n    laion2b_e16=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-laion2b_e16-af8dbd0c.pth\"),\n    laion2b_s34b_b79k=_pcfg(hf_hub=\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K/\"),\n)\n\n_VITB32_quickgelu = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e31-d867053b.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_32-quickgelu-laion400m_e32-46683a32.pt\"),\n)\n\n_VITB16 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16-laion400m_e31-00efa78f.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16-laion400m_e32-55e67d44.pt\"),\n    laion2b_s34b_b88k=_pcfg(hf_hub=\"laion/CLIP-ViT-B-16-laion2B-s34B-b88K/\"),\n)\n\n_EVAB16 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_B_psz14to16.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_B_psz14to16.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_B_psz16_s8B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_B_psz16_s8B.pt\"),\n)\n\n_VITB16_PLUS_240 = dict(\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16_plus_240-laion400m_e31-8fb26589.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_b_16_plus_240-laion400m_e32-699c4b84.pt\"),\n)\n\n_VITL14 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\"),\n    laion400m_e31=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_l_14-laion400m_e31-69988bb6.pt\"),\n    laion400m_e32=_pcfg(\"https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/vit_l_14-laion400m_e32-3d133497.pt\"),\n    laion2b_s32b_b82k=_pcfg(hf_hub=\"laion/CLIP-ViT-L-14-laion2B-s32B-b82K/\", mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n)\n\n_EVAL14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_L_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_L_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_s4B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_s4B.pt\"),\n)\n\n_VITL14_336 = dict(\n    openai=_pcfg(\"https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt\"),\n)\n\n_EVAL14_336 = dict(\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_336_psz14_s6B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_336_psz14_s6B.pt\"),\n    eva_clip_224to336=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_224to336.pt\"),\n    eva02_clip_224to336=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_L_psz14_224to336.pt\"),\n)\n\n_VITH14 = dict(\n    laion2b_s32b_b79k=_pcfg(hf_hub=\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K/\"),\n)\n\n_VITg14 = dict(\n    laion2b_s12b_b42k=_pcfg(hf_hub=\"laion/CLIP-ViT-g-14-laion2B-s12B-b42K/\"),\n    laion2b_s34b_b88k=_pcfg(hf_hub=\"laion/CLIP-ViT-g-14-laion2B-s34B-b88K/\"),\n)\n\n_EVAg14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/\"),\n    eva01=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_g_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_psz14_s11B.pt\"),\n    eva01_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_psz14_s11B.pt\"),\n)\n\n_EVAg14_PLUS = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/\"),\n    eva01=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_g_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_plus_psz14_s11B.pt\"),\n    eva01_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA01_CLIP_g_14_plus_psz14_s11B.pt\"),\n)\n\n_VITbigG14 = dict(\n    laion2b_s39b_b160k=_pcfg(hf_hub=\"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k/\"),\n)\n\n_EVAbigE14 = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_s4B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_s4B.pt\"),\n)\n\n_EVAbigE14_PLUS = dict(\n    eva=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva02=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_E_psz14.pt\"),\n    eva_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_plus_s9B.pt\"),\n    eva02_clip=_pcfg(hf_hub=\"QuanSun/EVA-CLIP/EVA02_CLIP_E_psz14_plus_s9B.pt\"),\n)\n\n_EVA_8B = dict(\n    eva=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B/EVA_8B_psz14.bin\"),\n    eva_clip=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B/EVA_CLIP_8B_psz14_s9B.pt\"),\n)\n\n_EVA_8B_PLUS = dict(\n    eva_clip=_pcfg(hf_hub=\"BAAI/EVA-CLIP-8B-448/EVA_CLIP_8B_psz14_plus_s0.6B.pt\"),\n)\n\n\n_PRETRAINED = {\n    # \"ViT-B-32\": _VITB32,\n    \"OpenaiCLIP-B-32\": _VITB32,\n    \"OpenCLIP-B-32\": _VITB32,\n    # \"ViT-B-32-quickgelu\": _VITB32_quickgelu,\n    \"OpenaiCLIP-B-32-quickgelu\": _VITB32_quickgelu,\n    \"OpenCLIP-B-32-quickgelu\": _VITB32_quickgelu,\n    # \"ViT-B-16\": _VITB16,\n    \"OpenaiCLIP-B-16\": _VITB16,\n    \"OpenCLIP-B-16\": _VITB16,\n    \"EVA02-B-16\": _EVAB16,\n    \"EVA02-CLIP-B-16\": _EVAB16,\n    # \"ViT-B-16-plus-240\": _VITB16_PLUS_240,\n    \"OpenCLIP-B-16-plus-240\": _VITB16_PLUS_240,\n    # \"ViT-L-14\": _VITL14,\n    \"OpenaiCLIP-L-14\": _VITL14,\n    \"OpenCLIP-L-14\": _VITL14,\n    \"EVA02-L-14\": _EVAL14,\n    \"EVA02-CLIP-L-14\": _EVAL14,\n    # \"ViT-L-14-336\": _VITL14_336,\n    \"OpenaiCLIP-L-14-336\": _VITL14_336,\n    \"EVA02-CLIP-L-14-336\": _EVAL14_336,\n    # \"ViT-H-14\": _VITH14,\n    # \"ViT-g-14\": _VITg14,\n    \"OpenCLIP-H-14\": _VITH14,\n    \"OpenCLIP-g-14\": _VITg14,\n    \"EVA01-CLIP-g-14\": _EVAg14,\n    \"EVA01-CLIP-g-14-plus\": _EVAg14_PLUS,\n    # \"ViT-bigG-14\": _VITbigG14,\n    \"OpenCLIP-bigG-14\": _VITbigG14,\n    \"EVA02-CLIP-bigE-14\": _EVAbigE14,\n    \"EVA02-CLIP-bigE-14-plus\": _EVAbigE14_PLUS,\n    \"EVA-CLIP-8B\": _EVA_8B,\n    \"EVA-CLIP-8B-448\": _EVA_8B_PLUS,\n    \"EVA-CLIP-8B-plus\": _EVA_8B_PLUS,\n}\n\n\ndef _clean_tag(tag: str):\n    # normalize pretrained tags\n    return tag.lower().replace(\"-\", \"_\")\n\n\ndef list_pretrained(as_str: bool = False):\n    \"\"\"returns list of pretrained models\n    Returns a tuple (model_name, pretrain_tag) by default or 'name:tag' if as_str == True\n    \"\"\"\n    return [\":\".join([k, t]) if as_str else (k, t) for k in _PRETRAINED.keys() for t in _PRETRAINED[k].keys()]\n\n\ndef list_pretrained_models_by_tag(tag: str):\n    \"\"\"return all models having the specified pretrain tag\"\"\"\n    models = []\n    tag = _clean_tag(tag)\n    for k in _PRETRAINED.keys():\n        if tag in _PRETRAINED[k]:\n            models.append(k)\n    return models\n\n\ndef list_pretrained_tags_by_model(model: str):\n    \"\"\"return all pretrain tags for the specified model architecture\"\"\"\n    tags = []\n    if model in _PRETRAINED:\n        tags.extend(_PRETRAINED[model].keys())\n    return tags\n\n\ndef is_pretrained_cfg(model: str, tag: str):\n    if model not in _PRETRAINED:\n        return False\n    return _clean_tag(tag) in _PRETRAINED[model]\n\n\ndef get_pretrained_cfg(model: str, tag: str):\n    if model not in _PRETRAINED:\n        return {}\n    model_pretrained = _PRETRAINED[model]\n    return model_pretrained.get(_clean_tag(tag), {})\n\n\ndef get_pretrained_url(model: str, tag: str):\n    cfg = get_pretrained_cfg(model, _clean_tag(tag))\n    return cfg.get(\"url\", \"\")\n\n\ndef download_pretrained_from_url(\n    url: str,\n    cache_dir: Union[str, None] = None,\n):\n    if not cache_dir:\n        cache_dir = os.path.expanduser(\"~/.cache/clip\")\n    os.makedirs(cache_dir, exist_ok=True)\n    filename = os.path.basename(url)\n\n    if \"openaipublic\" in url:\n        expected_sha256 = url.split(\"/\")[-2]\n    elif \"mlfoundations\" in url:\n        expected_sha256 = os.path.splitext(filename)[0].split(\"-\")[-1]\n    else:\n        expected_sha256 = \"\"\n\n    download_target = os.path.join(cache_dir, filename)\n\n    if os.path.exists(download_target) and not os.path.isfile(download_target):\n        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n    if os.path.isfile(download_target):\n        if expected_sha256:\n            if hashlib.sha256(open(download_target, \"rb\").read()).hexdigest().startswith(expected_sha256):\n                return download_target\n            else:\n                warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n        else:\n            return download_target\n\n    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n        with tqdm(total=int(source.headers.get(\"Content-Length\")), ncols=80, unit=\"iB\", unit_scale=True) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n\n                output.write(buffer)\n                loop.update(len(buffer))\n\n    if expected_sha256 and not hashlib.sha256(open(download_target, \"rb\").read()).hexdigest().startswith(expected_sha256):\n        raise RuntimeError(f\"Model has been downloaded but the SHA256 checksum does not not match\")\n\n    return download_target\n\n\ndef has_hf_hub(necessary=False):\n    if not _has_hf_hub and necessary:\n        # if no HF Hub module installed, and it is necessary to continue, raise error\n        raise RuntimeError(\"Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.\")\n    return _has_hf_hub\n\n\ndef download_pretrained_from_hf(\n    model_id: str,\n    filename: str = \"open_clip_pytorch_model.bin\",\n    revision=None,\n    cache_dir: Union[str, None] = None,\n):\n    has_hf_hub(True)\n    cached_file = hf_hub_download(model_id, filename, revision=revision, cache_dir=cache_dir)\n    return cached_file\n\n\ndef download_pretrained(\n    cfg: Dict,\n    force_hf_hub: bool = False,\n    cache_dir: Union[str, None] = None,\n):\n    target = \"\"\n    if not cfg:\n        return target\n\n    download_url = cfg.get(\"url\", \"\")\n    download_hf_hub = cfg.get(\"hf_hub\", \"\")\n    if download_hf_hub and force_hf_hub:\n        # use HF hub even if url exists\n        download_url = \"\"\n\n    if download_url:\n        target = download_pretrained_from_url(download_url, cache_dir=cache_dir)\n    elif download_hf_hub:\n        has_hf_hub(True)\n        # we assume the hf_hub entries in pretrained config combine model_id + filename in\n        # 'org/model_name/filename.pt' form. To specify just the model id w/o filename and\n        # use 'open_clip_pytorch_model.bin' default, there must be a trailing slash 'org/model_name/'.\n        model_id, filename = os.path.split(download_hf_hub)\n        if filename:\n            target = download_pretrained_from_hf(model_id, filename=filename, cache_dir=cache_dir)\n        else:\n            target = download_pretrained_from_hf(model_id, cache_dir=cache_dir)\n\n    return target\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/modified_resnet.py", "content": "from collections import OrderedDict\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .utils import freeze_batch_norm_2d\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.act1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.act2 = nn.ReLU(inplace=True)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.act3 = nn.ReLU(inplace=True)\n\n        self.downsample = None\n        self.stride = stride\n\n        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([(\"-1\", nn.AvgPool2d(stride)), (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)), (\"1\", nn.BatchNorm2d(planes * self.expansion))]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n\n        out = self.act1(self.bn1(self.conv1(x)))\n        out = self.act2(self.bn2(self.conv2(out)))\n        out = self.avgpool(out)\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.act3(out)\n        return out\n\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim**2 + 1, embed_dim) / embed_dim**0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x,\n            key=x,\n            value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0.0,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False,\n        )\n\n        return x[0]\n\n\nclass ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n\n    def __init__(self, layers, output_dim, heads, image_size=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim\n        self.image_size = image_size\n\n        # the 3-layer stem\n        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width // 2)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(width // 2)\n        self.act2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(width)\n        self.act3 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(2)\n\n        # residual layers\n        self._inplanes = width  # this is a *mutable* variable used during construction\n        self.layer1 = self._make_layer(width, layers[0])\n        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n\n        embed_dim = width * 32  # the ResNet feature dimension\n        self.attnpool = AttentionPool2d(image_size // 32, embed_dim, heads, output_dim)\n\n        self.init_parameters()\n\n    def _make_layer(self, planes, blocks, stride=1):\n        layers = [Bottleneck(self._inplanes, planes, stride)]\n\n        self._inplanes = planes * Bottleneck.expansion\n        for _ in range(1, blocks):\n            layers.append(Bottleneck(self._inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def init_parameters(self):\n        if self.attnpool is not None:\n            std = self.attnpool.c_proj.in_features**-0.5\n            nn.init.normal_(self.attnpool.q_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.k_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.v_proj.weight, std=std)\n            nn.init.normal_(self.attnpool.c_proj.weight, std=std)\n\n        for resnet_block in [self.layer1, self.layer2, self.layer3, self.layer4]:\n            for name, param in resnet_block.named_parameters():\n                if name.endswith(\"bn3.weight\"):\n                    nn.init.zeros_(param)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n        if freeze_bn_stats:\n            freeze_batch_norm_2d(self)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        # FIXME support for non-transformer\n        pass\n\n    def stem(self, x):\n        x = self.act1(self.bn1(self.conv1(x)))\n        x = self.act2(self.bn2(self.conv2(x)))\n        x = self.act3(self.bn3(self.conv3(x)))\n        x = self.avgpool(x)\n        return x\n\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.attnpool(x)\n\n        return x\n"}
{"type": "source_file", "path": "llava/model/language_model/llava_mpt.py", "content": "#    Copyright 2023 Haotian Liu\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n\nfrom typing import Optional, Tuple\n\nimport torch\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, MptConfig, MptForCausalLM, MptModel, GenerationConfig\nfrom llava.model.llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n\n\nclass LlavaMptConfig(MptConfig):\n    model_type = \"llava_mpt\"\n\n\nclass LlavaMptModel(LlavaMetaModel, MptModel):\n    config_class = LlavaMptConfig\n\n    def __init__(self, config: MptConfig):\n        config.hidden_size = config.d_model\n        super(LlavaMptModel, self).__init__(config)\n\n    def embed_tokens(self, x):\n        return self.wte(x)\n\n\nclass LlavaMptForCausalLM(MptForCausalLM, LlavaMetaForCausalLM):\n    config_class = LlavaMptConfig\n    supports_gradient_checkpointing = True\n\n    def __init__(self, config):\n        super(MptForCausalLM, self).__init__(config)\n\n        config.model_type = \"llava_mpt\"\n        config.rope_scaling = None\n        self.generation_config = GenerationConfig(\n            temperature=0.0,\n            max_new_tokens=1024,\n            do_sample=False,\n            top_p=None,\n        )\n\n        self.transformer = LlavaMptModel(config)\n        self.lm_head = torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_model(self):\n        return self.transformer\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, LlavaMptModel):\n            module.gradient_checkpointing = value\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position=None,\n        images=None,\n    ):\n\n        input_ids, attention_mask, past_key_values, inputs_embeds, labels = self.prepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\n\n        return super().forward(\n            input_ids,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            labels=labels,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n        images = kwargs.pop(\"images\", None)\n        _inputs = super().prepare_inputs_for_generation(input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs)\n        _inputs[\"images\"] = images\n        return _inputs\n\n\nAutoConfig.register(\"llava_mpt\", LlavaMptConfig)\nAutoModelForCausalLM.register(LlavaMptConfig, LlavaMptForCausalLM)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/eva_clip/eva_clip_processors.py", "content": "\"\"\"\n# Adapted from https://github.com/baaivision/EVA/tree/master/EVA-CLIP\n\"\"\"\n\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\nfrom transformers.image_processing_utils import BatchFeature\nfrom PIL import Image\nfrom transformers.image_transforms import convert_to_rgb\n\n\nclass BaseProcessor:\n    def __init__(self):\n        self.transform = lambda x: x\n        return\n\n    def __call__(self, item):\n        return self.transform(item)\n\n\nclass EvaClipImageBaseProcessor(BaseProcessor):\n    def __init__(self, mean=None, std=None):\n        self.mean = (0.48145466, 0.4578275, 0.40821073) if mean is None else mean\n        self.std = (0.26862954, 0.26130258, 0.27577711) if std is None else std\n\n        self.normalize = transforms.Normalize(self.mean, self.std)\n\n    @property\n    def image_mean(self):\n        return self.mean\n\n\nclass EvaClipImageTrainProcessor(EvaClipImageBaseProcessor):\n    def __init__(self, image_size=224, mean=None, std=None, min_scale=0.5, max_scale=1.0):\n        super().__init__(mean=mean, std=std)\n\n        self.transform = transforms.Compose(\n            [\n                convert_to_rgb,\n                transforms.Resize(\n                    image_size,\n                    interpolation=InterpolationMode.BICUBIC,\n                ),\n                transforms.CenterCrop(image_size),\n                transforms.ToTensor(),\n                self.normalize,\n            ]\n        )\n\n        self.image_size = image_size\n\n    def preprocess(self, images, return_tensors):\n        if isinstance(images, Image.Image):\n            images = [images]\n        else:\n            assert isinstance(images, list)\n\n        transformed_images = [self.transform(image).numpy() for image in images]\n        data = {\"pixel_values\": transformed_images}\n\n        return BatchFeature(data=data, tensor_type=return_tensors)\n\n    def __call__(self, item):\n        return self.transform(item)\n\n    @property\n    def crop_size(self):\n        return {\"height\": self.image_size, \"width\": self.image_size}\n\n    @property\n    def size(self):\n        return {\"shortest_edge\": self.image_size}\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/mlcd_encoder.py", "content": "import torch\nimport torch.nn as nn\nfrom llava.utils import rank0_print\n# from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\nfrom transformers import CLIPImageProcessor\nfrom .mlcd.vit_rope2d_hf import MLCDVisionModel, MLCDVisionConfig\n\ntry:\n    from s2wrapper import forward as multiscale_forward\nexcept:\n    pass\n\n\n# class CLIPVisionTower(nn.Module):\nclass MLCDVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.vision_tower_name = vision_tower\n        self.select_layer = args.mm_vision_select_layer\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n        else:\n            # self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\n            self.cfg_only = MLCDVisionConfig.from_pretrained(self.vision_tower_name)\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n        # self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n        self.vision_tower = MLCDVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n        self.vision_tower.requires_grad_(False)\n\n        self.is_loaded = True\n\n    def feature_select(self, image_forward_outs):\n        select_feature_type = self.select_feature\n\n        if self.select_feature in [\"slicefour_patch\", \"slicefour_cls_patch\"]:\n            select_every_k_layer = len(image_forward_outs.hidden_states) // 4\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in range(select_every_k_layer + self.select_layer, len(image_forward_outs.hidden_states), select_every_k_layer)], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slicefour_\", \"\")\n        elif self.select_feature in [\"slice_m25811_f6_patch\", \"slice_m25811_f6_cls_patch\"]:\n            select_layers = [-2, -5, -8, -11, 6]\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in select_layers], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slice_m25811_f6_\", \"\")\n        else:\n            image_features = image_forward_outs.hidden_states[self.select_layer]\n\n        if select_feature_type == \"patch\":\n            image_features = image_features[:, 1:]\n        elif select_feature_type == \"cls_patch\":\n            image_features = image_features\n        else:\n            raise ValueError(f\"Unexpected select feature: {select_feature_type}\")\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        return self.vision_tower.dtype\n\n    @property\n    def device(self):\n        return self.vision_tower.device\n\n    @property\n    def config(self):\n        if self.is_loaded:\n            return self.vision_tower.config\n        else:\n            return self.cfg_only\n\n    @property\n    def hidden_size(self):\n        _hidden_size = self.config.hidden_size\n        if \"slicefour\" in self.select_feature:\n            _hidden_size *= 4\n        if \"slice_m25811_f6\" in self.select_feature:\n            _hidden_size *= 5\n        return _hidden_size\n\n    @property\n    def num_patches_per_side(self):\n        return self.config.image_size // self.config.patch_size\n\n    @property\n    def num_patches(self):\n        _num_patches = (self.config.image_size // self.config.patch_size) ** 2\n        if \"cls_patch\" in self.select_feature:\n            _num_patches += 1\n        return _num_patches\n\n    @property\n    def image_size(self):\n        return self.config.image_size\n\n\n# class CLIPVisionTowerS2(CLIPVisionTower):\nclass MLCDVisionTowerS2(MLCDVisionTower):\n    def __init__(self, vision_tower, args, delay_load=False):\n\n        self.s2_scales = getattr(args, \"s2_scales\", \"336,672,1008\")\n        self.s2_scales = list(map(int, self.s2_scales.split(\",\")))\n        self.s2_scales.sort()\n        self.s2_split_size = self.s2_scales[0]\n        self.s2_image_size = self.s2_scales[-1]\n\n        super().__init__(vision_tower, args, delay_load)\n\n        # change resize/crop size in preprocessing to the largest image size in s2_scale\n        if not delay_load or getattr(args, \"unfreeze_mm_vision_tower\", False):\n            self.image_processor.size[\"shortest_edge\"] = self.s2_image_size\n            self.image_processor.crop_size[\"height\"] = self.image_processor.crop_size[\"width\"] = self.s2_image_size\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n        self.vision_tower = MLCDVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n        self.vision_tower.requires_grad_(False)\n\n        self.image_processor.size[\"shortest_edge\"] = self.s2_image_size\n        self.image_processor.crop_size[\"height\"] = self.image_processor.crop_size[\"width\"] = self.s2_image_size\n\n        self.is_loaded = True\n\n    def forward_feature(self, images):\n        image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n        image_features = self.feature_select(image_forward_outs).to(images.dtype)\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_feature = multiscale_forward(self.forward_feature, image.unsqueeze(0), img_sizes=self.s2_scales, max_split_size=self.s2_split_size, split_forward=True)\n                image_features.append(image_feature)\n        else:\n            image_features = multiscale_forward(self.forward_feature, images, img_sizes=self.s2_scales, max_split_size=self.s2_split_size, split_forward=True)\n\n        return image_features\n\n    @property\n    def hidden_size(self):\n        return self.config.hidden_size * len(self.s2_scales)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/clip_encoder.py", "content": "import torch\nimport torch.nn as nn\nfrom llava.utils import rank0_print\nfrom transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n\ntry:\n    from s2wrapper import forward as multiscale_forward\nexcept:\n    pass\n\n\nclass CLIPVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.vision_tower_name = vision_tower\n        self.select_layer = args.mm_vision_select_layer\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n        else:\n            self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n        self.vision_tower.requires_grad_(False)\n\n        self.is_loaded = True\n\n    def feature_select(self, image_forward_outs):\n        select_feature_type = self.select_feature\n\n        if self.select_feature in [\"slicefour_patch\", \"slicefour_cls_patch\"]:\n            select_every_k_layer = len(image_forward_outs.hidden_states) // 4\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in range(select_every_k_layer + self.select_layer, len(image_forward_outs.hidden_states), select_every_k_layer)], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slicefour_\", \"\")\n        elif self.select_feature in [\"slice_m25811_f6_patch\", \"slice_m25811_f6_cls_patch\"]:\n            select_layers = [-2, -5, -8, -11, 6]\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in select_layers], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slice_m25811_f6_\", \"\")\n        else:\n            image_features = image_forward_outs.hidden_states[self.select_layer]\n\n        if select_feature_type == \"patch\":\n            image_features = image_features[:, 1:]\n        elif select_feature_type == \"cls_patch\":\n            image_features = image_features\n        else:\n            raise ValueError(f\"Unexpected select feature: {select_feature_type}\")\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        return self.vision_tower.dtype\n\n    @property\n    def device(self):\n        return self.vision_tower.device\n\n    @property\n    def config(self):\n        if self.is_loaded:\n            return self.vision_tower.config\n        else:\n            return self.cfg_only\n\n    @property\n    def hidden_size(self):\n        _hidden_size = self.config.hidden_size\n        if \"slicefour\" in self.select_feature:\n            _hidden_size *= 4\n        if \"slice_m25811_f6\" in self.select_feature:\n            _hidden_size *= 5\n        return _hidden_size\n\n    @property\n    def num_patches_per_side(self):\n        return self.config.image_size // self.config.patch_size\n\n    @property\n    def num_patches(self):\n        _num_patches = (self.config.image_size // self.config.patch_size) ** 2\n        if \"cls_patch\" in self.select_feature:\n            _num_patches += 1\n        return _num_patches\n\n    @property\n    def image_size(self):\n        return self.config.image_size\n\n\nclass CLIPVisionTowerS2(CLIPVisionTower):\n    def __init__(self, vision_tower, args, delay_load=False):\n\n        self.s2_scales = getattr(args, \"s2_scales\", \"336,672,1008\")\n        self.s2_scales = list(map(int, self.s2_scales.split(\",\")))\n        self.s2_scales.sort()\n        self.s2_split_size = self.s2_scales[0]\n        self.s2_image_size = self.s2_scales[-1]\n\n        super().__init__(vision_tower, args, delay_load)\n\n        # change resize/crop size in preprocessing to the largest image size in s2_scale\n        if not delay_load or getattr(args, \"unfreeze_mm_vision_tower\", False):\n            self.image_processor.size[\"shortest_edge\"] = self.s2_image_size\n            self.image_processor.crop_size[\"height\"] = self.image_processor.crop_size[\"width\"] = self.s2_image_size\n\n    def load_model(self, device_map=None):\n        if self.is_loaded:\n            rank0_print(\"{} is already loaded, `load_model` called again, skipping.\".format(self.vision_tower_name))\n            return\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n        self.vision_tower.requires_grad_(False)\n\n        self.image_processor.size[\"shortest_edge\"] = self.s2_image_size\n        self.image_processor.crop_size[\"height\"] = self.image_processor.crop_size[\"width\"] = self.s2_image_size\n\n        self.is_loaded = True\n\n    def forward_feature(self, images):\n        image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n        image_features = self.feature_select(image_forward_outs).to(images.dtype)\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_feature = multiscale_forward(self.forward_feature, image.unsqueeze(0), img_sizes=self.s2_scales, max_split_size=self.s2_split_size, split_forward=True)\n                image_features.append(image_feature)\n        else:\n            image_features = multiscale_forward(self.forward_feature, images, img_sizes=self.s2_scales, max_split_size=self.s2_split_size, split_forward=True)\n\n        return image_features\n\n    @property\n    def hidden_size(self):\n        return self.config.hidden_size * len(self.s2_scales)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/eva_clip/factory.py", "content": "import json\nimport logging\nimport os\nimport pathlib\nimport re\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union, Dict, Any\nimport torch\n\n_MODEL_CONFIG_PATHS = [Path(__file__).parent / f\"model_configs/\"]\n_MODEL_CONFIGS = {}  # directory (model_name: config) of model architecture configs\n\n\ndef _natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\n\n\ndef _rescan_model_configs():\n    global _MODEL_CONFIGS\n\n    config_ext = (\".json\",)\n    config_files = []\n    for config_path in _MODEL_CONFIG_PATHS:\n        if config_path.is_file() and config_path.suffix in config_ext:\n            config_files.append(config_path)\n        elif config_path.is_dir():\n            for ext in config_ext:\n                config_files.extend(config_path.glob(f\"*{ext}\"))\n\n    for cf in config_files:\n        with open(cf, \"r\", encoding=\"utf8\") as f:\n            model_cfg = json.load(f)\n            if all(a in model_cfg for a in (\"embed_dim\", \"vision_cfg\", \"text_cfg\")):\n                _MODEL_CONFIGS[cf.stem] = model_cfg\n\n    _MODEL_CONFIGS = dict(sorted(_MODEL_CONFIGS.items(), key=lambda x: _natural_key(x[0])))\n\n\n_rescan_model_configs()  # initial populate of model config registry\n\n\ndef list_models():\n    \"\"\"enumerate available model architectures based on config files\"\"\"\n    return list(_MODEL_CONFIGS.keys())\n\n\ndef add_model_config(path):\n    \"\"\"add model config path or file and update registry\"\"\"\n    if not isinstance(path, Path):\n        path = Path(path)\n    _MODEL_CONFIG_PATHS.append(path)\n    _rescan_model_configs()\n\n\ndef get_model_config(model_name):\n    if model_name in _MODEL_CONFIGS:\n        return deepcopy(_MODEL_CONFIGS[model_name])\n    else:\n        return None\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/__init__.py", "content": "from .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\nfrom .factory import create_model, create_model_and_transforms, create_model_from_pretrained, get_tokenizer\nfrom .factory import list_models, add_model_config, get_model_config, load_checkpoint\nfrom .loss import ClipLoss\nfrom .model import CLIP, CustomCLIP, CLIPTextCfg, CLIPVisionCfg, convert_weights_to_lp, convert_weights_to_fp16, trace_model, get_cast_dtype\nfrom .openai import load_openai_model, list_openai_models\nfrom .pretrained import list_pretrained, list_pretrained_models_by_tag, list_pretrained_tags_by_model, get_pretrained_url, download_pretrained_from_url, is_pretrained_cfg, get_pretrained_cfg, download_pretrained\nfrom .tokenizer import SimpleTokenizer, tokenize\nfrom .transform import image_transform\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/open_clip_encoder.py", "content": "import torch\nimport torch.nn as nn\nfrom transformers import CLIPImageProcessor\nfrom llava.utils import rank0_print\n\ntry:\n    import open_clip\n    import torchvision\n    from open_clip.transformer import _expand_token\nexcept ImportError:\n    print(\"OpenCLIP not installed\")\n    open_clip = None\n\nHIDDEN_SIZE_DICT = {\n    \"ViT-H-14-378-quickgelu\": 1280,\n}\n\n\nclass OpenCLIPVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n        self.model_name = vision_tower.replace(\"open_clip_hub:\", \"\")\n        self.pretrained = args.vision_tower_pretrained\n        self.select_layer = args.mm_vision_select_layer\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n\n    def load_model(self, device_map=\"auto\"):\n        rank0_print(f\"Loading OpenCLIP model: {self.model_name}\")\n        rank0_print(f\"Pretrained: {self.pretrained}\")\n        vision_tower, _, image_processor = open_clip.create_model_and_transforms(model_name=self.model_name, pretrained=self.pretrained, precision=\"fp32\", device=\"cuda\")\n\n        resize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Resize)][0]\n        normalize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Normalize)][0]\n        self.resize_transform_size = resize_transform.size  # 224 or 384\n        self.patch_size = vision_tower.visual.conv1.kernel_size[0]  # 14 or 16\n\n        self.image_processor = CLIPImageProcessor.from_pretrained(\n            \"openai/clip-vit-large-patch14\",\n            crop_size=resize_transform.size,\n            size={\"shortest_edge\": resize_transform.size},\n            image_mean=list(normalize_transform.mean),\n            image_std=list(normalize_transform.std),\n        )\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower = vision_tower.visual\n        self.vision_tower.requires_grad_(False)\n\n        self.is_loaded = True\n\n    def feature_select(self, image_forward_outs):\n        image_features = image_forward_outs[self.select_layer]\n        if self.select_feature == \"patch\":\n            image_features = image_features[:, 1:]\n        elif self.select_feature == \"cls_patch\":\n            image_features = image_features\n        elif self.select_feature == \"conv_flatten\":\n            image_features = image_features.flatten(2).transpose(1, 2)\n        else:\n            raise ValueError(f\"Unexpected select feature: {self.select_feature}\")\n        return image_features\n\n    def forward_visual(self, x, output_hidden_states=False):\n        if hasattr(self.vision_tower, \"trunk\") and hasattr(self.vision_tower.trunk, \"_intermediate_layers\"):\n            return self.vision_tower.trunk._intermediate_layers(x, abs(self.select_layer))\n        else:\n\n            def forward_openclip(self, x: torch.Tensor):\n                features = []\n                x = self.conv1(x)  # shape = [*, width, grid, grid]\n                x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n                x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n\n                # class embeddings and positional embeddings\n                x = torch.cat(\n                    [_expand_token(self.class_embedding, x.shape[0]).to(x.dtype), x],\n                    dim=1,\n                )\n                # shape = [*, grid ** 2 + 1, width]\n                x = x + self.positional_embedding.to(x.dtype)\n\n                x = self.patch_dropout(x)\n                x = self.ln_pre(x)\n\n                x = x.permute(1, 0, 2)  # NLD -> LND\n                for r in self.transformer.resblocks:\n                    x = r(x, attn_mask=None)\n                    features.append(x)\n                return features\n\n            return forward_openclip(self.vision_tower, x)\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.forward_visual(image.to(self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.forward_visual(images.to(self.dtype), output_hidden_states=True)\n            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        if hasattr(self.vision_tower, \"conv1\"):\n            return self.vision_tower.conv1.weight.dtype\n        if hasattr(self.vision_tower, \"trunk\"):\n            return self.vision_tower.trunk.patch_embed.proj.weight.dtype\n        raise NotImplementedError\n\n    @property\n    def device(self):\n        if hasattr(self.vision_tower, \"conv1\"):\n            return self.vision_tower.conv1.weight.device\n        if hasattr(self.vision_tower, \"trunk\"):\n            return self.vision_tower.trunk.patch_embed.proj.weight.device\n        raise NotImplementedError\n\n    @property\n    def config(self):\n        return None\n\n    @property\n    def hidden_size(self):\n        if self.model_name in HIDDEN_SIZE_DICT:\n            return HIDDEN_SIZE_DICT[self.model_name]\n        else:\n            raise NotImplementedError\n\n    @property\n    def num_patches(self):\n        image_size = self.resize_transform_size if isinstance(self.resize_transform_size, int) else self.resize_transform_size[0]\n        _num_patches = (image_size // self.patch_size) ** 2\n        if \"cls_patch\" in self.select_feature:\n            _num_patches += 1\n        return _num_patches\n\n    @property\n    def image_size(self):\n        return self.resize_transform_size\n\n    @property\n    def num_patches_per_side(self):\n        return self.resize_transform_size // self.patch_size\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/eva_clip/eva_clip_encoder.py", "content": "import torch\nimport torch.nn as nn\n\nfrom .eva_clip_processors import EvaClipImageTrainProcessor\nfrom .eva_vit import EVAEncoderWrapper\nfrom .factory import list_models, add_model_config, get_model_config\n\nfrom llava.utils import rank0_print\n\n\nclass EvaClipVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n        self.vision_tower_name = vision_tower\n        self.vision_tower_pretrained = args.vision_tower_pretrained\n        self.config = get_model_config(vision_tower)\n\n        if not delay_load:\n            rank0_print(f\"Loading EVA ViT: {self.vision_tower_name}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n        else:\n            self.cfg_only = self.config\n\n    def load_model(self, device_map=None):\n        rank0_print(f\"Pretrained: {self.vision_tower_pretrained}\")\n        self.image_processor = EvaClipImageTrainProcessor(self.config[\"vision_cfg\"][\"image_size\"])\n        self.vision_tower = EVAEncoderWrapper(self.vision_tower_pretrained, self.config)\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower.requires_grad_(False)\n        self.is_loaded = True\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_feature = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0)).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_features = self.vision_tower(images.to(device=self.device, dtype=self.dtype)).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dtype(self):\n        return self.vision_tower.dtype\n\n    @property\n    def device(self):\n        return self.vision_tower.device\n\n    @property\n    def hidden_size(self):\n        return self.config[\"vision_cfg\"][\"width\"]\n\n    @property\n    def num_patches(self):\n        return (self.config[\"vision_cfg\"][\"image_size\"] // self.config[\"vision_cfg\"][\"patch_size\"]) ** 2\n\n    @property\n    def num_patches_per_side(self):\n        return self.config[\"vision_cfg\"][\"image_size\"] // self.config[\"vision_cfg\"][\"patch_size\"]\n\n    @property\n    def image_size(self):\n        return self.config[\"vision_cfg\"][\"image_size\"]\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/transform.py", "content": "from typing import Optional, Sequence, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as F\n\nfrom torchvision.transforms import Normalize, Compose, RandomResizedCrop, InterpolationMode, ToTensor, Resize, CenterCrop\n\nfrom .constants import OPENAI_DATASET_MEAN, OPENAI_DATASET_STD\n\n\nclass ResizeMaxSize(nn.Module):\n\n    def __init__(self, max_size, interpolation=InterpolationMode.BICUBIC, fn=\"max\", fill=0):\n        super().__init__()\n        if not isinstance(max_size, int):\n            raise TypeError(f\"Size should be int. Got {type(max_size)}\")\n        self.max_size = max_size\n        self.interpolation = interpolation\n        self.fn = min if fn == \"min\" else min\n        self.fill = fill\n\n    def forward(self, img):\n        if isinstance(img, torch.Tensor):\n            height, width = img.shape[:2]\n        else:\n            width, height = img.size\n        scale = self.max_size / float(max(height, width))\n        if scale != 1.0:\n            new_size = tuple(round(dim * scale) for dim in (height, width))\n            img = F.resize(img, new_size, self.interpolation)\n            pad_h = self.max_size - new_size[0]\n            pad_w = self.max_size - new_size[1]\n            img = F.pad(img, padding=[pad_w // 2, pad_h // 2, pad_w - pad_w // 2, pad_h - pad_h // 2], fill=self.fill)\n        return img\n\n\ndef _convert_to_rgb(image):\n    return image.convert(\"RGB\")\n\n\n# class CatGen(nn.Module):\n#     def __init__(self, num=4):\n#         self.num = num\n#     def mixgen_batch(image, text):\n#         batch_size = image.shape[0]\n#         index = np.random.permutation(batch_size)\n\n#         cat_images = []\n#         for i in range(batch_size):\n#             # image mixup\n#             image[i,:] = lam * image[i,:] + (1 - lam) * image[index[i],:]\n#             # text concat\n#             text[i] = tokenizer((str(text[i]) + \" \" + str(text[index[i]])))[0]\n#         text = torch.stack(text)\n#         return image, text\n\n\ndef image_transform(\n    image_size: int,\n    is_train: bool,\n    mean: Optional[Tuple[float, ...]] = None,\n    std: Optional[Tuple[float, ...]] = None,\n    resize_longest_max: bool = False,\n    fill_color: int = 0,\n):\n    mean = mean or OPENAI_DATASET_MEAN\n    if not isinstance(mean, (list, tuple)):\n        mean = (mean,) * 3\n\n    std = std or OPENAI_DATASET_STD\n    if not isinstance(std, (list, tuple)):\n        std = (std,) * 3\n\n    if isinstance(image_size, (list, tuple)) and image_size[0] == image_size[1]:\n        # for square size, pass size as int so that Resize() uses aspect preserving shortest edge\n        image_size = image_size[0]\n\n    normalize = Normalize(mean=mean, std=std)\n    if is_train:\n        return Compose(\n            [\n                RandomResizedCrop(image_size, scale=(0.9, 1.0), interpolation=InterpolationMode.BICUBIC),\n                _convert_to_rgb,\n                ToTensor(),\n                normalize,\n            ]\n        )\n    else:\n        if resize_longest_max:\n            transforms = [ResizeMaxSize(image_size, fill=fill_color)]\n        else:\n            transforms = [\n                Resize(image_size, interpolation=InterpolationMode.BICUBIC),\n                CenterCrop(image_size),\n            ]\n        transforms.extend(\n            [\n                _convert_to_rgb,\n                ToTensor(),\n                normalize,\n            ]\n        )\n        return Compose(transforms)\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/eva_clip/eva_vit.py", "content": "\"\"\"\n# Adapted from https://github.com/baaivision/EVA/tree/master/EVA-CLIP\n\"\"\"\n\nfrom math import pi\nimport torch\nfrom torch import nn\nfrom einops import rearrange, repeat\nimport logging\nfrom llava.utils import rank0_print\n\n\ndef broadcat(tensors, dim=-1):\n    num_tensors = len(tensors)\n    shape_lens = set(list(map(lambda t: len(t.shape), tensors)))\n    assert len(shape_lens) == 1, \"tensors must all have the same number of dimensions\"\n    shape_len = list(shape_lens)[0]\n    dim = (dim + shape_len) if dim < 0 else dim\n    dims = list(zip(*map(lambda t: list(t.shape), tensors)))\n    expandable_dims = [(i, val) for i, val in enumerate(dims) if i != dim]\n    assert all([*map(lambda t: len(set(t[1])) <= 2, expandable_dims)]), \"invalid dimensions for broadcastable concatentation\"\n    max_dims = list(map(lambda t: (t[0], max(t[1])), expandable_dims))\n    expanded_dims = list(map(lambda t: (t[0], (t[1],) * num_tensors), max_dims))\n    expanded_dims.insert(dim, (dim, dims[dim]))\n    expandable_shapes = list(zip(*map(lambda t: t[1], expanded_dims)))\n    tensors = list(map(lambda t: t[0].expand(*t[1]), zip(tensors, expandable_shapes)))\n    return torch.cat(tensors, dim=dim)\n\n\ndef rotate_half(x):\n    x = rearrange(x, \"... (d r) -> ... d r\", r=2)\n    x1, x2 = x.unbind(dim=-1)\n    x = torch.stack((-x2, x1), dim=-1)\n    return rearrange(x, \"... d r -> ... (d r)\")\n\n\nclass VisionRotaryEmbeddingFast(nn.Module):\n    def __init__(self, dim, pt_seq_len, ft_seq_len=None, custom_freqs=None, freqs_for=\"lang\", theta=10000, max_freq=10, num_freqs=1, patch_dropout=0.0):\n        super().__init__()\n        if custom_freqs:\n            freqs = custom_freqs\n        elif freqs_for == \"lang\":\n            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n        elif freqs_for == \"pixel\":\n            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n        elif freqs_for == \"constant\":\n            freqs = torch.ones(num_freqs).float()\n        else:\n            raise ValueError(f\"unknown modality {freqs_for}\")\n\n        if ft_seq_len is None:\n            ft_seq_len = pt_seq_len\n        t = torch.arange(ft_seq_len) / ft_seq_len * pt_seq_len\n\n        freqs = torch.einsum(\"..., f -> ... f\", t, freqs)\n        freqs = repeat(freqs, \"... n -> ... (n r)\", r=2)\n        freqs = broadcat((freqs[:, None, :], freqs[None, :, :]), dim=-1)\n\n        freqs_cos = freqs.cos().view(-1, freqs.shape[-1])\n        freqs_sin = freqs.sin().view(-1, freqs.shape[-1])\n\n        self.patch_dropout = patch_dropout\n\n        self.register_buffer(\"freqs_cos\", freqs_cos)\n        self.register_buffer(\"freqs_sin\", freqs_sin)\n\n        logging.info(f\"Shape of rope freq: {self.freqs_cos.shape}\")\n\n    def forward(self, t, patch_indices_keep=None):\n        if patch_indices_keep is not None:\n            batch = t.size()[0]\n            batch_indices = torch.arange(batch)\n            batch_indices = batch_indices[..., None]\n\n            freqs_cos = repeat(self.freqs_cos, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n            freqs_sin = repeat(self.freqs_sin, \"i j -> n i m j\", n=t.shape[0], m=t.shape[1])\n\n            freqs_cos = freqs_cos[batch_indices, patch_indices_keep]\n            freqs_cos = rearrange(freqs_cos, \"n i m j -> n m i j\")\n            freqs_sin = freqs_sin[batch_indices, patch_indices_keep]\n            freqs_sin = rearrange(freqs_sin, \"n i m j -> n m i j\")\n\n            return t * freqs_cos + rotate_half(t) * freqs_sin\n\n        return t * self.freqs_cos + rotate_half(t) * self.freqs_sin\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm (with cast back to input dtype).\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        return x.to(orig_type)\n\n\nclass PatchDropout(nn.Module):\n    \"\"\"\n    https://arxiv.org/abs/2212.00794\n    \"\"\"\n\n    def __init__(self, prob, exclude_first_token=True):\n        super().__init__()\n        assert 0 <= prob < 1.0\n        self.prob = prob\n        self.exclude_first_token = exclude_first_token  # exclude CLS token\n        logging.info(f\"os.getenv('RoPE')={os.getenv('RoPE')}\")\n\n    def forward(self, x):\n        if not self.training or self.prob == 0.0:\n            return x\n\n        if self.exclude_first_token:\n            cls_tokens, x = x[:, :1], x[:, 1:]\n        else:\n            cls_tokens = torch.jit.annotate(torch.Tensor, x[:, :1])\n\n        batch = x.size()[0]\n        num_tokens = x.size()[1]\n\n        batch_indices = torch.arange(batch)\n        batch_indices = batch_indices[..., None]\n\n        keep_prob = 1 - self.prob\n        num_patches_keep = max(1, int(num_tokens * keep_prob))\n\n        rand = torch.randn(batch, num_tokens)\n        patch_indices_keep = rand.topk(num_patches_keep, dim=-1).indices\n\n        x = x[batch_indices, patch_indices_keep]\n\n        if self.exclude_first_token:\n            x = torch.cat((cls_tokens, x), dim=1)\n\n        if self.training and os.getenv(\"RoPE\") == \"1\":\n            return x, patch_indices_keep\n\n        return x\n\n\n# --------------------------------------------------------\n# Adapted from  https://github.com/microsoft/unilm/tree/master/beit\n# --------------------------------------------------------\nimport math\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from timm.models.layers import drop_path, to_2tuple, trunc_normal_\nexcept:\n    from timm.layers import drop_path, to_2tuple, trunc_normal_\n\nif os.getenv(\"ENV_TYPE\") == \"deepspeed\":\n    try:\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return \"p={}\".format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        drop=0.0,\n        subln=False,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.ffn_ln(x)\n\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass SwiGLU(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.SiLU, drop=0.0, norm_layer=nn.LayerNorm, subln=False):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        self.w1 = nn.Linear(in_features, hidden_features)\n        self.w2 = nn.Linear(in_features, hidden_features)\n\n        self.act = act_layer()\n        self.ffn_ln = norm_layer(hidden_features) if subln else nn.Identity()\n        self.w3 = nn.Linear(hidden_features, out_features)\n\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x1 = self.w1(x)\n        x2 = self.w2(x)\n        hidden = self.act(x1) * x2\n        x = self.ffn_ln(hidden)\n        x = self.w3(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0, window_size=None, attn_head_dim=None, xattn=False, rope=None, subln=False, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.subln = subln\n        if self.subln:\n            self.q_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.k_proj = nn.Linear(dim, all_head_dim, bias=False)\n            self.v_proj = nn.Linear(dim, all_head_dim, bias=False)\n        else:\n            self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.inner_attn_ln = norm_layer(all_head_dim) if subln else nn.Identity()\n        # self.proj = nn.Linear(all_head_dim, all_head_dim)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n\n        self.rope = rope\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        B, N, C = x.shape\n        if self.subln:\n            q = F.linear(input=x, weight=self.q_proj.weight, bias=self.q_bias)\n            k = F.linear(input=x, weight=self.k_proj.weight, bias=None)\n            v = F.linear(input=x, weight=self.v_proj.weight, bias=self.v_bias)\n\n            q = q.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)  # B, num_heads, N, C\n            k = k.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n            v = v.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n        else:\n\n            qkv_bias = None\n            if self.q_bias is not None:\n                qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n\n            qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n            qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)  # 3, B, num_heads, N, C\n            q, k, v = qkv[0], qkv[1], qkv[2]\n\n        if self.rope:\n            # slightly fast impl\n            q_t = q[:, :, 1:, :]\n            ro_q_t = self.rope(q_t)\n            q = torch.cat((q[:, :, :1, :], ro_q_t), -2).type_as(v)\n\n            k_t = k[:, :, 1:, :]\n            ro_k_t = self.rope(k_t)\n            k = torch.cat((k[:, :, :1, :], ro_k_t), -2).type_as(v)\n\n        if self.xattn and xops is not None:\n            q = q.permute(0, 2, 1, 3)  # B, num_heads, N, C -> B, N, num_heads, C\n            k = k.permute(0, 2, 1, 3)\n            v = v.permute(0, 2, 1, 3)\n\n            x = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                p=self.xattn_drop,\n                scale=self.scale,\n            )\n            x = x.reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n\n            if self.relative_position_bias_table is not None:\n                relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0).type_as(attn)\n\n            if rel_pos_bias is not None:\n                attn = attn + rel_pos_bias.type_as(attn)\n\n            if attn_mask is not None:\n                attn_mask = attn_mask.bool()\n                attn = attn.masked_fill(~attn_mask[:, None, None, :], float(\"-inf\"))\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n            x = self.inner_attn_ln(x)\n            x = self.proj(x)\n            x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        init_values=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        window_size=None,\n        attn_head_dim=None,\n        xattn=False,\n        rope=None,\n        postnorm=False,\n        subln=False,\n        naiveswiglu=False,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim, xattn=xattn, rope=rope, subln=subln, norm_layer=norm_layer\n        )\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n\n        if naiveswiglu:\n            self.mlp = SwiGLU(\n                in_features=dim,\n                hidden_features=mlp_hidden_dim,\n                subln=subln,\n                norm_layer=norm_layer,\n            )\n        else:\n            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, subln=subln, drop=drop)\n\n        if init_values is not None and init_values > 0:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n        self.postnorm = postnorm\n\n    def forward(self, x, rel_pos_bias=None, attn_mask=None):\n        if self.gamma_1 is None:\n            if self.postnorm:\n                x = x + self.drop_path(self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            if self.postnorm:\n                x = x + self.drop_path(self.gamma_1 * self.norm1(self.attn(x, rel_pos_bias=rel_pos_bias, attn_mask=attn_mask)))\n                x = x + self.drop_path(self.gamma_2 * self.norm2(self.mlp(x)))\n            else:\n                x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, attn_mask=attn_mask))\n                x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n    def forward(self):\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass EVAVisionTransformer(nn.Module):\n    \"\"\"Vision Transformer with support for patch or hybrid CNN input stage\"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        num_classes=1000,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=nn.LayerNorm,\n        init_values=None,\n        patch_dropout=0.0,\n        use_abs_pos_emb=True,\n        use_rel_pos_bias=False,\n        use_shared_rel_pos_bias=False,\n        rope=False,\n        use_mean_pooling=True,\n        init_scale=0.001,\n        grad_checkpointing=False,\n        xattn=False,\n        postnorm=False,\n        pt_hw_seq_len=16,\n        intp_freq=False,\n        naiveswiglu=False,\n        subln=False,\n    ):\n        super().__init__()\n        self.image_size = img_size\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        if rope:\n            half_head_dim = embed_dim // num_heads // 2\n            hw_seq_len = img_size // patch_size\n            self.rope = VisionRotaryEmbeddingFast(\n                dim=half_head_dim,\n                pt_seq_len=pt_hw_seq_len,\n                ft_seq_len=hw_seq_len if intp_freq else None,\n                # patch_dropout=patch_dropout\n            )\n        else:\n            self.rope = None\n\n        self.naiveswiglu = naiveswiglu\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    init_values=init_values,\n                    window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None,\n                    xattn=xattn,\n                    rope=self.rope,\n                    postnorm=postnorm,\n                    subln=subln,\n                    naiveswiglu=naiveswiglu,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=0.02)\n\n        trunc_normal_(self.cls_token, std=0.02)\n        # trunc_normal_(self.mask_token, std=.02)\n\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n        if isinstance(self.head, nn.Linear):\n            trunc_normal_(self.head.weight, std=0.02)\n            self.head.weight.data.mul_(init_scale)\n            self.head.bias.data.mul_(init_scale)\n\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0.0 else nn.Identity()\n\n        self.grad_checkpointing = grad_checkpointing\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            if self.naiveswiglu:\n                rescale(layer.mlp.w3.weight.data, layer_id + 1)\n            else:\n                rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.blocks[0].mlp.fc2.weight.dtype\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        assert unlocked_groups == 0, \"partial locking not currently supported for this model\"\n        for param in self.parameters():\n            param.requires_grad = False\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"pos_embed\", \"cls_token\"}\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=\"\"):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x, return_all_features=False):\n\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        if os.getenv(\"RoPE\") == \"1\":\n            if self.training and not isinstance(self.patch_dropout, nn.Identity):\n                x, patch_indices_keep = self.patch_dropout(x)\n                # Directly pass patch_indices_keep to self.rope.forward\n                x = self.rope.forward(x, patch_indices_keep=patch_indices_keep)\n            else:\n                # Pass None or omit the patch_indices_keep argument for default behavior\n                x = self.rope.forward(x, patch_indices_keep=None)\n                x = self.patch_dropout(x)\n        else:\n            x = self.patch_dropout(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for i, blk in enumerate(self.blocks):\n            if i == len(self.blocks) - 1:\n                continue\n            if self.grad_checkpointing:\n                x = checkpoint(blk, x, (rel_pos_bias,))\n            else:\n                x = blk(x, rel_pos_bias=rel_pos_bias)\n\n        if not return_all_features:\n            x = self.norm(x)\n            if self.fc_norm is not None:\n                return self.fc_norm(x.mean(1))\n            else:\n                return x[:, 0]\n        return x\n\n    def forward(self, x, return_all_features=False):\n        if return_all_features:\n            return self.forward_features(x, return_all_features)\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\n\ndef load_state_dict(checkpoint_path: str, map_location: str = \"cpu\", model_key: str = \"model|module|state_dict\", is_openai: bool = False, skip_list: list = []):\n    if is_openai:\n        model = torch.jit.load(checkpoint_path, map_location=\"cpu\").eval()\n        state_dict = model.state_dict()\n        for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n            state_dict.pop(key, None)\n    else:\n        checkpoint = torch.load(checkpoint_path, map_location=map_location)\n        for mk in model_key.split(\"|\"):\n            if isinstance(checkpoint, dict) and mk in checkpoint:\n                state_dict = checkpoint[mk]\n                break\n            else:\n                state_dict = checkpoint\n        if next(iter(state_dict.items()))[0].startswith(\"module\"):\n            state_dict = {k[7:]: v for k, v in state_dict.items()}\n\n    for k in skip_list:\n        if k in list(state_dict.keys()):\n            logging.info(f\"Removing key {k} from pretrained checkpoint\")\n            del state_dict[k]\n\n    if os.getenv(\"RoPE\") == \"1\":\n        for k in list(state_dict.keys()):\n            if \"freqs_cos\" in k or \"freqs_sin\" in k:\n                del state_dict[k]\n    return state_dict\n\n\ndef load_clip_visual_state_dict(checkpoint_path: str, map_location: str = \"cpu\", is_openai: bool = False, skip_list: list = []):\n    state_dict = load_state_dict(checkpoint_path, map_location=map_location, is_openai=is_openai, skip_list=skip_list)\n    # for k in list(state_dict.keys()):\n    #     if not k.startswith(\"visual.\"):\n    #         del state_dict[k]\n    # for k in list(state_dict.keys()):\n    #     if k.startswith(\"visual.\"):\n    #         new_k = k[7:]\n    #         state_dict[new_k] = state_dict[k]\n    #         del state_dict[k]\n    return state_dict\n\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\n\ntry:\n    from apex.normalization import FusedLayerNorm\nexcept:\n    FusedLayerNorm = LayerNorm\n    # print(\"Please build and install Nvidia apex package with option '--cuda_ext' according to https://github.com/NVIDIA/apex#from-source .\")\n\n\n@dataclass\nclass CLIPVisionCfg:\n    layers: Union[Tuple[int, int, int, int], int] = 12\n    width: int = 768\n    head_width: int = 64\n    mlp_ratio: float = 4.0\n    patch_size: int = 16\n    image_size: Union[Tuple[int, int], int] = 224\n    ls_init_value: Optional[float] = None  # layer scale initial value\n    patch_dropout: float = 0.0  # what fraction of patches to dropout during training (0 would mean disabled and no patches dropped) - 0.5 to 0.75 recommended in the paper for optimal results\n    global_average_pool: bool = False  # whether to global average pool the last embedding layer, instead of using CLS token (https://arxiv.org/abs/2205.01580)\n    drop_path_rate: Optional[float] = None  # drop path rate\n    timm_model_name: str = None  # a valid model name overrides layers, width, patch_size\n    timm_model_pretrained: bool = False  # use (imagenet) pretrained weights for named model\n    timm_pool: str = \"avg\"  # feature pooling for timm model ('abs_attn', 'rot_attn', 'avg', '')\n    timm_proj: str = \"linear\"  # linear projection for timm model output ('linear', 'mlp', '')\n    timm_proj_bias: bool = False  # enable bias final projection\n    eva_model_name: str = None  # a valid eva model name overrides layers, width, patch_size\n    qkv_bias: bool = True\n    fusedLN: bool = False\n    xattn: bool = False\n    postnorm: bool = False\n    rope: bool = False\n    pt_hw_seq_len: int = 16  # 224/14\n    intp_freq: bool = False\n    naiveswiglu: bool = False\n    subln: bool = False\n\n\ndef create_norm_layer_factory(use_fused_ln, eps=1e-6):\n    # Otherwise, use the standard LayerNorm\n    return lambda num_features: nn.LayerNorm(num_features, eps=eps)\n\n\ndef _build_vision_tower(vision_tower_path: str, embed_dim: int, vision_cfg: CLIPVisionCfg, **kwargs):\n    if isinstance(vision_cfg, dict):\n        vision_cfg = CLIPVisionCfg(**vision_cfg)\n\n    if vision_cfg.eva_model_name:\n        vision_heads = vision_cfg.width // vision_cfg.head_width\n        # Determine the appropriate norm layer factory based on the configuration\n        norm_layer_factory = create_norm_layer_factory(vision_cfg.fusedLN, eps=1e-6)\n\n        visual = EVAVisionTransformer(\n            img_size=vision_cfg.image_size,\n            patch_size=vision_cfg.patch_size,\n            num_classes=embed_dim,\n            use_mean_pooling=vision_cfg.global_average_pool,  # False\n            init_values=vision_cfg.ls_init_value,\n            patch_dropout=vision_cfg.patch_dropout,\n            embed_dim=vision_cfg.width,\n            depth=vision_cfg.layers,\n            num_heads=vision_heads,\n            mlp_ratio=vision_cfg.mlp_ratio,\n            qkv_bias=vision_cfg.qkv_bias,\n            drop_path_rate=vision_cfg.drop_path_rate,\n            norm_layer=norm_layer_factory,\n            xattn=vision_cfg.xattn,\n            rope=vision_cfg.rope,\n            postnorm=vision_cfg.postnorm,\n            pt_hw_seq_len=vision_cfg.pt_hw_seq_len,  # 224/14\n            intp_freq=vision_cfg.intp_freq,\n            naiveswiglu=vision_cfg.naiveswiglu,\n            subln=vision_cfg.subln,\n        )\n\n        state_dict = load_clip_visual_state_dict(vision_tower_path)\n        incompatible_keys = visual.load_state_dict(state_dict, strict=False)\n        rank0_print(\"EVA-CLIP incompatible_keys:\", incompatible_keys)\n\n    return visual\n\n\nclass EVAEncoderWrapper(nn.Module):\n    def __init__(self, vision_tower_pretrained, config):\n        super(EVAEncoderWrapper, self).__init__()\n        self.config = config\n        self.config[\"vision_tower_path\"] = vision_tower_pretrained\n        self.model = _build_vision_tower(**self.config)\n\n    def forward(self, image, **kwargs):\n        encode = self.model(image, return_all_features=True)[:, 1:, :]  # remove the CLS token\n        return encode\n\n    @property\n    def dtype(self):\n        return list(self.parameters())[-1].dtype\n\n    @property\n    def device(self):\n        return list(self.parameters())[-1].device\n"}
{"type": "source_file", "path": "llava/model/language_model/modeling_llama.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LLaMA model.\"\"\"\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache, DynamicCache, StaticCache\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.models.llama.configuration_llama import LlamaConfig\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\nALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)\n\n\nclass LlamaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        super().__init__()\n        self.scaling_factor = scaling_factor\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        # For BC we register cos and sin cached\n        self.max_seq_len_cached = max_position_embeddings\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n        t = t / self.scaling_factor\n        freqs = torch.outer(t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"_cos_cached\", emb.cos().to(torch.get_default_dtype()), persistent=False)\n        self.register_buffer(\"_sin_cached\", emb.sin().to(torch.get_default_dtype()), persistent=False)\n\n    @property\n    def sin_cached(self):\n        logger.warning_once(\"The sin_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use \" \"the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\")\n        return self._sin_cached\n\n    @property\n    def cos_cached(self):\n        logger.warning_once(\"The cos_cached attribute will be removed in 4.39. Bear in mind that its contents changed in v4.38. Use \" \"the forward method of RoPE from now on instead. It is not used in the `LlamaAttention` class\")\n        return self._cos_cached\n\n    @torch.no_grad()\n    def forward(self, x, position_ids, seq_len=None):\n        if seq_len is not None:\n            logger.warning_once(\"The `seq_len` argument is deprecated and unused. It will be removed in v4.39.\")\n\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nclass LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def forward(self, x, position_ids, seq_len=None):\n        # difference to the original RoPE: a scaling factor is aplied to the position ids\n        position_ids = position_ids.float() / self.scaling_factor\n        cos, sin = super().forward(x, position_ids, seq_len)\n        return cos, sin\n\n\nclass LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def forward(self, x, position_ids, seq_len=None):\n        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n        seq_len = torch.max(position_ids) + 1\n        if seq_len > self.max_position_embeddings:\n            base = self.base * ((self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim))\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n\n        cos, sin = super().forward(x, position_ids, seq_len)\n        return cos, sin\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        if self.config.pretraining_tp > 1:\n            slice = self.intermediate_size // self.config.pretraining_tp\n            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n\n            gate_proj = torch.cat([F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n\n            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n            down_proj = [F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)]\n            down_proj = sum(down_proj)\n        else:\n            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n        return down_proj\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\" f\" and `num_heads`: {self.num_heads}).\")\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split((self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0)\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask\n            if cache_position is not None:\n                causal_mask = attention_mask[:, :, cache_position, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\" f\" {attn_output.size()}\")\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass LlamaRingFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\" f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\" f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_output = self._flash_attention_forward(query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate)\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`int`, *optional*):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(query_states, key_states, value_states, attention_mask, query_length)\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = zigzag_ring_flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            # pack qkv\n            # query_states: (batch_size, seqlen, nheads, headdim)\n            # qkv: (batch_size, seqlen, 3, nheads, headdim)\n            qkv = torch.stack([query_states, key_states, value_states], dim=2)\n            attn_output = zigzag_ring_flash_attn_qkvpacked_func(qkv, dropout, softmax_scale, causal=causal)\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        value_layer = index_first_axis(value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k)\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(batch_size + 1, dtype=torch.int32, device=query_layer.device)  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\" f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\" f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_output = self._flash_attention_forward(query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate)\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`int`, *optional*):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(query_states, key_states, value_states, attention_mask, query_length)\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal)\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        value_layer = index_first_axis(value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k)\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(batch_size + 1, dtype=torch.int32, device=query_layer.device)  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaSdpaAttention(LlamaAttention):\n    \"\"\"\n    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n\n    # Adapted from LlamaAttention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if output_attentions:\n            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n            logger.warning_once(\n                \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        # In case static cache is used, it is an instance attribute.\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n        if attention_mask is not None and cache_position is not None:\n            causal_mask = causal_mask[:, :, cache_position, : key_states.shape[-2]]\n\n        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n        if query_states.device.type == \"cuda\" and causal_mask is not None:\n            query_states = query_states.contiguous()\n            key_states = key_states.contiguous()\n            value_states = value_states.contiguous()\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=causal_mask,\n            dropout_p=self.attention_dropout if self.training else 0.0,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, None, past_key_value\n\n\ntry:\n    from ring_flash_attn import zigzag_ring_flash_attn_qkvpacked_func, zigzag_ring_flash_attn_varlen_func\nexcept ImportError:\n    print(\"Please install the ring-flash-attn package\")\n\nLLAMA_ATTENTION_CLASSES = {\n    \"eager\": LlamaAttention,\n    \"flash_attention_2\": LlamaFlashAttention2,\n    \"ring_flash_attention_2\": LlamaRingFlashAttention2,\n    \"sdpa\": LlamaSdpaAttention,\n}\n\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n\n        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n\n        self.mlp = LlamaMLP(config)\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*):\n                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                query_sequence_length, key_sequence_length)` if default attention is used.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n        if \"padding_mask\" in kwargs:\n            warnings.warn(\"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\")\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n            **kwargs,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n    _supports_flash_attn_2 = True\n    _supports_sdpa = True\n    _supports_cache_class = True\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _setup_cache(self, cache_cls, max_batch_size, max_cache_len: Optional[int] = None):\n        if self.config._attn_implementation == \"flash_attention_2\" and cache_cls == StaticCache:\n            raise ValueError(\"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \" \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\")\n\n        if max_cache_len > self.model.causal_mask.shape[-1] or self.device != self.model.causal_mask.device:\n            causal_mask = torch.full((max_cache_len, max_cache_len), fill_value=True, device=self.device, dtype=torch.bool)\n            self.register_buffer(\"causal_mask\", torch.triu(causal_mask, diagonal=1), persistent=False)\n\n        for layer in self.model.layers:\n            device = layer.input_layernorm.weight.device\n            if hasattr(self.config, \"_pre_quantization_dtype\"):\n                dtype = self.config._pre_quantization_dtype\n            else:\n                dtype = layer.self_attn.o_proj.weight.dtype\n            layer.self_attn.past_key_value = cache_cls(self.config, max_batch_size, max_cache_len, device=device, dtype=dtype)\n\n    def _reset_cache(self):\n        for layer in self.model.layers:\n            layer.self_attn.past_key_value = None\n\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n\n            Two formats are allowed:\n            - a [`~cache_utils.Cache`] instance;\n            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n            cache format.\n\n            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n            legacy cache format will be returned.\n\n            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n            of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)])\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.gradient_checkpointing = False\n\n        # Register a causal mask to separate causal and padding mask creation. Merging happens in the attention class.\n        # NOTE: This is not friendly with TorchScript, ONNX, ExportedProgram serialization for very large `max_position_embeddings`.\n        causal_mask = torch.full((config.max_position_embeddings, config.max_position_embeddings), fill_value=True, dtype=torch.bool)\n        self.register_buffer(\"causal_mask\", torch.triu(causal_mask, diagonal=1), persistent=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\")\n\n        if self.gradient_checkpointing and self.training and use_cache:\n            logger.warning_once(\"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\")\n            use_cache = False\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        past_seen_tokens = 0\n        if use_cache:  # kept for BC (cache positions)\n            if not isinstance(past_key_values, StaticCache):\n                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n                past_seen_tokens = past_key_values.get_seq_length()\n\n        if cache_position is None:\n            if isinstance(past_key_values, StaticCache):\n                raise ValueError(\"cache_position is a required argument when using StaticCache.\")\n            cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)\n\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n\n        causal_mask = self._update_causal_mask(attention_mask, inputs_embeds)\n\n        # embed positions\n        hidden_states = inputs_embeds\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = None\n\n        for decoder_layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    causal_mask,\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                    cache_position,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=causal_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    cache_position=cache_position,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = None\n        if use_cache:\n            next_cache = next_decoder_cache.to_legacy_cache() if isinstance(next_decoder_cache, Cache) else next_decoder_cache\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n\n    # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n    # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n    # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n    # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n    def _update_causal_mask(self, attention_mask, input_tensor):\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and 0.0 in attention_mask:\n                return attention_mask\n            return None\n\n        batch_size, seq_length = input_tensor.shape[:2]\n        dtype = input_tensor.dtype\n        device = input_tensor.device\n\n        # support going beyond cached `max_position_embedding`\n        if seq_length > self.causal_mask.shape[-1]:\n            causal_mask = torch.full((2 * self.causal_mask.shape[-1], 2 * self.causal_mask.shape[-1]), fill_value=1)\n            self.register_buffer(\"causal_mask\", torch.triu(causal_mask, diagonal=1), persistent=False)\n\n        # We use the current dtype to avoid any overflows\n        min_dtype = torch.finfo(dtype).min\n        causal_mask = self.causal_mask[None, None, :, :].repeat(batch_size, 1, 1, 1).to(dtype) * min_dtype\n\n        causal_mask = causal_mask.to(dtype=dtype, device=device)\n        if attention_mask is not None and attention_mask.dim() == 2:\n            mask_length = attention_mask.shape[-1]\n            padding_mask = causal_mask[..., :mask_length].eq(0.0) * attention_mask[:, None, None, :].eq(0.0)\n            causal_mask[..., :mask_length] = causal_mask[..., :mask_length].masked_fill(padding_mask, min_dtype)\n\n        if self.config._attn_implementation == \"sdpa\" and attention_mask is not None and attention_mask.device.type == \"cuda\":\n            # TODO: For dynamo, rather use a check on fullgraph=True once this is possible (https://github.com/pytorch/pytorch/pull/120400).\n            is_tracing = torch.jit.is_tracing() or isinstance(input_tensor, torch.fx.Proxy) or (hasattr(torch, \"_dynamo\") and torch._dynamo.is_compiling())\n            if not is_tracing and torch.any(attention_mask != 1):\n                # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n                # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n                # Details: https://github.com/pytorch/pytorch/issues/110213\n                causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n\n        return causal_mask\n\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n        )\n\n        hidden_states = outputs[0]\n        if self.config.pretraining_tp > 1:\n            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n            logits = torch.cat(logits, dim=-1)\n        else:\n            logits = self.lm_head(hidden_states)\n        logits = logits.float()\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs):\n        past_length = 0\n        if past_key_values is not None:\n            if isinstance(past_key_values, Cache):\n                cache_length = past_key_values.get_seq_length()\n                past_length = past_key_values.seen_tokens\n                max_cache_length = past_key_values.get_max_length()\n            else:\n                cache_length = past_length = past_key_values[0][0].shape[2]\n                max_cache_length = None\n\n            # Keep only the unprocessed tokens:\n            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as\n            # input)\n            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n            # input_ids based on the past_length.\n            elif past_length < input_ids.shape[1]:\n                input_ids = input_ids[:, past_length:]\n            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n\n            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n            if max_cache_length is not None and attention_mask is not None and cache_length + input_ids.shape[1] > max_cache_length:\n                attention_mask = attention_mask[:, -max_cache_length:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -input_ids.shape[1] :]\n\n        if self.generation_config.cache_implementation == \"static\":\n            # generation with static cache\n            cache_position = kwargs.get(\"cache_position\", None)\n            if cache_position is None:\n                past_length = 0\n            else:\n                past_length = cache_position[-1] + 1\n            input_ids = input_ids[:, past_length:]\n            position_ids = position_ids[:, past_length:]\n\n        # TODO @gante we should only keep a `cache_position` in generate, and do +=1.\n        # same goes for position ids. Could also help with continued generation.\n        input_length = position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]\n        cache_position = torch.arange(past_length, past_length + input_length, device=input_ids.device)\n        position_ids = position_ids.contiguous() if position_ids is not None else None\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise\n            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114\n            # TODO: use `next_tokens` directly instead.\n            model_inputs = {\"input_ids\": input_ids.contiguous()}\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"cache_position\": cache_position,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),)\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n\n    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n                sequence_lengths = sequence_lengths.to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"\nThe Llama Model transformer with a span classification head on top for extractive question-answering tasks like\nSQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForQuestionAnswering(LlamaPreTrainedModel):\n    base_model_prefix = \"transformer\"\n\n    # Copied from transformers.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Llama\n    def __init__(self, config):\n        super().__init__(config)\n        self.transformer = LlamaModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.transformer.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.transformer.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.transformer(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1).to(start_logits.device)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1).to(end_logits.device)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/utils.py", "content": "from itertools import repeat\nimport collections.abc\nimport logging\nimport math\nimport numpy as np\n\nimport torch\nfrom torch import nn as nn\nfrom torchvision.ops.misc import FrozenBatchNorm2d\nimport torch.nn.functional as F\n\n\n# open CLIP\ndef resize_clip_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    # Rescale the grid of position embeddings when loading from state_dict\n    old_pos_embed = state_dict.get(\"visual.positional_embedding\", None)\n    if old_pos_embed is None or not hasattr(model.visual, \"grid_size\"):\n        return\n    grid_size = to_2tuple(model.visual.grid_size)\n    extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n    new_seq_len = grid_size[0] * grid_size[1] + extra_tokens\n    if new_seq_len == old_pos_embed.shape[0]:\n        return\n\n    if extra_tokens:\n        pos_emb_tok, pos_emb_img = old_pos_embed[:extra_tokens], old_pos_embed[extra_tokens:]\n    else:\n        pos_emb_tok, pos_emb_img = None, old_pos_embed\n    old_grid_size = to_2tuple(int(math.sqrt(len(pos_emb_img))))\n\n    logging.info(\"Resizing position embedding grid-size from %s to %s\", old_grid_size, grid_size)\n    pos_emb_img = pos_emb_img.reshape(1, old_grid_size[0], old_grid_size[1], -1).permute(0, 3, 1, 2)\n    pos_emb_img = F.interpolate(\n        pos_emb_img,\n        size=grid_size,\n        mode=interpolation,\n        align_corners=True,\n    )\n    pos_emb_img = pos_emb_img.permute(0, 2, 3, 1).reshape(1, grid_size[0] * grid_size[1], -1)[0]\n    if pos_emb_tok is not None:\n        new_pos_embed = torch.cat([pos_emb_tok, pos_emb_img], dim=0)\n    else:\n        new_pos_embed = pos_emb_img\n    state_dict[\"visual.positional_embedding\"] = new_pos_embed\n\n\ndef resize_visual_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    # Rescale the grid of position embeddings when loading from state_dict\n    old_pos_embed = state_dict.get(\"positional_embedding\", None)\n    if old_pos_embed is None or not hasattr(model.visual, \"grid_size\"):\n        return\n    grid_size = to_2tuple(model.visual.grid_size)\n    extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n    new_seq_len = grid_size[0] * grid_size[1] + extra_tokens\n    if new_seq_len == old_pos_embed.shape[0]:\n        return\n\n    if extra_tokens:\n        pos_emb_tok, pos_emb_img = old_pos_embed[:extra_tokens], old_pos_embed[extra_tokens:]\n    else:\n        pos_emb_tok, pos_emb_img = None, old_pos_embed\n    old_grid_size = to_2tuple(int(math.sqrt(len(pos_emb_img))))\n\n    logging.info(\"Resizing position embedding grid-size from %s to %s\", old_grid_size, grid_size)\n    pos_emb_img = pos_emb_img.reshape(1, old_grid_size[0], old_grid_size[1], -1).permute(0, 3, 1, 2)\n    pos_emb_img = F.interpolate(\n        pos_emb_img,\n        size=grid_size,\n        mode=interpolation,\n        align_corners=True,\n    )\n    pos_emb_img = pos_emb_img.permute(0, 2, 3, 1).reshape(1, grid_size[0] * grid_size[1], -1)[0]\n    if pos_emb_tok is not None:\n        new_pos_embed = torch.cat([pos_emb_tok, pos_emb_img], dim=0)\n    else:\n        new_pos_embed = pos_emb_img\n    state_dict[\"positional_embedding\"] = new_pos_embed\n\n\ndef resize_evaclip_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    all_keys = list(state_dict.keys())\n    # interpolate position embedding\n    if \"visual.pos_embed\" in state_dict:\n        pos_embed_checkpoint = state_dict[\"visual.pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.visual.patch_embed.num_patches\n        # num_extra_tokens = model.visual.pos_embed.shape[-2] - num_patches\n        num_extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches**0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(pos_tokens, size=(new_size, new_size), mode=\"bicubic\", align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[\"visual.pos_embed\"] = new_pos_embed\n\n            patch_embed_proj = state_dict[\"visual.patch_embed.proj.weight\"]\n            patch_size = model.visual.patch_embed.patch_size\n            state_dict[\"visual.patch_embed.proj.weight\"] = torch.nn.functional.interpolate(patch_embed_proj.float(), size=patch_size, mode=\"bicubic\", align_corners=False)\n\n\ndef resize_eva_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    all_keys = list(state_dict.keys())\n    # interpolate position embedding\n    if \"pos_embed\" in state_dict:\n        pos_embed_checkpoint = state_dict[\"pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.visual.patch_embed.num_patches\n        # num_extra_tokens = model.visual.pos_embed.shape[-2] - num_patches\n        num_extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches**0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(pos_tokens, size=(new_size, new_size), mode=\"bicubic\", align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[\"pos_embed\"] = new_pos_embed\n\n            patch_embed_proj = state_dict[\"patch_embed.proj.weight\"]\n            patch_size = model.visual.patch_embed.patch_size\n            state_dict[\"patch_embed.proj.weight\"] = torch.nn.functional.interpolate(patch_embed_proj.float(), size=patch_size, mode=\"bicubic\", align_corners=False)\n\n\ndef resize_rel_pos_embed(state_dict, model, interpolation: str = \"bicubic\", seq_dim=1):\n    all_keys = list(state_dict.keys())\n    for key in all_keys:\n        if \"relative_position_index\" in key:\n            state_dict.pop(key)\n\n        if \"relative_position_bias_table\" in key:\n            rel_pos_bias = state_dict[key]\n            src_num_pos, num_attn_heads = rel_pos_bias.size()\n            dst_num_pos, _ = model.visual.state_dict()[key].size()\n            dst_patch_shape = model.visual.patch_embed.patch_shape\n            if dst_patch_shape[0] != dst_patch_shape[1]:\n                raise NotImplementedError()\n            num_extra_tokens = dst_num_pos - (dst_patch_shape[0] * 2 - 1) * (dst_patch_shape[1] * 2 - 1)\n            src_size = int((src_num_pos - num_extra_tokens) ** 0.5)\n            dst_size = int((dst_num_pos - num_extra_tokens) ** 0.5)\n            if src_size != dst_size:\n                print(\"Position interpolate for %s from %dx%d to %dx%d\" % (key, src_size, src_size, dst_size, dst_size))\n                extra_tokens = rel_pos_bias[-num_extra_tokens:, :]\n                rel_pos_bias = rel_pos_bias[:-num_extra_tokens, :]\n\n                def geometric_progression(a, r, n):\n                    return a * (1.0 - r**n) / (1.0 - r)\n\n                left, right = 1.01, 1.5\n                while right - left > 1e-6:\n                    q = (left + right) / 2.0\n                    gp = geometric_progression(1, q, src_size // 2)\n                    if gp > dst_size // 2:\n                        right = q\n                    else:\n                        left = q\n\n                # if q > 1.090307:\n                #     q = 1.090307\n\n                dis = []\n                cur = 1\n                for i in range(src_size // 2):\n                    dis.append(cur)\n                    cur += q ** (i + 1)\n\n                r_ids = [-_ for _ in reversed(dis)]\n\n                x = r_ids + [0] + dis\n                y = r_ids + [0] + dis\n\n                t = dst_size // 2.0\n                dx = np.arange(-t, t + 0.1, 1.0)\n                dy = np.arange(-t, t + 0.1, 1.0)\n\n                print(\"Original positions = %s\" % str(x))\n                print(\"Target positions = %s\" % str(dx))\n\n                all_rel_pos_bias = []\n\n                for i in range(num_attn_heads):\n                    z = rel_pos_bias[:, i].view(src_size, src_size).float().numpy()\n                    f = F.interpolate.interp2d(x, y, z, kind=\"cubic\")\n                    all_rel_pos_bias.append(torch.Tensor(f(dx, dy)).contiguous().view(-1, 1).to(rel_pos_bias.device))\n\n                rel_pos_bias = torch.cat(all_rel_pos_bias, dim=-1)\n\n                new_rel_pos_bias = torch.cat((rel_pos_bias, extra_tokens), dim=0)\n                state_dict[key] = new_rel_pos_bias\n\n    # interpolate position embedding\n    if \"pos_embed\" in state_dict:\n        pos_embed_checkpoint = state_dict[\"pos_embed\"]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.visual.patch_embed.num_patches\n        num_extra_tokens = model.visual.pos_embed.shape[-2] - num_patches\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches**0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(pos_tokens, size=(new_size, new_size), mode=\"bicubic\", align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[\"pos_embed\"] = new_pos_embed\n\n            patch_embed_proj = state_dict[\"patch_embed.proj.weight\"]\n            patch_size = model.visual.patch_embed.patch_size\n            state_dict[\"patch_embed.proj.weight\"] = torch.nn.functional.interpolate(patch_embed_proj.float(), size=patch_size, mode=\"bicubic\", align_corners=False)\n\n\ndef freeze_batch_norm_2d(module, module_match={}, name=\"\"):\n    \"\"\"\n    Converts all `BatchNorm2d` and `SyncBatchNorm` layers of provided module into `FrozenBatchNorm2d`. If `module` is\n    itself an instance of either `BatchNorm2d` or `SyncBatchNorm`, it is converted into `FrozenBatchNorm2d` and\n    returned. Otherwise, the module is walked recursively and submodules are converted in place.\n\n    Args:\n        module (torch.nn.Module): Any PyTorch module.\n        module_match (dict): Dictionary of full module names to freeze (all if empty)\n        name (str): Full module name (prefix)\n\n    Returns:\n        torch.nn.Module: Resulting module\n\n    Inspired by https://github.com/pytorch/pytorch/blob/a5895f85be0f10212791145bfedc0261d364f103/torch/nn/modules/batchnorm.py#L762\n    \"\"\"\n    res = module\n    is_match = True\n    if module_match:\n        is_match = name in module_match\n    if is_match and isinstance(module, (nn.modules.batchnorm.BatchNorm2d, nn.modules.batchnorm.SyncBatchNorm)):\n        res = FrozenBatchNorm2d(module.num_features)\n        res.num_features = module.num_features\n        res.affine = module.affine\n        if module.affine:\n            res.weight.data = module.weight.data.clone().detach()\n            res.bias.data = module.bias.data.clone().detach()\n        res.running_mean.data = module.running_mean.data\n        res.running_var.data = module.running_var.data\n        res.eps = module.eps\n    else:\n        for child_name, child in module.named_children():\n            full_child_name = \".\".join([name, child_name]) if name else child_name\n            new_child = freeze_batch_norm_2d(child, module_match, full_child_name)\n            if new_child is not child:\n                res.add_module(child_name, new_child)\n    return res\n\n\n# From PyTorch internals\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return x\n        return tuple(repeat(x, n))\n\n    return parse\n\n\nto_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = lambda n, x: _ntuple(n)(x)\n\n\ndef is_logging(args):\n    def is_global_master(args):\n        return args.rank == 0\n\n    def is_local_master(args):\n        return args.local_rank == 0\n\n    def is_master(args, local=False):\n        return is_local_master(args) if local else is_global_master(args)\n\n    return is_master\n\n\nclass AllGather(torch.autograd.Function):\n    \"\"\"An autograd function that performs allgather on a tensor.\n    Performs all_gather operation on the provided tensors.\n    *** Warning ***: torch.distributed.all_gather has no gradient.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, tensor, rank, world_size):\n        tensors_gather = [torch.empty_like(tensor) for _ in range(world_size)]\n        torch.distributed.all_gather(tensors_gather, tensor)\n        ctx.rank = rank\n        ctx.batch_size = tensor.shape[0]\n        return torch.cat(tensors_gather, 0)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return (grad_output[ctx.batch_size * ctx.rank : ctx.batch_size * (ctx.rank + 1)], None, None)\n\n\nallgather = AllGather.apply\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/mlcd/vit_rope2d_hf.py", "content": "from typing import Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom transformers.models.clip.modeling_clip import (CLIPMLP, BaseModelOutput,\n                                                    BaseModelOutputWithPooling,\n                                                    CLIPVisionConfig,\n                                                    PreTrainedModel)\n\n\ndef rotate_half(x):\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb_vision(tensor: torch.Tensor, freqs: torch.Tensor) -> torch.Tensor:\n    orig_dtype = tensor.dtype\n    tensor = tensor.float()\n    cos = freqs.cos()\n    sin = freqs.sin()\n    cos = cos.unsqueeze(1).repeat(1, 1, 2).unsqueeze(0).float()\n    sin = sin.unsqueeze(1).repeat(1, 1, 2).unsqueeze(0).float()\n    output = (tensor * cos) + (rotate_half(tensor) * sin)\n    output = output.to(orig_dtype)\n    return output\n\n\nclass VisionRotaryEmbedding(nn.Module):\n    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n        super().__init__()\n        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n    def forward(self, seqlen: int) -> torch.Tensor:\n        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n        freqs = torch.outer(seq, self.inv_freq)\n        return freqs\n\n\nclass MLCDVisionConfig(CLIPVisionConfig):\n\n    model_type = \"mlcd_vision_model\"\n\n    def __init__(self,**kwargs):\n        super().__init__(**kwargs)\n\n\nclass MLCDMLP(CLIPMLP):\n    def __init__(self, config: MLCDVisionConfig):\n        super().__init__(config)\n\n\nclass MLCDVisionEmbeddings(torch.nn.Module):\n    def __init__(self, config: MLCDVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.image_size = config.image_size\n        self.patch_size = config.patch_size\n\n        self.class_embedding = nn.Parameter(torch.randn(self.embed_dim))\n\n        self.patch_embedding = nn.Conv2d(\n            in_channels=config.num_channels,\n            out_channels=self.embed_dim,\n            kernel_size=self.patch_size,\n            stride=self.patch_size,\n            bias=False,\n        )\n\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        self.num_positions = self.num_patches + 1\n\n\n    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n        batch_size = pixel_values.shape[0]\n        target_dtype = self.patch_embedding.weight.dtype\n        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n\n        class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n\n        return embeddings\n\n\nclass MLCDSdpaAttention(torch.nn.Module):\n    \"\"\"Multi-headed attention from these papers\n\n    - Attention is all you need:\n        https://arxiv.org/abs/1706.03762\n\n    - RoFormer: Enhanced Transformer with Rotary Position Embedding:\n        https://arxiv.org/abs/2104.09864\n    \"\"\"\n\n    def __init__(self, config: MLCDVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n        if self.head_dim * self.num_heads != self.embed_dim:\n            raise ValueError(\n                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n                f\" {self.num_heads}).\"\n            )\n        self.scale = self.head_dim**-0.5\n        self.dropout = config.attention_dropout\n        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        rotary_pos_emb: torch.Tensor,\n        ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"Input shape: Batch x Seq x Hidden Size\"\"\"\n        batch_size, seq_length , hidden_size = hidden_states.size()\n        # Each of shape: [batch_size, seq_length, num_heads, head_dim]\n        q = self.q_proj(hidden_states).reshape((batch_size, seq_length, self.num_heads, self.head_dim))\n        k = self.k_proj(hidden_states).reshape((batch_size, seq_length, self.num_heads, self.head_dim))\n        v = self.v_proj(hidden_states).reshape((batch_size, seq_length, self.num_heads, self.head_dim))\n        q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)\n        k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)\n        q = q.permute(0, 2, 1, 3).contiguous()\n        k = k.permute(0, 2, 1, 3).contiguous()\n        v = v.permute(0, 2, 1, 3).contiguous()\n        # q (batch_size, num_heads, seq_length, head_dim)\n        # k (batch_size, num_heads, seq_length, head_dim)\n        # v (batch_size, num_heads, seq_length, head_dim)\n        attn_output = F.scaled_dot_product_attention(q, k, v, None, dropout_p=0.0)\n        attn_output = attn_output.permute(2, 0, 1, 3).contiguous()  # [seq_length, batch_size, num_heads, head_dim]\n        attn_output = attn_output.view(seq_length, batch_size, -1)  # [seq_length, batch_size, embedding_dim]\n        attn_output = self.out_proj(attn_output)\n        attn_output = attn_output.permute(1, 0, 2).contiguous()  # [batch_size, seq_length, embedding_dim]\n        return attn_output, None\n\n\nclass MLCDEncoderLayer(nn.Module):\n    def __init__(self, config: MLCDVisionConfig):\n        super().__init__()\n        self.embed_dim = config.hidden_size\n        self.self_attn = MLCDSdpaAttention(config)\n        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n        self.mlp = MLCDMLP(config)\n        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        rotary_pos_emb: torch.Tensor,\n    ) -> Tuple[torch.FloatTensor]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n                `(config.encoder_attention_heads,)`.\n        \"\"\"\n        residual = hidden_states\n\n        hidden_states = self.layer_norm1(hidden_states)\n        \n        hidden_states = self.self_attn(\n            hidden_states=hidden_states,\n            rotary_pos_emb=rotary_pos_emb,\n        )[0]\n        hidden_states = residual + hidden_states\n\n        residual = hidden_states\n        hidden_states = self.layer_norm2(hidden_states)\n\n        hidden_states = self.mlp(hidden_states)\n\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n        return outputs\n\n\nclass MLCDEncoder(nn.Module):\n    \"\"\"\n    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n    [`MLCDEncoderLayer`].\n\n    Args:\n        config: MLCDVisionConfig\n    \"\"\"\n\n    def __init__(self, config: MLCDVisionConfig):\n        super().__init__()\n        self.config = config\n        self.layers = nn.ModuleList([MLCDEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n        self.gradient_checkpointing = False\n\n    def forward(\n        self,\n        inputs_embeds,\n        rotary_pos_emb,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutput]:\n        r\"\"\"\n        Args:\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n                than the model's internal embedding lookup matrix.\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Causal mask for the text model. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        encoder_states = () if output_hidden_states else None\n\n        hidden_states = inputs_embeds\n        for idx, encoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                encoder_states = encoder_states + (hidden_states,)\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    encoder_layer.__call__,\n                    hidden_states,\n                    rotary_pos_emb\n                )\n            else:\n                layer_outputs = encoder_layer(\n                    hidden_states,\n                    rotary_pos_emb\n                )\n\n            hidden_states = layer_outputs[0]\n\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, encoder_states, None] if v is not None)\n        return BaseModelOutput(\n            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=None,\n        )\n\n\nclass MLCDVisionTransformer(nn.Module):\n    def __init__(self, config: MLCDVisionConfig):\n        super().__init__()\n        self.config = config\n        embed_dim = config.hidden_size\n\n        self.embeddings = MLCDVisionEmbeddings(config)\n        self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n        self.encoder = MLCDEncoder(config)\n        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n\n        self.vision_rotary_embedding = VisionRotaryEmbedding(config.hidden_size // config.num_attention_heads // 2)\n        self.class_pos_emb = nn.Parameter(torch.randn(1, config.hidden_size // config.num_attention_heads // 2))\n\n\n    def rot_pos_emb(self, grid_thw):\n        pos_ids = []\n        for t, h, w in grid_thw:\n            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n            hpos_ids = hpos_ids.reshape(h, 1, w, 1)\n            hpos_ids = hpos_ids.permute(0, 2, 1, 3)\n            hpos_ids = hpos_ids.flatten()\n\n            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n            wpos_ids = wpos_ids.reshape(h, 1, w, 1)\n            wpos_ids = wpos_ids.permute(0, 2, 1, 3)\n            wpos_ids = wpos_ids.flatten()\n            pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n        pos_ids = torch.cat(pos_ids, dim=0)\n        max_grid_size = grid_thw[:, 1:].max()\n        rotary_pos_emb_full = self.vision_rotary_embedding(max_grid_size)\n        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n        return rotary_pos_emb\n\n\n    def forward(\n        self,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        # output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n        r\"\"\"\n        Returns:\n\n        \"\"\"\n\n        twh = (1, pixel_values.size(3) // self.config.patch_size, pixel_values.size(2) // self.config.patch_size)\n        rotary_pos_emb = self.rot_pos_emb(torch.tensor([twh], device=pixel_values.device))\n        rotary_pos_emb = torch.cat([self.class_pos_emb, rotary_pos_emb], dim=0)\n\n        # output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if pixel_values is None:\n            raise ValueError(\"You have to specify pixel_values\")\n\n        hidden_states = self.embeddings(pixel_values)\n        hidden_states = self.pre_layrnorm(hidden_states)\n\n        encoder_outputs = self.encoder(\n            inputs_embeds=hidden_states,\n            rotary_pos_emb=rotary_pos_emb,\n            # output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        last_hidden_state = encoder_outputs[0]\n        pooled_output = last_hidden_state[:, 0, :]\n        pooled_output = self.post_layernorm(pooled_output)\n\n        if not return_dict:\n            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPooling(\n            last_hidden_state=last_hidden_state,\n            pooler_output=pooled_output,\n            hidden_states=encoder_outputs.hidden_states,\n            # attentions=encoder_outputs.attentions,\n        )\n\n\nclass MLCDPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n    config_class = MLCDVisionConfig\n    base_model_prefix = \"mlcd\"\n    supports_gradient_checkpointing = True\n    _supports_sdpa = True\n    # _supports_flash_attn_2 = True\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        factor = self.config.initializer_factor\n        if isinstance(module, MLCDVisionEmbeddings):\n            factor = self.config.initializer_factor\n            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n        elif isinstance(module, MLCDSdpaAttention):\n            factor = self.config.initializer_factor\n            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n            out_proj_std = (module.embed_dim**-0.5) * factor\n            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n        elif isinstance(module, MLCDMLP):\n            factor = self.config.initializer_factor\n            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n            nn.init.normal_(module.fc1.weight, std=fc_std)\n            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n\n\n        if isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n\nclass MLCDVisionModel(MLCDPreTrainedModel):\n    config_class = MLCDVisionConfig\n    main_input_name = \"pixel_values\"\n    _no_split_modules = [\"MLCDEncoderLayer\"]\n\n    def __init__(self, config: MLCDVisionConfig):\n        super().__init__(config)\n        self.vision_model = MLCDVisionTransformer(config)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self) -> nn.Module:\n        return self.vision_model.embeddings.patch_embedding\n\n    def forward(\n        self,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n        r\"\"\"\n        Returns:\n\n        Examples:\n\n        ```python\n        >>> from PIL import Image\n        >>> import requests\n        >>> from transformers import AutoProcessor, MLCDVisionModel\n\n        >>> model = MLCDVisionModel.from_pretrained(\"DeepGlint-AI/mlcd-vit-bigG-patch14\")\n        >>> processor = AutoProcessor.from_pretrained(\"DeepGlint-AI/mlcd-vit-bigG-patch14\")\n\n        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n        >>> image = Image.open(requests.get(url, stream=True).raw)\n\n        >>> inputs = processor(images=image, return_tensors=\"pt\")\n\n        >>> outputs = model(**inputs)\n        >>> last_hidden_state = outputs.last_hidden_state\n        >>> pooled_output = outputs.pooler_output  # pooled CLS states\n        ```\"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        return self.vision_model(\n            pixel_values=pixel_values,\n            # output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_vit.py", "content": "# Based on EVA, BEIT, timm and DeiT code bases\n# https://github.com/baaivision/EVA\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/facebookresearch/deit/\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\n# not tested yet\nimport math\nfrom transformers import CLIPImageProcessor\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import drop_path, to_2tuple, trunc_normal_\nfrom .eva_clip import create_model_and_transforms, get_model_config\nimport torch\nimport torchvision\nimport time\n\nfrom llava.utils import rank0_print\n\n\nclass EvaViTWrapper(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n        self.vision_tower_name = vision_tower\n        self.pretrained = args.vision_tower_pretrained\n        self.args = args\n\n        self.select_layer = args.mm_vision_select_layer\n        if self.select_layer < -1:\n            self.select_layer += 1\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        self.model_config = get_model_config(self.vision_tower_name)\n\n        if not delay_load:\n            rank0_print(f\"Loading vision tower: {vision_tower}\")\n            self.load_model()\n        elif getattr(args, \"unfreeze_mm_vision_tower\", False):\n            # TODO: better detector is needed.\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `unfreeze_mm_vision_tower`: True.\")\n            self.load_model()\n        elif hasattr(args, \"mm_tunable_parts\") and \"mm_vision_tower\" in args.mm_tunable_parts:\n            rank0_print(f\"The checkpoint seems to contain `vision_tower` weights: `mm_tunable_parts` contains `mm_vision_tower`.\")\n            self.load_model()\n\n    def load_model(self):\n        rank0_print(f\"Loading: {self.vision_tower_name}\")\n        rank0_print(f\"Pretrained: {self.pretrained}\")\n        time_start = time.time()\n        model, _, image_processor = create_model_and_transforms(self.vision_tower_name, self.pretrained, force_custom_clip=True, precision=\"fp16\")\n        time_end = time.time()\n        rank0_print(f\"Loaded: {self.vision_tower_name} in {time_end - time_start:.2f}s\")\n        self.device = next(model.parameters()).device\n        self.dtype = next(model.parameters()).dtype\n        if self.device.type != \"meta\":\n            model = model.to(\"cuda\")\n        self.vision_tower = model.visual\n        resize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Resize)][0]\n        normalize_transform = [t for t in image_processor.transforms if isinstance(t, torchvision.transforms.Normalize)][0]\n        self.resize_transform_size = resize_transform.size\n        self.image_processor = CLIPImageProcessor.from_pretrained(\n            \"openai/clip-vit-large-patch14\",\n            crop_size=resize_transform.size,\n            size={\"shortest_edge\": resize_transform.size},\n            image_mean=list(normalize_transform.mean),\n            image_std=list(normalize_transform.std),\n        )\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower.requires_grad_(False)\n        self.is_loaded = True\n\n    def feature_select(self, image_features):\n        select_feature_type = self.select_feature\n\n        # if self.select_feature in [\"slicefour_patch\", \"slicefour_cls_patch\"]:\n        #     select_every_k_layer = len(image_features) // 4\n        #     image_features = torch.cat([image_features[i] for i in range(select_every_k_layer + self.select_layer, len(image_features), select_every_k_layer)], dim=-1)\n        #     select_feature_type = select_feature_type.replace(\"slicefour_\", \"\")\n        # elif self.select_feature in [\"slice_m25811_f6_patch\", \"slice_m25811_f6_cls_patch\"]:\n        #     select_layers = [-1, -4, -7, -10, 6]\n        #     image_features = torch.cat([image_features[i] for i in select_layers], dim=-1)\n        #     select_feature_type = select_feature_type.replace(\"slice_m25811_f6_\", \"\")\n        # else:\n        #     image_features = image_features[self.select_layer]\n\n        if select_feature_type == \"patch\":\n            image_features = image_features[:, 1:]\n        elif select_feature_type == \"cls_patch\":\n            image_features = image_features\n        else:\n            raise ValueError(f\"Unexpected select feature: {select_feature_type}\")\n        return image_features\n\n    def train(self, mode=True):\n        self.training = mode\n\n        if self.is_loaded:\n            self.vision_tower.eval()\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_features = self.vision_tower.forward_features(image.to(self.dtype), return_all_features=True)\n                image_features = self.feature_select(image_features).to(self.dtype)\n                image_features.append(image_features)\n        else:\n            image_features = self.vision_tower.forward_features(images.to(self.dtype), return_all_features=True)\n            image_features = self.feature_select(image_features).to(self.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    @property\n    def hidden_size(self):\n        return self.model_config[\"vision_cfg\"][\"width\"]\n\n    @property\n    def num_patches(self):\n        return (self.model_config[\"vision_cfg\"][\"image_size\"] // self.model_config[\"vision_cfg\"][\"patch_size\"]) ** 2\n\n    @property\n    def num_patches_per_side(self):\n        return self.model_config[\"vision_cfg\"][\"image_size\"] // self.model_config[\"vision_cfg\"][\"patch_size\"]\n\n    @property\n    def config(self):\n        return self.model_config\n\n    @property\n    def image_size(self):\n        return self.model_config[\"vision_cfg\"][\"image_size\"]\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/timm_model.py", "content": "\"\"\" timm model adapter\n\nWraps timm (https://github.com/rwightman/pytorch-image-models) models for use as a vision tower in CLIP model.\n\"\"\"\n\nimport logging\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\ntry:\n    import timm\n    from timm.models.layers import Mlp, to_2tuple\n\n    try:\n        # old timm imports < 0.8.1\n        from timm.models.layers.attention_pool2d import RotAttentionPool2d\n        from timm.models.layers.attention_pool2d import AttentionPool2d as AbsAttentionPool2d\n    except ImportError:\n        # new timm imports >= 0.8.1\n        from timm.layers import RotAttentionPool2d\n        from timm.layers import AttentionPool2d as AbsAttentionPool2d\nexcept ImportError:\n    timm = None\n\nfrom .utils import freeze_batch_norm_2d\n\n\nclass TimmModel(nn.Module):\n    \"\"\"timm model adapter\n    # FIXME this adapter is a work in progress, may change in ways that break weight compat\n    \"\"\"\n\n    def __init__(self, model_name, embed_dim, image_size=224, pool=\"avg\", proj=\"linear\", proj_bias=False, drop=0.0, pretrained=False):\n        super().__init__()\n        if timm is None:\n            raise RuntimeError(\"Please `pip install timm` to use timm models.\")\n\n        self.image_size = to_2tuple(image_size)\n        self.trunk = timm.create_model(model_name, pretrained=pretrained)\n        feat_size = self.trunk.default_cfg.get(\"pool_size\", None)\n        feature_ndim = 1 if not feat_size else 2\n        if pool in (\"abs_attn\", \"rot_attn\"):\n            assert feature_ndim == 2\n            # if attn pooling used, remove both classifier and default pool\n            self.trunk.reset_classifier(0, global_pool=\"\")\n        else:\n            # reset global pool if pool config set, otherwise leave as network default\n            reset_kwargs = dict(global_pool=pool) if pool else {}\n            self.trunk.reset_classifier(0, **reset_kwargs)\n        prev_chs = self.trunk.num_features\n\n        head_layers = OrderedDict()\n        if pool == \"abs_attn\":\n            head_layers[\"pool\"] = AbsAttentionPool2d(prev_chs, feat_size=feat_size, out_features=embed_dim)\n            prev_chs = embed_dim\n        elif pool == \"rot_attn\":\n            head_layers[\"pool\"] = RotAttentionPool2d(prev_chs, out_features=embed_dim)\n            prev_chs = embed_dim\n        else:\n            assert proj, \"projection layer needed if non-attention pooling is used.\"\n\n        # NOTE attention pool ends with a projection layer, so proj should usually be set to '' if such pooling is used\n        if proj == \"linear\":\n            head_layers[\"drop\"] = nn.Dropout(drop)\n            head_layers[\"proj\"] = nn.Linear(prev_chs, embed_dim, bias=proj_bias)\n        elif proj == \"mlp\":\n            head_layers[\"mlp\"] = Mlp(prev_chs, 2 * embed_dim, embed_dim, drop=drop, bias=(True, proj_bias))\n\n        self.head = nn.Sequential(head_layers)\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        \"\"\"lock modules\n        Args:\n            unlocked_groups (int): leave last n layer groups unlocked (default: 0)\n        \"\"\"\n        if not unlocked_groups:\n            # lock full model\n            for param in self.trunk.parameters():\n                param.requires_grad = False\n            if freeze_bn_stats:\n                freeze_batch_norm_2d(self.trunk)\n        else:\n            # NOTE: partial freeze requires latest timm (master) branch and is subject to change\n            try:\n                # FIXME import here until API stable and in an official release\n                from timm.models.helpers import group_parameters, group_modules\n            except ImportError:\n                raise RuntimeError(\"Please install latest timm `pip install git+https://github.com/rwightman/pytorch-image-models`\")\n            matcher = self.trunk.group_matcher()\n            gparams = group_parameters(self.trunk, matcher)\n            max_layer_id = max(gparams.keys())\n            max_layer_id = max_layer_id - unlocked_groups\n            for group_idx in range(max_layer_id + 1):\n                group = gparams[group_idx]\n                for param in group:\n                    self.trunk.get_parameter(param).requires_grad = False\n            if freeze_bn_stats:\n                gmodules = group_modules(self.trunk, matcher, reverse=True)\n                gmodules = {k for k, v in gmodules.items() if v <= max_layer_id}\n                freeze_batch_norm_2d(self.trunk, gmodules)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        try:\n            self.trunk.set_grad_checkpointing(enable)\n        except Exception as e:\n            logging.warning(\"grad checkpointing not supported for this timm image tower, continuing without...\")\n\n    def forward(self, x):\n        x = self.trunk(x)\n        x = self.head(x)\n        return x\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/transformer.py", "content": "import os\nimport logging\nfrom collections import OrderedDict\nimport math\nfrom typing import Callable, Optional, Sequence\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\ntry:\n    from timm.models.layers import trunc_normal_\nexcept:\n    from timm.layers import trunc_normal_\n\nfrom .rope import VisionRotaryEmbedding, VisionRotaryEmbeddingFast\nfrom .utils import to_2tuple\n\nif os.getenv(\"ENV_TYPE\") == \"deepspeed\":\n    try:\n        import deepspeed\n        from deepspeed.runtime.activation_checkpointing.checkpointing import checkpoint\n    except:\n        print(\"Please 'pip install deepspeed'\")\n        deepspeed = None\n        from torch.utils.checkpoint import checkpoint\nelse:\n    from torch.utils.checkpoint import checkpoint\n\ntry:\n    import xformers.ops as xops\nexcept ImportError:\n    xops = None\n    # print(\"Please 'pip install xformers'\")\n\n\nclass LayerNormFp32(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16 (by casting to float32 and back).\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, x: torch.Tensor):\n        output = F.layer_norm(\n            x.float(),\n            self.normalized_shape,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,\n        )\n        return output.type_as(x)\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm (with cast back to input dtype).\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        return x.to(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    # NOTE This is slower than nn.GELU or nn.SiLU and uses more GPU memory\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass LayerScale(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n\n\nclass PatchDropout(nn.Module):\n    \"\"\"\n    https://arxiv.org/abs/2212.00794\n    \"\"\"\n\n    def __init__(self, prob, exclude_first_token=True):\n        super().__init__()\n        assert 0 <= prob < 1.0\n        self.prob = prob\n        self.exclude_first_token = exclude_first_token  # exclude CLS token\n        logging.info(f\"os.getenv('RoPE')={os.getenv('RoPE')}\")\n\n    def forward(self, x):\n        if not self.training or self.prob == 0.0:\n            return x\n\n        if self.exclude_first_token:\n            cls_tokens, x = x[:, :1], x[:, 1:]\n        else:\n            cls_tokens = torch.jit.annotate(torch.Tensor, x[:, :1])\n\n        batch = x.size()[0]\n        num_tokens = x.size()[1]\n\n        batch_indices = torch.arange(batch)\n        batch_indices = batch_indices[..., None]\n\n        keep_prob = 1 - self.prob\n        num_patches_keep = max(1, int(num_tokens * keep_prob))\n\n        rand = torch.randn(batch, num_tokens)\n        patch_indices_keep = rand.topk(num_patches_keep, dim=-1).indices\n\n        x = x[batch_indices, patch_indices_keep]\n\n        if self.exclude_first_token:\n            x = torch.cat((cls_tokens, x), dim=1)\n\n        if self.training and os.getenv(\"RoPE\") == \"1\":\n            return x, patch_indices_keep\n\n        return x\n\n\ndef _in_projection_packed(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    w: torch.Tensor,\n    b: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    https://github.com/pytorch/pytorch/blob/db2a237763eb8693a20788be94f8c192e762baa8/torch/nn/functional.py#L4726\n    \"\"\"\n    E = q.size(-1)\n    if k is v:\n        if q is k:\n            # self-attention\n            return F.linear(q, w, b).chunk(3, dim=-1)\n        else:\n            # encoder-decoder attention\n            w_q, w_kv = w.split([E, E * 2])\n            if b is None:\n                b_q = b_kv = None\n            else:\n                b_q, b_kv = b.split([E, E * 2])\n            return (F.linear(q, w_q, b_q),) + F.linear(k, w_kv, b_kv).chunk(2, dim=-1)\n    else:\n        w_q, w_k, w_v = w.chunk(3)\n        if b is None:\n            b_q = b_k = b_v = None\n        else:\n            b_q, b_k, b_v = b.chunk(3)\n        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=True, scaled_cosine=False, scale_heads=False, logit_scale_max=math.log(1.0 / 0.01), attn_drop=0.0, proj_drop=0.0, xattn=False, rope=False):\n        super().__init__()\n        self.scaled_cosine = scaled_cosine\n        self.scale_heads = scale_heads\n        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim**-0.5\n        self.logit_scale_max = logit_scale_max\n\n        # keeping in_proj in this form (instead of nn.Linear) to match weight scheme of original\n        self.in_proj_weight = nn.Parameter(torch.randn((dim * 3, dim)) * self.scale)\n        if qkv_bias:\n            self.in_proj_bias = nn.Parameter(torch.zeros(dim * 3))\n        else:\n            self.in_proj_bias = None\n\n        if self.scaled_cosine:\n            self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n        else:\n            self.logit_scale = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        if self.scale_heads:\n            self.head_scale = nn.Parameter(torch.ones((num_heads, 1, 1)))\n        else:\n            self.head_scale = None\n        self.out_proj = nn.Linear(dim, dim)\n        self.out_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n        self.rope = rope\n\n    def forward(self, x, attn_mask: Optional[torch.Tensor] = None):\n        L, N, C = x.shape\n        q, k, v = F.linear(x, self.in_proj_weight, self.in_proj_bias).chunk(3, dim=-1)\n        if self.xattn:\n            q = q.contiguous().view(L, N, self.num_heads, -1).transpose(0, 1)\n            k = k.contiguous().view(L, N, self.num_heads, -1).transpose(0, 1)\n            v = v.contiguous().view(L, N, self.num_heads, -1).transpose(0, 1)\n\n            x = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                p=self.xattn_drop,\n                scale=self.scale if self.logit_scale is None else None,\n                attn_bias=xops.LowerTriangularMask() if attn_mask is not None else None,\n            )\n        else:\n            q = q.contiguous().view(L, N * self.num_heads, -1).transpose(0, 1)\n            k = k.contiguous().view(L, N * self.num_heads, -1).transpose(0, 1)\n            v = v.contiguous().view(L, N * self.num_heads, -1).transpose(0, 1)\n\n            if self.logit_scale is not None:\n                attn = torch.bmm(F.normalize(q, dim=-1), F.normalize(k, dim=-1).transpose(-1, -2))\n                logit_scale = torch.clamp(self.logit_scale, max=self.logit_scale_max).exp()\n                attn = attn.view(N, self.num_heads, L, L) * logit_scale\n                attn = attn.view(-1, L, L)\n            else:\n                q = q * self.scale\n                attn = torch.bmm(q, k.transpose(-1, -2))\n\n            if attn_mask is not None:\n                if attn_mask.dtype == torch.bool:\n                    new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n                    new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n                    attn_mask = new_attn_mask\n                attn += attn_mask\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = torch.bmm(attn, v)\n\n        if self.head_scale is not None:\n            x = x.view(N, self.num_heads, L, C) * self.head_scale\n            x = x.view(-1, L, C)\n        x = x.transpose(0, 1).reshape(L, N, C)\n        x = self.out_proj(x)\n        x = self.out_drop(x)\n        return x\n\n\nclass CustomAttention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=True, scaled_cosine=True, scale_heads=False, logit_scale_max=math.log(1.0 / 0.01), attn_drop=0.0, proj_drop=0.0, xattn=False):\n        super().__init__()\n        self.scaled_cosine = scaled_cosine\n        self.scale_heads = scale_heads\n        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim**-0.5\n        self.logit_scale_max = logit_scale_max\n\n        # keeping in_proj in this form (instead of nn.Linear) to match weight scheme of original\n        self.in_proj_weight = nn.Parameter(torch.randn((dim * 3, dim)) * self.scale)\n        if qkv_bias:\n            self.in_proj_bias = nn.Parameter(torch.zeros(dim * 3))\n        else:\n            self.in_proj_bias = None\n\n        if self.scaled_cosine:\n            self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))))\n        else:\n            self.logit_scale = None\n        self.attn_drop = nn.Dropout(attn_drop)\n        if self.scale_heads:\n            self.head_scale = nn.Parameter(torch.ones((num_heads, 1, 1)))\n        else:\n            self.head_scale = None\n        self.out_proj = nn.Linear(dim, dim)\n        self.out_drop = nn.Dropout(proj_drop)\n        self.xattn = xattn\n        self.xattn_drop = attn_drop\n\n    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        q, k, v = _in_projection_packed(query, key, value, self.in_proj_weight, self.in_proj_bias)\n        N_q, B_q, C_q = q.shape\n        N_k, B_k, C_k = k.shape\n        N_v, B_v, C_v = v.shape\n        if self.xattn:\n            # B, N, C -> B, N, num_heads, C\n            q = q.permute(1, 0, 2).reshape(B_q, N_q, self.num_heads, -1)\n            k = k.permute(1, 0, 2).reshape(B_k, N_k, self.num_heads, -1)\n            v = v.permute(1, 0, 2).reshape(B_v, N_v, self.num_heads, -1)\n\n            x = xops.memory_efficient_attention(q, k, v, p=self.xattn_drop, scale=self.scale if self.logit_scale is None else None, attn_bias=xops.LowerTriangularMask() if attn_mask is not None else None)\n        else:\n            # B*H, L, C\n            q = q.contiguous().view(N_q, B_q * self.num_heads, -1).transpose(0, 1)\n            k = k.contiguous().view(N_k, B_k * self.num_heads, -1).transpose(0, 1)\n            v = v.contiguous().view(N_v, B_v * self.num_heads, -1).transpose(0, 1)\n\n            if self.logit_scale is not None:\n                # B*H, N_q, N_k\n                attn = torch.bmm(F.normalize(q, dim=-1), F.normalize(k, dim=-1).transpose(-1, -2))\n                logit_scale = torch.clamp(self.logit_scale, max=self.logit_scale_max).exp()\n                attn = attn.view(B_q, self.num_heads, N_q, N_k) * logit_scale\n                attn = attn.view(-1, N_q, N_k)\n            else:\n                q = q * self.scale\n                attn = torch.bmm(q, k.transpose(-1, -2))\n\n            if attn_mask is not None:\n                if attn_mask.dtype == torch.bool:\n                    new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n                    new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n                    attn_mask = new_attn_mask\n                attn += attn_mask\n\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n\n            x = torch.bmm(attn, v)\n\n        if self.head_scale is not None:\n            x = x.view(B_q, self.num_heads, N_q, C_q) * self.head_scale\n            x = x.view(-1, N_q, C_q)\n        x = x.transpose(0, 1).reshape(N_q, B_q, C_q)\n        x = self.out_proj(x)\n        x = self.out_drop(x)\n        return x\n\n\nclass CustomResidualAttentionBlock(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        n_head: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        scale_cosine_attn: bool = False,\n        scale_heads: bool = False,\n        scale_attn: bool = False,\n        scale_fc: bool = False,\n        cross_attn: bool = False,\n        xattn: bool = False,\n    ):\n        super().__init__()\n\n        self.ln_1 = norm_layer(d_model)\n        self.ln_1_k = norm_layer(d_model) if cross_attn else self.ln_1\n        self.ln_1_v = norm_layer(d_model) if cross_attn else self.ln_1\n        self.attn = CustomAttention(d_model, n_head, qkv_bias=True, attn_drop=0.0, proj_drop=0.0, scaled_cosine=scale_cosine_attn, scale_heads=scale_heads, xattn=xattn)\n\n        self.ln_attn = norm_layer(d_model) if scale_attn else nn.Identity()\n        self.ls_1 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n\n        self.ln_2 = norm_layer(d_model)\n        mlp_width = int(d_model * mlp_ratio)\n        self.mlp = nn.Sequential(OrderedDict([(\"c_fc\", nn.Linear(d_model, mlp_width)), (\"ln\", norm_layer(mlp_width) if scale_fc else nn.Identity()), (\"gelu\", act_layer()), (\"c_proj\", nn.Linear(mlp_width, d_model))]))\n\n        self.ls_2 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n\n    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        q = q + self.ls_1(self.ln_attn(self.attn(self.ln_1(q), self.ln_1_k(k), self.ln_1_v(v), attn_mask=attn_mask)))\n        q = q + self.ls_2(self.mlp(self.ln_2(q)))\n        return q\n\n\nclass CustomTransformer(nn.Module):\n    def __init__(\n        self,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        scale_cosine_attn: bool = True,\n        scale_heads: bool = False,\n        scale_attn: bool = False,\n        scale_fc: bool = False,\n        cross_attn: bool = False,\n        xattn: bool = False,\n    ):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.grad_checkpointing = False\n        self.xattn = xattn\n\n        self.resblocks = nn.ModuleList(\n            [\n                CustomResidualAttentionBlock(\n                    width,\n                    heads,\n                    mlp_ratio,\n                    ls_init_value=ls_init_value,\n                    act_layer=act_layer,\n                    norm_layer=norm_layer,\n                    scale_cosine_attn=scale_cosine_attn,\n                    scale_heads=scale_heads,\n                    scale_attn=scale_attn,\n                    scale_fc=scale_fc,\n                    cross_attn=cross_attn,\n                    xattn=xattn,\n                )\n                for _ in range(layers)\n            ]\n        )\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.resblocks[0].mlp.c_fc.weight.dtype\n\n    def forward(self, q: torch.Tensor, k: torch.Tensor = None, v: torch.Tensor = None, attn_mask: Optional[torch.Tensor] = None):\n        if k is None and v is None:\n            k = v = q\n        for r in self.resblocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                q = checkpoint(r, q, k, v, attn_mask)\n            else:\n                q = r(q, k, v, attn_mask=attn_mask)\n        return q\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(\n        self,\n        d_model: int,\n        n_head: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n    ):\n        super().__init__()\n\n        self.ln_1 = norm_layer(d_model)\n        if xattn:\n            self.attn = Attention(d_model, n_head, xattn=True)\n        else:\n            self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ls_1 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n\n        self.ln_2 = norm_layer(d_model)\n        mlp_width = int(d_model * mlp_ratio)\n        self.mlp = nn.Sequential(OrderedDict([(\"c_fc\", nn.Linear(d_model, mlp_width)), (\"gelu\", act_layer()), (\"c_proj\", nn.Linear(mlp_width, d_model))]))\n\n        self.ls_2 = LayerScale(d_model, ls_init_value) if ls_init_value is not None else nn.Identity()\n        self.xattn = xattn\n\n    def attention(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        attn_mask = attn_mask.to(x.dtype) if attn_mask is not None else None\n        if self.xattn:\n            return self.attn(x, attn_mask=attn_mask)\n        return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\n\n    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        x = x + self.ls_1(self.attention(self.ln_1(x), attn_mask=attn_mask))\n        x = x + self.ls_2(self.mlp(self.ln_2(x)))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float = 4.0,\n        ls_init_value: float = None,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n    ):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.grad_checkpointing = False\n\n        self.resblocks = nn.ModuleList([ResidualAttentionBlock(width, heads, mlp_ratio, ls_init_value=ls_init_value, act_layer=act_layer, norm_layer=norm_layer, xattn=xattn) for _ in range(layers)])\n\n    def get_cast_dtype(self) -> torch.dtype:\n        return self.resblocks[0].mlp.c_fc.weight.dtype\n\n    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n        for r in self.resblocks:\n            if self.grad_checkpointing and not torch.jit.is_scripting():\n                x = checkpoint(r, x, attn_mask)\n            else:\n                x = r(x, attn_mask=attn_mask)\n        return x\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(\n        self,\n        image_size: int,\n        patch_size: int,\n        width: int,\n        layers: int,\n        heads: int,\n        mlp_ratio: float,\n        ls_init_value: float = None,\n        patch_dropout: float = 0.0,\n        global_average_pool: bool = False,\n        output_dim: int = 512,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n    ):\n        super().__init__()\n        self.image_size = to_2tuple(image_size)\n        self.patch_size = to_2tuple(patch_size)\n        self.grid_size = (self.image_size[0] // self.patch_size[0], self.image_size[1] // self.patch_size[1])\n        self.output_dim = output_dim\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n\n        scale = width**-0.5\n        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n        self.positional_embedding = nn.Parameter(scale * torch.randn(self.grid_size[0] * self.grid_size[1] + 1, width))\n\n        # setting a patch_dropout of 0. would mean it is disabled and this function would be the identity fn\n        self.patch_dropout = PatchDropout(patch_dropout) if patch_dropout > 0.0 else nn.Identity()\n        self.ln_pre = norm_layer(width)\n\n        self.transformer = Transformer(width, layers, heads, mlp_ratio, ls_init_value=ls_init_value, act_layer=act_layer, norm_layer=norm_layer, xattn=xattn)\n\n        self.global_average_pool = global_average_pool\n        self.ln_post = norm_layer(width)\n        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n\n    def lock(self, unlocked_groups=0, freeze_bn_stats=False):\n        for param in self.parameters():\n            param.requires_grad = False\n\n        if unlocked_groups != 0:\n            groups = [\n                [\n                    self.conv1,\n                    self.class_embedding,\n                    self.positional_embedding,\n                    self.ln_pre,\n                ],\n                *self.transformer.resblocks[:-1],\n                [\n                    self.transformer.resblocks[-1],\n                    self.ln_post,\n                ],\n                self.proj,\n            ]\n\n            def _unlock(x):\n                if isinstance(x, Sequence):\n                    for g in x:\n                        _unlock(g)\n                else:\n                    if isinstance(x, torch.nn.Parameter):\n                        x.requires_grad = True\n                    else:\n                        for p in x.parameters():\n                            p.requires_grad = True\n\n            _unlock(groups[-unlocked_groups:])\n\n    def get_num_layers(self):\n        return self.transformer.layers\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {\"positional_embedding\", \"class_embedding\"}\n\n    def forward(self, x: torch.Tensor, return_all_features: bool = False):\n        x = self.conv1(x)  # shape = [*, width, grid, grid]\n        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n        x = x + self.positional_embedding.to(x.dtype)\n\n        # a patch_dropout of 0. would mean it is disabled and this function would do nothing but return what was passed in\n        x = self.patch_dropout(x)\n        x = self.ln_pre(x)\n\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n\n        if not return_all_features:\n            if self.global_average_pool:\n                x = x.mean(dim=1)  # x = x[:,1:,:].mean(dim=1)\n            else:\n                x = x[:, 0]\n\n            x = self.ln_post(x)\n\n            if self.proj is not None:\n                x = x @ self.proj\n\n        return x\n\n\nclass TextTransformer(nn.Module):\n    def __init__(\n        self,\n        context_length: int = 77,\n        vocab_size: int = 49408,\n        width: int = 512,\n        heads: int = 8,\n        layers: int = 12,\n        ls_init_value: float = None,\n        output_dim: int = 512,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = LayerNorm,\n        xattn: bool = False,\n        attn_mask: bool = True,\n    ):\n        super().__init__()\n        self.context_length = context_length\n        self.vocab_size = vocab_size\n        self.width = width\n        self.output_dim = output_dim\n\n        self.token_embedding = nn.Embedding(vocab_size, width)\n        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, width))\n        self.transformer = Transformer(width=width, layers=layers, heads=heads, ls_init_value=ls_init_value, act_layer=act_layer, norm_layer=norm_layer, xattn=xattn)\n\n        self.xattn = xattn\n        self.ln_final = norm_layer(width)\n        self.text_projection = nn.Parameter(torch.empty(width, output_dim))\n\n        if attn_mask:\n            self.register_buffer(\"attn_mask\", self.build_attention_mask(), persistent=False)\n        else:\n            self.attn_mask = None\n\n        self.init_parameters()\n\n    def init_parameters(self):\n        nn.init.normal_(self.token_embedding.weight, std=0.02)\n        nn.init.normal_(self.positional_embedding, std=0.01)\n\n        proj_std = (self.transformer.width**-0.5) * ((2 * self.transformer.layers) ** -0.5)\n        attn_std = self.transformer.width**-0.5\n        fc_std = (2 * self.transformer.width) ** -0.5\n        for block in self.transformer.resblocks:\n            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n\n        if self.text_projection is not None:\n            nn.init.normal_(self.text_projection, std=self.transformer.width**-0.5)\n\n    @torch.jit.ignore\n    def set_grad_checkpointing(self, enable=True):\n        self.transformer.grad_checkpointing = enable\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        # return {'positional_embedding', 'token_embedding'}\n        return {\"positional_embedding\"}\n\n    def get_num_layers(self):\n        return self.transformer.layers\n\n    def build_attention_mask(self):\n        # lazily create causal attention mask, with full attention between the vision tokens\n        # pytorch uses additive attention mask; fill with -inf\n        mask = torch.empty(self.context_length, self.context_length)\n        mask.fill_(float(\"-inf\"))\n        mask.triu_(1)  # zero out the lower diagonal\n        return mask\n\n    def forward(self, text, return_all_features: bool = False):\n        cast_dtype = self.transformer.get_cast_dtype()\n        x = self.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n\n        x = x + self.positional_embedding.to(cast_dtype)\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x, attn_mask=self.attn_mask)\n        # x = self.transformer(x) # no attention mask is applied\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        x = self.ln_final(x)\n\n        if not return_all_features:\n            # x.shape = [batch_size, n_ctx, transformer.width]\n            # take features from the eot embedding (eot_token is the highest number in each sequence)\n            x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n        return x\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/hf_vision.py", "content": "import torch\nimport torch.nn as nn\n\nfrom transformers import AutoModel, AutoImageProcessor, AutoConfig, CLIPImageProcessor\nfrom llava.utils import rank0_print\n\n\nclass HFVisionTower(nn.Module):\n    def __init__(self, vision_tower, args, delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.vision_tower_name = vision_tower.replace(\"hf:\", \"\", 1)\n        self.select_layer = args.mm_vision_select_layer\n        self.select_feature = getattr(args, \"mm_vision_select_feature\", \"patch\")\n\n        if not delay_load:\n            self.load_model()\n        else:\n            self.cfg_only = AutoConfig.from_pretrained(self.vision_tower_name)\n\n    def load_model(self):\n        try:\n            self.image_processor = AutoImageProcessor.from_pretrained(self.vision_tower_name)\n        except Exception as e:\n            if \"448\" in self.vision_tower_name:\n                image_size = 448\n                # use image processor with conig\n                self.image_processor = CLIPImageProcessor(size={\"shortest_edge\": image_size}, do_center_crop=True, crop_size=image_size)\n            else:\n                self.image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n        rank0_print(f\"Loaded image processor: {self.image_processor}\")\n        self.vision_tower = AutoModel.from_pretrained(self.vision_tower_name, torch_dtype=torch.bfloat16, trust_remote_code=True).to(\"cuda\")\n        self.device = self.vision_tower.device\n        self.dtype = self.vision_tower.dtype\n        self.config = self.vision_tower.config\n\n        if hasattr(self.vision_tower, \"vision_model\"):\n            self.vision_tower = self.vision_tower.vision_model\n        self.vision_tower.requires_grad_(False)\n        # self.vision_tower.eval()\n        self.is_loaded = True\n\n    def feature_select(self, image_forward_outs):\n        select_feature_type = self.select_feature\n\n        if self.select_feature in [\"slicefour_patch\", \"slicefour_cls_patch\"]:\n            select_every_k_layer = len(image_forward_outs.hidden_states) // 4\n            image_features = torch.cat([image_forward_outs.hidden_states[i] for i in range(select_every_k_layer + self.select_layer, len(image_forward_outs.hidden_states), select_every_k_layer)], dim=-1)\n            select_feature_type = select_feature_type.replace(\"slicefour_\", \"\")\n        else:\n            image_features = image_forward_outs.hidden_states[self.select_layer]\n\n        if select_feature_type == \"patch\":\n            image_features = image_features[:, 1:]\n        elif select_feature_type == \"cls_patch\":\n            image_features = image_features\n        else:\n            raise ValueError(f\"Unexpected select feature: {select_feature_type}\")\n        return image_features\n\n    def forward(self, images):\n        if type(images) is list:\n            image_features = []\n            for image in images:\n                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n                image_features.append(image_feature)\n        else:\n            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n\n        return image_features\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n\n    # @property\n    # def dtype(self):\n    #     return self.vision_tower.dtype\n\n    # @property\n    # def device(self):\n    #     return self.vision_tower.device\n\n    @property\n    def hidden_size(self):\n        try:\n            _hidden_size = self.config.hidden_size\n        except:\n            _hidden_size = self.config.vision_config.hidden_size\n        if \"slicefour\" in self.select_feature:\n            _hidden_size *= 4\n        return _hidden_size\n\n    @property\n    def num_patches(self):\n        _num_patches = (self.config.image_size // self.config.patch_size) ** 2\n        if \"cls_patch\" in self.select_feature:\n            _num_patches += 1\n        return _num_patches\n\n    @property\n    def num_patches_per_side(self):\n        return self.config.image_size // self.config.patch_size\n\n    @property\n    def image_size(self):\n        return self.config.image_size\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/dev_eva_clip/eva_clip/tokenizer.py", "content": "\"\"\" CLIP tokenizer\n\nCopied from https://github.com/openai/CLIP. Originally MIT License, Copyright (c) 2021 OpenAI.\n\"\"\"\n\nimport gzip\nimport html\nimport os\nfrom functools import lru_cache\nfrom typing import Union, List\n\nimport ftfy\nimport regex as re\nimport torch\n\n# https://stackoverflow.com/q/62691279\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n@lru_cache()\ndef default_bpe():\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"bpe_simple_vocab_16e6.txt.gz\")\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"¡\"), ord(\"¬\") + 1)) + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n\n\ndef whitespace_clean(text):\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = text.strip()\n    return text\n\n\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = default_bpe(), special_tokens=None):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split(\"\\n\")\n        merges = merges[1 : 49152 - 256 - 2 + 1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v + \"</w>\" for v in vocab]\n        for merge in merges:\n            vocab.append(\"\".join(merge))\n        if not special_tokens:\n            special_tokens = [\"<start_of_text>\", \"<end_of_text>\"]\n        else:\n            special_tokens = [\"<start_of_text>\", \"<end_of_text>\"] + special_tokens\n        vocab.extend(special_tokens)\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {t: t for t in special_tokens}\n        special = \"|\".join(special_tokens)\n        self.pat = re.compile(special + r\"\"\"|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n\n        self.vocab_size = len(self.encoder)\n        self.all_special_ids = [self.encoder[t] for t in special_tokens]\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + (token[-1] + \"</w>\",)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token + \"</w>\"\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \" \".join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        text = whitespace_clean(basic_clean(text)).lower()\n        for token in re.findall(self.pat, text):\n            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \"))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \"\".join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=\"replace\").replace(\"</w>\", \" \")\n        return text\n\n\n_tokenizer = SimpleTokenizer()\n\n\ndef tokenize(texts: Union[str, List[str]], context_length: int = 77) -> torch.LongTensor:\n    \"\"\"\n    Returns the tokenized representation of given input string(s)\n\n    Parameters\n    ----------\n    texts : Union[str, List[str]]\n        An input string or a list of input strings to tokenize\n    context_length : int\n        The context length to use; all CLIP models use 77 as the context length\n\n    Returns\n    -------\n    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length]\n    \"\"\"\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<start_of_text>\"]\n    eot_token = _tokenizer.encoder[\"<end_of_text>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        if len(tokens) > context_length:\n            tokens = tokens[:context_length]  # Truncate\n            tokens[-1] = eot_token\n        result[i, : len(tokens)] = torch.tensor(tokens)\n\n    return result\n\n\nclass HFTokenizer:\n    \"HuggingFace tokenizer wrapper\"\n\n    def __init__(self, tokenizer_name: str):\n        from transformers import AutoTokenizer\n\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\n    def __call__(self, texts: Union[str, List[str]], context_length: int = 77) -> torch.Tensor:\n        # same cleaning as for default tokenizer, except lowercasing\n        # adding lower (for case-sensitive tokenizers) will make it more robust but less sensitive to nuance\n        if isinstance(texts, str):\n            texts = [texts]\n        texts = [whitespace_clean(basic_clean(text)) for text in texts]\n        input_ids = self.tokenizer(texts, return_tensors=\"pt\", max_length=context_length, padding=\"max_length\", truncation=True).input_ids\n        return input_ids\n"}
{"type": "source_file", "path": "llava/model/multimodal_encoder/imagebind.py", "content": "import torch\nimport torch.nn as nn\n\nfrom transformers import CLIPImageProcessor\n\ntry:\n    from imagebind.models import imagebind_model\n    from imagebind.models.imagebind_model import ModalityType\n    from imagebind.data import load_and_transform_audio_data\nexcept ImportError:\n    pass\n\n\nclass ImageBindWrapper(nn.Module):\n    def __init__(self, vision_tower, select_layer, select_feature=\"patch\", delay_load=False):\n        super().__init__()\n\n        self.is_loaded = False\n\n        self.vision_tower_name = vision_tower\n        self.select_layer = select_layer\n        self.select_feature = select_feature\n\n        if not delay_load:\n            self.load_model()\n\n    def load_model(self):\n        self.image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n        self.vision_tower = imagebind_model.imagebind_huge(pretrained=True)\n        for p in self.vision_tower.parameters():\n            p.requires_grad = False\n        self.vision_tower.eval()\n        self.is_loaded = True\n\n    def train(self, mode=True):\n        self.training = mode\n\n        if self.is_loaded:\n            self.vision_tower.eval()\n\n    @torch.no_grad()\n    def forward(self, x):\n        if type(x) == dict:\n            if x[\"audios\"] is not None:\n                inputs = {ModalityType.AUDIO: load_and_transform_audio_data(x[\"audios\"], device=self.device).half()}\n                embeddings = self.vision_tower(inputs)\n                audio_embedding = embeddings[ModalityType.AUDIO]\n                return audio_embedding.unsqueeze(1)\n        else:\n            inputs = {ModalityType.VISION: x.to(dtype=self.dtype)}\n            embeddings = self.vision_tower(inputs)\n            vision_embedding = embeddings[ModalityType.VISION]\n            if vision_embedding.ndim == 2:\n                return vision_embedding.unsqueeze(1)\n            if vision_embedding.shape[1] == 257:\n                return vision_embedding[:, 1:]\n            raise ValueError(f\"Unexpected shape: {vision_embedding.shape}\")\n\n    @property\n    def dummy_feature(self):\n        return torch.zeros(1, 1024, device=self.device, dtype=self.dtype)\n\n    @property\n    def dtype(self):\n        return self.vision_tower.modality_preprocessors.vision.cls_token.dtype\n\n    @property\n    def device(self):\n        return self.vision_tower.modality_preprocessors.vision.cls_token.device\n\n    @property\n    def hidden_size(self):\n        return 1024\n"}
