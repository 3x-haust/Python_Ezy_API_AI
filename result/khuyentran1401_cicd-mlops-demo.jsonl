{"repo_info": {"repo_name": "cicd-mlops-demo", "repo_owner": "khuyentran1401", "repo_url": "https://github.com/khuyentran1401/cicd-mlops-demo"}}
{"type": "test_file", "path": "tests/conftest.py", "content": "import dvc.api\nimport pandas as pd\nimport pytest\nfrom deepchecks.tabular import Dataset\n\nfrom src.evaluate import load_model\nfrom src.helper import load_data\n\n\n@pytest.fixture\ndef train_data():\n    params = dvc.api.params_show()\n    X_train = load_data(f\"{params['data']['intermediate']}/X_train.pkl\")\n    y_train = load_data(f\"{params['data']['intermediate']}/y_train.pkl\")\n    df = pd.concat([X_train, y_train], axis=1)\n    return Dataset(df, label=params[\"process\"][\"feature\"], cat_features=[])\n\n\n@pytest.fixture\ndef test_data():\n    params = dvc.api.params_show()\n    X_test = load_data(f\"{params['data']['intermediate']}/X_test.pkl\")\n    y_test = load_data(f\"{params['data']['intermediate']}/y_test.pkl\")\n    df = pd.concat([X_test, y_test], axis=1)\n    return Dataset(df, label=params[\"process\"][\"feature\"], cat_features=[])\n\n\n@pytest.fixture\ndef model():\n    params = dvc.api.params_show()\n    return load_model(params[\"model\"])\n"}
{"type": "test_file", "path": "tests/__init__.py", "content": "# Avoid ModuleNotFoundError\n\nimport sys\n\nsys.path.append(\"./src\")\n"}
{"type": "test_file", "path": "tests/test_model.py", "content": "from deepchecks.tabular.checks import (ModelInferenceTime, PredictionDrift,\n                                       SimpleModelComparison,\n                                       TrainTestPerformance)\n\n\ndef test_model_inference_time(model, test_data):\n    \"\"\"Check measures the model's average inference time per sample. Fast runtime\n    can significantly impact the user experience or the system load.\"\"\"\n    check = ModelInferenceTime()\n    result = check.run(test_data, model)\n    assert result.passed_conditions()\n\n\ndef test_prediction_drift(train_data, test_data, model):\n    \"\"\"Detect if there is a drift in the predictions. A drift indicates that a changed\n    has happened in the data that actually affects model predictions.\"\"\"\n    check = PredictionDrift()\n    result = check.run(train_data, test_data, model)\n    assert result.passed_conditions()\n\n\ndef test_simple_model_comparison(model, train_data, test_data):\n    \"\"\"The simple model is used as a baseline for the minimum model performance.\n    If a user's model fails to surpass the simple model's performance, it may\n    imply potential issues with the model.\n    \"\"\"\n    check = SimpleModelComparison()\n    result = check.run(train_data, test_data, model)\n    assert result.passed_conditions()\n\n\ndef test_train_test_performance(model, train_data, test_data):\n    \"\"\"Compare modelâ€™s performance between the train and test datasets based on multiple scorers.\"\"\"\n    check = TrainTestPerformance(\n        scorers=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\", \"r2\"]\n    )\n    result = check.run(train_data, test_data, model)\n    assert result.passed_conditions()\n"}
{"type": "test_file", "path": "tests/test_process_data.py", "content": "import pandas as pd\nfrom pandas.testing import assert_frame_equal, assert_series_equal\n\nfrom src.process_data import get_X_y\n\n\ndef test_get_X_y():\n    \"\"\"Test get_X_y function\"\"\"\n    df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [0, 1, 0]})\n    feature = \"c\"\n    X, y = get_X_y(df, feature)\n    expected_X = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n    expected_y = pd.Series([0, 1, 0])\n    assert_frame_equal(X, expected_X)\n    assert_series_equal(y, expected_y, check_names=False)\n"}
{"type": "test_file", "path": "tests/test_train.py", "content": "import numpy as np\n\nfrom src.train import create_pipeline\n\n\ndef test_pipeline():\n    \"\"\"Test the pipeline's behavior\"\"\"\n    # Define the pipeline\n    pipeline = create_pipeline()\n\n    # Create some test data\n    X_train = np.array([[1, 2], [3, 4], [5, 6]])\n    y_train = np.array([1, 2, 3])\n    X_test = np.array([[7, 8], [9, 10]])\n\n    # Fit the pipeline on the training data\n    pipeline.fit(X_train, y_train)\n\n    # Test the pipeline's behavior on the test data\n    y_pred = pipeline.predict(X_test)\n\n    # Check that the pipeline's output is of the correct shape\n    assert y_pred.shape == (2,)\n\n    # Check that the pipeline's output is not all zeros\n    assert np.any(y_pred)\n\n    # Check that the pipeline's output is within a reasonable range\n    assert np.all(y_pred >= 0) and np.all(y_pred <= 3)\n"}
{"type": "source_file", "path": "src/__init__.py", "content": ""}
{"type": "source_file", "path": "src/helper.py", "content": "from pathlib import Path\n\nimport pandas as pd\n\n\ndef load_data(path: str, csv_delimeter=\",\"):\n    \"\"\"Load data from path\"\"\"\n    file_path = Path(path)\n    if file_path.suffix == \".csv\":\n        df = pd.read_csv(file_path, csv_delimeter)\n    elif file_path.suffix == \".pkl\":\n        df = pd.read_pickle(file_path)\n    else:\n        raise ValueError(\n            \"File format not supported. Please use a CSV or PKL file.\"\n        )\n\n    return df\n\n\ndef save_data(df: pd.DataFrame, path: str, name: str):\n    \"\"\"Save data to path\"\"\"\n    path = f\"{path}/{name}.pkl\"\n    Path(path).parent.mkdir(exist_ok=True)\n    df.to_pickle(path)\n"}
{"type": "source_file", "path": "src/process_data.py", "content": "import warnings\nfrom typing import Tuple\n\nimport dvc.api\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom helper import load_data, save_data\n\n# Ignore all future warnings\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\n\ndef get_X_y(\n    data: pd.DataFrame, feature: str\n) -> Tuple[pd.DataFrame, pd.Series]:\n    \"\"\"Split data into X and y\"\"\"\n    X = data.drop(columns=feature)\n    y = data[feature]\n    return X, y\n\n\ndef split_train_test(X: pd.DataFrame, y: pd.Series, test_size: float) -> dict:\n    \"\"\"Split data into train and test sets\"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=42\n    )\n    return {\n        \"X_train\": X_train,\n        \"X_test\": X_test,\n        \"y_train\": y_train,\n        \"y_test\": y_test,\n    }\n\n\ndef process_data():\n    params = dvc.api.params_show()\n    df = load_data(params[\"data\"][\"raw\"], csv_delimeter=\";\")\n    X, y = get_X_y(df, params[\"process\"][\"feature\"])\n    splitted_datasets = split_train_test(X, y, params[\"process\"][\"test_size\"])\n    for name, data in splitted_datasets.items():\n        save_data(data, params[\"data\"][\"intermediate\"], name)\n\n\nif __name__ == \"__main__\":\n    process_data()\n"}
{"type": "source_file", "path": "src/train.py", "content": "from pathlib import Path\n\nimport dvc.api\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom dvclive import Live\nfrom helper import load_data\nfrom mlem.api import save\n\ndef create_pipeline() -> Pipeline:\n    return Pipeline([(\"scaler\", StandardScaler()), (\"svm\", SVC())])\n\n\ndef train_model(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    pipeline: Pipeline,\n    hyperparameters: dict,\n    grid_params: dict,\n) -> GridSearchCV:\n    \"\"\"Train model using GridSearchCV\"\"\"\n    grid_search = GridSearchCV(\n        pipeline, dict(hyperparameters), **grid_params\n    )\n    grid_search.fit(X_train, y_train)\n    return grid_search\n\n\ndef save_model(model, path: str, X_train: pd.DataFrame):\n    \"\"\"Save model to path\"\"\"\n    Path(path).parent.mkdir(exist_ok=True)\n    save(model, path, sample_data=X_train)\n\n\ndef train() -> None:\n    \"\"\"Train model and save it\"\"\"\n    params = dvc.api.params_show()\n    with Live(save_dvc_exp=True) as live:\n        X_train = load_data(f\"{params['data']['intermediate']}/X_train.pkl\")\n        y_train = load_data(f\"{params['data']['intermediate']}/y_train.pkl\")\n        pipeline = create_pipeline()\n        grid_search = train_model(\n            X_train,\n            y_train,\n            pipeline,\n            params[\"train\"][\"hyperparameters\"],\n            params[\"train\"][\"grid_search\"],\n        )\n        live.log_params({\"Best hyperparameters\": grid_search.best_params_})\n        save_model(grid_search.best_estimator_, params[\"model\"], X_train)\n\n\nif __name__ == \"__main__\":\n    train()\n"}
{"type": "source_file", "path": "src/evaluate.py", "content": "import dvc.api\n\nfrom helper import load_data\nfrom sklearn.metrics import accuracy_score\nfrom dvclive import Live\nfrom mlem.api import load\n\n\ndef load_model(path: str):\n    \"\"\"Load model from path\"\"\"\n    return load(path)\n\n\ndef evaluate() -> None:\n    \"\"\"Evaluate model and log metrics\"\"\"\n    params = dvc.api.params_show()\n    with Live(save_dvc_exp=True, resume=True) as live:\n        X_test = load_data(f\"{params['data']['intermediate']}/X_test.pkl\")\n        y_test = load_data(f\"{params['data']['intermediate']}/y_test.pkl\")\n        model = load_model(params[\"model\"])\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        print(f\"The model's accuracy is {accuracy}\")\n        live.log_metric(\"accuracy\", accuracy)\n\nif __name__ == \"__main__\":\n    evaluate()\n"}
