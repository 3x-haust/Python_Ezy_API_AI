{"repo_info": {"repo_name": "PixEagle", "repo_owner": "alireza787b", "repo_url": "https://github.com/alireza787b/PixEagle"}}
{"type": "test_file", "path": "src/test_Ver.py", "content": "import cv2\nprint(cv2.__version__)\nprint(cv2.getBuildInformation())\n"}
{"type": "test_file", "path": "src/test_video_streaming.py", "content": "# video_streaming.py\nfrom flask import Flask, Response\nimport cv2\n\napp = Flask(__name__)\n\ndef generate_frames():\n    cap = cv2.VideoCapture('resources/test1.mp4')\n    while True:\n        success, frame = cap.read()\n        if not success:\n            break\n        else:\n            ret, buffer = cv2.imencode('.jpg', frame)\n            frame = buffer.tobytes()\n            yield (b'--frame\\r\\n'\n                   b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n\n@app.route('/video_feed')\ndef video_feed():\n    return Response(generate_frames(),\n                    mimetype='multipart/x-mixed-replace; boundary=frame')\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n"}
{"type": "source_file", "path": "src/classes/detectors/template_matching_detector.py", "content": "# src/classes/detectors/template_matching_detector.py\n\nimport cv2\nimport numpy as np\nfrom typing import Optional, Tuple\nfrom .base_detector import BaseDetector\nfrom classes.parameters import Parameters\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TemplateMatchingDetector(BaseDetector):\n    \"\"\"\n    TemplateMatchingDetector Class\n\n    Implements object detection using OpenCV's template matching methods with improvements\n    for robust redetection.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the TemplateMatchingDetector with the specified matching method.\n        \"\"\"\n        super().__init__()\n        self.template: Optional[np.ndarray] = None\n        self.latest_bbox: Optional[Tuple[int, int, int, int]] = None\n        self.method = self.get_matching_method(Parameters.TEMPLATE_MATCHING_METHOD)\n        self.initial_features: Optional[np.ndarray] = None\n        self.adaptive_features: Optional[np.ndarray] = None\n\n    @staticmethod\n    def get_matching_method(method_name: str):\n        \"\"\"\n        Maps the method name to the corresponding OpenCV template matching method.\n\n        Args:\n            method_name (str): Name of the template matching method.\n\n        Returns:\n            int: OpenCV method constant.\n        \"\"\"\n        methods = {\n            \"TM_CCOEFF\": cv2.TM_CCOEFF,\n            \"TM_CCOEFF_NORMED\": cv2.TM_CCOEFF_NORMED,\n            \"TM_CCORR\": cv2.TM_CCORR,\n            \"TM_CCORR_NORMED\": cv2.TM_CCORR_NORMED,\n            \"TM_SQDIFF\": cv2.TM_SQDIFF,\n            \"TM_SQDIFF_NORMED\": cv2.TM_SQDIFF_NORMED,\n        }\n        return methods.get(method_name, cv2.TM_CCOEFF_NORMED)\n\n    def extract_features(self, frame: np.ndarray, bbox: Tuple[int, int, int, int]) -> np.ndarray:\n        \"\"\"\n        Extracts features and initializes the template and adaptive features if not already set.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n            bbox (Tuple[int, int, int, int]): Bounding box (x, y, w, h).\n\n        Returns:\n            np.ndarray: The feature vector.\n        \"\"\"\n        features = super().extract_features(frame, bbox)\n        x, y, w, h = bbox\n\n        # Initialize the template only once\n        if self.template is None:\n            self.template = frame[y:y+h, x:x+w].copy()\n            self.initial_template = self.template.copy()\n            logger.debug(\"Template extracted and set for template matching.\")\n\n        # Initialize features if not set\n        if self.initial_features is None:\n            self.initial_features = features.copy()\n            self.adaptive_features = features.copy()\n            logger.debug(\"Initial features set for template matching.\")\n\n        self.latest_bbox = bbox\n        return features\n\n    def update_template(self, frame: np.ndarray, bbox: Tuple[int, int, int, int]) -> None:\n        \"\"\"\n        Updates the adaptive features based on the current frame.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n            bbox (Tuple[int, int, int, int]): Bounding box (x, y, w, h).\n        \"\"\"\n        features = super().extract_features(frame, bbox)\n        # Update adaptive features only\n        self.adaptive_features = (1 - Parameters.TEMPLATE_APPEARANCE_LEARNING_RATE) * self.adaptive_features + \\\n                                 Parameters.TEMPLATE_APPEARANCE_LEARNING_RATE * features\n        logger.debug(\"Adaptive features updated in TemplateMatchingDetector.\")\n\n    def smart_redetection(self, frame: np.ndarray, tracker=None, roi: Optional[Tuple[int, int, int, int]] = None) -> bool:\n        \"\"\"\n        Performs template matching to re-detect the object, with additional validation.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n            tracker (Optional[object]): The tracker instance.\n            roi (Optional[Tuple[int, int, int, int]]): Region of interest to limit search.\n\n        Returns:\n            bool: True if re-detection is successful, False otherwise.\n        \"\"\"\n        if self.template is None:\n            logger.warning(\"Template has not been set.\")\n            return False\n\n        frame_to_search = frame\n        x_offset, y_offset = 0, 0\n\n        if roi is not None:\n            x, y, w, h = roi\n            frame_to_search = frame[y:y+h, x:x+w]\n            x_offset, y_offset = x, y\n\n        # Check if frame_to_search is larger than or equal to the template\n        if frame_to_search.shape[0] < self.template.shape[0] or frame_to_search.shape[1] < self.template.shape[1]:\n            logger.warning(f\"Frame to search is smaller than template. Frame size: {frame_to_search.shape}, Template size: {self.template.shape}\")\n            return False\n\n        # Perform multi-scale template matching\n        match_found, match_result = self.perform_multiscale_template_matching(frame_to_search)\n        if not match_found:\n            logger.info(\"No matches found using template matching.\")\n            return False\n\n        # Extract the matched bounding box\n        top_left_x, top_left_y, w, h = match_result\n        # Adjust coordinates based on ROI offset\n        top_left_x += x_offset\n        top_left_y += y_offset\n        self.latest_bbox = (top_left_x, top_left_y, w, h)\n        logger.debug(f\"Redetection successful. New bbox: {self.latest_bbox}\")\n\n        # Validate the match\n        is_valid = self.validate_match(frame, self.latest_bbox)\n        if not is_valid:\n            logger.info(\"Match validation failed after redetection.\")\n            return False\n\n        # Update adaptive features\n        features = super().extract_features(frame, self.latest_bbox)\n        self.adaptive_features = (1 - Parameters.TEMPLATE_APPEARANCE_LEARNING_RATE) * self.adaptive_features + \\\n                                 Parameters.TEMPLATE_APPEARANCE_LEARNING_RATE * features\n\n        return True\n\n    def perform_multiscale_template_matching(self, frame_to_search: np.ndarray) -> Tuple[bool, Tuple[int, int, int, int]]:\n        \"\"\"\n        Performs multi-scale template matching without modifying the original template.\n\n        Args:\n            frame_to_search (np.ndarray): The image to search in.\n\n        Returns:\n            Tuple[bool, Tuple[int, int, int, int]]: (Match found, (top_left_x, top_left_y, w, h))\n        \"\"\"\n        scales = Parameters.TEMPLATE_MATCHING_SCALES\n        best_match_value = None\n        best_top_left = None\n        best_scale = 1.0\n\n        for scale in scales:\n            resized_template = cv2.resize(self.template, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n            if frame_to_search.shape[0] < resized_template.shape[0] or frame_to_search.shape[1] < resized_template.shape[1]:\n                continue\n\n            res = cv2.matchTemplate(frame_to_search, resized_template, self.method)\n            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n\n            if self.method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n                match_value = min_val\n                top_left = min_loc\n                is_better = best_match_value is None or match_value < best_match_value\n            else:\n                match_value = max_val\n                top_left = max_loc\n                is_better = best_match_value is None or match_value > best_match_value\n\n            if is_better:\n                best_match_value = match_value\n                best_top_left = top_left\n                best_scale = scale\n\n        if best_match_value is not None:\n            logger.debug(f\"Best match found at scale {best_scale} with value {best_match_value}\")\n            # Adjust the bounding box size based on the best scale\n            h_tmpl, w_tmpl = self.template.shape[:2]\n            h_resized, w_resized = int(h_tmpl * best_scale), int(w_tmpl * best_scale)\n            return True, (best_top_left[0], best_top_left[1], w_resized, h_resized)\n        else:\n            return False, (0, 0, 0, 0)\n\n    def validate_match(self, frame: np.ndarray, bbox: Tuple[int, int, int, int]) -> bool:\n        \"\"\"\n        Validates the matched area by comparing appearance features.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n            bbox (Tuple[int, int, int, int]): Bounding box of the matched area.\n\n        Returns:\n            bool: True if the match is valid, False otherwise.\n        \"\"\"\n        current_features = self.extract_features(frame, bbox)\n        confidence = self.compute_appearance_confidence(current_features, self.initial_features)\n        logger.debug(f\"Appearance confidence: {confidence:.2f}\")\n        return confidence >= Parameters.APPEARANCE_CONFIDENCE_THRESHOLD\n\n    def compute_appearance_confidence(self, features: np.ndarray, reference_features: np.ndarray) -> float:\n        \"\"\"\n        Computes the appearance confidence between the current features and the reference features.\n\n        Args:\n            features (np.ndarray): The features from the current detection.\n            reference_features (np.ndarray): The reference features to compare against.\n\n        Returns:\n            float: The confidence score between 0 and 1.\n        \"\"\"\n        # Use cosine similarity as an example\n        numerator = np.dot(features.flatten(), reference_features.flatten())\n        denominator = np.linalg.norm(features.flatten()) * np.linalg.norm(reference_features.flatten())\n        if denominator == 0:\n            return 0.0\n        else:\n            confidence = numerator / denominator\n            return confidence\n\n    def draw_detection(self, frame: np.ndarray, color=(0, 255, 255)) -> np.ndarray:\n        \"\"\"\n        Draws the detection bounding box on the frame.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n            color (tuple): Color for the bounding box.\n\n        Returns:\n            np.ndarray: The frame with detection drawn.\n        \"\"\"\n        if self.latest_bbox is None:\n            return frame\n        x, y, w, h = self.latest_bbox\n        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n        return frame\n\n    def get_latest_bbox(self) -> Optional[Tuple[int, int, int, int]]:\n        \"\"\"\n        Retrieves the latest bounding box from detection.\n\n        Returns:\n            Optional[Tuple[int, int, int, int]]: The latest bounding box or None.\n        \"\"\"\n        return self.latest_bbox\n\n    def set_latest_bbox(self, bbox: Tuple[int, int, int, int]) -> None:\n        \"\"\"\n        Sets the latest bounding box for detection.\n\n        Args:\n            bbox (Tuple[int, int, int, int]): The bounding box to set.\n        \"\"\"\n        self.latest_bbox = bbox\n"}
{"type": "source_file", "path": "src/classes/feature_matching_detector.py", "content": "#src/classes/feature_matching_detector.py\n\nimport cv2\nimport numpy as np\nfrom .detector_interface import DetectorInterface\nfrom .parameters import Parameters\n\n\nclass FeatureMatchingDetector(DetectorInterface):\n    def __init__(self):\n        self.feature_extractor = cv2.ORB_create(nfeatures=Parameters.ORB_FEATURES)\n        self.key_features = None\n        self.latest_bbox = None\n        self.key_features_img = None\n        self.frame = None\n    def extract_features(self, frame, bbox):\n        self.frame = frame\n        x, y, w, h = bbox\n        self.latest_bbox = bbox\n        roi = frame[y:y+h, x:x+w]\n        keypoints, descriptors = self.feature_extractor.detectAndCompute(roi, None)\n        self.key_features = (keypoints, descriptors)\n        self.key_features_img = roi.copy()\n\n    def smart_redetection(self, frame):\n        if self.key_features is None or self.feature_extractor is None:\n            print(\"Error: No key features stored or feature extractor not initialized.\")\n            return False\n\n        keypoints_current, descriptors_current = self.feature_extractor.detectAndCompute(frame, None)\n        if descriptors_current is None or self.key_features[1] is None:\n            print(\"Error: No descriptors to match.\")\n            return False\n\n        # Visualize keypoints on the current frame for debugging\n        img_keypoints_current = cv2.drawKeypoints(frame, keypoints_current, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n        #cv2.imshow(\"Current Frame Keypoints\", img_keypoints_current)\n\n        index_params = dict(algorithm=Parameters.FLANN_INDEX_LSH, \n                            table_number=Parameters.FLANN_TABLE_NUMBER, \n                            key_size=Parameters.FLANN_KEY_SIZE, \n                            multi_probe_level=Parameters.FLANN_MULTI_PROBE_LEVEL)\n        search_params = dict(checks=Parameters.FLANN_SEARCH_PARAMS[\"checks\"])\n        flann = cv2.FlannBasedMatcher(index_params, search_params)\n\n        matches = flann.knnMatch(self.key_features[1], descriptors_current, k=2)\n\n        good_matches = []\n        for match in matches:\n            if len(match) >= 2:\n                m, n = match\n                if m.distance < Parameters.ORB_FLENN_TRESH * n.distance:\n                    good_matches.append(m)\n\n        print(f\"Debug: {len(good_matches)} good matches found.\")\n\n        if len(good_matches) > Parameters.MIN_MATCH_COUNT:\n            src_pts = np.float32([self.key_features[0][m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n            dst_pts = np.float32([keypoints_current[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n\n            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n            if M is None:\n                print(\"Error: Homography could not be computed.\")\n                return False\n            frame_height, frame_width = self.frame.shape[:2]\n\n            pts = np.float32([[0, 0], [0, frame_height-1], [frame_width-1, frame_height-1], [frame_width-1, 0]]).reshape(-1, 1, 2)\n            dst = cv2.perspectiveTransform(pts, M)\n            last_x, last_y, last_w, last_h = self.latest_bbox \n            self.latest_bbox =  cv2.boundingRect(dst)           \n            x, y, w, h = self.latest_bbox\n            CONSTANT_BBOX_SIZE = True\n            if CONSTANT_BBOX_SIZE:\n                self.set_latest_bbox((x, y, last_w, last_h))\n            else:\n                self.set_latest_bbox((x, y, w, h))\n\n\n            # Corrected visualization of good matches\n            img_matches = cv2.drawMatches(self.key_features_img, self.key_features[0], frame, keypoints_current, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n            #cv2.imshow(\"Good Matches & Homography\", img_matches)\n            print(f\"Debug: New bounding box - X: {x}, Y: {y}, W: {w}, H: {h}\")\n            return True\n        else:\n            print(f\"Error: Not enough good matches found - {len(good_matches)}/{Parameters.MIN_MATCH_COUNT}\")\n            return False\n\n    def draw_detection(self, frame, color=(0, 255, 255)):\n        bbox = self.get_latest_bbox()\n        if bbox is None or len(bbox) != 4:\n            # If bbox is None or not in the expected format, return the frame as is.\n            #print(\"Warning: No bounding box available for drawing.\")\n            return frame\n\n        # Proceed with drawing only if bbox is valid.\n        p1 = (int(bbox[0]), int(bbox[1]))\n        p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n        cv2.rectangle(frame, p1, p2, color, 2, 1)\n        return frame\n\n    \n    def get_latest_bbox(self):\n        \"\"\"\n        Returns the latest bounding box.\n        \"\"\"\n        return self.latest_bbox\n    \n    def set_latest_bbox(self, bbox):\n        \"\"\"\n        Sets the latest bounding box, ensuring it does not go out of the frame.\n        \"\"\"\n        if bbox is None:\n            print(\"Warning: Attempted to set a None bounding box.\")\n            self.latest_bbox = None\n            return\n\n        # Ensure bbox coordinates are within the frame dimensions\n        frame_height, frame_width = self.frame.shape[:2]\n        x, y, w, h = bbox\n\n        # Correct the bounding box if it goes out of the frame\n        x = max(0, min(x, frame_width - 1))\n        y = max(0, min(y, frame_height - 1))\n        w = max(1, min(w, frame_width - x))\n        h = max(1, min(h, frame_height - y))\n\n        corrected_bbox = (x, y, w, h)\n        if corrected_bbox != bbox:\n            print(f\"Bounding box corrected from {bbox} to {corrected_bbox} to fit within the frame.\")\n\n        self.latest_bbox = corrected_bbox\n"}
{"type": "source_file", "path": "src/classes/frame_preprocessor.py", "content": "# src/classes/frame_preprocessor.py\n\nimport cv2\nimport numpy as np\nfrom classes.parameters import Parameters\n\nclass FramePreprocessor:\n    \"\"\"\n    Handles preprocessing of video frames before they are processed by other components.\n    Preprocessing steps can include noise reduction, contrast enhancement, color space conversion, etc.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the FramePreprocessor with the desired preprocessing techniques.\n        The techniques to apply are determined by parameters in the Parameters class.\n        \"\"\"\n        self.techniques = []\n\n        # Add preprocessing techniques based on configuration\n        if Parameters.PREPROCESSING_USE_BLUR:\n            self.techniques.append(self.apply_blur)\n\n        if Parameters.PREPROCESSING_USE_MEDIAN_BLUR:\n            self.techniques.append(self.apply_median_blur)\n\n        if Parameters.PREPROCESSING_USE_CLAHE:\n            self.techniques.append(self.apply_clahe)\n\n        if Parameters.PREPROCESSING_COLOR_SPACE != 'BGR':\n            self.techniques.append(self.convert_color_space)\n\n        # Additional techniques can be appended here\n\n    def preprocess(self, frame: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Applies the selected preprocessing techniques to the given frame.\n\n        Args:\n            frame (np.ndarray): The input video frame.\n\n        Returns:\n            np.ndarray: The preprocessed video frame.\n        \"\"\"\n        for technique in self.techniques:\n            frame = technique(frame)\n        return frame\n\n    def apply_blur(self, frame: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Applies Gaussian blur to the frame to reduce noise.\n\n        Args:\n            frame (np.ndarray): The input video frame.\n\n        Returns:\n            np.ndarray: The blurred video frame.\n        \"\"\"\n        ksize = Parameters.PREPROCESSING_BLUR_KERNEL_SIZE\n        return cv2.GaussianBlur(frame, (ksize, ksize), 0)\n\n    def apply_median_blur(self, frame: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Applies median blur to the frame to reduce noise, effective for salt-and-pepper noise.\n\n        Args:\n            frame (np.ndarray): The input video frame.\n\n        Returns:\n            np.ndarray: The blurred video frame.\n        \"\"\"\n        ksize = Parameters.PREPROCESSING_MEDIAN_BLUR_KERNEL_SIZE\n        return cv2.medianBlur(frame, ksize)\n\n    def apply_clahe(self, frame: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Applies Contrast Limited Adaptive Histogram Equalization (CLAHE) to enhance contrast.\n\n        Args:\n            frame (np.ndarray): The input video frame.\n\n        Returns:\n            np.ndarray: The frame with enhanced contrast.\n        \"\"\"\n        # Convert to LAB color space\n        lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n        l_channel, a_channel, b_channel = cv2.split(lab)\n\n        # Apply CLAHE to the L-channel\n        clahe = cv2.createCLAHE(clipLimit=Parameters.PREPROCESSING_CLAHE_CLIP_LIMIT,\n                                tileGridSize=(Parameters.PREPROCESSING_CLAHE_TILE_GRID_SIZE,\n                                              Parameters.PREPROCESSING_CLAHE_TILE_GRID_SIZE))\n        cl = clahe.apply(l_channel)\n\n        # Merge the channels back and convert to BGR\n        merged = cv2.merge((cl, a_channel, b_channel))\n        return cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n\n    def convert_color_space(self, frame: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Converts the frame to the specified color space.\n\n        Args:\n            frame (np.ndarray): The input video frame.\n\n        Returns:\n            np.ndarray: The frame converted to the desired color space.\n        \"\"\"\n        color_space = Parameters.PREPROCESSING_COLOR_SPACE\n        if color_space == 'GRAY':\n            return cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        elif color_space == 'HSV':\n            return cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n        elif color_space == 'LAB':\n            return cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n        # Add more color spaces as needed\n        else:\n            # Default to BGR if unknown color space\n            return frame\n\n    # Additional preprocessing methods can be added here\n"}
{"type": "source_file", "path": "src/classes/detectors/base_detector.py", "content": "# src/classes/detectors/base_detector.py\n\nimport cv2\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom typing import Tuple, Optional\nfrom classes.parameters import Parameters  # Ensure Parameters is correctly imported\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass BaseDetector(ABC):\n    \"\"\"\n    BaseDetector Abstract Class\n\n    Defines the interface and common functionalities for all detectors.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes common attributes for detectors.\n        \"\"\"\n        self.adaptive_features: Optional[np.ndarray] = None\n        self.initial_features: Optional[np.ndarray] = None\n        self.initial_template: Optional[np.ndarray] = None\n\n    def extract_features(self, frame: np.ndarray, bbox: Tuple[int, int, int, int]) -> np.ndarray:\n        \"\"\"\n        Extracts color histogram features from the specified bounding box in the frame.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n            bbox (Tuple[int, int, int, int]): Bounding box (x, y, w, h).\n\n        Returns:\n            np.ndarray: The feature vector.\n        \"\"\"\n        x, y, w, h = [int(v) for v in bbox]\n        roi = frame[y:y+h, x:x+w]\n        if roi is None or roi.size == 0:\n            # Return zero features if ROI is invalid\n            return np.zeros((16*16*16,), dtype=np.float32)\n        features = cv2.calcHist([roi], [0, 1, 2], None, [16, 16, 16],\n                                [0, 256, 0, 256, 0, 256])\n        features = cv2.normalize(features, features).flatten()\n        return features\n\n    def compute_appearance_confidence(self, current_features: np.ndarray, adaptive_features: np.ndarray) -> float:\n        \"\"\"\n        Computes confidence based on appearance consistency.\n\n        Args:\n            current_features (np.ndarray): Features of the current frame's ROI.\n            adaptive_features (np.ndarray): Adaptive features maintained over time.\n\n        Returns:\n            float: The appearance confidence score between 0.0 and 1.0.\n        \"\"\"\n        similarity = cv2.compareHist(adaptive_features, current_features, cv2.HISTCMP_BHATTACHARYYA)\n        confidence = max(0.0, 1.0 - similarity)\n        return confidence\n\n    def is_appearance_consistent(self, confidence: float) -> bool:\n        \"\"\"\n        Determines if the appearance is consistent based on confidence.\n\n        Args:\n            confidence (float): The appearance confidence score.\n\n        Returns:\n            bool: True if appearance is consistent, False otherwise.\n        \"\"\"\n        return confidence >= Parameters.APPEARANCE_CONFIDENCE_THRESHOLD\n\n    def compute_edge_similarity(self, initial_template: np.ndarray, roi: np.ndarray) -> float:\n        \"\"\"\n        Computes edge-based similarity between the initial template and the current region.\n\n        Args:\n            initial_template (np.ndarray): The initial template image.\n            roi (np.ndarray): The region of interest in the current frame.\n\n        Returns:\n            float: The edge similarity score.\n        \"\"\"\n        if roi is None or roi.size == 0 or initial_template is None or initial_template.size == 0:\n            return 1.0  # Maximum distance if invalid\n        initial_edge = self.extract_edge(initial_template)\n        current_edge = self.extract_edge(roi)\n        current_edge_resized = cv2.resize(current_edge, (initial_edge.shape[1], initial_edge.shape[0]))\n        similarity = cv2.matchTemplate(current_edge_resized, initial_edge, cv2.TM_CCOEFF_NORMED)\n        max_similarity = np.max(similarity)\n        return 1.0 - max_similarity  # Invert similarity to represent distance\n\n    def extract_edge(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Extracts edge features from the image using Canny edge detector.\n\n        Args:\n            image (np.ndarray): The input image.\n\n        Returns:\n            np.ndarray: The edge image.\n        \"\"\"\n        if image is None or image.size == 0:\n            # Return zero edge image if image is invalid\n            return np.zeros((1, 1), dtype=np.uint8)\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        edges = cv2.Canny(gray, Parameters.PF_CANNY_THRESHOLD1, Parameters.PF_CANNY_THRESHOLD2)\n        return edges\n\n    @abstractmethod\n    def smart_redetection(self, frame: np.ndarray, tracker=None, roi: Optional[Tuple[int, int, int, int]] = None) -> bool:\n        \"\"\"\n        Performs smart re-detection of the object in the frame.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n            tracker (Optional[object]): The tracker instance.\n            roi (Optional[Tuple[int, int, int, int]]): Region of interest to limit search.\n\n        Returns:\n            bool: True if re-detection is successful, False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def draw_detection(self, frame: np.ndarray, color=(0, 255, 255)) -> np.ndarray:\n        \"\"\"\n        Draws the detection bounding box on the frame.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n            color (tuple): Color for the bounding box.\n\n        Returns:\n            np.ndarray: The frame with detection drawn.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_latest_bbox(self) -> Optional[Tuple[int, int, int, int]]:\n        \"\"\"\n        Retrieves the latest bounding box from detection.\n\n        Returns:\n            Optional[Tuple[int, int, int, int]]: The latest bounding box or None.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def set_latest_bbox(self, bbox: Tuple[int, int, int, int]) -> None:\n        \"\"\"\n        Sets the latest bounding box for detection.\n\n        Args:\n            bbox (Tuple[int, int, int, int]): The bounding box to set.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "src/classes/estimators/estimator_factory.py", "content": "# src/classes/estimators/estimator_factory.py\n\n\"\"\"\nEstimator Factory Module\n------------------------\n\nThis module provides a factory function `create_estimator` to instantiate estimator objects based on a specified type.\n\nPurpose:\n--------\nThe factory pattern allows for the creation of estimator instances without exposing the creation logic to the client and refers to the newly created object using a common interface.\n\nUsage:\n------\nTo create an estimator:\n```python\nestimator = create_estimator(Parameters.ESTIMATOR_TYPE)\n```\n\nNotes:\n------\n- Supported estimator types should be added to the factory function.\n- If the estimator is disabled or an unrecognized type is specified, the function returns `None`.\n- This approach promotes flexibility and scalability within the PixEagle project.\n\n\"\"\"\n\nfrom classes.estimators.kalman_estimator import KalmanEstimator\n\ndef create_estimator(estimator_type):\n    \"\"\"\n    Factory function to create an estimator instance based on the specified type.\n\n    Args:\n        estimator_type (str): The type of estimator to create (e.g., \"Kalman\").\n\n    Returns:\n        BaseEstimator or None: An instance of the requested estimator, or None if the type is unrecognized.\n\n    Usage:\n        estimator = create_estimator(\"Kalman\")\n    \"\"\"\n    if estimator_type == \"Kalman\":\n        return KalmanEstimator()\n    else:\n        return None  # Return None if estimator is disabled or type is unrecognized\n"}
{"type": "source_file", "path": "src/classes/flow_controller.py", "content": "# src/classes/flow_controller.py\nimport asyncio\nimport logging\nimport threading\nimport signal\nimport cv2\nfrom uvicorn import Config, Server\nfrom classes.app_controller import AppController\nfrom classes.parameters import Parameters\n\nclass FlowController:\n    def __init__(self):\n        \"\"\"\n        Initializes the FlowController, including the AppController and FastAPI server.\n        \"\"\"\n        logging.debug(\"Initializing FlowController...\")\n\n        # Initialize AppController\n        self.controller = AppController()\n\n        # Initialize FastAPI server\n        self.server, self.server_thread = self.start_fastapi_server()\n\n        # Setup signal handling for graceful shutdown\n        signal.signal(signal.SIGINT, self.shutdown_handler)\n        signal.signal(signal.SIGTERM, self.shutdown_handler)\n\n        self.controller.shutdown_flag = False\n\n    def start_fastapi_server(self):\n        \"\"\"\n        Initializes and starts the FastAPI server in a separate thread.\n        \"\"\"\n        logging.debug(\"Initializing FastAPI server...\")\n        fastapi_handler = self.controller.api_handler\n        app = fastapi_handler.app\n\n        config = Config(app=app, host=Parameters.HTTP_STREAM_HOST, port=Parameters.HTTP_STREAM_PORT, log_level=\"info\")\n        server = Server(config)\n        \n        server_thread = threading.Thread(target=server.run)\n        server_thread.start()\n        logging.debug(\"FastAPI server started on a separate thread.\")\n        return server, server_thread\n\n    def main_loop(self):\n        \"\"\"\n        Main loop to handle video processing, user inputs, and the main application flow.\n        \"\"\"\n        try:\n            # Create a persistent event loop\n            loop = asyncio.get_event_loop()\n\n            while not self.controller.shutdown_flag:\n                frame = self.controller.video_handler.get_frame()\n                if frame is None:\n                    break\n\n                # Run the update loop within the persistent event loop\n                frame = loop.run_until_complete(self.controller.update_loop(frame))\n                self.controller.show_current_frame()\n\n                key = cv2.waitKey(self.controller.video_handler.delay_frame) & 0xFF\n                if key == ord('q'):\n                    logging.info(\"Quitting...\")\n                    self.controller.shutdown_flag = True\n                else:\n                    # Handle key input within the persistent event loop\n                    loop.run_until_complete(self.controller.handle_key_input_async(key, frame))\n\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n\n        # Ensure proper shutdown\n        loop.run_until_complete(self.controller.shutdown())\n        self.server.should_exit = True\n        self.server_thread.join()  # Wait for the FastAPI server thread to finish\n        cv2.destroyAllWindows()\n        logging.debug(\"Application shutdown complete.\")\n\n\n    def shutdown_handler(self, signum, frame):\n        \"\"\"\n        Signal handler to gracefully shutdown the application.\n\n        Args:\n            signum (int): The signal number.\n            frame (FrameType): The current stack frame.\n        \"\"\"\n        logging.info(\"Shutting down...\")\n        asyncio.run(self.controller.shutdown())\n        self.controller.shutdown_flag = True\n"}
{"type": "source_file", "path": "src/classes/estimators/kalman_estimator.py", "content": "# src/classes/estimators/kalman_estimator.py\n\n\"\"\"\nKalmanEstimator Module\n----------------------\n\nThis module implements the `KalmanEstimator` class, which provides a Kalman Filter-based estimator for 2D position, velocity, and acceleration estimation in the context of aerial target tracking.\n\nProject Information:\n- Project Name: PixEagle\n- Repository: https://github.com/alireza787b/PixEagle\n- Date: October 2024\n- Author: Alireza Ghaderi\n- LinkedIn: https://www.linkedin.com/in/alireza787b\n\nOverview:\n---------\nThe `KalmanEstimator` uses a constant acceleration model to predict and update the state of a tracked object. The Kalman Filter is a recursive optimal estimator that fuses noisy measurements to produce estimates of unknown variables that are more accurate than those based on a single measurement alone.\n\nPurpose:\n--------\nIn aerial target tracking, the Kalman Filter helps in smoothing out the noisy measurements obtained from the tracker, providing a more accurate and stable estimate of the target's position and motion. This is particularly useful when the measurements are noisy or when the target exhibits unpredictable motion due to acceleration.\n\nKey Features:\n-------------\n- **State Vector**: The state vector includes position `(x, y)`, velocity `(dx, dy)`, and acceleration `(ddx, ddy)`.\n- **Measurement Model**: Only position measurements `(x, y)` are used, as velocity and acceleration are not directly measured.\n- **Process and Measurement Noise Covariances**: Tunable parameters that affect the filter's responsiveness and smoothness.\n- **Normalization**: Provides normalized estimates suitable for control inputs that require normalized coordinates.\n\nUsage:\n------\nThe `KalmanEstimator` is integrated into the tracking system and can be enabled or disabled via parameters. It is used to improve the robustness and accuracy of the tracking by providing filtered estimates of the target's position.\n\nDisabling the Estimator:\n------------------------\nTo disable the estimator, set the parameter `USE_ESTIMATOR` to `False` in the `Parameters` class. This will cause the system to rely solely on the raw measurements from the tracker.\n\nTuning Parameters:\n------------------\n- **ESTIMATOR_PROCESS_NOISE_VARIANCE**: Higher values make the filter more responsive to changes but may introduce noise.\n- **ESTIMATOR_MEASUREMENT_NOISE_VARIANCE**: Higher values make the filter trust the measurements less, leading to smoother estimates but potential lag.\n- **ESTIMATOR_INITIAL_STATE_COVARIANCE**: Determines the initial uncertainty of the state estimates.\n\nIntegration:\n------------\nThe estimator is integrated into the tracking pipeline. The tracker provides measurements to the estimator, which then produces filtered estimates. The `AppController` uses these estimates for control decisions, such as commanding the UAV to follow the target.\n\nExtending and Building New Estimators:\n--------------------------------------\nTo build a new estimator:\n1. Create a new class that inherits from `BaseEstimator`.\n2. Implement all abstract methods defined in `BaseEstimator`.\n3. Add the new estimator to the `estimator_factory.py` for easy creation.\n\nNotes:\n------\n- Ensure that the time step `dt` is updated accurately to reflect the actual time between measurements.\n- The estimator assumes a constant acceleration model; if the target's motion model differs significantly, consider modifying the state transition matrix accordingly.\n- This estimator is part of the PixEagle project developed by Alireza Ghaderi in October 2024. For more information, visit the project repository or contact via LinkedIn.\n\nReferences:\n-----------\n- R. E. Kalman, \"A New Approach to Linear Filtering and Prediction Problems,\" Transactions of the ASMEâ€“Journal of Basic Engineering, 1960.\n- G. Welch and G. Bishop, \"An Introduction to the Kalman Filter,\" UNC-Chapel Hill, 1995.\n\n\"\"\"\n\nfrom typing import Optional, Tuple\nimport numpy as np\nimport logging\nfrom filterpy.kalman import KalmanFilter\nfrom .base_estimator import BaseEstimator\nfrom classes.parameters import Parameters  # Ensure correct import path\n\nlogger = logging.getLogger(__name__)\n\nclass KalmanEstimator(BaseEstimator):\n    \"\"\"\n    KalmanEstimator implements a Kalman Filter for estimating the position, velocity, and acceleration of a target in 2D space.\n\n    The filter uses a constant acceleration model and processes position measurements to produce smoothed estimates.\n\n    Attributes:\n        filter (KalmanFilter): The Kalman Filter instance.\n        dt (float): Time step between measurements.\n        measurement_noise_variance (float): Variance of the measurement noise.\n        process_noise_variance (float): Variance of the process noise.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the KalmanEstimator with a Kalman Filter configured for 2D position, velocity, and acceleration estimation.\n        \"\"\"\n        # Initialize the Kalman Filter with 6 state variables (x, y, dx, dy, ddx, ddy) and 2 measurement variables (x, y)\n        self.filter = KalmanFilter(dim_x=6, dim_z=2)\n        self.filter.x = np.zeros((6, 1))  # State vector: [x, y, dx, dy, ddx, ddy]\n\n        # Initial covariance matrix P\n        initial_covariance = Parameters.ESTIMATOR_INITIAL_STATE_COVARIANCE\n        self.filter.P = np.diag(initial_covariance)\n\n        # Measurement function H: we measure only position (x, y)\n        self.filter.H = np.array([\n            [1, 0, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0, 0]\n        ])\n\n        # Measurement noise covariance R: assumes independent measurement noise for x and y\n        self.measurement_noise_variance = Parameters.ESTIMATOR_MEASUREMENT_NOISE_VARIANCE\n        self.filter.R = np.eye(2) * self.measurement_noise_variance\n\n        # Process noise variance\n        self.process_noise_variance = Parameters.ESTIMATOR_PROCESS_NOISE_VARIANCE\n\n        # Default time step dt\n        self.dt = 0.1\n        self.update_F_and_Q(self.dt)\n\n    def predict_only(self):\n        \"\"\"\n        Performs only the predict step of the Kalman Filter without an update.\n\n        This is used when a measurement is not available, allowing the filter to propagate the state estimate based on the model.\n        \"\"\"\n        self.filter.predict()\n        logger.debug(\"Kalman Filter prediction step executed without measurement update.\")\n\n    def update_F_and_Q(self, dt):\n        \"\"\"\n        Updates the state transition matrix (F) and the process noise covariance matrix (Q) based on the new dt.\n\n        Args:\n            dt (float): Time step between the current and previous measurements.\n\n        The state transition matrix F models how the state evolves from one time step to the next.\n        The process noise covariance Q represents the uncertainty in the model, particularly due to acceleration.\n\n        For a constant acceleration model:\n            - Position changes according to velocity and acceleration.\n            - Velocity changes according to acceleration.\n            - Acceleration is assumed to be constant (but with some process noise).\n\n        The matrices are derived based on standard motion equations with constant acceleration.\n        \"\"\"\n        # State transition matrix (constant acceleration model)\n        dt2 = dt ** 2\n        dt3 = dt ** 3\n        dt4 = dt ** 4\n\n        self.filter.F = np.array([\n            [1, 0, dt, 0, dt2 / 2, 0],\n            [0, 1, 0, dt, 0, dt2 / 2],\n            [0, 0, 1, 0, dt, 0],\n            [0, 0, 0, 1, 0, dt],\n            [0, 0, 0, 0, 1, 0],\n            [0, 0, 0, 0, 0, 1]\n        ])\n\n        # Process noise covariance matrix Q based on the standard constant acceleration model\n        q = self.process_noise_variance\n\n        Q = q * np.array([\n            [dt4 / 4,      0,          dt3 / 2,    0,          dt2 / 2,    0],\n            [0,            dt4 / 4,    0,          dt3 / 2,    0,          dt2 / 2],\n            [dt3 / 2,      0,          dt2,        0,          dt,         0],\n            [0,            dt3 / 2,    0,          dt2,        0,          dt],\n            [dt2 / 2,      0,          dt,         0,          1,          0],\n            [0,            dt2 / 2,    0,          dt,         0,          1]\n        ])\n\n        self.filter.Q = Q\n        # logger.debug(f\"Updated F and Q matrices with dt = {dt}\")\n\n    def set_dt(self, dt):\n        \"\"\"\n        Sets the time step (dt) and updates the filter's state transition matrix and process noise accordingly.\n\n        Args:\n            dt (float): The new time step to use.\n\n        Raises:\n            ValueError: If dt is non-positive.\n\n        It's important to update dt whenever the time between measurements changes to ensure accurate predictions.\n        \"\"\"\n        if dt <= 0:\n            raise ValueError(\"Time step (dt) must be a positive value.\")\n\n        self.dt = dt\n        self.update_F_and_Q(dt)\n        # logger.debug(f\"Time step updated to {dt}\")\n\n    def predict_and_update(self, measurement):\n        \"\"\"\n        Performs the predict and update cycle of the Kalman Filter using the provided measurement.\n\n        Args:\n            measurement (list, tuple, or np.ndarray): The current measurement [x, y].\n\n        Raises:\n            ValueError: If the measurement is not a 2-element list, tuple, or array.\n\n        The predict step uses the state transition model to predict the next state.\n        The update step incorporates the new measurement to refine the state estimate.\n        \"\"\"\n        if not isinstance(measurement, (list, tuple, np.ndarray)) or len(measurement) != 2:\n            raise ValueError(\"Measurement must be a list, tuple, or numpy array with two elements [x, y].\")\n\n        self.filter.predict()\n        self.filter.update(np.array(measurement).reshape(2, 1))\n        # logger.debug(f\"Kalman Filter predicted and updated with measurement: {measurement}\")\n        # logger.debug(f\"Post-update state estimate: {self.filter.x.flatten().tolist()}\")\n        # logger.debug(f\"Post-update covariance P: {self.filter.P}\")\n\n    def get_estimate(self) -> list:\n        \"\"\"\n        Retrieves the current state estimate from the Kalman Filter.\n\n        Returns:\n            list: The estimated state [x, y, dx, dy, ddx, ddy].\n\n        The estimate includes position, velocity, and acceleration components.\n        \"\"\"\n        estimate = self.filter.x.flatten().tolist()\n        # logger.debug(f\"Current state estimate: {estimate}\")\n        return estimate\n\n    def get_normalized_estimate(self, frame_width: int, frame_height: int) -> Optional[Tuple[float, float]]:\n        \"\"\"\n        Returns the normalized estimated position based on frame dimensions.\n\n        Normalization is done such that the center of the frame is (0, 0),\n        the top-right is (1, 1), and the bottom-left is (-1, -1).\n\n        Args:\n            frame_width (int): Width of the video frame.\n            frame_height (int): Height of the video frame.\n\n        Returns:\n            Optional[Tuple[float, float]]: Normalized (x, y) coordinates or None.\n\n        This is useful for control inputs that require normalized coordinates, such as sending commands to a UAV.\n        \"\"\"\n        estimate = self.get_estimate()\n        if not estimate or len(estimate) < 2:\n            logging.warning(\"Estimate is not available or incomplete for normalization.\")\n            return None\n\n        x, y = estimate[0], estimate[1]\n\n        if frame_width <= 0 or frame_height <= 0:\n            logging.warning(\"Invalid frame dimensions for normalization.\")\n            return None\n\n        norm_x = (x - frame_width / 2) / (frame_width / 2)\n        norm_y = (y - frame_height / 2) / (frame_height / 2)\n\n        normalized_estimate = (norm_x, norm_y)\n        # logger.debug(f\"Normalized estimate: {normalized_estimate}\")\n\n        return normalized_estimate\n\n    def reset(self):\n        \"\"\"\n        Resets the Kalman Filter to its initial state.\n\n        This can be used when reinitializing tracking or when the filter's state becomes unreliable.\n        \"\"\"\n        self.filter.x = np.zeros((6, 1))  # Reset state vector\n        initial_covariance = Parameters.ESTIMATOR_INITIAL_STATE_COVARIANCE\n        self.filter.P = np.diag(initial_covariance)  # Reset covariance matrix\n        self.dt = 0.1\n        self.update_F_and_Q(self.dt)\n        logger.info(\"Kalman Filter state reset.\")\n\n    def is_estimate_reliable(self, uncertainty_threshold: float) -> bool:\n        \"\"\"\n        Checks if the current estimate is reliable based on the trace of the covariance matrix.\n\n        Args:\n            uncertainty_threshold (float): The maximum acceptable uncertainty.\n\n        Returns:\n            bool: True if the estimate is reliable, False otherwise.\n\n        A high uncertainty indicates that the filter's estimates may not be trustworthy, possibly due to lack of recent measurements.\n        \"\"\"\n        uncertainty = np.trace(self.filter.P)\n        logger.debug(f\"Current estimate uncertainty: {uncertainty}\")\n        return uncertainty < uncertainty_threshold\n\n# Additional Notes:\n# -----------------\n# - To disable the estimator, set `USE_ESTIMATOR: false` in Parameters config.yaml.\n# - When the estimator is disabled, the system will use raw measurements from the tracker.\n# - The estimator is designed to be modular; new estimators can be added by implementing the `BaseEstimator` interface.\n# - Ensure that the `filterpy` library is installed, as it provides the Kalman Filter implementation used here.\n\n"}
{"type": "source_file", "path": "src/classes/app_controller.py", "content": "# src/classes/app_controller.py\n\nimport asyncio\nimport logging\nimport time\nimport numpy as np\nfrom classes.parameters import Parameters\nfrom classes.follower import Follower\nfrom classes.setpoint_sender import SetpointSender\nfrom classes.video_handler import VideoHandler\nfrom classes.trackers.csrt_tracker import CSRTTracker  # Import other trackers as necessary\nfrom classes.segmentor import Segmentor\nfrom classes.trackers.tracker_factory import create_tracker\nimport cv2\nfrom classes.px4_interface_manager import PX4InterfaceManager  # Updated import path\nfrom classes.telemetry_handler import TelemetryHandler\nfrom classes.fastapi_handler import FastAPIHandler  # Correct import\nfrom typing import Dict, Optional, Tuple\nfrom classes.osd_handler import OSDHandler\nfrom classes.gstreamer_handler import GStreamerHandler\nfrom classes.mavlink_data_manager import MavlinkDataManager\nfrom classes.frame_preprocessor import FramePreprocessor\nfrom classes.estimators.estimator_factory import create_estimator\nfrom classes.detectors.detector_factory import create_detector\n\n\n\nclass AppController:\n    def __init__(self):\n        \"\"\"\n        Initializes the AppController with necessary components and starts the FastAPI handler.\n        \"\"\"\n        logging.debug(\"Initializing AppController...\")\n\n        # Initialize MAVLink Data Manager\n        self.mavlink_data_manager = MavlinkDataManager(\n            mavlink_host=Parameters.MAVLINK_HOST,\n            mavlink_port=Parameters.MAVLINK_PORT,\n            polling_interval=Parameters.MAVLINK_POLLING_INTERVAL,\n            data_points=Parameters.MAVLINK_DATA_POINTS,\n            enabled=Parameters.MAVLINK_ENABLED\n        )\n        \n        # Initialize the FramePreprocessor if enabled\n        if Parameters.ENABLE_PREPROCESSING:\n            self.preprocessor = FramePreprocessor()\n        else:\n            self.preprocessor = None\n        \n        # Start polling MAVLink data if enabled\n        if Parameters.MAVLINK_ENABLED:\n            self.mavlink_data_manager.start_polling()\n\n        # Initialize the estimator first\n        self.estimator = create_estimator(Parameters.ESTIMATOR_TYPE)\n\n        # Initialize video processing components\n        self.video_handler = VideoHandler()\n        self.video_streamer = None\n        self.detector = create_detector(Parameters.DETECTION_ALGORITHM)\n        self.tracker = create_tracker(Parameters.DEFAULT_TRACKING_ALGORITHM,\n                                      self.video_handler, self.detector, self)\n        self.segmentor = Segmentor(algorithm=Parameters.DEFAULT_SEGMENTATION_ALGORITHM)\n                \n        self.tracking_failure_start_time = None  # Initialize tracking failure timer\n\n        # Initialize frame counter\n        self.frame_counter = 0\n\n        # Flags to track the state of tracking and segmentation\n        self.tracking_started = False\n        self.segmentation_active = False\n\n        # Setup a named window and a mouse callback for interactions\n        if Parameters.SHOW_VIDEO_WINDOW:\n            cv2.namedWindow(\"Video\")\n            cv2.setMouseCallback(\"Video\", self.on_mouse_click)\n        self.current_frame = None\n\n        # Initialize PX4 interface manager and following mode flag\n        self.px4_interface = PX4InterfaceManager(app_controller=self)\n        self.following_active = False\n        self.follower = None\n        self.setpoint_sender = None\n\n        # Initialize telemetry handler with tracker and follower\n        self.telemetry_handler = TelemetryHandler(self, lambda: self.tracking_started)\n\n        # Initialize the FastAPI handler\n        logging.debug(\"Initializing FastAPIHandler...\")\n        self.api_handler = FastAPIHandler(self)\n        logging.debug(\"FastAPIHandler initialized.\")\n\n        # Initialize the OSD handler with access to the AppController\n        self.osd_handler = OSDHandler(self)\n        \n        # Initialize GStreamerHandler if streaming is enabled\n        if Parameters.ENABLE_GSTREAMER_STREAM:\n            self.gstreamer_handler = GStreamerHandler()\n            self.gstreamer_handler.initialize_stream()\n\n        logging.info(\"AppController initialized.\")\n\n    def on_mouse_click(self, event: int, x: int, y: int, flags: int, param: any):\n        \"\"\"\n        Handles mouse click events in the video window, specifically for initiating segmentation.\n        \"\"\"\n        if event == cv2.EVENT_LBUTTONDOWN and self.segmentation_active:\n            self.handle_user_click(x, y)\n\n    def toggle_tracking(self, frame: np.ndarray):\n        \"\"\"\n        Toggles the tracking state, starts or stops tracking based on the current state.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n        \"\"\"\n        if not self.tracking_started:\n            bbox = cv2.selectROI(Parameters.FRAME_TITLE, frame, False, False)\n            if bbox and bbox[2] > 0 and bbox[3] > 0:\n                self.tracker.start_tracking(frame, bbox)\n                self.tracking_started = True\n                self.frame_counter = 0  # Initialize frame counter\n                if self.detector:\n                    # Initialize detector's features and template\n                    self.detector.extract_features(frame, bbox)\n                    logging.debug(\"Detector's initial features and template set.\")\n                logging.info(\"Tracking activated.\")\n            else:\n                logging.info(\"Tracking canceled or invalid ROI.\")\n        else:\n            self.cancel_activities()\n            logging.info(\"Tracking deactivated.\")\n\n    def toggle_segmentation(self) -> bool:\n        \"\"\"\n        Toggles the segmentation state. Activates or deactivates segmentation.\n\n        Returns:\n            bool: The current state of segmentation after toggling.\n        \"\"\"\n        self.segmentation_active = not self.segmentation_active\n        logging.info(f\"Segmentation {'activated' if self.segmentation_active else 'deactivated'}.\")\n        return self.segmentation_active\n\n    async def start_tracking(self, bbox: Dict[str, int]):\n        \"\"\"\n        Starts tracking with the provided bounding box.\n\n        Args:\n            bbox (dict): The bounding box for tracking.\n        \"\"\"\n        if not self.tracking_started:\n            bbox_tuple = (bbox['x'], bbox['y'], bbox['width'], bbox['height'])\n            self.tracker.start_tracking(self.current_frame, bbox_tuple)\n            self.tracking_started = True\n            if hasattr(self.tracker, 'detector') and self.tracker.detector:\n                self.tracker.detector.extract_features(self.current_frame, bbox_tuple)\n            logging.info(\"Tracking activated.\")\n        else:\n            logging.info(\"Tracking is already active.\")\n\n    async def stop_tracking(self):\n        \"\"\"\n        Stops the tracking process if it is currently active.\n        \"\"\"\n        if self.tracking_started:\n            self.cancel_activities()\n            logging.info(\"Tracking deactivated.\")\n        else:\n            logging.info(\"Tracking is not active.\")\n\n    def cancel_activities(self):\n        \"\"\"\n        Cancels both tracking and segmentation activities, resetting their states.\n        \"\"\"\n        self.tracking_started = False\n        self.segmentation_active = False\n        if self.setpoint_sender:\n            self.setpoint_sender.stop()\n            self.setpoint_sender.join()\n            self.setpoint_sender = None\n        logging.info(\"All activities cancelled.\")\n\n    async def update_loop(self, frame: np.ndarray) -> np.ndarray:\n        \"\"\"\n        The main update loop for processing each video frame.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n\n        Returns:\n            np.ndarray: The processed video frame.\n        \"\"\"\n        try:\n            # Preprocessing step\n            if Parameters.ENABLE_PREPROCESSING and self.preprocessor:\n                frame = self.preprocessor.preprocess(frame)\n            \n            # Segmentation step\n            if self.segmentation_active:\n                frame = self.segmentor.segment_frame(frame)\n            \n            # Tracking and estimation\n            if self.tracking_started:\n                if self.tracking_failure_start_time is None:\n                    success, _ = self.tracker.update(frame)\n                else:\n                    success = False\n\n                if success:\n                    # Reset tracking failure timer\n                    self.tracking_failure_start_time = None\n                    frame = self.tracker.draw_tracking(frame, tracking_successful=True)\n                    if Parameters.ENABLE_DEBUGGING:\n                        self.tracker.print_normalized_center()\n                    if self.tracker.position_estimator:\n                        frame = self.tracker.draw_estimate(frame, tracking_successful=True)\n                    if self.following_active:\n                        await self.follow_target()\n                        await self.check_failsafe()\n\n                    # Increment frame counter\n                    self.frame_counter += 1\n                    # Update template if confidence is high and interval matched\n                    tracker_confidence = self.tracker.get_confidence()\n                    if (tracker_confidence >= Parameters.TRACKER_CONFIDENCE_THRESHOLD_FOR_TEMPLATE_UPDATE and\n                        self.frame_counter % Parameters.TEMPLATE_UPDATE_INTERVAL == 0):\n                        bbox = self.tracker.bbox\n                        if bbox:\n                            self.detector.update_template(frame, bbox)\n                            logging.debug(\"Template updated during tracking.\")\n                else:\n                    self.frame_counter = 0  # Reset on failure\n                    if self.tracking_failure_start_time is None:\n                        # First failure, start timer\n                        self.tracking_failure_start_time = time.time()\n                        logging.warning(\"Tracking lost. Starting failure timer.\")\n                    else:\n                        elapsed_time = time.time() - self.tracking_failure_start_time\n                        if elapsed_time > Parameters.TRACKING_FAILURE_TIMEOUT:\n                            logging.error(\"Tracking lost for too long. Handling failure.\")\n                            self.tracking_started = False\n                            self.tracking_failure_start_time = None\n                        else:\n                            logging.warning(\n                                f\"Tracking lost. Attempting to recover. Elapsed time: {elapsed_time:.2f} seconds.\"\n                            )\n                            self.tracker.update_estimator_without_measurement()\n                            frame = self.tracker.draw_estimate(frame, tracking_successful=False)\n                            if self.following_active:\n                                await self.follow_target()\n                                await self.check_failsafe()\n\n                            redetect_result = self.handle_tracking_failure()\n                            if redetect_result:\n                                self.tracking_failure_start_time = None\n            \n            # Telemetry handling\n            if self.telemetry_handler.should_send_telemetry():\n                self.telemetry_handler.send_telemetry()\n\n            # Update current frame (raw + OSD)\n            self.current_frame = frame\n            self.video_handler.current_osd_frame = frame  # The \"processed\" frame with OSD\n\n            # Draw OSD elements\n            frame = self.osd_handler.draw_osd(frame)\n\n            # At this point, frame is the fully processed OSD frame.\n            # If GStreamer is enabled, stream the processed frame\n            if Parameters.ENABLE_GSTREAMER_STREAM and self.gstreamer_handler:\n                self.gstreamer_handler.stream_frame(frame)\n\n            # --- SINGLE-POINT RESIZING STEP ---\n            # Update the resized versions (raw vs. OSD) only once here\n            self.video_handler.update_resized_frames(\n                Parameters.STREAM_WIDTH, Parameters.STREAM_HEIGHT\n            )\n            # ----------------------------------\n\n        except Exception as e:\n            logging.exception(f\"Error in update_loop: {e}\")\n        return frame\n\n    def handle_tracking_failure(self):\n        \"\"\"\n        Handles tracking failure by attempting re-detection using the existing detector.\n        \"\"\"\n        if Parameters.USE_DETECTOR and Parameters.AUTO_REDETECT:\n            logging.info(\"Attempting to re-detect the target using the detector.\")\n            redetect_result = self.initiate_redetection()\n            if redetect_result[\"success\"]:\n                logging.info(\"Target re-detected and tracker re-initialized.\")\n            else:\n                logging.info(f\"Re-detection attempt failed. Retrying...\")\n            return redetect_result\n        return {\"success\": False, \"message\": \"Detector not enabled or auto-redetect off.\"}\n\n    async def check_failsafe(self):\n        if self.px4_interface.failsafe_active:\n            await self.handle_failsafe()\n            self.px4_interface.failsafe_active = False\n\n    async def handle_failsafe(self):\n        # For now, just disconnect PX4 so it reverts to default hold/flight mode\n        await self.disconnect_px4()\n\n    async def handle_key_input_async(self, key: int, frame: np.ndarray):\n        \"\"\"\n        Handles key inputs for toggling segmentation, toggling tracking, etc.\n        \"\"\"\n        if key == ord('y'):\n            self.toggle_segmentation()\n        elif key == ord('t'):\n            self.toggle_tracking(frame)\n        elif key == ord('d'):\n            self.initiate_redetection()\n        elif key == ord('f'):\n            await self.connect_px4()\n        elif key == ord('x'):\n            await self.disconnect_px4()\n        elif key == ord('c'):\n            self.cancel_activities()\n\n    def handle_key_input(self, key: int, frame: np.ndarray):\n        \"\"\"\n        Handles key inputs synchronously by creating an async task.\n        \"\"\"\n        asyncio.create_task(self.handle_key_input_async(key, frame))\n\n    def handle_user_click(self, x: int, y: int):\n        \"\"\"\n        Identifies the object clicked by the user for tracking within the segmented area.\n        \"\"\"\n        if not self.segmentation_active:\n            return\n\n        detections = self.segmentor.get_last_detections()\n        selected_bbox = self.identify_clicked_object(detections, x, y)\n        if selected_bbox:\n            selected_bbox = tuple(map(lambda a: int(round(a)), selected_bbox))\n            self.tracker.reinitialize_tracker(self.current_frame, selected_bbox)\n            self.tracking_started = True\n            logging.info(f\"Object selected for tracking: {selected_bbox}\")\n\n    def identify_clicked_object(self, detections: list, x: int, y: int) -> Tuple[int, int, int, int]:\n        \"\"\"\n        Identifies the clicked object based on segmentation detections and mouse click coords.\n        \"\"\"\n        for det in detections:\n            x1, y1, x2, y2 = det\n            if x1 <= x <= x2 and y1 <= y <= y2:\n                return det\n        return None\n\n    def initiate_redetection(self) -> Dict[str, any]:\n        \"\"\"\n        Attempts to re-detect the object being tracked using the existing detector,\n        focusing around the estimated position if available.\n        \"\"\"\n        if Parameters.USE_DETECTOR:\n            estimate = self.tracker.get_estimated_position()\n            if estimate:\n                estimated_x, estimated_y = estimate[:2]\n                search_radius = Parameters.REDETECTION_SEARCH_RADIUS\n                x_min = max(0, int(estimated_x - search_radius))\n                x_max = min(self.video_handler.width, int(estimated_x + search_radius))\n                y_min = max(0, int(estimated_y - search_radius))\n                y_max = min(self.video_handler.height, int(estimated_y + search_radius))\n                search_region = (x_min, y_min, x_max - x_min, y_max - y_min)\n                redetect_result = self.detector.smart_redetection(\n                    self.current_frame, self.tracker, roi=search_region\n                )\n            else:\n                # No estimated position, full frame\n                redetect_result = self.detector.smart_redetection(self.current_frame, self.tracker)\n\n            if redetect_result:\n                detected_bbox = self.detector.get_latest_bbox()\n                self.tracker.reinitialize_tracker(self.current_frame, detected_bbox)\n                self.tracking_started = True\n                logging.info(\"Re-detection successful and tracker re-initialized.\")\n                return {\n                    \"success\": True,\n                    \"message\": \"Re-detection successful and tracker re-initialized.\",\n                    \"bounding_box\": detected_bbox\n                }\n            else:\n                logging.info(\"Re-detection failed or no new object found.\")\n                return {\n                    \"success\": False,\n                    \"message\": \"Re-detection failed or no new object found.\"\n                }\n        else:\n            return {\n                \"success\": False,\n                \"message\": \"Detector is not enabled.\"\n            }\n\n    def show_current_frame(self, frame_title: str = Parameters.FRAME_TITLE) -> np.ndarray:\n        \"\"\"\n        Displays the current frame in a window if SHOW_VIDEO_WINDOW is True.\n        \"\"\"\n        if Parameters.SHOW_VIDEO_WINDOW:\n            cv2.imshow(frame_title, self.current_frame)\n        return self.current_frame\n\n    async def connect_px4(self) -> Dict[str, any]:\n        \"\"\"\n        Connects to PX4 when following mode is activated.\n        \"\"\"\n        result = {\"steps\": [], \"errors\": []}\n        if not self.following_active:\n            try:\n                logging.debug(\"Activating Follow Mode to PX4!\")\n                await self.px4_interface.connect()\n                logging.debug(\"Connected to PX4 Drone!\")\n\n                initial_target_coords = (\n                    tuple(self.tracker.normalized_center)\n                    if Parameters.TARGET_POSITION_MODE == 'initial'\n                    else tuple(Parameters.DESIRE_AIM)\n                )\n                self.follower = Follower(self.px4_interface, initial_target_coords)\n                self.telemetry_handler.follower = self.follower\n                await self.px4_interface.set_hover_throttle()\n                await self.px4_interface.send_initial_setpoint()\n                await self.px4_interface.start_offboard_mode()\n                self.following_active = True\n                result[\"steps\"].append(\"Offboard mode started.\")\n            except Exception as e:\n                logging.error(f\"Failed to connect/start offboard mode: {e}\")\n                result[\"errors\"].append(f\"Failed to connect/start offboard mode: {e}\")\n        else:\n            result[\"steps\"].append(\"Follow mode already active.\")\n        return result\n\n    async def disconnect_px4(self) -> Dict[str, any]:\n        \"\"\"\n        Disconnects PX4 and stops offboard mode.\n        \"\"\"\n        result = {\"steps\": [], \"errors\": []}\n        if self.following_active:\n            try:\n                await self.px4_interface.stop_offboard_mode()\n                result[\"steps\"].append(\"Offboard mode stopped.\")\n                self.following_active = False\n                if self.setpoint_sender:\n                    self.setpoint_sender.stop()\n                    self.setpoint_sender.join()\n                    self.setpoint_sender = None\n            except Exception as e:\n                logging.error(f\"Failed to stop offboard mode: {e}\")\n                result[\"errors\"].append(f\"Failed to stop offboard mode: {e}\")\n        else:\n            result[\"steps\"].append(\"Follow mode is not active.\")\n        return result\n\n    async def follow_target(self):\n        \"\"\"\n        Prepares to follow the target based on tracking information.\n        \"\"\"\n        if self.tracking_started and self.following_active:\n            target_coords: Optional[Tuple[float, float]] = None\n\n            # Check if estimator is enabled\n            if Parameters.USE_ESTIMATOR_FOR_FOLLOWING and self.tracker.position_estimator:\n                # get dimensions\n                frame_width, frame_height = self.video_handler.width, self.video_handler.height\n                normalized_estimate = self.tracker.position_estimator.get_normalized_estimate(\n                    frame_width, frame_height\n                )\n                if normalized_estimate:\n                    target_coords = normalized_estimate\n                    logging.debug(f\"Using estimated normalized coords: {target_coords}\")\n                else:\n                    logging.warning(\"Estimator failed to provide a normalized estimate.\")\n\n            if not target_coords:\n                # fallback to tracker's normalized center\n                target_coords = self.tracker.normalized_center\n                logging.debug(f\"Using tracker's normalized center: {target_coords}\")\n\n            if target_coords:\n                self.follower.follow_target(target_coords)\n                self.px4_interface.update_setpoint()\n\n                control_type = self.follower.get_control_type()\n                if control_type == 'attitude_rate':\n                    await self.px4_interface.send_attitude_rate_commands()\n                elif control_type == 'velocity_body':\n                    await self.px4_interface.send_body_velocity_commands()\n                else:\n                    logging.warning(f\"Unknown control type: {control_type}\")\n            else:\n                logging.warning(\"No target coordinates available to follow.\")\n            return True\n        else:\n            return False\n\n    async def shutdown(self) -> Dict[str, any]:\n        \"\"\"\n        Shuts down the application gracefully.\n        \"\"\"\n        result = {\"steps\": [], \"errors\": []}\n        try:\n            # Stop MAVLink polling\n            if Parameters.MAVLINK_ENABLED:\n                self.mavlink_data_manager.stop_polling()\n\n            if self.following_active:\n                logging.debug(\"Stopping offboard mode and disconnecting PX4.\")\n                await self.px4_interface.stop_offboard_mode()\n                if self.setpoint_sender:\n                    self.setpoint_sender.stop()\n                    self.setpoint_sender.join()\n                self.following_active = False\n\n            self.video_handler.release()\n            logging.debug(\"Video handler released.\")\n            result[\"steps\"].append(\"Shutdown complete.\")\n        except Exception as e:\n            logging.error(f\"Error during shutdown: {e}\")\n            result[\"errors\"].append(f\"Error during shutdown: {e}\")\n        return result\n"}
{"type": "source_file", "path": "src/classes/fastapi_handler.py", "content": "# src/classes/fastapi_handler.py\n\nfrom fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect\nfrom pydantic import BaseModel\nfrom fastapi.responses import StreamingResponse, JSONResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nimport asyncio\nimport cv2\nimport logging\nimport time\nfrom classes.parameters import Parameters\nimport uvicorn\nfrom typing import Dict\nfrom classes.webrtc_manager import WebRTCManager  # Import the WebRTCManager\n\nclass BoundingBox(BaseModel):\n    x: float\n    y: float\n    width: float\n    height: float\n\nclass FastAPIHandler:\n    def __init__(self, app_controller):\n        \"\"\"\n        Initialize the FastAPIHandler with necessary dependencies and settings.\n\n        Args:\n            app_controller (AppController): An instance of the AppController class.\n        \"\"\"\n        # Dependencies\n        self.app_controller = app_controller\n        self.video_handler = app_controller.video_handler\n        self.telemetry_handler = app_controller.telemetry_handler\n\n        # Initialize WebRTC Manager\n        self.webrtc_manager = WebRTCManager(self.video_handler)\n\n        # FastAPI app initialization\n        self.app = FastAPI()\n        self.app.add_middleware(\n            CORSMiddleware,\n            allow_origins=[\"*\"],  # Update as needed for security\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n\n        # Setup logging\n        self.logger = logging.getLogger(__name__)\n        self.logger.setLevel(logging.INFO)  # Adjust the logging level as needed\n\n        # Define API routes\n        self.define_routes()\n\n        # Video streaming parameters\n        self.frame_rate = Parameters.STREAM_FPS\n        self.width = Parameters.STREAM_WIDTH\n        self.height = Parameters.STREAM_HEIGHT\n        self.quality = Parameters.STREAM_QUALITY\n        self.processed_osd = Parameters.STREAM_PROCESSED_OSD\n        self.frame_interval = 1.0 / self.frame_rate\n\n        # State variables\n        self.is_shutting_down = False\n        self.server = None\n\n        # Lock for thread-safe frame access\n        self.frame_lock = asyncio.Lock()\n\n        # For frame skipping/capping\n        self.last_http_send_time = 0.0\n        self.last_ws_send_time = 0.0\n\n    def define_routes(self):\n        \"\"\"\n        Define all the API routes for the FastAPIHandler.\n        \"\"\"\n        # HTTP streaming endpoint\n        self.app.get(\"/video_feed\")(self.video_feed)\n        # WebSocket streaming endpoint\n        self.app.websocket(\"/ws/video_feed\")(self.video_feed_websocket)\n        # WebRTC Signaling endpoint\n        self.app.websocket(\"/ws/webrtc_signaling\")(self.webrtc_manager.signaling_handler)\n\n        # Telemetry endpoints\n        self.app.get(\"/telemetry/tracker_data\")(self.tracker_data)\n        self.app.get(\"/telemetry/follower_data\")(self.follower_data)\n\n        # Command endpoints\n        self.app.post(\"/commands/start_tracking\")(self.start_tracking)\n        self.app.post(\"/commands/stop_tracking\")(self.stop_tracking)\n        self.app.post(\"/commands/toggle_segmentation\")(self.toggle_segmentation)\n        self.app.post(\"/commands/redetect\")(self.redetect)\n        self.app.post(\"/commands/cancel_activities\")(self.cancel_activities)\n        self.app.post(\"/commands/start_offboard_mode\")(self.start_offboard_mode)\n        self.app.post(\"/commands/stop_offboard_mode\")(self.stop_offboard_mode)\n        self.app.post(\"/commands/quit\")(self.quit)\n\n    async def start_tracking(self, bbox: BoundingBox):\n        \"\"\"\n        Endpoint to start tracking with the provided bounding box.\n\n        Args:\n            bbox (BoundingBox): The bounding box for tracking.\n\n        Returns:\n            dict: Status of the operation.\n        \"\"\"\n        try:\n            width = self.video_handler.width\n            height = self.video_handler.height\n\n            # Normalize bounding box if values are between 0 and 1\n            if all(0 <= value <= 1 for value in [bbox.x, bbox.y, bbox.width, bbox.height]):\n                bbox_pixels = {\n                    'x': int(bbox.x * width),\n                    'y': int(bbox.y * height),\n                    'width': int(bbox.width * width),\n                    'height': int(bbox.height * height)\n                }\n                self.logger.debug(f\"Received normalized bbox, converting to pixels: {bbox_pixels}\")\n            else:\n                bbox_pixels = bbox.dict()\n                self.logger.debug(f\"Received raw pixel bbox: {bbox_pixels}\")\n\n            # Start tracking using the app controller\n            await self.app_controller.start_tracking(bbox_pixels)\n            return {\"status\": \"Tracking started\", \"bbox\": bbox_pixels}\n        except Exception as e:\n            self.logger.error(f\"Error in start_tracking: {e}\")\n            raise HTTPException(status_code=500, detail=str(e))\n\n    async def stop_tracking(self):\n        \"\"\"\n        Endpoint to stop tracking.\n\n        Returns:\n            dict: Status of the operation.\n        \"\"\"\n        try:\n            await self.app_controller.stop_tracking()\n            return {\"status\": \"Tracking stopped\"}\n        except Exception as e:\n            self.logger.error(f\"Error in stop_tracking: {e}\")\n            raise HTTPException(status_code=500, detail=str(e))\n\n    async def video_feed(self):\n        \"\"\"\n        FastAPI route to serve the video feed over HTTP as an MJPEG stream.\n        \"\"\"\n        async def generate():\n            while not self.is_shutting_down:\n                # Simple capping: ensure we don't exceed STREAM_FPS\n                current_time = time.time()\n                if (current_time - self.last_http_send_time) < self.frame_interval:\n                    # We are still within the interval; skip frame sending\n                    await asyncio.sleep(0.001)\n                    continue\n                self.last_http_send_time = current_time\n\n                # Lock to safely access frames\n                async with self.frame_lock:\n                    # Select the proper resized frame\n                    frame = (self.video_handler.current_resized_osd_frame\n                             if self.processed_osd\n                             else self.video_handler.current_resized_raw_frame)\n\n                    if frame is None:\n                        self.logger.warning(\"No frame available to send (HTTP)\")\n                        break\n\n                    ret, buffer = cv2.imencode('.jpg', frame, [int(cv2.IMWRITE_JPEG_QUALITY), self.quality])\n                    if ret:\n                        frame_bytes = buffer.tobytes()\n                        # Yield MJPEG frame\n                        yield (b'--frame\\r\\n'\n                               b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame_bytes + b'\\r\\n')\n                    else:\n                        self.logger.error(\"Failed to encode frame (HTTP)\")\n\n        return StreamingResponse(generate(), media_type='multipart/x-mixed-replace; boundary=frame')\n\n    async def video_feed_websocket(self, websocket: WebSocket):\n        \"\"\"\n        WebSocket endpoint to stream video frames (MJPEG over WebSocket).\n        \"\"\"\n        await websocket.accept()\n        self.logger.info(f\"WebSocket connection accepted: {websocket.client}\")\n        try:\n            while not self.is_shutting_down:\n                # Simple capping: ensure we don't exceed STREAM_FPS\n                current_time = time.time()\n                if (current_time - self.last_ws_send_time) < self.frame_interval:\n                    await asyncio.sleep(0.001)\n                    continue\n                self.last_ws_send_time = current_time\n\n                async with self.frame_lock:\n                    frame = (self.video_handler.current_resized_osd_frame\n                             if self.processed_osd\n                             else self.video_handler.current_resized_raw_frame)\n                    if frame is not None:\n                        ret, buffer = cv2.imencode('.jpg', frame, [int(cv2.IMWRITE_JPEG_QUALITY), self.quality])\n                        if ret:\n                            await websocket.send_bytes(buffer.tobytes())\n                        else:\n                            self.logger.error(\"Failed to encode frame (WebSocket)\")\n                    else:\n                        self.logger.warning(\"No frame available to send (WebSocket)\")\n\n        except WebSocketDisconnect:\n            self.logger.info(f\"WebSocket disconnected: {websocket.client}\")\n        except Exception as e:\n            self.logger.error(f\"Error in WebSocket video feed: {e}\")\n            await websocket.close()\n\n    async def tracker_data(self):\n        \"\"\"\n        FastAPI route to provide tracker telemetry data.\n\n        Returns:\n            JSONResponse: The latest tracker data.\n        \"\"\"\n        try:\n            self.logger.debug(\"Received request at /telemetry/tracker_data\")\n            tracker_data = self.telemetry_handler.latest_tracker_data\n            self.logger.debug(f\"Returning tracker data: {tracker_data}\")\n            return JSONResponse(content=tracker_data or {})\n        except Exception as e:\n            self.logger.error(f\"Error in /telemetry/tracker_data: {e}\")\n            raise HTTPException(status_code=500, detail=str(e))\n\n    async def follower_data(self):\n        \"\"\"\n        FastAPI route to provide follower telemetry data.\n\n        Returns:\n            JSONResponse: The latest follower data.\n        \"\"\"\n        try:\n            self.logger.debug(\"Received request at /telemetry/follower_data\")\n            follower_data = self.telemetry_handler.latest_follower_data\n            self.logger.debug(f\"Returning follower data: {follower_data}\")\n            return JSONResponse(content=follower_data or {})\n        except Exception as e:\n            self.logger.error(f\"Error in /telemetry/follower_data: {e}\")\n            raise HTTPException(status_code=500, detail=str(e))\n\n    async def toggle_segmentation(self):\n        \"\"\"\n        Endpoint to toggle segmentation state (enable/disable YOLO).\n\n        Returns:\n            dict: Status of the operation and the current state of segmentation.\n        \"\"\"\n        try:\n            current_state = self.app_controller.toggle_segmentation()\n            return {\"status\": \"success\", \"segmentation_active\": current_state}\n        except Exception as e:\n            self.logger.error(f\"Error in toggle_segmentation: {e}\")\n            raise HTTPException(status_code=500, detail=str(e))\n\n    async def redetect(self):\n        \"\"\"\n        Endpoint to attempt redetection of the object being tracked.\n\n        Returns:\n            dict: Status of the operation and details of the redetection attempt.\n        \"\"\"\n        try:\n            result = self.app_controller.initiate_redetection()\n            return {\"status\": \"success\", \"detection_result\": result}\n        except Exception as e:\n            self.logger.error(f\"Error in redetect: {e}\")\n            raise HTTPException(status_code=500, detail=str(e))\n\n    async def cancel_activities(self):\n        \"\"\"\n        Endpoint to cancel all active tracking and segmentation activities.\n\n        Returns:\n            dict: Status of the operation.\n        \"\"\"\n        try:\n            self.app_controller.cancel_activities()\n            return {\"status\": \"success\"}\n        except Exception as e:\n            self.logger.error(f\"Error in cancel_activities: {e}\")\n            raise HTTPException(status_code=500, detail=str(e))\n\n    async def start_offboard_mode(self):\n        \"\"\"\n        Endpoint to start the offboard mode for PX4.\n\n        Returns:\n            dict: Status of the operation and details of the process.\n        \"\"\"\n        try:\n            result = await self.app_controller.connect_px4()\n            return {\"status\": \"success\", \"details\": result}\n        except Exception as e:\n            self.logger.error(f\"Error in start_offboard_mode: {e}\")\n            return {\"status\": \"failure\", \"error\": str(e)}\n\n    async def stop_offboard_mode(self):\n        \"\"\"\n        Endpoint to stop the offboard mode for PX4.\n\n        Returns:\n            dict: Status of the operation and details of the process.\n        \"\"\"\n        try:\n            result = await self.app_controller.disconnect_px4()\n            return {\"status\": \"success\", \"details\": result}\n        except Exception as e:\n            self.logger.error(f\"Error in stop_offboard_mode: {e}\")\n            return {\"status\": \"failure\", \"error\": str(e)}\n\n    async def quit(self):\n        \"\"\"\n        Endpoint to quit the application.\n\n        Returns:\n            dict: Status of the operation and details of the process.\n        \"\"\"\n        try:\n            self.logger.info(\"Received request to quit the application.\")\n            asyncio.create_task(self.app_controller.shutdown())\n            if self.server:\n                self.server.should_exit = True\n            return {\"status\": \"success\", \"details\": \"Application is shutting down.\"}\n        except Exception as e:\n            self.logger.error(f\"Error in quit: {e}\")\n            return {\"status\": \"failure\", \"error\": str(e)}\n\n    async def start(self, host='0.0.0.0', port=Parameters.HTTP_STREAM_PORT):\n        \"\"\"\n        Start the FastAPI server using uvicorn.\n\n        Args:\n            host (str): The hostname to listen on.\n            port (int): The port to listen on.\n        \"\"\"\n        config = uvicorn.Config(self.app, host=host, port=port, log_level=\"info\")\n        self.server = uvicorn.Server(config)\n        self.logger.info(f\"Starting FastAPI server on {host}:{port}\")\n        await self.server.serve()\n\n    async def stop(self):\n        \"\"\"\n        Stop the FastAPI server.\n        \"\"\"\n        if self.server:\n            self.logger.info(\"Stopping FastAPI server...\")\n            self.server.should_exit = True\n            await self.server.shutdown()\n            self.logger.info(\"Stopped FastAPI server\")\n"}
{"type": "source_file", "path": "src/classes/detectors/detector_factory.py", "content": "# src/classes/detectors/detector_factory.py\n\nimport logging\nfrom typing import Optional\n\nfrom .base_detector import BaseDetector\nfrom .template_matching_detector import TemplateMatchingDetector\nfrom classes.parameters import Parameters\n\nlogger = logging.getLogger(__name__)\n\ndef create_detector(algorithm_type: str) -> Optional[BaseDetector]:\n    \"\"\"\n    Factory method to create a detector based on the specified algorithm type.\n\n    Args:\n        algorithm_type (str): The type of detection algorithm to use.\n\n    Returns:\n        Optional[BaseDetector]: An instance of the chosen detector class or None if unsupported.\n    \"\"\"\n\n    if algorithm_type == \"TemplateMatching\":\n        logger.info(\"Initialized with TemplateMatching detector.\")\n        return TemplateMatchingDetector()\n\n\n    else:\n        logger.error(f\"Unsupported algorithm type: {algorithm_type}\")\n        return None\n"}
{"type": "source_file", "path": "src/classes/followers/ground_target_follower.py", "content": "from classes.followers.base_follower import BaseFollower\nfrom classes.followers.custom_pid import CustomPID\nfrom classes.parameters import Parameters\nimport logging\nfrom datetime import datetime\nfrom typing import Tuple, Dict\n\nlogger = logging.getLogger(__name__)\n\nclass GroundTargetFollower(BaseFollower):\n    \"\"\"\n    GroundTargetFollower class manages PID control to track a target on the ground using a drone.\n    It utilizes advanced PID features such as Proportional on Measurement and Anti-Windup.\n    \"\"\"\n    def __init__(self, px4_controller, initial_target_coords: Tuple[float, float]):\n        \"\"\"\n        Initializes the GroundTargetFollower with the given PX4 controller and initial target coordinates.\n\n        Args:\n            px4_controller (PX4Controller): Instance of PX4Controller to control the drone.\n            initial_target_coords (tuple): Initial target coordinates to set for the follower.\n        \"\"\"\n        super().__init__(px4_controller, \"Ground View\")  # Initialize with \"Ground View\" profile\n        self.target_position_mode = Parameters.TARGET_POSITION_MODE\n        self.initial_target_coords = initial_target_coords if self.target_position_mode == 'initial' else (0, 0)\n        self.initialize_pids()\n\n    def initialize_pids(self):\n        \"\"\"Initializes the PID controllers based on the initial target coordinates.\"\"\"\n        setpoint_x, setpoint_y = self.initial_target_coords\n        self.pid_x = CustomPID(\n            *self.get_pid_gains('x'), \n            setpoint=setpoint_x, \n            output_limits=(-Parameters.VELOCITY_LIMITS['x'], Parameters.VELOCITY_LIMITS['x'])\n        )\n        self.pid_y = CustomPID(\n            *self.get_pid_gains('y'), \n            setpoint=setpoint_y, \n            output_limits=(-Parameters.VELOCITY_LIMITS['y'], Parameters.VELOCITY_LIMITS['y'])\n        )\n        self.pid_z = CustomPID(\n            *self.get_pid_gains('z'), \n            setpoint=Parameters.MIN_DESCENT_HEIGHT, \n            output_limits=(-Parameters.MAX_RATE_OF_DESCENT, Parameters.MAX_RATE_OF_DESCENT)\n        )\n        logger.info(\"PID controllers initialized for GroundTargetFollower.\")\n\n    def get_pid_gains(self, axis: str) -> Tuple[float, float, float]:\n        \"\"\"Retrieves the PID gains based on the current altitude from the PX4Controller, applying gain scheduling if enabled.\"\"\"\n        if Parameters.ENABLE_GAIN_SCHEDULING:\n            current_value = getattr(self.px4_controller, Parameters.GAIN_SCHEDULING_PARAMETER, None)\n            if current_value is None:\n                logger.error(f\"Parameter {Parameters.GAIN_SCHEDULING_PARAMETER} not available in PX4Controller.\")\n                return Parameters.PID_GAINS[axis]['p'], Parameters.PID_GAINS[axis]['i'], Parameters.PID_GAINS[axis]['d']\n            \n            for (lower_bound, upper_bound), gains in Parameters.ALTITUDE_GAIN_SCHEDULE.items():\n                if lower_bound <= current_value < upper_bound:\n                    return gains[axis]['p'], gains[axis]['i'], gains[axis]['d']\n        \n        return Parameters.PID_GAINS[axis]['p'], Parameters.PID_GAINS[axis]['i'], Parameters.PID_GAINS[axis]['d']\n\n    def update_pid_gains(self):\n        \"\"\"Updates the PID gains based on current settings and altitude.\"\"\"\n        self.pid_x.tunings = self.get_pid_gains('x')\n        self.pid_y.tunings = self.get_pid_gains('y')\n        self.pid_z.tunings = self.get_pid_gains('z')\n        logger.debug(\"PID gains updated for GroundTargetFollower.\")\n\n    def apply_gimbal_corrections(self, target_coords: Tuple[float, float]) -> Tuple[float, float]:\n        \"\"\"\n        Applies orientation-based adjustments if the camera is not gimbaled.\n\n        Args:\n            target_coords (tuple): The target coordinates from image processing.\n\n        Returns:\n            tuple: Adjusted target coordinates considering gimbal corrections.\n        \"\"\"\n        if Parameters.IS_CAMERA_GIMBALED:\n            return target_coords\n\n        orientation = self.px4_controller.get_orientation()  # (yaw, pitch, roll)\n        roll = orientation[2]\n        pitch = orientation[1]\n\n        adjusted_target_x = target_coords[0] + Parameters.BASE_ADJUSTMENT_FACTOR_X * roll\n        adjusted_target_y = target_coords[1] - Parameters.BASE_ADJUSTMENT_FACTOR_Y * pitch\n\n        return adjusted_target_x, adjusted_target_y\n\n    def apply_adjustment_factors(self, adjusted_target_x: float, adjusted_target_y: float) -> Tuple[float, float]:\n        \"\"\"\n        Applies dynamic adjustment factors based on the altitude.\n\n        Args:\n            adjusted_target_x (float): The adjusted target x-coordinate.\n            adjusted_target_y (float): The adjusted target y-coordinate.\n\n        Returns:\n            tuple: Further adjusted target coordinates.\n        \"\"\"\n        current_altitude = self.px4_controller.current_altitude\n        adj_factor_x = Parameters.BASE_ADJUSTMENT_FACTOR_X / (1 + Parameters.ALTITUDE_FACTOR * current_altitude)\n        adj_factor_y = Parameters.BASE_ADJUSTMENT_FACTOR_Y / (1 + Parameters.ALTITUDE_FACTOR * current_altitude)\n\n        adjusted_target_x += adj_factor_x\n        adjusted_target_y += adj_factor_y\n\n        return adjusted_target_x, adjusted_target_y\n\n    def calculate_control_commands(self, target_coords: Tuple[float, float]) -> None:\n        \"\"\"Calculates and updates velocity commands based on the target coordinates.\"\"\"\n        self.update_pid_gains()\n\n        adjusted_target_x, adjusted_target_y = self.apply_gimbal_corrections(target_coords)\n        adjusted_target_x, adjusted_target_y = self.apply_adjustment_factors(adjusted_target_x, adjusted_target_y)\n\n        # Calculate errors\n        error_x = self.pid_x.setpoint - adjusted_target_x\n        error_y = self.pid_y.setpoint - (-1) * adjusted_target_y\n        \n        # Applying the PID control where error_y is used for vel_x and error_x for vel_y due to axis differences\n        vel_x = self.pid_y(error_y)  # error_y controls vel_x due to coordinate system differences\n        vel_y = self.pid_x(error_x)  # error_x controls vel_y due to coordinate system differences\n        vel_z = self.control_descent()\n        \n        # Update setpoint handler with calculated velocities\n        self.px4_controller.setpoint_handler.set_field('vel_x', vel_x)\n        self.px4_controller.setpoint_handler.set_field('vel_y', vel_y)\n        self.px4_controller.setpoint_handler.set_field('vel_z', vel_z)\n        logger.debug(f\"Velocity commands calculated: vel_x={vel_x}, vel_y={vel_y}, vel_z={vel_z}\")\n\n    def follow_target(self, target_coords: Tuple[float, float]):\n        \"\"\"Calculates and sends velocity commands to follow a target based on its coordinates.\"\"\"\n        self.calculate_control_commands(target_coords)\n        #await self.px4_controller.send_body_velocity_commands(self.setpoint_handler.get_fields())\n        logger.info(f\"Following target at coordinates: {target_coords}\")\n\n    def control_descent(self) -> float:\n        \"\"\"\n        Controls the descent of the drone based on current altitude, ensuring it doesn't go below the minimum descent height.\n        \"\"\"\n        if not Parameters.ENABLE_DESCEND_TO_TARGET:\n            logging.info(\"Descending to target is disabled.\")\n            return 0\n\n        current_altitude = self.px4_controller.current_altitude\n        logging.debug(f\"Current Altitude: {current_altitude}m, Minimum Descent Height: {Parameters.MIN_DESCENT_HEIGHT}m\")\n\n        if current_altitude > Parameters.MIN_DESCENT_HEIGHT:\n            return self.pid_z(-current_altitude)\n        else:\n            logging.info(\"Altitude is at or below the minimum descent height. Descent halted.\")\n            return 0\n"}
{"type": "source_file", "path": "src/classes/setpoint_sender.py", "content": "#src/classes/setpoint_sender.py\nimport asyncio\nimport threading\nimport time\nfrom classes.parameters import Parameters\nfrom classes.setpoint_handler import SetpointHandler\n\nclass SetpointSender(threading.Thread):\n    def __init__(self, px4_controller, setpoint_handler: SetpointHandler):\n        super().__init__(daemon=True)\n        self.px4_controller = px4_controller\n        self.setpoint_handler = setpoint_handler  # Inject the SetpointHandler\n        self.running = True\n        \n\n    def run(self):\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        while self.running:\n            # Fetch the current setpoints from the SetpointHandler\n            setpoints = self.setpoint_handler.get_fields()\n\n            # Send the velocity commands to the PX4 controller using the setpoints\n            #TODO: Depends on the profile of follower setpoint we might need to use other mavsdk offabord (pitch rate, yaw rate, rollrate , thrust , ...)\n            loop.run_until_complete(self.px4_controller.send_body_velocity_commands())\n\n            if Parameters.ENABLE_SETPOINT_DEBUGGING:\n                self.print_current_setpoint(setpoints)\n                \n            time.sleep(Parameters.SETPOINT_PUBLISH_RATE_S)\n        loop.close()\n\n    def print_current_setpoint(self, setpoints):\n        \"\"\"Prints the current setpoints for debugging purposes.\"\"\"\n        if setpoints:\n            print(f\"Sending body velocity commands: {setpoints}\")\n\n    def stop(self):\n        self.running = False\n        self.join()  # Wait for the thread to finish\n\n\n"}
{"type": "source_file", "path": "src/classes/mavlink_data_manager.py", "content": "from collections import deque\nimport threading\nimport time\nfrom numpy import uint16\nimport requests\nimport logging\nimport asyncio\nfrom .parameters import Parameters\nimport math\n\nclass MavlinkDataManager:\n    def __init__(self, mavlink_host, mavlink_port, polling_interval, data_points, enabled=True):\n        \"\"\"\n        Initialize the MavlinkDataManager with necessary parameters.\n\n        Args:\n            mavlink_host (str): The host address of the MAVLink server.\n            mavlink_port (int): The port number of the MAVLink server.\n            polling_interval (int): The interval at which data should be polled in seconds.\n            data_points (dict): A dictionary of data points to extract from MAVLink.\n            enabled (bool): Whether the polling should be enabled or not.\n        \"\"\"\n        self.mavlink_host = mavlink_host\n        self.mavlink_port = mavlink_port\n        self.polling_interval = polling_interval\n        self.data_points = data_points  # Dictionary of data points to extract\n        self.enabled = enabled\n        self.data = {}  # Stores the fetched data\n        self._stop_event = threading.Event()\n        self._lock = threading.Lock()\n        self.velocity_buffer = deque(maxlen=10)  # Buffer for velocity smoothing\n        self.min_velocity_threshold = 0.5  # m/s, adjust based on your drone's characteristics\n        self.gamma = 0\n\n        # Setup logging\n        logging.basicConfig(level=logging.DEBUG)  # Set to DEBUG for detailed logging\n        self.logger = logging.getLogger(__name__)\n\n    def start_polling(self):\n        \"\"\"\n        Start the polling thread if enabled.\n        \"\"\"\n        if self.enabled:\n            self._thread = threading.Thread(target=self._poll_data)\n            self._thread.start()\n            self.logger.info(\"Started polling MAVLink data.\")\n        else:\n            self.logger.info(\"MAVLink polling is disabled.\")\n\n    def stop_polling(self):\n        \"\"\"\n        Stop the polling thread.\n        \"\"\"\n        if self.enabled:\n            self._stop_event.set()\n            self._thread.join()\n            self.logger.info(\"Stopped polling MAVLink data.\")\n\n    def _poll_data(self):\n        \"\"\"\n        Poll data at regular intervals and store it in a dictionary.\n        \"\"\"\n        while not self._stop_event.is_set():\n            self._fetch_and_parse_all_data()\n            time.sleep(self.polling_interval)\n\n    def _fetch_and_parse_all_data(self):\n        \"\"\"\n        Fetch all MAVLink data in a single request and parse it into the data dictionary.\n        \"\"\"\n        try:\n            url = f\"http://{self.mavlink_host}:{self.mavlink_port}/v1/mavlink\"\n            response = requests.get(url)\n            response.raise_for_status()\n            json_data = response.json()\n\n            with self._lock:\n                # Iterate through the data points defined in Parameters\n                for point_name, json_path in self.data_points.items():\n                    if point_name in [\"vn\", \"ve\", \"vd\"]:\n                        value = self._extract_data_from_json(json_data, json_path)\n                        self.data[point_name] = float(value) if value is not None else 0.0\n                    elif point_name == \"flight_path_angle\":\n                        self.data[point_name] = self._calculate_flight_path_angle()\n                        self.gamma = self.data[point_name] #temporary we might need this\n                    elif point_name == \"arm_status\":\n                        base_mode = self._extract_data_from_json(json_data, \"/vehicles/1/components/191/messages/HEARTBEAT/message/base_mode/bits\")\n                        self.data[point_name] = self._determine_arm_status(base_mode)\n                    else:\n                        value = self._extract_data_from_json(json_data, json_path)\n                        if value is None:\n                            #self.logger.warning(f\"Failed to retrieve data for {point_name} using path {json_path}. Assigning 'N/A'.\")\n                            value = \"N/A\"\n                        else:\n                            if point_name in [\"latitude\", \"longitude\"]:\n                                try:\n                                    value = float(value) / 1e7\n                                except (ValueError, TypeError) as e:\n                                    self.logger.error(f\"Failed to convert {point_name} value to float for division: {e}\")\n                                    value = \"N/A\"\n                        self.data[point_name] = value\n        except requests.RequestException as e:\n            self.logger.error(f\"Error fetching data from {url}: {e}\")\n            \n            \n    def _calculate_flight_path_angle(self):\n        vn = self.data.get(\"vn\", 0.0)\n        ve = self.data.get(\"ve\", 0.0)\n        vd = self.data.get(\"vd\", 0.0)\n\n        # Calculate total velocity\n        v_total = math.sqrt(vn**2 + ve**2 + vd**2)\n\n        # Add to buffer for smoothing\n        self.velocity_buffer.append((vn, ve, vd))\n\n        # Calculate average velocity over the buffer\n        avg_vn = sum(v[0] for v in self.velocity_buffer) / len(self.velocity_buffer)\n        avg_ve = sum(v[1] for v in self.velocity_buffer) / len(self.velocity_buffer)\n        avg_vd = sum(v[2] for v in self.velocity_buffer) / len(self.velocity_buffer)\n\n        # Calculate horizontal speed using averaged values\n        v_horizontal = math.sqrt(avg_vn**2 + avg_ve**2)\n\n        # Check if the total velocity is above the threshold\n        if v_total < self.min_velocity_threshold:\n            return 0.0  # Return 0 when the drone is effectively stationary\n\n        # Calculate flight path angle using averaged values\n        if v_horizontal == 0:\n            return 90.0 if avg_vd < 0 else -90.0  # Vertical up or down\n\n        flight_path_angle = math.degrees(math.atan2(-avg_vd, v_horizontal))\n\n        # Apply additional smoothing\n        flight_path_angle = round(flight_path_angle, 1)  # Round to one decimal place\n\n        return flight_path_angle\n\n\n    def _determine_arm_status(self, base_mode_bits):\n        \"\"\"\n        Determine if the system is armed based on the base_mode bits.\n\n        Args:\n            base_mode_bits (int): The base mode bits from the MAVLink HEARTBEAT message.\n\n        Returns:\n            str: \"Armed\" if the system is armed, otherwise \"Disarmed\".\n        \"\"\"\n        ARM_BIT_MASK = 128  # Example mask, update with the correct one\n        if base_mode_bits is None:\n            return \"Unknown\"\n        return \"Armed\" if base_mode_bits & ARM_BIT_MASK else \"Disarmed\"\n\n    def _extract_data_from_json(self, data, json_path):\n        \"\"\"\n        Helper function to extract a value from a nested JSON structure using a json_path.\n\n        Args:\n            data (dict): The JSON data dictionary.\n            json_path (str): The path to the desired data within the JSON.\n\n        Returns:\n            any: The value found at the JSON path, or None if not found.\n        \"\"\"\n        keys = json_path.strip(\"/\").split('/')\n        for key in keys:\n            if key in data:\n                data = data[key]\n            else:\n                #self.logger.debug(f\"Key {key} not found in the current JSON level.\")\n                return None\n        return data\n\n    def get_data(self, point):\n        \"\"\"\n        Retrieve the most recent data for a specified point.\n\n        Args:\n            point (str): The name of the data point to retrieve.\n\n        Returns:\n            any: The latest value of the specified data point, or \"N/A\" if not available.\n        \"\"\"\n        with self._lock:\n            if not self.enabled:\n                return None  # Avoid unnecessary logging and simply return None\n            return self.data.get(point, 0)\n\n    async def fetch_data_from_uri(self, uri):\n        \"\"\"\n        Fetch data from a specific URI and return the parsed JSON response.\n\n        Args:\n            uri (str): The URI to fetch the data from.\n\n        Returns:\n            dict: The parsed JSON data from the response.\n        \"\"\"\n        url = f\"http://{self.mavlink_host}:{self.mavlink_port}{uri}\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            self.logger.error(f\"Error fetching data from {url}: {e}\")\n            return None\n\n    async def fetch_attitude_data(self):\n        \"\"\"\n        Fetch attitude data (roll, pitch, yaw) from MAVLink2Rest.\n\n        These values are specifically for follower usage and are independent of OSD data.\n        If USE_MAVLINK2REST = True is enabled in parameters, these values are required.\n        In case REST requests send invalid data, the roll, pitch, and yaw will fall back to 0.\n\n        Returns:\n            dict: A dictionary with roll, pitch, and yaw values in degrees.\n        \"\"\"\n        attitude_data = await self.fetch_data_from_uri(\"/v1/mavlink/vehicles/1/components/1/messages/ATTITUDE\")\n        if attitude_data:\n            message = attitude_data.get(\"message\", {})\n            try:\n                roll = float(message.get(\"roll\", 0))\n                pitch = float(message.get(\"pitch\", 0))\n                yaw = float(message.get(\"yaw\", 0))\n\n                # Convert radians to degrees\n                roll_deg = math.degrees(roll)\n                pitch_deg = math.degrees(pitch)\n                yaw_deg = math.degrees(yaw)\n                \n            except (ValueError, TypeError):\n                self.logger.warning(\"Invalid attitude data received, falling back to default values (0).\")\n                roll_deg, pitch_deg, yaw_deg = 0, 0, 0\n\n            return {\"roll\": roll_deg, \"pitch\": pitch_deg, \"yaw\": yaw_deg}\n        \n        return {\"roll\": 0, \"pitch\": 0, \"yaw\": 0}\n\n    async def fetch_altitude_data(self):\n        \"\"\"\n        Fetch altitude data from MAVLink2Rest.\n\n        These values are specifically for follower usage and are independent of OSD data.\n        If USE_MAVLINK2REST = True is enabled in parameters, these values are required.\n        In case REST requests send invalid data, the altitude_relative will fall back to Parameters.MIN_DESCENT_HEIGHT.\n\n        Returns:\n            dict: A dictionary with relative and AMSL altitudes.\n        \"\"\"\n        altitude_data = await self.fetch_data_from_uri(\"/v1/mavlink/vehicles/1/components/1/messages/ALTITUDE\")\n        if altitude_data:\n            message = altitude_data.get(\"message\", {})\n            try:\n                altitude_relative = float(message.get(\"altitude_relative\", Parameters.MIN_DESCENT_HEIGHT))\n                altitude_amsl = float(message.get(\"altitude_amsl\", Parameters.MIN_DESCENT_HEIGHT))\n            except (ValueError, TypeError):\n                self.logger.warning(\"Invalid altitude data received, falling back to default values (Parameters.MIN_DESCENT_HEIGHT).\")\n                altitude_relative, altitude_amsl = Parameters.MIN_DESCENT_HEIGHT, Parameters.MIN_DESCENT_HEIGHT\n            return {\"altitude_relative\": altitude_relative, \"altitude_amsl\": altitude_amsl}\n        return {\"altitude_relative\": Parameters.MIN_DESCENT_HEIGHT, \"altitude_amsl\": Parameters.MIN_DESCENT_HEIGHT}\n\n    async def fetch_ground_speed(self):\n        \"\"\"\n        Fetch ground speed data from MAVLink2Rest. (onyl speed in horizontal plane)\n\n        This value is critical for calculating the drone's speed over the ground, which is important for various control algorithms.\n\n        Returns:\n            float: The ground speed in m/s.\n        \"\"\"\n        velocity_data = await self.fetch_data_from_uri(\"/v1/mavlink/vehicles/1/components/1/messages/LOCAL_POSITION_NED\")\n        if velocity_data:\n            message = velocity_data.get(\"message\", {})\n            try:\n                vx = float(message.get(\"vx\", 0))\n                vy = float(message.get(\"vy\", 0))\n                ground_speed = float(math.sqrt(vx**2+vy**2))\n            except (ValueError, TypeError):\n                self.logger.warning(\"Invalid ground speed data received, falling back to 0.\")\n                ground_speed = 0.0\n            return ground_speed\n        return 0.0\n    \n    async def fetch_throttle_percent(self):\n        \"\"\"\n        Fetch throttle percent data from MAVLink2Rest.\n\n        This value is critical for calculating the drone's inital throttle when switching to offboard.\n\n        Returns:\n            uint16_t: Current throttle setting (0 to 100).\n        \"\"\"\n        throttle_data = await self.fetch_data_from_uri(\"/v1/mavlink/vehicles/1/components/1/messages/VFR_HUD\")\n        if throttle_data:\n            message = throttle_data.get(\"message\", {})\n            try:\n                throttle_percent = uint16(message.get(\"throttle\", 50))\n            except (ValueError, TypeError):\n                self.logger.warning(\"Invalid throttle data received, falling back to 50.\")\n                throttle_percent = uint16(50)\n            return throttle_percent\n        return 0.0\n"}
{"type": "source_file", "path": "src/classes/segmentor.py", "content": "import cv2\nimport numpy as np\nfrom .parameters import Parameters  \nfrom ultralytics import YOLO\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass Segmentor:\n    def __init__(self, algorithm=Parameters.DEFAULT_SEGMENTATION_ALGORITHM):\n        \"\"\"\n        Initializes the Segmentor with a specified segmentation algorithm.\n        \"\"\"\n        self.algorithm = algorithm\n        if 'yolov8' in self.algorithm:\n            self.model = YOLO(f\"{self.algorithm}.pt\")\n        self.previous_detections = []\n\n    def segment_frame(self, frame):\n        \"\"\"\n        Segments the given frame using the selected algorithm.\n        \"\"\"\n        if 'yolov8' in self.algorithm:\n            return self.yolov8_segmentation(frame)\n        else:\n            return self.generic_segmentation(frame)\n\n    def yolov8_segmentation(self, frame):\n        \"\"\"\n        Segments the frame using YOLOv8 and returns an annotated frame.\n        \"\"\"\n        try:\n            results = self.model(frame)\n            annotated_frame = results[0].plot()  \n            current_detections = self.extract_detections(results)\n            filtered_detections = self.manage_detections(current_detections)\n            return annotated_frame\n        except Exception as e:\n            logger.error(f\"Error during YOLOv8 segmentation: {e}\")\n            return frame\n\n    def generic_segmentation(self, frame):\n        \"\"\"\n        Placeholder for other segmentation methods, e.g., GrabCut.\n        \"\"\"\n        logger.warning(\"Generic segmentation method not implemented.\")\n        return frame\n\n    def extract_detections(self, results):\n        \"\"\"\n        Extracts bounding box detections from YOLOv8 results.\n        \"\"\"\n        try:\n            detections = []\n            for det in results[0].boxes.xyxy.tolist():\n                assert isinstance(det, (list, tuple)) and len(det) >= 4, \"Detection format error\"\n                detections.append(det[:4])\n            return detections\n        except Exception as e:\n            logger.error(f\"Error extracting detections: {e}\")\n            return []\n\n    def manage_detections(self, current_detections):\n        \"\"\"\n        Filters out duplicate detections based on IoU and temporal stability.\n        \"\"\"\n        if not self.previous_detections:\n            self.previous_detections = current_detections\n            return current_detections\n        \n        filtered_detections = []\n        for current in current_detections:\n            if not any(self.iou(current, prev) > 0.5 for prev in self.previous_detections):\n                filtered_detections.append(current)\n        \n        self.previous_detections = current_detections\n        return filtered_detections\n\n    def iou(self, boxA, boxB):\n        \"\"\"\n        Calculates the Intersection over Union (IoU) of two bounding boxes.\n        \"\"\"\n        try:\n            xA = max(boxA[0], boxB[0])\n            yA = max(boxA[1], boxB[1])\n            xB = min(boxA[2], boxB[2])\n            yB = min(boxB[3], boxB[3])\n\n            interArea = max(0, xB - xA) * max(0, yB - yA)\n            boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n            boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n\n            iou = interArea / float(boxAArea + boxBArea - interArea)\n            return iou\n        except Exception as e:\n            logger.error(f\"Error calculating IoU: {e}\")\n            return 0.0\n\n    def get_last_detections(self):\n        \"\"\"\n        Returns the last detections.\n        \"\"\"\n        return self.previous_detections\n\n    def user_click_coordinates(self, frame):\n        \"\"\"\n        Captures the coordinates of a user click on the frame.\n        \"\"\"\n        self.user_click = None\n        cv2.namedWindow(\"Select Object\")\n        cv2.setMouseCallback(\"Select Object\", self.set_click_coordinates)\n        while True:\n            cv2.imshow(\"Select Object\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q') or self.user_click is not None:\n                break\n        cv2.destroyWindow(\"Select Object\")\n        return self.user_click\n\n    def set_click_coordinates(self, event, x, y, flags, param):\n        \"\"\"\n        Callback to set the user click coordinates.\n        \"\"\"\n        if event == cv2.EVENT_LBUTTONDOWN:\n            self.user_click = (x, y)\n\n    def _segment_using_grabcut(self, frame, x, y):\n        \"\"\"\n        Segments an object using the GrabCut algorithm based on a user click.\n        \"\"\"\n        try:\n            mask = np.zeros(frame.shape[:2], np.uint8)\n            bgdModel = np.zeros((1, 65), np.float64)\n            fgdModel = np.zeros((1, 65), np.float64)\n\n            rect = (max(x-50, 0), max(y-50, 0), min(x+50, frame.shape[1]), min(y+50, frame.shape[0]))\n            cv2.grabCut(frame, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n\n            binMask = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n\n            contours, _ = cv2.findContours(binMask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            if contours:\n                c = max(contours, key=cv2.contourArea)\n                x, y, w, h = cv2.boundingRect(c)\n                return (x, y, w, h)\n            return None\n        except Exception as e:\n            logger.error(f\"Error during GrabCut segmentation: {e}\")\n            return None\n\n    def refine_bbox(self, frame, bbox):\n        \"\"\"\n        Refines a bounding box using segmentation.\n        \"\"\"\n        try:\n            x, y, w, h = bbox\n            mask = np.zeros(frame.shape[:2], np.uint8)\n            bgdModel = np.zeros((1, 65), np.float64)\n            fgdModel = np.zeros((1, 65), np.float64)\n            rect = (x, y, w, h)\n            cv2.grabCut(frame, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n            mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n            frame_cut = frame * mask2[:, :, np.newaxis]\n\n            contours, _ = cv2.findContours(mask2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            if contours:\n                c = max(contours, key=cv2.contourArea)\n                x, y, w, h = cv2.boundingRect(c)\n                return (x, y, w, h)\n            return bbox\n        except Exception as e:\n            logger.error(f\"Error refining bounding box: {e}\")\n            return bbox\n"}
{"type": "source_file", "path": "src/__init__.py", "content": ""}
{"type": "source_file", "path": "src/classes/followers/custom_pid.py", "content": "from simple_pid import PID\nfrom classes.parameters import Parameters  # Ensure Parameters class is imported or accessible\n\nclass CustomPID(PID):\n    \"\"\"\n    Custom PID controller that integrates standard PID functionalities with advanced features:\n    - Proportional on Measurement (PoM): Enhances stability by applying proportional control based on the current measurement.\n    - Anti-windup using back-calculation: Prevents integral windup by adjusting the integral term when output is at saturation limits.\n\n    Attributes:\n        last_output (float): Stores the last output value to assist in anti-windup calculations.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.last_output = 0  # Initialize last output for anti-windup calculation\n\n    def __call__(self, input_, dt=None):\n        # Apply Proportional on Measurement if enabled\n        if Parameters.PROPORTIONAL_ON_MEASUREMENT:\n            # Adjust the proportional error calculation to be based on the measurement\n            self.proportional = self.Kp * (self.setpoint - input_)\n        \n        output = super().__call__(input_, dt)\n\n        # Apply anti-windup correction if enabled\n        if Parameters.ENABLE_ANTI_WINDUP:\n            if output != self.last_output and (output >= self.output_limits[1] or output <= self.output_limits[0]):\n                # Back-calculate to adjust the integral term\n                diff = output - self.last_output\n                self._integral -= diff * Parameters.ANTI_WINDUP_BACK_CALC_COEFF\n\n        self.last_output = output  # Update last output\n        return output\n"}
{"type": "source_file", "path": "src/classes/__init__.py", "content": ""}
{"type": "source_file", "path": "src/classes/followers/chase_follower.py", "content": "# src/classes/followers/chase_follower.py\nfrom classes.followers.base_follower import BaseFollower\nfrom classes.followers.custom_pid import CustomPID\nfrom classes.parameters import Parameters\nimport logging\nfrom typing import Tuple\nimport numpy as np\n\nclass ChaseFollower(BaseFollower):\n    \"\"\"\n    ChaseFollower manages PID control to maintain a dynamic chase of the target.\n    It controls the roll, pitch, yaw rates, and thrust based on the target's position.\n    \"\"\"\n\n    def __init__(self, px4_controller, initial_target_coords: Tuple[float, float]):\n        \"\"\"\n        Initializes the ChaseFollower with the given PX4 controller and initial target coordinates.\n\n        Args:\n            px4_controller (PX4Controller): Instance of PX4Controller to control the drone.\n            initial_target_coords (tuple): Initial target coordinates to set for the follower.\n        \"\"\"\n        super().__init__(px4_controller, \"Chase Follower\")  # Initialize with \"Chase Follower\" profile\n        self.initial_target_coords = initial_target_coords\n        self.initialize_pids()\n        self.dive_started = False\n        \n    def initialize_pids(self):\n        \"\"\"\n        Initializes the PID controllers for roll, pitch, yaw rates, and thrust.\n        \"\"\"\n        setpoint_x, setpoint_y = self.initial_target_coords\n\n        # Initialize pitch, yaw, roll rate, bank angle, and thrust PID controllers\n        self.pid_pitch_rate = CustomPID(\n            *self.get_pid_gains('pitch_rate'),\n            setpoint=setpoint_y,  # Vertical control\n            output_limits=(-Parameters.MAX_PITCH_RATE, Parameters.MAX_PITCH_RATE)\n        )\n        self.pid_yaw_rate = CustomPID(\n            *self.get_pid_gains('yaw_rate'),\n            setpoint=setpoint_x,  # Horizontal control\n            output_limits=(-Parameters.MAX_YAW_RATE, Parameters.MAX_YAW_RATE)\n        )\n        self.pid_roll_rate = CustomPID(\n            *self.get_pid_gains('roll_rate'),\n            setpoint=0,  # This will be updated based on the bank angle\n            output_limits=(-Parameters.MAX_ROLL_RATE, Parameters.MAX_ROLL_RATE)\n        )\n        self.pid_thrust = CustomPID(\n            *self.get_pid_gains('thrust'),\n            setpoint=self.normalize_speed(Parameters.TARGET_SPEED),  # Target airspeed control\n            output_limits=(Parameters.MIN_THRUST, Parameters.MAX_THRUST)\n        )\n\n        logging.info(\"PID controllers initialized for ChaseFollower.\")\n\n    def get_pid_gains(self, axis: str) -> Tuple[float, float, float]:\n        \"\"\"\n        Retrieves the PID gains for the specified axis.\n\n        Args:\n            axis (str): The axis for which to retrieve the PID gains ('roll_rate', 'pitch_rate', 'yaw_rate', 'bank_angle', 'thrust').\n\n        Returns:\n            Tuple[float, float, float]: The proportional, integral, and derivative gains for the axis.\n        \"\"\"\n        # Return the PID gains from the parameters\n        return Parameters.PID_GAINS[axis]['p'], Parameters.PID_GAINS[axis]['i'], Parameters.PID_GAINS[axis]['d']\n\n    def update_pid_gains(self):\n        \"\"\"\n        Updates the PID gains for pitch, roll, yaw rates, bank angle, and thrust controllers based on the current settings.\n        \"\"\"\n        self.pid_pitch_rate.tunings = self.get_pid_gains('pitch_rate')\n        self.pid_yaw_rate.tunings = self.get_pid_gains('yaw_rate')\n        self.pid_roll_rate.tunings = self.get_pid_gains('roll_rate')\n        self.pid_thrust.tunings = self.get_pid_gains('thrust')\n\n        logging.debug(\"PID gains updated for ChaseFollower.\")\n\n    def calculate_control_commands(self, target_coords: Tuple[float, float]) -> None:\n        \"\"\"\n        Calculates and updates control commands based on the target coordinates and tracking quality.\n\n        Args:\n            target_coords (Tuple[float, float]): The target coordinates from image processing.\n        \"\"\"\n        # Update PID gains\n        self.update_pid_gains()\n\n        # Calculate errors for pitch (vertical) and yaw (horizontal)\n        error_y = (self.pid_pitch_rate.setpoint - target_coords[1]) * (-1)\n        error_x = (self.pid_yaw_rate.setpoint - target_coords[0]) * (+1)\n\n        # Calculate control rates using the PID controllers\n        pitch_rate = self.pid_pitch_rate(error_y)\n        yaw_rate = self.pid_yaw_rate(error_x)\n\n        # Calculate yaw error\n        yaw_error = error_x  # Assuming error_x is the yaw error\n        # Get current speed in meters per second\n        current_speed = self.px4_controller.current_ground_speed\n        current_roll = self.px4_controller.current_roll\n\n        # Thrust control: Adjust thrust based on tracking quality and airspeed control\n        thrust = self.control_thrust(current_speed)\n        \n        # Check yaw error before sending pitch commands\n        self.check_yaw_and_control(yaw_error, pitch_rate,thrust)\n\n        # Convert yaw rate from degrees/sec to radians/sec\n        yaw_rate_rad = yaw_rate * (np.pi / 180)\n\n        \n        logging.debug(f\"Current Roll: {current_roll:.2f}, Current Speed: {current_speed:.2f}\")\n\n        # Calculate the desired bank angle from yaw rate and speed\n        g = 9.81  # Acceleration due to gravity in m/s^2\n        target_bank_angle_rad = np.arctan((yaw_rate_rad * current_speed) / g)\n\n        # Convert the bank angle from radians to degrees\n        target_bank_angle = np.degrees(target_bank_angle_rad)\n\n        # Calculate the error between desired and current bank angle\n        bank_angle_error = (-1) * (target_bank_angle - current_roll)\n\n        # Use the bank angle error to calculate the required roll rate\n        roll_rate = self.pid_roll_rate(bank_angle_error)\n\n        \n\n        # Update the setpoint handler\n        self.px4_controller.setpoint_handler.set_field('roll_rate', roll_rate)\n        self.px4_controller.setpoint_handler.set_field('yaw_rate', yaw_rate)\n        # self.px4_controller.setpoint_handler.set_field('yaw_rate', yaw_rate)\n\n        # Log the calculated control commands for debugging\n        logging.debug(f\"Calculated commands - Roll rate: {roll_rate:.2f} degrees/sec to Bank angle: {target_bank_angle:.2f} degrees, \"\n                    f\"Yaw rate: {yaw_rate:.2f} degrees/sec\")\n        \n        \n    def check_yaw_and_control(self, yaw_error: float, pitch_command: float, thrust_command: float) -> None:\n        \"\"\"\n        Checks the yaw error and controls pitch commands based on the result.\n\n        Args:\n            yaw_error (float): The current yaw error of the drone.\n            pitch_command (float): The pitch command to be potentially sent.\n        \"\"\"\n        if Parameters.YAW_ERROR_CHECK_ENABLED & ~self.dive_started:\n            if abs(yaw_error) < Parameters.YAW_ERROR_THRESHOLD:\n                self.px4_controller.setpoint_handler.set_field('pitch_rate', pitch_command)\n                self.px4_controller.setpoint_handler.set_field('thrust', self.px4_controller.hover_throttle) # keep sending hover throttle\n                logging.debug(f\"Pitch and throttle command sent: {pitch_command:.2f}, {thrust_command:.2f}\")\n                self.dive_started = True\n            else:\n                logging.info(f\"Yaw error {yaw_error:.2f} exceeds threshold {Parameters.YAW_ERROR_THRESHOLD}. Pitch and thrust command not sent.\")\n                self.px4_controller.setpoint_handler.set_field('thrust', self.px4_controller.hover_throttle) # keep sending hover throttle\n        else:\n            self.px4_controller.setpoint_handler.set_field('pitch_rate', pitch_command)\n            self.px4_controller.setpoint_handler.set_field('thrust', thrust_command)\n            self.dive_started = True\n            logging.debug(f\"Yaw error check disabled. Pitch and Thrust command sent: {pitch_command:.2f}\")\n\n    def follow_target(self, target_coords: Tuple[float, float]):\n        \"\"\"\n        Sends control commands to follow the target based on the coordinates.\n\n        Args:\n            target_coords (Tuple[float, float]): The target coordinates to follow.\n        \"\"\"\n        self.check_altitude_safety()\n        self.calculate_control_commands(target_coords)\n\n    def control_thrust(self, ground_speed) -> float:\n        \"\"\"\n        Controls the thrust based on the current ground speed.\n\n        Args:\n            ground_speed (float): The current ground speed of the drone.\n\n        Returns:\n            float: The calculated thrust command.\n        \"\"\"\n        # Normalize ground speed\n        normalized_speed = self.normalize_speed(ground_speed)\n\n        # Use normalized speed as setpoint for thrust PID\n        current_thrust = self.pid_thrust(normalized_speed)\n\n        return current_thrust + self.px4_controller.hover_throttle*0\n    \n    \n    def normalize_speed(self, speed, min_speed=Parameters.MIN_GROUND_SPEED, max_speed=Parameters.MAX_GROUND_SPEED):\n        \"\"\"\n        Normalizes the given speed value between 0 and 1.\n\n        Args:\n            speed (float): The speed value to be normalized.\n            min_speed (float): The minimum speed value.\n            max_speed (float): The maximum speed value.\n\n        Returns:\n            float: The normalized speed value between 0 and 1.\n        \"\"\"\n        normalized_speed = (speed - min_speed) / (max_speed - min_speed)\n        return max(0.0, min(1.0, normalized_speed))\n    \n    def check_altitude_safety(self):\n        if Parameters.ALTITUDE_FAILSAFE_ENABLED:\n            current_altitude = self.px4_controller.current_altitude\n            if current_altitude < Parameters.MIN_DESCENT_HEIGHT or current_altitude > Parameters.MAX_CLIMB_HEIGHT:\n                logging.warning(f\"Altitude safety triggered! Current altitude: {current_altitude}\")\n                self.px4_controller.app_controller.disconnect_px4()\n                #self.px4_controller.failsafe_active = True\n                \n                \n    "}
{"type": "source_file", "path": "src/classes/detectors/__init__.py", "content": ""}
{"type": "source_file", "path": "src/classes/telemetry_handler.py", "content": "# src/classes/telemetry_handler.py\nimport logging\nimport socket\nimport json\nfrom datetime import datetime, timedelta\nfrom classes.parameters import Parameters\n\nclass TelemetryHandler:\n    def __init__(self, app_controller, tracking_started_flag):\n        \"\"\"\n        Initialize the TelemetryHandler with necessary parameters and dependencies.\n        \n        Args:\n            tracker (Tracker): An instance of the tracker to gather data from.\n            follower (Follower): An instance of the follower to gather data from.\n            tracking_started_flag (callable): A callable that returns the current tracking state.\n        \"\"\"\n        self.host = Parameters.UDP_HOST\n        self.port = Parameters.UDP_PORT\n        self.send_rate = Parameters.TELEMETRY_SEND_RATE\n        self.enable_udp = Parameters.ENABLE_UDP_STREAM\n\n        self.udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        self.server_address = (self.host, self.port)\n        self.send_interval = 1.0 / self.send_rate  # Convert rate to interval in seconds\n\n        self.last_sent_time = datetime.utcnow()\n        self.app_controller = app_controller\n        self.tracker = self.app_controller.tracker\n        self.follower = self.app_controller.follower\n        self.tracking_started_flag = tracking_started_flag  # Store the callable\n        self.latest_tracker_data = {}\n        self.latest_follower_data = {}\n\n    def should_send_telemetry(self):\n        \"\"\"\n        Check if the telemetry data should be sent based on the send interval.\n        \n        Returns:\n            bool: True if the telemetry data should be sent, False otherwise.\n        \"\"\"\n        current_time = datetime.utcnow()\n        return (current_time - self.last_sent_time).total_seconds() >= self.send_interval\n\n    def get_tracker_data(self):\n        \"\"\"\n        Get the latest tracker telemetry data.\n        \n        Returns:\n            dict: The tracker telemetry data.\n        \"\"\"\n        timestamp = datetime.utcnow().isoformat()\n        tracker_started = self.tracking_started_flag()  # Use the callable to check if tracking is started\n        return {\n            'bounding_box': self.tracker.normalized_bbox,\n            'center': self.tracker.normalized_center,\n            'timestamp': timestamp,\n            'tracker_started': tracker_started\n        }\n\n    def get_follower_data(self):\n        \"\"\"\n        Get the latest follower telemetry data using the SetpointHandler.\n        \n        Returns:\n            dict: The follower telemetry data.\n        \"\"\"\n        if self.follower is not None:\n            telemetry = self.follower.get_follower_telemetry() if Parameters.ENABLE_FOLLOWER_TELEMETRY else {}\n            telemetry['following_active'] = self.app_controller.following_active\n            telemetry['profile_name'] = self.follower.follower.profile_name\n            return telemetry\n        return {}\n\n    def gather_telemetry_data(self):\n        \"\"\"\n        Gather telemetry data from all enabled sources.\n        \n        Returns:\n            dict: A dictionary containing telemetry data from all sources.\n        \"\"\"\n        data = {\n            'tracker_data': self.get_tracker_data(),\n            'follower_data': self.get_follower_data(),\n        }\n        return data\n\n    def send_telemetry(self):\n        \"\"\"\n        Send the telemetry data via UDP if conditions are met and update the latest telemetry data.\n        \"\"\"\n        if self.should_send_telemetry() and self.enable_udp:\n            data = self.gather_telemetry_data()\n            message = json.dumps(data)\n            self.udp_socket.sendto(message.encode('utf-8'), self.server_address)\n            self.last_sent_time = datetime.utcnow()\n            logging.debug(f\"Telemetry sent: {data}\")\n        \n        # Update the latest telemetry data regardless of UDP sending\n        self.latest_tracker_data = self.get_tracker_data()\n        if Parameters.ENABLE_FOLLOWER_TELEMETRY:\n            self.latest_follower_data = self.get_follower_data()\n        # logging.debug(f\"Latest tracker data: {self.latest_tracker_data}\")\n        # logging.debug(f\"Latest follower data: {self.latest_follower_data}\")\n\n\n"}
{"type": "source_file", "path": "src/classes/gstreamer_handler.py", "content": "import cv2\nimport numpy as np\nimport logging\nfrom classes.parameters import Parameters\n\nclass GStreamerHandler:\n    \"\"\"\n    A class to handle streaming video frames to a GStreamer pipeline.\n    This class initializes a GStreamer pipeline that streams video frames over UDP in H.264 format.\n\n    Key Considerations and Best Practices:\n    -------------------------------------\n    1. Compatibility with QGroundControl (QGC):\n       - QGC expects an RTP/UDP stream in H.264 format. This requires careful setup of the GStreamer pipeline to ensure \n         proper encoding, payloading, and streaming.\n       - The `appsrc` element is used as the source in the pipeline, allowing OpenCV to push frames directly into the \n         GStreamer pipeline, which is necessary when working with processed frames in OpenCV.\n\n    2. Frame Format:\n       - OpenCV typically works with frames in BGR format, while the NVIDIA encoder (`nvvidconv`) and GStreamer pipeline \n         may require NV12 or other formats. Conversion is handled using `videoconvert`.\n       - Ensure frames are in 8-bit, 3-channel BGR format before pushing them into the pipeline.\n\n    3. Bitrate and Encoding Settings:\n       - Bitrate is critical for balancing video quality and network bandwidth. It is specified in kbps.\n       - The `x264enc` element is configured with `zerolatency` tuning for low-latency streaming, and `ultrafast` preset \n         for faster encoding at the expense of some quality.\n\n    4. Flip Method:\n       - The `nvvidconv` element's `flip-method` parameter can be adjusted to flip the video as needed. This is crucial \n         when working with different camera orientations.\n\n    5. Error Handling and Logging:\n       - Robust error handling ensures that issues with pipeline initialization or frame streaming are logged and managed \n         gracefully. This is critical in a live streaming environment.\n\n    6. Buffer Size:\n       - The UDP buffer size is configured to handle network jitter and ensure smooth streaming. Adjust based on network \n         conditions and application requirements.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the GStreamerHandler with parameters for frame width, height, framerate, and flip method.\n        The pipeline is constructed based on these parameters.\n        \"\"\"\n        self.out = None\n        self.FLIP_METHOD = Parameters.CSI_FLIP_METHOD\n        self.WIDTH = Parameters.GSTREAMER_WIDTH\n        self.HEIGHT = Parameters.GSTREAMER_HEIGHT\n        self.FRAMERATE = Parameters.GSTREAMER_FRAMERATE\n\n        self.pipeline = self._create_pipeline()\n\n    def _create_pipeline(self) -> str:\n        \"\"\"\n        Constructs the GStreamer pipeline string using parameters from the configuration.\n        This pipeline is designed to be compatible with QGroundControl, ensuring frames are properly formatted, encoded, \n        and streamed over RTP/UDP.\n\n        Returns:\n            str: The constructed GStreamer pipeline string.\n        \"\"\"\n        pipeline = (\n            f\"appsrc ! \"\n            f\"video/x-raw,format=BGR,width={self.WIDTH},height={self.HEIGHT},framerate={self.FRAMERATE}/1 ! \"\n            f\"videoconvert ! \"\n            f\"video/x-raw,format=NV12 ! \"  # Convert to NV12 format for compatibility with NVIDIA encoder\n            f\"nvvidconv flip-method={self.FLIP_METHOD} ! \"\n            f\"x264enc tune={Parameters.GSTREAMER_TUNE} \"\n            f\"bitrate={Parameters.GSTREAMER_BITRATE} \"  # Bitrate in kbps\n            f\"key-int-max={Parameters.GSTREAMER_KEY_INT_MAX} \"\n            f\"speed-preset={Parameters.GSTREAMER_SPEED_PRESET} ! \"\n            f\"rtph264pay config-interval=1 pt=96 ! \"\n            f\"udpsink host={Parameters.GSTREAMER_HOST} port={Parameters.GSTREAMER_PORT} buffer-size={Parameters.GSTREAMER_BUFFER_SIZE}\"\n        )\n\n        logging.debug(f\"GStreamer pipeline: {pipeline}\")\n        return pipeline\n\n    def initialize_stream(self):\n        \"\"\"\n        Initializes the GStreamer pipeline using OpenCV's VideoWriter.\n        This method sets up the pipeline and prepares it for streaming frames.\n        \"\"\"\n        try:\n            logging.info(\"Initializing GStreamer pipeline...\")\n            self.out = cv2.VideoWriter(self.pipeline, cv2.CAP_GSTREAMER, 0, self.FRAMERATE, (self.WIDTH, self.HEIGHT), True)\n            if not self.out.isOpened():\n                logging.error(\"Failed to open GStreamer pipeline.\")\n                self.out = None\n        except Exception as e:\n            logging.error(f\"Error initializing GStreamer pipeline: {e}\")\n            self.out = None\n\n    def stream_frame(self, frame: np.ndarray):\n        \"\"\"\n        Streams a video frame to the GStreamer pipeline.\n\n        Args:\n            frame (np.ndarray): The video frame to stream. Must be an 8-bit, 3-channel BGR format.\n        \"\"\"\n        if self.out:\n            try:\n                # Ensure the frame is in 8-bit, 3-channel BGR format\n                if frame.dtype != np.uint8:\n                    frame = frame.astype(np.uint8)\n                if len(frame.shape) == 2 or frame.shape[2] != 3:\n                    frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n                self.out.write(frame)\n            except Exception as e:\n                logging.error(f\"Error streaming frame to GStreamer pipeline: {e}\")\n\n    def release(self):\n        \"\"\"\n        Releases the GStreamer pipeline and associated resources.\n        This should be called to clean up resources when streaming is no longer needed.\n        \"\"\"\n        if self.out:\n            self.out.release()\n            logging.debug(\"GStreamer pipeline released.\")\n"}
{"type": "source_file", "path": "src/classes/followers/base_follower.py", "content": "#src\\classes\\followers\\base_follower.py\nfrom abc import ABC, abstractmethod\nfrom typing import Tuple, Dict\nfrom classes.setpoint_handler import SetpointHandler\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass BaseFollower(ABC):\n    \"\"\"\n    Abstract base class for different follower modes.\n    Defines the required methods that any follower mode should implement.\n    \"\"\"\n    def __init__(self, px4_controller, profile_name: str):\n        \"\"\"\n        Initializes the BaseFollower with the given PX4 controller and a SetpointHandler for managing setpoints.\n\n        Args:\n            px4_controller (PX4Controller): Instance of PX4Controller to control the drone.\n            profile_name (str): The name of the setpoint profile to use (e.g., \"Ground View\", \"Constant Position\").\n        \"\"\"\n        self.px4_controller = px4_controller\n        self.profile_name = profile_name\n        self.latest_velocities = {'timestamp': None, 'status': 'idle'}\n        logger.info(f\"BaseFollower initialized with profile: {profile_name}\")\n\n    @abstractmethod\n    def calculate_control_commands(self, target_coords: Tuple[float, float]) -> None:\n        \"\"\"\n        Calculate control commands based on target coordinates and update the setpoint handler.\n\n        Args:\n            target_coords (tuple): The coordinates of the target to follow.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def follow_target(self, target_coords: Tuple[float, float]):\n        \"\"\"\n        Asynchronously sends velocity commands to follow the target.\n\n        Args:\n            target_coords (tuple): The coordinates of the target to follow.\n        \"\"\"\n        pass\n    \n    def get_follower_telemetry(self) -> Dict[str, any]:\n        \"\"\"\n        Returns the latest velocity telemetry data.\n\n        Returns:\n            dict: The latest velocity telemetry data from the setpoint handler.\n        \"\"\"\n        telemetry = self.px4_controller.setpoint_handler.get_fields()\n        logger.debug(f\"Telemetry data retrieved: {telemetry}\")\n        return telemetry\n"}
{"type": "source_file", "path": "src/classes/trackers/base_tracker.py", "content": "# src/classes/trackers/base_tracker.py\n\n\"\"\"\nBaseTracker Module\n------------------\n\nThis module defines the abstract base class `BaseTracker` for all object trackers used in the tracking system.\n\nProject Information:\n- Project Name: PixEagle\n- Repository: https://github.com/alireza787b/PixEagle\n- Date: October 2024\n- Author: Alireza Ghaderi\n- LinkedIn: https://www.linkedin.com/in/alireza787b\n\nOverview:\n---------\nThe `BaseTracker` class provides a common interface and shared functionalities for different tracking algorithms. It defines the essential methods and attributes that all concrete tracker classes must implement or utilize.\n\nPurpose:\n--------\nIn the context of aerial target tracking, having a unified tracker interface ensures that different tracking algorithms can be swapped or tested with minimal changes to the overall system. This promotes modularity, scalability, and ease of maintenance.\n\nKey Features:\n-------------\n- **Abstract Methods**: Enforces implementation of essential methods like `start_tracking` and `update`.\n- **Common Attributes**: Manages shared properties such as bounding boxes, centers, and normalization.\n- **Estimator Integration**: Supports integration with estimators (e.g., Kalman Filter) to enhance tracking robustness.\n- **Visualization Tools**: Provides methods for drawing tracking information on video frames.\n- **Confidence Score**: Implements a standardized confidence score based on motion and appearance consistency.\n\nUsage:\n------\nThe `BaseTracker` class is intended to be subclassed by concrete tracker implementations (e.g., `CSRTTracker`). Developers should implement the abstract methods and can override or extend other methods as needed.\n\nExample:\n```python\nclass CustomTracker(BaseTracker):\n    def start_tracking(self, frame: np.ndarray, bbox: Tuple[int, int, int, int]) -> None:\n        # Custom implementation\n        pass\n\n    def update(self, frame: np.ndarray) -> Tuple[bool, Tuple[int, int, int, int]]:\n        # Custom implementation\n        pass\n```\n\nExtending and Building New Trackers:\n------------------------------------\nTo create a new tracker:\n1. Subclass `BaseTracker`.\n2. Implement all abstract methods (`start_tracking`, `update`).\n3. Override or extend other methods as necessary.\n4. Add the new tracker to the `tracker_factory.py` for easy instantiation.\n\nNotes:\n------\n- **Estimator Usage**: Trackers can utilize estimators by enabling `estimator_enabled` and providing a `position_estimator`.\n- **Normalization**: Provides methods to normalize coordinates, which is essential for control inputs that require normalized values.\n- **Visualization**: Includes methods for drawing bounding boxes and tracking information, aiding in debugging and monitoring.\n- **Confidence Score**: Standardizes confidence calculation across trackers for consistency.\n\nIntegration:\n------------\nThe tracker is integrated into the main application via the `AppController`. It receives frames from the video handler, processes them, and provides tracking information to other components like the estimator and follower.\n\nRecommendations:\n----------------\n- **Consistency Checks**: Implement motion and appearance consistency checks to enhance robustness.\n- **Logging**: Utilize logging to monitor tracker performance and catch potential issues.\n- **Parameter Tuning**: Adjust parameters in `Parameters` class to optimize tracking performance based on the specific use case.\n\nReferences:\n-----------\n- OpenCV Tracking API: https://docs.opencv.org/master/d9/df8/group__tracking.html\n- Object Tracking Concepts: https://www.learnopencv.com/object-tracking-using-opencv-cpp-python/\n\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom collections import deque\nimport time\nimport numpy as np\nfrom typing import Optional, Tuple\nimport cv2\nfrom classes.parameters import Parameters\nimport logging\n\nclass BaseTracker(ABC):\n    \"\"\"\n    Abstract Base Class for Object Trackers\n\n    Defines the interface and common functionalities for different tracking algorithms used in the system.\n\n    Attributes:\n    -----------\n    - video_handler (Optional[object]): Handler for video streaming and processing.\n    - detector (Optional[object]): Detector for appearance-based methods.\n    - app_controller (Optional[object]): Reference to the main application controller.\n    - bbox (Optional[Tuple[int, int, int, int]]): Current bounding box (x, y, width, height).\n    - prev_center (Optional[Tuple[int, int]]): Previous center of the bounding box.\n    - center (Optional[Tuple[int, int]]): Current center of the bounding box.\n    - normalized_bbox (Optional[Tuple[float, float, float, float]]): Normalized bounding box.\n    - normalized_center (Optional[Tuple[float, float]]): Normalized center coordinates.\n    - center_history (deque): History of center positions.\n    - estimator_enabled (bool): Indicates if the estimator is enabled.\n    - position_estimator (Optional[BaseEstimator]): Estimator instance for position estimation.\n    - estimated_position_history (deque): History of estimated positions.\n    - last_update_time (float): Timestamp of the last update.\n    - frame (Optional[np.ndarray]): Placeholder for the current video frame.\n    - confidence (float): Confidence score of the tracker.\n\n    Methods:\n    --------\n    - start_tracking(frame, bbox): Abstract method to start tracking with an initial frame and bounding box.\n    - update(frame): Abstract method to update the tracker with a new frame.\n    - compute_confidence(frame): Computes the confidence score based on motion and appearance consistency.\n    - get_confidence(): Returns the current confidence score.\n    - is_motion_consistent(): Checks if the motion is consistent based on displacement thresholds.\n    - update_time(): Updates the internal timer and calculates the time delta since the last update.\n    - normalize_center_coordinates(): Normalizes the center coordinates relative to the frame size.\n    - print_normalized_center(): Logs the normalized center coordinates.\n    - set_center(value): Sets the center coordinates and normalizes them.\n    - normalize_bbox(): Normalizes the bounding box coordinates relative to the frame size.\n    - reinitialize_tracker(frame, bbox): Reinitializes the tracker with a new bounding box.\n    - draw_tracking(frame, tracking_successful): Draws tracking bounding box and center on the frame.\n    - draw_normal_bbox(frame, tracking_successful): Draws a standard rectangle bounding box.\n    - draw_fancy_bbox(frame, tracking_successful): Draws a stylized bounding box with additional visuals.\n    - draw_estimate(frame, tracking_successful): Draws the estimated position from the estimator.\n    \"\"\"\n\n    def __init__(self, video_handler: Optional[object] = None, detector: Optional[object] = None, app_controller: Optional[object] = None):\n        \"\"\"\n        Initializes the base tracker with common attributes.\n\n        Args:\n            video_handler (Optional[object]): Handler for video streaming and processing.\n            detector (Optional[object]): Detector for appearance-based methods.\n            app_controller (Optional[object]): Reference to the main application controller.\n        \"\"\"\n        self.video_handler = video_handler\n        self.detector = detector\n        self.app_controller = app_controller  # Assign before using it\n\n        # Initialize tracking attributes\n        self.bbox: Optional[Tuple[int, int, int, int]] = None  # Current bounding box\n        self.prev_center: Optional[Tuple[int, int]] = None     # Previous center\n        self.center: Optional[Tuple[int, int]] = None          # Current center\n        self.normalized_bbox: Optional[Tuple[float, float, float, float]] = None  # Normalized bounding box\n        self.normalized_center: Optional[Tuple[float, float]] = None              # Normalized center\n        self.center_history = deque(maxlen=Parameters.CENTER_HISTORY_LENGTH)      # History of centers\n\n        # Estimator initialization\n        self.estimator_enabled = Parameters.USE_ESTIMATOR\n        self.position_estimator = self.app_controller.estimator if self.estimator_enabled else None\n        self.estimated_position_history = deque(maxlen=Parameters.ESTIMATOR_HISTORY_LENGTH)\n        self.last_update_time: float = 1e-6\n\n        # Confidence score\n        self.confidence: float = 1.0  # Initialize confidence score\n\n        # Frame placeholder\n        self.frame = None\n\n    @abstractmethod\n    def start_tracking(self, frame: np.ndarray, bbox: Tuple[int, int, int, int]) -> None:\n        \"\"\"\n        Abstract method to start tracking with the given frame and bounding box.\n\n        Args:\n            frame (np.ndarray): The initial video frame to start tracking.\n            bbox (Tuple[int, int, int, int]): A tuple representing the bounding box (x, y, width, height).\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update(self, frame: np.ndarray) -> Tuple[bool, Tuple[int, int, int, int]]:\n        \"\"\"\n        Abstract method to update the tracker with the new frame.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n\n        Returns:\n            Tuple[bool, Tuple[int, int, int, int]]: A tuple containing the success status and the new bounding box.\n        \"\"\"\n        pass\n\n    def compute_confidence(self, frame: np.ndarray) -> float:\n        \"\"\"\n        Computes the confidence score based on motion and appearance consistency.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n\n        Returns:\n            float: The updated confidence score.\n        \"\"\"\n        motion_confidence = self.compute_motion_confidence()\n        appearance_confidence = 1.0  # Default to maximum confidence\n\n        if self.detector and hasattr(self.detector, 'compute_appearance_confidence') and self.detector.adaptive_features is not None:\n            current_features = self.detector.extract_features(frame, self.bbox)\n            appearance_confidence = self.detector.compute_appearance_confidence(current_features, self.detector.adaptive_features)\n        else:\n            logging.warning(\"Detector is not available or adaptive features are not set.\")\n\n        # Combine motion and appearance confidence\n        self.confidence = (Parameters.MOTION_CONFIDENCE_WEIGHT * motion_confidence +\n                           Parameters.APPEARANCE_CONFIDENCE_WEIGHT * appearance_confidence)\n        return self.confidence\n\n    def get_confidence(self) -> float:\n        \"\"\"\n        Returns the current confidence score of the tracker.\n\n        Returns:\n            float: The confidence score.\n        \"\"\"\n        return self.confidence\n\n    def compute_motion_confidence(self) -> float:\n        \"\"\"\n        Computes confidence based on motion consistency.\n\n        Returns:\n            float: The motion confidence score between 0.0 and 1.0.\n\n        A lower confidence indicates unexpected large movements, possibly due to tracking errors.\n        \"\"\"\n        if self.prev_center is None:\n            return 1.0  # Maximum confidence on first frame\n        displacement = np.linalg.norm(np.array(self.center) - np.array(self.prev_center))\n        frame_diag = np.hypot(self.video_handler.width, self.video_handler.height)\n        confidence = max(0.0, 1.0 - (displacement / (Parameters.MAX_DISPLACEMENT_THRESHOLD * frame_diag)))\n        return confidence\n\n    def is_motion_consistent(self) -> bool:\n        \"\"\"\n        Checks if the motion between the previous and current center is within expected limits.\n\n        Returns:\n            bool: True if motion is consistent, False otherwise.\n        \"\"\"\n        return self.compute_motion_confidence() >= Parameters.MOTION_CONFIDENCE_THRESHOLD\n\n    def update_time(self) -> float:\n        \"\"\"\n        Updates the time interval (dt) between consecutive frames.\n\n        Returns:\n            float: The time difference since the last update.\n        \"\"\"\n        current_time = time.monotonic()\n        dt = current_time - self.last_update_time\n        if dt <= 0:\n            logging.warning(f\"Non-positive dt encountered: {dt}. Setting dt to minimal positive value.\")\n            dt = 1e-6  # Set a minimal positive dt\n        self.last_update_time = current_time\n        return dt\n\n    def normalize_center_coordinates(self) -> None:\n        \"\"\"\n        Normalizes and stores the center coordinates of the tracked target.\n\n        Normalization is done such that the center of the frame is (0, 0),\n        the top-right is (1, 1), and the bottom-left is (-1, -1).\n        \"\"\"\n        if self.center:\n            frame_width, frame_height = self.video_handler.width, self.video_handler.height\n            normalized_x = (self.center[0] - frame_width / 2) / (frame_width / 2)\n            normalized_y = (self.center[1] - frame_height / 2) / (frame_height / 2)\n            self.normalized_center = (normalized_x, normalized_y)\n\n    def print_normalized_center(self) -> None:\n        \"\"\"\n        Logs the normalized center coordinates of the tracked target.\n\n        Assumes `normalize_center_coordinates` has been called after the latest tracking update.\n        \"\"\"\n        if self.normalized_center:\n            # logging.debug(f\"Normalized Center Coordinates: {self.normalized_center}\")\n            pass\n        else:\n            logging.warning(\"Normalized center coordinates not calculated or available.\")\n\n    def set_center(self, value: Tuple[int, int]) -> None:\n        \"\"\"\n        Sets the center of the bounding box and normalizes it.\n\n        Args:\n            value (Tuple[int, int]): The (x, y) coordinates of the center.\n        \"\"\"\n        self.center = value\n        self.normalize_center_coordinates()  # Automatically normalize when center is updated\n\n    def normalize_bbox(self) -> None:\n        \"\"\"\n        Normalizes the bounding box coordinates relative to the frame size.\n\n        This is useful for consistent representation and for control inputs that require normalized values.\n        \"\"\"\n        if self.bbox and self.video_handler:\n            frame_width, frame_height = self.video_handler.width, self.video_handler.height\n            x, y, w, h = self.bbox\n            norm_x = (x - frame_width / 2) / (frame_width / 2)\n            norm_y = (y - frame_height / 2) / (frame_height / 2)\n            norm_w = w / frame_width\n            norm_h = h / frame_height\n            self.normalized_bbox = (norm_x, norm_y, norm_w, norm_h)\n            # logging.debug(f\"Normalized bbox: {self.normalized_bbox}\")\n\n    def reinitialize_tracker(self, frame: np.ndarray, bbox: Tuple[int, int, int, int]) -> None:\n        \"\"\"\n        Reinitializes the tracker with a new bounding box on the given frame.\n\n        Args:\n            frame (np.ndarray): The video frame to reinitialize tracking.\n            bbox (Tuple[int, int, int, int]): The new bounding box for tracking.\n\n        This can be used when the tracker loses the target or when manual reinitialization is required.\n        \"\"\"\n        logging.info(f\"Reinitializing tracker with bbox: {bbox}\")\n        self.start_tracking(frame, bbox)\n\n    def draw_tracking(self, frame: np.ndarray, tracking_successful: bool = True) -> np.ndarray:\n        \"\"\"\n        Draws the tracking bounding box and center on the frame.\n\n        Args:\n            frame (np.ndarray): The video frame.\n            tracking_successful (bool): Whether the tracking was successful.\n\n        Returns:\n            np.ndarray: The frame with tracking drawn.\n        \"\"\"\n        if self.bbox and self.center and self.video_handler:\n            if Parameters.TRACKED_BBOX_STYLE == 'fancy':\n                self.draw_fancy_bbox(frame, tracking_successful)\n            else:\n                self.draw_normal_bbox(frame, tracking_successful)\n\n            # Draw center dot\n            cv2.circle(frame, self.center, 5, (0, 255, 0), -1)\n\n            # Optionally display deviations\n            if Parameters.DISPLAY_DEVIATIONS:\n                self.print_normalized_center()\n\n        return frame\n\n    def draw_normal_bbox(self, frame: np.ndarray, tracking_successful: bool = True) -> None:\n        \"\"\"\n        Draws a normal rectangle bounding box on the frame.\n\n        Args:\n            frame (np.ndarray): The video frame.\n            tracking_successful (bool): Whether the tracking was successful.\n        \"\"\"\n        p1 = (int(self.bbox[0]), int(self.bbox[1]))\n        p2 = (int(self.bbox[0] + self.bbox[2]), int(self.bbox[1] + self.bbox[3]))\n        color = (255, 0, 0) if tracking_successful else (0, 0, 255)\n        cv2.rectangle(frame, p1, p2, color, 2)\n\n    def draw_fancy_bbox(self, frame, tracking_successful: bool = True):\n        \"\"\"\n        Draws a stylized bounding box with additional visuals, such as crosshairs and extended lines.\n\n        Args:\n            frame (np.ndarray): The video frame.\n            tracking_successful (bool): Whether the tracking was successful.\n        \"\"\"\n        if self.bbox is None or self.center is None:\n            return frame\n\n        # Determine color based on tracking status\n        color = (Parameters.FOLLOWER_ACTIVE_COLOR \n                 if self.app_controller.following_active \n                 else Parameters.FOLLOWER_INACTIVE_COLOR)\n\n        p1 = (int(self.bbox[0]), int(self.bbox[1]))\n        p2 = (int(self.bbox[0] + self.bbox[2]), int(self.bbox[1] + self.bbox[3]))\n        center_x, center_y = self.center\n        \n        # Draw crosshair\n        cv2.line(frame, \n                 (center_x - Parameters.CROSSHAIR_ARM_LENGTH, center_y), \n                 (center_x + Parameters.CROSSHAIR_ARM_LENGTH, center_y), \n                 color, \n                 Parameters.BBOX_LINE_THICKNESS)\n        cv2.line(frame, \n                 (center_x, center_y - Parameters.CROSSHAIR_ARM_LENGTH), \n                 (center_x, center_y + Parameters.CROSSHAIR_ARM_LENGTH), \n                 color, \n                 Parameters.BBOX_LINE_THICKNESS)\n        \n        # Draw bounding box with corners\n        corner_points = [\n            (p1, (p1[0] + Parameters.BBOX_CORNER_ARM_LENGTH, p1[1])),\n            (p1, (p1[0], p1[1] + Parameters.BBOX_CORNER_ARM_LENGTH)),\n            (p2, (p2[0] - Parameters.BBOX_CORNER_ARM_LENGTH, p2[1])),\n            (p2, (p2[0], p2[1] - Parameters.BBOX_CORNER_ARM_LENGTH)),\n            ((p1[0], p2[1]), (p1[0] + Parameters.BBOX_CORNER_ARM_LENGTH, p2[1])),\n            ((p1[0], p2[1]), (p1[0], p2[1] - Parameters.BBOX_CORNER_ARM_LENGTH)),\n            ((p2[0], p1[1]), (p2[0] - Parameters.BBOX_CORNER_ARM_LENGTH, p1[1])),\n            ((p2[0], p1[1]), (p2[0], p1[1] + Parameters.BBOX_CORNER_ARM_LENGTH))\n        ]\n        \n        for start, end in corner_points:\n            cv2.line(frame, start, end, color, Parameters.BBOX_LINE_THICKNESS)\n\n        # Draw extended lines from the center of each edge of the bounding box\n        height, width, _ = frame.shape\n        cv2.line(frame, (p1[0], center_y), (0, center_y), color, Parameters.EXTENDED_LINE_THICKNESS)  # Left edge to left\n        cv2.line(frame, (p2[0], center_y), (width, center_y), color, Parameters.EXTENDED_LINE_THICKNESS)  # Right edge to right\n        cv2.line(frame, (center_x, p1[1]), (center_x, 0), color, Parameters.EXTENDED_LINE_THICKNESS)  # Top edge to top\n        cv2.line(frame, (center_x, p2[1]), (center_x, height), color, Parameters.EXTENDED_LINE_THICKNESS)  # Bottom edge to bottom\n\n        # Draw smaller dots at the corners of the bounding box\n        for point in [p1, p2, (p1[0], p2[1]), (p2[0], p1[1])]:\n            cv2.circle(frame, point, Parameters.CORNER_DOT_RADIUS, color, -1)\n\n        return frame\n\n    def draw_estimate(self, frame: np.ndarray, tracking_successful: bool = True) -> np.ndarray:\n        \"\"\"\n        Draws the estimated position on the frame if the estimator is enabled.\n\n        Args:\n            frame (np.ndarray): The video frame.\n            tracking_successful (bool): Whether the tracking was successful.\n\n        Returns:\n            np.ndarray: The frame with the estimate drawn on it.\n        \"\"\"\n        if self.estimator_enabled and self.position_estimator and self.video_handler:\n            estimated_position = self.position_estimator.get_estimate()\n            if estimated_position:\n                estimated_x, estimated_y = estimated_position[:2]\n                color = Parameters.ESTIMATED_POSITION_COLOR if tracking_successful else Parameters.ESTIMATION_ONLY_COLOR\n                cv2.circle(frame, (int(estimated_x), int(estimated_y)), 5, color, -1)\n        return frame\n"}
{"type": "source_file", "path": "src/classes/setpoint_handler.py", "content": "# src\\classes\\setpoint_handler.py\nimport logging\nfrom datetime import datetime\nfrom typing import Dict\n\n# Set up logging\nlogger = logging.getLogger(__name__)\n\n# Define profiles for different application modes\nSETPOINT_PROFILES = {\n    \"Ground View\": [\"vel_x\", \"vel_y\", \"vel_z\"],\n    \"Constant Distance\": [\"vel_x\", \"vel_y\", \"vel_z\", \"yaw_rate\"],  # Full control of velocity and yaw\n    \"Constant Position\": [\"vel_z\", \"yaw_rate\"],  # Controls only yaw and vertical velocity\n    \"Chase Follower\": [\"roll_rate\", \"pitch_rate\", \"yaw_rate\", \"thrust\"],  # Control for roll, pitch, yaw, and thrust\n}\n\nclass SetpointHandler:\n    def __init__(self, profile_name: str):\n        \"\"\"\n        Initializes the SetpointHandler with the specified application-based profile.\n        \n        Args:\n            profile_name (str): The name of the application profile (e.g., \"Ground View\", \"Constant Distance\", \"Constant Position\", \"Chase Follower\").\n        \n        Raises:\n            ValueError: If the profile is not defined.\n        \"\"\"\n        self.profile_name = self.normalize_profile_name(profile_name)\n        self.fields: Dict[str, float] = {}\n\n        # Initialize the fields based on the profile\n        if self.profile_name in SETPOINT_PROFILES:\n            self.initialize_fields(SETPOINT_PROFILES[self.profile_name])\n            logger.info(f\"SetpointHandler initialized with profile: {self.profile_name}\")\n        else:\n            raise ValueError(f\"Profile '{self.profile_name}' is not defined. Available profiles: {list(SETPOINT_PROFILES.keys())}\")\n        \n    @staticmethod\n    def normalize_profile_name(profile_name: str) -> str:\n        \"\"\"\n        Normalizes the profile name to ensure it matches the defined profile keys.\n\n        Args:\n            profile_name (str): The raw profile name input.\n\n        Returns:\n            str: The normalized profile name (e.g., capitalized and formatted correctly).\n        \"\"\"\n        return profile_name.replace(\"_\", \" \").title()\n\n    def initialize_fields(self, field_names: list):\n        \"\"\"\n        Initializes the fields for the given profile.\n\n        Args:\n            field_names (list): A list of field names to initialize.\n        \"\"\"\n        for field in field_names:\n            self.fields[field] = 0.0  # Initialize all fields to 0.0\n        logger.debug(f\"Fields initialized for profile '{self.profile_name}': {self.fields}\")\n\n    def set_field(self, field_name: str, value: float):\n        \"\"\"\n        Sets the value of a specific field in the setpoint.\n        \n        Args:\n            field_name (str): The name of the field to set.\n            value (float): The value to assign to the field.\n        \n        Raises:\n            ValueError: If the field name is not valid for the current profile or if the value is not a float.\n        \"\"\"\n        if field_name in self.fields:\n            try:\n                self.fields[field_name] = float(value)  # Ensure the value is a float\n                logger.debug(f\"Setpoint updated: {field_name} = {value}\")\n            except ValueError:\n                raise ValueError(f\"The value for {field_name} must be a numeric type (int or float).\")\n        else:\n            raise ValueError(f\"Field '{field_name}' is not valid for profile '{self.profile_name}'. Valid fields: {list(self.fields.keys())}\")\n\n    def get_fields(self) -> Dict[str, float]:\n        \"\"\"\n        Returns the current fields of the setpoint.\n\n        Returns:\n            dict: The current fields of the setpoint.\n        \"\"\"\n        logger.debug(f\"Retrieving setpoint fields: {self.fields}\")\n        return self.fields\n    \n    def report(self) -> str:\n        \"\"\"\n        Generates a report of the current setpoint values.\n\n        Returns:\n            str: A human-readable report of the setpoint values.\n        \"\"\"\n        report = f\"Setpoint Profile: {self.profile_name}\\n\"\n        for field, value in self.fields.items():\n            report += f\"{field}: {value}\\n\"\n        logger.info(f\"Generated setpoint report: {report}\")\n        return report\n\n    def reset_setpoints(self):\n        \"\"\"\n        Resets all setpoints to their default values (0.0).\n        \"\"\"\n        for field in self.fields:\n            self.fields[field] = 0.0\n        logger.info(f\"All setpoints for profile '{self.profile_name}' have been reset to default (0.0)\")\n\n    def timestamp_setpoint(self):\n        \"\"\"\n        Adds a timestamp to the setpoints for telemetry or logging purposes.\n        \"\"\"\n        self.fields[\"timestamp\"] = datetime.utcnow().isoformat()\n        logger.debug(f\"Setpoint timestamp added: {self.fields['timestamp']}\")\n"}
{"type": "source_file", "path": "src/classes/parameters.py", "content": "# src/classes/parameters.py\n\nimport yaml\nimport os\n\nclass Parameters:\n    \"\"\"\n    Central configuration class for the PixEagle project.\n    Automatically loads all configuration parameters from the config.yaml file.\n    Configurations are set as class variables, maintaining compatibility with existing code.\n    \"\"\"\n\n    @classmethod\n    def load_config(cls, config_file='configs/config.yaml'):\n        \"\"\"\n        Class method to load configurations from the config.yaml file and set class variables.\n        \"\"\"\n        with open(config_file, 'r') as f:\n            config = yaml.safe_load(f)\n\n        # Iterate over all top-level keys (sections)\n        for section, params in config.items():\n            for key, value in params.items():\n                # Construct the attribute name in uppercase to match existing usage\n                attr_name = key.upper()\n                # Set the attribute as a class variable\n                setattr(cls, attr_name, value)\n\n    @classmethod\n    def get_section(cls, section_name):\n        \"\"\"\n        Optional method to get all parameters in a section.\n        \"\"\"\n        # This method can be used to retrieve a dictionary of parameters for a specific section\n        pass  # Implement if needed\n\n# Load the configurations upon module import\nParameters.load_config()\n"}
{"type": "source_file", "path": "src/classes/follower.py", "content": "#src\\classes\\follower.py\nfrom .parameters import Parameters\nfrom classes.followers.ground_target_follower import GroundTargetFollower\nfrom classes.followers.constant_distance_follower import ConstantDistanceFollower\nfrom classes.followers.constant_position_follower import ConstantPositionFollower\nfrom classes.followers.chase_follower import ChaseFollower\nimport logging\nfrom typing import Tuple\n\nlogger = logging.getLogger(__name__)\n\nclass Follower:\n    \"\"\"\n    Manages different follower modes for the drone, delegating tasks to specific follower classes\n    based on the configured mode.\n    \"\"\"\n\n    def __init__(self, px4_controller, initial_target_coords: Tuple[float, float]):\n        \"\"\"\n        Initializes the Follower with the PX4 controller and initial target coordinates.\n\n        Args:\n            px4_controller (PX4Controller): The PX4 controller instance for controlling the drone.\n            initial_target_coords (tuple): Initial target coordinates for the follower.\n\n        Raises:\n            ValueError: If the initial_target_coords are not valid.\n        \"\"\"\n        # Validate initial target coordinates\n        if not isinstance(initial_target_coords, tuple) or len(initial_target_coords) != 2:\n            raise ValueError(f\"Invalid initial_target_coords: {initial_target_coords}. Must be a tuple of (x, y) coordinates.\")\n\n        self.px4_controller = px4_controller\n        self.mode = Parameters.FOLLOWER_MODE\n        self.initial_target_coords = initial_target_coords\n        self.follower = self.get_follower_mode()\n        logger.info(f\"Initialized Follower with mode: {self.mode}\")\n\n    def get_follower_mode(self):\n        \"\"\"\n        Determines and returns the appropriate follower mode based on the configuration.\n\n        Returns:\n            BaseFollower: An instance of the appropriate follower class.\n\n        Raises:\n            ValueError: If an invalid follower mode is specified in the parameters.\n        \"\"\"\n        mode_map = {\n            'ground_view': GroundTargetFollower,\n            'constant_distance': ConstantDistanceFollower,\n            'constant_position': ConstantPositionFollower,\n            'chase_follower': ChaseFollower\n        }\n\n        if self.mode in mode_map:\n            logger.debug(f\"Selected follower mode: {self.mode}\")\n            return mode_map[self.mode](self.px4_controller, self.initial_target_coords)\n        else:\n            logger.error(f\"Invalid follower mode specified: {self.mode}\")\n            raise ValueError(f\"Invalid follower mode: {self.mode}\")\n\n    def follow_target(self, target_coords: Tuple[float, float]):\n        \"\"\"\n        Asynchronously sends velocity commands to follow a target based on its coordinates.\n\n        Args:\n            target_coords (tuple): The current target coordinates to follow.\n\n        Returns:\n            The result of the follower's `follow_target` method.\n        \"\"\"\n        logger.debug(f\"Following target at coordinates: {target_coords}\")\n        try:\n            self.follower.follow_target(target_coords)\n        except Exception as e:\n            logger.error(f\"Failed to follow target at coordinates {target_coords}: {e}\")\n            raise\n\n    def get_follower_telemetry(self):\n        \"\"\"\n        Returns the latest velocity telemetry data from the current follower.\n\n        Returns:\n            dict: The latest telemetry data from the follower.\n        \"\"\"\n        try:\n            telemetry = self.follower.get_follower_telemetry()\n            logger.debug(f\"Follower telemetry: {telemetry}\")\n            return telemetry\n        except Exception as e:\n            logger.error(f\"Failed to retrieve telemetry data: {e}\")\n            return {}\n\n\n    def get_control_type(self) -> str:\n        \"\"\"\n        Determines the type of control command to send based on the current follower mode.\n        \n        Returns:\n            str: 'attitude_rate' if the follower mode uses attitude rate control, \n                'velocity_body' if it uses velocity body control.\n        \"\"\"\n        if isinstance(self.follower, ChaseFollower):  # Assuming ChaseFollower uses attitude rate control\n            return 'attitude_rate'\n        else:\n            return 'velocity_body'\n"}
{"type": "source_file", "path": "src/classes/px4_interface_manager.py", "content": "import asyncio\nimport math\nimport logging\nfrom mavsdk import System\nfrom classes.parameters import Parameters\nfrom mavsdk.offboard import OffboardError, VelocityNedYaw, VelocityBodyYawspeed, AttitudeRate\nfrom classes.setpoint_handler import SetpointHandler\n\n# Configure logging\nlogger = logging.getLogger(__name__)\n\nclass PX4InterfaceManager:\n\n    FLIGHT_MODES = {\n        458752: 'Stabilized',\n        196608: 'Position',\n        100925440: 'Land',\n        393216: 'Offboard',\n        50593792: 'Hold',\n        84148224: 'Return',\n        131072: 'Altitude',\n        65536: 'Manual',\n        327680: 'Acro',\n        33816576: 'Takeoff',\n        67371008: 'Mission',\n        151257088: 'Precission Land'\n        \n    }\n\n    def __init__(self, app_controller=None):\n        \"\"\"\n        Initializes the PX4InterfaceManager and sets up the connection to the PX4 drone.\n        Uses MAVSDK for offboard control, and optionally uses MAVLink2Rest for telemetry data.\n        \"\"\"\n        self.app_controller = app_controller\n        self.current_yaw = 0.0  # Current yaw in radians\n        self.current_pitch = 0.0  # Current pitch in radians\n        self.current_roll = 0.0  # Current roll in radians\n        self.current_altitude = 0.0  # Current altitude in meters\n        self.current_ground_speed = 0.0  # Ground speed in m/s\n        self.camera_yaw_offset = Parameters.CAMERA_YAW_OFFSET\n        self.update_task = None  # Task for telemetry updates\n        normalized_profile_name = SetpointHandler.normalize_profile_name(Parameters.FOLLOWER_MODE)\n        self.setpoint_handler = SetpointHandler(normalized_profile_name)    \n        self.active_mode = False\n        self.hover_throttle = 0.0\n        self.failsafe_active = False\n\n        # Determine if we are using MAVLink2Rest for telemetry data\n        if Parameters.USE_MAVLINK2REST and self.app_controller:\n            self.mavlink_data_manager = self.app_controller.mavlink_data_manager\n            logger.info(\"Using MAVLink2Rest for telemetry data.\")\n        else:\n            logger.info(\"Using MAVSDK for telemetry and offboard control.\")\n        \n        # Setup MAVSDK connection for both telemetry and offboard control\n        if Parameters.EXTERNAL_MAVSDK_SERVER:\n            self.drone = System(mavsdk_server_address='localhost', port=50051)\n        else:\n            self.drone = System()\n\n    async def connect(self):\n        \"\"\"\n        Connects to the drone using MAVSDK and starts telemetry updates.\n        Even when using MAVLink2Rest for telemetry, MAVSDK is still used for offboard control.\n        \"\"\"\n        await self.drone.connect(system_address=Parameters.SYSTEM_ADDRESS)\n        self.active_mode = True\n        logger.info(\"Connected to the drone.\")\n        self.update_task = asyncio.create_task(self.update_drone_data())\n\n    async def update_drone_data(self):\n        \"\"\"\n        Continuously updates the drone's telemetry data using the selected source.\n        Uses MAVLink2Rest for telemetry if enabled, otherwise uses MAVSDK.\n        The refresh rate is controlled by FOLLOWER_DATA_REFRESH_RATE.\n        \"\"\"\n        refresh_rate = Parameters.FOLLOWER_DATA_REFRESH_RATE if hasattr(Parameters, 'FOLLOWER_DATA_REFRESH_RATE') else 1\n\n        while self.active_mode:\n            try:\n                if Parameters.USE_MAVLINK2REST:\n                    await self._update_telemetry_via_mavlink2rest()\n                else:\n                    await self._update_telemetry_via_mavsdk()\n            except asyncio.CancelledError:\n                logger.warning(\"Telemetry update task was cancelled.\")\n                break\n            except Exception as e:\n                logger.error(f\"Error updating telemetry: {e}\")\n            await asyncio.sleep(refresh_rate)  # Use the refresh rate to control the update frequency\n\n    async def _update_telemetry_via_mavlink2rest(self):\n        \"\"\"\n        Updates telemetry data using MAVLink2Rest.\n        Retrieves telemetry data through the MAVLink data manager using modular methods.\n        Default values are set to zero in case of data loss or missing data.\n        \"\"\"\n        try:\n            # Fetch attitude data (roll, pitch, yaw)\n            attitude_data = await self.mavlink_data_manager.fetch_attitude_data()\n            self.current_roll = attitude_data.get(\"roll\", 0.0)\n            self.current_pitch = attitude_data.get(\"pitch\", 0.0)\n            self.current_yaw = attitude_data.get(\"yaw\", 0.0)\n            \n            # Fetch altitude data\n            altitude_data = await self.mavlink_data_manager.fetch_altitude_data()\n            self.current_altitude = altitude_data.get(\"altitude_relative\", 0.0)  # Or use \"altitude_amsl\" if required\n            self.current_ground_speed = await self.mavlink_data_manager.fetch_ground_speed()\n\n        except Exception as e:\n            logger.error(f\"Error updating telemetry via MAVLink2Rest: {e}\")\n\n    async def _update_telemetry_via_mavsdk(self):\n        try:\n            async for position in self.drone.telemetry.position():\n                self.current_altitude = position.relative_altitude_m\n            async for attitude in self.drone.telemetry.attitude_euler():\n                self.current_yaw = attitude.yaw + self.camera_yaw_offset\n                self.current_pitch = attitude.pitch\n                self.current_roll = attitude.roll\n\n            async for velocity in self.drone.telemetry.velocity_body():\n                self.current_ground_speed = velocity.x_m_s  # Forward speed in m/s\n\n        except Exception as e:\n            logger.error(f\"Error updating telemetry via MAVSDK: {e}\")\n\n    def get_orientation(self):\n        \"\"\"\n        Returns the current orientation (yaw, pitch, roll) of the drone.\n        \"\"\"\n        return self.current_yaw, self.current_pitch, self.current_roll\n    \n    def get_ground_speed(self):\n        return self.current_ground_speed\n\n\n    async def send_body_velocity_commands(self):\n        \"\"\"\n        Sends body frame velocity commands to the drone in offboard mode, based on the active profile.\n        This operation uses MAVSDK.\n        \"\"\"\n        setpoint = self.setpoint_handler.get_fields()\n        try:\n            if setpoint is None:\n                logger.error(\"Setpoint is None, cannot send commands.\")\n                return\n\n            # Initialize variables to zero for the fields that might not be present\n            vx, vy, vz, yaw_rate = 0.0, 0.0, 0.0, 0.0\n            \n            # Update values only if they are present in the current profile's setpoints\n            if 'vel_x' in setpoint:\n                vx = float(setpoint['vel_x'])\n            if 'vel_y' in setpoint:\n                vy = float(setpoint['vel_y'])\n            if 'vel_z' in setpoint:\n                vz = float(setpoint['vel_z'])\n            if 'yaw_rate' in setpoint:\n                yaw_rate = float(setpoint['yaw_rate'])\n\n            logger.debug(f\"Setting VELOCITY_BODY setpoint: Vx={vx}, Vy={vy}, Vz={vz}, Yaw rate={yaw_rate}\")\n            \n            # Send the velocity commands to the drone\n            next_setpoint = VelocityBodyYawspeed(vx, vy, vz, yaw_rate)\n            await self.drone.offboard.set_velocity_body(next_setpoint)\n\n        except OffboardError as e:\n            logger.error(f\"Failed to send offboard velocity command: {e}\")\n        except ValueError as ve:\n            logger.error(f\"ValueError: An error occurred while processing setpoint: {ve}\")\n        except Exception as ex:\n            logger.error(f\"An unexpected error occurred: {ex}\")\n            \n    async def send_attitude_rate_commands(self):\n        \"\"\"\n        Sends attitude rate commands to the drone in offboard mode.\n        This operation uses MAVSDK.\n        \"\"\"\n        setpoint = self.setpoint_handler.get_fields()\n\n        try:\n            if not isinstance(setpoint, dict):\n                logger.error(\"Setpoint is not a dictionary. Cannot send commands.\")\n                return\n\n            # Initialize variables to zero for the fields that might not be present\n            roll_rate, pitch_rate, yaw_rate, thrust = 0.0, 0.0, 0.0, self.hover_throttle\n\n            # Update values only if they are present in the current profile's setpoints\n            roll_rate = float(setpoint.get('roll_rate', 0.0))\n            pitch_rate = float(setpoint.get('pitch_rate', 0.0))\n            yaw_rate = float(setpoint.get('yaw_rate', 0.0))\n            thrust = float(setpoint.get('thrust', self.hover_throttle))\n\n            logger.debug(f\"Setting ATTITUDE_RATE setpoint: Roll Rate={roll_rate}, Pitch Rate={pitch_rate}, Yaw Rate={yaw_rate}, Thrust={thrust}\")\n            \n            # Send the attitude rate commands to the drone\n            next_setpoint = AttitudeRate(roll_rate, pitch_rate, yaw_rate, thrust)\n            await self.drone.offboard.set_attitude_rate(next_setpoint)\n\n        except OffboardError as e:\n            logger.error(f\"Failed to send offboard attitude rate command: {e}\")\n        except ValueError as ve:\n            logger.error(f\"ValueError: An error occurred while processing setpoint: {ve}\")\n        except Exception as ex:\n            logger.error(f\"An unexpected error occurred: {ex}\")\n\n\n\n\n\n    def convert_to_ned(self, vel_x, vel_y, yaw):\n        \"\"\"\n        Converts local frame velocities to NED frame using the current yaw.\n        \"\"\"\n        ned_vel_x = vel_x * math.cos(yaw) - vel_y * math.sin(yaw)\n        ned_vel_y = vel_x * math.sin(yaw) + vel_y * math.cos(yaw)\n        return ned_vel_x, ned_vel_y\n\n    async def start_offboard_mode(self):\n        \"\"\"\n        Attempts to start offboard mode on the drone using MAVSDK.\n        \"\"\"\n        result = {\"steps\": [], \"errors\": []}\n        try:\n            await self.drone.offboard.start()\n            result[\"steps\"].append(\"Offboard mode started.\")\n            logger.info(\"Offboard mode started.\")\n        except Exception as e:\n            result[\"errors\"].append(f\"Failed to start offboard mode: {e}\")\n            logger.error(f\"Failed to start offboard mode: {e}\")\n        return result\n\n    async def stop_offboard_mode(self):\n        \"\"\"\n        Stops offboard mode on the drone using MAVSDK.\n        \"\"\"\n        logger.info(\"Stopping offboard mode...\")\n        await self.drone.offboard.stop()\n\n    async def stop(self):\n        \"\"\"\n        Stops all operations and disconnects from the drone.\n        \"\"\"\n        if self.update_task:\n            self.update_task.cancel()\n            await self.update_task\n        await self.stop_offboard_mode()\n        self.active_mode = False\n        logger.info(\"Disconnected from the drone.\")\n\n    async def send_initial_setpoint(self):\n        \"\"\"\n        Sends an initial setpoint to the drone based on the current profile's control type.\n        If the control type is 'velocity_body', send zero velocities.\n        If the control type is 'attitude_rate', send zero rates and thrust.\n        \"\"\"\n        try:\n            control_type = self.app_controller.follower.get_control_type()\n\n            if control_type == 'velocity_body':\n                \n                logger.debug(\"Sending initial velocity_body setpoint (all zeros).\")\n                await self.send_body_velocity_commands()\n\n            elif control_type == 'attitude_rate':\n                \n                logger.debug(\"Sending initial attitude_rate setpoint (all zeros).\")\n                await self.send_attitude_rate_commands()\n\n            else:\n                logger.error(f\"Unknown control type: {control_type}\")\n                return\n\n        except Exception as e:\n            logger.error(f\"Error sending initial setpoint: {e}\")\n\n\n    def update_setpoint(self):\n        \"\"\"\n        Updates the current setpoint for the drone.\n        \"\"\"\n        self.last_command = self.setpoint_handler\n\n    def get_flight_mode_text(self, mode_code):\n        \"\"\"\n        Convert the flight mode code to a text label.\n        \"\"\"\n        return self.FLIGHT_MODES.get(mode_code, f\"Unknown ({mode_code})\")\n    \n    async def trigger_return_to_launch(self):\n        \"\"\"\n        Send Return to Launch as a failsafe action\n        \"\"\"\n        await self.drone.action.return_to_launch()\n        logger.info(\"Initiating RTL.\")\n\n    async def set_hover_throttle(self):\n        hover_throttle_raw =await self.mavlink_data_manager.fetch_throttle_percent()\n        self.hover_throttle = float(hover_throttle_raw) / 100.0\n        \n        \n    async def trigger_failsafe(self):\n        logging.critical(\"Initiating Return to Launch due to altitude safety violation\")\n        await self.trigger_return_to_launch()"}
{"type": "source_file", "path": "src/classes/position_estimator.py", "content": "import numpy as np\nimport time\nimport logging\nfrom filterpy.kalman import KalmanFilter\nfrom filterpy.common import Q_discrete_white_noise\n\nlogger = logging.getLogger(__name__)\n\nclass PositionEstimator:\n    def __init__(self):\n        \"\"\"\n        Initializes the PositionEstimator with a Kalman Filter configured for 2D position estimation.\n        \"\"\"\n        # Initialize the Kalman Filter with 4 state dimensions (x, y, dx, dy) and 2 measurement dimensions (x, y)\n        self.filter = KalmanFilter(dim_x=4, dim_z=2)\n        self.filter.x = np.zeros((4, 1))  # Initial state vector: [x, y, dx, dy]\n        self.filter.P *= 100  # Initial uncertainty (covariance matrix)\n        \n        # Measurement function (only position is measured, not velocity)\n        self.filter.H = np.array([[1, 0, 0, 0], [0, 1, 0, 0]])\n        \n        # Measurement noise covariance matrix\n        self.filter.R = np.eye(2) * 5  # Adjust this value based on the measurement noise characteristics\n        \n        # Default time step (dt), updated dynamically in use\n        self.dt = 0.1\n        self.update_F_and_Q(self.dt)\n\n        # Timestamp tracking for dynamic dt updates (if needed)\n        self.last_timestamp = None\n\n    def update_F_and_Q(self, dt):\n        \"\"\"\n        Updates the state transition matrix (F) and the process noise covariance matrix (Q) based on the new dt.\n\n        Args:\n            dt (float): Time step between the current and previous measurements.\n        \"\"\"\n        self.filter.F = np.array([[1, 0, dt, 0],\n                                  [0, 1, 0, dt],\n                                  [0, 0, 1, 0],\n                                  [0, 0, 0, 1]])\n\n        self.filter.Q = Q_discrete_white_noise(dim=4, dt=dt, var=0.1)\n        logger.debug(f\"Updated F and Q matrices with dt = {dt}\")\n\n    def set_dt(self, dt):\n        \"\"\"\n        Sets the time step (dt) and updates the filter's state transition matrix and process noise accordingly.\n\n        Args:\n            dt (float): The new time step to use, typically the time elapsed between frames.\n\n        Raises:\n            ValueError: If dt is non-positive.\n        \"\"\"\n        if dt <= 0:\n            raise ValueError(\"Time step (dt) must be a positive value.\")\n        \n        self.dt = dt\n        self.update_F_and_Q(dt)\n        logger.info(f\"Time step updated to {dt}\")\n\n    def predict_and_update(self, measurement):\n        \"\"\"\n        Performs the predict and update cycle of the Kalman Filter using the provided measurement.\n\n        Args:\n            measurement (list or np.ndarray): The current measurement [x, y].\n\n        Raises:\n            ValueError: If the measurement is not a 2-element list or array.\n        \"\"\"\n        if not isinstance(measurement, (list, np.ndarray)) or len(measurement) != 2:\n            raise ValueError(\"Measurement must be a list or numpy array with two elements [x, y].\")\n\n        self.filter.predict()\n        self.filter.update(np.array(measurement).reshape(2, 1))\n        logger.debug(f\"Kalman Filter updated with measurement: {measurement}\")\n\n    def get_estimate(self):\n        \"\"\"\n        Retrieves the current state estimate from the Kalman Filter.\n\n        Returns:\n            list: The estimated state [x, y, dx, dy].\n        \"\"\"\n        estimate = self.filter.x.flatten().tolist()\n        logger.debug(f\"Current state estimate: {estimate}\")\n        return estimate\n"}
{"type": "source_file", "path": "src/classes/estimators/base_estimator.py", "content": "# src/classes/estimators/base_estimator.py\n\n\"\"\"\nBaseEstimator Module\n--------------------\n\nThis module defines the abstract base class `BaseEstimator` for all estimators used in the tracking system.\n\nPurpose:\n--------\nThe `BaseEstimator` class provides an interface that all concrete estimator classes must implement. This ensures consistency and allows for easy integration and substitution of different estimation algorithms.\n\nKey Methods:\n------------\n- `predict_and_update(measurement)`: Performs the prediction and update steps using the provided measurement.\n- `get_estimate()`: Retrieves the current state estimate.\n- `reset()`: Resets the estimator to its initial state.\n- `get_normalized_estimate(frame_width, frame_height)`: Returns the normalized estimated position based on frame dimensions.\n\nExtending the Estimator:\n------------------------\nTo create a new estimator:\n1. Subclass `BaseEstimator`.\n2. Implement all abstract methods.\n3. Add the new estimator to `estimator_factory.py` for easy creation and integration.\n\nNotes:\n------\n- The interface enforces that all estimators provide normalized outputs, which is essential for consistent control inputs.\n- By using an abstract base class, the system is flexible and can accommodate various estimation techniques (e.g., particle filters, extended Kalman filters).\n- This design promotes modularity and scalability within the PixEagle project.\n\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, Tuple\n\nclass BaseEstimator(ABC):\n    \"\"\"\n    Abstract Base Class for Estimators\n\n    Defines the interface and common methods that all estimators must implement.\n    \"\"\"\n\n    @abstractmethod\n    def predict_and_update(self, measurement):\n        \"\"\"\n        Performs the predict and update steps of the estimator using the provided measurement.\n\n        Args:\n            measurement (list, tuple, or np.ndarray): The current measurement [x, y].\n\n        Raises:\n            ValueError: If the measurement is not valid.\n\n        This method should handle the core estimation logic, updating the internal state based on the measurement.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_estimate(self) -> Optional[list]:\n        \"\"\"\n        Retrieves the current state estimate from the estimator.\n\n        Returns:\n            Optional[list]: The estimated state vector or None if not available.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self):\n        \"\"\"\n        Resets the estimator to its initial state.\n\n        This method should reinitialize all internal variables and states.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_normalized_estimate(self, frame_width: int, frame_height: int) -> Optional[Tuple[float, float]]:\n        \"\"\"\n        Returns the normalized estimated position based on frame dimensions.\n\n        Args:\n            frame_width (int): Width of the video frame.\n            frame_height (int): Height of the video frame.\n\n        Returns:\n            Optional[Tuple[float, float]]: Normalized (x, y) coordinates or None.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "src/classes/followers/constant_distance_follower.py", "content": "#src/classes/followers/constant_distance_follower.py\nfrom classes.followers.base_follower import BaseFollower\nfrom classes.followers.custom_pid import CustomPID\nfrom classes.parameters import Parameters\nimport logging\nfrom typing import Tuple\n\nclass ConstantDistanceFollower(BaseFollower):\n    \"\"\"\n    ConstantDistanceFollower manages PID control to maintain a constant distance from the target.\n    Yaw control is optional in this mode.\n    \"\"\"\n\n    def __init__(self, px4_controller, initial_target_coords: Tuple[float, float]):\n        \"\"\"\n        Initializes the ConstantDistanceFollower with the given PX4 controller and initial target coordinates.\n\n        Args:\n            px4_controller (PX4Controller): Instance of PX4Controller to control the drone.\n            initial_target_coords (tuple): Initial target coordinates to set for the follower.\n        \"\"\"\n        super().__init__(px4_controller, \"Constant Distance\")  # Initialize with \"Constant Distance\" profile\n        self.yaw_enabled = Parameters.ENABLE_YAW_CONTROL\n        self.initial_target_coords = initial_target_coords\n        self.initialize_pids()\n\n    def initialize_pids(self):\n        \"\"\"\n        Initializes the PID controllers for maintaining a constant distance from the target.\n        \"\"\"\n        setpoint_x, setpoint_y = self.initial_target_coords\n\n        # Initialize Y and Z axis PID controllers\n        self.pid_y = CustomPID(\n            *self.get_pid_gains('y'), \n            setpoint=setpoint_x, \n            output_limits=(-Parameters.VELOCITY_LIMITS['y'], Parameters.VELOCITY_LIMITS['y'])\n        )\n        self.pid_z = CustomPID(\n            *self.get_pid_gains('z'), \n            setpoint=setpoint_y, \n            output_limits=(-Parameters.VELOCITY_LIMITS['z'], Parameters.VELOCITY_LIMITS['z'])\n        )\n\n        # Initialize yaw PID controller if enabled\n        if self.yaw_enabled:\n            self.pid_yaw_rate = CustomPID(\n                *self.get_pid_gains('yaw_rate'),\n                setpoint=setpoint_x, \n                output_limits=(-Parameters.MAX_YAW_RATE, Parameters.MAX_YAW_RATE)\n            )\n\n        logging.info(\"PID controllers initialized for ConstantDistanceFollower.\")\n\n    def get_pid_gains(self, axis: str) -> Tuple[float, float, float]:\n        \"\"\"\n        Retrieves the PID gains for the specified axis.\n\n        Args:\n            axis (str): The axis for which to retrieve the PID gains ('x', 'y', 'z', 'yaw_rate').\n\n        Returns:\n            Tuple[float, float, float]: The proportional, integral, and derivative gains for the axis.\n        \"\"\"\n        # Return the PID gains from the parameters\n        return Parameters.PID_GAINS[axis]['p'], Parameters.PID_GAINS[axis]['i'], Parameters.PID_GAINS[axis]['d']\n\n    def update_pid_gains(self):\n        \"\"\"\n        Updates the PID gains for Y, Z, and optionally Yaw_rate controllers based on the current settings.\n        \"\"\"\n        self.pid_y.tunings = self.get_pid_gains('y')\n        self.pid_z.tunings = self.get_pid_gains('z')\n        if self.yaw_enabled:\n            self.pid_yaw_rate.tunings = self.get_pid_gains('yaw_rate')\n\n        logging.debug(\"PID gains updated for ConstantDistanceFollower.\")\n\n    def calculate_control_commands(self, target_coords: Tuple[float, float]) -> None:\n        \"\"\"\n        Calculates and updates velocity commands based on the target coordinates.\n\n        Args:\n            target_coords (Tuple[float, float]): The target coordinates from image processing.\n        \"\"\"\n        # Update PID gains\n        self.update_pid_gains()\n\n        # Calculate errors for Y and Z axes\n        error_x = self.pid_y.setpoint - target_coords[0]\n        error_y = self.pid_z.setpoint - target_coords[1]\n\n        # Calculate velocities using the PID controllers\n        vel_x = 0  # X-axis velocity is set to zero in this mode\n        vel_y = self.pid_y(error_x)\n        vel_z = self.control_descent_constant_distance(error_y)\n\n        # Handle yaw control if enabled\n        yaw_velocity = 0\n        if self.yaw_enabled and abs(error_x) > Parameters.YAW_CONTROL_THRESHOLD:\n            yaw_velocity = self.pid_yaw_rate(error_x)\n\n        # Update the setpoint handler\n        self.px4_controller.setpoint_handler.set_field('vel_x', vel_x)\n        self.px4_controller.setpoint_handler.set_field('vel_y', vel_y)\n        self.px4_controller.setpoint_handler.set_field('vel_z', vel_z)\n        self.px4_controller.setpoint_handler.set_field('yaw_rate', yaw_velocity)\n\n        # Log the calculated velocity commands\n        logging.debug(f\"Calculated velocities - Vx: {vel_x}, Vy: {vel_y}, Vz: {vel_z}, Yaw rate: {yaw_velocity}\")\n\n    def follow_target(self, target_coords: Tuple[float, float]):\n        \"\"\"\n        Sends velocity commands to follow the target based on the coordinates.\n\n        Args:\n            target_coords (Tuple[float, float]): The target coordinates to follow.\n        \"\"\"\n        self.calculate_control_commands(target_coords)\n        #await self.px4_controller.send_body_velocity_commands(self.setpoint_handler.get_fields())\n\n    def control_descent_constant_distance(self, error_y: float) -> float:\n        \"\"\"\n        Controls the descent or climb of the drone based on the error in the y-axis (vertical position error),\n        considering altitude limits.\n\n        Args:\n            error_y (float): Error in the y-axis (vertical position error).\n\n        Returns:\n            float: The calculated Z-axis velocity (descent or climb command).\n        \"\"\"\n        current_altitude = self.px4_controller.current_altitude\n        logging.debug(f\"Current Altitude: {current_altitude}m, \"\n                      f\"Minimum Descent Height: {Parameters.MIN_DESCENT_HEIGHT}m, \"\n                      f\"Maximum Climb Height: {Parameters.MAX_CLIMB_HEIGHT}m\")\n\n        # Calculate the PID-controlled vertical command (Z velocity)\n        command = self.pid_z(error_y)\n\n        # Handle descent command\n        if command > 0:  # Descending\n            if current_altitude >= Parameters.MIN_DESCENT_HEIGHT:\n                return command\n            else:\n                logging.info(\"Altitude is at or above the minimum descent height. Descent halted.\")\n                return 0\n\n        # Handle climb command\n        else:  # Climbing\n            if current_altitude < Parameters.MAX_CLIMB_HEIGHT:\n                return command\n            else:\n                logging.info(\"Already at maximum altitude. No further climb allowed.\")\n                return 0\n"}
{"type": "source_file", "path": "src/classes/osd_handler.py", "content": "import cv2\nimport time\nimport logging\nimport numpy as np\nfrom .parameters import Parameters\n\nclass OSDHandler:\n    def __init__(self, app_controller=None):\n        \"\"\"\n        Initialize the OSDHandler with a reference to AppController.\n        \"\"\"\n        self.app_controller = app_controller\n        self.mavlink_data_manager = self.app_controller.mavlink_data_manager\n        self.osd_config = Parameters.OSD_CONFIG\n        self.logger = logging.getLogger(__name__)\n\n    def draw_osd(self, frame):\n        \"\"\"\n        Draw all enabled OSD elements on the frame.\n        \"\"\"\n        for element_name, config in self.osd_config.items():\n            if config[\"enabled\"]:\n                if element_name == \"name\":\n                    self._draw_name(frame, config)\n                elif element_name == \"datetime\":\n                    self._draw_datetime(frame, config)\n                elif element_name == \"crosshair\":\n                    self._draw_crosshair(frame, config)\n                elif element_name == \"mavlink_data\" and Parameters.MAVLINK_ENABLED:\n                    self._draw_mavlink_data(frame, config)\n                elif element_name == \"attitude_indicator\":\n                    self._draw_attitude_indicator(frame, config)\n                elif element_name == \"tracker_status\":\n                    self._draw_tracker_status(frame, config)\n                elif element_name == \"follower_status\":\n                    self._draw_follower_status(frame, config)\n        return frame\n\n    def _draw_tracker_status(self, frame, config):\n        \"\"\"\n        Draw the tracker status on the frame.\n        \"\"\"\n        status = \"Active\" if self.app_controller.tracking_started else \"Not Active\"\n        # Change color based on the status\n        color = (0, 255, 0) if self.app_controller.tracking_started else config[\"color\"]\n        position = self._convert_position(frame, config[\"position\"])\n        text = f\"Tracker: {status}\"\n        cv2.putText(frame, text, position, cv2.FONT_HERSHEY_SIMPLEX, config[\"font_size\"], color, 2)\n\n    def _draw_follower_status(self, frame, config):\n        \"\"\"\n        Draw the follower status on the frame.\n        \"\"\"\n        status = \"Active\" if self.app_controller.following_active else \"Not Active\"\n        # Change color based on the status\n        color = (0, 255, 0) if self.app_controller.following_active else config[\"color\"]\n        position = self._convert_position(frame, config[\"position\"])\n        text = f\"Follower: {status}\"\n        cv2.putText(frame, text, position, cv2.FONT_HERSHEY_SIMPLEX, config[\"font_size\"], color, 2)\n\n\n    def _convert_position(self, frame, position):\n        \"\"\"\n        Convert a position from percentage to pixel coordinates.\n        \"\"\"\n        x_percent, y_percent = position\n        height, width = frame.shape[:2]\n        x = int(width * x_percent / 100)\n        y = int(height * y_percent / 100)\n        return (x, y)\n\n    def _draw_name(self, frame, config):\n        \"\"\"\n        Draw the name on the frame.\n        \"\"\"\n        position = self._convert_position(frame, config[\"position\"])\n        cv2.putText(frame, config[\"text\"], position, cv2.FONT_HERSHEY_SIMPLEX, config[\"font_size\"], config[\"color\"], 2)\n\n    def _draw_datetime(self, frame, config):\n        \"\"\"\n        Draw the current date and time on the frame.\n        \"\"\"\n        datetime_str = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        position = self._convert_position(frame, config[\"position\"])\n        text_size = cv2.getTextSize(datetime_str, cv2.FONT_HERSHEY_SIMPLEX, config[\"font_size\"], 2)[0]\n        adjusted_position = (position[0] - text_size[0], position[1])\n        cv2.putText(frame, datetime_str, adjusted_position, cv2.FONT_HERSHEY_SIMPLEX, config[\"font_size\"], config[\"color\"], 2)\n\n    def _draw_crosshair(self, frame, config):\n        \"\"\"\n        Draw a crosshair in the center of the frame.\n        \"\"\"\n        center_x = frame.shape[1] // 2\n        center_y = frame.shape[0] // 2\n        cv2.line(frame, (center_x - config[\"length\"], center_y), (center_x + config[\"length\"], center_y), config[\"color\"], config[\"thickness\"])\n        cv2.line(frame, (center_x, center_y - config[\"length\"]), (center_x, center_y + config[\"length\"]), config[\"color\"], config[\"thickness\"])\n\n    def _format_value(self, field, value):\n        \"\"\"\n        Format the value based on the field type for better display in OSD.\n        \"\"\"\n        if value == \"N/A\":\n            return value\n\n        try:\n            if field in [\"Airspeed\", \"Groundspeed\", \"Climb\"]:  # Speeds\n                return f\"{float(value):.1f} m/s\"\n            elif field in [\"Roll\", \"Pitch\"]:  # Angles\n                value = np.rad2deg(float(value))\n                return f\"{int(value)}\"\n            elif field == \"Heading\":  # Heading (ensure it's between 0 and 359 degrees)\n                heading = float(value) % 360\n                return f\"{int(heading)}\"\n            elif field in [\"Altitude Msl\", \"Altitude Agl\"]:  # Altitudes\n                return f\"{float(value):.1f} m\" if \"agl\" in field.lower() else f\"{int(float(value))} m\"\n            elif field == \"Voltage\":  # Voltage\n                return f\"{float(value):.1f} V\"\n            elif field in [\"Latitude\", \"Longitude\"]:  # Coordinates\n                return f\"{float(value):.6f}\"\n            elif field in [\"Hdop\", \"Vdop\"]:  # DOP \n                return f\"{float(value):.2f}\"\n            elif field in [\"Satellites Visible\" ,  \"Throttle\"]:  # Satellite count and throttle values\n                return f\"{int(float(value))}\"\n            elif field == \"Flight Mode\":  # Convert flight mode code to text using PX4InterfaceManager\n                mode = int(float(value))\n                return self.app_controller.px4_interface.get_flight_mode_text(mode)\n            else:\n                return value\n        except ValueError:\n            return \"N/A\"\n\n    def _draw_mavlink_data(self, frame, config):\n        \"\"\"\n        Draw MAVLink data on the frame.\n        \"\"\"\n        if not Parameters.MAVLINK_ENABLED:\n            self.logger.info(\"MAVLink integration is disabled. Skipping MAVLink data display.\")\n            return\n\n        if not self.app_controller.mavlink_data_manager:\n            self.logger.warning(\"Mavlink data manager is not initialized. Skipping MAVLink data display.\")\n            return\n\n        for field, field_config in config[\"fields\"].items():\n            if field == \"flight_path_angle\":\n                raw_value = self.app_controller.mavlink_data_manager.get_data(field.lower())\n                if raw_value == 0.0:\n                    formatted_value = \"Level\"\n                else:\n                    try:\n                        formatted_value = f\"{float(raw_value):.1f}\"\n                    except ValueError:\n                        # self.logger.warning(f\"Invalid flight path angle: '{raw_value}'. Displaying 'N/A'.\")\n                        formatted_value = \"N/A\"\n            else:\n                formatted_value = self._format_value(field.replace(\"_\", \" \").title(), self._safe_get_float(field.lower(), default=\"N/A\"))\n\n            position = self._convert_position(frame, field_config[\"position\"])\n            text = f\"{field.replace('_', ' ').title()}: {formatted_value}\"\n            cv2.putText(\n                frame,\n                text,\n                position,\n                cv2.FONT_HERSHEY_SIMPLEX,\n                field_config[\"font_size\"],\n                field_config[\"color\"],\n                2\n            )\n\n    \n    def _draw_attitude_indicator(self, frame, config):\n        \"\"\"\n        Draw the attitude indicator on the frame.\n        \"\"\"\n        # Safely retrieve and convert roll and pitch data\n        roll = np.rad2deg(self._safe_get_float(\"roll\"))\n        pitch = np.rad2deg(self._safe_get_float(\"pitch\"))\n\n        # Define the center position for the horizon line\n        center_x, center_y = self._convert_position(frame, config[\"position\"])\n        size_x, size_y = config[\"size\"]\n\n        # Calculate horizon line position based on pitch\n        horizon_y = center_y + int(pitch * size_y / 90)  # Simple linear mapping\n\n        # Calculate the rotation matrix for roll\n        rotation_matrix = cv2.getRotationMatrix2D((center_x, center_y), -roll, 1)\n\n        # Draw the horizon line (rotated)\n        horizon_line = np.array([[center_x - size_x, horizon_y], [center_x + size_x, horizon_y]], dtype=np.float32)\n        horizon_line = cv2.transform(np.array([horizon_line]), rotation_matrix)[0]\n\n        # Ensure the coordinates are integers\n        pt1 = tuple(map(int, horizon_line[0]))\n        pt2 = tuple(map(int, horizon_line[1]))\n\n        # Draw the horizon line\n        cv2.line(frame, pt1, pt2, config[\"horizon_color\"], config[\"thickness\"])\n\n        # Draw pitch lines (e.g., every 10 degrees up and down)\n        for i in range(-90, 100, 10):\n            tick_y = center_y + int(i * size_y / 90)\n            tick_line = np.array([\n                [center_x - size_x / 4, tick_y],\n                [center_x + size_x / 4, tick_y]\n            ], dtype=np.float32)\n            tick_line = cv2.transform(np.array([tick_line]), rotation_matrix)[0]\n\n            # Ensure the coordinates are integers\n            tick_pt1 = tuple(map(int, tick_line[0]))\n            tick_pt2 = tuple(map(int, tick_line[1]))\n\n            cv2.line(frame, tick_pt1, tick_pt2, config[\"grid_color\"], config[\"thickness\"])\n\n        # Draw roll indicator (semi-circle at the top of the screen)\n        cv2.ellipse(\n            frame,\n            (center_x, center_y),\n            (int(size_x / 2), int(size_y / 2)),\n            0,\n            0,\n            180,\n            config[\"grid_color\"],\n            config[\"thickness\"]\n        )\n\n    def _safe_get_float(self, field_name, default=0.0):\n        \"\"\"\n        Safely retrieve and convert MAVLink data to float.\n        \n        Args:\n            field_name (str): The name of the MAVLink field.\n            default (float): The default value to return if retrieval fails.\n        \n        Returns:\n            float: The converted float value or the default.\n        \"\"\"\n        raw_value = self.app_controller.mavlink_data_manager.get_data(field_name)\n        if raw_value is None:\n            # self.logger.warning(f\"No data received for '{field_name}'. Defaulting to {default}.\")\n            return default\n        try:\n            return float(raw_value)\n        except ValueError:\n            # self.logger.warning(f\"Invalid data for '{field_name}': '{raw_value}'. Defaulting to {default}.\")\n            return default\n"}
{"type": "source_file", "path": "src/classes/trackers/custom_tracker.py", "content": "import numpy as np\nfrom typing import Optional, Tuple\nimport cv2\nfrom classes.parameters import Parameters  # Ensure correct import path is set\nfrom classes.position_estimator import PositionEstimator  # Ensure correct import path is set\nfrom classes.trackers.base_tracker import BaseTracker  # Ensure correct import path is set\n\nclass CustomTracker(BaseTracker):\n    \"\"\"\n    Template for implementing a custom object tracking algorithm extending the BaseTracker class.\n    \n    This template serves as a guide for developers to implement their own tracking algorithms,\n    providing the necessary steps and methods to be overridden or utilized, ensuring compatibility\n    and functionality within the existing tracking framework.\n    \"\"\"\n    \n    def __init__(self, video_handler: Optional[object] = None, detector: Optional[object] = None,app_controller: Optional[object] = None):\n        \"\"\"\n        Initializes the custom tracker with an optional video handler and detector.\n        \n        Parameters:\n        - video_handler: Handler for video streaming and processing. This could be an object managing video input.\n        - detector: Object detector for initializing tracking. This could be a pre-trained model for detecting objects.\n        \n        Note: Ensure to initialize any tracker-specific resources or parameters here.\n        \"\"\"\n        super().__init__(video_handler, detector)\n        # Initialize tracker-specific resources or parameters\n        # Example: self.custom_tracker_resource = SomeResource()\n        \n    def start_tracking(self, frame: np.ndarray, bbox: Tuple[int, int, int, int]) -> None:\n        \"\"\"\n        Starts the tracking process with the given frame and bounding box.\n        \n        This method should initialize the tracking algorithm with the provided bounding box on the frame.\n        \n        Parameters:\n        - frame: The initial video frame to start tracking.\n        - bbox: A tuple representing the bounding box (x, y, width, height) for initializing tracking.\n        \n        Note: Implement the initialization logic for your tracking algorithm here.\n        \"\"\"\n        # Example initialization logic\n        # self.tracker_specific_method.initialize(frame, bbox)\n        pass\n\n    def update(self, frame: np.ndarray) -> Tuple[bool, Tuple[int, int, int, int]]:\n        \"\"\"\n        Updates the tracker with the current frame and returns the tracking success status and the new bounding box.\n        \n        This method should apply the tracking algorithm on the current frame and return whether tracking was\n        successful along with the updated bounding box if tracking was successful.\n        \n        Parameters:\n        - frame: The current video frame to be processed by the tracking algorithm.\n        \n        Returns:\n        - A tuple containing the tracking success status and the updated bounding box (x, y, width, height).\n        \n        Note: Implement the update logic for your tracking algorithm here, updating the bounding box and tracking status.\n        \"\"\"\n        # Example update logic\n        # success, updated_bbox = self.tracker_specific_method.update(frame)\n        # return success, updated_bbox\n        pass\n\n    # Optional: Override or implement additional methods specific to your tracking algorithm.\n    # For example, handling tracker reinitialization, providing debug information, etc.\n"}
{"type": "source_file", "path": "src/classes/template_matching_detector.py", "content": "#src\\classes\\template_matching_detector.py\nimport cv2\nimport numpy as np\nfrom .detector_interface import DetectorInterface\nfrom .parameters import Parameters\n\nclass TemplateMatchingDetector(DetectorInterface):\n    def __init__(self):\n        #super().__init__()\n        self.template = None  # This will hold the template image\n        self.latest_bbox = None  # To store the latest bounding box\n        self.method = self.get_matching_method(Parameters.TEMPLATE_MATCHING_METHOD)\n\n    @staticmethod\n    def get_matching_method(method_name):\n        \"\"\"\n        Maps the method name to the OpenCV constant.\n        \"\"\"\n        methods = {\n            \"TM_CCOEFF\": cv2.TM_CCOEFF,\n            \"TM_CCOEFF_NORMED\": cv2.TM_CCOEFF_NORMED,\n            \"TM_CCORR\": cv2.TM_CCORR,\n            \"TM_CCORR_NORMED\": cv2.TM_CCORR_NORMED,\n            \"TM_SQDIFF\": cv2.TM_SQDIFF,\n            \"TM_SQDIFF_NORMED\": cv2.TM_SQDIFF_NORMED,\n        }\n        return methods.get(method_name, cv2.TM_CCOEFF_NORMED)\n\n    def set_template(self, template):\n        self.template = template\n\n    def extract_features(self, frame, bbox):\n        \"\"\"\n        Sets the template based on the provided bounding box.\n        \"\"\"\n        x, y, w, h = bbox\n        self.template = frame[y:y+h, x:x+w]\n        self.latest_bbox = bbox\n\n    def smart_redetection(self, frame, tracker=None, roi=None):\n        \"\"\"\n        Perform template matching to find the template in the current frame or within a specified ROI.\n        If more than one detection is found, choose the one closest to the last known tracker position.\n        \"\"\"\n        if self.template is None:\n            print(\"Template has not been set.\")\n            return False\n\n        frame_to_search = frame\n        x_offset, y_offset = 0, 0  # Offsets for adjusting coordinates if ROI is used\n\n        if roi is not None:\n            x, y, w, h = roi\n            frame_to_search = frame[y:y+h, x:x+w]\n            x_offset, y_offset = x, y\n\n        res = cv2.matchTemplate(frame_to_search, self.template, self.method)\n        threshold = Parameters.TEMPLATE_MATCHING_THRESHOLD  # Adjust threshold from parameters\n\n        # For methods where the minimum value is the best match\n        if self.method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n            loc = np.where(res <= (1 - threshold))\n        else:  # For methods where the maximum value is the best match\n            loc = np.where(res >= threshold)\n\n        points = list(zip(*loc[::-1]))  # Get list of (x, y) match positions\n        if not points:\n            print(\"No matches found.\")\n            return False\n\n        if tracker and hasattr(tracker, 'center'):\n            # Calculate distances from each match center to the tracker's last known center\n            distances = []\n            for pt in points:\n                match_center = (pt[0] + self.template.shape[1] / 2 + x_offset,\n                                pt[1] + self.template.shape[0] / 2 + y_offset)\n                distances.append(np.linalg.norm(np.array(match_center) - np.array(tracker.center)))\n            closest_match_idx = np.argmin(distances)  # Index of the closest match\n            top_left = points[closest_match_idx]\n        else:\n            # If no tracker info is available, default to the first match found\n            top_left = points[0]\n\n        # Adjust coordinates if ROI was used\n        top_left = (top_left[0] + x_offset, top_left[1] + y_offset)\n\n        h, w = self.template.shape[:2]\n        self.latest_bbox = (top_left[0], top_left[1], w, h)\n        return True\n\n\n\n    def draw_detection(self, frame, color=(0, 255, 255)):\n        if self.latest_bbox is None:\n            return frame\n        x, y, w, h = self.latest_bbox\n        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n        return frame\n\n    def get_latest_bbox(self):\n        return self.latest_bbox\n\n    def set_latest_bbox(self, bbox):\n        self.latest_bbox = bbox\n"}
{"type": "source_file", "path": "src/classes/followers/constant_position_follower.py", "content": "#src/classes/followers/constant_position_follower.py\nfrom classes.followers.base_follower import BaseFollower\nfrom classes.followers.custom_pid import CustomPID\nfrom classes.parameters import Parameters\nimport logging\nfrom typing import Tuple\n\nclass ConstantPositionFollower(BaseFollower):\n    \"\"\"\n    ConstantPositionFollower manages the drone to maintain a constant position relative to the target.\n    Yaw control is always enabled in this mode, and altitude control is optional.\n    \"\"\"\n\n    def __init__(self, px4_controller, initial_target_coords: Tuple[float, float]):\n        \"\"\"\n        Initializes the ConstantPositionFollower with the given PX4 controller and initial target coordinates.\n\n        Args:\n            px4_controller (PX4Controller): Instance of PX4Controller to control the drone.\n            initial_target_coords (tuple): Initial target coordinates to set for the follower.\n        \"\"\"\n        super().__init__(px4_controller, \"Constant Position\")  # Initialize with \"Constant Position\" profile\n        self.yaw_enabled = True  # Yaw control is always enabled in this mode\n        self.altitude_control_enabled = Parameters.ENABLE_ALTITUDE_CONTROL\n        self.initial_target_coords = initial_target_coords\n        self.initialize_pids()\n\n    def initialize_pids(self):\n        \"\"\"\n        Initializes the PID controllers for maintaining a constant position relative to the target.\n        \"\"\"\n        setpoint_x, setpoint_y = self.initial_target_coords\n\n        # Initialize yaw PID controller\n        self.pid_yaw_rate = CustomPID(\n            *self.get_pid_gains('yaw_rate'),\n            setpoint=setpoint_x, \n            output_limits=(-Parameters.MAX_YAW_RATE, Parameters.MAX_YAW_RATE)\n        )\n\n        # Initialize Z axis PID controller if altitude control is enabled\n        if self.altitude_control_enabled:\n            self.pid_z = CustomPID(\n                *self.get_pid_gains('z'), \n                setpoint=setpoint_y, \n                output_limits=(-Parameters.VELOCITY_LIMITS['z'], Parameters.VELOCITY_LIMITS['z'])\n            )\n\n        logging.info(\"PID controllers initialized for ConstantPositionFollower.\")\n\n    def get_pid_gains(self, axis: str) -> Tuple[float, float, float]:\n        \"\"\"\n        Retrieves the PID gains for the specified axis.\n\n        Args:\n            axis (str): The axis for which to retrieve the PID gains ('x', 'y', 'z', 'yaw_rate').\n\n        Returns:\n            Tuple[float, float, float]: The proportional, integral, and derivative gains for the axis.\n        \"\"\"\n        # Return the PID gains from the parameters\n        return Parameters.PID_GAINS[axis]['p'], Parameters.PID_GAINS[axis]['i'], Parameters.PID_GAINS[axis]['d']\n\n    def update_pid_gains(self):\n        \"\"\"\n        Updates the PID gains for Z and Yaw Rate controllers based on the current settings.\n        \"\"\"\n        self.pid_yaw_rate.tunings = self.get_pid_gains('yaw_rate')\n        if self.altitude_control_enabled:\n            self.pid_z.tunings = self.get_pid_gains('z')\n\n        logging.debug(\"PID gains updated for ConstantPositionFollower.\")\n\n    def calculate_control_commands(self, target_coords: Tuple[float, float]) -> None:\n        \"\"\"\n        Calculates and updates velocity commands based on the target coordinates.\n\n        Args:\n            target_coords (Tuple[float, float]): The target coordinates from image processing.\n        \"\"\"\n        # Update PID gains\n        self.update_pid_gains()\n\n        # Calculate yaw control\n        error_x = self.pid_yaw_rate.setpoint - target_coords[0]\n        yaw_velocity = self.pid_yaw_rate(error_x) if abs(error_x) > Parameters.YAW_CONTROL_THRESHOLD else 0\n\n        # Calculate altitude control if enabled\n        vel_z = 0\n        if self.altitude_control_enabled:\n            error_y = self.pid_z.setpoint - target_coords[1]\n            vel_z = self.control_descent_constant_distance(error_y)\n\n        # Update the setpoint handler\n        self.px4_controller.setpoint_handler.set_field('vel_z', vel_z)\n        self.px4_controller.setpoint_handler.set_field('yaw_rate', yaw_velocity)\n\n        # Log the calculated velocity commands\n        logging.debug(f\"Calculated velocities - Vz: {vel_z}, Yaw rate: {yaw_velocity}\")\n\n    def follow_target(self, target_coords: Tuple[float, float]):\n        \"\"\"\n        Sends velocity commands to follow the target based on the coordinates.\n\n        Args:\n            target_coords (Tuple[float, float]): The target coordinates to follow.\n        \"\"\"\n        self.calculate_control_commands(target_coords)\n        #await self.px4_controller.send_body_velocity_commands(self.setpoint_handler.get_fields())\n\n    def control_descent_constant_distance(self, error_y: float) -> float:\n        \"\"\"\n        Controls the descent or climb of the drone based on the error in the y-axis (vertical position error),\n        considering altitude limits.\n\n        Args:\n            error_y (float): Error in the y-axis (vertical position error).\n\n        Returns:\n            float: The calculated Z-axis velocity (descent or climb command).\n        \"\"\"\n        current_altitude = self.px4_controller.current_altitude\n        logging.debug(f\"Current Altitude: {current_altitude}m, \"\n                      f\"Minimum Descent Height: {Parameters.MIN_DESCENT_HEIGHT}m, \"\n                      f\"Maximum Climb Height: {Parameters.MAX_CLIMB_HEIGHT}m\")\n\n        # Calculate the PID-controlled vertical command (Z velocity)\n        command = self.pid_z(error_y)\n\n        # Handle descent command\n        if command > 0:  # Descending\n            if current_altitude >= Parameters.MIN_DESCENT_HEIGHT:\n                return command\n            else:\n                logging.info(\"Altitude is at or above the minimum descent height. Descent halted.\")\n                return 0\n\n        # Handle climb command\n        else:  # Climbing\n            if current_altitude < Parameters.MAX_CLIMB_HEIGHT:\n                return command\n            else:\n                logging.info(\"Already at maximum altitude. No further climb allowed.\")\n                return 0\n"}
{"type": "source_file", "path": "src/classes/tracker_to_remove.py", "content": "# src/classes/tracker.py\n\nimport time\nimport cv2\nimport logging\nimport numpy as np\nfrom collections import deque\nfrom .parameters import Parameters\n# Remove direct import of PositionEstimator\n# from .position_estimator import PositionEstimator\nfrom classes.estimators.base_estimator import BaseEstimator  # Import the estimator interface\nfrom classes.estimators.estimator_factory import create_estimator\n\nlogger = logging.getLogger(__name__)\n\nclass Tracker:\n    def __init__(self, video_handler=None, detector=None, app_controller=None):\n        \"\"\"\n        Initializes the Tracker with a specific tracking algorithm and optional video handler, detector, and estimator.\n\n        Args:\n            video_handler (VideoHandler, optional): An instance of the VideoHandler class.\n            detector (Detector, optional): An instance of the Detector class.\n            app_controller (AppController, optional): Reference to the main app controller.\n            estimator (BaseEstimator, optional): An instance of an estimator implementing BaseEstimator.\n        \"\"\"\n        self.tracker = None\n        self.video_handler = video_handler\n        self.detector = detector\n        self.app_controller = app_controller\n\n        self.bbox = None  # Current bounding box\n        self.prev_bbox = None  # Previous bounding box\n        self.center = None  # Current center of the bounding box\n        self.prev_center = None  # Previous center\n        self.initial_features = None  # Features extracted from initial bounding box\n        self.last_update_time = None\n\n        self.center_history = deque(maxlen=Parameters.CENTER_HISTORY_LENGTH)  # History of center points\n\n        # Initialize the tracker algorithm\n        self.init_tracker(Parameters.DEFAULT_TRACKING_ALGORITHM)\n\n        # Estimator (if used)\n        estimator = self.app_controller.estimator\n        self.position_estimator = estimator\n        self.estimator_enabled = estimator is not None\n        self.estimated_position_history = deque(maxlen=Parameters.ESTIMATOR_HISTORY_LENGTH)\n\n    def init_tracker(self, algorithm):\n        \"\"\"\n        Initializes the tracking algorithm based on the specified algorithm name.\n\n        Args:\n            algorithm (str): The name of the tracking algorithm to use.\n\n        Raises:\n            ValueError: If an unsupported tracking algorithm is specified.\n        \"\"\"\n        try:\n            if algorithm == \"CSRT\":\n                self.tracker = cv2.legacy.TrackerCSRT_create()\n            elif algorithm == \"KCF\":\n                self.tracker = cv2.TrackerKCF_create()\n            elif algorithm == \"BOOSTING\":\n                self.tracker = cv2.TrackerBoosting_create()\n            elif algorithm == \"MIL\":\n                self.tracker = cv2.TrackerMIL_create()\n            elif algorithm == \"TLD\":\n                self.tracker = cv2.TrackerTLD_create()\n            elif algorithm == \"MEDIANFLOW\":\n                self.tracker = cv2.TrackerMedianFlow_create()\n            elif algorithm == \"MOSSE\":\n                self.tracker = cv2.TrackerMOSSE_create()\n            elif algorithm == \"GOTURN\":\n                self.tracker = cv2.TrackerGOTURN_create()\n            else:\n                raise ValueError(f\"Unsupported tracking algorithm: {algorithm}\")\n            logger.info(f\"Initialized {algorithm} tracker.\")\n        except Exception as e:\n            logger.error(f\"Failed to initialize tracker: {e}\")\n            raise\n\n    def start_tracking(self, frame, bbox):\n        \"\"\"\n        Starts the tracking process with the given frame and bounding box.\n\n        Args:\n            frame (np.ndarray): The video frame to initialize tracking.\n            bbox (tuple): The bounding box coordinates for tracking.\n\n        Raises:\n            Exception: If the tracker is not initialized.\n        \"\"\"\n        if not self.tracker:\n            raise Exception(\"Tracker not initialized\")\n        logger.debug(f\"Initializing tracker with bbox: {bbox}\")\n        self.tracker.init(frame, bbox)\n        self.bbox = bbox\n        self._update_center()\n\n        # Extract initial features for appearance validation\n        self.initial_features = self.extract_features(frame, bbox)\n\n        self.last_update_time = time.time()\n\n        # Reset the estimator if enabled\n        if self.estimator_enabled and self.position_estimator:\n            self.position_estimator.reset()  # Reset the estimator's state\n\n    def update(self, frame):\n        \"\"\"\n        Updates the tracker with the new frame, and manages bounding box, center, and estimator.\n\n        Args:\n            frame (np.ndarray): The video frame to update the tracker.\n\n        Returns:\n            tuple: A boolean indicating success and the updated bounding box.\n        \"\"\"\n        current_time = time.time()\n        success, new_bbox = self.tracker.update(frame)\n        dt = current_time - self.last_update_time if self.last_update_time else 0.1  # Default to 0.1 if first frame\n        self.last_update_time = current_time\n\n        if success:\n            self.prev_bbox = self.bbox\n            self.bbox = new_bbox\n            self.prev_center = self.center\n            self._update_center()\n            self.center_history.append(self.center)\n\n            # Perform motion and appearance consistency checks\n            motion_consistent = self.is_motion_consistent()\n            appearance_consistent = self.is_appearance_consistent(frame)\n            confidence = self.compute_confidence(motion_consistent, appearance_consistent)\n\n            if confidence < Parameters.CONFIDENCE_THRESHOLD:\n                logger.warning(\"Low tracking confidence detected.\")\n                success = False  # Treat as failure\n            else:\n                logger.debug(f\"Tracking confidence: {confidence:.2f}\")\n\n            # Update estimator if enabled\n            if success and self.estimator_enabled and self.position_estimator:\n                self._update_estimator(dt)\n\n        return success, self.bbox\n\n    def _update_center(self):\n        \"\"\"Calculates and stores the center of the current bounding box.\"\"\"\n        x, y, w, h = self.bbox\n        self.center = (int(x + w / 2), int(y + h / 2))\n        logger.debug(f\"Updated center: {self.center}\")\n\n    def is_motion_consistent(self):\n        \"\"\"\n        Checks if the motion between frames is consistent.\n\n        Returns:\n            bool: True if motion is consistent, False otherwise.\n        \"\"\"\n        if self.prev_center is None:\n            return True  # Can't compare on the first frame\n        displacement = np.linalg.norm(np.array(self.center) - np.array(self.prev_center))\n        max_displacement = Parameters.MAX_DISPLACEMENT_THRESHOLD\n        if displacement > max_displacement:\n            logger.warning(f\"Motion inconsistency detected. Displacement: {displacement:.2f}\")\n            return False\n        return True\n\n    def extract_features(self, frame, bbox):\n        \"\"\"\n        Extracts features from the given bounding box in the frame.\n\n        Args:\n            frame (np.ndarray): The video frame.\n            bbox (tuple): The bounding box coordinates.\n\n        Returns:\n            np.ndarray: The extracted feature vector.\n        \"\"\"\n        x, y, w, h = [int(v) for v in bbox]\n        roi = frame[y:y+h, x:x+w]\n        hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n        features = cv2.calcHist([hsv_roi], [0, 1], None, [16, 16], [0, 180, 0, 256])\n        features = cv2.normalize(features, features).flatten()\n        return features\n\n    def is_appearance_consistent(self, frame):\n        \"\"\"\n        Checks if the appearance of the tracked object remains consistent.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n\n        Returns:\n            bool: True if appearance is consistent, False otherwise.\n        \"\"\"\n        current_features = self.extract_features(frame, self.bbox)\n        similarity = cv2.compareHist(self.initial_features, current_features, cv2.HISTCMP_CORREL)\n        if similarity < Parameters.APPEARANCE_THRESHOLD:\n            logger.warning(f\"Appearance inconsistency detected. Similarity: {similarity:.2f}\")\n            return False\n        return True\n\n    def compute_confidence(self, motion_consistent, appearance_consistent):\n        \"\"\"\n        Computes the overall confidence score based on motion and appearance.\n\n        Args:\n            motion_consistent (bool): Result of motion consistency check.\n            appearance_consistent (bool): Result of appearance consistency check.\n\n        Returns:\n            float: The confidence score (0 to 1).\n        \"\"\"\n        motion_confidence = 1.0 if motion_consistent else 0.0\n        appearance_confidence = 1.0 if appearance_consistent else 0.0\n        total_confidence = (motion_confidence + appearance_confidence) / 2\n        return total_confidence\n\n    def _update_estimator(self, dt):\n        \"\"\"Updates the position estimator if enabled.\"\"\"\n        self.position_estimator.set_dt(dt)\n        self.position_estimator.predict_and_update(self.center)\n        estimated_position = self.position_estimator.get_estimate()\n        self.estimated_position_history.append(estimated_position)\n        logger.debug(f\"Estimated position: {estimated_position}\")\n\n    def draw_tracking(self, frame):\n        \"\"\"\n        Draws the tracking bounding box and center on the frame.\n\n        Args:\n            frame (np.ndarray): The video frame.\n\n        Returns:\n            np.ndarray: The frame with tracking drawn.\n        \"\"\"\n        x, y, w, h = [int(v) for v in self.bbox]\n        cv2.rectangle(frame, (x, y), (x + w, y + h), Parameters.TRACKER_BOX_COLOR, 2)\n        cv2.circle(frame, self.center, 4, Parameters.TRACKER_CENTER_COLOR, -1)\n        return frame\n\n    def draw_estimate(self, frame):\n        \"\"\"\n        Draws the estimated position on the frame if the estimator is enabled.\n\n        Args:\n            frame (np.ndarray): The video frame to draw the estimate on.\n\n        Returns:\n            np.ndarray: The frame with the estimate drawn on it.\n        \"\"\"\n        if self.estimator_enabled and self.position_estimator:\n            estimated_position = self.position_estimator.get_estimate()\n            if estimated_position:\n                estimated_x, estimated_y = estimated_position[:2]\n                cv2.circle(frame, (int(estimated_x), int(estimated_y)), 5, (0, 0, 255), -1)\n                if Parameters.DISPLAY_DEVIATIONS:\n                    self._display_deviation(estimated_x, estimated_y)\n        return frame\n\n    def _display_deviation(self, estimated_x, estimated_y):\n        \"\"\"Displays the relative deviation from the frame center.\"\"\"\n        frame_center = (self.video_handler.width / 2, self.video_handler.height / 2)\n        relative_deviation_x = (estimated_x - frame_center[0]) / frame_center[0]\n        relative_deviation_y = (estimated_y - frame_center[1]) / frame_center[1]\n        logger.info(f\"Estimated relative deviation from center: (X: {relative_deviation_x:.2f}, Y: {relative_deviation_y:.2f})\")\n\n    def reinitialize_tracker(self, frame, bbox):\n        \"\"\"\n        Reinitializes the tracker with a new bounding box.\n\n        Args:\n            frame (np.ndarray): The video frame to reinitialize tracking.\n            bbox (tuple): The new bounding box coordinates.\n        \"\"\"\n        logger.info(f\"Reinitializing tracker with bbox: {bbox}\")\n        self.init_tracker(Parameters.DEFAULT_TRACKING_ALGORITHM)\n        self.start_tracking(frame, bbox)\n\n    def print_normalized_center(self):\n        \"\"\"Prints the normalized center coordinates of the bounding box.\"\"\"\n        frame_width, frame_height = self.video_handler.width, self.video_handler.height\n        norm_x = self.center[0] / frame_width\n        norm_y = self.center[1] / frame_height\n        logger.debug(f\"Normalized center: ({norm_x:.2f}, {norm_y:.2f})\")\n\n    @property\n    def normalized_center(self):\n        \"\"\"Returns the normalized center coordinates.\"\"\"\n        frame_width, frame_height = self.video_handler.width, self.video_handler.height\n        norm_x = self.center[0] / frame_width\n        norm_y = self.center[1] / frame_height\n        return (norm_x, norm_y)\n\n\n\n\n"}
{"type": "source_file", "path": "src/classes/trackers/csrt_tracker.py", "content": "# src/classes/trackers/csrt_tracker.py\n\n\"\"\"\nCSRTTracker Module\n------------------\n\nThis module implements the `CSRTTracker` class, a concrete tracker that uses the CSRT (Channel and Spatial Reliability Tracking) algorithm provided by OpenCV.\n\nProject Information:\n- Project Name: PixEagle\n- Repository: https://github.com/alireza787b/PixEagle\n- Date: October 2024\n- Author: Alireza Ghaderi\n- LinkedIn: https://www.linkedin.com/in/alireza787b\n\nOverview:\n---------\nThe `CSRTTracker` class extends the `BaseTracker` and specializes in object tracking using the CSRT algorithm. CSRT is known for its accuracy in tracking objects with rotation, scale changes, and partial occlusions.\n\nPurpose:\n--------\nThe CSRT tracker is used for tracking objects in video streams where high accuracy is required, and the objects may undergo significant changes in appearance or motion.\n\nKey Features:\n-------------\n- **High Accuracy**: Utilizes CSRT algorithm for precise tracking.\n- **Estimator Integration**: Works seamlessly with the estimator for enhanced position estimation.\n- **Confidence Calculation**: Uses standardized confidence calculation from the base tracker.\n\nUsage:\n------\nThe `CSRTTracker` can be instantiated via the `tracker_factory.py` and requires a video handler, detector, and app controller.\n\nExample:\n```python\ntracker = CSRTTracker(video_handler, detector, app_controller)\ntracker.start_tracking(initial_frame, initial_bbox)\n```\n\nNotes:\n------\n- **Estimator Dependency**: If an estimator is enabled, ensure it is properly initialized and reset.\n- **Parameter Tuning**: Adjust parameters in the `Parameters` class, such as `APPEARANCE_THRESHOLD` and `CONFIDENCE_THRESHOLD`, to optimize performance.\n- **OpenCV Version**: Requires OpenCV with the tracking module (usually `opencv-contrib-python`).\n\nReferences:\n-----------\n- OpenCV CSRT Tracker\n- CSRT Paper: Lukezic et al., \"Discriminative Correlation Filter with Channel and Spatial Reliability,\" CVPR 2017.\n\n\"\"\"\n\nimport logging\nimport time\nimport cv2\nimport numpy as np\nfrom typing import Optional, Tuple\nfrom classes.parameters import Parameters\nfrom classes.trackers.base_tracker import BaseTracker\n\nclass CSRTTracker(BaseTracker):\n    \"\"\"\n    CSRTTracker Class\n\n    Implements object tracking using the CSRT algorithm, extending the `BaseTracker`.\n\n    Attributes:\n    -----------\n    - tracker (cv2.Tracker): OpenCV CSRT tracker instance.\n    - trackerName (str): Name identifier for the tracker.\n\n    Methods:\n    --------\n    - start_tracking(frame, bbox): Initializes the tracker with the provided bounding box.\n    - update(frame): Updates the tracker and performs consistency checks.\n    - update_estimator_without_measurement(): Updates the estimator when no measurement is available.\n    - get_estimated_position(): Retrieves the current estimated position from the estimator.\n    \"\"\"\n\n    def __init__(self, video_handler: Optional[object] = None, detector: Optional[object] = None, app_controller: Optional[object] = None):\n        \"\"\"\n        Initializes the CSRT tracker with a video handler, detector, and app controller.\n\n        Args:\n            video_handler (Optional[object]): Handler for video streaming and processing.\n            detector (Optional[object]): Object detector for appearance-based methods.\n            app_controller (Optional[object]): Reference to the main application controller.\n        \"\"\"\n        super().__init__(video_handler, detector, app_controller)\n        self.tracker = cv2.TrackerCSRT_create()  # Tracker specific to CSRT\n        self.trackerName: str = \"CSRT\"\n        if self.position_estimator:\n            self.position_estimator.reset()\n\n    def start_tracking(self, frame: np.ndarray, bbox: Tuple[int, int, int, int]) -> None:\n        \"\"\"\n        Initializes the tracker with the provided bounding box on the given frame.\n\n        Args:\n            frame (np.ndarray): The initial video frame.\n            bbox (Tuple[int, int, int, int]): A tuple representing the bounding box (x, y, width, height).\n        \"\"\"\n        logging.info(f\"Initializing {self.trackerName} tracker with bbox: {bbox}\")\n        self.tracker.init(frame, bbox)\n\n        # Initialize appearance models using the detector\n        if self.detector:\n            self.detector.initial_features = self.detector.extract_features(frame, bbox)\n            self.detector.adaptive_features = self.detector.initial_features.copy()\n\n        self.prev_center = None  # Reset previous center\n        self.last_update_time = time.time()\n\n    def update(self, frame: np.ndarray) -> Tuple[bool, Tuple[int, int, int, int]]:\n        \"\"\"\n        Updates the tracker with the current frame and returns the tracking success status and the new bounding box.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n\n        Returns:\n            Tuple[bool, Tuple[int, int, int, int]]: A tuple containing the success status and the new bounding box.\n        \"\"\"\n        dt = self.update_time()\n        success, detected_bbox = self.tracker.update(frame)\n        \n        if success:\n            self.prev_center = self.center  # Store the previous center\n            self.bbox = detected_bbox\n            self.set_center((int(self.bbox[0] + self.bbox[2] / 2), int(self.bbox[1] + self.bbox[3] / 2)))\n            self.normalize_bbox()\n            self.center_history.append(self.center)\n\n            # Update adaptive appearance model using the detector\n            if self.detector:\n                current_features = self.detector.extract_features(frame, self.bbox)\n                self.detector.adaptive_features = (1 - Parameters.CSRT_APPEARANCE_LEARNING_RATE) * self.detector.adaptive_features + \\\n                                          Parameters.CSRT_APPEARANCE_LEARNING_RATE * current_features\n\n            # Compute confidence scores\n            self.compute_confidence(frame)\n            total_confidence = self.get_confidence()\n            logging.debug(f\"Total Confidence: {total_confidence}\")\n\n            # Perform consistency checks\n            if self.confidence < Parameters.CONFIDENCE_THRESHOLD:\n                logging.warning(\"Tracking failed due to low confidence.\")\n                success = False\n\n            if success and self.estimator_enabled and self.position_estimator:\n                self.position_estimator.set_dt(dt)\n                self.position_estimator.predict_and_update(np.array(self.center))\n                estimated_position = self.position_estimator.get_estimate()\n                self.estimated_position_history.append(estimated_position)\n        else:\n            logging.warning(\"Tracking update failed in tracker algorithm.\")\n            # Optionally, handle estimator update without measurement\n            \n        return success, self.bbox\n\n    def update_estimator_without_measurement(self) -> None:\n        \"\"\"\n        Updates the position estimator when no measurement is available.\n\n        This is useful when the tracker fails to provide a measurement, allowing the estimator to predict the next state.\n        \"\"\"\n        dt = self.update_time()\n        if self.estimator_enabled and self.position_estimator:\n            self.position_estimator.set_dt(dt)\n            self.position_estimator.predict_only()\n            estimated_position = self.position_estimator.get_estimate()\n            self.estimated_position_history.append(estimated_position)\n            logging.debug(f\"Estimated position (without measurement): {estimated_position}\")\n        else:\n            logging.warning(\"Estimator is not enabled or not initialized.\")\n\n    def get_estimated_position(self) -> Optional[Tuple[float, float]]:\n        \"\"\"\n        Gets the current estimated position from the estimator.\n\n        Returns:\n            Optional[Tuple[float, float]]: The estimated (x, y) position or None if unavailable.\n        \"\"\"\n        if self.estimator_enabled and self.position_estimator:\n            estimated_position = self.position_estimator.get_estimate()\n            if estimated_position and len(estimated_position) >= 2:\n                return (estimated_position[0], estimated_position[1])\n        return None\n"}
{"type": "source_file", "path": "src/classes/trackers/tracker_factory.py", "content": "# src/classes/trackers/tracker_factory.py\n\n\"\"\"\nTracker Factory Module\n----------------------\n\nThis module provides a factory function `create_tracker` to instantiate tracker objects based on a specified algorithm.\n\nProject Information:\n- Project Name: PixEagle\n- Repository: https://github.com/alireza787b/PixEagle\n- Date: October 2024\n- Author: Alireza Ghaderi\n- LinkedIn: https://www.linkedin.com/in/alireza787b\n\nOverview:\n---------\nThe factory pattern allows for the creation of tracker instances without exposing the creation logic to the client and refers to the newly created object using a common interface.\n\nPurpose:\n--------\nBy using a factory, the system can easily switch between different tracking algorithms, facilitating experimentation and optimization.\n\nUsage:\n------\nTo create a tracker:\n```python\ntracker = create_tracker(\"CSRT\", video_handler, detector, app_controller)\n```\n\nSupported Algorithms:\n---------------------\n- \"CSRT\": Channel and Spatial Reliability Tracker\n- Additional trackers can be added by implementing their classes and updating the factory.\n\nNotes:\n------\n- If an unsupported algorithm is specified, the factory raises a `ValueError`.\n- Ensure that all tracker classes are properly imported and available in the factory.\n\n\"\"\"\n\nfrom classes.trackers.csrt_tracker import CSRTTracker\nfrom classes.trackers.particle_filter_tracker import ParticleFilterTracker\n# Import other trackers as necessary\n\ndef create_tracker(algorithm: str, video_handler=None, detector=None, app_controller=None):\n    \"\"\"\n    Factory function to create tracker instances based on the specified algorithm.\n\n    Args:\n        algorithm (str): The name of the tracking algorithm (e.g., \"CSRT\").\n        video_handler (Optional[object]): Video handler instance.\n        detector (Optional[object]): Detector instance.\n        app_controller (Optional[object]): AppController instance.\n\n    Returns:\n        BaseTracker: An instance of a tracker.\n\n    Raises:\n        ValueError: If an unsupported tracking algorithm is specified.\n\n    Example:\n        ```python\n        tracker = create_tracker(\"CSRT\", video_handler, detector, app_controller)\n        ```\n    \"\"\"\n    if algorithm == \"CSRT\":\n        return CSRTTracker(video_handler, detector, app_controller)\n    elif algorithm == \"ParticleFilter\":\n        return ParticleFilterTracker(video_handler, detector, app_controller)\n    # Add other algorithms here\n    else:\n        raise ValueError(f\"Unsupported tracking algorithm: {algorithm}\")\n\n"}
{"type": "source_file", "path": "src/mock_telemetry_generator.py", "content": "# src/mock_telemetry_generator.py\n\"\"\"\nMock Telemetry Generator\n\nThis script generates simulated telemetry data for a tracking and follower system.\nIt runs a Flask server to serve the generated telemetry data as JSON responses.\nThe data includes a normalized bounding box and velocities to simulate realistic motion.\n\n- OpenCV:\n  - Default: Top-left (0, 0)\n  - Normalized: Center (0, 0), Top-left (-1, 1), Bottom-right (1, -1)\n\n- Chart.js:\n  - Normalized: Center (0, 0), Top-left (-1, 1), Bottom-right (1, -1)\n\nAxis Definitions:\n- Both systems use normalized coordinates, but the y-axis needs to be inverted for Chart.js.\n- OpenCV coordinates are typically in a range relative to the frame, but we normalize to a range of [-1, 1].\n\nTransformation and Conversion:\n- Center Point:\n  - Inverted y-coordinate to match Chart.js: { x: center[0], y: center[1] * -1 }\n\n- Bounding Box:\n  - Provided as [x, y, width, height] in OpenCV\n  - To correctly scale the bounding box for Chart.js, we need to:\n    - Double the width and height for the normalized range.\n    - Invert the y-coordinates.\n  - Transformed Coordinates:\n    - Top-left: { x: x, y: -y }\n    - Top-right: { x: x + 2*width, y: -y }\n    - Bottom-right: { x: x + 2*width, y: -y - 2*height }\n    - Bottom-left: { x: x, y: -y - 2*height }\n\"\"\"\nimport logging\nfrom flask import Flask, jsonify, Response\nfrom flask_cors import CORS\nimport threading\nimport time\nimport random\nfrom datetime import datetime\nimport signal\nimport sys\nfrom simple_pid import PID\nfrom classes.parameters import Parameters\n\n# Configuring logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\napp = Flask(__name__)\nCORS(app)\n\n# Initialize global variables for telemetry simulation\ncurrent_center = [0, 0]\nbounding_box_size = [0.2, 0.2]\nvelocities = {'vel_x': 0, 'vel_y': 0, 'vel_z': 0}\n\n# Setup PID controllers\npid_controllers = {\n    'x': PID(1.0, 0.1, 0.05, setpoint=0, output_limits=(-5, 5)),\n    'y': PID(1.0, 0.1, 0.05, setpoint=0, output_limits=(-5, 5)),\n    'z': PID(1.0, 0.1, 0.05, setpoint=0, output_limits=(-5, 5))\n}\n\ndef normalize(value, min_value=-1, max_value=1):\n    \"\"\" Normalize a value to the range [-1, 1] \"\"\"\n    return (value - min_value) / (max_value - min_value) * 2 - 1\n\ndef move_center():\n    \"\"\" Simulate movement of the center of the bounding box \"\"\"\n    global current_center\n    max_move = 0.05\n    current_center = [max(-1, min(1, current_center[i] + random.uniform(-max_move, max_move))) for i in range(2)]\n\ndef generate_telemetry_data():\n    \"\"\" Generate the bounding box and center telemetry data \"\"\"\n    move_center()\n    x, y = current_center\n    return {\n        'bounding_box': [x - bounding_box_size[0] / 2, -y - bounding_box_size[1] / 2, bounding_box_size[0], bounding_box_size[1]],\n        'center': [x, -y],\n        'timestamp': datetime.utcnow().isoformat(),\n        'tracker_started': True\n    }\n\ndef update_velocities():\n    \"\"\" Update velocities based on PID controllers \"\"\"\n    global velocities\n    target_x, target_y = current_center\n    velocities['vel_x'] = pid_controllers['x'](target_x)\n    velocities['vel_y'] = pid_controllers['y'](-target_y)\n    velocities['vel_z'] = pid_controllers['z'](-target_y)\n\n@app.route('/telemetry/tracker_data', methods=['GET'])\ndef tracker_data():\n    data = generate_telemetry_data()\n    logging.info(f\"Generated Tracker Data: {data}\")\n    return jsonify(data)\n\n@app.route('/telemetry/follower_data', methods=['GET'])\ndef follower_data():\n    update_velocities()\n    data = {\n        'vel_x': velocities['vel_x'],\n        'vel_y': velocities['vel_y'],\n        'vel_z': velocities['vel_z'],\n        'timestamp': datetime.utcnow().isoformat(),\n        'status': 'active'\n    }\n    logging.info(f\"Generated Follower Data: {data}\")\n    return jsonify(data)\n\ndef run_server():\n    app.run(host=Parameters.HTTP_STREAM_HOST, port=Parameters.HTTP_STREAM_PORT)\n\ndef graceful_shutdown(signal, frame):\n    logging.info('Shutting down gracefully...')\n    sys.exit(0)\n\nif __name__ == \"__main__\":\n    logging.info(\"Starting Mock Telemetry Generator...\")\n    signal.signal(signal.SIGINT, graceful_shutdown)\n    signal.signal(signal.SIGTERM, graceful_shutdown)\n    server_thread = threading.Thread(target=run_server)\n    server_thread.daemon = True\n    server_thread.start()\n\n    try:\n        while True:\n            time.sleep(0.5)\n    except KeyboardInterrupt:\n        graceful_shutdown(None, None)\n"}
{"type": "source_file", "path": "src/webcam_list.py", "content": "import cv2\nimport platform\n\ndef list_available_cameras(max_tested=10):\n    \"\"\"\n    Lists all available USB cameras connected to the system across different platforms.\n    The function tests up to a maximum specified number of camera indices.\n    \"\"\"\n    # Initialize an empty list to store camera info\n    camera_list = []\n\n    # Identify the operating system to use appropriate settings\n    system = platform.system()\n\n    # Check each index up to max_tested to see if it corresponds to an active camera\n    for device_index in range(max_tested):\n        if system == \"Windows\":\n            cap = cv2.VideoCapture(device_index, cv2.CAP_MSMF)  # Use Media Foundation on Windows\n        else:\n            cap = cv2.VideoCapture(device_index)  # Use default backend on Linux/Mac\n\n        if cap.isOpened():\n            # If the camera opens successfully, fetch and print some basic information\n            camera_info = {\n                'ID': device_index,\n                'Resolution': f\"{int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))}x{int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}\",\n                'FPS': cap.get(cv2.CAP_PROP_FPS)\n            }\n            camera_list.append(camera_info)\n            cap.release()\n        else:\n            # If a camera cannot be opened, release it and try the next one\n            cap.release()\n\n    return camera_list\n\n# Get the list of available cameras\ncameras = list_available_cameras()\n\n# Print the list of cameras\nif cameras:\n    print(\"Available Cameras:\")\n    for camera in cameras:\n        print(f\"ID: {camera['ID']}, Resolution: {camera['Resolution']}, FPS: {camera['FPS']}\")\nelse:\n    print(\"No available USB cameras found.\")\n"}
{"type": "source_file", "path": "src/main.py", "content": "# src/main.py\nimport logging\nfrom classes.flow_controller import FlowController\n\ndef main():\n    \"\"\"\n    Main function to initialize the application and run the main loop.\n    \"\"\"\n    logging.basicConfig(level=logging.INFO)\n    logging.debug(\"Starting main application...\")\n\n    flow_controller = FlowController()\n    flow_controller.main_loop()\n\nif __name__ == \"__main__\":\n    main()\n "}
{"type": "source_file", "path": "src/classes/webrtc_manager.py", "content": "# src/classes/webrtc_manager.py\n\nfrom aiortc import RTCPeerConnection, RTCSessionDescription, VideoStreamTrack\nfrom av import VideoFrame\nfrom fastapi import WebSocket, WebSocketDisconnect\nimport asyncio\nimport json\nimport logging\nfrom typing import Dict\nimport time\nimport fractions\n\nfrom classes.parameters import Parameters\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\nclass VideoStreamTrackCustom(VideoStreamTrack):\n    \"\"\"\n    A video stream track that pulls frames from the VideoHandler.\n    \"\"\"\n\n    def __init__(self, video_handler, frame_rate=30):\n        super().__init__()  # Initialize the base VideoStreamTrack\n        self.video_handler = video_handler\n        self.frame_rate = frame_rate\n        self.frame_interval = 1.0 / self.frame_rate\n        self.last_frame_time = time.time()\n\n    async def recv(self):\n        \"\"\"\n        Receive the next video frame.\n\n        Returns:\n            VideoFrame: The next video frame.\n        \"\"\"\n        while True:\n            current_time = time.time()\n            elapsed = current_time - self.last_frame_time\n\n            if elapsed < self.frame_interval:\n                await asyncio.sleep(self.frame_interval - elapsed)\n\n            frame = self.video_handler.get_frame()\n            if frame is not None:\n                # Validate frame format\n                if frame.dtype != 'uint8' or len(frame.shape) != 3 or frame.shape[2] != 3:\n                    logger.error(\"Invalid frame format. Expected uint8 with 3 channels (BGR).\")\n                    await asyncio.sleep(0.01)\n                    continue\n\n                try:\n                    # Convert the frame (numpy.ndarray) to a VideoFrame\n                    video_frame = VideoFrame.from_ndarray(frame, format=\"bgr24\")\n                    video_frame.pts = time.time()\n                    video_frame.time_base = fractions.Fraction(1, 1000)\n                    self.last_frame_time = current_time\n                    return video_frame\n                except Exception as e:\n                    logger.error(f\"Error converting frame to VideoFrame: {e}\")\n                    await asyncio.sleep(0.01)\n            else:\n                # No frame available; wait before retrying\n                await asyncio.sleep(0.01)\n\nclass WebRTCManager:\n    \"\"\"\n    Manages WebRTC peer connections and signaling.\n    \"\"\"\n\n    def __init__(self, video_handler):\n        \"\"\"\n        Initialize the WebRTCManager with necessary dependencies.\n\n        Args:\n            video_handler (VideoHandler): An instance of the VideoHandler class.\n        \"\"\"\n        self.video_handler = video_handler\n        self.peer_connections: Dict[str, RTCPeerConnection] = {}\n        self.logger = logging.getLogger(self.__class__.__name__)\n        self.logger.setLevel(logging.INFO)\n\n    async def signaling_handler(self, websocket: WebSocket):\n        \"\"\"\n        Handle incoming signaling messages over WebSocket.\n\n        Args:\n            websocket (WebSocket): The WebSocket connection.\n        \"\"\"\n        await websocket.accept()\n        peer_id = None\n        try:\n            async for message in websocket.iter_text():\n                data = json.loads(message)\n                msg_type = data.get(\"type\")\n                payload = data.get(\"payload\")\n                peer_id = data.get(\"peer_id\")\n\n                if not peer_id:\n                    # Assign a unique peer_id if not provided\n                    peer_id = f\"peer_{int(time.time())}\"\n                    self.peer_connections[peer_id] = RTCPeerConnection()\n                    self.logger.info(f\"Created RTCPeerConnection for {peer_id}\")\n\n                    # Handle ICE candidates from server side\n                    @self.peer_connections[peer_id].on(\"icecandidate\")\n                    async def on_icecandidate(event, peer_id=peer_id):\n                        if event.candidate:\n                            await websocket.send_text(json.dumps({\n                                \"type\": \"ice-candidate\",\n                                \"peer_id\": peer_id,\n                                \"payload\": {\n                                    \"candidate\": event.candidate.to_json()\n                                }\n                            }))\n                            self.logger.debug(f\"Sent ICE candidate to {peer_id}\")\n\n                    @self.peer_connections[peer_id].on(\"connectionstatechange\")\n                    async def on_connectionstatechange():\n                        state = self.peer_connections[peer_id].connectionState\n                        self.logger.info(f\"Connection state for {peer_id}: {state}\")\n                        if state == \"failed\":\n                            await self.peer_connections[peer_id].close()\n                            del self.peer_connections[peer_id]\n                            self.logger.info(f\"RTCPeerConnection for {peer_id} failed and closed.\")\n\n                pc = self.peer_connections.get(peer_id)\n\n                if not pc:\n                    self.logger.error(f\"No RTCPeerConnection found for peer_id: {peer_id}\")\n                    await websocket.send_text(json.dumps({\n                        \"type\": \"error\",\n                        \"message\": \"Invalid peer_id\"\n                    }))\n                    continue\n\n                # Handle different message types\n                if msg_type == \"offer\":\n                    await self.handle_offer(pc, payload, websocket, peer_id)\n                elif msg_type == \"answer\":\n                    await self.handle_answer(pc, payload, websocket, peer_id)\n                elif msg_type == \"ice-candidate\":\n                    await self.handle_ice_candidate(pc, payload, websocket, peer_id)\n                else:\n                    self.logger.warning(f\"Unknown message type: {msg_type}\")\n        except WebSocketDisconnect:\n            self.logger.info(f\"WebRTC signaling WebSocket disconnected: {peer_id}\")\n        except Exception as e:\n            self.logger.error(f\"Error in signaling_handler: {e}\")\n        finally:\n            if peer_id and peer_id in self.peer_connections:\n                await self.peer_connections[peer_id].close()\n                del self.peer_connections[peer_id]\n                self.logger.info(f\"Closed and removed RTCPeerConnection for {peer_id}\")\n\n    async def handle_offer(self, pc: RTCPeerConnection, offer: Dict, websocket: WebSocket, peer_id: str):\n        \"\"\"\n        Handle WebRTC offer from the client.\n\n        Args:\n            pc (RTCPeerConnection): The peer connection.\n            offer (Dict): The SDP offer.\n            websocket (WebSocket): The WebSocket connection.\n            peer_id (str): The unique identifier for the peer.\n        \"\"\"\n        try:\n            await pc.setRemoteDescription(RTCSessionDescription(sdp=offer[\"sdp\"], type=offer[\"type\"]))\n            self.logger.info(f\"Set remote description for {peer_id}\")\n\n            # Add video track to the peer connection\n            video_track = VideoStreamTrackCustom(self.video_handler, frame_rate=Parameters.STREAM_FPS)\n            pc.addTrack(video_track)\n            self.logger.info(f\"Added VideoStreamTrack to {peer_id}\")\n\n            # Create answer\n            answer = await pc.createAnswer()\n            await pc.setLocalDescription(answer)\n\n            # Send the answer back to the client\n            await websocket.send_text(json.dumps({\n                \"type\": pc.localDescription.type,\n                \"payload\": {\n                    \"sdp\": pc.localDescription.sdp\n                },\n                \"peer_id\": peer_id\n            }))\n            self.logger.info(f\"Sent answer to {peer_id}\")\n        except Exception as e:\n            self.logger.error(f\"Error handling offer for {peer_id}: {e}\")\n            await websocket.send_text(json.dumps({\n                \"type\": \"error\",\n                \"message\": \"Failed to handle offer\"\n            }))\n\n    async def handle_answer(self, pc: RTCPeerConnection, answer: Dict, websocket: WebSocket, peer_id: str):\n        \"\"\"\n        Handle WebRTC answer from the client.\n\n        Args:\n            pc (RTCPeerConnection): The peer connection.\n            answer (Dict): The SDP answer.\n            websocket (WebSocket): The WebSocket connection.\n            peer_id (str): The unique identifier for the peer.\n        \"\"\"\n        try:\n            await pc.setRemoteDescription(RTCSessionDescription(sdp=answer[\"sdp\"], type=answer[\"type\"]))\n            self.logger.info(f\"Set remote description (answer) for {peer_id}\")\n        except Exception as e:\n            self.logger.error(f\"Error handling answer for {peer_id}: {e}\")\n            await websocket.send_text(json.dumps({\n                \"type\": \"error\",\n                \"message\": \"Failed to handle answer\"\n            }))\n\n    async def handle_ice_candidate(self, pc: RTCPeerConnection, candidate: Dict, websocket: WebSocket, peer_id: str):\n        \"\"\"\n        Handle ICE candidate from the client.\n\n        Args:\n            pc (RTCPeerConnection): The peer connection.\n            candidate (Dict): The ICE candidate.\n            websocket (WebSocket): The WebSocket connection.\n            peer_id (str): The unique identifier for the peer.\n        \"\"\"\n        try:\n            if candidate:\n                ice_candidate = candidate.get(\"candidate\")\n                sdp_mid = candidate.get(\"sdpMid\")\n                sdp_m_line_index = candidate.get(\"sdpMLineIndex\")\n                if ice_candidate and sdp_mid and sdp_m_line_index is not None:\n                    await pc.addIceCandidate({\n                        \"candidate\": ice_candidate,\n                        \"sdpMid\": sdp_mid,\n                        \"sdpMLineIndex\": sdp_m_line_index\n                    })\n                    self.logger.info(f\"Added ICE candidate for {peer_id}\")\n        except Exception as e:\n            self.logger.error(f\"Error handling ICE candidate for {peer_id}: {e}\")\n            await websocket.send_text(json.dumps({\n                \"type\": \"error\",\n                \"message\": \"Failed to handle ICE candidate\"\n            }))\n"}
{"type": "source_file", "path": "src/classes/trackers/particle_filter_tracker.py", "content": "# src/classes/trackers/particle_filter_tracker.py\n\n\"\"\"\nParticleFilterTracker Module\n----------------------------\n\nThis module implements the `ParticleFilterTracker` class, a concrete tracker that uses an enhanced Particle Filter algorithm for object tracking.\n\nProject Information:\n- Project Name: PixEagle\n- Repository: https://github.com/alireza787b/PixEagle\n- Date: October 2024\n- Author: Alireza Ghaderi\n- LinkedIn: https://www.linkedin.com/in/alireza787b\n\nOverview:\n---------\nThe `ParticleFilterTracker` class extends the `BaseTracker` and specializes in object tracking using a particle filter with advanced techniques to handle real-world challenges.\n\nKey Enhancements:\n-----------------\n- Enhanced Motion Model with Acceleration\n- Particle Diversity Maintenance\n- Contextual Information Usage\n- Failure Recovery Mechanism\n- Optimized Resampling (Stratified Resampling)\n- Efficient Computations with Vectorization\n\nUsage:\n------\nThe `ParticleFilterTracker` can be instantiated via the `tracker_factory.py` and requires a video handler, detector, and app controller.\n\nExample:\n```python\ntracker = ParticleFilterTracker(video_handler, detector, app_controller)\ntracker.start_tracking(initial_frame, initial_bbox)\n```\n\nDependencies:\n-------------\n- NumPy\n- OpenCV\n\nNotes:\n------\n- Appearance-related methods have been moved to the detector class.\n- Confidence calculation is standardized using the `compute_confidence` method in the base tracker.\n\n\"\"\"\n\nimport logging\nimport time\nimport cv2\nimport numpy as np\nfrom typing import Optional, Tuple\nfrom classes.parameters import Parameters\nfrom classes.trackers.base_tracker import BaseTracker\n\nclass ParticleFilterTracker(BaseTracker):\n    \"\"\"\n    ParticleFilterTracker Class\n\n    Implements object tracking using an enhanced Particle Filter algorithm, extending the `BaseTracker`.\n    \"\"\"\n\n    def __init__(self, video_handler: Optional[object] = None, detector: Optional[object] = None, app_controller: Optional[object] = None):\n        \"\"\"\n        Initializes the ParticleFilterTracker with a video handler, detector, and app controller.\n\n        Args:\n            video_handler (Optional[object]): Handler for video streaming and processing.\n            detector (Optional[object]): Object detector for appearance-based methods.\n            app_controller (Optional[object]): Reference to the main application controller.\n        \"\"\"\n        super().__init__(video_handler, detector, app_controller)\n        self.trackerName: str = \"ParticleFilter\"\n        self.num_particles = int(Parameters.PF_NUM_PARTICLES)\n        self.state_dim = 6  # State vector: [x, y, vx, vy, ax, ay]\n        self.particles = None\n        self.weights = None\n        # Correctly initialize the effective particle number threshold\n        self.effective_particle_num_threshold = float(self.get_effective_particle_num_threshold())\n        if self.position_estimator:\n            self.position_estimator.reset()\n\n    def get_effective_particle_num_threshold(self) -> float:\n        \"\"\"\n        Calculates the effective particle number threshold.\n\n        Returns:\n            float: The effective particle number threshold.\n        \"\"\"\n        return Parameters.PF_EFFECTIVE_PARTICLE_NUM_THRESHOLD * Parameters.PF_NUM_PARTICLES\n\n    def start_tracking(self, frame: np.ndarray, bbox: Tuple[int, int, int, int]) -> None:\n        \"\"\"\n        Initializes the particle filter with the provided bounding box on the given frame.\n\n        Args:\n            frame (np.ndarray): The initial video frame.\n            bbox (Tuple[int, int, int, int]): A tuple representing the bounding box (x, y, width, height).\n        \"\"\"\n        logging.info(f\"Initializing {self.trackerName} tracker with bbox: {bbox}\")\n        x, y, w, h = bbox\n        center_x = x + w / 2\n        center_y = y + h / 2\n\n        # Initialize particles around the initial position with some noise\n        self.particles = np.empty((self.num_particles, self.state_dim))\n        self.particles[:, 0] = np.random.normal(center_x, Parameters.PF_INIT_POS_STD, self.num_particles)\n        self.particles[:, 1] = np.random.normal(center_y, Parameters.PF_INIT_POS_STD, self.num_particles)\n        self.particles[:, 2] = np.random.normal(0, Parameters.PF_INIT_VEL_STD, self.num_particles)\n        self.particles[:, 3] = np.random.normal(0, Parameters.PF_INIT_VEL_STD, self.num_particles)\n        self.particles[:, 4] = np.random.normal(0, Parameters.PF_INIT_ACC_STD, self.num_particles)\n        self.particles[:, 5] = np.random.normal(0, Parameters.PF_INIT_ACC_STD, self.num_particles)\n\n        # Initialize weights uniformly\n        self.weights = np.ones(self.num_particles) / self.num_particles\n\n        # Initialize appearance models using the detector\n        if self.detector:\n            self.detector.initial_template = frame[y:y+h, x:x+w].copy()\n            self.detector.initial_features = self.detector.extract_features(frame, bbox)\n            self.detector.adaptive_features = self.detector.initial_features.copy()\n\n        self.prev_center = None  # Reset previous center\n        self.last_update_time = time.time()\n\n        # Set initial bbox and center\n        self.bbox = bbox\n        self.set_center((int(center_x), int(center_y)))\n        self.normalize_bbox()\n        self.center_history.append(self.center)\n\n    def update(self, frame: np.ndarray) -> Tuple[bool, Tuple[int, int, int, int]]:\n        \"\"\"\n        Updates the particle filter with the current frame and returns the tracking success status and the new bounding box.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n\n        Returns:\n            Tuple[bool, Tuple[int, int, int, int]]: A tuple containing the success status and the new bounding box.\n        \"\"\"\n        dt = self.update_time()\n\n        # Propagate particles\n        self.propagate_particles(dt)\n\n        # Compute weights based on appearance likelihood\n        self.compute_weights(frame)\n\n        # Check if weights are all zeros\n        if np.sum(self.weights) == 0:\n            logging.warning(\"All particle weights are zero. Tracking failed.\")\n            success = False\n            # self.update_estimator_without_measurement()\n            return success, self.bbox\n\n        # Normalize weights\n        self.weights /= np.sum(self.weights)\n\n        # Estimate state\n        estimated_state = self.estimate_state()\n\n        # Update bbox and center\n        estimated_center = (int(estimated_state[0]), int(estimated_state[1]))\n        self.prev_center = self.center\n        self.set_center(estimated_center)\n        self.bbox = self.get_bbox_from_state(estimated_state)\n        self.normalize_bbox()\n        self.center_history.append(self.center)\n\n        # Update adaptive appearance model using the detector\n        if self.detector:\n            current_features = self.detector.extract_features(frame, self.bbox)\n            self.detector.adaptive_features = (1 - Parameters.PF_APPEARANCE_LEARNING_RATE) * self.detector.adaptive_features + \\\n                                      Parameters.PF_APPEARANCE_LEARNING_RATE * current_features\n\n        # Resample particles\n        self.resample_particles()\n\n        # Maintain particle diversity\n        effective_particle_num = self.compute_effective_particle_number()\n        # Ensure both variables are floats\n        effective_particle_num = float(effective_particle_num)\n        threshold = float(self.effective_particle_num_threshold)\n        if effective_particle_num < threshold:\n            self.inject_random_particles()\n\n        # Compute confidence scores\n        self.compute_confidence(frame)\n        total_confidence = self.get_confidence()\n        logging.debug(f\"Total Confidence: {total_confidence}\")\n\n        # Perform consistency checks\n        success = True\n        if self.confidence < Parameters.CONFIDENCE_THRESHOLD:\n            logging.warning(\"Tracking failed due to low confidence.\")\n            success = False\n\n        if success:\n            if self.estimator_enabled and self.position_estimator:\n                self.position_estimator.set_dt(dt)\n                self.position_estimator.predict_and_update(np.array(self.center))\n                estimated_position = self.position_estimator.get_estimate()\n                self.estimated_position_history.append(estimated_position)\n        else:\n            logging.warning(\"Tracking update failed.\")\n            # Optionally, handle estimator update without measurement\n            self.update_estimator_without_measurement()\n\n        return success, self.bbox\n\n    def propagate_particles(self, dt: float) -> None:\n        \"\"\"\n        Propagates particles based on the enhanced motion model with acceleration.\n\n        Args:\n            dt (float): Time delta since the last update.\n        \"\"\"\n        # Add process noise\n        noise_pos = np.random.normal(0, Parameters.PF_POS_STD, (self.num_particles, 2))\n        noise_vel = np.random.normal(0, Parameters.PF_VEL_STD, (self.num_particles, 2))\n        noise_acc = np.random.normal(0, Parameters.PF_ACC_STD, (self.num_particles, 2))\n\n        # Update positions\n        self.particles[:, 0] += self.particles[:, 2] * dt + 0.5 * self.particles[:, 4] * dt**2 + noise_pos[:, 0]\n        self.particles[:, 1] += self.particles[:, 3] * dt + 0.5 * self.particles[:, 5] * dt**2 + noise_pos[:, 1]\n\n        # Update velocities\n        self.particles[:, 2] += self.particles[:, 4] * dt + noise_vel[:, 0]\n        self.particles[:, 3] += self.particles[:, 5] * dt + noise_vel[:, 1]\n\n        # Update accelerations\n        self.particles[:, 4] += noise_acc[:, 0]\n        self.particles[:, 5] += noise_acc[:, 1]\n\n        # Ensure particles are within frame bounds\n        frame_width = self.video_handler.width\n        frame_height = self.video_handler.height\n        self.particles[:, 0] = np.clip(self.particles[:, 0], 0, frame_width - 1)\n        self.particles[:, 1] = np.clip(self.particles[:, 1], 0, frame_height - 1)\n\n    def compute_weights(self, frame: np.ndarray) -> None:\n        \"\"\"\n        Computes weights for each particle based on combined appearance likelihood.\n\n        Args:\n            frame (np.ndarray): The current video frame.\n        \"\"\"\n        # Precompute common variables\n        half_width = int(self.bbox[2] / 2)\n        half_height = int(self.bbox[3] / 2)\n        frame_height, frame_width = frame.shape[:2]\n\n        # Vectorized computation\n        xs = self.particles[:, 0].astype(int)\n        ys = self.particles[:, 1].astype(int)\n        x1s = np.clip(xs - half_width, 0, frame_width - 1)\n        y1s = np.clip(ys - half_height, 0, frame_height - 1)\n        x2s = np.clip(xs + half_width, 0, frame_width - 1)\n        y2s = np.clip(ys + half_height, 0, frame_height - 1)\n\n        likelihoods = np.zeros(self.num_particles)\n\n        for i in range(self.num_particles):\n            x1, y1, x2, y2 = x1s[i], y1s[i], x2s[i], y2s[i]\n            particle_bbox = (x1, y1, x2 - x1, y2 - y1)\n            if x1 >= x2 or y1 >= y2:\n                likelihoods[i] = 0\n                continue\n\n            # Compute appearance likelihood using detector\n            if self.detector:\n                particle_features = self.detector.extract_features(frame, particle_bbox)\n                color_similarity = cv2.compareHist(self.detector.adaptive_features, particle_features, cv2.HISTCMP_BHATTACHARYYA)\n                roi = frame[y1:y2, x1:x2]\n                edge_similarity = self.detector.compute_edge_similarity(self.detector.initial_template, roi)\n                total_similarity = (Parameters.PF_COLOR_WEIGHT * color_similarity +\n                                    Parameters.PF_EDGE_WEIGHT * edge_similarity)\n                likelihoods[i] = np.exp(-Parameters.PF_APPEARANCE_LIKELIHOOD_SCALE * total_similarity)\n            else:\n                likelihoods[i] = 1.0  # If no detector, assign equal weight\n\n        self.weights = likelihoods\n\n    def resample_particles(self) -> None:\n        \"\"\"\n        Resamples particles based on their weights using stratified resampling.\n        \"\"\"\n        cumulative_sum = np.cumsum(self.weights)\n        cumulative_sum[-1] = 1.0  # Ensure sum is exactly one\n\n        positions = (np.arange(self.num_particles) + np.random.uniform(0, 1)) / self.num_particles\n\n        indexes = np.zeros(self.num_particles, dtype=int)\n        i, j = 0, 0\n        while i < self.num_particles:\n            if positions[i] < cumulative_sum[j]:\n                indexes[i] = j\n                i += 1\n            else:\n                j += 1\n        self.particles = self.particles[indexes]\n        self.weights = np.ones(self.num_particles) / self.num_particles\n\n    def estimate_state(self) -> np.ndarray:\n        \"\"\"\n        Estimates the state from particles and weights.\n\n        Returns:\n            np.ndarray: The estimated state vector.\n        \"\"\"\n        estimated_state = np.average(self.particles, weights=self.weights, axis=0)\n        return estimated_state\n\n    def compute_effective_particle_number(self) -> float:\n        \"\"\"\n        Computes the effective number of particles to assess diversity.\n\n        Returns:\n            float: The effective number of particles.\n        \"\"\"\n        return 1.0 / np.sum(self.weights ** 2)\n\n    def inject_random_particles(self) -> None:\n        \"\"\"\n        Injects random particles to maintain diversity.\n\n        This helps prevent particle degeneracy.\n        \"\"\"\n        num_random_particles = int(self.num_particles * Parameters.PF_RANDOM_PARTICLE_RATIO)\n        random_indexes = np.random.choice(self.num_particles, num_random_particles, replace=False)\n\n        # Re-initialize selected particles\n        self.particles[random_indexes, 0] = np.random.uniform(0, self.video_handler.width, num_random_particles)\n        self.particles[random_indexes, 1] = np.random.uniform(0, self.video_handler.height, num_random_particles)\n        self.particles[random_indexes, 2:] = 0  # Reset velocities and accelerations\n\n    def get_bbox_from_state(self, state: np.ndarray) -> Tuple[int, int, int, int]:\n        \"\"\"\n        Constructs a bounding box from the state vector.\n\n        Args:\n            state (np.ndarray): The state vector.\n\n        Returns:\n            Tuple[int, int, int, int]: The bounding box (x, y, w, h).\n        \"\"\"\n        x_center, y_center = state[0], state[1]\n        w, h = self.bbox[2], self.bbox[3]\n        x = int(x_center - w / 2)\n        y = int(y_center - h / 2)\n        return (x, y, int(w), int(h))\n\n    def update_estimator_without_measurement(self) -> None:\n        \"\"\"\n        Updates the position estimator when no measurement is available.\n        \"\"\"\n        dt = self.update_time()\n        if self.estimator_enabled and self.position_estimator:\n            self.position_estimator.set_dt(dt)\n            self.position_estimator.predict_only()\n            estimated_position = self.position_estimator.get_estimate()\n            self.estimated_position_history.append(estimated_position)\n            logging.debug(f\"Estimated position (without measurement): {estimated_position}\")\n        else:\n            logging.warning(\"Estimator is not enabled or not initialized.\")\n\n    def get_estimated_position(self) -> Optional[Tuple[float, float]]:\n        \"\"\"\n        Gets the current estimated position from the estimator.\n\n        Returns:\n            Optional[Tuple[float, float]]: The estimated (x, y) position or None if unavailable.\n        \"\"\"\n        if self.estimator_enabled and self.position_estimator:\n            estimated_position = self.position_estimator.get_estimate()\n            if estimated_position and len(estimated_position) >= 2:\n                return (estimated_position[0], estimated_position[1])\n        return None\n"}
{"type": "source_file", "path": "src/classes/video_handler.py", "content": "import cv2\nimport time\nimport logging\nfrom collections import deque\nfrom classes.parameters import Parameters\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\nclass VideoHandler:\n    \"\"\"\n    Handles video input from various sources (video files, USB cameras, RTSP streams, UDP, HTTP, and CSI cameras).\n    Provides a mechanism to store a recent history of frames.\n    \n    Supported VIDEO_SOURCE_TYPE values:\n      - \"VIDEO_FILE\": Reads from a video file.\n      - \"USB_CAMERA\": Reads from a USB camera.\n      - \"RTSP_OPENCV\": Reads from an RTSP stream using OpenCV's default backend.\n      - \"RTSP_GSTREAMER\": Reads from an RTSP stream using a custom GStreamer pipeline.\n      - \"UDP_STREAM\": Reads from a UDP stream.\n      - \"HTTP_STREAM\": Reads from an HTTP stream.\n      - \"CSI_CAMERA\": Reads from a CSI camera using a GStreamer pipeline.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the video source based on the configuration from Parameters.\n        Also initializes frame history and computes frame delay based on the stream's FPS.\n        \"\"\"\n        self.cap = None  # OpenCV VideoCapture object\n        self.frame_history = deque(maxlen=Parameters.STORE_LAST_FRAMES)  # Frame history storage\n        self.width = None   # Video frame width\n        self.height = None  # Video frame height\n        self.delay_frame = self.init_video_source()  # Frame delay in milliseconds\n\n        # Current frames for processing and streaming\n        self.current_raw_frame = None\n        self.current_osd_frame = None\n\n        # Resized versions for streaming\n        self.current_resized_raw_frame = None\n        self.current_resized_osd_frame = None\n\n    def gstreamer_pipeline_csi(self, sensor_id=0, capture_width=1280, capture_height=720,\n                               framerate=30, flip_method=0):\n        \"\"\"\n        Constructs a GStreamer pipeline string for a CSI camera.\n        \n        Returns:\n            str: A GStreamer pipeline for CSI camera input.\n        \"\"\"\n        pipeline = (\n            \"nvarguscamerasrc sensor-id=%d ! \"\n            \"video/x-raw(memory:NVMM), width=(int)%d, height=(int)%d, format=(string)NV12, framerate=(fraction)%d/1 ! \"\n            \"nvvidconv flip-method=%d ! \"\n            \"video/x-raw, format=(string)I420, width=(int)%d, height=(int)%d ! \"\n            \"videoconvert ! \"\n            \"video/x-raw, format=(string)BGR ! \"\n            \"videoscale ! \"\n            \"appsink\"\n            % (sensor_id, capture_width, capture_height, framerate,\n               flip_method, capture_width, capture_height)\n        )\n        logger.debug(f\"Constructed CSI GStreamer pipeline: {pipeline}\")\n        return pipeline\n\n    def rtsp_gstreamer_pipeline(self, rtsp_url, latency=100):\n        \"\"\"\n        Constructs a GStreamer pipeline string for an RTSP stream using the GStreamer backend.\n        \n        Args:\n            rtsp_url (str): The RTSP stream URL.\n            latency (int): Latency in milliseconds to control buffering.\n            \n        Returns:\n            str: A GStreamer pipeline for RTSP input.\n        \"\"\"\n        pipeline = (\n            f\"rtspsrc location={rtsp_url} latency={latency} ! \"\n            \"rtph264depay ! h264parse ! avdec_h264 ! videoconvert ! appsink\"\n        )\n        logger.debug(f\"Constructed RTSP GStreamer pipeline: {pipeline}\")\n        return pipeline\n\n    def init_video_source(self, max_retries=5, retry_delay=1):\n        \"\"\"\n        Initializes the video source based on VIDEO_SOURCE_TYPE from Parameters.\n        Retries opening the video source up to max_retries if needed.\n        \n        Returns:\n            int: Frame delay (ms) computed from FPS.\n        \n        Raises:\n            ValueError: If the video source cannot be opened after the maximum retries.\n        \"\"\"\n        for attempt in range(max_retries):\n            logger.debug(f\"Attempt {attempt + 1} to open video source.\")\n            try:\n                self.cap = self._create_capture_object()\n                if self.cap and self.cap.isOpened():\n                    logger.debug(\"Successfully opened video source.\")\n                    break\n                else:\n                    logger.warning(f\"Failed to open video source on attempt {attempt + 1}.\")\n            except Exception as e:\n                logger.error(f\"Exception while opening video source: {e}\")\n            time.sleep(retry_delay)\n        else:\n            raise ValueError(\"Could not open video source after max retries.\")\n\n        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = self.cap.get(cv2.CAP_PROP_FPS) or Parameters.DEFAULT_FPS\n        delay_frame = max(int(1000 / fps), 1)\n        logger.debug(f\"Video source properties - Width: {self.width}, Height: {self.height}, FPS: {fps}\")\n        return delay_frame\n\n    def _create_capture_object(self):\n        \"\"\"\n        Creates and returns a cv2.VideoCapture object based on the VIDEO_SOURCE_TYPE.\n        \n        Returns:\n            cv2.VideoCapture: The initialized video capture object.\n        \n        Raises:\n            ValueError: If an unsupported VIDEO_SOURCE_TYPE is specified.\n        \"\"\"\n        source_initializers = {\n            \"VIDEO_FILE\": lambda: cv2.VideoCapture(Parameters.VIDEO_FILE_PATH),\n            \"USB_CAMERA\": lambda: cv2.VideoCapture(Parameters.CAMERA_INDEX),\n            \"RTSP_OPENCV\": lambda: cv2.VideoCapture(Parameters.RTSP_URL),\n            \"RTSP_GSTREAMER\": lambda: cv2.VideoCapture(\n                self.rtsp_gstreamer_pipeline(Parameters.RTSP_URL, latency=Parameters.RTSP_LATENCY),\n                cv2.CAP_GSTREAMER\n            ),\n            \"UDP_STREAM\": lambda: cv2.VideoCapture(Parameters.UDP_URL, cv2.CAP_FFMPEG),\n            \"HTTP_STREAM\": lambda: cv2.VideoCapture(Parameters.HTTP_URL),\n            \"CSI_CAMERA\": lambda: cv2.VideoCapture(\n                self.gstreamer_pipeline_csi(\n                    sensor_id=Parameters.CSI_SENSOR_ID,\n                    capture_width=Parameters.CSI_WIDTH,\n                    capture_height=Parameters.CSI_HEIGHT,\n                    framerate=Parameters.CSI_FRAMERATE,\n                    flip_method=Parameters.CSI_FLIP_METHOD,\n                ),\n                cv2.CAP_GSTREAMER\n            ),\n        }\n        if Parameters.VIDEO_SOURCE_TYPE not in source_initializers:\n            raise ValueError(f\"Unsupported video source type: {Parameters.VIDEO_SOURCE_TYPE}\")\n        logger.debug(f\"Initializing video source: {Parameters.VIDEO_SOURCE_TYPE}\")\n        return source_initializers[Parameters.VIDEO_SOURCE_TYPE]()\n\n    def get_frame(self):\n        \"\"\"\n        Reads and returns the next frame from the video source.\n        Also stores the frame in the history.\n        \n        Returns:\n            np.ndarray or None: The captured frame or None if reading fails.\n        \"\"\"\n        if self.cap:\n            ret, frame = self.cap.read()\n            if ret:\n                self.current_raw_frame = frame\n                self.frame_history.append(frame)\n                return frame\n            else:\n                logger.warning(\"Failed to read frame from video source.\")\n                return None\n        else:\n            logger.error(\"Video capture object is not initialized.\")\n            return None\n\n    def get_last_frames(self):\n        \"\"\"\n        Returns:\n            list: A list of the most recent frames stored in history.\n        \"\"\"\n        return list(self.frame_history)\n\n    def clear_frame_history(self):\n        \"\"\"Clears the stored frame history.\"\"\"\n        self.frame_history.clear()\n\n    def update_resized_frames(self, width, height):\n        \"\"\"\n        Resizes the raw and OSD frames for streaming.\n        \n        Args:\n            width (int): The target width.\n            height (int): The target height.\n        \"\"\"\n        # Resize raw frame\n        if self.current_raw_frame is not None:\n            try:\n                self.current_resized_raw_frame = cv2.resize(self.current_raw_frame, (width, height))\n            except Exception as e:\n                logger.error(f\"Error resizing raw frame: {e}\")\n                self.current_resized_raw_frame = None\n        else:\n            self.current_resized_raw_frame = None\n\n        # Resize OSD frame\n        if self.current_osd_frame is not None:\n            try:\n                self.current_resized_osd_frame = cv2.resize(self.current_osd_frame, (width, height))\n            except Exception as e:\n                logger.error(f\"Error resizing OSD frame: {e}\")\n                self.current_resized_osd_frame = None\n        else:\n            self.current_resized_osd_frame = None\n\n    def release(self):\n        \"\"\"\n        Releases the video capture object and any associated resources.\n        \"\"\"\n        if self.cap:\n            self.cap.release()\n            logger.debug(\"Video source released.\")\n\n    def test_video_feed(self):\n        \"\"\"\n        Displays the video feed in a window for testing purposes.\n        Press 'q' to exit the test.\n        \"\"\"\n        logger.info(\"Testing video feed. Press 'q' to exit.\")\n        while True:\n            frame = self.get_frame()\n            if frame is None:\n                logger.info(\"No more frames or an error occurred.\")\n                break\n            cv2.imshow(\"Test Video Feed\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n        self.release()\n        cv2.destroyAllWindows()\n"}
