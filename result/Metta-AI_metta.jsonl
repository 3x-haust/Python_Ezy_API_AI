{"repo_info": {"repo_name": "metta", "repo_owner": "Metta-AI", "repo_url": "https://github.com/Metta-AI/metta"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_predicting_actor_critic.py", "content": "import unittest\nimport gymnasium as gym\nimport numpy as np\nimport torch\nfrom rl.sample_factory.predicting_actor_critic import PredictingActorCritic\nfrom types import SimpleNamespace\nfrom sample_factory.algo.utils.context import SampleFactoryContext\nfrom torch import nn\nfrom sample_factory.model.encoder import Encoder\n\nclass MockObsPredictor(nn.Module):\n    def __init__(self, obs_shape):\n        super().__init__()\n        self.obs_shape = obs_shape\n        self.return_value = torch.zeros(self.obs_shape)\n\n    def forward(self, x):\n        return self.return_value\n\nclass MockEncoder(Encoder):\n    def __init__(self, cfg, obs_space):\n        super().__init__(cfg)\n        self.obs_shape = obs_space[\"obs\"].shape\n        self.obs_size = np.prod(self.obs_shape)\n\n    def forward(self, obs_dict):\n        return torch.zeros((obs_dict[\"obs\"].shape[0], self.obs_size))\n\n    def get_out_size(self) -> int:\n        return self.obs_size\n\n\nclass TestPredictingActorCritic(unittest.TestCase):\n    def setUp(self):\n        sf_context = SampleFactoryContext()\n        sf_context.model_factory.register_encoder_factory(MockEncoder)\n\n        self.cfg = SimpleNamespace(\n            normalize_input=False,\n            obs_subtract_mean=False,\n            obs_scale=1,\n            normalize_returns=False,\n            nonlinearity=\"relu\",\n            use_rnn=True,\n            rnn_type=\"gru\",\n            rnn_size=256,\n            rnn_num_layers=2,\n            decoder_mlp_layers=[],\n            adaptive_stddev=False,\n            continuous_tanh_scale=0,\n            policy_init_gain=1.0,\n            policy_initialization=\"orthogonal\"\n            )\n        self.obs_space = {\n            'obs': gym.spaces.Box(\n                low=0, high=255, shape=(1, 2, 3), dtype=np.uint8)\n        }\n        self.action_space = gym.spaces.Tuple([gym.spaces.Discrete(4), gym.spaces.Box(-1, 1, (1,))])\n        self.actor_critic = PredictingActorCritic(sf_context.model_factory, self.obs_space, self.action_space, self.cfg)\n        self.actor_critic.obs_predictor = MockObsPredictor(self.actor_critic._obs_size)\n\n    def test_forward(self):\n        batch_size = 23\n        obs = torch.tensor([self.obs_space['obs'].sample() for _ in range(batch_size)])\n        obs_dict = {'obs': obs}\n\n        errors = torch.rand((batch_size, self.actor_critic._obs_size))\n        self.actor_critic.obs_predictor.return_value = \\\n            obs.view(batch_size, -1).to(torch.float32) + errors\n\n        rnn_states = torch.zeros((batch_size, 2, self.cfg.rnn_size))\n        result = self.actor_critic.forward(obs_dict, rnn_states, values_only=False)\n\n        torch.all((result[\"actions\"][:,1] - torch.mean(errors ** 2, dim=1)).eq(0))\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "source_file", "path": "agent/lib/lstm.py", "content": "from tensordict import TensorDict\nimport torch\nimport torch.nn as nn\n\nfrom agent.lib.metta_layer import LayerBase\n\n# file name test, delete this comment after testing\n\nclass LSTM(LayerBase):\n    def __init__(self, obs_shape, hidden_size, **cfg):\n        '''Taken from models.py.\n        Wraps your policy with an LSTM without letting you shoot yourself in the\n        foot with bad transpose and shape operations. This saves much pain.'''\n\n        super().__init__(**cfg)\n        self.obs_shape = obs_shape\n        self.hidden_size = hidden_size\n        self.num_layers = self._nn_params['num_layers']\n\n    def _make_net(self):\n        net = nn.LSTM(\n            self._input_size,\n            self.hidden_size,\n            **self._nn_params\n        )\n\n        for name, param in net.named_parameters():\n            if \"bias\" in name:\n                nn.init.constant_(param, 1) # Joseph originally had this as 0\n            elif \"weight\" in name:\n                nn.init.orthogonal_(param, 1.0) # torch's default is uniform\n\n        return net\n\n    def _forward(self, td: TensorDict):\n        x = td['x']\n        hidden = td[self._input_source]\n        state = td[\"state\"]\n\n        if state is not None:\n            split_size = self.num_layers\n            state = (state[:split_size], state[split_size:])\n\n        x_shape, space_shape = x.shape, self.obs_shape\n        x_n, space_n = len(x_shape), len(space_shape)\n        if x_shape[-space_n:] != space_shape:\n            raise ValueError('Invalid input tensor shape', x.shape)\n\n        if x_n == space_n + 1:\n            B, TT = x_shape[0], 1\n        elif x_n == space_n + 2:\n            B, TT = x_shape[:2]\n        else:\n            raise ValueError('Invalid input tensor shape', x.shape)\n\n        if state is not None:\n            assert state[0].shape[1] == state[1].shape[1] == B\n        assert hidden.shape == (B*TT, self._input_size)\n\n        hidden = hidden.reshape(B, TT, self._input_size)\n        hidden = hidden.transpose(0, 1)\n\n        hidden, state = self._net(hidden, state)\n\n        hidden = hidden.transpose(0, 1)\n        hidden = hidden.reshape(B*TT, self.hidden_size)\n\n        if state is not None:\n            state = tuple(s.detach() for s in state)\n            state = torch.cat(state, dim=0)\n\n        td[self._name] = hidden\n        td[\"state\"] = state\n\n        return td\n"}
{"type": "source_file", "path": "devops/aws/batch/compute_environment.py", "content": "#!/usr/bin/env python3\n\"\"\"\nAWS Batch Compute Environment Utilities\n\nThis module provides functions for interacting with AWS Batch compute environments.\n\"\"\"\n\nimport boto3\nfrom botocore.config import Config\nfrom tabulate import tabulate\n\ndef get_boto3_client(service_name='batch'):\n    \"\"\"Get a boto3 client with standard configuration.\"\"\"\n    config = Config(retries={'max_attempts': 10, 'mode': 'standard'}, max_pool_connections=50)\n    return boto3.client(service_name, config=config)\n\ndef list_compute_environments():\n    \"\"\"List all available AWS Batch compute environments.\"\"\"\n    batch = get_boto3_client()\n\n    try:\n        response = batch.describe_compute_environments()\n        compute_envs = response['computeEnvironments']\n\n        # Format the output\n        table_data = []\n        for ce in compute_envs:\n            name = ce['computeEnvironmentName']\n\n            # Get compute resources if available\n            compute_resources = ce.get('computeResources', {})\n            instance_types = compute_resources.get('instanceTypes', [])\n            instance_type_str = ', '.join(instance_types) if instance_types else 'N/A'\n\n            # Get ECS cluster and count instances\n            num_instances = 0\n            ecs_cluster_arn = ce.get('ecsClusterArn')\n            if ecs_cluster_arn:\n                ec2_instances = get_ec2_instances_for_cluster(ecs_cluster_arn)\n                num_instances = len(ec2_instances)\n\n            table_data.append([name, instance_type_str, num_instances])\n\n        # Print the table\n        headers = ['Name', 'Instance Types', 'Num Instances']\n        print(tabulate(table_data, headers=headers, tablefmt='grid'))\n\n        return [ce['computeEnvironmentName'] for ce in compute_envs]\n    except Exception as e:\n        print(f\"Error retrieving compute environments: {str(e)}\")\n        return []\n\ndef get_ec2_instances_for_cluster(cluster_arn):\n    \"\"\"Get EC2 instances for an ECS cluster.\"\"\"\n    ecs = get_boto3_client('ecs')\n    ec2 = get_boto3_client('ec2')\n\n    try:\n        # Get container instances for the cluster\n        container_instances = []\n        paginator = ecs.get_paginator('list_container_instances')\n        for page in paginator.paginate(cluster=cluster_arn):\n            if page.get('containerInstanceArns'):\n                container_instances.extend(page['containerInstanceArns'])\n\n        if not container_instances:\n            return []\n\n        # Get container instance details\n        container_instance_details = []\n        # Process in batches of 100 (AWS API limit)\n        for i in range(0, len(container_instances), 100):\n            batch = container_instances[i:i+100]\n            response = ecs.describe_container_instances(\n                cluster=cluster_arn,\n                containerInstances=batch\n            )\n            container_instance_details.extend(response['containerInstances'])\n\n        # Extract EC2 instance IDs\n        ec2_instance_ids = [ci['ec2InstanceId'] for ci in container_instance_details]\n\n        if not ec2_instance_ids:\n            return []\n\n        # Get EC2 instance details\n        ec2_instances = []\n        # Process in batches of 100 (AWS API limit)\n        for i in range(0, len(ec2_instance_ids), 100):\n            batch = ec2_instance_ids[i:i+100]\n            response = ec2.describe_instances(InstanceIds=batch)\n            for reservation in response['Reservations']:\n                ec2_instances.extend(reservation['Instances'])\n\n        return ec2_instances\n    except Exception as e:\n        print(f\"Error retrieving EC2 instances for cluster: {str(e)}\")\n        return []\n\ndef get_gpu_count(instance_type):\n    \"\"\"Get the number of GPUs for an instance type.\"\"\"\n    # Extract instance family and size\n    parts = instance_type.split('.')\n    if len(parts) < 2:\n        return 0\n\n    family = parts[0]\n    size = parts[1]\n\n    # Known GPU instance types and their GPU counts\n    gpu_instances = {\n        # P2 instances\n        'p2.xlarge': 1, 'p2.8xlarge': 8, 'p2.16xlarge': 16,\n\n        # P3 instances\n        'p3.2xlarge': 1, 'p3.8xlarge': 4, 'p3.16xlarge': 8, 'p3dn.24xlarge': 8,\n\n        # P4 instances\n        'p4d.24xlarge': 8, 'p4de.24xlarge': 8,\n\n        # G3 instances\n        'g3s.xlarge': 1, 'g3.4xlarge': 1, 'g3.8xlarge': 2, 'g3.16xlarge': 4,\n\n        # G4 instances\n        'g4dn.xlarge': 1, 'g4dn.2xlarge': 1, 'g4dn.4xlarge': 1, 'g4dn.8xlarge': 1,\n        'g4dn.16xlarge': 1, 'g4dn.12xlarge': 4, 'g4dn.metal': 8,\n\n        # G5 instances\n        'g5.xlarge': 1, 'g5.2xlarge': 1, 'g5.4xlarge': 1, 'g5.8xlarge': 1,\n        'g5.16xlarge': 1, 'g5.12xlarge': 4, 'g5.24xlarge': 4, 'g5.48xlarge': 8,\n\n        # G6 instances\n        'g6.xlarge': 1, 'g6.2xlarge': 1, 'g6.4xlarge': 1, 'g6.8xlarge': 1,\n        'g6.12xlarge': 4, 'g6.16xlarge': 1, 'g6.24xlarge': 4, 'g6.48xlarge': 8,\n    }\n\n    # Check if the instance type is in our known list\n    if instance_type in gpu_instances:\n        return gpu_instances[instance_type]\n\n    # For unknown instance types, try to infer based on family\n    if family == 'p2':\n        if size == 'xlarge':\n            return 1\n        elif size == '8xlarge':\n            return 8\n        elif size == '16xlarge':\n            return 16\n    elif family == 'p3':\n        if size == '2xlarge':\n            return 1\n        elif size == '8xlarge':\n            return 4\n        elif size == '16xlarge':\n            return 8\n        elif size == '24xlarge' or 'dn.24xlarge' in instance_type:\n            return 8\n    elif family == 'p4':\n        if '24xlarge' in size:\n            return 8\n    elif family == 'g3':\n        if 's.xlarge' in instance_type:\n            return 1\n        elif size == '4xlarge':\n            return 1\n        elif size == '8xlarge':\n            return 2\n        elif size == '16xlarge':\n            return 4\n    elif family == 'g4':\n        if 'dn.12xlarge' in instance_type:\n            return 4\n        elif 'dn.metal' in instance_type:\n            return 8\n        elif 'dn' in instance_type:\n            return 1\n    elif family == 'g5':\n        if size == '12xlarge':\n            return 4\n        elif size == '24xlarge':\n            return 4\n        elif size == '48xlarge':\n            return 8\n        else:\n            return 1\n    elif family == 'g6':\n        if size == '12xlarge':\n            return 4\n        elif size == '24xlarge':\n            return 4\n        elif size == '48xlarge':\n            return 8\n        else:\n            return 1\n\n    # Default to 0 if we can't determine\n    return 0\n\ndef get_instance_specs(instance_type):\n    \"\"\"Get the vCPU and memory specifications for an instance type.\"\"\"\n    # Extract instance family and size\n    parts = instance_type.split('.')\n    if len(parts) < 2:\n        return (0, 0)\n\n    family = parts[0]\n    size = parts[1]\n\n    # Common instance types and their specs (vCPUs, Memory in GiB)\n    instance_specs = {\n        # General Purpose\n        't2.micro': (1, 1), 't2.small': (1, 2), 't2.medium': (2, 4), 't2.large': (2, 8), 't2.xlarge': (4, 16), 't2.2xlarge': (8, 32),\n        't3.micro': (2, 1), 't3.small': (2, 2), 't3.medium': (2, 4), 't3.large': (2, 8), 't3.xlarge': (4, 16), 't3.2xlarge': (8, 32),\n        't4g.micro': (2, 1), 't4g.small': (2, 2), 't4g.medium': (2, 4), 't4g.large': (2, 8), 't4g.xlarge': (4, 16), 't4g.2xlarge': (8, 32),\n\n        'm4.large': (2, 8), 'm4.xlarge': (4, 16), 'm4.2xlarge': (8, 32), 'm4.4xlarge': (16, 64), 'm4.10xlarge': (40, 160), 'm4.16xlarge': (64, 256),\n        'm5.large': (2, 8), 'm5.xlarge': (4, 16), 'm5.2xlarge': (8, 32), 'm5.4xlarge': (16, 64), 'm5.8xlarge': (32, 128), 'm5.12xlarge': (48, 192), 'm5.16xlarge': (64, 256), 'm5.24xlarge': (96, 384),\n        'm6g.large': (2, 8), 'm6g.xlarge': (4, 16), 'm6g.2xlarge': (8, 32), 'm6g.4xlarge': (16, 64), 'm6g.8xlarge': (32, 128), 'm6g.12xlarge': (48, 192), 'm6g.16xlarge': (64, 256),\n\n        # Compute Optimized\n        'c4.large': (2, 3.75), 'c4.xlarge': (4, 7.5), 'c4.2xlarge': (8, 15), 'c4.4xlarge': (16, 30), 'c4.8xlarge': (36, 60),\n        'c5.large': (2, 4), 'c5.xlarge': (4, 8), 'c5.2xlarge': (8, 16), 'c5.4xlarge': (16, 32), 'c5.9xlarge': (36, 72), 'c5.18xlarge': (72, 144),\n        'c6g.large': (2, 4), 'c6g.xlarge': (4, 8), 'c6g.2xlarge': (8, 16), 'c6g.4xlarge': (16, 32), 'c6g.8xlarge': (32, 64), 'c6g.12xlarge': (48, 96), 'c6g.16xlarge': (64, 128),\n\n        # Memory Optimized\n        'r4.large': (2, 15.25), 'r4.xlarge': (4, 30.5), 'r4.2xlarge': (8, 61), 'r4.4xlarge': (16, 122), 'r4.8xlarge': (32, 244), 'r4.16xlarge': (64, 488),\n        'r5.large': (2, 16), 'r5.xlarge': (4, 32), 'r5.2xlarge': (8, 64), 'r5.4xlarge': (16, 128), 'r5.8xlarge': (32, 256), 'r5.12xlarge': (48, 384), 'r5.16xlarge': (64, 512), 'r5.24xlarge': (96, 768),\n        'r6g.large': (2, 16), 'r6g.xlarge': (4, 32), 'r6g.2xlarge': (8, 64), 'r6g.4xlarge': (16, 128), 'r6g.8xlarge': (32, 256), 'r6g.12xlarge': (48, 384), 'r6g.16xlarge': (64, 512),\n\n        # GPU Instances\n        'p2.xlarge': (4, 61), 'p2.8xlarge': (32, 488), 'p2.16xlarge': (64, 732),\n        'p3.2xlarge': (8, 61), 'p3.8xlarge': (32, 244), 'p3.16xlarge': (64, 488), 'p3dn.24xlarge': (96, 768),\n        'p4d.24xlarge': (96, 1152), 'p4de.24xlarge': (96, 1152),\n\n        'g3s.xlarge': (4, 30.5), 'g3.4xlarge': (16, 122), 'g3.8xlarge': (32, 244), 'g3.16xlarge': (64, 488),\n        'g4dn.xlarge': (4, 16), 'g4dn.2xlarge': (8, 32), 'g4dn.4xlarge': (16, 64), 'g4dn.8xlarge': (32, 128), 'g4dn.16xlarge': (64, 256), 'g4dn.12xlarge': (48, 192), 'g4dn.metal': (96, 384),\n        'g5.xlarge': (4, 16), 'g5.2xlarge': (8, 32), 'g5.4xlarge': (16, 64), 'g5.8xlarge': (32, 128), 'g5.16xlarge': (64, 256), 'g5.12xlarge': (48, 192), 'g5.24xlarge': (96, 384), 'g5.48xlarge': (192, 768),\n        'g6.xlarge': (4, 16), 'g6.2xlarge': (8, 32), 'g6.4xlarge': (16, 64), 'g6.8xlarge': (32, 128), 'g6.12xlarge': (48, 192), 'g6.16xlarge': (64, 256), 'g6.24xlarge': (96, 384), 'g6.48xlarge': (192, 768),\n    }\n\n    # Check if the instance type is in our known list\n    if instance_type in instance_specs:\n        return instance_specs[instance_type]\n\n    # For unknown instance types, try to infer based on family and size\n    vcpus = 0\n    memory = 0\n\n    # Estimate vCPUs based on instance size\n    if 'nano' in size:\n        vcpus = 1\n    elif 'micro' in size:\n        vcpus = 1\n    elif 'small' in size:\n        vcpus = 1\n    elif 'medium' in size:\n        vcpus = 2\n    elif 'large' in size and not 'xlarge' in size:\n        vcpus = 2\n    elif 'xlarge' in size:\n        # Extract the multiplier if present (e.g., 2xlarge, 4xlarge)\n        if size == 'xlarge':\n            vcpus = 4\n        else:\n            try:\n                multiplier = int(size.split('xlarge')[0])\n                vcpus = 4 * multiplier\n            except ValueError:\n                vcpus = 4  # Default if we can't parse\n\n    # Estimate memory based on instance family and vCPUs\n    if family in ['t2', 't3', 't4']:\n        memory = vcpus * 2  # Roughly 2 GiB per vCPU\n    elif family in ['m4', 'm5', 'm6']:\n        memory = vcpus * 4  # Roughly 4 GiB per vCPU\n    elif family in ['c4', 'c5', 'c6']:\n        memory = vcpus * 2  # Roughly 2 GiB per vCPU\n    elif family in ['r4', 'r5', 'r6']:\n        memory = vcpus * 8  # Roughly 8 GiB per vCPU\n    elif family in ['p2', 'p3', 'p4']:\n        memory = vcpus * 8  # Roughly 8 GiB per vCPU\n    elif family in ['g3', 'g4', 'g5', 'g6']:\n        memory = vcpus * 4  # Roughly 4 GiB per vCPU\n\n    # If we couldn't estimate, try to get information from EC2\n    if vcpus == 0 or memory == 0:\n        try:\n            ec2 = get_boto3_client('ec2')\n            response = ec2.describe_instance_types(InstanceTypes=[instance_type])\n            if response['InstanceTypes']:\n                instance_info = response['InstanceTypes'][0]\n                vcpus = instance_info.get('VCpuInfo', {}).get('DefaultVCpus', 0)\n                memory = instance_info.get('MemoryInfo', {}).get('SizeInMiB', 0) / 1024  # Convert MiB to GiB\n        except Exception:\n            # If API call fails, just use our estimates\n            pass\n\n    return (vcpus, memory)\n\ndef get_compute_environment_info(compute_env_name):\n    \"\"\"Get detailed information about a specific compute environment.\"\"\"\n    batch = get_boto3_client()\n\n    try:\n        response = batch.describe_compute_environments(computeEnvironments=[compute_env_name])\n\n        if not response['computeEnvironments']:\n            print(f\"Compute environment '{compute_env_name}' not found\")\n            return None\n\n        ce = response['computeEnvironments'][0]\n\n        # Print basic information\n        print(f\"\\nCompute Environment: {ce['computeEnvironmentName']}\")\n        print(f\"ARN: {ce['computeEnvironmentArn']}\")\n        print(f\"State: {ce['state']}\")\n        print(f\"Status: {ce['status']}\")\n        print(f\"Status Reason: {ce.get('statusReason', 'N/A')}\")\n        print(f\"Type: {ce['type']}\")\n\n        # Print compute resources if available\n        if 'computeResources' in ce:\n            cr = ce['computeResources']\n            print(\"\\nCompute Resources:\")\n            print(f\"  Type: {cr.get('type', 'N/A')}\")\n            print(f\"  Min vCPUs: {cr.get('minvCpus', 'N/A')}\")\n            print(f\"  Max vCPUs: {cr.get('maxvCpus', 'N/A')}\")\n            print(f\"  Desired vCPUs: {cr.get('desiredvCpus', 'N/A')}\")\n\n            # Print instance types\n            instance_types = cr.get('instanceTypes', [])\n            if instance_types:\n                print(\"\\n  Instance Types:\")\n                for it in instance_types:\n                    vcpus, memory = get_instance_specs(it)\n                    gpus = get_gpu_count(it)\n                    print(f\"    - {it} ({vcpus} vCPUs, {memory} GiB RAM, {gpus} GPUs)\")\n\n            # Print subnets\n            subnets = cr.get('subnets', [])\n            if subnets:\n                print(\"\\n  Subnets:\")\n                for subnet in subnets:\n                    print(f\"    - {subnet}\")\n\n            # Print security groups\n            security_groups = cr.get('securityGroupIds', [])\n            if security_groups:\n                print(\"\\n  Security Groups:\")\n                for sg in security_groups:\n                    print(f\"    - {sg}\")\n\n            # Print allocation strategy\n            print(f\"\\n  Allocation Strategy: {cr.get('allocationStrategy', 'N/A')}\")\n\n            # Print EC2 key pair\n            if 'ec2KeyPair' in cr:\n                print(f\"  EC2 Key Pair: {cr['ec2KeyPair']}\")\n\n            # Print instance role\n            if 'instanceRole' in cr:\n                print(f\"  Instance Role: {cr['instanceRole']}\")\n\n            # Print tags\n            if 'tags' in cr:\n                print(\"\\n  Tags:\")\n                for key, value in cr['tags'].items():\n                    print(f\"    {key}: {value}\")\n\n        # Get ECS cluster\n        ecs_cluster_arn = ce.get('ecsClusterArn')\n        if ecs_cluster_arn:\n            print(f\"\\nECS Cluster: {ecs_cluster_arn}\")\n\n            # Get EC2 instances for the cluster\n            ec2_instances = get_ec2_instances_for_cluster(ecs_cluster_arn)\n\n            if ec2_instances:\n                print(f\"\\nInstances ({len(ec2_instances)}):\")\n\n                # Format the output\n                table_data = []\n                for instance in ec2_instances:\n                    instance_id = instance['InstanceId']\n                    instance_type = instance['InstanceType']\n                    state = instance['State']['Name']\n\n                    # Get instance specifications\n                    vcpus, memory = get_instance_specs(instance_type)\n                    gpus = get_gpu_count(instance_type)\n\n                    # Get private and public IPs\n                    private_ip = instance.get('PrivateIpAddress', 'N/A')\n                    public_ip = instance.get('PublicIpAddress', 'N/A')\n\n                    table_data.append([instance_id, instance_type, state, gpus, vcpus, f\"{memory} GiB\", private_ip, public_ip])\n\n                # Print the table\n                headers = ['Instance ID', 'Type', 'State', 'GPUs', 'vCPUs', 'Memory', 'Private IP', 'Public IP']\n                print(tabulate(table_data, headers=headers, tablefmt='grid'))\n            else:\n                print(\"No instances found for this compute environment\")\n\n        # Get service role\n        if 'serviceRole' in ce:\n            print(f\"Service Role: {ce['serviceRole']}\")\n\n        return ce\n    except Exception as e:\n        print(f\"Error retrieving compute environment information: {str(e)}\")\n        return None\n\ndef stop_compute_environment(compute_env_name):\n    \"\"\"Stop a compute environment by setting its state to DISABLED.\"\"\"\n    batch = get_boto3_client()\n\n    try:\n        # First, check if the compute environment exists\n        response = batch.describe_compute_environments(computeEnvironments=[compute_env_name])\n\n        if not response['computeEnvironments']:\n            print(f\"Compute environment '{compute_env_name}' not found\")\n            return False\n\n        # Update the compute environment state to DISABLED\n        batch.update_compute_environment(\n            computeEnvironment=compute_env_name,\n            state='DISABLED'\n        )\n\n        print(f\"Compute environment '{compute_env_name}' has been disabled\")\n        return True\n    except Exception as e:\n        print(f\"Error stopping compute environment: {str(e)}\")\n        return False\n"}
{"type": "source_file", "path": "agent/lib/nn_layer_library.py", "content": "import torch.nn as nn\n\nfrom .metta_layer import ParamLayer, LayerBase\n\nclass Linear(ParamLayer):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.Linear(\n            self._input_size,\n            self._output_size,\n            **self._nn_params\n        )\n\nclass Conv1d(ParamLayer):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.Conv1d(\n            self._input_size,\n            self._output_size,\n            **self._nn_params\n        )\n\nclass Conv2d(ParamLayer):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.Conv2d(\n            self._input_size,\n            self._output_size,\n            **self._nn_params\n        )\n\nclass MaxPool1d(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.MaxPool1d(\n            self._input_size,\n            **self._nn_params\n        )\n\nclass MaxPool2d(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.MaxPool2d(\n            self._input_size,\n            **self._nn_params\n        )\n\nclass AdaptiveAvgPool1d(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.AdaptiveAvgPool1d(\n            self._input_size,\n            **self._nn_params\n        )\n\nclass AdaptiveAvgPool2d(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.AdaptiveAvgPool2d(\n            self._input_size,\n            **self._nn_params\n        )\n\nclass AdaptiveMaxPool1d(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.AdaptiveMaxPool1d(\n            self._input_size,\n            **self._nn_params\n        )\n\nclass AdaptiveMaxPool2d(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.AdaptiveMaxPool2d(\n            self._input_size,\n            **self._nn_params\n        )\n\nclass AvgPool1d(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.AvgPool1d(\n            self._input_size,\n            **self._nn_params\n        )\n\nclass AvgPool2d(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.AvgPool2d(\n            self._input_size,\n            **self._nn_params\n        )\n\nclass Dropout(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.Dropout(\n            **self._nn_params\n        )\n\nclass Dropout2d(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.Dropout2d(\n            **self._nn_params\n        )\n\nclass AlphaDropout(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.AlphaDropout(\n            **self._nn_params\n        )\n\nclass BatchNorm1d(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.BatchNorm1d(\n            self._input_size,\n            **self._nn_params\n        )\n\nclass BatchNorm2d(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.BatchNorm2d(\n            self._input_size,\n            **self._nn_params\n        )\n\nclass Flatten(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.Flatten()\n\nclass Identity(LayerBase):\n    def __init__(self, **cfg):\n        super().__init__(**cfg)\n\n    def _make_net(self):\n        return nn.Identity()\n\n"}
{"type": "source_file", "path": "devops/aws/batch/__init__.py", "content": "\"\"\"AWS Batch utilities for devops tasks.\"\"\"\n"}
{"type": "source_file", "path": "devops/aws/batch/job_logs.py", "content": "#!/usr/bin/env python3\n\"\"\"\nAWS Batch Job Logs Utilities\n\nThis module provides functions for retrieving and displaying logs from AWS Batch jobs.\n\"\"\"\n\nimport boto3\nimport time\nfrom datetime import datetime\nfrom botocore.config import Config\nimport re\n\ndef get_boto3_client(service_name):\n    \"\"\"Get a boto3 client with standard configuration.\"\"\"\n    config = Config(retries={'max_attempts': 10, 'mode': 'standard'}, max_pool_connections=50)\n    return boto3.client(service_name, config=config)\n\ndef get_job_attempts(job_id):\n    \"\"\"Get all attempts for a job.\"\"\"\n    batch = get_boto3_client('batch')\n\n    try:\n        response = batch.describe_jobs(jobs=[job_id])\n        if not response['jobs']:\n            print(f\"Job '{job_id}' not found\")\n            return []\n\n        job = response['jobs'][0]\n        return job.get('attempts', [])\n    except Exception as e:\n        print(f\"Error retrieving job attempts: {str(e)}\")\n        return []\n\ndef get_job_log_streams(job_id, attempt_index=None):\n    \"\"\"Get CloudWatch log streams for a job.\"\"\"\n    batch = get_boto3_client('batch')\n    logs = get_boto3_client('logs')\n\n    try:\n        # Get job details\n        response = batch.describe_jobs(jobs=[job_id])\n        if not response['jobs']:\n            print(f\"Job '{job_id}' not found\")\n            return []\n\n        job = response['jobs'][0]\n        attempts = job.get('attempts', [])\n\n        if not attempts:\n            print(f\"No attempts found for job '{job_id}'\")\n            return []\n\n        # If attempt_index is not specified, use the latest attempt\n        if attempt_index is None:\n            attempt_index = len(attempts) - 1\n        elif attempt_index >= len(attempts):\n            print(f\"Attempt index {attempt_index} is out of range (max: {len(attempts) - 1})\")\n            return []\n\n        attempt = attempts[attempt_index]\n        container = attempt.get('container', {})\n        log_stream_name = container.get('logStreamName')\n\n        if not log_stream_name:\n            print(f\"No log stream found for job '{job_id}' attempt {attempt_index}\")\n            return []\n\n        # For multi-node jobs, there might be multiple log streams\n        if job.get('nodeProperties') and job.get('nodeProperties').get('numNodes', 0) > 1:\n            # Extract the base log stream name (without the node index)\n            base_log_stream = re.sub(r'/\\d+$', '', log_stream_name)\n\n            # List all log streams with the base name\n            log_streams = []\n            try:\n                response = logs.describe_log_streams(\n                    logGroupName='/aws/batch/job',\n                    logStreamNamePrefix=base_log_stream\n                )\n\n                for stream in response.get('logStreams', []):\n                    log_streams.append(stream['logStreamName'])\n\n                return log_streams\n            except Exception as e:\n                print(f\"Error listing log streams: {str(e)}\")\n                # Fall back to the single log stream\n                return [log_stream_name]\n        else:\n            return [log_stream_name]\n    except Exception as e:\n        print(f\"Error retrieving job log streams: {str(e)}\")\n        return []\n\ndef find_alternative_log_streams(job_id, attempt_index=None, debug=False):\n    \"\"\"Find alternative log streams for a job when the standard method fails.\"\"\"\n    logs = get_boto3_client('logs')\n\n    try:\n        # Try to find log streams based on job ID pattern\n        log_streams = []\n\n        # Pattern 1: Standard AWS Batch log stream pattern\n        prefix = f\"jobDefinition/*/job/{job_id}\"\n        if attempt_index is not None:\n            prefix = f\"{prefix}/attempt/{attempt_index}\"\n\n        if debug:\n            print(f\"Searching for log streams with prefix: {prefix}\")\n\n        try:\n            response = logs.describe_log_streams(\n                logGroupName='/aws/batch/job',\n                logStreamNamePrefix=prefix\n            )\n\n            for stream in response.get('logStreams', []):\n                log_streams.append(stream['logStreamName'])\n        except Exception as e:\n            if debug:\n                print(f\"Error searching for log streams with prefix '{prefix}': {str(e)}\")\n\n        # Pattern 2: Alternative pattern with just the job ID\n        if not log_streams:\n            try:\n                response = logs.describe_log_streams(\n                    logGroupName='/aws/batch/job',\n                    logStreamNamePrefix=job_id\n                )\n\n                for stream in response.get('logStreams', []):\n                    log_streams.append(stream['logStreamName'])\n            except Exception as e:\n                if debug:\n                    print(f\"Error searching for log streams with job ID '{job_id}': {str(e)}\")\n\n        return log_streams\n    except Exception as e:\n        if debug:\n            print(f\"Error finding alternative log streams: {str(e)}\")\n        return []\n\ndef format_time_difference(timestamp, end_timestamp=None):\n    \"\"\"Format the time difference between a timestamp and now (or another timestamp).\"\"\"\n    if not timestamp:\n        return \"N/A\"\n\n    # Convert milliseconds to seconds if necessary\n    if timestamp > 1000000000000:  # If timestamp is in milliseconds\n        timestamp = timestamp / 1000\n\n    # If end_timestamp is provided, use it; otherwise use current time\n    if end_timestamp:\n        # Convert milliseconds to seconds if necessary\n        if end_timestamp > 1000000000000:  # If timestamp is in milliseconds\n            end_timestamp = end_timestamp / 1000\n\n        diff_seconds = end_timestamp - timestamp\n    else:\n        # Calculate difference from now\n        now = time.time()\n        diff_seconds = now - timestamp\n\n    # Format the difference\n    if diff_seconds < 0:\n        return \"Future\"\n\n    if diff_seconds < 60:\n        return f\"{int(diff_seconds)}s\"\n\n    if diff_seconds < 3600:\n        minutes = int(diff_seconds / 60)\n        seconds = int(diff_seconds % 60)\n        return f\"{minutes}m {seconds}s\"\n\n    if diff_seconds < 86400:\n        hours = int(diff_seconds / 3600)\n        minutes = int((diff_seconds % 3600) / 60)\n        return f\"{hours}h {minutes}m\"\n\n    days = int(diff_seconds / 86400)\n    hours = int((diff_seconds % 86400) / 3600)\n    return f\"{days}d {hours}h\"\n\ndef get_log_events(log_group, log_stream, start_time=None, tail=False):\n    \"\"\"Get log events from a CloudWatch log stream.\"\"\"\n    logs = get_boto3_client('logs')\n\n    try:\n        # Parameters for the get_log_events call\n        params = {\n            'logGroupName': log_group,\n            'logStreamName': log_stream,\n            'startFromHead': True\n        }\n\n        if start_time:\n            # Convert to milliseconds if necessary\n            if start_time < 1000000000000:  # If timestamp is in seconds\n                start_time = start_time * 1000\n            params['startTime'] = int(start_time)\n\n        # Get the first batch of log events\n        response = logs.get_log_events(**params)\n        events = response.get('events', [])\n\n        # If tail is True, we only want the last batch of events\n        if tail and not events:\n            # Try again with startFromHead=False to get the most recent events\n            params['startFromHead'] = False\n            response = logs.get_log_events(**params)\n            events = response.get('events', [])\n\n        # If not tailing, get all events\n        if not tail:\n            next_token = response.get('nextForwardToken')\n\n            # Continue getting events until we've retrieved all of them\n            while next_token:\n                params['nextToken'] = next_token\n                response = logs.get_log_events(**params)\n                new_events = response.get('events', [])\n\n                if not new_events:\n                    break\n\n                events.extend(new_events)\n\n                # Check if we've reached the end\n                if response.get('nextForwardToken') == next_token:\n                    break\n\n                next_token = response.get('nextForwardToken')\n\n        return events\n    except Exception as e:\n        print(f\"Error retrieving log events: {str(e)}\")\n        return []\n\ndef print_job_logs(job_id, attempt_index=None, node_index=None, tail=False, debug=False):\n    \"\"\"Print logs for a job.\"\"\"\n    batch = get_boto3_client('batch')\n\n    try:\n        # Get job details\n        response = batch.describe_jobs(jobs=[job_id])\n        if not response['jobs']:\n            print(f\"Job '{job_id}' not found\")\n            return\n\n        job = response['jobs'][0]\n        job_name = job.get('jobName', 'Unknown')\n        job_status = job.get('status', 'Unknown')\n\n        print(f\"Logs for job '{job_id}' (Name: {job_name}, Status: {job_status})\")\n\n        # Get job attempts\n        attempts = job.get('attempts', [])\n\n        if not attempts:\n            print(\"No attempts found for this job\")\n\n            # Try to find alternative log streams\n            log_streams = find_alternative_log_streams(job_id, attempt_index, debug)\n\n            if log_streams:\n                print(f\"Found {len(log_streams)} alternative log streams\")\n\n                for i, log_stream in enumerate(log_streams):\n                    if node_index is not None and i != node_index:\n                        continue\n\n                    print(f\"\\nLog Stream: {log_stream}\")\n                    events = get_log_events('/aws/batch/job', log_stream, tail=tail)\n\n                    if not events:\n                        print(\"No log events found\")\n                        continue\n\n                    for event in events:\n                        timestamp = event.get('timestamp', 0) / 1000  # Convert to seconds\n                        time_str = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n                        message = event.get('message', '')\n                        print(f\"[{time_str}] {message}\")\n            else:\n                print(\"No log streams found\")\n\n            return\n\n        # If attempt_index is not specified, use the latest attempt\n        if attempt_index is None:\n            attempt_index = len(attempts) - 1\n        elif attempt_index >= len(attempts):\n            print(f\"Attempt index {attempt_index} is out of range (max: {len(attempts) - 1})\")\n            return\n\n        attempt = attempts[attempt_index]\n        container = attempt.get('container', {})\n        log_stream_name = container.get('logStreamName')\n\n        if not log_stream_name:\n            print(f\"No log stream found for attempt {attempt_index}\")\n\n            # Try to find alternative log streams\n            log_streams = find_alternative_log_streams(job_id, attempt_index, debug)\n\n            if log_streams:\n                print(f\"Found {len(log_streams)} alternative log streams\")\n\n                for i, log_stream in enumerate(log_streams):\n                    if node_index is not None and i != node_index:\n                        continue\n\n                    print(f\"\\nLog Stream: {log_stream}\")\n                    events = get_log_events('/aws/batch/job', log_stream, tail=tail)\n\n                    if not events:\n                        print(\"No log events found\")\n                        continue\n\n                    for event in events:\n                        timestamp = event.get('timestamp', 0) / 1000  # Convert to seconds\n                        time_str = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n                        message = event.get('message', '')\n                        print(f\"[{time_str}] {message}\")\n            else:\n                print(\"No log streams found\")\n\n            return\n\n        # For multi-node jobs, there might be multiple log streams\n        if job.get('nodeProperties') and job.get('nodeProperties').get('numNodes', 0) > 1:\n            # Extract the base log stream name (without the node index)\n            base_log_stream = re.sub(r'/\\d+$', '', log_stream_name)\n\n            # List all log streams with the base name\n            log_streams = []\n            try:\n                logs = get_boto3_client('logs')\n                response = logs.describe_log_streams(\n                    logGroupName='/aws/batch/job',\n                    logStreamNamePrefix=base_log_stream\n                )\n\n                for stream in response.get('logStreams', []):\n                    log_streams.append(stream['logStreamName'])\n\n                if not log_streams:\n                    print(\"No log streams found\")\n                    return\n\n                # Sort log streams by node index\n                log_streams.sort(key=lambda s: int(re.search(r'/(\\d+)$', s).group(1)) if re.search(r'/(\\d+)$', s) else 0)\n\n                # If node_index is specified, only show logs for that node\n                if node_index is not None:\n                    node_stream = f\"{base_log_stream}/{node_index}\"\n                    if node_stream in log_streams:\n                        log_streams = [node_stream]\n                    else:\n                        print(f\"No log stream found for node {node_index}\")\n                        return\n                else:\n                    # If node_index is not specified, only show logs for the first node (main node)\n                    main_node = job.get('nodeProperties', {}).get('mainNode', 0)\n                    node_stream = f\"{base_log_stream}/{main_node}\"\n                    if node_stream in log_streams:\n                        log_streams = [node_stream]\n                        print(f\"Showing logs for main node ({main_node}). Use --node=X to view logs for other nodes.\")\n                    else:\n                        # If main node log stream not found, use the first available log stream\n                        if log_streams:\n                            log_streams = [log_streams[0]]\n                            node_match = re.search(r'/(\\d+)$', log_streams[0])\n                            node_idx = int(node_match.group(1)) if node_match else 0\n                            print(f\"Showing logs for node {node_idx}. Use --node=X to view logs for other nodes.\")\n\n                # Print logs for each stream\n                for log_stream in log_streams:\n                    # Extract node index from log stream name\n                    node_match = re.search(r'/(\\d+)$', log_stream)\n                    node_idx = int(node_match.group(1)) if node_match else 0\n\n                    print(f\"\\nNode {node_idx} Log Stream: {log_stream}\")\n                    events = get_log_events('/aws/batch/job', log_stream, tail=tail)\n\n                    if not events:\n                        print(\"No log events found\")\n                        continue\n\n                    for event in events:\n                        timestamp = event.get('timestamp', 0) / 1000  # Convert to seconds\n                        time_str = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n                        message = event.get('message', '')\n                        print(f\"[{time_str}] {message}\")\n            except Exception as e:\n                print(f\"Error listing log streams: {str(e)}\")\n                # Fall back to the single log stream\n                print(f\"\\nLog Stream: {log_stream_name}\")\n                events = get_log_events('/aws/batch/job', log_stream_name, tail=tail)\n\n                if not events:\n                    print(\"No log events found\")\n                    return\n\n                for event in events:\n                    timestamp = event.get('timestamp', 0) / 1000  # Convert to seconds\n                    time_str = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n                    message = event.get('message', '')\n                    print(f\"[{time_str}] {message}\")\n        else:\n            # Single node job\n            print(f\"\\nLog Stream: {log_stream_name}\")\n            events = get_log_events('/aws/batch/job', log_stream_name, tail=tail)\n\n            if not events:\n                print(\"No log events found\")\n                return\n\n            for event in events:\n                timestamp = event.get('timestamp', 0) / 1000  # Convert to seconds\n                time_str = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n                message = event.get('message', '')\n                print(f\"[{time_str}] {message}\")\n    except Exception as e:\n        print(f\"Error retrieving job logs: {str(e)}\")\n\ndef get_job_logs(job_id_or_name, attempt_index=None, node_index=None, tail=False, debug=False):\n    \"\"\"Get logs for a job by ID or name.\"\"\"\n    batch = get_boto3_client('batch')\n\n    try:\n        # First try to get the job by ID\n        response = batch.describe_jobs(jobs=[job_id_or_name])\n\n        # If no job found by ID, try to find by name\n        if not response['jobs']:\n            # We need to list jobs from all queues to find by name\n            job_id = None\n\n            # Get all job queues\n            try:\n                queues_response = batch.describe_job_queues()\n                job_queues = [queue['jobQueueName'] for queue in queues_response['jobQueues']]\n\n                # Search for the job in each queue\n                for queue in job_queues:\n                    # Check all job statuses\n                    for status in ['SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING', 'RUNNING', 'SUCCEEDED', 'FAILED']:\n                        try:\n                            jobs_response = batch.list_jobs(\n                                jobQueue=queue,\n                                jobStatus=status,\n                                maxResults=100\n                            )\n\n                            # Look for a job with the specified name\n                            for job_summary in jobs_response.get('jobSummaryList', []):\n                                if job_summary['jobName'] == job_id_or_name:\n                                    # Found a job with the specified name\n                                    job_id = job_summary['jobId']\n                                    break\n\n                            if job_id:\n                                break\n                        except Exception:\n                            continue\n\n                    if job_id:\n                        break\n\n                if not job_id:\n                    print(f\"No job found with ID or name '{job_id_or_name}'\")\n                    return\n            except Exception as e:\n                print(f\"Error retrieving job queues: {str(e)}\")\n                print(f\"Job '{job_id_or_name}' not found\")\n                return\n        else:\n            job_id = job_id_or_name\n\n        # Now that we have the job ID, print the logs\n        print_job_logs(job_id, attempt_index, node_index, tail, debug)\n    except Exception as e:\n        print(f\"Error retrieving job logs: {str(e)}\")\n"}
{"type": "source_file", "path": "agent/lib/observation_normalizer.py", "content": "import torch\nimport omegaconf\nfrom tensordict import TensorDict\n\nfrom agent.lib.metta_layer import LayerBase\n# ##ObservationNormalization\n# These are approximate maximum values for each feature. Ideally they would be defined closer to their source,\n# but here we are. If you add / remove a feature, you should add / remove the corresponding normalization.\nOBS_NORMALIZATIONS = {\n    'agent': 1,\n    'agent:group': 10,\n    'agent:hp': 30,\n    'agent:frozen': 1,\n    'agent:energy': 255,\n    'agent:orientation': 1,\n    'agent:shield': 1,\n    'agent:color': 255,\n    'agent:inv:ore.red': 100,\n    'agent:inv:ore.blue': 100,\n    'agent:inv:ore.green': 100,\n    'agent:inv:battery': 100,\n    'agent:inv:heart': 100,\n    'agent:inv:laser': 100,\n    'agent:inv:armor': 100,\n    'agent:inv:blueprint': 100,\n    'inv:ore.red': 100,\n    'inv:ore.blue': 100,\n    'inv:ore.green': 100,\n    'inv:battery': 100,\n    'inv:heart': 100,\n    'inv:laser': 100,\n    'inv:armor': 100,\n    'inv:blueprint': 100,\n    'wall': 1,\n    'generator': 1,\n    'mine': 1,\n    'altar': 1,\n    'armory': 1,\n    'lasery': 1,\n    'lab': 1,\n    'factory': 1,\n    'temple': 1,\n    'last_action': 10,\n    'temple:ready': 1,\n    'last_action': 10,\n    'last_action_argument': 10,\n    'agent:kinship': 10,\n    'hp': 30,\n    'ready': 1,\n    'converting': 1,\n    'color': 10,\n    'swappable': 1,\n}\n\nclass ObservationNormalizer(LayerBase):\n    def __init__(self, grid_features, **cfg):\n        super().__init__(**cfg)\n\n        num_objects = len(grid_features)\n\n        obs_norm = torch.tensor([OBS_NORMALIZATIONS[k] for k in grid_features], dtype=torch.float32)\n        obs_norm = obs_norm.view(1, num_objects, 1, 1)\n\n        self.register_buffer('obs_norm', obs_norm)\n\n    def _forward(self, td: TensorDict):\n        td[self._name] = td[self._input_source] / self.obs_norm\n        return td\n"}
{"type": "source_file", "path": "devops/aws/batch/cluster_info.py", "content": "import boto3\nimport argparse\nfrom colorama import init, Fore, Style\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom botocore.config import Config\n\n# Configure boto3 to use a higher max_pool_connections\nconfig = Config(\n    retries = {'max_attempts': 10, 'mode': 'standard'},\n    max_pool_connections = 50\n)\n\ndef get_job_details(job, job_descriptions, task_descriptions=None, container_instance_descriptions=None, ec2_instances=None):\n    \"\"\"Get details for a specific job.\"\"\"\n    job_id = job['jobId']\n    job_name = job['jobName']\n    job_status = job['status']\n    job_link = f\"https://console.aws.amazon.com/batch/home?region=us-east-1#jobs/detail/{job_id}\"\n\n    # Handle the case when job_descriptions is empty or the job is not found\n    if not job_descriptions:\n        return {\n            'jobId': job_id,\n            'jobName': job_name,\n            'status': job_status,\n            'link': job_link,\n            'publicIp': '',\n            'instanceType': '',\n            'numRetries': 0,\n            'createdAt': job.get('createdAt', 0),\n            'startedAt': job.get('startedAt', 0),\n            'stoppedAt': job.get('stoppedAt', 0)\n        }\n\n    job_desc = next((j for j in job_descriptions if j['jobId'] == job_id), {})\n    container = job_desc.get('container', {})\n    task_arn = container.get('taskArn')\n\n    num_retries = len(job_desc.get('attempts', [])) - 1 if job_desc.get('attempts') else 0\n\n    public_ip = ''\n    if task_descriptions and task_arn and task_arn in task_descriptions:\n        task_desc = task_descriptions.get(task_arn, {})\n        container_instance_arn = task_desc.get('containerInstanceArn')\n        if container_instance_descriptions and container_instance_arn and container_instance_arn in container_instance_descriptions:\n            container_instance_desc = container_instance_descriptions.get(container_instance_arn, {})\n            ec2_instance_id = container_instance_desc.get('ec2InstanceId')\n            if ec2_instances and ec2_instance_id and ec2_instance_id in ec2_instances:\n                ec2_instance = ec2_instances.get(ec2_instance_id, {})\n                public_ip = ec2_instance.get('PublicIpAddress', '')\n\n    stop_command = f\"aws batch terminate-job --reason man_stop --job-id {job_id}\" if job_status in ['RUNNING', 'RUNNABLE', 'STARTING'] else ''\n\n    return {\n        'name': job_name,\n        'status': job_status,\n        'retries': num_retries,\n        'link': job_link,\n        'public_ip': public_ip,\n        'stop_command': stop_command\n    }\n\ndef get_batch_job_queues():\n    batch = boto3.client('batch', config=config)\n    response = batch.describe_job_queues()\n    return [queue['jobQueueName'] for queue in response['jobQueues']]\n\ndef get_batch_jobs(job_queue, max_jobs):\n    batch = boto3.client('batch', config=config)\n    ecs = boto3.client('ecs', config=config)\n    ec2 = boto3.client('ec2', config=config)\n\n    def get_jobs_by_status(status):\n        response = batch.list_jobs(jobQueue=job_queue, jobStatus=status, maxResults=min(max_jobs, 100))\n        return response['jobSummaryList']\n\n    with ThreadPoolExecutor() as executor:\n        job_futures = [executor.submit(get_jobs_by_status, state) for state in ['RUNNING', 'SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING', 'SUCCEEDED', 'FAILED']]\n        all_jobs = [job for future in job_futures for job in future.result()]\n\n    all_jobs = sorted(all_jobs, key=lambda job: job['createdAt'], reverse=True)[:max_jobs]\n\n    job_ids = [job['jobId'] for job in all_jobs]\n    job_descriptions = []\n    # Only call describe_jobs if we have job IDs to process\n    if job_ids:\n        job_descriptions = batch.describe_jobs(jobs=job_ids)['jobs']\n\n    task_arns = []\n    cluster_arns = []\n    task_to_cluster = {}  # Map to track which cluster each task belongs to\n    for job in job_descriptions:\n        container = job.get('container', {})\n        if container and container.get('taskArn'):\n            task_arn = container.get('taskArn')\n            cluster_arn = container.get('containerInstanceArn')\n            if cluster_arn:  # Only add if we have cluster information\n                cluster_name = cluster_arn.split('/')[1]\n                task_arns.append(task_arn)\n                cluster_arns.append(cluster_arn)\n                task_to_cluster[task_arn] = cluster_name\n\n    # Skip task processing if there are no tasks\n    if not task_arns:\n        # If we have job descriptions, use them; otherwise, return basic job info\n        if job_descriptions:\n            return [get_job_details(job, job_descriptions) for job in all_jobs]\n        else:\n            return all_jobs\n\n    # Batch describe tasks by their respective clusters\n    tasks_by_cluster = {}\n    for task_arn in task_arns:\n        cluster_name = task_to_cluster.get(task_arn)\n        if cluster_name:\n            tasks_by_cluster.setdefault(cluster_name, []).append(task_arn)\n\n    task_descriptions = {}\n    for cluster_name, tasks in tasks_by_cluster.items():\n        for i in range(0, len(tasks), 100):\n            chunk = tasks[i:i+100]\n            try:\n                response = ecs.describe_tasks(cluster=cluster_name, tasks=chunk)\n                task_descriptions.update({task['taskArn']: task for task in response['tasks']})\n            except Exception as e:\n                print(f\"Warning: Failed to describe tasks for cluster {cluster_name}: {str(e)}\")\n                continue\n\n    container_instances_by_cluster = {}\n    for task in task_descriptions.values():\n        cluster_name = task['clusterArn'].split('/')[1]\n        container_instances_by_cluster.setdefault(cluster_name, set()).add(task['containerInstanceArn'])\n\n    container_instance_descriptions = {}\n    for cluster_name, container_instances in container_instances_by_cluster.items():\n        for i in range(0, len(container_instances), 100):\n            chunk = list(container_instances)[i:i+100]\n            response = ecs.describe_container_instances(cluster=cluster_name, containerInstances=chunk)\n            container_instance_descriptions.update({instance['containerInstanceArn']: instance for instance in response['containerInstances']})\n\n    # Batch describe EC2 instances\n    ec2_instance_ids = [instance['ec2InstanceId'] for instance in container_instance_descriptions.values()]\n    ec2_instances = {}\n    for i in range(0, len(ec2_instance_ids), 100):\n        chunk = ec2_instance_ids[i:i+100]\n        response = ec2.describe_instances(InstanceIds=chunk)\n        ec2_instances.update({instance['InstanceId']: instance for reservation in response['Reservations'] for instance in reservation['Instances']})\n\n    with ThreadPoolExecutor() as executor:\n        job_details = list(executor.map(\n            lambda job: get_job_details(job, job_descriptions, task_descriptions, container_instance_descriptions, ec2_instances),\n            all_jobs\n        ))\n\n    return job_details\n\ndef get_ecs_clusters():\n    ecs = boto3.client('ecs', config=config)\n    response = ecs.list_clusters()\n    return response['clusterArns']\n\ndef get_ecs_tasks(clusters, max_tasks):\n    ecs = boto3.client('ecs', config=config)\n    ec2 = boto3.client('ec2', config=config)\n\n    all_tasks = []\n    for cluster in clusters:\n        response = ecs.list_tasks(cluster=cluster, maxResults=min(max_tasks, 100))\n        all_tasks.extend([(cluster, task_arn) for task_arn in response['taskArns']])\n\n    all_tasks = all_tasks[:max_tasks]\n\n    task_descriptions = {}\n    for i in range(0, len(all_tasks), 100):\n        chunk = all_tasks[i:i+100]\n        cluster = chunk[0][0]  # Assuming all tasks in the chunk are from the same cluster\n        task_arns = [task[1] for task in chunk]\n        response = ecs.describe_tasks(cluster=cluster, tasks=task_arns)\n        task_descriptions.update({task['taskArn']: task for task in response['tasks']})\n\n    container_instances = set(task['containerInstanceArn'] for task in task_descriptions.values())\n    container_instance_descriptions = {}\n    for i in range(0, len(container_instances), 100):\n        chunk = list(container_instances)[i:i+100]\n        response = ecs.describe_container_instances(cluster=cluster, containerInstances=chunk)\n        container_instance_descriptions.update({instance['containerInstanceArn']: instance for instance in response['containerInstances']})\n\n    ec2_instance_ids = [instance['ec2InstanceId'] for instance in container_instance_descriptions.values()]\n    ec2_instances = {}\n    for i in range(0, len(ec2_instance_ids), 100):\n        chunk = ec2_instance_ids[i:i+100]\n        response = ec2.describe_instances(InstanceIds=chunk)\n        ec2_instances.update({instance['InstanceId']: instance for reservation in response['Reservations'] for instance in reservation['Instances']})\n\n    def get_task_details(cluster, task_arn):\n        task_desc = task_descriptions[task_arn]\n        task_id = task_arn.split('/')[-1]\n        task_name = task_desc['overrides']['containerOverrides'][0]['name']\n        task_status = task_desc['lastStatus']\n        task_link = f\"https://console.aws.amazon.com/ecs/home?region=us-east-1#/clusters/{cluster}/tasks/{task_id}/details\"\n\n        container_instance_arn = task_desc['containerInstanceArn']\n        container_instance_desc = container_instance_descriptions[container_instance_arn]\n        ec2_instance_id = container_instance_desc['ec2InstanceId']\n\n        public_ip = ec2_instances[ec2_instance_id].get('PublicIpAddress', '')\n\n        stop_command = f\"aws ecs stop-task --cluster {cluster} --task {task_arn}\" if task_status == 'RUNNING' else ''\n\n        return {\n            'name': task_name,\n            'status': task_status,\n            'retries': 0,\n            'link': task_link,\n            'public_ip': public_ip,\n            'stop_command': stop_command\n        }\n\n    with ThreadPoolExecutor() as executor:\n        task_details = list(executor.map(lambda x: get_task_details(*x), all_tasks))\n\n    return task_details\n\ndef print_row(key, value, use_color):\n    if use_color:\n        print(f\"  {Fore.BLUE}{key}:{Style.RESET_ALL} {value}\")\n    else:\n        print(f\"  {key}: {value}\")\n\ndef print_status(jobs_by_queue, tasks, use_color):\n    for job_queue, jobs in jobs_by_queue.items():\n        if use_color:\n            print(f\"{Fore.CYAN}AWS Batch Jobs - Queue: {job_queue}{Style.RESET_ALL}\")\n        else:\n            print(f\"AWS Batch Jobs - Queue: {job_queue}\")\n\n        for job in jobs:\n            status_color = {\n                'SUBMITTED': Fore.YELLOW,\n                'PENDING': Fore.YELLOW,\n                'RUNNABLE': Fore.YELLOW,\n                'STARTING': Fore.YELLOW,\n                'RUNNING': Fore.GREEN,\n                'SUCCEEDED': Fore.GREEN,\n                'FAILED': Fore.RED\n            }.get(job['status'], Fore.YELLOW)\n\n            print_row(\"Name\", job['name'], use_color)\n            print_row(\"Status\", f\"{status_color}{job['status']}{Style.RESET_ALL} ({job['retries']})\" if use_color else job['status'], use_color)\n            print_row(\"Link\", job['link'], use_color)\n            print_row(\"Public IP\", job['public_ip'], use_color)\n            print_row(\"Stop Command\", job['stop_command'], use_color)\n            print()\n\n    if tasks:\n        if use_color:\n            print(f\"{Fore.CYAN}ECS Tasks{Style.RESET_ALL}\")\n        else:\n            print(\"ECS Tasks\")\n\n        for task in tasks:\n            status_color = Fore.GREEN if task['status'] == 'RUNNING' else Fore.RED\n            print_row(\"Name\", task['name'], use_color)\n            print_row(\"Status\", f\"{status_color}{task['status']}{Style.RESET_ALL}\" if use_color else task['status'], use_color)\n            print_row(\"Link\", task['link'], use_color)\n            print_row(\"Public IP\", task['public_ip'], use_color)\n            print_row(\"Stop Command\", task['stop_command'], use_color)\n            print()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Get the status of AWS Batch jobs and ECS tasks.')\n    parser.add_argument('--max-jobs', type=int, default=10, help='The maximum number of jobs to display.')\n    parser.add_argument('--ecs', action='store_true', help='Include ECS tasks in the status dump.')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output.')\n    args = parser.parse_args()\n\n    # Initialize colorama\n    init()\n\n    # Get all job queues\n    job_queues = get_batch_job_queues()\n\n    # Get jobs for each queue\n    jobs_by_queue = {}\n    for queue in job_queues:\n        jobs = get_batch_jobs(queue, args.max_jobs)\n        if jobs:\n            jobs_by_queue[queue] = jobs\n\n    # Get ECS tasks if requested\n    tasks = []\n    if args.ecs:\n        clusters = get_ecs_clusters()\n        tasks = get_ecs_tasks(clusters, args.max_jobs)\n\n    # Print the status\n    print_status(jobs_by_queue, tasks, not args.no_color)\n"}
{"type": "source_file", "path": "agent/feature_encoder.py", "content": "\"\"\"\nThis file defines a feature encoding module for processing observations in a deep learning model.\nIt contains the FeatureSetEncoder class which:\n- Takes raw observation features and their names as input\n- Optionally normalizes the features\n- Embeds feature names into vectors\n- Processes features through neural network layers\n- Outputs encoded feature representations\n\nThe encoder is designed to work with Sample Factory RL framework and MettaAgent interface.\n\"\"\"\n\nfrom __future__ import annotations\nfrom typing import List\n\nfrom torch import nn\nimport torch\n\n\nfrom .lib.util import make_nn_stack, embed_strings\n\nimport numpy as np\nimport torch\n\nfrom omegaconf import OmegaConf\nfrom agent.lib.feature_normalizer import FeatureListNormalizer\nfrom .lib.util import make_nn_stack\n\n# this is not currently working\nclass FeatureSetEncoder(nn.Module):\n    def __init__(\n            self,\n            obs_space,\n            obs_key: str,\n            feature_names: List[str],\n            normalize_features: bool,\n            label_dim: int,\n            output_dim: int,\n            layers: int\n        ):\n        super().__init__()\n\n        assert len(feature_names) == obs_space[obs_key].shape[0], \\\n            f\"Number of feature names ({len(feature_names)}) must match\" \\\n            f\"the number of features in the observation space (\" \\\n            f\"{obs_space[obs_key].shape[0]})\"\n\n        self._obs_key = obs_key\n        self._normalize_features = normalize_features\n        self._feature_names = feature_names\n        self._obs_shape = obs_space[obs_key].shape[1:] or (1,)\n        self._normalizer = FeatureListNormalizer(feature_names, self._obs_shape)\n        self._input_dim = np.prod(self._obs_shape)\n        self._output_dim = output_dim\n        self._label_dim = label_dim\n\n        self._num_features = len(feature_names)\n\n        self._labels_emb = embed_strings(self._feature_names, label_dim)\n        self._labels_emb = self._labels_emb.unsqueeze(0)\n\n        self.embedding_net = make_nn_stack(\n            input_size=label_dim + self._input_dim,\n            output_size=self._output_dim,\n            hidden_sizes=[self._output_dim] * (layers - 1),\n        )\n\n    def forward(self, obs_dict):\n        batch_size = obs_dict[self._obs_key].size(0)\n\n        self._labels_emb = self._labels_emb.to(obs_dict[self._obs_key].device)\n        obs = obs_dict[self._obs_key].view(batch_size, -1, *self._obs_shape).to(torch.float32)\n\n        if self._normalize_features:\n            self._normalizer(obs)\n\n        labeled_obs = torch.cat([\n            self._labels_emb.expand(batch_size, -1, -1),\n            obs.view(batch_size, self._num_features, self._input_dim)\n        ], dim=-1)\n\n        x = torch.sum(self.embedding_net(labeled_obs), dim=1)\n        return x\n\n    def output_dim(self):\n        return self._output_dim\n\nclass MultiFeatureSetEncoder(nn.Module):\n    def __init__(self,\n                 obs_space,\n                 grid_features, global_features,\n                fc_cfg: OmegaConf, **cfg):\n        super().__init__()\n        cfg = OmegaConf.create(cfg)\n        if \"grid_obs\" in cfg:\n            cfg.grid_obs.feature_names = grid_features\n        if \"global_vars\" in cfg:\n            cfg.global_vars.feature_names = global_features\n\n        self.feature_set_encoders = nn.ModuleDict({\n            name: FeatureSetEncoder(obs_space, name, **fs_cfg)\n            for name, fs_cfg in cfg.items()\n            if len(fs_cfg.feature_names) > 0\n        })\n\n        self.merged_encoder = make_nn_stack(\n            input_size=sum(encoder.output_dim() for encoder in self.feature_set_encoders.values()),\n            output_size=fc_cfg.output_dim,\n            hidden_sizes=[fc_cfg.output_dim] * (fc_cfg.layers - 1),\n        )\n\n    def forward(self, obs_dict):\n        x = torch.cat([encoder(obs_dict) for encoder in self.feature_set_encoders.values()], dim=1)\n        x = self.merged_encoder(x)\n        return x\n"}
{"type": "source_file", "path": "devops/aws/batch/launch_cmd.py", "content": "#!/usr/bin/env python3\n\"\"\"\nAWS Batch Launch Command\n\nThis script provides a command-line interface for launching AWS Batch jobs\nusing the launch_task module.\n\nUsage:\n    launch_cmd.py --run RUN_ID --cmd COMMAND [options]\n\"\"\"\n\nimport sys\nimport os\n\ntry:\n    from devops.aws.batch import launch_task\nexcept ImportError as e:\n    print(f\"Error: Could not import devops.aws.batch.launch_task: {str(e)}\")\n    print(\"Please ensure that the module is available in your Python path\")\n    sys.exit(1)\n\ndef main():\n    \"\"\"Main entry point for the AWS Batch Launch CLI.\"\"\"\n    # Just call the main function from launch_task if it exists\n    if hasattr(launch_task, 'main'):\n        launch_task.main()\n    else:\n        # Otherwise, run the code from the module's __main__ section\n        import argparse\n\n        parser = argparse.ArgumentParser(description='Launch an AWS Batch task with a wandb key.')\n        parser.add_argument('--cluster', default=\"metta\", help='The name of the ECS cluster.')\n        parser.add_argument('--run', required=True, help='The run id.')\n        parser.add_argument('--cmd', required=True, choices=[\"train\", \"sweep\", \"evolve\"], help='The command to run.')\n        parser.add_argument('--git-branch', default=None, help='The git branch to use for the task. If not specified, will use the current commit.')\n        parser.add_argument('--git-commit', default=None, help='The git commit to use for the task. If not specified, will use the current commit.')\n        parser.add_argument('--mettagrid-branch', default=None, help='The mettagrid branch to use for the task. If not specified, will use the current commit.')\n        parser.add_argument('--mettagrid-commit', default=None, help='The mettagrid commit to use for the task. If not specified, will use the current commit.')\n        parser.add_argument('--gpus', type=int, default=4, help='Number of GPUs per node to use for the task.')\n        parser.add_argument('--node-gpus', type=int, default=4, help='Number of GPUs per node to use for the task.')\n        parser.add_argument('--gpu-cpus', type=int, default=6, help='Number of CPUs per GPU (vCPUs will be 2x this value).')\n        parser.add_argument('--cpu-ram-gb', type=int, default=20, help='RAM per node in GB.')\n        parser.add_argument('--copies', type=int, default=1, help='Number of job copies to submit.')\n        parser.add_argument('--profile', default=\"stem\", help='AWS profile to use. If not specified, uses the default profile.')\n        parser.add_argument('--job-queue', default=\"metta-jq\", help='AWS Batch job queue to use.')\n        parser.add_argument('--skip-push-check', action='store_true', help='Skip checking if commits have been pushed.')\n        parser.add_argument('--no-color', action='store_true', help='Disable colored output.')\n        parser.add_argument('--dry-run', action='store_true', help='Dry run mode, prints job details without submitting.')\n        args, task_args = parser.parse_known_args()\n\n        # Filter out --no-color from task_args\n        task_args = [arg for arg in task_args if arg != '--no-color']\n\n        args.num_nodes = max(1, args.gpus // args.node_gpus)\n\n        # Set default commit values if not specified\n        if args.git_branch is None and args.git_commit is None:\n            args.git_commit = launch_task.get_current_commit()\n\n        if args.mettagrid_branch is None and args.mettagrid_commit is None:\n            args.mettagrid_commit = launch_task.get_current_commit(\"deps/mettagrid\")\n\n        # Submit the job\n        for i in range(args.copies):\n            launch_task.submit_batch_job(args, task_args)\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "devops/aws/cmd.py", "content": "#!/usr/bin/env python3\n\"\"\"\nAWS Batch Command Line Interface\n\nThis script provides a command-line interface for interacting with AWS Batch resources.\nIt supports operations on job queues, compute environments, and jobs.\n\nUsage:\n    cmd.py [resource_type] [id] [command] [options]\n    cmd.py [command] [id] [options]\n\nResource Types:\n    - job-queue (jq): AWS Batch job queues\n    - compute-environment (ce): AWS Batch compute environments\n    - job (j): AWS Batch jobs\n    - jobs: List jobs in the default queue (metta-jq)\n\nCommands:\n    - list (l): List resources (default if not specified)\n    - info (d): Get detailed information about a resource\n    - logs (ls): Get logs for a job\n    - stop (s): Stop a job or compute environment\n    - ssh: Connect to the instance running a job via SSH\n    - launch (st): Launch a job\n\"\"\"\n\nimport sys\nimport argparse\nimport importlib.util\nimport os\nimport subprocess\nfrom devops.aws.batch.job_queue import list_job_queues, get_job_queue_info\nfrom devops.aws.batch.compute_environment import list_compute_environments, get_compute_environment_info, stop_compute_environment\nfrom devops.aws.batch.job import list_jobs, get_job_info, stop_job, launch_job, ssh_to_job\nfrom devops.aws.batch.job_logs import get_job_logs\n\n# Define the valid commands and resource types\nVALID_COMMANDS = ['list', 'l', 'info', 'd', 'logs', 'ls', 'stop', 's', 'ssh', 'launch', 'st']\nVALID_RESOURCE_TYPES = ['job-queue', 'jq', 'compute-environment', 'ce', 'job', 'j', 'jobs', 'compute']\n\ndef parse_args():\n    \"\"\"Parse command-line arguments.\"\"\"\n    parser = argparse.ArgumentParser(description='AWS Batch CLI')\n\n    # First positional argument could be either a resource type or a command\n    parser.add_argument('arg1', nargs='?', default='list',\n                        help='Resource type or command')\n\n    # Second positional argument could be either an ID or a command\n    parser.add_argument('arg2', nargs='?', default=None,\n                        help='Resource ID or command')\n\n    # Third positional argument could be a command\n    parser.add_argument('arg3', nargs='?', default=None,\n                        help='Command (if arg1 is resource type and arg2 is ID)')\n\n    # Options\n    parser.add_argument('--queue', '-q', default=None,\n                        help='Job queue name (default: metta-jq for job commands)')\n    parser.add_argument('--max', '-m', type=int, default=5,\n                        help='Maximum number of items to return (default: 5)')\n    parser.add_argument('--tail', '-t', action='store_true',\n                        help='Tail logs')\n    parser.add_argument('--attempt', '-a', type=int, default=0,\n                        help='Job attempt index')\n    parser.add_argument('--node', '-n', type=int, default=0,\n                        help='Node index for multi-node jobs')\n    parser.add_argument('--debug', action='store_true',\n                        help='Enable debug output')\n    parser.add_argument('--instance', '-i', action='store_true',\n                        help='Connect directly to the instance without attempting to connect to the container (for ssh command)')\n    parser.add_argument('--no-color', action='store_true',\n                        help='Disable colored output')\n\n    return parser.parse_args()\n\ndef normalize_command(cmd):\n    \"\"\"Normalize command aliases to their full form.\"\"\"\n    if cmd == 'l':\n        return 'list'\n    elif cmd == 'd':\n        return 'info'\n    elif cmd == 'ls':\n        return 'logs'\n    elif cmd == 's':\n        return 'stop'\n    elif cmd == 'st':\n        return 'launch'\n    return cmd\n\ndef normalize_resource_type(res_type):\n    \"\"\"Normalize resource type aliases to their full form.\"\"\"\n    if res_type == 'jq':\n        return 'job-queue'\n    elif res_type == 'ce':\n        return 'compute-environment'\n    elif res_type == 'j':\n        return 'job'\n    elif res_type == 'compute':\n        return 'compute-environment'\n    return res_type\n\ndef handle_compute_command(resource_id=None, command=None):\n    \"\"\"Handle the 'compute' command.\"\"\"\n    if command == 'info' or (resource_id and resource_id != 'all' and command != 'list'):\n        # Get detailed info about a specific compute environment, including its instances\n        get_compute_environment_info(resource_id)\n    else:\n        # List all compute environments with status, instance types, and number of instances\n        list_compute_environments()\n\ndef main():\n    \"\"\"Main entry point for the AWS Batch CLI.\"\"\"\n    args = parse_args()\n\n    # Special case for 'compute' command\n    if args.arg1 == 'compute' or args.arg1 == 'c':\n        if args.arg2 and args.arg2 != 'all':\n            # If a specific compute environment is provided, get detailed info about it\n            get_compute_environment_info(args.arg2)\n        else:\n            # Otherwise, list all compute environments\n            list_compute_environments()\n        return\n\n    # Special case for 'ssh' command with simplified syntax\n    if args.arg1 == 'ssh':\n        if not args.arg2:\n            print(\"Error: Job ID is required for ssh command\")\n            sys.exit(1)\n\n        if not ssh_to_job(args.arg2, instance_only=args.instance):\n            sys.exit(1)\n        return\n\n    # Special case for 'stop' command with simplified syntax\n    if args.arg1 == 'stop' or args.arg1 == 's':\n        if not args.arg2:\n            print(\"Error: Job ID is required for stop command\")\n            sys.exit(1)\n\n        result = stop_job(args.arg2, max_results=args.max)\n\n        # If result is a list, it means we found multiple jobs with the prefix\n        if isinstance(result, list):\n            matching_jobs = result\n            num_jobs = len(matching_jobs)\n\n            print(f\"\\nThere are {num_jobs} jobs with the prefix '{args.arg2}':\")\n            for i, job in enumerate(matching_jobs, 1):\n                job_id = job['jobId']\n                job_name = job['jobName']\n                job_status = job['status']\n                print(f\"{i}. {job_name} ({job_id}) - Status: {job_status}\")\n\n            # Ask for confirmation\n            confirmation = input(f\"\\nThere are {num_jobs} jobs with the prefix '{args.arg2}', stop all of them? (y/n): \")\n            if confirmation.lower() == 'y':\n                # Extract job IDs and stop them\n                job_ids = [job['jobId'] for job in matching_jobs]\n                from devops.aws.batch.job import stop_jobs\n                stop_jobs(job_ids)\n            else:\n                print(\"Operation cancelled.\")\n        return\n\n    # Determine if we're using the new simplified syntax or the old syntax\n    # New syntax: cmd.sh info <job_id>\n    # Old syntax: cmd.sh job <job_id> info\n\n    resource_type = None\n    resource_id = None\n    command = None\n\n    # Check if arg1 is a command\n    if args.arg1 in VALID_COMMANDS or normalize_command(args.arg1) in VALID_COMMANDS:\n        # New syntax: cmd.sh <command> <id>\n        command = normalize_command(args.arg1)\n        resource_id = args.arg2\n\n        # For info, logs, and stop commands, assume it's a job if no resource type is specified\n        if command in ['info', 'logs', 'stop']:\n            resource_type = 'job'\n        # For list command with no ID, we need to determine what to list\n        elif command == 'list' and not resource_id:\n            # Default to listing jobs in the default queue\n            resource_type = 'jobs'\n        # For launch command, assume it's a job\n        elif command == 'launch':\n            resource_type = 'job'\n    # Check if arg1 is a resource type\n    elif args.arg1 in VALID_RESOURCE_TYPES or normalize_resource_type(args.arg1) in VALID_RESOURCE_TYPES:\n        # Old syntax: cmd.sh <resource_type> <id> <command>\n        resource_type = normalize_resource_type(args.arg1)\n        resource_id = args.arg2\n        command = normalize_command(args.arg3) if args.arg3 else 'list'\n    else:\n        # If arg1 is neither a command nor a resource type, assume it's a job ID for info\n        resource_type = 'job'\n        resource_id = args.arg1\n        command = 'info'\n\n    # Special case for 'jobs' resource type\n    if resource_type == 'jobs':\n        # If resource_id is provided, it might be a job queue name\n        job_queue = resource_id if resource_id else 'metta-jq'\n        list_jobs(job_queue=job_queue, max_jobs=args.max, no_color=args.no_color)\n        return\n\n    # Execute the appropriate command based on resource type and command\n    if resource_type == 'job-queue':\n        if command == 'list':\n            list_job_queues()\n        elif command == 'info':\n            if not resource_id:\n                print(\"Error: Job queue ID is required for info command\")\n                sys.exit(1)\n            get_job_queue_info(resource_id, max_jobs=args.max)\n        else:\n            print(f\"Error: Command '{command}' is not supported for job queues\")\n            sys.exit(1)\n\n    elif resource_type == 'compute-environment':\n        if command == 'list':\n            list_compute_environments()\n        elif command == 'info':\n            if not resource_id:\n                print(\"Error: Compute environment ID is required for info command\")\n                sys.exit(1)\n            get_compute_environment_info(resource_id)\n        elif command == 'stop':\n            if not resource_id:\n                print(\"Error: Compute environment ID is required for stop command\")\n                sys.exit(1)\n            stop_compute_environment(resource_id)\n        else:\n            print(f\"Error: Command '{command}' is not supported for compute environments\")\n            sys.exit(1)\n\n    elif resource_type == 'job':\n        # Set default job queue to metta-jq if not specified\n        job_queue = args.queue if args.queue else 'metta-jq'\n\n        if command == 'list':\n            list_jobs(job_queue=job_queue, max_jobs=args.max, no_color=args.no_color)\n        elif command == 'info':\n            if not resource_id:\n                print(\"Error: Job ID is required for info command\")\n                sys.exit(1)\n\n            # Try to get job info\n            job = get_job_info(resource_id, no_color=args.no_color)\n\n            # If job not found, try to check if it's a compute environment or job queue\n            if job is None:\n                print(f\"Checking if '{resource_id}' is a compute environment or job queue...\")\n\n                # Check if it's a compute environment\n                try:\n                    ce_info = get_compute_environment_info(resource_id)\n                    if ce_info is not None:\n                        return\n                except Exception:\n                    pass\n\n                # Check if it's a job queue\n                try:\n                    jq_info = get_job_queue_info(resource_id, max_jobs=args.max)\n                    if jq_info is not None:\n                        return\n                except Exception:\n                    pass\n\n                print(f\"'{resource_id}' was not found as a job, compute environment, or job queue.\")\n\n        elif command == 'logs':\n            if not resource_id:\n                print(\"Error: Job ID is required for logs command\")\n                sys.exit(1)\n            get_job_logs(resource_id, attempt_index=args.attempt, node_index=args.node,\n                         tail=args.tail, debug=args.debug)\n        elif command == 'stop':\n            if not resource_id:\n                print(\"Error: Job ID is required for stop command\")\n                sys.exit(1)\n\n            result = stop_job(resource_id, max_results=args.max)\n\n            # If result is a list, it means we found multiple jobs with the prefix\n            if isinstance(result, list):\n                matching_jobs = result\n                num_jobs = len(matching_jobs)\n\n                print(f\"\\nThere are {num_jobs} jobs with the prefix '{resource_id}':\")\n                for i, job in enumerate(matching_jobs, 1):\n                    job_id = job['jobId']\n                    job_name = job['jobName']\n                    job_status = job['status']\n                    print(f\"{i}. {job_name} ({job_id}) - Status: {job_status}\")\n\n                # Ask for confirmation\n                confirmation = input(f\"\\nThere are {num_jobs} jobs with the prefix '{resource_id}', stop all of them? (y/n): \")\n                if confirmation.lower() == 'y':\n                    # Extract job IDs and stop them\n                    job_ids = [job['jobId'] for job in matching_jobs]\n                    from devops.aws.batch.job import stop_jobs\n                    stop_jobs(job_ids)\n                else:\n                    print(\"Operation cancelled.\")\n        elif command == 'ssh':\n            if not resource_id:\n                print(\"Error: Job ID is required for ssh command\")\n                sys.exit(1)\n            if not ssh_to_job(resource_id, instance_only=args.instance):\n                sys.exit(1)\n        elif command == 'launch':\n            print(\"The launch command is now handled directly by the cmd.sh script.\")\n            print(\"Please use: ./devops/aws/batch/cmd.sh launch --run RUN_ID --cmd COMMAND [options]\")\n            sys.exit(1)\n        else:\n            print(f\"Error: Command '{command}' is not supported for jobs\")\n            sys.exit(1)\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "devops/aws/__init__.py", "content": "\"\"\"AWS utilities for devops tasks.\"\"\"\n"}
{"type": "source_file", "path": "agent/lib/obs_shaper.py", "content": "import omegaconf\nfrom tensordict import TensorDict\n\nfrom agent.lib.metta_layer import LayerBase\n\nclass ObsShaper(LayerBase):\n    def __init__(self, obs_shape, num_objects, **cfg):\n        super().__init__(**cfg)\n        self._obs_shape = obs_shape\n        self._output_size = num_objects\n\n    def _forward(self, td: TensorDict):\n        x = td['x']\n\n        x_shape, space_shape = x.shape, self._obs_shape\n        x_n, space_n = len(x_shape), len(space_shape)\n        if x_shape[-space_n:] != space_shape:\n            raise ValueError('Invalid input tensor shape', x.shape)\n\n        if x_n == space_n + 1:\n            B, TT = x_shape[0], 1\n        elif x_n == space_n + 2:\n            B, TT = x_shape[:2]\n        else:\n            raise ValueError('Invalid input tensor shape', x.shape)\n\n        x = x.reshape(B * TT, *space_shape)\n        x = x.float()\n\n        # conv expects [batch, channel, w, h]. Below is hardcoded for [batch, w, h, channel]\n        if x.device.type == 'mps':\n            x = self._mps_permute(x)\n        else:\n            x = x.permute(0, 3, 1, 2)\n\n        td[self._name] = x\n        return td\n\n    def _mps_permute(self, x):\n        '''For compatibility with MPS, it throws an error on .permute()'''\n        bs, h, w, c = x.shape\n        x = x.contiguous().view(bs, h * w, c)\n        x = x.transpose(1, 2)\n        x = x.contiguous().view(bs, c, h, w)\n        return x\n"}
{"type": "source_file", "path": "devops/aws/batch/launch_task.py", "content": "import argparse\nimport netrc\nimport os\nimport random\nimport string\nimport sys\nimport subprocess\nimport json\nfrom colorama import Fore, Style, init\n\nimport boto3\n\n# Initialize colorama\ninit(autoreset=True)\n\nspecs = {\n    1: {\n        \"node_gpus\": 1,\n        \"node_ram_gb\": 60,\n        \"gpu_cpus\": 10,\n    },\n    4: {\n        \"node_gpus\": 4,\n        \"node_ram_gb\": 150,\n        \"gpu_cpus\": 12,\n    },\n    8: {\n        \"node_gpus\": 8,\n        \"node_ram_gb\": 300,\n        \"gpu_cpus\": 24,\n    },\n}\n\ndef get_current_commit(repo_path=None):\n    \"\"\"Get the current git commit hash.\"\"\"\n    try:\n        cmd = [\"git\", \"rev-parse\", \"HEAD\"]\n        if repo_path:\n            cmd = [\"git\", \"-C\", repo_path, \"rev-parse\", \"HEAD\"]\n        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n        return result.stdout.strip()\n    except subprocess.CalledProcessError:\n        return None\n\ndef is_commit_pushed(commit_hash, repo_path=None):\n    \"\"\"Check if a commit has been pushed to the remote repository.\"\"\"\n    try:\n        cmd = [\"git\", \"branch\", \"-r\", \"--contains\", commit_hash]\n        if repo_path:\n            cmd = [\"git\", \"-C\", repo_path, \"branch\", \"-r\", \"--contains\", commit_hash]\n        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n        # If there are any remote branches containing this commit, it has been pushed\n        return bool(result.stdout.strip())\n    except subprocess.CalledProcessError:\n        # If the command fails, assume the commit hasn't been pushed\n        return False\n\ndef submit_batch_job(args, task_args):\n    session_kwargs = {'region_name': 'us-east-1'}\n    if args.profile:\n        session_kwargs['profile_name'] = args.profile\n\n    # Create a new session with the specified profile\n    session = boto3.Session(**session_kwargs)\n    batch = session.client('batch')\n\n    random_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))\n    job_name = args.run.replace('.', '_') + \"_\" + random_id\n    job_queue = args.job_queue\n\n    request = {\n        'jobName': job_name,\n        'jobQueue': job_queue,\n        'jobDefinition': 'metta-batch-train-jd',\n        'containerOverrides': container_config(args, task_args, job_name)\n    }\n\n    # Choose job definition based on number of nodes\n    if args.num_nodes > 1:\n        request[\"jobDefinition\"] = \"metta-batch-dist-train\"\n        print(f\"Using multi-node job definition: {request['jobDefinition']} for {args.num_nodes} nodes\")\n        request[\"nodeOverrides\"] = {\n            'nodePropertyOverrides': [\n                {\n                    'targetNodes': '0:',\n                    'containerOverrides': container_config(args, task_args, job_name)\n                }\n            ],\n            'numNodes': args.num_nodes\n        }\n        del request[\"containerOverrides\"]\n\n    # Check if no_color attribute exists and is True\n    no_color = getattr(args, 'no_color', False)\n\n    # Check if dry_run attribute exists and is True\n    dry_run = getattr(args, 'dry_run', False)\n\n    if dry_run:\n        print(f\"\\n{'=' * 40}\")\n        print(f\"DRY RUN - Job would be submitted with the following details:\")\n        print(f\"{'=' * 40}\")\n        print(f\"Job Name: {job_name}\")\n        print(f\"Job Queue: {job_queue}\")\n        print(f\"Job Definition: {request['jobDefinition']}\")\n        if args.num_nodes > 1:\n            print(f\"Number of Nodes: {args.num_nodes}\")\n        print(f\"Number of GPUs per Node: {args.node_gpus}\")\n        print(f\"Total GPUs: {args.gpus}\")\n        print(f\"vCPUs per GPU: {args.gpu_cpus}\")\n        print(f\"RAM per Node: {args.node_ram_gb} GB\")\n        print(f\"Git Reference: {args.git_branch or args.git_commit}\")\n        print(f\"Mettagrid Reference: {args.mettagrid_branch or args.mettagrid_commit}\")\n        print(f\"{'-' * 40}\")\n        print(f\"Command: {args.cmd}\")\n        if task_args:\n            if no_color:\n                print(f\"\\nTask Arguments:\")\n                for i, arg in enumerate(task_args):\n                    print(f\"  {i+1}. {arg}\")\n            else:\n                print(f\"\\n{Fore.YELLOW}Task Arguments:{Style.RESET_ALL}\")\n                for i, arg in enumerate(task_args):\n                    print(f\"  {i+1}. {Fore.CYAN}{arg}{Style.RESET_ALL}\")\n        print(f\"\\n{'=' * 40}\")\n        print(\"DRY RUN - No job was actually submitted\")\n        print(f\"{'=' * 40}\")\n        return\n\n    response = batch.submit_job(**request)\n\n    # Print job information with colors if not disabled\n    job_id = response['jobId']\n    job_url = f\"https://us-east-1.console.aws.amazon.com/batch/v2/home?region=us-east-1#/jobs/detail/{job_id}\"\n\n    if no_color:\n        print(f\"Submitted job {job_name} to queue {job_queue} with job ID {job_id}\")\n        print(f\"{job_url}\")\n    else:\n        print(f\"Submitted job {job_name} to queue {job_queue} with job ID {Fore.GREEN}{Style.BRIGHT}{job_id}{Style.RESET_ALL}\")\n        print(f\"{Fore.BLUE}{Style.BRIGHT}{job_url}{Style.RESET_ALL}\")\n\n    # Pretty print task arguments\n    if task_args:\n        if no_color:\n            print(f\"\\nTask Arguments:\")\n            for i, arg in enumerate(task_args):\n                print(f\"  {i+1}. {arg}\")\n        else:\n            print(f\"\\n{Fore.YELLOW}Task Arguments:{Style.RESET_ALL}\")\n            for i, arg in enumerate(task_args):\n                print(f\"  {i+1}. {Fore.CYAN}{arg}{Style.RESET_ALL}\")\n\ndef container_config(args, task_args, job_name):\n    try:\n        # Get the wandb key from the .netrc file\n        netrc_info = netrc.netrc(os.path.expanduser('~/.netrc'))\n        wandb_key = netrc_info.authenticators('api.wandb.ai')[2]\n        if not wandb_key:\n            raise ValueError('WANDB_API_KEY not found in .netrc file')\n    except (FileNotFoundError, TypeError):\n        print(\"Error: Could not find WANDB_API_KEY in ~/.netrc file\")\n        print(\"Please ensure you have a valid ~/.netrc file with api.wandb.ai credentials\")\n        sys.exit(1)\n\n    # Calculate resource requirements\n    vcpus_per_gpu = args.gpu_cpus\n    total_vcpus = vcpus_per_gpu * args.node_gpus\n\n    # Memory in GB, convert to MB for AWS Batch API\n    memory_gb = int(args.node_ram_gb)\n    memory_mb = memory_gb * 1024\n\n    # Set up environment variables for distributed training\n    env_vars = [\n        {\n            'name': 'HYDRA_FULL_ERROR',\n            'value': '1'\n        },\n        {\n            'name': 'WANDB_API_KEY',\n            'value': wandb_key\n        },\n        {\n            'name': 'WANDB_SILENT',\n            'value': 'true'\n        },\n        {\n            'name': 'COLOR_LOGGING',\n            'value': 'false'\n        },\n        {\n            'name': 'WANDB_HOST',\n            'value': job_name\n        },\n        {\n            'name': 'METTA_HOST',\n            'value': job_name\n        },\n        {\n            'name': 'JOB_NAME',\n            'value': job_name\n        },\n        {\n            'name': 'METTA_USER',\n            'value': os.environ.get('USER', 'unknown')\n        },\n        {\n            'name': 'NCCL_DEBUG',\n            'value': 'ERROR'\n        },\n        {\n            'name': 'NCCL_IGNORE_DISABLED_P2P',\n            'value': '1'\n        },\n        {\n            'name': 'NCCL_IB_DISABLE',\n            'value': '1'\n        },\n        {\n            'name': 'NCCL_P2P_DISABLE',\n            'value': '1'\n        },\n    ]\n\n    # Add required environment variables for the entrypoint script\n    env_vars.extend([\n        {\n            'name': 'RUN_ID',\n            'value': args.run\n            },\n        {\n            'name': 'HARDWARE',\n            'value': 'aws'\n        },\n        {\n            'name': 'CMD',\n            'value': args.cmd\n        },\n        {\n            'name': 'NUM_GPUS',\n            'value': str(args.node_gpus)\n        },\n        {\n            'name': 'NUM_WORKERS',\n            'value': str(vcpus_per_gpu)\n        },\n        {\n            'name': 'GIT_REF',\n            'value': args.git_branch or args.git_commit\n        },\n        {\n            'name': 'METTAGRID_REF',\n            'value': args.mettagrid_branch or args.mettagrid_commit\n        },\n        {\n            'name': 'TASK_ARGS',\n            'value': ' '.join(task_args)\n        }\n    ])\n\n    # Build the command to run the entrypoint script\n    entrypoint_cmd = [\n        'git fetch',\n        f'git checkout {args.git_branch or args.git_commit}',\n        './devops/aws/batch/entrypoint.sh'\n    ]\n\n    print(\"\\n\".join([\n            f\"Resources: {args.num_nodes} nodes, {args.node_gpus} GPUs, {total_vcpus} vCPUs ({vcpus_per_gpu} per GPU), {memory_gb}GB RAM\"\n        ]))\n\n    # Create resource requirements\n    resource_requirements = [\n        {\n            'type': 'GPU',\n            'value': str(args.node_gpus)\n        }, {\n            'type': 'VCPU',\n            'value': str(total_vcpus)\n        }, {\n            'type': 'MEMORY',\n            'value': str(memory_mb)  # AWS Batch API expects MB\n        }\n    ]\n\n    return {\n        'command': [\"; \".join(entrypoint_cmd)],\n        'environment': env_vars,\n        'resourceRequirements': resource_requirements\n    }\n\ndef main():\n    \"\"\"Main entry point for the script.\"\"\"\n    parser = argparse.ArgumentParser(description='Launch an AWS Batch task with a wandb key.')\n    parser.add_argument('--cluster', default=\"metta\", help='The name of the ECS cluster.')\n    parser.add_argument('--run', required=True, help='The run id.')\n    parser.add_argument('--cmd', required=True, choices=[\"train\", \"sweep\", \"evolve\"], help='The command to run.')\n\n    parser.add_argument('--git-branch', default=None, help='The git branch to use for the task. If not specified, will use the current commit.')\n    parser.add_argument('--git-commit', default=None, help='The git commit to use for the task. If not specified, will use the current commit.')\n    parser.add_argument('--mettagrid-branch', default=None, help='The mettagrid branch to use for the task. If not specified, will use the current commit.')\n    parser.add_argument('--mettagrid-commit', default=None, help='The mettagrid commit to use for the task. If not specified, will use the current commit.')\n    parser.add_argument('--gpus', type=int, default=4, help='Number of GPUs per node to use for the task.')\n    parser.add_argument('--node-gpus', type=int, default=4, help='Number of GPUs per node to use for the task.')\n    parser.add_argument('--gpu-cpus', type=int, help='Number of CPUs per GPU (vCPUs will be 2x this value).')\n    parser.add_argument('--node-ram-gb', type=int, help='RAM per node in GB.')\n    parser.add_argument('--copies', type=int, default=1, help='Number of job copies to submit.')\n    parser.add_argument('--profile', default=\"stem\", help='AWS profile to use. If not specified, uses the default profile.')\n    parser.add_argument('--job-queue', default=\"metta-jq\", help='AWS Batch job queue to use.')\n    parser.add_argument('--skip-push-check', action='store_true', help='Skip checking if commits have been pushed.')\n    parser.add_argument('--no-color', action='store_true', help='Disable colored output.')\n    parser.add_argument('--dry-run', action='store_true', help='Dry run mode, prints job details without submitting.')\n    args, task_args = parser.parse_known_args()\n\n    args.num_nodes = max(1, args.gpus // args.node_gpus)\n    if args.node_ram_gb is None:\n        args.node_ram_gb = specs[args.node_gpus][\"node_ram_gb\"]\n    if args.gpu_cpus is None:\n        args.gpu_cpus = specs[args.node_gpus][\"gpu_cpus\"]\n\n    # Set default commit values if not specified\n    if args.git_branch is None and args.git_commit is None:\n        args.git_commit = get_current_commit()\n\n    if args.mettagrid_branch is None and args.mettagrid_commit is None:\n        args.mettagrid_commit = get_current_commit(\"deps/mettagrid\")\n\n    # Check if commits have been pushed\n    if not args.skip_push_check:\n        # Check if git commit has been pushed\n        if args.git_commit and not is_commit_pushed(args.git_commit):\n            print(f\"Error: Git commit {args.git_commit} has not been pushed to the remote repository.\")\n            print(\"Please push your changes or use --skip-push-check to bypass this check.\")\n            sys.exit(1)\n\n        # Check if mettagrid commit has been pushed\n        if args.mettagrid_commit and not is_commit_pushed(args.mettagrid_commit, \"deps/mettagrid\"):\n            print(f\"Error: Mettagrid commit {args.mettagrid_commit} has not been pushed to the remote repository.\")\n            print(\"Please push your changes or use --skip-push-check to bypass this check.\")\n            sys.exit(1)\n\n    # Submit the job\n    for i in range(args.copies):\n        submit_batch_job(args, task_args)\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "agent/policy_store.py", "content": "\"\"\"\nThis file implements a PolicyStore class that manages loading and caching of trained policies.\nIt provides functionality to:\n- Load policies from local files or remote URIs\n- Cache loaded policies to avoid reloading\n- Select policies based on metadata filters\n- Track policy metadata and versioning\n\nThe PolicyStore is used by the training system to manage opponent policies and checkpoints.\n\"\"\"\n\nfrom omegaconf import OmegaConf\nfrom omegaconf.listconfig import ListConfig\nimport wandb\nfrom agent.metta_agent import make_policy\nimport torch\nimport os\nimport warnings\nimport logging\nfrom typing import List, Union\nimport random\nfrom wandb.sdk import wandb_run\nfrom torch import nn\n\nlogger = logging.getLogger(\"policy_store\")\n\nclass PolicyRecord:\n    def __init__(self, policy_store, name: str, uri: str, metadata: dict):\n        self._policy_store = policy_store\n        self.name = name\n        self.uri = uri\n        self.metadata = metadata\n        self._policy = None\n        self._local_path = None\n\n        if self.uri.startswith(\"file://\"):\n            self._local_path = self.uri[len(\"file://\"):]\n\n    def policy(self) -> nn.Module:\n        if self._policy is None:\n            pr = self._policy_store._load_from_uri(self.uri)\n            self._policy = pr.policy()\n            self._local_path = pr.local_path()\n        return self._policy\n\n    def num_params(self):\n        return sum(p.numel() for p in self.policy().parameters() if p.requires_grad)\n\n    def local_path(self):\n        return self._local_path\n\nclass PolicyStore:\n    def __init__(self, cfg: OmegaConf, wandb_run: wandb_run.Run):\n        self._cfg = cfg\n        self._device = cfg.device\n        self._wandb_run = wandb_run\n        self._cached_prs = {}\n\n    def policy(self, policy: Union[str, OmegaConf]) -> PolicyRecord:\n        prs = self._policy_records(policy) if isinstance(policy, str) else self.policies(policy)\n        assert  len(prs) == 1, f\"Expected 1 policy, got {len(prs)}\"\n        return prs[0]\n\n    def _policy_records(self, uri, selector_type=\"top\", n=1, metric=\"epoch\"):\n        version = None\n        if uri.startswith(\"wandb://\"):\n            wandb_uri = uri[len(\"wandb://\"):]\n            if \":\" in wandb_uri:\n                wandb_uri, version = wandb_uri.split(\":\")\n            if wandb_uri.startswith(\"run/\"):\n                run_id = wandb_uri[len(\"run/\"):]\n                prs = self._prs_from_wandb_run(run_id, version)\n            elif wandb_uri.startswith(\"sweep/\"):\n                sweep_name = wandb_uri[len(\"sweep/\"):]\n                prs = self._prs_from_wandb_sweep(sweep_name, version)\n            else:\n                prs = self._prs_from_wandb_artifact(wandb_uri, version)\n\n        elif uri.startswith(\"file://\"):\n            prs = self._prs_from_path(uri[len(\"file://\"):])\n        else:\n            prs = self._prs_from_path(uri)\n\n        if selector_type == \"rand\":\n            return [random.choice(prs)]\n\n        elif selector_type == \"top\":\n            metric = metric\n\n            top = sorted(prs, key=lambda x: x.metadata.get(metric, 0))[-n:]\n            if len(top) < n:\n                logger.warning(f\"Only found {len(top)} policies matching criteria, requested {n}\")\n\n            logger.info(f\"Top {n} policies by {metric}:\")\n            logger.info(f\"{'Policy':<40} | {metric:<20}\")\n            logger.info(\"-\" * 62)\n            for pr in top:\n                logger.info(f\"{pr.name:<40} | {pr.metadata.get(metric, 0):<20.4f}\")\n\n            return top[-n:]\n        else:\n            raise ValueError(f\"Invalid selector type {selector_type}\")\n\n\n    def policies(self, policy_selector_cfg: OmegaConf) -> List[PolicyRecord]:\n        prs = []\n        if isinstance(policy_selector_cfg.uri, ListConfig):\n            for uri in policy_selector_cfg.uri:\n                prs += self._policy_records(uri, policy_selector_cfg.type, policy_selector_cfg.range, policy_selector_cfg.metric)\n        else:\n            prs = self._policy_records(policy_selector_cfg.uri, policy_selector_cfg.type, policy_selector_cfg.range, policy_selector_cfg.metric)\n\n        for k,v in policy_selector_cfg.filters.items():\n            prs = [pr for pr in prs if pr.metadata.get(k, None) == v]\n\n        if len(prs) == 0:\n            logger.warning(\"No policies found matching criteria\")\n\n        return prs\n\n    def make_model_name(self, epoch: int):\n        return f\"model_{epoch:04d}.pt\"\n\n    def create(self, env) -> PolicyRecord:\n        policy = make_policy(env, self._cfg)\n        name = self.make_model_name(0)\n        path = os.path.join(self._cfg.trainer.checkpoint_dir, name)\n        pr = self.save(name, path, policy, {\n            \"action_names\": env.action_names(),\n            \"agent_step\": 0,\n            \"epoch\": 0,\n            \"generation\": 0,\n            \"train_time\": 0,\n        })\n        pr._policy = policy\n        return pr\n\n    def save(self, name: str, path: str, policy: nn.Module, metadata: dict):\n        logger.info(f\"Saving policy to {path}\")\n        pr = PolicyRecord(self, path, \"file://\" + path, metadata)\n        pr._policy = policy\n        pr._policy_store = None\n        torch.save(pr, path)\n        pr._policy_store = self\n        # Don't cache the policy that we just saved,\n        # since it might be updated later. We always\n        # load the policy from the file when needed.\n        pr._policy = None\n        self._cached_prs[path] = pr\n        return pr\n\n    def add_to_wandb_run(self, run_id: str, pr: PolicyRecord, additional_files=None):\n        return self.add_to_wandb_artifact(\n            run_id,\n            \"model\",\n            pr.metadata,\n            pr.local_path(),\n            additional_files\n        )\n\n    def add_to_wandb_sweep(self, sweep_name: str, pr: PolicyRecord, additional_files=None):\n        return self.add_to_wandb_artifact(\n            sweep_name,\n            \"sweep_model\",\n            pr.metadata,\n            pr.local_path(),\n            additional_files\n        )\n\n    def add_to_wandb_artifact(self, name: str, type: str, metadata: dict, local_path: str, additional_files=None):\n        if self._wandb_run is None:\n            raise ValueError(\"PolicyStore was not initialized with a wandb run\")\n\n        additional_files = additional_files or []\n\n        artifact = wandb.Artifact(name, type=type, metadata=metadata)\n        artifact.add_file(local_path, name=\"model.pt\")\n        for file in additional_files:\n            artifact.add_file(file)\n        artifact.save()\n        artifact.wait()\n        logger.info(f\"Added artifact {artifact.qualified_name}\")\n        self._wandb_run.log_artifact(artifact)\n\n    def _prs_from_path(self, path: str) -> List[PolicyRecord]:\n        paths = []\n\n        if path.endswith(\".pt\"):\n            paths.append(path)\n        else:\n            paths.extend([os.path.join(path, p) for p in os.listdir(path) if p.endswith(\".pt\")])\n\n        return [\n            self._load_from_file(path, metadata_only=True)\n            for path in paths\n        ]\n\n    def _prs_from_wandb_artifact(self, uri: str, version: str = None) -> List[PolicyRecord]:\n        entity, project, artifact_type, name = uri.split(\"/\")\n        path = f\"{entity}/{project}/{name}\"\n        if not wandb.Api().artifact_collection_exists(type=artifact_type, name=path):\n            logger.warning(f\"No artifact collection found at {uri}\")\n            return []\n        artifact_collection = wandb.Api().artifact_collection(type_name=artifact_type, name=path)\n\n        artifacts = artifact_collection.artifacts()\n\n        if version is not None:\n            artifacts = [a for a in artifacts if a.version == version]\n\n        return [\n            PolicyRecord(\n                self,\n                name=a.name,\n                uri=\"wandb://\" + a.qualified_name,\n                metadata=a.metadata\n            ) for a in artifacts\n        ]\n\n    def _prs_from_wandb_sweep(self, sweep_name: str, version: str = None) -> List[PolicyRecord]:\n        return self._prs_from_wandb_artifact(\n            f\"{self._cfg.wandb.entity}/{self._cfg.wandb.project}/sweep_model/{sweep_name}\",\n            version\n        )\n\n    def _prs_from_wandb_run(self, run_id: str, version: str = None) -> List[PolicyRecord]:\n        return self._prs_from_wandb_artifact(\n            f\"{self._cfg.wandb.entity}/{self._cfg.wandb.project}/model/{run_id}\",\n            version\n        )\n\n    def _load_from_uri(self, uri: str):\n        if uri.startswith(\"wandb://\"):\n            return self._load_wandb_artifact(uri[len(\"wandb://\"):])\n        elif uri.startswith(\"file://\"):\n            return self._load_from_file(uri[len(\"file://\"):])\n        else:\n            return self._load_from_file(uri)\n\n    def _load_from_file(self, path: str, metadata_only: bool = False) -> PolicyRecord:\n        if path in self._cached_prs:\n            if metadata_only or self._cached_prs[path]._policy is not None:\n                return self._cached_prs[path]\n        if not path.endswith('.pt') and os.path.isdir(path):\n            path = os.path.join(path, os.listdir(path)[-1])\n        logger.info(f\"Loading policy from {path}\")\n\n        assert path.endswith('.pt'), f\"Policy file {path} does not have a .pt extension\"\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", category=FutureWarning)\n            pr = torch.load(\n                path,\n                map_location=self._device,\n                weights_only=False,\n            )\n            pr._policy_store = self\n            pr._local_path = path\n            self._cached_prs[path] = pr\n            if metadata_only:\n                pr._policy = None\n                pr._local_path = None\n            logger.info(f\"Loaded policy from {path} with metadata {pr.metadata}\")\n            return pr\n\n    def _load_wandb_artifact(self, qualified_name: str):\n        logger.info(f\"Loading policy from wandb artifact {qualified_name}\")\n\n        artifact = wandb.Api().artifact(qualified_name)\n\n        artifact_path = os.path.join(self._cfg.data_dir, \"artifacts\", artifact.name)\n\n        if not os.path.exists(artifact_path):\n            artifact.download(root=artifact_path)\n\n        logger.info(f\"Downloaded artifact {artifact.name} to {artifact_path}\")\n\n        pr = self._load_from_file(\n            os.path.join(artifact_path, \"model.pt\")\n        )\n        pr.metadata.update(artifact.metadata)\n        return pr\n"}
{"type": "source_file", "path": "agent/lib/action.py", "content": "import agent.lib.nn_layer_library as nn_layer_library\n\nclass ActionType(nn_layer_library.Linear):\n    def __init__(self, action_type_size, **cfg):\n        super().__init__(**cfg)\n        self._output_size = action_type_size\n\nclass ActionParam(nn_layer_library.Linear):\n    def __init__(self, action_param_size, **cfg):\n        super().__init__(**cfg)\n        self._output_size = action_param_size\n"}
{"type": "source_file", "path": "agent/lib/merge_layer.py", "content": "import omegaconf\nimport torch\nfrom tensordict import TensorDict\nfrom agent.lib.metta_layer import LayerBase\n\nclass MergeLayerBase(LayerBase):\n    def __init__(self, name, sources, **cfg):\n        super().__init__(name)\n        self.sources_list = list(sources)\n        self.default_dim = -1\n        self._ready = False\n\n        self.input_source = []\n        for src_cfg in self.sources_list:\n            self.input_source.append(src_cfg['source_name'])\n\n    @property\n    def ready(self):\n        return self._ready\n\n    def setup(self, input_source_components=None):\n        if self._ready:\n            return\n\n        self.input_source_components = input_source_components\n\n        self.sizes = []\n        self.dims = []\n        for src_cfg in self.sources_list:\n            source_name = src_cfg['source_name']\n            full_source_size = self.input_source_components[source_name]._output_size\n\n            processed_size = full_source_size\n            if src_cfg.get('slice') is not None:\n                slice_range = src_cfg['slice']\n                if isinstance(slice_range, omegaconf.listconfig.ListConfig):\n                    slice_range = list(slice_range)\n                if not (isinstance(slice_range, (list, tuple)) and len(slice_range) == 2):\n                    raise ValueError(f\"'slice' must be a two-element list/tuple for source {source_name}.\")\n\n                start, end = slice_range\n                slice_dim = src_cfg.get(\"dim\", self.default_dim)\n                length = end - start\n                src_cfg['_slice_params'] = {\n                    'start': start,\n                    'length': length,\n                    'dim': slice_dim\n                }\n                processed_size = length\n\n            self.sizes.append(processed_size)\n            self.dims.append(src_cfg.get(\"dim\", self.default_dim))\n\n        self._setup_merge_layer()\n        self._ready = True\n\n    def _setup_merge_layer(self):\n        raise NotImplementedError(\"Subclasses should implement this method.\")\n\n    def forward(self, td: TensorDict):\n        outputs = []\n        for src_cfg in self.sources_list:\n            source_name = src_cfg['source_name']\n            self.input_source_components[source_name].forward(td)\n            src_tensor = td[source_name]\n\n            if '_slice_params' in src_cfg:\n                params = src_cfg['_slice_params']\n                src_tensor = torch.narrow(src_tensor, dim=params['dim'], start=params['start'], length=params['length'])\n            outputs.append(src_tensor)\n\n        return self._merge(outputs, td)\n\n    def _merge(self, outputs, td):\n        raise NotImplementedError(\"Subclasses should implement this method.\")\n\n\nclass ConcatMergeLayer(MergeLayerBase):\n    def _setup_merge_layer(self):\n        if not all(d == self.dims[0] for d in self.dims):\n            raise ValueError(f\"For 'concat', all sources must have the same 'dim'. Got dims: {self.dims}\")\n        self._merge_dim = self.dims[0]\n        self._output_size = sum(self.sizes)\n\n    def _merge(self, outputs, td):\n        merged = torch.cat(outputs, dim=self._merge_dim)\n        td[self._name] = merged\n        return td\n\n\nclass AddMergeLayer(MergeLayerBase):\n    def _setup_merge_layer(self):\n        if not all(s == self.sizes[0] for s in self.sizes):\n            raise ValueError(f\"For 'add', all source sizes must match. Got sizes: {self.sizes}\")\n        self._output_size = self.sizes[0]\n\n    def _merge(self, outputs, td):\n        merged = outputs[0]\n        for tensor in outputs[1:]:\n            merged = merged + tensor\n        td[self._name] = merged\n        return td\n\n\nclass SubtractMergeLayer(MergeLayerBase):\n    def _setup_merge_layer(self):\n        if not all(s == self.sizes[0] for s in self.sizes):\n            raise ValueError(f\"For 'subtract', all source sizes must match. Got sizes: {self.sizes}\")\n        self._output_size = self.sizes[0]\n\n    def _merge(self, outputs, td):\n        if len(outputs) != 2:\n            raise ValueError(\"Subtract merge_op requires exactly two sources.\")\n        merged = outputs[0] - outputs[1]\n        td[self._name] = merged\n        return td\n\n\nclass MeanMergeLayer(MergeLayerBase):\n    def _setup_merge_layer(self):\n        if not all(s == self.sizes[0] for s in self.sizes):\n            raise ValueError(f\"For 'mean', all source sizes must match. Got sizes: {self.sizes}\")\n        self._output_size = self.sizes[0]\n\n    def _merge(self, outputs, td):\n        merged = outputs[0]\n        for tensor in outputs[1:]:\n            merged = merged + tensor\n        merged = merged / len(outputs)\n        td[self._name] = merged\n        return td\n"}
{"type": "source_file", "path": "agent/util/distribution_utils.py", "content": "import torch\nfrom typing import List, Union\nfrom torch.distributions.utils import logits_to_probs\n\n\ndef log_prob(logits, value):\n    \"\"\"\n    Compute log probability of a value given logits.\n    \n    Args:\n        logits: Unnormalized log probabilities\n        value: The value to compute probability for\n        \n    Returns:\n        Log probability of the value\n    \"\"\"\n    value = value.long().unsqueeze(-1)\n    value, log_pmf = torch.broadcast_tensors(value, logits)\n    value = value[..., :1]\n    return log_pmf.gather(-1, value).squeeze(-1)\n\n\ndef entropy(logits):\n    \"\"\"\n    Compute entropy of a categorical distribution given logits.\n    \n    Args:\n        logits: Unnormalized log probabilities\n        \n    Returns:\n        Entropy of the distribution\n    \"\"\"\n    min_real = torch.finfo(logits.dtype).min\n    logits = torch.clamp(logits, min=min_real)\n    p_log_p = logits * logits_to_probs(logits)\n    return -p_log_p.sum(-1)\n\n\ndef sample_logits(logits: Union[torch.Tensor, List[torch.Tensor]], action=None):\n    \"\"\"\n    Sample actions from logits and compute log probabilities and entropy.\n    \n    Args:\n        logits: Unnormalized log probabilities, either a single tensor or a list of tensors\n        action: Optional pre-specified actions to compute probabilities for\n        \n    Returns:\n        Tuple of (action, log_probability, entropy, normalized_logits)\n    \"\"\"\n    normalized_logits = [l - l.logsumexp(dim=-1, keepdim=True) for l in logits]\n\n    if action is None:\n        action = torch.stack([torch.multinomial(logits_to_probs(l), 1).squeeze() for l in logits])\n    else:\n        batch = logits[0].shape[0]\n        action = action.view(batch, -1).T\n\n    assert len(logits) == len(action)\n\n    logprob = torch.stack([log_prob(l, a) for l, a in zip(normalized_logits, action)]).T.sum(1)\n    logits_entropy = torch.stack([entropy(l) for l in normalized_logits]).T.sum(1)\n\n    return action.T, logprob, logits_entropy, normalized_logits "}
{"type": "source_file", "path": "agent/__init__.py", "content": ""}
{"type": "source_file", "path": "devops/aws/setup_sso.py", "content": "#!/usr/bin/env python3\nimport argparse\nimport os\nimport subprocess\nimport getpass\nimport sys\nimport time\nfrom pathlib import Path\nimport traceback\n\n# ANSI color codes for terminal output\nclass Colors:\n    HEADER = '\\033[95m'\n    BLUE = '\\033[94m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'\n    RED = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n\ndef print_status(message):\n    \"\"\"Print a status message with timestamp.\"\"\"\n    timestamp = time.strftime(\"%H:%M:%S\")\n    print(f\"{Colors.BLUE}[{timestamp}] STATUS: {message}{Colors.ENDC}\")\n\ndef print_success(message):\n    \"\"\"Print a success message with timestamp.\"\"\"\n    timestamp = time.strftime(\"%H:%M:%S\")\n    print(f\"{Colors.GREEN}[{timestamp}] SUCCESS: {message}{Colors.ENDC}\")\n\ndef print_error(message):\n    \"\"\"Print an error message with timestamp.\"\"\"\n    timestamp = time.strftime(\"%H:%M:%S\")\n    print(f\"{Colors.RED}[{timestamp}] ERROR: {message}{Colors.ENDC}\", file=sys.stderr)\n\ndef print_warning(message):\n    \"\"\"Print a warning message with timestamp.\"\"\"\n    timestamp = time.strftime(\"%H:%M:%S\")\n    print(f\"{Colors.YELLOW}[{timestamp}] WARNING: {message}{Colors.ENDC}\")\n\ndef run_command(cmd, check=True, capture_output=False):\n    \"\"\"Run a shell command and handle errors.\"\"\"\n    print_status(f\"Running command: {cmd}\")\n    try:\n        result = subprocess.run(cmd, shell=True, check=check,\n                               capture_output=capture_output, text=True)\n        if result.returncode == 0:\n            print_status(f\"Command completed successfully\")\n        else:\n            print_warning(f\"Command exited with code {result.returncode}\")\n\n        return result\n    except subprocess.CalledProcessError as e:\n        print_error(f\"Error executing command: {cmd}\")\n        print_error(f\"Error message: {e}\")\n        if capture_output and e.stderr:\n            print_error(f\"Error details: {e.stderr}\")\n        if check:\n            sys.exit(1)\n        return e\n\ndef setup_root_profile():\n    \"\"\"Set up AWS configuration for root account under stem-root profile.\"\"\"\n    print_status(\"Setting up AWS configuration for root account...\")\n\n    # Prompt for AWS access key and secret\n    print_status(\"Please enter your AWS root account credentials:\")\n    aws_access_key_id = input(\"AWS Access Key ID: \")\n    aws_secret_access_key = getpass.getpass(\"AWS Secret Access Key: \")\n\n    if not aws_access_key_id or not aws_secret_access_key:\n        print_error(\"AWS credentials cannot be empty\")\n        sys.exit(1)\n\n    # Create AWS credentials file\n    aws_dir = Path.home() / \".aws\"\n    aws_dir.mkdir(exist_ok=True)\n\n    credentials_path = aws_dir / \"credentials\"\n\n    # Read existing credentials if file exists\n    print_status(f\"Checking for existing credentials file at {credentials_path}\")\n    existing_credentials = {}\n    if credentials_path.exists():\n        print_status(\"Existing credentials file found, reading contents...\")\n        try:\n            with open(credentials_path, 'r') as f:\n                current_profile = None\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('[') and line.endswith(']'):\n                        current_profile = line[1:-1]\n                        existing_credentials[current_profile] = []\n                    elif current_profile:\n                        existing_credentials[current_profile].append(line)\n            print_status(f\"Found {len(existing_credentials)} existing profile(s)\")\n        except Exception as e:\n            print_error(f\"Error reading credentials file: {e}\")\n            traceback.print_exc()\n            sys.exit(1)\n\n    # Add or update stem-root profile\n    print_status(\"Updating credentials file with stem-root profile...\")\n    try:\n        with open(credentials_path, 'w') as f:\n            for profile, lines in existing_credentials.items():\n                if profile != \"stem-root\":  # Skip stem-root as we'll rewrite it\n                    f.write(f\"[{profile}]\\n\")\n                    for line in lines:\n                        f.write(f\"{line}\\n\")\n\n            # Write stem-root profile\n            f.write(f\"[stem-root]\\n\")\n            f.write(f\"aws_access_key_id = {aws_access_key_id}\\n\")\n            f.write(f\"aws_secret_access_key = {aws_secret_access_key}\\n\")\n            f.write(f\"region = us-east-1\\n\")\n        print_success(\"Successfully updated credentials file\")\n    except Exception as e:\n        print_error(f\"Error writing to credentials file: {e}\")\n        traceback.print_exc()\n        sys.exit(1)\n\n    # Update AWS config file\n    config_path = aws_dir / \"config\"\n\n    # Read existing config if file exists\n    print_status(f\"Checking for existing config file at {config_path}\")\n    existing_config = {}\n    if config_path.exists():\n        print_status(\"Existing config file found, reading contents...\")\n        try:\n            with open(config_path, 'r') as f:\n                current_section = None\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('[') and line.endswith(']'):\n                        current_section = line[1:-1]\n                        existing_config[current_section] = []\n                    elif current_section:\n                        existing_config[current_section].append(line)\n            print_status(f\"Found {len(existing_config)} existing section(s)\")\n        except Exception as e:\n            print_error(f\"Error reading config file: {e}\")\n            traceback.print_exc()\n            sys.exit(1)\n\n    # Add or update stem-root profile in config\n    print_status(\"Updating config file with stem-root profile...\")\n    try:\n        with open(config_path, 'w') as f:\n            for section, lines in existing_config.items():\n                if section != \"stem-root\" and section != \"profile stem-root\":  # Skip as we'll rewrite it\n                    f.write(f\"[{section}]\\n\")\n                    for line in lines:\n                        f.write(f\"{line}\\n\")\n\n            # Write stem-root profile\n            f.write(f\"[profile stem-root]\\n\")\n            f.write(f\"region = us-east-1\\n\")\n            f.write(f\"output = json\\n\")\n        print_success(\"Successfully updated config file\")\n    except Exception as e:\n        print_error(f\"Error writing to config file: {e}\")\n        traceback.print_exc()\n        sys.exit(1)\n\n    # Test the configuration\n    print_status(\"Testing AWS access with root credentials...\")\n    result = run_command(\"aws s3 ls --profile stem-root\", check=False)\n    if result.returncode != 0:\n        print_warning(\"Could not verify AWS access with the stem-root profile.\")\n        print_warning(\"Please check your credentials and try again.\")\n    else:\n        print_success(\"Successfully verified AWS access with stem-root profile.\")\n\n    # Create a helper script to open the AWS console\n    print_status(\"Creating AWS console helper script...\")\n    console_script = aws_dir / \"console.sh\"\n    try:\n        with open(console_script, 'w') as f:\n            f.write(\"\"\"#!/bin/bash\n# Helper script to open the AWS console in the browser\naws_signin_url=\"https://signin.aws.amazon.com/console\"\nopen \"$aws_signin_url\" 2>/dev/null || xdg-open \"$aws_signin_url\" 2>/dev/null || echo \"Could not open browser. Please visit $aws_signin_url manually.\"\n\"\"\")\n        print_success(\"Successfully created console helper script\")\n    except Exception as e:\n        print_error(f\"Error creating console script: {e}\")\n        traceback.print_exc()\n        sys.exit(1)\n\n    # Make the script executable\n    try:\n        os.chmod(console_script, 0o755)\n        print_status(\"Made console script executable\")\n    except Exception as e:\n        print_error(f\"Error making console script executable: {e}\")\n        traceback.print_exc()\n        sys.exit(1)\n\n    # Add an alias for the console command to shell config files\n    print_status(\"Adding aws-console alias to shell configuration files...\")\n    for rc_file in ['.bashrc', '.zshrc']:\n        rc_path = Path.home() / rc_file\n        if rc_path.exists():\n            try:\n                with open(rc_path, 'r') as f:\n                    content = f.read()\n\n                if 'alias aws-console=' not in content:\n                    with open(rc_path, 'a') as f:\n                        f.write(f\"\\nalias aws-console='{console_script}'\\n\")\n                    print_status(f\"Added aws-console alias to {rc_file}\")\n                else:\n                    print_status(f\"aws-console alias already exists in {rc_file}\")\n            except Exception as e:\n                print_error(f\"Error updating {rc_file}: {e}\")\n                traceback.print_exc()\n\n    # Set AWS_PROFILE in the current shell\n    os.environ[\"AWS_PROFILE\"] = \"stem-root\"\n    print_status(\"Set AWS_PROFILE=stem-root in current shell\")\n\n    # Update shell config files to use stem-root profile\n    print_status(\"Updating shell configuration files to use stem-root profile...\")\n    for rc_file in ['.bashrc', '.zshrc']:\n        rc_path = Path.home() / rc_file\n        if rc_path.exists():\n            try:\n                with open(rc_path, 'r') as f:\n                    content = f.read()\n\n                # Remove any existing AWS_PROFILE export\n                lines = content.split('\\n')\n                new_lines = [line for line in lines if not line.strip().startswith('export AWS_PROFILE=')]\n\n                # Add the new export\n                new_lines.append('export AWS_PROFILE=stem-root')\n\n                with open(rc_path, 'w') as f:\n                    f.write('\\n'.join(new_lines))\n\n                print_status(f\"Updated AWS_PROFILE in {rc_file} to use stem-root\")\n            except Exception as e:\n                print_error(f\"Error updating {rc_file}: {e}\")\n                traceback.print_exc()\n\n    print_success(\"Root account setup complete!\")\n    print_status(\"You can open the AWS Console in your browser by typing 'aws-console'\")\n\ndef setup_sso_profile():\n    \"\"\"Set up AWS configuration for SSO access.\"\"\"\n    print_status(\"Setting up AWS configuration for SSO access...\")\n\n    # Create AWS directory if it doesn't exist\n    aws_dir = Path.home() / \".aws\"\n    try:\n        aws_dir.mkdir(exist_ok=True)\n        print_status(f\"Created AWS config directory at {aws_dir}\")\n    except Exception as e:\n        print_error(f\"Failed to create AWS config directory: {e}\")\n        traceback.print_exc()\n        sys.exit(1)\n\n    # Create AWS config file with SSO configuration\n    config_path = aws_dir / \"config\"\n\n    # Read existing config if file exists to preserve other profiles\n    print_status(f\"Checking for existing config file at {config_path}\")\n    existing_config = {}\n    if config_path.exists():\n        print_status(\"Existing config file found, reading contents...\")\n        try:\n            with open(config_path, 'r') as f:\n                current_section = None\n                for line in f:\n                    line = line.strip()\n                    if line.startswith('[') and line.endswith(']'):\n                        current_section = line[1:-1]\n                        existing_config[current_section] = []\n                    elif current_section:\n                        existing_config[current_section].append(line)\n            print_status(f\"Found {len(existing_config)} existing section(s)\")\n        except Exception as e:\n            print_error(f\"Error reading config file: {e}\")\n            traceback.print_exc()\n            sys.exit(1)\n\n    # Add or update SSO profiles\n    print_status(\"Updating config file with SSO profiles...\")\n    try:\n        with open(config_path, 'w') as f:\n            # Preserve existing profiles except the ones we're updating\n            for section, lines in existing_config.items():\n                if section not in [\"stem\", \"profile stem\", \"sso-session stem-sso\"]:\n                    f.write(f\"[{section}]\\n\")\n                    for line in lines:\n                        f.write(f\"{line}\\n\")\n\n            # Write stem profile\n            f.write(\"\"\"[stem]\nregion = us-east-1\noutput = json\n\n[profile stem]\nsso_session = stem-sso\nsso_account_id = 767406518141\nsso_role_name = PowerUserAccess\nregion = us-east-1\n\n[sso-session stem-sso]\nsso_start_url = https://stemai.awsapps.com/start/\nsso_region = us-east-1\nsso_registration_scopes = sso:account:access\n\"\"\")\n        print_success(\"Successfully updated config file with SSO profiles\")\n    except Exception as e:\n        print_error(f\"Error writing to config file: {e}\")\n        traceback.print_exc()\n        sys.exit(1)\n\n    # Log in to AWS SSO\n    print_status(\"Logging in to AWS SSO...\")\n    print_status(\"This will open a browser window. Please complete the login process there.\")\n    result = run_command(\"aws sso login --profile stem\", check=False)\n    if result.returncode != 0:\n        print_error(\"Could not log in to AWS SSO.\")\n        print_error(\"Please check your network connection and try again.\")\n        sys.exit(1)\n\n    # Verify the profile works\n    print_status(\"Testing AWS access with stem profile...\")\n    result = run_command(\"aws s3 ls --profile stem\", check=False)\n    if result.returncode != 0:\n        print_error(\"Could not access AWS with the stem profile.\")\n        print_error(\"Please check your SSO configuration and try again.\")\n        sys.exit(1)\n    else:\n        print_success(\"Successfully verified AWS access with stem profile.\")\n\n    # Set AWS_PROFILE in the current shell\n    os.environ[\"AWS_PROFILE\"] = \"stem\"\n    print_status(\"Set AWS_PROFILE=stem in current shell\")\n\n    # Add profile to shell config files\n    print_status(\"Updating shell configuration files...\")\n    for rc_file in ['.bashrc', '.zshrc']:\n        rc_path = Path.home() / rc_file\n        if rc_path.exists():\n            try:\n                with open(rc_path, 'r') as f:\n                    content = f.read()\n\n                # Remove any existing AWS_PROFILE export\n                lines = content.split('\\n')\n                new_lines = [line for line in lines if not line.strip().startswith('export AWS_PROFILE=')]\n\n                # Add the new export\n                new_lines.append('export AWS_PROFILE=stem')\n\n                with open(rc_path, 'w') as f:\n                    f.write('\\n'.join(new_lines))\n\n                print_status(f\"Updated AWS_PROFILE in {rc_file}\")\n            except Exception as e:\n                print_error(f\"Error updating {rc_file}: {e}\")\n                traceback.print_exc()\n\n    print_success(\"SSO setup complete!\")\n\ndef main():\n    try:\n        parser = argparse.ArgumentParser(description='Set up AWS SSO or root account access')\n        parser.add_argument('--root', action='store_true', help='Set up root account access only (skips SSO)')\n        args = parser.parse_args()\n\n        print_status(f\"Starting AWS setup script (Python version)\")\n\n        if args.root:\n            print_status(f\"Mode: Root Account Only\")\n            print_status(\"Root account setup requested, skipping SSO setup\")\n            setup_root_profile()\n        else:\n            print_status(f\"Mode: SSO Only\")\n            setup_sso_profile()\n            print_status(\"Root account setup not requested (use --root flag to set up root credentials)\")\n\n        print_success(\"AWS setup completed successfully!\")\n        print_status(\"IMPORTANT: To apply changes immediately, run: source ~/.bashrc (or source ~/.zshrc)\")\n    except KeyboardInterrupt:\n        print_error(\"\\nSetup interrupted by user. Exiting...\")\n        sys.exit(1)\n    except Exception as e:\n        print_error(f\"Unexpected error: {e}\")\n        traceback.print_exc()\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "devops/aws/batch/register_job_definition.py", "content": "#!/usr/bin/env python3\nimport argparse\nimport json\nimport boto3\nimport os\nfrom pprint import pprint\n\ndef get_role_arn(role_name, profile=None):\n    \"\"\"Look up the ARN for a given IAM role name.\"\"\"\n    try:\n        # Use the specified profile or default to stem-root\n        session = boto3.Session(profile_name=profile or 'stem-root', region_name='us-east-1')\n        iam = session.client('iam')\n        response = iam.get_role(RoleName=role_name)\n        return response['Role']['Arn']\n    except Exception as e:\n        print(f\"Error: Could not retrieve ARN for role {role_name}: {e}\")\n        # Return a hardcoded ARN as fallback\n        return f\"arn:aws:iam::767406518141:role/{role_name}\"\n\ndef get_efs_id(name_tag=None):\n    \"\"\"Look up the EFS file system ID, optionally filtering by name tag.\"\"\"\n    try:\n        # Explicitly use the default profile\n        efs = boto3.client('efs', region_name='us-east-1')\n        response = efs.describe_file_systems()\n\n        # If no name tag is provided, return the first file system\n        if not name_tag:\n            if response['FileSystems']:\n                return response['FileSystems'][0]['FileSystemId']\n            raise ValueError(\"No EFS file systems found\")\n\n        # Filter by name tag if provided\n        for fs in response['FileSystems']:\n            # Get tags for this file system\n            tags_response = efs.describe_tags(FileSystemId=fs['FileSystemId'])\n            for tag in tags_response['Tags']:\n                if tag['Key'] == 'Name' and tag['Value'] == name_tag:\n                    return fs['FileSystemId']\n\n        # If we get here, no matching file system was found\n        if response['FileSystems']:\n            print(f\"Warning: No EFS with name tag '{name_tag}' found. Using first available.\")\n            return response['FileSystems'][0]['FileSystemId']\n        raise ValueError(\"No EFS file systems found\")\n    except Exception as e:\n        print(f\"Error: Could not retrieve EFS file system ID: {e}\")\n        # Return a hardcoded EFS ID as fallback\n        return \"fs-084c321137786b15c\"\n\ndef create_job_definition(args):\n    \"\"\"Create a job definition dictionary without registering it.\"\"\"\n    # Use the stem-root profile for AWS operations\n    session = boto3.Session(profile_name='stem-root', region_name='us-east-1')\n\n    # Look up role ARNs if not provided\n    job_role_arn = args.job_role_arn\n    if not job_role_arn:\n        job_role_arn = get_role_arn(args.job_role_name)\n\n    execution_role_arn = args.execution_role_arn\n    if not execution_role_arn:\n        execution_role_arn = get_role_arn(args.execution_role_name)\n\n    # Look up EFS ID if not provided\n    efs_id = args.efs_id\n    if not efs_id:\n        efs_id = get_efs_id(args.efs_name)\n\n    # Prepare job definition parameters\n    job_def = {\n        \"jobDefinitionName\": args.job_definition_name,\n        \"type\": \"multinode\",\n        \"nodeProperties\": {\n            \"mainNode\": 0,\n            \"nodeRangeProperties\": [\n                {\n                    \"targetNodes\": \"0:\",\n                    \"container\": {\n                        \"image\": args.image,\n                        \"command\": [\n                            \"/bin/bash\",\n                            \"-c\"\n                        ],\n                        \"jobRoleArn\": job_role_arn,\n                        \"executionRoleArn\": execution_role_arn,\n                        \"volumes\": [\n                            {\n                                \"name\": \"efs\",\n                                \"efsVolumeConfiguration\": {\n                                    \"fileSystemId\": efs_id\n                                }\n                            },\n                        ],\n                        \"environment\": [\n                            {\n                                \"name\": \"NCCL_DEBUG\",\n                                \"value\": \"INFO\"\n                            },\n                            {\n                                \"name\": \"NCCL_SOCKET_IFNAME\",\n                                \"value\": \"eth0\"\n                            }\n                        ],\n                        \"mountPoints\": [\n                            {\n                                \"containerPath\": \"/mnt/efs\",\n                                \"readOnly\": False,\n                                \"sourceVolume\": \"efs\"\n                            }\n                        ],\n                        \"ulimits\": [\n                            {\n                                \"hardLimit\": 640000,\n                                \"name\": \"nproc\",\n                                \"softLimit\": 640000\n                            },\n                            {\n                                \"hardLimit\": 640000,\n                                \"name\": \"nofile\",\n                                \"softLimit\": 640000\n                            }\n                        ],\n                        \"user\": \"root\",\n                        \"resourceRequirements\": [\n                            {\n                                \"value\": str(args.vcpus),\n                                \"type\": \"VCPU\"\n                            },\n                            {\n                                \"value\": str(args.memory),\n                                \"type\": \"MEMORY\"\n                            },\n                            {\n                                \"value\": str(args.gpus),\n                                \"type\": \"GPU\"\n                            }\n                        ],\n                        \"linuxParameters\": {\n                            \"devices\": [],\n                            \"sharedMemorySize\": args.shared_memory,\n                            \"tmpfs\": [],\n                            \"maxSwap\": 0,\n                            \"swappiness\": 0\n                        },\n                        \"logConfiguration\": {\n                            \"logDriver\": \"awslogs\",\n                            # \"options\": {\n                            #     \"awslogs-group\": f\"/aws/batch/{args.job_definition_name}\",\n                            #     \"awslogs-region\": \"us-east-1\",\n                            #     \"awslogs-stream-prefix\": \"batch-job\",\n                            #     \"awslogs-create-group\": \"true\",\n                            #     \"mode\": \"non-blocking\",\n                            #     \"max-buffer-size\": \"4m\"\n                            # }\n                        }\n                    }\n                }\n            ],\n            \"numNodes\": args.num_nodes\n        },\n        \"parameters\": {},\n        \"platformCapabilities\": [\n            \"EC2\"\n        ],\n        \"retryStrategy\": {\n            \"attempts\": 10,\n            \"evaluateOnExit\": [\n                {\n                    \"onExitCode\": \"1\",\n                    \"action\": \"retry\"\n                },\n                {\n                    \"onExitCode\": \"137\",\n                    \"action\": \"retry\"\n                },\n                {\n                    \"onExitCode\": \"139\",\n                    \"action\": \"retry\"\n                },\n                {\n                    \"onExitCode\": \"127\",\n                    \"action\": \"exit\"\n                }\n            ]\n        },\n        \"tags\": {\n            \"Purpose\": \"DistributedTraining\",\n            \"Framework\": \"PyTorch\"\n        },\n        \"propagateTags\": True\n    }\n\n    return job_def\n\ndef register_job_definition(args):\n    \"\"\"Register a new job definition or a new version of an existing one.\"\"\"\n    # Use the stem-root profile for AWS operations\n    session = boto3.Session(profile_name='stem-root', region_name='us-east-1')\n    batch = session.client('batch')\n\n    # Check if job definition already exists\n    try:\n        response = batch.describe_job_definitions(\n            jobDefinitionName=args.job_definition_name,\n            status='ACTIVE'\n        )\n        job_definitions = response.get('jobDefinitions', [])\n        if job_definitions:\n            print(f\"Job definition '{args.job_definition_name}' already exists. Registering a new version.\")\n    except Exception as e:\n        print(f\"Error checking existing job definition: {e}\")\n        print(\"Proceeding with registration...\")\n\n    # Create the job definition\n    job_def = create_job_definition(args)\n\n    # Register the job definition\n    try:\n        response = batch.register_job_definition(**job_def)\n        print(f\"Successfully registered job definition:\")\n        pprint(response)\n        print(f\"\\nARN: {response.get('jobDefinitionArn')}\")\n        print(f\"Revision: {response.get('revision')}\")\n    except Exception as e:\n        print(f\"Error registering job definition: {e}\")\n        print(\"Job definition that failed validation:\")\n        pprint(job_def)\n        raise\n\ndef main():\n    # Use stem-root profile instead of unsetting AWS_PROFILE\n    if 'AWS_PROFILE' in os.environ and os.environ['AWS_PROFILE'] != 'stem-root':\n        print(f\"Setting AWS_PROFILE environment variable to stem-root (was: {os.environ['AWS_PROFILE']})\")\n        os.environ['AWS_PROFILE'] = 'stem-root'\n    elif 'AWS_PROFILE' not in os.environ:\n        print(\"Setting AWS_PROFILE environment variable to stem-root\")\n        os.environ['AWS_PROFILE'] = 'stem-root'\n\n    parser = argparse.ArgumentParser(description='Register a multi-node AWS Batch job definition')\n    parser.add_argument('--job-definition-name', default='metta-batch-dist-train',\n                        help='Name of the job definition')\n    parser.add_argument('--image', default='767406518141.dkr.ecr.us-east-1.amazonaws.com/metta:latest',\n                        help='Docker image to use')\n\n    # Role ARN options\n    role_group = parser.add_argument_group('IAM Role Options')\n    role_group.add_argument('--job-role-arn', default=None,\n                        help='Job role ARN (if not provided, will look up by name)')\n    role_group.add_argument('--job-role-name', default='ecsTaskExecutionRole',\n                        help='Job role name to look up ARN')\n    role_group.add_argument('--execution-role-arn', default=None,\n                        help='Execution role ARN (if not provided, will look up by name)')\n    role_group.add_argument('--execution-role-name', default='ecsTaskExecutionRole',\n                        help='Execution role name to look up ARN')\n\n    # EFS options\n    efs_group = parser.add_argument_group('EFS Options')\n    efs_group.add_argument('--efs-id', default=None,\n                        help='EFS file system ID (if not provided, will look up)')\n    efs_group.add_argument('--efs-name', default=None,\n                        help='EFS name tag to look up file system ID')\n\n    parser.add_argument('--output-json', action='store_true',\n                        help='Output the job definition as JSON')\n\n    args = parser.parse_args()\n\n    # Hardcoded minimal resource values\n    args.num_nodes = 1\n    args.vcpus = 1\n    args.memory = 1024  # 1GB\n    args.gpus = 1\n    args.shared_memory = 230000\n\n    # Print resource configuration\n    print(\"Using resource configuration:\")\n    print(f\"  Job Definition: {args.job_definition_name}\")\n    print(f\"  Nodes: {args.num_nodes}\")\n    print(f\"  vCPUs: {args.vcpus}\")\n    print(f\"  Memory: {args.memory}MB\")\n    print(f\"  GPUs: {args.gpus}\")\n    print(f\"  Shared Memory: {args.shared_memory}MB\")\n\n    if args.output_json:\n        # Create job definition but don't register it\n        job_def = create_job_definition(args)\n        print(json.dumps(job_def, indent=2))\n    else:\n        register_job_definition(args)\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "agent/lib/metta_layer.py", "content": "from omegaconf import OmegaConf\nfrom tensordict import TensorDict\nfrom torch import nn\nimport torch\nimport numpy as np\n\nclass LayerBase(nn.Module):\n    '''The base class for components that make up the Metta agent. All components\n    are required to have a name and an input source, although the input source\n    can be None (or null in your YAML). Output size is optional depending on\n    your component.\n\n    All components must have a method called `setup` and it must accept\n    input_source_components. The setup assigns the output_size if it is not\n    already set. Once it has been called on all components, components can\n    initialize their parameters via `_initialize()`, if necessary. All\n    components must also have a property called `ready` that returns True if\n    the component has been setup.\n\n    The `_forward` method should only take a tensordict as input and return\n    only the tensordict. The tensor dict is constructed anew each time the\n    metta_agent forward pass is run. The component's `_forward` should read\n    from the value at the key name of its input_source. After performing its\n    computation via self.layer or otherwise, it should store its output in the\n    tensor dict at the key with its own name.\n\n    Before doing this, it should first check if the tensordict already has a\n    key with its own name, indicating that its computation has already been\n    performed (due to some other run up the DAG) and return. After this check,\n    it should check if its input source is not None and recursively call the\n    forward method of the layer above it.'''\n    def __init__(self, name, input_source=None, output_size=None, nn_params={}, **cfg):\n        super().__init__()\n        self._name = name\n        self._input_source = input_source\n        self._output_size = output_size\n        self._nn_params = nn_params\n        self._net = None\n        self._ready = False\n\n    @property\n    def ready(self):\n        return self._ready\n\n    def setup(self, input_source_component=None):\n        if self._ready:\n            return\n\n        self.__dict__['_input_source_component'] = input_source_component\n\n        if self._input_source_component is None:\n            self._input_size = None\n            if self._output_size is None:\n                raise ValueError(f\"Either input source or output size must be set for layer {self._name}\")\n        else:\n            self._input_size = self._input_source_component._output_size\n\n        if self._output_size is None:\n            self._output_size = self._input_size\n\n        self._initialize()\n        self._ready = True\n\n    def _initialize(self):\n        self._net = self._make_net()\n\n    def _make_net(self):\n        pass\n\n    def forward(self, td: TensorDict):\n        if self._name in td:\n            return td\n\n        if self._input_source_component is not None:\n            self._input_source_component.forward(td)\n\n        self._forward(td)\n\n        return td\n\n    def _forward(self, td: TensorDict):\n        td[self._name] = self._net(td[self._input_source])\n        return td\n\n    def clip_weights(self):\n        pass\n    def l2_reg_loss(self):\n        pass\n    def l2_init_loss(self):\n        pass\n    def update_l2_init_weight_copy(self):\n        pass\n    def effective_rank(self, delta: float = 0.01) -> dict:\n        pass\n\nclass ParamLayer(LayerBase):\n    '''This provides a few useful methods for components/nets that have parameters (weights).\n    Superclasses should have input_size and output_size already set.'''\n    def __init__(self, clip_scale=1, effective_rank=None, l2_norm_scale=None, l2_init_scale=None, nonlinearity='nn.ReLU', initialization='Orthogonal', clip_range=None, **cfg):\n        self.clip_scale = clip_scale\n        self.effective_rank_bool = effective_rank\n        self.l2_norm_scale = l2_norm_scale\n        self.l2_init_scale = l2_init_scale\n        self.nonlinearity = nonlinearity\n        self.initialization = initialization\n        self.global_clip_range = clip_range\n        super().__init__(**cfg)\n\n    def _initialize(self):\n        self.__dict__[\"weight_net\"] = self._make_net()\n\n        self._initialize_weights()\n\n        if self.clip_scale > 0:\n            self.clip_value = self.global_clip_range * self.largest_weight * self.clip_scale\n        else:\n            self.clip_value = 0 # disables clipping (not clip to 0)\n\n        self.initial_weights = None\n        if self.l2_init_scale != 0:\n            self.initial_weights = self.weight_net.weight.data.clone()\n\n        self._net = self.weight_net\n        if self.nonlinearity is not None:\n            # expecting a string of the form 'nn.ReLU'\n            try:\n                _, class_name = self.nonlinearity.split('.')\n                if class_name not in dir(nn):\n                    raise ValueError(f\"Unsupported nonlinearity: {self.nonlinearity}\")\n                nonlinearity_class = getattr(nn, class_name)\n                self._net = nn.Sequential(self.weight_net, nonlinearity_class())\n                self.__dict__[\"weight_net\"] = self._net[0]\n            except (AttributeError, KeyError, ValueError) as e:\n                raise ValueError(f\"Unsupported nonlinearity: {self.nonlinearity}\") from e\n\n    def _initialize_weights(self):\n        fan_in = self._input_size\n        fan_out = self._output_size\n\n        if self.initialization.lower() == 'orthogonal':\n            if self.nonlinearity == 'nn.Tanh':\n                gain = np.sqrt(2)\n            else:\n                gain = 1\n            nn.init.orthogonal_(self.weight_net.weight, gain=gain)\n            largest_weight = self.weight_net.weight.max().item()\n        elif self.initialization.lower() == 'xavier':\n            largest_weight = np.sqrt(6 / (fan_in + fan_out))\n            nn.init.xavier_uniform_(self.weight_net.weight)\n        elif self.initialization.lower() == 'normal':\n            largest_weight = np.sqrt(2 / fan_in)\n            nn.init.normal_(self.weight_net.weight, mean=0, std=largest_weight)\n        elif self.initialization.lower() == 'max_0_01':\n            #set to uniform with largest weight = 0.01\n            largest_weight = 0.01\n            nn.init.uniform_(self.weight_net.weight, a=-largest_weight, b=largest_weight)\n        else:\n            raise ValueError(f\"Invalid initialization method: {self.initialization}\")\n\n        if hasattr(self.weight_net, \"bias\") and isinstance(self.weight_net.bias, torch.nn.parameter.Parameter):\n            self.weight_net.bias.data.fill_(0)\n\n        self.largest_weight = largest_weight\n\n    def clip_weights(self):\n        if self.clip_value > 0:\n            with torch.no_grad():\n                self.weight_net.weight.data = self.weight_net.weight.data.clamp(-self.clip_value, self.clip_value)\n\n    def l2_reg_loss(self) -> torch.Tensor:\n        '''Also known as Weight Decay Loss or L2 Ridge Regularization'''\n        l2_reg_loss = torch.tensor(0.0, device=self.weight_net.weight.data.device)\n        if self.l2_norm_scale != 0 and self.l2_norm_scale is not None:\n            l2_reg_loss = (torch.sum(self.weight_net.weight.data ** 2))*self.l2_norm_scale\n        return l2_reg_loss\n\n    def l2_init_loss(self) -> torch.Tensor:\n        '''Also known as Delta Regularization Loss'''\n        l2_init_loss = torch.tensor(0.0, device=self.weight_net.weight.data.device)\n        if self.l2_init_scale != 0 and self.l2_init_scale is not None:\n            l2_init_loss = torch.sum((self.weight_net.weight.data - self.initial_weights) ** 2) * self.l2_init_scale\n        return l2_init_loss\n\n    def update_l2_init_weight_copy(self, alpha: float = 0.9):\n        '''Potentially useful to prevent catastrophic forgetting. Update the\n        initial weights copy with a weighted average of the previous and\n        current weights.'''\n        if self.initial_weights is not None:\n            self.initial_weights = (self.initial_weights * alpha + self.weight_net.weight.data * (1 - alpha)).clone()\n\n    def compute_effective_rank(self, delta: float = 0.01) -> dict:\n        '''Computes the effective rank of a matrix based on the given delta value.\n        Effective rank formula:\n        srank_\\delta(\\Phi) = min{k: sum_{i=1}^k _i / sum_{j=1}^d _j  1 - }\n        See the paper titled 'Implicit Under-Parameterization Inhibits Data-Efficient\n        Deep Reinforcement Learning' by A. Kumar et al.'''\n        if self.weight_net.weight.data.dim() != 2 or self.effective_rank_bool is None or self.effective_rank_bool == False:\n            return None\n        # Singular value decomposition. We only need the singular value matrix.\n        _, S, _ = torch.linalg.svd(self.weight_net.weight.data.detach())\n\n        # Calculate the cumulative sum of singular values\n        total_sum = S.sum()\n        cumulative_sum = torch.cumsum(S, dim=0)\n\n        # Find the smallest k that satisfies the effective rank condition\n        threshold = (1 - delta) * total_sum\n        effective_rank = torch.where(cumulative_sum >= threshold)[0][0].item() + 1  # Add 1 for 1-based indexing\n\n        return {'name': self._name, 'effective_rank': effective_rank}\n"}
{"type": "source_file", "path": "devops/aws/batch/job_queue.py", "content": "#!/usr/bin/env python3\n\"\"\"\nAWS Batch Job Queue Utilities\n\nThis module provides functions for interacting with AWS Batch job queues.\n\"\"\"\n\nimport boto3\nfrom botocore.config import Config\nfrom tabulate import tabulate\nfrom datetime import datetime\nfrom .job import format_time_difference\n\ndef get_boto3_client(service_name='batch'):\n    \"\"\"Get a boto3 client with standard configuration.\"\"\"\n    config = Config(retries={'max_attempts': 10, 'mode': 'standard'}, max_pool_connections=50)\n    return boto3.client(service_name, config=config)\n\ndef list_job_queues():\n    \"\"\"List all available AWS Batch job queues.\"\"\"\n    batch = get_boto3_client()\n\n    try:\n        response = batch.describe_job_queues()\n        queues = response['jobQueues']\n\n        # Format the output\n        table_data = []\n        for queue in queues:\n            name = queue['jobQueueName']\n            state = queue['state']\n            status = queue['status']\n            priority = queue['priority']\n\n            # Get compute environment names\n            compute_envs = [ce['computeEnvironment'].split('/')[-1] for ce in queue['computeEnvironmentOrder']]\n            compute_env_str = ', '.join(compute_envs)\n\n            table_data.append([name, state, status, priority, compute_env_str])\n\n        # Print the table\n        headers = ['Queue Name', 'State', 'Status', 'Priority', 'Compute Environments']\n        print(tabulate(table_data, headers=headers, tablefmt='grid'))\n\n        return [queue['jobQueueName'] for queue in queues]\n    except Exception as e:\n        print(f\"Error retrieving job queues: {str(e)}\")\n        return []\n\ndef get_job_queue_info(queue_name, max_jobs=5):\n    \"\"\"Get detailed information about a specific job queue.\"\"\"\n    batch = get_boto3_client()\n\n    try:\n        response = batch.describe_job_queues(jobQueues=[queue_name])\n\n        if not response['jobQueues']:\n            print(f\"Job queue '{queue_name}' not found\")\n            return None\n\n        queue = response['jobQueues'][0]\n\n        # Print basic information\n        print(f\"\\nJob Queue: {queue['jobQueueName']}\")\n        print(f\"ARN: {queue['jobQueueArn']}\")\n        print(f\"State: {queue['state']}\")\n        print(f\"Status: {queue['status']}\")\n        print(f\"Status Reason: {queue.get('statusReason', 'N/A')}\")\n        print(f\"Priority: {queue['priority']}\")\n\n        # Print compute environments\n        print(\"\\nCompute Environments:\")\n        for ce in queue['computeEnvironmentOrder']:\n            print(f\"  - {ce['computeEnvironment'].split('/')[-1]} (Order: {ce['order']})\")\n\n        # Get job statistics\n        print(\"\\nJob Statistics:\")\n        try:\n            # Get jobs by status\n            for status in ['SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING', 'RUNNING', 'SUCCEEDED', 'FAILED']:\n                try:\n                    response = batch.list_jobs(\n                        jobQueue=queue_name,\n                        jobStatus=status,\n                        maxResults=max_jobs\n                    )\n                    job_summaries = response.get('jobSummaryList', [])\n                    job_count = len(job_summaries)\n\n                    # If there are more jobs, get the total count\n                    if 'nextToken' in response:\n                        job_count = f\"{job_count}+ (more available)\"\n\n                    print(f\"  - {status}: {job_count}\")\n\n                    # If there are jobs, show details for the most recent ones\n                    if job_summaries:\n                        # Sort by creation time (newest first)\n                        job_summaries.sort(key=lambda x: x.get('createdAt', 0), reverse=True)\n\n                        # Limit to max_jobs\n                        job_summaries = job_summaries[:max_jobs]\n\n                        # Get job details\n                        job_ids = [job['jobId'] for job in job_summaries]\n                        job_details = batch.describe_jobs(jobs=job_ids)['jobs']\n\n                        # Create a lookup table for job details\n                        job_details_map = {job['jobId']: job for job in job_details}\n\n                        # Format the output\n                        table_data = []\n                        for summary in job_summaries:\n                            job_id = summary['jobId']\n                            job_name = summary['jobName']\n\n                            # Get additional details if available\n                            details = job_details_map.get(job_id, {})\n                            created_at = details.get('createdAt', summary.get('createdAt', 0))\n                            started_at = details.get('startedAt', 0)\n                            stopped_at = details.get('stoppedAt', 0)\n\n                            # Format timestamps\n                            created_str = datetime.fromtimestamp(created_at / 1000).strftime('%Y-%m-%d %H:%M:%S') if created_at else 'N/A'\n\n                            # Calculate duration\n                            if started_at and stopped_at:\n                                duration = format_time_difference(started_at, stopped_at)\n                            elif started_at:\n                                duration = format_time_difference(started_at)\n                            else:\n                                duration = 'N/A'\n\n                            # Get job definition\n                            job_definition = details.get('jobDefinition', '').split('/')[-1].split(':')[0]\n\n                            # Get number of attempts\n                            attempts = len(details.get('attempts', []))\n\n                            table_data.append([job_id, job_name, created_str, duration, job_definition, attempts])\n\n                        # Print the table\n                        if table_data:\n                            print(f\"\\n    Recent {status} Jobs:\")\n                            headers = ['Job ID', 'Name', 'Created', 'Duration', 'Job Definition', 'Attempts']\n                            print(tabulate(table_data, headers=headers, tablefmt='grid', maxcolwidths=[20, 30, 20, 10, 20, 8]))\n                except Exception as e:\n                    print(f\"  Error retrieving {status} jobs: {str(e)}\")\n        except Exception as e:\n            print(f\"  Error retrieving job statistics: {str(e)}\")\n\n        return queue\n    except Exception as e:\n        print(f\"Error retrieving job queue information: {str(e)}\")\n        return None\n"}
{"type": "source_file", "path": "agent/lib/feature_normalizer.py", "content": "from sample_factory.algo.utils.running_mean_std import RunningMeanStdInPlace\nfrom torch import nn\nimport torch\nfrom tensordict import TensorDict\nimport omegaconf\n\nfrom agent.lib.metta_layer import LayerBase\n\n# this is not currently working\nclass FeatureListNormalizer(LayerBase):\n    def __init__(self, metta_agent, **cfg):\n        super().__init__()\n        cfg = omegaconf.OmegaConf.create(cfg)\n        object.__setattr__(self, 'metta_agent', metta_agent)\n        self.cfg = cfg\n        self.metta_agent_components = self.metta_agent.components\n        self.name = self.cfg.name\n        self.input_source = self.cfg.input_source\n        self.output_size = None\n        self._feature_names = self.metta_agent.grid_features\n        self.input_shape = self.metta_agent.obs_input_shape\n        self._norms_dict = nn.ModuleDict({\n            **{\n                k: RunningMeanStdInPlace(self.input_shape)\n                for k in self._feature_names\n            },\n        })\n        self._normalizers = [self._norms_dict[k] for k in self._feature_names]\n\n    def forward(self, td: TensorDict):\n        if self.name in td:\n            return td[self.name]\n\n        self.metta_agent_components[self.input_source].forward(td)\n\n        with torch.no_grad():\n            normalized_values = []\n            for fidx, norm in enumerate(self._normalizers):\n                normalized_values.append(norm(td[self.input_source][:, fidx, :, :]))\n            td[self.name] = torch.stack(normalized_values, dim=1)\n\n        return td"}
{"type": "source_file", "path": "agent/metta_agent.py", "content": "import os\nfrom typing import List, Tuple, Union\n\nimport gymnasium as gym\nimport hydra\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nfrom torch.distributions.utils import logits_to_probs\n\nfrom omegaconf import OmegaConf\nfrom pufferlib.environment import PufferEnv\nfrom sample_factory.utils.typing import ActionSpace, ObsSpace\nfrom tensordict import TensorDict\nfrom torch import nn\nfrom torch.nn.parallel import DistributedDataParallel\n\nfrom agent.util.distribution_utils import sample_logits\n\nimport logging\nlogger = logging.getLogger(\"metta_agent\")\n\ndef make_policy(env: PufferEnv, cfg: OmegaConf):\n    obs_space = gym.spaces.Dict({\n        \"grid_obs\": env.single_observation_space,\n        \"global_vars\": gym.spaces.Box(\n            low=-np.inf, high=np.inf,\n            shape=[ 0 ],\n            dtype=np.int32)\n    })\n    return hydra.utils.instantiate(\n        cfg.agent,\n        obs_shape=env.single_observation_space.shape,\n        obs_space=obs_space,\n        action_space=env.single_action_space,\n        grid_features=env.grid_features,\n        global_features=env.global_features,\n        device=cfg.device,\n        _recursive_=False)\n\nclass DistributedMettaAgent(DistributedDataParallel):\n    def __init__(self, agent, device):\n        super().__init__(agent, device_ids=[device], output_device=device)\n\n    def __getattr__(self, name):\n        try:\n            return super().__getattr__(name)\n        except AttributeError:\n            return getattr(self.module, name)\n\nclass MettaAgent(nn.Module):\n    def __init__(\n        self,\n        obs_shape: Tuple[int, ...],\n        obs_space: ObsSpace,\n        action_space: ActionSpace,\n        grid_features: List[str],\n        device: str,\n        **cfg\n    ):\n        super().__init__()\n        cfg = OmegaConf.create(cfg)\n\n        self.hidden_size = cfg.components._core_.output_size\n        self.core_num_layers = cfg.components._core_.nn_params.num_layers\n        self.clip_range = cfg.clip_range\n\n        agent_attributes = {\n            'obs_shape': obs_shape,\n            'clip_range': self.clip_range,\n            'action_space': action_space,\n            'grid_features': grid_features,\n            'obs_key': cfg.observations.obs_key,\n            'obs_input_shape': obs_space[cfg.observations.obs_key].shape[1:],\n            'num_objects': obs_space[cfg.observations.obs_key].shape[2], # this is hardcoded for channel # at end of tuple\n            'hidden_size': self.hidden_size,\n            'core_num_layers': self.core_num_layers\n        }\n\n        agent_attributes['action_type_size'] = action_space.nvec[0]\n        agent_attributes['action_param_size'] = action_space.nvec[1]\n\n        # self.observation_space = obs_space # for use with FeatureSetEncoder\n        # self.global_features = global_features # for use with FeatureSetEncoder\n\n        self.components = nn.ModuleDict()\n        component_cfgs = OmegaConf.to_container(cfg.components, resolve=True)\n        for component_cfg in component_cfgs.keys():\n            component_cfgs[component_cfg]['name'] = component_cfg\n            component = hydra.utils.instantiate(component_cfgs[component_cfg], **agent_attributes)\n            self.components[component_cfg] = component\n\n        component = self.components['_value_']\n        self._setup_components(component)\n        component = self.components['_action_type_']\n        self._setup_components(component)\n        component = self.components['_action_param_']\n        self._setup_components(component)\n\n        for name, component in self.components.items():\n            if not getattr(component, 'ready', False):\n                raise RuntimeError(f\"Component {name} in MettaAgent was never setup. It might not be accessible by other components.\")\n\n        self.components = self.components.to(device)\n\n        self._total_params = sum(p.numel() for p in self.parameters())\n        print(f\"Total number of parameters in MettaAgent: {self._total_params:,}. Setup complete.\")\n\n    def _setup_components(self, component):\n        if component._input_source is not None:\n            if isinstance(component._input_source, str):\n                self._setup_components(self.components[component._input_source])\n            elif isinstance(component._input_source, list):\n                for input_source in component._input_source:\n                    self._setup_components(self.components[input_source])\n\n        if component._input_source is not None:\n            if isinstance(component._input_source, str):\n                component.setup(self.components[component._input_source])\n            elif isinstance(component._input_source, list):\n                input_source_components = {}\n                for input_source in component._input_source:\n                    input_source_components[input_source] = self.components[input_source]\n                component.setup(input_source_components)\n        else:\n            component.setup()\n\n    @property\n    def lstm(self):\n        return self.components[\"_core_\"]._net\n\n    @property\n    def total_params(self):\n        return self._total_params\n\n    def get_value(self, x, state=None):\n        td = TensorDict({\"x\": x, \"state\": state})\n        self.components[\"_value_\"](td)\n        return None, td[\"_value_\"], None\n\n    def get_action_and_value(self, x, state=None, action=None, e3b=None):\n        td = TensorDict({\"x\": x})\n\n        td[\"state\"] = None\n        if state is not None:\n            state = torch.cat(state, dim=0)\n            td[\"state\"] = state.to(x.device)\n\n        self.components[\"_value_\"](td)\n        self.components[\"_action_type_\"](td)\n        self.components[\"_action_param_\"](td)\n\n        logits = [td[\"_action_type_\"], td[\"_action_param_\"]]\n        value = td[\"_value_\"]\n        state = td[\"state\"]\n\n        # Convert state back to tuple to pass back to trainer\n        if state is not None:\n            split_size = self.core_num_layers\n            state = (state[:split_size], state[split_size:])\n\n        e3b, intrinsic_reward = self._e3b_update(td[\"_core_\"].detach(), e3b)\n        action, logprob, entropy, normalized_logits = sample_logits(logits, action)\n\n        return action, logprob, entropy, value, state, e3b, intrinsic_reward, normalized_logits\n\n    def forward(self, x, state=None, action=None, e3b=None):\n        return self.get_action_and_value(x, state, action, e3b)\n\n    def _e3b_update(self, phi, e3b):\n        intrinsic_reward = None\n        if e3b is not None:\n            u = phi.unsqueeze(1) @ e3b\n            intrinsic_reward = u @ phi.unsqueeze(2)\n            e3b = 0.99*e3b - (u.mT @ u) / (1 + intrinsic_reward)\n            intrinsic_reward = intrinsic_reward.squeeze()\n        return e3b, intrinsic_reward\n\n    def l2_reg_loss(self) -> torch.Tensor:\n        '''L2 regularization loss is on by default although setting l2_norm_coeff to 0 effectively turns it off. Adjust it by setting l2_norm_scale in your component config to a multiple of the global loss value or 0 to turn it off.'''\n        l2_reg_loss = 0\n        for component in self.components.values():\n            l2_reg_loss += component.l2_reg_loss() or 0\n        return torch.tensor(l2_reg_loss)\n\n    def l2_init_loss(self) -> torch.Tensor:\n        '''L2 initialization loss is on by default although setting l2_init_coeff to 0 effectively turns it off. Adjust it by setting l2_init_scale in your component config to a multiple of the global loss value or 0 to turn it off.'''\n        l2_init_loss = 0\n        for component in self.components.values():\n            l2_init_loss += component.l2_init_loss() or 0\n        return torch.tensor(l2_init_loss)\n\n    def update_l2_init_weight_copy(self):\n        '''Update interval set by l2_init_weight_update_interval. 0 means no updating.'''\n        for component in self.components.values():\n            component.update_l2_init_weight_copy()\n\n    def clip_weights(self):\n        '''Weight clipping is on by default although setting clip_range or clip_scale to 0, or a large positive value effectively turns it off. Adjust it by setting clip_scale in your component config to a multiple of the global loss value or 0 to turn it off.'''\n        if self.clip_range > 0:\n            for component in self.components.values():\n                component.clip_weights()\n\n    def compute_effective_rank(self, delta: float = 0.01) -> List[dict]:\n        '''Effective rank computation is off by default. Set effective_rank to True in the config to turn it on for a given component.'''\n        effective_ranks = []\n        for component in self.components.values():\n            rank = component.effective_rank(delta)\n            if rank is not None:\n                effective_ranks.append(rank)\n        return effective_ranks\n"}
{"type": "source_file", "path": "devops/aws/batch/job.py", "content": "#!/usr/bin/env python3\n\"\"\"\nAWS Batch Job Utilities\n\nThis module provides functions for interacting with AWS Batch jobs.\n\"\"\"\n\nimport boto3\nfrom botocore.config import Config\nfrom datetime import datetime\nimport time\nfrom tabulate import tabulate\nimport argparse\nimport sys\nimport importlib.util\nimport os\nimport subprocess\nfrom colorama import Fore, Style, init\n\n# Initialize colorama\ninit(autoreset=True)\n\ndef get_boto3_client(service_name='batch'):\n    \"\"\"Get a boto3 client with standard configuration.\"\"\"\n    config = Config(retries={'max_attempts': 10, 'mode': 'standard'}, max_pool_connections=50)\n    return boto3.client(service_name, config=config)\n\ndef format_time_difference(timestamp, end_timestamp=None):\n    \"\"\"Format the time difference between a timestamp and now (or another timestamp).\"\"\"\n    if not timestamp:\n        return \"N/A\"\n\n    # Convert milliseconds to seconds if necessary\n    if timestamp > 1000000000000:  # If timestamp is in milliseconds\n        timestamp = timestamp / 1000\n\n    # If end_timestamp is provided, use it; otherwise use current time\n    if end_timestamp:\n        # Convert milliseconds to seconds if necessary\n        if end_timestamp > 1000000000000:  # If timestamp is in milliseconds\n            end_timestamp = end_timestamp / 1000\n\n        diff_seconds = end_timestamp - timestamp\n    else:\n        # Calculate difference from now\n        now = time.time()\n        diff_seconds = now - timestamp\n\n    # Format the difference\n    if diff_seconds < 0:\n        return \"Future\"\n\n    if diff_seconds < 60:\n        return f\"{int(diff_seconds)}s\"\n\n    if diff_seconds < 3600:\n        minutes = int(diff_seconds / 60)\n        seconds = int(diff_seconds % 60)\n        return f\"{minutes}m {seconds}s\"\n\n    if diff_seconds < 86400:\n        hours = int(diff_seconds / 3600)\n        minutes = int((diff_seconds % 3600) / 60)\n        return f\"{hours}h {minutes}m\"\n\n    days = int(diff_seconds / 86400)\n    hours = int((diff_seconds % 86400) / 3600)\n    return f\"{days}d {hours}h\"\n\ndef format_time_ago(timestamp):\n    \"\"\"Format a timestamp as a human-readable 'time ago' string.\"\"\"\n    if not timestamp:\n        return \"N/A\"\n\n    # Convert milliseconds to seconds if necessary\n    if timestamp > 1000000000000:  # If timestamp is in milliseconds\n        timestamp = timestamp / 1000\n\n    # Calculate difference from now\n    now = time.time()\n    diff_seconds = now - timestamp\n\n    # Format the difference\n    if diff_seconds < 0:\n        return \"Future\"\n\n    if diff_seconds < 60:\n        return f\"({int(diff_seconds)}s ago)\"\n\n    if diff_seconds < 3600:\n        minutes = int(diff_seconds / 60)\n        return f\"({minutes}m ago)\"\n\n    if diff_seconds < 86400:\n        hours = int(diff_seconds / 3600)\n        return f\"({hours}h ago)\"\n\n    days = int(diff_seconds / 86400)\n    return f\"({days}d ago)\"\n\ndef list_jobs(job_queue=None, max_jobs=100, no_color=False):\n    \"\"\"List jobs in a job queue.\"\"\"\n    # If no_color is True, disable colorama\n    if no_color:\n        global Fore, Style\n        # Save the original values\n        orig_fore, orig_style = Fore, Style\n        # Create dummy objects that return empty strings\n        class DummyFore:\n            def __getattr__(self, _):\n                return \"\"\n        class DummyStyle:\n            def __getattr__(self, _):\n                return \"\"\n        Fore, Style = DummyFore(), DummyStyle()\n\n    batch = get_boto3_client()\n\n    if not job_queue:\n        # Get all job queues\n        try:\n            response = batch.describe_job_queues()\n            job_queues = [queue['jobQueueName'] for queue in response['jobQueues']]\n\n            if not job_queues:\n                print(\"No job queues found\")\n                return []\n\n            print(f\"Available job queues: {', '.join(job_queues)}\")\n            print(\"Please specify a job queue with --queue\")\n            print(\"Or use 'jobs' command to list jobs in the default queue (metta-jq)\")\n            return []\n        except Exception as e:\n            print(f\"Error retrieving job queues: {str(e)}\")\n            return []\n\n    all_jobs = []\n\n    try:\n        # Get jobs from the specified queue for each status\n        for status in ['SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING', 'RUNNING', 'SUCCEEDED', 'FAILED']:\n            try:\n                response = batch.list_jobs(\n                    jobQueue=job_queue,\n                    jobStatus=status,\n                    maxResults=min(100, max_jobs)  # AWS API limit is 100\n                )\n\n                job_summaries = response.get('jobSummaryList', [])\n\n                # Process in batches of 100 to avoid API limits\n                if job_summaries:\n                    job_ids = [job['jobId'] for job in job_summaries]\n\n                    # Only call describe_jobs if we have job IDs to process\n                    if job_ids:\n                        try:\n                            job_details = batch.describe_jobs(jobs=job_ids)['jobs']\n                            all_jobs.extend(job_details)\n                        except Exception as e:\n                            print(f\"Error retrieving job details: {str(e)}\")\n            except Exception as e:\n                if \"ArrayJob, Multi-node Job and job status are not applicable\" not in str(e):\n                    print(f\"Error retrieving jobs from queue {job_queue} with status {status}: {str(e)}\")\n                continue\n    except Exception as e:\n        print(f\"Error retrieving jobs: {str(e)}\")\n\n    # Sort jobs by creation time (newest first)\n    all_jobs.sort(key=lambda x: x.get('createdAt', 0), reverse=True)\n\n    # Limit to max_jobs\n    all_jobs = all_jobs[:max_jobs]\n\n    # Format the output\n    table_data = []\n    for job in all_jobs:\n        job_name = job['jobName']\n        job_status = job['status']\n\n        # Get timestamps\n        created_at = job.get('createdAt', 0)\n        started_at = job.get('startedAt', 0)\n        stopped_at = job.get('stoppedAt', 0)\n\n        # Format timestamps\n        created_str = datetime.fromtimestamp(created_at / 1000).strftime('%Y-%m-%d %H:%M:%S') if created_at else 'N/A'\n        created_ago = format_time_ago(created_at) if created_at else ''\n        created_display = f\"{created_str} {created_ago}\"\n\n        # Calculate duration\n        if started_at and stopped_at:\n            duration = format_time_difference(started_at, stopped_at)\n        elif started_at:\n            duration = format_time_difference(started_at)\n        else:\n            duration = 'N/A'\n\n        # Get number of attempts\n        attempts = len(job.get('attempts', []))\n\n        # Get number of nodes\n        num_nodes = 1  # Default for single-node jobs\n        if 'nodeProperties' in job:\n            num_nodes = job['nodeProperties'].get('numNodes', 1)\n\n        # Calculate total GPUs\n        num_gpus = 0\n        container = job.get('container', {})\n        if container:\n            # Check if it's a GPU job\n            resource_requirements = container.get('resourceRequirements', [])\n            for resource in resource_requirements:\n                if resource.get('type') == 'GPU':\n                    num_gpus = int(resource.get('value', 0))\n                    # For single-node jobs, multiply by number of nodes\n                    if 'nodeProperties' not in job:\n                        break\n\n        # For multi-node jobs, calculate total GPUs across all nodes\n        if 'nodeProperties' in job:\n            # Reset GPU count for multi-node jobs to avoid double counting\n            num_gpus = 0\n            node_ranges = job['nodeProperties'].get('nodeRangeProperties', [])\n            total_nodes = job['nodeProperties'].get('numNodes', 1)\n\n            # If no node ranges specified but we have numNodes, use the main container's GPU count\n            if not node_ranges and container:\n                resource_requirements = container.get('resourceRequirements', [])\n                for resource in resource_requirements:\n                    if resource.get('type') == 'GPU':\n                        num_gpus = int(resource.get('value', 0)) * total_nodes\n                        break\n\n            # Process each node range\n            for node_range in node_ranges:\n                node_container = node_range.get('container', {})\n                if node_container:\n                    # First check environment variables for NUM_GPUS\n                    env_vars = node_container.get('environment', [])\n                    gpus_per_node = 0\n                    for env in env_vars:\n                        if env.get('name') == 'NUM_GPUS':\n                            try:\n                                gpus_per_node = int(env.get('value', 0))\n                                break\n                            except (ValueError, TypeError):\n                                pass\n\n                    # If not found in environment, check resource requirements\n                    if gpus_per_node == 0:\n                        node_resources = node_container.get('resourceRequirements', [])\n                        for resource in node_resources:\n                            if resource.get('type') == 'GPU':\n                                gpus_per_node = int(resource.get('value', 0))\n                                break\n\n                    # Get the target nodes range (e.g., \"0:1\" for nodes 0 and 1)\n                    target_nodes = node_range.get('targetNodes', '')\n                    try:\n                        # Parse the range (e.g., \"0:1\" -> 2 nodes)\n                        if ':' in target_nodes:\n                            parts = target_nodes.split(':')\n                            if len(parts) == 2:\n                                start = int(parts[0])\n                                # If end is empty (e.g., \"0:\"), use total_nodes\n                                if parts[1] == '':\n                                    node_count = total_nodes - start\n                                else:\n                                    end = int(parts[1])\n                                    node_count = end - start + 1\n                            else:\n                                node_count = 1\n                        else:\n                            # Single node specified\n                            node_count = 1\n                        num_gpus += gpus_per_node * node_count\n                    except (ValueError, TypeError):\n                        # If we can't parse the range, assume 0 GPUs for this range\n                        pass\n\n        table_data.append([job_name, job_status, created_display, duration, attempts, num_nodes, num_gpus])\n\n    # Print the table\n    if table_data:\n        # Add color to job status and other fields\n        colored_table_data = []\n        for row in table_data:\n            job_name, status, created, duration, attempts, num_nodes, num_gpus = row\n\n            # Color for job status\n            if status == 'RUNNING':\n                colored_status = f\"{Fore.GREEN}{status}{Style.RESET_ALL}\"\n            elif status == 'SUCCEEDED':\n                colored_status = f\"{Fore.BLUE}{status}{Style.RESET_ALL}\"\n            elif status == 'FAILED':\n                colored_status = f\"{Fore.RED}{status}{Style.RESET_ALL}\"\n            elif status in ['SUBMITTED', 'PENDING', 'RUNNABLE']:\n                colored_status = f\"{Fore.YELLOW}{status}{Style.RESET_ALL}\"\n            elif status == 'STARTING':\n                colored_status = f\"{Fore.CYAN}{status}{Style.RESET_ALL}\"\n            else:\n                colored_status = status\n\n            # Color for job name\n            colored_job_name = f\"{Fore.MAGENTA}{job_name}{Style.RESET_ALL}\"\n\n            # Color for duration\n            if duration != 'N/A':\n                colored_duration = f\"{Fore.CYAN}{duration}{Style.RESET_ALL}\"\n            else:\n                colored_duration = duration\n\n            # Color for GPU count\n            if num_gpus > 0:\n                colored_num_gpus = f\"{Fore.YELLOW}{num_gpus}{Style.RESET_ALL}\"\n            else:\n                colored_num_gpus = num_gpus\n\n            colored_table_data.append([\n                colored_job_name,\n                colored_status,\n                created,\n                colored_duration,\n                attempts,\n                num_nodes,\n                colored_num_gpus\n            ])\n\n        print(f\"Jobs in queue '{Fore.CYAN}{job_queue}{Style.RESET_ALL}':\")\n        headers = ['Name', 'Status', 'Created', 'Duration', 'Attempts', 'NumNodes', 'Num GPUs']\n        print(tabulate(colored_table_data, headers=headers, tablefmt='grid'))\n    else:\n        print(f\"No jobs found in queue '{Fore.CYAN}{job_queue}{Style.RESET_ALL}'\")\n\n    # Restore colorama if it was disabled\n    if no_color:\n        Fore, Style = orig_fore, orig_style\n\n    return all_jobs\n\ndef get_job_info(job_id_or_name, no_color=False):\n    \"\"\"Get detailed information about a specific job by ID or name.\"\"\"\n    # If no_color is True, disable colorama\n    if no_color:\n        global Fore, Style\n        # Save the original values\n        orig_fore, orig_style = Fore, Style\n        # Create dummy objects that return empty strings\n        class DummyFore:\n            def __getattr__(self, _):\n                return \"\"\n        class DummyStyle:\n            def __getattr__(self, _):\n                return \"\"\n        Fore, Style = DummyFore(), DummyStyle()\n\n    batch = get_boto3_client()\n\n    try:\n        # First try to get the job by ID\n        response = batch.describe_jobs(jobs=[job_id_or_name])\n\n        # If no job found by ID, try to find by name\n        if not response['jobs']:\n            # We need to list jobs from all queues to find by name\n            job = None\n\n            # Get all job queues\n            try:\n                queues_response = batch.describe_job_queues()\n                job_queues = [queue['jobQueueName'] for queue in queues_response['jobQueues']]\n\n                # Search for the job in each queue\n                for queue in job_queues:\n                    # Check all job statuses\n                    for status in ['SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING', 'RUNNING', 'SUCCEEDED', 'FAILED']:\n                        try:\n                            jobs_response = batch.list_jobs(\n                                jobQueue=queue,\n                                jobStatus=status,\n                                maxResults=100\n                            )\n\n                            # Look for a job with the specified name\n                            for job_summary in jobs_response.get('jobSummaryList', []):\n                                if job_summary['jobName'] == job_id_or_name:\n                                    # Found a job with the specified name, get its details\n                                    job_details_response = batch.describe_jobs(jobs=[job_summary['jobId']])\n                                    if job_details_response['jobs']:\n                                        job = job_details_response['jobs'][0]\n                                        break\n\n                            if job:\n                                break\n                        except Exception:\n                            continue\n\n                    if job:\n                        break\n\n                if not job:\n                    print(f\"No job found with ID or name '{job_id_or_name}'\")\n                    return None\n            except Exception as e:\n                print(f\"Error retrieving job queues: {str(e)}\")\n                print(f\"Job '{job_id_or_name}' not found\")\n                return None\n        else:\n            job = response['jobs'][0]\n\n        # Print basic information\n        print(f\"\\nJob: {Fore.CYAN}{job['jobId']}{Style.RESET_ALL}\")\n        print(f\"Name: {Fore.MAGENTA}{job['jobName']}{Style.RESET_ALL}\")\n\n        # Color for job status\n        status = job['status']\n        if status == 'RUNNING':\n            status_color = f\"{Fore.GREEN}{status}{Style.RESET_ALL}\"\n        elif status == 'SUCCEEDED':\n            status_color = f\"{Fore.BLUE}{status}{Style.RESET_ALL}\"\n        elif status == 'FAILED':\n            status_color = f\"{Fore.RED}{status}{Style.RESET_ALL}\"\n        elif status in ['SUBMITTED', 'PENDING', 'RUNNABLE']:\n            status_color = f\"{Fore.YELLOW}{status}{Style.RESET_ALL}\"\n        elif status == 'STARTING':\n            status_color = f\"{Fore.CYAN}{status}{Style.RESET_ALL}\"\n        else:\n            status_color = status\n\n        print(f\"Status: {status_color}\")\n\n        status_reason = job.get('statusReason', 'N/A')\n        if status_reason != 'N/A':\n            print(f\"Status Reason: {Fore.RED}{status_reason}{Style.RESET_ALL}\")\n        else:\n            print(f\"Status Reason: {status_reason}\")\n\n        # Print timestamps\n        created_at = job.get('createdAt', 0)\n        started_at = job.get('startedAt', 0)\n        stopped_at = job.get('stoppedAt', 0)\n\n        if created_at:\n            created_str = datetime.fromtimestamp(created_at / 1000).strftime('%Y-%m-%d %H:%M:%S')\n            print(f\"Created: {Fore.YELLOW}{created_str}{Style.RESET_ALL}\")\n\n        if started_at:\n            started_str = datetime.fromtimestamp(started_at / 1000).strftime('%Y-%m-%d %H:%M:%S')\n            print(f\"Started: {Fore.GREEN}{started_str}{Style.RESET_ALL}\")\n\n        if stopped_at:\n            stopped_str = datetime.fromtimestamp(stopped_at / 1000).strftime('%Y-%m-%d %H:%M:%S')\n            if status == 'FAILED':\n                print(f\"Stopped: {Fore.RED}{stopped_str}{Style.RESET_ALL}\")\n            else:\n                print(f\"Stopped: {Fore.BLUE}{stopped_str}{Style.RESET_ALL}\")\n\n        # Calculate duration\n        if started_at and stopped_at:\n            duration = format_time_difference(started_at, stopped_at)\n            print(f\"Duration: {Fore.CYAN}{duration}{Style.RESET_ALL}\")\n        elif started_at:\n            duration = format_time_difference(started_at)\n            print(f\"Running for: {Fore.CYAN}{duration}{Style.RESET_ALL}\")\n\n        # Print job definition\n        job_definition = job.get('jobDefinition', '').split('/')[-1]\n        print(f\"Job Definition: {Fore.BLUE}{job_definition}{Style.RESET_ALL}\")\n\n        # Print job queue\n        job_queue = job.get('jobQueue', '').split('/')[-1]\n        print(f\"Job Queue: {Fore.BLUE}{job_queue}{Style.RESET_ALL}\")\n\n        # Print container details\n        if 'container' in job:\n            container = job['container']\n            print(f\"\\n{Fore.YELLOW}Container:{Style.RESET_ALL}\")\n            print(f\"  Image: {Fore.CYAN}{container.get('image', 'N/A')}{Style.RESET_ALL}\")\n            print(f\"  vCPUs: {Fore.GREEN}{container.get('vcpus', 'N/A')}{Style.RESET_ALL}\")\n            print(f\"  Memory: {Fore.GREEN}{container.get('memory', 'N/A')} MiB{Style.RESET_ALL}\")\n\n            # Print command\n            if 'command' in container:\n                command_str = ' '.join(container['command'])\n                print(f\"  Command: {Fore.MAGENTA}{command_str}{Style.RESET_ALL}\")\n\n            # Print environment variables\n            if 'environment' in container:\n                print(f\"\\n  {Fore.YELLOW}Environment Variables:{Style.RESET_ALL}\")\n                for env in container['environment']:\n                    print(f\"    {Fore.CYAN}{env['name']}{Style.RESET_ALL}: {env['value']}\")\n\n            # Print exit code\n            if 'exitCode' in container:\n                exit_code = container['exitCode']\n                if exit_code == 0:\n                    print(f\"  Exit Code: {Fore.GREEN}{exit_code}{Style.RESET_ALL}\")\n                else:\n                    print(f\"  Exit Code: {Fore.RED}{exit_code}{Style.RESET_ALL}\")\n\n            # Print reason\n            if 'reason' in container:\n                print(f\"  Reason: {Fore.RED}{container['reason']}{Style.RESET_ALL}\")\n\n        # Print attempts\n        attempts = job.get('attempts', [])\n        if attempts:\n            print(f\"\\n{Fore.YELLOW}Attempts:{Style.RESET_ALL}\")\n            for i, attempt in enumerate(attempts):\n                print(f\"  {Fore.CYAN}Attempt {i}:{Style.RESET_ALL}\")\n\n                # Color for attempt status\n                status = attempt.get('status', 'N/A')\n                if status == 'RUNNING':\n                    status_color = f\"{Fore.GREEN}{status}{Style.RESET_ALL}\"\n                elif status == 'SUCCEEDED':\n                    status_color = f\"{Fore.BLUE}{status}{Style.RESET_ALL}\"\n                elif status == 'FAILED':\n                    status_color = f\"{Fore.RED}{status}{Style.RESET_ALL}\"\n                elif status in ['SUBMITTED', 'PENDING', 'RUNNABLE']:\n                    status_color = f\"{Fore.YELLOW}{status}{Style.RESET_ALL}\"\n                elif status == 'STARTING':\n                    status_color = f\"{Fore.CYAN}{status}{Style.RESET_ALL}\"\n                else:\n                    status_color = status\n\n                print(f\"    Status: {status_color}\")\n\n                reason = attempt.get('statusReason', 'N/A')\n                if reason != 'N/A':\n                    print(f\"    Reason: {Fore.RED}{reason}{Style.RESET_ALL}\")\n                else:\n                    print(f\"    Reason: {reason}\")\n\n                # Print container details\n                container = attempt.get('container', {})\n                if container:\n                    if 'exitCode' in container:\n                        exit_code = container['exitCode']\n                        if exit_code == 0:\n                            print(f\"    Exit Code: {Fore.GREEN}{exit_code}{Style.RESET_ALL}\")\n                        else:\n                            print(f\"    Exit Code: {Fore.RED}{exit_code}{Style.RESET_ALL}\")\n                    if 'reason' in container:\n                        print(f\"    Reason: {Fore.RED}{container['reason']}{Style.RESET_ALL}\")\n                    if 'logStreamName' in container:\n                        print(f\"    Log Stream: {Fore.BLUE}{container['logStreamName']}{Style.RESET_ALL}\")\n\n        # Print node details for multi-node jobs\n        if 'nodeProperties' in job:\n            node_props = job['nodeProperties']\n            print(f\"\\n{Fore.YELLOW}Node Properties:{Style.RESET_ALL}\")\n            print(f\"  Number of Nodes: {Fore.GREEN}{node_props.get('numNodes', 'N/A')}{Style.RESET_ALL}\")\n            print(f\"  Main Node: {Fore.CYAN}{node_props.get('mainNode', 'N/A')}{Style.RESET_ALL}\")\n\n            # Print node ranges\n            if 'nodeRangeProperties' in node_props:\n                print(f\"\\n  {Fore.YELLOW}Node Ranges:{Style.RESET_ALL}\")\n                for i, node_range in enumerate(node_props['nodeRangeProperties']):\n                    print(f\"    {Fore.CYAN}Range {i}:{Style.RESET_ALL}\")\n                    print(f\"      Target Nodes: {Fore.MAGENTA}{node_range.get('targetNodes', 'N/A')}{Style.RESET_ALL}\")\n\n                    # Print container details\n                    container = node_range.get('container', {})\n                    if container:\n                        print(f\"      Image: {Fore.BLUE}{container.get('image', 'N/A')}{Style.RESET_ALL}\")\n                        print(f\"      vCPUs: {Fore.GREEN}{container.get('vcpus', 'N/A')}{Style.RESET_ALL}\")\n                        print(f\"      Memory: {Fore.GREEN}{container.get('memory', 'N/A')} MiB{Style.RESET_ALL}\")\n\n                        # Print command\n                        if 'command' in container:\n                            command_str = ' '.join(container['command'])\n                            print(f\"      Command: {Fore.MAGENTA}{command_str}{Style.RESET_ALL}\")\n\n        # Print dependencies\n        dependencies = job.get('dependencies', [])\n        if dependencies:\n            print(f\"\\n{Fore.YELLOW}Dependencies:{Style.RESET_ALL}\")\n            for dep in dependencies:\n                print(f\"  {Fore.CYAN}{dep.get('jobId', 'N/A')}{Style.RESET_ALL}: {dep.get('type', 'N/A')}\")\n\n        # Print tags\n        tags = job.get('tags', {})\n        if tags:\n            print(f\"\\n{Fore.YELLOW}Tags:{Style.RESET_ALL}\")\n            for key, value in tags.items():\n                print(f\"  {Fore.CYAN}{key}{Style.RESET_ALL}: {value}\")\n\n        # Restore colorama if it was disabled\n        if no_color:\n            Fore, Style = orig_fore, orig_style\n\n        return job\n    except Exception as e:\n        print(f\"Error retrieving job information: {str(e)}\")\n        return None\n\ndef find_jobs_by_prefix(prefix, max_results=5):\n    \"\"\"Find jobs that start with the given prefix.\"\"\"\n    batch = get_boto3_client()\n    matching_jobs = []\n\n    try:\n        # Get all job queues\n        queues_response = batch.describe_job_queues()\n        job_queues = [queue['jobQueueName'] for queue in queues_response['jobQueues']]\n\n        # Search for jobs in each queue\n        for queue in job_queues:\n            # Check all job statuses\n            for status in ['SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING', 'RUNNING']:\n                try:\n                    jobs_response = batch.list_jobs(\n                        jobQueue=queue,\n                        jobStatus=status,\n                        maxResults=100\n                    )\n\n                    # Look for jobs with the specified prefix\n                    for job_summary in jobs_response.get('jobSummaryList', []):\n                        if job_summary['jobName'].startswith(prefix) or job_summary['jobId'].startswith(prefix):\n                            # Get full job details\n                            job_details_response = batch.describe_jobs(jobs=[job_summary['jobId']])\n                            if job_details_response['jobs']:\n                                matching_jobs.append(job_details_response['jobs'][0])\n\n                                # Limit the number of results\n                                if len(matching_jobs) >= max_results:\n                                    return matching_jobs\n                except Exception:\n                    continue\n\n        return matching_jobs\n    except Exception as e:\n        print(f\"Error finding jobs by prefix: {str(e)}\")\n        return []\n\ndef stop_job(job_id_or_name, reason=\"Stopped by user\", max_results=5):\n    \"\"\"Stop a running job by ID or name.\"\"\"\n    batch = get_boto3_client()\n\n    try:\n        # First try to get the job by ID\n        response = batch.describe_jobs(jobs=[job_id_or_name])\n\n        # If no job found by ID, try to find by name\n        if not response['jobs']:\n            # We need to list jobs from all queues to find by name\n            job_id = None\n\n            # Get all job queues\n            try:\n                queues_response = batch.describe_job_queues()\n                job_queues = [queue['jobQueueName'] for queue in queues_response['jobQueues']]\n\n                # Search for the job in each queue\n                for queue in job_queues:\n                    # Check all job statuses\n                    for status in ['SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING', 'RUNNING']:\n                        try:\n                            jobs_response = batch.list_jobs(\n                                jobQueue=queue,\n                                jobStatus=status,\n                                maxResults=100\n                            )\n\n                            # Look for a job with the specified name\n                            for job_summary in jobs_response.get('jobSummaryList', []):\n                                if job_summary['jobName'] == job_id_or_name:\n                                    # Found a job with the specified name\n                                    job_id = job_summary['jobId']\n                                    job_details_response = batch.describe_jobs(jobs=[job_id])\n                                    if job_details_response['jobs']:\n                                        job = job_details_response['jobs'][0]\n                                        break\n\n                            if job_id:\n                                break\n                        except Exception:\n                            continue\n\n                    if job_id:\n                        break\n\n                if not job_id:\n                    # If no exact match found, try to find jobs with the prefix\n                    matching_jobs = find_jobs_by_prefix(job_id_or_name, max_results=max_results)\n                    if matching_jobs:\n                        return matching_jobs\n                    else:\n                        print(f\"No job found with ID or name '{job_id_or_name}'\")\n                        return False\n            except Exception as e:\n                print(f\"Error retrieving job queues: {str(e)}\")\n                print(f\"Job '{job_id_or_name}' not found\")\n                return False\n        else:\n            job = response['jobs'][0]\n            job_id = job['jobId']\n\n        # Check if the job is in a stoppable state\n        stoppable_states = ['SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING', 'RUNNING']\n        if job['status'] not in stoppable_states:\n            print(f\"Job '{job_id}' is in state '{job['status']}' and cannot be stopped\")\n            return False\n\n        # Stop the job\n        batch.terminate_job(\n            jobId=job_id,\n            reason=reason\n        )\n\n        print(f\"Job '{job_id}' has been stopped\")\n        return True\n    except Exception as e:\n        print(f\"Error stopping job: {str(e)}\")\n        return False\n\ndef stop_jobs(job_ids, reason=\"Stopped by user\"):\n    \"\"\"Stop multiple jobs by their IDs.\"\"\"\n    batch = get_boto3_client()\n    success_count = 0\n\n    for job_id in job_ids:\n        try:\n            # Check if the job is in a stoppable state\n            response = batch.describe_jobs(jobs=[job_id])\n            if not response['jobs']:\n                print(f\"Job '{job_id}' not found\")\n                continue\n\n            job = response['jobs'][0]\n            stoppable_states = ['SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING', 'RUNNING']\n            if job['status'] not in stoppable_states:\n                print(f\"Job '{job_id}' is in state '{job['status']}' and cannot be stopped\")\n                continue\n\n            # Stop the job\n            batch.terminate_job(\n                jobId=job_id,\n                reason=reason\n            )\n\n            print(f\"Job '{job_id}' has been stopped\")\n            success_count += 1\n        except Exception as e:\n            print(f\"Error stopping job '{job_id}': {str(e)}\")\n\n    return success_count > 0\n\ndef launch_job(job_queue=None):\n    \"\"\"Launch a new job.\"\"\"\n    print(\"The launch_job function is deprecated.\")\n    print(\"Please use the launch_cmd.py script directly:\")\n    print(\"  ../cmd.sh launch --run RUN_ID --cmd COMMAND [options]\")\n    return False\n\ndef get_job_ip(job_id_or_name):\n    \"\"\"Get the public IP address of the instance running a job.\"\"\"\n    batch = get_boto3_client('batch')\n    ecs = get_boto3_client('ecs')\n    ec2 = get_boto3_client('ec2')\n\n    try:\n        # First try to get the job by ID\n        response = batch.describe_jobs(jobs=[job_id_or_name])\n\n        # If no job found by ID, try to find by name\n        if not response['jobs']:\n            # We need to list jobs from all queues to find by name\n            job = None\n\n            # Get all job queues\n            queues_response = batch.describe_job_queues()\n            job_queues = [queue['jobQueueName'] for queue in queues_response['jobQueues']]\n\n            # Search for the job in each queue\n            for queue in job_queues:\n                # Check all job statuses\n                for status in ['RUNNING']:  # Only look for running jobs\n                    try:\n                        jobs_response = batch.list_jobs(\n                            jobQueue=queue,\n                            jobStatus=status,\n                            maxResults=100\n                        )\n\n                        # Look for a job with the specified name\n                        for job_summary in jobs_response.get('jobSummaryList', []):\n                            if job_summary['jobName'] == job_id_or_name:\n                                # Found a job with the specified name, get its details\n                                job_details_response = batch.describe_jobs(jobs=[job_summary['jobId']])\n                                if job_details_response['jobs']:\n                                    job = job_details_response['jobs'][0]\n                                    break\n\n                        if job:\n                            break\n                    except Exception:\n                        continue\n\n                if job:\n                    break\n\n            if not job:\n                print(f\"No running job found with ID or name '{job_id_or_name}'\")\n                return None\n        else:\n            job = response['jobs'][0]\n\n        # Check if the job is running\n        if job['status'] != 'RUNNING':\n            print(f\"Job '{job['jobId']}' is in state '{job['status']}' and not running\")\n            return None\n\n        # Check if it's a multi-node job\n        if 'nodeProperties' in job:\n            print(f\"Error: Job '{job['jobId']}' is a multi-node job. SSH is not supported for multi-node jobs.\")\n            return None\n\n        # Get the task ARN and cluster\n        container = job['container']\n        task_arn = container.get('taskArn')\n        cluster_arn = container.get('containerInstanceArn')\n\n        if not task_arn or not cluster_arn:\n            print(f\"Job '{job['jobId']}' does not have task or container instance information\")\n            return None\n\n        # Extract the cluster name from the cluster ARN\n        cluster_name = cluster_arn.split('/')[1]\n\n        # Get the container instance ARN\n        task_desc = ecs.describe_tasks(cluster=cluster_name, tasks=[task_arn])\n        if not task_desc['tasks']:\n            print(f\"No task found for job '{job['jobId']}'\")\n            return None\n\n        container_instance_arn = task_desc['tasks'][0]['containerInstanceArn']\n\n        # Get the EC2 instance ID\n        container_instance_desc = ecs.describe_container_instances(\n            cluster=cluster_name,\n            containerInstances=[container_instance_arn]\n        )\n        ec2_instance_id = container_instance_desc['containerInstances'][0]['ec2InstanceId']\n\n        # Get the public IP address\n        instances = ec2.describe_instances(InstanceIds=[ec2_instance_id])\n        if 'PublicIpAddress' in instances['Reservations'][0]['Instances'][0]:\n            public_ip = instances['Reservations'][0]['Instances'][0]['PublicIpAddress']\n            return public_ip\n        else:\n            print(f\"No public IP address found for job '{job['jobId']}'\")\n            return None\n\n    except Exception as e:\n        print(f\"Error retrieving job IP: {str(e)}\")\n        return None\n\ndef ssh_to_job(job_id_or_name, instance_only=False):\n    \"\"\"Connect to the instance running a job via SSH.\n\n    Args:\n        job_id_or_name: The job ID or name to connect to\n        instance_only: If True, connect directly to the instance without attempting to connect to the container\n    \"\"\"\n    import subprocess\n    import sys\n\n    # Get the IP address of the job\n    ip = get_job_ip(job_id_or_name)\n    if not ip:\n        return False\n\n    try:\n        # Establish SSH connection and check if it's successful\n        print(f\"Checking SSH connection to {ip}...\")\n        ssh_check_cmd = f\"ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 {ip} 'echo Connected'\"\n        ssh_check_output = subprocess.check_output(ssh_check_cmd, shell=True).decode().strip()\n        if ssh_check_output != \"Connected\":\n            raise subprocess.CalledProcessError(1, \"SSH connection check failed\")\n\n        if instance_only:\n            # Connect directly to the instance\n            print(f\"Connecting directly to the instance at {ip}...\")\n            ssh_cmd = f\"ssh -o StrictHostKeyChecking=no -t {ip}\"\n            subprocess.run(ssh_cmd, shell=True)\n        else:\n            # Retrieve container ID\n            print(f\"Finding container on {ip}...\")\n            container_cmd = f\"ssh -o StrictHostKeyChecking=no -t {ip} \\\"docker ps | grep 'mettaai/metta'\\\"\"\n            container_id_output = subprocess.check_output(container_cmd, shell=True).decode().strip()\n\n            if container_id_output:\n                container_id = container_id_output.split()[0]\n                print(f\"Connecting to container {container_id} on {ip}...\")\n                exec_cmd = f\"ssh -o StrictHostKeyChecking=no -t {ip} \\\"docker exec -it {container_id} /bin/bash\\\"\"\n                subprocess.run(exec_cmd, shell=True)\n            else:\n                print(f\"No container running the 'mettaai/metta' image found on the instance {ip}.\")\n                print(\"Connecting to the instance directly...\")\n                ssh_cmd = f\"ssh -o StrictHostKeyChecking=no -t {ip}\"\n                subprocess.run(ssh_cmd, shell=True)\n\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Error: {str(e)}\")\n        if \"Connection timed out\" in str(e):\n            print(f\"SSH connection to {ip} timed out. Please check the instance status and network connectivity.\")\n        elif \"Connection refused\" in str(e):\n            print(f\"SSH connection to {ip} was refused. Please check if the instance is running and accepts SSH connections.\")\n        else:\n            print(f\"An error occurred while connecting to {ip}. Please check the instance status and SSH configuration.\")\n        return False\n"}
{"type": "source_file", "path": "agent/lib/position.py", "content": "\nimport torch\nimport math\n\ndef position_embeddings(width, height, embedding_dim=128):\n    x = torch.linspace(-1, 1, width)\n    y = torch.linspace(-1, 1, height)\n    pos_x, pos_y = torch.meshgrid(x, y, indexing='xy')\n    return torch.stack((pos_x, pos_y), dim=-1)\n\ndef sinusoidal_position_embeddings(width, height, embedding_dim=128):\n    # Generate a grid of positions for x and y coordinates\n    x = torch.linspace(-1, 1, width, dtype=torch.float32)\n    y = torch.linspace(-1, 1, height, dtype=torch.float32)\n    pos_x, pos_y = torch.meshgrid(x, y, indexing='xy')\n\n    # Prepare to generate sinusoidal embeddings\n    assert embedding_dim % 2 == 0, \"Embedding dimension must be even.\"\n\n    # Create a series of frequencies exponentially spaced apart\n    freqs = torch.exp2(torch.linspace(0, math.log(embedding_dim // 2 - 1), embedding_dim // 2))\n\n    # Apply sinusoidal functions to the positions\n    embeddings_x = torch.cat([torch.sin(pos_x[..., None] * freqs), torch.cos(pos_x[..., None] * freqs)], dim=-1)\n    embeddings_y = torch.cat([torch.sin(pos_y[..., None] * freqs), torch.cos(pos_y[..., None] * freqs)], dim=-1)\n\n    # Combine x and y embeddings by summing, you could also concatenate or average\n    embeddings = embeddings_x + embeddings_y\n\n    # Add float embeddings\n    if embedding_dim >= 2:\n        embeddings[:,:,-2] = pos_x\n        embeddings[:,:,-1] = pos_y\n\n    return embeddings\n"}
{"type": "source_file", "path": "agent/resnet_encoder.py", "content": "import torch\nfrom torch import nn\nfrom omegaconf import OmegaConf\nfrom pufferlib.pytorch import layer_init\n\n\nfrom agent.lib.util import name_to_activation\nfrom agent.lib.observation_normalizer import ObservationNormalizer\n\nclass _ResidualBlock(nn.Module):\n    def __init__(self, depth: int, activation: str):\n        super().__init__()\n        self.conv1 = layer_init(nn.Conv2d(depth, depth, kernel_size=3, stride=1, padding=\"same\"))\n        self.conv2 = layer_init(nn.Conv2d(depth, depth, kernel_size=3, stride=1, padding=\"same\"))\n        self.activation = name_to_activation(activation)\n    \n    def forward(self, x):\n        xp = self.activation(self.conv1(x))\n        xp = self.activation(self.conv2(xp))\n        return x + xp\n    \nclass _IMPALAishBlock(nn.Module):\n    def __init__(self, input_channels: int, depth: int, activation: str):\n        super().__init__()\n        self.conv = layer_init(nn.Conv2d(input_channels, depth, kernel_size=3, stride=1, padding=\"same\"))\n        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n        self.residual1 = _ResidualBlock(depth, activation)\n        self.residual2 = _ResidualBlock(depth, activation)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.pool(x)\n        x = self.residual1(x)\n        x = self.residual2(x)\n        return x\n\n\ndef _convolution_shape(shape, kernel_size, stride):\n    return tuple((x - kernel_size) // stride + 1 for x in shape)\n\ndef _product(iterable):\n    prod = 1\n    for i in iterable:\n        prod *= i\n    return prod\n\nclass IMPALAishCNN(nn.Module):\n    def __init__(self,\n                 obs_space,\n                 grid_features: list[str],\n                 global_features: list[str],\n                 **cfg):\n        super().__init__()\n        cfg = OmegaConf.create(cfg)\n\n        grid_shape = obs_space[cfg.obs_key].shape\n\n        self._output_dim = cfg.fc.output_dim\n        self._obs_key = cfg.obs_key\n        self.activation = name_to_activation(cfg.activation)\n\n        self.object_normalizer = None\n        if cfg.normalize_features:\n            self.object_normalizer = ObservationNormalizer(grid_features)\n        \n        if isinstance(cfg.cnn_channels, int):\n            channels = (grid_shape[0], cfg.cnn_channels)\n        else:\n            channels = (grid_shape[0], *cfg.cnn_channels)\n\n        grid_width_height = grid_shape[1:]\n\n        cnn_blocks = []\n        for in_channels, out_channels in zip(channels, channels[1:]):\n            cnn_blocks.append(_IMPALAishBlock(in_channels, out_channels, cfg.activation))\n            grid_width_height = _convolution_shape(grid_width_height, 3, 2)\n\n        self.cnn_layers = nn.Sequential(*cnn_blocks)\n\n        cnn_flattened_size = _product(grid_width_height) * channels[-1]\n        self.fc_layer = layer_init(nn.Linear(cnn_flattened_size, self._output_dim))\n\n\n    def forward(self, obs_dict):\n        obs = obs_dict[self._obs_key]\n\n        if self.object_normalizer is not None:\n            obs = self.object_normalizer(obs)\n\n        x = self.cnn_layers(obs)\n        x = torch.flatten(x, start_dim=1)\n        \n        x = self.activation(x)\n        x = self.fc_layer(x)\n        x = self.activation(x)\n\n        return x\n\n    def output_dim(self):\n        return self._output_dim\n"}
{"type": "source_file", "path": "env/wrapper/reward_sharing.py", "content": "import numpy as np\nimport gymnasium as gym\nimport numpy as np\n\n\n# class RewardAllocator():\n#     def __init__(self, num_agents) -> None:\n#         self._num_agents = num_agents\n\n#     def compute_shared_rewards(self, rewards):\n#         return rewards\n\n#     def obs(self, agent_id, agent_obs):\n#         return np.array([agent_obs])\n\n# class MatrixRewardAllocator(RewardAllocator):\n#     def __init__(self, num_agents, reward_sharing_matrix) -> None:\n#         super().__init__(num_agents)\n#         self._reward_sharing_matrix = reward_sharing_matrix\n#         self._agent_kinship = np.array(range(num_agents))\n\n#     def compute_shared_rewards(self, rewards):\n#         if self._reward_sharing_matrix is None:\n#             return rewards\n\n#         return self._reward_sharing_matrix @ rewards.transpose()\n\n# class FamillySparseAllocator(RewardAllocator):\n#     def __init__(self, num_agents, families, family_reward_coef) -> None:\n#         super().__init__(num_agents)\n#         assert family_reward_coef > 0\n#         self._families = families\n#         self._member_reward_coef = np.zeros(num_agents, dtype=np.float32)\n#         self._self_reward_coef = np.zeros(num_agents, dtype=np.float32)\n#         self._agent_to_family_id = np.zeros(num_agents, dtype=np.int32)\n#         for fid, family in enumerate(families):\n#             mc = family_reward_coef / len(family)\n#             self._member_reward_coef[family] = mc\n#             self._self_reward_coef[family] = 1 - family_reward_coef\n#             self._agent_to_family_id[family] = fid\n\n#     def compute_shared_rewards(self, rewards):\n#         shared_rewards = np.zeros_like(rewards)\n#         for agent, reward in enumerate(rewards):\n#             if reward != 0:\n#                 family_id = self._agent_to_family_id[agent]\n#                 family = self._families[family_id]\n#                 shared_rewards[family] += reward * self._member_reward_coef[agent]\n#                 shared_rewards[agent] += reward * self._self_reward_coef[agent]\n#         return shared_rewards\n\n\n# class FamillyAllocator(FamillySparseAllocator):\n#     def __init__(self, num_agents, num_families, family_reward_coef) -> None:\n#         assert num_families > 0 and family_reward_coef > 0\n\n#         agents = np.array(range(num_agents))\n#         np.random.shuffle(agents)\n#         families = np.array_split(agents, num_families)\n#         super().__init__(num_agents, families, family_reward_coef)\n\n# class FamillyMatrixAllocator(MatrixRewardAllocator):\n#     def __init__(self, num_agents, num_families, family_reward) -> None:\n#         assert num_families > 0 and family_reward > 0\n\n#         agents = np.array(range(num_agents))\n#         np.random.shuffle(agents)\n#         families = np.array_split(agents, num_families)\n\n#         rsm = np.zeros((len(agents), len(agents)), dtype=np.float32)\n#         # share the reward among the families\n#         for family_id, family in enumerate(families):\n#             fr = family_reward / ( len(family) - 1 )\n#             for a in family:\n#                 self._agent_kinship[a] = family_id\n#                 rsm[a, family] = fr\n#                 rsm[a, a] = 1 - family_reward\n\n#         # normalize\n#         rsm = rsm / rsm.sum(axis=1, keepdims=True)\n\n#         super().__init__(num_agents, rsm)\n\n\n# class TeamRewardSharing(gym.wrappers.Wrapper):\n#     def __init__(self, env):\n#         super().__init__(env)\n#         self._reward_sharing = None\n#         # set up reward sharing\n#         self._reward_sharing = RewardAllocator(self._num_agents)\n#         num_families = self._level_generator.sample_cfg(\"rsm_num_families\")\n#         family_reward = self._level_generator.sample_cfg(\"rsm_family_reward\")\n#         if num_families > 0:\n#             self._reward_sharing = FamillyAllocator(self._num_agents, num_families, family_reward)\n\n#     def reset(self):\n#         self._last_actions = np.zeros((self._num_agents, 2), dtype=np.int32)\n#         return self.env.reset()\n\n#     def step(self, actions):\n#         obs, rewards, terms, truncs, infos = self.env.step(actions)\n#         rewards = self._reward_sharing.compute_shared_rewards(rewards)\n\n#         return self._augment_observations(obs), rewards, terms, truncs, infos\n\n#     def _augment_observations(self, obs):\n#         return [{\n#             \"last_action\": np.array(self._last_actions[agent]),\n#             **agent_obs\n#         } for agent, agent_obs in enumerate(obs)]\n\n\n#     def observation_space(self):\n#         return gym.spaces.Dict({\n#             \"last_action\": gym.spaces.Box(\n#                 low=0, high=255, shape=(2,), dtype=np.int32),\n#             **self.env.observation_space()\n#         })\n"}
{"type": "source_file", "path": "rl/eval/eval_stats_analyzer.py", "content": "import logging\nfrom omegaconf import DictConfig\nfrom typing import Dict, Any, List, Optional\nfrom rl.eval.eval_stats_db import EvalStatsDB\nfrom tabulate import tabulate\nimport fnmatch\nimport numpy as np\nimport pandas as pd\nimport os\nimport wandb\nlogger = logging.getLogger(\"eval_stats_analyzer\")\n\nclass EvalStatsAnalyzer:\n    def __init__(\n        self,\n        stats_db: EvalStatsDB,\n        analysis: DictConfig,\n        policy_uri: str,\n        **kwargs):\n        self.analysis = analysis\n        self.stats_db = stats_db\n        self.candidate_policy_uri = policy_uri\n        self.global_filters = analysis.get('filters', None)\n\n        metric_configs = {}\n        metrics = []\n        for cfg in self.analysis.metrics:\n            metric_configs[cfg] = fnmatch.filter(self.stats_db.available_metrics, cfg.metric)\n            metrics.extend(metric_configs[cfg])\n        self.metric_configs = metric_configs\n        self.analysis.metrics = metrics\n\n\n    def _filters(self, item):\n        local_filters = item.get('filters')\n\n        if not self.global_filters:\n            return local_filters\n\n        if not local_filters:\n            return self.global_filters\n\n        merged = {}\n        for key in set(self.global_filters) | set(local_filters):\n            global_val = self.global_filters.get(key)\n            local_val = local_filters.get(key)\n            # If both values exist and are lists, concatenate them.\n            if isinstance(global_val, list) and isinstance(local_val, list):\n                merged[key] = global_val + local_val\n            # Otherwise, prefer the local value if it exists; if not, use the global value.\n            else:\n                merged[key] = local_val if key in local_filters else global_val\n\n        return merged\n\n    def log_result(self, result, metric, filters):\n        result_table = tabulate(result, headers=list(result.keys()), tablefmt=\"grid\", maxcolwidths=25)\n        logger.info(f\"Results for {metric} with filters {filters}:\\n{result_table}\")\n\n\n    def _analyze_metrics(self, metric_configs):\n        result_dfs = []\n        policy_fitness_records = []\n        for cfg, metrics in metric_configs.items():\n            filters = self._filters(cfg)\n            group_by_episode = cfg.get('group_by_episode', False)\n            for metric in metrics:\n                metric_result = self.stats_db._metric(metric, filters, group_by_episode)\n                if len(metric_result) == 0:\n                    logger.info(f\"No data found for {metric} with filters {filters}\" + \"\\n\")\n                    continue\n                policy_fitness = self.policy_fitness(metric_result, metric)\n                policy_fitness_records.extend(policy_fitness)\n                result_dfs.append(metric_result)\n                # self.log_result(metric_result, metric, filters)\n\n        return result_dfs, policy_fitness_records\n\n    def analyze(self):\n        if all(len(self.metric_configs[cfg]) == 0 for cfg in self.metric_configs):\n            logger.info(f\"No metrics to analyze yet for {self.candidate_policy_uri}\")\n            return [], []\n\n        result_dfs, policy_fitness_records = self._analyze_metrics(self.metric_configs)\n\n        # Create table with eval_name and mean for each policy\n        if len(result_dfs) > 0 and self.analysis.log_all:\n            df = tabulate(result_dfs[0], headers=list(result_dfs[0].keys()), tablefmt=\"grid\", maxcolwidths=25)\n            logger.info(f\"Mean metrics by eval:\\n{df}\")\n\n        policy_fitness_df = pd.DataFrame(policy_fitness_records)\n        if len(policy_fitness_df) > 0:\n            policy_fitness_table = tabulate(policy_fitness_df, headers=[self.candidate_policy_uri] + list(policy_fitness_df.keys()), tablefmt=\"grid\", maxcolwidths=25)\n            logger.info(f\"Policy fitness results for candidate policy {self.candidate_policy_uri} and baselines {self.analysis.baseline_policies}:\\n{policy_fitness_table}\")\n\n        return result_dfs, policy_fitness_records\n\n    @staticmethod\n    def get_latest_policy(all_policies, uri):\n        if uri in all_policies:\n            return uri\n        if \"wandb\" in uri:\n            uri = uri.replace(\"wandb://metta-research/metta/\", \"\")\n            uri = uri.replace(\"wandb://run/\", \"\")\n        policy_versions = [i for i in all_policies if uri in i]\n        if len(policy_versions) == 0:\n            raise ValueError(f\"No policy found in DB for candidate policy: {uri}, options are {all_policies}\")\n        if len(policy_versions) > 1 and 'wandb' in uri:\n            policy_versions.sort(key=lambda x: int(x.split(':v')[-1]))\n        candidate_uri = policy_versions[-1]\n\n        return candidate_uri\n\n    def policy_fitness(self, metric_data, metric_name):\n        policy_fitness = []\n        if \"wandb\" in self.candidate_policy_uri:\n            uri = self.candidate_policy_uri.replace(\"wandb://run/\", \"\")\n        elif \"file\" in self.candidate_policy_uri:\n            uri = self.candidate_policy_uri.replace(\"file://\", \"\")\n        else:\n            uri = self.candidate_policy_uri\n\n        all_policies = metric_data['policy_name'].unique()\n\n        # Get the latest version of the candidate policy\n        candidate_uri = self.get_latest_policy(all_policies, uri)\n\n        baseline_policies = list(set([self.get_latest_policy(all_policies, b) for b in self.analysis.baseline_policies or all_policies]))\n\n        metric_data = metric_data.set_index('policy_name')\n        eval, metric_mean, metric_std = metric_data.keys()\n\n        evals = metric_data[eval].unique()\n\n        if len(evals) == 1:\n            candidate_mean = metric_data.loc[candidate_uri][metric_mean]\n            baseline_mean = metric_data.loc[baseline_policies][metric_mean].mean()\n            fitness = candidate_mean - baseline_mean # / np.std(baseline_data.loc[eval][metric_std])\n            policy_fitness.append({\"eval\": eval, \"metric\": metric_name, \"candidate_mean\": candidate_mean, \"baseline_mean\": baseline_mean, \"fitness\": fitness})\n            return policy_fitness\n\n\n        candidate_data = pd.DataFrame(metric_data.loc[candidate_uri]).set_index(eval)\n        baseline_data = metric_data.loc[baseline_policies].set_index(eval)\n\n        evals = metric_data[eval].unique()\n\n        for eval in evals:\n            # Is the difference the correct way to do this?\n            if eval not in candidate_data.index:\n                continue\n            candidate_mean = candidate_data.loc[eval][metric_mean]\n            baseline_mean = np.mean(baseline_data.loc[eval][metric_mean])\n            fitness = candidate_mean - baseline_mean # / np.std(baseline_data.loc[eval][metric_std])\n            policy_fitness.append({\"eval\": eval, \"metric\": metric_name, \"candidate_mean\": candidate_mean, \"baseline_mean\": baseline_mean, \"fitness\": fitness})\n        return policy_fitness\n"}
{"type": "source_file", "path": "rl/pufferlib/dashboard/dashboard.py", "content": "import time\nfrom threading import Thread\nimport os\nfrom omegaconf import OmegaConf\n\nimport rich\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.live import Live\nfrom util.logging import remap_io, restore_io\n\n\nimport sys\n\nclass Dashboard(Thread):\n    def __init__(self, cfg: OmegaConf, delay=1, components=None):\n        super().__init__()\n        self.cfg = cfg\n        self.delay = delay\n\n        self._components = components or []\n        for component in self._components:\n            component._dashboard = self\n\n        tty_file = open('/dev/tty', 'w')\n        self.console = Console(file=tty_file, force_terminal=True)\n        remap_io(os.path.join(cfg.data_dir, cfg.experiment))\n        self._stopped = False\n        self.start()\n\n    def _render_component(self, component, container):\n        if isinstance(component, DashboardComponent):\n            container.add_row(component.render())\n        elif isinstance(component, list):\n            container.add_row(*[self.render_component(c, container) for c in component])\n        else:\n            raise ValueError(f\"Invalid component type: {type(component)}\")\n\n    def _clear_console(self):\n        # Clear console for different operating systems\n        if os.name == 'nt':  # For Windows\n            os.system('cls')\n        else:  # For Unix/Linux/macOS\n            os.system('clear')\n\n    def run(self):\n        start_time = time.time()\n        cleared = False\n        with Live(self._render(),\n                  console=self.console,\n                  redirect_stderr=False,\n                  redirect_stdout=False,\n                  refresh_per_second=1/self.delay) as live:\n            while not self._stopped:\n                if time.time() - start_time > 5 and not cleared:\n                    self._clear_console()\n                    cleared = True\n                live.update(self._render())\n                time.sleep(self.delay)\n\n    def stop(self):\n        for component in self._components:\n            component.stop()\n        self._stopped = True\n        restore_io()\n\n\n\n    def _render(self):\n        dashboard = Table(box=ROUND_OPEN, expand=True,\n            show_header=False, border_style='bright_cyan')\n\n        for component in self._components:\n            self._render_component(component, dashboard)\n\n        return dashboard\n\nclass DashboardComponent:\n    def __init__(self):\n        self._dashboard = None\n\n    def render(self):\n        pass\n\n    def stop(self):\n        pass\n\n\nROUND_OPEN = rich.box.Box(\n    \"\\n\"\n    \"  \\n\"\n    \"  \\n\"\n    \"  \\n\"\n    \"  \\n\"\n    \"  \\n\"\n    \"  \\n\"\n    \"\\n\"\n)\n\nc1 = '[bright_cyan]'\nc2 = '[white]'\nc3 = '[cyan]'\nb1 = '[bright_cyan]'\nb2 = '[bright_white]'\n\ndef abbreviate(num):\n    if num < 1e-3:\n        return f'{b2}{num:.000f}'\n    elif num < 0.1:\n        return f'{b2}{num:.00f}'\n    elif num < 1:\n        return f'{b2}{num:.00f}'\n    elif num < 1e3:\n        return f'{b2}{num:.0f}'\n    elif num < 1e6:\n        return f'{b2}{num/1e3:.1f}{c2}k'\n    elif num < 1e9:\n        return f'{b2}{num/1e6:.1f}{c2}m'\n    elif num < 1e12:\n        return f'{b2}{num/1e9:.1f}{c2}b'\n    else:\n        return f'{b2}{num/1e12:.1f}{c2}t'\n\ndef duration(seconds):\n    seconds = int(seconds)\n    h = seconds // 3600\n    m = (seconds % 3600) // 60\n    s = seconds % 60\n    return f\"{b2}{h}{c2}h {b2}{m}{c2}m {b2}{s}{c2}s\" if h else f\"{b2}{m}{c2}m {b2}{s}{c2}s\" if m else f\"{b2}{s}{c2}s\"\n\ndef fmt_perf(name, time, uptime):\n    percent = 0 if uptime == 0 else int(100*time/uptime - 1e-5)\n    return f'{c1}{name}', duration(time), f'{b2}{percent:2d}%'\n"}
{"type": "source_file", "path": "rl/pufferlib/__init__.py", "content": ""}
{"type": "source_file", "path": "devops/wandb/wandb_ttl.py", "content": "import argparse\nimport wandb\n\nENTITY_NAME = \"metta-research\"\nPROJECT_NAME = \"metta\"\n\ndef apply_ttl_to_artifacts(artifact_project: str, version_start: int, version_end: int, ttl_days: int) -> None:\n    \"\"\"\n    Update the TTL of a range of artifact versions.\n    \"\"\"\n    run = wandb.init(project=PROJECT_NAME, entity=ENTITY_NAME)\n\n    version = version_start\n    while version <= version_end:\n        try:\n            artifact_name = f'{ENTITY_NAME}/{PROJECT_NAME}/{artifact_project}:v{version}'\n            artifact = run.use_artifact(artifact_name)\n            print(f'Applying TTL to artifact: {artifact_name}')\n            artifact.ttl = ttl_days\n            artifact.save()\n            version += 1\n        except wandb.errors.CommError:\n            print(f'No more artifacts found after version {version - 1}.')\n            break\n        except Exception as e:\n            print(f'An error occurred with artifact {artifact_name}: {e}')\n            version += 1\n\n    print(\"All artifact versions processed.\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--artifact_project', required=True, help='The project containing the artifacts e.g. p2.train.norm.feat.')\n    parser.add_argument('--version_start', type=int, required=True, help='The starting version number of the artifacts.')\n    parser.add_argument('--version_end', type=int, required=True, help='The ending version number of the artifacts.')\n    parser.add_argument('--ttl_days', type=int, required=True, help='The TTL in number of days to apply to each artifact. Note that it\\'s in days from artifact version creation, not the current date.')\n\n    args = parser.parse_args()\n\n    apply_ttl_to_artifacts(args.artifact_project, args.version_start, args.version_end, args.ttl_days)\n"}
{"type": "source_file", "path": "rl/carbs/metta_carbs.py", "content": "import __future__\nimport math\nimport time\n\nfrom carbs import (\n    CARBS,\n    CARBSParams,\n    LinearSpace,\n    LogitSpace,\n    LogSpace,\n    Param,\n)\nfrom omegaconf import DictConfig, OmegaConf\nfrom collections import defaultdict\nfrom wandb_carbs import Pow2WandbCarbs\nimport numpy as np\nimport logging\nlogger = logging.getLogger(\"sweep_rollout\")\n\n_carbs_space = {\n    \"log\": LogSpace,\n    \"linear\": LinearSpace,\n    \"pow2\": LinearSpace,\n    \"logit\": LogitSpace,\n}\n\ndef carbs_params_from_cfg(cfg: OmegaConf):\n    param_spaces = []\n    pow2_params = set()\n    params = _fully_qualified_parameters(cfg.sweep)\n    for param_name, param in params.items():\n        train_cfg_param = cfg\n        if param.search_center is None:\n            for k in param_name.split(\".\"):\n                train_cfg_param = train_cfg_param[k]\n            OmegaConf.set_struct(param, False)\n            param.search_center = train_cfg_param\n            OmegaConf.set_struct(param, True)\n\n        if param.space == \"pow2\":\n            try:\n                param.min = int(math.log2(param.min))\n                param.max = int(math.log2(param.max))\n                param.search_center = int(math.log2(param.search_center))\n            except Exception as e:\n                print(f\"Error setting pow2 params for {param_name}=({param.min}, {param.max}, {param.search_center}): {e}\")\n                raise e\n            pow2_params.add(param_name)\n        scale = param.get(\"scale\", 1)\n\n        if param.space == \"pow2\" or param.get(\"is_int\", False):\n            scale = 4\n        if param.search_center < param.min or param.search_center > param.max:\n            raise ValueError(f\"Search center for {param_name}: {param.search_center} is not in range [{param.min}, {param.max}]\")\n\n        param_spaces.append(\n            Param(\n                name=param_name,\n                space=_carbs_space[param.space](\n                    min=param.min,\n                    max=param.max,\n                    is_integer=param.get(\"is_int\", False) or param.space == \"pow2\",\n                    rounding_factor=param.get(\"rounding_factor\", 1),\n                    scale=scale,\n                ),\n                search_center=param.search_center,\n            )\n        )\n    return param_spaces, pow2_params\n\n\ndef _fully_qualified_parameters(nested_dict, prefix=''):\n    qualified_params = {}\n    if \"space\" in nested_dict:\n        return {prefix: nested_dict}\n    for key, value in nested_dict.items():\n        new_prefix = f\"{prefix}.{key}\" if prefix else key\n        if isinstance(value, DictConfig):\n            qualified_params.update(_fully_qualified_parameters(value, new_prefix))\n    return qualified_params\n\n\nclass MettaCarbs(Pow2WandbCarbs):\n    def __init__(self, cfg: OmegaConf, run):\n        self.cfg = cfg\n        carbs_params, pow2_params = carbs_params_from_cfg(cfg)\n        self.generation = 0\n        self.runs = []\n\n        super().__init__(\n            CARBS(\n                CARBSParams(\n                    better_direction_sign=1,\n                    resample_frequency=5,\n                    num_random_samples=cfg.num_random_samples,\n                    is_wandb_logging_enabled=False,\n                    seed=int(time.time()),\n                    is_saved_on_every_observation=False,\n                    checkpoint_dir=None,\n                ),\n                carbs_params\n            ),\n            pow2_params, run)\n\n    def _get_runs_from_wandb(self):\n        runs = super()._get_runs_from_wandb()\n        if not hasattr(self.cfg, 'generation') or not self.cfg.generation.enabled:\n            return runs\n\n        generations = defaultdict(list)\n        for run in runs:\n            if run.summary[\"carbs.state\"] == \"success\":\n                generation = run.summary.get(\"generation\", 0)\n                generations[generation].append(run)\n        max_gen = 0\n        if len(generations) > 0:\n            max_gen = max(generations.keys())\n        if len(generations[max_gen]) < self.cfg.generation.min_samples:\n            self.generation = max_gen\n            logger.info(f\"Updating newest generation: {self.generation} with {len(generations[self.generation])} samples\")\n        elif np.random.random() >= self.cfg.generation.regen_pct:\n            self.generation = max_gen + 1\n            logger.info(f\"New creating a new generation: {self.generation}\")\n        else:\n            self.generation = np.random.randint(max_gen + 1)\n            logger.info(f\"Updating generation: {self.generation} with {len(generations[self.generation])} samples\")\n\n        self.runs = [run for run in runs if run.summary.get(\"generation\", 0) == self.generation]\n        return self.runs\n"}
{"type": "source_file", "path": "env/wrapper/petting_zoo.py", "content": "import pettingzoo\nimport gymnasium as gym\nimport numpy as np\n\nclass PettingZooEnvWrapper(pettingzoo.ParallelEnv):\n    def __init__(self, gym_env: gym.Env, render_mode='rgb_array'):\n        super().__init__()\n        self._gym_env = gym_env\n        self.possible_agents = [i+1 for i in range(self.num_agents)]\n        # agents gets manipulated\n        self.agents = [i+1 for i in range(self.num_agents)]\n        self.render_mode = render_mode\n\n    @property\n    def num_agents(self):\n        return self._gym_env.unwrapped.player_count\n\n    def observation_space(self, agent: int) -> gym.Space:\n        return self._gym_env.observation_space\n\n    def action_space(self, agent: int) -> gym.Space:\n        return self._gym_env.action_space\n\n    def render(self, *args):\n        return self._gym_env.render()\n\n    def _handle_info(self, infos):\n        if \"episode_extra_stats\" in infos:\n            infos = {i+1: infos[\"episode_extra_stats\"][i] for i in range(self.num_agents)}\n        else:\n            infos = {i+1: {} for i in range(self.num_agents)}\n        return infos\n\n    def reset(self, seed=0, **kwargs):\n        obs, infos = self._gym_env.reset()\n        observations = {i+1: obs[i] for i in range(self.num_agents)}\n        return observations, self._handle_info(infos)\n\n    def step(self, actions):\n        actions = np.array(list(actions.values()), dtype=np.uint32)\n        obs, rewards, terminated, truncated, infos = self._gym_env.step(actions)\n        observations = {i+1: obs[i] for i in range(self.num_agents)}\n        rewards = {i+1: rewards[i] for i in range(self.num_agents)}\n        terminated = {i+1: terminated for i in range(self.num_agents)}\n        truncated = {i+1: truncated for i in range(self.num_agents)}\n        infos = self._handle_info(infos)\n\n        return observations, rewards, terminated, truncated, infos\n"}
{"type": "source_file", "path": "env/wrapper/feature_masker.py", "content": "from functools import lru_cache\nimport gymnasium as gym\nfrom matplotlib.pylab import f\nimport numpy as np\n\nclass FeatureMasker(gym.Wrapper):\n    def __init__(self, env, masked_features):\n        super().__init__(env)\n\n        self._masked_grid_obs = [\n            self.env.unwrapped.grid_features.index(feature)\n            for feature in masked_features.grid_obs\n        ]\n        self._grid_obs_mask = np.ones(\n            self.env.unwrapped.observation_space[\"grid_obs\"].shape,\n            dtype=np.uint8)\n\n        self._grid_obs_mask[self._masked_grid_obs] = 0\n        self._grid_obs_mask[self._masked_grid_obs,\n                            self.env.unwrapped._obs_width // 2,\n                            self.env.unwrapped._obs_height // 2] = 1\n\n    def reset(self, **kwargs):\n        obs, infos = self.env.reset(**kwargs)\n        return self._augment_observations(obs), infos\n\n    def step(self, actions):\n        obs, rewards, terms, truncs, infos = self.env.step(actions)\n        return self._augment_observations(obs), rewards, terms, truncs, infos\n\n    def _augment_observations(self, obs):\n        if len(self._masked_grid_obs):\n            for agent_obs in obs:\n                agent_obs[\"grid_obs\"] *= self._grid_obs_mask\n        return obs\n"}
{"type": "source_file", "path": "rl/carbs/delete_runs.py", "content": "import argparse\nimport wandb\nfrom typing import List\n\ndef delete_init_runs(sweep_id: str, entity: str, project: str) -> List[str]:\n    deleted_runs = []\n    api = wandb.Api()\n\n    try:\n        sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")\n    except wandb.errors.CommError:\n        print(f\"Sweep not found: {sweep_id}\")\n        return deleted_runs\n\n    for run in sweep.runs:\n        if run.name.endswith(\".init\"):\n            try:\n                run.delete()\n                deleted_runs.append(run.name)\n                print(f\"Deleted run: {run.name}\")\n            except Exception as e:\n                print(f\"Error deleting run {run.name}: {e}\")\n\n    return deleted_runs\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Delete .init runs from a sweep using wandb API\")\n    parser.add_argument(\"--sweep\", type=str, required=True, help=\"Sweep ID\")\n    parser.add_argument(\"--entity\", type=str, default=\"metta-research\", help=\"W&B entity\")\n    parser.add_argument(\"--project\", type=str, default=\"metta\", help=\"W&B project\")\n    args = parser.parse_args()\n\n    deleted_runs = delete_init_runs(args.sweep, args.entity, args.project)\n\n    if deleted_runs:\n        print(f\"Successfully deleted {len(deleted_runs)} .init runs\")\n    else:\n        print(\"No .init runs found to delete\")\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "rl/__init__.py", "content": ""}
{"type": "source_file", "path": "rl/eval/eval_stats_db.py", "content": "import os\nimport json\nimport duckdb\nimport wandb\nimport pandas as pd\nfrom typing import Optional, Dict, Any, List\nimport logging\nimport shutil\nimport gzip\n\nfrom rl.eval.queries import all_fields, total_metric\n\n\nlogger = logging.getLogger(\"eval_stats_db.py\")\n\nclass EvalStatsDB:\n    def __init__(self, data: pd.DataFrame):\n        self._db = duckdb.connect(database=':memory:')\n        self._table_name = \"eval_data\"\n        self._db.register(self._table_name, data)\n        self.available_metrics = self._query(all_fields())\n        logger.info(f\"Loaded {len(self.available_metrics)} metrics from {self._table_name}\")\n\n    @staticmethod\n    def _prepare_data(data) -> List[dict]:\n        \"\"\"\n        Each record is augmented with:\n          - 'episode_index': the index of the episode.\n          - 'agent_index': the index of the record within the episode.\n\n          data: list (per episode) of lists of dicts (per agent)\n          eg [\n                [{'action.use ': 1.0, 'r2.gained': 100}, {'action.use ': 3.0, 'r2.gained': 200}]\n             ...\n                [{'action.use ': 1.0, 'r2.gained': 100}, {'action.use ': 3.0, 'r2.gained': 200}]\n             ]\n\n          flattened: list of dicts, where each dict is a record from a single episode.\n          eg [\n            {'episode_index': 0, 'agent_index': 0, 'action.use ': 1.0, 'r2.gained': 100}\n            ...\n            {'episode_index': n, 'agent_index': n, 'action.use ': 1.0, 'r2.gained': 100}\n          ]\n        \"\"\"\n        flattened = []\n        for episode_index, episode in enumerate(data):\n            for agent_index, record in enumerate(episode):\n                record[\"episode_index\"] = episode_index\n                record[\"agent_index\"] = agent_index\n                flattened.append(record)\n        return flattened\n\n    def _query(self, sql_query: str) -> pd.DataFrame:\n        try:\n            result = self._db.execute(sql_query).fetchdf()\n            return result\n        except Exception as e:\n            raise RuntimeError(f\"SQL query failed: {sql_query}\\nError: {e}\")\n\n    def _metric(self, metric_field: str, filters: Optional[Dict[str, Any]] = None, group_by_episode: bool = False) -> pd.DataFrame:\n        long_df = self._query(total_metric(metric_field, filters))\n        # Average over unique policy_name while maintaining eval_name and episode_index\n        df_per_episode = long_df.pivot(index='episode_index', columns=['eval_name', 'policy_name'], values='total_metric').fillna(0)\n        metric_df = df_per_episode.copy()\n        if not group_by_episode:\n            stats_df = df_per_episode.agg(['mean', 'std'])\n            metric_df = stats_df.T.reset_index()\n            metric_df.columns = ['eval_name', 'policy_name', 'mean', 'std']\n        return metric_df\n\n    @staticmethod\n    def from_uri(uri: str, run_dir: str, wandb_run = None):\n        # We want eval stats to be the same for train, analysis and eval for a particular run\n        save_dir = run_dir.replace(\"analyze\", \"train\").replace(\"eval\", \"train\")\n        uri = uri or os.path.join(save_dir, \"eval_stats\")\n        if uri.startswith(\"wandb://\"):\n            artifact_name = uri.split(\"/\")[-1]\n            return EvalStatsDbWandb(artifact_name, wandb_run)\n        else:\n            if uri.startswith(\"file://\"):\n                json_path = uri.split(\"file://\")[1]\n            else:\n                json_path = uri\n            if json_path.endswith(\".json\"):\n                json_path = json_path + \".gz\"\n            elif not json_path.endswith(\".json.gz\"):\n                json_path = json_path + \".json.gz\"\n            return EvalStatsDbFile(json_path)\n\n\nclass EvalStatsDbFile(EvalStatsDB):\n    \"\"\"\n    Database for loading eval stats from a file.\n    \"\"\"\n    def __init__(self, json_path: str):\n        if not os.path.exists(json_path):\n            raise FileNotFoundError(f\"File not found: {json_path}\")\n\n        with gzip.open(json_path, 'rt') as f:\n            data = json.load(f)\n        logger.info(f\"Loading eval stats from {json_path}\")\n\n        data = self._prepare_data(data)\n\n        super().__init__(pd.DataFrame(data))\n\nclass EvalStatsDbWandb(EvalStatsDB):\n    \"\"\"\n    Database for loading eval stats from wandb.\n    \"\"\"\n    def __init__(self, artifact_name: str, wandb_run, from_cache: bool = False):\n        self.api = wandb.Api()\n        self.artifact_identifier = os.path.join(wandb_run.entity, wandb_run.project, artifact_name)\n\n        cache_dir = os.path.join(wandb.env.get_cache_dir(), \"artifacts\", self.artifact_identifier)\n        artifact_versions = self.api.artifacts(type_name=artifact_name,name=self.artifact_identifier)\n        artifact_dirs = [os.path.join(cache_dir, v.name) for v in artifact_versions]\n        for dir, artifact in zip(artifact_dirs, artifact_versions):\n            if os.path.exists(dir) and from_cache:\n                logger.info(f\"Loading from cache: {dir}\")\n            else:\n                artifact.download(root=dir)\n                path = os.path.join(dir, os.listdir(dir)[0])\n                if path.endswith('.json.gz'):\n                    with gzip.open(path, \"rb\") as f_in:\n                        with open(path.replace('.gz', ''), \"wb\") as f_out:\n                            shutil.copyfileobj(f_in, f_out)\n\n        logger.info(f\"Loaded {len(artifact_dirs)} artifacts\")\n        all_records = []\n        for artifact_dir in artifact_dirs:\n            json_files = [f for f in os.listdir(artifact_dir) if f.endswith('.json')]\n            if len(json_files) != 1:\n                raise FileNotFoundError(f\"Expected exactly one JSON file in {artifact_dir}, found {len(json_files)}\")\n            json_path = os.path.join(artifact_dir, json_files[0])\n            with open(json_path, \"r\") as f:\n                data = json.load(f)\n            data = self._prepare_data(data)\n            all_records.extend(data)\n\n        df = pd.DataFrame(all_records)\n        super().__init__(df)\n"}
{"type": "source_file", "path": "rl/pufferlib/dashboard/dashboards.py", "content": "\nfrom rl.pufferlib.dashboard.logs import Logs\nfrom rl.pufferlib.dashboard.policy import Policy\nfrom rl.pufferlib.dashboard.training import Training\nfrom rl.pufferlib.dashboard.utilization import Utilization\nfrom rl.pufferlib.dashboard.carbs import Carbs\nfrom rl.pufferlib.dashboard.wandb import WanDb\nfrom rl.pufferlib.dashboard.dashboard import Dashboard\nfrom rl.pufferlib.train import PufferTrainer\nfrom rl.carbs.carbs_controller import CarbsController\n\nfrom omegaconf import OmegaConf\n\ndef train_dashboard(trainer: PufferTrainer):\n    return Dashboard(trainer.cfg, components=[\n        Utilization(),\n        WanDb(trainer.cfg.wandb),\n        Training(trainer),\n        Policy(trainer.policy_checkpoint),\n        Logs(logs_path),\n    ])\n\ndef eval_dashboard(cfg: OmegaConf, stats: Dict):\n    return Dashboard(cfg, components=[\n        Utilization(),\n        WanDb(cfg.wandb),\n        Eval(stats),\n    ])\n\ndef sweep_dashboard(cfg: OmegaConf, carbs_controller: CarbsController):\n    return Dashboard(cfg, components=[\n        Utilization(),\n        WanDb(cfg.wandb),\n        Carbs(carbs_controller),\n        Logs(logs_path),\n    ])\n\n"}
{"type": "source_file", "path": "devops/wandb/project.py", "content": "import argparse\nimport yaml\nimport wandb\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--wandb_user', required=True)\n    parser.add_argument('--wandb_project', required=True)\n    parser.add_argument('--yaml_path', required=True)\n    parser.add_argument('--load', action='store_true')\n    parser.add_argument('--save', action='store_true')\n\n    args = parser.parse_args()\n\n    api = wandb.Api()\n    run = api.runs(f\"{args.wandb_user}/{args.wandb_project}\")\n\n    runs = api.runs(f\"{args.wandb_user}/{args.wandb_project}\")\n    for run in runs:\n        if args.load:\n            with open(args.yaml_path, 'r') as f:\n                view = yaml.safe_load(f)\n                run.config.update(view)\n\n        if args.save:\n            view = dict(run.config)\n            with open(args.yaml_path, 'w') as f:\n                yaml.safe_dump(view, f)\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "env/wrapper/reward_tracker.py", "content": "from functools import lru_cache\nimport gymnasium as gym\nimport numpy as np\n\nclass RewardTracker(gym.Wrapper):\n    def __init__(self, env):\n        super(RewardTracker, self).__init__(env)\n        self._last_rewards = None\n\n    def reset(self, **kwargs):\n        self._last_rewards = np.zeros(self.unwrapped.player_count, dtype=np.float32)\n        obs, infos = self.env.reset(**kwargs)\n        return self._augment_observations(obs), infos\n\n    def step(self, actions):\n        obs, rewards, terms, truncs, infos = self.env.step(actions)\n        self._last_rewards = rewards\n\n        return self._augment_observations(obs), rewards, terms, truncs, infos\n\n    def _augment_observations(self, obs):\n        return [{\n            \"last_reward\": np.array(self._last_rewards[agent]),\n            **agent_obs\n        } for agent, agent_obs in enumerate(obs)]\n\n    @property\n    def observation_space(self):\n        return gym.spaces.Dict({\n            \"last_reward\": gym.spaces.Box(\n                -np.inf, high=np.inf,\n                shape=(1,), dtype=np.float32\n            ),\n            **self.env.observation_space.spaces\n        })\n"}
{"type": "source_file", "path": "env/wrapper/replay.py", "content": "import torch\nimport os\n\nclass Replay():\n    def __init__(self, filename):\n        self.filename = filename\n        self.data = {\n            \"steps\": [],\n            \"global_obs\": [],\n        }\n\n    def record_step(self, actions, obs, rewards, infos, global_obs):\n        self.data[\"global_obs\"].append(global_obs)\n\n    def close(self):\n        print(f\"Saving replay to {self.filename}\")\n        os.makedirs(os.path.dirname(self.filename), exist_ok=True)\n        torch.save(self.data, self.filename)\n\n    @staticmethod\n    def load(filename):\n        data = torch.load(filename, weights_only=False)\n        r = Replay(filename)\n        r.data = data\n        return r\n"}
{"type": "source_file", "path": "env/wrapper/last_action_tracker.py", "content": "from functools import lru_cache\nimport re\nimport gymnasium as gym\nimport numpy as np\n\nclass LastActionTracker(gym.Wrapper):\n    def __init__(self, env):\n        super(LastActionTracker, self).__init__(env)\n        self._last_actions = None\n\n    def reset(self, **kwargs):\n        self._last_actions = np.zeros((self.unwrapped.player_count, 2), dtype=np.int32)\n        obs, infos = self.env.reset(**kwargs)\n        return self._augment_observations(obs), infos\n\n\n    def step(self, actions):\n        obs, rewards, terms, truncs, infos = self.env.step(actions)\n        self._last_actions = actions\n\n        return self._augment_observations(obs), rewards, terms, truncs, infos\n\n    def _augment_observations(self, obs):\n        return [{\n            \"last_action\": self._last_actions[agent],\n            **agent_obs\n        } for agent, agent_obs in enumerate(obs)]\n\n\n    @property\n    def observation_space(self):\n        return gym.spaces.Dict({\n            \"last_action\": gym.spaces.Box(\n                low=0, high=255, shape=(2,), dtype=np.int32\n            ),\n            **self.env.observation_space.spaces\n        })\n"}
{"type": "source_file", "path": "rl/pufferlib/dashboard/carbs.py", "content": "\nimport numpy as np\nfrom rich.table import Table\nfrom rl.carbs.carbs_controller import CarbsController\nfrom .dashboard import DashboardComponent\nfrom .dashboard import c1, b1, c2, b2, c3, ROUND_OPEN\nfrom .training import Training\nfrom .dashboard import abbreviate\n\nclass Carbs(DashboardComponent):\n    def __init__(self, carbs_controller: CarbsController):\n        super().__init__()\n        self.carbs = carbs_controller\n\n    def render(self):\n        c = Table(box=ROUND_OPEN, pad_edge=False)\n        c.add_column(f\"{c1}Carbs\", justify='left', vertical='top')\n        c.add_column(f\"{c1}{self.carbs._stage}\", justify='right', vertical='top')\n        c.add_row(f'{c2}Suggestions', abbreviate(self.carbs._num_suggestions))\n        c.add_row(f'{c2}Observations', abbreviate(self.carbs._num_observations))\n        c.add_row(f'{c2}Failures', abbreviate(self.carbs._num_failures))\n        if self.carbs._last_rollout_result:\n            r = self.carbs._last_rollout_result\n            c.add_row(f'{c1}Last Observation')\n            c.add_row(f'{c2}Score', abbreviate(r[\"score\"]))\n            c.add_row(f'{c2}Rollout Time', abbreviate(r[\"rollout_time\"]))\n            c.add_row(f'{c2}Train Time', abbreviate(r[\"train_time\"]))\n            c.add_row(f'{c2}Eval Time', abbreviate(r[\"eval_time\"]))\n\n        table = Table(box=ROUND_OPEN, expand=True, pad_edge=False)\n        components = []\n        if self.carbs._trainer:\n            components.append(Training(self.carbs._trainer).render())\n        components.append(c)\n\n        table.add_row(*components)\n\n        return table\n"}
{"type": "source_file", "path": "rl/pufferlib/dashboard/__init__.py", "content": ""}
{"type": "source_file", "path": "devops/vast/do.py", "content": "#!/usr/bin/env python3\nimport argparse\nimport subprocess\nimport os\nimport json\nimport time\nimport shlex\nfrom typing import List\n\ndef gen_search_cmd(args):\n    \"\"\" Generate the search command that matches the given criteria. \"\"\"\n    cmd_str = f'vastai search offers \\\n        \"num_gpus={args.num_gpus}\" \\\n        \"cpu_cores_effective>{args.min_cpu_cores}\" \\\n        \"inet_down>{args.min_inet}\" \\\n        \"inet_up>{args.min_inet}\" \\\n        \"cuda_vers>={args.min_cuda}\" \\\n        \"geolocation={args.geo}\" \\\n        \"gpu_name={args.gpu_name}\" \\\n        \"rented=False\" \\\n        \"dph<{args.max_dph}\" \\\n        -o dph-'\n    return cmd_str\n\ndef search_command(args):\n    \"\"\" Search for machines that match the given criteria. \"\"\"\n    cmd_str = gen_search_cmd(args)\n    subprocess.run(cmd_str, shell=True, check=True)\n\ndef rent_command(args):\n    \"\"\" Rent a machine with the given label. \"\"\"\n    print(f\"Renting with label: {args.label}\")\n    cmd_str = gen_search_cmd(args)\n    print(cmd_str)\n\n    output = subprocess.run(\n        cmd_str,\n        shell=True,\n        check=True,\n        capture_output=True,\n        text=True\n    )\n    try:\n        server_id = output.stdout.splitlines()[1].split()[0]\n    except IndexError:\n        quit(\"No server found\")\n\n    cmd_str = f\"vastai create instance {server_id} \\\n        --image {args.image} \\\n        --disk 60 \\\n        --onstart-cmd '/bin/bash' \\\n        --label {args.label} \\\n        --ssh --direct \\\n        --args --ulimit nofile=unlimited --ulimit nproc=unlimited\"\n    subprocess.run(cmd_str, shell=True, check=True)\n\ndef label_to_instance(label):\n    \"\"\" Get the instance from the label. \"\"\"\n    output = subprocess.run(\n        \"vastai show instances --raw\",\n        shell=True,\n        check=True,\n        capture_output=True,\n        text=True\n    )\n    instances = json.loads(output.stdout)\n    for instance in instances:\n        if instance['label'] == label:\n            return instance\n    raise Exception(f\"Instance {label} not found\")\n\ndef label_to_id(label):\n    \"\"\" Get the instance ID from the label. \"\"\"\n    return label_to_instance(label)['id']\n\ndef kill_command(args):\n    \"\"\" Destroy a machine with the given label. \"\"\"\n    cmd_str = f\"vastai destroy instance {label_to_id(args.label)}\"\n    subprocess.run(cmd_str, shell=True, check=True)\n\ndef get_str(dict, value):\n    \"\"\" Get display string even if missing or None. \"\"\"\n    return str(dict.get(value)).strip()\n\ndef show_command(args):\n    \"\"\" Show all current instances. \"\"\"\n    cmd_str = \"vastai show instances --raw\"\n    output = subprocess.run(\n        cmd_str,\n        shell=True,\n        check=True,\n        capture_output=True,\n        text=True\n    )\n    instances = json.loads(output.stdout)\n    for instance in instances:\n        print(f\"{instance['label']}\"\n          f\" status:{get_str(instance, 'actual_status')}\"\n          f\" CPU:{get_str(instance, 'cpu_util')}%\"\n          f\" GPU:{get_str(instance, 'gpu_util')}%\"\n          f\" [{get_str(instance, 'status_msg')}]\"\n          f\" root@{instance['ssh_host']}:{instance['ssh_port']}\"\n        )\n\ndef wait_for_ready(label):\n    \"\"\" Wait for a machine with the given label to become ready. \"\"\"\n    instance = label_to_instance(label)\n    # Wait for the instance to become ready.\n    while instance['actual_status'] != 'running':\n        print(\n          f\"Waiting for instance {label} to become ready... \"\n          f\"({instance['actual_status']})\"\n        )\n        time.sleep(5)\n        instance = label_to_instance(label)\n\ndef wait_for_ssh(label):\n    # Wait for the SSH key on the server to be ready.\n    instance = label_to_instance(label)\n    ssh_host = instance['ssh_host']\n    ssh_port = instance['ssh_port']\n    cmd = f\"ssh -o StrictHostKeyChecking=no -p {ssh_port} root@{ssh_host} 'echo 1'\"\n    while True:\n      try:\n        subprocess.run(cmd, shell=True, check=True)\n        break\n      except Exception as e:\n        print(f\"Waiting for instance {label} to become ready... {e}\")\n        time.sleep(5)\n\ndef ssh_command(args):\n    \"\"\" SSH into a machine with the given label. \"\"\"\n    wait_for_ready(args.label)\n    instance = label_to_instance(args.label)\n    ssh_host = instance['ssh_host']\n    ssh_port = instance['ssh_port']\n    cmd = f\"ssh -o StrictHostKeyChecking=no -p {ssh_port} root@{ssh_host}\"\n    subprocess.run(cmd, shell=True, check=True)\n\ndef screen_command(args):\n    \"\"\" Like SSH, but starts or attaches to a screen session \"\"\"\n    wait_for_ready(args.label)\n    instance = label_to_instance(args.label)\n    ssh_host = instance['ssh_host']\n    ssh_port = instance['ssh_port']\n    cmd = f\"ssh -t -o \\\n      StrictHostKeyChecking=no -p {ssh_port} \\\n      root@{ssh_host} \\\n      'cd /workspace/metta && screen -Rq'\"\n    print(\"Use ^A-D to detach from screen session.\")\n    subprocess.run(cmd, shell=True, check=True)\n\ndef tmux_command(args):\n    \"\"\" Like SSH, but starts or attaches to a tmux session \"\"\"\n    wait_for_ready(args.label)\n    instance = label_to_instance(args.label)\n    ssh_host = instance['ssh_host']\n    ssh_port = instance['ssh_port']\n    cmd = f\"ssh -t -o \\\n      StrictHostKeyChecking=no -p {ssh_port} \\\n      root@{ssh_host} \\\n      'cd /workspace/metta && tmux new-session -A -s metta'\"\n    print(\"Use ^B-D to detach from tmux session.\")\n    subprocess.run(cmd, shell=True, check=True)\n\ndef setup_command(args):\n    \"\"\" Setup the machine for training. \"\"\"\n    wait_for_ready(args.label)\n    instance = label_to_instance(args.label)\n    ssh_host = instance['ssh_host']\n    ssh_port = instance['ssh_port']\n\n    # Add local SSH key.\n    ssh_keys = [\n        \"id_rsa.pub\",\n        \"id_ed25519.pub\",\n    ]\n    for key in ssh_keys:\n        key_path = os.path.expanduser(f\"~/.ssh/{key}\")\n        if os.path.exists(key_path):\n          print(f\"Adding ssh key {key_path}\")\n          ssh_key = open(key_path).read()\n          cmd_attach = f\"vastai attach ssh {instance['id']} '\" + ssh_key + \"'\"\n          subprocess.run(cmd_attach, shell=True, check=True)\n\n    wait_for_ssh(args.label)\n\n    cmd_setup = [\n      \"cd /workspace/metta\",\n      \"git config --global --add safe.directory /workspace/metta\"\n    ]\n    if args.clean or args.branch != 'main':\n      cmd_setup.append(\"git reset --hard\")\n      cmd_setup.append(\"git clean -fdx\")\n      cmd_setup.append(f\"git fetch origin {args.branch}\")\n      cmd_setup.append(f\"git checkout {args.branch}\")\n      cmd_setup.append(\"git pull\")\n    cmd_setup.extend([\n      \"pip install -r requirements.txt\",\n      \"bash devops/setup_build.sh\"\n    ])\n    cmd = (\n      f\"ssh -t -o \"\n      f\"StrictHostKeyChecking=no -p {ssh_port} \"\n      f\"root@{ssh_host} \"\n      f\"'{' && '.join(cmd_setup)}'\"\n    )\n    subprocess.run(cmd, shell=True, check=True)\n    # Copy the .netrc file\n    scp_cmd = (\n      f\"scp -P {ssh_port} $HOME/.netrc root@{ssh_host}:/root/.netrc\"\n    )\n    subprocess.run(scp_cmd, shell=True, check=True)\n\ndef rsync_command(args):\n    \"\"\" Rsync a working directory to a machine. \"\"\"\n    wait_for_ready(args.label)\n    instance = label_to_instance(args.label)\n    ssh_host = instance['ssh_host']\n    ssh_port = instance['ssh_port']\n    # Walk dirs and recursively rsync only .py, .pyd, .pyx .yaml, and .sh files.\n    # Current folder to /workspace/metta.\n    # Show files transferred.\n    cmd = (\n        f\"rsync -avz -e 'ssh -p {ssh_port}' --progress \"\n        f\"--include='*/' \"  # Include all directories\n        f\"--include='**/*.py' --include='**/*.pyd' --include='**/*.pyx' \"\n        f\"--include='**/*.yaml' --include='**/*.sh' --exclude='*' \"\n        f\"./ root@{ssh_host}:/workspace/metta\"\n    )\n    subprocess.run(cmd, shell=True, check=True)\n\ndef send_keys(process, keys):\n    \"\"\"Send keyboard input to a process\"\"\"\n    process.stdin.write(f\"{keys}\\n\".encode())\n    process.stdin.flush()\n\ndef main():\n    \"\"\" Swiss Army Knife for vast.ai. \"\"\"\n\n    parser = argparse.ArgumentParser(description='VAST CLI tool')\n    subparsers = parser.add_subparsers(\n        dest='command',\n        help='Available commands'\n    )\n\n    search_parser = subparsers.add_parser(\n      'search',\n      help='Search for machines'\n    )\n    search_parser.add_argument(\n      '--num-gpus',\n      type=int,\n      default=1,\n      help='Number of GPUs'\n    )\n    search_parser.add_argument(\n      '--min-cpu-cores',\n      type=int,\n      default=8,\n      help='Minimum CPU cores'\n    )\n    search_parser.add_argument(\n      '--min-inet',\n      type=int,\n      default=100,\n      help='Minimum internet speed (up/down) in Mbps'\n    )\n    search_parser.add_argument(\n      '--min-cuda',\n      type=str,\n      default='12.1',\n      help='Minimum CUDA version'\n    )\n    search_parser.add_argument(\n      '--geo',\n      type=str,\n      default='US',\n      help='Geolocation'\n    )\n    search_parser.add_argument(\n      '--gpu-name',\n      type=str,\n      default='RTX_4090',\n      help='GPU name'\n    )\n    search_parser.add_argument(\n      '--max-dph',\n      type=int,\n      default=10,\n      help='Maximum daily price in dollars'\n    )\n\n    rent_parser = subparsers.add_parser(\n      'rent',\n      help='Rent a machine'\n    )\n    rent_parser.add_argument(\n      'label',\n      type=str,\n      help='Label for the instance'\n    )\n    rent_parser.add_argument(\n      '--image',\n      type=str,\n      default='mettaai/metta:latest',\n      help='Image to use'\n    )\n    rent_parser.add_argument(\n      '--num-gpus',\n      type=int,\n      default=1,\n      help='Number of GPUs'\n    )\n    rent_parser.add_argument(\n      '--min-cpu-cores',\n      type=int,\n      default=8,\n      help='Minimum CPU cores'\n    )\n    rent_parser.add_argument(\n      '--min-inet',\n      type=int,\n      default=100,\n      help='Minimum internet speed (up/down) in Mbps'\n    )\n    rent_parser.add_argument(\n      '--min-cuda',\n      type=str,\n      default='12.1',\n      help='Minimum CUDA version'\n    )\n    rent_parser.add_argument(\n      '--geo',\n      type=str,\n      default='US',\n      help='Geolocation'\n    )\n    rent_parser.add_argument(\n      '--gpu-name',\n      type=str,\n      default='RTX_4090',\n      help='GPU name'\n    )\n    rent_parser.add_argument(\n      '--max-dph',\n      type=int,\n      default=1,\n      help='Maximum daily price in dollars'\n    )\n\n    kill_parser = subparsers.add_parser(\n      'kill',\n      help='Destroy a machine'\n    )\n    kill_parser.add_argument(\n      'label',\n      type=str,\n      help='Instance ID'\n    )\n\n    show_parser = subparsers.add_parser(\n      'show',\n      help='Show a machine'\n    )\n\n    ssh_parser = subparsers.add_parser(\n      'ssh',\n      help='SSH into a machine'\n    )\n    ssh_parser.add_argument(\n      'label',\n      type=str,\n      help='Instance ID'\n    )\n\n    screen_parser = subparsers.add_parser(\n      'screen',\n      help='SSH into machine and start or attach to existing screen session'\n    )\n    screen_parser.add_argument('label', type=str, help='Instance ID')\n\n    tmux_parser = subparsers.add_parser(\n      'tmux',\n      help='SSH into machine and open start or attach to existing tmux session'\n    )\n    tmux_parser.add_argument(\n      'label',\n      type=str,\n      help='Instance ID'\n    )\n\n    setup_parser = subparsers.add_parser(\n      'setup',\n      help='Setup a machine'\n    )\n    setup_parser.add_argument(\n      'label',\n      type=str,\n      help='Instance ID'\n    )\n    setup_parser.add_argument(\n      '--clean',\n      type=bool,\n      default=True,\n      help='Cleans the workspace before setup'\n    )\n    setup_parser.add_argument(\n      '--branch',\n      type=str,\n      default='main',\n      help='Git branch to checkout'\n    )\n\n    rsync_parser = subparsers.add_parser(\n      'rsync',\n      help='Rsync a working directory to a machine'\n    )\n    rsync_parser.add_argument('label', type=str, help='Instance ID')\n\n    args = parser.parse_args()\n    if args.command == 'search':\n        search_command(args)\n    elif args.command == 'rent':\n        rent_command(args)\n    elif args.command == 'kill':\n        kill_command(args)\n    elif args.command == 'show':\n        show_command(args)\n    elif args.command == 'ssh':\n        ssh_command(args)\n    elif args.command == 'screen':\n        screen_command(args)\n    elif args.command == 'tmux':\n        tmux_command(args)\n    elif args.command == 'setup':\n        setup_command(args)\n    elif args.command == 'rsync':\n        rsync_command(args)\n    else:\n        parser.print_help()\n\nif __name__ == '__main__':\n    main()\n"}
{"type": "source_file", "path": "rl/eval/queries.py", "content": "from omegaconf import OmegaConf\nfrom typing import Dict, Any\n\ndef all_fields():\n    return \"SELECT * FROM eval_data\"\n\ndef total_metric(metric_field: str, filters: Dict[str, Any]):\n    where_clause = build_where_clause(filters)\n    query = f\"\"\"\n        SELECT\n            episode_index,\n            policy_name,\n            eval_name,\n            SUM(CAST(\"{metric_field}\" AS DOUBLE)) AS total_metric\n        FROM eval_data\n        {where_clause}\n        GROUP BY episode_index, policy_name, eval_name\n        ORDER BY episode_index, policy_name, eval_name;\"\"\"\n\n    return query\n\ndef build_where_clause(filters: Dict[str, Any]) -> str:\n    \"\"\"Build WHERE clause from a filters dictionary.\"\"\"\n    if not filters:\n        return \"\"\n    conditions = []\n\n    # Convert OmegaConf objects to plain Python types if necessary.\n    if OmegaConf.is_config(filters):\n        filters = OmegaConf.to_container(filters, resolve=True)\n\n    for field, value in filters.items():\n        # If field names contain dots, wrap them in quotes.\n        if OmegaConf.is_config(value):\n            value = OmegaConf.to_container(value, resolve=True)\n        if '.' in field and not field.startswith('\"'):\n            field = f'\"{field}\"'\n        if isinstance(value, (list, tuple)):\n            formatted_values = [f\"'{v}'\" if isinstance(v, str) else str(v) for v in value]\n            conditions.append(f\"{field} IN ({', '.join(formatted_values)})\")\n        elif isinstance(value, str):\n            value = value.strip()\n            if value.startswith(('>', '<', '=', '!=', '>=', '<=', 'IN', 'BETWEEN', 'IS')):\n                conditions.append(f\"{field} {value}\")\n            else:\n                conditions.append(f\"{field} = '{value}'\")\n        else:\n            conditions.append(f\"{field} = {value}\")\n    return f\"WHERE {' AND '.join(conditions)}\" if conditions else \"\"\n"}
{"type": "source_file", "path": "env/wrapper/kinship.py", "content": "import math\nimport gymnasium as gym\nimport numpy as np\n\nclass Kinship(gym.Wrapper):\n    def __init__(self, team_size: int, team_reward: float, env: gym.Env):\n        super().__init__(env)\n        self._team_size = team_size\n        self._team_reward = team_reward\n        self._num_agents = self.env.unwrapped.player_count\n        self._num_teams = int(math.ceil(self._num_agents / self._team_size))\n\n        self._agent_team = np.array([\n            agent // self._team_size for agent in range(self._num_agents)])\n        self._team_to_agents = {\n            team: np.array([\n                agent for agent in range(self._num_agents)\n                if self._agent_team[agent] == team\n            ]) for team in range(self._num_teams)\n        }\n\n        self._agent_id_feature_idx = self.env.unwrapped.grid_features.index(\"agent:id\")\n\n        grid_shape = self.env.unwrapped.observation_space[\"grid_obs\"].shape\n        self._kinship_shape = (1, grid_shape[1], grid_shape[2])\n\n    def reset(self, **kwargs):\n        obs, infos = self.env.reset(**kwargs)\n        return self._augment_observations(obs), infos\n\n    def step(self, actions):\n        obs, rewards, terms, truncs, infos = self.env.step(actions)\n        rewards = np.array(rewards)\n        agent_idxs = rewards.nonzero()[0]\n\n        if len(agent_idxs) > 0:\n            team_rewards = np.zeros(self._num_teams)\n            for agent in agent_idxs:\n                team = self._agent_team[agent]\n                team_rewards[team] += self._team_reward * rewards[agent]\n                rewards[agent] -= self._team_reward * rewards[agent]\n            team_idxs = team_rewards.nonzero()[0]\n            for team in team_idxs:\n                team_agents = self._team_to_agents[team]\n                team_reward = team_rewards[team] / len(team_agents)\n                rewards[team_agents] += team_reward\n\n        return self._augment_observations(obs), rewards, terms, truncs, infos\n\n    def _team_id_obs(self, agent_obs):\n        agent_id_obs = agent_obs[\"grid_obs\"][self._agent_id_feature_idx]\n        idxs = agent_id_obs.nonzero()\n        agent_ids = agent_id_obs[idxs] - 1\n        team_ids = self._agent_team[agent_ids]\n        team_id_obs = np.zeros_like(agent_id_obs)\n        team_id_obs[idxs] = team_ids + 1\n        return team_id_obs\n\n    def _augment_observations(self, obs):\n        return [{\n            \"kinship\": self._team_id_obs(agent_obs),\n            **agent_obs\n        } for agent, agent_obs in enumerate(obs)]\n\n    @property\n    def observation_space(self):\n        return gym.spaces.Dict({\n            \"kinship\": gym.spaces.Box(\n                -np.inf, high=np.inf,\n                shape=self._kinship_shape, dtype=np.float32\n            ),\n            **self.env.observation_space.spaces\n        })\n"}
{"type": "source_file", "path": "rl/eval/eval_stats_logger.py", "content": "import os\nimport json\nfrom datetime import datetime\nimport wandb\nfrom util.datastruct import flatten_config\nimport logging\nimport gzip\nfrom omegaconf import OmegaConf\nlogger = logging.getLogger(\"eval_stats_logger.py\")\n\nclass EvalStatsLogger:\n    def __init__(self, cfg, env_cfg, wandb_run, save_path: str=None):\n        self._cfg = cfg\n        self._env_cfg = env_cfg\n        self._wandb_run = wandb_run\n        # We want local stats dir to be the same for train, analysis and eval for a particular run\n        save_dir = (save_path or cfg.run_dir).replace(\"analyze\", \"train\").replace(\"eval\", \"train\")\n\n        artifact_name = None\n        if cfg.eval.eval_db_uri is None:\n            json_path = os.path.join(save_dir, \"eval_stats\")\n        elif cfg.eval.eval_db_uri.startswith(\"wandb://\"):\n            artifact_name = cfg.eval.eval_db_uri.split(\"/\")[-1]\n            json_path = os.path.join(save_dir, \"eval_stats\")\n        elif cfg.eval.eval_db_uri.startswith(\"file://\"):\n            json_path = cfg.eval.eval_db_uri.split(\"file://\")[1]\n        else:\n            if \"://\" in cfg.eval.eval_db_uri:\n                raise ValueError(f\"Invalid eval_db_uri: {cfg.eval.eval_db_uri}\")\n            json_path = cfg.eval.eval_db_uri\n\n        self.json_path = json_path if json_path.endswith('.json') else  f\"{json_path}.json\"\n        os.makedirs(os.path.dirname(self.json_path), exist_ok=True)\n        self.artifact_name = artifact_name\n\n    def _add_additional_fields(self, eval_stats, eval_name = \"eval\"):\n        additional_fields = {}\n        additional_fields['run_id'] = self._cfg.get(\"run_id\", self._wandb_run.id)\n        additional_fields['eval_name'] = eval_name or self._cfg.eval.get(\"name\", None)\n        if self._cfg.eval.npc_policy_uri is not None:\n            additional_fields['npc'] = self._cfg.eval.npc_policy_uri\n        additional_fields['timestamp'] = datetime.now().isoformat()\n\n        # Convert the environment configuration to a dictionary and flatten it.\n        game_cfg = OmegaConf.to_container(self._env_cfg.game, resolve=False)\n        flattened_env = flatten_config(game_cfg, parent_key = \"game\")\n        additional_fields.update(flattened_env)\n\n        for episode in eval_stats:\n            for record in episode:\n                record.update(additional_fields)\n\n        return eval_stats\n\n    def _log_to_file(self, eval_stats):\n        # If file exists, load and merge with existing data\n        gzip_path = self.json_path + \".gz\"\n        if os.path.exists(gzip_path):\n            try:\n                logger.info(f\"Loading existing eval stats from {gzip_path}\")\n                with gzip.open(gzip_path, \"rt\", encoding='utf-8') as f:\n                    existing_stats = json.load(f)\n                eval_stats.extend(existing_stats)\n            except Exception as e:\n                logger.error(f\"Error loading existing eval stats from {gzip_path}: {e}, will overwrite\")\n\n        with gzip.open(gzip_path, \"wt\", encoding='utf-8') as f:\n            json.dump(eval_stats, f, indent=4)\n        logger.info(f\"Saved eval stats to {gzip_path}\")\n\n    def _log_to_wandb(self, artifact_name: str, eval_stats):\n        artifact = wandb.Artifact(name=artifact_name, type=artifact_name)\n        zip_file_path = self.json_path + \".gz\"\n        with gzip.open(zip_file_path, 'wt', encoding='utf-8') as f:\n            json.dump(eval_stats, f)\n        artifact = wandb.Artifact(name=artifact_name, type=artifact_name)\n        artifact.add_file(zip_file_path)\n        artifact.save()\n        artifact.wait()\n        self._wandb_run.log_artifact(artifact)\n        logger.info(f\"Logged artifact {artifact_name} to wandb\")\n\n    def log(self, eval_stats):\n\n        # If we are running eval suite, we need to add additional fields for each eval\n        # since the eval_name is different\n        if isinstance(eval_stats, dict):\n            eval_suite_stats = []\n            for eval_name, stats in eval_stats.items():\n                self._add_additional_fields(stats, eval_name = eval_name)\n                eval_suite_stats.extend(stats)\n            eval_stats = eval_suite_stats\n        else:\n            self._add_additional_fields(eval_stats)\n\n        self._log_to_file(eval_stats)\n        if self.artifact_name is not None:\n            self._log_to_wandb(self.artifact_name, eval_stats)\n\n        return eval_stats\n"}
{"type": "source_file", "path": "devops/aws/batch/task_logs.py", "content": "#!/usr/bin/env python3\nimport argparse\nimport boto3\nimport re\nimport sys\nimport time\nfrom datetime import datetime, timedelta\n\n# Import functions from the job module\nfrom devops.aws.batch.job import (\n    get_job_details,\n    get_job_attempts,\n    get_job_log_streams,\n    find_alternative_log_streams,\n    get_log_events,\n    print_job_logs,\n    format_time_difference\n)\n\ndef get_batch_job_queues():\n    \"\"\"Get a list of all AWS Batch job queues.\"\"\"\n    batch = boto3.client('batch')\n    response = batch.describe_job_queues()\n    return [queue['jobQueueName'] for queue in response['jobQueues']]\n\ndef list_recent_jobs(job_queue=None, max_jobs=10, interactive=False, debug=False):\n    \"\"\"\n    List recent jobs and optionally allow the user to select one to view logs.\n\n    Args:\n        job_queue (str): The job queue to list jobs from\n        max_jobs (int): The maximum number of jobs to list\n        interactive (bool): Whether to allow the user to select a job\n        debug (bool): Whether to show debug information\n\n    Returns:\n        str: The selected job ID if interactive is True, None otherwise\n    \"\"\"\n    jobs = get_job_details(job_prefix=None, job_queue=job_queue, max_jobs=max_jobs)\n\n    if not jobs:\n        print(f\"No recent jobs found\" + (f\" in queue {job_queue}\" if job_queue else \"\"))\n        return None\n\n    print(f\"\\nRecent jobs\" + (f\" in queue {job_queue}\" if job_queue else \"\") + \":\")\n    print(f\"{'#':<3} {'Name':<30} {'ID':<36} {'Status':<10} {'Age':<10} {'Created At'}\")\n    print(\"-\" * 100)\n\n    for i, job in enumerate(jobs):\n        job_id = job.get('jobId', 'Unknown')\n        job_name = job.get('jobName', 'Unknown')\n        status = job.get('status', 'Unknown')\n        created_at = job.get('createdAt', 0)\n\n        # Format the created_at timestamp\n        if created_at:\n            created_at_str = datetime.fromtimestamp(created_at / 1000).strftime('%Y-%m-%d %H:%M:%S')\n            age = format_time_difference(created_at / 1000)\n        else:\n            created_at_str = 'Unknown'\n            age = 'Unknown'\n\n        print(f\"{i+1:<3} {job_name[:30]:<30} {job_id:<36} {status:<10} {age:<10} {created_at_str}\")\n\n    if interactive:\n        while True:\n            try:\n                choice = input(\"\\nEnter job number to view logs (or press Enter to exit): \")\n                if not choice:\n                    return None\n\n                choice = int(choice)\n                if 1 <= choice <= len(jobs):\n                    selected_job = jobs[choice - 1]\n                    job_id = selected_job.get('jobId')\n\n                    # Show logs for the selected job\n                    print_job_logs(job_id, tail=False, debug=debug)\n\n                    return job_id\n                else:\n                    print(f\"Invalid choice. Please enter a number between 1 and {len(jobs)}\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n            except KeyboardInterrupt:\n                print(\"\\nOperation cancelled.\")\n                return None\n\n    return None\n\ndef show_job_logs(job, tail=False, latest_attempt=False, attempt_index=None, node_index=None, debug=False):\n    \"\"\"\n    Show logs for a specific job.\n\n    Args:\n        job (str): The job ID or name to show logs for\n        tail (bool): Whether to continuously poll for new logs\n        latest_attempt (bool): Whether to show logs only for the latest attempt\n        attempt_index (int): The specific attempt index to show logs for\n        node_index (int): The specific node index to show logs for (for multi-node jobs)\n        debug (bool): Whether to show debug information\n    \"\"\"\n    # Check if the input is a job ID or job name\n    if re.match(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', job):\n        # Input is a job ID\n        job_id = job\n    else:\n        # Input is a job name, try to find the job ID\n        jobs = get_job_details(job_prefix=job)\n        if not jobs:\n            print(f\"No job found with name or ID matching '{job}'\")\n            return\n\n        if len(jobs) > 1:\n            print(f\"Multiple jobs found with name or ID matching '{job}':\")\n            for i, j in enumerate(jobs):\n                job_id = j.get('jobId', 'Unknown')\n                job_name = j.get('jobName', 'Unknown')\n                status = j.get('status', 'Unknown')\n                print(f\"{i+1}. {job_name} (ID: {job_id}, Status: {status})\")\n\n            while True:\n                try:\n                    choice = input(\"\\nEnter job number to view logs (or press Enter to exit): \")\n                    if not choice:\n                        return\n\n                    choice = int(choice)\n                    if 1 <= choice <= len(jobs):\n                        job_id = jobs[choice - 1].get('jobId')\n                        break\n                    else:\n                        print(f\"Invalid choice. Please enter a number between 1 and {len(jobs)}\")\n                except ValueError:\n                    print(\"Invalid input. Please enter a number.\")\n                except KeyboardInterrupt:\n                    print(\"\\nOperation cancelled.\")\n                    return\n        else:\n            job_id = jobs[0].get('jobId')\n\n    # Get job attempts\n    attempts = get_job_attempts(job_id)\n\n    # Determine which attempt to show logs for\n    attempt_num = None\n\n    if latest_attempt and attempts:\n        # Show logs for the latest attempt\n        attempt_num = len(attempts) - 1\n    elif attempt_index is not None:\n        # Show logs for the specified attempt\n        if 0 <= attempt_index < len(attempts):\n            attempt_num = attempt_index\n        else:\n            print(f\"Invalid attempt index {attempt_index}. Job has {len(attempts)} attempts (0-{len(attempts)-1})\")\n            return\n    elif attempts and len(attempts) > 1:\n        # If there are multiple attempts and no specific one was requested, ask the user\n        print(f\"\\nJob has {len(attempts)} attempts:\")\n        for i, attempt in enumerate(attempts):\n            container = attempt.get('container', {})\n            exit_code = container.get('exitCode', 'N/A')\n            reason = container.get('reason', 'N/A')\n\n            # Format timestamps\n            started_at = attempt.get('startedAt')\n            if started_at:\n                started_at_str = datetime.fromtimestamp(started_at / 1000).strftime('%Y-%m-%d %H:%M:%S')\n            else:\n                started_at_str = 'N/A'\n\n            stopped_at = attempt.get('stoppedAt')\n            if stopped_at:\n                stopped_at_str = datetime.fromtimestamp(stopped_at / 1000).strftime('%Y-%m-%d %H:%M:%S')\n                duration = format_time_difference(started_at / 1000, stopped_at / 1000) if started_at else 'N/A'\n            else:\n                stopped_at_str = 'N/A'\n                duration = 'Running' if started_at else 'N/A'\n\n            print(f\"Attempt {i+1}: Started: {started_at_str}, Stopped: {stopped_at_str}, Duration: {duration}, Exit Code: {exit_code}, Reason: {reason}\")\n\n        while True:\n            try:\n                choice = input(\"\\nEnter attempt number to view logs (or press Enter to view all attempts): \")\n                if not choice:\n                    # View all attempts\n                    break\n\n                choice = int(choice)\n                if 1 <= choice <= len(attempts):\n                    attempt_num = choice - 1\n                    break\n                else:\n                    print(f\"Invalid choice. Please enter a number between 1 and {len(attempts)}\")\n            except ValueError:\n                print(\"Invalid input. Please enter a number.\")\n            except KeyboardInterrupt:\n                print(\"\\nOperation cancelled.\")\n                return\n\n    # Show logs for the job\n    print_job_logs(job_id, attempt_index=attempt_num, node_index=node_index, tail=tail, debug=debug)\n\ndef main():\n    \"\"\"Main entry point for the script.\"\"\"\n    parser = argparse.ArgumentParser(description='View logs for AWS Batch jobs')\n    parser.add_argument('--job', help='Job ID or name to view logs for')\n    parser.add_argument('--queue', default='metta-jq', help='Job queue to list jobs from')\n    parser.add_argument('--tail', action='store_true', help='Continuously poll for new logs')\n    parser.add_argument('--max-jobs', type=int, default=10, help='Maximum number of jobs to list')\n    parser.add_argument('--latest', action='store_true', help='Show logs for the latest attempt only')\n    parser.add_argument('--attempt', type=int, help='Show logs for a specific attempt (0-based index)')\n    parser.add_argument('--node', type=int, help='Show logs for a specific node (for multi-node jobs)')\n    parser.add_argument('--debug', action='store_true', help='Show debug information')\n\n    args = parser.parse_args()\n\n    if args.job:\n        show_job_logs(args.job, tail=args.tail, latest_attempt=args.latest,\n                     attempt_index=args.attempt, node_index=args.node, debug=args.debug)\n    else:\n        list_recent_jobs(job_queue=args.queue, max_jobs=args.max_jobs, debug=args.debug)\n\nif __name__ == '__main__':\n    main()\n"}
