{"repo_info": {"repo_name": "ReBase", "repo_owner": "para-lost", "repo_url": "https://github.com/para-lost/ReBase"}}
{"type": "source_file", "path": "data_preparation/api_tools.py", "content": "\"\"\"Tools for accessing API-based models.\"\"\"\n\nfrom __future__ import annotations  # noqa FI58\n\nimport asyncio\nimport json\nimport logging\nimport re\nimport time\n\nimport aiolimiter\nimport litellm.utils\nimport openai\nimport tiktoken\nfrom aiohttp import ClientSession\nfrom litellm import acompletion, completion\nfrom tqdm.asyncio import tqdm_asyncio\n\n# Note that litellm converts all API errors into openai errors,\n# so openai errors are valid even when using other services.\nAPI_ERRORS = (\n    openai.APIError,\n    openai.APITimeoutError,\n    openai.RateLimitError,\n    openai.BadRequestError,\n    openai.APIStatusError,\n    json.decoder.JSONDecodeError,\n    AssertionError,\n)\n\nERROR_ERRORS_TO_MESSAGES = {\n    openai.BadRequestError: \"API Invalid Request: Prompt was filtered\",\n    openai.RateLimitError: \"API rate limit exceeded. Sleeping for 10 seconds.\",\n    openai.APIConnectionError: \"Error Communicating with API\",\n    openai.APITimeoutError: \"API Timeout Error: API Timeout\",\n    openai.APIStatusError: \"API service unavailable error: {e}\",\n    openai.APIError: \"API error: {e}\",\n}\nBUFFER_DURATION = 2\n\n\nclass APIAgent:\n    \"\"\"A class for accessing API-based models.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"gpt-4\",\n        max_tokens: int | None = 4000,\n        api_base: str | None = None,\n        api_key: str | None = None\n    ):\n        \"\"\"Initialize APIAgent with model_name and max_tokens.\n\n        Args:\n            model_name: Name of the model to use (by default, gpt-4).\n            max_tokens: The maximum number of tokens to generate. Defaults to the max\n                value for the model if available through litellm.\n            api_base: Custom endpoint for Hugging Face's inference API.\n        \"\"\"\n        self.model_name = model_name\n        self.max_tokens = max_tokens\n        self.api_base = api_base\n        self.api_key = api_key\n        if max_tokens is None:\n            try:\n                self.max_tokens = litellm.utils.get_max_tokens(model_name)\n                if isinstance(self.max_tokens, dict):\n                    self.max_tokens = self.max_tokens[\"max_tokens\"]\n            except Exception:\n                pass\n\n    def generate_one_completion(\n        self,\n        prompt: str,\n        temperature: float = 0,\n        presence_penalty: float = 0,\n        frequency_penalty: float = 0,\n        token_buffer: int = 300,\n    ) -> openai.Completion:\n        \"\"\"Generate a chat completion using an API-based model.\n\n        Args:\n            prompt: A prompt asking for a response.\n            temperature: What sampling temperature to use, between 0 and 2. Higher\n                values like 0.8 will make the output more random, while lower values\n                like 0.2 will make it more focused and deterministic.\n            presence_penalty: Float between -2.0 and 2.0. Positive values penalize new\n                tokens based on whether they appear in the text so far, increasing the\n                model's likelihood to talk about new topics.\n            frequency_penalty: Float between -2.0 and 2.0. Positive values penalize new\n                tokens based on their existing frequency in the text so far, decreasing\n                the model's likelihood of repeating the same line verbatim.\n            token_buffer: Number of tokens below the LLM's limit to generate. In case\n                our tokenizer does not exactly match the LLM API service's perceived\n                number of tokens, this prevents service errors. On the other hand, this\n                may lead to generating fewer tokens in the completion than is actually\n                possible.\n\n        Returns:\n            An OpenAI-like response object if there were no errors in generation.\n            In case of API-specific error, Exception object is captured and returned.\n        \"\"\"\n        num_prompt_tokens = count_tokens_from_string(prompt)\n        if self.max_tokens:\n            max_tokens = self.max_tokens - num_prompt_tokens - token_buffer\n        else:\n            max_tokens = 3 * num_prompt_tokens\n        response = completion(  # completion gets the key from os.getenv\n            model=self.model_name,\n            messages=[\n                {\"role\": \"user\", \"content\": f\"{prompt}\"},\n            ],\n            api_key=self.api_key,\n            api_base=self.api_base,\n            temperature=temperature,\n            presence_penalty=presence_penalty,\n            frequency_penalty=frequency_penalty,\n            max_tokens=max_tokens,\n        )\n        return response\n\n    async def generate_batch_completion(\n        self,\n        prompts: list[str],\n        temperature: float = 1,\n        responses_per_request: int = 1,\n        requests_per_minute: int = 80,\n        token_buffer: int = 300,\n    ) -> list[openai.Completion]:\n        \"\"\"Generate a batch responses from OpenAI Chat Completion API.\n\n        Args:\n            prompts: List of prompts to generate from.\n            model_config: Model configuration.\n            temperature: Temperature to use.\n            responses_per_request: Number of responses for each request.\n                i.e. the parameter n of API call.\n            requests_per_minute: Number of requests per minute to allow.\n            token_buffer: Number of tokens below the LLM's limit to generate. In case\n                our tokenizer does not exactly match the LLM API service's perceived\n                number of tokens, this prevents service errors. On the other hand, this\n                may lead to generating fewer tokens in the completion than is actually\n                possible.\n\n        Returns:\n            List of generated responses.\n        \"\"\"\n        async with ClientSession() as _:\n            limiter = aiolimiter.AsyncLimiter(requests_per_minute)\n\n            async def _throttled_completion_acreate(\n                model: str,\n                messages: list[dict[str, str]],\n                temperature: float,\n                max_tokens: int,\n                n: int,\n                top_p: float,\n                limiter: aiolimiter.AsyncLimiter,\n            ):\n                async with limiter:\n                    for _ in range(3):\n                        try:\n                            return await acompletion(\n                                api_key=self.api_key,\n                                model=model,\n                                messages=messages,\n                                api_base=self.api_base,\n                                temperature=temperature,\n                                max_tokens=max_tokens,\n                                n=n,\n                                top_p=top_p,\n                            )\n                        except tuple(ERROR_ERRORS_TO_MESSAGES.keys()) as e:\n                            if isinstance(\n                                e,\n                                (\n                                    openai.APIStatusError,\n                                    openai.APIError,\n                                ),\n                            ):\n                                logging.warning(\n                                    ERROR_ERRORS_TO_MESSAGES[type(e)].format(e=e)\n                                )\n                            elif isinstance(e, openai.BadRequestError):\n                                logging.warning(ERROR_ERRORS_TO_MESSAGES[type(e)])\n                                return {\n                                    \"choices\": [\n                                        {\n                                            \"message\": {\n                                                \"content\": \"Invalid Request: Prompt was filtered\"  # noqa E501\n                                            }\n                                        }\n                                    ]\n                                }\n                            else:\n                                logging.warning(ERROR_ERRORS_TO_MESSAGES[type(e)])\n                            await asyncio.sleep(10)\n                    return {\"choices\": [{\"message\": {\"content\": \"\"}}]}\n\n            num_prompt_tokens = max(\n                count_tokens_from_string(prompt) for prompt in prompts\n            )\n            if self.max_tokens:\n                max_tokens = self.max_tokens - num_prompt_tokens - token_buffer\n            else:\n                max_tokens = 3 * num_prompt_tokens\n\n            async_responses = [\n                _throttled_completion_acreate(\n                    model=self.model_name,\n                    messages=[\n                        {\"role\": \"user\", \"content\": f\"{prompt}\"},\n                    ],\n                    temperature=temperature,\n                    max_tokens=max_tokens,\n                    n=responses_per_request,\n                    top_p=1,\n                    limiter=limiter,\n                )\n                for prompt in prompts\n            ]\n            responses = await tqdm_asyncio.gather(*async_responses)\n        # Note: will never be none because it's set, but mypy doesn't know that.\n        return responses\n\n\ndef handle_api_error(e, backoff_duration=1) -> None:\n    \"\"\"Handle OpenAI errors or related errors that the API may raise.\n\n    Sleeps incase error is some type of timeout, else throws error.\n\n    Args:\n        e: The error to handle. This could be an OpenAI error or a related\n           non-fatal error, such as JSONDecodeError or AssertionError.\n        backoff_duration: The duration (in s) to wait before retrying.\n\n    Raises:\n        e: If the error is not an instance of APIError, Timeout, or RateLimitError.\n\n    Returns:\n        None\n    \"\"\"\n    logging.error(e)\n\n    if not isinstance(e, API_ERRORS):\n        raise e\n\n    if isinstance(\n        e,\n        (openai.APIError, openai.APITimeoutError, openai.RateLimitError),\n    ):\n\n        match = re.search(r\"Please retry after (\\d+) seconds\", str(e))\n        # If OpenAI mentions how long to sleep, use that. Otherwise, do\n        # exponential backoff.\n        if match is not None:\n            backoff_duration = int(match.group(1)) + BUFFER_DURATION\n\n        logging.info(f\"Retrying in {backoff_duration} seconds...\")\n        time.sleep(backoff_duration)\n\n\ndef count_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n    \"\"\"Handle count the tokens in a string with OpenAI's tokenizer.\n\n    Args:\n        string: The string to count.\n        encoding_name: The name of the tokenizer to use.\n\n    Returns:\n        The number of tokens in the string.\n    \"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n"}
{"type": "source_file", "path": "data_preparation/config.py", "content": "API_KEY = \"\""}
{"type": "source_file", "path": "data_preparation/embed_corpus.py", "content": "from sentence_transformers import SentenceTransformer\nimport json\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\n# Initialize the sentence-transformer model\nmodel = SentenceTransformer('distiluse-base-multilingual-cased').to('cuda') # Example model, choose according to needs\n\ndef save_values():\n    json_file_path = 'datasets_merged_flattened_final.jsonl'\n\n    # Check if stored values already exist\n    stored_values_file = 'stored_values.pkl'\n    try:\n        with open(stored_values_file, 'rb') as f:\n            stored_values = pickle.load(f)\n        print(\"Loaded stored values from disk.\")\n    except FileNotFoundError:\n        print(\"Stored values not found. Processing JSON file.\")\n        stored_values = []\n        with open(json_file_path, 'r') as file:\n            for num, line in enumerate(file):\n                data = json.loads(line)  # Assuming each line is a valid JSON object\n                stored_values.append(data['value'])\n        \n        # Save stored values to disk\n        with open(stored_values_file, 'wb') as f:\n            pickle.dump(stored_values, f)\n        print(\"Stored values saved to disk.\")\n\n\ndef save_dict():\n    json_file_path = 'datasets_merged_flattened_final.jsonl'\n\n    # Check if stored values already exist\n    stored_values_file = 'stored_dict.pkl'\n    try:\n        with open(stored_values_file, 'rb') as f:\n            stored_values = pickle.load(f)\n        print(\"Loaded stored values from disk.\")\n    except FileNotFoundError: \n        print(\"Stored values not found. Processing JSON file.\")\n        stored_values = {}\n        with open(json_file_path, 'r') as file:\n            for num, line in enumerate(file):\n                data = json.loads(line)  # Assuming each line is a valid JSON object\n                value = data['value']\n                location = str(data['location'])\n                column = data['column']\n                if location not in stored_values.keys():\n                    stored_values[location] = {column: value}\n                else:\n                    stored_values[location][column] = value\n        # Save stored values to disk\n        with open(stored_values_file, 'wb') as f:\n            pickle.dump(stored_values, f)\n        print(\"Stored values saved to disk.\")\n\ndef save_embedding_info(dataset_index_path, json_file_path, embeddings_file_path_save):\n        \n    start = 0\n    embeddings = None\n\n    info_dict = {}\n    with open(dataset_index_path, 'r') as f:\n        info_dict = json.load(f)\n    \n    embedding_list = []\n    name_to_description_dict = {}\n    ids = []\n    with open(json_file_path, 'r') as file:\n        for num, line in enumerate(file):\n            if num < start:\n                continue\n            data = json.loads(line)  # Assuming each line is a valid JSON object\n            name = data['location']['dataset_id']\n            ids.append(str(data['location']))\n            name_dict = info_dict[name]\n            description = name_dict['description']\n            if name not in name_to_description_dict.keys():\n                name_dict_embedding = model.encode([description], batch_size=1, show_progress_bar=True,convert_to_numpy=True) \n                name_to_description_dict[name] = name_dict_embedding\n                if (num - start) == 0:\n                    embeddings = name_dict_embedding\n            if (num-start) > 0:\n                embedding = name_to_description_dict[name]\n                embedding_list.append(embedding)\n            if (num-start) % 1000 == 0:\n                if (num-start) > 0:\n                    embedding = np.vstack(embedding_list)\n                    embeddings = np.vstack((embeddings, embedding))\n                with open(embeddings_file_path_save, \"wb\") as fOut:\n                    pickle.dump({'ids': ids, 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n                embedding_list = []\n    \n    # Save stored values to disk\n    embedding = np.vstack(embedding_list)\n    embeddings = np.vstack((embeddings, embedding))\n    with open(embeddings_file_path_save, \"wb\") as fOut:\n        pickle.dump({'ids': ids, 'embeddings': embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)\n      \nif __name__==\"__main__\":\n    save_values()\n    save_dict()\n    dataset_index_path = 'dataset_index.json'  # Path to dataset index\n    json_file_path = 'datasets_merged_flattened_final.jsonl'  # Path to merged corpus\n    embeddings_file_path_save = 'embeddings_dataset_final.pkl'  # Path to corpus embeddings\n    save_embedding_info(dataset_index_path, json_file_path, embeddings_file_path_save)"}
{"type": "source_file", "path": "data_preparation/merge_datasets.py", "content": "import pyarrow.parquet as pq\nimport pyarrow as pa\nfrom datasets import load_dataset\nimport json\nimport time\nimport datasets\nimport pandas as pd\nimport traceback\nimport random\nimport os\nimport threading\nfrom collections.abc import MutableMapping\n\nLAST_PROCESSED_NAME = \"last_processed_dataset_flatten.txt\"\n\ndef fetch_first_row_with_timeout(dataset: datasets.Dataset, timeout: int = 30): \n    \"\"\"\n    Fetch the first row of a dataset within a specified timeout period.\n\n    Args:\n        dataset: The dataset from which to fetch the first row.\n        timeout: The maximum time in seconds towait for\n                 fetching the row (default is 30 seconds).\n\n    Returns:\n        dict or None: The first row of the dataset as a dictionary,\n                      or None if the operation times out.\n    \"\"\"\n\n    def fetch_sample_row(container):\n        try:\n            container.append(next(iter(dataset)))\n        except Exception as e:\n            container.append(e)\n\n    result_container = []\n    fetch_thread = threading.Thread(target=fetch_sample_row, args=(result_container,))\n    fetch_thread.start()\n    fetch_thread.join(timeout)\n\n    if fetch_thread.is_alive() or result_container[0] is None:\n        # Operation took too long or failed\n        return None\n\n    return result_container[0] if isinstance(result_container[0], dict) else None\n\n\ndef flatten_dict(d: MutableMapping, parent_key: str = \"\", sep: str = \".\"):\n    \"\"\"\n    Flatten the sample rows of streaming dataset.\n\n    Streaming Datasets from HF don't inherently have the flatten function.\n\n    Args:\n        d (MutableMapping): The dictionary to flatten.\n        parent_key (str): The base key string to use for the flattened keys.\n        sep (str): Separator used between nested keys (default is '.').\n\n    Returns:\n        dict: A flattened dictionary with no nested structures.\n    \"\"\"\n    items: list[tuple[str, Any]] = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, MutableMapping):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\ndef replace_duplicate_columns(original_dataset_columns: list):\n    \"\"\"Replace duplicate column names in a dataset after flattening.\n\n    Args:\n        original_dataset_columns: List of original column names in the dataset.\n\n    Returns:\n        tuple: A tuple containing two elements:\n                1. A list of new column names with duplicates handled.\n                2. A dictionary mapping original column names to new column names.\n    \"\"\"\n    columns_mapping: dict[str, str] = {}\n    new_columns = []\n    counter: dict[str, int] = {}\n    # convert flattened columns like answer.text -> answer_text\n    for col in original_dataset_columns:\n        new_col = col.replace(\".\", \"_\")\n        if new_col in columns_mapping.values():\n            counter[new_col] = counter.get(new_col, 0) + 1\n            new_col = f\"{new_col}_{counter[new_col]}\"\n        columns_mapping[col] = new_col\n        new_columns.append(new_col)\n    return new_columns, columns_mapping\n    \n\ndef save_last_processed_dataset(dataset_name):\n    with open(LAST_PROCESSED_NAME, \"w\") as file:\n        file.write(dataset_name)\n\n\ndef get_last_processed_dataset():\n    try:\n        with open(LAST_PROCESSED_NAME, \"r\") as file:\n            print(\"The process starts from last processed dataset.\")\n            return file.read()\n    except FileNotFoundError:\n        print(\"The process starts from the beginning.\")\n        return None  \n\n\ndef get_label_names(dataset):\n    \"\"\"\n    Check if the 'features' attribute has a 'label' field with names\n    \"\"\"\n    label_names = None\n    if 'label' in dataset.features and hasattr(dataset.features['label'], 'names'):\n        label_names = dataset.features['label'].names\n    return label_names\n\n\ndef get_dataset_length(dataset_name, config_name):\n    \"\"\"\n    Attempt to get the dataset length from its metadata\n    \"\"\"\n    try:\n        dataset_info = datasets.get_dataset_infos(dataset_name)[config_name]\n        return dataset_info.splits['train'].num_examples\n    except Exception as e:\n        print(f\"Could not determine length for {dataset_name} with config {config_name}: {e}\")\n        return None\n            \n\ndef process_row(row, dataset_id, config_id, row_id, label_names):\n    processed_data = []\n    row = flatten_dict(row)\n    # Creating a list of dictionaries, each with 'value' and 'location' keys\n    for column_name, value in row.items():\n        # If column is label, convert the number to the actual content.\n        if ('id' in column_name or 'label' in column_name) and label_names:\n            value = label_names[value] if value in range(len(label_names)) else value\n\n        processed_data.append({\n            'column': column_name,\n            'value': str(value), \n            'location': {'dataset_id': dataset_id, 'config_id':config_id, 'row_id': row_id}\n        })\n    \n    return processed_data\n\ndef write_to_jsonl(data, file_path):\n    with open(file_path, 'a') as file:\n        for item in data:\n            json.dump(item, file)\n            file.write('\\n')\n\n\ndef process_dataset_config(dataset_name, config_name, file_path, dataset_id, chunk_size=10000, max_samples=1000):\n    dataset_length = get_dataset_length(dataset_name, config_name)\n    # Taking too long to load, neglect for now\n    if dataset_length > 50000:\n        return\n    dataset = datasets.load_dataset(\n        dataset_name,\n        config_name,\n        split=\"train\",\n        streaming=True,\n        download_mode=\"force_redownload\",\n        cache_dir=\"./.cache\",\n    )\n    sample_rows = fetch_first_row_with_timeout(dataset, timeout=30)\n    if not sample_rows:\n        return\n    sample_rows = flatten_dict(sample_rows)\n    if any(\n        \"ImageFile\" in sample_rows[key].__class__.__name__\n        or \"DateTime\" in sample_rows[key].__class__.__name__\n        for key in sample_rows\n    ):\n        return\n    columns, columns_mapping = replace_duplicate_columns(list(sample_rows.keys()))\n    label_names = get_label_names(dataset)\n    fields = [\n        pa.field('column', pa.string()),  # New field for the column name\n        pa.field('value', pa.string()),\n        pa.field('location', pa.struct([\n            pa.field('dataset_id', pa.string()), \n            pa.field('config_id', pa.string()), \n            pa.field('row_id', pa.string())\n        ]))\n    ]\n    table_schema = pa.schema(fields)\n    processed_chunk = []\n    # Keep track of total rows processed\n    total_rows = 0  \n\n    if dataset_length is not None:\n        k = max(1, dataset_length // max_samples)\n    else:\n        k = 1  # or any other default value you choose\n    start_time = time.time()\n    for row_index, row in enumerate(dataset):\n        if row_index % k == 0:\n            processed_chunk.extend(process_row(row, dataset_id, config_name, str(total_rows), label_names))\n            total_rows += 1\n            # Check if the processed_chunk size has reached the chunk_size\n            if len(processed_chunk) >= chunk_size:\n                table = pa.Table.from_pandas(pd.DataFrame(processed_chunk), schema=table_schema)\n                write_to_jsonl(processed_chunk, file_path)\n                processed_chunk = []  # Reset chunk\n            end_time = time.time()\n        if total_rows >= max_samples or abs(end_time-start_time) > 300 :\n            break\n\n    # Process any remaining rows in the last chunk\n    if processed_chunk:\n        table = pa.Table.from_pandas(pd.DataFrame(processed_chunk), schema=table_schema)\n        write_to_jsonl(processed_chunk, file_path)\n         \ndef process_dataset(dataset_name, parquet_file, dataset_id, chunk_size=10000):\n    config_names = datasets.get_dataset_config_names(dataset_name)\n    all_configs = {}\n    start_time = time.time()\n    for config_name in config_names:\n        if \"train\" not in datasets.get_dataset_split_names(dataset_name, config_name):\n            continue\n        process_dataset_config(dataset_name, config_name, parquet_file, dataset_id, chunk_size=10000)\n        end_time = time.time()\n        if abs(end_time - start_time) > 300:\n            return\n\n\ndef get_dataset_names(dataset_index_path):\n    with open(dataset_index_path, 'r') as json_file:\n        return json.load(json_file)\n    dataset_names = [details['name'] for details in json_file.values()]\n    return dataset_names\n\ndef process_dataset_with_error_handling(dataset_name, parquet_file, dataset_id, chunk_size=10000):\n    try:\n        process_dataset(dataset_name, parquet_file, dataset_id, chunk_size)\n    except Exception as e:\n        print(f\"An error occurred while processing {dataset_name}: {e}\")\n        # Print error's stack trace\n        traceback.print_exc()  \n\n        \nif __name__ == \"__main__\":\n    last_processed = get_last_processed_dataset()\n    start_processing = False if last_processed else True\n    \n    # TODO: change dataset_index_path\n    dataset_index_path = './dataset_index.json'\n    dataset_names = get_dataset_names(dataset_index_path)  \n    for name in dataset_names:\n        if start_processing or name == last_processed:\n            start_processing = True  \n            # Start or resume processing\n            if name == last_processed:\n                continue\n            start_time = time.time()\n            process_dataset_with_error_handling(name, 'datasets_merged_flattened_final.jsonl', name)\n            end_time = time.time()\n            print(f\"Finished processing {name}, Process took {(end_time-start_time)} seconds\")\n            save_last_processed_dataset(name)"}
{"type": "source_file", "path": "data_preparation/dataset_transformer.py", "content": "from __future__ import annotations\nimport json\nimport anthropic\nfrom tqdm import tqdm\nimport asyncio\nfrom collections.abc import Callable\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport datasets\nfrom api_tools import APIAgent\nimport csv\nimport ast\nimport re\nimport asyncio\nfrom typing import List\nfrom config import API_KEY, API_BASE, MODEL_NAME\nimport argparse\n\nimport os\nos.environ['TIKTOKEN_CACHE_DIR'] = '../cache_dir'\n                \ndef load_json_file(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            data = json.load(file)\n        return data\n    except FileNotFoundError:\n        print(f\"File not found: {file_path}\")\n        return None\n    except json.JSONDecodeError:\n        print(f\"Error decoding JSON from file: {file_path}\")\n        return None\n\n\ndef parse_json(\n    response, required_keys: list, optional_keys: list\n) -> dict | None:\n    \"\"\"Parse stuctured fields from the API response.\n\n    Args:\n        response: API response.\n        required_keys: Required keys from the response\n        optional_keys: Optional keys from the response\n\n    Returns:\n        If the API response is a valid JSON object and contains the\n        required and optional keys then returns the\n        final response as a Dictionary\n        Else returns None.\n    \"\"\"\n    # usage = response.choices[0][\"usage\"][\"total_tokens\"]\n    response_text = response.choices[0][\"message\"][\"content\"]\n    response_text = response_text.replace('{\\n', '{')\n    response_text = response_text.replace('}\\n', '}')\n    response_text = response_text.strip(\"```json\\n\")\n    response_text = response_text.strip(\"```json\")\n    response_text = response_text.strip(\"\\n```\")\n    response_text = response_text.strip(\"```\")\n    response_json = {}\n    try:\n        response_json = json.loads(response_text, strict=False)\n    except json.decoder.JSONDecodeError:\n        try: \n            response_json = {}\n            response_text = str(response_text)\n            response_text = response_text.strip(\"{\").strip(\"}\")\n\n            if \"\\\"input\\\":\" in response_text:\n                response_json['input'] = response_text.split(\"\\\"input\\\":\")[1].split(\",\\n\")[0]\n                response_json['output'] = response_text.split(\"\\\"output\\\":\")[1].split(\",\\n\")[0]\n            elif \"'input':\" in response_text:\n                response_json['input'] = response_text.split(\"'input':\")[1].split(\"'output'\")[0]\n                response_json['output'] = response_text.split(\"'output':\")[1].split(\",\")[0]\n            else:\n                response_json['input'] = response_text.split(\"input\")[1].split(\",\")[0]\n                response_json['output'] = response_text.split(\"output\")[1].split(\",\")[0]\n\n        except:\n            return None\n        return None\n    missing_keys = [key for key in required_keys if key not in response_json]\n    if len(missing_keys) != 0:\n        \n        return None\n\n    final_response = {}\n    for key in required_keys + optional_keys:\n        if key not in response_json:\n            # This is an optional key, so exclude it from the final response.\n            continue\n        if type(response_json[key]) == str:\n            final_response[key] = response_json[key].strip()\n        else:\n            final_response[key] = response_json[key]\n    return final_response\n    \ndef parse_json_azure(\n    response, required_keys: list\n) -> dict | None:\n    \"\"\"Parse stuctured fields from the API response.\n\n    Args:\n        response: API response.\n        required_keys: Required keys from the response\n        optional_keys: Optional keys from the response\n\n    Returns:\n        If the API response is a valid JSON object and contains the\n        required and optional keys then returns the\n        final response as a Dictionary\n        Else returns None.\n    \"\"\"\n    response = response.replace('{\\n', '{')\n    response = response.replace('}\\n', '}')\n    try:\n        response_json = json.loads(response, strict=False)\n    except json.decoder.JSONDecodeError:\n        try: \n            response_json = {}\n            response = str(response)\n            response = response.strip(\"{\").strip(\"}\")\n            response_json['input'] = response.split('\"input\":')[1].split(\",\\n\")[0]\n            response_json['output'] = response.split('\"output\":')[1].split(\",\\n\")[0]\n        except:\n            return None\n        \n\n    missing_keys = [key for key in required_keys if key not in response_json]\n    if len(missing_keys) != 0:\n        return None\n\n    final_response = {}\n    for key in required_keys:\n        if key not in response_json:\n            # This is an optional key, so exclude it from the final response.\n            continue\n        if type(response_json[key]) == str:\n            final_response[key] = response_json[key].strip()\n        else:\n            final_response[key] = response_json[key]\n    return final_response\n    \ndef str_to_dict(str_dict):\n    try:\n        # Convert the string to a dictionary\n        actual_dict = ast.literal_eval(str_dict)\n        return actual_dict\n    except ValueError as e:\n        # Handle the error if the string is not a valid dictionary\n        print(f\"Error converting string to dict: {e}\")\n        return None\n\n    \ndef save_dataset_to_csv(dataset, file_path='transformed_dataset_japanese.csv'):\n    \"\"\"Save the dataset to a CSV file.\"\"\"\n    with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        writer.writerow(['input', 'output'])  # Writing header\n\n        # Assuming dataset['train'] contains the data\n        for row in dataset['train']:\n            writer.writerow([row['input_col'], row['output_col']])\n    print(f\"Dataset saved to {file_path}\")\n\ndef save_input_output_to_csv(input, output, file_path='transformed_dataset_japanese.csv'):\n    \"\"\"Save the dataset to a CSV file.\"\"\"\n    with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        writer.writerow(['input', 'output'])  # Writing header\n\n        # Assuming dataset['train'] contains the data\n        for input_col, output_col in zip(input, output):\n            writer.writerow([input_col, output_col])\n    print(f\"Dataset saved to {file_path}\")\n\n\nTRANSFORM_DATA_PROMPT_VERSION = \"\"\"\nI would like you to create questions for a test. The directions for the test are:\n\n```\n{task_description}\n```\nThe format should be in json like this:\n{example}\n\nNow I will provide you with a JSON file from a different dataset. Please create a question where the format and type of question is similar to the examples provided above, but the content is inspired by the example provided below.\nYou need to decide which part of the dataset to use.\n{dataset_row}\n\nYour response MUST be a JSON with exactly 2 fields: \"input\" and \"output\". \nResponse (JSON ONLY):\n\"\"\"  \n\n\nFILTER_DATA_PROMPT_VERSION = \"\"\"\nYou will be given a task description. Your task is to determine whether a data is fitful for this task.\n\n# Instruction:\n{task_description}\n\n# Fitful Examples that meet the task's request:\n{example}\n\nNow, there is a new data. Your task is to determine whether this data is fitful for this task.\nNew Data:\n{{\n\"input\": \"{input_data}\",\n\"output\": \"{output_data}\",\n}}\nResponse (Yes or No):\n\"\"\"  \n\n\n\n\nEXAMPLE_TEMPLATE = \"\"\"\n```json\n{{\n\"input\": \"{input_one}\",\n\"output\": \"{output_one}\",\n}}\n\n{{\n\"input\": \"{input_two}\",\n\"output\": \"{output_two}\",\n}}\n\n{{\n\"input\": \"{input_three}\",\n\"output\": \"{output_three}\",\n}}\n```\n\"\"\"\n\n\ndef truncate_row(example_row: dict, max_length=250) -> str:\n    \"\"\"Truncate the row before displaying if it is too long.\"\"\"\n    truncated_row = {}\n    for key in example_row.keys():\n        curr_row = json.dumps(example_row[key])\n        truncated_row[key] = (\n            curr_row\n            if len(curr_row) <= max_length - 3\n            else curr_row[:max_length] + \"...\"\n        )\n    return json.dumps(truncated_row)\n\n\n\ndef construct_prompt_for_transform_data(\n    task_description: str, dataset_row: dict, example: str, task_specific_instruction: str\n) -> str:\n    \"\"\"Construct prompt for transform data.\"\"\"\n    return TRANSFORM_DATA_PROMPT_VERSION.format(\n        task_description=task_description,\n        dataset_row=truncate_row(dataset_row),\n        example=example,\n        task_specific_instruction=task_specific_instruction,\n    )\n\ndef construct_prompt_for_filter_data(\n    task_description: str, input_row: str, output_row:str, example: str, task_specific_instruction: str\n) -> str:\n    \"\"\"Construct prompt for transform data.\"\"\"\n    return FILTER_DATA_PROMPT_VERSION.format(\n        task_description=task_description,\n        input_data=input_row,\n        output_data=output_row,\n        example=example,\n        task_specific_instruction=task_specific_instruction,\n    )\n\n\n\nclass PromptBasedDatasetTransformer():\n    \"\"\"Transform data based on a transform prompt.\"\"\"\n\n    def __init__(\n        self,\n        transform_prompt_fn: Callable[\n            [str, dict, str, str], str\n        ] = construct_prompt_for_transform_data,\n    ):\n        \"\"\"Initialize the class.\"\"\"\n        self.transform_prompt_fn = transform_prompt_fn\n\n    def make_dataset_from_samples(\n        self,\n        inputs: list[str],\n        outputs: list[str],\n    ) -> datasets.DatasetDict:\n        \"\"\"Given a list of inputs and outputs, make a dataset.\n\n        This function takes in inputs and outputs, both as list of strings,\n        and returns a DatasetDict object with a single split, \"train\". It has\n        two columns, \"input_col\" and \"output_col\".\n\n\n        Args:\n            inputs: A list of inputs, each input is a string.\n            outputs: A list of outputs, each output is a string.\n\n        Returns:\n            A DatasetDict object with a single split, \"train\". It has two\n            columns, \"input_col\" and \"output_col\".\n        \"\"\"\n        if len(inputs) <= 0 or len(inputs) != len(outputs):\n            raise ValueError(f\"Length of inputs and outputs must be >0 and equal. Cur length of inputs is: {len(inputs)}, cur length of outputs is: {len(outputs)}\")\n\n        dataset_dict = {}\n        dataset_dict[\"train\"] = datasets.Dataset.from_dict(\n            {\"input_col\": inputs, \"output_col\": outputs}\n        )\n        return datasets.DatasetDict(dataset_dict)\n\n    def transform_data(\n        self,\n        instruction,\n        examples,\n        task_specific_instruction,\n        dataset,\n        num_points_to_transform: int,\n        file_path: str,\n        use_azure=True\n    ) -> datasets.DatasetDict:\n\n        inputs = []\n        outputs = []\n\n        max_len = min(num_points_to_transform, len(dataset))\n        len_count = 0\n        transform_prompts = []\n        for row in dataset:\n            transform_prompt = self.transform_prompt_fn(\n                instruction,\n                row,\n                examples,\n                task_specific_instruction,\n            )\n            transform_prompts.append(transform_prompt)\n\n            len_count += 1\n            if len_count >= max_len:\n                break\n        batch_size = 20\n        num_batches = int((len(transform_prompts) + batch_size - 1) / batch_size)\n        def fetch_response(prompt):\n            try:\n                message = client.messages.create(\n                    model=\"claude-3-haiku-20240307\",\n                    max_tokens=1000,\n                    temperature=0.7,\n                    system=\"You are a useful assistant\",\n                    messages=[\n                        {\"role\": \"user\", \"content\": prompt}\n                    ]\n                )\n                return message.content[0].text\n            except Exception as e:\n                return str(e)\n        async def generate_responses_openai(transform_prompts):\n            default_api_agent = APIAgent(model_name=MODEL_NAME, api_base=API_BASE, api_key=API_KEY)\n            responses = await default_api_agent.generate_batch_completion(\n                transform_prompts,\n                temperature=0,\n                responses_per_request=1,\n                requests_per_minute=20,\n            )\n            return responses\n        def generate_responses(prompts: List[str]) -> List[str]:\n            responses = []\n            while len(prompts) > 0:\n                un_completed_prompts = []\n                with ThreadPoolExecutor(max_workers=200) as executor:\n                    future_to_prompt = {executor.submit(fetch_response, prompt): prompt for prompt in prompts}\n                    for future in tqdm(as_completed(future_to_prompt), total=len(prompts)):\n                        try:\n                            response = future.result()\n                            if response != 'error':\n                                responses.append(response)\n                            else:\n                                un_completed_prompts.append(future_to_prompt[future])\n                                \n                        except Exception as exc:\n                            print(f'Prompt generated an exception: {exc}')\n                prompts = un_completed_prompts\n            return responses\n        if use_azure:\n            client = anthropic.Anthropic(\n                api_key=API_KEY\n            )\n            all_response = []\n            for i in range(num_batches):\n                batch_start = i * batch_size\n                batch_end = min((i + 1) * batch_size, len(transform_prompts))\n                current_batch = transform_prompts[batch_start:batch_end]\n\n                try:\n                    responses = generate_responses(current_batch)\n                except Exception as e:\n                    print('error:', e)\n\n                for response in responses:\n                    print(f'response in transform = {response}')\n                    try:\n                        if use_azure:\n                            extraction = parse_json_azure(response, [\"input\", \"output\"])\n                        else:\n                            extraction = parse_json(response, [\"input\", \"output\"], [])\n                        print(f'extraction in transform = {extraction}')\n                        if extraction is not None:\n                            inputs.append(extraction[\"input\"])\n                            outputs.append(extraction[\"output\"])\n                        else:\n                            print(\"error:\\n\"+response)\n                    except Exception as e:\n                        print('error:', e)\n                        print('response:', response)\n\n                all_response.extend(responses)\n                \n        else:\n            try:\n                loop = asyncio.get_event_loop()\n                responses = loop.run_until_complete(generate_responses_openai(transform_prompts))\n            except Exception as e:\n                print('error:', e)\n\n            for response in responses:\n                print(f'response = {response}')\n                \n                try:\n                    if use_azure:\n                        extraction = parse_json_azure(response, [\"input\", \"output\"])\n                    else:\n                        extraction = parse_json(response, [\"input\", \"output\"], [])\n                    print(f'extraction in transform = {extraction}')\n                    if extraction is not None:\n                        inputs.append(extraction[\"input\"])\n                        outputs.append(extraction[\"output\"])\n                except Exception as e:\n                    print(f'error: {e}')\n\n            if inputs and outputs:  # Ensure there's something to save\n                save_input_output_to_csv(inputs, outputs, file_path)\n\n        return self.make_dataset_from_samples(inputs, outputs)\n    \n    def filter_data(\n        self,\n        instruction,\n        examples,\n        task_specific_instruction,\n        dataset,\n        num_points_to_transform: int,\n        file_path: str,\n        use_azure=True\n    ) -> datasets.DatasetDict:\n\n        inputs = []\n        outputs = []\n        \n        max_len = min(num_points_to_transform, len(dataset))\n        len_count = 0\n        transform_prompts = []\n        \n        for row in dataset:\n            transform_prompt = self.transform_prompt_fn(\n                instruction,\n                row['input'],\n                row['output'],\n                examples,\n                task_specific_instruction,\n            )\n            transform_prompts.append(transform_prompt)\n\n            len_count += 1\n            if len_count >= max_len:\n                break\n        batch_size = 1000\n        num_batches = int((len(transform_prompts) + batch_size - 1) / batch_size)\n        async def generate_responses(transform_prompts):\n            responses = await api_tools.default_api_agent.generate_batch_completion(\n                transform_prompts,\n                temperature=0,\n                responses_per_request=5,\n                requests_per_minute=80,\n            )\n            return responses\n        cur = 0\n        for i in range(num_batches):\n            batch_start = i * batch_size\n            batch_end = min((i + 1) * batch_size, len(transform_prompts))\n            current_batch = transform_prompts[batch_start:batch_end]\n            original_current_batch = dataset[batch_start:batch_end]\n            try:\n                loop = asyncio.get_event_loop()\n                responses = loop.run_until_complete(generate_responses(current_batch))\n            except Exception as e:\n                print(f\"error: {e}\")\n\n            for response, row in zip(responses, original_current_batch):\n                try:\n                    response_text = response.choices[0][\"message\"][\"content\"]\n                    answer = \"yes\" in response_text.lower()\n                    if answer:\n                        inputs.append(row[\"input\"])\n                        outputs.append(row[\"output\"])\n                    else:\n                        cur += 1\n                except Exception as e:\n                    continue\n\n            if inputs and outputs:  # Ensure there's something to save\n                save_input_output_to_csv(inputs, outputs, file_path)\n    \n        if inputs and outputs:\n            return self.make_dataset_from_samples(inputs, outputs)\n        else:\n            dataset_dict = {}\n            dataset_dict[\"train\"] = datasets.Dataset.from_dict(\n                {\"input_col\": [\"\"], \"output_col\": [\"\"]}\n            )\n            return datasets.DatasetDict(dataset_dict)\n\n    \ndef get_transformed_data_input_output(path='./selected_dataset', task_name='mconala', use_score=True, use_full_input=True, use_azure=True, instruction='', examples='', number=2000):\n    if use_full_input and use_score:\n        json_file_path = \"./tasks/\" + task_name + \"/selected_dataset.json\"\n        json_file_path2 = \"./tasks/\" + task_name + \"/retrieved_data_rank_score_dataset.json\"\n        data_list2 = load_json_file(json_file_path2)\n    elif use_score:\n        json_file_path = \"./tasks/\" + task_name + \"/retrieved_data_rank_score.json\"\n    else:\n        json_file_path = \"./tasks/\" + task_name + \"/retrieved_data_rank2.json\"\n    data_list = load_json_file(json_file_path)\n    if instruction == '':\n        if task_name == 'mconala':\n            instruction = \"\"\"Japanese-To-Python Generation\n                    Pythonで1行のコードを生成し、StackOverflowの日本語の質問を解決してください。コメントや式は含めないでください。インポート文も不要です。\n                    このタスクでは、入力は日本語のテキストで、変数名や操作が記述されています。出力は、そのタスクを達成するためのPythonの1行のコードです。コメントや式は含めないでください。インポート文も不要です。\n                    \n                    Given a Japanese instruction, generate the according python code.\n                    \"\"\"\n            examples = \"\"\"\n            ```json\n            {\n            \\\"input\\\": \\\"スペースで区切られた入力`stdin`を変数に格納して表示する\\\",\n            \\\"output\\\": \\\"for line in stdin: a = line.rstrip().split(' ') print(a)\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"リスト`word_list'内に出現する単語を数える\\\",\n            \\\"output\\\": \\\"Counter(word_list)\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"tweepyインスタンス`api`を使い、文字列`word`を含んだツイートを検索し、結果をリストとして得る\\\",\n            \\\"output\\\": \\\"search = api.search(q=word)\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"データベースの設定を表示する\\\",\n            \\\"output\\\": \\\"print(settings.DATABASES)\\\",\n            }\n            \n            {        \n            \\\"input\\\": \\\"ネストされているリスト`li`を見やすく表示する\\\",\n            \\\"output\\\": \\\"pprint.pprint(li)\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"HTMLファイル'test.html'を開き、テキストオブジェクト'text'をutf-8で保存する\\\",\n            \\\"output\\\": \\\"f = open('test.html', 'w') f.write(text.encode('utf-8'))\\\",\n            }\n                    \"\"\"\n            task_specific_instruction = \"\"\"\n            You need to make sure that the input is a Japanese instruction to write a Python code. the output is the corresponding Python code! \n            You may need to do modification with the provided data, eg translate english instruction into Japanese instruction.\n            Don't try to be brief, make sure you output the complete information (no omission).\n            \"\"\"\n\n        elif task_name == 'mnli':\n            instruction = \"\"\"\n                Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral).\n            \"\"\"\n            examples =  \"\"\"\n            {\n            \\\"input\\\": \\\"Premise: She smiled back. Hypothesis: She was so happy she couldn't stop smiling.\\\",\n            \\\"output\\\": \\\"The premise states that she smiled back, which does not show whether she couldn't stop smiling or not. so the answer is Neutral\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"Premise: And to show just how fast Japan's new rulers were catching on, two punitive expeditions were launched against Korea and China in the grand manner of 19th-century gunboat diplomacy. Hypothesis: Japan's new rulers were catching on quickly.\\\",\n            \\\"output\\\": \\\"The premise states that the Japan's new rulers were catching on quickly and uses an example to demonstrate this. This entails the hypothesis that Japan's new rulers were catching on quickly. so the answer is Entailment\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"Premise: Fun for adults and children. Hypothesis: Fun for only children.\\\",\n            \\\"output\\\": \\\"The premise indicates that the fun is for both adults and children, both contradicts with \\\"only children\\\", so the answer is Contradiction\\\",\n            }\n            \"\"\"\n            task_specific_instruction = \"\"\"\n            Try to use the Data Sample. \n            Don't try to be brief, make sure you output the complete information (no omission).\n            \"\"\"\n\n        elif task_name == 'squad':\n            instruction = \"\"\"\n            Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. The context is a descriptive passage related to the question and contains the answer. And the question can range from Math, Cultural, Social, Geometry, Biology, History, Sports, Technology, Science, and so on.\n            \"\"\"\n            examples =  \"\"\"\n            ```json\n            {\n            \\\"input\\\": \\\"Question: What city did Super Bowl 50 take place in? Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\\\",\n            \\\"output\\\": \\\"Santa Clara\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"Question: What river runs through Warsaw? Context: Warsaw (Polish: Warszawa [varˈʂava] ( listen); see also other names) is the capital and largest city of Poland. It stands on the Vistula River in east-central Poland, roughly 260 kilometres (160 mi) from the Baltic Sea and 300 kilometres (190 mi) from the Carpathian Mountains. Its population is estimated at 1.740 million residents within a greater metropolitan area of 2.666 million residents, which makes Warsaw the 9th most-populous capital city in the European Union. The city limits cover 516.9 square kilometres (199.6 sq mi), while the metropolitan area covers 6,100.43 square kilometres (2,355.39 sq mi).\\\",\n            \\\"output\\\": \\\"Vistula River\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"Question: The Ottoman empire controlled territory on three continents, Africa, Asia and which other? Context: The Ottoman Empire was an imperial state that lasted from 1299 to 1923. During the 16th and 17th centuries, in particular at the height of its power under the reign of Suleiman the Magnificent, the Ottoman Empire was a powerful multinational, multilingual empire controlling much of Southeast Europe, Western Asia, the Caucasus, North Africa, and the Horn of Africa. At the beginning of the 17th century the empire contained 32 provinces and numerous vassal states. Some of these were later absorbed into the empire, while others were granted various types of autonomy during the course of centuries.\\\",\n            \\\"output\\\": \\\"Europe\\\",\n            }\n            ```\n            \"\"\"\n            task_specific_instruction = ''\n    \n    else:\n        task_specific_instruction = ''\n    dataset = []\n    if use_full_input and use_score:\n        for item, item2 in zip(data_list, data_list2):\n            row = item\n            if len(str(row)) > 4000:\n                row_input = str(item2[2])\n                row_output = str(item2[3])\n                row = {\"input\": row_input, \"output\": row_output}\n            dataset.append(row)\n    else:\n        for num, item in enumerate(data_list):\n            if use_full_input:\n                row = item\n            elif use_score:\n                row_input = str(item[2])\n                row_output = str(item[3])\n                row = {\"input\": row_input, \"output\": row_output}\n            else:\n                scores, rank = item[0], item[1]\n                row_input = str(scores[3])\n                row_output = str(scores[4])\n                row = {\"input\": row_input, \"output\": row_output}\n            dataset.append(row)\n    prompt_transformer = PromptBasedDatasetTransformer()\n    if use_full_input:\n        save_path = \"./tasks/\"+task_name+\"/transformed_data_score_use_full_row_dataset.csv\"\n    elif use_score:\n        save_path = \"./tasks/\"+task_name+\"/transformed_data_score.csv\"\n    else:\n        save_path = \"./tasks/\"+task_name+\"/transformed_data.csv\"\n    data = prompt_transformer.transform_data(instruction, examples, task_specific_instruction, dataset, number, save_path, use_azure=use_azure)\n    save_dataset_to_csv(data, save_path)\n\n    \ndef get_filtered_data_input_output(path='./selected_rows_rank', task_name='squad', use_score=True, use_full_input=True, use_azure=True, instruction='', examples='', number=2000):\n    if use_full_input:\n        file_path = \"./tasks/\"+task_name+\"/transformed_data_score_use_full_row_dataset.csv\"\n    elif use_score:\n        file_path = \"./tasks/\"+task_name+\"/transformed_data_score.csv\"\n    else:\n        file_path = \"./tasks/\"+task_name+\"/transformed_data.csv\"\n    data_list = []\n    with open(file_path, 'r') as f:\n        csvFile = csv.DictReader(f)\n        for lines in csvFile:\n            row = {\"input\": lines['input'], \"output\": lines['output']}\n            data_list.append(row)\n    if instruction == '':\n        if task_name == 'mconala':\n            instruction = \"\"\"Japanese-To-Python Generation\n                    Pythonで1行のコードを生成し、StackOverflowの日本語の質問を解決してください。コメントや式は含めないでください。インポート文も不要です。\n                    このタスクでは、入力は日本語のテキストで、変数名や操作が記述されています。出力は、そのタスクを達成するためのPythonの1行のコードです。コメントや式は含めないでください。インポート文も不要です。\n                    \n                    Given a Japanese instruction, generate the according python code.\n                    \"\"\"\n            examples = \"\"\"\n            ```json\n            {\n            \\\"input\\\": \\\"スペースで区切られた入力`stdin`を変数に格納して表示する\\\",\n            \\\"output\\\": \\\"for line in stdin: a = line.rstrip().split(' ') print(a)\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"リスト`word_list'内に出現する単語を数える\\\",\n            \\\"output\\\": \\\"Counter(word_list)\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"tweepyインスタンス`api`を使い、文字列`word`を含んだツイートを検索し、結果をリストとして得る\\\",\n            \\\"output\\\": \\\"search = api.search(q=word)\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"データベースの設定を表示する\\\",\n            \\\"output\\\": \\\"print(settings.DATABASES)\\\",\n            }\n            \n            {        \n            \\\"input\\\": \\\"ネストされているリスト`li`を見やすく表示する\\\",\n            \\\"output\\\": \\\"pprint.pprint(li)\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"HTMLファイル'test.html'を開き、テキストオブジェクト'text'をutf-8で保存する\\\",\n            \\\"output\\\": \\\"f = open('test.html', 'w') f.write(text.encode('utf-8'))\\\",\n            }\n                    \"\"\"\n            task_specific_instruction = \"\"\"\n            You need to make sure that the input is a Japanese instruction to write a Python code. the output is the corresponding Python code! \n            You may need to do modification with the provided data, eg translate english instruction into Japanese instruction.\n            Don't try to be brief, make sure you output the complete information (no omission).\n            \"\"\"\n\n        elif task_name == 'mnli':\n            instruction = \"\"\"\n                Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral).\n            \"\"\"\n            examples =  \"\"\"\n            {\n            \\\"input\\\": \\\"Premise: She smiled back. Hypothesis: She was so happy she couldn't stop smiling.\\\",\n            \\\"output\\\": \\\"The premise states that she smiled back, which does not show whether she couldn't stop smiling or not. so the answer is Neutral\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"Premise: And to show just how fast Japan's new rulers were catching on, two punitive expeditions were launched against Korea and China in the grand manner of 19th-century gunboat diplomacy. Hypothesis: Japan's new rulers were catching on quickly.\\\",\n            \\\"output\\\": \\\"The premise states that the Japan's new rulers were catching on quickly and uses an example to demonstrate this. This entails the hypothesis that Japan's new rulers were catching on quickly. so the answer is Entailment\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"Premise: Fun for adults and children. Hypothesis: Fun for only children.\\\",\n            \\\"output\\\": \\\"The premise indicates that the fun is for both adults and children, both contradicts with \\\"only children\\\", so the answer is Contradiction\\\",\n            }\n            \"\"\"\n            task_specific_instruction = \"\"\"\n            Try to use the Data Sample. \n            Don't try to be brief, make sure you output the complete information (no omission).\n            \"\"\"\n\n        elif task_name == 'squad':\n            instruction = \"\"\"\n            Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. The context is a descriptive passage related to the question and contains the answer. And the question can range from Math, Cultural, Social, Geometry, Biology, History, Sports, Technology, Science, and so on.\n            \"\"\"\n            examples =  \"\"\"\n            ```json\n            {\n            \\\"input\\\": \\\"Question: What city did Super Bowl 50 take place in? Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\\\",\n            \\\"output\\\": \\\"Santa Clara\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"Question: What river runs through Warsaw? Context: Warsaw (Polish: Warszawa [varˈʂava] ( listen); see also other names) is the capital and largest city of Poland. It stands on the Vistula River in east-central Poland, roughly 260 kilometres (160 mi) from the Baltic Sea and 300 kilometres (190 mi) from the Carpathian Mountains. Its population is estimated at 1.740 million residents within a greater metropolitan area of 2.666 million residents, which makes Warsaw the 9th most-populous capital city in the European Union. The city limits cover 516.9 square kilometres (199.6 sq mi), while the metropolitan area covers 6,100.43 square kilometres (2,355.39 sq mi).\\\",\n            \\\"output\\\": \\\"Vistula River\\\",\n            }\n            \n            {\n            \\\"input\\\": \\\"Question: The Ottoman empire controlled territory on three continents, Africa, Asia and which other? Context: The Ottoman Empire was an imperial state that lasted from 1299 to 1923. During the 16th and 17th centuries, in particular at the height of its power under the reign of Suleiman the Magnificent, the Ottoman Empire was a powerful multinational, multilingual empire controlling much of Southeast Europe, Western Asia, the Caucasus, North Africa, and the Horn of Africa. At the beginning of the 17th century the empire contained 32 provinces and numerous vassal states. Some of these were later absorbed into the empire, while others were granted various types of autonomy during the course of centuries.\\\",\n            \\\"output\\\": \\\"Europe\\\",\n            }\n            ```\n            \"\"\"\n            task_specific_instruction = ''\n    \n    else:\n        task_specific_instruction = ''\n    \n    prompt_transformer = PromptBasedDatasetTransformer(construct_prompt_for_filter_data)\n    if use_full_input:\n        save_path = \"./tasks/\"+task_name+\"/transformed_data_score_use_full_row_dataset_filtered.csv\"\n    elif use_score:\n        save_path = \"./tasks/\"+task_name+\"/transformed_data_score_filtered.csv\"\n    else:\n        save_path = \"./tasks/\"+task_name+\"/transformed_data_filtered.csv\"\n    data = prompt_transformer.filter_data(instruction, examples, task_specific_instruction, data_list, 1000, save_path, use_azure=use_azure)\n    save_dataset_to_csv(data, save_path)\n\n\nif __name__ == \"__main__\":\n    \n    parser = argparse.ArgumentParser(description=\"Retrieve data for a specific task.\") \n    parser.add_argument(\"--task_name\", type=str, help=\"Name of the task.\")  \n    args = parser.parse_args()\n    TASK_NAME_LIST = [args.task_name]    \n    use_filter = False\n    use_azure = False\n    \n    for task_name in TASK_NAME_LIST:\n        if \"bbh\" not in task_name:\n            if use_filter:\n                get_filtered_data_input_output(task_name=task_name, use_azure=use_azure, number=2000)\n            else:\n                get_transformed_data_input_output(task_name=task_name, use_azure=use_azure, number=2000)\n        else:\n            with open('./bbh/instructions.json', 'r') as file:\n                data_instructions = json.load(file)\n            with open('./bbh/template.json', 'r') as file:\n                data_examples = json.load(file)\n            for task_name in data_examples.keys():\n                # with open(\"./tasks/bbh/\"+task_name+\"/transformed_data_score_use_full_row_dataset.csv\", 'r') as file:\n                #     reader = csv.reader(file)\n                #     # Skip the header\n                #     next(reader)\n                #     # Count the rows\n                #     row_count = sum(1 for row in reader)\n                    \n                instruction = data_instructions[task_name]\n                example_list = data_examples[task_name]\n                input_prompt_list = [example[0] for example in example_list]\n                output_prompt_list = [example[1] for example in example_list]\n                example = EXAMPLE_TEMPLATE.format(\n                    input_one=input_prompt_list[0],\n                    input_two=input_prompt_list[1],\n                    input_three=input_prompt_list[2],\n                    output_one=output_prompt_list[0],\n                    output_two=output_prompt_list[1],\n                    output_three=output_prompt_list[2],\n                )\n                if use_filter:\n                    get_filtered_data_input_output(task_name=\"bbh/\"+task_name, use_azure=use_azure, instruction=instruction, examples=example, number=2000)\n                else:\n                    get_transformed_data_input_output(task_name=\"bbh/\"+task_name, use_azure=use_azure, instruction=instruction, examples=example, number=2000)            "}
{"type": "source_file", "path": "finetune/config.py", "content": "HF_TOKEN = \"\""}
{"type": "source_file", "path": "finetune/finetune.py", "content": "from unsloth import FastLanguageModel\nimport torch\nimport csv\nfrom datasets import Dataset\nfrom prompt import squad_prompt_template\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport argparse\nfrom config import HF_TOKEN\n\ndef formatting_prompts_func(examples):\n    texts = []\n    num_examples = len(examples['question'])\n    for index in range(num_examples): \n        \n        input = examples['question'][index]\n        output = examples['answer'][index]\n        text = prompt_template.format(input=str(input), response=str(output)) + EOS_TOKEN\n        texts.append(text)\n        \n    return { \"text\" : texts, }\n\n\ndef load_dataset_from_csv(file_path):\n    \"\"\"Load the dataset from a CSV file back into a Python dictionary.\"\"\"    \n    with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n        reader = csv.DictReader(file)\n        dataset_rows = {'question': [], 'answer': []}\n        for row in reader:\n            # Each row is a dictionary with keys matching the CSV column headers ('input' and 'output' )\n            dataset_rows['question'].append(row['input'])\n            dataset_rows['answer'].append(row['output'])\n    return Dataset.from_dict(dataset_rows)\n\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser(description=\"Train and fine-tune a language model.\")\n    parser.add_argument(\"--model_name\", type=str, help=\"Name of the pre-trained model.\")\n    parser.add_argument(\"--data_path\", type=str, help=\"Path to the training data CSV file.\")\n    parser.add_argument(\"--finetuned_model_name\", type=str, help=\"Name of the fine-tuned model.\")\n\n    args = parser.parse_args()\n    max_seq_length = 2048 \n    dtype = None\n\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = args.model_name,\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = True,\n        token = HF_TOKEN\n    )\n\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                        \"gate_proj\", \"up_proj\", \"down_proj\",],\n        lora_alpha = 16,\n        lora_dropout = 0, \n        bias = \"none\",    \n        use_gradient_checkpointing = \"unsloth\", \n        random_state = 3407,\n        use_rslora = False,\n        loftq_config = None,\n    )\n\n    prompt_template = squad_prompt_template\n\n    EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\n    dataset = load_dataset_from_csv(args.data_path)\n    dataset = dataset.map(formatting_prompts_func, batched = True,)\n\n    # Train model\n    trainer = SFTTrainer(\n        model = model,\n        tokenizer = tokenizer,\n        train_dataset = dataset,\n        dataset_text_field = \"text\",\n        max_seq_length = max_seq_length,\n        dataset_num_proc = 2,\n        packing = False, # Can make training 5x faster for short sequences.\n        args = TrainingArguments(\n            per_device_train_batch_size = 2,\n            gradient_accumulation_steps = 4,\n            warmup_steps = 20,\n            max_steps = -1,\n            num_train_epochs=1,\n            learning_rate = 3e-4,\n            fp16 = not torch.cuda.is_bf16_supported(),\n            bf16 = torch.cuda.is_bf16_supported(),\n            logging_steps = 1,\n            optim = \"adamw_8bit\",\n            weight_decay = 0.01,\n            lr_scheduler_type = \"linear\",\n            seed = 3407,\n            output_dir = \"unsloth_outputs_8_shots\",\n            report_to=\"none\", \n        ),\n    )\n    trainer_stats = trainer.train()\n\n    # Save model to huggingface hub\n    model.push_to_hub(args.finetuned_model_name, token = HF_TOKEN)\n    tokenizer.push_to_hub(args.finetuned_model_name, token = HF_TOKEN)"}
{"type": "source_file", "path": "finetune/prompt.py", "content": "mnli_3_shots_prompt_template = \"\"\"Below is an instruction that describes a task, paired with some examples that provide further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGiven a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral).\n\n### Examples:\nInput: Premise: She smiled back. Hypothesis: She was so happy she couldn't stop smiling.\nResponse: Neutral.\nInput: Premise: And to show just how fast Japan's new rulers were catching on, two punitive expeditions were launched against Korea and China in the grand manner of 19th-century gunboat diplomacy. Hypothesis: Japan's new rulers were catching on quickly.\nResponse: Entailment.\nInput: Premise: Fun for adults and children. Hypothesis: Fun for only children.\nResponse: Contradiction.\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nbbh_3_shots_cot_prompt_template = \"\"\"\nBelow is an instruction that describes a task, paired with some examples that provide further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Examples:\nInput: {input1}\nResponse: {response1}\nInput: {input2}\nResponse: {response2}\nInput: {input3}\nResponse: {response3}\n\n### Input:\n{input}\n\n### Response:\n{response}\n\"\"\"\nbbh_all_prompts = {}\n\"\"\"Load all the bbh prompts\"\"\"\nimport json\nimport csv\nwith open('../data_preparation/bbh/instructions.json', 'r') as file:\n    data_instructions = json.load(file)\nwith open('../data_preparation/bbh/template.json', 'r') as file:\n    data_examples = json.load(file)\nfor task_name in data_examples.keys():\n    instruction = data_instructions[task_name]\n    example_list = data_examples[task_name]\n    input_prompt_list = [example[0] for example in example_list]\n    output_prompt_list = [\"Let's think step by step. \"+example[1] for example in example_list]\n    example = bbh_3_shots_cot_prompt_template.format(\n        instruction=instruction,\n        input1=input_prompt_list[0],\n        input2=input_prompt_list[1],\n        input3=input_prompt_list[2],\n        response1=output_prompt_list[0],\n        response2=output_prompt_list[1],\n        response3=output_prompt_list[2],\n        input=\"{input}\",\n        response=\"{response}\"\n    )\n    bbh_all_prompts[task_name] = example\n    \njapanese_prompt_template =\"\"\"Below is an instruction that describes a task, paired with some examples that provide further context. Write a response that appropriately completes the request.\n\n### Instruction:\nJapanese-To-Python Generation\nPythonで1行のコードを生成し、StackOverflowの日本語の質問を解決してください。コメントや式は含めないでください。インポート文も不要です。\nこのタスクでは、入力は日本語のテキストで、変数名や操作が記述されています。出力は、そのタスクを達成するためのPythonの1行のコードです。コメントや式は含めないでください。インポート文も不要です。\n                \nGiven a Japanese instruction, generate the according python code.\n\n### Examples:\nInput: スペースで区切られた入力`stdin`を変数に格納して表示する\nResponse: for line in stdin: a = line.rstrip().split(' ') print(a)\nInput: リスト`word_list'内に出現する単語を数える\nResponse: Counter(word_list)\nInput: tweepyインスタンス`api`を使い、文字列`word`を含んだツイートを検索し、結果をリストとして得る\nResponse: search = api.search(q=word)\nInput: データベースの設定を表示する\nResponse: print(settings.DATABASES)\nInput: ネストされているリスト`li`を見やすく表示する\nResponse: pprint.pprint(li)\nInput: HTMLファイル'test.html'を開き、テキストオブジェクト'text'をutf-8で保存する\nResponse: f = open('test.html', 'w') f.write(text.encode('utf-8'))\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nsquad_prompt_template =\"\"\"Below is an instruction that describes a task, paired with some examples that provide further context. Write a response that appropriately completes the request.\n\n### Instruction:\nYour task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. The context is a descriptive passage related to the question and contains the answer. And the question can range from Math, Cultural, Social, Geometry, Biology, History, Sports, Technology, Science, and so on.\n\n### Examples:\nInput: \"Question: What city did Super Bowl 50 take place in? Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\"\nResponse: \"Santa Clara\"\nInput: \"Question: What river runs through Warsaw? Context: Warsaw (Polish: Warszawa [varˈʂava] ( listen); see also other names) is the capital and largest city of Poland. It stands on the Vistula River in east-central Poland, roughly 260 kilometres (160 mi) from the Baltic Sea and 300 kilometres (190 mi) from the Carpathian Mountains. Its population is estimated at 1.740 million residents within a greater metropolitan area of 2.666 million residents, which makes Warsaw the 9th most-populous capital city in the European Union. The city limits cover 516.9 square kilometres (199.6 sq mi), while the metropolitan area covers 6,100.43 square kilometres (2,355.39 sq mi).\"\nResponse: \"Vistula River\"\nInput: \"Question: The Ottoman empire controlled territory on three continents, Africa, Asia and which other? Context: The Ottoman Empire was an imperial state that lasted from 1299 to 1923. During the 16th and 17th centuries, in particular at the height of its power under the reign of Suleiman the Magnificent, the Ottoman Empire was a powerful multinational, multilingual empire controlling much of Southeast Europe, Western Asia, the Caucasus, North Africa, and the Horn of Africa. At the beginning of the 17th century the empire contained 32 provinces and numerous vassal states. Some of these were later absorbed into the empire, while others were granted various types of autonomy during the course of centuries.\"\nResponse: \"Europe\"\n\n### Input:\n{input}\n\n### Response:\n{response}\"\"\"\n"}
{"type": "source_file", "path": "data_preparation/retrieve_data.py", "content": "from sentence_transformers import SentenceTransformer, util\nimport numpy as np\nimport json\nimport torch\nimport pickle\nfrom pathlib import Path\nimport os\nimport argparse\n\n\ndef get_model(device=\"cuda\"):\n    \"\"\"Start with sentence bert, could go with more complicated designs\"\"\"\n    model = SentenceTransformer('distiluse-base-multilingual-cased')\n    model = model.to(device)\n    return model\n\n\ndef batch_encode(model, texts, batch_size=128):\n    all_embeddings = []\n    for start_index in range(0, len(texts), batch_size):\n        batch_texts = texts[start_index:start_index+batch_size]\n        embeddings = model.encode(batch_texts, show_progress_bar=False, convert_to_tensor=True)\n        all_embeddings.extend(embeddings)\n    return all_embeddings\n\n\ndef load_full_embedding():\n    #Load sentences & embeddings from disc\n    embeddings_file_path = 'embeddings_dataset_final.pkl'  # File to store embeddings_1\n    with open(embeddings_file_path, \"rb\") as fIn:\n        stored_data = pickle.load(fIn)\n        stored_sentences = stored_data['ids']\n        stored_embeddings = stored_data['embeddings']\n    stored_values_file = 'stored_values.pkl'\n    with open(stored_values_file, 'rb') as f:\n        stored_values = pickle.load(f)\n    return stored_sentences, stored_embeddings, stored_values\n\n\ndef load_dataset_embedding():\n    #Load sentences & embeddings from disc\n    embeddings_file_path = 'embeddings_dataset_final.pkl'  # File to store embeddings_1\n    with open(embeddings_file_path, \"rb\") as fIn:\n        stored_data = pickle.load(fIn)\n        stored_sentences = stored_data['ids']\n        stored_embeddings = stored_data['embeddings']\n    \n    stored_values_file = 'stored_dict.pkl'\n    with open(stored_values_file, 'rb') as f:\n        stored_values = pickle.load(f)\n    return stored_sentences,stored_embeddings\n\n\nstored_sentences, stored_embeddings, stored_values = load_full_embedding()\nstored_sentences2,stored_dataset_embeddings = load_dataset_embedding()\n\n\ndef score_data_points(input_prompt_list, output_prompt_list, instruction, model, use_sample=False, batch_size=64):\n    scores = {}\n    prompt_embedding_list = []\n    dataset_embedding = model.encode([instruction], convert_to_tensor=True)\n    for input_prompt, output_prompt in zip(input_prompt_list, output_prompt_list):\n        prompt_embedding = model.encode([input_prompt, output_prompt], convert_to_tensor=True)\n        prompt_embedding_list.append(prompt_embedding)\n\n    stored_embeddings_tensor = torch.tensor(stored_embeddings).to(prompt_embedding.device) \n    input_scores_list = []\n    output_scores_list = []\n    # Compute cosine similarities\n    for prompt_embedding in prompt_embedding_list:\n        input_scores = util.pytorch_cos_sim(prompt_embedding[0], stored_embeddings_tensor)\n        output_scores = util.pytorch_cos_sim(prompt_embedding[1], stored_embeddings_tensor)\n        input_scores_list.append(input_scores)\n        output_scores_list.append(output_scores)\n    input_scores = torch.stack(input_scores_list).mean(dim=0)\n    output_scores = torch.stack(output_scores_list).mean(dim=0)\n    del stored_embeddings_tensor\n    stored_dataset_embeddings_tensor = torch.tensor(stored_dataset_embeddings).to(prompt_embedding.device)\n    dataset_scores = util.pytorch_cos_sim(dataset_embedding[0], stored_dataset_embeddings_tensor)\n    \n    # Iterate through each stored sentence to populate the scores dictionary\n    for idx, sentence_id in enumerate(stored_sentences):\n        key = sentence_id  # Assuming sentence_id can be used directly as key\n        value = stored_values[idx]\n        if key not in scores:\n            scores[key] = {'input': [], 'output': [], 'dataset': []}\n\n        # Append the cosine similarity scores and corresponding values\n        # Note: The actual 'value' associated with each embedding needs to be retrieved if necessary\n        dataset_score_float = float(dataset_scores[0][idx])\n        input_score_float = float(input_scores[0][idx])\n        output_score_float = float(output_scores[0][idx])\n\n        scores[key]['input'].append((input_score_float, key, value))\n        scores[key]['output'].append((output_score_float, key, value))\n        scores[key]['dataset'].append((dataset_score_float, key, value))\n\n    return scores\n                \n\ndef select_top_data_points_average_score(scores, max_points=3000, banned_name=None):\n    final_scores_with_values = []\n    for key, score_lists in scores.items():\n        # Find the data point with the maximum input score and its corresponding value\n        max_input = max(score_lists['input'], key=lambda x: x[0])\n        max_input_score, max_input_id, max_input_value = max_input\n\n        # Find the data point with the maximum output score and its corresponding value\n        max_output = max(score_lists['output'], key=lambda x: x[0])\n        max_output_score, max_output_id, max_output_value = max_output\n\n        # Find the data point with the maximum output score and its corresponding value\n        max_dataset = max(score_lists['dataset'], key=lambda x: x[0])\n        max_dataset_score, max_dataset_id, max_dataset_value = max_dataset\n        \n        # Calculate the final score\n        final_score = (max_input_score + max_output_score + max_dataset_score) / 3\n        final_scores_with_values.append((final_score, key, max_input_value, max_output_value))\n\n    # Sort by final score and select top data points\n    final_scores_with_values.sort(key=lambda x: x[0], reverse=True)\n    tot = 0\n    top_data_points = []\n    for item in final_scores_with_values:\n        if banned_name in item[1]:\n            continue\n        if tot == max_points:\n            break\n        tot += 1\n        top_data_points.append(item)\n    return top_data_points\n\n\ndef select_top_data_points_average_rank(scores, max_points=3000, use_method='average'):\n    final_scores_with_values = []\n    for key, score_lists in scores.items():\n        # Find the data point with the maximum input score and its corresponding value\n        max_input = max(score_lists['input'], key=lambda x: x[0])\n        max_input_score, max_input_id, max_input_value = max_input\n\n        # Find the data point with the maximum output score and its corresponding value\n        max_output = max(score_lists['output'], key=lambda x: x[0])\n        max_output_score, max_output_id, max_output_value = max_output\n\n        # Calculate the final score\n        final_scores_with_values.append((max_input_score, max_output_score, key,  max_input_value, max_output_value))\n\n    # Sort by final score and select top data points\n    if use_method == 'average':\n        final_scores_with_values = sort_by_average_rank(final_scores_with_values)\n    elif use_method == 'max':\n        final_scores_with_values = sort_by_max_rank(final_scores_with_values)\n    top_data_points = final_scores_with_values[:max_points]\n    return top_data_points\n\n\ndef calculate_ranks(items, score_index):\n    # Sort items based on a specific score and calculate ranks\n    sorted_items = sorted(items, key=lambda x: x[score_index], reverse=True)\n    ranks = {item: rank for rank, item in enumerate(sorted_items, start=1)}\n    return ranks\n    \n\ndef sort_by_average_rank(items):\n    # Calculate ranks based on input and output scores\n    input_ranks = calculate_ranks(items, 0)  # 0 is the index for input_score\n    output_ranks = calculate_ranks(items, 1)  # 1 is the index for output_score\n    new_item_list = []\n    # Calculate average rank for each item\n    for item in items:\n        input_rank = input_ranks[item]\n        output_rank = output_ranks[item]\n        avg_rank = (input_rank + output_rank) / 2\n        new_item_list.append((item, avg_rank))\n        # item.append(avg_rank)  # Append average rank to each item\n\n    # Sort items based on average rank\n    sorted_by_avg_rank = sorted(new_item_list, key=lambda x: x[-1])  # Sorting by the last element, which is avg_rank\n\n    return sorted_by_avg_rank\n\n\ndef sort_by_max_rank(items):\n    # Calculate ranks based on input and output scores\n    input_ranks = calculate_ranks(items, 0)  # 0 is the index for input_score\n    output_ranks = calculate_ranks(items, 1)  # 1 is the index for output_score\n    new_item_list = []\n    # Calculate average rank for each item\n    for item in items:\n        input_rank = input_ranks[item]\n        output_rank = output_ranks[item]\n        max_rank = max(input_rank, output_rank)\n        new_item_list.append((item, max_rank))\n        # item.append(avg_rank)  # Append average rank to each item\n\n    # Sort items based on average rank\n    sorted_by_avg_rank = sorted(new_item_list, key=lambda x: x[-1])  # Sorting by the last element, which is avg_rank\n\n    return sorted_by_avg_rank\n\n\ndef retrieve_data(jsonl_file_path, input_prompt, output_prompt, instruction, model, task_name, compare_type=\"score\"):\n    scored_data = score_data_points(input_prompt, output_prompt, instruction, model)\n    if compare_type == \"score\":\n        top_data_points = select_top_data_points_average_score(scored_data, banned_name=task_name)\n    else:\n        top_data_points = select_top_data_points_average_rank(scored_data)\n    return top_data_points\n\n\ndef write_data_to_json(file_path, data):\n    print('get into write data to json')\n    folder_path = os.path.dirname(file_path)\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n    with open(file_path, 'w', encoding='utf-8') as file:\n        json.dump(data, file, ensure_ascii=False, indent=4)\n    \n\ndef get_dataset(input_example, output_example, instruction, task_name):\n    model = get_model()\n    jsonl_file_path = \"datasets_merged_flattened_final.jsonl\"\n    retrieved_data = retrieve_data(jsonl_file_path, input_example, output_example, instruction, model, task_name)\n    return retrieved_data\n\n\ndef retrieve(task_name):\n    if task_name=='mconala':\n        instruction = \"\"\"\n        Given a Japanese instruction, generate the according python code.\n        \"\"\"\n        input_prompt = \"スペースで区切られた入力`stdin`を変数に格納して表示する\"\n        output_prompt = \"for line in stdin: a = line.rstrip().split(' ') print(a)\"\n        input_prompt2 = \"HTMLファイル'test.html'を開き、テキストオブジェクト'text'をutf-8で保存する\"\n        output_prompt2 = \"f = open('test.html', 'w') f.write(text.encode('utf-8'))\"\n        input_prompt3 = \"tweepyインスタンス`api`を使い、文字列`word`を含んだツイートを検索し、結果をリストとして得る\"\n        output_prompt3 = \"search = api.search(q=word)\"\n        input_prompt_list = [input_prompt, input_prompt2, input_prompt3]\n        output_prompt_list = [output_prompt, output_prompt2, output_prompt3]\n        \n    elif task_name == 'mnli':\n        instruction = \"\"\"\n        Given a premise sentence and a hypothesis sentence, predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral).\n        \"\"\"\n        input_prompt = \"Premise: She smiled back. Hypothesis: She was so happy she couldn't stop smiling.\"\n        output_prompt = \"Neutral\"   \n        input_prompt2 = \"Premise: And to show just how fast Japan's new rulers were catching on, two punitive expeditions were launched against Korea and China in the grand manner of 19th-century gunboat diplomacy. Hypothesis: Japan's new rulers were catching on quickly.\"\n        output_prompt2 = \"Entailment\"\n        input_prompt3 = \"Premise: Fun for adults and children. Hypothesis: Fun for only children.\"\n        output_prompt3 = \"Contradiction\"\n        input_prompt_list = [input_prompt, input_prompt2, input_prompt3]\n        output_prompt_list = [output_prompt, output_prompt2, output_prompt3]\n\n    elif task_name == 'squad':\n        instruction = \"\"\"\n        Your task is to generate an answer to a natural question. In this task, the input is a string that consists of both a question and a context passage. The context is a descriptive passage related to the question and contains the answer. And the question can range from Math, Cultural, Social, Geometry, Biology, History, Sports, Technology, Science, and so on.\n        \"\"\"\n        input_prompt = \"\"\"Question: What city did Super Bowl 50 take place in? Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\"\"\"\n        output_prompt = \"\"\"Santa Clara\"\"\"\n        input_prompt2 = \"\"\"Question: What river runs through Warsaw? Context: Warsaw (Polish: Warszawa [varˈʂava] ( listen); see also other names) is the capital and largest city of Poland. It stands on the Vistula River in east-central Poland, roughly 260 kilometres (160 mi) from the Baltic Sea and 300 kilometres (190 mi) from the Carpathian Mountains. Its population is estimated at 1.740 million residents within a greater metropolitan area of 2.666 million residents, which makes Warsaw the 9th most-populous capital city in the European Union. The city limits cover 516.9 square kilometres (199.6 sq mi), while the metropolitan area covers 6,100.43 square kilometres (2,355.39 sq mi).\"\"\"\n        output_prompt2 = \"\"\"Vistula River\"\"\"\n        input_prompt3 = \"\"\"Question: The Ottoman empire controlled territory on three continents, Africa, Asia and which other? Context: The Ottoman Empire was an imperial state that lasted from 1299 to 1923. During the 16th and 17th centuries, in particular at the height of its power under the reign of Suleiman the Magnificent, the Ottoman Empire was a powerful multinational, multilingual empire controlling much of Southeast Europe, Western Asia, the Caucasus, North Africa, and the Horn of Africa. At the beginning of the 17th century the empire contained 32 provinces and numerous vassal states. Some of these were later absorbed into the empire, while others were granted various types of autonomy during the course of centuries.\"\"\"\n        output_prompt3 = \"\"\"Europe\"\"\"\n        input_prompt_list = [input_prompt, input_prompt2, input_prompt3]\n        output_prompt_list = [output_prompt, output_prompt2, output_prompt3]\n    \n    elif task_name == 'humaneval':\n        instruction = \"\"\"\n        You will be given a partially written Python function along with a detailed docstring describing its behavior. Your task is to complete the function such that it adheres to the given signature and correctly implements the described functionality.\n        \"\"\"\n        input_prompt = \"\"\"\n        def is_strictly_increasing(lst: list[int]) -> bool:\n            \\\"\\\"\\\"\n            Check whether a list of integers is strictly increasing.\n\n            Args:\n                lst (list[int]): A list of integers.\n\n            Returns:\n                bool: True if the list is strictly increasing, False otherwise.\n\n            Examples:\n                >>> is_strictly_increasing([1, 2, 3, 4])\n                True\n                >>> is_strictly_increasing([1, 2, 2, 3])\n                False\n            \\\"\\\"\\\"\n        \"\"\"\n        output_prompt = \"\"\"\n        return all(lst[i] < lst[i + 1] for i in range(len(lst) - 1))\n        \"\"\"\n        input_prompt2 = \"\"\"\n        def twoSum(self, nums: List[int], target: int) -> List[int]:\n            \\\"\\\"\\\"\n            Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. You may assume that each input would have exactly one solution, and you may not use the same element twice. You can return the answer in any order.\n        \n            Input: nums = [2,7,11,15], target = 9\n            Output: [0,1]\n\n            Input: nums = [3,2,4], target = 6\n            Output: [1,2]\n\n            Input: nums = [3,3], target = 6\n            Output: [0,1]\n            \\\"\\\"\\\"\n        \"\"\"\n        output_prompt2 = \"\"\"\n        for i in range(len(nums)):\n            for j in range(i + 1, len(nums)):\n                if nums[j] == target - nums[i]:\n                    return [i, j]\n        return []\n        \"\"\"\n        input_prompt3 = \"\"\"\n        # Definition for singly-linked list.\n        class ListNode:\n            def __init__(self, val=0, next=None):\n                self.val = val\n                self.next = next\n        def mergeTwoLists(self, list1: Optional[ListNode], list2: Optional[ListNode]) -> Optional[ListNode]:\n            \\\"\\\"\\\"\n            You are given the heads of two sorted linked lists list1 and list2. Merge the two lists into one sorted list. The list should be made by splicing together the nodes of the first two lists. Return the head of the merged linked list.\n            \n            Example 1:\n            Input: list1 = [1,2,4], list2 = [1,3,4]\n            Output: [1,1,2,3,4,4]\n            \n            Example 2:\n            Input: list1 = [], list2 = []\n            Output: []\n            \n            Example 3:\n            Input: list1 = [], list2 = [0]\n            Output: [0]\n            \\\"\\\"\\\"\n        \"\"\"\n        output_prompt3 = \"\"\"\n        if l1 is None:\n            return l2\n        elif l2 is None:\n            return l1\n        elif l1.val < l2.val:\n            l1.next = self.mergeTwoLists(l1.next, l2)\n            return l1\n        else:\n            l2.next = self.mergeTwoLists(l1, l2.next)\n            return l2\n        \"\"\"\n        input_prompt_list = [input_prompt, input_prompt2, input_prompt3]\n        output_prompt_list = [output_prompt, output_prompt2, output_prompt3]\n\n    # elif task_name == '':\n    #     instruction = \"\"\"\n    #     \"\"\"\n    #     input_prompt = \"\"\"\n    #     \"\"\"\n    #     output_prompt = \"\"\"\n    #     \"\"\"\n    #     input_prompt2 = \"\"\"\n    #     \"\"\"\n    #     output_prompt2 = \"\"\"\n    #     \"\"\"\n    #     input_prompt3 = \"\"\"\n    #     \"\"\"\n    #     output_prompt3 = \"\"\"\n    #     \"\"\"\n    #     input_prompt_list = [input_prompt, input_prompt2, input_prompt3]\n    #     output_prompt_list = [output_prompt, output_prompt2, output_prompt3]\n\n    retrieved_data = get_dataset(input_prompt_list, output_prompt_list, instruction, task_name)\n    output_file_path = \"./tasks/\"+task_name+\"/retrieved_data_rank_score_dataset.json\"\n    write_data_to_json(output_file_path, retrieved_data)\n\ndef write_data_to_json(file_path, data):\n    folder_path = os.path.dirname(file_path)\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n    with open(file_path, 'w', encoding='utf-8') as file:\n        json.dump(data, file, ensure_ascii=False, indent=4)\n\n                \ndef load_json_file(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            data = json.load(file)\n        return data\n    except FileNotFoundError:\n        print(f\"File not found: {file_path}\")\n        return None\n    except json.JSONDecodeError:\n        print(f\"Error decoding JSON from file: {file_path}\")\n        return None\n\ndef get_original_rows_from_file(task_name, use_sample=False, start=0, tot_num=3000, neglect_dataset=False):\n    \n    stored_values_file = 'stored_dict.pkl'\n    with open(stored_values_file, 'rb') as f:\n        stored_values = pickle.load(f)\n\n    json_file_path = \"./tasks/\" + task_name + \"/retrieved_data_rank_score_dataset.json\"\n    data_list = load_json_file(json_file_path)\n    row_list = []\n    cur_num = 0\n    for num, item in enumerate(data_list):\n        if cur_num==tot_num:\n            break\n        location = item[1]\n        if neglect_dataset:\n            if \"'dataset_id': 'gsm8k'\" in location:\n                print(\"found original gsm8k dataset!\")\n                continue\n        try:\n            # Retrieve the row\n            row = stored_values[location]\n            row_list.append(row)\n        except:\n            print(\"cannot find\" + location)\n        cur_num += 1\n\n    output_file_path = \"./tasks/\" + task_name + \"/selected_dataset.json\"\n    write_data_to_json(output_file_path, row_list)\n    \n    return row_list\n   \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Retrieve data for a specific task.\") \n    parser.add_argument(\"--task_name\", type=str, help=\"Name of the task.\")  \n    args = parser.parse_args()\n    TASK_NAME_LIST = [args.task_name]\n    for task_name in TASK_NAME_LIST:\n        if \"bbh\" not in task_name:\n            retrieve(task_name)\n            get_original_rows_from_file(task_name)\n        else:\n            with open('./bbh/instructions.json', 'r') as file:\n                data_instructions = json.load(file)\n            with open('./bbh/template.json', 'r') as file:\n                data_examples = json.load(file)\n            for task_name in data_examples.keys():\n                instruction = data_instructions[task_name]\n                example_list = data_examples[task_name]\n                input_prompt_list = [example[0] for example in example_list]\n                output_prompt_list = [example[1].split(\"answer is \")[1] for example in example_list]\n                path = Path(\"./tasks/bbh/\"+task_name)\n                path.mkdir(parents=True, exist_ok=True)\n                output_file_path = \"./tasks/bbh/\"+task_name+\"/retrieved_data_rank_score_dataset.json\"\n                jsonl_file_path = \"datasets_merged_flattened_final.jsonl\"\n                retrieved_data = get_dataset(input_prompt_list, output_prompt_list, instruction, task_name)\n                write_data_to_json(output_file_path, retrieved_data)\n                get_original_rows_from_file(\"bbh/\"+task_name)\n                \n            \n    "}
