{"repo_info": {"repo_name": "AVeriTeC", "repo_owner": "MichSchli", "repo_url": "https://github.com/MichSchli/AVeriTeC"}}
{"type": "source_file", "path": "data_loaders/DualEncoderDataLoader.py", "content": "from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\nimport pandas as pd\nimport numpy as np\nimport torch\nimport pytorch_lightning as pl\nimport json\nfrom nltk.tokenize import word_tokenize\nimport tqdm\nimport random\n\nclass DualEncoderDataLoader(pl.LightningDataModule):\n  def __init__(self, tokenizer, data_file, batch_size):\n    super().__init__()\n    self.tokenizer = tokenizer\n    self.data_file = data_file\n    self.batch_size = batch_size\n\n  def encode_sentences(self, positives, negatives, max_length=512, pad_to_max_length=False, return_tensors=\"pt\"):\n    encoded_dict = self.tokenizer(\n            positives,\n            max_length=max_length,\n            padding=\"max_length\" if pad_to_max_length else \"longest\",\n            truncation=True,\n            return_tensors=return_tensors\n        )\n\n    pos_input_ids = encoded_dict['input_ids']\n    pos_attention_masks = encoded_dict['attention_mask']\n\n    negatives_unstacked = list(np.reshape(negatives, -1))\n\n    encoded_dict = self.tokenizer(\n            negatives_unstacked,\n            max_length=max_length,\n            padding=\"max_length\" if pad_to_max_length else \"longest\",\n            truncation=True,\n            return_tensors=return_tensors\n        )\n\n    neg_input_ids = encoded_dict['input_ids'].view(len(negatives), len(negatives[0]), -1)\n    neg_attention_masks = encoded_dict['attention_mask'].view(len(negatives), len(negatives[0]), -1)\n  \n    batch = {\n        \"pos_input_ids\": pos_input_ids,\n        \"pos_attention_masks\": pos_attention_masks,\n        \"neg_input_ids\": neg_input_ids,\n        \"neg_attention_masks\": neg_attention_masks,\n    }\n\n    return batch\n\n  def triple_to_string(self, x):\n    return \" </s> \".join([item.strip() for item in x])\n  \n  def load_tsv_files(self, filepaths):\n    srcs = []\n\n    for filepath in filepaths:\n        with open(filepath) as tsv:\n            for line in tsv:\n                parts = line.strip().split(\"\\t\")\n\n                if len(parts) == 2:\n                    claim = parts[0].split(\"||\")[0].replace(\"[CLAIM]\", \"\").strip()\n                    question = parts[0].split(\"||\")[1].replace(\"[QUESTION]\", \"\").strip()\n                    answer = parts[1].split(\"||\")[1]\n\n                    srcs.append([claim, question, answer])\n   \n    return {\"positives\": srcs}\n\n  def add_negatives(self, dataset, neg_count=3):\n    srcs = dataset[\"positives\"]\n    negatives = []\n\n    for i,src in enumerate(srcs):\n        negatives.append([])\n        for _ in range(neg_count):\n            neg = random.randint(0, len(srcs)-1)\n            while srcs[neg][0] == srcs[i][0] and srcs[neg][1] == srcs[neg][1]: # Reject if we happen to sample the same claim/question pair\n                neg = random.randint(0, len(srcs)-1)\n\n            claim_neg = (srcs[neg][0], src[1], src[2])\n            question_neg = (src[0], srcs[neg][1], src[2])\n            answer_neg = (src[0], src[1], srcs[neg][2])\n\n            negatives[-1].append(self.triple_to_string(claim_neg))\n            negatives[-1].append(self.triple_to_string(question_neg))\n            negatives[-1].append(self.triple_to_string(answer_neg))\n\n    return {\"positives\": [self.triple_to_string(s) for s in srcs], \"negatives\": negatives}\n\n\n  def load_averitec_file(self, filepath):\n    with open(filepath) as f:\n      j = json.load(f)\n      examples = j\n\n    srcs = []\n    for example in examples:\n      for question in example[\"questions\"]:\n        for answer in question[\"answers\"]:\n          if \"answer\" not in answer:\n            continue\n          a = answer[\"answer\"]\n          if answer[\"answer_type\"] == \"Boolean\":\n            a += \". \" + answer[\"boolean_explanation\"]\n          srcs.append([example[\"claim\"], question[\"question\"], a])\n\n    return {\"positives\": srcs}\n\n  def load_tsv_file(self, filepath):\n    return self.load_tsv_files([filepath])\n\n  def load_dataset_files(self, averitec_filepaths, tsv_filepaths):\n    averitec_data = [self.load_averitec_file(f) for f in averitec_filepaths]\n    tsv_data = self.load_tsv_files(tsv_filepaths)\n\n    all_data = {\"positives\": []}\n\n    for data in averitec_data:\n      all_data[\"positives\"].extend(data[\"positives\"])\n\n    all_data[\"positives\"].extend(tsv_data[\"positives\"])\n\n    return all_data\n \n  # Loads and splits the data into training, validation and test sets with a 60/20/20 split\n  def prepare_data(self):\n    #self.train = self.load_averitec_file(\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date-cleaned.train.json\")\n    self.train = self.load_dataset_files(\n      [\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date-cleaned.train.json\"],\n      [\"/rds/user/mss84/hpc-work/datasets/averitec/initial_test_dataset/fcb.train_retriever.tsv\"]\n    )\n    self.train = self.add_negatives(self.train)\n\n    self.validate = self.load_averitec_file(\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date-cleaned.dev.json\")\n    self.validate = self.add_negatives(self.validate)\n\n    self.test = self.load_averitec_file(\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date-cleaned.test.json\")\n    self.test = self.add_negatives(self.test)\n\n  # encode the sentences using the tokenizer  \n  def setup(self, stage):\n    self.train = self.encode_sentences(self.train['positives'], self.train['negatives'])\n    \n    self.validate = self.encode_sentences(self.validate['positives'], self.validate['negatives'])\n    \n    self.test = self.encode_sentences(self.test['positives'], self.test['negatives'])\n\n  # Load the training, validation and test sets in Pytorch Dataset objects\n  def train_dataloader(self):\n    dataset = TensorDataset(self.train['pos_input_ids'], self.train['pos_attention_masks'], self.train['neg_input_ids'], self.train['neg_attention_masks'])                          \n    train_data = DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = self.batch_size)\n    return train_data\n\n  def val_dataloader(self):\n    dataset = TensorDataset(self.validate['pos_input_ids'], self.validate['pos_attention_masks'], self.validate['neg_input_ids'], self.validate['neg_attention_masks']) \n    val_data = DataLoader(dataset, batch_size = self.batch_size)                       \n    return val_data\n\n  def test_dataloader(self):\n    dataset = TensorDataset(self.test['pos_input_ids'], self.test['pos_attention_masks'], self.test['neg_input_ids'], self.test['neg_attention_masks']) \n    test_data = DataLoader(dataset, batch_size = self.batch_size)                   \n    return test_data\n\n\n"}
{"type": "source_file", "path": "retrieval_coarse/prompt_question_generation.py", "content": "import numpy as np\nimport argparse\nimport json\nimport nltk\nfrom rank_bm25 import BM25Okapi\nimport tqdm\nimport torch\nfrom transformers import BloomTokenizerFast, BloomModel, BloomForCausalLM\nfrom accelerate import Accelerator\n\nparser = argparse.ArgumentParser(description='Generate extra questions based on claims with a prompt. Useful for searching.')\nparser.add_argument('--reference_corpus', default=\"data/train.json\", help='')\nparser.add_argument('--target_file', default=\"data/dev.json\", help='')\nparser.add_argument('--n', default=10, help='')\nargs = parser.parse_args()\n\nwith open(args.target_file) as f:\n    j = json.load(f)\n    examples = j #[\"examples\"]\n\nwith open(args.reference_corpus) as f:\n    j = json.load(f)\n    train_examples = j\n\nall_data_corpus = []\ntokenized_corpus = []\n\nfor train_example in train_examples:\n    train_claim = train_example[\"claim\"]\n\n    speaker = train_example[\"speaker\"].strip() if train_example[\"speaker\"] is not None and len(train_example[\"speaker\"]) > 1 else \"they\"\n\n    questions = [q[\"question\"] for q in train_example[\"questions\"]]\n\n    claim_dict_builder = {}\n    claim_dict_builder[\"claim\"] = train_claim\n    claim_dict_builder[\"speaker\"] = speaker\n    claim_dict_builder[\"questions\"] = questions\n\n    tokenized_corpus.append(nltk.word_tokenize(claim_dict_builder[\"claim\"]))\n    all_data_corpus.append(claim_dict_builder)\n\nbm25 = BM25Okapi(tokenized_corpus)\n\n# Define methods to transform retrieved docs into a prompt:\ndef doc2prompt(doc):\n    prompt_parts = \"Outrageously, \" + doc[\"speaker\"] + \" claimed that \\\"\" + doc[\"claim\"].strip() + \"\\\". Criticism includes questions like: \"\n    questions = [q.strip() for q in doc[\"questions\"]]\n    return prompt_parts + \" \".join(questions)\n\ndef docs2prompt(top_docs):\n    return \"\\n\\n\".join([doc2prompt(d) for d in top_docs])\n\n# Define the bloom model:\naccelerator = Accelerator()\naccel_device = accelerator.device\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-7b1\")\nmodel = BloomForCausalLM.from_pretrained(\n    \"bigscience/bloom-7b1\",\n    torch_dtype=torch.bfloat16,\n).to(device)\n\nfor example in tqdm.tqdm(examples):\n    test_claim = example[\"claim\"]\n    speaker = example[\"speaker\"].strip() if example[\"speaker\"] is not None and len(example[\"speaker\"]) > 1 else \"they\"\n\n    s = bm25.get_scores(nltk.word_tokenize(test_claim))\n    top_n = np.argsort(s)[::-1][:args.n]\n    docs = [all_data_corpus[i] for i in top_n]\n\n    prompt = docs2prompt(docs) + \"\\n\\n\" + \"Outrageously, \" + speaker + \" claimed that \\\"\"+ test_claim.strip() + \"\\\". Criticism includes questions like: \"\n    sentences = [prompt]\n\n    inputs = tokenizer(\n    sentences, \n    padding=True,\n    return_tensors=\"pt\").to(device)\n\n    outputs = model.generate(inputs[\"input_ids\"],\n        max_length=2000, \n        num_beams=2, \n        no_repeat_ngram_size=2,\n        early_stopping=True\n    )\n\n    tgt_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n    in_len = len(sentences[0])\n    questions_str = tgt_text[in_len:].split(\"\\n\")[0]\n\n    qs = questions_str.split(\"?\")\n    qs = [q.strip() + \"?\" for q in qs if q.strip() and len(q.strip()) < 300]\n\n    example[\"questions\"] = [{\"question\": q, \"answers\": []} for q in qs]\n\nprint(json.dumps(examples, indent=4))"}
{"type": "source_file", "path": "models/SequenceClassificationModule.py", "content": "import pytorch_lightning as pl\nimport torch\nimport numpy as np\nimport datasets\nfrom transformers import MaxLengthCriteria, StoppingCriteriaList\nfrom transformers.optimization import AdamW\nimport itertools\nfrom utils import count_stats, f1_metric, pairwise_meteor\nfrom torchmetrics.text.rouge import ROUGEScore\nimport torch.nn.functional as F\nimport torchmetrics\nfrom torchmetrics.classification import F1Score\n\nclass SequenceClassificationModule(pl.LightningModule):\n  # Instantiate the model\n  def __init__(self, tokenizer, model, use_question_stance_approach=True, learning_rate=1e-3):\n    super().__init__()\n    self.tokenizer = tokenizer\n    self.model = model\n    self.learning_rate = learning_rate\n\n    self.train_acc = torchmetrics.Accuracy()\n    self.val_acc = torchmetrics.Accuracy()\n    self.test_acc = torchmetrics.Accuracy()\n\n    self.train_f1 = F1Score(num_classes=4, average=\"macro\")\n    self.val_f1 = F1Score(num_classes=4, average=None)\n    self.test_f1 = F1Score(num_classes=4, average=None)\n\n    self.use_question_stance_approach = use_question_stance_approach\n\n  \n  # Do a forward pass through the model\n  def forward(self, input_ids, **kwargs):\n    return self.model(input_ids, **kwargs)\n  \n  def configure_optimizers(self):\n    optimizer = AdamW(self.parameters(), lr = self.learning_rate)\n    return optimizer\n\n  def training_step(self, batch, batch_idx):\n    x, x_mask, y = batch\n\n    outputs = self(x, attention_mask=x_mask, labels=y)\n    logits = outputs.logits\n    loss = outputs.loss\n\n    #cross_entropy = torch.nn.CrossEntropyLoss()\n    #loss = cross_entropy(logits, y)\n    \n    preds = torch.argmax(logits, axis=1)\n\n    self.log(\"train_loss\", loss)\n\n    return {'loss': loss}\n\n  def validation_step(self, batch, batch_idx):\n    x, x_mask, y = batch\n\n    outputs = self(x, attention_mask=x_mask, labels=y)\n    logits = outputs.logits\n    loss = outputs.loss\n\n    preds = torch.argmax(logits, axis=1)\n\n    if not self.use_question_stance_approach:\n      self.val_acc(preds, y)\n      self.log('val_acc_step', self.val_acc)\n\n      self.val_f1(preds, y)      \n      self.log(\"val_loss\", loss)\n\n    return {'val_loss':loss, \"src\": x, \"pred\": preds, \"target\": y}\n\n  def validation_epoch_end(self, outs):\n    if self.use_question_stance_approach:\n      self.handle_end_of_epoch_scoring(outs, self.val_acc, self.val_f1)\n    \n    self.log('val_acc_epoch', self.val_acc)\n\n    f1 = self.val_f1.compute()\n    self.val_f1.reset()\n\n    self.log('val_f1_epoch', torch.mean(f1))\n\n    class_names = [\"supported\", \"refuted\", \"nei\", \"conflicting\"]\n    for i, c_name in enumerate(class_names):\n      self.log(\"val_f1_\" + c_name, f1[i])\n\n\n  def test_step(self, batch, batch_idx):\n    x, x_mask, y = batch\n    \n    outputs = self(x, attention_mask=x_mask)\n    logits = outputs.logits\n\n    preds = torch.argmax(logits, axis=1)\n    \n    if not self.use_question_stance_approach:\n      self.test_acc(preds, y)      \n      self.log('test_acc_step', self.test_acc)\n      self.test_f1(preds, y)      \n\n    return {\"src\": x, \"pred\": preds, \"target\": y}\n\n  def test_epoch_end(self, outs):\n    if self.use_question_stance_approach:\n      self.handle_end_of_epoch_scoring(outs, self.test_acc, self.test_f1)\n    \n    self.log('test_acc_epoch', self.test_acc)\n\n    f1 = self.test_f1.compute()\n    self.test_f1.reset()\n    self.log('test_f1_epoch', torch.mean(f1))\n\n    class_names = [\"supported\", \"refuted\", \"nei\", \"conflicting\"]\n    for i, c_name in enumerate(class_names):\n      self.log(\"test_f1_\" + c_name, f1[i])\n\n  def handle_end_of_epoch_scoring(self, outputs, acc_scorer, f1_scorer):\n      gold_labels = {}\n      question_support = {}\n      for out in outputs:\n        srcs = out['src']\n        preds = out['pred']\n        tgts = out['target']\n\n        tokens = self.tokenizer.batch_decode(\n          srcs, \n          skip_special_tokens=True, \n          clean_up_tokenization_spaces=True\n        )\n\n        for src, pred, tgt in zip(tokens, preds, tgts):\n          claim_id = src.split(\"[ question ]\")[0]\n\n          if claim_id not in gold_labels:\n            gold_labels[claim_id] = tgt\n            question_support[claim_id] = []\n\n          question_support[claim_id].append(pred)\n\n      for k,gold_label in gold_labels.items():\n        support = question_support[k]\n\n        has_unansw = False\n        has_true = False\n        has_false = False\n\n        for v in support:\n          if v == 0:\n            has_true = True\n          if v == 1:\n            has_false = True\n          if v == 2 or v == 3: # TODO very ugly hack -- we cant have different numbers of labels for train and test so we do this\n            has_unansw = True\n\n        if has_unansw:\n          answer = 2\n        elif has_true and not has_false:\n          answer = 0\n        elif has_false and not has_true:\n          answer = 1\n        elif has_true and has_false:\n          answer = 3\n\n\n        # TODO this is very hacky and wont work if the device is literally anything other than cuda:0\n        acc_scorer(torch.as_tensor([answer]).to(\"cuda:0\"), torch.as_tensor([gold_label]).to(\"cuda:0\"))    \n        f1_scorer(torch.as_tensor([answer]).to(\"cuda:0\"), torch.as_tensor([gold_label]).to(\"cuda:0\"))\n\n        "}
{"type": "source_file", "path": "models/NaiveSeqClassModule.py", "content": "import pytorch_lightning as pl\nimport torch\nimport numpy as np\nimport datasets\nfrom transformers import MaxLengthCriteria, StoppingCriteriaList\nfrom transformers.optimization import AdamW\nimport itertools\nfrom utils import count_stats, f1_metric, pairwise_meteor\nfrom torchmetrics.text.rouge import ROUGEScore\nimport torch.nn.functional as F\nimport torchmetrics\nfrom torchmetrics.classification import F1Score\n\nclass NaiveSeqClassModule(pl.LightningModule):\n  # Instantiate the model\n  def __init__(self, tokenizer, model, use_question_stance_approach=True, learning_rate=1e-3):\n    super().__init__()\n    self.tokenizer = tokenizer\n    self.model = model\n    self.learning_rate = learning_rate\n\n    self.train_acc = torchmetrics.Accuracy()\n    self.val_acc = torchmetrics.Accuracy()\n    self.test_acc = torchmetrics.Accuracy()\n\n    self.train_f1 = F1Score(num_classes=4, average=\"macro\")\n    self.val_f1 = F1Score(num_classes=4, average=None)\n    self.test_f1 = F1Score(num_classes=4, average=None)\n\n    self.use_question_stance_approach = use_question_stance_approach\n\n  \n  # Do a forward pass through the model\n  def forward(self, input_ids, **kwargs):\n    return self.model(input_ids, **kwargs)\n  \n  def configure_optimizers(self):\n    optimizer = AdamW(self.parameters(), lr = self.learning_rate)\n    return optimizer\n\n  def training_step(self, batch, batch_idx):\n    x, x_mask, y = batch\n\n    outputs = self(x, attention_mask=x_mask, labels=y)\n    logits = outputs.logits\n    loss = outputs.loss\n\n    #cross_entropy = torch.nn.CrossEntropyLoss()\n    #loss = cross_entropy(logits, y)\n    \n    preds = torch.argmax(logits, axis=1)\n\n    self.train_acc(preds.cpu(), y.cpu())      \n    self.train_f1(preds.cpu(), y.cpu())      \n    \n    self.log(\"train_loss\", loss)\n\n    return {'loss': loss}\n\n  def training_epoch_end(self, outs):\n    self.log('train_acc_epoch', self.train_acc)\n    self.log('train_f1_epoch', self.train_f1)\n\n  def validation_step(self, batch, batch_idx):\n    x, x_mask, y = batch\n\n    outputs = self(x, attention_mask=x_mask, labels=y)\n    logits = outputs.logits\n    loss = outputs.loss\n\n    preds = torch.argmax(logits, axis=1)\n\n    if not self.use_question_stance_approach:\n      self.val_acc(preds, y)\n      self.log('val_acc_step', self.val_acc)\n\n      self.val_f1(preds, y)      \n      self.log(\"val_loss\", loss)\n\n    return {'val_loss':loss, \"src\": x, \"pred\": preds, \"target\": y}\n\n  def validation_epoch_end(self, outs):\n    if self.use_question_stance_approach:\n      self.handle_end_of_epoch_scoring(outs, self.val_acc, self.val_f1)\n    \n    self.log('val_acc_epoch', self.val_acc)\n\n    f1 = self.val_f1.compute()\n    self.val_f1.reset()\n\n    self.log('val_f1_epoch', torch.mean(f1))\n\n    class_names = [\"supported\", \"refuted\", \"nei\", \"conflicting\"]\n    for i, c_name in enumerate(class_names):\n      self.log(\"val_f1_\" + c_name, f1[i])\n\n\n  def test_step(self, batch, batch_idx):\n    x, x_mask, y = batch\n    \n    outputs = self(x, attention_mask=x_mask)\n    logits = outputs.logits\n\n    preds = torch.argmax(logits, axis=1)\n    \n    if not self.use_question_stance_approach:\n      self.test_acc(preds, y)      \n      self.log('test_acc_step', self.test_acc)\n      self.test_f1(preds, y)      \n\n    return {\"src\": x, \"pred\": preds, \"target\": y}\n\n  def test_epoch_end(self, outs):\n    if self.use_question_stance_approach:\n      self.handle_end_of_epoch_scoring(outs, self.test_acc, self.test_f1)\n    \n    self.log('test_acc_epoch', self.test_acc)\n\n    f1 = self.test_f1.compute()\n    self.test_f1.reset()\n    self.log('test_f1_epoch', torch.mean(f1))\n\n    class_names = [\"supported\", \"refuted\", \"nei\", \"conflicting\"]\n    for i, c_name in enumerate(class_names):\n      self.log(\"test_f1_\" + c_name, f1[i])\n\n  def handle_end_of_epoch_scoring(self, outputs, acc_scorer, f1_scorer):\n      gold_labels = {}\n      question_support = {}\n      for out in outputs:\n        srcs = out['src']\n        preds = out['pred']\n        tgts = out['target']\n\n        tokens = self.tokenizer.batch_decode(\n          srcs, \n          skip_special_tokens=True, \n          clean_up_tokenization_spaces=True\n        )\n\n        for src, pred, tgt in zip(tokens, preds, tgts):\n          acc_scorer(torch.as_tensor([pred]).to(\"cuda:0\"), torch.as_tensor([tgt]).to(\"cuda:0\"))    \n          f1_scorer(torch.as_tensor([pred]).to(\"cuda:0\"), torch.as_tensor([tgt]).to(\"cuda:0\"))\n\n        "}
{"type": "source_file", "path": "eval.py", "content": "\nfrom utils import pairwise_meteor, compute_all_pairwise_scores\nimport numpy as np\nimport argparse\nimport json\nimport scipy\nimport sklearn\n\nparser = argparse.ArgumentParser(description='Evaluation script for averitec.')\nparser.add_argument('--predictions', default=\"your_prediction_here.json\", help='')\nparser.add_argument('--references', default=\"data/dev.json\", help='')\nargs = parser.parse_args()\n\nclass AveritecEvaluator:\n\n    verdicts = [\n        \"Supported\", \n        \"Refuted\", \n        \"Not Enough Evidence\",\n        \"Conflicting Evidence/Cherrypicking\"\n    ]\n    pairwise_metric = None\n    max_questions = 10\n    metric = None\n    averitec_reporting_levels = [0.1, 0.2, 0.25, 0.3, 0.4, 0.5]\n\n    def __init__(self, metric=\"meteor\"):\n        self.metric = metric\n        if metric == \"meteor\":\n            self.pairwise_metric = pairwise_meteor\n\n    def evaluate_justifications(self, srcs, tgts):\n        scores = []\n        for src, tgt in zip(srcs, tgts):\n            # If there is no justification, fallback to qa-pairs (or string evidence)\n            if \"justification\" not in src:\n                src_strings = self.extract_full_comparison_strings(src)[:self.max_questions]\n                pred_justifications = \" \".join(src_strings)\n            else:\n                pred_justifications = src[\"justification\"]\n            score = self.pairwise_metric(pred_justifications, tgt[\"justification\"])\n            scores.append(score)\n        return np.mean(scores)\n\n    def evaluate_averitec_veracity_by_type(self, srcs, tgts, threshold=0.3):\n        types = {}\n        for src, tgt in zip(srcs, tgts):\n            score = self.compute_pairwise_evidence_score(src, tgt)\n\n            if score <= threshold:\n                score = 0\n\n            for t in tgt[\"claim_types\"]:\n                if t not in types:\n                    types[t] = []\n\n                types[t].append(score)\n\n        return {t:np.mean(v) for t,v in types.items()}\n\n    def evaluate_averitec_score(self, srcs, tgts):\n        scores = []\n        justification_scores = []\n        for src, tgt in zip(srcs, tgts):\n            score = self.compute_pairwise_evidence_score(src, tgt)\n\n            if \"justification\" not in src:\n                src_strings = self.extract_full_comparison_strings(src)[:self.max_questions]\n                pred_justifications = \" \".join(src_strings)\n            else:\n                pred_justifications = src[\"justification\"]\n\n            justification_score = self.pairwise_metric(pred_justifications, tgt[\"justification\"])\n\n            this_example_scores = [0.0 for _ in self.averitec_reporting_levels]\n            this_example_j_scores = [0.0 for _ in self.averitec_reporting_levels]\n            for i, level in enumerate(self.averitec_reporting_levels):\n                if score > level:\n                    this_example_scores[i] = src[\"label\"] == tgt[\"label\"]\n                    this_example_j_scores[i] = justification_score\n\n            scores.append(this_example_scores)\n            justification_scores.append(this_example_j_scores)\n\n        return np.mean(np.array(scores), axis=0), np.mean(np.array(justification_scores), axis=0)\n\n    def evaluate_veracity(self, src, tgt):\n        src_labels = [x[\"label\"] for x in src]\n        tgt_labels = [x[\"label\"] for x in tgt]\n\n        acc = np.mean([s == t for s,t in zip(src_labels, tgt_labels)])\n\n        f1 = {self.verdicts[i]: x for i,x in enumerate(sklearn.metrics.f1_score(tgt_labels, src_labels, labels=self.verdicts, average=None))}\n        f1[\"macro\"] = sklearn.metrics.f1_score(tgt_labels, src_labels, labels=self.verdicts, average='macro')\n        f1[\"acc\"] = acc\n        return f1\n\n    def evaluate_questions_only(self, srcs, tgts):\n        all_utils = []\n        for src, tgt in zip(srcs, tgts):\n            if \"questions\" not in src:\n                # If there was no question, use the string evidence\n                src_questions = self.extract_full_comparison_strings(src)[:self.max_questions]\n            else:\n                src_questions = [qa[\"question\"] for qa in src[\"questions\"][:self.max_questions]]\n            tgt_questions = [qa[\"question\"] for qa in tgt[\"questions\"]]\n\n            pairwise_scores = compute_all_pairwise_scores(src_questions, tgt_questions, self.pairwise_metric)\n\n            assignment = scipy.optimize.linear_sum_assignment(pairwise_scores, maximize=True)\n\n            assignment_utility = pairwise_scores[assignment[0], assignment[1]].sum()\n\n            # Reweight to account for unmatched target questions\n            reweight_term = 1 / float(len(tgt_questions))\n            assignment_utility *= reweight_term\n\n            all_utils.append(assignment_utility)\n\n        return np.mean(all_utils)\n\n    def get_n_best_qas(self, srcs, tgts, n=3):\n        all_utils = []\n        for src, tgt in zip(srcs, tgts):\n            assignment_utility = self.compute_pairwise_evidence_score(src, tgt)\n\n            all_utils.append(assignment_utility)\n\n        idxs = np.argsort(all_utils)[::-1][:n]\n\n        examples = [(srcs[i][\"questions\"] if \"questions\" in srcs[i] else srcs[i][\"string_evidence\"], tgts[i][\"questions\"], all_utils[i]) for i in idxs]\n\n        return examples\n\n    def compute_pairwise_evidence_score(self, src, tgt):\n        src_strings = self.extract_full_comparison_strings(src)[:self.max_questions]\n        tgt_strings = self.extract_full_comparison_strings(tgt)\n        pairwise_scores = compute_all_pairwise_scores(src_strings, tgt_strings, self.pairwise_metric)\n        assignment = scipy.optimize.linear_sum_assignment(pairwise_scores, maximize=True)\n        assignment_utility = pairwise_scores[assignment[0], assignment[1]].sum()\n\n            # Reweight to account for unmatched target questions\n        reweight_term = 1 / float(len(tgt_strings))\n        assignment_utility *= reweight_term\n        return assignment_utility\n\n    def evaluate_questions_and_answers(self, srcs, tgts):\n        all_utils = []\n        for src, tgt in zip(srcs, tgts):\n            src_strings = self.extract_full_comparison_strings(src)[:self.max_questions]\n            tgt_strings = self.extract_full_comparison_strings(tgt)\n\n            pairwise_scores = compute_all_pairwise_scores(src_strings, tgt_strings, self.pairwise_metric)\n\n            assignment = scipy.optimize.linear_sum_assignment(pairwise_scores, maximize=True)\n\n            assignment_utility = pairwise_scores[assignment[0], assignment[1]].sum()\n\n            # Reweight to account for unmatched target questions\n            reweight_term = 1 / float(len(tgt_strings))\n            assignment_utility *= reweight_term\n\n            all_utils.append(assignment_utility)\n\n        return np.mean(all_utils)\n\n    def extract_full_comparison_strings(self, example):\n        example_strings = []\n        if \"questions\" in example:\n            for question in example[\"questions\"]:\n                # If the answers is not a list, make them a list:\n                if not isinstance(question[\"answers\"], list):\n                    question[\"answers\"] = [question[\"answers\"]]\n                    \n                for answer in question[\"answers\"]:\n                    example_strings.append(question[\"question\"] + \" \" + answer[\"answer\"])\n                    if \"answer_type\" in answer and answer[\"answer_type\"] == \"Boolean\":\n                        example_strings[-1] += \". \" + answer[\"boolean_explanation\"]\n\n                if len(question[\"answers\"]) == 0:\n                    example_strings.append(question[\"question\"] + \" No answer could be found.\")\n        \n        if \"string_evidence\" in example:\n            for full_string_evidence in example[\"string_evidence\"]:\n                example_strings.append(full_string_evidence)\n\n        return example_strings\n\nwith open(args.predictions) as f:\n    j = json.load(f)\n    predictions = j\n\nwith open(args.references) as f:\n    j = json.load(f)\n    references = j\n\ndef print_with_space(left, right, left_space = 50):\n    print_spaces = \" \" * (40 - len(left))\n    print(left + print_spaces + right)\n\nprint(\"AVeriTeC evaluation:\")\nprint(\"====================\")\n\nscorer = AveritecEvaluator()\nq_score = scorer.evaluate_questions_only(predictions, references)\nprint_with_space(\"Question-only score (HU-\" +scorer.metric + \"):\", str(q_score))\np_score = scorer.evaluate_questions_and_answers(predictions, references)\nprint_with_space(\"Question-answer score (HU-\" +scorer.metric + \"):\", str(p_score))\nprint(\"====================\")\nv_score = scorer.evaluate_veracity(predictions, references)\nprint(\"Veracity F1 scores:\")\nfor k,v in v_score.items():\n    print_with_space(\" * \"+k+\":\", str(v))\nj_score = scorer.evaluate_justifications(predictions, references)\nprint_with_space(\"Justification score (\" +scorer.metric + \"):\", str(j_score))\nprint(\"--------------------\")\nprint(\"Averitec scores:\")\nv_score, j_score = scorer.evaluate_averitec_score(predictions, references)\nfor i, level in enumerate(scorer.averitec_reporting_levels):\n    print_with_space(\" * Veracity scores (\" +scorer.metric + \" @ \" + str(level) + \"):\", str(v_score[i]))\n    print_with_space(\" * Justification scores (\" +scorer.metric  + \" @ \" + str(level) + \"):\", str(j_score[i]))\nprint(\"--------------------\")\ntype_scores = scorer.evaluate_averitec_veracity_by_type(predictions, references, threshold=0.2)\nfor t,v in type_scores.items():\n    print_with_space(\" * Veracity scores (\" +t + \"):\", str(v))\nprint(\"--------------------\")\ntype_scores = scorer.evaluate_averitec_veracity_by_type(predictions, references,  threshold=0.3)\nfor t,v in type_scores.items():\n    print_with_space(\" * Veracity scores (\" +t + \"):\", str(v))\nprint(\"====================\")\nprint(\"Printing 5 best examples in terms of QA pairs:\")\nbest_examples = scorer.get_n_best_qas(predictions, references, n=5)\nprint(json.dumps(best_examples, indent=4))"}
{"type": "source_file", "path": "retrieval_reranking/decorate_with_questions.py", "content": "import argparse\nimport json\nimport pandas as pd\nimport nltk\nfrom rank_bm25 import BM25Okapi\nimport numpy as np\nimport os\nimport sys\nimport torch\nimport tqdm\nfrom transformers import BloomTokenizerFast, BloomModel, BloomForCausalLM\nfrom accelerate import Accelerator\n\n\nparser = argparse.ArgumentParser(description='Use a prompt to generate questions that could be answered by top-k retrieved evidence. Output generated questions.')\nparser.add_argument('--reference_corpus', default=\"data/train.json\", help='')\nparser.add_argument('--target_file', default=\"data/dev.json\", help='')\nparser.add_argument('--url_file', default=\"search_results.tsv\", help='')\nparser.add_argument('--store_folder', default=\"store/retrieved_docs\", help='')\nparser.add_argument('--top_k', default=100, type=int, help='How many documents should we pick out with BM25')\nparser.add_argument('--start_idx', default=0, type=int, help='Which claim to start at. Useful for larger corpus.')\nparser.add_argument('--n_to_compute', default=-1, type=int, help='How many claims to work through. Useful for larger datasets.')\nargs = parser.parse_args()\n\n# Construct the prompts to retrieve and set up BM25 to find similar evidence:\n\nwith open(args.reference_corpus) as f:\n    train_examples = json.load(f)\n\ndef claim2prompts(example):\n    claim = example[\"claim\"]\n\n    #claim_str = \"Claim: \" + claim + \"||Evidence: \"\n    claim_str = \"Evidence: \"\n\n    for question in example[\"questions\"]:\n        q_text = question[\"question\"].strip()\n        if len(q_text) == 0:\n            continue\n\n        if not q_text[-1] == \"?\":\n            q_text += \"?\"\n\n        answer_strings = []\n\n        for a in question[\"answers\"]:\n            if a[\"answer_type\"] in [\"Extractive\", \"Abstractive\"]:\n                answer_strings.append(a[\"answer\"])\n            if a[\"answer_type\"] == \"Boolean\":\n                answer_strings.append(a[\"answer\"]  + \", because \" + a[\"boolean_explanation\"].lower().strip())\n\n        for a_text in answer_strings:\n            if not a_text[-1] in [\".\", \"!\", \":\", \"?\"]:\n                a_text += \".\"\n\n            #prompt_lookup_str = claim + \" \" + a_text\n            prompt_lookup_str = a_text\n            this_q_claim_str = claim_str + \" \" + a_text.strip() + \"||Question answered: \" + q_text\n            yield (prompt_lookup_str, this_q_claim_str.replace(\"\\n\", \" \").replace(\"||\", \"\\n\"))\n\nprompt_corpus = []\ntokenized_corpus = []\n\nfor example in tqdm.tqdm(train_examples):\n    for lookup_str, prompt in  claim2prompts(example):\n        entry = nltk.word_tokenize(lookup_str)\n        tokenized_corpus.append(entry)\n        prompt_corpus.append(prompt)\n    \nprompt_bm25 = BM25Okapi(tokenized_corpus)\n\n\n# Attach evidence to examples:\n\nwith open(args.target_file) as f:\n    examples = json.load(f)\n\nwith open(args.url_file) as url_file:\n    first = True\n    for line in url_file:\n        if first:\n            first = False\n        else:\n            line_parts = line.strip().split(\"\\t\")\n\n            if len(line_parts) < 7:\n                continue\n            idx = int(line_parts[0])\n\n            #claim = line_parts[1]\n            url = line_parts[2]\n            store_file = args.store_folder + \"/\" + line_parts[6].split(\"/\")[-1]\n\n            if \"retrieved_store_files\" not in examples[idx]:\n                examples[idx][\"retrieved_store_files\"] = []\n            examples[idx][\"retrieved_store_files\"].append(store_file)\n\n# Define the bloom model:\naccelerator = Accelerator()\naccel_device = accelerator.device\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-7b1\")\nmodel = BloomForCausalLM.from_pretrained(\n    \"bigscience/bloom-7b1\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    offload_folder=\"./offload\"\n)\n\ndef shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n    \"\"\"\n    Shift input ids one token to the right.\n    https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/modeling_bart.py.\n    \"\"\"\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n    shifted_input_ids[:, 0] = decoder_start_token_id\n\n    if pad_token_id is None:\n        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n    # replace possible -100 values in labels by `pad_token_id`\n    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n    return shifted_input_ids\n\n\n# Go through the dataset, generating questions for the evidence: \nend_idx = -1\nif args.n_to_compute != -1:\n    end_idx = args.start_idx+args.n_to_compute\n    \nfor idx,example in enumerate(tqdm.tqdm(examples[args.start_idx:end_idx])):\n    # First, retrieve top 50 documents with bm25:\n    tokenized_corpus = []\n    all_data_corpus = []\n\n    this_example_store_files = [] if \"retrieved_store_files\" not in example else example[\"retrieved_store_files\"] \n    for store_file in this_example_store_files:\n        with open(store_file, 'r') as f:\n            first = True\n            for line in f:\n                line = line.strip()\n\n                if first:\n                    first = False\n                    location_url = line\n                    continue\n                \n                if len(line) > 3:\n                    entry = nltk.word_tokenize(line)\n                    if (location_url, line) not in all_data_corpus:\n                        tokenized_corpus.append(entry)\n                        all_data_corpus.append((location_url, line))\n\n    if len(tokenized_corpus) == 0:\n        print(\"\")\n        continue\n    \n    bm25 = BM25Okapi(tokenized_corpus)\n    s = bm25.get_scores(nltk.word_tokenize(example[\"claim\"]))\n    n_coarse = args.top_k\n    top_n = np.argsort(s)[::-1][:n_coarse]\n    docs = [all_data_corpus[i] for i in top_n]\n\n    tracker = []\n    docs_with_qs = []\n\n    # Then, generate questions for those top 50:\n    for doc in docs:\n        #prompt_lookup_str = example[\"claim\"] + \" \" + doc[1]\n        prompt_lookup_str = doc[1]\n\n        prompt_s = prompt_bm25.get_scores(nltk.word_tokenize(prompt_lookup_str))\n        prompt_n = 10\n        prompt_top_n = np.argsort(prompt_s)[::-1][:prompt_n]\n        prompt_docs = [prompt_corpus[i] for i in prompt_top_n]\n\n        claim_prompt = \"Evidence: \" + doc[1].replace(\"\\n\", \" \") + \"\\nQuestion answered: \"\n\n        prompt = \"\\n\\n\".join(prompt_docs + [claim_prompt])\n\n        sentences = [prompt]\n\n        inputs = tokenizer(\n        sentences, \n        padding=True,\n        return_tensors=\"pt\").to(device)\n\n        outputs = model.generate(inputs[\"input_ids\"],\n            max_length=5000, \n            num_beams=2, \n            no_repeat_ngram_size=2,\n            early_stopping=True\n        )\n\n        tgt_text = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)[0]\n\n        # We are not allowed to generate more than 250 characters:\n        tgt_text = tgt_text[:250]\n        \n        qa_pair = [tgt_text.strip().split(\"?\")[0].replace(\"\\n\", \" \") + \"?\",  doc[1].replace(\"\\n\", \" \"), doc[0]]\n\n        if not \"bm25_qas\" in example:\n            example[\"bm25_qas\"] = []\n\n        example[\"bm25_qas\"].append(qa_pair)\n\nprint(json.dumps(examples[args.start_idx:args.start_idx+args.n_to_compute], indent=4))"}
{"type": "source_file", "path": "retrieval_coarse/combine_search_results.py", "content": "import argparse\nimport pandas as pd\nimport os\nimport sys\n\nparser = argparse.ArgumentParser(description='Combine downloaded search result tsv files.')\nparser.add_argument('--pattern', default=\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/search_test_vicuna/search.\", help='')\nargs = parser.parse_args()\n\nfolder_name = os.path.dirname(args.pattern)\nfile_name = os.path.basename(args.pattern)\n\nline = [\"index\", \"claim\", \"link\", \"page\", \"search_string\", \"search_type\", \"store_file\"]\nline = \"\\t\".join(line)\nprint(line)\n\nfor f in sorted(os.listdir(folder_name)):\n    if f.startswith(file_name):\n        print(f, file=sys.stderr)\n        with open(folder_name + \"/\" + f) as url_file:\n            first = True\n            for line in url_file:\n                if first:\n                    first = False\n                else:\n                    if line.strip():\n                        print(line.strip())"}
{"type": "source_file", "path": "models/DualEncoderModule.py", "content": "import pytorch_lightning as pl\nimport torch\nimport numpy as np\nimport datasets\nfrom transformers import MaxLengthCriteria, StoppingCriteriaList\nfrom transformers.optimization import AdamW\nimport itertools\nfrom utils import count_stats, f1_metric, pairwise_meteor\nfrom torchmetrics.text.rouge import ROUGEScore\nimport torch.nn.functional as F\nimport torchmetrics\nfrom torchmetrics.classification import F1Score\n\nclass DualEncoderModule(pl.LightningModule):\n  # Instantiate the model\n  def __init__(self, tokenizer, model, learning_rate=1e-3):\n    super().__init__()\n    self.tokenizer = tokenizer\n    self.model = model\n    self.learning_rate = learning_rate\n\n    self.train_acc = torchmetrics.Accuracy()\n    self.val_acc = torchmetrics.Accuracy()\n    self.test_acc = torchmetrics.Accuracy()\n\n  # Do a forward pass through the model\n  def forward(self, input_ids, **kwargs):\n    return self.model(input_ids, **kwargs)\n  \n  def configure_optimizers(self):\n    optimizer = AdamW(self.parameters(), lr = self.learning_rate)\n    return optimizer\n\n  def training_step(self, batch, batch_idx):\n    pos_ids, pos_mask, neg_ids, neg_mask = batch\n\n    neg_ids = neg_ids.view(-1, neg_ids.shape[-1])\n    neg_mask = neg_mask.view(-1, neg_mask.shape[-1])\n\n    pos_outputs = self(pos_ids, attention_mask=pos_mask, labels=torch.ones(pos_ids.shape[0], dtype=torch.uint8).to(pos_ids.get_device()))\n    neg_outputs = self(neg_ids, attention_mask=neg_mask, labels=torch.zeros(neg_ids.shape[0], dtype=torch.uint8).to(neg_ids.get_device()))\n\n    loss_scale = 1.0\n    loss = pos_outputs.loss + loss_scale * neg_outputs.loss\n    \n    pos_logits = pos_outputs.logits\n    pos_preds = torch.argmax(pos_logits, axis=1)\n    self.train_acc(pos_preds.cpu(), torch.ones(pos_ids.shape[0], dtype=torch.uint8).cpu())      \n\n    neg_logits = neg_outputs.logits\n    neg_preds = torch.argmax(neg_logits, axis=1)\n    self.train_acc(neg_preds.cpu(), torch.zeros(neg_ids.shape[0], dtype=torch.uint8).cpu())  \n\n\n    return {'loss': loss}\n\n  def validation_step(self, batch, batch_idx):\n    pos_ids, pos_mask, neg_ids, neg_mask = batch\n\n    neg_ids = neg_ids.view(-1, neg_ids.shape[-1])\n    neg_mask = neg_mask.view(-1, neg_mask.shape[-1])\n\n    pos_outputs = self(pos_ids, attention_mask=pos_mask, labels=torch.ones(pos_ids.shape[0], dtype=torch.uint8).to(pos_ids.get_device()))\n    neg_outputs = self(neg_ids, attention_mask=neg_mask, labels=torch.zeros(neg_ids.shape[0], dtype=torch.uint8).to(neg_ids.get_device()))\n\n    loss_scale = 1.0\n    loss = pos_outputs.loss + loss_scale * neg_outputs.loss\n    \n    pos_logits = pos_outputs.logits\n    pos_preds = torch.argmax(pos_logits, axis=1)\n    self.val_acc(pos_preds.cpu(), torch.ones(pos_ids.shape[0], dtype=torch.uint8).cpu())      \n\n    neg_logits = neg_outputs.logits\n    neg_preds = torch.argmax(neg_logits, axis=1)\n    self.val_acc(neg_preds.cpu(), torch.zeros(neg_ids.shape[0], dtype=torch.uint8).cpu())   \n    \n    self.log('val_acc', self.val_acc)    \n    \n    return {'loss': loss}\n\n  def test_step(self, batch, batch_idx):\n    pos_ids, pos_mask, neg_ids, neg_mask = batch\n\n    neg_ids = neg_ids.view(-1, neg_ids.shape[-1])\n    neg_mask = neg_mask.view(-1, neg_mask.shape[-1])\n\n    pos_outputs = self(pos_ids, attention_mask=pos_mask, labels=torch.ones(pos_ids.shape[0], dtype=torch.uint8).to(pos_ids.get_device()))\n    neg_outputs = self(neg_ids, attention_mask=neg_mask, labels=torch.zeros(neg_ids.shape[0], dtype=torch.uint8).to(neg_ids.get_device()))\n\n    loss_scale = 1.0\n    loss = pos_outputs.loss + loss_scale * neg_outputs.loss\n    \n    pos_logits = pos_outputs.logits\n    pos_preds = torch.argmax(pos_logits, axis=1)\n    self.test_acc(pos_preds.cpu(), torch.ones(pos_ids.shape[0], dtype=torch.uint8).cpu())      \n\n    neg_logits = neg_outputs.logits\n    neg_preds = torch.argmax(neg_logits, axis=1)\n    self.test_acc(neg_preds.cpu(), torch.zeros(neg_ids.shape[0], dtype=torch.uint8).cpu())      \n\n\n    self.log('test_acc', self.test_acc)\n"}
{"type": "source_file", "path": "retrieval_coarse/split_sentences_for_downloaded_pages.py", "content": "import argparse\nimport os\nimport tqdm\n\nfrom html2lines import line_correction\n\nparser = argparse.ArgumentParser(description='Do proper sentence splitting for all downloaded pages.')\nparser.add_argument('--store_folder', default=\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/retrieved_docs\", help='')\nparser.add_argument('--start_idx', default=0, type=int, help='Which claim to start at. Useful for larger corpus.')\nparser.add_argument('--n_to_compute', default=50000, type=int, help='How many claims to work through. Useful for larger corpus.')\nargs = parser.parse_args()\n\nif not os.path.exists(args.store_folder + \".formatted\"):\n    os.makedirs(args.store_folder + \".formatted\")\n\nend_idx = -1\nif args.n_to_compute != -1:\n    end_idx = args.start_idx+args.n_to_compute\n\nfor file in tqdm.tqdm(sorted(list(os.listdir(args.store_folder)))[args.start_idx:end_idx]):\n    full_path = args.store_folder + \"/\" + file\n\n    lines = []\n    with open(full_path, \"r\") as f:\n        for l in f:\n            lines.append(l.strip())\n\n    fixed_lines = line_correction(lines)\n\n    formatted_path = args.store_folder + \".formatted\" + \"/\" + file\n\n    with open(formatted_path, \"w\") as out_f:\n        print(\"\\n\".join(fixed_lines), file=out_f)   "}
{"type": "source_file", "path": "retrieval_coarse/html2lines.py", "content": "from distutils.command.config import config\nimport requests\nfrom time import sleep\nimport trafilatura\nfrom trafilatura.meta import reset_caches\nfrom trafilatura.settings import DEFAULT_CONFIG\nimport spacy\nimport sys\nnlp = spacy.load('en_core_web_lg')\nimport sys\n\nDEFAULT_CONFIG.MAX_FILE_SIZE = 50000\n\ndef get_page(url):\n    page = None\n    for i in range(3):\n        try:\n            page = trafilatura.fetch_url(url, config=DEFAULT_CONFIG)\n            assert page is not None\n            print(\"Fetched \"+url, file=sys.stderr)\n            break\n        except:\n            sleep(3)\n    return page\n\ndef url2lines(url):\n    page = get_page(url)\n\n    if page is None:\n        return []\n    \n    lines = html2lines(page)\n    return lines\n\ndef line_correction(lines, max_size=100):\n    out_lines = []\n    for line in lines:\n        if len(line) < 4:\n            continue\n\n        if len(line) > max_size:\n            doc = nlp(line[:5000]) # We split lines into sentences, but for performance we take only the first 5k characters per line\n            stack = \"\"\n            for sent in doc.sents:\n                if len(stack) > 0:\n                    stack += \" \"\n                stack += str(sent).strip()\n                if len(stack) > max_size:\n                    out_lines.append(stack)\n                    stack = \"\"\n\n            if len(stack) > 0:\n                out_lines.append(stack)\n        else:\n            out_lines.append(line)\n    \n    return out_lines\n\ndef html2lines(page):\n    out_lines = []\n\n    if len(page.strip()) == 0 or page is None:\n        return out_lines\n\n    text = trafilatura.extract(page, config=DEFAULT_CONFIG)\n    reset_caches()\n\n    if text is None:\n        return out_lines\n\n    return text.split(\"\\n\") # We just spit out the entire page, so need to reformat later."}
{"type": "source_file", "path": "data_loaders/SequenceClassificationDataLoader.py", "content": "import random\nfrom torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\nimport pandas as pd\nimport numpy as np\nimport torch\nimport pytorch_lightning as pl\nimport json\nfrom nltk.tokenize import word_tokenize\nimport tqdm\n\nclass SequenceClassificationDataLoader(pl.LightningDataModule):\n  def __init__(self, tokenizer, data_file, batch_size, add_extra_nee=False):\n    super().__init__()\n    self.tokenizer = tokenizer\n    self.data_file = data_file\n    self.batch_size = batch_size\n    self.add_extra_nee = add_extra_nee\n\n  def tokenize_strings(self, source_sentences, max_length=512, pad_to_max_length=False, return_tensors=\"pt\"):\n    encoded_dict = self.tokenizer(\n            source_sentences,\n            max_length=max_length,\n            padding=\"max_length\" if pad_to_max_length else \"longest\",\n            truncation=True,\n            return_tensors=return_tensors\n        )\n\n    input_ids = encoded_dict['input_ids']\n    attention_masks = encoded_dict['attention_mask']\n\n    return input_ids, attention_masks\n\n  def encode_sentences(self, source_sentences, labels):\n    input_ids = []\n    attention_masks = []\n\n    input_ids, attention_masks = self.tokenize_strings(source_sentences)\n\n    labels = torch.as_tensor(labels)\n  \n    batch = {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_masks,\n        \"labels\": labels,\n    }\n\n    return batch\n\n  def load_tsv_files(self, filepaths):\n    srcs = []\n    tgts = []\n\n    for filepath in filepaths:\n        with open(filepath) as tsv:\n            for line in tsv:\n                parts = line.strip().split(\"\\t\")\n\n                if len(parts) == 2:\n                    srcs.append(parts[0].strip())\n                    tgts.append(int(parts[1].strip()))\n\n    print(len(tgts))\n    print(\"===\")\n    \n    return {\"source\": srcs, \"target\": tgts}\n\n  def quadruple_to_string(self, claim, question, answer, bool_explanation=\"\"):\n    if bool_explanation is not None and len(bool_explanation) > 0:\n      bool_explanation = \", because \" + bool_explanation.lower().strip()\n    else:\n      bool_explanation = \"\"\n    return \"[CLAIM] \" + claim.strip() + \" [QUESTION] \" + question.strip() + \" \" + answer.strip() + bool_explanation\n\n  def load_tsv_file(self, filepath):\n    examples = self.load_tsv_files([filepath])\n\n    return examples\n\n  def load_averitec_file(self, filepath, add_extra_nee=False, qa_level_labels=False):\n    label_map = {\n      \"Supported\": 0,\n      \"Refuted\": 1,\n      \"Not Enough Evidence\": 2,\n      \"Conflicting Evidence/Cherrypicking\": 3\n    }\n\n    with open(filepath) as f:\n      j = json.load(f)\n      examples = j\n\n    data_points = []\n    for example in examples:\n      label = label_map[example[\"label\"]]\n      for question in example[\"questions\"]:\n        for answer in question[\"answers\"]:\n          if \"boolean_explanation\" in answer:\n            quadruple = example[\"claim\"], question[\"question\"], answer[\"answer\"], answer[\"boolean_explanation\"]\n          else:\n            quadruple = example[\"claim\"], question[\"question\"], answer[\"answer\"], \"\"\n\n          if label == 3 and qa_level_labels: # Discard all conflicting evidence during training\n            continue\n\n          if answer[\"answer_type\"] == \"Unanswerable\" and qa_level_labels: # Set unanswerable questions as nee during training\n            this_dp_label = 2\n          else:\n            this_dp_label = label\n\n          data_points.append((quadruple, this_dp_label))\n\n    out = []\n    for data_point, label in data_points:\n      out.append((self.quadruple_to_string(*data_point), label))\n\n    if add_extra_nee:\n      for data_point, label in data_points:\n        random_other_dp, _ = random.choice(data_points)\n        while random_other_dp == data_point: # Reject if we select the same datapoint twice\n          random_other_dp, _ = random.choice(data_points)\n        \n        out.append((self.quadruple_to_string(data_point[0], random_other_dp[1], random_other_dp[2], random_other_dp[3]), 2))\n\n    print(len(out))\n    print(\"===\")\n    \n    return {\"source\": [o[0] for o in out], \"target\": [o[1] for o in out]}     \n \n  def prepare_data(self): # TODO set up to load answer only baseline\n    self.train = self.load_averitec_file(\n      \"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date-cleaned.train.json\", \n      add_extra_nee=self.add_extra_nee, \n      qa_level_labels=True)\n    self.validate = self.load_averitec_file(\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date-cleaned.dev.json\")\n    self.test = self.load_averitec_file(\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date-cleaned.test.json\")\n\n  # encode the sentences using the tokenizer  \n  def setup(self, stage):\n    self.train = self.encode_sentences(self.train['source'], self.train['target'])\n    \n    self.validate = self.encode_sentences(self.validate['source'], self.validate['target'])\n    \n    self.test = self.encode_sentences(self.test['source'], self.test['target'])\n\n  # Load the training, validation and test sets in Pytorch Dataset objects\n  def train_dataloader(self):\n    dataset = TensorDataset(self.train['input_ids'], self.train['attention_mask'], self.train['labels'])                          \n    train_data = DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = self.batch_size)\n    return train_data\n\n  def val_dataloader(self):\n    dataset = TensorDataset(self.validate['input_ids'], self.validate['attention_mask'], self.validate['labels']) \n    val_data = DataLoader(dataset, batch_size = self.batch_size)                       \n    return val_data\n\n  def test_dataloader(self):\n    dataset = TensorDataset(self.test['input_ids'], self.test['attention_mask'], self.test['labels']) \n    test_data = DataLoader(dataset, batch_size = self.batch_size)                   \n    return test_data\n\n\n"}
{"type": "source_file", "path": "retrieval_reranking/combine_decorated_files.py", "content": "import argparse\nfrom pathlib import Path\nimport os\nimport json\nimport sys\n\nparser = argparse.ArgumentParser(description='Combine averitec files split according to the combine_decorate_files scheme.')\nparser.add_argument('--scheme', default=\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date.test.with_qs.X.json\", help='')\nargs = parser.parse_args()\n\np = Path(args.scheme)\nfolder = p.parent\nfile_scheme = p.name\nprefix, suffix = file_scheme.split(\"X\")\n\nexamples = {}\n\nfor file in os.listdir(folder):\n    if file.startswith(prefix) and file.endswith(suffix) and len(file.split(\".\")) == len(file_scheme.split(\".\")):\n        span = [int(x) for x in file[len(prefix):-len(suffix)].split(\"-\")]\n        print(\"Reading examples \" + str(span[0]) + \" to \" + str(span[1]) +  \".\", file=sys.stderr)\n\n        with open(str(folder) + \"/\" +file) as f:\n            file_examples = json.load(f)\n            for idx, example in enumerate(file_examples):\n                if idx+span[0] in examples:\n                    print(\"Error: Index overlap at index \"+str(idx+span[0]), file=sys.stderr)\n                    exit()\n                examples[idx+span[0]] = example\n\nprint(\"Combining \"+str(len(examples)) + \" examples.\", file=sys.stderr)\nout_examples = [None] * len(examples)\n\nfor idx, example in examples.items():\n    out_examples[idx] = example\n\nfor example in out_examples:\n    if example is None:\n        print(\"Error: Index mismatch\", file=sys.stderr)\n        exit()\n\nprint(json.dumps(out_examples, indent=4))"}
{"type": "source_file", "path": "models/JustificationGenerationModule.py", "content": "import pytorch_lightning as pl\nimport torch\nimport numpy as np\nimport datasets\nfrom transformers import MaxLengthCriteria, StoppingCriteriaList\nfrom transformers.optimization import AdamW\nimport itertools\nfrom utils import count_stats, f1_metric, pairwise_meteor\nfrom torchmetrics.text.rouge import ROUGEScore\nimport torch.nn.functional as F\nimport torchmetrics\nfrom torchmetrics.classification import F1Score\n\ndef freeze_params(model):\n  for layer in model.parameters():\n    layer.requires_grade = False\n\nclass JustificationGenerationModule(pl.LightningModule):\n    \n  def __init__(self, tokenizer, model, learning_rate=1e-3, gen_num_beams=2, gen_max_length=100, should_pad_gen=True):\n    super().__init__()\n    self.tokenizer = tokenizer\n    self.model = model\n    self.learning_rate = learning_rate\n\n    self.gen_num_beams = gen_num_beams\n    self.gen_max_length = gen_max_length\n    self.should_pad_gen = should_pad_gen\n\n    #self.metrics =  datasets.load_metric('meteor')\n\n    freeze_params(self.model.get_encoder())\n    self.freeze_embeds()\n  \n  def freeze_embeds(self):\n    ''' freeze the positional embedding parameters of the model; adapted from finetune.py '''\n    freeze_params(self.model.model.shared)\n    for d in [self.model.model.encoder, self.model.model.decoder]:\n      freeze_params(d.embed_positions)\n      freeze_params(d.embed_tokens)\n\n  # Do a forward pass through the model\n  def forward(self, input_ids, **kwargs):\n    return self.model(input_ids, **kwargs)\n  \n  def configure_optimizers(self):\n    optimizer = AdamW(self.parameters(), lr = self.learning_rate)\n    return optimizer\n\n  def shift_tokens_right(self, input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n    \"\"\"\n    Shift input ids one token to the right.\n    https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/modeling_bart.py.\n    \"\"\"\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n    shifted_input_ids[:, 0] = decoder_start_token_id\n\n    if pad_token_id is None:\n        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n    # replace possible -100 values in labels by `pad_token_id`\n    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n    return shifted_input_ids\n\n  def run_model(self, batch):\n    src_ids, src_mask, tgt_ids = batch[0], batch[1], batch[2]\n\n    decoder_input_ids = self.shift_tokens_right(\n                tgt_ids, self.tokenizer.pad_token_id, self.tokenizer.pad_token_id # BART uses the EOS token to start generation as well. Might have to change for other models.\n            )\n\n    outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n    return outputs\n\n  def compute_loss(self, batch):\n    tgt_ids = batch[2]\n    logits = self.run_model(batch)[0]\n\n    cross_entropy = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n    loss = cross_entropy(logits.view(-1, logits.shape[-1]), tgt_ids.view(-1))\n\n    return loss\n\n  def training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n\n    self.log(\"train_loss\", loss, on_epoch=True)\n\n    return {'loss':loss}\n\n  def validation_step(self, batch, batch_idx):\n    preds, loss, tgts = self.generate_and_compute_loss_and_tgts(batch)\n    if self.should_pad_gen:\n      preds = F.pad(preds, pad=(0, self.gen_max_length - preds.shape[1]), value=self.tokenizer.pad_token_id)\n\n    self.log('val_loss', loss, prog_bar=True, sync_dist=True)\n\n    return {'loss': loss, 'pred': preds, 'target': tgts}\n\n  def test_step(self, batch, batch_idx):\n    test_preds, test_loss, test_tgts = self.generate_and_compute_loss_and_tgts(batch)\n    if self.should_pad_gen:\n      test_preds = F.pad(test_preds, pad=(0, self.gen_max_length - test_preds.shape[1]), value=self.tokenizer.pad_token_id)\n\n    self.log('test_loss', test_loss, prog_bar=True, sync_dist=True)\n\n    return {'loss': test_loss, 'pred': test_preds, 'target': test_tgts}\n\n  def test_epoch_end(self, outputs):\n    self.handle_end_of_epoch_scoring(outputs, \"test\")\n\n  def validation_epoch_end(self, outputs):\n    self.handle_end_of_epoch_scoring(outputs, \"val\")\n\n  def handle_end_of_epoch_scoring(self, outputs, prefix):\n      gen = {}\n      tgt = {}\n      rouge = ROUGEScore()\n      rouge_metric = lambda x, y: rouge(x,y)[\"rougeL_precision\"]\n      for out in outputs:\n        preds = out['pred']\n        tgts = out['target']\n\n        preds = self.do_batch_detokenize(preds)\n        tgts = self.do_batch_detokenize(tgts)\n\n        for pred, t in zip(preds, tgts):\n          rouge_d = rouge_metric(pred, t)\n          self.log(prefix+\"_rouge\", rouge_d)\n\n          meteor_d = pairwise_meteor(pred, t)\n          self.log(prefix+\"_meteor\", meteor_d)\n\n  def generate_and_compute_loss_and_tgts(self, batch):\n    src_ids = batch[0]\n    loss = self.compute_loss(batch)\n    pred_ids, _ = self.generate_for_batch(src_ids)\n\n    tgt_ids = batch[2]\n\n    return pred_ids, loss, tgt_ids\n\n  def do_batch_detokenize(self, batch):\n    tokens = self.tokenizer.batch_decode(\n      batch, \n      skip_special_tokens=True, \n      clean_up_tokenization_spaces=True\n      )\n\n    # Huggingface skipping of special tokens doesn't work for all models, so we do it manually as well for safety:\n    tokens = [p.replace(\"<pad>\", \"\") for p in tokens]\n    tokens = [p.replace(\"<s>\", \"\") for p in tokens]\n    tokens = [p.replace(\"</s>\", \"\") for p in tokens]\n\n    return [t for t in tokens if t != \"\"]\n  \n  def generate_for_batch(self, batch):\n    generated_ids = self.model.generate(\n      batch, \n      decoder_start_token_id = self.tokenizer.pad_token_id,\n      num_beams = self.gen_num_beams,\n      max_length = self.gen_max_length\n      )\n\n    generated_tokens = self.tokenizer.batch_decode(\n      generated_ids, \n      skip_special_tokens=True, \n      clean_up_tokenization_spaces=True\n      )\n\n    return generated_ids, generated_tokens\n\n\n  def generate(self, text, max_input_length=512, device=None):\n    encoded_dict = self.tokenizer(\n            [text],\n            max_length=max_input_length,\n            padding=\"longest\",\n            truncation=True,\n            return_tensors=\"pt\",\n            add_prefix_space = True\n        )\n\n    input_ids = encoded_dict['input_ids']\n\n    if device is not None:\n      input_ids = input_ids.to(device)\n\n    with torch.no_grad():\n        _, generated_tokens = self.generate_for_batch(input_ids)\n    \n    return generated_tokens[0]"}
{"type": "source_file", "path": "retrieval_coarse/prompt_question_generation_vicuna.py", "content": "import numpy as np\nimport argparse\nimport json\nimport nltk\nfrom rank_bm25 import BM25Okapi\nimport tqdm\nimport torch\nfrom accelerate import Accelerator\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\n\nparser = argparse.ArgumentParser(description='Generate extra questions based on claims with a prompt. Useful for searching.')\nparser.add_argument('--reference_corpus', default=\"data/train.json\", help='')\nparser.add_argument('--target_file', default=\"data/dev.json\", help='')\nparser.add_argument('--n', default=5, help='')\nargs = parser.parse_args()\n\nwith open(args.target_file) as f:\n    j = json.load(f)\n    examples = j #[\"examples\"]\n\nwith open(args.reference_corpus) as f:\n    j = json.load(f)\n    train_examples = j\n\nall_data_corpus = []\ntokenized_corpus = []\n\nfor train_example in train_examples:\n    train_claim = train_example[\"claim\"]\n\n    speaker = train_example[\"speaker\"].strip() if train_example[\"speaker\"] is not None and len(train_example[\"speaker\"]) > 1 else \"they\"\n\n    questions = [q[\"question\"] for q in train_example[\"questions\"]]\n\n    claim_dict_builder = {}\n    claim_dict_builder[\"claim\"] = train_claim\n    claim_dict_builder[\"speaker\"] = speaker\n    claim_dict_builder[\"questions\"] = questions\n\n    tokenized_corpus.append(nltk.word_tokenize(claim_dict_builder[\"claim\"]))\n    all_data_corpus.append(claim_dict_builder)\n\nbm25 = BM25Okapi(tokenized_corpus)\n\n# Define methods to transform retrieved docs into a prompt:\ndef doc2prompt(doc):\n    prompt_parts = \"USER: Outrageously, \" + doc[\"speaker\"] + \" claimed that \\\"\" + doc[\"claim\"].strip() + \"\\\".\\nASSISTANT: \"\n    questions = [q.strip().replace(\"?\", \"\") + \"?\" for q in doc[\"questions\"]]\n    return prompt_parts + \" \".join(questions)\n\ndef docs2prompt(top_docs):\n    return \"\\n\\n\".join([doc2prompt(d) for d in top_docs])\n\n# Define the bloom model:\naccelerator = Accelerator()\naccel_device = accelerator.device\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = LlamaTokenizer.from_pretrained(\"/rds/user/mss84/hpc-work/vicuna/13B/\")\nmodel = LlamaForCausalLM.from_pretrained(\n    \"/rds/user/mss84/hpc-work/vicuna/13B/\",\n).to(device)\n\nfor example in tqdm.tqdm(examples):\n    test_claim = example[\"claim\"]\n    speaker = example[\"speaker\"].strip() if example[\"speaker\"] is not None and len(example[\"speaker\"]) > 1 else \"they\"\n\n    s = bm25.get_scores(nltk.word_tokenize(test_claim))\n    top_n = np.argsort(s)[::-1][:args.n]\n    docs = [all_data_corpus[i] for i in top_n]\n\n    prompt = \"A chat between a curious user and an artificial intelligence assistant. The assistant provides questions that can be used to gather evidence for the claim via web search. Some examples:\\n\\n\" + docs2prompt(docs) + \"\\n\\n\" + \"</s> USER: Outrageously, \" + speaker + \" claimed that \\\"\"+ test_claim.strip() + \"\\\". Please ask 3-5 questions ending with \\'?\\' that I can use as search queries when looking for evidence.\\n</s> ASSISTANT:\"\n    sentences = [prompt]\n\n    inputs = tokenizer(\n    sentences, \n    return_tensors=\"pt\").to(device)\n\n    outputs = model.generate(inputs[\"input_ids\"],\n        max_length=2000,\n    )\n\n    prompt_len = inputs[\"input_ids\"].shape[-1]\n\n    tgt_text = tokenizer.batch_decode(outputs[:, prompt_len:], skip_special_tokens=True)[0]\n    #questions_str = tgt_text.split(\"\\n\")[0]\n\n    qs = tgt_text.split(\"?\")\n    qs = [q.strip() + \"?\" for q in qs if q.strip() and len(q.strip()) < 300]\n\n    example[\"questions\"] = [{\"question\": q, \"answers\": []} for q in qs]\n\nprint(json.dumps(examples, indent=4))"}
{"type": "source_file", "path": "justification_production/trained_model_justification_generation.py", "content": "import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom transformers import BartTokenizer, BartForConditionalGeneration, AdamW, BartConfig\nimport os\nimport sys\nimport argparse\nimport json\nimport tqdm\nimport torch\n\nsys.path.insert(1, os.path.join(sys.path[0], '..'))\n\nfrom data_loaders.JustificationProductionDataLoader import JustificationProductionDataLoader\nfrom models.JustificationGenerationModule import JustificationGenerationModule\n\nparser = argparse.ArgumentParser(description='Perform veracity prediction using a stance detection model..')\nparser.add_argument('--averitec_file', default=\"data/dev.json\", help='')\nparser.add_argument('--train', action='store_true', help='Marks that training should happen. Otherwise, only inference is executed.')\nparser.add_argument('--gpus', default=1, help='The number of available GPUs')\nargs = parser.parse_args()\n\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large', add_prefix_space=True)\nbart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\nmodel = JustificationGenerationModule(tokenizer = tokenizer, model = bart_model, learning_rate = 1e-5)\n\ndataLoader = JustificationProductionDataLoader(\n  tokenizer = tokenizer, \n  batch_size = 32\n  )\n\nexperiment_name = \"bart_justifications_verdict\"\n\ncheckpoint = ModelCheckpoint(\n  dirpath='/rds/user/mss84/hpc-work/checkpoint_files/averitec',\n  filename=experiment_name+\"-{epoch:02d}-{val_loss:.2f}-{val_meteor:.2f}\", \n  save_top_k=1, \n  monitor=\"val_meteor\",\n  mode=\"max\"\n  )\n\nlogger = pl.loggers.TensorBoardLogger(\n                save_dir=os.getcwd(),\n                version=experiment_name,\n                name='lightning_logs'\n            )\n\ntrainer = pl.Trainer(gpus=args.gpus,\n  max_epochs=20,\n  min_epochs=20,\n  auto_lr_find=False,\n  progress_bar_refresh_rate=1,\n  callbacks=[checkpoint],\n  logger=logger,\n  accumulate_grad_batches=4,\n  strategy=\"dp\", #I tried ddp, it breaks\n  num_nodes=1\n)\n\nif args.train:\n    trainer.validate(model, dataLoader) # This makes pytorch lightning log initial values for dev loss etc. Nice for tensorboard.\n    trainer.fit(model, dataLoader)\n    best_checkpoint = checkpoint.best_model_path\n    print(\"Finished training. The best checkpoint is stored at '\" + best_checkpoint + \"'.\")\nelse:\n    best_checkpoint = \"/rds/user/mss84/hpc-work/checkpoint_files/averitec/bart_justifications-epoch=16-val_loss=2.04-val_meteor=0.27.ckpt\"\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntrained_model = JustificationGenerationModule.load_from_checkpoint(best_checkpoint, tokenizer = tokenizer, model = bart_model).to(device)\n\nif args.train:\n    print(\"Running inference...\")\n    trainer.test(trained_model, dataLoader)\nelse:\n    with open(args.averitec_file) as f:\n        examples = json.load(f)\n    \n\n    for example in tqdm.tqdm(examples):\n        claim_str = dataLoader.extract_claim_str(example)\n        claim_str.strip()\n\n        example[\"justification\"] = trained_model.generate(claim_str, device=device)\n\nprint(json.dumps(examples, indent=4))"}
{"type": "source_file", "path": "retrieval_reranking/no_reranker.py", "content": "import argparse\nimport json\nimport pandas as pd\nimport nltk\nfrom rank_bm25 import BM25Okapi\nimport numpy as np\nimport os\nimport sys\nimport torch\nimport tqdm\nfrom transformers import BloomTokenizerFast, BloomModel, BloomForCausalLM\nfrom accelerate import Accelerator\n\n\nparser = argparse.ArgumentParser(description='A tiny script that does not do any reranking, but simply spits out BM25 top-3N as QA pairs..')\nparser.add_argument('--averitec_file', default=\"/rds/user/mss84/hpc-work/datasets/averitec/initial_test_dataset/dev.neat.top_100_with_questions.recombined.json\", help='')\nparser.add_argument('--n', default=3, help='')\nargs = parser.parse_args()\n\nwith open(args.averitec_file) as f:\n    examples = json.load(f)\n\nfor example in tqdm.tqdm(examples):\n    top_n = example[\"bm25_qas\"][:args.n] if \"bm25_qas\" in example else []\n    pass_through = [{\"question\": qa[0], \"answers\": [{\"answer\": qa[1]}]} for qa in top_n]\n\n    example[\"questions\"] = pass_through\n\nprint(json.dumps(examples, indent=4))"}
{"type": "source_file", "path": "data_loaders/JustificationProductionDataLoader.py", "content": "import random\nfrom torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\nimport pandas as pd\nimport numpy as np\nimport torch\nimport pytorch_lightning as pl\nimport json\nfrom nltk.tokenize import word_tokenize\nimport tqdm\n\nclass JustificationProductionDataLoader(pl.LightningDataModule):\n  def __init__(self, tokenizer, batch_size):\n    super().__init__()\n    self.tokenizer = tokenizer\n    self.batch_size = batch_size\n\n  def tokenize_strings(self, source_sentences, max_length=512, pad_to_max_length=False, return_tensors=\"pt\"):\n    encoded_dict = self.tokenizer(\n            source_sentences,\n            max_length=max_length,\n            padding=\"max_length\" if pad_to_max_length else \"longest\",\n            truncation=True,\n            return_tensors=return_tensors\n        )\n\n    input_ids = encoded_dict['input_ids']\n    attention_masks = encoded_dict['attention_mask']\n\n    return input_ids, attention_masks\n\n  def encode_sentences(self, source_sentences, target_sentences):\n    src_input_ids, src_attention_masks = self.tokenize_strings(source_sentences)\n    tgt_input_ids, tgt_attention_masks = self.tokenize_strings(target_sentences)\n  \n    batch = {\n        \"src_input_ids\": src_input_ids,\n        \"src_attention_mask\": src_attention_masks,\n        \"tgt_input_ids\": tgt_input_ids,\n        \"tgt_attention_mask\": tgt_attention_masks,\n    }\n\n    return batch\n\n  def load_averitec_file(self, filepath):\n    with open(filepath) as f:\n      j = json.load(f)\n      examples = j\n\n    out_src = []\n    out_tgt = []\n    for example in examples:\n        claim_str = self.extract_claim_str(example)\n\n        claim_str.strip()\n        out_src.append(claim_str)\n        out_tgt.append(example[\"justification\"].strip())\n\n    print(len(out_src))\n    print(\"===\")\n    \n    return {\"source\": out_src, \"target\": out_tgt} \n\n  def extract_claim_str(self, example):\n      claim_str = \"[CLAIM] \" + example[\"claim\"] + \" [EVIDENCE] \"\n      for question in example[\"questions\"]:\n          q_text = question[\"question\"].strip()\n\n          if len(q_text) == 0:\n              continue\n\n          if not q_text[-1] == \"?\":\n              q_text += \"?\"\n\n          answer_strings = []\n\n          for answer in question[\"answers\"]:\n              if \"answer_type\" in answer and answer[\"answer_type\"] == \"Boolean\":\n                  answer_strings.append(answer[\"answer\"] + \". \" + answer[\"boolean_explanation\"])\n              else:\n                  answer_strings.append(answer[\"answer\"])\n\n          claim_str += q_text\n          for a_text in answer_strings:\n              if not a_text[-1] == \".\":\n                  a_text += \".\"\n\n              claim_str += \" \" + a_text.strip()\n\n          claim_str += \" \"\n      \n      claim_str += \" [VERDICT] \" + example[\"label\"]\n      return claim_str    \n \n  def prepare_data(self): # TODO set up to load answer only baseline\n    self.train = self.load_averitec_file(\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date-cleaned.train.json\")\n    self.validate = self.load_averitec_file(\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date-cleaned.dev.json\")\n    self.test = self.load_averitec_file(\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date-cleaned.test.json\")\n\n  # encode the sentences using the tokenizer  \n  def setup(self, stage):\n    self.train = self.encode_sentences(self.train['source'], self.train['target'])\n    self.validate = self.encode_sentences(self.validate['source'], self.validate['target'])\n    self.test = self.encode_sentences(self.test['source'], self.test['target'])\n\n  # Load the training, validation and test sets in Pytorch Dataset objects\n  def train_dataloader(self):\n    dataset = TensorDataset(self.train['src_input_ids'], self.train['src_attention_mask'], self.train['tgt_input_ids'], self.train['tgt_attention_mask'])                          \n    train_data = DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = self.batch_size)\n    return train_data\n\n  def val_dataloader(self):\n    dataset = TensorDataset(self.validate['src_input_ids'], self.validate['src_attention_mask'], self.validate['tgt_input_ids'], self.validate['tgt_attention_mask']) \n    val_data = DataLoader(dataset, batch_size = self.batch_size)                       \n    return val_data\n\n  def test_dataloader(self):\n    dataset = TensorDataset(self.test['src_input_ids'], self.test['src_attention_mask'], self.test['tgt_input_ids'], self.test['tgt_attention_mask']) \n    test_data = DataLoader(dataset, batch_size = self.batch_size)                   \n    return test_data\n\n\n"}
{"type": "source_file", "path": "retrieval_reranking/trained_model_reranker.py", "content": "import argparse\nimport json\nimport pandas as pd\nimport nltk\nfrom rank_bm25 import BM25Okapi\nimport numpy as np\nimport os\nimport sys\nimport torch\nimport tqdm\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\nsys.path.insert(1, os.path.join(sys.path[0], '..'))\nfrom models.DualEncoderModule import DualEncoderModule\n\nparser = argparse.ArgumentParser(description='A script that reranks by relying on a trained model to score claim-question-answer triples.')\nparser.add_argument('--averitec_file', default=\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date.test.with_qs_combined.json\", help='')\nparser.add_argument('--n', default=3, help='')\nargs = parser.parse_args()\n\nwith open(args.averitec_file) as f:\n    examples = json.load(f)\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2, problem_type=\"single_label_classification\") # Must specify single_label for some reason\nbest_checkpoint = \"/rds/user/mss84/hpc-work/checkpoint_files/averitec/bert_dual_encoder_true_withfcb-epoch=19-val_loss=0.00-val_acc=0.93.ckpt\"\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntrained_model = DualEncoderModule.load_from_checkpoint(best_checkpoint, tokenizer = tokenizer, model = bert_model).to(device)\n\ndef triple_to_string(x):\n    return \" </s> \".join([item.strip() for item in x])\n\nfor example in tqdm.tqdm(examples):\n    strs_to_score = []\n    values = []\n\n    bm25_qas = example[\"bm25_qas\"] if \"bm25_qas\" in example else []\n\n    for question,answer, source in bm25_qas:\n        claim = example[\"claim\"]\n\n        str_to_score = triple_to_string([claim, question, answer])\n\n        strs_to_score.append(str_to_score)\n        values.append([question, answer, source])\n\n    if len(bm25_qas) > 0:\n        encoded_dict = tokenizer(\n            strs_to_score,\n            max_length=512,\n            padding=\"longest\",\n            truncation=True,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        input_ids = encoded_dict['input_ids']\n        attention_masks = encoded_dict['attention_mask']\n\n        scores = torch.softmax(trained_model(input_ids, attention_mask=attention_masks).logits, axis=-1)[:, 1]\n    \n        top_n = torch.argsort(scores, descending=True)[:args.n]\n        pass_through = [{\"question\": values[i][0], \"answers\": [{\"answer\": values[i][1], \"source_url\": values[i][2]}]} for i in top_n]\n    else:\n        pass_through = []    \n\n    example[\"questions\"] = pass_through\n\nprint(json.dumps(examples, indent=4))"}
{"type": "source_file", "path": "utils.py", "content": "import os\nimport nltk\nfrom nltk import word_tokenize\nimport numpy as np\nfrom leven import levenshtein\nfrom sklearn.cluster import DBSCAN, dbscan\n\ndef delete_if_exists(filepath):\n    if os.path.exists(filepath):\n        os.remove(filepath)\n\ndef pairwise_meteor(candidate, reference): # Todo this is not thread safe, no idea how to make it so\n    return nltk.translate.meteor_score.single_meteor_score(word_tokenize(reference), word_tokenize(candidate))\n\ndef count_stats(candidate_dict, reference_dict):\n    count_match = [0 for _ in candidate_dict]\n    count_diff = [0 for _ in candidate_dict]\n\n    for i, k in enumerate(candidate_dict.keys()):\n      pred_parts = candidate_dict[k]\n      tgt_parts = reference_dict[k]\n\n      if len(pred_parts) == len(tgt_parts):\n        count_match[i] = 1\n\n      count_diff[i] = abs(len(pred_parts) - len(tgt_parts))\n\n    count_match_score = np.mean(count_match)\n    count_diff_score = np.mean(count_diff)\n\n    return {\n        \"count_match_score\": count_match_score,\n        \"count_diff_score\": count_diff_score\n    }\n\ndef f1_metric(candidate_dict, reference_dict, pairwise_metric):\n    all_best_p = [0 for _ in candidate_dict]\n    all_best_t = [0 for _ in candidate_dict]\n    p_unnorm = []\n\n    for i, k in enumerate(candidate_dict.keys()):\n      pred_parts = candidate_dict[k]\n      tgt_parts = reference_dict[k]\n\n      best_p_score = [0 for _ in pred_parts]\n      best_t_score = [0 for _ in tgt_parts]\n\n      for p_idx in range(len(pred_parts)):\n        for t_idx in range(len(tgt_parts)):\n          #meteor_score = pairwise_meteor(pred_parts[p_idx], tgt_parts[t_idx])\n          metric_score = pairwise_metric(pred_parts[p_idx], tgt_parts[t_idx])\n\n          if metric_score > best_p_score[p_idx]:\n            best_p_score[p_idx] = metric_score\n\n          if metric_score > best_t_score[t_idx]:\n            best_t_score[t_idx] = metric_score\n\n      all_best_p[i] = np.mean(best_p_score) if len(best_p_score) > 0 else 1.0\n      all_best_t[i] = np.mean(best_t_score) if len(best_t_score) > 0 else 1.0       \n\n      p_unnorm.extend(best_p_score) \n\n    p_score = np.mean(all_best_p)\n    r_score = np.mean(all_best_t)\n    avg_score = (p_score + r_score) / 2\n    f1_score = 2 * p_score * r_score / (p_score + r_score + 1e-8)\n\n    p_unnorm_score = np.mean(p_unnorm)\n\n    return {\n        \"p\": p_score,\n        \"r\": r_score,\n        \"avg\": avg_score,\n        \"f1\": f1_score,\n        \"p_unnorm\": p_unnorm_score,\n    }\n\ndef edit_distance_dbscan(data):\n  # Inspired by https://scikit-learn.org/stable/faq.html#how-do-i-deal-with-string-data-or-trees-graphs\n  def lev_metric(x, y):\n    i, j = int(x[0]), int(y[0])\n    return levenshtein(data[i], data[j])\n\n  X = np.arange(len(data)).reshape(-1, 1)\n\n  clustering = dbscan(X, metric=lev_metric, eps=20, min_samples=2, algorithm='brute')\n  return clustering\n\ndef compute_all_pairwise_edit_distances(data):\n  X = np.empty((len(data), len(data)))\n\n  for i in range(len(data)):\n    for j in range(len(data)):\n      X[i][j] = levenshtein(data[i], data[j])\n\n  return X\n\ndef compute_all_pairwise_scores(src_data, tgt_data, metric):\n  X = np.empty((len(src_data), len(tgt_data)))\n\n  for i in range(len(src_data)):\n    for j in range(len(tgt_data)):\n      X[i][j] = (metric(src_data[i], tgt_data[j]))\n\n  return X\n\ndef compute_all_pairwise_meteor_scores(data):\n  X = np.empty((len(data), len(data)))\n\n  for i in range(len(data)):\n    for j in range(len(data)):\n      X[i][j] = (pairwise_meteor(data[i], data[j]) + pairwise_meteor(data[j], data[i])) / 2\n\n  return X\n\ndef edit_distance_custom(data, X, eps=0.5, min_samples=3):\n  clustering = DBSCAN(metric=\"precomputed\", eps=eps, min_samples=min_samples).fit(X)\n  return clustering.labels_\n"}
{"type": "source_file", "path": "data_loaders/NoEvidenceDataLoader.py", "content": "import random\nfrom torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\nimport pandas as pd\nimport numpy as np\nimport torch\nimport pytorch_lightning as pl\nimport json\nfrom nltk.tokenize import word_tokenize\nimport tqdm\n\nclass NoEvidenceDataLoader(pl.LightningDataModule):\n  def __init__(self, tokenizer, data_file, batch_size, add_extra_nee=False):\n    super().__init__()\n    self.tokenizer = tokenizer\n    self.data_file = data_file\n    self.batch_size = batch_size\n    self.add_extra_nee = add_extra_nee\n\n  def tokenize_strings(self, source_sentences, max_length=512, pad_to_max_length=False, return_tensors=\"pt\"):\n    encoded_dict = self.tokenizer(\n            source_sentences,\n            max_length=max_length,\n            padding=\"max_length\" if pad_to_max_length else \"longest\",\n            truncation=True,\n            return_tensors=return_tensors\n        )\n\n    input_ids = encoded_dict['input_ids']\n    attention_masks = encoded_dict['attention_mask']\n\n    return input_ids, attention_masks\n\n  def encode_sentences(self, source_sentences, labels):\n    input_ids = []\n    attention_masks = []\n\n    input_ids, attention_masks = self.tokenize_strings(source_sentences)\n\n    labels = torch.as_tensor(labels)\n  \n    batch = {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_masks,\n        \"labels\": labels,\n    }\n\n    return batch\n\n  def load_averitec_file(self, filepath, add_extra_nee=False, qa_level_labels=False):\n    label_map = {\n      \"Supported\": 0,\n      \"Refuted\": 1,\n      \"Not Enough Evidence\": 2,\n      \"Conflicting Evidence/Cherrypicking\": 3\n    }\n\n    with open(filepath) as f:\n      j = json.load(f)\n      examples = j\n\n    data_points = []\n    for example in examples:\n      label = label_map[example[\"label\"]]\n      claim = example[\"claim\"]\n\n      data_points.append((claim, label))\n\n    print(len(data_points))\n    print(\"===\")\n    \n    return {\"source\": [o[0] for o in data_points], \"target\": [o[1] for o in data_points]}     \n \n  def prepare_data(self): # TODO set up to load answer only baseline\n    self.train = self.load_averitec_file(\n      \"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date-cleaned.train.json\", \n      add_extra_nee=self.add_extra_nee, \n      qa_level_labels=True)\n    self.validate = self.load_averitec_file(\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date-cleaned.dev.json\")\n    self.test = self.load_averitec_file(\"/rds/user/mss84/hpc-work/datasets/averitec/full_data/date-cleaned.test.json\")\n\n  # encode the sentences using the tokenizer  \n  def setup(self, stage):\n    self.train = self.encode_sentences(self.train['source'], self.train['target'])\n    \n    self.validate = self.encode_sentences(self.validate['source'], self.validate['target'])\n    \n    self.test = self.encode_sentences(self.test['source'], self.test['target'])\n\n  # Load the training, validation and test sets in Pytorch Dataset objects\n  def train_dataloader(self):\n    dataset = TensorDataset(self.train['input_ids'], self.train['attention_mask'], self.train['labels'])                          \n    train_data = DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = self.batch_size)\n    return train_data\n\n  def val_dataloader(self):\n    dataset = TensorDataset(self.validate['input_ids'], self.validate['attention_mask'], self.validate['labels']) \n    val_data = DataLoader(dataset, batch_size = self.batch_size)                       \n    return val_data\n\n  def test_dataloader(self):\n    dataset = TensorDataset(self.test['input_ids'], self.test['attention_mask'], self.test['labels']) \n    test_data = DataLoader(dataset, batch_size = self.batch_size)                   \n    return test_data\n\n\n"}
{"type": "source_file", "path": "retrieval_reranking/decorate_with_questions_vicuna.py", "content": "import argparse\nimport json\nimport pandas as pd\nimport nltk\nfrom rank_bm25 import BM25Okapi\nimport numpy as np\nimport os\nimport sys\nimport torch\nimport tqdm\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\nfrom accelerate import Accelerator\n\n\nparser = argparse.ArgumentParser(description='Use a prompt to generate questions that could be answered by top-k retrieved evidence. Output generated questions.')\nparser.add_argument('--reference_corpus', default=\"data/train.json\", help='')\nparser.add_argument('--target_file', default=\"data/dev.json\", help='')\nparser.add_argument('--url_file', default=\"search_results.tsv\", help='')\nparser.add_argument('--store_folder', default=\"store/retrieved_docs\", help='')\nparser.add_argument('--top_k', default=100, type=int, help='How many documents should we pick out with BM25')\nparser.add_argument('--start_idx', default=0, type=int, help='Which claim to start at. Useful for larger corpus.')\nparser.add_argument('--n_to_compute', default=-1, type=int, help='How many claims to work through. Useful for larger datasets.')\nargs = parser.parse_args()\n\n# Construct the prompts to retrieve and set up BM25 to find similar evidence:\n\nwith open(args.reference_corpus) as f:\n    train_examples = json.load(f)\n\ndef claim2prompts(example):\n    claim = example[\"claim\"]\n\n    #claim_str = \"Claim: \" + claim + \"||Evidence: \"\n    claim_str = \"USER: If the answer is \\\"\"\n\n    for question in example[\"questions\"]:\n        q_text = question[\"question\"].strip()\n        if len(q_text) == 0:\n            continue\n\n        if not q_text[-1] == \"?\":\n            q_text += \"?\"\n\n        answer_strings = []\n\n        for a in question[\"answers\"]:\n            if a[\"answer_type\"] in [\"Extractive\", \"Abstractive\"]:\n                answer_strings.append(a[\"answer\"])\n            if a[\"answer_type\"] == \"Boolean\":\n                answer_strings.append(a[\"answer\"]  + \", because \" + a[\"boolean_explanation\"].lower().strip())\n\n        for a_text in answer_strings:\n            if not a_text[-1] in [\".\", \"!\", \":\", \"?\"]:\n                a_text += \".\"\n\n            #prompt_lookup_str = claim + \" \" + a_text\n            prompt_lookup_str = a_text\n            this_q_claim_str = claim_str + a_text.strip() + \"\\\", what is the question?||ASSISTANT: \" + q_text\n            yield (prompt_lookup_str, this_q_claim_str.replace(\"\\n\", \" \").replace(\"||\", \"\\n\")[:1500]) # Put a max length of 1.5k tokens because some qa pairs are very long\n\nprompt_corpus = []\ntokenized_corpus = []\n\nfor example in tqdm.tqdm(train_examples):\n    for lookup_str, prompt in  claim2prompts(example):\n        entry = nltk.word_tokenize(lookup_str)\n        tokenized_corpus.append(entry)\n        prompt_corpus.append(prompt)\n    \nprompt_bm25 = BM25Okapi(tokenized_corpus)\n\n\n# Attach evidence to examples:\n\nwith open(args.target_file) as f:\n    examples = json.load(f)\n\nwith open(args.url_file) as url_file:\n    first = True\n    for line in url_file:\n        if first:\n            first = False\n        else:\n            line_parts = line.strip().split(\"\\t\")\n\n            if len(line_parts) < 7:\n                continue\n            idx = int(line_parts[0])\n\n            #claim = line_parts[1]\n            url = line_parts[2]\n            store_file = args.store_folder + \"/\" + line_parts[-1].split(\"/\")[-1]\n\n            if \"retrieved_store_files\" not in examples[idx]:\n                examples[idx][\"retrieved_store_files\"] = []\n            examples[idx][\"retrieved_store_files\"].append(store_file)\n\n# Define the vicuna model:\naccelerator = Accelerator()\naccel_device = accelerator.device\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = LlamaTokenizer.from_pretrained(\"/rds/user/mss84/hpc-work/vicuna/13B/\")\nmodel = LlamaForCausalLM.from_pretrained(\n    \"/rds/user/mss84/hpc-work/vicuna/13B/\",\n).to(device)\n\ndef shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n    \"\"\"\n    Shift input ids one token to the right.\n    https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/modeling_bart.py.\n    \"\"\"\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n    shifted_input_ids[:, 0] = decoder_start_token_id\n\n    if pad_token_id is None:\n        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n    # replace possible -100 values in labels by `pad_token_id`\n    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n    return shifted_input_ids\n\n\n# Go through the dataset, generating questions for the evidence: \nend_idx = -1\nif args.n_to_compute != -1:\n    end_idx = args.start_idx+args.n_to_compute\n    \nfor idx,example in enumerate(tqdm.tqdm(examples[args.start_idx:end_idx])):\n    # First, retrieve top 50 documents with bm25:\n    tokenized_corpus = []\n    all_data_corpus = []\n\n    this_example_store_files = [] if \"retrieved_store_files\" not in example else example[\"retrieved_store_files\"] \n    for store_file in this_example_store_files:\n        with open(store_file, 'r') as f:\n            first = True\n            for line in f:\n                line = line.strip()\n\n                if first:\n                    first = False\n                    location_url = line\n                    continue\n                \n                if len(line) > 3:\n                    entry = nltk.word_tokenize(line)\n                    if (location_url, line) not in all_data_corpus:\n                        tokenized_corpus.append(entry)\n                        all_data_corpus.append((location_url, line))\n\n    if len(tokenized_corpus) == 0:\n        print(\"\")\n        continue\n    \n    bm25 = BM25Okapi(tokenized_corpus)\n    s = bm25.get_scores(nltk.word_tokenize(example[\"claim\"]))\n    n_coarse = args.top_k\n    top_n = np.argsort(s)[::-1][:n_coarse]\n    docs = [all_data_corpus[i] for i in top_n]\n\n    tracker = []\n    docs_with_qs = []\n\n    # Then, generate questions for those top 50:\n    for doc in docs:\n        #prompt_lookup_str = example[\"claim\"] + \" \" + doc[1]\n        prompt_lookup_str = doc[1] \n\n        prompt_s = prompt_bm25.get_scores(nltk.word_tokenize(prompt_lookup_str))\n        prompt_n = 3\n        prompt_top_n = np.argsort(prompt_s)[::-1][:prompt_n]\n        prompt_docs = [prompt_corpus[i] for i in prompt_top_n]\n\n        # Cutoff at 5k characters, otherwise some docs are just too big  \n        claim_prompt = \"\\n\\n</s> USER: If the answer is \\\"\" + doc[1][:5000].replace(\"\\n\", \" \") + \"\\\", what is the question?</s> ASSISTANT:\"\n\n        prompt = \"A chat between a curious user and an artificial intelligence assistant. The assistant reformulates evidence sentences into questions and answers. Some examples:\\n\\n\" + \"\\n\\n\".join(prompt_docs) + claim_prompt\n\n        sentences = [prompt]\n\n        inputs = tokenizer(\n        sentences, \n        return_tensors=\"pt\").to(device)\n\n        outputs = model.generate(inputs[\"input_ids\"],\n            max_length=5000,\n        )\n\n        tgt_text = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)[0]\n\n        # We are not allowed to generate more than 250 characters:\n        tgt_text = tgt_text[:250]\n        \n        qa_pair = [tgt_text.strip().split(\"?\")[0].replace(\"\\n\", \" \") + \"?\",  doc[1].replace(\"\\n\", \" \"), doc[0]]\n\n        if not \"bm25_qas\" in example:\n            example[\"bm25_qas\"] = []\n\n        example[\"bm25_qas\"].append(qa_pair)\n\nprint(json.dumps(examples[args.start_idx:args.start_idx+args.n_to_compute], indent=4))"}
{"type": "source_file", "path": "retrieval_coarse/averitec_search.py", "content": "import argparse\nfrom time import sleep\nimport pandas as pd\nimport tqdm\nfrom googleapiclient.discovery import build\nimport argparse\nimport json\nfrom bs4 import BeautifulSoup\nimport requests\nfrom urllib.parse import urlparse\nimport requests\nimport io\nfrom bs4 import BeautifulSoup\nfrom html2lines import url2lines\nfrom nltk import pos_tag, word_tokenize\nimport threading\nimport gc\nimport os\n\nparser = argparse.ArgumentParser(description='Download and store search pages for FCB files.')\nparser.add_argument('--averitec_file', default=\"data/dev.generated_questions.json\", help='')\nparser.add_argument('--misinfo_file', default=\"data/misinfo_list.txt\", help='')\nparser.add_argument('--n_pages', default=3, help='')\nparser.add_argument('--store_folder', default=\"store/retrieved_docs\", help='')\nparser.add_argument('--start_idx', default=0, type=int, help='Which claim to start at. Useful for larger corpus.')\nparser.add_argument('--n_to_compute', default=-1, type=int, help='How many claims to work through. Useful for larger corpus.')\nparser.add_argument('--resume', default=\"\", help='Resume work from a particular file. Useful for larger corpus.')\nargs = parser.parse_args()\n\nexisting = {}\nfirst = True\nif args.resume != \"\":\n    next_claim = {\"claim\": None}\n    for line in open(args.resume, \"r\"):\n        # skip the first line\n        if first:\n            first = False\n            continue\n\n        parts = line.strip().split(\"\\t\")\n        claim = parts[1]\n\n        if claim != next_claim[\"claim\"]: # Bit of a hack but I am intertionally causing a fencepost error here to rebuild the last claim, as we do not know if it was finished or not\n            if next_claim[\"claim\"] is not None:\n                existing[next_claim[\"claim\"]] = next_claim\n            next_claim = {\"claim\": claim, \"lines\": []}\n\n        next_claim[\"lines\"].append(line.strip())\n\nif not os.path.exists(args.store_folder):\n    os.makedirs(args.store_folder)\n\napi_key = \"YOUR_GOOGLE_CSE_API_KEY\"\nsearch_engine_id = \"YOUR_CSE_ID\"\n\nstart_idx = 0\nmisinfo_list_file = args.misinfo_file\nmisinfo_list = []\n\nblacklist = [\n    \"jstor.org\", # Blacklisted because their pdfs are not labelled as such, and clog up the download\n    \"facebook.com\", # Blacklisted because only post titles can be scraped, but the scraper doesn't know this,\n    \"ftp.cs.princeton.edu\", # Blacklisted because it hosts many large NLP corpora that keep showing up\n    \"nlp.cs.princeton.edu\",\n    \"huggingface.co\"\n]\n\nblacklist_files = [ # Blacklisted some NLP nonsense that crashes my machine with OOM errors\n    \"/glove.\", \n    \"ftp://ftp.cs.princeton.edu/pub/cs226/autocomplete/words-333333.txt\",\n    \"https://web.mit.edu/adamrose/Public/googlelist\",\n]\n\n\nfor line in open(misinfo_list_file, \"r\"):\n    if line.strip():\n        misinfo_list.append(line.strip().lower())\n\ndef get_domain_name(url):\n    if '://' not in url:\n        url = 'http://' + url\n\n    domain = urlparse(url).netloc\n\n    if domain.startswith(\"www.\"):\n        return domain[4:]\n    else:\n        return domain\n\ndef google_search(search_term, api_key, cse_id, **kwargs):\n    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n\n    if \"items\" in res:\n        return res['items']\n    else:\n        return []\n\npages = 0\na_pages = 0\nfound_pages = 0\nfound_pages_1hop = 0\nn_pages = 0\n\nwith open(args.averitec_file) as f:\n    j = json.load(f)\n    examples = j\n\ndef string_to_search_query(text, author):\n    parts = word_tokenize(text.strip())\n    tags = pos_tag(parts)\n\n    keep_tags = [\"CD\", \"JJ\", \"NN\", \"VB\"]\n\n    if author is not None:\n        search_string = author.split()\n    else:\n        search_string = []\n\n    for token, tag in zip(parts, tags):\n        for keep_tag in keep_tags:\n            if tag[1].startswith(keep_tag):\n                search_string.append(token)\n\n    search_string = \" \".join(search_string)\n    return search_string\n\ndef get_google_search_results(api_key, search_engine_id, google_search, sort_date, search_string, page=0):\n    search_results = []\n    for i in range(3):\n        try:\n            search_results += google_search(\n            search_string,\n            api_key,\n            search_engine_id,\n            num=10,\n            start=0 + 10 * page,\n            sort=\"date:r:19000101:\"+sort_date,\n            dateRestrict=None,\n            gl=\"US\"\n            )\n            break\n        except:\n            sleep(3)\n\n    return search_results\n\ndef get_and_store(url_link, fp, worker):\n    page_lines = url2lines(url_link)\n\n    with open(fp, \"w\") as out_f:\n        print(\"\\n\".join([url_link] + page_lines), file=out_f)   \n\n    worker_stack.append(worker)  \n    gc.collect()\n\nline = [\"index\", \"claim\", \"link\", \"page\", \"search_string\", \"search_type\", \"store_file\"]\nline = \"\\t\".join(line)\nprint(line)\n\nworker_stack = list(range(10))\n\nend_idx = -1\nif args.n_to_compute != -1:\n    end_idx = args.start_idx+args.n_to_compute\n\nindex = args.start_idx -1\nfor _, example in tqdm.tqdm(list(enumerate(examples[args.start_idx:end_idx]))):\n    index += 1\n    claim = example[\"claim\"]\n\n    # If we already have this claim in the file we are resuming from, skip it\n    if claim in existing:\n        for line in existing[claim][\"lines\"]:\n            print(line)\n        continue\n\n    speaker = example[\"speaker\"].strip() if example[\"speaker\"] else None\n\n    questions = [q[\"question\"] for q in example[\"questions\"]]\n\n    try:\n        year, month, date = example[\"check_date\"].split(\"-\")\n    except:\n        month, date, year = \"01\", \"01\", \"2022\"\n    \n    if len(year) == 2 and int(year) <= 30:\n        year = \"20\" + year\n    elif len(year) == 2:\n        year = \"19\" + year\n    elif len(year) == 1:\n        year = \"200\" + year\n\n    if len(month) == 1:\n        month = \"0\" + month\n    \n    if len(date) == 1:\n        date = \"0\" + date\n\n    sort_date = year + month + date\n    \n    search_strings = []\n    search_types = []\n\n    if speaker is not None:\n        search_string = string_to_search_query(claim, speaker)\n        search_strings.append(search_string)\n        search_types += [\"claim+author\"]\n\n    search_string_2 = string_to_search_query(claim, None)\n    \n    search_strings += [\n        search_string_2,\n        claim,\n        ]\n\n    search_types += [\n        \"claim\",\n        \"claim-noformat\",\n        ]\n\n    search_strings += questions\n    search_types += [\"question\" for _ in questions]\n\n    search_results = []\n    visited = {}\n    \n    store_counter = 0\n    ts = []\n    for this_search_string, this_search_type in zip(search_strings, search_types):\n        for page_num in range(args.n_pages):\n            search_results = get_google_search_results(api_key, search_engine_id, google_search, sort_date, this_search_string, page=page_num)\n\n            for result in search_results:\n                link = str(result[\"link\"])\n\n                domain = get_domain_name(link)\n\n                if domain in blacklist:\n                    continue\n\n                broken = False\n                for b_file in blacklist_files:\n                    if b_file in link:\n                        broken = True\n        \n                if broken:\n                    continue\n\n                if link.endswith(\".pdf\") or link.endswith(\".doc\"):\n                    continue\n\n                store_file_path = \"\"\n\n                if link in visited:\n                    store_file_path = visited[link]\n                else:\n                    store_counter += 1\n\n                    store_file_path = args.store_folder + \"/search_result_\" + str(index) + \"_\" + str(store_counter) + \".store\"\n                    visited[link] = store_file_path \n\n                    while len(worker_stack) == 0: # Wait for a wrrker to become available. Check every second.\n                        sleep(1)\n\n                    worker = worker_stack.pop()\n\n                    t = threading.Thread(target=get_and_store, args=(link, store_file_path, worker))\n                    t.start()\n                            \n    \n                line = [str(index), claim, link, str(page_num), this_search_string, this_search_type, store_file_path]\n                line = \"\\t\".join(line)\n                print(line)"}
{"type": "source_file", "path": "veracity_prediction/no_evidence.py", "content": "import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport os\nimport sys\nimport argparse\nimport json\nimport tqdm\nimport torch\n\nsys.path.insert(1, os.path.join(sys.path[0], '..'))\n\nfrom data_loaders.NoEvidenceDataLoader import NoEvidenceDataLoader\n\nfrom models.NaiveSeqClassModule import NaiveSeqClassModule\n\nsys.path.insert(1, os.path.join(sys.path[0], '..'))\n\nfrom data_loaders.SequenceClassificationDataLoader import SequenceClassificationDataLoader\nfrom models.SequenceClassificationModule import SequenceClassificationModule\nfrom utils import compute_all_pairwise_edit_distances, compute_all_pairwise_meteor_scores, delete_if_exists, edit_distance_custom, edit_distance_dbscan\nfrom datasets import ClassLabel\n\nparser = argparse.ArgumentParser(description='Perform veracity prediction using a sequence classifier without any evidence')\nparser.add_argument('--averitec_file', default=\"data/dev.json\", help='')\nparser.add_argument('--train', action='store_true', help='Marks that training should happen. Otherwise, only inference is executed.')\nparser.add_argument('--gpus', default=1, help='The number of available GPUs')\nargs = parser.parse_args()\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4, problem_type=\"single_label_classification\") # Must specify single_label for some reason\n\nmodel = NaiveSeqClassModule(tokenizer = tokenizer, model = bert_model, learning_rate = 1e-5)\n\ndataLoader = NoEvidenceDataLoader(\n  tokenizer = tokenizer, \n  data_file = \"this_is_discontinued\", \n  batch_size = 32,\n  add_extra_nee=True\n  )\n\nexperiment_name = \"bert_veracity-no_evidence\"\n\ncheckpoint = ModelCheckpoint(\n  dirpath='/rds/user/mss84/hpc-work/checkpoint_files/averitec',\n  filename=experiment_name+\"-{epoch:02d}-{val_loss:.2f}-{val_f1_epoch:.2f}\", \n  save_top_k=1, \n  monitor=\"val_f1_epoch\",\n  mode=\"max\"\n  )\n\nlogger = pl.loggers.TensorBoardLogger(\n                save_dir=os.getcwd(),\n                version=experiment_name,\n                name='lightning_logs'\n            )\n\ntrainer = pl.Trainer(gpus=args.gpus,\n  max_epochs=20,\n  min_epochs=20,\n  auto_lr_find=False,\n  progress_bar_refresh_rate=1,\n  callbacks=[checkpoint],\n  logger=logger,\n  accumulate_grad_batches=4,\n  strategy=\"dp\", #I tried ddp, it breaks\n  num_nodes=1\n)\n\nif args.train:\n    trainer.validate(model, dataLoader) # This makes pytorch lightning log initial values for dev loss etc. Nice for tensorboard.\n    trainer.fit(model, dataLoader)\n    best_checkpoint = checkpoint.best_model_path\n    print(\"Finished training. The best checkpoint is stored at '\" + best_checkpoint + \"'.\")\nelse:\n    best_checkpoint = \"/rds/user/mss84/hpc-work/checkpoint_files/averitec/bert_averitec_naive_extra_nee-epoch=16-val_loss=0.00-val_acc=0.00.ckpt\"\n\ntrained_model = SequenceClassificationModule.load_from_checkpoint(best_checkpoint, tokenizer = tokenizer, model = bert_model)\n\n\nif args.train:\n    print(\"Running inference...\")\n    trainer.test(trained_model, dataLoader)\nelse:\n    with open(args.averitec_file) as f:\n        examples = json.load(f)\n\n    for example in tqdm.tqdm(examples):\n        example_strings = [example[\"claim\"]]\n\n        tokenized_strings, attention_mask = dataLoader.tokenize_strings(example_strings)\n        example_support = torch.argmax(trained_model(tokenized_strings, attention_mask=attention_mask).logits, axis=1)\n        \n        label_map = {\n          0: \"Supported\",\n          1: \"Refuted\",\n          2: \"Not Enough Evidence\",\n          3: \"Conflicting Evidence/Cherrypicking\"\n          }\n        example[\"label\"] = label_map[int(example_support)]\n\n    print(json.dumps(examples, indent=4))"}
{"type": "source_file", "path": "veracity_prediction/veracity_prediction.py", "content": "import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport os\nimport sys\nimport argparse\nimport json\nimport tqdm\nimport torch\n\nsys.path.insert(1, os.path.join(sys.path[0], '..'))\n\nfrom data_loaders.SequenceClassificationDataLoader import SequenceClassificationDataLoader\nfrom models.SequenceClassificationModule import SequenceClassificationModule\nfrom utils import compute_all_pairwise_edit_distances, compute_all_pairwise_meteor_scores, delete_if_exists, edit_distance_custom, edit_distance_dbscan\nfrom datasets import ClassLabel\n\nparser = argparse.ArgumentParser(description='Perform veracity prediction using a stance detection model..')\nparser.add_argument('--averitec_file', default=\"data/dev.json\", help='')\nparser.add_argument('--train', action='store_true', help='Marks that training should happen. Otherwise, only inference is executed.')\nparser.add_argument('--gpus', default=1, help='The number of available GPUs')\nargs = parser.parse_args()\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4, problem_type=\"single_label_classification\") # Must specify single_label for some reason\n\nmodel = SequenceClassificationModule(tokenizer = tokenizer, model = bert_model, learning_rate = 1e-5)\n\ndataLoader = SequenceClassificationDataLoader(\n  tokenizer = tokenizer, \n  data_file = \"this_is_discontinued\", \n  batch_size = 32,\n  add_extra_nee=False\n  )\n\nexperiment_name = \"bert_veracity-true\"\n\ncheckpoint = ModelCheckpoint(\n  dirpath='/rds/user/mss84/hpc-work/checkpoint_files/averitec',\n  filename=experiment_name+\"-{epoch:02d}-{val_loss:.2f}-{val_f1_epoch:.2f}\", \n  save_top_k=1, \n  monitor=\"val_f1_epoch\",\n  mode=\"max\"\n  )\n\nlogger = pl.loggers.TensorBoardLogger(\n                save_dir=os.getcwd(),\n                version=experiment_name,\n                name='lightning_logs'\n            )\n\ntrainer = pl.Trainer(gpus=args.gpus,\n  max_epochs=20,\n  min_epochs=20,\n  auto_lr_find=False,\n  progress_bar_refresh_rate=1,\n  callbacks=[checkpoint],\n  logger=logger,\n  accumulate_grad_batches=4,\n  strategy=\"dp\", #I tried ddp, it breaks\n  num_nodes=1\n)\n\nif args.train:\n    trainer.validate(model, dataLoader) # This makes pytorch lightning log initial values for dev loss etc. Nice for tensorboard.\n    trainer.fit(model, dataLoader)\n    best_checkpoint = checkpoint.best_model_path\n    print(\"Finished training. The best checkpoint is stored at '\" + best_checkpoint + \"'.\")\nelse:\n    best_checkpoint = \"/rds/user/mss84/hpc-work/checkpoint_files/averitec/bert_veracity-true-epoch=06-val_loss=0.00-val_f1_epoch=0.51.ckpt\"\n\ntrained_model = SequenceClassificationModule.load_from_checkpoint(best_checkpoint, tokenizer = tokenizer, model = bert_model)\n\n\nif args.train:\n    print(\"Running inference...\")\n    trainer.test(trained_model, dataLoader)\nelse:\n    with open(args.averitec_file) as f:\n        examples = json.load(f)\n\n    for example in tqdm.tqdm(examples):\n        example_strings = []\n        for question in example[\"questions\"]:\n            for answer in question[\"answers\"]:\n                 example_strings.append(dataLoader.quadruple_to_string(example[\"claim\"], question[\"question\"], answer[\"answer\"], \"\"))\n\n        if len(example_strings) == 0: # If we found no evidence e.g. because google returned 0 pages, just output NEI.\n          example[\"label\"] = \"Not Enough Evidence\"\n          continue\n\n        tokenized_strings, attention_mask = dataLoader.tokenize_strings(example_strings)\n        example_support = torch.argmax(trained_model(tokenized_strings, attention_mask=attention_mask).logits, axis=1)\n\n        has_unansw = False\n        has_true = False\n        has_false = False\n\n        for v in example_support:\n          if v == 0:\n            has_true = True\n          if v == 1:\n            has_false = True\n          if v == 2 or v == 3: # TODO another hack -- we cant have different labels for train and test so we do this\n            has_unansw = True\n\n        if has_unansw:\n          answer = 2\n        elif has_true and not has_false:\n          answer = 0\n        elif has_false and not has_true:\n          answer = 1\n        elif has_true and has_false:\n          answer = 3\n\n        label_map = {\n          0: \"Supported\",\n          1: \"Refuted\",\n          2: \"Not Enough Evidence\",\n          3: \"Conflicting Evidence/Cherrypicking\"\n          }\n\n        example[\"label\"] = label_map[answer]\n\nprint(json.dumps(examples, indent=4))"}
